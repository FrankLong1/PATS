unique_id,year,title,authors,pdf_url,paper_text,abstract
1511.06279,2016,Neural Programmer-Interpreters,"['Neural Programmer-Interpreters\nScott Reed', 'Nando de Freitas']",https://arxiv.org/pdf/1511.06279,"6 1 0 2     b e F 9 2         ]  G L . s c [      4 v 9 7 2 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  NEURAL PROGRAMMER-INTERPRETERS  Scott Reed & Nando de Freitas Google DeepMind London, UK scott.ellison.reed@gmail.com nandodefreitas@google.com  ABSTRACT  We propose the neural programmer-interpreter (NPI): a recurrent and composi- tional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value pro- gram memory, and domain-speciﬁc encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to- sequence LSTMs. The program memory allows efﬁcient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of com- putation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these pro- grams and all 21 associated subprograms.  INTRODUCTION  1 Teaching machines to learn new programs, to rapidly compose new programs from existing pro- grams, and to conditionally execute these programs automatically so as to solve a wide variety of tasks is one of the central challenges of AI. Programs appear in many guises in various AI prob- lems; including motor behaviours, image transformations, reinforcement learning policies, classical algorithms, and symbolic relations. In this paper, we develop a compositional architecture that learns to represent and interpret pro- grams. We refer to this architecture as the Neural Programmer-Interpreter (NPI). The core module is an LSTM-based sequence model that takes as input a learnable program embedding, program arguments passed on by the calling program, and a feature representation of the environment. The output of the core module is a key indicating what program to call next, arguments for the following program and a ﬂag indicating whether the program should terminate. In addition to the recurrent core, the NPI architecture includes a learnable key-value memory of program embeddings. This program-memory is essential for learning and re-using programs in a continual manner. Figures 1 and 2 illustrate the NPI on two different tasks. We show in our experiments that the NPI architecture can learn 21 programs, including addition, sorting, and trajectory planning from image pixels. Crucially, this can be achieved using a single core model with the same parameters shared across all tasks. Different environments (for example images, text, and scratch-pads) may require speciﬁc perception modules or encoders to produce the features used by the shared core, as well as environment-speciﬁc actuators. Both perception modules and actuators can be learned from data when training the NPI architecture. To train the NPI we use curriculum learning and supervision via example execution traces. Each program has example sequences of calls to the immediate subprograms conditioned on the input.  1  Published as a conference paper at ICLR 2016  Figure 1: Example execution of canonicalizing 3D car models. The task is to move the camera such that a target angle and elevation are reached. There is a read-only scratch pad containing the target (angle 1, elevation 2 here). The image encoder is a convnet trained from scratch on pixels.  Figure 2: Example execu- tion trace of single-digit addi- tion. The task is to perform a single-digit add on the num- bers at pointer locations in the ﬁrst two rows. The carry (row 3) and output (row 4) should be updated to reﬂect the addi- tion. At each time step, an ob- servation of the environment (viewed from each pointer on a scratch pad) is encoded into a ﬁxed-length vector.  By using neural networks to represent the subprograms and learning these from data, the approach can generalize on tasks involving rich perceptual inputs and uncertainty. We may envision two approaches to provide supervision. In one, we provide a very large number of labeled examples, as in object recognition, speech and machine translation. In the other, the approached followed in this paper, the aim is to provide far fewer labeled examples, but where the labels contain richer information allowing the model to learn compositional structure. While unsupervised and reinforcement learning play important roles in perception and motor control, other cognitive abilities are possible thanks to rich supervision and curriculum learning. This is indeed the reason for sending our children to school. An advantage of our approach to model building and training is that the learned programs exhibit strong generalization. Speciﬁcally, when trained to sort sequences of up to twenty numbers in length, they can sort much longer sequences at test time. In contrast, the experiments will show that more standard sequence to sequence LSTMs only exhibit weak generalization, see Figure 6. A trained NPI with ﬁxed parameters and a learned library of programs, can act both as an interpreter and as a programmer. As an interpreter, it takes input in the form of a program embedding and input data and subsequently executes the program. As a programmer, it uses samples drawn from a new task to generate a new program embedding that can be added to its library of programs.  2 RELATED WORK Several ideas related to our approach have a long history. For example, the idea of using dynam- ically programmable networks in which the activations of one network become the weights (the  2  INPUTGOTOKEYARGENDhMkeyMprogGOTO()HGOTO()LGOTO()VGOTO()LGOTO()ACT(LEFT)DGOTO()ACT(DOWN)ACT(LEFT)end state..................HGOTO121212GOTO()1212121212121212INPUTHGOTOKEYARGENDhLGOTOINPUTLGOTOKEYARGENDhACTINPUTACTKEYARGENDhACTINPUTLGOTOKEYARGENDhINPUTACTKEYARGENDhVGOTOINPUTGOTOKEYARGENDhINPUTVGOTOKEYARGENDhINPUTDGOTOKEYARGENDhDGOTOACTINPUTACTKEYARGENDh9343489343482ADD1()ACT (4,2,WRITE) 9343482CARRY()9343482ACT (3,LEFT)93434812ACT (3,1,WRITE)9343482ADD1()9343482CARRY()INPUTADD1KEYARGENDhACTMkeyMprogINPUTACTKEYARGENDhINPUTADD1KEYARGENDhINPUTCARRYKEYARGENDhCARRYACTINPUTACTKEYARGENDhINPUTCARRYKEYARGENDhACTINPUTACTKEYARGENDhPublished as a conference paper at ICLR 2016  program) of a second network was mentioned in the Sigma-Pi units section of the inﬂuential PDP paper (Rumelhart et al., 1986). This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Related problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al. (1999) and Schaul et al. (2015)), imitation and apprenticeship learning (e.g., Kolter et al. (2008) and Rothkopf & Ballard (2013)) and elicita- tion of options through human interaction (Subramanian et al., 2011). These ideas have held great promise, but have not enjoyed signiﬁcant impact. We believe the recurrent compositional neural representations proposed in this paper could help these approaches in the future, and in particular in overcoming feature engineering. Several recent advancements have extended recurrent networks to solve problems beyond simple sequence prediction. Graves et al. (2014) developed a neural Turing machine capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Vinyals et al. (2015) developed Pointer Networks that generalize the notion of encoder attention in order to provide the decoder a variable-sized output space depending on the input sequence length. This model was shown to be effective for combinatorial optimization problems such as the traveling salesman and Delaunay triangulation. While our proposed model is trained on execution traces in- stead of input and output pairs, in exchange for this richer supervision we beneﬁt from compositional program structure, improving data efﬁciency on several problems. This work is also closely related to program induction. Most previous work on program induc- tion, i.e. inducing a program given example input and output pairs, has used genetic program- ming (Banzhaf et al., 1998) to evolve useful programs from candidate populations. Mou et al. (2014) process program symbols to learn max-margin program embeddings with the help of parse trees. Zaremba & Sutskever (2014) trained LSTM models to read in the text of simple programs character-by-character and correctly predict the program output. Joulin & Mikolov (2015) aug- mented a recurrent network with a pushdown stack, allowing for generalization to longer input sequences than seen during training for several algorithmic patterns. Contemporary to this work, several papers have also studied program induction with variants of recurrent neural networks (Zaremba & Sutskever, 2015; Zaremba et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Neelakantan et al., 2015). While we share a similar motivation, our approach is distinct in that we explicitly incorporate compositional structure into the network using a program memory, allowing the model to learn new programs by combining sub-programs.  3 MODEL The NPI core is a long short-term memory (LSTM) network (Hochreiter & Schmidhuber, 1997) that acts as a router between programs conditioned on the current state observation and previous hidden unit states. At each time step, the core module can select another program to invoke using content-based addressing. It emits the probability of ending the current program with a single binary unit. If this probability is over threshold (we used 0.5), control is returned to the caller by popping the caller’s LSTM hidden units and program embedding off of a program call stack and resuming execution in this context. The NPI may also optionally write arguments (ARG) that are passed by reference or value to the invoked sub-programs. For example, an argument could indicate a speciﬁc location in the input sequence (by reference), or it could specify a number to write down at a particular location in the sequence (by value). The subsequent state consists of these arguments and observations of the environment. The approach is illustrated in Figures 1 and 2. It must be emphasized that there is a single inference core. That is, all the LSTM instantiations executing arbitrary programs share the same parameters. Different programs correspond to program embeddings, which are stored in a learnable persistent memory. The programs therefore have a more  3  Published as a conference paper at ICLR 2016  INFERENCE  succinct representation than neural programs encoded as the full set of weights in a neural network (Rumelhart et al., 1986; Graves et al., 2014). The output of an NPI, conditioned on an input state and a program to run, is a sequence of actions in a given environment. In this work, we consider several environments: a 1-D array with read-only pointers and a swap action, a 2-D scratch pad with read-write pointers, and a CAD renderer with controllable elevation and azimuth movements. Note that the sequence of actions for a program is not ﬁxed, but dependent also on the input state. 3.1 Denote the environment observation at time t as et ∈ E, and the current program arguments as at ∈ A. The form of et can vary dramatically by environment; for example it could be a color image or an array of numbers. The program arguments at can also vary by environment, but in the experiments for this paper we always used a 3-tuple of integers (at(1), at(2), at(3)). Given the environment and arguments at time t, a ﬁxed-length state encoding st ∈ RD is extracted by a domain-speciﬁc encoder fenc : E×A → RD. In section 4 we provide examples of several encoders. Note that a single NPI network can have multiple encoders for multiple environments, and encoders can potentially also be shared across tasks. We denote the current program embedding as pt ∈ RP . The previous hidden unit and cell states t−1 ∈ RM , l = 1, ..., L where L is the number of layers in the LSTM. are h(l) The program and state vectors are then propagated forward through an LSTM mapping flstm as in (Sutskever et al., 2014). How to fuse pt and st within flstm is an implementation detail, but in this work we concatenate and feed through a 2-layer MLP with rectiﬁed linear (ReLU) hidden activation and linear decoder. t , several decoders generate the outputs. The probability of From the top LSTM hidden state hL ﬁnishing the program and returning to the caller 1 is computed by fend : RM → [0, 1]. The lookup key embedding used for retrieving the next program from memory is computed by fprog : RM → RK. Note that RK can be much smaller than RP because the key only need act as the identiﬁer of a program, while the program embedding must have enough capacity to conditionally generate a sequence of actions. The contents of the arguments to the next program to be called are generated by farg : RM → A. The feed-forward steps of program inference are summarized below:  t−1 ∈ RM and c(l)  st = fenc(et, at) ht = flstm(st, pt, ht−1) rt = fend(ht), kt = fprog(ht), at+1 = farg(ht)  (1) (2) (3) where rt, kt and at+1 correspond to the end-of-program probability, program key embedding, and output arguments at time t, respectively. These yield input arguments at time t + 1. To simplify the notation, we have abstracted properties such as layers and cell memory in the sequence-to-sequence LSTM of equation (2); see (Sutskever et al., 2014) for details. The NPI representation is equipped with key-value memory structures M key ∈ RN×K and M prog ∈ RN×P storing program keys and program embeddings, respectively, where N is the current number of programs in memory. We can add more programs by adding rows to memory. During training, the next program identiﬁer is provided to the model as ground-truth, so that its embedding can be retrieved from the corresponding row of M prog. At test time, we compute the “program ID” by comparing the key embedding kt to each row of M key storing all program keys. Then the program embedding is retrieved from M prog as follows:  i∗ = arg max  i=1..N  (M key  i,: )T kt , pt+1 = M prog i∗,:  (4)  The next environmental state et+1 will be determined by the dynamics of the environment and can be affected by both the choice of program pt and the contents of the output arguments at, i.e.  (5) The transition mapping fenv is domain-speciﬁc and will be discussed in Section 4. A description of the inference procedure is given in Algorithm 1.  et+1 ∼ fenv(et, pt, at)  1In our implementation, a program may ﬁrst call a subprogram before itself ﬁnishing. The only exception is the ACT program that signals a low-level action to the environment, e.g. moving a pointer one step left or writing a value. By convention ACT does not call any further sub-programs.  4  Published as a conference paper at ICLR 2016  Algorithm 1 Neural programming inference 1: Inputs: Environment observation e, program id i, arguments a, stop threshold α 2: function RUN(i, a) 3: 4: 5: 6: 7:  h ← 0, r ← 0, p ← M prog while r < α do  s ← fenc(e, a), h ← flstm(s, p, h) r ← fend(h), k ← fprog(h), a2 ← farg(h) i2 ← arg max if i == ACT then e ← fenv(e, p, a) else RUN(i2, a2)  j,: )T k  (M key  j=1..N  8: 9:  i,:  (cid:46) Decide the next program to run.  (cid:46) Update the environment based on ACT. (cid:46) Run subprogram i2 with arguments a2  (cid:46) Init LSTM and return probability.  (cid:46) Feed-forward NPI one step.  θ  (ξinp,ξout)  (cid:88)  T(cid:88)  Each task has a set of actions that affect the environment. For example, in addition there are LEFT and RIGHT actions that move a speciﬁed pointer, and a WRITE action which writes a value at a speciﬁed location. These actions are encapsulated into a general-purpose ACT program shared across tasks, and the concrete action to be taken is indicated by the NPI-generated arguments at. Note that the core LSTM module of our NPI representation is completely agnostic to the data modal- ity used to produce the state encoding. As long as the same ﬁxed-length embedding is extracted, the same module can in practice route between programs related to sorting arrays just as easily as between programs related to rotating 3D objects. In the experimental sections, we provide details of the modality-speciﬁc deep neural networks that we use to produce these ﬁxed-length state vectors. 3.2 TRAINING : {it+1, at+1, rt}, t = 1, ...T , where T is To train we use execution traces ξinp the sequence length. Program IDs it and it+1 are row-indices in M key and M prog of the programs to run at time t and t+1, respectively. We propose to directly maximize the probability of the correct execution trace output ξout conditioned on ξinp:  : {et, it, at} and ξout  t  t  θ∗ = arg max  log P (ξout|ξinp; θ)  (6)  where θ are the parameters of our model. Since the traces are variable in length depending on the input, we apply the chain rule to model the joint probability over ξout  as follows:  , ..., ξout  1  T  log P (ξout|ξinp; θ) =  log P (ξout  t  |ξinp  1  , ..., ξinp  t  ; θ)  (7)  t=1  t  t  t  1  1  , ..., ξinp  , ..., ξinp  log P (ξout  ) = log P (it+1|ht) + log P (at+1|ht) + log P (rt|ht)  Note that for many problems the input history ξinp is critical to deciding future actions because the environment observation at the current time-step et alone does not contain enough in- formation. The hidden unit activations of the LSTM in NPI are capable of capturing these temporal dependencies. The single-step conditional probability in equation (7) can be factorized into three further conditional distributions, corresponding to predicting the next program, next arguments, and whether to halt execution: |ξinp  (8) where ht is the output of flstm at time t, carrying information from previous time steps. We train by gradient ascent on the likelihood in equation (7). We used an adaptive curriculum in which training examples for each mini-batch are fetched with fre- quency proportional to the model’s current prediction error for the corresponding program. Specif- ically, we set the sampling frequency using a softmax over average prediction error across all pro- grams, with conﬁgurable temperature. Every 1000 steps of training we re-estimated these prediction errors. Intuitively, this forces the model to focus on learning the program for which it currently per- forms worst in executing. We found that the adaptive curriculum immediately worked much better than our best-performing hand-designed curriculum, allowing a multi-task NPI to achieve compara- ble performance to single-task NPI on all tasks. We also note that our program has a distinct memory advantage over basic LSTMs because all sub- programs can be trained in parallel. For programs whose execution length grows e.g. quadratically  5  Published as a conference paper at ICLR 2016  Figure 3: Illustration of the addition environment used in our experiments.  (a) Example scratch pad and pointers used for computing “96 + 125 = 221”. Carry step is being implemented.  (b) Actual trace of addition program generated by our model on the problem shown to the left. Note that we substituted the ACT calls in the trace with more human-readable steps.  with the input sequence length, an LSTM will by highly constrained by device memory to train on short sequences. By exploiting compositionality, an effective curriculum can often be developed with sublinear-length subprograms, enabling our NPI model to train on order of magnitude larger sequences than the LSTM.  4 EXPERIMENTS This section describes the environment and state encoder function for each task, and shows example outputs and prediction accuracy results. For all tasks, the core LSTM had two layers of size 256. We trained the NPI using the ADAM solver (Kingma & Ba, 2015) with base learning rate 0.0001, batch size 1, and decayed the learning rate by a factor of 0.95 every 10,000 steps.  4.1 TASK AND ENVIRONMENT DESCRIPTIONS In this section we provide an overview of the tasks used to evaluate our model. Table 2 in the appendix provides a full listing of all the programs and subprograms learned by our model.  ADDITION  The task in this environment is to read in the digits of two base-10 numbers and produce the digits of the answer. Our goal is to teach the model the standard (at least in the US) grade school algorithm of adding, in which one works from right to left applying single-digit add and carry operations. In this environment, the network is endowed with a “scratch pad” with which to store intermediate computations; e.g. to record carries. There are four pointers; one for each of the two input numbers, one for the carry, and another to write the output. At each time step, a pointer can be moved left or right, or it can record a value to the pad. Figure 3a illustrates the environment of this model, and Figure 3b provides a real execution trace generated by our model. For the state encoder fenc, the model is allowed a view of the scratch pad from the perspective of each of the four pointers. That is, the model sees the current values at pointer locations of the two inputs, the carry row and the output row, as 1-of-K encodings, where K is 10 because we are working in base 10. We also append the values of the input argument tuple at:  fenc(Q, i1, i2, i3, i4, at) = M LP ([Q(1, i1), Q(2, i2), Q(3, i3), Q(4, i4), at(1), at(2), at(3)]) (9) where Q ∈ R4×N×K, and i1, ..., i4 are pointers, one per scratch pad row. The ﬁrst dimension of Q corresponds to scratch pad rows, N is the number of columns (digits) and K is the one-hot encoding dimension. To begin the ADD program, we set the initial arguments to a default value and initialize all pointers to be at the rightmost column. The only subprogram with non-default arguments is ACT, in which case the arguments indicate an action to be taken by a speciﬁed pointer.  SORTING  In this section we apply our model to a setting with potentially much longer execution traces: sorting an array of numbers using bubblesort. As in the case of addition we can use a scratch pad to store intermediate states of the array. We deﬁne the encoder as follows:  fenc(Q, i1, i2, at) = M LP ([Q(1, i1), Q(1, i2), at(1), at(2), at(3)])  (10)  6  input 1input 2carryoutput00096001250011100021ADD  ADD1    WRITE OUT 1    CARRY      PTR CARRY LEFT      WRITE CARRY 1      PTR CARRY RIGHT  LSHIFT    PTR INP1 LEFT    PTR INP2 LEFT    PTR CARRY LEFT    PTR OUT LEFT  ADD1    WRITE OUT 2    CARRY      PTR CARRY LEFT      WRITE CARRY 1      PTR CARRY RIGHT  LSHIFT    PTR INP1 LEFT    PTR INP2 LEFT    PTR CARRY LEFT    PTR OUT LEFT  ADD1    WRITE OUT 2  LSHIFT    PTR INP1 LEFT    PTR INP2 LEFT    PTR CARRY LEFT    PTR OUT LEFTPublished as a conference paper at ICLR 2016  Figure 4: Illustration of the sorting environment used in our experiments.  (a) Example scratch pad and pointers used for sorting. Several steps of the BUBBLE subprogram are shown.  (b) Excerpt from the trace of the learned bubblesort program.  where Q ∈ R1×N×K is the pad, N is the array length and K is the array entry embedding dimension. Figure 4 shows an example series of array states and an excerpt of an execution trace.  CANONICALIZING 3D MODELS  We also apply our model to a vision task with a very different perceptual environment - pixels. Given a rendering of a 3D car, we would like to learn a visual program that “canonicalizes” the model with respect to its pose. Whatever the starting position, the program should generate a trajectory of actions that delivers the camera to the target view, e.g. frontal pose at a 15◦ elevation. For training data, we used renderings of the 3D car CAD models from (Fidler et al., 2012). This is a nontrivial problem because different starting positions will require quite different trajec- tories to reach the target. Further complicating the problem is the fact that the model will need to generalize to different car models than it saw during training. We again use a scratch pad, but here it is a very simple read-only pad that only contains a target camera elevation and azimuth – i.e., the “canonical pose”. Since observations come in the form of image pixels, we use a convolutional neural network fCN N as the image encoder:  fenc(Q, x, i1, i2, at) = M LP ([Q(1, i1), Q(2, i2), fCN N (x), at(1), at(2), at(3)])  (11) where x ∈ RH×W×3 is a car rendering at the current pose, Q ∈ R2×1×K is the pad containing canonical azimuth and elevation, i1, i2 are the (ﬁxed at 1) pointer locations, and K is the one-hot encoding dimension of pose coordinates. We set K = 24 corresponding to 15◦ pose increments. Note, critically, that our NPI model only has access to pixels of the rendering and the target pose, and is not provided the pose of query frames. We are also aware that one solution to this problem would be to train a pose classiﬁer network and then ﬁnd the shortest path to canonical pose via classical methods. That is also a sensible approach. However, our purpose here is to show that our method generalizes beyond the scratch pad domain to detailed images of 3D objects, and also to other environments with a single multi-task model.  4.2 SAMPLE COMPLEXITY AND GENERALIZATION Both LSTMs and Neural Turing Machines can learn to perform sorting to a limited degree, although they have not been shown to generalize well to much longer arrays than were seen during training. However, we are interested not only in whether sorting can be accomplished, but whether a particular sorting algorithm (e.g. bubblesort) can be learned by the model, and how effectively in terms of sample complexity and generalization. We compare the generalization ability of our model to a ﬂat sequence-to-sequence LSTM (Sutskever et al., 2014), using the same number of layers (2) and hidden units (256). Note that a ﬂat 2 version of NPI could also learn sorting of short arrays, but because bubblesort runs in O(N 2) for arrays of length N, the execution traces quickly become far too long to store the required number of LSTM states in memory. Our NPI architecture can train on much larger arrays by exploiting compositional structure; the memory requirements of any given subprogram can be restricted to O(N ).  2By ﬂat in this case, we mean non-compositional, not making use of subprograms, and only making calls  to ACT in order to swap values and move pointers.  7  t=032491324912349123491t=1t=2t=3arrayBUBBLESORT  BUBBLE    PTR 2 RIGHT    BSTEP      COMPSWAP        SWAP 1 2      RSHIFT        PTR 1 RIGHT        PTR 2 RIGHT    …    BSTEP      COMPSWAP              RSHIFT        PTR 1 RIGHT        PTR 2 RIGHT   RESET        …    LSHIFT      PTR 1 LEFT      PTR 2 LEFT    LSHIFT      PTR 1 LEFT      PTR 2 LEFT     …     LSHIFT      PTR 1 LEFT      PTR 2 LEFTBUBBLE         …     PTR 2 RIGHT    BSTEP      COMPSWAP        SWAP 1 2      RSHIFT        PTR 1 RIGHT        PTR 2 RIGHT   ...    BSTEP      COMPSWAP              RSHIFT        PTR 1 RIGHT        PTR 2 RIGHTPublished as a conference paper at ICLR 2016  Figure 5: Sample complexity. Test accuracy of sequence-to-sequence LSTM versus NPI on length-20 arrays of single-digit numbers. Note that NPI is able to mine and train on subprogram traces from each bubblesort example.  Figure 6: Strong vs. weak generalization. Test accuracy of sequence-to-sequence LSTM ver- sus NPI on varying-length arrays of single-digit numbers. Both models were trained on arrays of single-digit numbers up to length 20.  A strong indicator of whether a neural network has learned a program well is whether it can run the program on inputs of previously-unseen sizes. To evaluate this property, we train both the sequence- to-sequence LSTM and NPI to perform bubblesort on arrays of single-digit numbers from length 2 to length 20. Compared to ﬁxed-length inputs this raises the challenge level during training, but in exchange we can get a more ﬂexible and generalizable sorting program. To handle variable-sized inputs, the state representation must have some information about input se- quence length and the number of steps taken so far. For example, the main BUBBLESORT program naturally needs to call its helper function BUBBLE a number of times dependent on the sequence length. We enable this in our model by adding a third pointer that acts as a counter; each time BUB- BLE is called the pointer is advanced by one step. The scratch pad environment also provides a bit indicating whether a pointer is at the start or end of a sequence, equivalent in purpose to end tokens used in a sequence-to-sequence model. For each length, we provided 64 example bubblesort traces, for a total of 1,216 examples. Then, we evaluated whether the network can learn to sort arrays beyond length 20. We found that the trained model generalizes well, and is capable of sorting arrays up to size 60; see Figure 6. At 60 and beyond, we observed a failure mode in which sweeps of pointers across the array would take the wrong number of steps, suggesting that the limiting performance factor is related to counting. In stark contrast, when provided with the 1,216 examples, the sequence-to-sequence LSTMs fail to generalize beyond arrays of length 25 as shown in Figure 6. To study sample complexity further, we ﬁx the length of the arrays to 20 and vary the number of training examples. We see in Figure 5 that NPI starts learning with 2 examples and is able to sort almost perfectly with only 8 examples. The sequence-to-sequence model on the other hand requires 64 examples to start learning and only manages to sort well with over 250 examples. Figure 7 shows several example canonicalization trajectories generated by our model, starting from the leftmost car. The image encoder was a convolutional network with three passes of stride-2 convolution and pooling, trained on renderings of size 128 × 128. The canonical target pose in this case is frontal with 15◦ elevation. At test time, from an initial rendering, NPI is able to canonicalize cars of varying appearance from multiple starting positions. Importantly, it can generalize to car appearances not encountered in the training set as shown in Figure 7.  4.3 LEARNING NEW PROGRAMS WITH A FIXED CORE One challenge for continual learning of neural-network-based agents is that training on new tasks and experiences can lead to degraded performance in old tasks. The learning of new tasks may require that the network weights change substantially, so care must be taken to avoid catastrophic forgetting (Mccloskey & Cohen, 1989; OReilly et al., 2014). Using NPI, one solution is to ﬁx the weights of the core routing module, and only make sparse updates to the program memory. When adding a new program the core module’s routing computation will be completely unaffected; all the learning for a new task occurs in program embedding space. Of course, the addition of new programs to the memory adds a new choice of program at each time step, and an old program could  8  Training sequence lengthsPublished as a conference paper at ICLR 2016  Figure 7: Example canonicalization of several different test set cars. The network is able to generate and execute the appropriate plan based on the starting car image. This NPI was trained on trajectories starting at azimuth (−75◦...75◦) , elevation (0◦...60◦) in 15◦ increments. The training trajectories target azimuth 0◦ and elevation 15◦, as in the generated traces above.  mistakenly call a newly added program. To overcome this, when learning a new set of program vectors with a ﬁxed core, in practice we train not only on example traces of the new program, but also traces of existing programs. Alternatively, a simpler approach is to prevent existing programs from calling subsequently added programs, allowing addition of new programs without ever looking back at training data for known programs. In either case, note that only the memory slots of the new programs are updated, and all other weights, including other program embeddings, are ﬁxed. Table 1 shows the result of adding a maximum-ﬁnding program MAX to a multitask NPI trained on addition, sorting and canonicalization. MAX ﬁrst calls BUBBLESORT and then a new program RJMP, which moves pointers to the right of the sorted array, where the max element can be read. During training we froze all weights except for the two newly-added program embeddings. We ﬁnd that NPI learns MAX perfectly without forgetting the other tasks. In particular, after training a single multi-task model as outlined in the following section, learning the MAX program with this ﬁxed-core multi-task NPI results in no performance deterioration for all three tasks. 4.4 SOLVING MULTIPLE TASKS WITH A SINGLE NETWORK In this section we perform a controlled experiment to compare the performance of a multi-task NPI with several single-task NPI models. Table 1 shows the results for addition, sorting and canonical- izing 3D car models. We trained and evaluated on 10-digit numbers for addition, length-5 arrays for sorting, and up to four-step trajectories for canonicalization. As shown in Table 1, one multi-task NPI can learn all three programs (and necessarily the 21 subprograms) with comparable accuracy compared to each single-task NPI. Task Addition Sorting Canon. seen car Canon. unseen Maximum  Table 1: Per-sequence % accuracy. “+ Max” indicates performance after addition of the ad- ditional max-ﬁnding subprograms to memory. “unseen” uses a test set with disjoint car mod- els from the training set, while “seen car” uses the same car models but different trajectories.  Single Multi 97.0 100.0 100.0 100.0 91.4 89.5 88.7 89.9  + Max 97.0 100.0 91.4 89.9 100.0  -  -  5 CONCLUSION We have shown that the NPI can learn programs in very dissimilar environments with different affordances. In the context of sorting we showed that NPI exhibits very strong generalization in comparison to sequence-to-sequence LSTMs. We also showed how a trained NPI with a ﬁxed core can continue to learn new programs without forgetting already learned programs.  ACKNOWLEDGMENTS  We sincerely thank Arun Nair and Ed Grefenstette for helpful suggestions.  9  GOTO 1 2  HGOTO    LGOTO      ACT(LEFT)  VGOTO    DGOTO      ACT(DOWN)GOTO 1 2  HGOTO    RGOTO      ACT(RIGHT)      ACT(RIGHT)      ACT(RIGHT)  VGOTO    DGOTO      ACT(DOWN)      ACT(DOWN)7GOTO 1 2  HGOTO    RGOTO      ACT(RIGHT)  VGOTO    UGOTO      ACT(UP)GOTO 1 2  HGOTO    LGOTO      ACT(LEFT)      ACT(LEFT)      ACT(LEFT)      ACT(LEFT)      ACT(LEFT)  VGOTO    UGOTO      ACT(UP)123123456123123456Published as a conference paper at ICLR 2016  REFERENCES Anderson, Michael L. Neural reuse: A fundamental organizational principle of the brain. Behavioral  and Brain Sciences, 33:245–266, 8 2010.  Andre, David and Russell, Stuart J. Programmable reinforcement learning agents. In Advances in  Neural Information Processing Systems, pp. 1019–1025. 2001.  Banzhaf, Wolfgang, Nordin, Peter, Keller, Robert E, and Francone, Frank D. Genetic programming:  An introduction, volume 1. Morgan Kaufmann San Francisco, 1998.  Dietterich, Thomas G. Hierarchical reinforcement learning with the MAXQ value function decom-  position. Journal of Artiﬁcial Intelligence Research, 13:227–303, 2000.  Donnarumma, Francesco, Prevete, Roberto, and Trautteur, Giuseppe. Programming in the brain: A  neural network theoretical framework. Connection Science, 24(2-3):71–90, 2012.  Donnarumma, Francesco, Prevete, Roberto, Chersi, Fabian, and Pezzulo, Giovanni. A programmer- interpreter neural network architecture for prefrontal cognitive control. International Journal of Neural Systems, 25(6):1550017, 2015.  Fidler, Sanja, Dickinson, Sven, and Urtasun, Raquel. 3D object detection and viewpoint estimation with a deformable 3D cuboid model. In Advances in neural information processing systems, 2012.  Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural Turing machines.  arXiv:1410.5401, 2014.  arXiv preprint  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Joulin, Armand and Mikolov, Tomas. Inferring algorithmic patterns with stack-augmented recurrent  nets. In NIPS, 2015.  Kaiser, Łukasz and Sutskever, Ilya. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228,  2015.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. 2015.  Kolter, Zico, Abbeel, Pieter, and Ng, Andrew Y. Hierarchical apprenticeship learning with appli- In Advances in Neural Information Processing Systems, pp.  cation to quadruped locomotion. 769–776. 2008.  Kurach, Karol, Andrychowicz, Marcin, and Sutskever, Ilya. Neural random-access machines. arXiv  preprint arXiv:1511.06392, 2015.  Mccloskey, Michael and Cohen, Neal J. Catastrophic interference in connectionist networks: The sequential learning problem. In The psychology of learning and motivation, volume 24, pp. 109– 165. 1989.  Mou, Lili, Li, Ge, Liu, Yuxuan, Peng, Hao, Jin, Zhi, Xu, Yan, and Zhang, Lu. Building program  vector representations for deep learning. arXiv preprint arXiv:1409.3358, 2014.  Neelakantan, Arvind, Le, Quoc V, and Sutskever, Ilya. Neural programmer: Inducing latent pro-  grams with gradient descent. arXiv preprint arXiv:1511.04834, 2015.  OReilly, Randall C., Bhattacharyya, Rajan, Howard, Michael D., and Ketz, Nicholas. Complemen-  tary learning systems. Cognitive Science, 38(6):1229–1248, 2014.  Rothkopf, ConstantinA. and Ballard, DanaH. Modular inverse reinforcement learning for visuomo-  tor behavior. Biological Cybernetics, 107(4):477–490, 2013.  Rumelhart, D. E., Hinton, G. E., and McClelland, J. L. Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter A General Framework for Parallel Distributed Processing, pp. 45–76. MIT Press, 1986.  10  Published as a conference paper at ICLR 2016  Schaul, Tom, Horgan, Daniel, Gregor, Karol, and Silver, David. Universal value function approxi-  mators. In International Conference on Machine Learning, 2015.  Schmidhuber, J¨urgen. Learning to control fast-weight memories: An alternative to dynamic recur-  rent networks. Neural Computation, 4(1):131–139, 1992.  Schneider, Walter and Chein, Jason M. Controlled and automatic processing: behavior, theory, and  biological mechanisms. Cognitive Science, 27(3):525–559, 2003.  Subramanian, Kaushik, Isbell, Charles, and Thomaz, Andrea. Learning options through human  interaction. In IJCAI Workshop on Agents Learning Interactively from Human Teachers, 2011.  Sutskever, Ilya and Hinton, Geoffrey E. Using matrices to model symbolic relationship. In Advances  in Neural Information Processing Systems, pp. 1593–1600. 2009.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural net-  works. In Advances in neural information processing systems, pp. 3104–3112, 2014.  Sutton, Richard S., Precup, Doina, and Singh, Satinder. Between MDPs and semi-MDPs: A frame- work for temporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112(1-2):181– 211, 1999.  Vinyals, Oriol, Fortunato, Meire, and Jaitly, Navdeep. Pointer networks. Advances in Neural Infor-  mation Processing Systems (NIPS), 2015.  Zaremba, Wojciech and Sutskever, Ilya. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.  Zaremba, Wojciech and Sutskever, Ilya. Reinforcement learning neural turing machines. arXiv  preprint arXiv:1505.00521, 2015.  Zaremba, Wojciech, Mikolov, Tomas, Joulin, Armand, and Fergus, Rob. Learning simple algorithms  from examples. arXiv preprint arXiv:1511.07275, 2015.  11  Published as a conference paper at ICLR 2016  6 APPENDIX  6.1 LISTING OF LEARNED PROGRAMS  Below we list the programs learned by our model:  Program ADD ADD1 CARRY LSHIFT RSHIFT ACT BUBBLESORT BUBBLE RESET BSTEP COMPSWAP LSHIFT RSHIFT ACT GOTO HGOTO LGOTO RGOTO VGOTO UGOTO DGOTO ACT RJMP MAX  Descriptions Perform multi-digit addition Perform single-digit addition Mark a 1 in the carry row one unit left Shift a speciﬁed pointer one step left Shift a speciﬁed pointer one step right Move a pointer or write to the scratch pad Perform bubble sort (ascending order) Perform one sweep of pointers left to right Move both pointers all the way left Conditionally swap and advance pointers Conditionally swap two elements Shift a speciﬁed pointer one step left Shift a speciﬁed pointer one step right Swap two values at pointer locations or move a pointer Change 3D car pose to match the target Move horizontally to the target angle Move left to match the target angle Move right to match the target angle Move vertically to the target elevation Move up to match the target elevation Move down to match the target elevation Move camera 15◦ up, down, left or right Move all pointers to the rightmost posiiton Find maximum element of an array  Calls ADD1, LSHIFT ACT, CARRY ACT ACT ACT - BUBBLE, RESET ACT, BSTEP LSHIFT COMPSWAP, RSHIFT ACT ACT ACT - HGOTO, VGOTO LGOTO, RGOTO ACT ACT UGOTO, DGOTO ACT ACT - RSHIFT BUBBLESORT,RJMP  Table 2: Programs learned for addition, sorting and 3D car canonicalization. Note the the ACT program has a different effect depending on the environment and on the passed-in arguments.  6.2 GENERATED EXECUTION TRACE OF BUBBLESORT Figure 8 shows the sequence of program calls for BUBBLESORT. Pointers 1 and 2 are used to im-  Figure 8: Generated execution trace from our trained NPI sorting the array [9,2,5].  plement the “bubble” operation involving the comparison and swapping of adjacent array elements. The third pointer (referred to in the trace as “PTR 3”) is used to count the number of calls to BUB- BLE. After every call to RESET the swapping pointers are moved to the beginning of the array and the counting pointer is advanced by 1. When it has reached the end of the scratch pad, the model learns to halt execution of BUBBLESORT.  12  BUBBLESORT  BUBBLE    PTR 2 RIGHT    BSTEP      COMPSWAP        SWAP 1 2      RSHIFT        PTR 1 RIGHT        PTR 2 RIGHT    BSTEP      COMPSWAP        SWAP 1 2      RSHIFT        PTR 1 RIGHT        PTR 2 RIGHT  RESET    LSHIFT      PTR 1 LEFT      PTR 2 LEFT    LSHIFT      PTR 1 LEFT      PTR 2 LEFT    PTR 3 RIGHT  BUBBLE    PTR 2 RIGHT    BSTEP      COMPSWAP      RSHIFT        PTR 1 RIGHT        PTR 2 RIGHT    BSTEP      COMPSWAP      RSHIFT        PTR 1 RIGHT        PTR 2 RIGHT  RESET    LSHIFT      PTR 1 LEFT      PTR 2 LEFT    LSHIFT      PTR 1 LEFT      PTR 2 LEFT    PTR 3 RIGHT  BUBBLE    PTR 2 RIGHT    BSTEP      COMPSWAP      RSHIFT        PTR 1 RIGHT        PTR 2 RIGHT    BSTEP      COMPSWAP      RSHIFT        PTR 1 RIGHT        PTR 2 RIGHT  RESET    LSHIFT      PTR 1 LEFT      PTR 2 LEFT    LSHIFT      PTR 1 LEFT      PTR 2 LEFT    PTR 3 RIGHTPublished as a conference paper at ICLR 2016  6.3 ADDITIONAL EXPERIMENT ON ADDITION GENERALIZATION  Based on reviewer feedback, we conducted an additional comparison of NPI and sequence-to- sequence models for the addition task, to evaluate the generalization ability. we implemented addi- tion in a sequence to sequence model, training to model sequences of the following form, e.g. for “90 + 160 = 250” we represent the sequence as:  90X160X250  For the simple Seq2Seq baseline above (same number of LSTM layers and hidden units as NPI), we observed that the model could predict one or two digits reliably, but did not generalize even up to 20-digit addition. However, we are aware that others have gotten multi-digit addition of the above form to work to some extent with curriculum learning (Zaremba & Sutskever, 2014). In order to make a more competitive baseline, we helped Seq2Seq in two ways: 1) reverse input digits and stack the two numbers on top of each other to form a 2-channel sequence, and 2) reverse input digits and generate reversed output digits immediately at each time step. In the approach of 1), the seq2seq model schematically looks like this:  output: XXXX250 input 1: 090XXXX input 2: 061XXXX  In the approach of 2), the sequence looks like this:  output: 052 input 1: 090 input 2: 061  Both 1) which we call s2s-stacked and 2) which we call s2s-easy are much stronger competitors to NPI than even the proposed addition baseline. We compare the generalization performance of NPI to these baselines in the ﬁgure below:  Figure 9: Comparing NPI and Seq2Seq variants on addition generalization to longer sequences.  We found that NPI trained on 32 examples for problem lengths 1,...,20 generalizes with 100% ac- curacy to all the lengths we tried (up to 3000). s2s-easy trained on twice as many examples gen- eralizes to just over length 2000 problems. s2s-stacked barely generalizes beyond 5, even with far more data. This suggests that locality of computation makes a large impact on generalization per- formance. Even when we carefully ordered and stacked the input numbers for Seq2Seq, NPI still had an edge in performance. In contrast to Seq2Seq, NPI is taught (supervised for now) to move its pointers so that the key operations (e.g. single digit add, carry) can be done using only local information, and this appears to help generalization.  13  ",
1511.08400,2016,Regularizing RNNs by Stabilizing Activations,"['Regularizing RNNs by Stabilizing Activations\nDavid Krueger', 'Roland Memisevic']",https://arxiv.org/pdf/1511.08400,"6 1 0 2    r p A 6 2         ] E N . s c [      7 v 0 0 4 8 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  REGULARIZING RNNS BY STABILIZING ACTIVATIONS  David Krueger & Roland Memisevic Department of Computer Science and Operations Research University of Montreal Montreal, QC H3T 1J4, Canada {david.krueger@umontreal.ca, memisevr@iro.umontreal.ca}  ABSTRACT  We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states’ norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout. We achieve competitive performance (18.6% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer. With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN’s activations outside of their training horizon, allowing them to generalize to much longer sequences.  1  INTRODUCTION  Overﬁtting in machine learning is addressed by restricting the space of hypotheses ( i.e. functions) considered. This can be accomplished by reducing the number of parameters or using a regularizer with an inductive bias for simpler models, such as early stopping. More effective regularization can be achieved by incorporating more sophisticated prior knowledge. Keeping an RNN’s hidden activations on a reasonable path can be difﬁcult, especially across long time-sequences. With this in mind, we devise a regularizer for the state representation learned by temporal models, such as RNNs, that aims to encourage stability of the path taken through representation space. Speciﬁcally, we propose the following additional cost term for Recurrent Neural Networks (RNNs):  T(cid:88) ((cid:107)ht(cid:107)2 − (cid:107)ht−1(cid:107)2)2  β  1 T  t=1  Where ht is the vector of hidden activations at time-step t, and β is a hyperparameter controlling the amounts of regularization. We call this penalty the norm-stabilizer, as it successfully encourages the norms of the hiddens to be stable (i.e. approximately constant across time). Unlike the “temporal coherence” penalty of Jonschkowski & Brock (2015), our penalty does not encourage the state representation to remain constant, only its norm. In the absence of inputs and nonlinearities, a constant norm would imply orthogonality of the hidden- to-hidden transition matrix for simple RNNs (SRNNs). However, in the case of an orthogonal tran- sition matrix, inputs and nonlinearities can still change the norm of the hidden state, resulting in instability. This makes targeting the hidden activations directly a more attractive option for achiev- ing norm stability. Stability becomes especially important when we seek to generalize to longer sequences at test time than those seen during training (the “training horizon”). The hidden state in LSTM (Hochreiter & Schmidhuber, 1997) is usually the product of two squash- ing nonlinearities, and hence bounded. The norm of the memory cell, however, can grow linearly when the input, input modulation, and forget gates are all saturated at 1. Nonetheless, we ﬁnd that the memory cells exhibit norm stability far past the training horizon, and suggest that this may be part of what makes LSTM so successful.  1  Published as a conference paper at ICLR 2016  Table 1: LSTM Performance (bits-per-character) on PennTreebank for different values of β.  penalize hidden state penalize memory cell  β = 0 1.47 1.49  β = 50  β = 500  1.41 1.42  1.39 1.40  The activation norms of simple RNNs (SRNNs) with saturating nonlinearities are bounded. With ReLU nonlinearities, however, activations can explode instead of saturating. When the transition matrix, Whh has any eigenvalues λ with absolute value greater than 1, the part of the hidden state that is aligned with the corresponding eigenvector will grow exponentially to the extent that the ReLU or inputs fails to cancel out this growth. Simple RNNs with ReLU (Le et al., 2015) or clipped ReLU (Hannun et al., 2014) nonlinearities have performed competitively on several tasks, suggesting they can learn to be stable. We show, however, that IRNNs performance can rapidly degrade outside of their training horizon, while the norm-stabilizer prevents activations from exploding outside of the training horizon allowing IRNNs to generalize to much longer sequences. Additionally, we show that this penalty results in im- proved validation performance for IRNNs. Somewhat surprisingly, it also improves performance for LSTMs, but not tanh-RNNs. To the best of our knowledge, our proposal is entirely novel. Pascanu et al. (2012) proposed vanish- ing gradient regularization, which encourages the hidden transition to preserve norm in the direction of the cost derivative. Like the norm-stabilizer, their cost depends on the path taken through rep- resentation space, but the norm stabilzer does not prioritize cost-relevant directions, and accounts for the effects of inputs as well. A hard constraint (clipping) on the activations of LSTM memory cells was previously proposed by Sak et al. (2015). Hannun et al. (2014) use a clipped ReLU, which also has the effect of limiting activations. Both of these techniques operate element-wise however, whereas we target the activations’ norms. Several other works have used penalties on the difference of hidden states rather than their norms (Jonschkowski & Brock, 2015; Wen et al., 2015). Other regularizers for RNNs that do not target norm stability include weight noise (Jim et al., 1996) and dropout (Pham et al., 2013; Pachitariu & Sahani, 2013; Zaremba et al., 2014).  2 EXPERIMENTS  2.1 CHARACTER-LEVEL LANGUAGE MODELING ON PENNTREEBANK  We show that the norm-stabilizer improves performance for character-level language modeling on PennTreebank (Marcus et al., 1993) for LSTM and IRNNs 1, but not tanh-RNNs. We present results for β ∈ {0, 50, 500}. We found that values of β > 500 could slightly improve performance, but also resulted in much longer training time on this task. Scheduling β to increase throughout training might allow for faster training. Unless otherwise speciﬁed, we use 1000/1600 units for LSTM/SRNN, and SGD with learning rate=.002, momentum=.99, and gradient clipping=1. We train for a maximum of 1000 epochs and use sequences of length 50 taken without overlap. When we encounter a NaN in the cost function, we divide the learning rate by 2, and restart with the previous epoch’s parameters. For LSTMs, we either apply the norm-stabilizer penalty only to the memory cells, or only to the hidden state (in which case we remove the output tanh, as in (Gers & Schmidhuber, 2000)). Al- though Greff et al. (2015) found the output tanh to be essential for good performance, removing it gave us a slight improvement in this task. We compare to tanh and ReLU (with and without bias), with a grid search across cost weight, gradient clipping, and learning rate. For simple RNNs, we found that the zero-bias ReLU (i.e. TRec (Konda et al., 2014) with threshold 0) gave the best per- formance. The best performance for ReLU activation functions is obtained with the penalty applied. For tanh-RNNs, the best performance is obtained without any regularization. Results are better with the penalty than without for 9 out of 12 experiment settings.  1As in Le et al. (2015), we initialize Whh to be an identity matrix in our experiments  2  Published as a conference paper at ICLR 2016  Figure 1: Learning Curves for LSTM with different values of β. Penalty is applied to the hidden state (Left), or the memory cells (Right).  2.1.1 ALTERNATIVE COSTS  We compare 8 alternatives to the norm-stabilizer cost on PennTreeBank for IRNNs without biases (see Table 3), using the same setup as in 2.1. These include relative error, L1 norm, absolute differ- ence, and penalties that don’t target successive time-steps. The following two penalties performed very poorly and were not included in the table: |∆(cid:107)ht(cid:107)2 |, (cid:107)ht(cid:107)2 2. We ﬁnd that our proposal of penalizing successive states’ norms gives the best performance, but some alternatives seem promising and deserve further investigation. In particular, the relative error could be more appropriate; unlike the norm-stabilizer cost, it cannot be reduced simply by dividing all of the hidden states by a constant. The value 5 was chosen as a target for the norms based on the value found by our proposed cost; in practice it would be another hyperparameter to tune. The success of the other regularizers which encourage (L2) norm stability indicates that our inductive bias in favor of stable norms is useful.  Table 2: Performance with and without norm-stabilizer penalty for different activation functions. Gradients are clipped at 1 in the ﬁrst and third, and 106 in the second and fourth columns.  tanh, β = 0 tanh, β = 500 ReLU, β = 0 ReLU, β = 500 TRec, β = 0 TRec, β = 500  lr = .002, gc = 1  lr = .002  lr = .0002, gc = 1  lr = .0002  1.71 1.57 1.78 1.74 1.62 1.48  1.55 2.70 1.69 1.73 1.63 1.49  2.15 1.79 1.93 1.65 1.95 1.56  2.15 1.80 1.93 2.04 1.88 1.56  Table 3: Performance (bits-per-character) of zero-bias IRNN with various penalty terms designed to encourage norm stability.  (∆ht)2 1.84 2.19  (∆(cid:107)ht(cid:107)2)2  1.48  ( ∆(cid:107)ht(cid:107)2 (cid:107)ht(cid:107)2 1.60 1.50  β = 50 β = 500  )2  (∆(cid:107)ht(cid:107)1)2  ((cid:107)h(cid:107)2 − 5)2  ((cid:107)h0(cid:107)2 − (cid:107)hT(cid:107)2)2  2.96 3.18  1.49 1.50  3.81 1.54  3  Published as a conference paper at ICLR 2016  Table 4: Phoneme Error Rate (PER) on TIMIT for different experiment settings, average of 5 ex- periments. Norm-stabilized networks achieve the best performance. The regularization parameters are: β - norm stabilizer, p - dropout probability, σ - standard deviation of additive Gaussian weight noise.  β = 0 σ = 0 p = 0 21.8 19.6  β = 50 σ = 0 p = 0 20.7 18.6  β = 500 σ = 0 p = 0  19.0 16.9  β = 0 σ = .05 p = 0 21.5 19.1  β = 0 σ = 0 p = .5 21.9 19.5  β = 50 σ = 0 p = .5 20.9 18.5  β = 500 σ = 0 p = .5 19.4 17.0  β = 0 σ = .05 p = .5 21.1 18.9  test dev  2.2 PHONEME RECOGNITION ON TIMIT  We show that the norm-stabilizer improves phoneme recognition on the TIMIT dataset, outperform- ing networks regularized with weight noise and/or dropout. For these experiments, we use a similar setup to the previous state of the art for an RNN on this task (Graves et al., 2013), with CTC (Graves et al., 2006) and bidirectional LSTMs with 3 layers of 500 hidden units (for each direction). We train with Adam (Kingma & Ba, 2014) using learning rate=.001 and gradient clipping=200. Unlike Graves et al. (2013), we do not use beam search or an RNN transducer. We early stop after 25 epochs without improvement on the development set. We apply norm-stabilization to the hidden activations (in this case we do use the output tanh as is standard) with β ∈ {0, 50, 500}, and use standard deviation .05 for weight noise and p=.5 for dropout. We try all pair-wise combinations of the regularization techniques. We run 5 experiments for each of these 10 settings, and report the average phoneme error rate (PER). Combining weight noise and norm-stabilization gave poor performance, with some networks failing to train, these re- sults are omitted. Adding dropout had a minor effect on results. Norm-stabilized networks had the best performance (see ﬁgure 2 and table 4). Inspired by these results, we decided to train larger networks with more regularization, and observed further performance improvements (see table 5). We also used a higher “patience” for our early stopping criterion here, terminating after 100 epochs without improvement. Unlike previous experiments, we only ran one experiment with each of these settings. The network with 750 hidden units and β = 1000 gave the best performance on the devel- opment set, with dev/test PER of 16.2%/18.6%. This is competitive with the state of the art results on this task from Graves et al. (2013) and we evaluate without beam search or RNN transducer. al- though T´oth (2014) achieved 13.9%/16.7% using convolutional neural networks. The network with 1000 hidden units and β = 1000 achieved dev/test PER of 16.7%/17.5%.  Figure 2: Average PER on TIMIT core test set for different combinations or regularizers. The norm-stabilizer (β) shows a clear positive effect on performance. Weight noise (WN) also improves performance but less so. Combining weight noise with norm-stabilization gives poor results.  2.3 ADDING TASK  The adding task (Hochreiter & Schmidhuber, 1997) is a toy problem used to test an RNN’s ability to model long-term dependencies. The goal is to output the sum of two numbers seen at random  4  Published as a conference paper at ICLR 2016  Table 5: Phoneme Error Rate (PER) on TIMIT for experiments with n hidden units and more norm- stabilizer regularization (β). Networks regularized with weight noise σ = .05 when β = 0. The network with 750 units and β = 1000 achieved the best dev PER (16.17).  β = 0 n = 750  21.9 19.6  β = 500 n = 750  18.8 16.8  β = 1000 n = 750 18.6 16.2∗  β = 1500 n = 750  18.0 16.2  β = 0 n = 999  21.8 19.1  β = 500 n = 999  19.5 17.4  β = 1000 n = 999 17.5 16.7  β = 1500 n = 999  18.6 16.7  test dev  time-steps during training; inputs at other time-steps carry no information. Each element of an input sequence consists of a pair {n, i}, where n ∈ [0, 1] is chosen at uniform random and i ∈ {0, 1} indicates which two numbers to add. We use sequences of length 400. In Le et al. (2015), none of the models were able to reduce the cost below the “short-sighted” baseline set by predicting the ﬁrst (or second) of the indicated numbers (which gives an expected cost of 1 12) for this sequence length. We are able to solve this task more successfully. We use uniform initialization in [−.01, .01], learning rate=.01, gradient clipping=1. We compare across nine random seeds with and without the norm-stabilizer (using β = 1). The norm-stabilized networks reduced the test cost below 1 12 in 8/9 cases, averaging .059 MSE. The unregularized networks averaged .105 MSE, and only outperformed the short-sighted baseline in 4/9 cases, also failing to improve over a constant predictor in 4/9 cases.  2.4 VISUALIZING THE EFFECTS OF NORM-STABILIZATION  To test our hypothesis that stability helps networks generalize to longer sequences than they were trained on, we examined the costs and hidden norms at each time-step. Comparing identical SRNNs trained with and without norm-stabilizer penalty, we found LSTMs and RNNs with tanh activation functions continued to perform well far beyond the training horizon. Al- though the activations of LSTM’s memory cells could potentially grow linearly, in our experiments they are stable. Applying the norm-stabilizer does signiﬁcantly decrease their average norm and the variability of the norm, however (see ﬁgure 3). IRNNs, on the other hand, suffered from exploding activations, resulting in poor performance, but the norm-stabilizer effectively controls the norms and maintains a high level of performance; see ﬁgure 4. Norm-stabilized IRNNs’ performance and norms were both stable for the longest horizon we evaluated (10,000 time-steps).  Figure 3: Norm (y-axis) of LSTM memory cells (Left) and hidden states (Right) for different values of β, across time-steps (x-axis). Non-zero values dramatically reduce the mean and variance of the norms. LSTM memory cells have the potential to grow linearly, but instead exhibit natural stability.  For more insight on why the norm-stabilizer outperforms alternative costs, we examined the hidden norms of networks trained with values of β ranging from 0 to 200 on a dataset of 1000 length-50 sequences taken from wikipedia (Hutter, 2012). When we penalize the difference of the initial and ﬁnal norms, or the difference of the norms from some ﬁxed value, increasing the cost does not change the shape of the norms; they still begin to explode within the training horizon (see ﬁgure 5). For the norm-stabilizer, however, increasing the penalty signiﬁcantly delayed (but did not completely eradicate) activation explosions on this dataset.  5  Published as a conference paper at ICLR 2016  We also noticed that the distribution of activations was more concentrated in fewer hidden units when applying norm-stabilization on PennTreebank. Similarly, we found that the forget gates in LSTM networks had a more peaked distribution (see ﬁgure 6), while the average across dimensions was lower (so the network was forgetting more on average at each time step, but a small number of units were forgetting less). Finally, we found that the eigenvalues of regularized IRNN’s hidden transition matrices had a larger number of large eigenvalues, while the unregularized IRNN had a much larger number of eigenvalues closer to 1 in absolute value (see ﬁgure 6). This supports our hypothesis that orthogonal transitions are not inherently desirable in an RNN. By explicitly encouraging stability, the norm-stabilizer seems to favor solutions that maintain stability via selection of active units, rather than restricting the choice of transition matrix.  Figure 4: Top: average logarithm of hidden norms as a function of time-step. Bottom: average cost as a function of time-step. Solid blue - β = 500, dashed red - β = 0. Notice that IRNN’s activations explode exponentially (linearly in the log-scale) within the training horizon, causing cost quickly go to inﬁnity outside of the training horizon (50 time-steps).  Figure 5: Hidden norms as a function of time-step for values from 0 to 200 of the norm-stabilizer (Left and Center) vs. a penalty on the initial and ﬁnal norms (Right). The norm-stabilizer delays the explosion of activations by changing the shape of the curve, extending the ﬂat region.  6  Published as a conference paper at ICLR 2016  Figure 6: Left: sorted distribution of average forget-gates for different memory cells in LSTM. Right: sorted absolute value of eigenvalues of Whh in IRNN. Blue - β = 0, Green - β = 500  3 CONCLUSION  We introduced norm-based regularization of RNNs to prevent exploding or vanishing activations. We compare a range of novel methods for encouraging or enforcing norm stability. The best per- formance is achieved by penalizing the squared difference of subsequent hidden states’ norms. This penalty, the norm-stabilizer, improved performance on the tasks of language modeling and addition tasks, and gave state of the art RNN performance on phoneme recognition on the TIMIT dataset. Future work could involve:  • Exploring the relationship between stability and generative modeling with RNNs • Applying norm-regularized IRNNs to more challenging tasks • Applying similar regularization techniques to feedforward nets  ACKNOWLEDGMENTS  This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laborotory (AFRL) . The views, opinions and/or ﬁndings expressed are those of the authors and should not be interpreted as representing the ofﬁcial views or policies of the Department of Defense or the U.S. Government. We appreciate the many k80 GPUs provided by ComputeCanada. The authors would like to thank the developers of Theano (Bastien et al., 2012) and Blocks (van Merri¨enboer et al., 2015). Special thanks to Alex Lamb, Amar Shah, Asja Fischer, Caglar Gulcehre, Cesar Laurent, Dmitriy Serdyuk, Dzmitry Bahdanau, Faruk Ahmed, Harm de Vries, Jose Sotelo, Marcin Moczulski, Martin Arjovsky, Mohammad Pezeshki, Philemon Brakel, and Saizhen Zhang for useful discussions and/or sharing code.  REFERENCES Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, Warde-Farley, David, and Bengio, Yoshua. Theano: new features and speed improvements. CoRR, abs/1211.5590, 2012. URL http://arxiv.org/abs/1211. 5590.  Gers, Felix A. and Schmidhuber, J¨urgen. Recurrent nets that time and count. In IJCNN (3), pp. 189–194, 2000. doi: 10.1109/IJCNN.2000.861302. URL http://dx.doi.org/10.1109/ IJCNN.2000.861302.  Graves, A., Mohamed, A.-R., and Hinton, G. Speech recognition with deep recurrent neural net- works. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Confer- ence on, pp. 6645–6649, May 2013. doi: 10.1109/ICASSP.2013.6638947.  Graves, Alex, Fern´andez, Santiago, Gomez, Faustino, and Schmidhuber, J¨urgen. Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd International Conference on Machine Learning, ICML ’06, pp. 369–376, New York, NY, USA, 2006. ACM. ISBN 1-59593-383-2. doi: 10.1145/1143844.1143891. URL http://doi.acm.org/10.1145/1143844.1143891.  7  Published as a conference paper at ICLR 2016  Greff, Klaus, Srivastava, Rupesh Kumar, Koutn´ık, Jan, Steunebrink, Bas R., and Schmidhuber, J¨urgen. LSTM: A search space odyssey. CoRR, abs/1503.04069, 2015. URL http://arxiv. org/abs/1503.04069.  Hannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen, E., Prenger, R., Satheesh, S., Sengupta, S., Coates, A., and Ng, A. Y. Deep Speech: Scaling up end-to-end speech recognition. ArXiv e-prints, December 2014.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural Comput., 9(8):1735– 1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx. doi.org/10.1162/neco.1997.9.8.1735.  Hutter, Marcus. The human knowledge compression contest. 2012. URL http://prize.  hutter1.net/.  Jim, Kam-Chuen, Giles, C. Lee, and Horne, Bill G. An analysis of noise in recurrent neural net- works: convergence and generalization. IEEE Transactions on Neural Networks, 7(6):1424–1438, 1996. doi: 10.1109/72.548170. URL http://dx.doi.org/10.1109/72.548170.  Jonschkowski, Rico and Brock, Oliver. Learning state representations with robotic priors. Au-  tonomous Robots, 39(3):407–428, 2015. ISSN 0929-5593.  Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. CoRR,  abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.  Konda, K., Memisevic, R., and Krueger, D. Zero-bias autoencoders and the beneﬁts of co-adapting  features. ArXiv e-prints, February 2014.  Le, Quoc V., Jaitly, Navdeep, and Hinton, Geoffrey E. A simple way to initialize recurrent networks of rectiﬁed linear units. CoRR, abs/1504.00941, 2015. URL http://arxiv.org/abs/ 1504.00941.  Marcus, Mitchell P., Marcinkiewicz, Mary Ann, and Santorini, Beatrice. Building a large annotated ISSN  corpus of english: The penn treebank. Comput. Linguist., 19(2):313–330, June 1993. 0891-2017. URL http://dl.acm.org/citation.cfm?id=972470.972475.  Pachitariu, M. and Sahani, M. Regularization and nonlinearities for neural language models: when  are they needed? ArXiv e-prints, January 2013.  Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. Understanding the exploding gradient  problem. CoRR, abs/1211.5063, 2012. URL http://arxiv.org/abs/1211.5063.  Pham, Vu, Kermorvant, Christopher, and Louradour, J´erˆome. Dropout improves recurrent neural networks for handwriting recognition. CoRR, abs/1312.4569, 2013. URL http://arxiv. org/abs/1312.4569.  Sak, Hasim, Senior, Andrew, Rao, Kanishka, Irsoy, Ozan, Graves, Alex, Beaufays, Franc¸oise, and Schalkwyk, Johan. Learning acoustic frame labeling for speech recognition with recurrent neu- ral networks. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pp. 4280–4284. IEEE, 2015.  T´oth, L´aszl´o. Combining time- and frequency-domain convolution in convolutional neural network- In IEEE International Conference on Acoustics, Speech and Signal based phone recognition. Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014, pp. 190–194. IEEE, 2014. doi: 10.1109/ICASSP.2014.6853584. URL http://dx.doi.org/10.1109/ICASSP.2014. 6853584.  van Merri¨enboer, Bart, Bahdanau, Dzmitry, Dumoulin, Vincent, Serdyuk, Dmitriy, Warde-Farley, David, Chorowski, Jan, and Bengio, Yoshua. Blocks and fuel: Frameworks for deep learning. CoRR, abs/1506.00619, 2015. URL http://arxiv.org/abs/1506.00619.  Wen, Tsung-Hsien, Gasic, Milica, Mrksic, Nikola, Su, Pei-hao, Vandyke, David, and Young, Steve J. Semantically conditioned lstm-based natural language generation for spoken dialogue systems. CoRR, abs/1508.01745, 2015. URL http://arxiv.org/abs/1508.01745.  8  Published as a conference paper at ICLR 2016  Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol. Recurrent neural network regularization.  CoRR, abs/1409.2329, 2014. URL http://arxiv.org/abs/1409.2329.  9  ",
1511.06909,2016,BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies,"['BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies [code]\nShihao Ji', 'Swaminathan Vishwanathan', 'Nadathur Satish', 'Michael Anderson', 'Pradeep Dubey']",https://arxiv.org/pdf/1511.06909,"6 1 0 2    r a     M 1 3      ]  G L . s c [      7 v 9 0 9 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  BLACKOUT: SPEEDING UP RECURRENT NEURAL NET- WORK LANGUAGE MODELS WITH VERY LARGE VO- CABULARIES  Shihao Ji Parallel Computing Lab, Intel shihao.ji@intel.com  S. V. N. Vishwanathan Univ. of California, Santa Cruz vishy@ucsc.edu  Nadathur Satish, Michael J. Anderson & Pradeep Dubey Parallel Computing Lab, Intel {nadathur.rajagopalan.satish,michael.j.anderson,pradeep.dubey}@intel.com  ABSTRACT  We propose BlackOut, an approximation algorithm to efﬁciently train massive recurrent neural network language models (RNNLMs) with million word vocab- ularies. BlackOut is motivated by using a discriminative loss, and we describe a weighted sampling strategy which signiﬁcantly reduces computation while im- proving stability, sample efﬁciency, and rate of convergence. One way to under- stand BlackOut is to view it as an extension of the DropOut strategy to the out- put layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sam- pling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalabil- ity and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers.  1  INTRODUCTION  Statistical language models are a crucial component of speech recognition, machine translation and information retrieval systems. In order to handle the data sparsity problem associated with tradi- tional n-gram language models (LMs), neural network language models (NNLMs) (Bengio et al., 2001) represent the history context in a continuous vector space that can be learned towards error rate reduction by sharing data among similar contexts. Instead of using ﬁxed number of words to represent context, recurrent neural network language models (RNNLMs) (Mikolov et al., 2010) use a recurrent hidden layer to represent longer and variable length histories. RNNLMs signiﬁcantly outperform traditional n-gram LMs, and are therefore becoming an increasingly popular choice for practitioners (Mikolov et al., 2010; Sundermeyer et al., 2013; Devlin et al., 2014). Consider a standard RNNLM, depicted in Figure 1. The network has an input layer x, a hidden layer s (also called context layer or state) with a recurrent connection to itself, and an output layer y. Typically, at time step t the network is fed as input xt ∈ RV , where V denotes the vocabulary size, and st−1 ∈ Rh, the previous state. It produces a hidden state st ∈ Rh, where h is the size of the hidden layer, which in turn is transformed to the output yt ∈ RV . Different layers are fully connected, with the weight matrices denoted by Ω = {W V ×h For language modeling applications, the input xt is a sparse vector of a 1-of-V (or one-hot) encoding with the element corresponding to the input word wt−1 being 1 and the rest of components of xt set to 0; the state of the network st is a dense vector, summarizing the history context {wt−1,··· , w0}  out }. , W V ×h  , W h×h  in  r  1  Published as a conference paper at ICLR 2016  Figure 1: The network architecture of a standard RNNLM and its unrolled version for an example input sentence: <s> A cat is sitting on a sofa </s>.  preceding the output word wt; and the output yt is a dense vector, with the i-th element denoting the probability of the next word being wi, that is, p(wi|wt−1,··· , w0), or more concisely, p(wi|st). The input to output transformation occurs via:  (1) (2) where σ(v) = 1/(1 + exp(−v)) is the sigmoid activation function, and f (·) is the softmax function  st = σ(W T yt = f (Woutst),  inxt + Wrst−1)  f (ui) := exp(ui)/(cid:80)V  j=1 exp(uj).  One can immediately see that if xt uses a 1-of-V encoding, then the computations in equation (1) are relatively inexpensive (typically h is of the order of a few thousand, and the computations are O(h2)), while the computations in equation (2) are expensive (typically V is of the order of a million, and the computations are O(V h)). Similarly, back propagating the gradients from the output layer to the hidden layer is expensive. Consequently, the training times for some of the largest models reported in literature are of the order of weeks (Mikolov et al., 2011; Williams et al., 2015). In this paper, we ask the following question: Can we design an approximate training scheme for RNNLM which will improve on the state of the art models, while using signiﬁcantly less compu- tational resources? Towards this end, we propose BlackOut an approximation algorithm to efﬁ- ciently train massive RNNLMs with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a weighted sampling strategy which signiﬁcantly reduces com- putation while improving stability, sample efﬁciency, and rate of convergence. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE) (Gutmann & Hyv¨arinen, 2012; Mnih & Teh, 2012), and demonstrate that BlackOut mitigates some of the limitations of both previous methods. Our experiments, on the recently released one billion word language modeling benchmark (Chelba et al., 2014), demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, achieving the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single CPU machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. One way to understand BlackOut is to view it as an extension of the DropOut strategy (Srivastava et al., 2014) to the output layer, wherein we use a discriminative training loss and a weighted sam- pling scheme. The connection to DropOut is mainly from the way they operate in model training and model evaluation. Similar to DropOut, in BlackOut training a subset of output layer is sampled and trained at each training batch and when evaluating, the full network participates. Also, like DropOut, a regularization technique, our experiments show that the models trained by BlackOut are less prone to overﬁtting. A primary difference between them is that DropOut is routinely used at input and/or hidden layers of deep neural networks, while BlackOut only operates at output layer. We chose the name BlackOut in light of the similarities between our method and DropOut, and the complementary they offer to train deep neural networks.  2  𝑖𝑛𝑝𝑢𝑡𝑜𝑢𝑡𝑝𝑢𝑡ℎ𝑖𝑑𝑑𝑒𝑛𝑝(𝑤𝑡|𝑤𝑡−1,⋯,𝑤0)𝑊𝑖𝑛𝑊𝑜𝑢𝑡𝑊𝑟<s>                   A                  cat          …         sofaA                    cat                  is           …        </s>⋯𝑥0𝑦0𝑠0𝑥1𝑦1𝑠1𝑥2𝑦2𝑠2𝑥𝑛𝑦𝑛𝑠𝑛𝑤𝑡−1=𝑊𝑖𝑛𝑊𝑖𝑛𝑊𝑖𝑛𝑊𝑖𝑛𝑊𝑜𝑢𝑡𝑊𝑜𝑢𝑡𝑊𝑜𝑢𝑡𝑊𝑜𝑢𝑡𝑊𝑟𝑊𝑟𝑊𝑟Published as a conference paper at ICLR 2016  2 BLACKOUT: A SAMPLING-BASED APPROXIMATION  We will primarily focus on estimation of the matrix Wout. To simplify notation, in the sequel we will use θ to denote Wout and θj to denote the j-th row of Wout. Moreover, let (cid:104)·,·(cid:105) denote the dot product between two vectors. Given these notations, one can rewrite equation (2) as  pθ (wi|s) =  (cid:80)V exp ((cid:104)θi, s(cid:105)) j=1 exp ((cid:104)θj, s(cid:105))  ∀i ∈ {1,··· , V }.  (3)  RNNLMs with a softmax output layer are typically trained using cross-entropy as the loss function, which is equivalent to maximum likelihood (ML) estimation, that is, to ﬁnd the model parameter θ which maximizes the log-likelihood of target word wi, given a history context s:  ml(θ) = log pθ(wi|s), J s  (4)  whose gradient is given by  ∂J s  ml(θ) ∂θ  =  ∂ ∂θ  (cid:104)θi, s(cid:105) − V(cid:88)  j=1  pθ (wj|s)  (cid:20) ∂  (cid:104)θj, s(cid:105) ,  ∂ ∂θ (cid:104)θw, s(cid:105)  (cid:21)  (cid:104)θi, s(cid:105) − Epθ(w|s)  ∂ ∂θ  =  (5) The gradient of log-likelihood is expensive to evaluate because (1) the cost of computing pθ(wj|s) is O(V h) and (2) the summation above takes time linear in the vocabulary size O(V ). To alleviate the computational bottleneck of computing the gradient (5), we propose to use the following discriminative objective function for training RNNLM:  ∂θ  .  disc(θ) = log ˜pθ(wi|s) + J s  log(1 − ˜pθ(wj|s)),  (6)  (cid:88)  j∈SK  where SK is a set of indices of K words drawn from the vocabulary, and i /∈ SK. Typically, K is a tiny fraction of V , and in our experiments we use K ≈ V /200. To generate SK we will sample K words from the vocabulary using an easy to sample distribution Q(w), and set qj := 1 Q(wj ) in order to compute  ˜pθ(wi|s) =  qi exp ((cid:104)θi, s(cid:105)) +(cid:80)  qi exp ((cid:104)θi, s(cid:105))  j∈SK  qj exp ((cid:104)θj, s(cid:105))  .  (7)  Equation 6 is the cost function of a standard logistic regression classiﬁer that discriminates one positive sample wi from K negative samples wj,∀j ∈ SK. The ﬁrst term in (6) corresponds to the traditional maximum likelihood training, and the second term explicitly pushes down the probability of negative samples in addition to the implicit shrinkage enforced by the denominator of (7). In our experiments, we found the discriminative training (6) outperforms the maximum likelihood training (the ﬁrst term of Eq. 6) in all the cases, with varying degree of accuracy improvement depending on K. The weighted softmax function (7) can be considered as a stochastic version of the standard soft- max (3) on a different base measure. While the standard softmax (3) uses a base measure which gives equal weights to all words, and has support over the entire vocabulary, the base measure used in (7) has support only on K + 1 words: the target word wi and K samples from Q(w). The noise portion of (7) has the motivation from the sampling scheme, and the qi term for target word wi is introduced mainly to balance the contributions from target word and noisy sample words.1 Other justiﬁcations are discussed in Sec. 2.1 and Sec. 2.2, where we establish close connections between BlackOut, importance sampling, and noise contrastive estimation. Due to the weighted sampling property of BlackOut, some words might be sampled multiple times according to the proposal distribution Q(w), and thus their indices may appear multiple times in SK. As wi is the target word, which is assumed to be included in computing (7), we therefore set i /∈ SK explicitly.  1It’s shown empirically in our experiments that setting qi = 1 in (7) hurts the accuracy signiﬁcantly.  3  Published as a conference paper at ICLR 2016  Then taking derivatives with respect to uj,∀j ∈ {i} ∪ SK, yields  Substituting (7) into (6) and letting uj = (cid:104)θj, s(cid:105) and ˜pj = ˜pθ (wj|s), we have  disc(θ) ∝ ui − (K + 1) log J s  qk exp(uk) − qj exp(uj)  k∈{i}∪SK  k∈{i}∪SK  (cid:88)  j∈SK  log  1  1 − ˜pj  qk exp(uk) +  (cid:88) K + 1 − (cid:88) K + 1 − (cid:88)  j∈SK  1  1 − ˜pk  k∈SK\{j}   (cid:88)  ˜pi  ˜pj,  ∂J s  disc(θ) ∂ui  ∂J s  disc(θ) ∂uj  = 1 −  = −  for j ∈ SK.   .  (8)  (9)  (10)  By the chain rule of derivatives, we can propagate the errors backward to previous layers and com- pute the gradients with respect to the full model parameters Ω. In contrast to Eq. 5, Eqs. 9 and 10 are much cheaper to evaluate as (1) the cost of computing ˜pj is O(Kh) and (2) the summation takes O(K), hence roughly a V /K times of speed-up. Next we turn our attention to the proposal distribution Q(w). In the past, a uniform distribution or the unigram distribution have been advocated as promising candidates for sampling distributions (Bengio & Sen´ecal, 2003; Jean et al., 2015; Bengio & Sen´ecal, 2008; Mnih & Teh, 2012). As we will see in the experiments, neither one is suitable for a wide range of datasets, and we ﬁnd that the power-raised unigram distribution of Mikolov et al. (2013) is very important in this context:  Qα(w) ∝ pα  uni(w), α ∈ [0, 1].  (11) Note that Qα(w) is a generalization of uniform distribution (when α = 0) and unigram distribution (when α = 1). The rationale behind our choice is that by tuning α, one can interpolate smoothly between sampling popular words, as advocated by the unigram distribution, and sampling all words equally. The best α is typically dataset and/or problem dependent; in our experiments, we use a holdout set to ﬁnd the best value of α. It’s worth noting that this sampling strategy has been used by Mikolov et al. (2013) in a similar context of word embedding, while here we explore its effect in the language modeling applications. After BlackOut training, we evaluate the predictive performance of RNNLM by perplexity. To cal- culate perplexity, we explicitly normalize the output distribution by using the exact softmax func- tion (3). This is similar to DropOut (Srivastava et al., 2014), wherein a subset of network is sampled and trained at each training batch and when evaluating, the full network participates.  2.1 CONNECTION TO IMPORTANCE SAMPLING  BlackOut has a close connection to importance sampling (IS). To see this, differentiating the loga- rithm of Eq. 7 with respect to model parameter θ, we have  (cid:80)  (cid:104)θi, s(cid:105) −  log ˜pθ(wi|s) =  ∂ ∂θ  =  ∂ ∂θ  ∂ ∂θ  k∈{i}∪SK  1 qk exp((cid:104)θk, s(cid:105)) (cid:104)θw, s(cid:105)  (cid:21)  .  (cid:20) ∂  ∂θ  (cid:104)θi, s(cid:105) − E ˜pθ(w|s)  (cid:88)  j∈{i}∪SK  qj exp((cid:104)θj, s(cid:105))  (cid:104)θj, s(cid:105)  ∂ ∂θ  (12)  In contrast with Eq. 5, it shows that the weighted softmax function (7) corresponds to an IS-based estimator of the standard softmax (3) with a proposal distribution Q(w). Importance sampling has been applied to NNLMs with large output layers in previous works (Bengio & Sen´ecal, 2003; 2008; Jean et al., 2015). However, either uniform distribution or unigram distri- bution is used for sampling and all aforementioned works exploit the maximum likelihood learning of model parameter θ. By contrast, BlackOut uses a discriminative training (6) and a power-raised unigram distribution Qα(w) for sampling; these two changes are important to mitigate some of lim- itations of IS-based approaches. While an IS-based approach with a uniform proposal distribution  4  Published as a conference paper at ICLR 2016  is very stable for training, it suffers from large bias due to the apparent divergence of the uniform distribution from the true data distribution pθ(w|s). On the other hand, a unigram-based IS esti- mate can make learning unstable due to the high variance (Bengio & Sen´ecal, 2003; 2008). Using a power-raised unigram distribution Qα(w) entails a better trade-off between bias and variance, and thus strikes a better balance between these two extremes. In addition, as we will see from the experi- ments, the discriminative training of BlackOut speeds up the rate of convergence over the traditional maximum likelihood learning.  2.2 CONNECTION TO NOISE CONTRASTIVE ESTIMATION  The basic idea of NCE is to transform the density estimation problem to the problem of learning by comparison, e.g., estimating the parameters of a binary classiﬁer that distinguishes samples from the data distribution pd from samples generated by a known noise distribution pn (Gutmann & Hyv¨arinen, 2012). In the language modeling setting, the data distribution pd will be the distribution pθ(w|s) of interest, and the noise distribution pn is often chosen from the ones that are easy to sample from and possibly close to the true data distribution (so that the classiﬁcation problem isn’t trivial). While Mnih & Teh (2012) uses a context-independent (unigram) noise distribution pn(w), BlackOut can be formulated into the NCE framework by considering a context-dependent noise distribution pn(w|s), estimated from K samples drawn from Q(w), by  pn(wi|s) =  1 K  pθ(wj|s),  (13)  Q(w): SK ∼ Q(w) since ESK∼Q(w)(pn(wi|s)) = Q(wi) and ESK∼Q(w)((cid:80)V  which is a probability distribution function under the expectation that K samples are drawn from i=1 pn(wi|s)) = 1  (See the proof in Appendix A). Similar to Gutmann & Hyv¨arinen (2012), noise samples are assumed K times more frequent than K+1 pθ(w|s) data samples so that data points are generated from a mixture of two distributions: K+1 pn(w|s). Then the conditional probability of sample wi being generated from the data and K distribution is  1  (cid:88)  j∈SK  qj qi  pθ(D = 1|wi, s) =  pθ(wi|s)  pθ(wi|s) + Kpn(wi|s)  .  (14)  (15)  ,  Inserting Eq. 13 into Eq. 14, we have pθ(D = 1|wi, s) =  qi exp((cid:104)θi, s(cid:105)) +(cid:80)  qi exp((cid:104)θi, s(cid:105))  j∈SK  qj exp((cid:104)θj, s(cid:105))  which is exactly the weighted softmax function deﬁned in (7). Note that due to the noise distribution proposed in Eq. 13, the expensive denominator (or the partition function Z) of pθ(wj|s) is canceled out, while in Mnih & Teh (2012) the partition function Z is either treated as a free parameter to be learned or approximated by a constant. Mnih & Teh (2012) recommended to set Z = 1.0 in the NCE training. However, from our experiments, setting Z = 1.0 often leads to sub-optimal solutions2 and different settings of Z sometimes incur numerical instability since the log-sum-exp trick3 can not be used there to shift the scores of the output layer to a range that is amenable to the exponential function. BlackOut does not have this hyper-parameter to tune and the log-sum-exp trick still works for the weighted softmax function (7). Due to the discriminative training of NCE and BlackOut, they share the same objective function (6). We shall emphasize that according to the theory of NCE, the K samples should be sampled from the noise distribution pn(w|s). But in order to calculate pn(w|s), we need the K samples drawn from Q(w) beforehand. As an approximation, we use the same K samples drawn from Q(w) as the K samples from pn(w|s), and only use the expression of pn(w|s) in (13) to evaluate the noise density value required by Eq. 14. This approximation is accurate since ESK∼Q(w)(pn(wi|s)) = Q(wi) as proved in Appendix A, and we ﬁnd empirically that it performs much better (with improved stability) than using a unigram noise distribution as in Mnih & Teh (2012).  2Similarly, Chen et al. (2015) reported that setting ln(Z) = 9 gave them the best results. 3https://en.wikipedia.org/wiki/LogSumExp  5  Published as a conference paper at ICLR 2016  2.3 RELATED WORK  Many approaches have been proposed to address the difﬁculty of training deep neural networks with large output spaces. In general, they can be categorized into four categories:  • Hierarchical softmax (Morin & Bengio, 2005; Mnih & Hinton, 2008) uses a hierarchical binary tree representation of the output layer with the V words as its leaves. It allows ex- ponentially faster computation of word probabilities and their gradients, but the predictive performance of the resulting model is heavily dependent on the tree used, which is of- ten constructed heuristically. Moreover, by relaxing the constraint of a binary structure, Le et al. (2011) introduces a structured output layer with an arbitrary tree structure constructed from word clustering. All these methods speed up both the model training and evaluation considerably. • Sampling-based approximations select at random or heuristically a small subset of the out- put layer and estimate gradient only from those samples. The use of importance sampling in Bengio & Sen´ecal (2003; 2008); Jean et al. (2015), and the use of NCE (Gutmann & Hyv¨arinen, 2012) in Mnih & Teh (2012) all fall under this category, so does the more recent use of Locality Sensitive Hashing (LSH) techniques (Shrivastava & Li, 2014; Vi- jayanarasimhan et al., 2014) to select a subset of good samples. BlackOut, with close con- nections to importance sampling and NCE, also falls in this category. All these approaches only speed up the model training, while the model evaluation still remains computationally challenging. • Self normalization (Devlin et al., 2014) extends the cross-entropy loss function by explicitly encouraging the partition function of softmax to be as close to 1.0 as possible. Initially, this approach only speeds up the model evaluation and more recently it’s extended to facilitate the training as well with some theoretical guarantees (Andreas & Klein, 2014; Andreas et al., 2015). • Exact gradient on limited loss functions (Vincent et al., 2015) introduces an algorithmic approach to efﬁciently compute the exact loss, gradient update for the output weights in O(h2) per training example instead of O(V h). Unfortunately, it only applies to a lim- ited family of loss functions that includes squared error and spherical softmax, while the standard softmax isn’t included.  As discussed in the introduction, BlackOut also shares some similarity to DropOut (Srivastava et al., 2014). While DropOut is often applied to input and/or hidden layers of deep neural networks to avoid feature co-adaptation and overﬁtting by uniform sampling, BlackOut applies to a softmax output layer, uses a weighted sampling, and employs a discriminative training loss. We chose the name BlackOut in light of the similarities between our method and DropOut, and the complementary they offer to train deep neural networks.  IMPLEMENTATION AND FURTHER SPEED-UP  3 We implemented BlackOut on a standard machine with a dual-socket 28-core Intel R(cid:13)Xeon R(cid:13)4 Haswell CPU. To achieve high throughput, we train RNNLM with Back-Propagation Through Time (BPTT) (Rumelhart et al., 1988) with mini-batches (Chen et al., 2014). We use RMSProp (Hinton, 2012) for learning rate scheduling and gradient clipping (Bengio et al., 2013) to avoid the gradi- ent explosion issue of recurrent networks. We use the latest Intel MKL library (version 11.3.0) for SGEMM calls, which has improved support for tall-skinny matrix-matrix multiplications, which consume about 80% of the run-time of RNNLMs. It is expensive to access and update large models with billions of parameters. Fortunately, due to the 1-of-V encoding at input layer and the BlackOut sampling at output layer, the model update on Win and Wout is sparse, i.e., only the model parameters corresponding to input/output words and the samples in SK are updated at each training batch. However, subnet updates have to be done carefully due to the dependency within RMSProp updating procedure. We therefore propose an approximated RMSProp that enables an efﬁcient subnet update and thus speeds up the algorithm even further. Details can be found in Appendix C.  4Intel and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries.  6  Published as a conference paper at ICLR 2016  4 EXPERIMENTS  In our experiments, we ﬁrst compare BlackOut, NCE and exact softmax (without any approxima- tion) using a small dataset. We then evaluate the performance of BlackOut on the recently released one billion word language modeling benchmark (Chelba et al., 2014) with a vocabulary size of up to one million. We compare the performance of BlackOut on a standard CPU machine versus the state- of-the-arts reported in the literature that are achieved on GPUs or on clusters of CPU nodes. Our implementation and scripts are open sourced at https://github.com/IntelLabs/rnnlm.  Corpus Models are trained and evaluated on two different corpora: a small dataset provided by the RNNLM Toolkit5, and the recently released one billion word language modeling benchmark6, which is perhaps the largest public dataset in language modeling. The small dataset has 10,000 training sentences, with 71,350 words in total and 3,720 unique words; and the test perplexity is evaluated on 1,000 test sentences. The one billion word benchmark was constructed from a mono- lingual/English corpora; after all necessary preprocessing including de-duplication, normalization and tokenization, 30,301,028 sentences (about 0.8 billion words) are randomly selected for training, 6,075 sentences are randomly selected for test and the remaining 300,613 sentences are reserved for future development and can be used as holdout set.  4.1 RESULTS ON SMALL DATASET  We evaluate BlackOut, NCE and exact softmax (without any approximation) on the small dataset described above. This small dataset is used so that we can train the standard RNNLM algorithm with exact softmax within a reasonable time frame and hence to provide a baseline of expected perplexity. There are many other techniques involved in the training, such as RMSProp for learning rate scheduling (Hinton, 2012), subnet update (Appendix C), and mini-batch splicing (Chen et al., 2014), etc., which can affect the perplexity signiﬁcantly. For a fair comparison, we use the same tricks and settings for all the algorithms, and only evaluate the impact of the different approximations (or no approximation) on the softmax output layer. Moreover, there are a few hyper-parameters that have strong impact on the predictive performance, including α of the proposal distribution Qα(w) for BlackOut and NCE, and additionally for NCE, the partition function Z. We pay an equal amount of effort to tune these hyper-parameters for BlackOut and NCE on the validation set as number of samples increases. Figure 2 shows the perplexity reduction as a function of number of samples K under two different vocabulary settings: (a) a full vocabulary of 3,720 words, and (b) using the most frequent 2,065 words as vocabulary. The latter is a common approach used in practice to accelerate RNNLM computation by using RNNLM to predict only the most frequent words and handling the rest using an n-gram model (Schwenk & Gauvain, 2005). We will see similar vocabulary settings when we evaluate BlackOut on the large scale one billion word benchmark. As can be seen, when the size of the samples increases, in general both BlackOut and NCE improve their prediction accuracy under the two vocabulary settings, and even with only 2 samples both al- gorithms still converge to reasonable solutions. BlackOut can utilize samples much more effectively than NCE as manifested by the signiﬁcantly lower perplexities achieved by BlackOut, especially when number of samples is small; Given about 20-50 samples, BlackOut and NCE reach similar perplexities as the exact softmax, which is expensive to train as it requires to evaluate all the words in the vocabularies. When the vocabulary size is 2,065, BlackOut achieves even better perplexity than that of the exact softmax. This is possible since BlackOut does stochastic sampling at each training example and uses the full softmax output layer in prediction; this is similar to DropOut that is routinely used in input layer and/or hidden layers of deep neural networks (Srivastava et al., 2014). As in DropOut, BlackOut has the beneﬁt of regularization and avoids feature co-adaption and is pos- sibly less prone to overﬁtting. To verify this hypothesis, we evaluate the perplexities achieved on the training set for different algorithms and provide the results in Figure 5 at Appendix B. As can been seen, the exact softmax indeed overﬁts to the training set and reaches lower training perplexities than NCE and BlackOut.  5http://www.rnnlm.org/ 6https://code.google.com/p/1-billion-word-language-modeling-benchmark/  7  Published as a conference paper at ICLR 2016  Figure 2: Test perplexity evolution as a function of number of samples K (a) with a full vocabulary of 3,720 words, and (b) with the most frequent 2,065 words in vocabulary. The experiments are executed on the RNNLMs with 16 hidden units.  Next we compare the convergence rates of BlackOut and NCE when training the RNNLMs with 16 hidden units for a full vocabulary of 3,720 words. Figures 3(a) and 3(b) plot the learning curves of BlackOut and NCE when 10 samples or 50 samples are used in training, respectively. The ﬁgure shows that BlackOut enjoys a much faster convergence rate than NCE, especially when number of samples is small (Figure 3(a)); but this advantage gets smaller when number of samples increases (Figure 3(b)). We also observed similar behavior when we evaluated BlackOut and NCE on the large scale one billion word benchmark.  Figure 3: The learning curves of BlackOut and NCE when training the RNNLMs with 16 hidden units with (a) 10 samples, and (b) 50 samples.  4.2 RESULTS ON ONE BILLION WORD BENCHMARK  We follow the experiments from Williams et al. (2015) and Le et al. (2015) and compare the perfor- mance of BlackOut with the state-of-the-art results provided by them. While we evaluated Black- Out on a dual-socket 28-core Intel R(cid:13)Xeon R(cid:13)Haswell machine, Williams et al. (2015) implemented RNNLM with the NCE approximation on NVIDIA GTX Titan GPUs, and Le et al. (2015) executed an array of recurrent networks, including deep RNN and LSTM, without approximation on a CPU cluster. Besides the time-to-solution comparison, these published results enable us to cross-check the predictive performance of BlackOut with another implementation of NCE or with other compet- itive network architectures.  4.2.1 WHEN VOCABULARY SIZE IS 64K  Following the experiments in Williams et al. (2015), we evaluate the performance of BlackOut on a vocabulary of 64K most frequent words. This is similar to the scenario in Figure 2(b) where the  8  0501001502006080100120140160180200220240260Ktest perplexity  NCEBlackOutExact Softmax0501001502006570758085Ktest perplexity  NCEBlackOutExact Softmax01020304050020004000600080001000012000iterationtest perplexity  NCEBlackOut010203040506080100120140160180200220iterationtest perplexity  NCEBlackOutPublished as a conference paper at ICLR 2016  most frequent words are kept in vocabulary and the rest rare words are mapped to a special <unk> token. We ﬁrst study the importance of α of the proposal distribution Qα(w) and the discrimina- tive training (6) as proposed in BlackOut. As we discussed in Sec. 2, when α = 0, the proposal distribution Qα(w) degenerates to a uniform distribution over all the words in the vocabulary, and when α = 1, we recover the unigram distribution. Thus, we evaluate the impact of α in the range of [0, 1]. Figure 4(a) shows the evolution of test perplexity as a function of α for the RNNLMs with 256 hidden units. As can be seen, α has a signiﬁcant impact on the prediction accuracy. The com- monly used uniform distribution (when α = 0) and unigram distribution (when α = 1) often yield sub-optimal solutions. For the dataset and experiment considered, α = 0.4 gives the best perplexity (consistent on holdout set and test set). We therefore use α = 0.4 in the experiments that follow. The number of samples used is 500, which is about 0.8% of the vocabulary size.  Figure 4: (a) The impact of α evaluated when 256 hidden units are used; (b) The learning curves of maximum likelihood and discriminative training when 512 hidden units are used.  Figure 4(b) demonstrates the impact of discriminative training (6) over the maximum likelihood training (the ﬁrst term of Eq. 6) on the RNNLMs with 512 hidden units using two different α’s. In general, we observe 1-3 points of perplexity reduction due to discriminative training over traditional maximum likelihood training. Finally, we evaluate the scalability of BlackOut when number of hidden units increases. As the dataset is large, we observed that the performance of RNNLM depends on the size of the hidden layer: they perform better as the size of the hidden layer gets larger. As a truncated 64K word vocabulary is used, we interpolate the RNNLM scores with a full size 5-gram to ﬁll in rare word probabilities (Schwenk & Gauvain, 2005; Park et al., 2010). We report the interpolated perplexities BlackOut achieved and compare them with the results from Williams et al. (2015) in Table 1. As can be seen, BlackOut reaches lower perplexities than those reported in Williams et al. (2015) within comparable time frames (often 10%-40% faster). We achieved a perplexity of 42.0 when the hidden layer size is 4096. To the best of our knowledge, this is the lowest perplexity reported on this benchmark.  Table 1: Performance on the one billion word benchmark by interpolating RNNLM on a 64K word vocabulary with a full-size KN 5-gram LM.  #Params [millions]  Test Perplexity  Time to Solution  Published1 BlackOut  Published1 BlackOut  66.95  45m  Model  KN 5-gram RNN-128 + KN 5-gram RNN-256 + KN 5-gram RNN-512 + KN 5-gram RNN-1024 + KN 5-gram RNN-2048 + KN 5-gram RNN-4096 + KN 5-gram 1Data from Table 1 of Williams et al. (2015).  1,748 1,764 1,781 1,814 1,880 2,014 2,289  59.0 55.1 51.5 47.6 43.9 42.0  6h 16h 1d2h 2d2h 4d7h 14d5h  9h 14h 1d  1d14h 2d15h 10d  60.8 57.3 53.2 48.9 45.2 42.4  9  00.20.40.60.81708090100110120130140αtest perplexity0510152065707580859095100105iterationstest perplexity  maximum likelihood (α=0)discriminative (α=0)maximum likelihood (α=0.4)discriminative (α=0.4)Published as a conference paper at ICLR 2016  4.2.2 WHEN VOCABULARY SIZE IS 1M  In the ﬁnal set of experiments, we evaluate the performance of BlackOut with a very large vocabulary of 1,000,000 words, and the results are provided in Table 2. This is the largest vocabulary used on this benchmark that we could ﬁnd in existing literature. We consider the RNNLM with 1,024 hidden units (about 2 billion parameters) and 2,048 hidden units (about 4.1 billion parameters) and compare their test perplexities with the results from Le et al. (2015). We use 2,000 samples, 0.2% of the vocabulary size, for BlackOut training with α = 0.1. Comparing to the experiments with the 64K word vocabulary, a much smaller α is used here since the sampling rate (0.2%) is much lower than that is used (0.8%) when the vocabulary size is 64K, and a smaller α strikes a better balance between sample coverage per training example and convergence rate. In contrast, NCE with the same setting converges very slowly (similar to Figure 3(a)) and couldn’t reach a competitive perplexity within the time frame considered, and its results are not reported here. As the standard RNN/LSTM algorithms (without approximation) are used in Le et al. (2015), a cluster of 32 CPU machines (at least 20 cores each) are used to train the models for about 60 hours. BlackOut enables us to train this large model using a single CPU machine for 175 hours. Since different model architectures are used in the experiments (deep RNN/LSTM vs. standard RNNLM), the direct comparison of test perplexity isn’t very meaningful. However, this experiment demonstrates that even though our largest model is about 2-3 times larger than the models evaluated in Le et al. (2015), BlackOut, along with a few other optimization techniques, make this large scale learning problem still feasible on a single box machine without using GPUs or CPU clusters.  Table 2: Performance on the one billion word benchmark with a vocabulary of 1,000,000 words. Single model (RNN/LSTM-only) perplexities are reported; no interpolation is applied to any models.  60 hours  Results from Le et al. (2015)  Model LSTM (512 units) IRNN (4 layers, 512 units) IRNN (1 layer, 1024 units + 512 linear units) RNN (4 layers, 512 tanh units) RNN (1 layer, 1024 tanh units + 512 linear units) RNN (1 layer, 1024 sigmoid units) 175 hours, 1 machine RNN (1 layer, 2048 sigmoid units)  32 machines  Our Results  Perplexity  68.8 69.4 70.2 71.8 72.5 78.4 68.3  Last, we collect all the state of the art results we are aware of on this benchmark and summarize them in Table 3. Since all the models are the interpolated ones, we interpolate our best RNN model7 from Table 2 with the KN 5-gram model and achieve a perplexity score of 47.3. Again, different papers provide their best models trained with different architectures and vocabulary settings. Hence, an absolutely fair comparison isn’t possible. Regardless of these discrepancies, our models, within different groups of vocabulary settings, are very competitive in terms of prediction accuracy and model size.  Table 3: Comparison with the state of the art results reported on the one billion word benchmark.  Model  #Params [billions]  RNN-1024 (full vocab) + MaxEnt1 RNN-2048 (full vocab) + KN 5-gram2 RNN-1024 (full vocab) + MaxEnt + 3 models1 RNN-4096 (64K vocab) + KN 5-gram3 RNN-4096 (64K vocab) + KN 5-gram2 1Data from Chelba et al. (2014); 2Our results; 3Data from Williams et al. (2015).  20 5.0 42.9 2.3 2.3  Test Perplexity  51.3 47.3 43.8 42.4 42.0  7To be consistent with the benchmark in Chelba et al. (2014), we retrained it with the full-size vocabulary  of about 0.8M words.  10  Published as a conference paper at ICLR 2016  5 CONCLUSION  We proposed BlackOut, a sampling-based approximation, to train RNNLMs with very large vocabu- laries (e.g., 1 million). We established its connections to importance sampling and noise contrastive estimation (NCE), and demonstrated its stability, sample efﬁciency and rate of convergence on the recently released one billion word language modeling benchmark. We achieved the lowest reported perplexity on this benchmark without using GPUs or CPU clusters. As for future extensions, our plans include exploring other proposal distributions Q(w), and theo- retical properties of the generalization property and sample complexity bounds for BlackOut. We will also investigate a multi-machine distributed implementation.  ACKNOWLEDGMENTS  We would like to thank Oriol Vinyals, Andriy Mnih and the anonymous reviewers for their excellent comments and suggestions, which helped improve the quality of this paper.  REFERENCES Andreas, Jacob and Klein, Dan. When and why are log-linear models self-normalizing? In Proceed- ings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics, 2014.  Andreas, Jacob, Rabinovich, Maxim, Klein, Dan, and Jordan, Michael I. On the accuracy of self-  normalized log-linear models. In NIPS, 2015.  Bengio, Yoshua and Sen´ecal, Jean-S´ebastien. Quick training of probabilistic neural nets by impor-  tance sampling. In AISTATS, 2003.  Bengio, Yoshua and Sen´ecal, Jean-S´ebastien. Adaptive importance sampling to accelerate training of a neural probabilistic language model. In IEEE Transactions on Neural Networks, volume 19, pp. 713–722, 2008.  Bengio, Yoshua, Ducharme, Rjean, and Vincent, Pascal. A neural probabilistic language model. In  NIPS, pp. 932–938, 2001.  Bengio, Yoshua, Boulanger-Lewandowski, Nicolas, and Pascanu, Razvan. Advances in optimizing  recurrent networks. In ICASSP, pp. 8624–8628, 2013.  Chelba, Ciprian, Mikolov, Tomas, Schuster, Mike, Ge, Qi, Brants, Thorsten, Koehn, Phillipp, and Robinson, Tony. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH, pp. 2635–2639, 2014.  Chen, Xie, Wang, Yongqiang, Liu, Xunying, Gales, Mark JF, and Woodland, Philip C. Efﬁcient gpu-based training of recurrent neural network language models using spliced sentence bunch. In INTERSPEECH, 2014.  Chen, Xie, Liu, Xunying, Gales, Mark JF, and Woodland, Philip C. Recurrent neural network In ICASSP,  language model training with noise contrastive estimation for speech recognition. 2015.  Devlin, Jacob, Zbib, Rabih, Huang, Zhongqiang, Lamar, Thomas, Schwartz, Richard, and Makhoul, John. Fast and robust neural network joint models for statistical machine translation. In ACL, 2014.  Gutmann, Michael U. and Hyv¨arinen, Aapo. Noise-contrastive estimation of unnormalized statisti-  cal models, with applications to natural image statistics. JMLR, 13:307–361, 2012.  Hinton, Geoffrey. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent  magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  11  Published as a conference paper at ICLR 2016  Jean, Sbastien, Cho, Kyunghyun, Memisevic, Roland, and Bengio, Yoshua. On using very large  target vocabulary for neural machine translation. In ACL, 2015.  Le, Hai-Son, Oparin, Ilya, Allauzen, Alexandre, Gauvain, Jean-Luc, and Yvon, Franc¸ois. Structured  output layer neural network language model. In ICASSP, pp. 5524–5527, 2011.  Le, Quoc V., Jaitly, Navdeep, and Hinton, Geoffrey. A simple way to initialize recurrent networks  of rectiﬁed linear units. arXiv preprint arxiv:1504.00941, 2015.  Mikolov, Tomas, Karaﬁ´at, Martin, Burget, Luk´as, Cernock´y, Jan, and Khudanpur, Sanjeev. Recur-  rent neural network based language model. In INTERSPEECH, pp. 1045–1048, 2010.  Mikolov, Tomas, Deoras, Anoop, Povey, Dan, Burget, Lukar, and Cernocky, Jan Honza. Strategies for training large scale neural network language models. IEEE Automatic Speech Recognition and Understanding Workshop, 2011.  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Distributed rep- resentations of words and phrases and their compositionality. In Burges, Chris, Bottou, Leon, Welling, Max, Ghahramani, Zoubin, and Weinberger, Kilian (eds.), Advances in Neural Informa- tion Processing Systems 26, 2013.  Mnih, Andriy and Hinton, Geoffrey E. A scalable hierarchical distributed language model. In NIPS,  volume 21, pp. 1081–1088, 2008.  Mnih, Andriy and Teh, Yee Whye. A fast and simple algorithm for training neural probabilistic  language models. In Proceedings of the International Conference on Machine Learning, 2012.  Morin, Frederic and Bengio, Yoshua. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on artiﬁcial intelligence and statistics, pp. 246–252. Citeseer, 2005.  Park, Junho, Liu, Xunying, Gales, Mark J. F., and Woodland, P. C. Improved neural network based  language modelling and adaptation. In Proc. ISCA Interspeech, pp. 10411044, 2010.  Rumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pp. 696–699. MIT Press, Cambridge, MA, USA, 1988.  Schwenk, Holger and Gauvain, Jean-Luc. Training neural network language models on very large corpora. In Proceedings of Human Language Technology Conference and Conference on Empir- ical Methods in Natural Language Processing, pp. 201–208, 2005.  Shrivastava, Anshumali and Li, Ping. Asymmetric lsh (alsh) for sublinear time maximum inner  product search (mips). In NIPS, volume 27, pp. 2321–2329, 2014.  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁtting. JMLR, 15:1929–1958, 2014.  Sundermeyer, Martin, Oparin, Ilya, Gauvain, Jean-Luc, Freiberg, Ben, Schl  ”uter, Ralf, and Ney, Hermann. Comparison of feedforward and recurrent neural network lan- guage models. In ICASSP, pp. 8430–8434, 2013.  Vijayanarasimhan, Sudheendra, Shlens, Jonathon, Monga, Rajat, and Yagnik, Jay. Deep networks  with large output spaces. arXiv preprint arxiv:1412.7479, 2014.  Vincent, Pascal, de Brbisson, Alexandre, and Bouthillier, Xavier. Efﬁcient exact gradient update for  training deep networks with very large sparse targets. In NIPS, 2015.  Williams, Will, Prasad, Niranjani, Mrva, David, Ash, Tom, and Robinson, Tony. Scaling recurrent  neural network language models. In ICASSP, 2015.  12  Published as a conference paper at ICLR 2016  BlackOut: Speeding up Recurrent Neural Network Language  Models with Very Large Vocabularies  (Supplementary Material)  Proof  i=1 pn(wi|s)) = 1.  A NOISE DISTRIBUTION pn(wi|s) Theorem 1 The noise distribution function pn(wi|s) deﬁned in Eq. 13 is a probability distribution such that ESK∼Q(w)(pn(wi|s)) = Q(wi) and ESK∼Q(w)((cid:80)V function under the expectation that K samples in SK are drawn from Q(w) randomly, SK ∼ Q(w), (cid:88) (cid:88)  (cid:89)    Q(wk) · (cid:88)  ESK∼Q(w)(pn(wi|s)) = ESK∼Q(w)  pθ(wj|s) Q(wj)   1  ESK∼Q(w)  pθ(wj|s)  (cid:88)    Q(wi)  Q(wi)  j∈SK  j∈SK  qj qi  K  K  =  =  pθ(wj|s) Q(wj)  K  wk,∀k∈SK  k∈SK  j∈SK  =  Q(wi)  K  K  = Q(wi)  (cid:33)  pn(wi|s)  =  V(cid:88)  i=1  ESK∼Q(w) (pn(wi|s)) =  V(cid:88)  i=1  (Q(wi)) = 1  (cid:32) V(cid:88)  i=1  ESK∼Q(w)  B PERPLEXITIES ON TRAINING SET  Figure 5: Training perplexity evolution as a function of number of samples K (a) with a full vocab- ulary of 3,720 words, and (b) with the most frequent 2,065 words in vocabulary. The experiments are executed on the RNNLMs with 16 hidden units.  13  050100150200406080100120140160180200220240Ktrain perplexity  NCEBlackOutExact Softmax0501001502004550556065707580Ktrain perplexity  NCEBlackOutExact SoftmaxPublished as a conference paper at ICLR 2016  C SUBNET UPDATE WITH APPROXIMATED RMSPROP  RMSProp (Hinton, 2012) is an adaptive learning rate method that has found much success in prac- tice. Instead of using a single learning rate to all the model parameters in Ω, RMSProp dedicates a learning rate for each model parameter and normalizes the gradient by an exponential moving average of the magnitude of the gradient:  where β ∈ (0, 1) denotes the decay rate. The model update at time step t is then given by  vt = βvt−1 + (1 − β)(∇J)2  θt = θt−1 + (cid:15)  ∇J(θt−1) √ vt + λ  (16)  (17)  where (cid:15) is the learning rate and λ is a damping factor, e.g., λ = 10−6. While RMSProp is one of the most effective learning rate scheduling techniques, it requires a large amount of memory to store per-parameter vt in addition to model parameter Ω and their gradients. It is expensive to access and update large models with billions of parameters. Fortunately, due to the 1-of-V encoding at input layer and the BlackOut sampling at output layer, the model update on Win and Wout is sparse, e.g., only the model parameters corresponding to input/output words and the samples in SK are to be updated.8 For Eq. 16, however, even a model parameter is not involved in the current training, its vt value still needs to be updated by vt = βvt−1 since its (∇J)2 = 0. Ignoring this update has detrimental effect on the predictive performance; in our experiments, we observed 5 − 10 point perplexity loss if we ignore this update completely. We resort to an approximation to vt = βvt−1. Given pu(w) is the probability of a word w being selected for update, the number of time steps elapsed when it is successfully selected follows a geometric distribution with a success rate pu(w), whose mean value is 1/pu(w). Assume that an input/output word is selected according to the unigram distribution puni(w) and the samples in SK are drawn from Qα(w), Eq. 16 can be approximated by  vt ≈ β1/pu vt−n + (1 − β)(∇J)2  with  pu(w) =  (cid:26)puni(w) × B × T  puni(w) × B × T + Qα(w) × K × T  for word w at input layer for word w at output layer,  (18)  (19)  where B is the mini-batch size and T is the BPTT block size. Now we can only update the model parameters, typically a tiny fraction of Ω, that are really involved in the current training, and thus speed up the RNNLM training further.  8The parameter update on Wr is still dense, but its size is several orders of magnitude smaller than those of  Win and Wout.  14  ",
1511.02301,2016,The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations,"[""The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations\nFelix Hill"", 'Antoine Bordes', 'Sumit Chopra', 'Jason Weston']",https://arxiv.org/pdf/1511.02301,"6 1 0 2    r p A 1         ] L C . s c [      4 v 1 0 3 2 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS  Felix Hill∗, Antoine Bordes, Sumit Chopra & Jason Weston Facebook AI Research 770 Broadway New York, USA felix.hill@cl.cam.ac.uk,{abordes,spchopra,jase}@fb.com  ABSTRACT  We introduce a new test of how well language models capture meaning in chil- dren’s books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower- frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previ- ously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting seman- tic content words, although this advantage is not observed for syntactic function words. Interestingly, we ﬁnd that the amount of text encoded in a single memory representation is highly inﬂuential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Fur- ther, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by ap- plying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.  1  INTRODUCTION  Humans do not interpret language in isolation. The context in which words and sentences are un- derstood, whether a conversation, book chapter or road sign, plays an important role in human com- prehension (Altmann & Steedman, 1988; Binder & Desai, 2011). In this work, we investigate how well statistical models can exploit such wider contexts to make predictions about natural language. Our analysis is based on a new benchmark dataset (The Children’s Book Test or CBT) designed to test the role of memory and context in language processing and understanding. The test requires predictions about different types of missing words in children’s books, given both nearby words and a wider context from the book. Humans taking the test predict all types of word with similar levels of accuracy. However, they rely on the wider context to make accurate predictions about named entities or nouns, whereas it is unimportant when predicting higher-frequency verbs or prepositions. As we show, state-of-the-art language modelling architectures, Recurrent Neural Networks (RNNs) with Long-Short Term Memory (LSTMs), perform differently to humans on this task. They are excellent predictors of prepositions (on, at) and verbs (run, eat), but lag far behind humans when predicting nouns (ball, table) or named entities (Elvis, France). This is because their predictions are based almost exclusively on local contexts. In contrast, Memory Networks (Weston et al., 2015b) are one of a class of ‘contextual models’ that can interpret language at a given point in text conditioned directly on both local information and explicit representation of the wider context. On the CBT, Memory Networks designed in a particular way can exploit this information to achieve markedly better prediction of named-entities and nouns than conventional language models. This is important for applications that require coherent semantic processing and/or language generation, since nouns and entities typically encode much of the important semantic information in language.  ∗The majority of this work was done while FH was at Facebook AI Research, and was completed at his  current afﬁliation, University of Cambridge, Computer Laboratory, Cambridge, UK.  1  Published as a conference paper at ICLR 2016  However, not all contextual models reach this level of performance. We ﬁnd the way in which wider context is represented in memory to be critical. If memories are encoded from a small window around important words in the context, there is an optimal size for memory representations between single words and entire sentences, that depends on the class of word to be predicted. We have nick- named this effect the Goldilocks Principle after the well-known English fairytale (Hassall, 1904). In the case of Memory Networks, we also ﬁnd that self-supervised training of the memory access mechanism yields a clear performance boost when predicting named entities, a class of word that has typically posed problems for neural language models. Indeed, we train a Memory Network with these design features to beat the best reported performance on the CNN QA test of entity prediction from news articles (Hermann et al., 2015).  2 THE CHILDREN’S BOOK TEST  The experiments in this paper are based on a new resource, the Children’s Book Test, designed to measure directly how well language models can exploit wider linguistic context. The CBT is built from books that are freely available thanks to Project Gutenberg.1 Using children’s books guarantees a clear narrative structure, which can make the role of context more salient. After allocating books to either training, validation or test sets, we formed example ‘questions’ (denoted x) from chapters in the book by enumerating 21 consecutive sentences. In each question, the ﬁrst 20 sentences form the context (denoted S), and a word (denoted a) is removed from the 21st sentence, which becomes the query (denoted q). Models must identify the answer word a among a selection of 10 candidate answers (denoted C) appearing in the context sentences and the query. Thus, for a question answer pair (x, a): x = (q, S, C); S is an ordered list of sentences; q is a sentence (an ordered list q = q1, . . . ql of words) containing a missing word symbol; C is a bag of unique words such that a ∈ C, its cardinality |C| is 10 and every candidate word w ∈ C is such that w ∈ q ∪ S. An example question is given in Figure 1.  Figure 1: A Named Entity question from the CBT (right), created from a book passage (left, in blue). In this case, the candidate answers C are both entities and common nouns, since fewer than ten named entities are found in the context.  For ﬁner-grained analyses, we evaluated four classes of question by removing distinct types of word: Named Entities, (Common) Nouns, Verbs and Prepositions (based on output from the POS tagger and named-entity-recogniser in the Stanford Core NLP Toolkit (Manning et al., 2014)). For a given question class, the nine incorrect candidates are selected at random from words in the context having the same type as the answer. The exact number of questions in the training, validation and test sets is shown in Table 1. Full details of the candidate selection algorithm (e.g. how candidates are selected if there are insufﬁcient words of a given type in the context) can be found with the dataset.2  1https://www.gutenberg.org/ 2The dataset can be downloaded from http://fb.ai/babi/.  2  Published as a conference paper at ICLR 2016  Classical language modelling evaluations are based on average perplexity across all words in a text. They therefore place proportionally more emphasis on accurate prediction of frequent words such as prepositions and articles than the less frequent words that transmit the bulk of the meaning in language (Baayen & Lieber, 1996). In contrast, because the CBT allows focused analyses on semantic content-bearing words, it should be a better proxy for how well a language model can lend semantic coherence to applications including machine translation, dialogue and question-answering systems.  TRAINING VALIDATION  TEST  NUMBER OF BOOKS NUMBER OF QUESTIONS (CONTEXT+QUERY) AVERAGE WORDS IN CONTEXTS AVERAGE WORDS IN QUERIES DISTINCT CANDIDATES VOCABULARY SIZE  669,343  98  465 31  37,242  5  8,000 435 27  5,485 53,628  5  10,000  445 29  7,108  Table 1: Statistics of the CBT. Breakdown by question class is provided with the data set ﬁles.  2.1 RELATED RESOURCES  There are clear parallels between the CBT and the Microsoft Research Sentence Completion Chal- lenge (MSRCC) (Zweig & Burges, 2011), which is also based on Project Gutenberg (but not chil- dren’s books, speciﬁcally). A fundamental difference is that, where examples in the MSRCC are made of a single sentence, each query in the CBT comes with a wider context. This tests the sen- sitivity of language models to semantic coherence beyond sentence boundaries. The CBT is also larger than the MRSCC (10,000 vs 1,040 test questions), requires models to select from more can- didates on each question (10 vs 5), covers missing words of different (POS) types and contains large training and validation sets that match the form of the test set. There are also similarities between the CBT and the CNN/Daily Mail (CNN QA) dataset recently released by Hermann et al. (2015). This task requires models to identify missing entities from bullet- point summaries of online news articles. The CNN QA task therefore focuses more on paraphrasing parts of a text, rather than making inferences and predictions from contexts as in the CBT. It also differs in that all named entities in both questions and articles are anonymised so that models cannot apply knowledge that is not apparent from the article. We do not anonymise entities in the CBT, as we hope to incentivise models that can apply background knowledge and information from im- mediate and wider contexts to the language understanding problem.3 At the same time, the CBT can be used as a benchmark for general-purpose language models whose downstream application is semantically focused generation, prediction or correction. The CBT is also similar to the MCTest of machine comprehension (Richardson et al., 2013), in which children’s stories written by annotators are accompanied by four multiple-choice questions. However, it is very difﬁcult to train statistical models only on MCTest because its training set consists of only 300 examples.  3 STUDYING MEMORY REPRESENTATION WITH MEMORY NETWORKS  Memory Networks (Weston et al., 2015b) have shown promising performance at various tasks such as reasoning on the bAbI tasks (Weston et al., 2015a) or language modelling (Sukhbaatar et al., 2015). Applying them on the CBT enables us to examine the impact of various ways of encoding context on their semantic processing ability over naturally occurring language.  3.1 ENCODING MEMORIES AND QUERIES  Context sentences of S are encoded into memories, denoted mi, using a feature-map φ(s) mapping sequences of words s ∈ S from the context to one-hot representations in [0, 1]d, where d is typically the size of the word vocabulary. We considered several formats for storing the phrases s: • Lexical memory: Each word occupies a separate slot in the memory (each phrase s is a single word and φ(s) has only one non-zero feature). To encode word order, time features are added as embeddings indicating the index of each memory, following Sukhbaatar et al. (2015).  3See Appendix D for a sense of how anonymisation changes the CBT.  3  Published as a conference paper at ICLR 2016  • Window memory: Each phrase s corresponds to a window of text from the context S centred on an individual mention of a candidate c in S. Hence, memory slots are ﬁlled using windows of words {wi−(b−1)/2 . . . wi . . . wi+(b−1)/2} where wi ∈ C is an instance of one of the candidate words in the question.4 Note that the number of phrases s is typically greater than |C| since candidates can occur multiple times in S. The window size b is tuned on the validation set. We experimented with encoding as a standard bag-of-words, or by having one dictionary per window position, where the latter performed best.  • Sentential memory: This setting follows the original implementation of Memory Networks for the bAbI tasks where the phrases s correspond to complete sentences of S. For the CBT, this means that each question yields exactly 20 memories. We also use Positional Encoding (PE) as introduced by Sukhbaatar et al. (2015) to encode the word positions.  The order of occurrence of memories is less important for sentential and window formats than for lexical memory. So, instead of using a full embedding for each time index, we simply use a scalar value which indicates the position in the passage, ranging from 1 to the number of memories. An additional parameter (tuned on the validation set) scales the importance of this feature. As we show in Appendix C, time features only gave a marginal performance boost in those cases. For sentential and window memory formats, queries are encoded in a similar way to the memories: as a bag-of-words representation of the whole sentence and a window of size b centred around the missing word position respectively. For the lexical memory, memories are made of the n words preceding the word to be predicted, whether these n words come from the context or from the query, and the query embedding is set to a constant vector 0.1.  3.2 END-TO-END MEMORY NETWORKS  The MemN2N architecture, introduced by Sukhbaatar et al. (2015), allows for a direct training of Memory Networks through backpropagation, and consists of two main steps. First, ‘supporting memories’, those useful to ﬁnd the correct answer to the query q, are retrieved. This is done by embedding both the query and all memories into a single space of dimension p using an embedding matrix A ∈ Rp×d yielding the query embedding q = Aφ(q) and memory embeddings {ci = Aφ(si)}i=1,...n, with n the number of memories.The match between q and each memory ci in the embedding space is fed through a softmax layer giving a distribution {αi}i=1,...n of matching scores which are used as an attention mechanism over the memories to return the ﬁrst supporting memory:  (cid:88)  i=1...n  i q(cid:80) ec(cid:62) j ec(cid:62)  j q  mo1 =  αimi ,  with αi =  , i = 1, . . . n,  (1)  and where {mi}i=1,...n is a set of memory embeddings obtained in the same way as the ci, but using another embedding matrix B ∈ Rp×d. A characteristic of Memory Networks is their ability to perform several hops in the memory before returning an answer. Hence the above process can be repeated K times by recursively using qk = qk−1 + mok−1 instead of the original q1 = q. There are several ways of connecting the layers corresponding to distinct hops. We chose to share the embedding matrices A and B across all layers and add a linear mapping across hops, that is qk = Hqk−1 + mok−1 with H ∈ Rp×p. For the lexical memory setting, we also applied ReLU operations to half of the units in each layer following Sukhbaatar et al. (2015).5 In a second stage, an answer distribution ˆa = softmax(UqK+1) is returned given K retrieved mem- ories mo1, . . . moK and the query q. Here, U ∈ Rd×p is a separate weight matrix that can potentially be tied with A, and ˆa ∈ Rd is a distribution over the whole vocabulary. The predicted answer ˆa among candidates is then simply ˆa = arg maxw∈C ˆa(c), with ˆa(w) indicating the probability of word w in ˆa. For the lexical memory variant, ˆa is selected not only by using the probability of each of the ten candidate words, but also of any words that follow the missing word marker in the query.  4See Appendix E for discussion and analysis of using candidates in window representations and training. 5For the lexical memory we use the code available at https://github.com/facebook/MemNN.  4  Published as a conference paper at ICLR 2016  During training, ˆa is used to minimise a standard cross-entropy loss with the true label a against all other words in the dictionary (i.e. the candidates are not used in the training loss), and optimization is carried out using stochastic gradient descent (SGD). Extra experimental details and hyperparameters are given in Appendix A.  3.3 SELF-SUPERVISION FOR WINDOW MEMORIES  After initial experiments, we observed that the capacity to execute multiple hops in accessing mem- ories was only beneﬁcial in the lexical memory model. We therefore also tried a simpler, single-hop Memory Network, i.e. using a single memory to answer, that exploits a stronger signal for learning memory access. A related approach was successfully applied by Bordes et al. (2015) to question answering about knowledge bases. Memory supervision (knowing which memories to attend to) is not provided at training time but is inferred automatically using the following procedure: since we know the correct answer during training, we hypothesize the correct supporting memory to be among the window memories whose corresponding candidate is the correct answer. In the common case where more than one memory contains the correct answer, the model picks the single memory ˜m that is already scored highest by itself, i.e. scored highest by the query in the embedding space deﬁned by A.6 We train by making gradient steps using SGD to force the model, for each example, to give a higher score to the supporting memory ˜m relative to any other memory from any other candidate. Instead of using eq (1), the model selects its top relevant memory using:  mo1 = arg max i=1,...n  c(cid:62) i q .  (2)  If mo1 happens to be different from ˜m, then the model is updated. At test time, rather than use a hard selection as in eq (2) the model scores each candidate not only with its highest scoring memory but with the sum of the scores of all its corresponding windows after passing all scores through a softmax. That is, the score of a candidate is deﬁned by the sum of the αi (as used in eq (1)) of the windows it appears in. This relaxes the effects of the max operation and allows for all windows associated with a candidate to contribute some information about that candidate. As shown in the ablation study in Appendix C, this results in slightly better performance on the CNN QA benchmark compared to hard selection at test time. Note that self-supervised Memory Networks do not exploit any new label information beyond the training data. The approach can be understood as a way of achieving hard attention over memories, to contrast with the soft attention-style selection described in Section 3.2. Hard attention yields signiﬁcant improvements in image captioning (Xu et al., 2015). However, where Xu et al. (2015) use the REINFORCE algorithm (Williams, 1992) to train through the max of eq (2), our self-supervision heuristic permits direct backpropagation.  4 BASELINE AND COMPARISON MODELS  In addition to memory network variants, we also applied many different types of language modelling and machine reading architectures to the CBT.  4.1 NON-LEARNING BASELINES  We implemented two simple baselines based on word frequencies. For the ﬁrst, we selected the most frequent candidate in the entire training corpus. In the second, for a given question we selected the most frequent candidate in its context. In both cases we broke ties with a random choice. We also tried two more sophisticated ways to rank the candidates that do not require any learning on the training data. The ﬁrst is the ‘sliding window’ baseline applied to the MCTest by Richardson et al. (2013). In this method, ten ‘windows’ of the query concatenated with each possible candidate are slid across the context word-by-word, overlapping with a different subsequence at each position.  6TF-IDF similarity worked almost as well in our experiments, but a random choice over positives did not.  5  Published as a conference paper at ICLR 2016  The overlap score at a given position is simply word-overlap weighted TFIDF-style based on fre- quencies in the context (to emphasize less frequent words). The chosen candidate corresponds to the window that achieves the maximum single overlap score for any position. Ties are broken randomly. The second method is the word distance benchmark applied by Hermann et al. (2015). For a given instance of a candidate wi in the context, the query q is ‘superimposed’ on the context so that the missing word lines up with wi, deﬁning a subsequence s of the context. For each word qi in q, an alignment penalty P = min(minj=1...|s|{|i − j| : sj = qi}, m) is incurred. The model predicts the candidate with the instance in the context that incurs the lowest alignment penalty. We tuned the maximum single penalty m = 5 on the validation data.  4.2 N-GRAM LANGUAGE MODELS  We trained an n-gram language model using the KenLM toolkit (Heaﬁeld et al., 2013). We used Knesser-Ney smoothing, and a window size of 5, which performed best on the validation set. We also compare with a variant of language model with cache (Kuhn & De Mori, 1990), where we linearly interpolate the n-gram model probabilities with unigram probabilities computed on the context.  4.3 SUPERVISED EMBEDDING MODELS  To directly test how much of the CBT can be resolved by good quality dense representations of words (word embeddings), we implement a supervised embedding model similar to that of (Weston et al., 2010). In these models we learn both input and output embedding matrices A, B ∈ Rp×d for each word in the vocabulary (p is still the embedding dimension and d the vocabulary size). For a given input passage q and possible answer word w, the score is computed as S(q, w) = φ(q)A(cid:62)Bφ(w), with φ the feature function deﬁned in Section 3. These models can be considered as lobotomised Memory Networks with zero hops, i.e. the attention over the memory component is removed. We encode various parts of the question as the input passage: the entire context + query, just the query, a sub-sequence of the query deﬁned by a window of maximum b words centred around the missing word, and a version (window + position) in which we use a different embedding matrix for encoding each position of the window. We tune the window-size d = 5 on the validation set.  4.4 RECURRENT LANGUAGE MODELS  We trained probabilistic RNN language models with LSTM activation units on the training stories (5.5M words of text) using minibatch SGD to maximise the negative log-likelihood of the next word. Hyper-parameters were tuned on the validation set. The best model had both hidden layer and word embeddings of dimension 512. When answering the questions in the CBT, we allow one variant of this model (context + query) to ‘burn in’ by reading the entire context followed by the query and another version to read only the query itself (and thus have no access to the context). Unlike the canonical language-modelling task, all models have access to the query words after the missing word (i.e if k is the position of the missing word, we rank candidate c based on p(q1 . . . qk−1, c, qk+1 . . . ql) rather than simply p(q1 . . . qk−1, c)). Mikolov & Zweig (2012) previously observed performance boosts for recurrent language models by adding the capacity to jointly learn a document-level representation. We similarly apply a context- based recurrent model to our language-modelling tasks, but opt for the convolutional representation of the context applied by Rush et al. (2015) for summarisation. Our Contextual LSTM (CLSTM) learns a convolutional attention over windows of the context given the objective of predicting all words in the query. We tuned the window size (w = 5) on the validation set. As with the standard LSTM, we trained the CLSTM on the running-text of the CBT training set (rather than the structured query and context format used with the Memory Networks) since this proved much more effective, and we report results in the best setting for each method.  4.5 HUMAN PERFORMANCE  We recruited 15 native English speakers to attempt a randomly-selected 10% from each question type of the CBT, in two modes either with question only or with question+context (shown to different  6  Published as a conference paper at ICLR 2016  PREPOSITIONS  METHODS HUMANS (QUERY)(∗) HUMANS (CONTEXT+QUERY)(∗) MAXIMUM FREQUENCY (CORPUS) MAXIMUM FREQUENCY (CONTEXT) SLIDING WINDOW WORD DISTANCE MODEL KNESER-NEY LANGUAGE MODEL KNESER-NEY LANGUAGE MODEL + CACHE EMBEDDING MODEL (CONTEXT+QUERY) EMBEDDING MODEL (QUERY) EMBEDDING MODEL (WINDOW) EMBEDDING MODEL (WINDOW+POSITION) LSTMS (QUERY) LSTMS (CONTEXT+QUERY) CONTEXTUAL LSTMS (WINDOW CONTEXT) MEMNNS (LEXICAL MEMORY) MEMNNS (WINDOW MEMORY) MEMNNS (SENTENTIAL MEMORY + PE) MEMNNS (WINDOW MEMORY + SELF-SUP.)  0.676 0.708 0.315 0.275 0.101 0.237 0.768 0.679 0.315 0.535 0.589 0.670 0.802 0.791 0.806 0.764 0.674 0.326 0.703 Table 2: Results on CBT test set. (∗)Human results were collected on 10% of the test set.  NAMED ENTITIES COMMON NOUNS VERBS 0.716 0.828 0.373 0.285 0.182 0.380 0.778 0.772 0.421 0.614 0.637 0.736 0.813 0.818 0.805 0.798 0.692 0.502 0.690  0.644 0.816 0.158 0.281 0.196 0.364 0.544 0.577 0.259 0.400 0.415 0.506 0.541 0.560 0.582 0.562 0.554 0.305 0.630  0.520 0.816 0.120 0.335 0.168 0.398 0.390 0.439 0.253 0.351 0.362 0.402 0.408 0.418 0.436 0.431 0.493 0.318 0.666  annotators), giving 2000 answers in total. To our knowledge, this is the ﬁrst time human performance has been quantiﬁed on a language modelling task based on different word types and context lengths.  4.6 OTHER RELATED APPROACHES  The idea of conditioning language models on extra-sentential context is not new. Access to document-level features can improve both classical language models (Mikolov & Zweig, 2012) and word embeddings (Huang et al., 2012). Unlike the present work, these studies did not explore different representation strategies for the wider context or their effect on interpreting and predicting speciﬁc word types. The original Memory Networks (Weston et al., 2015b) used hard memory selection with additional labeled supervision for the memory access component, and were applied to question-answering tasks over knowledge bases or simulated worlds. Sukhbaatar et al. (2015) and Kumar et al. (2015) trained Memory Networks with RNN components end-to-end with soft memory access, and applied them to additional language tasks. The attention-based reading models of Hermann et al. (2015) also have many commonalities with Memory Networks, differing in word representation choices and attention procedures. Both Kumar et al. (2015) and Hermann et al. (2015) propose bidirectional RNNs as a way of representing previously read text. Our experiments in Section 5 provide a possible explana- tion for why this is an effective strategy for semantically-focused language processing: bidirectional RNNs naturally focus on small windows of text in similar way to window-based Memory Networks. Other recent papers have proposed RNN-like architectures with new ways of reading, storing and updating information to improve their capacity to learn algorithmic or syntactic patterns (Joulin & Mikolov, 2015; Dyer et al., 2015; Grefenstette et al., 2015). While we do not study these models in the present work, the CBT would be ideally suited for testing this class of model on semantically- focused language modelling.  5 RESULTS  Modelling syntactic ﬂow In general, there is a clear difference in model performance according to the type of word to be predicted. Our main results in Table 2 show conventional language models are very good at predicting prepositions and verbs, but less good at predicting named entities and nouns. Among these language models, and in keeping with established results, RNNs with LSTMs demonstrate a small gain on n-gram models across the board, except for named entities where the cache is beneﬁcial. In fact, LSTM models are better than humans at predicting prepositions, which suggests that there are cases in which several of the candidate prepositions are ‘correct’, but anno- tators prefer the less frequent one. Even more surprisingly, when only local context (the query) is available, both LSTMs and n-gram models predict verbs more accurately than humans. This may be because the models are better attuned to the distribution of verbs in children’s books, whereas  7  Published as a conference paper at ICLR 2016  humans are unhelpfully inﬂuenced by their wider knowledge of all language styles.7 When access to the full context is available, humans do predict verbs with slightly greater accuracy than RNNs.  Capturing semantic coherence The best performing Memory Networks predict common nouns and named entities more accurately than conventional language models. Clearly, in doing so, these models rely on access to the wider context (the supervised EMBEDDING MODEL (QUERY), which is equivalent to the memory network but with no contextual memory, performs poorly in this regard). The fact that LSTMs without attention perform similarly on nouns and named entities whether or not the context is available conﬁrms that they do not effectively exploit this context. This may be a symptom of the difﬁculty of storing and retaining information across large numbers of time steps that has been previously observed in recurrent networks (See e.g. Bengio et al. (1994)).  Getting memory representations ‘just right’ Not all memory networks that we trained exploited the context to achieve decent prediction of nouns and named entities. For instance, when each sentence in the context is stored as an ordered sequence of word embeddings (sentence mem + PE), performance is quite poor in general. Encoding the context as an unbroken sequence of individual words (lexical memory) works well for capturing prepositions and verbs, but is less effective with nouns and entities. In contrast, window memories centred around the candidate words are more useful than either word-level or sentence-level memories when predicting named entities and nouns.  Figure 2: Correct predictions of MemNNs (window memory + self-supervision) on CBT on Named Entity (left) and Verb (right). Circled phrases indicate all considered windows; red ones are the ones corresponding to the returned (correct) answer; the blue windows represent the queries.  Self-supervised memory retrieval The window-based Memory Network with self-supervision (in which a hard attention selection is made among window memories during training) outperforms all others at predicting named entities and common nouns. Examples of predictions made by this model for two CBT questions are shown in Figure 2. It is notable that this model is able to achieve the strongest performance with only a simple window-based strategy for representing questions.  5.1 NEWS ARTICLE QUESTION ANSWERING  To examine how well our conclusions generalise to different machine reading tasks and language styles, we also tested the best-performing Memory Networks on the CNN QA task (Hermann et al., 2015).8 This dataset consists of 93k news articles from the CNN website, each coupled with a question derived from a bullet point summary accompanying the article, and a single-word answer. The answer is always a named entity, and all named entities in the article function as possible candidate answers.  7We did not require the human annotators warm up by reading the 98 novels in the training data, but this  might have led to a fairer comparison.  8The CNN QA dataset was released after our primary experiments were completed, hence we experiment  only with one of the two large datasets released with that paper.  8  Published as a conference paper at ICLR 2016  METHODS MAXIMUM FREQUENCY (ARTICLE)(∗) SLIDING WINDOW WORD DISTANCE MODEL(∗) DEEP LSTMS (ARTICLE+QUERY)(∗) CONTEXTUAL LSTMS (“ATTENTIVE READER”)(∗) CONTEXTUAL LSTMS (“IMPATIENT READER”)(∗) MEMNNS (WINDOW MEMORY) MEMNNS (WINDOW MEMORY + SELF-SUP.) MEMNNS (WINDOW MEMORY + ENSEMBLE) MEMNNS (WINDOW MEMORY + SELF-SUP. + ENSEMBLE) MEMNNS (WINDOW + SELF-SUP. + ENSEMBLE + EXCLUD. COOCURRENCES)  VALIDATION  0.305 0.005 0.505 0.550 0.616 0.618 0.580 0.634 0.612 0.649 0.662  TEST 0.332 0.006 0.509 0.570 0.630 0.638 0.606 0.668 0.638 0.684 0.694  Table 3: Results on CNN QA. (∗)Results taken from Hermann et al. (2015).  As shown in Table 3, our window model without self-supervision achieves similar performance to the best approach proposed for the task by Hermann et al. (2015) when using an ensemble of MemNN models. Our use of an ensemble is an alternative way of replicating the application of dropout (Hinton et al., 2012) in the previous best approaches (Hermann et al., 2015) as ensemble averaging has similar effects to dropout (Wan et al., 2013). When self-supervision is added, the Memory Network greatly surpasses the state-of-the-art on this task. Finally, the last line of Table 3 (excluding co-occurrences) shows how an additional heuristic, removing from the candidate list all named entities already appearing in the bullet point summary, boosts performance even further. Some common principles may explain the strong performance of the best performing models on this task. The attentive/impatient reading models encode the articles using bidirectional RNNs (Graves et al., 2008). For each word in the article, the combined hidden state of such an RNN naturally focuses on a window-like chunk of surrounding text, much like the window-based memory network or the CLSTM. Together, these results therefore support the principle that the most informative representations of text correspond to sub-sentential chunks. Indeed, the observation that the most informative representations for neural language models correspond to small chunks of text is also consistent with recent work on neural machine translation, in which Luong et al. (2015) demon- strated improved performance by restricting their attention mechanism to small windows of the source sentence. Given these commonalities in how the reading models and Memory Networks represent context, the advantage of the best-performing Memory Network instead seems to stem from how it accesses or retrieves this information; in particular, the hard attention and self-supervision. Jointly learning to access and use information is a difﬁcult optimization. Self-supervision in particular makes effective Memory Network learning more tractable.9  6 CONCLUSION  We have presented the Children’s Book Test, a new semantic language modelling benchmark. The CBT measures how well models can use both local and wider contextual information to make pre- dictions about different types of words in children’s stories. By separating the prediction of syntactic function words from more semantically informative terms, the CBT provides a robust proxy for how much language models can impact applications requiring a focus on semantic coherence. We tested a wide range of models on the CBT, each with different ways of representing and retaining previously seen content. This enabled us to draw novel insights into the optimal strategies for repre- senting and accessing semantic information in memory. One consistent ﬁnding was that memories that encode sub-sentential chunks (windows) of informative text seem to be most useful to neural nets when interpreting and modelling language. However, our results indicate that the most useful text chunk size depends on the modeling task (e.g. semantic content vs. syntactic function words). We showed that Memory Networks that adhere to this principle can be efﬁciently trained using a simple self-supervision to surpass all other methods for predicting named entities on both the CBT and the CNN QA benchmark, an independent test of machine reading.  9See the appendix for an ablation study in which optional features of the memory network are removed.  9  Published as a conference paper at ICLR 2016  ACKNOWLEDGMENTS  The authors would like to thank Harsha Pentapelli and Manohar Paluri for helping to collect the human annotations and Gabriel Synnaeve for processing the QA CNN data.  REFERENCES Altmann, Gerry and Steedman, Mark. Interaction with context during human sentence processing.  Cognition, 30(3):191–238, 1988.  Baayen, R Harald and Lieber, Rochelle. Word frequency distributions and lexical semantics. Com-  puters and the Humanities, 30(4):281–291, 1996.  Bengio, Yoshua, Simard, Patrice, and Frasconi, Paolo. Learning long-term dependencies with gra-  dient descent is difﬁcult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.  Binder, Jeffrey R and Desai, Rutvik H. The neurobiology of semantic memory. Trends in cognitive  sciences, 15(11):527–536, 2011.  Bordes, Antoine, Usunier, Nicolas, Chopra, Sumit, and Weston, Jason. Large-scale simple question  answering with memory networks. arXiv preprint arXiv:1506.02075, 2015.  Dyer, Chris, Ballesteros, Miguel, Ling, Wang, Matthews, Austin, and Smith, Noah A. Transition- In Proceedings of the Annual  based dependency parsing with stack long short-term memory. Meeting of the Association for Computational Linguistics, 2015.  Graves, Alex, Liwicki, Marcus, Bunke, Horst, Schmidhuber, J¨urgen, and Fern´andez, Santiago. Un- constrained on-line handwriting recognition with recurrent neural networks. In Advances in Neu- ral Information Processing Systems, pp. 577–584, 2008.  Grefenstette, Edward, Hermann, Karl Moritz, Suleyman, Mustafa, and Blunsom, Phil. Learning to  transduce with unbounded memory. NIPS, 2015.  Hassall, John. Goldilocks and the three bears. In The Old Nursery Stories and Rhymes. Blackie &  Son: London, 1904.  Heaﬁeld, Kenneth, Pouzyrevsky, Ivan, Clark, Jonathan H., and Koehn, Philipp. Scalable modi- ﬁed Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pp. 690–696, Soﬁa, Bulgaria, August 2013.  Hermann, Karl Moritz, Koˇcisk´y, Tom´aˇs, Grefenstette, Edward, Espeholt, Lasse, Kay, Will, Suley- In Advances man, Mustafa, and Blunsom, Phil. Teaching machines to read and comprehend. in Neural Information Processing Systems (NIPS), 2015. URL http://arxiv.org/abs/ 1506.03340.  Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Rus- lan R. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.  Huang, Eric H, Socher, Richard, Manning, Christopher D, and Ng, Andrew Y.  Improving word In Proceedings of the 50th representations via global context and multiple word prototypes. Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pp. 873–882. Association for Computational Linguistics, 2012.  Joulin, Armand and Mikolov, Tomas. Inferring algorithmic patterns with stack-augmented recurrent  nets. NIPS, 2015.  Kuhn, Roland and De Mori, Renato. A cache-based natural language model for speech recognition.  Pattern Analysis and Machine Intelligence, IEEE Transactions on, 12(6):570–583, 1990.  Kumar, Ankit, Irsoy, Ozan, Su, Jonathan, Bradbury, James, English, Robert, Pierce, Brian, On- druska, Peter, Gulrajani, Ishaan, and Socher, Richard. Ask me anything: Dynamic memory net- works for natural language processing. http://arxiv.org/abs/1506.07285, 2015.  10  Published as a conference paper at ICLR 2016  Luong, Minh-Thang, Pham, Hieu, and Manning, Christopher D. Effective approaches to attention-  based neural machine translation. Proceedings of EMNLP, 2015.  Manning, Christopher D, Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J, and Mc- Closky, David. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55–60, 2014.  Mikolov, Tomas and Zweig, Geoffrey. Context dependent recurrent neural network language model.  In SLT, pp. 234–239, 2012.  Richardson, Matthew, Burges, Christopher JC, and Renshaw, Erin. Mctest: A challenge dataset for  the open-domain machine comprehension of text. In EMNLP, volume 1, pp. 2, 2013.  Rush, Alexander M, Chopra, Sumit, and Weston, Jason. A neural attention model for abstractive  sentence summarization. Proceedings of EMNLP, 2015.  Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and Fergus, Rob. End-to-end memory net-  works. Proceedings of NIPS, 2015.  Wan, Li, Zeiler, Matthew, Zhang, Sixin, Cun, Yann L, and Fergus, Rob. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 1058–1066, 2013.  Weston, Jason, Bengio, Samy, and Usunier, Nicolas. Large scale image annotation: learning to rank  with joint word-image embeddings. Machine learning, 81(1):21–35, 2010.  Weston, Jason, Bordes, Antoine, Chopra, Sumit, and Mikolov, Tomas. Towards ai-complete question  answering: a set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015a.  Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Memory networks. Proceedings of ICLR,  2015b.  Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement  learning. Machine learning, 8(3-4):229–256, 1992.  Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Courville, Aaron, Salakhutdinov, Ruslan, Zemel, Richard, and Bengio, Yoshua. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the International Conference on Machine Learning (ICML’15), 2015. URL http://arxiv.org/abs/1502.03044.  Zweig, Geoffrey and Burges, Christopher JC. The microsoft research sentence completion chal-  lenge. Technical report, Technical Report MSR-TR-2011-129, Microsoft, 2011.  A EXPERIMENTAL DETAILS  Setting The text of questions is lowercased for all Memory Networks as well as for all non- learning baselines. LSTMs models use the raw text (although we also tried lowercasing, which made little difference). Hyperparameters of all learning models have been set using grid search on the validation set. The main hyperparameters are embedding dimension p, learning rate λ, window size b, number of hops K, maximum memory size n (n = all means using all potential memories). All models were implemented using the Torch library (see torch.ch). For CBT, all models have been trained on all question types altogether. We did not try to experiment with word embeddings pre-trained on a bigger corpus.  Optimal hyper-parameter values on CBT:  • Embedding model (context+query): p = 300, λ = 0.01. • Embedding model (query): p = 300, λ = 0.01. • Embedding model (window): p = 300, λ = 0.005, b = 5.  11  Published as a conference paper at ICLR 2016  learning rate shrinking factor: 2.  • Embedding model (window+position): p = 300, λ = 0.01, b = 5. • LSTMs (query & context+query): p = 512, λ = 0.5, 1 layer, gradient clipping factor: 5, • Contextual LSTMs: p = 256, λ = 0.5, 1 layer, gradient clipping factor: 10, learning rate • MemNNs (lexical memory): n = 200, λ = 0.01, p = 200, K = 7. • MemNNs (window memory): n = all, b = 5, λ = 0.005, p = 100, K = 1. • MemNNs (sentential memory + PE): n = all, λ = 0.001, p = 100, K = 1. • MemNNs (window memory + self-sup.): n = all, b = 5, λ = 0.01, p = 300.  shrinking factor: 2.  Optimal hyper-parameter values on CNN QA:  • MemNNs (window memory): n = all, b = 5, λ = 0.005, p = 100, K = 1. • MemNNs (window memory + self-sup.): n = all, b = 5, λ = 0.025, p = 300, K = 1. • MemNNs (window memory + ensemble): 7 models with b = 5. • MemNNs (window memory + self-sup. + ensemble): 11 models with b = 5.  B RESULTS ON CBT VALIDATION SET METHODS MAXIMUM FREQUENCY (CORPUS) MAXIMUM FREQUENCY (CONTEXT) SLIDING WINDOW WORD DISTANCE MODEL KNESER-NEY LANGUAGE MODEL KNESER-NEY LANGUAGE MODEL + CACHE EMBEDDING MODEL (CONTEXT+QUERY) EMBEDDING MODEL (QUERY) EMBEDDING MODEL (WINDOW) EMBEDDING MODEL (WINDOW+POSITION) LSTMS (QUERY) LSTMS (CONTEXT+QUERY) CONTEXTUAL LSTMS (WINDOW CONTEXT) MEMNNS (LEXICAL MEMORY) MEMNNS (WINDOW MEMORY) MEMNNS (SENTENTIAL MEMORY + PE) MEMNNS (WINDOW MEMORY + SELF-SUP.)  NAMED ENTITIES COMMON NOUNS VERBS 0.301 0.219 0.200 0.332 0.762 0.755 0.368 0.575 0.622 0.722 0.811 0.820 0.803 0.818 0.693 0.451 0.688  0.192 0.273 0.199 0.371 0.577 0.612 0.297 0.462 0.486 0.555 0.613 0.626 0.628 0.647 0.591 0.342 0.642  0.052 0.299 0.178 0.436 0.481 0.500 0.235 0.418 0.457 0.488 0.500 0.512 0.535 0.519 0.542 0.297 0.704  PREPOSITIONS  0.346 0.312 0.091 0.259 0.791 0.693 0.356 0.560 0.619 0.683 0.819 0.812 0.798 0.785 0.704 0.360 0.696  C ABLATION STUDY ON CNN QA TEST METHODS 0.684 MEMNNS (WINDOW MEMORY + SELF-SUP. + EXCLUD. COOCURRENCES) 0.668 MEMNNS (WINDOW MEMORY + SELF-SUP.) 0.659 MEMNNS (WINDOW MEM. + SELF-SUP.) -TIME 0.620 MEMNNS (WINDOW MEM. + SELF-SUP.) -SOFT MEMORY WEIGHTING 0.613 MEMNNS (WINDOW MEM. + SELF-SUP.) -TIME -SOFT MEMORY WEIGHTING 0.684 MEMNNS (WINDOW MEM. + SELF-SUP. + ENSEMBLE) 0.679 MEMNNS (WINDOW MEM. + SELF-SUP. + ENSEMBLE) -TIME 0.641 MEMNNS (WINDOW MEM. + SELF-SUP. + ENSEMBLE) -SOFT MEMORY WEIGHTING MEMNNS (WINDOW MEM. + SELF-SUP. + ENSEMBLE) -TIME -SOFT MEMORY WEIGHTING 0.640 (Soft memory weighting: the softmax to select the best candidate in test as deﬁned in Section 3.3)  0.635 0.634 0.625 0.604 0.592 0.649 0.642 0.612 0.600  VALIDATION  D EFFECTS OF ANONYMISING ENTITIES IN CBT METHODS MEMNNS (WORD MEM.) MEMNNS (WINDOW MEM.) MEMNNS (SENTENCE MEM.+PE) MEMNNS (WINDOW MEM.+SELF-SUP.) ANONYMIZED MEMNNS (WINDOW +SELF-SUP.)  NAMED ENTITIES COMMON NOUNS VERBS 0.798 0.692 0.502 0.690 0.474  0.562 0.554 0.305 0.630 0.473  0.431 0.493 0.318 0.666 0.581  PREPOSITIONS  0.764 0.674 0.326 0.703 0.522  To see the impact of the anonymisation of entities and words as done in CNN QA on the self- supervised Memory Networks on the CBT, we conducted an experiment where we replaced the  12  Published as a conference paper at ICLR 2016  mentions of the ten candidates in each question by anonymised placeholders in train, validation and test. The table above shows results on CBT test set in an anonymised setting (last row) compared to MemNNs in a non-anonymised setting (rows 2-5). Results indicate that this has a relatively low impact on named entities but a larger one on more syntactic tasks like prepositions or verbs.  E CANDIDATES AND WINDOW MEMORIES IN CBT  In our main results in Table 2 the window memory is constructed as the set of windows over the candidates being considered for a given question. Training of MEMNNS (WINDOW MEMORY) is performed by making gradient steps for questions, with the true answer word as the target compared against all words in the dictionary as described in Sec. 3.2. Training of MEMNNS (WINDOW MEMORY + SELF-SUP.) is performed by making gradient steps for questions, with the true answer word as the target compared against all other candidates as described in Sec. 3.3. As MEMNNS (WINDOW MEMORY + SELF-SUP.) is the best performing method for named entities and common nouns, to see the impact of these choices we conducted some further experiments with variants of it. Firstly, window memories do not have to be restricted to candidates, we could consider all possible windows. Note that this does not make any difference at evaluation time on CBT as one would still evaluate by multiple choice using the candidates, and those extra windows would not contribute to the scores of the candidates. However, this may make a difference to the weights if used at training time. We call this “all windows” in the experiments to follow. Secondly, the self-supervision process does not have to rely on there being known candidates: all that is required is a positive label, in that case we can perform gradient steps with the true answer word as the target compared against all words in the dictionary (as opposed to only candidates) as described in Sec. 3.2, while still using hard attention supervision as described in 3.3. We call this “all targets” in the experiments to follow. Thirdly, one does not have to try to train on only the questions in CBT, but can treat the children’s books as a standard language modeling task. In that case, all targets and all windows must be used, as multiple choice questions have not been constructed for every single word (although indeed many of them are covered by the four word classes). We call this “LM” (for language modeling) in the experiments to follow. Results with these alternatives are presented in Table 4, the new variants are the last three rows. Overall, the differing approaches have relatively little impact on the results, as all of them provide superior results on named entities and common nouns than without self-supervision. However, we note that the use of all windows or LM rather than candidate windows does impact training and testing speed.  METHODS MEMNNS (LEXICAL MEMORY) MEMNNS (WINDOW MEMORY) MEMNNS (SENTENTIAL MEMORY + PE) MEMNNS (WINDOW MEMORY + SELF-SUP.) MEMNNS (ALL WINDOWS + SELF-SUP.) MEMNNS (ALL WINDOWS + ALL TARGETS + SELF-SUP.) MEMNNS (LM + SELF-SUP.)  NAMED ENTITIES COMMON NOUNS VERBS 0.798 0.692 0.502 0.690 0.711 0.698 0.692  0.431 0.493 0.318 0.666 0.648 0.639 0.638  0.562 0.554 0.305 0.630 0.604 0.602 0.605  PREPOSITIONS  0.764 0.674 0.326 0.703 0.693 0.667 0.647  Table 4: Results on CBT test set when considering all windows or targets.  13  ",
1511.08198,2016,Towards Universal Paraphrastic Sentence Embeddings,"['Towards Universal Paraphrastic Sentence Embeddings [code]\nJohn Wieting', 'Mohit Bansal', 'Kevin Gimpel', 'Karen Livescu']",https://arxiv.org/pdf/1511.08198,"6 1 0 2    r a  M 4         ] L C . s c [      3 v 8 9 1 8 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  TOWARDS UNIVERSAL PARAPHRASTIC SENTENCE EMBEDDINGS  John Wieting Mohit Bansal Kevin Gimpel Karen Livescu Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA {jwieting,mbansal,kgimpel,klivescu}@ttic.edu  ABSTRACT  We consider the problem of learning general-purpose, paraphrastic sentence em- beddings based on supervision from the Paraphrase Database (Ganitkevitch et al., 2013). We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training data and from a wide range of other domains. We ﬁnd that the most complex ar- chitectures, such as long short-term memory (LSTM) recurrent neural networks, perform best on the in-domain data. However, in out-of-domain scenarios, sim- ple architectures such as word averaging vastly outperform LSTMs. Our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efﬁcient and easy to use. In order to better understand how these architectures compare, we conduct further experiments on three supervised NLP tasks: sentence similarity, entailment, and sentiment classiﬁcation. We again ﬁnd that the word averaging models perform well for sentence similarity and entailment, outperforming LSTMs. However, on sentiment classiﬁcation, we ﬁnd that the LSTM performs very strongly—even recording new state-of-the-art performance on the Stanford Sentiment Treebank. We then demonstrate how to combine our pretrained sentence embeddings with these supervised tasks, using them both as a prior and as a black box feature extractor. This leads to performance rivaling the state of the art on the SICK similarity and entailment tasks. We release all of our resources to the research community1 with the hope that they can serve as the new baseline for further work on universal sentence embeddings.  1  INTRODUCTION  Word embeddings have become ubiquitous in natural language processing (NLP). Several re- searchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015). There has also been recent work on creating representations for word sequences such as phrases or sentences. Many functional architectures have been proposed to model compositionality in such sequences, ranging from those based on simple operations like addition (Mitchell & Lapata, 2010; Yu & Dredze, 2015; Iyyer et al., 2015) to those based on richly-structured functions like re- cursive neural networks (Socher et al., 2011), convolutional neural networks (Kalchbrenner et al., 2014), and recurrent neural networks using long short-term memory (LSTM) (Tai et al., 2015). However, there is little work on learning sentence representations that can be used across domains with the same ease and effectiveness as word embeddings. In this paper, we explore compositional models that can encode arbitrary word sequences into a vector with the property that sequences with similar meaning have high cosine similarity, and that can, importantly, also transfer easily across domains. We consider six compositional architectures based on neural networks and train them on noisy phrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013).  1Trained  models  and  code  for  http://ttic.uchicago.edu/˜wieting.  training  and  evaluation  are  available  at  1  Published as a conference paper at ICLR 2016  We consider models spanning the range of complexity from word averaging to LSTMs. With the simplest word averaging model, there are no additional compositional parameters. The only param- eters are the word vectors themselves, which are learned to produce effective sequence embeddings when averaging is performed over the sequence. We add complexity by adding layers, leading to variants of deep averaging networks (Iyyer et al., 2015). We next consider several recurrent network variants, culminating in LSTMs because they have been found to be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015).  To evaluate our models, we consider two tasks drawn from the same distribution as the training data, as well as 22 SemEval textual similarity datasets from a variety of domains (such as news, tweets, web forums, and image and video captions). Interestingly, we ﬁnd that the LSTM performs well on the in-domain task, but performs much worse on the out-of-domain tasks. We discover surprisingly strong performance for the models based on word averaging, which perform well on both the in- domain and out-of-domain tasks, beating the best LSTM model by 16.5 Pearson’s r on average. Moreover, we ﬁnd that learning word embeddings in the context of vector averaging performs much better than simply averaging pretrained, state-of-the-art word embeddings. Our average Pearson’s r over all 22 SemEval datasets is 17.1 points higher than averaging GloVe vectors2 and 12.8 points higher than averaging PARAGRAM-SL999 vectors.3 Our ﬁnal sentence embeddings4 place in the top 25% of all submitted systems in every SemEval STS task from 2012 through 2015, being best or tied for best on 4 of the datasets.5 This is surprising because the submitted systems were designed for those particular tasks, with access to training and tuning data speciﬁcally developed for each task.  While the above experiments focus on transfer, we also consider the fully supervised setting (Ta- ble 5). We compare the same suite of compositional architectures for three supervised NLP tasks: sentence similarity and textual entailment using the 2014 SemEval SICK dataset (Marelli et al., 2014), and sentiment classiﬁcation using the Stanford Sentiment Treebank (Socher et al., 2013). We again ﬁnd strong performance for the word averaging models for both similarity and entailment, out- performing the LSTM. However, for sentiment classiﬁcation, we see a different trend. The LSTM now performs best, achieving 89.2% on the coarse-grained sentiment classiﬁcation task. This result, to our knowledge, is the new state of the art on this task.  We then demonstrate how to combine our PPDB-trained sentence embedding models with super- vised NLP tasks. We ﬁrst use our model as a prior, yielding performance on the similarity and entailment tasks that rivals the state of the art. We also use our sentence embeddings as an effective black box feature extractor for downstream tasks, comparing favorably to recent work (Kiros et al., 2015).  We release our strongest sentence embedding model, which we call PARAGRAM-PHRASE XXL, to the research community.6 Since it consists merely of a new set of word embeddings, it is extremely efﬁcient and easy to use for downstream applications. Our hope is that this model can provide a new simple and strong baseline in the quest for universal sentence embeddings.  2 RELATED WORK  Researchers have developed many ways to embed word sequences for NLP. They mostly focus on the question of compositionality: given vectors for words, how should we create a vector for a word sequence? Mitchell & Lapata (2008; 2010) considered bigram compositionality, compar- ing many functions for composing two word vectors into a single vector to represent their bigram. Follow-up work by Blacoe & Lapata (2012) found again that simple operations such as vector ad-  2We used the publicly available 300-dimensional vectors that were trained on the 840 billion token Common  Crawl corpus, available at http://nlp.stanford.edu/projects/glove/.  3These  are  at http://ttic.uchicago.edu/˜wieting. They give human-level performance on two commonly used word similarity datasets, WordSim353 (Finkelstein et al., 2001) and Simlex-999 (Hill et al., 2015).  from Wieting et al.  300-dimensional  available  vectors  (2015)  and  are  4Denoted PARAGRAM-PHRASE-XXL and discussed in Section 4.3. 5As measured by the average Pearson’s r over all datasets in each task; see Table 4. 6Available at http://ttic.uchicago.edu/˜wieting.  2  Published as a conference paper at ICLR 2016  dition performed strongly. Many other compositional architectures have been proposed. Some have been based on distributional semantics (Baroni et al., 2014; Paperno et al., 2014; Polajnar et al., 2015; Tian et al., 2015), while the current trend is toward development of neural network archi- tectures. These include neural bag-of-words models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al., 2015), feature-weighted averaging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2012; 2013; ˙Irsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on non-syntactic hierarchical structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Sch¨utze, 2015; He et al., 2015), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015). In this paper, we compare six architectures: word averaging, word averaging followed by a single linear projection, DANs, and three variants of recurrent neural networks, including LSTMs.7 Most of the work mentioned above learns compositional models in the context of supervised learn- ing. That is, a training set is provided with annotations and the composition function is learned for the purposes of optimizing an objective function based on those annotations. The models are then evaluated on a test set drawn from the same distribution as the training set.  In this paper, in contrast, we are primarily interested in creating general purpose, domain indepen- dent embeddings for word sequences. There have been research efforts also targeting this goal. One approach is to train an autoencoder in an attempt to learn the latent structure of the sequence, whether it be a sentence with a parse tree (Socher et al., 2011), or a longer sequence such as a paragraph or document (Li et al., 2015b). Other recently proposed methods, including paragraph vectors (Le & Mikolov, 2014) and skip-thought vectors (Kiros et al., 2015), learn sequence repre- sentations that are predictive of words inside the sequence or in neighboring sequences. These methods produce generic representations that can be used to provide features for text classiﬁcation or sentence similarity tasks. While skip-thought vectors capture similarity in terms of discourse con- text, in this paper we are interested in capturing paraphrastic similarity, i.e., whether two sentences have the same meaning.  Our learning formulation draws from a large body of related work on learning input repre- sentations in order to maximize similarity in the learned space (Weston et al., 2010; Yih et al., 2011; Huang et al., 2013; Hermann & Blunsom, 2014; Socher et al., 2014; Faruqui & Dyer, 2014; Bordes et al., 2014b;a; Lu et al., 2015), including our prior work (Wieting et al., 2015). We focus our exploration here on modeling and keep the learning methodology mostly ﬁxed, though we do include certain choices about the learning procedure in our hyperparameter tuning space for each model.  3 MODELS AND TRAINING  Our goal is to embed sequences into a low-dimensional space such that cosine similarity in the space corresponds to the strength of the paraphrase relationship between the sequences. We exper- imented with six models of increasing complexity. The simplest model embeds a word sequence x = hx1, x2, ..., xni by averaging the vectors of its tokens. The only parameters learned by this model are the word embedding matrix Ww:  gPARAGRAM-PHRASE (x) =  1 n  W xi w  n  Xi  where W xi PHRASE embeddings.  w is the word embedding for word xi. We call the learned embeddings PARAGRAM-  In our second model, we learn a projection in addition to the word embeddings:  gproj(x) = Wp  1  n  W xi  w ! + b  n  Xi  7In prior work, we experimented with recursive neural networks on binarized parses of  the PPDB (Wieting et al., 2015), but we found that many of the phrases in PPDB are not sentences or even con- stituents, causing the parser to have unexpected behavior.  3  Published as a conference paper at ICLR 2016  where Wp is the projection matrix and b is a bias vector. Our third model is the deep averaging network (DAN) of Iyyer et al. (2015). This is a generalization of the above models that typically uses multiple layers as well as nonlinear activation functions. In our experiments below, we tune over the number of layers and choice of activation function.  Our fourth model is a standard recurrent network (RNN) with randomly initialized weight matrices and nonlinear activations:  ht = f (WxW xt gRNN(x) = h−1  w + Whht−1 + b)  where f is the activation function (either tanh or rectiﬁed linear unit; the choice is tuned), Wx and Wh are parameter matrices, b is a bias vector, and h−1 refers to the hidden vector of the last token. Our ﬁfth model is a special RNN which we call an identity-RNN. In the identity-RNN, the weight matrices are initialized to identity, the bias is initialized to zero, and the activation is the identity function. We divide the ﬁnal output vector of the identity-RNN by the number of tokens in the sequence. Thus, before any updates to the parameters, the identity-RNN simply averages the word embeddings. We also regularize the identity-RNN parameters to their initial values. The idea is that, with high regularization, the identity-RNN is simply averaging word embeddings. However, it is a richer architecture and can take into account word order and hopefully improve upon the averaging baseline.  is the most expressive. We use long short-term memory Our sixth and ﬁnal model (LSTM) (Hochreiter & Schmidhuber, 1997), a recurrent neural network (RNN) architecture de- signed to model sequences with long-distance dependencies. LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015). We use the version from Gers et al. (2003) which has the following equations:  w + Whiht−1 + Wcict−1 + bi) w + Whf ht−1 + Wcf ct−1 + bf )  w + Whcht−1 + bc)  w + Whoht−1 + Wcoct + bo)  it = σ (WxiW xt ft = σ (Wxf W xt ct = ftct−1 + it tanh (WxcW xt ot = σ (WxoW xt ht = ot tanh(ct) gLSTM(x) = h−1  where σ is the logistic sigmoid function. We found that the choice of whether or not to include the output gate had a signiﬁcant impact on performance, so we used two versions of the LSTM model, one with the output gate and one without. For all models, we learn the word embeddings themselves, denoting the trainable word embedding parameters by Ww. We denote all other trainable parameters by Wc (“compositional parameters”), though the PARAGRAM-PHRASE model has no compositional parameters. We initialize Ww using some embeddings pretrained from large corpora.  3.1 TRAINING  We mostly follow the approach of Wieting et al. (2015). The training data consists of (possibly noisy) pairs taken directly from the original Paraphrase Database (PPDB) and we optimize a margin- based loss. Our training data consists of a set X of phrase pairs hx1, x2i, where x1 and x2 are assumed to be paraphrases. The objective function follows:  min Wc,Ww  1  |X|  Xhx1,x2i∈X  max(0, δ − cos(g(x1), g(x2)) + cos(g(x1), g(t1)))  + max(0, δ − cos(g(x1), g(x2)) + cos(g(x2), g(t2)))(cid:19)  +λc kWck2 + λw kWwinitial − Wwk2  (1)  4  Published as a conference paper at ICLR 2016  where g is the embedding function in use (e.g., gLSTM), δ is the margin, λc and λw are regulariza- tion parameters, Wwinitial is the initial word embedding matrix, and t1 and t2 are carefully-selected negative examples taken from a mini-batch during optimization. The intuition is that we want the two phrases to be more similar to each other (cos(g(x1), g(x2))) than either is to their respective negative examples t1 and t2, by a margin of at least δ.  3.1.1 SELECTING NEGATIVE EXAMPLES  To select t1 and t2 in Eq. 1, we tune the choice between two approaches. The ﬁrst, MAX, simply chooses the most similar phrase in some set of phrases (other than those in the given phrase pair). For simplicity and to reduce the number of tunable parameters, we use the mini-batch for this set, but it could be a separate set. Formally, MAX corresponds to choosing t1 for a given hx1, x2i as follows:  t1 =  argmax  cos(g(x1), g(t))  t:ht,·i∈Xb\{hx1,x2i}  where Xb ⊆ X is the current mini-batch. That is, we want to choose a negative example ti that is similar to xi according to the current model parameters. The downside of this approach is that we may occasionally choose a phrase ti that is actually a true paraphrase of xi. The second strategy selects negative examples using MAX with probability 0.5 and selects them randomly from the mini-batch otherwise. We call this sampling strategy MIX. We tune over the strategy in our experiments.  4 EXPERIMENTS  4.1 DATA  We experiment on 24 textual similarity datasets, covering many domains, including all datasets from every SemEval semantic textual similarity (STS) task (2012-2015). We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks that use PPDB data (Wieting et al., 2015; Pavlick et al., 2015).  The ﬁrst STS task was held in 2012 and these tasks have been held every year since. Given two sen- tences, the objective of the task is to predict how similar they are on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 different datasets and the tasks cover a wide variety of domains which we have cat- egorized below. Most submissions for these tasks use supervised models that are trained and tuned on either provided training data or similar datasets from older tasks. Details on the number of teams and submissions for each task and the performance of the submitted systems for each dataset are included in Table 1 and Table 2 respectively. For more details on these tasks please refer to the rel- evant publications for the 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013), 2014 (Agirre et al., 2014), and 2015 (Agirre et al., 2015) tasks.  Dataset 2012 STS 2013 STS 2014 STS 2015 STS 2014 SICK 2015 Twitter  No. of teams No. of submissions  35 34 15 29 17 19  88 89 38 74 66 26  Table 1: Details on numbers of teams and submissions in the STS tasks used for evaluation.  Below are the textual domains contained in the STS tasks: News: Newswire was used in the 2012 task (MSRpar) and the 2013 and 2014 tasks (deft news). Image and Video Descriptions: Image descriptions generated via crowdsourcing were used in the 2013 and 2014 tasks (images). Video descriptions were used in the 2012 task (MSRvid). Glosses: Glosses from WordNet, OntoNotes, and FrameNet were used in the 2012, 2013, and 2014 tasks (OnWN and FNWN).  5  Published as a conference paper at ICLR 2016  MT evaluation: The output of machine translation systems with their reference translations was used in the 2012 task (SMT-eur and SMT-news) and the 2013 task (SMT). Headlines: Headlines of news articles were used in the 2013, 2014, and 2015 tasks (headline). Web Forum: Forum posts were used in the 2014 task (deft forum). Twitter: Pairs containing a tweet related to a news headline and a sentence pertaining to the same news headline. This dataset was used in the 2014 task (tweet news). Belief: Text from the Deft Committed Belief Annotation (LDC2014E55) was used in the 2015 task (belief). Questions and Answers: Paired answers to the same question from StackExchange (answers- forums) and the BEETLE corpus (Dzikovska et al., 2010) (answers-students) were used in 2015.  For tuning, we use two datasets that contain PPDB phrase pairs scored by human annotators on the strength of their paraphrase relationship. One is a large sample of 26,456 annotated phrase pairs developed by Pavlick et al. (2015). The second, called Annotated-PPDB, was developed in our prior work (Wieting et al., 2015) and is a small set of 1,000 annotated phrase pairs that were ﬁltered to focus on challenging paraphrase phenomena.  4.2 TRANSFER LEARNING  4.2.1 EXPERIMENTAL SETTINGS  As training data, we used the XL section8 of PPDB which contains 3,033,753 unique phrase pairs. However, for hyperparameter tuning we only used 100k examples sampled from PPDB XXL and trained for 5 epochs. Then after ﬁnding the hyperparameters that maximize Spearman’s ρ on the Pavlick et al. PPDB task, we trained on the entire XL section of PPDB for 10 epochs. We used PARAGRAM-SL999 embeddings to initialize the word embedding matrix (Ww) for all models. We chose the Pavlick et al. task for tuning because we wanted our entire procedure to only make use of PPDB and use no other resources. In particular, we did not want to use any STS tasks for training or hyperparameter tuning. We chose the Pavlick et al. dataset over Annotated-PPDB due to its larger size. But in practice the datasets are very similar and tuning on either produces similar results.  To learn model parameters for all experiments in this section, we minimize Eq. 1. Our models have the following tunable hyperparameters:9 λc, the L2 regularizer on the compositional param- eters Wc (not applicable for the word averaging model), the pool of phrases used to obtain neg- ative examples (coupled with mini-batch size B, to reduce the number of tunable hyperparame- ters), λw, the regularizer on the word embeddings, and δ, the margin. We also tune over opti- mization method (either AdaGrad (Duchi et al., 2011) or Adam (Kingma & Ba, 2014)), learning rate (from {0.05, 0.005, 0.0005}), whether to clip the gradients with threshold 1 (Pascanu et al., 2012), and whether to use MIX or MAX sampling. For the classic RNN, we further tuned whether to use tanh or rectiﬁed linear unit activation functions; for the identity-RNN, we tuned λc over {1000, 100, 10, 1} because we wanted higher regularization on the composition parameters; for the DANs we tuned over activation function (tanh or rectiﬁed linear unit) and the number of layers (either 1 or 2); for the LSTMs we tuned on whether to include an output gate. We ﬁx the output dimensionalities of all models that require doing so to the dimensionality of our word embeddings (300).  4.2.2 RESULTS  The results on all STS tasks as well as the SICK and Twitter tasks are shown in Table 2. We include results on the PPDB tasks in Table 3. In Table 2, we ﬁrst show the median, 75th percentile, and highest score from the ofﬁcial task rankings. We then report the performance of our seven mod- els: PARAGRAM-PHRASE (PP), identity-RNN (iRNN), projection (proj.), deep-averaging network (DAN), recurrent neural network (RNN), LSTM with output gate (o.g.), and LSTM without output  8PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The phrases are sorted by a conﬁdence measure and so the smaller sets contain higher precision paraphrases.  9For λc we searched over {10−3, 10−4, 10−5, 10−6}, for b we searched over {25, 50, 100}, for λw we searched over {10−5, 10−6, 10−7, 10−8} as well as the setting in which we do not update Ww, and for δ we searched over {0.4, 0.6, 0.8}.  6  Published as a conference paper at ICLR 2016  gate (no o.g.). We compare to three baselines: skip-thought vectors10 (Kiros et al., 2015), denoted “ST”, averaged GloVe11 vectors (Pennington et al., 2014), and averaged PARAGRAM-SL999 vec- tors (Wieting et al., 2015), denoted “PSL”. Note that the GloVe vectors were used to initialize the PARAGRAM-SL999 vectors which were, in turn, used to initialize our PARAGRAM-PHRASE embed- dings. We compare to skip-thought vectors because trained models are publicly available and they show impressive performance when used as features on several tasks including textual similarity.  Dataset  50% 75% Max  PP  proj. DAN RNN iRNN  LSTM (no o.g.)  MSRpar MSRvid SMT-eur OnWN SMT-news STS 2012 Average headline OnWN FNWN SMT STS 2013 Average deft forum deft news headline images OnWN tweet news STS 2014 Average answers-forums answers-students belief headline images STS 2015 Average 2014 SICK 2015 Twitter  51.5 75.5 44.4 608 40.1 54.5 64.0 52.8 32.7 31.8 45.3 36.6 66.2 67.1 75.6 78.0 64.7 64.7 61.3 67.6 67.7 74.2 80.4 70.2 71.4 49.9  57.6 80.3 48.1 65.9 45.4 59.5 68.3 64.8 38.1 34.6 51.4 46.8 74.0 75.4 79.0 81.1 72.2 71.4 68.2 73.6 72.2 80.8 84.3 75.8 79.9 52.5  73.4 88.0 56.7 72.7 60.9 70.3 78.4 84.3 58.2 40.4 65.3 53.1 78.5 78.4 83.4 87.5 79.2 76.7 73.9 78.8 77.2 84.2 87.1 80.2 82.8 61.9  42.6 74.5 47.3 70.6 58.4 58.7 72.4 67.7 43.9 39.2 55.8 48.7 73.1 69.7 78.5 78.8 76.4 70.9 68.3 78.2 76.2 74.8 81.4 75.8 71.6 52.9  43.7 74.0 49.4 70.1 62.8 60.0 72.6 68.0 46.8 39.8 56.8 51.1 72.2 70.8 78.1 79.5 75.8 71.3 65.1 77.8 75.4 75.2 80.3 74.8 71.6 52.8  40.3 70.0 43.8 65.9 60.0 56.0 71.2 64.1 43.1 38.3 54.2 49.0 71.7 69.2 76.9 75.7 74.2 69.5 62.6 78.1 72.0 73.5 77.5 72.7 70.7 53.7  18.6 66.5 40.9 63.1 51.3 48.1 59.5 54.6 30.9 33.8 44.7 41.5 53.7 57.5 67.6 67.7 58.0 57.7 32.8 64.7 51.9 65.3 71.4 57.2 61.2 45.1  43.4 73.4 47.1 70.1 58.1 58.4 72.8 69.4 45.3 39.4 56.7 49.0 72.4 70.2 78.2 78.8 76.9 70.9 67.4 78.2 75.9 75.1 81.1 75.6 71.2 52.9  16.1 71.3 41.8 65.2 60.8 51.0 57.4 68.5 24.7 30.1 45.2 44.2 52.8 57.5 68.5 76.9 58.7 59.8 51.9 71.5 61.7 64.0 70.4 63.9 63.9 47.6  LSTM (o.g.) 9.3 71.3 44.3 56.4 51.0 46.4 48.5 50.4 38.4 28.8 41.5 46.1 39.1 50.9 62.9 61.7 48.2 51.5 50.7 55.7 52.6 56.6 64.2 56.0 59.0 36.1  ST  GloVe  PSL  16.8 41.7 35.2 29.7 30.8 30.8 34.6 10.0 30.4 24.3 24.8 12.9 23.5 37.8 51.2 23.3 39.9 31.4 36.1 33.0 24.6 43.6 17.7 31.0 49.8 24.7  47.7 63.9 46.0 55.1 49.6 52.5 63.8 49.0 34.2 22.3 42.3 27.1 68.0 59.5 61.0 58.4 51.2 54.2 30.5 63.0 40.5 61.8 67.5 52.7 65.9 30.3  41.6 60.0 42.4 63.0 57.0 52.8 68.8 48.0 37.9 31.0 46.4 37.2 67.0 65.3 62.0 61.1 64.7 59.5 38.8 69.2 53.2 69.0 69.9 60.0 66.4 36.3  Table 2: Results on SemEval textual similarity datasets (Pearson’s r × 100). The highest score in each row is in boldface (omitting the ofﬁcial task score columns). The results in Table 2 show strong performance of our two simplest models: the PARAGRAM- PHRASE embeddings (PP) and our projection model (proj.). They outperform the other models on all but 5 of the 22 datasets. The iRNN model has the next best performance, while the LSTM models lag behind. These results stand in marked contrast to those in Table 3, which shows very similar performance across models on the in-domain PPDB tasks, with the LSTM models slightly outperforming the others. For the LSTM models, it is also interesting to note that removing the output gate results in stronger performance on the textual similarity tasks. Removing the output gate improves performance on 18 of the 22 datasets. The LSTM without output gate also performs reasonably well compared to our strong PARAGRAM-SL999 addition baseline, beating it on 12 of the 22 datasets.  4.3  PARAGRAM-PHRASE XXL  Since we found that PARAGRAM-PHRASE embeddings have such strong performance, we trained this model on more data from PPDB and also used more data for hyperparameter tuning. For tuning, we used all of PPDB XL and trained for 10 epochs, then trained our ﬁnal model for 10 epochs on the entire phrase section of PPDB XXL, consisting of 9,123,575 unique phrase pairs.12 We show the results of this improved model, which we call PARAGRAM-PHRASE XXL, in Table 4. We also report the median, 75th percentile, and maximum score from our suite of textual similarity tasks.  10Note that we pre-processed the training data with the tokenizer from Stanford CoreNLP (Manning et al., 2014) rather than the included NLTK (Bird et al., 2009) tokenizer. We found that doing so signiﬁcantly im- proves the performance of the skip-thought vectors.  11We used the publicly available 300-dimensional vectors that were trained on the 840 billion token Common  Crawl corpus, available at http://nlp.stanford.edu/projects/glove/.  12We ﬁxed batchsize to 100 and δ to 0.4, as these were the optimal values for the experiment in Table 2. Then, for λw we searched over {10−6, 10−7, 10−8}, and tuned over MIX and MAX sampling. To optimize, we used AdaGrad with a learning rate of 0.05.  7  Published as a conference paper at ICLR 2016  Model  PARAGRAM-PHRASE projection DAN RNN iRNN LSTM (no o.g.) LSTM (o.g.) skip-thought GloVe PARAGRAM-SL999  Pavlick et al.  (oracle)  60.3 61.0 60.9 60.5 60.3 61.6 61.5 39.3 44.8 55.3  Pavlick et al.  (test) 60.0 58.4 60.1 60.3 60.0 61.3 60.9 39.3 44.8 55.3  Annotated-PPDB  (test) 53.5 52.8 52.3 51.8 53.9 53.4 52.9 31.9 25.3 40.4  Table 3: Results on the PPDB tasks (Spearman’s ρ × 100). For the task in Pavlick et al. (2015), we include the oracle result (the max Spearman’s ρ on the dataset), since this dataset was used for model selection for all other tasks, as well as test results where models were tuned on Annotated-PPDB.  Dataset  50% 75% Max  MSRpar MSRvid SMT-eur OnWN SMT-news STS 2012 Average headline OnWN FNWN SMT STS 2013 Average deft forum deft news headline images OnWN tweet news STS 2014 Average answers-forums answers-students belief headline images STS 2015 Average 2014 SICK∗ 2015 Twitter  51.5 75.5 44.4 60.8 40.1 54.5 64.0 52.8 32.7 31.8 45.3 36.6 66.2 67.1 75.6 78.0 64.7 64.7 61.3 67.6 67.7 74.2 80.4 70.2 71.4 49.9  57.6 80.3 48.1 65.9 45.4 59.5 68.3 64.8 38.1 34.6 51.4 46.8 74.0 75.4 79.0 81.1 72.2 71.4 68.2 73.6 72.2 80.8 84.3 75.8 79.9 52.5  73.4 88.0 56.7 72.7 60.9 70.3 78.4 84.3 58.2 40.4 65.3 53.1 78.5 78.4 83.4 87.5 79.2 76.7 73.9 78.8 77.2 84.2 87.1 80.2 82.8 61.9  PARAGRAM-  PHRASE-  XXL 44.8 79.6 49.5 70.4 63.3 61.5 73.9 73.8 47.7 40.4 58.9 53.4 74.4 71.5 80.4 81.5 77.4 73.1 69.1 78.0 78.2 76.4 83.4 77.0 72.7 52.4  Table 4: Results on SemEval textual similarity datasets (Pearson’s r ×100) for PARAGRAM-PHRASE XXL embeddings. Results that match or exceed the best shared task system are shown in bold. ∗For the 2014 SICK task, the median, 75th percentile, and maximum include only the primary runs as the full set of results was not available.  PARAGRAM-PHRASE XXL matches or exceeds the best performance on 4 of the datasets (SMT- news, SMT, deft forum, and belief) and is within 3 points of the best performance on 8 out of 22. We have made this trained model available to the research community.13  4.4 USING REPRESENTATIONS IN LEARNED MODELS  We explore two natural questions regarding our representations learned from PPDB: (1) can these embeddings improve the performance of other models through initialization and regularization? (2) can they effectively be used as features for downstream tasks? To address these questions, we  13Available at http://ttic.uchicago.edu/˜wieting.  8  Published as a conference paper at ICLR 2016  Task  similarity (SICK) entailment (SICK) binary sentiment (SST)  word  averaging  86.40 84.6 83.0  proj.  85.93 84.0 83.0  DAN  85.96 84.5 83.4  RNN  73.13 76.4 86.5  LSTM LSTM (o.g.) (no o.g.) 83.41 82.0 89.2  85.45 83.2 86.6  w/ universal regularization  86.84 85.3 86.9  Table 5: Results from supervised training of each compositional architecture on similarity, entail- ment, and sentiment tasks. The last column shows results regularizing to our universal parameters from the models in Table 2. The ﬁrst row shows Pearson’s r × 100 and the last two show accuracy.  used three tasks: The SICK similarity task, the SICK entailment task, and the Stanford Sentiment Treebank (SST) binary classiﬁcation task (Socher et al., 2013). For the SICK similarity task, we minimize the objective function14 from Tai et al. (2015). Given a score for a sentence pair in the range [1, K], where K is an integer, with sentence representations hL and hR, and model parameters θ, they ﬁrst compute:  h× = hL ⊙ hR, h+ = |hL − hR|,  hs = σ(cid:16)W (×)h× + W (+)h+ + b(h)(cid:17) , ˆpθ = softmax(cid:16)W (p)hs + b(p)(cid:17) ,  ˆy = rT ˆpθ,  where rT = [1 2 . . . K]. They then deﬁne a sparse target distribution p that satisﬁes y = rT p:  y − ⌊y⌋, ⌊y⌋ − y + 1, 0  i = ⌊y⌋ + 1 i = ⌊y⌋ otherwise  pi =   for 1 ≤ i ≤ K. Then they use the following loss, the regularized KL-divergence between p and ˆpθ:  J(θ) =  1 m  m  Xk=1  KL(cid:16)p(k) (cid:13)(cid:13)(cid:13)  ˆp(k)  θ (cid:17),  (2)  where m is the number of training pairs and where we always use L2 regularization on all composi- tional parameters15 but omit these terms for clarity. We use nearly the same model for the entailment task, with the only differences being that the ﬁnal softmax layer has three outputs and the cost function is the negative log-likelihood of the class labels. For sentiment, since it is a binary sentence classiﬁcation task, we ﬁrst encoded the sentence and then used a fully-connected layer with a sigmoid activation followed by a softmax layer with two outputs. We used negative log-likelihood of the class labels as the cost function. All models use L2 regularization on all parameters, except for the word embeddings, which are regularized back to their initial values with an L2 penalty. We ﬁrst investigated how these models performed in the standard setting, without using any models trained using PPDB data. We tuned hyperparameters on the development set of each dataset16 as well as on two optimization schemes: AdaGrad with learning rate of 0.05 and Adam with a learning rate of 0.001. We trained the models for 10 epochs and initialized the word embeddings with PARAGRAM-SL999 embeddings.  14This objective function has been shown to perform very strongly on text similarity tasks, signiﬁcantly  better than squared or absolute error.  15Word embeddings are regularized toward their initial state. 16For all models, we tuned batch-size over {25, 50, 100}, output dimension over {50, 150, 300}, λc over {10−3, 10−4, 10−5, 10−6}, λs = λc, and λw over {10−3, 10−4, 10−5, 10−6, 10−7, 10−8} as well as the option of not updating the embeddings for all models except the word averaging model. We again ﬁx the output dimensionalities of all models which require this speciﬁcation, to the dimensionality of our word embeddings (300). Additionally, for the classic RNN, we further tuned whether to use tanh or rectiﬁed linear unit activation functions; for the DANs we tuned over activation function (tanh or rectiﬁed linear unit) and the number of layers (either 1 or 2).  9  Published as a conference paper at ICLR 2016  The results are shown in Table 5. We ﬁnd that using word averaging as the compositional archi- tecture outperforms the other architectures for similarity and entailment. However, for sentiment classiﬁcation, the LSTM is much stronger than the averaging models. This suggests that the supe- riority of a compositional architecture can vary widely depending on the evaluation, and motivates future work to compare these architectures on additional tasks.  These results are very competitive with the state of the art on these tasks. Recent strong results on the SICK similarity task include 86.86 using a convolutional neural network (He et al., 2015) and 86.76 using a tree-LSTM (Tai et al., 2015). For entailment, the best result we are aware of is 85.1 (Beltagy et al., 2015). On sentiment, the best previous result is 88.1 (Kim, 2014), which our LSTM surprisingly outperforms by a signiﬁcant margin. We note that these experiments simply compare compositional architectures using only the provided training data for each task, tuning on the respective development sets. We did not use any PPDB data for these results, other than that used to train the initial PARAGRAM-SL999 embeddings. Our results appear to show that standard neural architectures can perform surprisingly well given strong word embeddings and thorough tuning over the hyperparameter space.  4.4.1 REGULARIZATION AND INITIALIZATION TO IMPROVE TEXTUAL SIMILARITY MODELS  In this setting, we initialize each respective model to the parameters learned from PPDB (calling them universal parameters) and augment Eq. 2 with three separate regularization terms with the following weights: λs which regularizes the classiﬁcation parameters (the two layers used in the classiﬁcation step after obtaining representations), λw for regularizing the word parameters toward the learned Ww from PPDB, and λc for regularizing the compositional parameters (for all models except for the word averaging model) back to their initial values.17 In all cases, we regularize to the universal parameters using L2 regularization. The results are shown in the last column of Table 5, and we only show results for the best per- forming models on each task (word averaging for similarity/entailment, LSTM with output gate for sentiment). Interestingly, it seems that regularizing to our universal parameters signiﬁcantly improves results for the similarity and entailment tasks which are competitive or better than the state-of-the-art, but harms the LSTM’s performance on the sentiment classiﬁcation task.  4.4.2 REPRESENTATIONS AS FEATURES  Task  similarity (SICK) entailment (SICK) binary sentiment (SST)  PARAGRAM-PHRASE 300 2400 84.94 82.15 83.1 80.2 79.7 79.4  1200 82.85 80.1 78.8  skip-thought  uni-skip 84.77  bi-skip 84.05  - -  - -  Table 6: Results from supervised training on similarity, entailment, and sentiment tasks, except that we keep the sentence representations ﬁxed to our PARAGRAM-PHRASE model. The ﬁrst row shows Pearson’s r × 100 and the last two show accuracy, with boldface showing the highest score in each row.  We also investigate how our PARAGRAM-PHRASE embeddings perform as features for supervised tasks. We use a similar set-up as in Kiros et al. (2015) and encode the sentences by averaging our PARAGRAM-PHRASE embeddings and then just learn the classiﬁcation parameters without updating the embeddings. To provide a more apt comparison to skip-thought vectors, we also learned a linear projection matrix to increase dimensionality of our PARAGRAM-PHRASE embeddings. We chose 1200 and 2400 dimensions in order to both see the dependence of dimension on performance, and so that they can be compared fairly with skip-thought vectors. Note that 2400 dimensions is the same dimensionality as the uni-skip and bi-skip models in Kiros et al. (2015).  The 300 dimension case corresponds to the PARAGRAM-PHRASE embeddings from Table 2. We tuned our higher dimensional models on PPDB as described previously in Section 4.2.2 before train-  17We tuned λs over {10−3, 10−4, 10−5, 10−6}, λc over {10−2, 10−3, 10−4, 10−5, 10−6}, and λw over  {10−3, 10−4, 10−5, 10−6, 10−7, 10−8}. All other hyperparameters were tuned as previously described.  10  Published as a conference paper at ICLR 2016  ing on PPDB XL.18 Then we trained the same models for the similarity, entailment, and sentiment tasks as described in Section 4.4 for 20 epochs. We again tuned λs over {10−3, 10−4, 10−5, 10−6} and tuned over the two optimization schemes of AdaGrad with learning rate of 0.05 and Adam with a learning rate of 0.001. Note that we are not updating the word embeddings or the projection matrix during training.  The results are shown in Table 6. The similarity and entailment tasks show clear improvements as we project the embeddings into the 2400 dimensional space. In fact, our results outperform both types of skip-thought embeddings on the single task that we overlap. However, the sentiment task does not beneﬁt from higher dimensional representations, which is consistent with our regulariza- tion experiments in which sentiment also did not show improvement. Therefore, it seems that our models learned from PPDB are more effective for similarity tasks than classiﬁcation tasks, but this hypothesis requires further investigation.  5 DISCUSSION  It is interesting that the LSTM, with or without output gates, is outperformed by much simpler models on the similarity and entailment tasks studied in this paper. We now consider possible explanations for this trend.  The ﬁrst hypothesis we test is based on length. Since PPDB contains short text snippets of a few words, the LSTM may not know how to handle the longer sentences that occur in our evaluation tasks. If this is true, the LSTM would perform much better on short text snippets and its performance would degrade as their length increases. To test this hypothesis, we took all 12,108 pairs from the 20 SemEval STS tasks and binned them by length.19 We then computed the Pearson’s r for each bin. The results are shown in Table 7 and show that while the LSTM models do perform better on the shortest text pairs, they are still outperformed, at all lengths, by the PARAGRAM-PHRASE model.20  Max Length  ≤ 4 5 6 7 8 9  ≥ 10  PARAGRAM-  PHRASE  72.7 74.0 70.5 73.7 75.5 73.0 72.6  LSTM (no o.g.)  63.4 54.5 52.6 56.9 60.2 58.0 55.6  LSTM (o.g.) 58.8 48.4 48.2 50.6 52.4 48.8 53.8  PARAGRAM-  SL999 66.3 65.0 50.1 56.4 60.1 58.8 58.4  Table 7: Performance (Pearson’s r × 100) as a function of the maximum number of tokens in the sentence pairs over all 20 SemEval STS datasets.  We next consider whether the LSTM has worse generalization due to overﬁtting on the training data. To test this, we analyzed how the models performed on the training data (PPDB XL) by computing the average difference between the cosine similarity of the gold phrase pairs and the negative examples.21 We found that all models had very similar scores: 0.7535, 0.7572, 0.7565, and 0.7463 for PARAGRAM-PHRASE, projection, LSTM (o.g.), and LSTM (no o.g.). This, along with the similar performance of the models on the PPDB tasks in Table 3, suggests that overﬁtting is not the cause of the worse performance of the LSTM model.  Lastly, we consider whether the LSTM’s weak performance was a result of insufﬁcient tuning or optimization. We ﬁrst note that we actually ran more hyperparameter tuning experiments for the  18Note that we ﬁxed batch-size to 100, δ to 0.4, and used MAX sampling as these were the optimal parameters for the PARAGRAM-PHRASE embeddings. We tuned the other hyperparameters as described in Section 4.2.2 with the exception of λc which was tuned over {10−4, 10−5, 10−6, 10−7, 10−8}.  19For each pair, we computed the number of tokens in each of the two pieces of text, took the max, and then  binned based on this value.  20Note that for the analysis in Sections 5 and 6, the models used were selected from earlier experiments.  They are not the same as those used to obtain the results in Table 2.  21More precisely, for each gold pair hg1, g2i, and ni, the respective negative example of each gi, we com-  puted 2 · cos(g1, g2) − cos(n1, g1) − cos(n2, g2) and averaged this value over all pairs.  11  Published as a conference paper at ICLR 2016  LSTM models than either the PARAGRAM-PHRASE or projection models, since we tuned the deci- sion to use an output gate. Secondly, we note that Tai et al. (2015) had a similar LSTM result on the SICK dataset (Pearson’s r of 85.28 to our 85.45) to show that our LSTM implementation/tuning procedure is able to match or exceed performance of another published LSTM result. Thirdly, the similar performance across models on the PPDB tasks (Table 3) suggests that no model had a large advantage during tuning; all found hyperparameters that comfortably beat the PARAGRAM-SL999 addition baseline. Finally, we point out that we tuned over learning rate and optimization strategy, as well as experimented with clipping gradients, in order to rule out optimization issues.  5.1 UNDER-TRAINED EMBEDDINGS  One limitation of our new PARAGRAM-PHRASE vectors is that many of our embeddings are under- trained. The number of unique tokens occurring in our training data, PPDB XL, is 37,366. However, the number of tokens appearing more than 100 times is just 7,113. Thus, one clear source of im- provement for our model would be to address under-trained embeddings for tokens appearing in our test data.  In order to gauge the effect under-trained embeddings and unknown words have on our model, we calculated the fraction of words in each of our 22 SemEval datasets that do not occur at least 100 times in PPDB XL along with our performance deviation from the 75th percentile of each dataset. We found that this fraction had a Spearman’s ρ of -45.1 with the deviation from the 75th percentile indicating that there is a signiﬁcant negative correlation between the fraction of OOV words and performance on these STS tasks.  5.2 USING MORE PPDB  5.2.1 PERFORMANCE VERSUS AMOUNT OF TRAINING DATA  Models in related work such as Kiros et al. (2015) and Li et al. (2015a) require signiﬁcant training time on GPUs, on the order of multiple weeks. Moreover, dependence of model performance upon training data size is unclear. To investigate this dependence for our PARAGRAM-PHRASE model, we trained on different amounts of data and plotted the performance. The results are shown in Figure 1. We start with PPDB XL which has 3,033,753 unique phrase pairs and then divide by two until there are fewer than 10 phrase pairs.22 For each data point (each division by two), we trained a model with that number of phrase pairs for 10 epochs. We use the average Pearson correlation for all 22 datasets in Table 2 as the dependent variable in our plot.  We experimented with two different ways of selecting training data. The ﬁrst (“Ordered”) retains the order of the phrase pairs in PPDB, which ensures the smaller datasets contain higher conﬁdence phrase pairs. The second (“Random”) randomly permutes PPDB XL before constructing the smaller datasets. In both methods, each larger dataset contains the previous one plus as many new phase pairs.  We make three observations about the plot in Figure 1. The ﬁrst is that performance continually increases as more training data is added. This is encouraging as our embeddings can continually improve with more data. Secondly, we note the sizable improvement (4 points) over the PARAGRAM- SL999 baseline by training on just 92 phrase pairs from PPDB. Finally, we note the difference between randomly permuting the training data and using the order from PPDB (which reﬂects the conﬁdence that the phrases in each pair possess the paraphrase relationship). Performance of the randomly permuted data is usually slightly better than that of the ordered data, until the performance gap vanishes once half of PPDB XL is used. We suspect this behavior is due to the safe phrase pairs that occur in the beginning of PPDB. These high-conﬁdence phrase pairs usually have only slight differences and therefore are not as useful for training our model.  6 QUALITATIVE ANALYSIS  To explore other differences between our PARAGRAM-PHRASE vectors and the PARAGRAM-SL999 vectors that were used for initialization, we inspected lists of nearest neighbors in each vector space.  22The smallest dataset contained 5 pairs.  12  Published as a conference paper at ICLR 2016  Performance vs. Training Data Size  0.7  0.65  0.6  0.55  r  s ’ n o s r a e P e g a r e v A  Random Ordered  PARAGRAM-SL999  GloVe  0.5  0  1  2  3  4  5  6  7  Log10 of Training Data Size  Figure 1: Performance of the PARAGRAM-PHRASE embeddings as measured by the average Pear- son’s r on 22 textual similarity datasets versus the amount of training data from PPDB on a log scale. Each datapoint contains twice as much training data as the previous one. Random and Ordered refer to whether we shufﬂed the XL paraphrase pairs from PPDB or kept them in order. We also show baselines of averaging PARAGRAM-SL999 and GloVe embeddings.  Word unlike 2 ladies lookin disagree  PARAGRAM-PHRASE Nearest Neighbors contrary, contrast, opposite, versa, conversely, opposed, contradiction 2.0, two, both, ii, 2nd, couple, 02 girls, daughters, honorable, females, girl, female, dear staring, looking, watching, look, searching, iooking, seeking agree, concur, agreeing, differ, accept  PARAGRAM-SL999 Nearest Neighbors than, although, whilst, though, albeit, kinda, alike 2.0, 3, 1, b, ii, two, 2nd gentlemen, colleague, fellow, girls, mr, madam, dear doin, goin, talkin, sayin, comin, outta, somethin disagreement, differ, dispute, difference, disagreements  Table 8: Nearest neighbors of PARAGRAM-PHRASE and PARAGRAM-SL999 word embeddings sorted by cosine similarity.  When obtaining nearest neighbors, we restricted our search to the 10,000 most common tokens in PPDB XL to ensure that the PARAGRAM-PHRASE vectors were not too under-trained. Some informative neighbors are shown in Table 8. In the ﬁrst four rows, we see that the PARAGRAM- PHRASE embeddings have neighbors with a strong paraphrasing relationship. They tend to avoid having neighbors that are antonyms or co-hyponyms such as unlike and alike or 2 and 3 which are an issue for the PARAGRAM-SL999 embeddings. In contrast to the ﬁrst four rows, the last row shows a problematic effect of our bag-of-words composition function: agree is the nearest neighbor of disagree. The reason for this is that there are numerous pairs in PPDB XL such as i disagree and i do not agree that encourage disagree and agree to have high cosine similarity. A model that takes context into account could resolve this issue. The difﬁculty would be ﬁnding a model that does so while still generalizing well, as we found that our PARAGRAM-PHRASE embeddings generalize better than learning a weight matrix or using a recurrent neural network. We leave this for future work.  When we take a closer look at our PARAGRAM-PHRASE embeddings, we ﬁnd that information- bearing content words, such as poverty, kidding, humanitarian, 18, and july have the largest L2 norms, while words such as of, it, to, hereby and the have the smallest. Pham et al. (2015) noted this same phenomenon in their closely-related compositional model. Interestingly, we found that this weighting explains much of the success of our model. In order to quantify exactly how much, we calculated a weight for each token in our working vocabulary23 simply by summing up the absolute  23This corresponds to the 42,091 tokens that appear in the intersection of our PARAGRAM-SL999 vocabulary,  the test sets of all STS tasks in our evaluation, and PPDB XL plus an unknown word token.  13  Published as a conference paper at ICLR 2016  value of all components of its PARAGRAM-PHRASE vector. Then we multiplied each weight by its corresponding PARAGRAM-SL999 word vector. We computed the average Pearson’s r over all 22 datasets in Table 2. The PARAGRAM-SL999 vectors have an average correlation of 54.94, the PARAGRAM-PHRASE vectors have 66.83, and the scaled PARAGRAM-SL999 vectors, where each is multiplied by its computed weight, have an average Pearson’s r of 62.64. Therefore, it can be surmised that at least 64.76% of the improvement over the initial PARAGRAM-SL999 vectors is due to weighting tokens by their importance.24 We also investigated the connection between these multiplicative weights and word frequency. To do so, we calculated the frequency of all tokens in PPDB XL.25 We then normalized these by the total number of tokens in PPDB XL and used the reciprocal of these scores as the multiplicative weights. Thus less frequent words have more weight than more frequent words. With this baseline weighting method, the average Pearson’s r is 45.52, indicating that the weights we obtain for these words are more sophisticated than mere word frequency. These weights are potentially useful for other applications that can beneﬁt from modeling word importance, such as information retrieval.  7 CONCLUSION  We introduced an approach to create universal sentence embeddings and propose our model as the new baseline for embedding sentences, as it is simple, efﬁcient, and performs strongly across a broad range of tasks and domains. Moreover, our representations do not require the use of any neu- ral network architecture. The embeddings can be simply averaged for a given sentence in an NLP application to create its sentence embedding. We also ﬁnd that our representations can improve gen- eral text similarity and entailment models when used as a prior and can achieve strong performance even when used as ﬁxed representations in a classiﬁer. Future work will focus on improving our embeddings by effectively handling undertrained words as well as by exploring new models that generalize even better to the large suite of text similarity tasks used in our experiments.  ACKNOWLEDGMENTS  We would like to thank Yoon Kim, the anonymous reviewers, and the area chair for their valu- able comments. We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and thank NVIDIA Corporation for donating GPUs used in this research.  REFERENCES  Agirre, Eneko, Diab, Mona, Cer, Daniel, and Gonzalez-Agirre, Aitor. SemEval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics, 2012.  Agirre, Eneko, Cer, Daniel, Diab, Mona, Gonzalez-Agirre, Aitor, and Guo, Weiwei. *SEM 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computa- tional Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, 2013.  Agirre, Eneko, Banea, Carmen, Cardie, Claire, Cer, Daniel, Diab, Mona, Gonzalez-Agirre, Aitor, Guo, Weiwei, Mihalcea, Rada, Rigau, German, and Wiebe, Janyce. SemEval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), 2014.  24We also trained a model in which we only a learn a single multiplicative parameter for each word in our vocabulary, keeping the word embeddings ﬁxed to the PARAGRAM-SL999 embeddings. We trained for 10 epochs on all phrase pairs in PPDB XL. The resulting average Pearson’s r, after tuning on the Pavlick et al. PPDB task, was 62.06, which is slightly lower than using the absolute value of each PARAGRAM-PHRASE vector as its multiplicative weight.  25Tokens that did not appear in PPDB XL were assigned a frequency of 1.  14  Published as a conference paper at ICLR 2016  Agirre, Eneko, Banea, Carmen, Cardie, Claire, Cer, Daniel, Diab, Mona, Gonzalez-Agirre, Aitor, Guo, Weiwei, Lopez-Gazpio, Inigo, Maritxalar, Montse, Mihalcea, Rada, Rigau, German, Uria, Larraitz, and Wiebe, Janyce. SemEval-2015 task 2: Semantic textual similarity, English, Span- ish and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), 2015.  Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Tailoring continuous word representations for dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2014.  Baroni, Marco, Bernardi, Raffaela, and Zamparelli, Roberto. Frege in space: A program of compo-  sitional distributional semantics. Linguistic Issues in Language Technology, 9, 2014.  Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements, 2012.  Belinkov, Yonatan and Glass, James. Arabic diacritization with recurrent neural networks. In Pro- ceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015.  Beltagy, Islam, Roller, Stephen, Cheng, Pengxiang, Erk, Katrin, and Mooney, Raymond J. arXiv preprint  Representing meaning with a combination of logical form and vectors. arXiv:1505.06816, 2015.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010.  Bird, Steven, Klein, Ewan, and Loper, Edward. Natural language processing with Python. O’Reilly  Media, Inc., 2009.  Blacoe, William and Lapata, Mirella. A comparison of vector-based representations for semantic In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural  composition. Language Processing and Computational Natural Language Learning, 2012.  Bordes, Antoine, Chopra, Sumit, and Weston, Jason. Question answering with subgraph embed- dings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP), 2014a.  Bordes, Antoine, Weston, Jason, and Usunier, Nicolas. Open question answering with weakly super- vised embedding models. In Machine Learning and Knowledge Discovery in Databases. Springer, 2014b.  Chen, Xinchi, Qiu, Xipeng, Zhu, Chenxi, Liu, Pengfei, and Huang, Xuanjing. Long short-term memory neural networks for Chinese word segmentation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015a.  Chen, Xinchi, Qiu, Xipeng, Zhu, Chenxi, Wu, Shiyu, and Huang, Xuanjing. Sentence modeling with gated recursive neural network. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015b.  Collobert, Ronan, Weston, Jason, Bottou, L´eon, Karlen, Michael, Kavukcuoglu, Koray, and Kuksa,  Pavel. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12, 2011.  Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning  and stochastic optimization. J. Mach. Learn. Res., 12, 2011.  Dzikovska, Myroslava O, Moore, Johanna D, Steinhauser, Natalie, Campbell, Gwendolyn, Farrow, Elaine, and Callaway, Charles B. Beetle II: a system for tutoring and computational linguistics experimentation. In Proceedings of the ACL 2010 System Demonstrations, 2010.  15  Published as a conference paper at ICLR 2016  Faruqui, Manaal and Dyer, Chris. Improving vector space word representations using multilingual correlation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, 2014.  Filippova, Katja, Alfonseca, Enrique, Colmenares, Carlos A., Kaiser, Lukasz, and Vinyals, Oriol. Sentence compression by deletion with LSTMs. In Proceedings of the 2015 Conference on Em- pirical Methods in Natural Language Processing, 2015.  Finkelstein, Lev, Gabrilovich, Evgeniy, Matias, Yossi, Rivlin, Ehud, Solan, Zach, Wolfman, Gadi, and Ruppin, Eytan. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web. ACM, 2001.  Ganitkevitch, Juri, Durme, Benjamin Van, and Callison-Burch, Chris. Ppdb: The paraphrase  database. In HLT-NAACL. The Association for Computational Linguistics, 2013.  Gers, Felix A, Schraudolph, Nicol N, and Schmidhuber, J¨urgen. Learning precise timing with lstm  recurrent networks. The Journal of Machine Learning Research, 3, 2003.  Graves, Alex, Liwicki, Marcus, Bunke, Horst, Schmidhuber, J¨urgen, and Fern´andez, Santiago. Un- constrained on-line handwriting recognition with recurrent neural networks. In Advances in Neu- ral Information Processing Systems 20. 2008.  Graves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep re- current neural networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.  Greff, Klaus, Srivastava, Rupesh Kumar, Koutn´ık, Jan, Steunebrink, Bas R, and Schmidhuber,  J¨urgen. LSTM: A search space odyssey. arXiv preprint arXiv:1503.04069, 2015.  He, Hua, Gimpel, Kevin, and Lin, Jimmy. Multi-perspective sentence similarity modeling with convolutional neural networks. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015.  Hermann, Karl Moritz and Blunsom, Phil. Multilingual models for compositional distributed seman- tics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2014.  Hermann, Karl Moritz, Koˇcisk´y, Tom´aˇs, Grefenstette, Edward, Espeholt, Lasse, Kay, Will, Suley- man, Mustafa, and Blunsom, Phil. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, 2015.  Hill, Felix, Reichart, Roi, and Korhonen, Anna. SimLex-999: Evaluating semantic models with  (genuine) similarity estimation. Computational Linguistics, 41(4), 2015.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8),  1997.  Hu, Baotian, Lu, Zhengdong, Li, Hang, and Chen, Qingcai. Convolutional neural network archi- tectures for matching natural language sentences. In Advances in Neural Information Processing Systems, 2014.  Huang, Po-Sen, He, Xiaodong, Gao, Jianfeng, Deng, Li, Acero, Alex, and Heck, Larry. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, 2013.  ˙Irsoy, Ozan and Cardie, Claire. Deep recursive neural networks for compositionality in language.  In Advances in Neural Information Processing Systems 27. 2014.  Iyyer, Mohit, Manjunatha, Varun, Boyd-Graber, Jordan, and Daum´e III, Hal. Deep unordered com- position rivals syntactic methods for text classiﬁcation. In Proceedings of the 53rd Annual Meet- ing of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2015.  16  Published as a conference paper at ICLR 2016  Kalchbrenner, Nal, Grefenstette, Edward, and Blunsom, Phil. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), 2014.  Kim, Yoon. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the 2014  Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980, 2014.  Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan, Zemel, Richard S, Torralba, Antonio, Urtasun,  Raquel, and Fidler, Sanja. Skip-thought vectors. arXiv preprint arXiv:1506.06726, 2015.  Le, Quoc V and Mikolov, Tomas. Distributed representations of sentences and documents. arXiv  preprint arXiv:1405.4053, 2014.  Li, Jiwei, Luong, Minh-Thang, and Jurafsky, Dan. A hierarchical neural autoencoder for paragraphs  and documents. arXiv preprint arXiv:1506.01057, 2015a.  Li, Jiwei, Luong, Thang, and Jurafsky, Dan. A hierarchical neural autoencoder for paragraphs and documents. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2015b.  Ling, Wang, Dyer, Chris, Black, Alan W, Trancoso, Isabel, Fermandez, Ramon, Amir, Silvio, Marujo, Luis, and Luis, Tiago. Finding function in form: Compositional character models for open vocabulary word representation. In Proceedings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing, 2015.  Liu, Pengfei, Qiu, Xipeng, Chen, Xinchi, Wu, Shiyu, and Huang, Xuanjing. Multi-timescale long short-term memory neural network for modelling sentences and documents. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015.  Lu, Ang, Wang, Weiran, Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Deep multilingual correlation for improved word embeddings. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, 2015.  Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2014.  Marelli, Marco, Bentivogli, Luisa, Baroni, Marco, Bernardi, Raffaella, Menini, Stefano, and Zam- parelli, Roberto. SemEval-2014 task 1: Evaluation of compositional distributional semantic mod- els on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), 2014.  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed repre- sentations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, 2013.  Mitchell, Jeff and Lapata, Mirella. Vector-based models of semantic composition. In Proceedings  of the 46th Annual Meeting of the Association for Computational Linguistics, 2008.  Mitchell, Jeff and Lapata, Mirella. Composition in distributional models of semantics. Cognitive  Science, 34(8), 2010.  Paperno, Denis, Pham, Nghia The, and Baroni, Marco. A practical and linguistically-motivated approach to compositional distributional semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2014.  Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the difﬁculty of training recurrent  neural networks. arXiv preprint arXiv:1211.5063, 2012.  17  Published as a conference paper at ICLR 2016  Pavlick, Ellie, Rastogi, Pushpendre, Ganitkevich, Juri, Durme, Benjamin Van, and Callison-Burch, Chris. PPDB 2.0: Better paraphrase ranking, ﬁne-grained entailment relations, word embeddings, and style classiﬁcation. In Proceedings of the Annual Meeting of the Association for Computa- tional Linguistics, 2015.  Pennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: Global vectors for word representation. Proceedings of Empirical Methods in Natural Language Processing (EMNLP 2014), 2014.  Pham, Nghia The, Kruszewski, Germ´an, Lazaridou, Angeliki, and Baroni, Marco. Jointly optimiz- ing word representations for lexical and sentential tasks with the c-phrase model. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna- tional Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2015.  Polajnar, Tamara, Rimell, Laura, and Clark, Stephen. An exploration of discourse-based sentence spaces for compositional distributional semantics. In Proceedings of the First Workshop on Link- ing Computational Models of Lexical, Sentential and Discourse-level Semantics, 2015.  Socher, Richard, Huang, Eric H, Pennin, Jeffrey, Manning, Christopher D, and Ng, Andrew Y. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems, 2011.  Socher, Richard, Huval, Brody, Manning, Christopher D., and Ng, Andrew Y. Semantic compo- sitionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 2012.  Socher, Richard, Perelygin, Alex, Wu, Jean, Chuang, Jason, Manning, Christopher D., Ng, Andrew, and Potts, Christopher. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013.  Socher, Richard, Karpathy, Andrej, Le, Quoc V., Manning, Christopher D., and Ng, Andrew Y. Grounded compositional semantics for ﬁnding and describing images with sentences. TACL, 2, 2014.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural net-  works. In Advances in Neural Information Processing Systems, 2014.  Tai, Kai Sheng, Socher, Richard, and Manning, Christopher D. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.  Tian, Ran, Okazaki, Naoaki, and Inui, Kentaro. The mechanism of additive composition. arXiv  preprint arXiv:1511.08407, 2015.  Turian, Joseph, Ratinov, Lev-Arie, and Bengio, Yoshua. Word representations: A simple and gen- In Proceedings of the 48th Annual Meeting of the  eral method for semi-supervised learning. Association for Computational Linguistics, 2010.  Vinyals, Oriol, Kaiser, Lukasz, Koo, Terry, Petrov, Slav, Sutskever, Ilya, and Hinton, Geoffrey.  Grammar as a foreign language. arXiv preprint arXiv:1412.7449, 2014.  Wang, Di and Nyberg, Eric. A long short-term memory model for answer sentence selection in question answering. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), 2015.  Wen, Tsung-Hsien, Gasic, Milica, Mrkˇsi´c, Nikola, Su, Pei-Hao, Vandyke, David, and Young, Steve. Semantically conditioned LSTM-based natural language generation for spoken dialogue systems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015.  Weston, Jason, Bengio, Samy, and Usunier, Nicolas. Large scale image annotation: learning to rank  with joint word-image embeddings. Machine learning, 81(1), 2010.  18  Published as a conference paper at ICLR 2016  Wieting, John, Bansal, Mohit, Gimpel, Kevin, Livescu, Karen, and Roth, Dan. From paraphrase database to compositional paraphrase model and back. Transactions of the Association for Com- putational Linguistics, 3, 2015.  Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun, Courville, Aaron C., Salakhutdinov, Ruslan, Zemel, Richard S., and Bengio, Yoshua. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on Machine Learning, ICML, 2015a.  Xu, Wei, Callison-Burch, Chris, and Dolan, William B. SemEval-2015 task 1: Paraphrase and se- mantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval), 2015b.  Xu, Yan, Mou, Lili, Li, Ge, Chen, Yunchuan, Peng, Hao, and Jin, Zhi. Classifying relations via long short term memory networks along shortest dependency paths. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015c.  Yih, Wen-tau, Toutanova, Kristina, Platt, John C., and Meek, Christopher. Learning discriminative projections for text similarity measures. In Proceedings of the Fifteenth Conference on Computa- tional Natural Language Learning, 2011.  Yin, Wenpeng and Sch¨utze, Hinrich. Convolutional neural network for paraphrase identiﬁcation. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2015.  Yu, Mo and Dredze, Mark. Learning composition models for phrase embeddings. Transactions of  the Association for Computational Linguistics, 3, 2015.  Zhao, Han, Lu, Zhengdong, and Poupart, Pascal. Self-adaptive hierarchical sentence model.  Proceedings of IJCAI, 2015.  In  19  ",
1511.07543,2016,Convergent Learning: Do different neural networks learn the same representations?,"['Convergent Learning: Do different neural networks learn the same representations?\nYixuan Li', 'Jason Yosinski', 'Jeff Clune', 'Hod Lipson', 'John Hopcroft']",https://arxiv.org/pdf/1511.07543,"6 1 0 2     b e F 8 2         ]  G L . s c [      3 v 3 4 5 7 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  CONVERGENT LEARNING: DO DIFFERENT NEURAL NETWORKS LEARN THE SAME REPRESENTATIONS?  Yixuan Li1∗, Jason Yosinski1∗, Jeff Clune2, Hod Lipson3, & John Hopcroft1 1Cornell University 2University of Wyoming 3Columbia University {yli,yosinski,jeh}@cs.cornell.edu jeffclune@uwyo.edu, hod.lipson@columbia.edu  ABSTRACT  Recent successes in training large, deep neural networks have prompted active investigation into the representations learned on their intermediate layers. Such research is difﬁcult because it requires making sense of non-linear computations performed by millions of learned parameters, but valuable because it increases our ability to understand current models and training algorithms and thus create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representa- tions learned by multiple nets converge to a set of features which are either indi- vidually similar between networks or where subsets of features span similar low- dimensional spaces. We propose a speciﬁc method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question by introducing three techniques to approximately align different neural networks on a feature or subspace level: a bipartite matching ap- proach that makes one-to-one assignments between neurons, a sparse prediction and clustering approach that ﬁnds one-to-many mappings, and a spectral cluster- ing approach that ﬁnds many-to-many mappings. This initial investigation reveals a few interesting, previously unknown properties of neural networks, and we ar- gue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned re- liably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the speciﬁc basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local (single unit) code and slightly, but not fully, distributed codes across multiple units; (4) that the average activation values of neurons vary considerably within a network, yet the mean activation values across different networks converge to an almost identical distribution. 1  1  INTRODUCTION  Many recent studies have focused on understanding deep neural networks from both a theoreti- cal perspective (Arora et al., 2014; Neyshabur & Panigrahy, 2013; Montavon et al., 2011; Paul & Venkatasubramanian, 2014; Goodfellow et al., 2014) and from an empirical perspective (Erhan et al., 2009; Eigen et al., 2013; Szegedy et al., 2013; Simonyan et al., 2013; Zeiler & Fergus, 2014; Nguyen et al., 2014; Yosinski et al., 2014; Mahendran & Vedaldi, 2014; Yosinski et al., 2015; Zhou et al., 2014). In this paper we continue this trajectory toward attaining a deeper understanding of neural net training by proposing a new approach. We begin by noting that modern deep neural networks (DNNs) exhibit an interesting phenomenon: networks trained starting at different random initializations frequently converge to solutions with similar performance (see Dauphin et al. (2014) and Section 2 below). Such similar performance by different networks raises the question of to  ∗Authors contributed equally. 1A preliminary version of this work was presented at the NIPS 2015 Feature Extraction workshop.  1  Published as a conference paper at ICLR 2016  what extent the learned internal representations differ: Do the networks learn radically different sets of features that happen to perform similarly, or do they exhibit convergent learning, meaning that their learned feature representations are largely the same? This paper makes a ﬁrst attempt at asking and answering these questions. Any improved understanding of what neural networks learn should improve our ability to design better architectures, learning algorithms, and hyperparameters, ultimately enabling more capable models. For instance, distributed data-parallel neural network training is more complicated than distributed data-parallel training of convex models because pe- riodic direct averaging of model parameters is not an effective strategy: perhaps solving a neuron correspondence problem before averaging would mitigate the need for constant synchronization. As another example, if networks converge to diverse solutions, then perhaps additional performance im- provements are possible via training multiple models and then using model compilation techniques to realize the resulting ensemble in a single model. In this paper, we investigate the similarities and differences between the representations learned by neural networks with the same architecture trained from different random initializations. We employ an architecture derived from AlexNet (Krizhevsky et al., 2012) and train multiple networks on the ImageNet dataset (Deng et al., 2009) (details in Section 2). We then compare the representations learned across different networks. We demonstrate the effectiveness of this method by both visually and quantitatively showing that the features learned by some neuron clusters in one network can be quite similar to those learned by neuron clusters in an independently trained neural network. Our speciﬁc contributions are asking and shedding light on the following questions:  1. By deﬁning a measure of similarity between units2 in different neural networks, can we come up with a permutation for the units of one network to bring it into a one-to-one alignment with the units of another network trained on the same task? Is this matching or alignment close, because features learned by one network are learned nearly identically somewhere on the same layer of the second network, or is the approach ill-fated, because the representations of each network are unique? (Answer: a core representation is shared, but some rare features are learned in one network but not another; see Section 3).  2. Are the above one-to-one alignment results robust with respect to different measures of neuron similarity? (Answer: yes, under both linear correlation and estimated mutual infor- mation metrics; see Section 3.2).  3. To the extent that an accurate one-to-one neuron alignment is not possible, is it simply because one network’s representation space is a rotated version3 of another’s? If so, can we ﬁnd and characterize these rotations? (Answers: by learning a sparse weight LASSO model to predict one representation from only a few units of the other, we can see that the transform from one space to the other can be possibly decoupled into transforms between small subspaces; see Section 4).  4. Can we further cluster groups of neurons from one network with a similar group from another network? (Answer: yes. To approximately match clusters, we adopt a spectral clustering algorithm that enables many-to-many mappings to be found between networks. See Section S.3).  5. For two neurons detecting similar patterns, are the activation statistics similar as well?  (Answer: mostly, but with some differences; see Section S.4).  2 EXPERIMENTAL SETUP All networks in this study follow the basic architecture laid out by Krizhevsky et al. (2012), with parameters learned in ﬁve convolutional layers (conv1 – conv5) followed by three fully connected layers (fc6 – fc8). The structure is modiﬁed slightly in two ways. First, Krizhevsky et al. (2012) employed limited connectivity between certain pairs of layers to enable splitting the model across two GPUs.4 Here we remove this artiﬁcial group structure and allow all channels on each layer to connect to all channels on the preceding layer, as we wish to study only the group structure, if any, that arises naturally, not that which is created by architectural choices. Second, we place the local response normalization layers after the pooling layers following the defaults released with the Caffe  2Note that we use the words “ﬁlters”, “channels”, “neurons”, and “units” interchangeably to mean channels  for a convolutional layer or individual units in a fully connected layer.  3Or, more generally, a space that is an afﬁne transformation of the ﬁrst network’s representation space. 4In Krizhevsky et al. (2012) the conv2, conv4, and conv5 layers were only connected to half of the preceding  layer’s channels.  2  Published as a conference paper at ICLR 2016  (a)  (b)  (c)  (d)  Figure 1: Correlation matrices for the conv1 layer, displayed as images with minimum value at black and maximum at white. (a,b) Within-net correlation matrices for Net1 and Net2, respectively. (c) Between-net correlation for Net1 vs. Net2. (d) Between-net correlation for Net1 vs. a version of Net2 that has been permuted to approximate Net1’s feature order. The partially white diagonal of this ﬁnal matrix shows the extent to which the alignment is successful; see Figure 3 for a plot of the values along this diagonal and further discussion.  framework, which does not signiﬁcantly impact performance (Jia et al., 2014). Networks are trained using Caffe on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 dataset (Deng et al., 2009). Further details and the complete code necessary to reproduce these experiments is available at https://github.com/yixuanli/convergent_learning. We trained four networks in the above manner using four different random initializations. We refer to these as Net1, Net2, Net3, and Net4. The four networks perform very similarly on the validation set, achieving top-1 accuracies of 58.65%, 58.73%, 58.79%, and 58.84%, which are similar to the top-1 performance of 59.3% reported in the original study (Krizhevsky et al., 2012). We then aggregate certain statistics of the activations within the networks. Given a net- work Netn trained in this manner, the scalar random variable X (n) l,i denotes the series of ac- tivation values produced over the entire ILSVRC validation dataset by unit i on layer l ∈ {conv1, conv2, conv3, conv4, conv5, fc6, fc7}.5 We collect the following statistics by aggregating over the validation set (and in the case of convolutional layers also over spatial positions):  Mean:  Standard deviation:  Within-net correlation:  Between-net correlation:  l,i = (cid:112)(E[(X (n)  l,i = E[X (n) µ(n) l,i ] σ(n) l,i,j = E[(X (n) c(n) = E[(X (n) c(n,m) l,i,j  l,i − µ(n) l,i − µ(n)  l,i − µ(n)  l,i )2]) l,i )(X (n) l,i )(X (m)  l,j − µ(n) l,j − µ(m)  l,j )]/σ(n) l,j )]/σ(n)  l,i σ(n) l,i σ(m)  l,j  l,j  Intuitively, we compute the mean and standard deviation of the activation of each unit in the network over the validation set. For convolutional layers, we compute the mean and standard deviation of each channel. The mean and standard deviation for a given network and layer is a vector with length equal to the number of channels (for convolutional layers) or units (for fully connected layers).6 The within-net correlation values for each layer can be considered as a symmetric square matrix with side length equal to the number of units in that layer (e.g. a 96 × 96 matrix for conv1 as in Figure 1a,b). For a pair of networks, the between-net correlation values also form a square matrix, which in this case is not symmetric (Figure 1c,d).  5 For the fully connected layers, the random variable X (n)  for an FC layer, we pick a random image from the validation set; to sample X (n) l,i  the convolutional layers, the value of X (n) l,i sample an X (n) l,i layer, we sample a random image and a random position within the conv layer. channels of = {96, 256, 384, 384, 256, 4096, 4096, 1000}. The corresponding size of the correlation matrix in each layer is: {s2 | ∀s ∈ S}. Furthermore, the spatial extents of each channel in each convolutional layer is given by: {conv1 : 55 × 55, conv2 : 27 × 27, conv3 : 13 × 13, conv4 : 13 × 13, conv5 : 13 × 13}  l,i has one speciﬁc value for each input image; for takes on different values at each spatial position. In other words, to for a conv S  conv1 to  reference,  number  6 For  fc8 is  given  by:  the  for  3  c(Net2)conv1within-netc(Net1,Net2)conv1between-netnatural(unaligned)orderpermuted(aligned)orderc(Net1)conv1Published as a conference paper at ICLR 2016  We use these correlation values as a way of measuring how related the activations of one unit are to another unit, either within the network or between networks. We use correlation to measure similar- ity because it is independent of the scale of the activations of units. Within-net correlation quantiﬁes the similarity between two neurons in the same network; whereas the between-net correlation matrix quantiﬁes the similarity of two neurons from different neural networks. Note that the units compared are always on the same layer on the network; we do not compare units between different layers. In the Supplementary Information (see Figure S1), we plot the activation values for several example high correlation and low correlation pairs of units from conv1 and conv2 layers; the simplicity of the distribution of values suggests that the correlation measurement is an adequate indicator of the similarity between two neurons. To further conﬁrm this suspicion, we also tested with a full esti- mate of the mutual information between units and found it to yield similar results to correlation (see Section 3.2).  3  IS THERE A ONE-TO-ONE ALIGNMENT BETWEEN FEATURES LEARNED BY DIFFERENT NEURAL NETWORKS?  We would like to investigate the similarities and differences between multiple training runs of same network architecture. Due to symmetries in the architecture and weight initialization procedures, for any given parameter vector that is found, one could create many equivalent solutions simply by permuting the unit orders within a layer (and permuting the outgoing weights accordingly). Thus, as a ﬁrst step toward analyzing the similarities and differences between different networks, we ask the following question: if we allow ourselves to permute the units of one network, to what extent can we bring it into alignment with another? To do so requires ﬁnding equivalent or nearly-equivalent units across networks, and for this task we adopt the magnitude independent measures of correlation and mutual information. We primarily give results with the simpler, computationally faster correla- tion measure (Section 3.1), but then conﬁrm the mutual information measure provides qualitatively similar results (Section 3.2).  3.1 ALIGNMENT VIA CORRELATION  As discussed in Section 2, we compute within-net and between-net unit correlations. Figure 1 shows the within-net correlation values computed between units on a network and other units on the same network (panels a,b) as well as the between-net correlations between two different networks (panel c). We ﬁnd matching units between a pair of networks — here Net1 and Net2 — in two ways. In the ﬁrst approach, for each unit in Net1, we ﬁnd the unit in Net2 with maximum correlation to it, which is the max along each row of Figure 1c. This type of assignment is known as a bipartite semi-matching in graph theory (Lawler, 1976), and we adopt the same nomenclature here. This procedure can result in multiple units of Net1 being paired with the same unit in Net2. Figure 2 shows the eight highest correlation matched features and eight lowest correlation matched features using the semi-matching approach (corresponding to the leftmost eight and rightmost eight points in Figure 3). To visualize the functionality each unit, we plot the image patch from the validation set that causes the highest activation for that unit. For all the layers shown, the most correlated ﬁlters (on the left) reveal that there are nearly perfect counterparts in each network, whereas the low-correlation ﬁlters (on the right) reveal that there are many features learned by one network that are unique and thus have no corollary in the other network. An alternative approach is to ﬁnd the one-to-one assignment between units in Net1 and Net2 without replacement, such that every unit in each network is paired with a unique unit in the other network. This more common approach is known as bipartite matching.7 A matching that maximizes the sum of the chosen correlation values may be found efﬁciently via the Hopcroft-Karp algorithm (Hopcroft & Karp, 1973) after turning the between-net correlation matrix into a weighted bipartite graph. Figure 1c shows an example between-net correlation matrix; the max weighted matching can be thought of as a path through the matrix such that each row and each column are selected exactly once, and the sum of elements along the path is maximized. Once such a path is found, we can permute the units of Net2 to bring it into the best possible alignment with Net1, so that the ﬁrst channel of Net2 approximately matches (has high correlation with) the ﬁrst channel of Net1, the second channels of each also approximately match, and so on. The correlation matrix of  7Note that the semi-matching is “row-wise greedy” and will always have equal or better sum of correlation  than the matching, which maximizes the same objective but must also satisfy global constraints.  4  Published as a conference paper at ICLR 2016  Figure 2: With assignments chosen by semi-matching, the eight best (highest correlation, left) and eight worst (lowest correlation, right) matched features between Net1 and Net2 for the conv1 – conv3 layers. For all layers visualized, (1) the most correlated ﬁlters are near perfect matches, showing that many similar features are learned by independently trained neural networks, and (2) the least correlated features show that many features are learned by one network and are not learned by the other network, at least not by a single neuron in the other network. The results for the conv4 and conv5 layers can be found in the Supplementary Material (see Figure S2).  Net1 with the permuted version of Net2 is shown in Figure 1d. Whereas the diagonal of the self correlation matrices are exactly one, the diagonal of the permuted between-net correlation matrix contains values that are generally less than one. Note that the diagonal of the permuted between-net correlation matrix is bright (close to white) in many places, which shows that for many units in Net1 it is possible to ﬁnd a unique, highly correlated unit in Net2. Figure 3 shows a comparison of assignments produced by the semi-matching and matching methods for the conv1 layer (Figure S3 shows results for other layers). Insights into the differing represen- tations learned can be gained from both assignment methods. The ﬁrst conclusion is that for most units, particularly those with the higher semi-matching and matching correlations (Figure 3, left), the semi-matching and matching assignments coincide, revealing that for many units a one-to-one assignment is possible. Both methods reveal that the average correlation for one-to-one alignments varies from layer to layer (Figure 4), with the highest matches in the conv1 and conv5 layers, but worse matches in between. This pattern implies that the path from a relatively matchable conv1 representation to conv5 representation passes through an intermediate middle region where match- ing is more difﬁcult, suggesting that what is learned by different networks on conv1 and conv2 is more convergent than conv3, conv4 and conv5. This result may be related to previously observed greater complexity in the intermediate layers as measured through the lens of optimization difﬁculty (Yosinski et al., 2014). Next, we can see that where the semi-matching and matching differ, the matching is often much worse.One hypothesis for why this occurs is that the two networks learn different numbers of units to span certain subspaces. For example, Net1 might learn a representation that uses six ﬁlters to span a subspace of human faces, but Net2 learns to span the same subspace with ﬁve ﬁlters. With unique matching, ﬁve out of the six ﬁlters from Net1 may be matched to their nearest counterpart in Net2, but the sixth Net1 unit will be left without a counterpart and will end up paired with an almost unrelated ﬁlter. Finally, with reference to Figure 3 (but similarly observable in Figure S3 for other layers), another salient observation is that the correlation of the semi-matching falls signiﬁcantly from the best- matched unit (correlations near 1) to the lowest-matched (correlations near 0.3). This indicates that some ﬁlters in Net1 can be paired up with ﬁlters in Net2 with high correlation, but other ﬁlters in Net1 and Net2 are network-speciﬁc and have no high-correlation pairing in the alternate network, implying that those ﬁlters are rare and not always learned. This holds across the conv1 – conv5 layers.  5  0.400.370.360.350.330.280.280.18.........Net1Net2Net1Net2Net1Net2best matchworst matchconv1conv2conv31.000.980.980.970.970.960.960.95Published as a conference paper at ICLR 2016  Figure 3: Correlations between paired conv1 units in Net1 and Net2. Pairings are made via semi-matching (light green), which allows the same unit in Net2 to be matched with multiple units in Net1, or matching (dark green), which forces a unique Net2 neuron to be paired with each Net1 neuron. Units are sorted by their semi-matching values. See text for discussion.  Figure 4: Average correlations between paired conv1 units in Net1 and Net2. Both semi-matching (light green) and matching (dark green) methods suggest that features learned in different net- works are most convergent on conv1 and least convergent on conv4.  3.2 ALIGNMENT VIA MUTUAL INFORMATION  Because correlation is a relatively simple mathematical metric that may miss some forms of sta- tistical dependence, we also performed one-to-one alignments of neurons by measuring the mutual information between them. Mutual information measures how much knowledge one gains about one variable by knowing the value of another. Formally, the mutual information of the two random variables X (n) representing the activation of the i-th neuron in Netn and the j-th neuron l,j in Netm, is deﬁned as:  l,i and X (m)  (cid:16)  (cid:17)  (cid:88)  (cid:88)  a∈X (n)  l,i  b∈X (m)  l,j  I  X (n)  l,i ; X (m)  l,j  =  p(a, b) log  (cid:16) p(a, b)  p(a)p(b)  (cid:17)  ,  l,i and X (m)  where p(a, b) is the joint probability distribution of X (n) l,j , and p(a) and p(b) are their marginal probability distributions, respectively. The within-net mutual information matrix and between-net mutual information matrix have the same shapes as their equivalent correlation ma- trices. We apply the same matching technique described in Section 3.1 to the between-net mutual informa- tion matrix,8 and compare the highest and lowest mutual information matches (Figure S4) to those obtained via correlation (Figure 2). The results are qualitatively the same. For example, seven out of eight best matched pairs in the conv1 layer stay the same. These results suggest that correlation is an adequate measurement of the similarity between two neurons, and that switching to a mu- tual information metric would not qualitatively change the correlation-based conclusions presented above.  4 RELAXING THE ONE-TO-ONE CONSTRAINT TO FIND SPARSE,  FEW-TO-ONE MAPPINGS  The preceding section showed that, while some neurons have a one-to-one match in another net- work, for other neurons no one-to-one match exists (with correlation above some modest threshold). For example, 17% of conv1 neurons in Net1 have no match in Net2 with a correlation above 0.5 (Figure 3); this number rises to 37% for conv2, 63% for conv3, and 92% for conv4, before dropping to 75% for conv5 (see Figure S3). These numbers indicate that, particularly for intermediate layers, a simple one-to-one mapping is not a sufﬁcient model to predict the activations of some neurons in one network given the activations of neurons in another network (even with the same architecture trained on the same task). That result could either be because the representations are unique (i.e. not convergent), or because the  8The mutual information between each pair of neurons is estimated using 1D and 2D histograms of paired activation values over 60,000 random activation samples. We discretize the activation value distribution into percentile bins along each dimension, each of which captures 5% of the marginal distribution mass. We also add a special bin with range (− inf, 10−6] in order to capture the signiﬁcant mass around 0.  6  020406080unit index (sorted by correlation of semi-matching assignment)0.00.20.40.60.81.0correlation with assigned unitsemi-matchingmatchingconv1conv2conv3conv4conv5convolutional layers0.300.350.400.450.500.550.600.650.700.75average correlationsemi-matchingmatchingPublished as a conference paper at ICLR 2016  decay 0 0.170 0.372 0.434 0.478 0.484  conv1 conv2 conv3 conv4 conv5  0.169 0.368 0.427 0.470 0.478  Sparse Prediction Loss (after 4,500 iterations) decay 10−2  decay 10−4  decay 10−3  decay 10−5  0.162 0.337 0.383 0.423 0.439  0.172 0.392 0.462 0.477 0.436  0.484 0.518 0.497 0.489 0.478  decay 10−1  0.517 0.514 0.496 0.488 0.477  Table 1: Average prediction error for mapping layers with varying L1 penalties (i.e. decay terms). Larger decay parameters enforce stronger sparsity in the learned weight matrix. Notably, on conv1 and conv2, the prediction errors do not rise much compared to the dense (decay = 0) case with the imposition of a sparsity penalty until after an L1 penalty weight of over 10−3 is used. This region of roughly constant performance despite increasing sparsity pressure is shown in bold. That such extreme sparsity does not hurt performance implies that each neuron in one network can be predicted by only one or a few neurons in another network. For the conv3, conv4, and conv5 layers, the overall error is higher, so it is difﬁcult to draw any strong conclusions regarding those layers. The high errors could be because of the uniqueness of the learned representations, or the optimization could be learning a suboptimal mapping layer for other reasons.  best possible one-to-one mapping is insufﬁcient to tell the complete story of how one representation is related to another. We can think of a one-to-one mapping as a model that predicts activations in the second network by multiplying the activations of the ﬁrst by a permutation matrix — a square matrix constrained such that each row and each column contain a single one and the rest zeros. Can we do better if we learn a model without this constraint? We can relax this one-to-one constraint to various degrees by learning a mapping layer with an L1 penalty (known as a LASSO model, (Tibshirani, 1996)), where stronger penalties will lead to sparser (more few-to-one or one-to-one) mappings. This sparsity pressure can be varied from quite strong (encouraging a mostly one-to-one mapping) all the way to zero, which encourages the learned linear model to be dense. More speciﬁcally, to predict one layer’s representation from another, we learn a single mapping layer from one to the other (similar to the “stitching layer” in Lenc & Vedaldi (2015)). In the case of convolutional layers, this mapping layer is a convolutional layer with 1 × 1 kernel size and number of output channels equal to the number of input channels. The mapping layer’s parameters can be considered as a square weight matrix with side length equal to the number of units in the layer; the layer learns to predict any unit in one network via a linear weighted sum of any number of units in the other. The model and resulting square weight matrices are shown in Figure 5. This layer is then trained to minimize the sum of squared prediction errors plus an L1 penalty, the strength of which is varied.9 Mapping layers are trained for layers conv1 – conv5. The average squared prediction errors for each are shown in Table 1 for a variety of L1 penalty weights (i.e. different decay values). For the conv1 and conv2 layers, the prediction errors do not rise with the imposition of a sparsity penalty until a penalty greater than 10−3. A sparsity penalty as high as 10−3 results in mapping layer models that are nearly as accurate as their dense counterparts, but that contain mostly zero weights. Additional experiments for conv1 revealed that a penalty multiplier of 10−2.6 provides a good trade-off between sparsity and accuracy, resulting in a model with sparse prediction loss 0.235, and an average of 4.7 units used to predict each unit in the target network (i.e. an average of 4.7 signiﬁcantly non-zero weights). For conv2 the 10−3 multiplier worked well, producing a model with an average of 2.5 non- zero connections per predicted unit. The mapping layers for higher layers (conv3 – conv5) showed poor performance even without regularization, for reasons we do not yet fully understand, so further  √ 9Both representations (input and target) are taken after the relu is applied. Before training, each channel is normalized to have mean zero and standard deviation 1/ N, where N is the number of dimensions of the representation at that layer (e.g. N = 55 · 55 · 96 = 290400 for conv1). This normalization has two effects. First, the channels in a layer are all given equal importance, without which the channels with large activation values (see Figure S10) dominate the cost and are predicted well at the expense of less active channels, a solution which provides little information about the less active channels. Second, the representation at any layer for a single image becomes approximately unit length, making the initial cost about the same on all layers and allowing the same learning rate and SGD momentum hyperparameters to be used for all layers. It also makes the effect of speciﬁc L1 multipliers approximately commensurate and allows for rough comparison of prediction performance between layers, because the scale is constant.  7  Published as a conference paper at ICLR 2016  Figure 5: A visualization of the network-to-network sparse “mapping layers” (green squares). The layers are trained independently of each other and with an L1 weight penalty to encourage sparse weights.  results on those layers are not included here. Future investigation with different hyperparameters or different architectures (e.g. multiple hidden layers) could train more powerful predictive models for these layers. The one-to-one results of Section 3 considered in combination with these results on sparse prediction shed light on the open, long-standing debate about the extent to which learned representations are local vs. distributed : The units that match well one-to-one suggest the presence of a local code, where each of these dimensions is important enough, independent enough, or privileged enough in some other way to be relearned by different networks. Units that do not match well one-to-one, but are predicted well by a sparse model, suggest the presence, along those dimensions, of slightly distributed codes. The results could have been otherwise: if all units could accurately be matched one-to-one, we would suspect a local code across the whole layer. On the other hand, if making predictions from one network to another required a dense afﬁne transformation, then we would interpret the code as fully distributed, with each unit serving only as a basis vector used to span a large dimensional subspace, whose only requirement was to have large projection onto the subspace (to be useful) and small projection onto other basis vectors (to be orthogonal). The story that actually emerges is that the ﬁrst two layers use a mix of a local and a slightly distributed code. Figure 6 shows a visualization of the learned weight matrix for conv1, along with a permuted weight matrix that aligns units from Net2 with the Net1 units that most predict them. We also show two example units in Net2 and, for each, the only three units in Net1 that are needed to predict their activation values. These sparse prediction results suggest that small groups of units in each network span similar subspaces, but we have not yet identiﬁed or visualized the particular subspaces that are spanned. Below we present one approach to do so by using the sparse prediction matrix directly, and in the Supplementary Section S.3 we discuss a related approach using spectral clustering. For a layer with s channels we begin by creating a 2s × 2s block matrix B by concatenating the blocks [I, W ; W T , I] where I is the s×s identity matrix and W is the learned weight matrix. Then we use Hierarchical Agglomerative Clustering (HAC), as implemented in Scikit-learn (Pedregosa et al., 2011) to recursively cluster individual units into clusters, and those clusters into larger clusters, until all have been joined into one cluster. The HAC algorithm as adapted to this application works in the following way: (1) For all pairs of units, we ﬁnd the biggest off-diagonal value in B, i.e. the largest prediction weight; (2) We pair those two units together into a cluster and consider it as a single entity for the remainder of the algorithm; (3) We start again from step 2 using the same process (still looking for the biggest value), but whenever we need to compare unit ↔ cluster or cluster ↔ cluster, we use the average unit ↔ unit weight over all cross-cluster pairs;(4) Eventually the process terminates when there is a single cluster.10  10For example, in the conv1 layer with 96 channels, this happens after 96 · 2 − 1 = 191 steps.  8  55 x 5527 x 2713 x 1313 x 1313 x 1355 x 5527 x 2713 x 1313 x 1313 x 1396 x 96256 x 256384 x 384384 x 384256 x 256Net2  feature mapNet1  feature mapSparserepresentationsPublished as a conference paper at ICLR 2016  Figure 6: (left) The learned mapping layer from Net1 to Net2 for the conv1 layer. (right) Two ex- ample units (bottom) in Net2 — which correspond to the same colored rows in the left weight matrix — together with, for each, the only three units in Net1 that are needed to predict their activation. To fully visualize the functionality each unit, we plot the top 9 image patches from the validation set that causes the highest activation for that unit (“maxim”), as well as its corresponding “deconv” visualization introduced by Zeiler & Fergus (2014). We also show the actual weight associated with each unit in Net1 in the sparse prediction matrix.  The resulting clustering can be interpreted as a tree with units as leaves, clusters as intermediate nodes, and the single ﬁnal cluster as the root node. In Figure 7 we plot the B matrix with rows and columns permuted together in the order leaves are encountered when traversing the tree, and intermediate nodes are overlaid as lines joining their subordinate units or clusters. For clarity, we color the diagonal pixels (which all have value one) with green or red if the associated unit came from Net1 or Net2, respectively. Plotted in this way, structure is readily visible: Most parents of leaf clusters (the smallest merges shown as two blue lines of length two covering a 2 × 2 region) contain one unit from Net1 and one from Net2. These units can be considered most predictive of each other.11 Slightly higher level clusters show small subspaces, comprised of multiple units from each network, where multiple units from one network are useful for predicting activations from the other network (see the example zoomed regions on the right side of Figure 7). The HAC method employs greedy merges, which could in some cases be suboptimal. In the Sup- plementary Section S.3 we explore a related method that is less greedy, but operates on the denser correlation matrix instead. Future work investigating or developing other methods for analyzing the structure of the sparse prediction matrices may shed further light on the shared, learned subspaces of independently trained networks.  5 CONCLUSIONS  We have demonstrated a method for quantifying the feature similarity between different, indepen- dently trained deep neural networks. We show how insights may be gain by approximately aligning different neural networks on a feature or subspace level by blending three approaches: a bipartite matching that makes one-to-one assignments between neurons, a sparse prediction and clustering approach that ﬁnds one-to-many mappings, and a spectral clustering approach that ﬁnds many-to- many mappings. Our main ﬁndings include:  1. Some features are learned reliably in multiple networks, yet other features are not consis-  tently learned.  2. Units learn to span low-dimensional subspaces and, while these subspaces are common to  multiple networks, the speciﬁc basis vectors learned are not.  3. The representation codes are a mix between a local (single unit) code and slightly, but not  fully, distributed codes across multiple units.  4. The average activation values of neurons vary considerably within a network, yet the mean  activation values across different networks converge to an almost identical distribution.  11Note that the upper right corner of B is W = W1→2, the matrix predicting Net1 → Net2, and the lower  left is just the transpose W T  1→2. The corners could instead be W1→2 and W2→1, respectively.  9  0.3960.1160.268Net2,Unit1Net1,Unit35Net1,Unit30Net1,Unit96maximdevonvNet2,Unit44Net1,Unit34Net1,Unit29Net1,Unit520.0080.3450.266Published as a conference paper at ICLR 2016  Figure 7: The results of the Hierarchical Agglomerative Clustering (HAC) algorithm described in Section 4 on the conv1 layer. Left: The B matrix permuted by the tree-traversal order of leaf nodes. Pixels on the diagonal are leaf nodes and represent original units of either network (green for Net1 and red for Net2). The brighter the gray pixel is, the larger the weight is in the matrix. See text for a complete interpretation. Right: Two zoomed in regions of the diagonal, showing two different four-dimensional subspaces spanned by four units in each network. The top 9 and bottom 9 images correspond to the maxim and deconv visualizations, respectively. Best viewed digitally with zoom.  6 FUTURE WORK  The ﬁndings in this paper open up new future research directions, for example: (1) Model compres- sion. How would removing low-correlation, rare ﬁlters affect performance? (2) Optimizing ensem- ble formation. The results show some features (and subspaces) are shared between independently trained DNNs, and some are not. This suggests testing how feature correlation among different DNNs in an ensemble affects ensemble performance. For example, the “shared cores” of multiple networks could be deduplicated, but the unique features in the tails of their feature sets could be kept. (3) Similarly, one could (a) post-hoc assemble ensembles with greater diversity, or even (b) directly encourage ensemble feature diversity during training. (4) Certain visualization techniques, e.g., deconv (Zeiler & Fergus, 2014), DeepVis (Yosinski et al., 2015), have revealed neurons with multiple functions (e.g. detectors that ﬁre for wheels and faces). The proposed matching methods could reveal more about why these arise. Are these units consistently learned because they are help- ful or are they just noisy, imperfect features found in local optima? (5) Model combination: can multiple models be combined by concatenating their features, deleting those with high overlap, and then ﬁne-tuning? (6) Apply the analysis to networks with different architectures — for example, networks with different numbers of layers or different layer sizes — or networks trained on different subsets of the training data. (7) Study the correlations of features in the same network, but across training iterations, which could show whether some features are trained early and not changed much later, versus perhaps others being changed in the later stages of ﬁne-tuning. This could lead to complementary insights on learning dynamics to those reported by Erhan et al. (2009). (8) Study whether particular regularization or optimization strategies (e.g., dropout, ordered dropout, path SGD, etc.) increase or decrease the convergent properties of the representations to facilitate differ- ent goals (more convergent would be better for data-parallel training, and less convergent would be better for ensemble formation and compilation).  ACKNOWLEDGMENTS The authors are grateful to the NASA Space Technology Research Fellowship (JY) for funding, Wendy Shang for conversations and initial ideas, Yoshua Bengio for modeling suggestions, and Anh Nguyen and Kilian Weinberger for helpful comments and edits. This work was supported in part by US Army Research Ofﬁce W911NF-14-1-0477, NSF grant 1527232. Jeff Clune was  10  Published as a conference paper at ICLR 2016  supported by an NSF CAREER award (CAREER: 1453549) and a hardware donation from the NVIDIA Corporation.  REFERENCES Arora, Sanjeev, Bhaskara, Aditya, Ge, Rong, and Ma, Tengyu. Provable bounds for learning some  deep representations. In ICML, pp. 584–592, 2014.  Dauphin, Yann, Pascanu, Razvan, G¨ulc¸ehre, C¸ aglar, Cho, Kyunghyun, Ganguli, Surya, and Bengio, Identifying and attacking the saddle point problem in high-dimensional non-convex  Yoshua. optimization. CoRR, abs/1406.2572, 2014. URL http://arxiv.org/abs/1406.2572.  Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248–255. IEEE, 2009.  Eigen, David, Rolfe, Jason, Fergus, Rob, and LeCun, Yann. Understanding deep architectures using  a recursive convolutional network. arXiv preprint arXiv:1312.1847, 2013.  Erhan, Dumitru, Manzagol, Pierre-Antoine, Bengio, Yoshua, Bengio, Samy, and Vincent, Pascal. The difﬁculty of training deep architectures and the effect of unsupervised pre-training. In Inter- national Conference on artiﬁcial intelligence and statistics, pp. 153–160, 2009.  Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Christian. Explaining and Harnessing Adversarial  Examples. ArXiv e-prints, December 2014.  Hopcroft, John E and Karp, Richard M. An nˆ5/2 algorithm for maximum matchings in bipartite  graphs. SIAM Journal on computing, 2(4):225–231, 1973.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Imagenet classiﬁcation with deep convolu- In Advances in Neural Information Processing Systems 25, pp. 1106–  tional neural networks. 1114, 2012.  Lawler, Eugene L. Combinatorial optimization: networks and matroids. Courier Corporation, 1976.  Lenc, Karel and Vedaldi, Andrea. Understanding image representations by measuring their equiv- ariance and equivalence. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.  Mahendran, Aravindh and Vedaldi, Andrea. Understanding deep image representations by inverting  them. arXiv preprint arXiv:1412.0035, 2014.  Montavon, Gr´egoire, Braun, Mikio L, and M¨uller, Klaus-Robert. Kernel analysis of deep networks.  The Journal of Machine Learning Research, 12:2563–2581, 2011.  Neyshabur, Behnam and Panigrahy, Rina.  arXiv:1311.3315, 2013.  Sparse matrix factorization.  arXiv preprint  Nguyen, Anh, Yosinski, Jason, and Clune, Jeff. Deep neural networks are easily fooled: High  conﬁdence predictions for unrecognizable images. arXiv preprint arXiv:1412.1897, 2014.  Paul, Arnab and Venkatasubramanian, Suresh. Why does deep learning work?-a perspective from  group theory. arXiv preprint arXiv:1412.6621, 2014.  Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Pret- tenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.  Simonyan, Karen, Vedaldi, Andrea, and Zisserman, Andrew. Deep inside convolutional networks: Visualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034, pre- sented at ICLR Workshop 2014, 2013.  11  Published as a conference paper at ICLR 2016  Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow,  Ian J., and Fergus, Rob. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.  Tibshirani, Robert. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical  Society. Series B (Methodological), pp. 267–288, 1996.  Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are features in deep neural net- works? In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 27, pp. 3320–3328. Curran Associates, Inc., December 2014.  Yosinski, Jason, Clune, Jeff, Nguyen, Anh, Fuchs, Thomas, and Lipson, Hod. Understanding neural networks through deep visualization. In Deep Learning Workshop, International Conference on Machine Learning (ICML), 2015.  Zeiler, Matthew D and Fergus, Rob. Visualizing and understanding convolutional networks.  ECCV, pp. 818–833, 2014.  In  Zhou, Bolei, Khosla, Aditya, Lapedriza, `Agata, Oliva, Aude, and Torralba, Antonio. Object detec- tors emerge in deep scene cnns. In International Conference on Learning Representations (ICLR), volume abs/1412.6856, 2014. URL http://arxiv.org/abs/1412.6856.  12  Published as a conference paper at ICLR 2016  SUPPLEMENTARY INFORMATION  S.1 ACTIVATION VALUES: HIGH CORRELATION VS. LOW CORRELATION  (a) conv1, high correlation c(1,2)  1,34,48: 0.995  (b) conv1, low correlation c(1,2)  1,2,74: 0.178  (c) conv2, high correlation c(1,2)  2,122,236: 0.92  (d) conv2, low correlation c(1,2)  2,30,26: 0.24  Figure S1: Activation values are computed across 5,000 randomly sampled images and all spatial positions (55× 55 and 27× 27 for layers conv1 and conv2, respectively). The joint distributions ap- pear simple enough to suggest that a correlation measureis sufﬁcient to ﬁnd matching units between networks.  S.2 ADDITIONAL MATCHING RESULTS  Figure S3 shows additional results of comparison of assignments produced by semi-matching and matching methods in conv2 – conv5. For each unit, both the semi-matching and matching are found, then the units are sorted in order of decreasing semi-matching value and both correlation values are plotted.  13  Published as a conference paper at ICLR 2016  Figure S2: With assignments chosen by semi-matching, the eight best (highest correlation, left) and eight worst (lowest correlation, right) matched features between Net1 and Net2 for the conv1 through conv5 layers. To visualize the functionality each unit, we plot the nine image patches (in a three by three block) from the validation set that causes the highest activation for that unit and directly beneath that block show the “deconv” visualization of each of the nine images. Best view with siginicant zoom in.  14  Net1Net2best matchworst matchconv1conv2conv3...............Net1Net2Net1Net2Net1Net2Net1Net2conv4conv5Published as a conference paper at ICLR 2016  (a) conv2  (b) conv3  (c) conv4  (d) conv5  Figure S3: Correlations between units in conv2 - conv5 layers of Net1 and their paired units in Net2, where pairings are made via semi-matching (large light green circles) or matching (small dark green dots).  Figure S4: The eight best (highest mutual information, left) and eight worst (lowest mutual informa- tion, right) features in the semi-matching between Net1 and Net2 for the conv1 and conv2 layers.  15  050100150200250unit index (sorted by correlation of semi-matching assignment)0.00.20.40.60.8correlation with assigned unitsemi-matchingmatching050100150200250300350unit index (sorted by correlation of semi-matching assignment)0.00.20.40.60.8correlation with assigned unitsemi-matchingmatching050100150200250300350unit index (sorted by correlation of semi-matching assignment)0.00.10.20.30.40.50.6correlation with assigned unitsemi-matchingmatching050100150200250unit index (sorted by correlation of semi-matching assignment)0.00.10.20.30.40.50.60.7correlation with assigned unitsemi-matchingmatching   best match  worst matchNet1Net2Net1Net2conv1conv2Published as a conference paper at ICLR 2016  S.3 DOES RELAXING THE ONE-TO-ONE CONSTRAINT TO FIND  MANY-TO-MANY GROUPINGS REVEAL MORE SIMILARITIES BETWEEN WHAT DIFFERENT NETWORKS LEARN?  Since the preceding sections have shown that neurons may not necessarily correspond via a globally consistent one-to-one matching between networks, we now seek to ﬁnd many-to-many matchings between networks using a spectral clustering approach.  S.3.1 NEURON SIMILARITY GRAPHS  We deﬁne three types of similarity graphs based on the correlation matrices obtained above. Single-net neuron similarity graphs. Given a fully trained DNN X and a speciﬁed layer l, we ﬁrst construct the single-net neuron similarity graph GX,l = (V, E). Each vertex vp in this graph represents a unit p in layer l. Two vertices are connected by an edge of weight cpq if the correlation value cpq in the self-correlation matrix corr(Xl, Xl) between unit p and unit q is greater than a certain threshold τ. Between-net neuron similarity graphs. Given a pair of fully trained DNNs X and Y , the between- net neuron similarity graph GXY,l = (V, E) can be constructed in a similar manner. Note that GXY,l is a bipartite graph and contains twice as many vertices as that in GX,l since it incorporates units from both networks. Two vertices are connected by an edge of weight cpq if the correlation value cpq in the between-net correlation matrix corr(Xl, Yl) between unit p in X and unit q in Y is greater than a certain threshold τ. Combined similarity graphs. The problem of matching neurons in different networks can now be reformulated as ﬁnding a partition in the combined neuron similarity graphs GX+Y,l = GX,l + GY,l + GXY,l, such that the edges between different groups have very low weights and the edges within a group have relatively high weights.  S.3.2 SPECTRAL CLUSTERING AND METRICS FOR NEURON CLUSTERS  We deﬁne three types of similarity graphs based on the correlation matrices obtained above (see Sec- tion S.3.1 for deﬁnition). Deﬁne Wl ∈ R2Sl×2Sl to be the combined correlation matrices between two DNNs, X and Y in layer l, where wjk is the entry at jth row and kth column of that matrix. Sl is the number of channels (units) in layer l. Wl is given by  (cid:20) corr(Xl, Xl) the diagonal matrix with the degrees dj = (cid:80)2Sl  corr(Xl, Yl)(cid:62)  Wl =  (cid:21)  corr(Xl, Yl) corr(Yl, Yl)  .  The unnormalized Laplacian matrix is deﬁned as Ll = Dl − Wl, where the degree matrix Dl is k=1 wjk. The unnormalized Laplacian matrix and its eigenvalues and eigenvectors can be used to effectively embed points in a lower-dimension rep- resentation without losing too information about spatial relationships. If neuron clusters can be identiﬁed, then the Laplacian Ll is approximately block-diagonal, with each block corresponding a cluster. Assuming there are k clusters in the graph, spectral clustering would then take the ﬁrst k eigenvectors12, U ∈ R2Sl×k, corresponding to the k smallest eigenvalues, and partition neurons in the eigenspace with the k-means algorithm.  S.3.3 SPECTRAL CLUSTERING RESULTS  We use Net1 and Net2 as an example for showing the results of matching neurons between DNNs. Figure S5 shows the permuted combined correlation matrix after apply the spectral clustering algo- rithm for conv1 – conv5. Figure S6 displays 12 neuron clusters with high between-net similarity measurement in conv1 layer, and Figure S7 displays the top 8 neuron clusters with highest between- net similarity measurement in conv2 layer. The matching results imply that there exists many-to- many correspondence of the feature maps between two fully trained networks with different random initializations, and the number of neurons learning the same feature can be different between net- works. For example, the four units of {89, 90, 134, 226} in Net1 and three units of {2, 39, 83} in Net2 are learning the features about green objects.  12In practice, when the number of clusters is unknown, the best value of k to choose is where where the  eigenvalue shows a relatively abrupt change.  16  Published as a conference paper at ICLR 2016  (a) conv1  (b) conv2  (c) conv3  (d) conv4  (e) conv5  Figure S5: The permuted combined correlation matrix after apply spectral clustering method (conv1 – conv5). The diagonal block structure represents the groups of neurons that are clustered together. The value of k adopted for these ﬁve layers are: {40,100,100,100,100}, which is consistent with the parameter setting for other experiments in this paper.  17  05010015005010015001002003004005000100200300400500Published as a conference paper at ICLR 2016  Figure S6: The neuron matchings between two DNNs (Net1 and Net2) in conv1 layer. Here we display the 12 neuron clusters with relatively high between-net similarity measurement. Each la- beled half-row corresponds to one cluster, where the ﬁlter visualizations for neurons from Net1 and Net2 are separated by white space slot. The matching results imply that there exists many-to-many correspondence of the feature maps between two fully trained networks with different random ini- tializations. For instance, in cluster #6, neurons from Net1 and Net2 are both learning 135◦ diagonal edges; and neurons in cluster #10 and #12 are learning 45◦ diagonal edges.  Figure S7: The neuron matchings between two DNNs: Net1 and Net2. Each of the 3 × 3 block displays the top 9 image patches that cause the highest activations to each neuron. Each labeled half- row corresponds to one cluster, where the ﬁlter visualizations with dashed boxes represent neurons from Net1 and those without are from Net2. For example, there are 7 neurons learning similar features in cluster #3, where the left four neurons are in Net1 and the right three are from Net2. Best viewed in electronic form with zoom.  We also computed and visualized the matching neurons in other layers as well. The results of conv1 are shown in Figure S6. As noted earlier, the conv1 layer tends to learn more general features like Gabor ﬁlters (edge detectors) and blobs of color. Our approach ﬁnds many matching Gabor ﬁlters (e.g., clusters #5, #6, #10, #11 and #12), and also some matching color blobs (e.g., clusters #1, #3 and #4).  S.3.4 HIERARCHICAL SPECTRAL CLUSTERING RESULTS  Due to the stochastic effects of randomly initializing centroids in k-means clustering, some of the initial clusters contains more neurons than others. To get more ﬁne-grained cluster structure, we recurrently apply k-means clustering on any clusters with size > 2α · Sl, where α is a tunable pa- rameter for adjusting the maximum size of the leaf clusters. Figure S8 shows the partial hierarchical structure of neuron matchings in the conv2 layer. The cluster at the root of the tree is a ﬁrst-level cluster that contains many similar units from both DNNs. Here we adopt α = 0.025 for the conv2 layer, resulting in a hierarchical neuron cluster tree structure with leaf clusters containing less than 6 neurons from each network. The bold box of each subcluster contains neurons from Net1 and the remaining neurons are from Net2. For example, in subcluster #3, which shows conv2 features, units  18  Sl(cid:88)  Sl(cid:88)  Published as a conference paper at ICLR 2016  {62, 137, 148} from Net1 learned similar features as units {33, 64, 230} from Net2, namely, red and magenta objects.  Figure S8: The hierarchical structure of neuron matchings between two DNNs: Net1 and Net2 (conv2 layer). The initial clusters are obtained using spectral clustering with the number of clusters k = 100 and threshold τ = 0.2.  S.3.5 METRICS FOR NEURON CLUSTERS  Here we introduce two metrics for quantifying the similarity among neurons grouped together after applying the clustering algorithm above.  Between-net similarity:  SimXl→Yl = (  corr(Xl, Yl)pq)/S 2  l  Within-net similarity:  SimXl,Yl = (SimXl→Xl + SimYl→Yl )/2  p=1  q=1  We further performed experiments in quantifying the similarity among neurons that are clustered to- gether. Figure S9 shows the between-net and within-net similarity measurement for conv1 – conv5. The value of k for initial clustering is set to be 40 for conv1 layer and 100 for all the other layers. In our experiments, the number of ﬁnal clusters obtained after further hierarchical branching is {43, 113, 130, 155, 131}. The tail in those curves with value 0 is due to the non-existence of between-net similarity for the clusters containing neurons from only one of the two DNNs. To better capture the distribution of non-zero similarity values, we leave out the tail after 100 in the plot for conv3 - conv5 layers.  (a) conv1  (b) conv2  (c) conv3  (d) conv4  (e) conv5  Figure S9: The distribution of between-net and within-net similarity measurement after clustering neurons (conv1 – conv5). The x-axis represents obtained clusters, which is reshufﬂed according to the sorted between-net similarity value.  19  Published as a conference paper at ICLR 2016  S.4 COMPARING AVERAGE NEURAL ACTIVATIONS WITHIN AND BETWEEN  NETWORKS  The ﬁrst layer of networks trained on natural images (here the conv1 layer) tends to learn channels matching patterns similar to Gabor ﬁlters (oriented edge ﬁlters) and blobs of color. As shown in Figures S10, S11, and S12, there are certain systematic biases in the relative magnitudes of the acti- vations of the different channels of the ﬁrst layer. Responses of the low frequency ﬁlters have much higher magnitude than that of the high frequency ﬁlters. This phenomenon is likely a consequence of the 1/f power spectrum of natural images in which, on average, low spatial frequencies tend to contain higher energy (because they are more common) than high spatial frequencies.  Figure S10: The average activation values of each unit on all layers of Net1 – Net4. A couple salient effects are observable. First, in a given network, average activations vary widely within each layer. While most activations fall within a relatively narrow band (the middle of each plot), a rare few highly active units have one or two orders of magnitude higher average output than the least active. Second, the overall distribution of activation values is similar across networks. However, also note that the max single activation does vary across networks in some cases, e.g. on the conv2 layer by a factor of two between networks. For clarity, on layers other than conv1 circle markers are shown only at the line endpoints.  In Figure S10 we show the mean activations for each unit of four networks, plotted in sorted order from highest to lowest. First and most saliently, we see a pattern of widely varying mean activation values across units, with a gap between the most active and least active units of one or two orders of magnitude (depending on the layer). Second, we observe a rough overall correspondence in the spectrum of activations between the networks. However, the correspondence is not perfect: although much of the spectrum matches well, the most active ﬁlters converged to solutions of somewhat different magnitudes. For example, the average activation value of the ﬁlter on conv2 with the highest average activation varies between 49 to 120 over the four networks; the range for conv1 was 98 to 130.13 This effect is more interesting considering that all ﬁlters were learned with constant weight decay, which pushes all individual ﬁlter weights and biases (and thus subsequent activations) toward zero with the same force. Figure S11 shows the conv1 units with the highest and lowest activations for each of the four net- works. As mentioned earlier (and as expected), ﬁlters for lower spatial frequencies have higher average activation, and vice versa. What is surprising is the relative lack of ordering between the four networks. For example, the top two most active ﬁlters in Net1 respond to constant color re- gions of black or light blue, whereas none of the top eight ﬁlters in Net2 or Net3 respond to such patterns. One might have thought that whatever inﬂuence from the dataset caused the largest ﬁlters to be black and blue in the ﬁrst network would have caused similar constant color patches to dom- inate the other networks, but we did not observe such consistency. Similar differences exist when observing the learned edge ﬁlters: in Net1 and Net4 the most active edge ﬁlter is horizontal; in Net2 and Net3 it is vertical. The right half of Figure S11 depicts the least active ﬁlters. The same lack of alignment arises, but here the activation values are more tightly packed, so the exact ordering is less meaningful. Figure S12 shows the even more widely varying activations from conv2.  13Recall that the units use rectiﬁed linear activation functions, so the activation magnitude is unbounded.  The max activation over all channels and all spatial positions of the ﬁrst layer is often over 2000.  20  050100150200250020406080100120conv205010015020025030035005101520conv3050100150200250300350024681012conv40501001502002500.00.51.01.52.02.53.03.54.0conv5010002000300040000.00.51.01.5fc6010002000300040000.00.10.20.30.40.50.60.70.8fc702004006008001000−2−1012fc8Published as a conference paper at ICLR 2016  Figure S11: The most active (left) to least active (right) conv1 ﬁlters from Net1 – Net4, with average activation values printed above each ﬁlter. The most active ﬁlters generally respond to low spatial frequencies, and the least active ﬁltered to high spatial frequencies, but the lack of alignment is interesting (see text).  Figure S12: Most active (left) to least active (right) conv2 ﬁlters as in Figure S11. Compared to the conv1 ﬁlters, here the separation and misalignment between the top ﬁlters is even larger. For example, the top unit responding to horizontal lines in Net1 has average activation of 121.8, whereas similar units in Net2 and Net4 average 27.2 and 26.9, respectively. The unit does not appear in the top eight units of Net3 at all. The least active units seem to respond to rare speciﬁc concepts.  21  ............Net1Net2Net3Net4highest average activationlowest average activation............Net1Net2Net3Net4highest average activationlowest average activation",
1511.05641,2016,Net2Net: Accelerating Learning via Knowledge Transfer,"['Net2Net: Accelerating Learning via Knowledge Transfer\nTianqi Chen', 'Ian Goodfellow', 'Jon Shlens']",https://arxiv.org/pdf/1511.05641,"6 1 0 2    r p A 3 2         ]  G L . s c [      4 v 1 4 6 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  Net2Net: ACCELERATING LEARNING VIA KNOWLEDGE TRANSFER  Tianqi Chen∗, Ian Goodfellow, and Jonathon Shlens Google Inc., Mountain View, CA tqchen@cs.washington.edu, {goodfellow,shlens}@google.com  ABSTRACT  We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the train- ing of a signiﬁcantly larger neural net. During real-world workﬂows, one often trains very many different neural networks during the experimentation and de- sign process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by in- stantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function- preserving transformations between neural network speciﬁcations. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.  INTRODUCTION  1 We propose a new kind of operation to perform on large neural networks: rapidly transfering knowl- edge contained in one neural network to another neural network. We call this the Net2Net fam- ily of operations. We use Net2Net as a general term describing any process of training a stu- dent network signiﬁcantly faster than would otherwise be possible by leveraging knowledge from a teacher network that was already trained on the same task. In this article, we propose two speciﬁc Net2Net methodologies. Both are based on the idea of function-preserving transformations of neural networks. Speciﬁcally, we initialize the student to be a neural network that represents the same function as the teacher, but using a different parameterization. One of these transformations, Net2WiderNet allows replacing a model with an equivalent model that is wider (has more units in each hidden layer). Another of these transformations, Net2DeeperNet allows replacing a model that satisﬁes some properties with an equivalent, deeper model. After initializing the larger network to contain all of the knowledge previously acquired by the smaller network, the larger network may be trained to improve its performance. Traditionally, machine learning algorithms have been designed to receive a ﬁxed dataset as input, initialize a new model with no knowledge, and train that model to convergence on that dataset. Real workﬂows are considerably more complicated than this idealized scenario. We advocate Net2Net operations as a useful tool for accelerating real-world workﬂows. One way that real workﬂows deviate from the idealized scenario is that machine learning practition- ers usually do not train only a single model on each dataset. Instead, one typically trains multiple models, with each model designed to improve upon the previous model in some way. Each step in the iterative design process relies on fully training and evaluating the innovation from the previous step. For many large models, training is a long process, lasting for a week or even for a month. This makes data-driven iterative design slow, due to the latency of evaluating whether each change to the model caused an improvement. Net2Net operations accelerate these workﬂows by rapidly transferring knowledge from the pre- vious best model into each new model that an experimenter proposes. Instead of training each considered design of model for as much as a month, the experimenter can use Net2Net to train the  ∗Tianqi Chen is also a PhD student at University of Washington.  1  Published as a conference paper at ICLR 2016  Figure 1: Comparison between a traditional workﬂow and the Net2Net Workﬂow; Net2Net reuses information from an already trained model to speed up the training of a new model. model for a shorter period of time beginning from the function learned by the previous best model. Fig 1 demonstrates the difference of this approach from traditional one. More ambitiously, real machine learning systems will eventually become lifelong learning sys- tems (Thrun, 1995; Silver et al., 2013; Mitchell et al., 2015). These machine learning systems need to continue to function for long periods of time and continually experience new training examples as these examples become available. We can think of a lifelong learning system as experiencing a continually growing training set. The optimal model complexity changes as training set size changes over time. Initially, a small model may be preferred, in order to prevent overﬁtting and to reduce the computational cost of using the model. Later, a large model may be necessary to fully utilize the large dataset. Net2Net operations allow us to smoothly instantiate a signiﬁcantly larger model and immediately begin using it in our lifelong learning system, rather than needing to spend weeks or months re-train a larger model from scratch on the latest, largest version of the training set.  2 METHODOLOGY In this section, we describe our new Net2Net operations and how we applied them on real deep neural nets.  2.1 FEATURE PREDICTION We brieﬂy experimented with a method that proved not to offer a signiﬁcant advantage: training a large student network beginning from a random initialization, and introducing a set of extra “teacher prediction” layers into the student network. Speciﬁcally, several convolutional hidden layers of the student network were provided as input to new, learned, convolutional layers. The cost function was modiﬁed to include terms encouraging the output of these auxiliary layers to be close to a corresponding layer in the teacher network. In other words, the student is trained to use each of its hidden layers to predict the values of the hidden layers in the teacher. The goal was that the teacher would provide a good internal representation for the task that the stu- dent could quickly copy and then begin to reﬁne. The approach resembles the FitNets (Romero et al., 2014) strategy for training very thin networks of moderate depth. Unfortunately, we did not ﬁnd that this method offered any compelling speedup or other advantage relative to the baseline approach. This may be because our baseline was very strong, based on training with batch normalization (Ioffe & Szegedy, 2015). Mahayri et al. (2015) independently observed that the beneﬁts of the FitNets training strategy were eliminated after changing the model to use batch normalization. The FitNets-style approach to Net2Net learning is very general, in the sense that, if successful, it would allow any architecture of student network to learn from any architecture of teacher network. Though we were not able to make this general approach work, we encourage other researchers to attempt to design fully general Net2Net strategies in the future. We instead turned to different Net2Net strategies that were limited in scope but more effective.  2.2 FUNCTION-PRESERVING INITIALIZATIONS We introduce two effective Net2Net strategies. Both are based on initializing the student network to represent the same function as the teacher, then continuing to train the student network by normal means. Speciﬁcally, suppose that a teacher network is represented by a function y = f (x; θ) where  2  Published as a conference paper at ICLR 2016  Algorithm 1: Net2WiderNet Input: {W (i)|i = 1, 2,··· n}, the weight matrix of teacher net Use forward inference to generate a consistent random mapping {g(i)} for i ∈ 1, 2,··· n do  cj ← 0 for j ∈ 1, 2,··· q do cg(i−1)(j) ← cg(i−1)(j) + 1 end for j ∈ 1, 2,··· q do W (i)  k,j ← 1 U (i)  g(i−1)(k),g(i)(j)  cj  end  end Output: {U (i)|i = 1, 2,··· n}: the transformed weight matrix for wider net.  x is the input to the network, y is the output of the network, and θ is the parameters of the network. In our experiments, f is deﬁned by a convolutional network, x is an input image, and y is a vector of probabilities representing the conditional distribution over object categories given x. Our strategy is to choose a new set of parameters θ(cid:48) for a student network g(x; θ(cid:48)) such that  ∀x, f (x; θ) = g(x; θ(cid:48)).  In our description of the method, we assume for simplicity that both the teacher and student networks contain compositions of standard linear neural network layers of the form h(i) = φ(W (i)(cid:62)h(i−1)) where h(i) is the activations of hidden layer i, W (i) is the matrix of incoming weights for this layer, and φ is an activation function, such as as the rectiﬁed linear activation function (Jarrett et al., 2009; Glorot et al., 2011) or the maxout activation function (Goodfellow et al., 2013). We use this layer formalization for clarity, but our method generalizes to arbitrary composition structure of these layers, such as Inception (Szegedy et al., 2014). All of our experiments are in fact performed using Inception networks. Though we describe the method in terms of matrix multiplication for simplicity, it readily extends to convolution (by observing that convolution is multiplication by a doubly block circulant matrix) and to the addition of biases (by treating biases as a row of W that are multiplied by a constant input of 1). We will also give a speciﬁc description for the convolutional case in subsequent discussions of the details of the method. Function-preserving initialization carries many advantages over other approaches to growing the network:  spending time passing through a period of low performance.  • The new, larger network immediately performs as well as the original network, rather than • Any change made to the network after initialization is guaranteed to be an improvement, so long as each local step is an improvement. Previous methods could fail to improve over the baseline even if each local step was guaranteed to be an improvement, because the initial change to the larger model size could worsen performance. • It is always “safe” to optimize all parameters in the network. There is never a stage where some layers receive harmful gradients and must be frozen. This is in contrast to approaches such as cascade correlation, where the old units are frozen in order to avoid making poor adaptations while attempting to inﬂuence the behavior of new randomly connected units.  2.3 NET2WIDERNET Our ﬁrst proposed transformation is Net2WiderNet transformation. This allows a layer to be replaced with a wider layer, meaning a layer that has more units. For convolution architectures, this means the layers will have more convolution channels. Let us start with a simple special case to illustrate the operation. Suppose that layer i and layer i + 1 are both fully connected layers, and layer i uses an elementwise non-linearity. To widen layer i, we replace W (i) and W (i+1). If layer i has m inputs and n outputs, and layer i + 1 has p outputs, then W (i) ∈ Rm×n and W (i+1) ∈ Rn×p. Net2WiderNet allows us to replace  3  Published as a conference paper at ICLR 2016  Figure 2: The Net2WiderNet transformation. In this example, the teacher network has an input layer with two inputs x[1] and x[2], a hidden layer with two rectiﬁed linear hidden units h[1] and h[2], and an output y. We use the Net2WiderNet operator to create a student network that represents the same function as the teacher network. The student network is larger because we replicate the h[2] unit of the teacher. The labels on the edges indicate the value of the associated weights. To replicate the h[2] unit, we copy its weights c and d to the new h[3] unit. The weight f, going out of h[2], must be copied to also go out of h[3]. This outgoing weight must also be divided by 2 to compensate for the replication of h[2]. This is a simple example intended to illustrate the conceptual idea. For a practical application, we would simultaneously replicate many randomly chosen units, and we would add a small amount of noise to break symmetry after the replication. We also typically widen many layers rather than just one layer, by recursively applying the Net2WiderNet operator. layer i with a layer that has q outputs, with q > n. We will introduce a random mapping function g : {1, 2,··· , q} → {1, 2,··· , n}, that satisﬁes  (cid:26) j  g(j) =  random sample from {1, 2,··· n}  j ≤ n j > n  We introduce a new weight matrix U (i) and U (i+1) representing the weights for these layers in the new student network. Then the new weights are given by  U (i)  k,j = W (i)  k,g(j), U (i+1)  j,h =  1  |{x|g(x) = g(j)}| W (i+1) g(j),h.  1  Here, the ﬁrst n columns of W (i) are copied directly into U (i). Columns n+1 through q of U (i) are created by choosing a random as deﬁned in g. The random selection is performed with replacement, so each column of W (i) is copied potentially many times. For weights in U (i+1), we must account for the replication by dividing the weight by replication factor given by |{x|g(x)=g(j)}|, so all the units have the exactly the same value as the unit in the original net. This description can be generalized to making multiple layers wider, with the layers composed as described by an arbitrary directed acyclic computation graph. This general procedure is illustrated by Fig. 2. So far we have only discussed the use of a single random mapping function to expand one layer. We can in fact introduce a random mapping function g(i) for every non-output layer. Importantly, these g(i) are subject to some constraints as deﬁned by the computation graph. Care needs to be taken to ensure that the remapping functions do in fact result in function preservation. To explain, we provide examples of two computational graph structures that impose speciﬁc con- straints on the random remapping functions. One example is the layer structure used by batch normalization (Ioffe & Szegedy, 2015). The layer involves both a standard linear transformation, but also involves elementwise multiplication by learned parameters that allow the layer to represent any range of outputs despite the normaliza- tion operation. The random remapping for the multiplication parameters must match the random remapping for the weight matrix. Otherwise we could generate a new unit that uses the weight vector for pre-existing unit i but is scaled by the multiplication parameter for unit j. The new unit would not implement the same function as the old unit i or as the old unit j.  4  yh[1]h[2]x[1]x[2]abcdefyh[1]h[2]x[1]x[2]abcdef/2h[3]cdf/2Published as a conference paper at ICLR 2016  Figure 3: The Net2DeeperNet Transformation  Another example is concatenation. If we concatenate the output of layer 1 and layer 2, then pass this concatenated output to layer 3, the remapping function for layer 3 needs to take the concatenation into account. The width of the output of layer 1 will determine the offset of the coordinates of units originating in layer 2. To make Net2WiderNet a fully general algorithm, we would need a remapping inference algo- rithm that makes a forward pass through the graph, querying each operation in the graph about how to make the remapping functions consistent. For our experiments, we manually designed all of the inference necessary rules for the Inception network, which also works for most feed forward net- work. This is similar to the existing shape inference functionality that allows us to predict the shape of any tensor in the computational graph by making a forward pass through the graph, beginning with input tensors of known shape. After we get the random mapping, we can copy the weight over and divide by the replication factor, which is formally given by the following equation.  U (i)  k,j =  |{x|g(i−1)(x) = g(i−1)(k)}| W (i)  g(i−1)(k),g(i)(j)  1  It is essential that each unit be replicated at least once, hence the constraint that the resulting layer be wider. This operator can be applied arbitrarily many times; we can expand only one layer of the network, or we can expand all non-output layers. In the setting where several units need to share the same weights, for example convolution opera- tions. We can add such constraint to the random mapping generation, such that source of weight is consistent. This corresponds to make a random mapping on the channel level, instead of unit level, the rest procedure remains the same. When training with certain forms of randomization on the widened layer, such as dropout (Srivastava et al., 2014), it is acceptable to use perfectly transformation preserving Net2WiderNet, as we have described so far. When using a training algorithm that does not use randomization to encourage identical units to learn different functions, one should add a small amount of noise to all but the ﬁrst copy of each column of weights. This results in the student network representing only approximately the same function as the teacher, but this approximation is necessary to ensure that the student can learn to use its full capacity when training resumes.  2.4 NET2DEEPERNET We also introduce a second function-preserving transformation, Net2DeeperNet. This allows us to transform an existing net into a deeper one. Speciﬁcally, the Net2DeeperNet replaces a layer  h(i) = φ(h(i−1)(cid:62)W (i)) with two layers h(i) = φ(cid:0)U (i)(cid:62)φ(cid:0)W (i)(cid:62)h(i−1)(cid:1)(cid:1) . The new matrix U is  initialized to an identity matrix, but remains free to learn to take on any value later. This operation is only applicable when φ is chosen such that φ(Iφ(v)) = φ(v) for all vectors v. This property holds for the rectiﬁed linear activation. To obtain Net2DeeperNet for maxout units, one must use a matrix that is similar to identity, but with replicated columns. Unfortunately, for some popular activation functions, such as the logistic sigmoid, it is not possible to insert a layer of the same type that represents an identity function over the required domain. When we apply it to convolution networks, we can simply set the convolution kernels to be identity ﬁlters. In some cases, to build an identity layer requires additional work. For example, when using batch normalization, we must set the output scale and output bias of the normalization layer to undo the normalization of the layer’s statistics. This requires running forward propagation through the network on training data in order to estimate the activation statistics. The approach we take is a speciﬁc case of a more general approach, that is to build a multiple layer network that factorizes the original layer. Making a deeper but equivalent representation. However it is hard to do general factorization of layers which non-linear transformation units, such as rectiﬁed  5  Published as a conference paper at ICLR 2016  linear and batch normalization. Restricting to adding identity transformation allows us to handle such non-linear transformations, and apply the methodology to the network architectures that we care about. It also saves the computation cost of doing such factorization. We think support for general factorization of layers is an interesting future direction that is worth exploring. When using Net2DeeperNet, there is no need to add noise. We can begin training with a student network that represents exactly the same function as the teacher network. Nevertheless, a small amount of noise can be added to break the symmetry more rapidly. At ﬁrst glance, it appears that Net2Net can only add a new layer with the same width as the layer below it, due to the use of the identity operation. However, Net2WiderNet may be composed with Net2DeeperNet, so we may in fact add any hidden layer that is at least as wide as the layer below it.  3 EXPERIMENTS 3.1 EXPERIMENTAL SETUP We evaluated our Net2Net operators in three different settings. In all cases we used an Inception- BN network (Ioffe & Szegedy, 2015) trained on ImageNet. In the ﬁrst setting, we demonstrate that Net2WiderNet can be used to accelerate the training of a standard Inception network by initializing it with a smaller network. In the second setting, we demonstrate that Net2DeeperNet allows us to increase the depth of the Inception modules. Finally, we use our Net2Net operators in a realistic setting, where we make more dramatic changes to the model size and explore the model space for better performance. In this setting, we demonstrate an improved result on ImageNet. We will be comparing our method to some baseline methods:  • “Random pad”: This is a baseline for Net2WiderNet. We widen the network by adding new units with random weights, rather than by replicating units to perform function- preserving initialization. This operation is implemented by padding the pre-existing weight matrices with additional random values.  • “Random initialization” : This is a baseline for Net2DeeperNet. We compare the train- ing of a deep network accelerated by initialization from a shallow one against the training of an identical deep network initialized randomly.  √  3.2 NET2WIDERNET We start by evaluating the method of Net2WiderNet. We began by constructing a teacher network that was narrower than the standard Inception. We reduced the number of convolution channels at each layer within all Inception modules by a factor of 0.3. Because both the input and output number of channels are reduced, this reduces the number of parameters in most layers to 30% of the original amount. To simplify the software for our experiments, we did not modify any component of the network other than the Inception modules. After training this small teacher network, we used it to accelerated the training of a standard-sized student network. Fig. 4 shows the comparison of different approaches. We can ﬁnd that the proposed approach gives faster convergence than the baseline approaches. Importantly, Net2WiderNet gives the same level of ﬁnal accuracy as the model trained from random initialization. This indicates that the true size of the model governs the accuracy that the training process can attain. There is no loss in accuracy that comes from initializing the model to mimic a smaller one. Net2WiderNet can thus be safely used to get to the same accuracy quicker, reducing the time required to run new experiments.  3.3 NET2DEEPERNET We conducted experiments with using Net2DeeperNet to make the network deeper. For these experiments, we used a standard Inception model as the teacher network, and increased the depth of each inception module. The convolutional layers in Inception modules use rectangular kernels. The convolutional layers are arranged in pairs, with a layer using a vertical kernel followed by a layer using a horizontal kernel. Each of these layers is a complete layer with a rectifying non- linearity and batch normalization; it is not just a factorization of a linear convolution operation into separable parts. Everywhere that a pair of vertical-horizontal convolution layers appears, we added two more pairs of such layers, conﬁgured to result in an identity transformation. The results are shown in Fig. 5. We ﬁnd that Net2DeeperNet obtains good accuracy much faster than training from random initialization, both in terms of training and validation accuracy.  6  Published as a conference paper at ICLR 2016  (a) Training Accuracy of Different Methods  (b) Validation Accuracy of Different Methods  Figure 4: Comparison of various approaches for training a wider model. Net2Net provides a model that is useful almost immediately and reaches high levels of performance faster than the baseline approaches. In particular, Net2WiderNet converges to roughly its ﬁnal validation set accuracy after roughly 3 × 106 minibatches. The randomly initialized baseline converges to the same validation set accuracy but requires roughly an additional 2 × 106 minibatches to do so.  3.4 EXPLORING MODEL DESIGN SPACE WITH Net2Net One of important property of Net2Net is that it enables quick exploration of modelp space, by transforming an existing state-of-art architecture. In this experiment, we made an ambitious ex- ploration of model design space in both wider and deeper directions. Speciﬁcally, we enlarged the 2 times of the original one. We also built another deeper net by width of an Inception model to adding four vertical-horizontal convolutional layer pairs on top of every inception modules in the original Inception model. The results are shown in Fig. 6. This last approach paid off, yielding a model that sets a new state of the art of 78.5% on our ImageNet validation set. We did not train these larger models from scratch, due to resource and time constraints. However, we reported the convergence curve of the original  √  7  0.00.20.40.60.81.0Number of Mini-batches passed1e70.500.550.600.650.700.750.800.850.900.95Accuracy on Training SetNet2WiderNet learning-rate=0.002Net2WiderNet learning-rate=0.005Net2WiderNet learning-rate=0.01random pad learning-rate=0.002random pad learning-rate=0.005random pad learning-rate=0.01Training from random initializationAccuracy of Teacher Model0.00.20.40.60.81.0Number of Mini-batches passed1e70.500.550.600.650.700.750.80Accuracy on Validation SetNet2WiderNet learning-rate=0.002Net2WiderNet learning-rate=0.005Net2WiderNet learning-rate=0.01random pad learning-rate=0.002random pad learning-rate=0.005random pad learning-rate=0.01Training from random initializationAccuracy of Teacher ModelPublished as a conference paper at ICLR 2016  (a) Training Accuracy of Different Methods  (b) Validation Accuracy of Different Methods  Figure 5: Comparison of methods of training a deeper model  inception model for reference, which should be easier to train than these larger models. We can ﬁnd that the models initialized with Net2Net operations converge even faster than the standard model. This example really demonstrate the advantage of Net2Net approach which helps us to explore the design space faster and advance the results in deep learning.  4 DISCUSSION Our Net2Net operators have demonstrated that it is possible to rapidly transfer knowledge from a small neural network to a signiﬁcantly larger neural network under some architectural constraints. We have demonstrated that we can train larger neural networks to improve performance on ImageNet recognition using this approach. Net2Net may also now be used as a technique for exploring model families more rapidly, reducing the amount of time needed for typical machine learning workﬂows. We hope that future research will uncover new ways of transferring knowledge between neural net- works. In particular, we hope future research will reveal more general knowledge transfer methods  8  0.00.20.40.60.81.0Number of Mini-batches passed1e70.50.60.70.80.91.0Accuracy on Training SetNet2DeeperNetTraining from random initializationPrecision of Teacher Model0.00.20.40.60.81.0Number of Mini-batches passed1e70.500.550.600.650.700.750.80Accuracy on Validation SetNet2DeeperNetTraining from random initializationPrecision of Teacher ModelPublished as a conference paper at ICLR 2016  (a) Training Accuracy of Different Methods  (b) Validation Accuracy of Different Methods  Figure 6: Using Net2Net to quickly explore designs of larger nets. By adding eight layers to each Inception module, we obtained a new state of the art test error on ImageNet. that can rapidly initialize a student network whose architecture is not constrained to resemble that of the teacher network.  ACKNOWLEDGMENTS We would like to thank Jeff Dean and George Dahl for helpful discussions. We also thank the developers of TensorFlow (Abadi et al., 2015), which we used for all of our experiments. We would like to thank Conrado Miranda for helpful feedback that we used to improve the clarity and comprehensiveness of this manuscript.  REFERENCES Abadi, Mart´ın, Agarwal, Ashish, Barham, Paul, Brevdo, Eugene, Chen, Zhifeng, Citro, Craig, Corrado, Greg S., Davis, Andy, Dean, Jeffrey, Devin, Matthieu, Ghemawat, Sanjay, Goodfellow, Ian, Harp, Andrew, Irving, Geoffrey, Isard, Michael, Jia, Yangqing, Jozefowicz, Rafal, Kaiser, Lukasz, Kudlur, Manjunath, Leven-  9  0.00.20.40.60.81.0Number of Mini-batches passed1e70.700.750.800.850.900.951.00Accuracy on Training SetNet2DeeperNet: Adding 8 layers to each inception towerNet2WiderNet: Widen the channels in inception tower by factor of 2Convergence of Original InceptionBest Accuracy of Original Inception0.00.20.40.60.81.0Number of Mini-batches passed1e70.700.710.720.730.740.750.760.770.780.79Accuracy on Validation SetNet2DeeperNet: Adding 8 layers to each inception towerNet2WiderNet: Widen the channels in inception tower by factor of 2Convergence of Original InceptionBest Accuracy of Original InceptionPublished as a conference paper at ICLR 2016  berg, Josh, Man´e, Dan, Monga, Rajat, Moore, Sherry, Murray, Derek, Olah, Chris, Schuster, Mike, Shlens, Jonathon, Steiner, Benoit, Sutskever, Ilya, Talwar, Kunal, Tucker, Paul, Vanhoucke, Vincent, Vasudevan, Vijay, Vi´egas, Fernanda, Vinyals, Oriol, Warden, Pete, Wattenberg, Martin, Wicke, Martin, Yu, Yuan, and Zheng, Xiaoqiang. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorﬂow.org.  Buciluˇa, Cristian, Caruana, Rich, and Niculescu-Mizil, Alexandru. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535–541. ACM, 2006.  Fahlman, Scott E. and Lebiere, Christian. The cascade-correlation learning architecture. pp. 524–532, Denver,  CO, 1990. Morgan Kaufmann, San Mateo.  Glorot, X., Bordes, A., and Bengio, Y. Deep sparse rectiﬁer neural networks. In AISTATS’2011, 2011.  Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout net-  works. In ICML’2013, 2013.  Gutstein, Steven, Fuentes, Olac, and Freudenthal, Eric. Knowledge transfer in deep convolutional neural nets.  International Journal on Artiﬁcial Intelligence Tools, 17(03):555–567, 2008.  Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distilling the knowledge in a neural network. arXiv preprint  arXiv:1503.02531, 2015.  Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye. A fast learning algorithm for deep belief nets.  Neural Computation, 18:1527–1554, 2006.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing  internal covariate shift. 2015.  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pp. 2146–2153. IEEE, 2009.  Mahayri, Amjad, Ballas, Nicolas, and Courville, Aaron. FitNets and batch normalization. Technical report,  (unpublished), 2015.  Mitchell, T., Cohen, W., Hruschka, E., Talukdar, P., Betteridge, J., Carlson, A., Dalvi, B., Gardner, M., Kisiel, B., Krishnamurthy, J., Lao, N., Mazaitis, K., Mohamed, T., Nakashole, N., Platanios, E., Ritter, A., Samadi, M., Settles, B., Wang, R., Wijaya, D., Gupta, A., Chen, X., Saparov, A., Greaves, M., and Welling, J. Never- ending learning. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence (AAAI-15), 2015.  Romero, Adriana, Ballas, Nicolas, Ebrahimi Kahou, Samira, Chassang, Antoine, Gatta, Carlo, and Bengio,  Yoshua. FitNets: Hints for thin deep nets. Technical Report Arxiv report 1412.6550, arXiv, 2014.  Silver, DL, Yang, Q, and Li, L. Lifelong machine learning systems: Beyond learning algorithms. In AAAI  Spring Symposium-Technical Report, 2013.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition.  In ICLR, 2015.  Socher, Richard, Bauer, John, Manning, Christopher D., and Ng, Andrew Y. Parsing with compositional vector  grammars. In In Proceedings of the ACL conference, 2013.  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research, 15: 1929–1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du- mitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. Technical report, arXiv:1409.4842, 2014.  Thrun, Sebastian. Lifelong learning: A case study. Technical Report CMU-CS-95-208, School of Computer  Science, Carnegie Mellon University, Pittsburgh, PA 15213, November 1995.  Tieleman, T and Hinton, G. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent  magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012.  10  Published as a conference paper at ICLR 2016  A CHOICE OF HYPERPARAMETERS FOR FINE TUNING Net2Net  TRANSFORMED MODELS  Net2Net involves training neural networks in a different setting than usual, so it is natural to wonder whether the hyperparameters must be changed in some way. We found that, fortunately, nearly all of the hyperparame- ters that are typically used to train a network from scratch may be used to train a network with Net2Net. The teacher network does not need to have any of its hyperparameters changed at all. In our experience, the student network needs only one modiﬁcation, which is to use a smaller learning rate than usual. We ﬁnd that the initial learning rate for the student network should be approximately 1 10 the initial learning rate for the teacher net- work. This makes sense because we can think of the student network as continuing the training process begun by the teacher network, and typically the teacher network training process involves shrinking the learning rate to a smaller value than the initial learning rate.  B RELATED WORK Cascade-correlation (Fahlman & Lebiere, 1990) includes a strategy for learning by enlarging a pre-existing model architecture. This approach uses a very speciﬁc architecture where each new hidden unit receives input from all previous hidden units. Also, the pre-existing hidden units do not continue to learn after the latest hidden unit has been added. A related approach is to add whole new layers while leaving the lower layers ﬁxed (Gutstein et al., 2008). Both these approaches can be considered part of the Net2Net family of operations for rapidly training a new model given a pre-existing one. However, these variants require passing through a temporary period of low performance after a new component has been added to the network but not yet been adapted. Our function-preserving initializations avoid that period of low performance. (In principle we could avoid this period entirely, but in practice we usually add some noise to the student model in order to break symmetry more rapidly—this results in a brief period of reduced performance, but it is a shorter period and the performance is less impaired than in previous approaches) Also, these approaches do not allow pre-existing portions of the model to co-adapt with new portions of the model to attain greater performance, while our method does. Some work on knowledge transfer between neural networks is motivated by a desire to train deeper networks than would otherwise be possible, by incrementally training deeper networks with knowledge provided by smaller ones (Romero et al., 2014; Simonyan & Zisserman, 2015). These works trained convolutional networks with up to 19 weight layers. Somewhat perplexingly, we were able to train models of considerably greater depth without needing to use knowledge transfer, suggesting that deep models do not pose nearly as much difﬁculty as previous authors have believed. This may be due to our use of a very strong baseline: Inception (Szegedy et al., 2014) networks with batch normalization (Ioffe & Szegedy, 2015) trained using RMSProp (Tieleman & Hinton, 2012). The models we refer to as “standard size” have 25 weight layers on the shortest path from input to output, and 47 weight layers on the longest path (3 convolution layers + 11 Inception modules, each Inception module featuring a convolution layer followed by three forked paths, the shortest of which involves only one more convolution and the longest of which involves three more). Rather than using knowledge transfer to make greater depth possible, our goal is merely to accelerate the training of a new model when a pre-existing one is available. Model compression (Buciluˇa et al., 2006; Hinton et al., 2015) is a technique for transferring knowledge from many models to a single model. It serves a different purpose than our Net2Net technique. Model compression aims to regularize the ﬁnal model by causing it to learn a function that is similar to the average function learned by many different models. Net2Net aims to train the ﬁnal model very rapidly by leveraging the knowledge in a pre-existing model, but makes no attempt to cause the ﬁnal model to be more regularized than if it had been trained from scratch. Our Net2DeeperNet operator involves inserting layers initialized to represent identity functions into deep networks. Networks with identity weights at initialization have been used before by, for example, Socher et al. (2013), but to our knowledge that have not previously been used to design function-preserving transformations of pre-existing neural networks. The speciﬁc operators we propose are both based on the idea of transformations of a neural network that preserve the function it represents. This is a closely related concept to the justiﬁcation for greedy layerwise pretraining of deep belief networks (Hinton et al., 2006). A deep belief network is a generative model that draws samples by ﬁrst drawing a sample from a restricted Boltzmann machine then using this sample as input to the ancestral sampling process through a directed sigmoid belief network. This can be transformed into a deep belief network with one more layer by inserting a new directed layer in between the RBM and the directed network. If the new directed layer is initialized with weights from the RBM, then this added layer is equivalent to performing one extra step of Gibbs sampling in the RBM. It thus does not change the probability distribution represented by the deep belief network. Our function-preserving transformations are similar in spirit. However, our transformations preserve the function represented by a feed-forward network. Note that DBN layer stacking does not preserve the function represented by the deterministic upward pass through DBNs that is commonly used to deﬁne a feed-forward neural network. Our function-preserving transformations are more general in the  11  Published as a conference paper at ICLR 2016  sense that they allow adding a layer with any width greater than the layer below it, while DBN growth is only known to be distribution-preserving for one speciﬁc size of new layer.  12  ",
1511.06499,2016,Variational Gaussian Process,"['Variational Gaussian Process\nDustin Tran', 'Rajesh Ranganath', 'David Blei']",https://arxiv.org/pdf/1511.06499,"6 1 0 2    r p A 7 1         ] L M  . t a t s [      4 v 9 9 4 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  THE VARIATIONAL GAUSSIAN PROCESS  Dustin Tran Harvard University dtran@g.harvard.edu  Rajesh Ranganath Princeton University rajeshr@cs.princeton.edu  David M. Blei Columbia University david.blei@columbia.edu  ABSTRACT  Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We de- velop the variational Gaussian process (VGP), a Bayesian nonparametric varia- tional family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art re- sults for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.  1  INTRODUCTION  Variational inference is a powerful tool for approximate posterior inference. The idea is to posit a family of distributions over the latent variables and then ﬁnd the member of that family closest to the posterior. Originally developed in the 1990s (Hinton & Van Camp, 1993; Waterhouse et al., 1996; Jordan et al., 1999), variational inference has enjoyed renewed interest around developing scalable optimization for large datasets (Hoffman et al., 2013), deriving generic strategies for easily ﬁtting many models (Ranganath et al., 2014), and applying neural networks as a ﬂexible parametric family of approximations (Kingma & Welling, 2014; Rezende et al., 2014). This research has been particularly successful for computing with deep Bayesian models (Neal, 1990; Ranganath et al., 2015a), which require inference of a complex posterior distribution (Hinton et al., 2006). Classical variational inference typically uses the mean-ﬁeld family, where each latent variable is independent and governed by its own variational distribution. While convenient, the strong inde- pendence limits learning deep representations of data. Newer research aims toward richer families that allow dependencies among the latent variables. One way to introduce dependence is to con- sider the variational family itself as a model of the latent variables (Lawrence, 2000; Ranganath et al., 2015b). These variational models naturally extend to Bayesian hierarchies, which retain the mean-ﬁeld “likelihood” but introduce dependence through variational latent variables. In this paper we develop a powerful new variational model—the variational Gaussian process (VGP). The VGP is a Bayesian nonparametric variational model; its complexity grows efﬁciently and to- wards any distribution, adapting to the inference problem at hand. We highlight three main contri- butions of this work:  1  Published as a conference paper at ICLR 2016  1. We prove a universal approximation theorem: under certain conditions, the VGP can capture any continuous posterior distribution—it is a variational family that can be speciﬁed to be as expressive as needed.  2. We derive an efﬁcient stochastic optimization algorithm for variational inference with the VGP. Our algorithm can be used in a wide class of models. Inference with the VGP is a black box variational method (Ranganath et al., 2014).  3. We study the VGP on standard benchmarks for unsupervised learning, applying it to per- form inference in deep latent Gaussian models (Rezende et al., 2014) and DRAW (Gregor et al., 2015), a latent attention model. For both models, we report the best results to date.  Technical summary. Generative models hypothesize a distribution of observations x and latent variables z, p(x, z). Variational inference posits a family of the latent variables q(z; λ) and tries to ﬁnd the variational parameters λ that are closest in KL divergence to the posterior. When we use a variational model, q(z; λ) itself might contain variational latent variables; these are implicitly marginalized out in the variational family (Ranganath et al., 2015b). The VGP is a ﬂexible variational model. It draw inputs from a simple distribution, warps those inputs through a non-linear mapping, and then uses the output of the mapping to govern the distribution of the latent variables z. The non-linear mapping is itself a random variable, constructed from a Gaussian process. The VGP is inspired by ideas from both the Gaussian process latent variable model (Lawrence, 2005) and Gaussian process regression (Rasmussen & Williams, 2006). The variational parameters of the VGP are the kernel parameters for the Gaussian process and a set of variational data, which are input-output pairs. The variational data is crucial: it anchors the non-linear mappings at given inputs and outputs. It is through these parameters that the VGP learns complex representations. Finally, given data x, we use stochastic optimization to ﬁnd the variational parameters that minimize the KL divergence to the model posterior.  2 VARIATIONAL GAUSSIAN PROCESS  Variational models introduce latent variables to the variational family, providing a rich construction for posterior approximation (Ranganath et al., 2015b). Here we introduce the variational Gaussian process (VGP), a Bayesian nonparametric variational model that is based on the Gaussian process. The Gaussian process (GP) provides a class of latent variables that lets us capture downstream dis- tributions with varying complexity. We ﬁrst review variational models and Gaussian processes. We then outline the mechanics of the VGP and prove that it is a universal approximator.  2.1 VARIATIONAL MODELS Let p(z| x) denote a posterior distribution over d latent variables z = (z1, . . . , zd) conditioned on a data set x. For a family of distributions q(z; λ) parameterized by λ, variational inference seeks to minimize the divergence KL(q(z; λ)(cid:107) p(z| x)). This is equivalent to maximizing the evidence lower bound (ELBO) (Wainwright & Jordan, 2008). The ELBO can be written as a sum of the expected log likelihood of the data and the KL divergence between the variational distribution and the prior,  L = Eq(z;λ)[log p(x | z)] − KL(q(z; λ)(cid:107)p(z)).  for its density. A common speciﬁcation is a fully factorized distribution(cid:81)  (1) Traditionally, variational inference considers a tractable family of distributions with analytic forms i q(zi; λi), also known as the mean-ﬁeld family. While mean-ﬁeld families lead to efﬁcient computation, they limit the expressiveness of the approximation. The variational family of distributions can be interpreted as a model of the latent variables z, and it can be made richer by introducing new latent variables. Hierarchical variational models consider distributions speciﬁed by a variational prior of the mean-ﬁeld parameters q(λ; θ) and a factorized  i q(zi | λi). This speciﬁes the variational model,  “likelihood”(cid:81)  (cid:90) (cid:104)(cid:89)  (cid:105)  q(z; θ) =  q(zi | λi)  q(λ; θ) dλ,  (2)  i  2  Published as a conference paper at ICLR 2016  D = {(s, t)}  θ  ξ  fi  zi  d  zi  d  x  (a) VARIATIONAL MODEL  (b) GENERATIVE MODEL  Figure 1: (a) Graphical model of the variational Gaussian process. The VGP generates samples of latent variables z by evaluating random non-linear mappings of latent inputs ξ, and then drawing mean-ﬁeld samples parameterized by the mapping. These latent variables aim to follow the posterior distribution for a generative model (b), conditioned on data x.  which is governed by prior hyperparameters θ. Hierarchical variational models are richer than classical variational families—their expressiveness is determined by the complexity of the prior q(λ). Many expressive variational approximations can be viewed under this construct (Saul & Jordan, 1996; Jaakkola & Jordan, 1998; Rezende & Mohamed, 2015; Tran et al., 2015).  2.2 GAUSSIAN PROCESSES  We now review the Gaussian process (GP) (Rasmussen & Williams, 2006). Consider a data set of m source-target pairs D = {(sn, tn)}m n=1, where each source sn has c covariates paired with a multi- dimensional target tn ∈ Rd. We aim to learn a function over all source-target pairs, tn = f (sn), where f : Rc → Rd is unknown. Let the function f decouple as f = (f1, . . . , fd), where each fi : Rc → R. GP regression estimates the functional form of f by placing a prior,  d(cid:89)  i=1  p(f ) =  GP(fi; 0, Kss),  d(cid:89)  where Kss denotes a covariance function k(s, s(cid:48)) evaluated over pairs of inputs s, s(cid:48) ∈ Rc. In this paper, we consider automatic relevance determination (ARD) kernels ωj(sj − s(cid:48)  (cid:16) − 1  k(s, s(cid:48)) = σ2  c(cid:88)  j)2(cid:17)  ARD exp  (3)  ,  2  j=1  ARD, ω1, . . . , ωc). The weights ωj tune the importance of each dimension.  with parameters θ = (σ2 They can be driven to zero during inference, leading to automatic dimensionality reduction. Given data D, the conditional distribution of the GP forms a distribution over mappings which inter- polate between input-output pairs,  p(f |D) =  GP(fi; KξsK−1  ss ti, Kξξ − KξsK−1  ss K(cid:62) ξs).  (4)  Here, Kξs denotes the covariance function k(ξ, s) for an input ξ and over all data inputs sn, and ti represents the ith output dimension.  i=1  2.3 VARIATIONAL GAUSSIAN PROCESSES  We describe the variational Gaussian process (VGP), a Bayesian nonparametric variational model that admits arbitrary structures to match posterior distributions. The VGP generates z by generat- ing latent inputs, warping them with random non-linear mappings, and using the warped inputs as parameters to a mean-ﬁeld distribution. The random mappings are drawn conditional on “varia- tional data,” which are variational parameters. We will show that the VGP enables samples from the mean-ﬁeld to follow arbitrarily complex posteriors. The VGP speciﬁes the following generative process for posterior latent variables z:  3  Published as a conference paper at ICLR 2016  1. Draw latent input ξ ∈ Rc: ξ ∼ N (0, I).  2. Draw non-linear mapping f : Rc → Rd conditioned on D: f ∼(cid:81)d 3. Draw approximate posterior samples z ∈ supp(p): z = (z1, . . . , zd) ∼(cid:81)d  i=1 GP(0, Kξξ)|D. i=1 q(fi(ξ)).  Figure 1 displays a graphical model for the VGP. Here, D = {(sn, tn)}m n=1 represents variational data, comprising input-output pairs that are parameters to the variational distribution. Marginalizing over all latent inputs and non-linear mappings, the VGP is  (cid:90)(cid:90) (cid:34) d(cid:89)  (cid:35)(cid:34) d(cid:89)  (cid:35)  qVGP(z; θ,D) =  q(zi | fi(ξ))  GP(fi; 0, Kξξ)|D  N (ξ; 0, I) df dξ.  (5)  i=1  i=1  The VGP is parameterized by kernel hyperparameters θ and variational data. As a variational model, the VGP forms an inﬁnite ensemble of mean-ﬁeld distributions. A mean- ﬁeld distribution is given in the ﬁrst term of the integrand above. It is conditional on a ﬁxed function f (·) and input ξ; the d outputs fi(ξ) = λi are the mean-ﬁeld’s parameters. The VGP is a form of a hierarchical variational model (Eq.2) (Ranganath et al., 2015b). It places a continuous Bayesian nonparametric prior over mean-ﬁeld parameters. Unlike the mean-ﬁeld, the VGP can capture correlation between the latent variables. The reason is that it evaluates the d independent GP draws at the same latent input ξ. This induces correlation between their outputs, the mean-ﬁeld parameters, and thus also correlation between the latent vari- ables. Further, the VGP is ﬂexible. The complex non-linear mappings drawn from the GP allow it to capture complex discrete and continuous posteriors. We emphasize that the VGP needs variational data. Unlike typical GP regression, there are no ob- served data available to learn a distribution over non-linear mappings of the latent variables z. Thus the ""data"" are variational parameters that appear in the conditional distribution of f in Eq.4. They anchor the random non-linear mappings at certain input-ouput pairs. When optimizing the VGP, the learned variational data enables ﬁnds a distribution of the latent variables that closely follows the posterior.  2.4 UNIVERSAL APPROXIMATION THEOREM  To understand the capacity of the VGP for representing complex posterior distributions, we analyze the role of the Gaussian process. For simplicity, suppose the latent variables z are real-valued, and the VGP treats the output of the function draws from the GP as posterior samples. Consider the optimal function f∗, which is the transformation such that when we draw ξ ∼ N (0, I) and calculate z = f∗(ξ), the resulting distribution of z is the posterior distribution. An explicit construction of f∗ exists if the dimension of the latent input ξ is equal to the number of latent variables. Let P −1 denote the inverse posterior CDF and Φ the standard normal CDF. Using techniques common in copula literature (Nelsen, 2006), the optimal function is  f∗(ξ) = P −1(Φ(ξ1), . . . , Φ(ξd)).  Imagine generating samples z using this function. For latent input ξ ∼ N (0, I), the standard normal CDF Φ applies the probability integral transform: it squashes ξi such that its output ui = Φ(ξi) is uniformly distributed on [0, 1]. The inverse posterior CDF then transforms the uniform random variables P −1(u1, . . . , ud) = z to follow the posterior. The function produces exact posterior samples. In the VGP, the random function interpolates the values in the variational data, which are optimized to minimize the KL divergence. Thus, during inference, the distribution of the GP learns to concentrate around this optimal function. This perspective provides intuition behind the following result. Theorem 1 (Universal approximation). Let q(z; θ,D) denote the variational Gaussian process. Consider a posterior distribution p(z| x) with a ﬁnite number of latent variables and continuous quantile function (inverse CDF). There exists a sequence of parameters (θk,Dk) such that  k→∞ KL(q(z; θk,Dk)(cid:107) p(z| x)) = 0.  lim  4  Published as a conference paper at ICLR 2016  auxiliary inference  R  variational inference  Q  P  Figure 2: Sequence of domain mappings during inference, from variational latent variable space R to posterior latent variable space Q to data space P. We perform variational inference in the posterior space and auxiliary inference in the variational space.  See Appendix B for a proof. Theorem 1 states that any posterior distribution with strictly posi- tive density can be represented by a VGP. Thus the VGP is a ﬂexible model for learning posterior distributions.  3 BLACK BOX INFERENCE  We derive an algorithm for black box inference over a wide class of generative models.  3.1 VARIATIONAL OBJECTIVE  The original ELBO (Eq.1) is analytically intractable due to the log density, log qVGP(z) (Eq.5). To ad- dress this, we present a tractable variational objective inspired by auto-encoders (Kingma & Welling, 2014). A tractable lower bound to the model evidence log p(x) can be derived by subtracting an expected KL divergence term from the ELBO,  log p(x) ≥ EqVGP [log p(x| z)] − KL(qVGP(z)(cid:107)p(z)) − EqVGP  KL(q(ξ, f | z)(cid:107)r(ξ, f | z))  ,  where r(ξ, f | z) is an auxiliary model (we describe r in the next subsection). Various versions of this objective have been considered in the literature (Jaakkola & Jordan, 1998; Agakov & Barber, 2004), and it has been recently revisited by Salimans et al. (2015) and Ranganath et al. (2015b). We perform variational inference in the posterior latent variable space, minimizing KL(q(cid:107)p) to learn the variational model; for this to occur we perform auxiliary inference in the variational latent variable space, minimizing KL(q(cid:107)r) to learn an auxiliary model. See Figure 2. Unlike previous approaches, we rewrite this variational objective to connect  to auto-  (cid:104)  (cid:105)  encoders: (cid:101)L(θ, φ) = EqVGP [log p(x | z)] − EqVGP  (cid:104)  KL(q(z| f (ξ))(cid:107)p(z))  (cid:105)  − EqVGP  KL(q(f | ξ; θ)(cid:107)r(f | ξ, z; φ)) + log q(ξ) − log r(ξ | z)  (cid:105)  (6)  ,  (cid:104)  where the KL divergences are now taken over tractable distributions (see Appendix C). In auto- encoder parlance, we maximize the expected negative reconstruction error, regularized by two terms: an expected divergence between the variational model and the original model’s prior, and an ex- pected divergence between the auxiliary model and the variational model’s prior. This is simply a nested instantiation of the variational auto-encoder bound (Kingma & Welling, 2014): a divergence between the inference model and a prior is taken as regularizers on both the posterior and variational spaces. This interpretation justiﬁes the previously proposed bound for variational models; as we shall see, it also enables lower variance gradients during stochastic optimization.  3.2 AUTO-ENCODING VARIATIONAL MODELS  An inference network provide a ﬂexible parameterization of approximating distributions as used in Helmholtz machines (Hinton & Zemel, 1994), deep Boltzmann machines (Salakhutdinov &  5  Published as a conference paper at ICLR 2016  Larochelle, 2010), and variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014). It replaces local variational parameters with global parameters coming from a neural network. For latent variables zn (which correspond to a data point xn), an inference network speciﬁes a neural network which takes xn as input and its local variational parameters λn as output. This amortizes inference by only deﬁning a set of global parameters. To auto-encode the VGP we specify inference networks to parameterize both the variational and auxiliary models:  xn (cid:55)→ q(zn | xn; θn),  xn, zn (cid:55)→ r(ξn, fn | xn, zn; φn).  Formally, the output of these mappings are the parameters θn and φn respectively. We write the output as distributions above to emphasize that these mappings are a (global) parameterization of the variational model q and auxiliary model r. The local variational parameters θn for q are the variational data Dn. The auxiliary model r is speciﬁed as a fully factorized Gaussian with local variational parameters φn = (µn ∈ Rc+d, σ2 We maximize the variational objective (cid:101)L(θ, φ) over both θ and φ, where θ newly denotes both  3.3 STOCHASTIC OPTIMIZATION  n ∈ Rc+d). 1  the kernel hyperparameters and the inference network’s parameters for the VGP, and φ denotes the inference network’s parameters for the auxiliary model. Following the black box methods, we write the gradient as an expectation and apply stochastic approximations (Robbins & Monro, 1951), sampling from the variational model and evaluating noisy gradients. First, we reduce variance of the stochastic gradients by analytically deriving any tractable expec- tations. The KL divergence between q(z| f (ξ)) and p(z) is commonly used to reduce variance in traditional variational auto-encoders: it is analytic for deep generative models such as the deep latent Gaussian model (Rezende et al., 2014) and deep recurrent attentive writer (Gregor et al., 2015). The KL divergence between r(f | ξ, z) and q(f | ξ) is analytic as the distributions are both Gaussian. The difference log q(ξ) − log r(ξ | z) is simply a difference of Gaussian log densities. See Appendix C for more details. To derive black box gradients, we can ﬁrst reparameterize the VGP, separating noise generation of samples from the parameters in its generative process (Kingma & Welling, 2014; Rezende et al., 2014). The GP easily enables reparameterization: for latent inputs ξ ∼ N (0, I), the transformation ss K(cid:62) f (ξ; θ) = Lξ + KξsK−1 ξs. This is equivalent to evaluating ξ with a random mapping from the GP. Suppose the mean-ﬁeld q(z| f (ξ)) is also reparameterizable, and let (cid:15) ∼ w such that z((cid:15); f ) is a function of ξ whose output z ∼ q(z| f (ξ)). This two-level reparameterization is equivalent to the generative process for z outlined in Section 2.3. We now rewrite the variational objective as log p(x | z((cid:15); f ))  ss ti is a location-scale transform, where LL(cid:62) = Kξξ − KξsK−1  (cid:105) − KL(q(z| f )(cid:107)p(z))  (cid:101)L(θ, φ) = EN (ξ)  (cid:105)  (7)  − EN (ξ)  KL(q(f | ξ; θ)(cid:107)r(f | ξ, z((cid:15); f ); φ)) + log q(ξ) − log r(ξ | z((cid:15); f ))  (cid:104)Ew((cid:15)) (cid:104) (cid:104)Ew((cid:15))  (cid:104)  (cid:105)(cid:105)  .  Eq.7 enables gradients to move inside the expectations and backpropagate over the nested reparam- eterization. Thus we can take unbiased stochastic gradients, which exhibit low variance due to both the analytic KL terms and reparameterization. The gradients are derived in Appendix D, including the case when the ﬁrst KL is analytically intractable. We outline the method in Algorithm 1. For massive data, we apply subsampling on x (Hoffman et al., 2013). For gradients of the model log-likelihood, we employ convenient differentiation tools such as those in Stan and Theano (Carpenter et al., 2015; Bergstra et al., 2010). For non-differentiable latent variables z, or mean-ﬁeld distributions without efﬁcient reparameterizations, we apply the black box gradient estimator from Ranganath et al. (2014) to take gradients of the inner expectation.  1We let the kernel hyperparameters of the VGP be ﬁxed across data points. Note also that unique from other auto-encoder approaches, we let r’s inference network take both xn and zn as input: this avoids an explicit speciﬁcation of the conditional distribution r((cid:15), f | z), which may be difﬁcult to model. This idea was ﬁrst suggested (but not implemented) in Ranganath et al. (2015b).  6  Published as a conference paper at ICLR 2016  Algorithm 1: Black box inference with a variational Gaussian process  Input: Model p(x, z), Mean-ﬁeld family(cid:81)  i q(zi | fi(ξ)).  Output: Variational and auxiliary parameters (θ, φ). Initialize (θ, φ) randomly. while not converged do  Draw noise samples ξ ∼ N (0, I), (cid:15) ∼ w. Parameterize variational samples z = z((cid:15); f (ξ)), f (ξ) = f (ξ; θ).  Update (θ, φ) with stochastic gradients ∇θ(cid:101)L, ∇φ(cid:101)L.  end  3.4 COMPUTATIONAL AND STORAGE COMPLEXITY The algorithm has O(d + m3 + LH 2) complexity, where d is the number of latent variables, m is the size of the variational data, and L is the number of layers of the neural networks with H the average hidden layer size. In particular, the algorithm is linear in the number of latent vari- ables, which is competitive with other variational inference methods. The number of variational and auxiliary parameters has O(c + LH) complexity; this complexity comes from storing the kernel hyperparameters and the neural network parameters. Unlike most GP literature, we require no low rank constraints, such as the use of inducing variables for scalable computation (Quiñonero-Candela & Rasmussen, 2005). The variational data serve a similar purpose, but inducing variables reduce the rank of a (ﬁxed) kernel matrix; the variational data directly determine the kernel matrix and thus the kernel matrix is not ﬁxed. Although we haven’t found it necessary in practice, see Appendix E for scaling the size of variational data.  4 RELATED WORK  Recently, there has been interest in applying parametric transformations for approximate inference. Parametric transformations of random variables induce a density in the transformed space, with a Jacobian determinant that accounts for how the transformation warps unit volumes. Kucukelbir et al. (2016) consider this viewpoint for automating inference, in which they posit a transformation from the standard normal to a possibly constrained latent variable space. In general, however, calculating the Jacobian determinant incurs a costly O(d3) complexity, cubic in the number of latent variables. Dinh et al. (2015) consider volume-preserving transformations which avoid calculating Jacobian determinants. Salimans et al. (2015) consider volume-preserving transformations deﬁned by Markov transition operators. Rezende & Mohamed (2015) consider a slightly broader class of parametric transformations, with Jacobian determinants having at most O(d) complexity. Instead of specifying a parametric class of mappings, the VGP posits a Bayesian nonparametric prior over all continuous mappings. The VGP can recover a certain class of parametric transformations by using kernels which induce a prior over that class. In the context of the VGP, the GP is an inﬁnitely wide feedforward network which warps latent inputs to mean-ﬁeld parameters. Thus, the VGP offers complete ﬂexibility on the space of mappings—there are no restrictions such as invertibility or linear complexity—and is fully Bayesian. Further, it is a hierarchical variational model, using the GP as a variational prior over mean-ﬁeld parameters (Ranganath et al., 2015b). This enables inference over both discrete and continuous latent variable models. In addition to its ﬂexibility over parametric methods, the VGP is more computationally efﬁcient. Parametric methods must consider transformations with Jacobian determinants of at most O(d) complexity. This restricts the ﬂexibility of the mapping and therefore the ﬂexibility of the varia- tional model (Rezende & Mohamed, 2015). In comparison, the distribution of outputs using a GP prior does not require any Jacobian determinants (following Eq.4); instead it requires auxiliary in- ference for inferring variational latent variables (which is fast). Further, unlike discrete Bayesian  7  Published as a conference paper at ICLR 2016  Model DLGM + VAE [1] DLGM + HVI (8 leapfrog steps) [2] DLGM + NF (k = 80) [3] EoNADE-5 2hl (128 orderings) [4] DBN 2hl [5] DARN 1hl [6] Convolutional VAE + HVI [2] DLGM 2hl + IWAE (k = 50) [1] DRAW [7] DLGM 1hl + VGP DLGM 2hl + VGP DRAW + VGP  − log p(x)  85.51  84.68 84.55 84.13 81.94  ≤ 86.76 88.30 85.10  83.49 82.90 80.97 84.79 81.32 79.88  Table 1: Negative predictive log-likelihood for binarized MNIST. Previous best results are [1] (Burda et al., 2016), [2] (Salimans et al., 2015), [3] (Rezende & Mohamed, 2015), [4] (Raiko et al., 2014), [5] (Murray & Salakhutdinov, 2009), [6] (Gregor et al., 2014), [7] (Gregor et al., 2015).  nonparametric priors such as an inﬁnite mixture of mean-ﬁeld distributions, the GP enables black box inference with lower variance gradients—it applies a location-scale transform for reparameteri- zation and has analytically tractable KL terms. Transformations, which convert samples from a tractable distribution to the posterior, is a classic technique in Bayesian inference. It was ﬁrst studied in Monte Carlo methods, where it is core to the development of methods such as path sampling, annealed importance sampling, and sequential Monte Carlo (Gelman & Meng, 1998; Neal, 1998; Chopin, 2002). These methods can be recast as specifying a discretized mapping ft for times t0 < . . . < tk, such that for draws ξ from the tractable distribution, ft0(ξ) outputs the same samples and ftk (ξ) outputs exact samples following the poste- rior. By applying the sequence in various forms, the transformation bridges the tractable distribution to the posterior. Specifying a good transformation—termed “schedule” in the literature—is crucial to the efﬁciency of these methods. Rather than specify it explicitly, the VGP adaptively learns this transformation and avoids discretization. Limiting the VGP in various ways recovers well-known probability models as variational approxima- tions. Speciﬁcally, we recover the discrete mixture of mean-ﬁeld distributions (Bishop et al., 1998; Jaakkola & Jordan, 1998). We also recover a form of factor analysis (Tipping & Bishop, 1999) in the variational space. Mathematical details are in Appendix A.  5 EXPERIMENTS  Following standard benchmarks for variational inference in deep learning, we learn generative mod- els of images. In particular, we learn the deep latent Gaussian model (DLGM) (Rezende et al., 2014), a layered hierarchy of Gaussian random variables following neural network architecures, and the recently proposed Deep Recurrent Attentive Writer (DRAW) (Gregor et al., 2015), a latent attention model that iteratively constructs complex images using a recurrent architecture and a sequence of variational auto-encoders (Kingma & Welling, 2014). For the learning rate we apply a version of RMSProp (Tieleman & Hinton, 2012), in which we scale the value with a decaying schedule 1/t1/2+(cid:15) for (cid:15) > 0. We ﬁx the size of variational data to be 500 across all experiments and set the latent input dimension equal to the number of latent variables.  5.1 BINARIZED MNIST  The binarized MNIST data set (Salakhutdinov & Murray, 2008) consists of 28x28 pixel images with binary-valued outcomes. Training a DLGM, we apply two stochastic layers of 100 random vari- ables and 50 random variables respectively, and in-between each stochastic layer is a deterministic  8  Published as a conference paper at ICLR 2016  Model DRAW  DRAW + VGP  Epochs ≤ − log p(x) 100 200 300 100 200 300  526.8 479.1 464.5 460.1 444.0 423.9  Table 2: Negative predictive log-likelihood for Sketch, learned over hundreds of epochs over all 18,000 training examples.  Figure 3: Generated images from DRAW with a VGP (top), and DRAW with the original variational auto-encoder (bottom). The VGP learns texture and sharpness, able to sketch more complex shapes.  layer with 100 units using tanh nonlinearities. We apply mean-ﬁeld Gaussian distributions for the stochastic layers and a Bernoulli likelihood. We train the VGP to learn the DLGM for the cases of one stochastic layer and two stochastic layers. For DRAW (Gregor et al., 2015), we augment the mean-ﬁeld Gaussian distribution originally used to generate the latent samples at each time step with the VGP, as it places a complex variational prior over its parameters. The encoding recurrent neural network now outputs variational data (used for the variational model) as well as mean-ﬁeld Gaussian parameters (used for the auxiliary model). We use the same architecture hyperparameters as in Gregor et al. (2015). After training we evaluate test set log likelihood, which are lower bounds on the true value. See Table 1 which reports both approximations and lower bounds of log p(x) for various methods. The VGP achieves the highest known results on log-likelihood using DRAW, reporting a value of -79.88 compared to the original highest of -80.97. The VGP also achieves the highest known results among the class of non-structure exploiting models using the DLGM, with a value of -81.32 compared to the previous best of -82.90 reported by Burda et al. (2016).  5.2 SKETCH  As a demonstration of the VGP’s complexity for learning representations, we also examine the Sketch data set (Eitz et al., 2012). It consists of 20,000 human sketches equally distributed over 250 object categories. We partition it into 18,000 training examples and 2,000 test examples. We ﬁx the archi- tecture of DRAW to have a 2x2 read window, 5x5 write attention window, and 64 glimpses—these values were selected using a coarse grid search and choosing the set which lead to the best training log likelihood. For inference we use the original auto-encoder version as well as the augmented version with the VGP. See Table 2. DRAW with the VGP achieves a signiﬁcantly better lower bound, performing better than the original version which has seen state-of-the-art success in many computer vision tasks. (Until the results presented here, the results from the original DRAW were the best reported performance for this data set.). Moreover, the model inferred using the VGP is able to generate more complex images than the original version—it not only performs better but maintains higher visual ﬁdelity.  6 DISCUSSION  We present the variational Gaussian process (VGP), a variational model which adapts its shape to match complex posterior distributions. The VGP draws samples from a tractable distribution, and posits a Bayesian nonparametric prior over transformations from the tractable distribution to mean- ﬁeld parameters. The VGP learns the transformations from the space of all continuous mappings—it is a universal approximator and ﬁnds good posterior approximations via optimization. In future work the VGP will be explored for application in Monte Carlo methods, where it may be an efﬁcient proposal distribution for importance sampling and sequential Monte Carlo. An important avenue of research is also to characterize local optima inherent to the objective function. Such analysis will improve our understanding of the limits of the optimization procedure and thus the limits of variational inference.  9  Published as a conference paper at ICLR 2016  ACKNOWLEDGEMENTS  We thank David Duvenaud, Alp Kucukelbir, Ryan Giordano, and the anonymous reviewers for their helpful comments. This work is supported by NSF IIS-0745520, IIS-1247664, IIS-1009542, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009, N66001-15-C-4032, Facebook, Adobe, Amazon, and the Seibel and John Templeton Foundations.  REFERENCES  Agakov, Felix V and Barber, David. An auxiliary variational method. In Neural Information Pro-  cessing, pp. 561–566. Springer, 2004.  Bergstra, James, Breuleux, Olivier, Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), 2010.  Bishop, Christopher M., Lawrence, Neil D., Jordan, Michael I., and Jaakkola, Tommi. Approximat- ing posterior distributions in belief networks using mixtures. In Neural Information Processing Systems, 1998.  Burda, Yuri, Grosse, Roger, and Salakhutdinov, Ruslan.  Importance weighted autoencoders.  In  International Conference on Learning Representations, 2016.  Carpenter, Bob, Hoffman, Matthew D., Brubaker, Marcus, Lee, Daniel, Li, Peter, and Betancourt, Michael. The Stan Math Library: Reverse-mode automatic differentiation in C++. arXiv preprint arXiv:1509.07164, 2015.  Chopin, Nicolas. A sequential particle ﬁlter method for static models. Biometrika, 89(3):539–552,  2002.  Cunningham, John P, Shenoy, Krishna V, and Sahani, Maneesh. Fast Gaussian process methods for point process intensity estimation. In International Conference on Machine Learning. ACM, 2008.  Dinh, Laurent, Krueger, David, and Bengio, Yoshua. NICE: Non-linear independent components  estimation. In International Conference on Learning Representations Workshop, 2015.  Eitz, Mathias, Hays, James, and Alexa, Marc. How do humans sketch objects? ACM Trans. Graph.  (Proc. SIGGRAPH), 31(4):44:1–44:10, 2012.  Gelman, Andrew and Meng, Xiao-Li. Simulating normalizing constants: From importance sampling  to bridge sampling to path sampling. Statistical Science, 1998.  Gregor, Karol, Danihelka, Ivo, Mnih, Andriy, Blundell, Charles, and Wierstra, Daan. Deep autore-  gressive networks. In International Conference on Machine Learning, 2014.  Gregor, Karol, Danihelka, Ivo, Graves, Alex, Rezende, Danilo Jimenez, and Wierstra, Daan. DRAW: A recurrent neural network for image generation. In International Conference on Ma- chine Learning, 2015.  Hinton, G. and Van Camp, D. Keeping the neural networks simple by minimizing the description  length of the weights. In Computational Learning Theory, pp. 5–13. ACM, 1993.  Hinton, Geoffrey E and Zemel, Richard S. Autoencoders, minimum description length, and  helmholtz free energy. In Neural Information Processing Systems, 1994.  Hinton, Geoffrey E, Osindero, Simon, and Teh, Yee-Whye. A fast learning algorithm for deep belief  nets. Neural computation, 18(7):1527–1554, 2006.  Hoffman, Matthew D, Blei, David M, Wang, Chong, and Paisley, John. Stochastic variational infer-  ence. Journal of Machine Learning Research, 14:1303–1347, 2013.  Jaakkola, Tommi S and Jordan, Michael I.  Improving the mean ﬁeld approximation via the use of mixture distributions. In Learning in Graphical Models, pp. 163–173. Springer Netherlands, Dordrecht, 1998.  10  Published as a conference paper at ICLR 2016  Jordan, Michael I, Ghahramani, Zoubin, Jaakkola, Tommi S, and Saul, Lawrence K. An introduction  to variational methods for graphical models. Machine Learning, 37(2):183–233, 1999.  Kingma, Diederik P and Welling, Max. Auto-encoding variational Bayes. In International Confer-  ence on Learning Representations, 2014.  Kucukelbir, Alp, Tran, Dustin, Ranganath, Rajesh, Gelman, Andrew, and Blei, David M. Automatic  differentiation variational inference. arXiv preprint arXiv:1603.00788, 2016.  Lawrence, Neil. Variational Inference in Probabilistic Models. PhD thesis, 2000. Lawrence, Neil. Probabilistic non-linear principal component analysis with Gaussian process latent  variable models. The Journal of Machine Learning Research, 6:1783–1816, 2005.  Murray, Iain and Salakhutdinov, Ruslan R. Evaluating probabilities under high-dimensional latent  variable models. In Advances in neural information processing systems, pp. 1137–1144, 2009.  Neal, Radford M. Learning stochastic feedforward networks. Department of Computer Science,  University of Toronto, 1990.  Neal, Radford M. Annealed importance sampling. Statistics and Computing, 11(2):125–139, 1998. Nelsen, Roger B. An Introduction to Copulas (Springer Series in Statistics). Springer-Verlag New  York, Inc., 2006.  Osborne, Michael. Bayesian Gaussian processes for sequential prediction, optimisation and quadra-  ture. PhD thesis, Oxford University New College, 2010.  Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward. A unifying view of sparse approximate  Gaussian process regression. Journal of Machine Learning Research, 6:1939–1959, 2005.  Raiko, Tapani, Li, Yao, Cho, Kyunghyun, and Bengio, Yoshua.  Iterative neural autoregressive distribution estimator nade-k. In Advances in Neural Information Processing Systems, pp. 325– 333, 2014.  Ranganath, Rajesh, Gerrish, Sean, and Blei, David M. Black box variational inference. In Artiﬁcial  Intelligence and Statistics, 2014.  Ranganath, Rajesh, Tang, Linpeng, Charlin, Laurent, and Blei, David M. Deep exponential families.  In Artiﬁcial Intelligence and Statistics, 2015a.  Ranganath, Rajesh, Tran, Dustin, and Blei, David M. Hierarchical variational models. arXiv preprint  arXiv:1511.02386, 2015b.  Rasmussen, Carl Edward and Williams, Christopher K I. Gaussian processes for machine learning.  Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA, 2006.  Rezende, Danilo Jimenez and Mohamed, Shakir. Variational inference with normalizing ﬂows. In  International Conference on Machine Learning, 2015.  Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learn- ing, 2014.  Robbins, Herbert and Monro, S. A stochastic approximation method. The Annals of Mathematical  Statistics, 1951.  Salakhutdinov, Ruslan and Larochelle, Hugo. Efﬁcient learning of deep Boltzmann machines. In  International Conference on Artiﬁcial Intelligence and Statistics, pp. 693–700, 2010.  Salakhutdinov, Ruslan and Murray, Iain. On the quantitative analysis of deep belief networks. In  International Conference on Machine Learning, 2008.  Salimans, Tim, Kingma, Diederik P, and Welling, Max. Markov chain Monte Carlo and variational  inference: Bridging the gap. In International Conference on Machine Learning, 2015.  Saul, Lawrence K and Jordan, Michael I. Exploiting tractable substructures in intractable networks.  In Neural Information Processing Systems, 1996.  11  Published as a conference paper at ICLR 2016  Tieleman, T. and Hinton, G. Lecture 6.5—RmsProp: Divide the gradient by a running average of its  recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  Tipping, Michael E and Bishop, Christopher M. Probabilistic principal component analysis. Journal  of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622, 1999.  Tran, Dustin, Blei, David M., and Airoldi, Edoardo M. Copula variational inference. In Neural  Information Processing Systems, 2015.  Van Der Vaart, Aad and Van Zanten, Harry. Information rates of nonparametric Gaussian process  methods. The Journal of Machine Learning Research, 12:2095–2119, 2011.  Wainwright, Martin J and Jordan, Michael I. Graphical models, exponential families, and variational  inference. Foundations and Trends R(cid:13) in Machine Learning, 1(1-2):1–305, 2008.  Waterhouse, S., MacKay, D., and Robinson, T. Bayesian methods for mixtures of experts. In Neural  Information Processing Systems, 1996.  A SPECIAL CASES OF THE VARIATIONAL GAUSSIAN PROCESS  We now analyze two special cases of the VGP: by limiting its generative process in various ways, we recover well-known models. This provides intuition behind the VGP’s complexity. In Section 4 we show many recently proposed models can also be viewed as special cases of the VGP. Special Case 1. A mixture of mean-ﬁeld distributions is a VGP without a kernel.  A discrete mixture of mean-ﬁeld distributions (Bishop et al., 1998; Jaakkola & Jordan, 1998; Lawrence, 2000) is a classically studied variational model with dependencies between latent vari- ables. Instead of a mapping which interpolates between inputs of the variational data, suppose the VGP simply performs nearest-neighbors for a latent input ξ—selecting the output tn tied to the near- est variational input sn. This induces a multinomial distribution of outputs, which samples one of the variational outputs’ mean-ﬁeld parameters.2 Thus, with a GP prior that interpolates between inputs, the VGP can be seen as a kernel density smoothing of the nearest-neighbor function. Special Case 2. Variational factor analysis is a VGP with linear kernel and no variational data.  Consider factor analysis (Tipping & Bishop, 1999) in the variational space: 3  ξ ∼ N (0, I),  zi ∼ N (w(cid:62)ξ, I).  Marginalizing over the latent inputs induces linear dependence in z, q(z; w) = N (z; 0, ww(cid:62)). Consider the dual interpretation  fi ∼ GP(0, k(·,·)), k(s, s(cid:48)) = s(cid:62)s(cid:48),  ξ ∼ N (0, I), with q(z| ξ) = N (z; 0, ξξ ). The maximum likelihood estimate of w in factor analysis is the maximum a posteriori estimate of ξ in the GP formulation. More generally, use of a non-linear kernel induces non-linear dependence in z. Learning the set of kernel hyperparameters θ thus learns the set capturing the most variation in its latent embedding of z (Lawrence, 2005).  zi = fi(ξ),  (cid:62)  B PROOF OF THEOREM 1 Theorem 1. Let q(z; θ,D) denote the variational Gaussian process. Consider a posterior distri- bution p(z| x) with a ﬁnite number of latent variables and continuous quantile function (inverse CDF). There exists a sequence of parameters (θk,Dk) such that  k→∞ KL(q(z; θk,Dk)(cid:107) p(z| x)) = 0.  lim  2Formally, given variational input-output pairs {(sn, tn)}, the nearest-neighbor function is deﬁned as f (ξ) = tj, such that (cid:107)ξ − sj(cid:107) < (cid:107)ξ − sk(cid:107) for all k. Then the output’s distribution is multinomial with probabilities P (f (ξ) = tj), proportional to areas of the partitioned nearest-neighbor space.  3 For simplicity, we avoid discussion of the VGP’s underlying mean-ﬁeld distribution, i.e., we specify each  mean-ﬁeld factor to be a degenerate point mass at its parameter value.  12  Published as a conference paper at ICLR 2016  Proof. Let the mean-ﬁeld distribution be given by degenerate delta distributions  q(zi | fi) = δfi(zi).  Let the size of the latent input be equivalent to the number of latent variables c = d and ﬁx σ2 ARD = 1 and ωj = 1. Furthermore for simplicity, we assume that ξ is drawn uniformly on the d-dimensional hypercube. Then as explained in Section 2.4, if we let P −1 denote the inverse posterior cumulative distribution function, the optimal f denoted f∗ such that  is  KL(q(z; θ)(cid:107) p(z| x)) = 0  f∗(ξ) = P −1(ξ1, ..., ξd).  Deﬁne Ok to be the set of points j/2k for j = 0 to 2k, and deﬁne Sk to be the d-dimensional product of Ok. Let Dk be the set containing the pairs (si, f∗(si)), for each element si in Sk. Denote f k as the GP mapping conditioned on the dataset Dk, this random mapping satisﬁes f k(si) = f∗(si) for all si ∈ Sk by the noise free prediction property of Gaussian processes (Rasmussen & Williams, 2006). Then by continuity, as k → ∞, f k converges to f∗.  A broad condition under which the quantile function of a distribution is continuous is if that distri- bution has positive density with respect to the Lebesgue measure. The rate of convergence for ﬁnite sizes of the variational data can be studied via posterior contraction rates for GPs under random covariates (Van Der Vaart & Van Zanten, 2011). Only an additional assumption using stronger continuity conditions for the posterior quantile and the use of Matern covariance functions is required for the theory to be applicable in the variational setting.  C VARIATIONAL OBJECTIVE  We derive the tractable lower bound to the model evidence log p(x) presented in Eq.6. To do this, we ﬁrst penalize the ELBO with an expected KL term,  log p(x) ≥ L = EqVGP [log p(x| z)] − KL(qVGP(z)(cid:107)p(z))  ≥ EqVGP [log p(x| z)] − KL(qVGP(z)(cid:107)p(z)) − EqVGP  KL(q(ξ, f | z)(cid:107)r(ξ, f | z))  .  (cid:104)  We can combine all terms into the expectations as follows:  log p(x| z) − log q(z) + log p(z) − log q(ξ, f | z) + log r(ξ, f | z) log p(x| z) − log q(z| f (ξ)) + log p(z) − log q(ξ, f ) + log r(ξ, f | z)  (cid:101)L = Eq(z,ξ,f )  = Eq(z,ξ,f )  (cid:104) (cid:104)  where we apply the product rule q(z)q(ξ, f | z) = q(z| f (ξ))q(ξ, f ). Recombining terms as KL divergences, and written with parameters (θ, φ), this recovers the auto-encoded variational objective  in Section 3:(cid:101)L(θ, φ) = EqVGP [log p(x | z)] − EqVGP  (cid:104)  (cid:105)  KL(q(z| f (ξ))(cid:107)p(z))  − EqVGP  KL(q(f | ξ; θ)(cid:107)r(f | ξ, z; φ)) + log q(ξ) − log r(ξ | z)  .  (cid:104)  The KL divergence between the mean-ﬁeld q(z| f (ξ)) and the model prior p(z) is analytically tractable for certain popular models. For example, in the deep latent Gaussian model (Rezende et al., 2014) and DRAW (Gregor et al., 2015), both the mean-ﬁeld distribution and model prior are Gaussian, leading to an analytic KL term: for Gaussian random variables of dimension d,  KL(N (x; m1, Σ1)(cid:107)N (x; m2, Σ2)) =  (cid:0)(m1 − m2)(cid:62)Σ−1  1 (m1 − m2) + tr(Σ−1  1 Σ2 + log Σ1 − log Σ2) − d(cid:1) .  1 2  13  (cid:105)  (cid:105)  ,  (cid:105)  (cid:105)  Published as a conference paper at ICLR 2016  In general, when the KL is intractable, we combine the KL term with the reconstruction term, and maximize the variational objective  (cid:101)L(θ, φ) = EqVGP [log p(x, z) − log q(z| f (ξ))]  − EqVGP  KL(q(f | ξ; θ)(cid:107)r(f | ξ, z; φ)) + log q(ξ) − log r(ξ | z)  (8)  .  (cid:104)  (cid:105)  We expect that this experiences slightly higher variance in the stochastic gradients during optimiza- tion. We now consider the second term. Recall that we specify the auxiliary model to be a fully factorized Gaussian, r(ξ, f | z) = N ((ξ, f (ξ))(cid:62) | z; m, S), where m ∈ Rc+d, S ∈ Rc+d. Further, the varia- tional priors q(ξ) and q(f | ξ) are both deﬁned to be Gaussian. Therefore it is also a KL divergence between Gaussian distributed random variables. Similarly, log q(ξ) − log r(ξ | z) is simply a dif- ference of Gaussian log densities. The second expression is simple to compute and backpropagate gradients.  D GRADIENTS OF THE VARIATIONAL OBJECTIVE  We derive gradients for the variational objective (Eq.7). This follows trivially by backpropaga- tion:  ∇θ(cid:101)L(θ, φ) = EN (ξ)[Ew((cid:15))[∇θf (ξ)∇f z((cid:15))∇z log p(x| z)]] (cid:104)∇θ KL(q(z| f (ξ; θ))(cid:107)p(z)) (cid:105)(cid:105) (cid:104)∇θ KL(q(f | ξ; θ)(cid:107)r(f | ξ, z; φ))  (cid:104)Ew((cid:15)) (cid:104)Ew((cid:15))  − EN (ξ) − EN (ξ)  (cid:105)(cid:105)  ∇φ(cid:101)L(θ, φ) = −EN (ξ)[Ew((cid:15))[∇φ KL(q(f | ξ; θ)(cid:107)r(f | ξ, z; φ)) − ∇φ log r(ξ | z; φ)]],  ,  where we assume the KL terms are analytically written from Appendix C and gradients are prop- agated similarly through their computational graph. In practice, we need only be careful about the expectations, and the gradients of the functions written above are taken care of with automatic differentiation tools. We also derive gradients for the general variational bound of Eq.8—it assumes that the ﬁrst KL term, measuring the divergence between q and the prior for p, is not necessarily tractable. Following the reparameterizations described in Section 3.3, this variational objective can be rewritten as  log p(x, z((cid:15); f )) − log q(z((cid:15); f )| f )  (cid:105)(cid:105)  (cid:101)L(θ, φ) = EN (ξ)  (cid:104)Ew((cid:15)) (cid:104) (cid:104)Ew((cid:15))  (cid:104)  − EN (ξ)  KL(q(f | ξ; θ)(cid:107)r(f | ξ, z((cid:15); f ); φ)) + log q(ξ) − log r(ξ | z((cid:15); f ))  We calculate gradients by backpropagating over the nested reparameterizations:  ∇θ(cid:101)L(θ, φ) = EN (ξ)[Ew((cid:15))[∇θf (ξ)∇f z((cid:15))[∇z log p(x, z) − ∇z log q(z| f )]]] ∇φ(cid:101)L(θ, φ) = −EN (ξ)[Ew((cid:15))[∇φ KL(q(f | ξ; θ)(cid:107)r(f | ξ, z; φ)) − ∇φ log r(ξ | z; φ)]].  (cid:104)∇θ KL(q(f | ξ; θ)(cid:107)r(f | ξ, z; φ))  (cid:104)Ew((cid:15))  − EN (ξ)  (cid:105)(cid:105)  (cid:105)(cid:105)  .  E SCALING THE SIZE OF VARIATIONAL DATA  If massive sizes of variational data are required, e.g., when its cubic complexity due to inversion of a m × m matrix becomes the bottleneck during computation, we can scale it further. Consider ﬁxing the variational inputs to lie on a grid. For stationary kernels, this allows us to exploit Toeplitz structure for fast m × m matrix inversion. In particular, one can embed the Toeplitz matrix into a circulant matrix and apply conjugate gradient combined with fast Fourier transforms in order to compute inverse-matrix vector products in O(m log m) computation and O(m) storage (Cunning- ham et al., 2008). For product kernels, we can further exploit Kronecker structure to allow fast m × m matrix inversion in O(P m1+1/P ) operations and O(P m2/P ) storage, where P > 1 is the number of kernel products (Osborne, 2010). The ARD kernel speciﬁcally leads to O(cm1+1/c) complexity, which is linear in m.  14  ",
1511.00830,2016,The Variational Fair Autoencoder,"['The Variational Fair Autoencoder\nChristos Louizos', 'Kevin Swersky', 'Yujia Li', 'Max Welling', 'Richard Zemel']",https://arxiv.org/pdf/1511.00830,"7 1 0 2     g u A 0 1         ] L M  . t a t s [      6 v 0 3 8 0 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  THE VARIATIONAL FAIR AUTOENCODER  Christos Louizos∗, Kevin Swersky×, Yujia Li×, Max Welling∗†‡, Richard Zemel×† ∗ Machine Learning Group, University of Amsterdam ×Department of Computer Science, University of Toronto † Canadian Institute for Advanced Research (CIFAR) ‡ University of California, Irvine C.Louizos@uva.nl, {kswersky, yujiali}@cs.toronto.edu M.Welling@uva.nl, zemel@cs.toronto.edu  ABSTRACT  We investigate the problem of learning representations that are invariant to cer- tain nuisance or sensitive factors of variation in the data while retaining as much of the remaining information as possible. Our model is based on a variational autoencoding architecture (Kingma & Welling, 2014; Rezende et al., 2014) with priors that encourage independence between sensitive and latent factors of varia- tion. Any subsequent processing, such as classiﬁcation, can then be performed on this purged latent representation. To remove any remaining dependencies we in- corporate an additional penalty term based on the “Maximum Mean Discrepancy” (MMD) (Gretton et al., 2006) measure. We discuss how these architectures can be efﬁciently trained on data and show in experiments that this method is more effective than previous work in removing unwanted sources of variation while maintaining informative latent representations.  1  INTRODUCTION  In “Representation Learning” one tries to ﬁnd representations of the data that are informative for a particular task while removing the factors of variation that are uninformative and are typically detrimental for the task under consideration. Uninformative dimensions are often called “noise” or “nuisance variables” while informative dimensions are usually called latent or hidden factors of variation. Many machine learning algorithms can be understood in this way: principal component analysis, nonlinear dimensional reduction and latent Dirichlet allocation are all models that extract informative factors (dimensions, causes, topics) of the data which can often be used to visualize the data. On the other hand, linear discriminant analysis and deep (convolutional) neural nets learn representations that are good for classiﬁcation. In this paper we consider the case where we wish to learn latent representations where (almost) all of the information about certain known factors of variation are purged from the representation while still retaining as much information about the data as possible. In other words, we want a latent rep- resentation z that is maximally informative about an observed random variable y (e.g., class label) while minimally informative about a sensitive or nuisance variable s. By treating s as a sensitive variable, i.e. s is correlated with our objective, we are dealing with “fair representations”, a problem previously considered by Zemel et al. (2013). If we instead treat s as a nuisance variable we are deal- ing with “domain adaptation”, in other words by removing the domain s from our representations we will obtain improved performance. In this paper we introduce a novel model based on deep variational autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014). These models can naturally encourage separation between latent variables z and sensitive variables s by using factorized priors p(s)p(z). However, some dependencies may still remain when mapping data-cases to their hidden representation using the variational posterior q(z|x, s), which we stamp out using a “Maximum Mean Discrepancy” (Gretton et al., 2006) term that penalizes differences between all order moments of the marginal posterior distributions q(z|s = k) and q(z|s = k(cid:48)) (for a discrete RV s). In experiments we show that this combined approach is highly successful in learning representations that are devoid of unwanted information while retaining as much information as possible from what remains.  1  Published as a conference paper at ICLR 2016  2 LEARNING INVARIANT REPRESENTATIONS  Figure 1: Unsupervised model  Figure 2: Semi-supervised model  2.1 UNSUPERVISED MODEL  Factoring out undesired variations from the data can be easily formulated as a general probabilistic model which admits two distinct (independent) “sources”; an observed variable s, which denotes the variations that we want to remove, and a continuous latent variable z which models all the remaining information. This generative process can be formally deﬁned as: x ∼ pθ(x|z, s)  z ∼ p(z);  where pθ(x|z, s) is an appropriate probability distribution for the data we are modelling. With this formulation we explicitly encode a notion of ‘invariance’ in our model, since the latent represen- tation is marginally independent of the factors of variation s. Therefore the problem of ﬁnding an invariant representation for a data point x and variation s can be cast as performing inference on this graphical model and obtaining the posterior distribution of z, p(z|x, s). For our model we will employ a variational autoencoder architecture (Kingma & Welling, 2014; Rezende et al., 2014); namely we will parametrize the generative model (decoder) pθ(x|z, s) and the variational posterior (encoder) qφ(z|x, s) as (deep) neural networks which accept as inputs z, s and x, s respectively and produce the parameters of each distribution after a series of non-linear transformations. Both the model (θ) and variational (φ) parameters will be jointly optimized with the SGVB (Kingma & Welling, 2014) algorithm according to a lower bound on the log-likelihood. This parametrization will allow us to capture most of the salient information of x in our embedding z. Furthermore the distributed representation of a neural network would allow us to better resolve the dependencies between x and s thus yielding a better disentangling between the independent factors z and s. By choosing a Gaussian posterior qφ(z|x, s) and standard isotropic Gaussian prior p(z) = N (0, I) we can obtain the following lower bound:  Eqφ(zn|xn,sn)[log pθ(xn|zn, sn)] − KL(qφ(zn|xn, sn)||p(z))  (1)  n=1  n=1  = F(φ, θ; xn, sn)  with qφ(zn|xn, sn) = N (zn|µn = fφ(xn, sn), σn = efφ(xn,sn)) and pθ(xn|zn, sn) = fθ(zn, sn) with fθ(zn, sn) being an appropriate probability distribution for the data we are modelling.  2.2 SEMI-SUPERVISED MODEL  Factoring out variations in an unsupervised way can however be harmful in cases where we want to use this invariant representation for a subsequent prediction task. In particular if we have a situation where the nuisance variable s and the actual label y are correlated, then training an unsupervised model could yield random or degenerate representations with respect to y. Therefore it is more appropriate to try to “inject” the information about the label during the feature extraction phase. This can be quite simply achieved by introducing a second “layer” of latent variables to our generative model where we try to correlate z with the prediction task. Assuming that the invariant features are now called z1 we enrich the generative story by similarly providing two distinct (independent)  2  N(cid:88)  log p(xn|sn) ≥ N(cid:88)  xzsNxz1sz2yNPublished as a conference paper at ICLR 2016  sources for z1; a discrete (in case of classiﬁcation)variable y which denotes the label of the data point x and a continuous latent variable z2 which encodes the variation on z1 that is not explained by y (x dependent noise). The process now can be formally deﬁned as:  y, z2 ∼ Cat(y)p(z2);  z1 ∼ pθ(z1|z2, y);  x ∼ pθ(x|z1, s)  Similarly to the unsupervised case we use a variational auto-encoder and jointly optimize the varia- tional and model parameters. The lower bound now becomes:  Eqφ(z1n,z2 n,yn|xn,sn)[log p(z2) + log p(yn) + log pθ(z1n|z2n, yn)+  where we qφ(z1n, z2n, yn|xn, sn) = qφ(z1n|xn, sn)qφ(yn|z1n)qφ(z2n|z1n, yn), and where:  assume is qφ(z1n|xn, sn) = N (z1n|µn = fφ(xn, sn), σn = efφ(xn,sn))  that  the  + log pθ(xn|z1n, sn) − log qφ(z1n, z2n, yn|xn, sn)] factorized posterior  qφ(z1n, z2n, yn|xn, sn)  (2) as  N(cid:88)  log p(xn|sn) ≥ N(cid:88)  n=1  n=1  qφ(yn|z1n) = Cat(yn|πn = softmax(fφ(z1n)))  qφ(z2n|z1n, yn) = N (z2n|µn = fφ(z1n, yn), σn = efφ(z1n,yn)) pθ(z1n|z2n, yn) = N (z1n|µn = fθ(z2n, yn), σn = efθ(z2n,yn)) pθ(xn|z1n, sn) = fθ(z1n, sn)  with fθ(z1n, sn) again being an appropriate probability distribution for the data we are modelling. The model proposed here can be seen as an extension to the ‘stacked M1+M2’ model originally proposed from Kingma et al. (2014), where we have additionally introduced the nuisance variable s during the feature extraction. Thus following Kingma et al. (2014) we can also handle the ‘semi- supervised’ case, i.e., missing labels. In situations where the label is observed the lower bound takes the following form (exploiting the fact that we can compute some Kullback-Leibler divergences explicitly in our case):  N(cid:88)  Ns(cid:88)  Ls(φ, θ; xn, sn, yn) =  Eqφ(z1n|xn,sn)[−KL(qφ(z2n|z1n, yn)||p(z2)) + log pθ(xn|z1n, sn)]+  n=1  n=1  + Eqφ(z1n|xn,sn)qφ(z2 n|z1n,yn)[log pθ(z1|z2n, yn) − log qφ(z1n|xnsn)]  M(cid:88) and in the case that it is not observed we use q(yn|z1n) to ‘impute’ our data:  M(cid:88)  Lu(φ, θ; xm, sm) =  Eqφ(z1m|xm,sm)[−KL(q(ym|z1m)||p(ym)) + log pθ(xm|z1m, sm)]+  m=1  m=1  + Eqφ(z1m,ym|xm,sm)[−KL(qφ(z2m|z1m, ym)||p(z2))]+ + Eqφ(z1m,ym,z2m|xm,sm)[log pθ(z1m|z2m, ym) − log qφ(z1m|xm, sm)]  (3)  (4)  therefore the ﬁnal objective function is: FVAE(φ, θ; xn, xm, sn, sm, yn) =  N(cid:88)  n=1  M(cid:88)  m=1  Lu(φ, θ; xm, sm)+  Ls(φ, θ; xn, sn, yn) +  N(cid:88)  n=1  + α  Eq(z1n|xn,sn)[− log qφ(yn|z1n)]  (5) where the last term is introduced so as to ensure that the predictive posterior qφ(y|z1) learns from both labeled and unlabeled data. This semi-supervised model will be called “VAE” in our experi- ments. However, there is a subtle difference between the approach of Kingma et al. (2014) and our model. Instead of training separately each layer of stochastic variables we optimize the model jointly. The potential advantages of this approach are two fold: as we previously mentioned if the label y and the nuisance information s are correlated then training a (conditional) feature extractor separately poses the danger of creating a degenerate representation with respect to the label y. Furthermore the label information will also better guide the feature extraction towards the more salient parts of the data, thus maintaining most of the (predictive) information.  3  Published as a conference paper at ICLR 2016  2.3 FURTHER INVARIANCE VIA MAXIMUM MEAN DISCREPANCY  Despite the fact that we have a model that encourages statistical independence between s and z1 a-priori we might still have some dependence in the (approximate) marginal posterior qφ(z1|s). In particular, this can happen if the label y is correlated with the sensitive variable s, which can allow information about s to “leak” into the posterior. Thus instead we could maximize a “penalized” lower bound where we impose some sort of regularization on the marginal qφ(z1|s). In the following we will describe one way to achieve this regularization through the Maximum Mean Discrepancy (MMD) (Gretton et al., 2006) measure.  .  (6)  2.3.1 MAXIMUM MEAN DISCREPANCY Consider the problem of determining whether two datasets {X} ∼ P0 and {X(cid:48)} ∼ P1 are drawn from the same distribution, i.e., P0 = P1. A simple test is to consider the distance between empirical  N1(cid:88)  i=1  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2  ψ(xi) − 1 N1  ψ(x  (cid:48) i)  N0(cid:88)  i=1  statistics ψ(·) of the two datasets:(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 N0(cid:88)  N0(cid:88)  N0  (cid:96)MMD(X, X  (cid:48)) =  1 N 2 0  n=1  m=1  k(xn, xm) +  Expanding the square yields an estimator composed only of inner products on which the kernel trick can be applied. The resulting estimator is known as Maximum Mean Discrepancy (MMD) (Gretton et al., 2006):  N1(cid:88)  N1(cid:88)  n=1  m=1  1 N 2 1  k(x  (cid:48) n, x  m) − 2 (cid:48) N0N1  k(xn, x  (cid:48) m).  N0(cid:88)  N1(cid:88)  n=1  m=1  (7) Asymptotically, for a universal kernel such as the Gaussian kernel k(x, x(cid:48)) = e−γ(cid:107)x−x(cid:48)(cid:107)2, (cid:96)MMD(X, X(cid:48)) is 0 if and only if P0 = P1. Equivalently, minimizing MMD can be viewed as matching all of the moments of P0 and P1. Therefore, we can use it as an extra “regularizer” and force the model to try to match the moments between the marginal posterior distributions of our latent variables, i.e., qφ(z1|s = 0) and qφ(z1|s = 1) (in the case of binary nuisance information s1). By adding the MMD penalty into the lower bound of our aforementioned VAE architecture we obtain our proposed model, the “Variational Fair Autoencoder” (VFAE): FVFAE(φ, θ; xn, xm, sn, sm, yn) = FVAE(φ, θ; xn, xm, sn, sm, yn) − β(cid:96)MMD(Z1s=0, Z1s=1) (8) where: (cid:96)MMD(Z1s=0, Z1s=1) = (cid:107) E  ˜p(x|s=0)[Eq(z1|x,s=0)[ψ(z1)]] − E ˜p(x|s=1)[Eq(z1|x,s=1)[ψ(z1)]](cid:107)2 (9)  2.4 FAST MMD VIA RANDOM FOURIER FEATURES  A naive implementation of MMD in minibatch stochastic gradient descent would require computing the M ×M Gram matrix for each minibatch during training, where M is the minibatch size. Instead, we can use random kitchen sinks (Rahimi & Recht, 2009) to compute a feature expansion such that computing the estimator (6) approximates the full MMD (7). To compute this, we draw a random K × D matrix W, where K is the dimensionality of x, D is the number of random features and each entry of W is drawn from a standard isotropic Gaussian. The feature expansion is then given as:  (cid:114) 2  (cid:18)(cid:114) 2  γ  (cid:19)  ψW(x) =  cos  D  xW + b  .  (10)  where b is a D-dimensional uniform random vector with entries in [0, 2π]. Zhao & Meng (2015) have successfully applied the idea of using random kitchen sinks to approximate MMD. This esti- mator is fairly accurate, and is typically much faster than the full MMD penalty. We use D = 500 in our experiments.  between each marginal q(z|s = k) and q(z), i.e.,(cid:80)K  1In case that we have more than two states for the nuisance information s, we minimize the MMD penalty  k=1 (cid:96)MMD(Z1s=k, Z1) for all possible states K of s.  4  Published as a conference paper at ICLR 2016  3 EXPERIMENTS  We performed experiments on the three datasets that correspond to a “fair” classiﬁcation scenario and were previously used by Zemel et al. (2013). In these datasets the “nuisance” or sensitive vari- able s is signiﬁcantly correlated with the label y thus making the proper removal of s challenging. Furthermore, we also experimented with the Amazon reviews dataset to make a connection with the “domain-adaptation” literature. Finally, we also experimented with a more general task on the extended Yale B dataset; that of learning invariant representations.  3.1 DATASETS  For the fairness task we experimented with three datasets that were previously used by Zemel et al. (2013). The German dataset is the smallest one with 1000 data points and the objective is to predict whether a person has a good or bad credit rating. The sensitive variable is the gender of the individ- ual. The Adult income dataset contains 45, 222 entries and describes whether an account holder has over $50, 000 dollars in their account. The sensitive variable is age. Both of these are obtained from the UCI machine learning repository (Frank & Asuncion, 2010). The health dataset is derived from the Heritage Health Prize2. It is the largest of the three datasets with 147, 473 entries. The task is to predict whether a patient will spend any days in the hospital in the next year and the sensitive vari- able is the age of the individual. We use the same train/test/validation splits as Zemel et al. (2013) for our experiments. Finally we also binarized the data and used a multivariate Bernoulli distribution for pθ(xn|z1n, sn) = Bern(xn|πn = σ(fθ(z1n, sn))), where σ(·) is the sigmoid function 3. For the domain adaptation task we used the Amazon reviews dataset (with similar preprocessing) that was also employed by Chen et al. (2012) and Ganin et al. (2015). It is composed from text reviews about particular products, where each product belongs to one out of four different domains: “books”, “dvd”, “electronics” and “kitchen”. As a result we performed twelve domain adaptation tasks. The labels y correspond to the sentiment of each review, i.e. either positive or negative. Since each feature vector x is composed from counts of unigrams and bigrams we used a Poisson distribution for pθ(xn|z1n, sn) = Poisson(xn|λn = efθ(z1n,sn)). It is also worthwhile to mention that we can fully exploit the semi-supervised nature of our model in this dataset, and thus for training we only use the source domain labels and consider the labels of the target domain as “missing”. For the general task of learning invariant representations we used the Extended Yale B dataset, which was also employed in a similar fashion by Li et al. (2014). It is composed from face images of 38 people under different lighting conditions (directions of the light source). Similarly to Li et al. (2014), we created 5 states for the nuisance variable s: light source in upper right, lower right, lower left, upper left and the front. The labels y correspond to the identity of the person. Following Li et al. (2014), we used the same training, test set and no validation set. For the p(xn|z1n, sn) distribution we used a Gaussian with means constrained in the 0-1 range (since we have intensity images) by a sigmoid, i.e. pθ(xn|z1n, sn) = N (xn|µn = σ(fθ(z1n, sn)), σn = efθ(z1 n,sn)).  3.2 EXPERIMENTAL SETUP  For the Adult dataset both encoders, for z1 and z2, and both decoders, for z1 and x, had one hidden layer of 100 units. For the Health dataset we had one hidden layer of 300 units for the z1 encoder and x decoder and one hidden layer of 150 units for the z2 encoder and z1 decoder. For the much smaller German dataset we used 60 hidden units for both encoders and decoders. Finally, for the Amazon reviews and Extended Yale B datasets we had one hidden layer with 500, 400 units respectively for the z1 encoder, x decoder, and 300, 100 units respectively for the z2 encoder and z1 decoder. On all of the datasets we used 50 latent dimensions for z1 and z2, except for the small German dataset, where we used 30 latent dimensions for both variables. For the predictive posterior qφ(y|z1) we used a simple Logistic regression classiﬁer. Optimization of the objective function was done with Adam (Kingma & Ba, 2015) using the default values for the hyperparameters, minibatches of 100 data points and temporal averaging. The MMD penalty was simply multiplied by the minibatch size so as to keep the scale of the penalty similar to the lower bound. Furthermore, the extra strength  2www.heritagehealthprize.com 3σ(t) = 1  1+e−t  5  Published as a conference paper at ICLR 2016  Nbatch source  of the MMD, β, was tuned according to a validation set. The scaling of the supervised cost was low (α = 1) for the Adult, Health and German datasets due to the correlation of s with y. On the Amazon reviews and Extended Yale B datasets however the scaling of the supervised cost was higher: α = 100 · Nbatch source+Nbatch target for the Amazon reviews dataset (empirically determined after observing the classiﬁcation loss on the ﬁrst few iterations on the ﬁrst source-target pair) and α = 200 for the Extended Yale B dataset. Similarly, the scaling of the MMD penalty was β = 100· Nbatch for the Amazon reviews dataset and β = 200 · Nbatch for the Extended Yale B. Our evaluation is geared towards two fronts; removing information about s and classiﬁcation accu- racy for y. To measure the information about s in our new representation we simply train a classiﬁer to predict s from z1. We utilize both Logistic Regression (LR) which is a simple linear classiﬁer, and Random Forest (RF) which is a powerful non-linear classiﬁer. Since on the datasets that we exper- imented with the nuisance variable s is binary we can easily ﬁnd the random chance accuracy for s and measure the discriminatory information of s in z1. Furthermore, we also used the discrimination metric from Zemel et al. (2013) as well a more “informed” version of the discrimination metric that instead of the predictions, takes into account the probabilities of the correct class. They are provided in the appendix A. Finally, for the classiﬁcation performance on y we used the predictive posterior qφ(y|z1) for the VAE/VFAE and a simple Logistic Regression for the original representations x. It should be noted that for the VFAE and VAE models we use a sample from qφ(z1|x, s) to make predictions, instead of using the mean. We found that the extra noise helps with invariance. We implemented the Learning Fair Representations (Zemel et al., 2013) method (LFR) as a baseline using K = 50 dimensions for the latent space. To measure the accuracy on y in the results below we similarly used the LFR model predictions.  3.3 RESULTS  3.3.1 FAIR CLASSIFICATION  The results for all three datasets can be seen in Figure 3. Since we are dealing with the “fair” classiﬁcation scenario here, low accuracy and discrimination against s is more important than the accuracy on y (as long as we do not produce degenerate representations). On the Adult dataset, the highest accuracy on the label y and the lowest discrimination against s is obtained by our LFR baseline. Despite the fact that LFR appears to give the best tradeoff between accuracy and discrimination, it appears to retain information about s in its representation, which is discovered from the random forest classiﬁer. In that sense, the VFAE method appears to do the best job in actually removing the sensitive information and maintaining most of the predictive infor- mation. Furthermore, the introduction of the MMD penalty in the VFAE model seems to provide a signiﬁcant beneﬁt with respect to our discrimination metrics, as both were reduced considerably compared to the regular VAE. On the German dataset, all methods appear to be invariant with respect to the sensitive information s. However this is not the case for the discrimination metric, since LFR does appear to retain information compared to the VAE and VFAE. The MMD penalty in VFAE did seem improve the discrimination scores over the original VAE, while the accuracy on the labels y remained similar. As for the Health dataset; this dataset is extremely imbalanced, with only 15% of the patients being admitted to a hospital. Therefore, each of the classiﬁers seems to predict the majority class as the label y for every point. For the invariance against s however, the results were more interesting. On the one hand, the VAE model on this dataset did maintain some sensitive information, which could be identiﬁed both linearly and non-linearly. On the other hand, VFAE and the LFR methods were able to retain less information in their latent representation, since only Random Forest was able to achieve higher than random chance accuracy. This further justiﬁes our choice for including the MMD penalty in the lower bound of the VAE. . In order to further assess the nature of our new representations, we visualized two dimensional Barnes-Hut SNE (van der Maaten, 2013) embeddings of the z1 representations, obtained from the model trained on the Adult dataset, in Figure 4. As we can see, the nuisance/sensitive variables s can be identiﬁed both on the original representation x and on a latent representation z1 that does not have the MMD penalty and the independence properties between z1 and s in the prior. By  6  Published as a conference paper at ICLR 2016  (a) Adult dataset  (b) German dataset  (c) Health dataset  Figure 3: Fair classiﬁcation results. Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y. Note that the objective of a “fair” encoding is to have low accuracy on S (where LR is a linear classiﬁer and RF is nonlinear), low discrimination against S and high accuracy on Y.  introducing these independence properties as well as the MMD penalty the nuisance variable groups become practically indistinguishable.  (a)  (b)  (c)  (d)  Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent z1 without s and MMD, (c): latent z1 with s and without MMD, (d): latent z1 with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females.  3.3.2 DOMAIN ADAPTATION  As for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in Table 1. Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance (which for this dataset is 0.5). We should also mention that, on this dataset at least, completely removing information about the domain does not guarantee a better performance on y. The same effect was also observed by Ganin et al. (2015) and Chen et al. (2012). As far as the accuracy on y  7  xLFRVAEVFAE0.650.700.750.800.85Random chance SRandom Forest (RF)Logistic Regression (LR)xLFRVAEVFAE0.000.020.040.060.080.100.120.140.160.18Discrimination SDiscrimination prob. SxLFRVAEVFAE0.720.740.760.780.800.820.84Random chance YModel accuracy YxLFRVAEVFAE0.780.800.820.840.86Random chance SRandom Forest (RF)Logistic Regression (LR)xLFRVAEVFAE0.000.020.040.060.080.100.120.14Discrimination SDiscrimination prob. SxLFRVAEVFAE0.660.680.700.720.740.76Random chance YModel accuracy YxLFRVAEVFAE0.550.600.650.700.750.80Random chance SRandom Forest (RF)Logistic Regression (LR)xLFRVAEVFAE0.000.010.020.030.040.05Discrimination SDiscrimination prob. SxLFRVAEVFAE0.800.820.840.860.88Random chance YModel accuracy Y80604020020406080806040200204060151050510153020100102030151050510153020100102030151050510153020100102030Published as a conference paper at ICLR 2016  is concerned, we compared against a recent neural network based state of the art method for domain adaptation, Domain Adversarial Neural Network (DANN) (Ganin et al., 2015). As we can observe in table 1, our accuracy on the labels y is higher on 9 out of the 12 domain adaptation tasks whereas on the remaining 3 it is quite similar to the DANN architecture.  Table 1: Results on the Amazon reviews dataset. The DANN column is taken directly from Ganin et al. (2015) (the column that uses the original representation as input).  Source - Target books - dvd books - electronics books - kitchen dvd - books dvd - electronics dvd - kitchen electronics - books electronics - dvd electronics - kitchen kitchen - books kitchen - dvd kitchen - electronics  S  RF 0.535 0.541 0.537 0.537 0.538 0.543 0.562 0.556 0.536 0.560 0.561 0.533  LR 0.564 0.562 0.583 0.563 0.566 0.589 0.590 0.586 0.570 0.593 0.599 0.565  Y  VFAE DANN 0.799 0.784 0.792 0.733 0.816 0.779 0.755 0.723 0.786 0.754 0.822 0.783 0.727 0.713 0.765 0.738 0.854 0.850 0.720 0.709 0.740 0.733 0.843 0.838  3.4 LEARNING INVARIANT REPRESENTATIONS  Regarding the more general task of learning invariant representations; our results on the Extended Yale B dataset also demonstrate our model’s ability to learn such representations. As expected, on the original representation x the lighting conditions, s, are well identiﬁable with almost perfect accuracy from both RF and LR. This can also be seen in the two dimensional embeddings of the original space x in Figure 5a: the images are mostly clustered according to the lighting conditions. As soon as we utilize our VFAE model we simultaneously decrease the accuracy on s, from 96% to about 50%, and increase our accuracy on y, from 78% to about 85%. This effect can also be seen in Figure 5b: the images are now mostly clustered according to the person ID (the label y). It is clear that in this scenario the information about s is purely “nuisance” with respect to the labels y. Therefore, by using our VFAE model we are able to obtain improved generalization and classiﬁcation performance by effectively removing s from our representations.  Table 2: Results on the Extended Yale B dataset. We also included the best result from Li et al. (2014) under the NN + MMD row.  Method Original x NN + MMD - VFAE  RF 0.952  0.435  S  LR 0.961 - 0.565  Y 0.78 0.82 0.846  4 RELATED WORK  Most related to our “fair” representations view is the work from Zemel et al. (2013). They proposed a neural network based semi-supervised clustering model for learning fair representations. The idea is to learn a localised representation that maps each datapoint to a cluster in such a way that each cluster gets assigned roughly equal proportions of data from each group in s. Although their approach was successfully applied on several datasets, the restriction to clustering means that it cannot leverage the representational power of a distributed representation. Furthermore, this penalty does not account for higher order moments in the latent distribution. For example, if p(zk = 1|xi, s = 0) always  8  Published as a conference paper at ICLR 2016  (a)  (b)  Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent z1 from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.  returns 1 or 0, while p(zk = 1|xi, s = 1) returns values between values 0 and 1, then the penalty could still be satisﬁed, but information could still leak through. We addressed both of these issues in this paper. Domain adaptation can also be cast as learning representations that are “invariant” with respect to a discrete variable s, the domain. Most similar to our work are neural network approaches which try to match the feature distributions between the domains. This was performed in an unsupervised way with mSDA (Chen et al., 2012) by training denoising autoencoders jointly on all domains, thus implicitly obtaining a representation general enough to explain both the domain and the data. This is in contrast to our approach where we instead try to learn representations that explicitly remove domain information during the learning process. For the latter we ﬁnd more similarities with “domain-regularized” supervised approaches that simultaneously try to predict the label for a data point and remove domain speciﬁc information. This is done with either MMD (Long & Wang, 2015; Tzeng et al., 2014) or adversarial (Ganin et al., 2015) penalties at the hidden layers of the network. In our model however the main “domain-regularizer” stems from the independence properties of the prior over the domain and latent representations. We also employ MMD on our model but from a different perspective since we consider a slightly more difﬁcult case where the domain s and label y are correlated; we need to ensure that we remain as “invariant” as possible since qφ(y|z1) might ‘leak’ information about s.  5 CONCLUSION  We introduce the Variational Fair Autoencoder (VFAE), an extension of the semi-supervised varia- tional autoencoder in order to learn representations that are explicitly invariant with respect to some known aspect of a dataset while retaining as much remaining information as possible. We further use a Maximum Mean Discrepancy regularizer in order to further promote invariance in the posterior distribution over latent variables. We apply this model to tasks involving developing fair classiﬁers that are invariant to sensitive demographic information and show that it produces a better tradeoff with respect to accuracy and invariance. As a second application, we consider the task of domain adaptation, where the goal is to improve classiﬁcation by training a classiﬁer that is invariant to the domain. We ﬁnd that our model is competitive with recently proposed adversarial approaches. Fi- nally, we also consider the more general task of learning invariant representations. We can observe that our model provides a clear improvement against a neural network that incorporates a Maximum Mean Discrepancy penalty.  9  012345678910111213141516171819202122232425262728293031323334353637012345678910111213141516171819202122232425262728293031323334353637012345678910111213141516171819202122232425262728293031323334353637012345678910111213141516171819202122232425262728293031323334353637012345678910111213141516171819202122232425262728293031323334353637012345678910111213141516171819202122232425262728293031323334353637012345678910111213141516171819202122232425262728293031323334353637012345678910111213141516171819202122232425262728293031323334353637012345678910111213141516171819202122232425262728293031323334353637012345678910111213141516171819202122232425262728293031323334353637Published as a conference paper at ICLR 2016  REFERENCES Ben-David, Shai, Blitzer, John, Crammer, Koby, Pereira, Fernando, et al. Analysis of representations  for domain adaptation. Advances in neural information processing systems, 19:137, 2007.  Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira, Fernando, and Vaughan, Jennifer Wortman. A theory of learning from different domains. Machine learning, 79(1-2): 151–175, 2010.  Chen, Minmin, Xu, Zhixiang, Weinberger, Kilian, and Sha, Fei. Marginalized denoising autoen-  coders for domain adaptation. International Conference on Machine Learning (ICML), 2012.  Frank, A. and Asuncion, A. UCI machine learning repository, 2010. URL http://archive.  ics.uci.edu/ml.  Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and  Lempitsky, V. Domain-Adversarial Training of Neural Networks. ArXiv e-prints, May 2015.  Gretton, Arthur, Borgwardt, Karsten M, Rasch, Malte, Sch¨olkopf, Bernhard, and Smola, Alex J. In Advances in neural information processing  A kernel method for the two-sample-problem. systems, pp. 513–520, 2006.  Kifer, Daniel, Ben-David, Shai, and Gehrke, Johannes. Detecting change in data streams. In Pro- ceedings of the Thirtieth international conference on Very large data bases-Volume 30, pp. 180– 191. VLDB Endowment, 2004.  Kingma, Diederik P and Ba, Jimmy. Adam: A method for stochastic optimization. International  Conference on Learning Representations (ICLR), 2015.  Kingma, Diederik P and Welling, Max. Auto-encoding variational bayes. International Conference  on Learning Representations (ICLR), 2014.  Kingma, Diederik P, Mohamed, Shakir, Rezende, Danilo Jimenez, and Welling, Max. Semi- supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581–3589, 2014.  Li, Yujia, Swersky, Kevin, and Zemel, Richard. Learning unbiased features. arXiv preprint  arXiv:1412.5244, 2014.  Long, Mingsheng and Wang, Jianmin. Learning transferable features with deep adaptation networks.  arXiv preprint arXiv:1502.02791, 2015.  Rahimi, Ali and Recht, Benjamin. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in neural information processing systems, pp. 1313– 1320, 2009.  Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and approximate inference in deep generative models. International Conference on Machine Learning (ICML), 2014.  Tzeng, Eric, Hoffman, Judy, Zhang, Ning, Saenko, Kate, and Darrell, Trevor. Deep domain confusion: Maximizing for domain invariance. CoRR, abs/1412.3474, 2014. URL http: //arxiv.org/abs/1412.3474.  van der Maaten, L. Barnes-Hut-SNE. ArXiv e-prints, January 2013.  Zemel, Rich, Wu, Yu, Swersky, Kevin, Pitassi, Toni, and Dwork, Cynthia. Learning fair representa- tions. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 325–333, 2013.  Zhao, Ji and Meng, Deyu. Fastmmd: Ensemble of circular discrepancy for efﬁcient two-sample test.  Neural computation, 2015.  10  Published as a conference paper at ICLR 2016  A DISCRIMINATION METRICS  The Discrimination metric (Zemel et al., 2013) and the Discrimination metric that takes into account the probabilities of the correct class are mathematically formalized as:  Discrimination =  Discrimination prob. =  (cid:12)(cid:12)(cid:12)(cid:12)(cid:80)N (cid:12)(cid:12)(cid:12)(cid:12)(cid:80)N  (cid:80)N (cid:80)N  I[ys=0 n ]  n=1  Ns=0 n=1 p(ys=0 n )  −  −  Ns=0  I[ys=1 n ]  n=1  Ns=1 n=1 p(ys=1 n )  Ns=1  (cid:12)(cid:12)(cid:12)(cid:12)  (cid:12)(cid:12)(cid:12)(cid:12)  where I[ys=0 n ] = 1 for the predictions yn that were done on the datapoints with nuisance variable s = 0, Ns=0 denotes the total amount of datapoints that had nuisance variable s = 0 and p(ys=0 n ) denotes the probability of the prediction yn for the datapoints with s = 0. For the predictions and their respective probabilities we used a Logistic Regression classiﬁer.  B PROXY A-DISTANCE (PAD) FOR AMAZON REVIEWS DATASET  Similarly to Ganin et al. (2015), we also calculated the Proxy A-distance (PAD) (Ben-David et al., 2007; 2010) scores for the raw data x and for the z1 representations of VFAE. Brieﬂy, Proxy A- distance is an approximation to the H-divergence measure of domain distinguishability proposed in Kifer et al. (2004) and Ben-David et al. (2007; 2010). To compute it we ﬁrst need to train a learning algorithm on the task of discriminating examples from the source and target domain. Afterwards we can use the test error (cid:15) of that algorithm in the following formula:  PAD((cid:15)) = 2(1 − 2(cid:15))  It is clear that low PAD scores correspond to low discrimination of the source and target domain examples from the classiﬁer. To obtain (cid:15) for our model we used Logistic Regression. The resulting plot can be seen in Figure 6, where we have also added the plot from DANN (Ganin et al., 2015), where they used a linear Support Vector Machine for the classiﬁer, as a reference. It can be seen that our VFAE model can factor out the information about s better, since the PAD scores on our new representation are, overall, lower than the ones obtained from the DANN architecture.  Figure 6: Proxy A-distances (PAD) for the Amazon reviews dataset: left from our VFAE model, right from the DANN model (taken from Ganin et al. (2015))  11  0.00.51.01.52.0PAD on raw input0.00.51.01.52.0PAD on z1 from VFAEB→DB→EB→KD→BD→ED→KE→BE→DE→KK→BK→DK→E0.00.51.01.52.0PADonrawinput0.00.51.01.52.0PADDANNrepresentationsB→DB→EB→KD→ED→KE→KK→EK→DK→BE→DD→B",
1511.01844,2016,A note on the evaluation of generative models,"['A note on the evaluation of generative models\nLucas Theis', 'Aäron van den Oord', 'Matthias Bethge']",https://arxiv.org/pdf/1511.01844,"6 1 0 2    r p A 4 2         ] L M  . t a t s [      3 v 4 4 8 1 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  A NOTE ON THE EVALUATION OF GENERATIVE MODELS  Lucas Theis∗ University of T¨ubingen 72072 T¨ubingen, Germany lucas@bethgelab.org  Matthias Bethge University of T¨ubingen 72072 T¨ubingen, Germany matthias@bethgelab.org  A¨aron van den Oord∗† Ghent University 9000 Ghent, Belgium aaron.vandenoord@ugent.be  ABSTRACT  Probabilistic generative models can be used for compression, denoising, inpaint- ing, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often dif- ﬁcult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most com- monly used criteria—average log-likelihood, Parzen window estimates, and vi- sual ﬁdelity of samples—are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.  1  INTRODUCTION  Generative models have many applications and can be evaluated in many ways. For density esti- mation and related tasks, log-likelihood (or equivalently Kullback-Leibler divergence) has been the de-facto standard for training and evaluating generative models. However, the likelihood of many interesting models is computationally intractable. For example, the normalization constant of un- normalized energy-based models is generally difﬁcult to compute, and latent-variable models often require us to solve complex integrals to compute the likelihood. These models may still be trained with respect to a different objective that is more or less related to log-likelihood, such as contrastive divergence (Hinton, 2002), score matching (Hyv¨arinen, 2005), lower bounds on the log-likelihood (Bishop, 2006), noise-contrastive estimation (Gutmann & Hyv¨arinen, 2010), probability ﬂow (Sohl- Dickstein et al., 2011), maximum mean discrepancy (MMD) (Gretton et al., 2007; Li et al., 2015), or approximations to the Jensen-Shannon divergence (JSD) (Goodfellow et al., 2014). For computational reasons, generative models are also often compared in terms of properties more readily accessible than likelihood, even when the task is density estimation. Examples include vi- sualizations of model samples, interpretations of model parameters (Hyv¨arinen et al., 2009), Parzen window estimates of the model’s log-likelihood (Breuleux et al., 2009), and evaluations of model performance in surrogate tasks such as denoising or missing value imputation. In this paper, we look at some of the implications of choosing certain training and evaluation criteria. We ﬁrst show that training objectives such as JSD and MMD can result in very different optima than  ∗These authors contributed equally to this work. †Now at Google DeepMind.  1  Published as a conference paper at ICLR 2016  Data  KLD  MMD  JSD  Figure 1: An isotropic Gaussian distribution was ﬁt to data drawn from a mixture of Gaussians by either minimizing Kullback-Leibler divergence (KLD), maximum mean discrepancy (MMD), or Jensen-Shannon divergence (JSD). The different ﬁts demonstrate different tradeoffs made by the three measures of distance between distributions.  log-likelihood. We then discuss the relationship between log-likelihood, classiﬁcation performance, visual ﬁdelity of samples and Parzen window estimates. We show that good or bad performance with respect to one metric is no guarantee of good or bad performance with respect to the other metrics. In particular, we show that the quality of samples is generally uninformative about the likelihood and vice versa, and that Parzen window estimates seem to favor models with neither good likelihood nor samples of highest possible quality. Using Parzen window estimates as a criterion, a simple model based on k-means outperforms the true distribution of the data.  2 TRAINING OF GENERATIVE MODELS  Many objective functions and training procedures have been proposed for optimizing generative models. The motivation for introducing new training methods is typically the wish to ﬁt probabilistic models with computationally intractable likelihoods, rendering direct maximum likelihood learning impractical. Most of the available training procedures are consistent in the sense that if the data is drawn from a model distribution, then this model distribution will be optimal under the training objective in the limit of an inﬁnite number of training examples. That is, if the model is correct, and for extremely large amounts of data, all of these methods will produce the same result. However, when there is a mismatch between the data distribution and the model, different objective functions can lead to very different results. Figure 1 illustrates this on a simple toy example where an isotropic Gaussian distribution has been ﬁt to a mixture of Gaussians by minimizing various measures of distance. Maximum mean discrepancy (MMD) has been used with generative moment matching networks (Li et al., 2015; Dziugaite et al., 2015) and Jensen-Shannon divergence (JSD) has connections to the objective function optimized by generative adversarial networks (Goodfellow et al., 2014) (see box for a deﬁnition). Minimizing MMD or JSD yields a Gaussian which ﬁts one mode well, but which ignores other parts of the data. On the other hand, maximizing average log-likelihood or equivalently minimizing Kullback-Leibler divergence (KLD) avoids assigning extremely small probability to any data point but assigns a lot of probability mass to non-data regions. Understanding the trade-offs between different measures is important for several reasons. First, different applications require different trade-offs, and we want to choose the right metric for a given application. Assigning sufﬁcient probability to all plausible images is important for compression, but it may be enough to generate a single plausible example in certain image reconstruction applications (e.g., Hays & Efros, 2007). Second, a better understanding of the trade-offs allows us to better interpret and relate empirical ﬁndings. Generative image models are often assessed based on the visual ﬁdelity of generated samples (e.g., Goodfellow et al., 2014; Gregor et al., 2015; Denton et al., 2015; Li et al., 2015). Figure 1 suggests that a model optimized with respect to KLD is more likely to produce atypical samples than the same model optimized with respect to one of the other two measures. That is, plausible samples—in the sense of having large density under the target  2  Published as a conference paper at ICLR 2016  MMD (Gretton et al., 2007) is deﬁned as,  MMD[p, q] = (Ep,q[k(x, x(cid:48)) − 2k(x, y) + k(y, y(cid:48))])  (1) where x, x(cid:48) are indepent and distributed according to the data distribution p, and y, y(cid:48) are independently distributed according to the model distribution q. We followed the approach of Li et al. (2015), optimizing an empirical estimate of MMD and using a mixture of Gaussian kernels with various bandwidths for k.  2 ,  1  JSD is deﬁned as  JSD[p, q] =  KLD[p || m] +  1 2  KLD[q || m],  1 2  (2)  where m = (p+q)/2 is an equal mixture of distributions p and q. We optimized JSD directly using the data density, which is generally not possible in practice where we only have access to samples from the data distribution. In this case, generative adversarial networks (GANs) may be used to approximately optimize JSD, although in practical applications the objective function optimized by GANs can be very different from JSD. Parameters were initialized at the maximum likelihood solution in all cases, but the same optimum was consistently found using random initializations.  distribution—are not necessarily an indication of a good density model as measured by KLD, but may be expected when optimizing JSD.  3 EVALUATION OF GENERATIVE MODELS  Just as choosing the right training method is important for achieving good performance in a given application, so is choosing the right evaluation metric for drawing the right conclusions. In the following, we ﬁrst continue to discuss the relationship between average log-likelihood and the visual appearance of model samples. Model samples can be a useful diagnostic tool, often allowing us to build an intuition for why a model might fail and how it could be improved. However, qualitative as well as quantitative analyses based on model samples can be misleading about a model’s density estimation performance, as well as the probabilistic model’s performance in applications other than image synthesis. Below we summarize a few examples demonstrating this.  3.1 LOG-LIKELIHOOD  Average log-likelihood is widely considered as the default measure for quantifying generative image modeling performance. However, care needs to be taken to ensure that the numbers measured are meaningful. While natural images are typically stored using 8-bit integers, they are often modeled using densities, i.e., an image is treated as an instance of a continuous random variable. Since the discrete data distribution has differential entropy of negative inﬁnity, this can lead to arbitrary high likelihoods even on test data. To avoid this case, it is becoming best practice to add real-valued noise to the integer pixel values to dequantize the data (e.g., Uria et al., 2013; van den Oord & Schrauwen, 2014; Theis & Bethge, 2015). If we add the right amount of uniform noise, the log-likelihood of the continuous model on the dequantized data is closely related to the log-likelihood of a discrete model on the discrete data. Maximizing the log-likelihood on the continuous data also optimizes the log-likelihood of the dis- crete model on the original data. This can be seen as follows. Consider images x ∈ {0, ..., 255}D with a discrete probability distribution P (x), uniform noise u ∈ [0, 1[D, and noisy data y = x + u. If p refers to the noisy data density and q refers to the model density, then we have for the average log-likelihood:  3  Published as a conference paper at ICLR 2016  (cid:90)  p(y) log q(y) dy =  (cid:90)  P (x)  (cid:90)  [0,1[D  log q(x + u) du  q(x + u) du  P (x) log  [0,1[D  P (x) log Q(x),  x  (cid:88) ≤(cid:88) (cid:88) (cid:90)  =  x  x  (3)  (4)  (5)  (6)  where the second step follows from Jensen’s inequality and we have deﬁned  Q(x) =  q(x + u) du  [0,1[D  for x ∈ ZD. The left-hand side in Equation 3 is the expected log-likelihood which would be es- timated in a typical benchmark. The right-hand side is the log-likelihood of the probability mass function Q on the original discrete-valued image data. The negative of this log-likelihood is equiv- alent to the average number of bits (assuming base-2 logarithm) required to losslessly compress the discrete data with an entropy coding scheme optimized for Q (Shannon, 2001).  SEMI-SUPERVISED LEARNING  (cid:90)  A second motivation for using log-likelihood comes from semi-supervised learning. Consider a dataset consisting of images X and corresponding labels Y for some but not necessarily all of the images. In classiﬁcation, we are interested in the prediction of a class label y for a previously unseen query image x. For a given model relating x, y, and parameters θ, the only correct way to infer the distribution over y—from a Bayesian point of view —is to integrate out the parameters (e.g., Lasserre et al., 2006),  p(y | x,X ,Y) =  p(θ | X ,Y)p(y | x, θ) dθ.  (7) With sufﬁcient data and under certain assumptions, the above integral will be close to p(y | x, ˆθMAP), where  ˆθMAP = argmaxθ p(θ | X ,Y)  = argmaxθ [log p(θ) + log p(X | θ) + log p(Y | X , θ)] .  (8) (9) When no training labels are given, i.e., in the unsupervised setting, and for a uniform prior over parameters, it is therefore natural to try to optimize the log-likelihood, log p(X | θ). In practice, this approach might fail because of a mismatch between the model and the data, because of an inability to solve Equation 9, or because of overﬁtting induced by the MAP approximation. These issues can be addressed by better image models (e.g., Kingma et al., 2014), better optimization and inference procedures, or a more Bayesian treatment of the parameters (e.g., Lacoste-Julien et al., 2011; Welling & Teh, 2011).  3.2 SAMPLES AND LOG-LIKELIHOOD  For many interesting models, average log-likelihood is difﬁcult to evaluate or even approximate. For some of these models at least, generating samples is a lot easier. It would therefore be useful if we could use generated samples to infer something about a model’s log-likelihood. This approach is also intuitive given that a model with zero KL divergence will produce perfect samples, and visual inspection can work well in low dimensions for assessing a model’s ﬁt to data. Unfortunately these intuitions can be misleading when the image dimensionality is high. A model can have poor log- likelihood and produce great samples, or have great log-likelihood and produce poor samples.  POOR LOG-LIKELIHOOD AND GREAT SAMPLES  A simple lookup table storing enough training images will generate convincing looking images but will have poor average log-likelihood on unseen test data. Somewhat more generally we might  4  Published as a conference paper at ICLR 2016  consider a mixture of Gaussian distributions, 1 N  q(x) =  (cid:88)  n  N (x; xn, ε2I),  (10)  where the means xn are either training images or a number of plausible images derived from the training set (e.g., using a set of image transformations). If ε is small enough such that the Gaussian noise becomes imperceptible, this model will generate great samples but will still have very poor log-likelihood. This shows that plausible samples are clearly not sufﬁcient for a good log-likelihood. Gerhard et al. (2013) empirically found a correlation between some models’ log-likelihoods and their samples’ ability to fool human observers into thinking they were extracted from real images. However, the image patches were small and all models used in the study were optimized to mini- mize KLD. The correlation between log-likelihood and sample quality may disappear, for example, when considering models optimized for different objective functions or already when considering a different set of models.  GREAT LOG-LIKELIHOOD AND POOR SAMPLES  Perhaps surprisingly, the ability to produce plausible samples is not only not sufﬁcient, but also not necessary for high likelihood as a simple argument by van den Oord & Dambre (2015) shows: Assume p is the density of a model for d dimensional data x which performs arbitrarily well with respect to average log-likelihood and q corresponds to some bad model (e.g., white noise). Then samples generated by the mixture model  0.01p(x) + 0.99q(x)  (11)  will come from the poor model 99% of the time. Yet the log-likelihood per pixel will hardly change if d is large:  log [0.01p(x) + 0.99q(x)] ≥ log [0.01p(x)] = log p(x) − log 100  (12)  For high-dimensional data, log p(x) will be proportional to d while log 100 stays constant. For instance, already for the 32 by 32 images found in the CIFAR-10 dataset the difference between log-likelihoods of different models can be in the thousands, while log(100) is only about 4.61 nats (van den Oord & Dambre, 2015). This shows that a model can have large average log-likelihood but generate very poor samples.  GOOD LOG-LIKELIHOOD AND GREAT SAMPLES  Note that we could have also chosen q (Equation 11) such that it reproduces training examples, e.g., by choosing q as in Equation 10. In this case, the mixture model would generate samples indistin- guishable from real images 99% of the time while the log-likelihood would again only change by at most 4.61 nats. This shows that any model can be turned into a model which produces realistic samples at little expense to its log-likelihood. Log-likelihood and visual appearance of samples are therefore largely independent.  3.3 SAMPLES AND APPLICATIONS  One might conclude that something must be wrong with log-likelihood if it does not care about a model’s ability to generate plausible samples. However, note that the mixture model in Equation 11 might also still work very well in applications. While q is much more likely a priori, p is going to be much more likely a posteriori in tasks like inpainting, denoising, or classiﬁcation. Consider prediction of a quantity y representing, for example, a class label or missing pixels. A model with joint distribution  0.01p(x)p(y | x) + 0.99q(x)q(y | x)  (13)  may again generate poor samples 99% of the time. For a given ﬁxed x, the posterior over y will be a mixture  αp(y | x) + (1 − α)q(y | x),  (14)  5  Published as a conference paper at ICLR 2016  A  e c n a t s d  i  n a e d  i l  c u E  6,000  4,000  2,000  6,000  4,000  2,000  0  0  2  1 3 Shift [pixels]  4  0  0  2  1 3 Shift [pixels]  4  B  ]  %  [  i  i  n o s c e r P  100  80  60  40  20  0  0  2  1 3 Shift [pixels]  4  Figure 2: A: Two examples demonstrating that small changes of an image can lead to large changes in Euclidean distance affecting the choice of nearest neighbor. The images shown represent the query image shifted by between 1 and 4 pixels (left column, top to bottom), and the corresponding nearest neighbor from the training set (right column). The gray lines indicate Euclidean distance of the query image to 100 randomly picked images from the training set. B: Fraction of query images assigned to the correct training image. The average was estimated from 1,000 images. Dashed lines indicate a 90% conﬁdence interval.  where a few simple calculations show that  (15) and σ is the sigmoidal logistic function. Since we assume that p is a good model, q is a poor model, and x is high-dimensional, we have  α = σ (ln p(x) − ln q(x) − ln 99)  (16) and therefore α ≈ 1. That is, mixing with q has hardly changed the posterior over y. While the samples are dominated by q, the classiﬁcation performance is dominated by p. This shows that high visual ﬁdelity of samples is generally not necessary for achieving good performance in applications.  ln p(x) (cid:29) ln q(x) + ln 99  3.4 EVALUATION BASED ON SAMPLES AND NEAREST NEIGHBORS  A qualitative assessment based on samples can be biased towards models which overﬁt (Breuleux et al., 2009). To detect overﬁtting to the training data, it is common to show samples next to nearest neighbors from the training set. In the following, we highlight two limitations of this approach and argue that it is unﬁt to detect any but the starkest forms of overﬁtting. Nearest neighbors are typically determined based on Euclidean distance. But already perceptually small changes can lead to large changes in Euclidean distance, as is well known in the psychophysics literature (e.g., Wang & Bovik, 2009). To illustrate this property, we used the top-left 28 by 28 pixels of each image from the 50,000 training images of the CIFAR-10 dataset. We then shifted this 28 by 28 window one pixel down and one pixel to the right and extracted another set of images. We repeated this 4 times, giving us 4 sets of images which are increasingly different from the training set. Figure 2A shows nearest neighbors of corresponding images from the query set. Although the images have hardly changed visually, a shift by only two pixels already caused a different nearest neighbor. The plot also shows Euclidean distances to 100 randomly picked images from the training set. Note that with a bigger dataset, a switch to a different nearest neighbor becomes more likely. Figure 2B shows the fraction of query images assigned to the correct training image in our example. A model which stores transformed training images can trivially pass the nearest-neighbor overﬁtting test. This problem can be alleviated by choosing nearest neighbors based on perceptual metrics, and by showing more than one nearest neighbor. A second problem concerns the entropy of the model distribution and is harder to address. There are different ways a model can overﬁt. Even when overﬁtting, most models will not reproduce perfect or trivially transformed copies of the training data. In this case, no distance metric will ﬁnd a close match in the training set. A model which overﬁts might still never generate a plausible image or might only be able to generate a small fraction of all plausible images (e.g., a model as in Equation 10 where instead of training images we store several transformed versions of the  6  Published as a conference paper at ICLR 2016  ] t  a n  [  d o o h  i l  e k  i l - g o L  240  200  160  120  80  40  0  Log-likelihood  Estimate  Model  Parzen est. [nat]  Stacked CAE  DBN GMMN Deep GSN Diffusion  GAN  True distribution  GMMN + AE  k-means  121 138 147 214 220 225 243 282 313  101 102 103 104 105 106 107  Number of samples  Figure 3: Parzen window estimates for a Gaus- sian evaluated on 6 by 6 pixel image patches from the CIFAR-10 dataset. Even for small patches and a very large number of samples, the Parzen window estimate is far from the true log- likelihood.  Table 1: Using Parzen window estimates to evaluate various models trained on MNIST, samples from the true distribution perform worse than samples from a simple model trained with k-means.  training images, or a model which only describes data in a lower-dimensional subspace). Because the number of images we can process is vanishingly small compared to the vast number of possible images, we would not be able to detect this by looking at samples from the model.  3.5 EVALUATION BASED ON PARZEN WINDOW ESTIMATES  When log-likelihoods are unavailable, a common alternative is to use Parzen window estimates. Here, samples are generated from the model and used to construct a tractable model, typically a kernel density estimator with Gaussian kernel. A test log-likelihood is then evaluated under this model and used as a proxy for the true model’s log-likelihood (Breuleux et al., 2009). Breuleux et al. (2009) suggested to ﬁt the Parzen windows on both samples and training data, and to use at least as many samples as there are images in the training set. Following Bengio et al. (2013a), Parzen windows are in practice commonly ﬁt to only 10,000 samples (e.g., Bengio et al., 2013b; Goodfellow et al., 2014; Li et al., 2015; Sohl-Dickstein et al., 2015). But even for a large number of samples Parzen window estimates generally do not come close to a model’s true log-likelihood when the data dimensionality is high. In Figure 3 we plot Parzen window estimates for a multivariate Gaussian distribution ﬁt to small CIFAR-10 image patches (of size 6 by 6). We added uniform noise to the data (as explained in Section 3.1) and rescaled between 0 and 1. As we can see, a completely infeasible number of samples would be needed to get close to the actual log-likelihood even for this small scale example. For higher dimensional data this effect would only be more pronounced. While the Parzen window estimate may be far removed from a model’s true log-likelihood, one could still hope that it produces a similar or otherwise useful ranking when applied to different models. Counter to this idea, Parzen window estimates of the likelihood have been observed to produce rank- ings different from other estimates (Bachman & Precup, 2015). More worryingly, a GMMN+AE (Li et al., 2015) is assigned a higher score than images from the training set (which are samples from the true distribution) when evaluated on MNIST (Table 1). Furthermore it is relatively easy to exploit the Parzen window loss function to achieve even better results. To illustrate this, we ﬁtted 10,000 centroids to the training data using k-means. We then generated 10,000 independent samples by sampling centroids with replacement. Note that this corresponds to the model in Equation 10, where the standard deviation of the Gaussian noise is zero and instead of training examples we use the centroids. We ﬁnd that samples from this k-means based model are assigned a higher score than any other model, while its actual log-likelihood would be −∞.  7  Published as a conference paper at ICLR 2016  4 CONCLUSION  We have discussed the optimization and evaluation of generative image models. Different metrics can lead to different trade-offs, and different evaluations favor different models. It is therefore important that training and evaluation match the target application. Furthermore, we should be cautious not to take good performance in one application as evidence of good performance in another application. An evaluation based on samples is biased towards models which overﬁt and therefore a poor indi- cator of a good density model in a log-likelihood sense, which favors models with large entropy. Conversely, a high likelihood does not guarantee visually pleasing samples. Samples can take on arbitrary form only a few bits from the optimum. It is therefore unsurprising that other approaches than density estimation are much more effective for image synthesis (Portilla & Simoncelli, 2000; Dosovitskiy et al., 2015; Gatys et al., 2015). Samples are in general also an unreliable proxy for a model’s performance in applications such as classiﬁcation or inpainting, as discussed in Section 3.3. A subjective evaluation based on visual ﬁdelity of samples is still clearly appropriate when the goal is image synthesis. Such an analysis at least has the property that the data distribution will perform very well in this task. This cannot be said about Parzen window estimates, where the data distribution performs worse than much less desirable models1. We therefore argue Parzen window estimates should be avoided for evaluating generative models, unless the application speciﬁcally requires such a loss function. In this case, we have shown that a k-means based model can perform better than the true density. To summarize, our results demonstrate that for generative models there is no one-ﬁts-all loss function but a proper assessment of model performance is only possible in the the context of an application.  ACKNOWLEDGMENTS  The authors would like to thank Jascha Sohl-Dickstein, Ivo Danihelka, Andriy Mnih, and Leon Gatys for their valuable input on this manuscript.  REFERENCES Bachman, P. and Precup, D. Variational Generative Stochastic Networks with Collaborative Shap- ing. Proceedings of the 32nd International Conference on Machine Learning, pp. 1964–1972, 2015.  Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. Better mixing via deep representations.  Proceedings of the 30th International Conference on Machine Learning, 2013a.  In  Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. Deep generative stochastic networks  trainable by backprop, 2013b. arXiv:1306.1091.  Bishop, C. M. Pattern Recognition and Machine Learning. Springer, 2006.  Breuleux, O., Bengio, Y., and Vincent, P. Unlearning for better mixing. Technical report, Universite  de Montreal, 2009.  Denton, E., Chintala, S., Szlam, A., and Fergus, R. Deep Generative Image Models using a Lapla-  cian Pyramid of Adversarial Networks. arXiv.org, 2015.  Dosovitskiy, A., Springenberg, J. T., and Brox, T. Learning to Generate Chairs with Convolutional Neural Networks. In IEEE International Conference on Computer Vision and Pattern Recogni- tion, 2015.  Dziugaite, G. K., Roy, D. M., and Ghahramani, Z. Training generative neural networks via maxi-  mum mean discrepancy optimization, 2015. arXiv:1505.0390.  Gatys, L. A., Ecker, A. S., and Bethge, M. Texture synthesis and the controlled generation of natural  stimuli using convolutional neural networks, 2015. arXiv:1505.07376.  1In decision theory, such a metric is called an improper scoring function.  8  Published as a conference paper at ICLR 2016  Gerhard, H. E., Wichmann, F. A., and Bethge, M. How sensitive is the human visual system to the  local statistics of natural images? PLoS Computational Biology, 9(1), 2013.  Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, 2014.  Gregor, K., Danihelka, I., Graves, A., and Wierstra, D. DRAW: A recurrent neural network for image generation. In Proceedings of the 32nd International Conference on Machine Learning, 2015.  Gretton, A., Borgwardt, K. M., Rasch, M., Sch¨olkopf, B., and Smola, A. J. A kernel method for the  two-sample-problem. In Advances in Neural Information Processing Systems 20, 2007.  Gutmann, M. and Hyv¨arinen, A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics, 2010.  Hays, J. and Efros, A. A. Scene completion using millions of photographs. ACM Transactions on  Graphics (SIGGRAPH), 26, 2007.  Hinton, G. E. Training Products of Experts by Minimizing Contrastive Divergence. Neural Compu-  tation, 14(8):1771–1800, 2002.  Hyv¨arinen, A., Hurri, J., and Hoyer, P. O. Natural Image Statistics: A Probabilistic Approach to  Early Computational Vision. Springer, 2009.  Hyv¨arinen, A. Estimation of non-normalized statistical models using score matching. Journal of  Machine Learning Research, pp. 695–709, 2005.  Kingma, D. P., Rezende, D. J., Mohamed, S., and Welling, M. Semi-supervised learning with deep  generative models. In Advances in Neural Information Processing Systems 27, 2014.  Lacoste-Julien, S., Huszar, F., and Ghahramani, Z. Approximate inference for the loss-calibrated In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and  Bayesian. Statistics, 2011.  Lasserre, J. A., Bishop, C. M., and Minka, T. P. Principled hybrids of generative and discriminative  models. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2006.  Li, Y., Swersky, K., and Zemel, R. Generative moment matching networks. In Proceedings of the  32nd International Conference on Machine Learning, 2015.  Portilla, J. and Simoncelli, E. P. A parametric texture model based on joint statistics of complex  wavelet coefﬁcients. International Journal of Computer Vision, 40:49–70, 2000.  Shannon, C. E. A mathematical theory of communication. ACM SIGMOBILE Mobile Computing  and Communications Review, 5(1):3–55, 2001.  Sohl-Dickstein, J., Battaglino, P., and DeWeese, M. R. Minimum Probability Flow Learning. In  Proceedings of the 28th International Conference on Machine Learning, 2011.  Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, 2015.  Theis, L. and Bethge, M. Generative Image Modeling Using Spatial LSTMs. In Advances in Neural  Information Processing Systems 28, 2015.  Uria, B., Murray, I., and Larochelle, H. RNADE: The real-valued neural autoregressive density-  estimator. In Advances in Neural Information Processing Systems 26, 2013.  van den Oord, A. and Dambre, J. Locally-connected transformations for deep GMMs, 2015. Deep  Learning Workshop, ICML.  9  Published as a conference paper at ICLR 2016  van den Oord, A. and Schrauwen, B. Factoring Variations in Natural Images with Deep Gaussian  Mixture Models. In Advances in Neural Information Processing Systems 27, 2014.  Wang, Z. and Bovik, A. C. Mean squared error: Love it or leave it?  Magazine, 2009.  IEEE Signal Processing  Welling, M. and Teh, Y. W. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In  Proceedings of the 28th International Conference on Machine Learning, 2011.  10  ",
1510.00149,2016,"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding","['Deep Compression: Compressing Deep Neural Networks with Pruning', 'Trained Quantization and Huffman Coding\nSong Han', 'Huizi Mao', 'Bill Dally']",https://arxiv.org/pdf/1510.00149,"6 1 0 2     b e F 5 1         ]  V C . s c [      5 v 9 4 1 0 0  .  0 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING  Song Han Stanford University, Stanford, CA 94305, USA songhan@stanford.edu  Huizi Mao Tsinghua University, Beijing, 100084, China mhz12@mails.tsinghua.edu.cn  William J. Dally Stanford University, Stanford, CA 94305, USA NVIDIA, Santa Clara, CA 95050, USA dally@stanford.edu  ABSTRACT  Neural networks are both computationally intensive and memory intensive, making them difﬁcult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce “deep compression”, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method ﬁrst prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, ﬁnally, we apply Huffman coding. After the ﬁrst two steps we retrain the network to ﬁne tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows ﬁtting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efﬁciency.  1  INTRODUCTION  Deep neural networks have evolved to the state-of-the-art technique for computer vision tasks (Krizhevsky et al., 2012)(Simonyan & Zisserman, 2014). Though these neural networks are very powerful, the large number of weights consumes considerable storage and memory bandwidth. For example, the AlexNet Caffemodel is over 200MB, and the VGG-16 Caffemodel is over 500MB (BVLC). This makes it difﬁcult to deploy deep neural networks on mobile system. First, for many mobile-ﬁrst companies such as Baidu and Facebook, various apps are updated via different app stores, and they are very sensitive to the size of the binary ﬁles. For example, App Store has the restriction “apps above 100 MB will not download until you connect to Wi-Fi”. As a result, a feature that increases the binary size by 100MB will receive much more scrutiny than one that increases it by 10MB. Although having deep neural networks running on mobile has many great  1  Published as a conference paper at ICLR 2016  Figure 1: The three stage compression pipeline: pruning, quantization and Huffman coding. Pruning reduces the number of weights by 10×, while quantization further improves the compression rate: between 27× and 31×. Huffman coding gives more compression: between 35× and 49×. The compression rate already included the meta-data for sparse representation. The compression scheme doesn’t incur any accuracy loss.  features such as better privacy, less network bandwidth and real time processing, the large storage overhead prevents deep neural networks from being incorporated into mobile apps. The second issue is energy consumption. Running large neural networks require a lot of memory bandwidth to fetch the weights and a lot of computation to do dot products— which in turn consumes considerable energy. Mobile devices are battery constrained, making power hungry applications such as deep neural networks hard to deploy. Energy consumption is dominated by memory access. Under 45nm CMOS technology, a 32 bit ﬂoating point add consumes 0.9pJ, a 32bit SRAM cache access takes 5pJ, while a 32bit DRAM memory access takes 640pJ, which is 3 orders of magnitude of an add operation. Large networks do not ﬁt in on-chip storage and hence require the more costly DRAM accesses. Running a 1 billion connection neural network, for example, at 20fps would require (20Hz)(1G)(640pJ) = 12.8W just for DRAM access - well beyond the power envelope of a typical mobile device. Our goal is to reduce the storage and energy required to run inference on such large networks so they can be deployed on mobile devices. To achieve this goal, we present “deep compression”: a three- stage pipeline (Figure 1) to reduce the storage required by neural network in a manner that preserves the original accuracy. First, we prune the networking by removing the redundant connections, keeping only the most informative connections. Next, the weights are quantized so that multiple connections share the same weight, thus only the codebook (effective weights) and the indices need to be stored. Finally, we apply Huffman coding to take advantage of the biased distribution of effective weights. Our main insight is that, pruning and trained quantization are able to compress the network without interfering each other, thus lead to surprisingly high compression rate. It makes the required storage so small (a few megabytes) that all weights can be cached on chip instead of going to off-chip DRAM which is energy consuming. Based on “deep compression”, the EIE hardware accelerator Han et al. (2016) was later proposed that works on the compressed model, achieving signiﬁcant speedup and energy efﬁciency improvement.  2 NETWORK PRUNING  Network pruning has been widely studied to compress CNN models. In early work, network pruning proved to be a valid way to reduce the network complexity and over-ﬁtting (LeCun et al., 1989; Hanson & Pratt, 1989; Hassibi et al., 1993; Str¨om, 1997). Recently Han et al. (2015) pruned state- of-the-art CNN models with no loss of accuracy. We build on top of that approach. As shown on the left side of Figure 1, we start by learning the connectivity via normal network training. Next, we prune the small-weight connections: all connections with weights below a threshold are removed from the network. Finally, we retrain the network to learn the ﬁnal weights for the remaining sparse connections. Pruning reduced the number of parameters by 9× and 13× for AlexNet and VGG-16 model.  2  Train ConnectivityPrune ConnectionsTrain WeightsCluster the WeightsGenerate Code BookQuantize the Weights with Code BookRetrain Code BookPruning: less number of weightsQuantization: less bits per weightoriginal   size   9x-13x reduction  27x-31x reduction   same accuracy   same accuracyoriginal networkEncode WeightsEncode IndexHuffman Encoding  35x-49x reduction   same accuracyPublished as a conference paper at ICLR 2016  Figure 2: Representing the matrix sparsity with relative index. Padding ﬁller zero to prevent overﬂow.  Figure 3: Weight sharing by scalar quantization (top) and centroids ﬁne-tuning (bottom).  We store the sparse structure that results from pruning using compressed sparse row (CSR) or compressed sparse column (CSC) format, which requires 2a + n + 1 numbers, where a is the number of non-zero elements and n is the number of rows or columns. To compress further, we store the index difference instead of the absolute position, and encode this difference in 8 bits for conv layer and 5 bits for fc layer. When we need an index difference larger than the bound, we the zero padding solution shown in Figure 2: in case when the difference exceeds 8, the largest 3-bit (as an example) unsigned number, we add a ﬁller zero.  3 TRAINED QUANTIZATION AND WEIGHT SHARING  Network quantization and weight sharing further compresses the pruned network by reducing the number of bits required to represent each weight. We limit the number of effective weights we need to store by having multiple connections share the same weight, and then ﬁne-tune those shared weights. Weight sharing is illustrated in Figure 3. Suppose we have a layer that has 4 input neurons and 4 output neurons, the weight is a 4 × 4 matrix. On the top left is the 4 × 4 weight matrix, and on the bottom left is the 4 × 4 gradient matrix. The weights are quantized to 4 bins (denoted with 4 colors), all the weights in the same bin share the same value, thus for each weight, we then need to store only a small index into a table of shared weights. During update, all the gradients are grouped by the color and summed together, multiplied by the learning rate and subtracted from the shared centroids from last iteration. For pruned AlexNet, we are able to quantize to 8-bits (256 shared weights) for each CONV layers, and 5-bits (32 shared weights) for each FC layer without any loss of accuracy. To calculate the compression rate, given k clusters, we only need log2(k) bits to encode the index. In general, for a network with n connections and each connection is represented with b bits, constraining the connections to have only k shared weights will result in a compression rate of:  r =  nb  nlog2(k) + kb  (1)  For example, Figure 3 shows the weights of a single layer neural network with four input units and four output units. There are 4× 4 = 16 weights originally but there are only 4 shared weights: similar weights are grouped together to share the same value. Originally we need to store 16 weights each  3  2.09-0.981.480.090.05-0.14-1.082.12-0.911.920-1.031.8701.531.493011110303103122-0.03-0.010.030.02-0.010.01-0.020.12-0.010.020.040.01-0.07-0.020.01-0.020.040.020.04-0.03-0.030.120.02-0.070.030.010.02-0.010.010.04 -0.01-0.02-0.010.01cluster   weights (32 bit float)centroidsgradient3021110303103122cluster index  (2 bit uint)2.001.500.00-1.00-0.02-0.02group byfine-tuned centroidsreduce1.961.48-0.04-0.971:lr0:2:3:Published as a conference paper at ICLR 2016  Figure 4: Left: Three different methods for centroids initialization. Right: Distribution of weights (blue) and distribution of codebook before (green cross) and after ﬁne-tuning (red dot).  has 32 bits, now we need to store only 4 effective weights (blue, green, red and orange), each has 32 bits, together with 16 2-bit indices giving a compression rate of 16 ∗ 32/(4 ∗ 32 + 2 ∗ 16) = 3.2  3.1 WEIGHT SHARING  We use k-means clustering to identify the shared weights for each layer of a trained network, so that all the weights that fall into the same cluster will share the same weight. Weights are not shared across layers. We partition n original weights W = {w1, w2, ..., wn} into k clusters C = {c1, c2, ..., ck}, n (cid:29) k, so as to minimize the within-cluster sum of squares (WCSS):  k(cid:88)  (cid:88)  i=1  w∈ci  arg min  C  |w − ci|2  (2)  Different from HashNet (Chen et al., 2015) where weight sharing is determined by a hash function before the networks sees any training data, our method determines weight sharing after a network is fully trained, so that the shared weights approximate the original network.  3.2  INITIALIZATION OF SHARED WEIGHTS  Centroid initialization impacts the quality of clustering and thus affects the network’s prediction accuracy. We examine three initialization methods: Forgy(random), density-based, and linear initialization. In Figure 4 we plotted the original weights’ distribution of conv3 layer in AlexNet (CDF in blue, PDF in red). The weights forms a bimodal distribution after network pruning. On the bottom it plots the effective weights (centroids) with 3 different initialization methods (shown in blue, red and yellow). In this example, there are 13 clusters. Forgy (random) initialization randomly chooses k observations from the data set and uses these as the initial centroids. The initialized centroids are shown in yellow. Since there are two peaks in the bimodal distribution, Forgy method tend to concentrate around those two peaks. Density-based initialization linearly spaces the CDF of the weights in the y-axis, then ﬁnds the horizontal intersection with the CDF, and ﬁnally ﬁnds the vertical intersection on the x-axis, which becomes a centroid, as shown in blue dots. This method makes the centroids denser around the two peaks, but more scatted than the Forgy method. Linear initialization linearly spaces the centroids between the [min, max] of the original weights. This initialization method is invariant to the distribution of the weights and is the most scattered compared with the former two methods. Larger weights play a more important role than smaller weights (Han et al., 2015), but there are fewer of these large weights. Thus for both Forgy initialization and density-based initialization, very few centroids have large absolute value which results in poor representation of these few large weights. Linear initialization does not suffer from this problem. The experiment section compares the accuracy  4  0.100.050.000.050.10weight value0.00.20.40.60.81.0cummulative distributionCDFPDFdensity initializationlinear initializationrandom initialization0.040.020.000.020.040.06weight value05000100001500020000densitylinear quantizationnonlinear quantization byclustring and finetuningPublished as a conference paper at ICLR 2016  Figure 5: Distribution for weight (Left) and index (Right). The distribution is biased.  of different initialization methods after clustering and ﬁne-tuning, showing that linear initialization works best.  3.3 FEED-FORWARD AND BACK-PROPAGATION  The centroids of the one-dimensional k-means clustering are the shared weights. There is one level of indirection during feed forward phase and back-propagation phase looking up the weight table. An index into the shared weight table is stored for each connection. During back-propagation, the gradient for each shared weight is calculated and used to update the shared weight. This procedure is shown in Figure 3. We denote the loss by L, the weight in the ith column and jth row by Wij, the centroid index of element Wi,j by Iij, the kth centroid of the layer by Ck. By using the indicator function 1(.), the gradient of the centroids is calculated as: ∂L ∂Wij  ∂L ∂Wij  (cid:88)  (cid:88)  1(Iij = k)  ∂Wij ∂Ck  (3)  =  =  i,j  i,j  ∂L ∂Ck 4 HUFFMAN CODING  A Huffman code is an optimal preﬁx code commonly used for lossless data compression(Van Leeuwen, 1976). It uses variable-length codewords to encode source symbols. The table is derived from the occurrence probability for each symbol. More common symbols are represented with fewer bits. Figure 5 shows the probability distribution of quantized weights and the sparse matrix index of the last fully connected layer in AlexNet. Both distributions are biased: most of the quantized weights are distributed around the two peaks; the sparse matrix index difference are rarely above 20. Experiments show that Huffman coding these non-uniformly distributed values saves 20% − 30% of network storage.  5 EXPERIMENTS  We pruned, quantized, and Huffman encoded four networks: two on MNIST and two on ImageNet data-sets. The network parameters and accuracy-1 before and after pruning are shown in Table 1. The compression pipeline saves network storage by 35× to 49× across different networks without loss of accuracy. The total size of AlexNet decreased from 240MB to 6.9MB, which is small enough to be put into on-chip SRAM, eliminating the need to store the model in energy-consuming DRAM memory. Training is performed with the Caffe framework (Jia et al., 2014). Pruning is implemented by adding a mask to the blobs to mask out the update of the pruned connections. Quantization and weight sharing are implemented by maintaining a codebook structure that stores the shared weight, and group-by-index after calculating the gradient of each layer. Each shared weight is updated with all the gradients that fall into that bucket. Huffman coding doesn’t require training and is implemented ofﬂine after all the ﬁne-tuning is ﬁnished.  5.1 LENET-300-100 AND LENET-5 ON MNIST  We ﬁrst experimented on MNIST dataset with LeNet-300-100 and LeNet-5 network (LeCun et al., 1998). LeNet-300-100 is a fully connected network with two hidden layers, with 300 and 100  1Reference model is from Caffe model zoo, accuracy is measured without data augmentation  5  0250005000075000100000135791113151719212325272931CountWeight Index (32 Effective Weights)055000110000165000220000135791113151719212325272931CountSparse Matrix Location Index (Max Diff is 32)Published as a conference paper at ICLR 2016 Table 1: The compression pipeline can save 35× to 49× parameter storage with no loss of accuracy.  Network LeNet-300-100 Ref LeNet-300-100 Compressed LeNet-5 Ref LeNet-5 Compressed AlexNet Ref AlexNet Compressed VGG-16 Ref VGG-16 Compressed  Top-1 Error Top-5 Error 1.64% 1.58% 0.80% 0.74% 42.78% 42.78% 31.50% 31.17%  - - - - 19.73% 19.70% 11.32% 10.91%  Parameters Compress 1070 KB 27 KB 1720 KB 44 KB 240 MB 6.9 MB 552 MB 11.3 MB  Rate 40× 39× 35× 49×  Table 2: Compression statistics for LeNet-300-100. P: pruning, Q:quantization, H:Huffman coding.  Layer  ip1 ip2 ip3 Total  235K 30K 1K 266K  #Weights Weights%  (P) 8% 9% 26% 8%(12×)  Weight bits (P+Q) 6 6 6 6  Weight bits (P+Q+H) 4.4 4.4 4.3 5.1  Index bits (P+Q) 5 5 5 5  Index bits (P+Q+H) 3.7 4.3 3.2 3.7  Compress rate (P+Q) 3.1% 3.8% 15.7% 3.1% (32×)  Compress rate (P+Q+H) 2.32% 3.04% 12.70% 2.49% (40×)  Table 3: Compression statistics for LeNet-5. P: pruning, Q:quantization, H:Huffman coding.  Layer  conv1 conv2 ip1 ip2 Total  0.5K 25K 400K 5K 431K  #Weights Weights%  (P) 66% 12% 8% 19% 8%(12×)  Weight bits (P+Q) 8 8 5 5 5.3  Weight bits (P+Q+H) 7.2 7.2 4.5 5.2 4.1  Index bits (P+Q) 5 5 5 5 5  Index bits (P+Q+H) 1.5 3.9 4.5 3.7 4.4  Compress rate (P+Q) 78.5% 6.0% 2.7% 6.9% 3.05% (33×)  Compress rate (P+Q+H) 67.45% 5.28% 2.45% 6.13% 2.55% (39×)  neurons each, which achieves 1.6% error rate on Mnist. LeNet-5 is a convolutional network that has two convolutional layers and two fully connected layers, which achieves 0.8% error rate on Mnist. Table 2 and table 3 show the statistics of the compression pipeline. The compression rate includes the overhead of the codebook and sparse indexes. Most of the saving comes from pruning and quantization (compressed 32×), while Huffman coding gives a marginal gain (compressed 40×)  5.2 ALEXNET ON IMAGENET  We further examine the performance of Deep Compression on the ImageNet ILSVRC-2012 dataset, which has 1.2M training examples and 50k validation examples. We use the AlexNet Caffe model as the reference model, which has 61 million parameters and achieved a top-1 accuracy of 57.2% and a top-5 accuracy of 80.3%. Table 4 shows that AlexNet can be compressed to 2.88% of its original size without impacting accuracy. There are 256 shared weights in each CONV layer, which are encoded with 8 bits, and 32 shared weights in each FC layer, which are encoded with only 5 bits. The relative sparse index is encoded with 4 bits. Huffman coding compressed additional 22%, resulting in 35× compression in total.  5.3 VGG-16 ON IMAGENET  With promising results on AlexNet, we also looked at a larger, more recent network, VGG-16 (Si- monyan & Zisserman, 2014), on the same ILSVRC-2012 dataset. VGG-16 has far more convolutional layers but still only three fully-connected layers. Following a similar methodology, we aggressively compressed both convolutional and fully-connected layers to realize a signiﬁcant reduction in the number of effective weights, shown in Table5. The VGG16 network as a whole has been compressed by 49×. Weights in the CONV layers are represented with 8 bits, and FC layers use 5 bits, which does not impact the accuracy. The two largest fully-connected layers can each be pruned to less than 1.6% of their original size. This reduction  6  Published as a conference paper at ICLR 2016  Table 4: Compression statistics for AlexNet. P: pruning, Q: quantization, H:Huffman coding.  #Weights Weights%  Layer  conv1 conv2 conv3 conv4 conv5 fc6 fc7 fc8 Total  35K 307K 885K 663K 442K 38M 17M 4M 61M  (P) 84% 38% 35% 37% 37% 9% 9% 25% 11%(9×)  Weight bits (P+Q) 8 8 8 8 8 5 5 5 5.4  Weight bits (P+Q+H) 6.3 5.5 5.1 5.2 5.6 3.9 3.6 4 4  Index bits (P+Q) 4 4 4 4 4 4 4 4 4  Index bits (P+Q+H) 1.2 2.3 2.6 2.5 2.5 3.2 3.7 3.2 3.2  Compress rate (P+Q) 32.6% 14.5% 13.1% 14.1% 14.0% 3.0% 3.0% 7.3% 3.7% (27×)  Compress rate (P+Q+H) 20.53% 9.43% 8.44% 9.11% 9.43% 2.39% 2.46% 5.85% 2.88% (35×)  Table 5: Compression statistics for VGG-16. P: pruning, Q:quantization, H:Huffman coding.  #Weights Weights%  Layer  conv1 1 conv1 2 conv2 1 conv2 2 conv3 1 conv3 2 conv3 3 conv4 1 conv4 2 conv4 3 conv5 1 conv5 2 conv5 3 fc6 fc7 fc8 Total  2K 37K 74K 148K 295K 590K 590K 1M 2M 2M 2M 2M 2M 103M 17M 4M 138M  (P) 58% 22% 34% 36% 53% 24% 42% 32% 27% 34% 35% 29% 36% 4% 4% 23% 7.5%(13×)  Weigh bits (P+Q) 8 8 8 8 8 8 8 8 8 8 8 8 8 5 5 5 6.4  Weight bits (P+Q+H) 6.8 6.5 5.6 5.9 4.8 4.6 4.6 4.6 4.2 4.4 4.7 4.6 4.6 3.6 4 4 4.1  Index bits (P+Q) 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5  Index bits (P+Q+H) 1.7 2.6 2.4 2.3 1.8 2.9 2.2 2.6 2.9 2.5 2.5 2.7 2.3 3.5 4.3 3.4 3.1  Compress rate (P+Q) 40.0% 9.8% 14.3% 14.7% 21.7% 9.7% 17.0% 13.1% 10.9% 14.0% 14.3% 11.7% 14.8% 1.6% 1.5% 7.1% 3.2% (31×)  Compress rate (P+Q+H) 29.97% 6.99% 8.91% 9.31% 11.15% 5.67% 8.96% 7.29% 5.93% 7.47% 8.00% 6.52% 7.79% 1.10% 1.25% 5.24% 2.05% (49×)  is critical for real time image processing, where there is little reuse of these layers across images (unlike batch processing). This is also critical for fast object detection algorithms where one CONV pass is used by many FC passes. The reduced layers will ﬁt in an on-chip SRAM and have modest bandwidth requirements. Without the reduction, the bandwidth requirements are prohibitive.  6 DISCUSSIONS  6.1 PRUNING AND QUANTIZATION WORKING TOGETHER  Figure 6 shows the accuracy at different compression rates for pruning and quantization together or individually. When working individually, as shown in the purple and yellow lines, accuracy of pruned network begins to drop signiﬁcantly when compressed below 8% of its original size; accuracy of quantized network also begins to drop signiﬁcantly when compressed below 8% of its original size. But when combined, as shown in the red line, the network can be compressed to 3% of original size with no loss of accuracy. On the far right side compared the result of SVD, which is inexpensive but has a poor compression rate. The three plots in Figure 7 show how accuracy drops with fewer bits per connection for CONV layers (left), FC layers (middle) and all layers (right). Each plot reports both top-1 and top-5 accuracy. Dashed lines only applied quantization but without pruning; solid lines did both quantization and pruning. There is very little difference between the two. This shows that pruning works well with quantization. Quantization works well on pruned network because unpruned AlexNet has 60 million weights to quantize, while pruned AlexNet has only 6.7 million weights to quantize. Given the same amount of centroids, the latter has less error.  7  Published as a conference paper at ICLR 2016  Figure 6: Accuracy v.s. compression rate under different compression methods. Pruning and quantization works best when combined.  Figure 7: Pruning doesn’t hurt quantization. Dashed: quantization on unpruned network. Solid: quantization on pruned network; Accuracy begins to drop at the same number of quantization bits whether or not the network has been pruned. Although pruning made the number of parameters less, quantization still works well, or even better(3 bits case on the left ﬁgure) as in the unpruned network.  Figure 8: Accuracy of different initialization methods. Left: top-1 accuracy. Right: top-5 accuracy. Linear initialization gives best result.  The ﬁrst two plots in Figure 7 show that CONV layers require more bits of precision than FC layers. For CONV layers, accuracy drops signiﬁcantly below 4 bits, while FC layer is more robust: not until 2 bits did the accuracy drop signiﬁcantly.  6.2 CENTROID INITIALIZATION  Figure 8 compares the accuracy of the three different initialization methods with respect to top-1 accuracy (Left) and top-5 accuracy (Right). The network is quantized to 2 ∼ 8 bits as shown on x-axis. Linear initialization outperforms the density initialization and random initialization in all cases except at 3 bits. The initial centroids of linear initialization spread equally across the x-axis, from the min value to the max value. That helps to maintain the large weights as the large weights play a more important role than smaller ones, which is also shown in network pruning Han et al. (2015). Neither random nor density-based initialization retains large centroids. With these initialization methods, large weights are clustered to the small centroids because there are few large weights. In contrast, linear initialization allows large weights a better chance to form a large centroid.  8  Published as a conference paper at ICLR 2016  Figure 9: Compared with the original network, pruned network layer achieved 3× speedup on CPU, 3.5× on GPU and 4.2× on mobile GPU on average. Batch size = 1 targeting real time processing. Performance number normalized to CPU.  Figure 10: Compared with the original network, pruned network layer takes 7× less energy on CPU, 3.3× less on GPU and 4.2× less on mobile GPU on average. Batch size = 1 targeting real time processing. Energy number normalized to CPU.  6.3 SPEEDUP AND ENERGY EFFICIENCY  Deep Compression is targeting extremely latency-focused applications running on mobile, which requires real-time inference, such as pedestrian detection on an embedded processor inside an autonomous vehicle. Waiting for a batch to assemble signiﬁcantly adds latency. So when bench- marking the performance and energy efﬁciency, we consider the case when batch size = 1. The cases of batching are given in Appendix A. Fully connected layer dominates the model size (more than 90%) and got compressed the most by Deep Compression (96% weights pruned in VGG-16). In state-of-the-art object detection algorithms such as fast R-CNN (Girshick, 2015), upto 38% computation time is consumed on FC layers on uncompressed model. So it’s interesting to benchmark on FC layers, to see the effect of Deep Compression on performance and energy. Thus we setup our benchmark on FC6, FC7, FC8 layers of AlexNet and VGG-16. In the non-batched case, the activation matrix is a vector with just one column, so the computation boils down to dense / sparse matrix-vector multiplication for original / pruned model, respectively. Since current BLAS library on CPU and GPU doesn’t support indirect look-up and relative indexing, we didn’t benchmark the quantized model. We compare three different off-the-shelf hardware: the NVIDIA GeForce GTX Titan X and the Intel Core i7 5930K as desktop processors (same package as NVIDIA Digits Dev Box) and NVIDIA Tegra K1 as mobile processor. To run the benchmark on GPU, we used cuBLAS GEMV for the original dense layer. For the pruned sparse layer, we stored the sparse matrix in in CSR format, and used cuSPARSE CSRMV kernel, which is optimized for sparse matrix-vector multiplication on GPU. To run the benchmark on CPU, we used MKL CBLAS GEMV for the original dense model and MKL SPBLAS CSRMV for the pruned sparse model. To compare power consumption between different systems, it is important to measure power at a consistent manner (NVIDIA, b). For our analysis, we are comparing pre-regulation power of the entire application processor (AP) / SOC and DRAM combined. On CPU, the benchmark is running on single socket with a single Haswell-E class Core i7-5930K processor. CPU socket and DRAM power are as reported by the pcm-power utility provided by Intel. For GPU, we used nvidia-smi utility to report the power of Titan X. For mobile GPU, we use a Jetson TK1 development board and measured the total power consumption with a power-meter. We assume 15% AC to DC conversion loss, 85% regulator efﬁciency and 15% power consumed by peripheral components (NVIDIA, a) to report the AP+DRAM power for Tegra K1.  9  Published as a conference paper at ICLR 2016  Table 6: Accuracy of AlexNet with different aggressiveness of weight sharing and quantization. 8/5 bit quantization has no loss of accuracy; 8/4 bit quantization, which is more hardware friendly, has negligible loss of accuracy of 0.01%; To be really aggressive, 4/2 bit quantization resulted in 1.99% and 2.60% loss of accuracy.  #CONV bits / #FC bits  Top-1 Error Top-5 Error  32bits / 32bits 8 bits / 5 bits 8 bits / 4 bits 4 bits / 2 bits  42.78% 42.78% 42.79% 44.77%  19.73% 19.70% 19.73% 22.33%  Top-1 Error  Increase  -  0.00% 0.01% 1.99%  Top-5 Error  Increase  -  -0.03% 0.00% 2.60%  The ratio of memory access over computation characteristic with and without batching is different. When the input activations are batched to a matrix the computation becomes matrix-matrix multipli- cation, where locality can be improved by blocking. Matrix could be blocked to ﬁt in caches and reused efﬁciently. In this case, the amount of memory access is O(n2), and that of computation is O(n3), the ratio between memory access and computation is in the order of 1/n. In real time processing when batching is not allowed, the input activation is a single vector and the computation is matrix-vector multiplication. In this case, the amount of memory access is O(n2), and the computation is O(n2), memory access and computation are of the same magnitude (as opposed to 1/n). That indicates MV is more memory-bounded than MM. So reducing the memory footprint is critical for the non-batching case. Figure 9 illustrates the speedup of pruning on different hardware. There are 6 columns for each benchmark, showing the computation time of CPU / GPU / TK1 on dense / pruned network. Time is normalized to CPU. When batch size = 1, pruned network layer obtained 3× to 4× speedup over the dense network on average because it has smaller memory footprint and alleviates the data transferring overhead, especially for large matrices that are unable to ﬁt into the caches. For example VGG16’s FC6 layer, the largest layer in our experiment, contains 25088 × 4096 × 4 Bytes ≈ 400M B data, which is far from the capacity of L3 cache. In those latency-tolerating applications , batching improves memory locality, where weights could be blocked and reused in matrix-matrix multiplication. In this scenario, pruned network no longer shows its advantage. We give detailed timing results in Appendix A. Figure 10 illustrates the energy efﬁciency of pruning on different hardware. We multiply power consumption with computation time to get energy consumption, then normalized to CPU to get energy efﬁciency. When batch size = 1, pruned network layer consumes 3× to 7× less energy over the dense network on average. Reported by nvidia-smi, GPU utilization is 99% for both dense and sparse cases.  6.4 RATIO OF WEIGHTS, INDEX AND CODEBOOK  Pruning makes the weight matrix sparse, so extra space is needed to store the indexes of non-zero elements. Quantization adds storage for a codebook. The experiment section has already included these two factors. Figure 11 shows the breakdown of three different components when quantizing four networks. Since on average both the weights and the sparse indexes are encoded with 5 bits, their storage is roughly half and half. The overhead of codebook is very small and often negligible.  Figure 11: Storage ratio of weight, index and codebook.  10  Published as a conference paper at ICLR 2016  Table 7: Comparison with other compression methods on AlexNet. (Collins & Kohli, 2014) reduced the parameters by 4× and with inferior accuracy. Deep Fried Convnets(Yang et al., 2014) worked on fully connected layers and reduced the parameters by less than 4×. SVD save parameters but suffers from large accuracy loss as much as 2%. Network pruning (Han et al., 2015) reduced the parameters by 9×, not including index overhead. On other networks similar to AlexNet, (Denton et al., 2014) exploited linear structure of convnets and compressed the network by 2.4× to 13.4× layer wise, with 0.9% accuracy loss on compressing a single layer. (Gong et al., 2014) experimented with vector quantization and compressed the network by 16× to 24×, incurring 1% accuracy loss.  Network Baseline Caffemodel (BVLC) Fastfood-32-AD (Yang et al., 2014) Fastfood-16-AD (Yang et al., 2014) Collins & Kohli (Collins & Kohli, 2014) SVD (Denton et al., 2014) Pruning (Han et al., 2015) Pruning+Quantization Pruning+Quantization+Huffman  Top-1 Error Top-5 Error 42.78% 41.93% 42.90% 44.40% 44.02% 42.77% 42.78% 42.78%  19.73% - - - 20.56% 19.67% 19.70% 19.70%  Parameters Compress 240MB 131MB 64MB 61MB 47.6MB 27MB 8.9MB 6.9MB  Rate 1× 2× 3.7× 4× 5× 9× 27× 35×  7 RELATED WORK  Neural networks are typically over-parametrized, and there is signiﬁcant redundancy for deep learning models(Denil et al., 2013). This results in a waste of both computation and memory usage. There have been various proposals to remove the redundancy: Vanhoucke et al. (2011) explored a ﬁxed- point implementation with 8-bit integer (vs 32-bit ﬂoating point) activations. Hwang & Sung (2014) proposed an optimization method for the ﬁxed-point network with ternary weights and 3-bit activations. Anwar et al. (2015) quantized the neural network using L2 error minimization and achieved better accuracy on MNIST and CIFAR-10 datasets.Denton et al. (2014) exploited the linear structure of the neural network by ﬁnding an appropriate low-rank approximation of the parameters and keeping the accuracy within 1% of the original model. The empirical success in this paper is consistent with the theoretical study of random-like sparse networks with +1/0/-1 weights (Arora et al., 2014), which have been proved to enjoy nice properties (e.g. reversibility), and to allow a provably polynomial time algorithm for training. Much work has been focused on binning the network parameters into buckets, and only the values in the buckets need to be stored. HashedNets(Chen et al., 2015) reduce model sizes by using a hash function to randomly group connection weights, so that all connections within the same hash bucket share a single parameter value. In their method, the weight binning is pre-determined by the hash function, instead of being learned through training, which doesn’t capture the nature of images. Gong et al. (2014) compressed deep convnets using vector quantization, which resulted in 1% accuracy loss. Both methods studied only the fully connected layer, ignoring the convolutional layers. There have been other attempts to reduce the number of parameters of neural networks by replacing the fully connected layer with global average pooling. The Network in Network architecture(Lin et al., 2013) and GoogLenet(Szegedy et al., 2014) achieves state-of-the-art results on several benchmarks by adopting this idea. However, transfer learning, i.e. reusing features learned on the ImageNet dataset and applying them to new tasks by only ﬁne-tuning the fully connected layers, is more difﬁcult with this approach. This problem is noted by Szegedy et al. (2014) and motivates them to add a linear layer on the top of their networks to enable transfer learning. Network pruning has been used both to reduce network complexity and to reduce over-ﬁtting. An early approach to pruning was biased weight decay (Hanson & Pratt, 1989). Optimal Brain Damage (LeCun et al., 1989) and Optimal Brain Surgeon (Hassibi et al., 1993) prune networks to reduce the number of connections based on the Hessian of the loss function and suggest that such pruning is more accurate than magnitude-based pruning such as weight decay. A recent work (Han et al., 2015) successfully pruned several state of the art large scale networks and showed that the number of parameters could be reduce by an order of magnitude. There are also attempts to reduce the number of activations for both compression and acceleration Van Nguyen et al. (2015).  11  Published as a conference paper at ICLR 2016  8 FUTURE WORK  While the pruned network has been benchmarked on various hardware, the quantized network with weight sharing has not, because off-the-shelf cuSPARSE or MKL SPBLAS library does not support indirect matrix entry lookup, nor is the relative index in CSC or CSR format supported. So the full advantage of Deep Compression that ﬁt the model in cache is not fully unveiled. A software solution is to write customized GPU kernels that support this. A hardware solution is to build custom ASIC architecture specialized to traverse the sparse and quantized network structure, which also supports customized quantization bit width. We expect this architecture to have energy dominated by on-chip SRAM access instead of off-chip DRAM access.  9 CONCLUSION  We have presented “Deep Compression” that compressed neural networks without affecting accuracy. Our method operates by pruning the unimportant connections, quantizing the network using weight sharing, and then applying Huffman coding. We highlight our experiments on AlexNet which reduced the weight storage by 35× without loss of accuracy. We show similar results for VGG-16 and LeNet networks compressed by 49× and 39× without loss of accuracy. This leads to smaller storage requirement of putting convnets into mobile app. After Deep Compression the size of these networks ﬁt into on-chip SRAM cache (5pJ/access) rather than requiring off-chip DRAM memory (640pJ/access). This potentially makes deep neural networks more energy efﬁcient to run on mobile. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained.  REFERENCES Anwar, Sajid, Hwang, Kyuyeon, and Sung, Wonyong. Fixed point optimization of deep convolutional neural networks for object recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pp. 1131–1135. IEEE, 2015.  Arora, Sanjeev, Bhaskara, Aditya, Ge, Rong, and Ma, Tengyu. Provable bounds for learning some deep representations. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, pp. 584–592, 2014.  BVLC. Caffe model zoo. URL http://caffe.berkeleyvision.org/model_zoo.  Chen, Wenlin, Wilson, James T., Tyree, Stephen, Weinberger, Kilian Q., and Chen, Yixin. Compress-  ing neural networks with the hashing trick. arXiv preprint arXiv:1504.04788, 2015.  Collins, Maxwell D and Kohli, Pushmeet. Memory bounded deep convolutional networks. arXiv  preprint arXiv:1412.1442, 2014.  Denil, Misha, Shakibi, Babak, Dinh, Laurent, de Freitas, Nando, et al. Predicting parameters in deep  learning. In Advances in Neural Information Processing Systems, pp. 2148–2156, 2013.  Denton, Emily L, Zaremba, Wojciech, Bruna, Joan, LeCun, Yann, and Fergus, Rob. Exploiting linear structure within convolutional networks for efﬁcient evaluation. In Advances in Neural Information Processing Systems, pp. 1269–1277, 2014.  Girshick, Ross. Fast r-cnn. arXiv preprint arXiv:1504.08083, 2015.  Gong, Yunchao, Liu, Liu, Yang, Ming, and Bourdev, Lubomir. Compressing deep convolutional  networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.  Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for  efﬁcient neural networks. In Advances in Neural Information Processing Systems, 2015.  Han, Song, Liu, Xingyu, Mao, Huizi, Pu, Jing, Pedram, Ardavan, Horowitz, Mark A, and Dally, William J. EIE: Efﬁcient inference engine on compressed deep neural network. arXiv preprint arXiv:1602.01528, 2016.  12  Published as a conference paper at ICLR 2016  Hanson, Stephen Jos´e and Pratt, Lorien Y. Comparing biases for minimal network construction with  back-propagation. In Advances in neural information processing systems, pp. 177–185, 1989.  Hassibi, Babak, Stork, David G, et al. Second order derivatives for network pruning: Optimal brain  surgeon. Advances in neural information processing systems, pp. 164–164, 1993.  Hwang, Kyuyeon and Sung, Wonyong. Fixed-point feedforward deep neural network design using weights+ 1, 0, and- 1. In Signal Processing Systems (SiPS), 2014 IEEE Workshop on, pp. 1–6. IEEE, 2014.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.  convolutional neural networks. In NIPS, pp. 1097–1105, 2012.  Imagenet classiﬁcation with deep  LeCun, Yann, Denker, John S, Solla, Sara A, Howard, Richard E, and Jackel, Lawrence D. Optimal  brain damage. In NIPs, volume 89, 1989.  LeCun, Yann, Bottou, Leon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. arXiv:1312.4400, 2013.  NVIDIA. Technical brief: NVIDIA jetson TK1 development kit bringing GPU-accelerated computing  to embedded systems, a. URL http://www.nvidia.com.  NVIDIA. Whitepaper: GPU-based deep learning inference: A performance and power analysis, b.  URL http://www.nvidia.com/object/white-papers.html.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. arXiv preprint arXiv:1409.1556, 2014.  Str¨om, Nikko. Phoneme probability estimation with dynamic sparsely connected artiﬁcial neural  networks. The Free Speech Journal, 1(5):1–41, 1997.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.  Van Leeuwen, Jan. On the construction of huffman trees. In ICALP, pp. 382–410, 1976.  Van Nguyen, Hien, Zhou, Kevin, and Vemulapalli, Raviteja. Cross-domain synthesis of medical images using efﬁcient location-sensitive deep network. In Medical Image Computing and Computer- Assisted Intervention–MICCAI 2015, pp. 677–684. Springer, 2015.  Vanhoucke, Vincent, Senior, Andrew, and Mao, Mark Z. Improving the speed of neural networks on  cpus. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2011.  Yang, Zichao, Moczulski, Marcin, Denil, Misha, de Freitas, Nando, Smola, Alex, Song, Le, and  Wang, Ziyu. Deep fried convnets. arXiv preprint arXiv:1412.7149, 2014.  13  Published as a conference paper at ICLR 2016  A APPENDIX: DETAILED TIMING / POWER REPORTS OF DENSE & SPARSE  NETWORK LAYERS  Table 8: Average time on different layers. To avoid variance, we measured the time spent on each layer for 4096 input samples, and averaged the time regarding each input sample. For GPU, the time consumed by cudaMalloc and cudaMemcpy is not counted. For batch size = 1, gemv is used; For batch size = 64, gemm is used. For sparse case, csrmv and csrmm is used, respectively.  Time (us)  Titan X  Core i7-5930k  Tegra K1  dense (batch=1) sparse (batch=1) dense (batch=64) sparse (batch=64) dense (batch=1) sparse (batch=1) dense (batch=64) sparse (batch=64) dense (batch=1) sparse (batch=1) dense (batch=64) sparse (batch=64)  AlexNet FC6 541.5 134.8 19.8 94.6 7516.2 3066.5 318.4 1417.6 12437.2 2879.3 1663.6 4003.9  AlexNet FC7 243.0 65.8 8.9 51.5 6187.1 1282.1 188.9 682.1 5765.0 1256.5 2056.8 1372.8  AlexNet FC8 80.5 54.6 5.9 23.2 1134.9 890.5 45.8 407.7 2252.1 837.0 298.0 576.7  VGG16 FC6 1467.8 167.0 53.6 121.5 35022.8 3774.3 1056.0 1780.3 35427.0 4377.2 2001.4 8024.8  VGG16 FC7 243.0 39.8 8.9 24.4 5372.8 545.1 188.3 274.9 5544.3 626.3 2050.7 660.2  VGG16 FC8 80.5 48.0 5.9 22.0 774.2 777.3 45.7 363.1 2243.1 745.1 483.9 544.1  Table 9: Power consumption of different layers. We measured the Titan X GPU power with nvidia-smi, Core i7-5930k CPU power with pcm-power and Tegra K1 mobile GPU power with an external power meter (scaled to AP+DRAM, see paper discussion). During power measurement, we repeated each computation multiple times in order to get stable numbers. On CPU, dense matrix multiplications consume 2x energy than sparse ones because it is accelerated with multi-threading.  Power (Watts)  TitanX  Core i7-5930k  Tegra K1  dense (batch=1) sparse (batch=1) dense (batch=64) sparse (batch=64) dense (batch=1) sparse (batch=1) dense (batch=64) sparse (batch=64) dense (batch=1) sparse (batch=1) dense (batch=64) sparse (batch=64)  AlexNet FC6 157 181 168 156 83.5 42.3 85.4 37.2 5.1 5.9 5.6 5.0  AlexNet FC7 159 183 173 158 72.8 37.4 84.7 37.1 5.1 6.1 5.6 4.6  AlexNet FC8 159 162 166 163 77.6 36.5 101.6 38 5.4 5.8 6.3 5.1  VGG16 FC6 166 189 173 160 70.6 38.0 83.1 39.5 5.3 5.6 5.4 4.8  VGG16 FC7 163 166 173 158 74.6 37.4 97.1 36.6 5.3 6.3 5.6 4.7  VGG16 FC8 159 162 167 161 77.0 36.0 87.5 38.2 5.4 5.8 6.3 5.0  14  ",
1510.03009,2016,Neural Networks with Few Multiplications,"['Neural Networks with Few Multiplications\nZhouhan Lin', 'Matthieu Courbariaux', 'Roland Memisevic', 'Yoshua Bengio']",https://arxiv.org/pdf/1510.03009,"6 1 0 2     b e F 6 2         ]  G L . s c [      3 v 9 0 0 3 0  .  0 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  NEURAL NETWORKS WITH FEW MULTIPLICATIONS  Zhouhan Lin Universit´e de Montr´eal Canada zhouhan.lin@umontreal.ca  Matthieu Courbariaux Universit´e de Montr´eal Canada matthieu.courbariaux@gmail.com  Roland Memisevic Universit´e de Montr´eal Canada roland.umontreal@gmail.com  Yoshua Bengio Universit´e de Montr´eal Canada  ABSTRACT  For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on ﬂoating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 pop- ular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classiﬁcation performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardware- friendly training of neural networks.  1  INTRODUCTION  Training deep neural networks has long been computational demanding and time consuming. For some state-of-the-art architectures, it can take weeks to get models trained (Krizhevsky et al., 2012). Another problem is that the demand for memory can be huge. For example, many common models in speech recognition or machine translation need 12 Gigabytes or more of storage (Gulcehre et al., 2015). To deal with these issues it is common to train deep neural networks by resorting to GPU or CPU clusters and to well designed parallelization strategies (Le, 2013). Most of the computation performed in training a neural network are ﬂoating point multiplications. In this paper, we focus on eliminating most of these multiplications to reduce computation. Based on our previous work (Courbariaux et al., 2015), which eliminates multiplications in computing hidden representations by binarizing weights, our method deals with both hidden state computations and backward weight updates. Our approach has 2 components. In the forward pass, weights are stochastically binarized using an approach we call binary connect or ternary connect, and for back- propagation of errors, we propose a new approach which we call quantized back propagation that converts multiplications into bit-shifts. 1  2 RELATED WORK  Several approaches have been proposed in the past to simplify computations in neural networks. Some of them try to restrict weight values to be an integer power of two, thus to reduce all the mul- tiplications to be binary shifts (Kwan & Tang, 1993; Marchesi et al., 1993). In this way, multiplica- tions are eliminated in both training and testing time. The disadvantage is that model performance can be severely reduced, and convergence of training can no longer be guaranteed.  1The codes  for BinaryConnect  these approaches are available online at https://github.com/hantek/  1  Published as a conference paper at ICLR 2016  Kim & Paris (2015) introduces a completely Boolean network, which simpliﬁes the test time com- putation at an acceptable performance hit. The approach still requires a real-valued, full precision training phase, however, so the beneﬁts of reducing computations does not apply to training. Sim- ilarly, Machado et al. (2015) manage to get acceptable accuracy on sparse representation classiﬁ- cation by replacing all ﬂoating-point multiplications by integer shifts. Bit-stream networks (Burge et al., 1999) also provides a way of binarizing neural network connections, by substituting weight connections with logical gates. Similar to that, Cheng et al. (2015) proves deep neural networks with binary weights can be trained to distinguish between multiple classes with expectation back propagation. There are some other techniques, which focus on reducing the training complexity. For instance, instead of reducing the precision of weights, Simard & Graf (1994) quantizes states, learning rates, and gradients to powers of two. This approach manages to eliminate multiplications with negligible performance reduction.  3 BINARY AND TERNARY CONNECT  3.1 BINARY CONNECT REVISITED  In Courbariaux et al. (2015), we introduced a weight binarization technique which removes mul- tiplications in the forward pass. We summarize this approach in this subsection, and introduce an extension to it in the next. Consider a neural network layer with N input and M output units. The forward computation is y = h(W x + b) where W and b are weights and biases, respectively, h is the activation function, and x and y are the layer’s inputs and outputs. If we choose ReLU as h, there will be no multiplications in computing the activation function, thus all multiplications reside in the matrix product W x. For each input vector x, N M ﬂoating point multiplications are needed. Binary connect eliminates these multiplications by stochastically sampling weights to be −1 or 1. Full precision weights ¯w are kept in memory as reference, and each time when y is needed, we sample a stochastic weight matrix W according to ¯w. For each element of the sampled matrix W , the probability of getting a 1 is proportional to how “close” its corresponding entry in ¯w is to 1. i.e.,  P (Wij = 1) =  ¯wij + 1  2  ; P (Wij = −1) = 1 − P (Wij = 1)  (1)  It is necessary to add some edge constraints to ¯w. To ensure that P (Wij = 1) lies in a reasonable range, values in ¯w are forced to be a real value in the interval [-1, 1]. If during the updates any of its value grows beyond that interval, we set it to be its corresponding edge values −1 or 1. That way ﬂoating point multiplications become sign changes. A remaining question concerns the use of multiplications in the random number generator involved in the sampling process. Sampling an integer has to be faster than multiplication for the algorithm to be worth it. To be precise, in most cases we are doing mini-batch learning and the sampling process is performed only once for the whole mini-batch. Normally the batch size B varies up to several hundreds. So, as long as one sampling process is signiﬁcantly faster than B times of multiplications, it is still worth it. Fortunately, efﬁciently generating random numbers has been studied in Jeavons et al. (1994); van Daalen et al. (1993). Also, it is possible to get random numbers according to real random processes, like CPU temperatures, etc. We are not going into the details of random number generation as this is not the focus of this paper.  3.2 TERNARY CONNECT The binary connect introduced in the former subsection allows weights to be −1 or 1. However, in a trained neural network, it is common to observe that many learned weights are zero or close to zero. Although the stochastic sampling process would allow the mean value of sampled weights to be zero, this suggests that it may be beneﬁcial to explicitly allow weights to be zero. To allow weights to be zero, some adjustments are needed for Eq. 1. We split the interval of [-1, 1], within which the full precision weight value ¯wij lies, into two sub-intervals: [−1, 0] and (0, 1]. If a  2  Published as a conference paper at ICLR 2016  weight value ¯wij drops into one of them, we sample ¯wij to be the two edge values of that interval, according to their distance from ¯wij, i.e., if ¯wij > 0:  P (Wij = 1) = ¯wij; P (Wij = 0) = 1 − ¯wij  and if ¯wij <= 0:  P (Wij = −1) = − ¯wij; P (Wij = 0) = 1 + ¯wij  Like binary connect, ternary connect also eliminates all multiplications in the forward pass.  (2)  (3)  4 QUANTIZED BACK PROPAGATION  In the former section we described how multiplications can be eliminated from the forward pass. In this section, we propose a way to eliminate multiplications from the backward pass. Suppose the i-th layer of the network has N input and M output units, and consider an error signal δ propagating downward from its output. The updates for weights and biases would be the outer product of the layer’s input and the error signal:  ∆W = η  δ (cid:12) h (cid:48)  (W x + b)  xT  (4)  (cid:104) (cid:104)  δ (cid:12) h (cid:48)  (cid:105) (cid:105)  (5) where η is the learning rate, and x the input to the layer. The operator (cid:12) stands for element-wise multiply. While propagating through the layers, the error signal δ needs to be updated, too. Its update taking into account the next layer below takes the form:  (W x + b)  ∆b = η  δ =(cid:2)W T δ(cid:3) (cid:12) h  (cid:48)  (W x + b)  (6)  (cid:48) There are 3 terms that appear repeatedly in Eqs. 4 to 6: δ, h (W x + b) and x. The latter two terms introduce matrix outer products. To eliminate multiplications, we can quantize one of them to be an integer power of 2, so that multiplications involving that term become binary shifts. The expression (cid:48) (W x + b) contains downﬂowing gradients, which are largely determined by the cost function and h network parameters, thus it is hard to bound its values. However, bounding the values is essential for quantization because we need to supply a ﬁxed number of bits for each sampled value, and if that value varies too much, we will need too many bits for the exponent. This, in turn, will result in the need for more bits to store the sampled value and unnecessarily increase the required amount of computation.  (cid:48) While h (W x + b) is not a good choice for quantization, x is a better choice, because it is the hidden representation at each layer, and we know roughly the distribution of each layer’s activation. Our approach is therefore to eliminate multiplications in Eq. 4 by quantizing each entry in x to an integer power of 2. That way the outer product in Eq. 4 becomes a series of bit shifts. Experi- mentally, we ﬁnd that allowing a maximum of 3 to 4 bits of shift is sufﬁcient to make the network work well. This means that 3 bits are already enough to quantize x. As the ﬂoat32 format has 24 bits of mantissa, shifting (to the left or right) by 3 to 4 bits is completely tolerable. We refer to this approach of back propagation as “quantized back propagation.” If we choose ReLU as the activation function, and since we are reusing the (W x + b) that was (cid:48) computed during the forward pass, computing the term h (W x + b) involves no additional sampling or multiplications. In addition, quantized back propagation eliminates the multiplications in the outer product in Eq. 4. The only places where multiplications remain are the element-wise products. In Eq. 5, multiplying by η and σ requires 2 × M multiplications, while in Eq. 4 we can reuse the result of Eq. 5. To update δ would need another M multiplications, thus 3 × M multiplications  3  Published as a conference paper at ICLR 2016  are needed for all computations from Eqs. 4 through 6. Pseudo code in Algorithm 1 outlines how quantized back propagation is conducted.  Algorithm 1 Quantized Back Propagation (QBP). C is the cost function. binarize(W ) and clip(W ) stands for binarize and clip methods. L is the number of layers. Require: a deep model with parameters W , b at each layer. Input data x, its corresponding targets  1. Forward propagation: for each layer i in range(1, L) do Wb ← binarize(W ) Compute activation ai according to its previous layer output ai−1, Wb and b.  y, and learning rate η.  1: procedure QBP(model, x, y, η) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12:  ∂C  ∂ak−1  2. Backward propagation: Initialize output layer’s error signal δ = ∂C ∂aL for each layer i in range(L, 1) do  .  Compute ∆W and ∆b according to Eqs. 4 and 5. Update W : W ← clip(W − ∆W ) Update b: b ← b − ∆b Compute  by updating δ according to Eq. 6.  Like in the forward pass, most of the multiplications are used in the weight updates. Compared with standard back propagation, which would need 2M N + 3M multiplications at least, the amount of multiplications left is negligible in quantized back propagation. Our experiments in Section 5 show that this way of dramatically decreasing multiplications does not necessarily entail a loss in performance.  5 EXPERIMENTS  We tried our approach on both fully connected networks and convolutional networks. Our imple- mentation uses Theano (Bastien et al., 2012). We experimented with 3 datasets: MNIST, CIFAR10, and SVHN. In the following subsection we show the performance that these multiplier-light neural networks can achieve. In the subsequent subsections we study some of their properties, such as convergence and robustness, in more detail.  5.1 GENERAL PERFORMANCE  We tested different variations of our approach, and compare the results with Courbariaux et al. (2015) and full precision training (Table 1). All models are trained with stochastic gradient descent (SGD) without momentum. We use batch normalization for all the models to accelerate learning. At training time, binary (ternary) connect and quantized back propagation are used, while at test time, we use the learned full resolution weights for the forward propagation. For each dataset, all hyper-parameters are set to the same values for the different methods, except that the learning rate is adapted independently for each one.  Table 1: Performances across different datasets  Full precision Binary connect  MNIST CIFAR10 SVHN  1.33% 15.64% 2.85%  1.23% 12.04% 2.47%  Binary connect + Quantized backprop  1.29% 12.08% 2.48%  Ternary connect + Quantized backprop  1.15% 12.01% 2.42%  4  Published as a conference paper at ICLR 2016  5.1.1 MNIST  The MNIST dataset (LeCun et al., 1998) has 50000 images for training and 10000 for testing. All images are grey value images of size 28 × 28 pixels, falling into 10 classes corresponding to the 10 digits. The model we use is a fully connected network with 4 layers: 784-1024-1024-1024-10. At the last layer we use the hinge loss as the cost. The training set is separated into two parts, one of which is the training set with 40000 images and the other the validation set with 10000 images. Training is conducted in a mini-batch way, with a batch size of 200. With ternary connect, quantized backprop, and batch normalization, we reach an error rate of 1.15%. This result is better than full precision training (also with batch normalization), which yields an error rate 1.33%. If without batch normalization, the error rates rise to 1.48% and 1.67%, respectively. We also explored the performance if we sample those weights during test time. With ternary connect at test time, the same model (the one reaches 1.15% error rate) yields 1.49% error rate, which is still fairly acceptable. Our experimental results show that despite removing most multiplications, our approach yields a comparable (in fact, even slightly higher) performance than full precision training. The performance improvement is likely due to the regularization effect implied by the stochastic sampling. Taking this network as a concrete example, the actual amount of multiplications in each case can be estimated precisely. Multiplications in the forward pass is obvious, and for the backward pass section 4 has already given an estimation. Now we estimate the amount of multiplications incurred by batch normalization. Suppose we have a pre-hidden representation h with mini-batch size B on a layer which has M output units (thus h should have shape B × M), then batch normalization can be formalized as γ h−mean(h) std(h) + β. One need to compute the mean(h) over a mini-batch, which takes M multiplications, and BM + 2M multiplication to compute the standard deviation std(h). The fraction takes BM divisions, which should be equal to the same amount of multiplication. Multiplying that by the γ parameter, adds another BM multiplications. So each batch normalization layer takes an extra 3BM + 3M multiplications in the forward pass. The backward pass takes roughly twice as many multiplications in addition, if we use SGD. These amount of multiplications are the same no matter we use binarization or not. Bearing those in mind, the total amount of multiplications invoked in a mini-batch update are shown in Table 2. The last column lists the ratio of multiplications left, after applying ternary connect and quantized back propagation.  Table 2: Estimated number of multiplications in MNIST net  Full precision without BN 1.7480 × 109 1.7535 × 109 with BN  Ternary connect + Quantized backprop  1.8492 × 106 7.4245 × 106  ratio  0.001058 0.004234  5.1.2 CIFAR10 CIFAR10 (Krizhevsky & Hinton, 2009) contains images of size 32 × 32 RGB pixels. Like for MNIST, we split the dataset into 40000, 10000, and 10000 training-, validation-, and test-cases, respectively. We apply our approach in a convolutional network for this dataset. The network has 6 convolution/pooling layers, 1 fully connected layer and 1 classiﬁcation layer. We use the hinge loss for training, with a batch size of 100. We also tried using ternary connect at test time. On the model trained by ternary connect and quantized back propagation, it yields 13.54% error rate. Similar to what we observed in the fully connected network, binary (ternary) connect and quantized back propagation yield a slightly higher performance than ordinary SGD.  5.1.3 SVHN  The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) contains RGB images of house numbers. It contains more than 600,000 images in its extended training set, and roughly 26,000 images in its test set. We remove 6,000 images from the training set for validation. We use 7 layers of convolution/pooling, 1 fully connected layer, and 1 classiﬁcation layer. Batch size is also  5  Published as a conference paper at ICLR 2016  set to be 100. The performances we get is consistent with our results on CIFAR10. Extending the ternary connect mechanism to its test time yields 2.99% error rate on this dataset. Again, it improves over ordinary SGD by using binary (ternary) connect and quantized back propagation.  5.2 CONVERGENCE  Taking the convolutional networks on CIFAR10 as a test-bed, we now study the learning behaviour in more detail. Figure 1 shows the performance of the model in terms of test set errors during training. The ﬁgure shows that binarization makes the network converge slower than ordinary SGD, but yields a better optimum after the algorithm converges. Compared with binary connect (red line), adding quantization in the error propagation (yellow line) doesn’t hurt the model accuracy at all. Moreover, having ternary connect combined with quantized back propagation (green line) surpasses all the other three approaches.  Figure 1: Test set error rate at each epoch for ordinary back propagation, binary connect, binary connect with quantized back propagation, and ternary connect with quantized back propagation. Vertical axis is represented in logarithmic scale.  5.3 THE EFFECT OF BIT CLIPPING  In Section 4 we mentioned that quantization will be limited by the number of bits we use. The maximum number of bits to shift determines the amount of memory needed, but it also determines in what range a single weight update can vary. Figure 2 shows the model performance as a function of the maximum allowed bit shifts. These experiments are conducted on the MNIST dataset, with the aforementioned fully connected model. For each case of bit clipping, we repeat the experiment for 10 times with different initial random instantiations. The ﬁgure shows that the approach is not very sensible to the number of bits used. The maximum allowed shift in the ﬁgure varies from 2 bits to 10 bits, and the performance remains roughly the same. Even by restricting bit shifts to 2, the model can still learn successfully. The fact that the performance is not very sensitive to the maximum of allowed bit shifts suggests that we do not need to redeﬁne the number of bits used for quantizing x for different tasks, which would be an important practical advantage. The x to be quantized is not necessarily distributed symmetrically around 2. For example, Figure 3 shows the distribution of x at each layer in the middle of training. The maximum amount of shift to the left does not need to be the same as that on the right. A more efﬁcient way is to use different values for the maximum left shift and the maximum right shift. Bearing that in mind, we set it to 3 bits maximum to the right and 4 bits to the left.  6  Published as a conference paper at ICLR 2016  Figure 2: Model performance as a function of the maximum bit shifts allowed in quantized back propagation. The dark blue line indicates mean error rate over 10 independent runs, while light blue lines indicate their corresponding maximum and minimum error rates.  Figure 3: Histogram of representations at each layer while training a fully connected network for MNIST. The ﬁgure represents a snap-shot in the middle of training. Each subﬁgure, from bottom up, represents the histogram of hidden states from the ﬁrst layer to the last layer. The horizontal axes stand for the exponent of the layers’ representations, i.e., log2 x.  6 CONCLUSION AND FUTURE WORK  We proposed a way to eliminate most of the ﬂoating point multiplications used during training a feedforward neural network. This could make it possible to dramatically accelerate the training of neural networks by using dedicated hardware implementations. A somewhat surprising fact is that instead of damaging prediction accuracy the approach tends im- prove it, which is probably due to several facts. First is the regularization effect that the stochastic sampling process entails. Noise injection brought by sampling the weight values can be viewed as a regularizer, and that improves the model generalization. The second fact is low precision weight val- ues. Basically, the generalization error bounds for neural nets depend on the weights precision. Low precision prevents the optimizer from ﬁnding solutions that require a lot of precision, which corre- spond to very thin (high curvature) critical points, and these minima are more likely to correspond to overﬁtted solutions then broad minima (there are more functions that are compatible with such solutions, corresponding to a smaller description length and thus better generalization). Similarly,  7  Published as a conference paper at ICLR 2016  Neelakantan et al. (2015) adds noise into gradients, which makes the optimizer prefer large-basin areas and forces it to ﬁnd broad minima. It also lowers the training loss and improves generalization. Directions for future work include exploring actual implementations of this approach (for example, using FPGA), seeking more efﬁcient ways of binarization, and the extension to recurrent neural networks.  ACKNOWLEDGMENTS  The authors would like to thank the developers of Theano (Bastien et al., 2012). We acknowledge the support of the following agencies for research funding and computing support: Samsung, NSERC, Calcul Qu´ebec, Compute Canada, the Canada Research Chairs and CIFAR.  REFERENCES Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Burge, Peter S., van Daalen, Max R., Rising, Barry J. P., and Shawe-Taylor, John S. Stochastic bit- stream neural networks. In Maass, Wolfgang and Bishop, Christopher M. (eds.), Pulsed Neural Networks, pp. 337–352. MIT Press, Cambridge, MA, USA, 1999. ISBN 0-626-13350-4. URL http://dl.acm.org/citation.cfm?id=296533.296552.  Cheng, Zhiyong, Soudry, Daniel, Mao, Zexi, and Lan, Zhenzhong. Training binary multilayer arXiv preprint  neural networks for image classiﬁcation using expectation backpropagation. arXiv:1503.03562, 2015.  Courbariaux, Matthieu, Bengio, Yoshua, and David, Jean-Pierre. Binaryconnect: Training deep neu-  ral networks with binary weights during propagations. arXiv preprint arXiv:1511.00363, 2015.  Gulcehre, Caglar, Firat, Orhan, Xu, Kelvin, Cho, Kyunghyun, Barrault, Loic, Lin, Huei-Chi, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. On using monolingual corpora in neural machine translation. arXiv preprint arXiv:1503.03535, 2015.  Jeavons, Peter, Cohen, David A., and Shawe-Taylor, John. Generating binary sequences for stochas-  tic computing. Information Theory, IEEE Transactions on, 40(3):716–720, 1994.  Kim, Minje and Paris, Smaragdis. Bitwise neural networks. In Proceedings of The 31st International  Conference on Machine Learning, pp. 0–0, 2015.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images, 2009.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Kwan, Hon Keung and Tang, CZ. Multiplierless multilayer feedforward neural network design  suitable for continuous input-output mapping. Electronics Letters, 29(14):1259–1260, 1993.  Le, Quoc V. Building high-level features using large scale unsupervised learning.  In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8595– 8598. IEEE, 2013.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  Machado, Emerson Lopes, Miosso, Cristiano Jacques, von Borries, Ricardo, Coutinho, Murilo, Berger, Pedro de Azevedo, Marques, Thiago, and Jacobi, Ricardo Pezzuol. Computational cost reduction in learned transform classiﬁcations. arXiv preprint arXiv:1504.06779, 2015.  Marchesi, Michele, Orlandi, Gianni, Piazza, Francesco, and Uncini, Aurelio. Fast neural networks  without multipliers. Neural Networks, IEEE Transactions on, 4(1):53–62, 1993.  8  Published as a conference paper at ICLR 2016  Neelakantan, Arvind, Vilnis, Luke, Le, Quoc V, Sutskever, Ilya, Kaiser, Lukasz, Kurach, Karol, and Martens, James. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.  Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Read- ing digits in natural images with unsupervised feature learning. In NIPS workshop on deep learn- ing and unsupervised feature learning, pp. 5. Granada, Spain, 2011.  Simard, Patrice Y and Graf, Hans Peter. Backpropagation without multiplication. In Advances in  Neural Information Processing Systems, pp. 232–239, 1994.  van Daalen, Max, Jeavons, Pete, Shawe-Taylor, John, and Cohen, Dave. Device for generating  binary sequences for stochastic computing. Electronics Letters, 29(1):80–81, 1993.  9  ",
1511.06361,2016,Order-Embeddings of Images and Language,"['Order-Embeddings of Images and Language [code]\nIvan Vendrov', 'Ryan Kiros', 'Sanja Fidler', 'Raquel Urtasun']",https://arxiv.org/pdf/1511.06361,"6 1 0 2    r a  M 1         ]  G L . s c [      6 v 1 6 3 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  ORDER-EMBEDDINGS OF IMAGES AND LANGUAGE  Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun Department of Computer Science University of Toronto {vendrov,rkiros,fidler,urtasun}@cs.toronto.edu  ABSTRACT  Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierar- chy. Towards this goal, we introduce a general method for learning ordered repre- sentations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.  1  INTRODUCTION  Computer vision and natural language processing are becoming increasingly intertwined. Recent work in vision has moved beyond discriminating between a ﬁxed set of object classes, to automati- cally generating open-ended lingual descriptions of images (Vinyals et al., 2015). Recent methods for natural language processing such as Young et al. (2014) learn the semantics of language by grounding it in the visual world. Looking to the future, autonomous artiﬁcial agents will need to jointly model vision and language in order to parse the visual world and communicate with people. But what, precisely, is the relationship between images and the words or captions we use to describe them? It is akin to the hypernym relation between words, and textual entailment among phrases: captions are simply abstractions of images. In fact, all three relations can be seen as special cases of a partial order over images and language, illustrated in Figure 1, which we refer to as the visual- semantic hierarchy. As a partial order, this relation is transitive: “woman walking her dog”, “woman walking”, “person walking”, “person”, and “entity” are all valid abstractions of the rightmost image. Our goal in this work is to learn representations that respect this partial order structure.  Figure 1: A slice of the visual-semantic hierarchy  Most recent approaches to modeling the hypernym, entailment, and image-caption relations involve learning distributed representations or embeddings. This is a very powerful and general approach which maps the objects of interest—words, phrases, images— to points in a high-dimensional vector space. One line of work, exempliﬁed by Chopra et al. (2005) and ﬁrst applied to the caption-image relationship by Socher et al. (2014), requires the mapping to be distance-preserving: semantically  1  Published as a conference paper at ICLR 2016  similar objects are mapped to points that are nearby in the embedding space. A symmetric distance measure such as Euclidean or cosine distance is typically used. Since the visual-semantic hierarchy is an antisymmetric relation, we expect this approach to introduce systematic model error. Other approaches do not have such explicit constraints, learning a more-or-less general binary rela- tion between the objects of interest, e.g. Bordes et al. (2011); Socher et al. (2013); Ma et al. (2015). Notably, no existing approach directly imposes the transitivity and antisymmetry of the partial order, leaving the model to induce these properties from data. In contrast, we propose to exploit the partial order structure of the visual-semantic hierarchy by learning a mapping which is not distance-preserving but order-preserving between the visual- semantic hierarchy and a partial order over the embedding space. We call embeddings learned in this way order-embeddings. This idea can be integrated into existing relational learning meth- ods simply by replacing their comparison operation with ours. By modifying existing methods in this way, we ﬁnd that order-embeddings provide a marked improvement over the state-of-art for hypernymy prediction and caption-image retrieval, and near state-of-the-art performance for natural language inference. This paper is structured as follows. We begin, in Section 2, by giving a uniﬁed mathematical treat- ment of our tasks, and describing the general approach of learning order-embeddings. In the next three sections we describe in detail the tasks we tackle, how we apply the order-embeddings idea to each of them, and the results we obtain. The tasks are hypernym prediction (Section 3), caption- image retrieval (Section 4), and textual entailment (Section 5). In the supplementary material, we visualize novel vector regularities that emerge in our learned embeddings of images and language.  2 LEARNING ORDER-EMBEDDINGS  To unify our treatment of various tasks, we introduce the problem of partial order completion. In partial order completion, we are given a set of positive examples P = {(u, v)} of ordered pairs drawn from a partially ordered set (X,(cid:22)X ), and a set of negative examples N which we know to be unordered. Our goal is to predict whether an unseen pair (u(cid:48), v(cid:48)) is ordered. Note that hypernym prediction, caption-image retrieval, and textual entailment are all special cases of this task, since they all involve classifying pairs of concepts in the (partially ordered) visual-semantic hierarchy. We tackle this problem by learning a mapping from X into a partially ordered embedding space (Y,(cid:22)Y ). The idea is to predict the ordering of an unseen pair in X based on its ordering in the embedding space. This is possible only if the mapping satisﬁes the following crucial property: Deﬁnition 1. A function f : (X,(cid:22)X ) → (Y,(cid:22)Y ) is an order-embedding if for all u, v ∈ X,  u (cid:22)X v if and only if f (u) (cid:22)Y f (v)  This deﬁnition implies that each combination of embedding space Y , order (cid:22)Y , and order- embedding f determines a unique completion of our data as a partial order (cid:22)X. In the following, we ﬁrst consider the choice of Y and (cid:22)Y , and then discuss how to ﬁnd an appropriate f.  +  2.1 THE REVERSED PRODUCT ORDER ON RN The choice of Y and (cid:22)Y is somewhat application-dependent. For the purpose of modeling the semantic hierarchy, our choices are narrowed by the following considerations. Much of the expressive power of human language comes from abstraction and composition. For any two concepts, say “dog” and “cat”, we can name a concept that is an abstraction of the two, such as “mammal”, as well as a concept that composes the two, such as “dog chasing cat”. So, in order to represent the visual-semantic hierarchy, we need to choose an order (cid:22)Y that is rich enough to embed these two relations. We also restrict ourselves to orders (cid:22)Y with a top element, which is above every other element in the order. In the visual-semantic hierarchy, this element represents the most general possible concept; practically, it provides an anchor for the embedding.  2  Published as a conference paper at ICLR 2016  N(cid:94)  Finally, we choose the embedding space Y to be continuous in order to allow optimization with gradient-based methods. A natural choice that satisﬁes all three properties is the reversed product order on RN the conjunction of total orders on each coordinate:  + , deﬁned by  x (cid:22) y if and only if  xi ≥ yi  (1)  i=1  for all vectors x, y with nonnegative coordinates. Note the reversal of direction: smaller coordinates imply higher position in the partial order. The origin is then the top element of the order, representing the most general concept. Instead of viewing our embeddings as single points x ∈ RN + , we can also view them as sets {y : x (cid:22) y}. The meaning of a word is then the union of all concepts of which it is a hypernym, and the meaning of a sentence is the union of all sentences that entail it. The visual-semantic hierarchy can then be seen as a special case of the subset relation, a connection also used by Young et al. (2014).  2.2 PENALIZING ORDER VIOLATIONS  Having ﬁxed the embedding space and order, we now consider the problem of ﬁnding an order- embedding into this space. In practice, the order embedding condition (Deﬁnition 1) is too restrictive to impose as a hard constraint. Instead, we aim to ﬁnd an approximate order-embedding: a mapping which violates the order-embedding condition, imposed as a soft constraint, as little as possible. More precisely, we deﬁne a penalty that measures the degree to which a pair of points violates the product order. In particular, we deﬁne the penalty for an ordered pair (x, y) of points in RN  E(x, y) = || max(0, y − x)||2  (2) Crucially, E(x, y) = 0 ⇐⇒ x (cid:22) y according to the reversed product order; if the order is not satisﬁed, E(x, y) is positive. This effectively imposes a strong prior on the space of relations, en- couraging our learned relation to satisfy the partial order properties of transitivity and antisymmetry. This penalty is key to our method. Throughout the remainder of the paper, we will use it where previous work has used symmetric distances or learned comparison operators. Recall that P and N are our positive and negative examples, respectively. Then, to learn an approx- imate order-embedding f, we could use a max-margin loss which encourages positive examples to have zero penalty, and negative examples to have penalty greater than a margin:  + as  E(f (u), f (v)) +  max{0, α − E(f (u(cid:48)), f (v(cid:48)))}  (3)  (cid:88)  (u(cid:48),v(cid:48))∈N  (cid:88)  (u,v)∈P  In practice we are often not given negative examples, in which case this loss admits the trivial solution of mapping all objects to the same point. The best way of dealing with this problem depends on the application, so we will describe task-speciﬁc variations of the loss in the next several sections.  3 HYPERNYM PREDICTION  To test the ability of our model to learn partial orders from incomplete data, our ﬁrst task is to predict withheld hypernym pairs in WordNet (Miller, 1995). A hypernym pair is a pair of concepts where the ﬁrst concept is a specialization or an instance of the second, e.g., (woman, person) or (New York, city). Our setup differs signiﬁcantly from previous work in that we use only the WordNet hierarchy as training data. The most similar evaluation has been that of Baroni et al. (2012), who use external linguistic data in the form of distributional semantic vectors. Bordes et al. (2011) and Socher et al. (2013) also evaluate on the WordNet hierarchy, but they use other relations in WordNet as training data (and external linguistic data, in Socher’s case). Additionally, the latter two consider only direct hypernyms, rather than the full, transitive hyper- nymy relation. But predicting the transitive hypernym relation is a better-deﬁned problem because individual hypernym edges in WordNet vary dramatically in the degree of abstraction they require. For instance, (person, organism) is a direct hypernym pair, but it takes eight hypernym edges to get from cat to organism.  3  Published as a conference paper at ICLR 2016  3.1 LOSS FUNCTION  To apply order-embeddings to hypernymy, we follow the setup of Socher et al. (2013) in learning an N-dimensional vector for each concept in WordNet, but we replace their neural tensor network with our order-violation penalty deﬁned in Eq. (2). Just like them, we corrupt each hypernym pair by replacing one of the two concepts with a randomly chosen concept, and use these corrupted pairs as negative examples for both training and evaluation. We use their max-margin loss, which encourages the order-violation penalty to be zero for positive examples, and greater than a margin α  E(f (u), f (v)) + max{0, α − E(f (u(cid:48)), f (v(cid:48)))}  (4)  for negative examples: (cid:88)  (u,v)∈W ordN et  where E is our order-violation penalty, and (u(cid:48), v(cid:48)) is a corrupted version of (u, v). Since we learn an independent embedding for each concept, the mapping f is simply a lookup table.  3.2 DATASET  The transitive closure of the WordNet hierarchy gives us 838073 edges between 82192 concepts in WordNet. Like Bordes et al. (2011), we randomly select 4000 edges for the test split, and another 4000 for the development set. Note that the majority of test set edges can be inferred simply by applying transitivity, giving us a strong baseline.  3.3 DETAILS OF TRAINING  We learn a 50-dimensional nonnegative vector for each concept in WordNet using the max-margin objective (4) with margin α = 1, sampling 500 true and 500 false hypernym pairs in each batch. We train for 30-50 epochs using the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.01 and early stopping on the validation set. During evaluation, we ﬁnd the optimal classiﬁcation threshold on the validation set, then apply it to the test set.  3.4 RESULTS  Since our setup is novel, there are no published numbers to compare to. We therefore compare three variants of our model to two baselines, with results shown in Table 1. The transitive closure baseline involves no learning; it simply classiﬁes hypernyms pairs as positive if they are in the transitive closure of the union of edges in the train- ing and validation sets. The word2gauss baseline eval- uates the approach of Vilnis & McCallum (2015) to rep- resent words as Gaussian densities rather than points in the embedding space. This allows a natural representa- tion of hierarchies using the KL divergence. We used 50-dimensional diagonal Gaussian embeddings, trained for 200 epochs on a max-margin objective with margin 7, chosen by grid search1. order-embeddings (symmetric) is our full model, but using symmetric cosine distance instead of our asymmet- ric penalty. order-embeddings (bilinear) replaces our penalty with the bilinear model used by Socher et al. (2013). order-embeddings is our full model. Only our full model can do better than the transitive baseline, showing the value of exploiting partial order structure in contrast to using symmetric similarity or learning a general binary relation as most previous work and our bilinear baseline do. The resulting 50-dimensional embeddings are difﬁcult to visualize. To give some intuition for the structure being learned, Figure 2 shows the results of a toy 2D experiment.  Figure 2: 2-dim order-embedding of a small subset of the WordNet hypernym rela- tion. All the true hypernym pairs (green ar- rows) are correctly embedded, but two spu- rious pairs (pink arrows), are introduced. Only direct hypernyms are shown.  1We used the code of http://github.com/seomoz/word2gauss  4  Published as a conference paper at ICLR 2016  Method  Accuracy (%)  transitive closure word2gauss order-embeddings (symmetric) order-embeddings (bilinear) order-embeddings  88.2 86.6 84.2 86.3 90.6  Table 1: Binary classiﬁcation accuracy on 4000 withheld edges from WordNet.  4 CAPTION-IMAGE RETRIEVAL  The caption-image retrieval task has become a standard evaluation of joint models of vision and language (Hodosh et al., 2013; Lin et al., 2014a). The task involves ranking a large dataset of images by relevance for a query caption (Image Retrieval), and ranking captions by relevance for a query image (Caption Retrieval). Given a set of aligned image-caption pairs as training data, the goal is then to learn a caption-image compatibility score S(c, i) to be used at test time. Many modern approaches model the caption-image relationship symmetrically, either by embedding into a common “visual-semantic” space with inner-product similarity (Socher et al., 2014; Kiros et al., 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al., 2015). While Karpathy & Li (2015) and Plummer et al. (2015) model a ﬁner-grained alignment between regions in the image and segments of the caption, the similarity they use is still symmetric. An alternative is to learn an unconstrained binary relation, either with a neural language model conditioned on the image (Vinyals et al., 2015; Mao et al., 2015) or using a multimodal CNN (Ma et al., 2015). In contrast to these lines of work, we propose to treat the caption-image pairs as a two-level partial order with captions above the images they describe, and let  S(c, i) = −E(fi(i), fc(c))  with E our order-violation penalty deﬁned in Eq (2), and fc, fi are embedding functions from cap- + . tions and images into RN  4.1 LOSS FUNCTION  To facilitate comparison, we use the same pairwise ranking loss that Socher et al. (2014), Kiros et al. (2014) and Karpathy & Li (2015) have used on this task—simply replacing their symmetric similar- ity measure with our asymmetric order-violation penalty. This loss function encourages S(c, i) for ground truth caption-image pairs to be greater than that for all other pairs, by a margin:  max{0, α − S(c, i) + S(c(cid:48), i)} +  max{0, α − S(c, i) + S(c, i(cid:48))}  (5)  (c,i)  c(cid:48)  i(cid:48)  where (c, i) is a ground truth caption-image pair, c(cid:48) goes over captions that no describe i, and i(cid:48) goes over image not described by c.  4.2  IMAGE AND CAPTION EMBEDDINGS  To learn fc and fi, we use the approach of Kiros et al. (2014) except, since we are embedding into RN + , we constrain the embedding vectors to have nonnegative entries by taking their absolute value. Thus, to embed images, we use (6) where Wi is a learned N × 4096 matrix, N being the dimensionality of the embedding space. CN N (i) is the same image feature used by Klein et al. (2015): we rescale images to have smallest side 256 pixels, we take 224 × 224 crops from the corners, center, and their horizontal reﬂections, run the 10 crops through the 19-layer VGG network of Simonyan & Zisserman (2015) (weights pre-trained on ImageNet and ﬁxed during training), and average their fc7 features.  fi(i) = |Wi · CN N (i)|  5  (cid:32)(cid:88)  (cid:88)  (cid:88)  (cid:33)  Published as a conference paper at ICLR 2016  Model  Caption Retrieval  R@1 R@10 Med  r  Mean  r  Image Retrieval  R@1 R@10 Med  r  MNLM (Kiros et al., 2014) m-RNN (Mao et al., 2015) DVSA (Karpathy & Li, 2015) STV (Kiros et al., 2015) FV (Klein et al., 2015) m-CNN (Ma et al., 2015) m-CNNEN S order-embeddings (reversed) order-embeddings (1-crop) order-embeddings (symm.) order-embeddings  DVSA FV order-embeddings (symm.) order-embeddings  43.4 41.0 38.4  33.8 39.4 38.3 42.8 11.2 41.4 45.4 46.7  11.8 17.3 21.5 23.3  85.8 83.5 80.5  82.1 80.9 81.0 84.1 44.0 84.2 88.7 88.9  45.4 50.2 62.9 65.0  2 2 1  3 2 2 2  14.2 2.0 2.0 2.0  12.2 10.0 6.0 5.0  1k Test Images 31.0 29.0 27.4  * * *  *  * *  10.4  25.9 25.1 27.4 32.6 12.3 86.6 33.5 8.7 36.3 5.8 5.7 37.9 5k Test Images  *  46.4 24.4 24.4  8.9 10.8 16.8 18.0  79.9 77.0 74.8  74.6 76.6 79.5 82.8 53.5 82.2 85.8 85.9  36.3 40.1 56.3 57.6  3 3 3  4 4 3 3 9.0 2.6 2.0 2.0  19.5 17.0 8.0 7.0  Mean  r  * * *  11.1  *  * *  30.1 10.0 9.0 8.1  *  49.3 40.4 35.9  Table 2: Results of caption-image retrieval evaluation on COCO. R@K is Recall@K, in %. Med r is median rank. Metrics for our models on 1k test images are averages over ﬁve 1000-image splits of the 5000-image test set, as in (Klein et al., 2015). Best results overall are in bold; best results using 1-crop VGG features are underlined.  To embed the captions, we use a recurrent neural net encoder with GRU activations (Cho et al., 2014), so fc(c) = |GRU (c)|, the absolute value of hidden state after processing the last word.  4.3 DATASET  We evaluate on the Microsoft COCO dataset (Lin et al., 2014b), which has over 120,000 images, each with at least ﬁve human-annotated captions per image. This is by far the largest dataset com- monly used for caption-image retrieval. We use the data splits of Karpathy & Li (2015) for training (113,287 images), validation (5000 images), and test (5000 images).  4.4 DETAILS OF TRAINING  To train the model, we use the standard pairwise ranking objective from Eq. (5). We sample mini- batches of 128 random image-caption pairs, and draw all contrastive terms from the minibatch, giving us 127 contrastive images for each caption and captions for each image. We train for 15-30 epochs using the Adam optimizer with learning rate 0.001, and early stopping on the validation set. We set the dimension of the embedding space and the GRU hidden state N to 1024, the dimension of the learned word embeddings to 300, and the margin α to 0.05. All these hyperparameters, as well as the learning rate and batchsize, were selected using the validation set. For consistency with Kiros et al. (2014) and to mitigate overﬁtting, we constrain the caption and image embeddings to have unit L2 norm. This constraint implies that no two points can be exactly ordered with zero order-violation penalty, but since we use a ranking loss, only the relative size of the penalties matters.  4.5 RESULTS  Given a query caption or image, we sort all the images or captions of the test set in order of increasing penalty. We use standard ranking metrics for evaluation. We measure Recall@K, the percent of queries for which the GT term is one of the ﬁrst K retrieved; and median and mean rank, which are statistics over the position of the GT term in the retrieval order.  6  Published as a conference paper at ICLR 2016  Table 2 shows a comparison between all state-of-the-art and some older methods2 along with our own; see Ma et al. (2015) for a more complete listing. The best results overall are in bold, and the best results using 1-crop VGG image features are under- lined. Note that the comparison is additionally complicated by the following:  • m-CNNEN S is an ensemble of four different models, whereas the other entries are all  single models.  • STV and FV use external text corpora to learn their language features, whereas the other  methods learn them from scratch.  To facilitate the comparison and to evaluate the contributions of various components of our model, we evaluate four variations of order-embeddings: order-embeddings is our full model as described above. order-embeddings (reversed) reverses the order of captions and image embeddings in our order- violation penalty—placing images above captions in the partial order learned by our model. This seemingly slight variation performs atrociously, conﬁrming our prior that captions are much more abstract than images, and should be placed higher in the semantic hierarchy. order-embeddings (1-crop) computes the image feature using just the center crop, instead of aver- aging over 10 crops. order-embeddings (symm.) replaces our asymmetric penalty with the symmetric cosine distance, and allows embedding coordinates to be negative—essentially replicating MNLM, but with better image features. Here we ﬁnd that a different margin (α = 0.2) works best. Between these four models, the only previous work whose results are incommensurable with ours is DVSA, since it uses the less discriminative CNN of Krizhevsky et al. (2012) but 20 region features instead of a single whole-image feature. Aside from this limitation, and if only single models are considered, order-embeddings signiﬁcantly outperform the state-of-art approaches for image retrieval even when we control for image features.  4.6 EXPLORATION  Why would order-embeddings do well on such a shallow partial order? Why are they much more helpful for image retrieval than for caption retrieval? Intuitively, symmetric similarity should fail when an image has captions with very different levels of detail, because the captions are so dissimilar that it is impossible to map both their embeddings close to the same image embedding. Order-embeddings don’t have this problem: the less detailed caption can be embedded very far away from the image while remaining above it in the partial order. To evaluate this intuition, we use caption length as a proxy for level of detail and select, among pairs of co-referring captions in our validation set, the 100 pairs with the biggest length difference. For image retrieval with 1000 target images, the mean rank over captions in this set is 6.4 for order- embeddings and 9.7 for cosine similarity, a much bigger difference than over the entire dataset. Some particularly dramatic examples of this are shown in Figure 3. Moreover, if we use the shorter caption as a query, and retrieve captions in order of increasing error, the mean rank of the longer caption is 34.0 for order-embeddings and 47.6 for cosine similarity, showing that order-embeddings are able to capture the relatedness of co-referring captions with very different lengths. This also explains why order-embeddings provide a much smaller improvement for caption retrieval than for image retrieval: all the caption retrieval metrics are based on the position of the ﬁrst ground truth caption in the retrieval order, so the embeddings need only learn to retrieve one of each image’s ﬁve captions well, which symmetric similarity is well suited for.  2Note that the numbers for MNLM come not from the published paper but from the recently released code  at http://github.com/ryankiros/visual-semantic-embedding.  7  Published as a conference paper at ICLR 2016  Captions  Image rank  cosine  order-emb  a sitting area with furniture and ﬂowers makes a back- drop for a boy with headphones sitting in the fore- ground at one of the chairs at a dining table that holds glasses and a handbag working at a laptop  a kid is wearing headphone while on a laptop  view of top of a white building with tan speckled area an uncovered awning with a pigeon in ﬁght below and a red umbrella behind balcony wall  a pigeon ﬂying near white beams of a building  4  286  3  91  8  24  5  6  Figure 3: Images with captions of very different lengths, and the rank of the GT image when using each caption as a query.  5 TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE  Natural language inference can be seen as a generalization of hypernymy from words to sentences. For example, from “woman walking her dog in a park” we can infer both “woman walking her dog” and “dog in a park”, but not ”old woman” or ”black dog”. Given a pair of sentences, our task is to predict whether we can infer the second sentence (the hypothesis) from the ﬁrst (the premise).  5.1 LOSS FUNCTION  (cid:88)  (cid:88)  To apply order-embeddings to this task, we again view it as partial order completion—we can infer a hypothesis from a premise exactly when the hypothesis is above the premise in the visual-semantic hierarchy. Unlike our other tasks, for which we had to generate contrastive negatives, datasets for natural language inference include labeled negative examples. So, we can simply use a max-margin loss:  E(f (p), f (h)) +  max{0, α − E(f (p(cid:48)), f (h(cid:48)))}  (7)  (p,h)  (p(cid:48),h(cid:48))  where (p, h) are positive and (p(cid:48), h(cid:48)) negative pairs of premise and hypothesis. To embed sentences, we use the same GRU encoder as in the caption-image retrieval task.  5.2 DATASET  To evaluate order-embeddings on the natural language inference task, we use the recently proposed SNLI corpus (Bowman et al., 2015), which contains 570,000 pairs of sentences, each labeled with “entailment” if the inference is valid, “contradiction” if the two sentences contradict, or “neutral” if the inference is invalid but there is no contradiction. Our method only allows us to discrimi- nate between entailment and non-entailment, so we merge the “contradiction” and “neutral” classes together to serve as our negative examples.  5.3  IMPLEMENTATION DETAILS  Just as for caption-image ranking, we set the dimensions of the embedding space and GRU hidden state to be 1024, the dimension of the word embeddings to be 300, and constrain the embeddings to have unit L2 norm. We train for 10 epochs with batches of 128 sentence pairs. We use the Adam optimizer with learning rate 0.001 and early stopping on the validation set. During evaluation, we ﬁnd the optimal classiﬁcation threshold on validation, then use the threshold to classify the test set.  8  Published as a conference paper at ICLR 2016  Method  2-class  3-class  Neural Attention (Rockt¨aschel et al., 2015) EOP classiﬁer (Bowman et al., 2015) skip-thoughts order-embeddings (symmetric) order-embeddings  *  75.0 87.7 79.3 88.6  Table 3: Test accuracy (%) on SNLI.  83.5 *  81.5  * *  5.4 RESULTS  The state-of-the-art method for 3-class classiﬁcation on SNLI is that of Rockt¨aschel et al. (2015). Unfortunately, they do not compute 2-class accuracy, so we cannot compare to them directly. As a bridge to facilitate comparison, we use a challenging baseline which can be evaluated on both the 2-class and 3-class problems. The baseline, referred to as skip-thoughts, involves a feedfor- ward neural network on top of skip-thought vectors (Kiros et al., 2015), a state-of-the-art semantic representation of sentences. Given pairs of sentence vectors u and v, the input to the network is the concatenation of u, v and the absolute difference |u − v|. We tuned the number of layers, layer dimensionality and dropout rates to optimize performance on the development set, using the Adam optimizer. Batch normalization (Ioffe & Szegedy, 2015) and PReLU units (He et al., 2015) were used. Our best network used 2 hidden layers of 1000 units each, with dropout rate of 0.5 across both the input and hidden layers. We did not backpropagate through the skip-thought encoder. We also evaluate against EOP classiﬁer, a 2-class baseline introduced by (Bowman et al., 2015), and against a version of our model where our order-violation penalty is replaced with the symmetric cosine distance, order-embeddings (symmetric). The results for all models are shown in Table 3. We see that order-embeddings outperform the skip- thought baseline despite not using external text corpora. While our method is almost certainly worse than the state-of-the-art method of Rockt¨aschel et al. (2015), which uses a word-by-word attention mechanism, it is also much simpler.  6 CONCLUSION AND FUTURE WORK  We introduced a simple method to encode order into learned distributed representations, which al- lows us to explicitly model the partial order structure of the visual-semantic hierarchy. Our method can be easily integrated into existing relational learning methods, as we demonstrated on three chal- lenging tasks involving computer vision and natural language processing. On two of these tasks, hypernym prediction and caption-image retrieval, our methods outperform all previous work. A promising direction of future work is to learn better classiﬁers on ImageNet (Deng et al., 2009), which has over 21k image classes arranged by the WordNet hierarchy. Previous approaches, includ- ing Frome et al. (2013) and Norouzi et al. (2014) have embedded words and images into a shared semantic space with symmetric similarity—which our experiments suggest to be a poor ﬁt with the partial order structure of WordNet. We expect signiﬁcant progress on ImageNet classiﬁcation, and the related problems of one-shot and zero-shot learning, to be possible using order-embeddings. Going further, order-embeddings may enable learning the entire semantic hierarchy in a single model which jointly reasons about hypernymy, entailment, and the relationship between perception and language, unifying what have been until now almost independent lines of work.  ACKNOWLEDGMENTS  We thank Kaustav Kundu for many fruitful discussions throughout the development of this paper. The work was supported in part by an NSERC Graduate Scholarship.  9  Published as a conference paper at ICLR 2016  REFERENCES Baroni, Marco, Bernardi, Raffaella, Do, Ngoc-Quynh, and Shan, Chung-chieh. Entailment above  the word level in distributional semantics. In EACL, 2012.  Bordes, Antoine, Weston, Jason, Collobert, Ronan, and Bengio, Yoshua. Learning structured em-  beddings of knowledge bases. In AAAI, 2011.  Bowman, Samuel R., Angeli, Gabor, Potts, Christopher, and Manning, Christopher D. A large  annotated corpus for learning natural language inference. In EMNLP, 2015.  Cho, Kyunghyun, Van Merri¨enboer, Bart, Gulcehre, Caglar, Bahdanau, Dzmitry, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using rnn encoder- decoder for statistical machine translation. In EMNLP, 2014.  Chopra, Sumit, Hadsell, Raia, and LeCun, Yann. Learning a similarity metric discriminatively, with  application to face veriﬁcation. In CVPR, 2005.  Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale  hierarchical image database. In CVPR, 2009.  Frome, Andrea, Corrado, Greg S, Shlens, Jon, Bengio, Samy, Dean, Jeff, Mikolov, Tomas, et al.  Devise: A deep visual-semantic embedding model. In NIPS, 2013.  He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers: Surpass-  ing human-level performance on imagenet classiﬁcation. ICCV, 2015.  Hodosh, Micah, Young, Peter, and Hockenmaier, Julia. Framing image description as a ranking  task: Data, models and evaluation metrics. JAIR, 2013.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by  reducing internal covariate shift. ICML, 2015.  Karpathy, Andrej and Li, Fei-Fei. Deep visual-semantic alignments for generating image descrip-  tions. In CVPR, 2015.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. In ICLR, 2015.  Kiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S. Unifying visual-semantic embeddings  with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.  Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan, Zemel, Richard S, Torralba, Antonio, Urtasun,  Raquel, and Fidler, Sanja. Skip-thought vectors. NIPS, 2015.  Klein, Benjamin, Lev, Guy, Sadeh, Gil, and Wolf, Lior. Associating neural word embeddings with  deep image representations using ﬁsher vectors. In CVPR, 2015.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, 2012.  Lin, Dahua, Fidler, Sanja, Kong, Chen, and Urtasun, Raquel. Visual semantic search: Retrieving videos via complex textual queries. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014a.  Lin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Doll´ar, Piotr, and Zitnick, C Lawrence. Microsoft coco: Common objects in context. In ECCV, 2014b.  Ma, Lin, Lu, Zhengdong, Shang, Lifeng, and Li, Hang. Multimodal convolutional neural networks  for matching image and sentence. ICCV, 2015.  Mao, Junhua, Xu, Wei, Yang, Yi, Wang, Jiang, and Yuille, Alan. Deep captioning with multimodal  recurrent neural networks (m-rnn). In ICLR, 2015.  Mikolov, Tomas, Yih, Wen-tau, and Zweig, Geoffrey. Linguistic regularities in continuous space  word representations. In HLT-NAACL, pp. 746–751, 2013.  10  Published as a conference paper at ICLR 2016  Miller, George A. Wordnet: a lexical database for english. Communications of the ACM, 1995.  Norouzi, Mohammad, Mikolov, Tomas, Bengio, Samy, Singer, Yoram, Shlens, Jonathon, Frome, Andrea, Corrado, Greg S, and Dean, Jeffrey. Zero-shot learning by convex combination of se- mantic embeddings. In ICLR, 2014.  Plummer, Bryan, Wang, Liwei, Cervantes, Chris, Caicedo, Juan, Hockenmaier, Julia, and Lazebnik, Svetlana. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to- sentence models. arXiv preprint arXiv:1505.04870, 2015.  Rockt¨aschel, Tim, Grefenstette, Edward, Hermann, Karl Moritz, Koˇcisk`y, Tom´aˇs, and Blunsom, Phil. Reasoning about entailment with neural attention. arXiv preprint arXiv:1509.06664, 2015.  Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recogni-  tion. In ICLR, 2015.  Socher, Richard, Chen, Danqi, Manning, Christopher D, and Ng, Andrew. Reasoning with neural  tensor networks for knowledge base completion. In NIPS, 2013.  Socher, Richard, Karpathy, Andrej, Le, Quoc V, Manning, Christopher D, and Ng, Andrew Y. Grounded compositional semantics for ﬁnding and describing images with sentences. TACL, 2014.  Van der Maaten, Laurens and Hinton, Geoffrey. Visualizing data using t-sne. Journal of Machine  Learning Research, 9(2579-2605):85, 2008.  Vilnis, Luke and McCallum, Andrew. Word representations via gaussian embedding. In ICLR, 2015.  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural  image caption generator. In CVPR, 2015.  Young, Peter, Lai, Alice, Hodosh, Micah, and Hockenmaier, Julia. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2014.  11  Published as a conference paper at ICLR 2016  7 SUPPLEMENTARY MATERIAL  Mikolov et al. (2013) showed that word representations learned using word2vec exhibit semantic regularities, such as king − man + woman ∼ queen. Kiros et al. (2014) showed that similar regularities hold for joint image-language models. We ﬁnd that order-embeddings exhibit a novel form of regularity, shown in Figure 4. The elementwise max and min operations in the embedding space roughly correspond to composition and abstraction, respectively.  Figure 4: Multimodal regularities found with embeddings learned for the caption-image retrieval task. Note that some images have been slightly cropped for easier viewing, but no relevant objects have been removed.  12  Nearest non-query images in COCO trainmax(“man”, “cat”)  max(“black dog”, “park”) Querymax(               ),min(               ),min(               ),“dog”max(               ),“man”",
1511.02793,2016,Generating Images from Captions with Attention,"['Generating Images from Captions with Attention\nElman Mansimov', 'Emilio Parisotto', 'Jimmy Ba', 'Ruslan Salakhutdinov']",https://arxiv.org/pdf/1511.02793,"6 1 0 2     b e F 9 2         ]  G L . s c [      2 v 3 9 7 2 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  GENERATING IMAGES FROM CAPTIONS WITH ATTENTION  Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba & Ruslan Salakhutdinov Department of Computer Science University of Toronto Toronto, Ontario, Canada {emansim,eparisotto,rsalakhu}@cs.toronto.edu, jimmy@psi.utoronto.ca  ABSTRACT  Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model itera- tively draws patches on a canvas, while attending to the relevant words in the de- scription. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demon- strate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously un- seen captions in the dataset.  1  INTRODUCTION  Statistical natural image modelling remains a fundamental problem in computer vision and image understanding. The challenging nature of this task has motivated recent approaches to exploit the inference and generative capabilities of deep neural networks. Previously studied deep generative models of images often deﬁned distributions that were restricted to being either unconditioned or conditioned on classiﬁcation labels. In real world applications, however, images rarely appear in isolation as they are often accompanied by unstructured textual descriptions, such as on web pages and in books. The additional information from these descriptions could be used to simplify the image modelling task. Moreover, learning generative models conditioned on text also allows a better understanding of the generalization performance of the model, as we can create textual descriptions of completely new scenes not seen at training time. There are numerous ways to learn a generative model over both image and text modalities. One approach is to learn a generative model of text conditioned on images, known as caption generation (Kiros et al., 2014a; Karpathy & Li, 2015; Vinyals et al., 2015; Xu et al., 2015). These models take an image descriptor and generate unstructured texts using a recurrent decoder. In contrast, in this paper we explore models that condition in the opposite direction, i.e. taking textual descriptions as input and using them to generate relevant images. Generating high dimensional realistic images from their descriptions combines the two challenging components of language modelling and image generation, and can be considered to be more difﬁcult than caption generation. In this paper, we illustrate how sequential deep learning techniques can be used to build a conditional probabilistic model over natural image space effectively. By extending the Deep Recurrent Atten- tion Writer (DRAW) (Gregor et al., 2015), our model iteratively draws patches on a canvas, while attending to the relevant words in the description. Overall, the main contributions of this work are the following: we introduce a conditional alignDRAW model, a generative model of images from captions using a soft attention mechanism. The images generated by our alignDRAW model are reﬁned in a post-processing step by a deterministic Laplacian pyramid adversarial network (Denton et al., 2015). We further illustrate how our method, learned on Microsoft COCO (Lin et al., 2014), generalizes to captions describing novel scenes that are not seen in the dataset, such as “A stop sign is ﬂying in blue skies” (see Fig. 1).  1  Published as a conference paper at ICLR 2016  A stop sign is ﬂying in blue skies.  A herd of elephants ﬂy- ing in the blue skies.  A toilet seat sits open in the grass ﬁeld.  A person skiing on sand clad vast desert.  Figure 1: Examples of generated images based on captions that describe novel scene compositions that are highly unlikely to occur in real life. The captions describe a common object doing unusual things or set in a strange location.  2 RELATED WORK  Deep Neural Networks have achieved signiﬁcant success in various tasks such as image recognition (Krizhevsky et al., 2012), speech transcription (Graves et al., 2013), and machine translation (Bah- danau et al., 2015). While most of the recent success has been achieved by discriminative models, generative models have not yet enjoyed the same level of success. Most of the previous work in generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdi- nov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difﬁcult to scale them to large datasets. Kingma & Welling (2014), Rezende et al. (2014) have introduced the Variational Auto-Encoder (VAE) which can be seen as a neural network with continuous latent variables. The encoder is used to approximate a posterior distribution and the decoder is used to stochastically reconstruct the data from latent variables. Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer (DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are another type of generative models that use noise-contrastive estimation (Gutmann & Hyv¨arinen, 2010) to avoid calculating an intractable partition function. The model consists of a generator that generates samples using a uniform distribution and a discriminator that discriminates between real and generated images. Recently, Denton et al. (2015) have scaled those models by training conditional GANs at each level of a Laplacian pyramid of images. While many of the previous approaches have focused on unconditional models or models condi- tioned on labels, in this paper we develop a generative model of images conditioned on captions.  3 MODEL  Our proposed model deﬁnes a generative process of images conditioned on captions. In particular, captions are represented as a sequence of consecutive words and images are represented as a se- quence of patches drawn on a canvas ct over time t = 1, ..., T . The model can be viewed as a part of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al., 2015).  3.1 LANGUAGE MODEL: THE BIDIRECTIONAL ATTENTION RNN  Let y be the input caption, represented as a sequence of 1-of-K encoded words y = (y1, y2, ..., yN ), where K is the size of the vocabulary and N is the length of the sequence. We obtain the caption sen- tence representation by ﬁrst transforming each word yi to an m-dimensional vector representation hlang , i = 1, .., N using the Bidirectional RNN. In a Bidirectional RNN, the two LSTMs (Hochre- i iter & Schmidhuber, 1997) with forget gates (Gers et al., 2000) process the input sequence from both forward and backward directions. The Forward LSTM computes the sequence of forward hidden −→ −→ h lang h lang N ] , whereas the Backward LSTM computes the sequence of back- states [ , ..., ←− ←− ←− h lang h lang h lang ward hidden states [ N ]. These hidden states are then concatenated together , 2 into the sequence hlang = [hlang , ..., hlang  ], 1 ≤ i ≤ N.  , ..., , hlang  −→ h lang  ←− h lang  −→ h lang  = [  N ], with hlang  1  2  1  2  1  ,  ,  i  i  i  2  Published as a conference paper at ICLR 2016  Figure 2: AlignDRAW model for generating images by learning an alignment between the input captions and generating canvas. The caption is encoded using the Bidirectional RNN (left). The generative RNN takes a latent sequence z1:T sampled from the prior along with the dynamic caption representation s1:T to generate the canvas matrix cT , which is then used to generate the ﬁnal image x (right). The inference RNN is used to compute approximate posterior Q over the latent sequence.  IMAGE MODEL: THE CONDITIONAL DRAW NETWORK  3.2 To generate an image x conditioned on the caption information y, we extended the DRAW net- work (Gregor et al., 2015) to include caption representation hlang at each step, as shown in Fig. 2. The conditional DRAW network is a stochastic recurrent neural network that consists of a sequence of latent variables Zt ∈ RD, t = 1, .., T , where the output is accumulated over all T time-steps. For simplicity in notation, the images x ∈ Rh×w are assumed to have size h-by-w and only one color channel. Unlike the original DRAW network where latent variables are independent spherical Gaussians N (0, I), the latent variables in the proposed alignDRAW model have their mean and variance de- t−1, except for P (Z1) = N (0, I). (cid:19) pend on the previous hidden states of the generative LSTM hgen Namely, the mean and variance of the prior distribution over Zt are parameterized by: t−1) = exp(cid:0) tanh(Wσhgen t−1)(cid:1),  P (Zt|Z1:t−1) = N  t−1) = tanh(Wµhgen  µ(hgen σ(hgen  t−1), σ(hgen t−1)  where Wµ ∈ RD×n, Wσ ∈ RD×n are the learned model parameters, and n is the dimensional- ity of hgen , the hidden state of the generative LSTM. Similar to (Bachman & Precup, 2015), we have observed that the model performance is improved by including dependencies between latent variables. Formally, an image is generated by iteratively computing the following set of equations for t = 1, ..., T (see Fig. 2), with hgen  and c0 initialized to learned biases:  µ(hgen  (cid:18)  t−1),  ,  t  0  (cid:18)  (cid:19)  zt ∼ P (Zt|Z1:t−1) = N  µ(hgen  t−1), σ(hgen t−1)  t−1, hlang),  st = align(hgen t = LSTM gen(hgen hgen ct = ct−1 + write(hgen ˜x ∼ P (x| y, Z1:T ) =  (cid:89)  ),  t  t−1, [zt, st]),  P (xi | y, Z1:T ) =  i  ,  (cid:89)  i  Bern(σ(cT,i)).  The align function is used to compute the alignment between the input caption and intermediate image generative steps (Bahdanau et al., 2015). Given the caption representation from the language model, hlang = [hlang N ], the align operator outputs a dynamic sentence representa- tion st at each step through a weighted sum using alignment probabilities αt  , ..., hlang  , hlang  1  2  st = align(hgen  t−1, hlang) = αt  1hlang  1 + αt  2hlang  2 + ... + αt  1...N : N hlang N .  (1)  (2) (3) (4) (5)  (6)  3  Published as a conference paper at ICLR 2016  (7)  The corresponding alignment probability αt caption representation hlang and the current hidden state of the generative model hgen t−1:  k for the kth word in the caption is obtained using the  (cid:16)  (cid:16)  exp  (cid:80)N  αt  k =  v(cid:62) tanh(U hlang  k + W hgen  t−1 + b)  i=1 exp  v(cid:62) tanh(U hlang  i  + W hgen  t−1 + b)  (cid:17)  (cid:17) ,  where v ∈ Rl, U ∈ Rl×m, W ∈ Rl×n and b ∈ Rl are the learned model parameters of the alignment model. The LSTM gen function of Eq. 3 is deﬁned by the LSTM network with forget gates (Gers et al., 2000) at a single time-step. To generate the next hidden state hgen , the LSTM gen takes the previous hidden state hgen t−1 and combines it with the input from both the latent sample zt and the sentence representation st. The output of the LSTM gen function hgen is then passed through the write operator which is added to a cumulative canvas matrix ct ∈ Rh×w (Eq. 4). The write operator produces two arrays of 1D ) ∈ Rw×p whose ﬁlter locations and scales are Gaussian ﬁlter banks Fx(hgen computed from the generative LSTM hidden state hgen (same as deﬁned in Gregor et al. (2015)). ) ∈ Rp×p, The Gaussian ﬁlter banks are then applied to the generated p-by-p image patch K(hgen placing it onto the canvas:  ) ∈ Rh×p and Fy(hgen  t  t  t  t  t  t  ∆ct = ct − ct−1 = write(hgen  t  ) = Fx(hgen  t  )K(hgen  t  )Fy(hgen  t  )(cid:62).  (8)  Finally, each entry cT,i from the ﬁnal canvas matrix cT is transformed using a sigmoid function σ to produce a conditional Bernoulli distribution with mean vector σ(cT ) over the h × w image pixels x given the latent variables Z1:T and the input caption y1. In practice, when generating an image x, instead of sampling from the conditional Bernoulli distribution, we simply use the conditional mean x = σ(cT ).  3.3 LEARNING The model is trained to maximize a variational lower bound L on the marginal likelihood of the correct image x given the input caption y:  L =  Q(Z | x, y) log P (x| y, Z) − DKL (Q(Z | x, y)(cid:107) P (Z | y)) ≤ log P (x| y).  (9)  (cid:88)  Z  Similar to the DRAW model, the inference recurrent network produces an approximate posterior Q(Z1:T | x, y) via a read operator, which reads a patch from an input image x using two arrays of 1D Gaussian ﬁlters (inverse of write from section 3.2) at each time-step t. Speciﬁcally,  ˆxt = x − σ(ct−1), rt = read (xt, ˆxt, hgen  Q(Zt|x, y, Z1:t−1) = N(cid:16)  t−1), = LSTM inf er(hinf er t−1 , [rt, hgen ), σ(hinf er ,  µ(hinf er  hinf er t  (cid:17)  )  t  t  t−1]),  (10) (11) (12)  (13)  0  where ˆx is the error image and hinf er is initialized to the learned bias b. Note that the inference LSTM inf er takes as its input both the output of the read operator rt ∈ Rp×p, which depends on the original input image x, and the previous state of the generative decoder hgen t−1, which depends on the latent sample history z1:t−1 and dynamic sentence representation st−1 (see Eq. 3). Hence, the approximate posterior Q will depend on the input image x, the corresponding caption y, and the latent history Z1:t−1, except for the ﬁrst step Q(Z1|x), which depends only on x. The terms in the variational lower bound Eq. 9 can be rearranged using the law of total expectation. Therefore, the variational bound L is calculated as follows:  L =EQ(Z1:T | y,x)  DKL (Q(Zt | Z1:t−1, y, x)(cid:107) P (Zt | Z1:t−1, y))  − DKL (Q(Z1 | x)(cid:107) P (Z1)) .  t=2  (14)  1We also experimented with a conditional Gaussian observation model, but it worked worse compared to  the Bernoulli model.  4  (cid:34) log p(x| y, Z1:T ) − T(cid:88)  (cid:35)  Published as a conference paper at ICLR 2016  A yellow school bus parked in a parking lot.  A red school bus parked in a parking lot.  A green parked in a parking lot.  school  bus  A blue school bus parked in a parking lot.  The decadent chocolate desert is on the table.  A bowl of bananas is on the table.  A vintage photo of a cat. A vintage photo of a dog.  Figure 3: Top: Examples of changing the color while keeping the caption ﬁxed. Bottom: Examples of changing the object while keeping the caption ﬁxed. The shown images are the probabilities σ(cT ). Best viewed in colour. The expectation can be approximated by L Monte Carlo samples ˜z1:T from Q(Z1:T | y, x):  1:T ) − T(cid:88)  (cid:0)Q(Zt | ˜zl  DKL  1:t−1, y, x)(cid:107) P (Zt | ˜zl  1:t−1, y)(cid:1)(cid:35)  (cid:34)  L(cid:88)  log p(x| y, ˜zl  L ≈ 1 L − DKL (Q(Z1 | x)(cid:107) P (Z1)) .  l=1  t=2  (15) The model can be trained using stochastic gradient descent. In all of our experiments, we used only a single sample from Q(Z1:T | y, x) for parameter learning. Training details, hyperparameter settings, and the overall model architecture are speciﬁed in Appendix B. The code is available at https://github.com/emansim/text2image.  3.4 GENERATING IMAGES FROM CAPTIONS During the image generation step, we discard the inference network and instead sample from the prior distribution. Due to the blurriness of samples generated by the DRAW model, we perform an additional post processing step where we use an adversarial network trained on residuals of a Lapla- cian pyramid conditioned on the skipthought representation (Kiros et al., 2015) of the captions to sharpen the generated images, similar to (Denton et al., 2015). By ﬁxing the prior of the adversarial generator to its mean, it gets treated as a deterministic neural network that allows us to deﬁne the conditional data term in Eq. 14 on the sharpened images and estimate the variational lower bound accordingly.  4 EXPERIMENTS  4.1 MICROSOFT COCO Microsoft COCO (Lin et al., 2014) is a large dataset containing 82,783 images, each annotated with at least 5 captions. The rich collection of images with a wide variety of styles, backgrounds and objects makes the task of learning a good generative model very challenging. For consistency with related work on caption generation, we used only the ﬁrst ﬁve captions when training and evaluating our model. The images were resized to 32× 32 pixels for consistency with other tiny image datasets (Krizhevsky, 2009). In the following subsections, we analyzed both the qualitative and quantitative aspects of our model as well as compared its performance with that of other, related generative models.2 Appendix A further reports some additional experiments using the MNIST dataset.  4.1.1 ANALYSIS OF GENERATED IMAGES The main goal of this work is to learn a model that can understand the semantic meaning expressed in the textual descriptions of images, such as the properties of objects, the relationships between them, and then use that knowledge to generate relevant images. To examine the understanding of  2To see more generated images, visit http://www.cs.toronto.edu/˜emansim/cap2im.html  5  Published as a conference paper at ICLR 2016  A very large commercial plane ﬂying in blue skies.  A very large commer- cial plane ﬂying in rainy skies.  A herd of elephants walk- ing across a dry grass ﬁeld.  A herd of elephants walk- ing across a green grass ﬁeld.  Figure 4: Bottom: Examples of changing the background while keeping the caption ﬁxed. Top: The respective nearest training images based on pixel-wise L2 distance. The nearest images from the training set also indicate that the model was not simply copying the patterns it observed during the learning phase.  our model, we wrote a set of captions inspired by the COCO dataset and changed some words in the captions to see whether the model made the relevant changes in the generated samples. First, we explored whether the model understood one of the most basic properties of any object, the color. In Fig. 3, we generated images of school buses with four different colors: yellow, red, green and blue. Although, there are images of buses with different colors in the training set, all mentioned school buses are speciﬁcally colored yellow. Despite that, the model managed to generate images of an object that is visually reminiscent of a school bus that is painted with the speciﬁed color. Apart from changing the colors of objects, we next examined whether changing the background of the scene described in a caption would result in the appropriate changes in the generated samples. The task of changing the background of an image is somewhat harder than just changing the color of an object because the model will have to make alterations over a wider visual area. Nevertheless, as shown in Fig. 4 changing the skies from blue to rainy in a caption as well as changing the grass type from dry to green in another caption resulted in the appropriate changes in the generated image. Despite a large number of ways of changing colors and backgrounds in descriptions, in general we found that the model made appropriate changes as long as some similar pattern was present in the training set. However, the model struggled when the visual difference between objects was very small, such as when the objects have the same general shape and color. In Fig. 3, we demonstrate that when we swap two objects that are both visually similar, for example cats and dogs, it is difﬁcult to discriminate solely from the generated samples whether it is an image of a cat or dog, even though we might notice an animal-like shape. This highlights a limitation of the model in that it has difﬁculty modelling the ﬁne-grained details of objects. As a test of model generalization, we tried generating images corresponding to captions that describe scenarios that are highly unlikely to occur in real life. These captions describe a common object doing unusual things or set in a strange location, for example “A toilet seat sits open in the grass ﬁeld”. Even though some of these scenarios may never occur in real life, it is very easy for humans to imagine the corresponding scene. Nevertheless, as you can see in Fig. 1, the model managed to generate reasonable images.  4.1.2 ANALYSIS OF ATTENTION After ﬂipping sets of words in the captions, we further explored which words the model attended to when generating images. It turned out that during the generation step, the model mostly focused on the speciﬁc words (or nearby words) that carried the main semantic meaning expressed in the sentences. The attention values of words in sentences helped us interpret the reasons why the model made the changes it did when we ﬂipped certain words. For example, in Fig. 5, top row, we can see that when we ﬂipped the word “desert” to “forest”, the attention over words in the sentence did not change drastically. This suggests that, in their respective sentences, the model looked at “desert” and “forest” with relatively equal probability, and thus made the correct changes. In contrast, when we swap words “beach” and “sun”, we can see a drastic change between sentences in the probability distribution over words. By noting that the model completely ignores the word “sun” in the second  6  Published as a conference paper at ICLR 2016  A rider on a blue motor- cycle in the desert.  A rider on a blue motor- cycle in the forest.  A surfer, a woman, and a child walk on the beach.  A surfer, a woman, and a child walk on the sun.  alignDRAW  LAPGAN  Conv-Deconv VAE  Fully-Conn VAE  Figure 5: Top: Examples of most attended words while changing the background in the caption. Bottom: Four different models displaying results from sampling caption A group of people walk on a beach with surf boards.  sentence, we can therefore gain a more thorough understanding of why we see no visual differences between the images generated by each caption. We also tried to analyze the way the model generated images. Unfortunately, we found that there was no signiﬁcant connection between the patches drawn on canvas and the most attended words at particular time-steps.  4.1.3 COMPARISON WITH OTHER MODELS Quantitatively evaluating generative models remains a challenging task in itself as each method of evaluation suffers from its own speciﬁc drawbacks. Compared to reporting classiﬁcation accuracies in discriminative models, the measures deﬁning generative models are intractable most of the times and might not correctly deﬁne the real quality of the model. To get a better comparison between performances of different generative models, we report results on two different metrics as well as a qualitative comparison of different generative models. We compared the performance of the proposed model to the DRAW model conditioned on cap- tions without the align function (noalignDRAW) as well as the DRAW model conditioned on the skipthought vectors of (Kiros et al., 2015) (skipthoughtDRAW). All of the conditional DRAW models were trained with a binary cross-entropy cost function, i.e. they had Bernoulli conditional likelihoods. We also compared our model with Fully-Connected (Fully-Conn) and Convolutional- Deconvolutional (Conv-Deconv) Variational Autoencoders which were trained with the least squares cost function. The LAPGAN model of (Denton et al., 2015) was trained on a two level Lapla- cian Pyramid with a GAN as a top layer generator and all stages were conditioned on the same skipthought vector. In Fig. 5, bottom row, we generated several samples from the prior of each of the current state-of- the-art generative models, conditioned on the caption “A group of people walk on a beach with surf boards”. While all of the samples look sharp, the images generated by LAPGAN look more noisy and it is harder to make out deﬁnite objects, whereas the images generated by variational models trained with least squares cost function have a watercolor effect on the images. We found that the quality of generated samples was similar among different variants of conditional DRAW models. As for the quantitative comparison of different models, we ﬁrst compare the performances of the model trained with variational methods. We rank the images in the test set conditioned on the captions based on the variational lower bound of the log-probabilities and then report the Precision- Recall metric as an evaluation of the quality of the generative model (see Table 1.). Perhaps un- surprisingly, generative models did not perform well on the image retrieval task. To deal with the large computational complexity involved in looping through each test image, we create a shortlist of one hundred images including the correct one, based on the images having the closest Euclidean distance in the last fully-connected feature space of a VGG-like model (Simonyan & Zisserman, 2015) trained on the CIFAR dataset3 (Krizhevsky, 2009). Since there are “easy” images for which  3The architecture of the model is described here http://torch.ch/blog/2015/07/30/cifar. html. The shortlist of test images used for evaluation can be downloaded from http://www.cs. toronto.edu/˜emansim/cap2im/test-nns.pkl.  7  Published as a conference paper at ICLR 2016  Model  LAPGAN  Fully-Conn VAE Conv-Deconv VAE skipthoughtDRAW  noalignDRAW alignDRAW  Microsoft COCO (prior to sharpening)  Image Retrieval  R@1 R@5 R@10 R@50 Med r  - 1.0 1.0 2.0 2.8 3.0  - 6.6 6.5 11.2 14.1 14.0  -  12.0 12.0 18.9 23.1 22.9  -  53.4 52.9 63.3 68.0 68.5  - 47 48 36 31 31  Image Similarity  SSI  0.08 ± 0.07 0.156 ± 0.10 0.164 ± 0.10 0.157 ± 0.11 0.155 ± 0.11 0.156 ± 0.11  Table 1: Retrieval results of different models. R@K is Recall@K (higher is better). Med r is the median rank (lower is better). SSI is Structural Similarity Index, which is between −1 and 1 (higher is better).  the model assigns high log-probabilities independent of the query caption, we instead look at the ratio of the likelihood of the image conditioned on the sentence to the likelihood of the image con- ditioned on the mean sentence representation in the training set, following the retrieval protocol of (Kiros et al., 2014b). We also found that the lower bound on the test log-probabilities decreased for sharpened images, and that sharpening considerably hurt the retrieval results. Since sharpening changes the statistics of images, the estimated log-probabilities of image pixels is not necessarily a good metric. Some examples of generated images before and after sharpening are shown in Ap- pendix C. Instead of calculating error per pixel, we turn to a smarter metric, the Structural Similarity Index (SSI) (Wang et al., 2004), which incorporates luminance and contrast masking into the error cal- culation. Strong inter-dependencies of closer pixels are also taken into account and the metric is calculated on small windows of the images. Due to independence property of test captions, we sam- pled ﬁfty images from the prior of each generative model for every caption in the test set in order to calculate SSI. As you can see on Table 1, SSI scores achieved by variational models were higher compared to SSI score achieved by LAPGAN.  5 DISCUSSION In this paper, we demonstrated that the alignDRAW model, a combination of a recurrent variational autoencoder with an alignment model over words, succeeded in generating images that correspond to a given input caption. By extensively using attentional mechanisms, our model gained several advantages. Namely, the use of the visual attention mechanism allowed us to decompose the problem of image generation into a series of steps instead of a single forward pass, while the attention over words provided us an insight whenever our model failed to generate a relevant image. Additionally, our model generated images corresponding to captions which generalized beyond the training set, such as sentences describing novel scenarios which are highly unlikely to occur in real life. Because the alignDRAW model tends to output slightly blurry samples, we augmented the model with a sharpening post-processing step in which GAN generated edges which were added to the alignDRAW samples. Unfortunately, this is not an ideal solution due to the fact that the whole model was not trained in an end-to-end fashion. Therefore a direction of future work would be to ﬁnd methods that can bypass the separate post-processing step and output sharp images directly in an end-to-end manner. Acknowledgments: This work was supported by Samsung and IARPA, Raytheon BBN Contract No. D11PC20071. We would like to thank developers of Theano (Bastien et al., 2012), the authors of (Denton et al., 2015) for open sourcing their code, and Ryan Kiros and Nitish Srivastava for helpful discussions.  REFERENCES Bachman, Philip and Precup, Doina. Data generation as sequential decision making. In NIPS, 2015.  Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. In  ICLR, 2015.  Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, Warde-Farley, David, and Bengio, Yoshua. Theano: new features and speed improve- ments. CoRR, abs/1211.5590, 2012.  8  Published as a conference paper at ICLR 2016  Cho, K., van Merrienboer, B., G¨ulc¸ehre, C¸ ., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning  phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, 2014.  Denton, Emily L., Chintala, Soumith, Szlam, Arthur, and Fergus, Robert. Deep generative image models using  a laplacian pyramid of adversarial networks. In NIPS, 2015.  Gers, Felix, Schmidhuber, J¨urgen, and Cummins, Fred. Learning to forget: Continual prediction with lstm.  Neural Computation, 2000.  Goodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil,  Courville, Aaron C., and Bengio, Yoshua. Generative adversarial nets. In NIPS, 2014.  Graves, A., Jaitly, N., and Mohamed, A.-r. Hybrid speech recognition with deep bidirectional LSTM. In IEEE  Workshop on Automatic Speech Recognition and Understanding, 2013.  Gregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra, Daan. DRAW: A recurrent neural network for  image generation. In ICML, 2015.  Gutmann, Michael and Hyv¨arinen, Aapo. Noise-contrastive estimation: A new estimation principle for unnor-  malized statistical models. In AISTATS, 2010.  Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye. A fast learning algorithm for deep belief nets.  Neural Computation, 2006.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural Computation, 1997.  Karpathy, Andrej and Li, Fei-Fei. Deep visual-semantic alignments for generating image descriptions.  CVPR, 2015.  In  Kingma, Diederik P. and Welling, Max. Auto-encoding variational bayes. In ICLR, 2014. Kiros, R., Salakhutdinov, R., and Zemel, R. Multimodal neural language models. In ICML, 2014a.  Kiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S. Unifying visual-semantic embeddings with multi-  modal neural language models. CoRR, abs/1411.2539, 2014b.  Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan, Zemel, Richard S., Torralba, Antonio, Urtasun, Raquel, and  Fidler, Sanja. Skip-thought vectors. In NIPS, 2015.  Krizhevsky, Alex. Learning multiple layers of features from tiny images. Master’s Thesis, Department of  Computer Science, University of Toronto, 2009.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convolutional  neural networks. In NIPS, 2012.  Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft  COCO: Common objects in context. In ECCV, 2014.  Rezende, Danilo J., Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and variational infer-  ence in deep latent gaussian models. In ICML, 2014.  Salakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltzmann machines. In AISTATS, 2009.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition.  In ICLR, 2015.  Smolensky, Paul.  Information processing in dynamical systems: foundations of harmony theory. Parallel  Distributed Processing: Explorations in the Microstructure of Cognition, 1986.  Srivastava, Nitish, Mansimov, Elman, and Salakhutdinov, Ruslan. Unsupervised learning of video representa-  tions using LSTMs. In ICML, 2015.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural networks. In NIPS,  2014.  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption  generator. In CVPR, 2015.  Wang, Zhou, Bovik, Alan C., Sheikh, Hamid R., and Simoncelli, Eero P. Image quality assessment: from error  visibility to structural similarity. IEEE Transactions on Image Processing, 2004.  Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun, Courville, Aaron C., Salakhutdinov, Ruslan, Zemel, Richard S., and Bengio, Yoshua. Show, attend and tell: Neural image caption generation with visual atten- tion. In ICML, 2015.  9  Published as a conference paper at ICLR 2016  Figure 6: Examples of generating 60 × 60 MNIST images corresponding to respective captions. The captions on the left column were part of the training set. The digits described in the captions on the right column were hidden during training for the respective conﬁgurations.  APPENDIX A: MNIST WITH CAPTIONS  As an additional experiment, we trained our model on the MNIST dataset with artiﬁcial captions. Either one or two digits from the MNIST training dataset were placed on a 60 × 60 blank image. One digit was placed in one of the four (top-left, top-right, bottom-left or bottom-right) corners of the image. Two digits were either placed horizontally or vertically in non-overlapping fashion. The corresponding artiﬁcial captions speciﬁed the identity of each digit along with their relative positions, e.g. “The digit three is at the top of the digit one”, or “The digit seven is at the bottom left of the image”. The generated images together with the attention alignments are displayed in Figure 6. The model correctly displayed the speciﬁed digits at the described positions and even managed to generalize reasonably well to the conﬁgurations that were never present during training. In the case of gener- ating two digits, the model would dynamically attend to the digit in the caption it was drawing at that particular time-step. Similarly, in the setting where the caption speciﬁed only a single digit, the model would correctly attend to the digit in the caption during the whole generation process. In both cases, the model placed small attention values on the words describing the position of digits in the images.  APPENDIX B: TRAINING DETAILS  HYPERPARAMETERS  Each parameter in alignDRAW was initialized by sampling from a Gaussian distribution with mean 0 and standard deviation 0.01. The model was trained using RMSprop with an initial learning rate of 0.001. For the Microsoft COCO task, we trained our model for 18 epochs. The learning rate was reduced to 0.0001 after 11 epochs. For the MNIST with Captions task, the model was trained for 150 epochs and the learning rate was reduced to 0.0001 after 110 epochs. During each epoch, randomly created 10, 000 training samples were used for learning. The norm of the gradients was clipped at 10 during training to avoid the exploding gradients problem. We used a vocabulary size of K = 25323 and K = 22 for the Microsoft COCO and MNIST with Captions datasets respectively. All capital letters in the words were converted to small letters as a in the language model had preprocessing step. For all tasks, the hidden states 128 units. Hence the dimensionality of the concatenated state of the Bidirectional LSTM hlang = −→ h lang ] was 256. The parameters in the align operator (Eq. 7) had a dimensionality of [ l = 512, so that v ∈ R512, U ∈ R512×256, W ∈ R512×ngen and b ∈ R512. The architectural conﬁgurations of the alignDRAW models are shown in Table 2.  −→ h lang  ←− h lang  ←− h lang  and  ,  i  i  i  i  i  10  Published as a conference paper at ICLR 2016  Task  MS COCO  MNIST  alignDRAW Model  #glimpses  Inference  Generative  Latent Dim of hinf er Dim of hgen Dim of Z  T 32 32 Table 2: The architectural conﬁgurations of alignDRAW models.  550 300  550 300  275 150  Read Size Write Size  p 9 8  p 9 8  The GAN model used for sharpening had the same conﬁguration as the 28 × 28 model trained by Denton et al. (2015) on the edge residuals of the CIFAR dataset. The conﬁguration can be found at https://gist.github.com/soumith/e3f722173ea16c1ea0d9. The model was trained for 6 epochs.  EVALUATION  Table 3 shows the estimated variational lower bounds on the average train/validation/test log- probabilities. Note that the alignDRAW model does not suffer much from overﬁtting. The results substantially worsen after sharpening test images.  Model  Train  skipthoughtDRAW -1794.29 -1792.14 -1792.15  noalignDRAW alignDRAW  Validation -1797.41 -1796.94 -1797.24  Test  -1791.37 -1791.15 -1791.53  Test (after sharpening)  -2045.84 -2051.07 -2042.31  Table 3: The lower bound on the average test log-probabilities of conditional DRAW models, trained on the Microsoft COCO dataset.  11  Published as a conference paper at ICLR 2016  APPENDIX C: EFFECT OF SHARPENING IMAGES. Some examples of generated images before (top row) and after (bottom row) sharpening im- ages using an adversarial network trained on residuals of a Laplacian pyramid conditioned on the skipthought vectors of the captions.  Figure 7: Effect of sharpening images.  12  ",
1511.06281,2016,Density Modeling of Images using a Generalized Normalization Transformation,"['Density Modeling of Images using a Generalized Normalization Transformation\nJohannes Ballé', 'Valero Laparra', 'Eero Simoncelli']",https://arxiv.org/pdf/1511.06281,"6 1 0 2     b e F 9 2         ]  G L . s c [      4 v 1 8 2 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  DENSITY MODELING OF IMAGES USING A GENERALIZED NORMALIZATION TRANSFORMATION  Johannes Ballé, Valero Laparra & Eero P. Simoncelli ∗ Center for Neural Science New York University New York, NY 10004, USA {johannes.balle,valero,eero.simoncelli}@nyu.edu  ABSTRACT  We introduce a parametric nonlinear transformation that is well-suited for Gaus- sianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by ex- ponentiating a weighted sum of rectiﬁed and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimiz- ing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a signiﬁcantly smaller mutual information be- tween transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efﬁciently in- verted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.  1  INTRODUCTION  The learning of representations for classiﬁcation of complex patterns has experienced an impres- sive series of successes in recent years. But these results have relied heavily on large labeled data sets, leaving open the question of whether such representations can be learned directly from ob- served examples, without supervision. Density estimation is the mother of all unsupervised learning problems. A direct approach to this problem involves ﬁtting a probability density model, either drawn from a parametric family, or composed as a nonparametric superposition of kernels, to the data. An indirect alternative, which can offer access to different families of densities, and in some cases an easier optimization problem, is to seek an invertible and differentiable parametric function y = g(x; θ) that best maps the data onto a ﬁxed target density py(y). The inverse image of this target density then provides a density model for the input space. Many unsupervised learning methods may be interpreted in this context. As a simple example, con- sider principal component analysis (PCA; Jolliffe, 2002): we might ﬁx py as a multivariate standard normal and think of PCA as either a linear whitening transformation, or as a density model px de- scribing the data as a normal distribution with arbitrary covariance. Independent component analysis (ICA; Cardoso, 2003) can be cast in the same framework: In this case, the data x is modeled as a i pyi to be a product of independent marginal densities of unknown form. Alternatively, we can apply nonparametric nonlinearities to the marginals of the linearly transformed data so as to Gaussianize them (i.e., histogram equalization). For this combined ICA-marginal-Gaussianization (ICA-MG) operation, py is again standard normal, and the transformation is a composition of a linear transform and marginal nonlinearities. Another model that aims for the same outcome is radial Gaussianiza- ∗EPS is also afﬁliated with the Courant Institute of Mathematical Sciences at NYU; VL is also afﬁliated  linear combination of independent heavy-tailed sources. We may ﬁx g to be linear and py =(cid:81)  with the University of València, Spain.  1  Published as a conference paper at ICLR 2016  tion (RG; Lyu & Simoncelli, 2009b; Sinz & Bethge, 2010), in which g is the composition of a linear transformation and a radial (i.e., operating on the vector length) Gaussianizing nonlinearity. The induced density model is the family of elliptically symmetric distributions. The notion of optimizing a transformation so as to achieve desired statistical properties at the output is central to theories of efﬁcient sensory coding in neurobiology (Barlow, 1961; Ruderman, 1994; Rieke et al., 1995; Bell & Sejnowski, 1997; Schwartz & Simoncelli, 2001), and also lends itself nat- urally to the design of cascaded representations such as deep neural networks. Speciﬁcally, variants of ICA-MG transformations have been applied in iterative cascades to learn densities (Friedman et al., 1984; Chen & Gopinath, 2000; Laparra et al., 2011). Each stage seeks a linear transformation that produces the “least Gaussian” marginal directions, and then Gaussianizes these using nonpara- metric scalar nonlinear transformations. In principle, this series of transformations can be shown to converge for any data density. However, the generality of these models is also their weakness: implementing the marginal nonlinearities in a non-parametric way makes the model prone to error and requires large amounts of data. In addition, since the nonlinearities operate only on marginals, convergence can be slow, requiring a lengthy sequence of transformations (i.e., a very deep network). To address these shortcomings, we develop a joint transformation that is highly effective in Gaus- sianizing local patches of natural images. The transformation is a generalization of divisive normal- ization, a form of local gain control ﬁrst introduced as a means of modeling nonlinear properties of cortical neurons (Heeger, 1992), in which linear responses are divided by pooled responses of their rectiﬁed neighbors. Variants of divisive normalization have been found to reduce dependen- cies when applied to natural images or sounds and to produce approximately Gaussian responses (Ruderman, 1994; Schwartz & Simoncelli, 2001; Malo & Laparra, 2010). Simple forms of divisive normalization have been shown to offer improvements in recognition performance of deep neural networks (Jarrett et al., 2009). But the Gaussianity of these representations has not been carefully optimized, and typical forms of normalization do not succeed in capturing all forms of dependency found in natural images (Lyu, 2010; Sinz & Bethge, 2013). In this paper, we deﬁne a generalized divisive normalization (GDN) transform that includes paramet- ric forms of both ICA-MG and RG as special cases. We solve for the parameters of the transform by optimizing an unsupervised learning objective for the non-Gaussianity of the transformed data. The transformation is continuous and differentiable, and we present an effective method of inverting it. We demonstrate that the resulting GDN transform provides a signiﬁcantly better model for natural photographic images than either ICA-MG or RG. Speciﬁcally, we show that GDN provides a better ﬁt to the pairwise statistics of local ﬁlter responses, that it generates more natural samples of image patches, and that it produces better results when used as a prior for image processing problems such as denoising. Finally, we show that a two-stage cascade of GDN transformations offers additional improvements in capturing image statistics, laying the groundwork for its use as a general tool for unsupervised learning of deep networks.  2 PARAMETRIC GAUSSIANIZATION  Given a parametric family of transformations y = g(x; θ), we wish to select parameters θ so as to transform the input vector x into a standard normal random vector (i.e., zero mean, identity covariance matrix). For a differentiable transformation, the input and output densities are related by:  (cid:12)(cid:12)(cid:12)(cid:12) ∂g(x; θ)  ∂x  (cid:12)(cid:12)(cid:12)(cid:12) py  (cid:0)g(x; θ)(cid:1),  px(x) =  (1)  where | · | denotes the absolute value of the matrix determinant. If py is the standard normal distri- bution (denoted N ), the shape of px is determined solely by the transformation. Thus, g induces a density model on x, speciﬁed by the parameters θ. Given px, or data drawn from it, the density estimation problem can be solved by minimizing the Kullback–Leibler (KL) divergence between the transformed density and the standard normal, known as the negentropy:  (cid:17)  (cid:16)  log px(x) − log  2  (cid:12)(cid:12)(cid:12) ∂g(x;θ)  ∂x  (cid:12)(cid:12)(cid:12) − log N (g(x; θ))  (cid:17)  ,  (2)  J(py) = Ey  log py(y) − log N (y)  = Ex  (cid:16)  Published as a conference paper at ICLR 2016  where we have rewritten the standard deﬁnition (an expected value over y) as an expectation over x (see appendix). Differentiating with respect to the parameter vector θ yields:  −(cid:88)  ij  (cid:20) ∂g(x, θ)  (cid:21)−(cid:62)  ∂x  ij  ∂J(py)  ∂θ  = Ex  (cid:88)  i  ∂2gi(x, θ)  ∂xj∂θ  +  gi(x, θ)  ∂gi(x, θ)  ∂θ   ,  where the expectation can be evaluated by summing over data samples, allowing the model to be ﬁt using stochastic gradient descent. It can be shown that this optimization is equivalent to maximizing the log likelihood of the induced density model. Note that, while optimization is feasible, measuring success in terms of the actual KL divergence in eq. (2) is difﬁcult in practice, as it requires evaluating the entropy of px. Instead, we can monitor the difference in negentropy between the input and output densities:  (cid:16) 1  2  (cid:13)(cid:13)y(cid:13)(cid:13)2  2 − log  (cid:12)(cid:12)(cid:12) ∂y  ∂x  (cid:12)(cid:12)(cid:12) − 1  2  (cid:13)(cid:13)x(cid:13)(cid:13)2  2  (cid:17)  .  ∆J ≡ J(py) − J(px) = Ex  (3)  (4)  This quantity provides a measure of how much more Gaussian the data become as a result of the transformation g(x; θ).  3 DIVISIVE NORMALIZATION TRANSFORMATIONS  Divisive normalization, a form of gain control in which responses are divided by pooled activity of neighbors, has become a standard model for describing the nonlinear properties of sensory neurons (Carandini & Heeger, 2012). A commonly used form for this transformation is:  βα +(cid:80)  xα i  ,  j xα j  yi = γ  where θ = {α, β, γ} are parameters. Loosely speaking, the transformation adjusts responses to lie within a desired operating range, while maintaining their relative values. A weighted form of normalization (with exponents ﬁxed at α = 2) was introduced in (Schwartz & Simoncelli, 2001), and shown to produce approximately Gaussian responses with greatly reduced dependencies. The weights were optimized over a collection of photographic images so as to maximize the likelihood of responses under a Gaussian model. Normalization has also been derived as an inference method for a Gaussian scale mixture (GSM) model for wavelet coefﬁcients of natural images (Wainwright & Simoncelli, 2000). This model factorizes local groups of coefﬁcients into a Gaussian vector and a positive-valued scalar. In a speciﬁc instance of the model, the optimal estimator for the Gaussian vector (after decorrelation) can be shown to be a modiﬁed form of divisive normalization that uses a weighted L2-norm (Lyu & Simoncelli, 2008):  (cid:0)β2 +(cid:80)  xi j γjx2 j  (cid:1) 1  2  .  yi =  However, the above instances of divisive normalization have only been shown to be effective when applied to spatially local groups of ﬁlter responses. In what follows, we introduce a more general form, with better Gaussianization capabilities that extend to to more distant responses, as well as those arising from distinct ﬁlters.  3.1 PROPOSED GENERALIZED DIVISIVE NORMALIZATION (GDN) TRANSFORM  We deﬁne a vector-valued parametric transformation as a composition of a linear transformation followed by a generalized form of divisive normalization:  y = g(x; θ)  s.t.  yi =  (cid:0)βi +(cid:80)  j γij|zj|αij(cid:1)εi  zi  (5) The full parameter vector θ consists of the vectors β and ε, as well as the matrices H, α, and γ, for a total of 2N + 3N 2 parameters (where N is the dimensionality of the input space). We refer to this transformation as generalized divisive normalization (GDN), since it generalizes several previous models. Speciﬁcally:  z = Hx.  and  3  Published as a conference paper at ICLR 2016  transformation (Carandini & Heeger, 2012), with exponents set to 1.  • Choosing εi ≡ 1, αij ≡ 1, and γij ≡ 1 yields the classic form of the divisive normalization • Choosing γ to be diagonal eliminates the cross terms in the normalization pool, and the model is then a particular form of ICA-MG, or the ﬁrst iteration of the Gaussianization algorithms described in Chen & Gopinath (2000) or Laparra et al. (2011): a linear “unmix- ing” transform, followed by a pointwise, Gaussianizing nonlinearity. • Choosing αij ≡ 2 and setting all elements of β, ε, and γ identical, the transformation  assumes a radial form:  (cid:0)β + γ(cid:80)  z  j z2 j  (cid:1)ε =  y =  z (cid:107)z(cid:107)2  g2  (cid:0)(cid:107)z(cid:107)2  (cid:1)  where g2(r) = r/(β + γr2)ε is a scalar-valued transformation on the radial component of z, ensuring that the normalization operation preserves the vector direction of z. If, in addition, H is a whitening transformation such as ZCA (Bell & Sejnowski, 1997), the overall transformation is a form of RG (Lyu & Simoncelli, 2009b). • More generally, if we allow exponents αij ≡ p, the induced distribution is an Lp-symmetric distribution, a family which has been shown to capture various statistical properties of natural images (Sinz & Bethge, 2010). The corresponding transformation on the Lp-radius is given by gp(r) = r/(β + γrp)ε. • Another special case of interest arises when partitioning the space into distinct, spheri- cally symmetric subspaces, with the kth subspace comprising the set of vector indices Sk. k, all for i, j ∈ Sk (and γij = 0 if i or j Choosing αij ≡ 2, βi = β(cid:48) is not in the same set), the nonlinear transformation can be written as  k, and γij = γ(cid:48)  k, εi = ε(cid:48)  (cid:0)β(cid:48)  yi =  (cid:80)  zi  k + γ(cid:48)  k  j∈Sk  z2 j  (cid:1)ε(cid:48)  k  ,  where k is chosen such that i ∈ Sk. This is the Independent Subspace Analysis model (ISA; Hyvärinen & Hoyer, 2000), expressed as a Gaussianizing transformation.  The topographic ICA model (TICA; Hyvärinen et al., 2001) and the model presented in Köster & Hyvärinen (2010) are generalizations of ISA that are related to our model, but have more constrained nonlinearities. They are formulated directly as density models, which makes them difﬁcult to nor- malize. For this reason, the authors must optimize approximated likelihood or use score matching (Hyvärinen, 2005) to ﬁt these models.  3.2 WELL-DEFINEDNESS AND INVERTIBILITY  (6)  For the density function in eq. (1) to be well deﬁned, we require the transformation in eq. (5) to be continuous and invertible. For the linear portion of the transformation, we need only ensure that the matrix H is non-singular. For the normalization portion, consider the partial derivatives:  (cid:0)βi +(cid:80)  j γij|zj|αij(cid:1)εi − αikγikεizi|zk|αik−1 sgn(zk) j γij|zj|αij(cid:1)εi+1  (cid:0)βi +(cid:80)  δik  ∂yi ∂zk  =  To ensure continuity, we require all partial derivatives to be ﬁnite for all z ∈ RN . More speciﬁcally, we require all exponents in eq. (6) to be non-negative, as well as the parenthesized expression in the denominator to be positive. It can be shown that the normalization part of the transformation is invertible if the Jacobian matrix containing the partial derivatives in eq. (6) is positive deﬁnite everywhere (see appendix). In all practical cases, we observed this to be the case, but expressing this precisely as a condition on the parameters is difﬁcult. A necessary (but generally not sufﬁcient) condition for invertibility can be established as follows. First, note that, as the denominator is positive, each vector z is mapped to a vector y in the same orthant. The cardinal axes of z are mapped to themselves, and for this one-dimensional mapping to be continuous and invertible, it must be monotonic. Along the cardinal axes, the following bound holds:  |yi| =  (cid:0)βi + γii|zi|αij(cid:1)εi ≤  |zi|  |zi|  ii |zi|αiiεi γεi  = γ  −εi ii  |zi|1−αiiεi.  4  Published as a conference paper at ICLR 2016  ii . We initialize the parameters such that ∂y  For the magnitude of yi to grow monotonically with |zi|, the exponent 1 − αiiεi must be positive. In summary, the constraints we enforce during the optimization are αij ≥ 1, βi > 0, γij ≥ 0, and 0 ≤ εi ≤ α−1 ∂z is positive deﬁnite everywhere (for example, by letting γ be diagonal, such that the Jacobian is diagonal, the transformation is separable, and the necessary constraint on the cardinal axes becomes sufﬁcient). Suppose that during the course of optimization, the matrix should cease to be positive deﬁnite. Following a continuous path, the matrix must then become singular at some point, because going from positive deﬁnite to indeﬁnite or negative deﬁnite would require at least one of the eigenval- ues to change signs. However, the optimization objective heavily penalizes singularity: The term − log | ∂y ∂x|) grows out of bounds as the determinant of the Jacobian approaches zero. Therefore, given a sensible initialization and a sufﬁciently small step size, it is highly unlikely that the Jacobian should cease to be positive deﬁnite during the optimization, and we haven’t observed this in practice. Finally, we ﬁnd that the GDN transformation can be efﬁciently inverted using a ﬁxed point iteration:  ∂x| in the objective (which separates into − log | ∂y  ∂z| and − log | ∂z  i = sgn(yi)(cid:0)γεi ii |yi|(cid:1) (cid:88) (cid:12)(cid:12)z(n)  (cid:16)  βi +  γij  =  j  z(0)  1−αiiεi  1  (cid:12)(cid:12)αij(cid:17)εi  z(n+1) i  yi.  Other iterative inverse solutions have been proposed for this purpose (Malo et al., 2006; Lyu & Simoncelli, 2008), but these only apply to special cases of the form in eq. (5).  j  4 EXPERIMENTS  The model was optimized to capture the distribution of image data using stochastic descent of the gradient expressed in eq. (3). We then conducted a series of experiments to assess the validity of the ﬁtted model for natural images.  4.1  JOINT DENSITY OF PAIRS OF WAVELET COEFFICIENTS  We examined the pairwise statistics of model responses, both for our GDN model, as well as the ICA model and the RG model. First, we computed the responses of an oriented ﬁlter (speciﬁcally, we used a subband of the steerable pyramid (Simoncelli & Freeman, 1995)) to images taken from the van Hateren dataset (van Hateren & van der Schaaf, 1998) and extracted pairs of coefﬁcients within subbands at different spatial offsets up to d = 1024. We then transformed these two-dimensional datasets using ICA, RG, and GDN. Figure 1 (modelled after ﬁgure 4 of Lyu & Simoncelli (2009b)) shows the mutual information in the transformed data (note that, in our case, mutual information is related to the negentropy by an additive constant.) For very small distances, a linear ICA transfor- mation reduces some of the dependencies in the raw data. However, for larger distances, a linear transformation is not sufﬁcient to eliminate the dependencies between the coefﬁcients, and the mu- tual information of the ICA-transformed data is identical to that of the raw data. An elliptically symmetric model is good for modeling the density when the distance is small, and the RG trans- form reduces the mutual information to neglegible levels. However, the ﬁt worsens as the distance increases. As described in (Lyu & Simoncelli, 2009b), RG can even lead to an increase in the mu- tual information relative to that of the raw data, as seen in the right hand side of the plot. The GDN transform, however, captures the dependencies at all separations, and consistently leaves a very small residual level of mutual information. In ﬁgure 2, we compare histogram estimates of the joint wavelet coefﬁcient densities against model- ﬁtted densities for selected spatial offsets. The GDN ﬁts are seen to account well for the shapes of the densities, particularly in the center of the plot, where the bulk of the data lie. Note that GDN is able to capture elliptically symmetric distributions just as well as distributions which are closer to being marginally independent, whereas RG and ICA each fail in one of these cases, respectively.  5  Published as a conference paper at ICLR 2016  Figure 1: Mutual information in pairs of wavelet coefﬁcients after various transformations, plotted as a function of the spatial separation between the coefﬁcients.  ICA-MG  RG  GDN  Figure 2: Contour plots of pairwise wavelet coefﬁcient densities. Each row corresponds to a model arising from a different transformation (ICA-MG, RG, GDN). Each column corresponds to a pair of coefﬁcients spatially separated by distance d (pixels). Gray: contour lines of histogram density estimate. Black: contour lines of densities induced by best-ﬁtting transformations. As distance increases, the empirical density between the coefﬁcients transitions from elliptical but correlated to separable. The RG density captures the former, and the ICA density captures the latter. Only the GDN density has sufﬁcient ﬂexibility to capture the full range of behaviors.  6  101102103spatial separation (pixels)0.000.050.100.150.20mutual informationrawICARGGDN1.00.50.00.51.01.00.50.00.51.02e-092e-092e-082e-081e-071e-078e-078e-076e-066e-065e-055e-050.00030.00030.0020.0020.020.117d=21.00.50.00.51.01.00.50.00.51.06e-066e-066e-066e-065e-055e-055e-055e-050.00030.00030.00030.00030.0020.0020.0020.0020.020.117d=161.00.50.00.51.01.00.50.00.51.06e-066e-066e-066e-065e-055e-055e-055e-050.00030.00030.00030.00030.0020.0020.0020.0020.020.117d=10241.00.50.00.51.01.00.50.00.51.03e-103e-102e-092e-092e-082e-081e-071e-078e-078e-076e-066e-065e-055e-050.00030.00030.0020.020.117d=21.00.50.00.51.01.00.50.00.51.05e-055e-050.00030.00030.00030.00030.0020.020.117d=161.00.50.00.51.01.00.50.00.51.05e-055e-055e-055e-050.00030.0020.020.117d=10241.00.50.00.51.01.00.50.00.51.02e-092e-092e-082e-081e-071e-078e-078e-076e-066e-065e-055e-050.00030.00030.0020.020.117d=21.00.50.00.51.01.00.50.00.51.05e-055e-055e-055e-050.00030.00030.00030.00030.0020.020.117d=161.00.50.00.51.01.00.50.00.51.05e-055e-055e-055e-050.00030.00030.00030.00030.0020.0020.020.117d=1024Published as a conference paper at ICLR 2016  Figure 3: Histograms of transformed data. Top: Radial component for ICA-MG and GDN. Bottom left and right: Marginals for RG and GDN, respectively. Gray lines indicate the expected distribu- tions (Chi for radial, and Gaussian for marginals).  JOINT DENSITY OVER IMAGE PATCHES  4.2 We also examined model behavior when applied to vectorized 16 × 16 blocks of pixels drawn from the Kodak set1. We used the stochastic optimization algorithm ADAM to facilitate the optimiza- tion (Kingma & Ba, 2014) and somewhat reduced the complexity of the model by forcing α to be constant along its columns (i.e., αij ≡ αj). We also ﬁtted versions of the model in which the nor- malization (denominator of eq. (5)) is constrained to marginal transformations (ICA-MG) or radial transformations (RG). For higher dimensional data, it is difﬁcult to visualize the densities, so we use other measures to evaluate the effectiveness of the model: Negentropy reduction. As an overall metric of model ﬁt, we evaluated the negentropy difference ∆J given in (4) on the full GDN model, as well as the marginal and radial models model (ICA-MG and RG, respectively). We ﬁnd that ICA-MG and RG reduce negentropy by 2.04 and 2.11 nats per pixel, respectively, whereas GDN reduces it by 2.43 nats. Marginal/radial distributions of transformed data. If the transformed data is multivariate stan- dard normal, its marginals should be standard normal, as well, and the radial component should be Chi distributed with degree 256. Figure 3 shows these distributions, in comparison to those of ICA- MG and RG. As expected from (Lyu & Simoncelli, 2009b), RG fails to Gaussianize the marginals, and ICA-MG fails to transform the radial component into a Chi distribution. GDN comes close to achieving both goals. Sampling. The density model induced by a transformation can also be visualized by examining samples drawn from a standard normal distribution that have been passed through the inverse trans- formation. Figure 4 compares sets of 25 image patches drawn from the GDN model, the ICA-MG model, and randomly selected from a database of images. GDN notably captures two features of natural images: First, a substantial fraction of the samples are constant or nearly so (as in the nat- ural images, which include patches of sky or untextured surfaces). Second, in the cases with more activity, the samples contain sparse “organic” structures (although less so than those drawn from the natural images). In comparison, the samples from the ICA-MG model are more jumbled, and ﬁlled with random mixtures of oriented elements. Denoising. The negentropy provides a particular metric for assessing the quality of our results, but it need not agree with other measures (Theis et al., 2015). Another test of a probability model comes  1downloaded from http://www.cipr.rpi.edu/resource/stills/kodak.html  7  051015202530||y||20.00.10.20.30.40.50.6chiICA-MGGDN3210123yi;i∈[0,1,,255]10-310-210-11003210123yi;i∈[0,1,,255]10-310-210-1100Published as a conference paper at ICLR 2016  Figure 4: Sample image patches. From left to right, 25 samples drawn from: the image training set; the ICA-MG model; the GDN model.  ˆx = ˜x + σ2 ∇ log p˜x(˜x),  from using it as a prior in a Bayesian inference problem. The most basic example is that of removing additive Gaussian noise. For GDN, we use the empirical Bayes solution of Miyasawa (1961), which expresses the least-squares optimal solution directly as a function of the distribution of the noisy data:  (7) where ˜x is the noisy observation, p˜x is the density of the noisy data, σ2 is the noise variance, and ˆx is the optimal estimate. Note that, counterintuitively, this expression does not refer directly to the prior density, but it is nevertheless exactly equivalent to the Bayesian least squares solution (Raphan & Simoncelli, 2011). Although the GDN model was developed for modeling the distribution of clean image data, we use it here to estimate the distribution of the noisy image data. We ﬁnd that, since the noisy density is a Gaussian-smoothed version of the original density, the model ﬁts the data well (results not shown). For comparison, we implemented two denoising methods that operate on orthogonal wavelet coef- ﬁcients, one assuming a marginal model (Figueiredo & Nowak, 2001), and the other an elliptically symmetric Gaussian scale mixture (GSM) model (Portilla et al., 2003). Since the GDN model is applied to 16 × 16 patches of pixels and is restricted to a complete (i.e., square matrix) linear trans- formation, we restrict the wavelet transform employed in the other two models to be orthogonal, and to include three scales. We also report numerical scores: the peak signal to noise ratio (PSNR), and the structural similarity index (SSIM; Wang et al., 2004) which provides a measure of perceptual quality. Fig. 5 shows the denoising results. Both marginal and spherical models produce results with strong artifacts resembling the basis functions of the respective linear transform. The GDN solution has artifacts that are less perceptually noticeable, while at the same time leaving a larger amount of background noise. Average model likelihood. To further assess how our model compares to existing work, we trained the model on image patches of 8 × 8 pixels from the BSDS300 dataset which had the patch mean removed (see Theis & Bethge, 2015, left column of table 1). We followed the same evaluation procedures as in that reference, and measured a cross-validated average log likelihood of 126.8 nats for ICA-MG and 151.5 nats for GDN, similar to the values reported there for the RIDE model, but worse than the best-performing MCGSM and RNADE models. On the other hand, GDN achieves a model likelihood of 3.47 bits/pixel for image patches of 8 × 8 pixels without mean removal, which is essentially equal to the best reported performance in the middle column of table 1 (ibid.).2  4.3 TWO-STAGE CASCADED MODEL  In the previous section, we show that the GDN transformation works well on local patches of im- ages. However, this cannot capture statistical dependencies over larger spatial distances (i.e., across adjacent patches). One way of achieving this is to cascade Gaussianizing transformations (Chen & Gopinath, 2000; Laparra et al., 2011). In previous implementations of such cascades, each stage of the transformation consists of a linear transformation (to rotate the previous responses, expos-  2Note, however, that the middle column in table 1 of Theis & Bethge (2015) was generated under the assumption that the patch mean is statistically independent from the rest of the data, which artiﬁcially impedes the performance of the reported models.  8  Published as a conference paper at ICLR 2016  noisy  marginal: PSNR 20.6, SSIM 0.68  GSM: PSNR 22.4, SSIM 0.75  GDN: PSNR 22.6, SSIM 0.78  Figure 5: Bayesian least squares denoising using different prior models. Top: noise-corrupted orig- inal; denoised with marginal model in an orthonormal wavelet decomposition. Bottom: denoised with GSM model in an orthonormal wavelet decomposition; denoised with GDN-induced density model. Below each image, errors against the original image are quantiﬁed with PSNR in dB, and the perceptual SSIM metric (for both measures, bigger is better).  ing additional non-Gaussian directions) and a Gaussianizing nonlinear transformation applied to the marginals. We have implemented a cascade based on GDN that beneﬁts from two innovations. First, by jointly Gaussianizing groups of coefﬁcients (rather than transforming each one independently), GDN achieves a much more signiﬁcant reduction in negentropy than MG (see Figure 1), thereby reducing the total number of stages that would be needed to fully Gaussianize the data. Second, we replace the ICA rotations with convolutional ICA (CICA; Ballé & Simoncelli, 2014). This is a better solution than either partitioning the image into non-overlapping blocks (which produces artifacts at block boundaries) or simply increasing the size of the transformation, which would require a much larger number of parameters for the linear transform than a convolutional solution (which allows “weight sharing” (LeCun et al., 1990)). The central question that determines effectiveness of a multi-layer model based on the above in- gredients is whether the parametric form of the normalization is suitable for Gaussianizing the data after it has been transformed by previous layers. According to our preliminary results, this seems to be the case. We constructed a two-stage model, trained greedily layer-by-layer, consisting of the transformations CICA–GDN–CICA–GDN. The ﬁrst CICA instance implements a complete, invert- ible linear transformation with a set of 256 convolutional ﬁlters of support 48 × 48, with each ﬁlter response subsampled by a factor of 16 (both horizontally and vertically). The output thus consists of 256 reduced-resolution feature maps. The ﬁrst GDN operation then acts on the 256-vectors of re- sponses at a given spatial location across all maps. Thus, the responses of the ﬁrst CICA–GDN stage are Gaussianized across maps, but not across spatial locations. The second-stage CICA instance is applied to vectors of ﬁrst-stage responses across all maps within a 9× 9 spatial neighborhood – thus seeking new non-Gaussian directions across spatial locations and across maps. Histogram estimates of the marginals of these directions are shown in ﬁgure 6. The distributions are qualitatively similar to those found for the ﬁrst stage CICA operating on image pixels, although their heavy-tailedness is less pronounced. The ﬁgure also shows histograms of the second-stage GDN marginals, indicating that the new directions have been effectively Gaussianized.  9  Published as a conference paper at ICLR 2016  Figure 6: Marginal histograms of two-stage model responses. Left: marginals of 256 features, obtained by performing linear CICA on ﬁrst stage GDN responses. Right: marginals after the second stage GDN. The thick gray line corresponds to a Gaussian distribution.  5 CONCLUSION  We have introduced a new probability model for natural images, implicitly deﬁned in terms of an in- vertible nonlinear transformation that is optimized so as to Gaussianize the data. This transformation is formed as the composition of a linear operation and a generalized form of divisive normalization, a local gain control operation commonly used to model response properties of sensory neurons. We developed an efﬁcient algorithm for ﬁtting the parameters of this transformation, minimizing the KL divergence of the distribution of transformed data against a Gaussian target. The resulting density model is not closed-form (because we need to evaluate the determinant of the Jacobian matrix), but it does allow direct computation of probability/likelihood, and is readily used for sampling and inference. Our parametric transformation includes previous variants of divisive normalization as special cases, and the induced density model generalizes forms of ICA/ISA and elliptically symmetric models. We show that the additional complexity of our generalized normalization transform allows a signiﬁcant increase in performance, in terms of Gaussianization, denoising, and sampling. In addition, we found that the ﬁtted parameters of our model (in particular, the interactions governed by γ) do not resemble any of these special cases (not shown), and we expect that their detailed structure will be useful in elucidating novel statistical properties of images. It will also be important to compare this induced density model more thoroughly to other model forms that have been proposed in the literature (e.g., ﬁnite mixtures of Gaussians or GSMs (Guerrero-Colón et al., 2008; Lyu & Simoncelli, 2009a; Zoran & Weiss, 2012; Theis et al., 2012), and sparse factorization (Culpepper et al., 2011)). Our method arises as a natural combination of concepts drawn from two different research endeav- ors. The ﬁrst aims to explain the architecture and functional properties of biological sensory systems as arising from principles of coding efﬁciency (Barlow, 1961; Rieke et al., 1995; Bell & Sejnowski, 1997; Schwartz & Simoncelli, 2001). A common theme in these studies is the idea that the hi- erarchical organization of the system acts to transform the raw sensory inputs into more compact, and statistically factorized, representations. Divisive normalization has been proposed as a trans- formation that contributes to this process. The new form we propose here is highly effective: the transformed data are signiﬁcantly closer to Gaussian than data transformed by either marginal or radial Gaussianization, and the induced density is thus a more factorized representation of the data. The second endeavor arises from the statistics literature on projection pursuit, and the use of Gaus- sianization in problems of density estimation (Friedman et al., 1984). More recent examples include marginal and radial transformations (Chen & Gopinath, 2000; Lyu & Simoncelli, 2009b; Laparra et al., 2011), as well as rectiﬁed-linear transformations (Dinh et al., 2014). Our preliminary exper- iments indicate that the fusion of a generalized variant of the normalization computation with the iterated Gaussianization architecture is feasible, both in terms of optimization and statistical validity. We believe this architecture offers a promising platform for unsupervised learning of probabilistic structures from data, and are currently investigating techniques to jointly optimize the stages of more deeply stacked models.  10  3210123yi;i∈[0,1,,255]10-310-210-11003210123yi;i∈[0,1,,255]10-310-210-1100Published as a conference paper at ICLR 2016  6 APPENDIX  6.1 NEGENTROPY  To see that the negentropy J of the transformed data y can be written as an expectation over the original data, consider a change of variables:  (cid:17)  (cid:16)  (cid:90) (cid:90)  =  =  J(py) = Ey  log py(y) − log N (y)  py(y)  log py(y) − log N (y)  (cid:16) (cid:12)(cid:12)(cid:12) ∂y  ∂x  (cid:12)(cid:12)(cid:12)−1(cid:32)  dy  (cid:17) (cid:33)(cid:12)(cid:12)(cid:12) ∂y (cid:12)(cid:12)(cid:12) ∂y (cid:12)(cid:12)(cid:12)−1(cid:17) − log N (y) (cid:33) (cid:12)(cid:12)(cid:12) − log N (y)  ∂x  ∂x  (cid:16) (cid:12)(cid:12)(cid:12) ∂y  ∂x  log  px(x)  (cid:12)(cid:12)(cid:12) dx  px(x)  (cid:32)  = Ex  log px(x) − log  INVERTIBILITY  6.2 Here, we show that a transformation g : x (cid:55)→ y is invertible if it is continuous and its Jacobian g(cid:48) : x (cid:55)→ ∂y ∂x positive deﬁnite everywhere. First note that g is invertible if and only if any two nonidentical inputs xa, xb are mapped to nonidentical outputs ya, yb, and vice versa:  ∀xa, xb : xa (cid:54)= xb ⇔ ya (cid:54)= yb.  Since g is a function, the left-hand inequality follows trivially from the right. To see the converse direction, we can write the inequality of the two right-hand side vectors as  where ∆y is their difference. Second, we can compute ∆y by integrating the Jacobian along a straight line Lab between xa and xb:  Writing the integral as a Riemann limit, invertibility can be stated as:  xa (cid:54)= xb ⇔ ∃u : u(cid:62)∆y = lim T→∞  xa + t  ∆x T  (cid:17) ∆x  T  (cid:54)= 0.  If ∆x (cid:54)= 0 (the left-hand inequality is true) and g(cid:48) is positive deﬁnite everywhere, all terms in the T . Hence, for g(cid:48) positive deﬁnite, the right-hand sum can be made positive by choosing u = ∆x inequality follows from the left.  6.3 PREPROCESSING  We performed two preprocessing steps on the van Hateren dataset before ﬁtting our model: removal of images with saturation artifacts and passing the intensity values through a nonlinearity. To remove heavily saturated images, we computed a histogram of intensity values for each of the images. If more than 0.1% of the pixel values were contained in the highest-valued histogram bin, we removed the image from the dataset. We passed the remaining 2904 images through a pointwise nonlinearity. In the literature, a logarithm is most commonly used, although there is no particularly convincing reason for using precisely this function. Since the GDN densities are zero-mean by deﬁnition, mean removal is necessary to ﬁt the density. Instead of the logarithm, we used the inverse of a generalized logistic function, which is very similar, but can be chosen to marginally Gaussianize the intensity values, which is in line with our objective and also removes the mean. For the Kodak dataset, we converted the integer RGB values to linear luminance intensities using the transformation speciﬁed in the sRGB colorspace deﬁnition. Then, a pointwise nonlinearity was ﬁtted to the data to remove the mean and marginally Gaussianize the intensity values, analogous to the nonlinearity used on the van Hateren dataset. To follow established conventions for the denoising experiment, the gamma removal/pointwise nonlinearity was considered part of the model.  11  ∃u : u(cid:62)∆y (cid:54)= 0,  (cid:90)  ∆y =  Lab  g(cid:48)(x) dx.  u(cid:62)g(cid:48)(cid:16)  T−1(cid:88)  t=0  Published as a conference paper at ICLR 2016  ACKNOWLEDGMENTS  JB and EPS were supported by the Howard Hughes Medical Institute. VL was supported by the APOSTD/2014/095 Generalitat Valenciana grant (Spain).  REFERENCES Ballé, Johannes and Simoncelli, Eero P. Learning sparse ﬁlterbank transforms with convolutional ICA. In 2014 IEEE International Conference on Image Processing (ICIP), 2014. doi: 10.1109/ ICIP.2014.7025815.  Barlow, Horace B. Possible principles underlying the transformations of sensory messages.  Sensory Communication, pp. 217–234. M.I.T. Press, 1961. ISBN 978-0-262-51842-0.  In  Bell, Anthony J. and Sejnowski, Terrence J. The independent components of natural scenes are edge  ﬁlters. Vision Research, 37(23), 1997. doi: 10.1016/S0042-6989(97)00121-1.  Carandini, Matteo and Heeger, David J. Normalization as a canonical neural computation. Nature  Reviews Neuroscience, 13, January 2012. doi: 10.1038/nrn3136.  Cardoso, Jean-François. Dependence, correlation and Gaussianity in independent component anal-  ysis. Journal of Machine Learning Research, 4:1177–1203, 2003. ISSN 1533-7928.  Chen, Scott Saobing and Gopinath, Ramesh A. Gaussianization. In Advances in Neural Information  Processing Systems 13, pp. 423–429, 2000.  Culpepper, B. J., Sohl-Dickstein, J., and Olshausen, Bruno. Building a better probabilistic model of images by factorization. In 2011 IEEE International Conference on Computer Vision (ICCV), 2011. doi: 10.1109/ICCV.2011.6126473.  Dinh, Laurent, Krueger, David, and Bengio, Yoshua. NICE: Non-linear independent components estimation. arXiv e-prints, 2014. Published as a conference paper at the 3rd International Confer- ence for Learning Representations, San Diego, 2015.  Figueiredo, M. A. T. and Nowak, R. D. Wavelet-based image estimation: an empirical bayes ap- IEEE Transactions on Image Processing, 10(9),  proach using Jeffrey’s noninformative prior. September 2001. doi: 10.1109/83.941856.  Friedman, Jerome H., Stuetzle, Werner, and Schroeder, Anne. Projection pursuit density estimation. Journal of the American Statistical Association, 79(387), 1984. doi: 10.1080/01621459.1984. 10478086.  Guerrero-Colón, J. A., Simoncelli, Eero P., and Portilla, Javier. Image denoising using mixtures of Gaussian scale mixtures. In 15th IEEE International Conference on Image Processing, 2008, 2008. doi: 10.1109/ICIP.2008.4711817.  Heeger, David J. Normalization of cell responses in cat striate cortex. Visual Neuroscience, 9(2),  1992. doi: 10.1017/S0952523800009640.  Hyvärinen, Aapo. Estimation of non-normalized statistical models by score matching. Journal of  Machine Learning Research, 6:695–709, 2005. ISSN 1533-7928.  Hyvärinen, Aapo and Hoyer, Patrik. Emergence of phase- and shift-invariant features by decompo- sition of natural images into independent feature subspaces. Neural Computation, 12(7), 2000. doi: 10.1162/089976600300015312.  Hyvärinen, Aapo, Hoyer, Patrik, and Inki, Mika. Topographic independent component analysis.  Neural Computation, 13(7), 2001. doi: 10.1162/089976601750264992.  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. What is the best multi-stage architecture for object recognition? In 2009 IEEE 12th International Conference on Computer Vision, 2009. doi: 10.1109/ICCV.2009.5459469.  Jolliffe, I. T. Principal Component Analysis. Springer, 2 edition, 2002. ISBN 978-0-387-95442-4.  12  Published as a conference paper at ICLR 2016  Kingma, Diederik P. and Ba, Jimmy Lei. Adam: A method for stochastic optimization. arXiv e- prints, 2014. Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.  Köster, Urs and Hyvärinen, Aapo. A two-layer model of natural stimuli estimated with score match-  ing. Neural Computation, 22(9), 2010. doi: 10.1162/NECO_a_00010.  Laparra, Valero, Camps-Valls, Gustavo, and Malo, Jesús. Iterative Gaussianization: From ICA to random rotations. IEEE Transactions on Neural Networks, 22(4), April 2011. doi: 10.1109/TNN. 2011.2106511.  LeCun, Yann, Matan, O., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., Jackel, L. D., and Baird, H. S. Handwritten zip code recognition with multilayer networks. In Proceedings, 10th International Conference on Pattern Recognition, volume 2, 1990. doi: 10. 1109/ICPR.1990.119325.  Lyu, Siwei. Divisive normalization: Justiﬁcation and effectiveness as efﬁcient coding transform. In  Advances in Neural Information Processing Systems 23, pp. 1522–1530, 2010.  Lyu, Siwei and Simoncelli, Eero P. Nonlinear image representation using divisive normalization. In 2008 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2008. doi: 10.1109/CVPR.2008.4587821.  Lyu, Siwei and Simoncelli, Eero P. Modeling multiscale subbands of photographic images with ﬁelds of Gaussian scale mixtures. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 31(4), April 2009a. doi: 10.1109/TPAMI.2008.107.  Lyu, Siwei and Simoncelli, Eero P. Nonlinear extraction of independent components of natural images using radial Gaussianization. Neural Computation, 21(6), 2009b. doi: 10.1162/neco. 2009.04-08-773.  Malo, Jesús and Laparra, Valero. Psychophysically tuned divisive normalization approximately factorizes the PDF of natural images. Neural Computation, 22(12), 2010. doi: 10.1162/NECO_ a_00046.  Malo, Jesús, Epifanio, I., Navarro, R., and Simoncelli, Eero P. Non-linear image representation for efﬁcient perceptual coding. IEEE Transactions on Image Processing, 15(1), January 2006. doi: 10.1109/TIP.2005.860325.  Miyasawa, K. An empirical bayes estimator of the mean of a normal population. Bulletin de l’Institut  international de Statistique, 38:181–188, 1961.  Portilla, Javier, Strela, Vasily, Wainwright, Martin J., and Simoncelli, Eero P. Image denoising using scale mixtures of Gaussians in the wavelet domain. IEEE Transactions on Image Processing, 12 (11), November 2003. doi: 10.1109/TIP.2003.818640.  Raphan, Martin and Simoncelli, Eero P. Least squares estimation without priors or supervision.  Neural Computation, 23(2), February 2011. doi: 10.1162/NECO_a_00076.  Rieke, F., Bodnar, D. A., and Bialek, W. Naturalistic stimuli increase the rate and efﬁciency of information transmission by primary auditory afferents. Proceedings of the Royal Society of London B: Biological Sciences, 262(1365), 1995. doi: 10.1098/rspb.1995.0204.  Ruderman, Daniel L. The statistics of natural images. Network: Computation in Neural Systems, 5,  1994. doi: 10.1088/0954-898X_5_4_006.  Schwartz, Odelia and Simoncelli, Eero P. Natural signal statistics and sensory gain control. Nature  Neuroscience, 4(8), August 2001. doi: 10.1038/90526.  Simoncelli, Eero P. and Freeman, William T. The steerable pyramid: A ﬂexible architecture for multi-scale derivative computation. In 1995 IEEE International Conference on Image Processing (ICIP), volume 3, 1995. doi: 10.1109/ICIP.1995.537667.  13  Published as a conference paper at ICLR 2016  Sinz, Fabian and Bethge, Matthias. Lp-nested symmetric distributions. Journal of Machine Learning  Research, 11:3409–3451, 2010. ISSN 1533-7928.  Sinz, Fabian and Bethge, Matthias. What is the limit of redundancy reduction with divisive normal-  ization? Neural Computation, 25(11), 2013. doi: 10.1162/NECO_a_00505.  Theis, Lucas and Bethge, Matthias. Generative image modeling using spatial LSTMs. In Advances  in Neural Information Processing Systems 28, pp. 1918–1926, 2015.  Theis, Lucas, Hosseini, Reshad, and Bethge, Matthias. Mixtures of conditional Gaussian scale mixtures applied to multiscale image representations. PLoS one, 7(7), 2012. doi: 10.1371/ journal.pone.0039857.  Theis, Lucas, van den Oord, Aäron, and Bethge, Matthias. A note on the evaluation of genera- tive models. arXiv e-prints, 2015. Under review as a conference paper at the 4th International Conference for Learning Representations, San Juan, 2016.  van Hateren, J. H. and van der Schaaf, A. Independent component ﬁlters of natural images com- pared with simple cells in primary visual cortex. Proceedings of the Royal Society of London B: Biological Sciences, 265(1394), 1998. doi: 10.1098/rspb.1998.0303.  Wainwright, Martin J. and Simoncelli, Eero P. Scale mixtures of Gaussians and the statistics of  natural images. In Advances in Neural Information Processing Systems 12, pp. 855–861, 2000.  Wang, Zhou, Bovik, Alan Conrad, Sheikh, H. R., and Simoncelli, Eero P. Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4), April 2004. doi: 10.1109/TIP.2003.819861.  Zoran, Daniel and Weiss, Yair. Natural images, Gaussian mixtures and dead leaves. In Advances in  Neural Information Processing Systems 25, pp. 1736–1744, 2012.  14  ",
1511.07122,2016,Multi-Scale Context Aggregation by Dilated Convolutions,"['Multi-Scale Context Aggregation by Dilated Convolutions\nFisher Yu', 'Vladlen Koltun']",https://arxiv.org/pdf/1511.07122,"6 1 0 2    r p A 0 3         ]  V C . s c [      3 v 2 2 1 7 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS  Fisher Yu Princeton University  Vladlen Koltun Intel Labs  ABSTRACT  State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classiﬁca- tion. However, dense prediction problems such as semantic segmentation are structurally different from image classiﬁcation. In this work, we develop a new convolutional network module that is speciﬁcally designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi- scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive ﬁeld without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classiﬁcation networks to dense prediction and show that simplifying the adapted network can increase accuracy.  1  INTRODUCTION  Many natural problems in computer vision are instances of dense prediction. The goal is to com- pute a discrete or continuous label for each pixel in the image. A prominent example is semantic segmentation, which calls for classifying each pixel into one of a given set of categories (He et al., 2004; Shotton et al., 2009; Kohli et al., 2009; Kr¨ahenb¨uhl & Koltun, 2011). Semantic segmenta- tion is challenging because it requires combining pixel-level accuracy with multi-scale contextual reasoning (He et al., 2004; Galleguillos & Belongie, 2010). Signiﬁcant accuracy gains in semantic segmentation have recently been obtained through the use of convolutional networks (LeCun et al., 1989) trained by backpropagation (Rumelhart et al., 1986). Speciﬁcally, Long et al. (2015) showed that convolutional network architectures that had originally been developed for image classiﬁcation can be successfully repurposed for dense prediction. These reporposed networks substantially outperform the prior state of the art on challenging semantic seg- mentation benchmarks. This prompts new questions motivated by the structural differences between image classiﬁcation and dense prediction. Which aspects of the repurposed networks are truly nec- essary and which reduce accuracy when operated densely? Can dedicated modules designed specif- ically for dense prediction improve accuracy further? Modern image classiﬁcation networks integrate multi-scale contextual information via succes- sive pooling and subsampling layers that reduce resolution until a global prediction is obtained (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015). In contrast, dense prediction calls for multi- scale contextual reasoning in combination with full-resolution output. Recent work has studied two approaches to dealing with the conﬂicting demands of multi-scale reasoning and full-resolution dense prediction. One approach involves repeated up-convolutions that aim to recover lost resolu- tion while carrying over the global perspective from downsampled layers (Noh et al., 2015; Fischer et al., 2015). This leaves open the question of whether severe intermediate downsampling was truly necessary. Another approach involves providing multiple rescaled versions of the image as input to the network and combining the predictions obtained for these multiple inputs (Farabet et al., 2013; Lin et al., 2015; Chen et al., 2015b). Again, it is not clear whether separate analysis of rescaled input images is truly necessary.  1  Published as a conference paper at ICLR 2016  In this work, we develop a convolutional network module that aggregates multi-scale contextual information without losing resolution or analyzing rescaled images. The module can be plugged into existing architectures at any resolution. Unlike pyramid-shaped architectures carried over from image classiﬁcation, the presented context module is designed speciﬁcally for dense prediction. It is a rectangular prism of convolutional layers, with no pooling or subsampling. The module is based on dilated convolutions, which support exponential expansion of the receptive ﬁeld without loss of resolution or coverage. As part of this work, we also re-examine the performance of repurposed image classiﬁcation net- works on semantic segmentation. The performance of the core prediction modules can be uninten- tionally obscured by increasingly elaborate systems that involve structured prediction, multi-column architectures, multiple training datasets, and other augmentations. We therefore examine the leading adaptations of deep image classiﬁcation networks in a controlled setting and remove vestigial com- ponents that hinder dense prediction performance. The result is an initial prediction module that is both simpler and more accurate than prior adaptations. Using the simpliﬁed prediction module, we evaluate the presented context network through con- trolled experiments on the Pascal VOC 2012 dataset (Everingham et al., 2010). The experiments demonstrate that plugging the context module into existing semantic segmentation architectures re- liably increases their accuracy.  (cid:88) (cid:88)  s+t=p  2 DILATED CONVOLUTIONS Let F : Z2 → R be a discrete function. Let Ωr = [−r, r]2 ∩ Z2 and let k : Ωr → R be a discrete ﬁlter of size (2r + 1)2. The discrete convolution operator ∗ can be deﬁned as  (F ∗ k)(p) =  F (s) k(t).  (1)  We now generalize this operator. Let l be a dilation factor and let ∗l be deﬁned as  (F ∗l k)(p) =  s+lt=p  F (s) k(t).  (2) We will refer to ∗l as a dilated convolution or an l-dilated convolution. The familiar discrete convo- lution ∗ is simply the 1-dilated convolution. The dilated convolution operator has been referred to in the past as “convolution with a dilated ﬁlter”. It plays a key role in the algorithme `a trous, an algorithm for wavelet decomposition (Holschneider et al., 1987; Shensa, 1992).1 We use the term “dilated convolution” instead of “convolution with a dilated ﬁlter” to clarify that no “dilated ﬁlter” is constructed or represented. The convolution opera- tor itself is modiﬁed to use the ﬁlter parameters in a different way. The dilated convolution operator can apply the same ﬁlter at different ranges using different dilation factors. Our deﬁnition reﬂects the proper implementation of the dilated convolution operator, which does not involve construction of dilated ﬁlters. In recent work on convolutional networks for semantic segmentation, Long et al. (2015) analyzed ﬁlter dilation but chose not to use it. Chen et al. (2015a) used dilation to simplify the architec- ture of Long et al. (2015). In contrast, we develop a new convolutional network architecture that systematically uses dilated convolutions for multi-scale context aggregation. Our architecture is motivated by the fact that dilated convolutions support exponentially expanding receptive ﬁelds without losing resolution or coverage. Let F0, F1, . . . , Fn−1 : Z2 → R be discrete functions and let k0, k1, . . . , kn−2 : Ω1 → R be discrete 3×3 ﬁlters. Consider applying the ﬁlters with exponentially increasing dilation:  Fi+1 = Fi ∗2i ki  for i = 0, 1, . . . , n − 2.  (3)  Deﬁne the receptive ﬁeld of an element p in Fi+1 as the set of elements in F0 that modify the value of Fi+1(p). Let the size of the receptive ﬁeld of p in Fi+1 be the number of these elements. It is 1Some recent work mistakenly referred to the dilated convolution operator itself as the algorithme `a trous. This is incorrect. The algorithme `a trous applies a ﬁlter at multiple scales to produce a signal decomposition. The algorithm uses dilated convolutions, but is not equivalent to the dilated convolution operator itself.  2  Published as a conference paper at ICLR 2016  (a)  (b)  (c)  Figure 1: Systematic dilation supports exponential expansion of the receptive ﬁeld without loss of resolution or coverage. (a) F1 is produced from F0 by a 1-dilated convolution; each element in F1 has a receptive ﬁeld of 3×3. (b) F2 is produced from F1 by a 2-dilated convolution; each element in F2 has a receptive ﬁeld of 7× 7. (c) F3 is produced from F2 by a 4-dilated convolution; each element in F3 has a receptive ﬁeld of 15×15. The number of parameters associated with each layer is identical. The receptive ﬁeld grows exponentially while the number of parameters grows linearly.  easy to see that the size of the receptive ﬁeld of each element in Fi+1 is (2i+2 − 1)×(2i+2 − 1). The receptive ﬁeld is a square of exponentially increasing size. This is illustrated in Figure 1.  3 MULTI-SCALE CONTEXT AGGREGATION  The context module is designed to increase the performance of dense prediction architectures by aggregating multi-scale contextual information. The module takes C feature maps as input and produces C feature maps as output. The input and output have the same form, thus the module can be plugged into existing dense prediction architectures. We begin by describing a basic form of the context module. In this basic form, each layer has C channels. The representation in each layer is the same and could be used to directly obtain a dense per-class prediction, although the feature maps are not normalized and no loss is deﬁned inside the module. Intuitively, the module can increase the accuracy of the feature maps by passing them through multiple layers that expose contextual information. The basic context module has 7 layers that apply 3×3 convolutions with different dilation factors. The dilations are 1, 1, 2, 4, 8, 16, and 1. Each convolution operates on all layers: strictly speaking, these are 3×3×C convolutions with dilation in the ﬁrst two dimensions. Each of these convolutions is followed by a pointwise truncation max(·, 0). A ﬁnal layer performs 1×1×C convolutions and produces the output of the module. The architecture is summarized in Table 1. Note that the front- end module that provides the input to the context network in our experiments produces feature maps at 64×64 resolution. We therefore stop the exponential expansion of the receptive ﬁeld after layer 6. Our initial attempts to train the context module failed to yield an improvement in prediction accuracy. Experiments revealed that standard initialization procedures do not readily support the training of the module. Convolutional networks are commonly initialized using samples from random distributions (Glorot & Bengio, 2010; Krizhevsky et al., 2012; Simonyan & Zisserman, 2015). However, we found that random initialization schemes were not effective for the context module. We found an alternative initialization with clear semantics to be much more effective:  kb(t, a) = 1[t=0]1[a=b],  (4)  where a is the index of the input feature map and b is the index of the output map. This is a form of identity initialization, which has recently been advocated for recurrent networks (Le et al., 2015). This initialization sets all ﬁlters such that each layer simply passes the input directly to the next. A natural concern is that this initialization could put the network in a mode where backpropagation cannot signiﬁcantly improve the default behavior of simply passing information through. However, experiments indicate that this is not the case. Backpropagation reliably harvests the contextual information provided by the network to increase the accuracy of the processed maps.  3  Published as a conference paper at ICLR 2016  Layer Convolution Dilation Truncation Receptive ﬁeld Output channels Basic Large  1 3×3 1 Yes 3×3  2 3×3 1 Yes 5×5  3 3×3 2 Yes 9×9  4 3×3 4 Yes 17×17  5 3×3 8 Yes 33×33  6 3×3 16 Yes 65×65  7 3×3 1 Yes 67×67  8 1×1 1 No  67×67  C 2C  C 2C  C 4C  C 8C  C  16C  C  32C  C  32C  C C  Table 1: Context network architecture. The network processes C feature maps by aggregating contextual information at progressively increasing scales without losing resolution.  This completes the presentation of the basic context network. Our experiments show that even this basic module can increase dense prediction accuracy both quantitatively and qualitatively. This is particularly notable given the small number of parameters in the network: ≈ 64C 2 parameters in total. We have also trained a larger context network that uses a larger number of feature maps in the deeper layers. The number of maps in the large network is summarized in Table 1. We generalize the initialization scheme to account for the difference in the number of feature maps in different layers. Let ci and ci+1 be the number of feature maps in two consecutive layers. Assume that C divides both ci and ci+1. The initialization is   C  ε  ci+1  t = 0 and  otherwise  (cid:22) bC  (cid:23)  (cid:22) aC  (cid:23)  ci  =  kb(t, a) =  ci+1  (5)  Here ε ∼ N (0, σ2) and σ (cid:28) C/ci+1. The use of random noise breaks ties among feature maps with a common predecessor.  4 FRONT END  We implemented and trained a front-end prediction module that takes a color image as input and produces C = 21 feature maps as output. The front-end module follows the work of Long et al. (2015) and Chen et al. (2015a), but was implemented separately. We adapted the VGG-16 network (Simonyan & Zisserman, 2015) for dense prediction and removed the last two pooling and striding layers. Speciﬁcally, each of these pooling and striding layers was removed and convolutions in all subsequent layers were dilated by a factor of 2 for each pooling layer that was ablated. Thus convolutions in the ﬁnal layers, which follow both ablated pooling layers, are dilated by a factor of 4. This enables initialization with the parameters of the original classiﬁcation network, but produces higher-resolution output. The front-end module takes padded images as input and produces feature maps at resolution 64× 64. We use reﬂection padding: the buffer zone is ﬁlled by reﬂecting the image about each edge. Our front-end module is obtained by removing vestiges of the classiﬁcation network that are counter- productive for dense prediction. Most signiﬁcantly, we remove the last two pooling and striding layers entirely, whereas Long et al. kept them and Chen et al. replaced striding by dilation but kept the pooling layers. We found that simplifying the network by removing the pooling layers made it more accurate. We also remove the padding of the intermediate feature maps. Intermediate padding was used in the original classiﬁcation network, but is neither necessary nor justiﬁed in dense prediction. This simpliﬁed prediction module was trained on the Pascal VOC 2012 training set, augmented by the annotations created by Hariharan et al. (2011). We did not use images from the VOC-2012 validation set for training and therefore only used a subset of the annotations of Hariharan et al. (2011). Training was performed by stochastic gradient descent (SGD) with mini-batch size 14, learning rate 10−3, and momentum 0.9. The network was trained for 60K iterations. We now compare the accuracy of our front-end module to the FCN-8s design of Long et al. (2015) and the DeepLab network of Chen et al. (2015a). For FCN-8s and DeepLab, we evaluate the public  4  Published as a conference paper at ICLR 2016  (a) Image  (b) FCN-8s  (c) DeepLab  (d) Our front end  (e) Ground truth  Figure 2: Semantic segmentations produced by different adaptations of the VGG-16 classiﬁcation network. From left to right: (a) input image, (b) prediction by FCN-8s (Long et al., 2015), (c) prediction by DeepLab (Chen et al., 2015a), (d) prediction by our simpliﬁed front-end module, (e) ground truth.  o r e a  e k i b  d r i b  t a o b  e l t t o b  s u b  r a c  t a c  r i a h c  w o c  e l b a t  g o d  n a e m 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2 FCN-8s DeepLab 72 31 71.2 53.7 60.5 77 71.9 73.1 25.2 62.6 49.1 68.7 63.3 73.9 73.6 50.8 72.3 42.1 67.9 52.6 62.1 DeepLab-Msc 74.9 34.1 72.6 52.9 61.0 77.9 73.0 73.7 26.4 62.2 49.3 68.4 64.1 74.0 75.0 51.7 72.7 42.5 67.2 55.7 62.9 70 51.6 73.1 72.8 81.5 79.1 56.6 77.1 49.9 75.3 60.9 67.6 Our front end  82.2 37.4 72.7 57.1 62.7 82.8 77.8 78.9 28  e s r o h  e k i b m  n o s r e p  t n a l p  p e e h s  a f o s  n i a r t  v t  U o I  Table 2: Our front-end prediction module is simpler and more accurate than prior models. This table reports accuracy on the VOC-2012 test set.  models trained by the original authors on VOC-2012. Segmentations produced by the different models on images from the VOC-2012 dataset are shown in Figure 2. The accuracy of the models on the VOC-2012 test set is reported in Table 2. Our front-end prediction module is both simpler and more accurate than the prior models. Specif- ically, our simpliﬁed model outperforms both FCN-8s and the DeepLab network by more than 5 percentage points on the test set. Interestingly, our simpliﬁed front-end module outperforms the leaderboard accuracy of DeepLab+CRF on the test set by more than a percentage point (67.6% vs. 66.4%) without using a CRF.  5  Published as a conference paper at ICLR 2016  5 EXPERIMENTS  Our implementation is based on the Caffe library (Jia et al., 2014). Our implementation of dilated convolutions is now part of the stanfard Caffe distribution. For fair comparison with recent high-performing systems, we trained a front-end module that has the same structure as described in Section 4, but is trained on additional images from the Microsoft COCO dataset (Lin et al., 2014). We used all images in Microsoft COCO with at least one object from the VOC-2012 categories. Annotated objects from other categories were treated as background. Training was performed in two stages. In the ﬁrst stage, we trained on VOC-2012 images and Microsoft COCO images together. Training was performed by SGD with mini-batch size 14 and momentum 0.9. 100K iterations were performed with a learning rate of 10−3 and 40K subsequent iterations were performed with a learning rate of 10−4. In the second stage, we ﬁne-tuned the network on VOC-2012 images only. Fine-tuning was performed for 50K iterations with a learning rate of 10−5. Images from the VOC-2012 validation set were not used for training. The front-end module trained by this procedure achieves 69.8% mean IoU on the VOC-2012 vali- dation set and 71.3% mean IoU on the test set. Note that this level of accuracy is achieved by the front-end alone, without the context module or structured prediction. We again attribute this high accuracy in part to the removal of vestigial components originally developed for image classiﬁcation rather than dense prediction.  Controlled evaluation of context aggregation. We now perform controlled experiments to eval- uate the utility of the context network presented in Section 3. We begin by plugging each of the two context modules (Basic and Large) into the front end. Since the receptive ﬁeld of the context net- work is 67×67, we pad the input feature maps by a buffer of width 33. Zero padding and reﬂection padding yielded similar results in our experiments. The context module accepts feature maps from the front end as input and is given this input during training. Joint training of the context module and the front-end module did not yield a signiﬁcant improvement in our experiments. The learning rate was set to 10−3. Training was initialized as described in Section 3. Table 3 shows the effect of adding the context module to three different architectures for semantic segmentation. The ﬁrst architecture (top) is the front end described in Section 4. It performs seman- tic segmentation without structured prediction, akin to the original work of Long et al. (2015). The second architecture (Table 3, middle) uses the dense CRF to perform structured prediction, akin to the system of Chen et al. (2015a). We use the implementation of Kr¨ahenb¨uhl & Koltun (2011) and train the CRF parameters by grid search on the validation set. The third architecture (Table 3, bot- tom) uses the CRF-RNN for structured prediction (Zheng et al., 2015). We use the implementation of Zheng et al. (2015) and train the CRF-RNN in each condition. The experimental results demonstrate that the context module improves accuracy in each of the three conﬁgurations. The basic context module increases accuracy in each conﬁguration. The large context module increases accuracy by a larger margin. The experiments indicate that the context module and structured prediction are synergisic: the context module increases accuracy with or without subsequent structured prediction. Qualitative results are shown in Figure 3.  Evaluation on the test set. We now perform an evaluation on the test set by submitting our re- sults to the Pascal VOC 2012 evaluation server. The results are reported in Table 4. We use the large context module for these experiments. As the results demonstrate, the context module yields a signiﬁcant boost in accuracy over the front end. The context module alone, without subsequent structured prediction, outperforms DeepLab-CRF-COCO-LargeFOV (Chen et al., 2015a). The con- text module with the dense CRF, using the original implementation of Kr¨ahenb¨uhl & Koltun (2011), performs on par with the very recent CRF-RNN (Zheng et al., 2015). The context module in com- bination with the CRF-RNN further increases accuracy over the performance of the CRF-RNN.  6 CONCLUSION  We have examined convolutional network architectures for dense prediction. Since the model must produce high-resolution output, we believe that high-resolution operation throughout the network  6  Published as a conference paper at ICLR 2016  (a) Image  (b) Front end  (c) + Context  (d) + CRF-RNN  (e) Ground truth  Figure 3: Semantic segmentations produced by different models. From left to right: (a) input image, (b) prediction by the front-end module, (c) prediction by the large context network plugged into the front end, (d) prediction by the front end + context module + CRF-RNN, (e) ground truth.  o r e a  e k i b  d r i b  t a o b  e l t t o b  s u b  r a c  t a c  r i a h c  w o c  e l b a t  g o d  n a e m 86.3 38.2 76.8 66.8 63.2 87.3 78.7 82 33.7 76.7 53.5 73.7 76 76.6 83 51.9 77.8 44 79.9 66.3 69.8 Front end 86.4 37.6 78.5 66.3 64.1 89.9 79.9 84.9 36.1 79.4 55.8 77.6 81.6 79 83.1 51.2 81.3 43.7 82.3 65.7 71.3 Front + Basic 87.3 39.2 80.3 65.6 66.4 90.2 82.6 85.8 34.8 81.9 51.7 79 84.1 80.9 83.2 51.2 83.2 44.7 83.4 65.6 72.1 Front + Large 89.2 38.8 80 69.8 63.2 88.8 80 85.2 33.8 80.6 55.5 77.1 80.8 77.3 84.3 53.1 80.4 45 80.7 67.9 71.6 Front end + CRF 57 79.6 83.6 79.9 84.6 52.7 83.3 44.3 82.6 67.2 72.7 Front + Basic + CRF 89.1 38.7 81.4 67.4 65 Front + Large + CRF 89.6 39.9 82.7 66.7 67.5 91.1 83.3 87.4 36 83.3 52.5 80.7 85.7 81.8 84.4 52.6 84.4 45.3 83.7 66.7 73.3 88.8 38.1 80.8 69.1 65.6 89.9 79.6 85.7 36.3 83.6 57.3 77.9 83.2 77 84.6 54.7 82.1 46.9 80.9 66.7 72.5 Front end + RNN Front + Basic + RNN 89 38.4 82.3 67.9 65.2 91.5 80.4 87.2 38.4 82.1 57.7 79.9 85 79.6 84.5 53.5 84 45 82.8 66.2 73.1 Front + Large + RNN 89.3 39.2 83.6 67.2 69 92.1 83.1 88 38.4 84.8 55.3 81.2 86.7 81.3 84.3 53.6 84.4 45.8 83.8 67 73.9  81 86.7 37.5 81  91  e s r o h  e k i b m  n o s r e p  t n a l p  p e e h s  a f o s  n i a r t  v t  U o I  Table 3: Controlled evaluation of the effect of the context module on the accuracy of three different architectures for semantic segmentation. Experiments performed on the VOC-2012 validation set. Validation images were not used for training. Top: adding the context module to a semantic segmen- tation front end with no structured prediction (Long et al., 2015). The basic context module increases accuracy, the large module increases it by a larger margin. Middle: the context module increases accuracy when plugged into a front-end + dense CRF conﬁguration (Chen et al., 2015a). Bottom: the context module increases accuracy when plugged into a front-end + CRF-RNN conﬁguration (Zheng et al., 2015).  is both feasible and desirable. Our work shows that the dilated convolution operator is particularly suited to dense prediction due to its ability to expand the receptive ﬁeld without losing resolution or coverage. We have utilized dilated convolutions to design a new network structure that reliably increases accuracy when plugged into existing semantic segmentation systems. As part of this work, we have also shown that the accuracy of existing convolutional networks for semantic segmentation can be increased by removing vestigial components that had been developed for image classiﬁcation.  7  Published as a conference paper at ICLR 2016  o r e a  e k i b  d r i b  t a o b  e l t t o b  s u b  r a c  t a c  r i a h c  w o c  e l b a t  g o d  DeepLab++ DeepLab-MSc++ CRF-RNN Front end Context Context + CRF Context + CRF-RNN 91.7 39.6 87.8 63.1 71.8 89.7 82.9 89.8 37.2 84  n a e m 89.1 38.3 88.1 63.3 69.7 87.1 83.1 85 29.3 76.5 56.5 79.8 77.9 85.8 82.4 57.4 84.3 54.9 80.5 64.1 72.7 89.2 46.7 88.5 63.5 68.4 87.0 81.2 86.3 32.6 80.7 62.4 81.0 81.3 84.3 82.1 56.2 84.6 58.3 76.2 67.2 73.9 90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 86.4 81.8 58.6 82.4 53.5 77.4 70.1 74.7 86.6 37.3 84.9 62.4 67.3 86.2 81.2 82.1 32.6 77.4 58.3 75.9 81 83.6 82.3 54.2 81.5 50.1 77.5 63 71.3 89.1 39.1 86.8 62.6 68.9 88.2 82.6 87.7 33.8 81.2 59.2 81.8 87.2 83.3 83.6 53.6 84.9 53.7 80.5 62.9 73.5 91.3 39.9 88.9 64.3 69.8 88.9 82.6 89.7 34.7 82.7 59.5 83 88.4 84.2 85 55.3 86.7 54.4 81.9 63.6 74.7 63 83.3 89 83.8 85.1 56.8 87.6 56 80.2 64.7 75.3  e s r o h  e k i b m  n o s r e p  t n a l p  p e e h s  a f o s  n i a r t  v t  U o I  Table 4: Evaluation on the VOC-2012 test set. ‘DeepLab++’ stands for DeepLab-CRF-COCO- LargeFOV and ‘DeepLab-MSc++’ stands for DeepLab-MSc-CRF-LargeFOV-COCO-CrossJoint (Chen et al., 2015a). ‘Context’ refers to the large context module plugged into our front end. The context network yields very high accuracy, ourperforming the DeepLab++ architecture without performing structured prediction. Combining the context network with the CRF-RNN structured prediction module increases the accuracy of the CRF-RNN system.  ‘CRF-RNN’ is the system of Zheng et al. (2015).  We believe that the presented work is a step towards dedicated architectures for dense prediction that are not constrained by image classiﬁcation precursors. As new sources of data become available, future architectures may be trained densely end-to-end, removing the need for pre-training on image classiﬁcation datasets. This may enable architectural simpliﬁcation and uniﬁcation. Speciﬁcally, end-to-end dense training may enable a fully dense architecture akin to the presented context net- work to operate at full resolution throughout, accepting the raw image as input and producing dense label assignments at full resolution as output. State-of-the-art systems for semantic segmentation leave signiﬁcant room for future advances. Fail- ure cases of our most accurate conﬁguration are shown in Figure 4. We will release our code and trained models to support progress in this area.  ACKNOWLEDGEMENTS  We thank Vibhav Vineet for proofreading, help with experiments, and related discussions. We are also grateful to Jonathan Long and the Caffe team for their feedback and for rapidly pulling our implementation into the Caffe library.  REFERENCES Badrinarayanan, Vijay, Handa, Ankur, and Cipolla, Roberto. SegNet: A deep convolutional encoder-decoder  architecture for robust semantic pixel-wise labelling. arXiv:1505.07293, 2015.  Brostow, Gabriel J., Fauqueur, Julien, and Cipolla, Roberto. Semantic object classes in video: A high-deﬁnition  ground truth database. Pattern Recognition Letters, 30(2), 2009.  Chen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Semantic  image segmentation with deep convolutional nets and fully connected CRFs. In ICLR, 2015a.  Image  Our result  Ground truth  Image  Our result  Ground truth  Figure 4: Failure cases from the VOC-2012 validation set. The most accurate architecture we trained (Context + CRF-RNN) performs poorly on these images.  8  SofaChairHorsePersonPersonCatDogHorsePublished as a conference paper at ICLR 2016  Chen, Liang-Chieh, Yang, Yi, Wang, Jiang, Xu, Wei, and Yuille, Alan L. Attention to scale: Scale-aware  semantic image segmentation. arXiv:1511.03339, 2015b.  Cordts, Marius, Omran, Mohamed, Ramos, Sebastian, Rehfeld, Timo, Enzweiler, Markus, Benenson, Rodrigo, Franke, Uwe, Roth, Stefan, and Schiele, Bernt. The Cityscapes dataset for semantic urban scene understand- ing. In CVPR, 2016.  Everingham, Mark, Gool, Luc J. Van, Williams, Christopher K. I., Winn, John M., and Zisserman, Andrew.  The Pascal visual object classes (VOC) challenge. IJCV, 88(2), 2010.  Farabet, Cl´ement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical features for  scene labeling. PAMI, 35(8), 2013.  Fischer, Philipp, Dosovitskiy, Alexey, Ilg, Eddy, H¨ausser, Philip, Hazrba, Caner, Golkov, Vladimir, van der Smagt, Patrick, Cremers, Daniel, and Brox, Thomas. Learning optical ﬂow with convolutional neural net- works. In ICCV, 2015.  Galleguillos, Carolina and Belongie, Serge J. Context based object categorization: A critical survey. Computer  Vision and Image Understanding, 114(6), 2010.  Geiger, Andreas, Lenz, Philip, Stiller, Christoph, and Urtasun, Raquel. Vision meets robotics: The KITTI  dataset. International Journal of Robotics Research, 32(11), 2013.  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural networks.  In AISTATS, 2010.  Hariharan, Bharath, Arbelaez, Pablo, Bourdev, Lubomir D., Maji, Subhransu, and Malik, Jitendra. Semantic  contours from inverse detectors. In ICCV, 2011.  He, Xuming, Zemel, Richard S., and Carreira-Perpi˜n´an, Miguel ´A. Multiscale conditional random ﬁelds for  image labeling. In CVPR, 2004.  Holschneider, M., Kronland-Martinet, R., Morlet, J., and Tchamitchian, Ph. A real-time algorithm for signal analysis with the help of the wavelet transform. In Wavelets: Time-Frequency Methods and Phase Space. Proceedings of the International Conference, 1987.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross B., Guadar- rama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature embedding. In Proc. ACM Multimedia, 2014.  Kohli, Pushmeet, Ladicky, Lubor, and Torr, Philip H. S. Robust higher order potentials for enforcing label  consistency. IJCV, 82(3), 2009.  Kr¨ahenb¨uhl, Philipp and Koltun, Vladlen. Efﬁcient inference in fully connected CRFs with Gaussian edge  potentials. In NIPS, 2011.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. ImageNet classiﬁcation with deep convolutional  neural networks. In NIPS, 2012.  Kundu, Abhijit, Vineet, Vibhav, and Koltun, Vladlen. Feature space optimization for semantic video segmen-  tation. In CVPR, 2016.  Ladicky, Lubor, Russell, Christopher, Kohli, Pushmeet, and Torr, Philip H. S. Associative hierarchical CRFs  for object class image segmentation. In ICCV, 2009.  Le, Quoc V., Jaitly, Navdeep, and Hinton, Geoffrey E. A simple way to initialize recurrent networks of rectiﬁed  linear units. arXiv:1504.00941, 2015.  LeCun, Yann, Boser, Bernhard, Denker, John S., Henderson, Donnie, Howard, Richard E., Hubbard, Wayne, and Jackel, Lawrence D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4), 1989.  Lin, Guosheng, Shen, Chunhua, Reid, Ian, and van dan Hengel, Anton. Efﬁcient piecewise training of deep  structured models for semantic segmentation. arXiv:1504.01013, 2015.  Lin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Doll´ar, Piotr,  and Zitnick, C. Lawrence. Microsoft COCO: Common objects in context. In ECCV, 2014.  Liu, Buyu and He, Xuming. Multiclass semantic video segmentation with object-level active inference. In  CVPR, 2015.  Long, Jonathan, Shelhamer, Evan, and Darrell, Trevor. Fully convolutional networks for semantic segmenta-  tion. In CVPR, 2015.  9  Published as a conference paper at ICLR 2016  Noh, Hyeonwoo, Hong, Seunghoon, and Han, Bohyung. Learning deconvolution network for semantic seg-  mentation. In ICCV, 2015.  Ros, Germ´an, Ramos, Sebastian, Granados, Manuel, Bakhtiary, Amir, V´azquez, David, and L´opez, Anto-  nio Manuel. Vision-based ofﬂine-online perception paradigm for autonomous driving. In WACV, 2015.  Rumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. Learning representations by back-  propagating errors. Nature, 323, 1986.  Shensa, Mark J. The discrete wavelet transform: wedding the `a trous and Mallat algorithms. IEEE Transactions  on Signal Processing, 40(10), 1992.  Shotton, Jamie, Winn, John M., Rother, Carsten, and Criminisi, Antonio. TextonBoost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV, 81 (1), 2009.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition.  In ICLR, 2015.  Sturgess, Paul, Alahari, Karteek, Ladicky, Lubor, and Torr, Philip H. S. Combining appearance and structure  from motion features for road scene understanding. In BMVC, 2009.  Tighe, Joseph and Lazebnik, Svetlana. Superparsing – scalable nonparametric image parsing with superpixels.  IJCV, 101(2), 2013.  Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du, Dalong,  Huang, Chang, and Torr, Philip. Conditional random ﬁelds as recurrent neural networks. In ICCV, 2015.  10  Published as a conference paper at ICLR 2016  APPENDIX A URBAN SCENE UNDERSTANDING  In this appendix, we report experiments on three datasets for urban scene understanding: the CamVid dataset (Brostow et al., 2009), the KITTI dataset (Geiger et al., 2013), and the new Cityscapes dataset (Cordts et al., 2016). As the accuracy measure we use the mean IoU (Everingham et al., 2010). We only train our model on the training set, even when a validation set is available. The results reported in this section do not use conditional random ﬁelds or other forms of structured prediction. They were obtained with convolutional networks that combine a front-end module and a context module, akin to the “Front + Basic” network evaluated in Table 3. The trained models can be found at https://github.com/fyu/dilation. We now summarize the training procedure used for training the front-end module. This procedure applies to all datasets. Training is performed with stochastic gradient descent. Each mini-batch contains 8 crops from randomly sampled images. Each crop is of size 628×628 and is randomly sampled from a padded image. Images are padded using reﬂection padding. No padding is used in the intermediate layers. The learning rate is 10−4 and momentum is set to 0.99. The number of iterations depends on the number of images in the dataset and is reported for each dataset below. The context modules used for these datasets are all derived from the “Basic” network, using the terminology of Table 1. The number of channels in each layer is the number of predicted classes C. (For example, C = 19 for the Cityscapes dataset.) Each layer in the context module is padded such that the input and response maps have the same size. The number of layers in the context module depends on the resolution of the images in the dataset. Joint training of the complete model, composed of the front-end and the context module, is summarized below for each dataset.  A.1 CAMVID  We use the split of Sturgess et al. (2009), which partitions the dataset into 367 training images, 100 validation images, and 233 test images. 11 semantic classes are used. The images are downsampled to 640×480. The context module has 8 layers, akin to the model used for the Pascal VOC dataset in the main body of the paper. The overall training procedure is as follows. First, the front-end module is trained for 20K iterations. Then the complete model (front-end + context) is jointly trained by sampling crops of size 852× 852 with batch size 1. The learning rate for joint training is set to 10−5 and the momentum is set to 0.9. Results on the CamVid test set are reported in Table 5. We refer to our complete convolutional network (front-end + context) as Dilation8, since the context module has 8 layers. Our model out- performs the prior work. This model was used as the unary classiﬁer in the recent work of Kundu et al. (2016).  g n i d l i u B 73.4 70.4 66.8 68.7 81.5 82.6  e e r T  70.2 54.8 66.6 52.0 74.6 76.2  y k S  r a C  91.1 83.5 90.1 87.0 89.0 89.9  64.2 43.3 62.9 58.5 82.2 84.0  n g i S  24.4 25.4 21.4 13.4 42.3 46.9  d a o R  91.1 83.4 85.8 86.2 92.2 92.2  n a i r t s e d e P  29.1 11.6 28.0 25.3 48.4 56.3  e c n e F  31.0 18.3 17.8 17.9 27.2 35.8  e l o P  13.6 5.2 8.3 16.0 14.3 23.4  k l a w e d i S  72.4 57.4 63.5 60.5 75.4 75.3  t s i l c y c i B 28.6 8.9 8.5 24.8 50.1 55.5  U o I  n a e m 53.6 42.0 47.2 46.4 61.6 65.3  ALE SuperParsing Liu and He SegNet DeepLab-LFOV Dilation8  Table 5: Semantic segmentation results on the CamVid dataset. Our model (Dilation8) is com- pared to ALE (Ladicky et al., 2009), SuperParsing (Tighe & Lazebnik, 2013), Liu and He (Liu & He, 2015), SegNet (Badrinarayanan et al., 2015), and the DeepLab-LargeFOV model (Chen et al., 2015a). Our model outperforms the prior work.  11  Published as a conference paper at ICLR 2016  A.2 KITTI  We use the training and validation split of Ros et al. (2015): 100 training images and 46 test images. The images were all collected from the KITTI visual odometry/SLAM dataset. The image resolution is 1226×370. Since the vertical resolution is small compared to the other datasets, we remove Layer 6 in Table 1. The resulting context module has 7 layers. The complete network (front-end + context) is referred to as Dilation7. The front-end is trained for 10K iterations. Next, the front-end and the context module are trained jointly. For joint training, the crop size is 900×900 and momentum is set to 0.99, while the other parameters are the same as the ones used for the CamVid dataset. Joint training is performed for 20K iterations. The results are shown in Table 6. As the table demonstrates, our model outperforms the prior work.  g n i d l i u B Ros et al. 71.8 DeepLab-LFOV 82.8 84.6 Dilation7  e e r T  69.5 78.6 81.1  y k S  r a C  84.4 82.4 83  51.2 78.0 81.4  n g i S  4.2 28.8 41.8  d a o R  72.4 91.3 92.9  n a i r t s e d e P  1.7 0.0 4.6  e c n e F  32.4 39.4 47.1  e l o P  2.6 29.9 35.2  k l a w e d i S  45.3 72.4 73.1  t s i l c y c i B 3.2 12.9 26.4  U o I n a e m 39.9 54.2 59.2  Table 6: Semantic segmentation results on the KITTI dataset. We compare our results to Ros et al. (2015) and to the DeepLab-LargeFOV model (Chen et al., 2015a). Our network (Dilation7) yields higher accuracy than the prior work.  A.3 CITYSCAPES  The Cityscapes dataset contains 2975 training images, 500 validation images, and 1525 test im- ages (Cordts et al., 2016). Due to the high image resolution (2048×1024), we add two layers to the context network after Layer 6 in Table 1. These two layers have dilation 32 and 64, respectively. The total number of layers in the context module is 10 and we refer to the complete model (front-end + context) as Dilation10. The Dilation10 network was trained in three stages. First, the front-end prediction module was trained for 40K iterations. Second, the context module was trained for 24K iterations on whole (uncropped) images, with learning rate 10−4, momentum 0.99, and batch size 100. Third, the complete model (front-end + context) was jointly trained for 60K iterations on halves of images (input size 1396× 1396, including padding), with learning rate 10−5, momentum 0.99, and batch size 1. Figure 5 visualizes the effect of the training stages on the performance of the model. Quantitative results are given in Tables 7 and 8. The performance of Dilation10 was compared to prior work on the Cityscapes dataset by Cordts et al. (2016). In their evaluation, Dilation10 outperformed all prior models (Cordts et al., 2016). Dilation10 was also used as the unary classiﬁer in the recent work of Kundu et al. (2016), which used structured prediction to increase accuracy further.  12  Published as a conference paper at ICLR 2016  Figure 5: Results produced by the Dilation10 model after different training stages. (a) Input image. (b) Ground truth segmentation. (c) Segmentation produced by the model after the ﬁrst stage of training (front-end only). (d) Segmentation produced after the second stage, which trains the context module. (e) Segmentation produced after the third stage, in which both modules are trained jointly.  d a o R  k l a w e d i  S  g n i d l i u B  l l a  W  e c n e F  e l o P  t h g i L  n g i  S  n o i t a t e g e V  n i a r r e T  y k S  n o s r e P  r e d i R  r a C  k c u r T  s u B  n i a r T  e l c y c r o t o M  e l c y c i B  U o I n a e m  97.2 79.5 90.4 44.9 52.4 55.1 56.7 69  91 58.7 92.6 75.7 50 92.2 56.2 72.6 54.3 46.2 70.1 68.7  Validation set  Test set  97.6 79.2 89.9 37.3 47.6 53.2 58.6 65.2 91.8 69.4 93.7 78.9 55 93.3 45.5 53.4 47.7 52.2 66  67.1  Table 7: Per-class and mean class-level IoU achieved by our model (Dilation10) on the Cityscapes dataset.  Flat  98.2  98.3  Nature  Object  Sky  Construction  Human  Vehicle  mean IoU  91.4  91.4  62.3  60.5  Validation set  92.6  93.7  Test set  90.7  90.2  77.6  79.8  91  91.8  86.3  86.5  Table 8: Per-category and mean category-level IoU on the Cityscapes dataset.  13  (a) Image(b) Ground truth(c) Front end(d) +Context(e) +Joint(f) Ground truth",
1511.03677,2016,Learning to Diagnose with LSTM Recurrent Neural Networks,"['Learning to Diagnose with LSTM Recurrent Neural Networks\nZachary Lipton', 'David Kale', 'Charles Elkan', 'Randall Wetzel']",https://arxiv.org/pdf/1511.03677,"7 1 0 2    r a     M 1 2      ]  G L . s c [      7 v 7 7 6 3 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  LEARNING TO DIAGNOSE WITH LSTM RECURRENT NEURAL NETWORKS  Zachary C. Lipton ∗† Department of Computer Science and Engineering University of California, San Diego La Jolla, CA 92093, USA zlipton@cs.ucsd.edu  David C. Kale ∗‡ Department of Computer Science PUSH OVER TO LEFT University of Southern California Los Angeles, CA 90089 dkale@usc.edu  Charles Elkan Department of Computer Science and Engineering University of California, San Diego La Jolla, CA 92093, USA elkan@cs.ucsd.edu  Randall Wetzel Laura P. and Leland K. Whittier Virtual PICU Children’s Hospital Los Angeles Los Angeles, CA 90027 rwetzel@chla.usc.edu  ABSTRACT  Clinical medical data, especially in the intensive care unit (ICU), consist of multi- variate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient’s Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difﬁcult to mine effectively, owing to varying length, irregular sampling and missing data. Recur- rent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the ﬁrst study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical mea- surements. Speciﬁcally, we consider multilabel classiﬁcation of diagnoses, train- ing a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features.  1  INTRODUCTION  Time series data comprised of clinical measurements, as recorded by caregivers in the pediatric in- tensive care unit (PICU), constitute an abundant and largely untapped source of medical insights. Potential uses of such data include classifying diagnoses accurately, predicting length of stay, pre- dicting future illness, and predicting mortality. However, besides the difﬁculty of acquiring data, several obstacles stymie machine learning research with clinical time series. Episodes vary in length, with stays ranging from just a few hours to multiple months. Observations, which include sensor data, vital signs, lab test results, and subjective assessments, are sampled irregularly and plagued by missing values (Marlin et al., 2012). Additionally, long-term time dependencies complicate learning with many algorithms. Lab results that, taken together, might imply a particular diagnosis may be separated by days or weeks. Long delays often separate onset of disease from the appearance of symptoms. For example, symptoms of acute respiratory distress syndrome may not appear until 24-48 hours after lung injury (Mason et al., 2010), while symptoms of an asthma attack may present shortly after admission but change or disappear following treatment.  ∗Equal contributions †Author website: http://zacklipton.com ‡Author website: http://www-scf.usc.edu/˜dkale/  1  Published as a conference paper at ICLR 2016  Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recogni- tion, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk´y, 2001; Xu et al., 2007). LSTMs can capture long range dependencies and nonlinear dynamics. Some sequence models, such as Markov models, conditional random ﬁelds, and Kalman ﬁlters, deal with sequential data but are ill-equipped to learn long-range dependencies. Other models require domain knowledge or feature engineering, offering less chance for serendipitous discovery. In contrast, neural networks learn representations and can discover unforeseen structure. This paper presents the ﬁrst empirical study using LSTMs to classify diagnoses given multivari- ate PICU time series. Speciﬁcally, we formulate the problem as multilabel classiﬁcation, since diagnoses are not mutually exclusive. Our examples are clinical episodes, each consisting of 13 frequently but irregularly sampled time series of clinical measurements, including body tempera- ture, heart rate, diastolic and systolic blood pressure, and blood glucose, among others. Associated with each patient are a subset of 429 diagnosis codes. As some are rare, we focus on the 128 most common codes, classifying each episode with one or more diagnoses. Because LSTMs have never been used in this setting, we ﬁrst verify their utility and compare their performance to a set of strong baselines, including both a linear classiﬁer and a MultiLayer Per- ceptron (MLP). We train the baselines on both a ﬁxed window and hand-engineered features. We then test a straightforward target replication strategy for recurrent neural networks, inspired by the deep supervision technique of Lee et al. (2015) for training convolutional neural networks. We com- pose our optimization objective as a convex combination of the loss at the ﬁnal sequence step and the mean of the losses over all sequence steps. Additionally, we evaluate the efﬁcacy of using additional information in the patient’s chart as auxiliary outputs, a technique previously used with feedforward nets (Caruana et al., 1996), showing that it reduces overﬁtting. Finally, we apply dropout to non- recurrent connections, which improves the performance further. LSTMs with target replication and dropout surpass the performance of the best baseline, namely an MLP trained on hand-engineered features, even though the LSTM has access only to raw time series.  2 RELATED WORK  Our research sits at the intersection of LSTMs, medical informatics, and multilabel classiﬁcation, three mature ﬁelds, each with a long history and rich body of research. While we cannot do justice to all three, we highlight the most relevant works below.  2.1 LSTM RNNS  LSTMs were originally introduced in Hochreiter & Schmidhuber (1997), following a long line of research into RNNs for sequence learning. Notable earlier work includes Rumelhart et al. (1985), which introduced backpropagation through time, and Elman (1990), which successfully trained RNNs to perform supervised machine learning tasks with sequential inputs and outputs. The de- sign of modern LSTM memory cells has remained close to the original, with the commonly used addition of forget gates (Gers et al., 2000) (which we use), and peep-hole connections (Gers & Schmidhuber, 2000) (which we do not use). The connectivity pattern among multiple LSTM layers in our models follows the architecture described by Graves (2013). Pascanu et al. (2014) explores other mechanisms by which an RNN could be made deep. Surveys of the literature include Graves (2012), a thorough dissertation on sequence labeling with RNNs, De Mulder et al. (2015), which surveys natural language applications, and Lipton et al. (2015), which provides a broad overview of RNNs for sequence learning, focusing on modern applications.  2.2 NEURAL NETWORKS FOR MEDICAL DATA  Neural networks have been applied to medical problems and data for at least 20 years (Caruana et al., 1996; Baxt, 1995), although we know of no work on applying LSTMs to multivariate clinical time series of the type we analyze here. Several papers have applied RNNs to physiologic signals, including electrocardiograms (Silipo & Marchesi, 1998; Amari & Cichocki, 1998; ¨Ubeyli, 2009) and  2  Published as a conference paper at ICLR 2016  glucose measurements (Tresp & Briegel, 1998). RNNs have also been used for prediction problems in genomics (Pollastri et al., 2002; Xu et al., 2007; Vohradsk´y, 2001). Multiple recent papers apply modern deep learning techniques (but not RNNs) to modeling psychological conditions (Dabek & Caban, 2015), head injuries (Rughani et al., 2010), and Parkinson’s disease (Hammerla et al., 2015). Recently, feedforward networks have been applied to medical time series in sliding window fashion to classify cases of gout, leukemia (Lasko et al., 2013), and critical illness (Che et al., 2015).  2.3 NEURAL NETWORKS FOR MULTILABEL CLASSIFICATION  Only a few published papers apply LSTMs to multilabel classiﬁcation tasks, all of which, to our knowledge, are outside of the medical context. Liu et al. (2014) formulates music composition as a multilabel classiﬁcation task, using sigmoidal output units. Most recently, Yeung et al. (2015) uses LSTM networks with multilabel outputs to recognize actions in videos. While we could not locate any published papers using LSTMs for multilabel classiﬁcation in the medical domain, several papers use feedforward nets for this task. One of the earliest papers to investigate multi-task neural networks modeled risk in pneumonia patients (Caruana et al., 1996). More recently, Che et al. (2015) formulated diagnosis as multilabel classiﬁcation using a sliding window multilayer perceptron.  2.4 MACHINE LEARNING FOR CLINICAL TIME SERIES  Neural network methodology aside, a growing body of research applies machine learning to tem- poral clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al., 2014a; Henry et al., 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015). Many recent papers use models with latent factors to cap- ture nonlinear dynamics in clinical time series and to discover meaningful representations of health and illness. Gaussian processes are popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015). Saria et al. (2010) combined a hierarchical dirichlet process with autoregressive models to infer latent disease “topics” in the heart rate signals of pre- mature babies. Quinn et al. (2009) used linear dynamical systems with latent switching variables to model physiologic events like bradycardias. Seeking deeper models, Stanculescu et al. (2014b) proposed a second “layer” of latent factors to capture correlations between latent states.  2.5 TARGET REPLICATION  In this work, we make the task of classifying entire sequences easier by replicating targets at every time step, inspired by Lee et al. (2015), who place an optimization objective after each layer in convolutional neural network. While they have a separate set of weights to learn each intermediate objective, our model is simpler owing to the weight tying in recurrent nets, having only one set of output weights. Additionally, unlike Lee et al. (2015), we place targets at each time step, but not following each layer between input and output in the LSTM. After ﬁnishing this manuscript, we learned that target replication strategies similar to ours have also been developed by Ng et al. (2015) and Dai & Le (2015) for the tasks of video classiﬁcation and character-level document classiﬁcation respectively. Ng et al. (2015) linearly scale the importance of each intermediate target, emphasiz- ing performance at later sequence steps over those in the beginning of the clip. Dai & Le (2015) also use a target replication strategy with linearly increasing weight for character-level document classiﬁcation, showing signiﬁcant improvements in accuracy. They call this technique linear gain.  2.6 REGULARIZING RECURRENT NEURAL NETWORKS  Given the complexity of our models and modest scale of our data, regularization, including judicious use of dropout, is crucial to our performance. Several prior works use dropout to regularize RNNs. Pham et al. (2014), Zaremba et al. (2014), and Dai & Le (2015) all describe an application of dropout to only the non-recurrent weights of a network. The former two papers establish the method and apply it to tasks with sequential outputs, including handwriting recognition, image captioning, and machine translation. The setting studied by Dai & Le (2015) most closely resembles ours as the authors apply it to the task of applying static labels to varying length sequences.  3  Published as a conference paper at ICLR 2016  2.7 KEY DIFFERENCES  Our experiments show that LSTMs can accurately classify multivariate time series of clinical mea- surements, a topic not addressed in any prior work. Additionally, while some papers use LSTMs for multilabel classiﬁcation, ours is the ﬁrst to address this problem in the medical context. Moreover, for multilabel classiﬁcation of sequential clinical data with ﬁxed length output vectors, this paper is the ﬁrst, to our knowledge, to demonstrate the efﬁcacy of a target replication strategy, achieving both faster training and better generalization.  3 DATA DESCRIPTION  Our experiments use a collection of anonymized clinical time series extracted from the EHR system at Children’s Hospital LA (Marlin et al., 2012; Che et al., 2015) as part of an IRB-approved study. The data consists of 10, 401 PICU episodes, each a multivariate time series of 13 variables: diastolic and systolic blood pressure, peripheral capillary reﬁll rate, end-tidal CO2, fraction of inspired O2, Glascow coma scale, blood glucose, heart rate, pH, respiratory rate, blood oxygen saturation, body temperature, and urine output. Episodes vary in length from 12 hours to several months. Each example consists of irregularly sampled multivariate time series with both missing values and, occasionally, missing variables. We resample all time series to an hourly rate, taking the mean mea- surement within each one hour window. We use forward- and back-ﬁlling to ﬁll gaps created by the window-based resampling. When a single variable’s time series is missing entirely, we impute a clinically normal value as deﬁned by domain experts. These procedures make reasonable as- sumptions about clinical practice: many variables are recorded at rates proportional to how quickly they change, and when a variable is absent, it is often because clinicians believed it to be normal and chose not to measure it. Nonetheless, these procedures are not appropriate in all settings. Back- ﬁlling, for example, passes information from the future backwards. This is acceptable for classifying entire episodes (as we do) but not for forecasting. Finally, we rescale all variables to [0, 1], using ranges deﬁned by clinical experts. In addition, we use published tables of normal values from large population studies to correct for differences in heart rate, respiratory rate, (Fleming et al., 2011) and blood pressure (NHBPEP Working Group 2004) due to age and gender. Each episode is associated with zero or more diagnostic codes from an in-house taxonomy used for research and billing, similar to the Ninth Revision of the International Classiﬁcation of Diseases (ICD-9) codes (World Health Organization, 2004). The dataset contains 429 distinct labels indicat- ing a variety of conditions, such as acute respiratory distress, congestive heart failure, seizures, renal failure, and sepsis. Because many of the diagnoses are rare, we focus on the most common 128, each of which occurs more than 50 times in the data. These diagnostic codes are recorded by attending physicians during or shortly after each patient episode and subject to limited review afterwards. Because the diagnostic codes were assigned by clinicians, our experiments represent a comparison of an LSTM-based diagnostic system to human experts. We note that an attending physician has access to much more data about each patient than our LSTM does, including additional tests, medications, and treatments. Additionally, the physician can access a full medical history including free-text notes, can make visual and physical inspections of the patient, and can ask questions. A more fair comparison might require asking additional clinical experts to assign diagnoses given access only to the 13 time series available to our models. However, this would be prohibitively expensive, even for just the 1000 examples, and difﬁcult to justify to our medical collaborators, as this annotation would provide no immediate beneﬁt to patients. Such a study will prove more feasible in the future when this line of research has matured.  4 METHODS  In this work, we are interested in recognizing diagnoses and, more broadly, the observable physi- ologic characteristics of patients, a task generally termed phenotyping (Oellrich et al., 2015). We cast the problem of phenotyping clinical time series as multilabel classiﬁcation. Given a series of observations x(1), ..., x(T ), we learn a classiﬁer to generate hypotheses ˆy of the true labels y. Here, t indexes sequence steps, and for any example, T stands for the length of the sequence. Our pro- posed LSTM RNN uses memory cells with forget gates (Gers et al., 2000) but without peephole  4  Published as a conference paper at ICLR 2016  connections (Gers et al., 2003). As output, we use a fully connected layer atop the highest LSTM layer followed by an element-wise sigmoid activation function, because our problem is multilabel. We use log loss as the loss function at each output.  The following equations give the update for a layer of memory cells h(t) l−1 stands for the previous layer at the same sequence step (a previous LSTM layer or the input x(t)) and h(t−1) stands for the same layer at the previous sequence step:  l where h(t)  l  l  l  gx l h(t)  l h(t−1) gh l h(t−1) l h(t−1) l h(t−1)  g(t) l = φ(W l = σ(W ix l h(t) i(t) l = σ(W fx l h(t) f (t) l = σ(W ox o(t) l h(t) l (cid:12) i(i) l = g(t) s(t) l = φ(s(t) h(t)  l−1 + W l−1 + W ih l−1 + W fh l−1 + W oh l + s(t−1) l ) (cid:12) o(t)  l  l  l  l  (cid:12) f (t) .  l  g l )  + b + bi l) + bf l ) + bo l )  In these equations, σ stands for an element-wise application of the sigmoid (logistic) function, φ stands for an element-wise application of the tanh function, and (cid:12) is the Hadamard (element-wise) product. The input, output, and forget gates are denoted by i, o, and f respectively, while g is the input node and has a tanh activation.  4.1 LSTM ARCHITECTURES FOR MULTILABEL CLASSIFICATION  We explore several recurrent neural network architectures for multilabel classiﬁcation of time series. The ﬁrst and simplest (Figure 1) passes over all inputs in chronological order, generating outputs only at the ﬁnal sequence step. In this approach, we only have output ˆy at the ﬁnal sequence step, at which our loss function is the average of the losses at each output node. Thus the loss calculated at a single sequence step is the average of log loss calculated separately on each label.  loss(ˆy, y) =  1 |L|  −(yl · log(ˆyl) + (1 − yl) · log(1 − ˆyl)).  l=|L|(cid:88)  l=1  Figure 1: A simple RNN model for multilabel classiﬁcation. Green rectangles represent inputs. The recurrent hidden layers separating input and output are represented with a single blue rectangle. The red rectangle represents targets.  4.2 SEQUENTIAL TARGET REPLICATION  One problem with the simple approach is that the network must learn to pass information across many sequence steps in order to affect the output. We attack this problem by replicating our static targets at each sequence step (Figure 2), providing a local error signal at each step. This approach is inspired by the deep supervision technique that Lee et al. (2015) apply to convolutional nets. This technique is especially sensible in our case because we expect the model to predict accurately even if the sequence were truncated by a small amount. The approach differs from Lee et al. (2015) because  5  Published as a conference paper at ICLR 2016  we use the same output weights to calculate ˆy(t) for all t. Further, we use this target replication to generate output at each sequence step, but not at each hidden layer.  For the model with target replication, we generate an output ˆy(t) at every sequence step. Our loss is then a convex combination of the ﬁnal loss and the average of the losses over all steps:  T(cid:88)  t=1  α · 1 T  loss(ˆy(t), y(t)) + (1 − α) · loss(ˆy(T ), y(T ))  where T is the total number of sequence steps and α ∈ [0, 1] is a hyper-parameter which determines the relative importance of hitting these intermediary targets. At prediction time, we take only the output at the ﬁnal step. In our experiments, networks using target replication outperform those with a loss applied only at the ﬁnal sequence step.  Figure 2: An RNN classiﬁcation model with target replication. The primary target (depicted in red) at the ﬁnal step is used at prediction time, but during training, the model back-propagates errors from the intermediate targets (purple) at every sequence step.  4.3 AUXILIARY OUTPUT TRAINING  Recall that our initial data contained 429 diagnostic labels but that our task is to predict only 128. Given the well-documented successes of multitask learning with shared representations and feed- forward networks, we wish to train a stronger model by using the remaining 301 labels or other information in the patient’s chart, such as diagnostic categories, as auxiliary targets (Caruana et al., 1996). These additional targets serve reduce overﬁtting as the model aims to minimize the loss on the labels of interest while also minimizing loss on the auxiliary targets (Figure 3).  Figure 3: Our dataset contains many labels. For our task, a subset of 128 are of interest (depicted in red). Our Auiliary Output neural network makes use of extra labels as additional training targets (depicted in purple). At inference time we generate predictions for only the labels of interest.  6  Published as a conference paper at ICLR 2016  4.4 REGULARIZATION  Because we have only 10, 401 examples, overﬁtting is a considerable obstacle. Our experiments show that both target replication and auxiliary outputs improve performance and reduce overﬁtting. In addition to these less common techniques we deploy (cid:96)2 2 weight decay and dropout. Following the example of Zaremba et al. (2014) and Pham et al. (2014), we apply dropout to the non-recurrent connections only. We ﬁrst compute each hidden layer’s sequence of activations in the left-to-right direction and then apply dropout before computing the next layer’s activations. In our experiments, we ﬁnd that dropout decreases overﬁtting, enabling us to double the size of each hidden layer.  5 EXPERIMENTS  All models are trained on 80% of the data and tested on 10%. The remaining 10% is used as a validation set. We train each LSTM for 100 epochs using stochastic gradient descent (SGD) with momentum. To combat exploding gradients, we scale the norm of the gradient and use (cid:96)2 2 weight decay of 10−6, both hyperparameters chosen using validation data. Our ﬁnal networks use 2 hidden layers and either 64 memory cells per layer with no dropout or 128 cells per layer with dropout of 0.5. These architectures are also chosen based on validation performance. Throughout training, we save the model and compute three performance metrics (micro AUC, micro F1, and precision at 10) on the validation set for each epoch. We then test the model that scores best on at least two of the three validation metrics. To break ties, we choose the earlier epoch. We evaluate a number of baselines as well as LSTMs with various combinations of target replication (TR), dropout (DO), and auxiliary outputs (AO), using either the additional 301 diagnostic labels or 12 diagnostic categories. To explore the regularization effects of each strategy, we record and plot both training and validation performance after each epoch. Additionally, we report performance of a target replication model (Linear Gain) that scales the weight of each intermediate target linearly as opposed our proposed approach. Finally, to show that our LSTM learns a model complementary to the baselines, we evaluate an ensemble of the best LSTM with the best baseline.  5.1 MULTILABEL EVALUATION METHODOLOGY  We report micro- and macro-averaged versions of Area Under the ROC Curve (AUC). By micro AUC, we mean a single AUC computed on ﬂattened ˆY and Y matrices, whereas we calculate macro AUC by averaging each per-label AUC. The blind classiﬁer achieves 0.5 macro AUC but can exceed 0.5 on micro AUC by predicting labels in descending order by base rate. Additionally, we report micro- and macro-averaged F1 score, computed in similar fashion to the respective micro and macro AUCs. F1 metrics require a thresholding strategy, and here we select thresholds based upon valida- tion set performance. We refer to Lipton et al. (2014) for an analysis of the strengths and weaknesses of each type of multilabel F-score and a characterization of optimal thresholds. Finally, we report precision at 10, which captures the fraction of true diagnoses among the model’s top 10 predictions, with a best possible score of 0.2281 on the test split of this data set because there are on average 2.281 diagnoses per patient. While F1 and AUC are both useful for determining the relative quality of a classiﬁer’s predictions, neither is tailored to a real-world application. Thus, we consider a medically plausible use case to motivate this more interpretable metric: generating a short list of the 10 most probable diagnoses. If we could create a high recall, moderate precision list of 10 likely diagnoses, it could be a valuable hint-generation tool for differential diagnosis. Testing for only the 10 most probable conditions is much more realistic than testing for all conditions.  5.2 BASELINE CLASSIFIERS  We provide results for a base rate model that predicts diagnoses in descending order by incidence to provide a minimum performance baseline for micro-averaged metrics. We also report the perfor- mance of logistic regression, which is widely used in clinical research. We train a separate classiﬁer for each diagnosis but choose an overall (cid:96)2 2 penalty for all individual classiﬁers based on validation performance. For a much stronger baseline, we train a multilabel MLP with 3 hidden layers of 300 hidden units each, rectiﬁed linear activations, and dropout of 0.5. All MLPs were trained for 1000 epochs, with hyperparameters chosen based on validation set performance. Each baseline is tested  7  Published as a conference paper at ICLR 2016  with two sets of inputs: raw time series and hand-engineered features. For raw time series, we use the ﬁrst and last six hours. This provides classiﬁers with temporal information about changes in patient state from admission to discharge within a ﬁxed-size input, as required by all baselines. We ﬁnd this works better than providing the ﬁrst or last 12 hours alone. Our hand-engineered features are inspired by those used in state-of-the-art severity of illness scores (Pollack et al., 1996): for each variable, we compute the ﬁrst and last measurements and their dif- ference scaled by episode length, mean and standard deviation, median and quartiles, minimum and maximum, and slope of a line ﬁt with least squares. These 143 features capture many of the indi- cators that clinicians look for in critical illness, including admission and discharge state, extremes, central tendencies, variability, and trends. They previously have been shown to be effective for these data (Marlin et al., 2012; Che et al., 2015). Our strongest baseline is an MLP using these features.  5.3 RESULTS  Our best performing LSTM (LSTM-DO-TR) used two layers of 128 memory cells, dropout of prob- ability 0.5 between layers, and target replication, and outperformed the MLP with hand-engineered features. Moreover simple ensembles of the best LSTM and MLP outperformed both on all metrics. Table 1 shows summary results for all models. Table 2 shows the LSTM’s predictive performance for six diagnoses with the highest F1 scores. Full per-diagnosis results can be found in Appendix C. Target replication improves performance on all metrics, accelerating learning and reducing overﬁt- ting (Figure 4). We also ﬁnd that the LSTM with target replication learns to output correct diagnoses earlier in the time series, a virtue that we explore qualitatively in Appendix A. As a comparison, we trained a LSTM-DO-TR variant using the linear gain strategy of Ng et al. (2015); Dai & Le (2015). In general, this model did not perform as well as our simpler target replication strategy, but it did achieve the highest macro F1 score among the LSTM models.  Classiﬁcation performance for 128 ICU phenotypes  Model Base Rate Log. Reg., First 6 + Last 6 Log. Reg., Expert features MLP, First 6 + Last 6 MLP, Expert features  LSTM LSTM, AuxOut (Diagnoses) LSTM-AO (Categories) LSTM-TR LSTM-TR-AO (Diagnoses) LSTM-TR-AO (Categories)  0.0343 0.1081 0.1373 0.1286 0.1475  0.5 0.7404 0.7644 0.7770 0.8030  Micro AUC Macro AUC Micro F1 Macro F1 0.7128 0.8122 0.8285 0.8375 0.8551  0.1346 0.2324 0.2502 0.2698 0.2930 LSTM Models with two 64-cell hidden layers 0.2450 0.2627 0.2651 0.2702 0.2599 0.2774  0.1170 0.1309 0.1351 0.1348 0.1317 0.1330  0.8241 0.8351 0.8382 0.8429 0.8391 0.8439  0.7573 0.7746 0.7748 0.7870 0.7866 0.7860  LSTM Models with Dropout (probability 0.5) and two 128-cell hidden layers  LSTM-DO LSTM-DO-AO (Diagnoses) LSTM-DO-AO (Categories) LSTM-DO-TR LSTM-DO-TR-AO (Diagnoses) LSTM-DO-TR-AO (Categories) LSTM-DO-TR (Linear Gain)  Mean of LSTM-DO-TR & MLP Max of LSTM-DO-TR & MLP  0.7741 0.7785 0.7783 0.8075 0.7929 0.8015 0.7986  0.8377 0.8365 0.8399 0.8560 0.8470 0.8543 0.8480  0.2748 0.2581 0.2804 0.2938 0.2735 0.2887 0.2896 Ensembles of Best MLP and Best LSTM 0.2981 0.3035  0.8611 0.8643  0.8143 0.8194  0.1371 0.1366 0.1361 0.1485 0.1488 0.1446 0.1530  0.1553 0.1571  Prec. at 10 0.0788 0.1016 0.1087 0.1096 0.1170  0.1047 0.1110 0.1099 0.1115 0.1085 0.1138  0.1110 0.1104 0.1123 0.1172 0.1149 0.1161 0.1160  0.1201 0.1218  Table 1: Results on performance metrics calculated across all labels. DO, TR, and AO indicate dropout, target replication, and auxiliary outputs, respectively. AO (Diagnoses) uses the extra diag- nosis codes and AO (Categories) uses diagnostic categories as additional targets during training.  8  Published as a conference paper at ICLR 2016  Auxiliary outputs improved performance for most metrics and reduced overﬁtting. While the perfor- mance improvement is not as dramatic as that conferred by target replication, the regularizing effect is greater. These gains came at the cost of slower training: the auxiliary output models required more epochs (Figure 4 and Appendix B), especially when using the 301 remaining diagnoses. This may be due in part to severe class imbalance in the extra labels. For many of these labels it may take an entire epoch just to learn that they are occasionally nonzero.  Label Diabetes mellitus with ketoacidosis Scoliosis, idiopathic Asthma, unspeciﬁed with status asthmaticus Neoplasm, brain, unspeciﬁed Delayed milestones Acute Respiratory Distress Syndrome (ARDS)  Top 6 diagnoses measured by F1 score AUC 0.9966 0.8543 0.9232 0.8522 0.8178 0.9595  F1 0.8571 0.6809 0.5641 0.5430 0.4751 0.4688  Precision Recall 0.7500 1.0000 0.6667 0.6957 0.4400 0.7857 0.4317 0.7315 0.5733 0.4057 0.3409 0.7500  Table 2: LSTM-DO-TR performance on the 6 diagnoses with highest F1 scores.  The LSTMs appear to learn models complementary to the MLP trained on hand-engineered fea- tures. Supporting this claim, simple ensembles of the LSTM-DO-TR and MLP (taking the mean or maximum of their predictions) outperform the constituent models signiﬁcantly on all metrics (Ta- ble 1). Further, there are many diseases for which one model substantially outperforms the other, e.g., intracranial hypertension for the LSTM, septic shock for the MLP (Appendix C).  6 DISCUSSION  Our results indicate that LSTM RNNs, especially with target replication, can successfully classify diagnoses of critical care patients given clinical time series data. The best LSTM beat a strong MLP baseline using hand-engineered features as input, and an ensemble combining the MLP and LSTM improves upon both. The success of target replication accords with results by both Ng et al. (2015) and Dai & Le (2015), who observed similar beneﬁts on their respective tasks. However, while they saw improvement using a linearly increasing weight on each target from start to end, this strategy performed worse in our diagnostic classiﬁcation task than our uniform weighting of intermediate tar- gets. We believe this may owe to the peculiar nature of our data. Linear gain emphasizes evidence from later in the sequence, an assumption which often does not match the progression of symptoms in critical illnesses. Asthma patients, for example, are often admitted to the ICU severely symp- tomatic, but once treatment begins, patient physiology stabilizes and observable signs of disease may abate or change. Further supporting this idea, we observed that when training ﬁxed-window baselines, using the ﬁrst 6 and last 6 hours outperformed using the last 12 hours only. While our data is of large scale by clinical standards, it is small relative to datasets found in deep learning tasks like vision and speech recognition. At this scale, regularization is crit- ical. Our experiments demonstrate that tar- get replication, auxiliary outputs, and dropout all work to reduce the generalization gap. as shown in Figure 4 and Appendix B. How- ever, some of these techniques are complemen- tary while others seem to cancel each other out. For example, our best model combined target replication with dropout. This combination sig- niﬁcantly improved upon the performance us- ing target replication alone, and enabled the ef- fective use of larger capacity models. In con- trast, the beneﬁts of dropout and auxiliary out- put training appear to wash each other out. This  Figure 4: Training curves showing the impact of the DO, AO, and TR strategies on overﬁtting.  9  Published as a conference paper at ICLR 2016  may be because target replication confers more than regularization, mitigating the difﬁculty of learn- ing long range dependencies by providing local objectives.  7 CONCLUSION  While our LSTMs produce promising results, this is only a ﬁrst step in this line of research. Recog- nizing diagnoses given full time series of sensor data demonstrates that LSTMs can capture meaning- ful signal, but ultimately we would like to predict developing conditions and events, outcomes such as mortality, and treatment responses. In this paper we used diagnostic labels without timestamps, but we are obtaining timestamped diagnoses, which will enable us to train models to perform early diagnosis by predicting future conditions. In addition, we are extending this work to a larger PICU data set with 50% more patients and hundreds of variables, including treatments and medications. On the methodological side, we would like to both better exploit and improve the capabilities of LSTMs. Results from speech recognition have shown that LSTMs shine in comparison to other models using raw features, minimizing need for preprocessing and feature engineering. In contrast, our current data preparation pipeline removes valuable structure and information from clinical time series that could be exploited by an LSTM. For example, our forward- and back-ﬁlling imputation strategies discard useful information about when each observation is recorded. Imputing normal values for missing time series ignores the meaningful distinction between truly normal and miss- ing measurements. Also, our window-based resampling procedure reduces the variability of more frequently measured vital signs (e.g., heart rate). In future work, we plan to introduce indicator variables to allow the LSTM to distinguish actual from missing or imputed measurements. Additionally, the ﬂexibility of the LSTM architecture should enable us to eliminate age-based corrections and to incorporate non-sequential inputs, such as age, weight, and height (or even hand-engineered features), into predictions. Other next steps in this direction include developing LSTM architectures to directly handle missing values and irregular sampling. We also are encouraged by the success of target replication and plan to explore other variants of this technique and to apply it to other domains and tasks. Additionally, we acknowledge that there remains a debate about the interpretability of neural networks when applied to complex medical problems. We are developing methods to interpret the representations learned by LSTMs in order to better expose patterns of health and illness to clinical users. We also hope to make practical use of the distributed representations of patients for tasks such as patient similarity search.  8 ACKNOWLEDGEMENTS  Zachary C. Lipton was supported by the Division of Biomedical Informatics at the University of California, San Diego, via training grant (T15LM011271) from the NIH/NLM. David Kale was supported by the Alfred E. Mann Innovation in Engineering Doctoral Fellowship. The VPICU was supported by grants from the Laura P. and Leland K. Whittier Foundation. We acknowledge NVIDIA Corporation for Tesla K40 GPU hardware donation and Professors Julian McAuley and Greg Ver Steeg for their support and advice. Finally, we thank the anonymous ICLR reviewers for their feedback, which helped us to make signiﬁcant improvements to this work and manuscript.  REFERENCES  Aleks, Norm, Russell, Stuart J, Madden, Michael G, Morabito, Diane, Staudenmayer, Kristan, Co- hen, Mitchell, and Manley, Geoffrey T. Probabilistic detection of short events, with application to critical care monitoring. In Advances in Neural Information Processing Systems (NIPS) 21, pp. 49–56, 2009.  Amari, Shun-ichi and Cichocki, Andrzej. Adaptive blind signal processing-neural network ap-  proaches. Proceedings of the IEEE, 86(10):2026–2048, 1998.  Auli, Michael, Galley, Michel, Quirk, Chris, and Zweig, Geoffrey. Joint language and translation modeling with recurrent neural networks. In Empirical Methods in Natural Language Proessing (EMNPL), volume 3, 2013.  10  Published as a conference paper at ICLR 2016  Baxt, W.G. Application of artiﬁcial neural networks to clinical medicine. The Lancet, 346(8983):  1135–1138, 1995.  Caruana, Rich, Baluja, Shumeet, Mitchell, Tom, et al. Using the future to “sort out” the present: Rankprop and multitask learning for medical risk evaluation. In Advances in Neural Information Processing Systems (NIPS) 8, pp. 959–965, 1996.  Che, Zhengping, Kale, David C., Li, Wenzhe, Bahadori, Mohammad Taha, and Liu, Yan. Deep computational phenotyping. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 507–516. ACM, 2015.  Dabek, Filip and Caban, Jesus J. A neural network based model for predicting psychological condi-  tions. In Brain Informatics and Health, pp. 252–261. Springer, 2015.  Dai, Andrew M and Le, Quoc V. Semi-supervised sequence learning.  Information Processing Systems (NIPS) 28, pp. 3061–3069, 2015.  In Advances in Neural  De Mulder, Wim, Bethard, Steven, and Moens, Marie-Francine. A survey on the application of recurrent neural networks to statistical language modeling. Computer Speech & Language, 30(1): 61–98, 2015.  Elman, Jeffrey L. Finding structure in time. Cognitive Science, 14(2):179–211, 1990.  Fleming, Susannah, Thompson, Matthew, Stevens, Richard, Heneghan, Carl, Plddemann, Annette, Maconochie, Ian, Tarassenko, Lionel, and Mant, David. Normal ranges of heart rate and respi- ratory rate in children from birth to 18 years: A systematic review of observational studies. The Lancet, pp. 1011–1018, 2011.  Gers, Felix and Schmidhuber, J¨urgen. Recurrent nets that time and count. In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN), volume 3, pp. 189–194. IEEE, 2000.  Gers, Felix A., Schmidhuber, J¨urgen, and Cummins, Fred. Learning to forget: Continual prediction  with LSTM. Neural Computation, 12(10):2451–2471, 2000.  Gers, Felix A., Schraudolph, Nicol N., and Schmidhuber, J¨urgen. Learning precise timing with  LSTM recurrent networks. The Journal of Machine Learning Research, 3:115–143, 2003.  Ghassemi, Marzyeh, Pimentel, Marco AF, Naumann, Tristan, Brennan, Thomas, Clifton, David A, Szolovits, Peter, and Feng, Mengling. A multivariate timeseries modeling approach to severity of illness assessment and forecasting in ICU with sparse, heterogeneous clinical data. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015.  Graves, Alex. Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of  Studies in Computational Intelligence. Springer-Verlag Berlin Heidelberg, 2012.  Graves, Alex.  Generating sequences with recurrent neural networks.  arXiv:1308.0850, 2013.  arXiv preprint  Graves, Alex, Liwicki, Marcus, Fern´andez, Santiago, Bertolami, Roman, Bunke, Horst, and Schmidhuber, J¨urgen. A novel connectionist system for unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(5):855–868, 2009.  Hammerla, Nils Y, Fisher, James M, Andras, Peter, Rochester, Lynn, Walker, Richard, and Pl¨otz, Thomas. PD disease state assessment in naturalistic environments using deep learning. In Pro- ceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015.  Henry, Katharine E, Hager, David N, Pronovost, Peter J, and Saria, Suchi. A targeted real-time early warning score (trewscore) for septic shock. Science Translational Medicine, 7(299 299ra122): 1–9, 2015.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural Computation, 9(8):  1735–1780, 1997.  11  Published as a conference paper at ICLR 2016  Karpathy, Andrej and Fei-Fei, Li. Deep visual-semantic alignments for generating image descrip- tions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3128–3137, June 2015.  Lasko, Thomas A., Denny, Joshua C., and Levy, Mia A. Computational phenotype discovery using unsupervised feature learning over noisy, sparse, and irregular clinical data. PLoS ONE, 8(6): e66341, 06 2013.  Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang, Zhengyou, and Tu, Zhuowen. Deeply- supervised nets. In Proceedings of the Eighteenth International Conference on Artiﬁcial Intelli- gence and Statistics (AISTATS), 2015.  Lipton, Zachary C, Elkan, Charles, and Naryanaswamy, Balakrishnan. Optimal thresholding of clas- siﬁers to maximize F1 measure. In Machine Learning and Knowledge Discovery in Databases, pp. 225–239. Springer, 2014.  Lipton, Zachary C., Berkowitz, John, and Elkan, Charles. A critical review of recurrent neural  networks for sequence learning. arXiv preprint arXiv:1506.00019, 2015.  Liu, I, Ramakrishnan, Bhiksha, et al. Bach in 2014: Music composition with recurrent neural  network. arXiv preprint arXiv:1412.3191, 2014.  Liwicki, Marcus, Graves, Alex, Bunke, Horst, and Schmidhuber, J¨urgen. A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks. In Proceedings of the Ninth International Conference on Document Analysis and Recognition, vol- ume 1, pp. 367–371, 2007.  Marlin, Ben M., Kale, David C., Khemani, Robinder G., and Wetzel, Randall C. Unsupervised pat- tern discovery in electronic health care data using probabilistic clustering models. In Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium (IHI), 2012.  Mason, Robert J., Broaddus, V. Courtney, Martin, Thomas, King Jr., Talmadge E., Schraufnagel, Dean, Murray, John F., and Nadel, Jay A. Murray and Nadel’s textbook of respiratory medicine: 2-volume set. Elsevier Health Sciences, 2010.  National High Blood Pressure Education Program Working Group on Children and Adolescents. The fourth report on the diagnosis, evaluation, and treatment of high blood pressure in children and adolescents. Pediatrics, 114:555–576, 2004.  Ng, Joe Yue-Hei, Hausknecht, Matthew, Vijayanarasimhan, Sudheendra, Vinyals, Oriol, Monga, Rajat, and Toderici, George. Beyond short snippets: Deep networks for video classiﬁcation. arXiv preprint arXiv:1503.08909, 2015.  Oellrich, Anika, Collier, Nigel, Groza, Tudor, Rebholz-Schuhmann, Dietrich, Shah, Nigam, Boden- reider, Olivier, Boland, Mary Regina, Georgiev, Ivo, Liu, Hongfang, Livingston, Kevin, Luna, Augustin, Mallon, Ann-Marie, Manda, Prashanti, Robinson, Peter N., Rustici, Gabriella, Simon, Michelle, Wang, Liqin, Winnenburg, Rainer, and Dumontier, Michel. The digital revolution in phenotyping. Brieﬁngs in Bioinformatics, 2015.  Pascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, and Bengio, Yoshua. How to construct deep In International Conference on Learning Representations (ICLR),  recurrent neural networks. 2014.  Pham, Vu, Bluche, Th´eodore, Kermorvant, Christopher, and Louradour, J´erˆome. Dropout improves recurrent neural networks for handwriting recognition. In Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on, pp. 285–290. IEEE, 2014.  Pollack, M. M., Patel, K. M., and Ruttimann, U. E. PRISM III: an updated Pediatric Risk of Mor-  tality score. Critical Care Medicine, 24(5):743–752, 1996.  Pollastri, Gianluca, Przybylski, Darisz, Rost, Burkhard, and Baldi, Pierre. Improving the prediction of protein secondary structure in three and eight classes using recurrent neural networks and proﬁles. Proteins: Structure, Function, and Bioinformatics, 47(2):228–235, 2002.  12  Published as a conference paper at ICLR 2016  Quinn, John, Williams, Christopher KI, McIntosh, Neil, et al. Factorial switching linear dynamical systems applied to physiological condition monitoring. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(9):1537–1551, 2009.  Rughani, Anand I., Dumont, Travis M., Lu, Zhenyu, Bongard, Josh, Horgan, Michael A., Penar, Paul L., and Tranmer, Bruce I. Use of an artiﬁcial neural network to predict head injury outcome: clinical article. Journal of Neurosurgery, 113(3):585–590, 2010.  Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning internal representations  by error propagation. Technical report, DTIC Document, 1985.  Saria, Suchi, Koller, Daphne, and Penn, Anna. Learning individual and population level traits from clinical temporal data. In Proc. Neural Information Processing Systems (NIPS), Predictive Models in Personalized Medicine Workshop. Citeseer, 2010.  Schulam, Peter, Wigley, Fredrick, and Saria, Suchi. Clustering longitudinal clinical marker tra- jectories from electronic health data: Applications to phenotyping and endotype discovery. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015.  Silipo, Rosaria and Marchesi, Carlo. Artiﬁcial neural networks for automatic ecg analysis. IEEE  Transactions on Signal Processing, 46(5):1417–1425, 1998.  Stanculescu, Ioan, Williams, Christopher K, Freer, Yvonne, et al. Autoregressive hidden markov models for the early detection of neonatal sepsis. Biomedical and Health Informatics, IEEE Journal of, 18(5):1560–1570, 2014a.  Stanculescu, Ioan, Williams, Christopher KI, and Freer, Yvonne. A hierarchical switching linear dynamical system applied to the detection of sepsis in neonatal condition monitoring. In Pro- ceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2014b.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural net- works. In Advances in Neural Information Processing Systems (NIPS) 27, pp. 3104–3112, 2014.  Tresp, Volker and Briegel, Thomas. A solution for missing data in recurrent neural networks with an application to blood glucose prediction. In Advances in Neural Information Processing Systems (NIPS) 10, pp. 971–977. 1998.  ¨Ubeyli, Elif Derya. Combining recurrent neural networks with eigenvector methods for classiﬁcation  of ecg beats. Digital Signal Processing, 19(2):320–329, 2009.  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural im- age caption generator. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3156–3164, June 2015.  Vohradsk´y, Jiˇr´ı. Neural network model of gene expression. The FASEB Journal, 15(3):846–854,  2001.  World Health Organization.  International statistical classiﬁcation of diseases and related health  problems, volume 1. World Health Organization, 2004.  Xu, Rui, Wunsch II, Donald, and Frank, Ronald.  Inference of genetic regulatory networks with recurrent neural network models using particle swarm optimization. IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB), 4(4):681–692, 2007.  Yeung, Serena, Russakovsky, Olga, Jin, Ning, Andriluka, Mykhaylo, Mori, Greg, and Fei-Fei, Li. Every moment counts: Dense detailed labeling of actions in complex videos. arXiv preprint arXiv:1507.05738, 2015.  Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol. Recurrent neural network regularization.  arXiv preprint arXiv:1409.2329, 2014.  13  Published as a conference paper at ICLR 2016  Appendices  A HOURLY DIAGNOSTIC PREDICTIONS  Our LSTM networks predict 128 diagnoses given sequences of clinical measurements. Because each network is connected left-to-right, i.e., in chronological order, we can output predictions at each sequence step. Ultimately, we imagine that this capability could be used to make continuously updated real-time alerts and diagnoses. Below, we explore this capability qualitatively. We choose examples of patients with a correctly classiﬁed diagnosis and visualize the probabilities assigned by each LSTM model at each sequence step. In addition to improving the quality of the ﬁnal output, the LSTMs with target replication (LSTM-TR) arrive at correct diagnoses quickly compared to the simple multilabel LSTM model (LSTM-Simple). When auxiliary outputs are also used (LSTM- TR,AO), the diagnoses appear to be generally more conﬁdent.  (a) Asthma with Status Asthmaticus  (b) Acute Respiratory Distress Syndrome  (c) Diabetic Ketoacidosis  (d) Brain Neoplasm, Unspeciﬁed Nature  (e) Septic Shock  (f) Scoliosis  Figure 5: Each chart depicts the probabilities assigned by each of four models at each (hourly re- sampled) time step. LSTM-Simple uses only targets at the ﬁnal time step. LSTM-TR uses target replication. LSTM-AO uses auxiliary outputs (diagnoses), and LSTM-TR,AO uses both techniques. LSTMs with target replication learn to make accurate diagnoses earlier.  14  Published as a conference paper at ICLR 2016  Our LSTM-TR,AO effectively predicts status asthmaticus and acute respiratory distress syndrome, likely owing to the several measures of pulmonary function among our inputs. Diabetic ketoaci- dosis also proved easy to diagnose, likely because glucose and pH are included among our clinical measurements. We were surprised to see that the network classiﬁed scoliosis reliably, but a deeper look into the medical literature suggests that scoliosis often results in respiratory symptoms. This analysis of step-by-step predictions is preliminary and informal, and we note that for a small number of examples our data preprocessing introduces a target leak by back-ﬁlling missing values. In future work, when we explore this capability in greater depth, we will reprocess the data.  B LEARNING CURVES  We present visualizations of the performance of LSTM, LSTM-DO (with dropout probability 0.5), LSTM-AO (using the 301 additional diagnoses), and LSTM-TR (with α = 0.5), during training. These charts are useful for examining the effects of dropout, auxiliary outputs, and target replication on both the speed of learning and the regularization they confer. Speciﬁcally, for each of the four models, we plot the training and validation micro AUC and F1 score every ﬁve epochs in Figure 6. Additionally, we plot a scatter of the performance on the training set vs. the performance on the validation set. The LSTM with target replication learns more quickly than a simple LSTM and also suffers less overﬁtting. With both dropout and auxiliary outputs, the LSTM trains more slowly than a simple LSTM but suffers considerably less overﬁtting.  (a) AUC learning curves  (b) F1 learning curves  (c) AUC training vs. validation  (d) F1 training vs. validation  Figure 6: Training and validation performance plotted for the simple multilabel network (LSTM- Simple), LSTM with target replication (LSTM-TR), and LSTM with auxiliary outputs (LSTM-AO). Target replication appears to increase the speed of learning and confers a small regularizing effect. Auxiliary outputs slow down the speed of learning but impart a strong regularizing effect.  15  Published as a conference paper at ICLR 2016  C PER DIAGNOSIS RESULTS  While averaged statistics provide an efﬁcient way to check the relative quality of various models, considerable information is lost by reducing performance to a single scalar quantity. For some labels, our classiﬁer makes classiﬁcations with surprisingly high accuracy while for others, our features are uninformative and thus the classiﬁer would not be practically useful. To facilitate a more granular investigation of our model’s predictive power, we present individual test set F1 and AUC scores for each individual diagnostic label in Table 3. We compare the performance our best LSTM, which combines two 128-cell hidden layers with dropout of probability 0.5 and target replication, against the strongest baseline, an MLP trained on the hand-engineered features, and an ensemble predicts the maximum probability of the two. The results are sorted in descending order using the F1 performance of the LSTM, providing insights into the types of conditions that the LSTM can successfully classify.  Classiﬁer Performance on Each Diagnostic Code, Sorted by F1  Condition Diabetes mellitus with ketoacidosis Scoliosis, idiopathic Asthma, unspeciﬁed with status asthmaticus Neoplasm, brain, unspeciﬁed nature Developmental delay Acute respiratory distress syndrome (ARDS) Hypertension, unspeciﬁed Arteriovenous malformation of brain End stage renal disease on dialysis Acute respiratory failure Renal transplant status post Epilepsy, unspeciﬁed, not intractable Septic shock Other respiratory symptom Biliary atresia Acute lymphoid leukemia, without remission Congenital hereditary muscular dystrophy Liver transplant status post Respiratory complications, prodecure status post Grand mal status Intracranial injury, closed Diabetes insipidus Acute renal failure, unspeciﬁed Other diseases of the respiratory system Croup syndrome Bronchiolitis due to other infectious organism Congestive heart failure Infantile cerebral palsy, unspeciﬁed Congenital hydrocephalus Cerebral edema Craniosynostosis Anoxic brain damage Pneumonitis due to inhalation of food or vomitus Acute and subacute necrosis of the liver Respiratory syncytial virus Unspeciﬁed disorder of kidney and ureter Craniofacial malformation Pulmonary hypertension, secondary Bronchopulmonary dysplasia Drowning and non-fatal submersion Genetic abnormality Other and unspeciﬁed coagulation defects Vehicular trauma  LSTM-DO-TR F1 AUC  0.8571 0.6809 0.5641 0.5430 0.4751 0.4688 0.4118 0.4000 0.3889 0.3864 0.3846 0.3740 0.3721 0.3690 0.3636 0.3486 0.3478 0.3448 0.3143 0.3067 0.3048 0.2963 0.2553 0.2529 0.2500 0.2466 0.2439 0.2400 0.2393 0.2222 0.2222 0.2222 0.2222 0.2182 0.2154 0.2069 0.2059 0.2000 0.1905 0.1905 0.1828 0.1818 0.1778  0.9966 0.8543 0.9232 0.8522 0.8178 0.9595 0.8593 0.8620 0.8436 0.7960 0.9692 0.7577 0.8182 0.8088 0.9528 0.8601 0.8233 0.8431 0.8545 0.8003 0.8589 0.9455 0.8806 0.7999 0.9171 0.9386 0.8857 0.8538 0.7280 0.8823 0.8305 0.8108 0.6547 0.8674 0.9118 0.8367 0.8688 0.9377 0.8427 0.8341 0.6727 0.7081 0.8655  MLP, Expert features  F1  AUC  0.8571 0.6169 0.6296 0.5263 0.4023 0.3913 0.3704 0.3750 0.3810 0.4128 0.4828 0.3145 0.3210 0.3642 0.5000 0.3288 0.0000 0.3333 0.2133 0.3883 0.3095 0.3774 0.2472 0.1864 0.1538 0.2353 0.0000 0.1569 0.2247 0.2105 0.5333 0.1333 0.0326 0.2778 0.1143 0.1667 0.4444 0.0870 0.1404 0.1538 0.1077 0.0000 0.2642  0.9966 0.8467 0.9544 0.8463 0.8294 0.9645 0.8637 0.8633 0.8419 0.7990 0.9693 0.7265 0.8640 0.7898 0.9338 0.8293 0.8337 0.8104 0.8614 0.7917 0.8621 0.9372 0.8698 0.7920 0.9183 0.9315 0.8797 0.8492 0.7337 0.9143 0.8521 0.8134 0.6776 0.9039 0.8694 0.8496 0.8633 0.8969 0.8438 0.8905 0.6343 0.7507 0.8505  Max Ensemble F1 AUC  0.8571 0.6689 0.6667 0.5616 0.4434 0.4211 0.3636 0.3600 0.3902 0.4155 0.4800 0.3795 0.3519 0.3955 0.4444 0.3175 0.2727 0.3846 0.3438 0.3529 0.3297 0.4068 0.2951 0.2400 0.0000 0.2712 0.0000 0.2083 0.1875 0.2500 0.6154 0.2500 0.0462 0.2381 0.1622 0.1667 0.3158 0.2105 0.1333 0.1429 0.1111 0.1600 0.2295  0.9966 0.8591 0.9490 0.8618 0.8344 0.9650 0.8652 0.8684 0.8464 0.8016 0.9713 0.7477 0.8546 0.8114 0.9541 0.8441 0.8778 0.8349 0.8672 0.8088 0.8820 0.9578 0.8821 0.8131 0.9263 0.9425 0.8872 0.8515 0.7444 0.9190 0.8658 0.8193 0.6905 0.8964 0.9031 0.8559 0.8866 0.9343 0.8617 0.8792 0.6745 0.7328 0.8723  Table 3: F1 and AUC scores for individual diagnoses.  16  Published as a conference paper at ICLR 2016  Classiﬁer Performance on Each Diagnostic Code, Sorted by F1  Condition Other speciﬁed cardiac dysrhythmia Acute pancreatitis Esophageal reﬂux Cardiac arrest, outside hospital Unspeciﬁed pleural effusion Mycoplasma pneumoniae Unspeciﬁed immunologic disorder Congenital alveolar hypoventilation syndrome Septicemia, unspeciﬁed Pneumonia due to adenovirus Insomnia with sleep apnea Deﬁbrination syndrome Unspeciﬁed injury, unspeciﬁed site Pneumococcal pneumonia Genetic or other unspeciﬁed anomaly Other spontaneous pneumothorax Bone marrow transplant status Other primary cardiomyopathies Intracranial hemorrhage Benign intracranial hypertension Encephalopathy, unspeciﬁed Ventricular septal defect Crushing injury, unspeciﬁed Malignant neoplasm, disseminated Orthopaedic surgery, post status Thoracic surgery, post status Ostium secundum type atrial septal defect Malignant neoplasm, in gastrointestinal organs Coma Pneumonia due to inhalation of food or vomitus Extradural hemorrage from injury, no open wound Prematurity (less than 37 weeks gestation) Asthma, unspeciﬁed, without status asthmaticus Gastrointestinal surgery, post status Nervous disorder, not elsewhere classiﬁed Unspeciﬁed gastrointestinal disorder Pulmonary congestion and hypostasis Thrombocytopenia, unspeciﬁed Lung contusion, no open wound Acute pericarditis, unspeciﬁed Nervous system complications from implant Heart disease, unspeciﬁed Suspected infection in newborn or infant  Max Ensemble AUC F1  0.0800 0.1379 0.1739 0.1765 0.1250 0.1505 0.1111 0.0000 0.1905 0.1277 0.0899 0.2500 0.1250 0.1461 0.1429 0.1156 0.2353 0.1212 0.1587 0.1379 0.0000 0.0833 0.1200 0.0667 0.0845 0.0463 0.1154 0.1412 0.1250 0.0952 0.0988 0.1316 0.0678 0.0851 0.1404 0.0317 0.0000 0.0000 0.2222 0.0000 0.0419 0.0000 0.0606  0.8179 0.8440 0.8090 0.8964 0.8656 0.8955 0.8692 0.7246 0.8663 0.8947 0.8089 0.9460 0.8314 0.8727 0.7905 0.8122 0.8638 0.6635 0.7540 0.8829 0.8300 0.6667 0.9111 0.7812 0.8106 0.9137 0.7998 0.7991 0.7224 0.8422 0.8246 0.7530 0.6867 0.7069 0.7429 0.6713 0.8687 0.7360 0.9359 0.9089 0.7129 0.8264 0.6954  LSTM-DO-TR F1 AUC  MLP, Expert features  F1  AUC  0.7698 0.8286 0.8236 0.8562 0.8777 0.8978 0.8481 0.6381 0.8595 0.8467 0.7892 0.9339 0.8749 0.8706 0.7830 0.8029 0.8136 0.6862 0.7498 0.9118 0.8466 0.6781 0.9183 0.7639 0.7605 0.9160 0.7876 0.8067 0.7255 0.8282 0.7829 0.7542 0.6679 0.7183 0.7127 0.6372 0.8359 0.7652 0.9237 0.8601 0.6727 0.8372 0.6593  0.1250 0.1053 0.0000 0.1333 0.1194 0.1067 0.1000 0.0000 0.1695 0.0690 0.0752 0.1935 0.0000 0.1149 0.0870 0.0972 0.0000 0.0000 0.1458 0.0909 0.0909 0.0741 0.0952 0.0588 0.1290 0.0432 0.1538 0.1111 0.1111 0.0923 0.0000 0.1628 0.0784 0.0984 0.1374 0.0348 0.0000 0.0000 0.0000 0.0000 0.0368 0.0000 0.0000  0.8411 0.8087 0.7774 0.9004 0.8190 0.8852 0.8692 0.7609 0.8640 0.9121 0.7211 0.9461 0.7673 0.8664 0.7812 0.8058 0.8854 0.6371 0.7306 0.7613 0.7886 0.6534 0.8742 0.7635 0.8234 0.7401 0.8068 0.7226 0.6542 0.8090 0.8339 0.7345 0.6914 0.6999 0.7589 0.6831 0.8633 0.7185 0.9129 0.9132 0.7082 0.8020 0.7090  0.1667 0.1622 0.1515 0.1500 0.1458 0.1429 0.1429 0.1429 0.1395 0.1379 0.1359 0.1333 0.1333 0.1290 0.1277 0.1212 0.1176 0.1176 0.1071 0.1053 0.1053 0.1053 0.1017 0.0984 0.0976 0.0930 0.0923 0.0853 0.0833 0.0800 0.0769 0.0759 0.0734 0.0714 0.0708 0.0702 0.0678 0.0660 0.0639 0.0625 0.0597 0.0588 0.0588  17  Published as a conference paper at ICLR 2016  Classiﬁer Performance on Each Diagnostic Code, Sorted by F1  Condition Anemia, unspeciﬁed Muscular disorder, not elsewhere classiﬁed Malignant neoplasm, adrenal gland Hematologic disorder, unspeciﬁed Hematemesis Dehydration Unspeciﬁed disease of spinal cord Neuroﬁbromatosis, unspeciﬁed Intra-abdominal injury, no open wound Thyroid disorder, unspeciﬁed Hereditary hemolytic anemia, unspecifed Subdural hemorrage, no open wound Unspeciﬁed intestinal obstruction Hyposmolality and/or hyponatremia Primary malignant neoplasm, thorax Supraventricular premature beats Injury to intrathoracic organs, no open wound Child abuse, unspeciﬁed Acidosis Infantile spinal muscular atrophy Fracture, femoral shaft Cystic ﬁbrosis with pulmonary manifestations Panhypopituitarism Blood in stool Sickle-cell anemia, unspeciﬁed Cardiac dysrhythmia, unspeciﬁed Agranulocytosis Malignancy of bone, no site speciﬁed Pneumonia, organism unspeciﬁed Unspeciﬁed metabolic disorder Urinary tract infection, no site speciﬁed Obesity, unspeciﬁed Apnea Respiratory arrest Hypovolemic shock Hemophilus meningitis Diabetes mellitus, type I, stable Tetralogy of fallot Congenital heart disease, unspeciﬁed Mechanical complication of V-P shunt Respiratory complications due to procedure Teenage cerebral artery occlusion and infarction  Max Ensemble AUC F1  0.0727 0.1000 0.0548 0.0714 0.0588 0.0870 0.0537 0.0613 0.0690 0.0336 0.0000 0.0444 0.0606 0.0000 0.0323 0.0299 0.0000 0.1818 0.0000 0.0000 0.0513 0.0571 0.0500 0.0000 0.0000 0.0000 0.1667 0.0667 0.0000 0.0000 0.2286 0.0000 0.0000 0.0000 0.0000 0.0000 0.0833 0.0000 0.0000 0.0000 0.0000 0.0000  0.7380 0.7276 0.6846 0.7446 0.8103 0.7552 0.7388 0.7671 0.8220 0.6062 0.6962 0.7731 0.7277 0.7502 0.5996 0.8146 0.8604 0.9406 0.9306 0.9641 0.9233 0.8852 0.8872 0.8872 0.7867 0.8523 0.8028 0.8318 0.8171 0.7283 0.7890 0.7872 0.8083 0.8346 0.8296 0.7721 0.7410 0.6738 0.7319 0.7205 0.7323 0.6507  LSTM-DO-TR F1 AUC  MLP, Expert features  F1  AUC  0.7782 0.6996 0.6960 0.7315 0.8116 0.7317 0.7153 0.7494 0.7682 0.5969 0.7474 0.7620 0.6210 0.6999 0.6154 0.8278 0.8354 0.9273 0.9191 0.9158 0.9116 0.8927 0.8799 0.8424 0.8268 0.8202 0.8157 0.8128 0.8008 0.7914 0.7867 0.7826 0.7822 0.7729 0.7686 0.7649 0.7329 0.7326 0.7270 0.7173 0.7024 0.6377  0.0488 0.0000 0.0727 0.1194 0.0674 0.1739 0.0571 0.0516 0.1569 0.0548 0.0000 0.1132 0.2041 0.0000 0.0364 0.0190 0.0000 0.3158 0.1176 0.0000 0.0000 0.0000 0.2222 0.0000 0.0000 0.0702 0.1818 0.0870 0.0952 0.0000 0.0840 0.0556 0.2703 0.0000 0.0000 0.0000 0.0667 0.0000 0.1333 0.0000 0.0000 0.0000  0.7019 0.7354 0.6682 0.7404 0.7887 0.7287 0.7481 0.7458 0.8602 0.5653 0.6182 0.7353 0.7684 0.7565 0.6086 0.7577 0.8681 0.9417 0.9260 0.8511 0.9372 0.8086 0.8799 0.8443 0.7317 0.8372 0.8011 0.7763 0.8146 0.6719 0.7719 0.7550 0.8189 0.8592 0.8293 0.7877 0.7435 0.6134 0.7251 0.7308 0.7244 0.5982  0.0541 0.0536 0.0472 0.0465 0.0455 0.0435 0.0432 0.0403 0.0333 0.0293 0.0290 0.0263 0.0260 0.0234 0.0233 0.0185 0.0115 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000  18  ",
1511.05952,2016,Prioritized Experience Replay,"['Prioritized Experience Replay\nTom Schaul', 'John Quan', 'Ioannis Antonoglou', 'David Silver']",https://arxiv.org/pdf/1511.05952,"6 1 0 2     b e F 5 2         ]  G L . s c [      4 v 2 5 9 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  PRIORITIZED EXPERIENCE REPLAY  Tom Schaul, John Quan, Ioannis Antonoglou and David Silver Google DeepMind {schaul,johnquan,ioannisa,davidsilver}@google.com  ABSTRACT  Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their signiﬁcance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efﬁciently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state- of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.  1  INTRODUCTION  Online reinforcement learning (RL) agents incrementally update their parameters (of the policy, value function or model) while they observe a stream of experience. In their simplest form, they discard incoming data immediately, after a single update. Two issues with this are (a) strongly correlated updates that break the i.i.d. assumption of many popular stochastic gradient-based algo- rithms, and (b) the rapid forgetting of possibly rare experiences that would be useful later on. Experience replay (Lin, 1992) addresses both of these issues: with experience stored in a replay memory, it becomes possible to break the temporal correlations by mixing more and less recent experience for the updates, and rare experience will be used for more than just a single update. This was demonstrated in the Deep Q-Network (DQN) algorithm (Mnih et al., 2013; 2015), which stabilized the training of a value function, represented by a deep neural network, by using experience replay. Speciﬁcally, DQN used a large sliding window replay memory, sampled from it uniformly at random, and revisited each transition1 eight times on average. In general, experience replay can reduce the amount of experience required to learn, and replace it with more computation and more memory – which are often cheaper resources than the RL agent’s interactions with its environment. In this paper, we investigate how prioritizing which transitions are replayed can make experience replay more efﬁcient and effective than if all transitions are replayed uniformly. The key idea is that an RL agent can learn more effectively from some transitions than from others. Transitions may be more or less surprising, redundant, or task-relevant. Some transitions may not be immediately useful to the agent, but might become so when the agent competence increases (Schmidhuber, 1991). Experience replay liberates online learning agents from processing transitions in the exact order they are experienced. Prioritized replay further liberates agents from considering transitions with the same frequency that they are experienced. In particular, we propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead to a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which we correct with importance sampling. Our resulting algorithms are robust and scalable, which we demonstrate on the Atari 2600 benchmark suite, where we obtain faster learning and state-of-the-art performance.  1 A transition is the atomic unit of interaction in RL, in our case a tuple of (state St−1, action At−1, reward Rt, discount γt, next state St). We choose this for simplicity, but most of the arguments in this paper also hold for a coarser ways of chunking experience, e.g. into sequences or episodes.  1  Published as a conference paper at ICLR 2016  2 BACKGROUND  Numerous neuroscience studies have identiﬁed evidence of experience replay in the hippocampus of rodents, suggesting that sequences of prior experience are replayed, either during awake resting or sleep. Sequences associated with rewards appear to be replayed more frequently (Atherton et al., 2015; ´Olafsd´ottir et al., 2015; Foster & Wilson, 2006). Experiences with high magnitude TD error also appear to be replayed more often (Singer & Frank, 2009; McNamara et al., 2014). It is well-known that planning algorithms such as value iteration can be made more efﬁcient by prioritizing updates in an appropriate order. Prioritized sweeping (Moore & Atkeson, 1993; Andre et al., 1998) selects which state to update next, prioritized according to the change in value, if that update was executed. The TD error provides one way to measure these priorities (van Seijen & Sutton, 2013). Our approach uses a similar prioritization method, but for model-free RL rather than model-based planning. Furthermore, we use a stochastic prioritization that is more robust when learning a function approximator from samples. TD-errors have also been used as a prioritization mechanism for determining where to focus re- sources, for example when choosing where to explore (White et al., 2014) or which features to select (Geramifard et al., 2011; Sun et al., 2011). In supervised learning, there are numerous techniques to deal with imbalanced datasets when class identities are known, including re-sampling, under-sampling and over-sampling techniques, possibly combined with ensemble methods (for a review, see Galar et al., 2012). A recent paper introduced a form of re-sampling in the context of deep RL with experience replay (Narasimhan et al., 2015); the method separates experience into two buckets, one for positive and one for negative rewards, and then picks a ﬁxed fraction from each to replay. This is only applicable in domains that (unlike ours) have a natural notion of ‘positive/negative’ experience. Furthermore, Hinton (2007) introduced a form of non-uniform sampling based on error, with an importance sampling correction, which led to a 3x speed-up on MNIST digit classiﬁcation. There have been several proposed methods for playing Atari with deep reinforcement learning, in- cluding deep Q-networks (DQN) (Mnih et al., 2013; 2015; Guo et al., 2014; Stadie et al., 2015; Nair et al., 2015; Bellemare et al., 2016), and the Double DQN algorithm (van Hasselt et al., 2016), which is the current published state-of-the-art. Simultaneously with our work, an architectural innovation that separates advantages from the value function (see the co-submission by Wang et al., 2015) has also led to substantial improvements on the Atari benchmark.  3 PRIORITIZED REPLAY  Using a replay memory leads to design choices at two levels: which experiences to store, and which experiences to replay (and how to do so). This paper addresses only the latter: making the most effective use of the replay memory for learning, assuming that its contents are outside of our control (but see also Section 6).  3.1 A MOTIVATING EXAMPLE  To understand the potential gains of prioritization, we introduce an artiﬁcial ‘Blind Cliffwalk’ en- vironment (described in Figure 1, left) that exempliﬁes the challenge of exploration when rewards are rare. With only n states, the environment requires an exponential number of random steps until the ﬁrst non-zero reward; to be precise, the chance that a random sequence of actions will lead to the reward is 2−n. Furthermore, the most relevant transitions (from rare successes) are hidden in a mass of highly redundant failure cases (similar to a bipedal robot falling over repeatedly, before it discovers how to walk). We use this example to highlight the difference between the learning times of two agents. Both agents perform Q-learning updates on transitions drawn from the same replay memory. The ﬁrst agent replays transitions uniformly at random, while the second agent invokes an oracle to prioritize transitions. This oracle greedily selects the transition that maximally reduces the global loss in its current state (in hindsight, after the parameter update). For the details of the setup, see Appendix B.1. Figure 1 (right) shows that picking the transitions in a good order can lead to exponential speed-ups  2  Published as a conference paper at ICLR 2016  Figure 1: Left: Illustration of the ‘Blind Cliffwalk’ example domain: there are two actions, a ‘right’ and a ‘wrong’ one, and the episode is terminated whenever the agent takes the ‘wrong’ action (dashed red arrows). Taking the ‘right’ action progresses through a sequence of n states (black arrows), at the end of which lies a ﬁnal reward of 1 (green arrow); reward is 0 elsewhere. We chose a representation such that generalizing over what action is ‘right’ is not possible. Right: Median number of learning steps required to learn the value function as a function of the size of the total number of transitions in the replay memory. Note the log-log scale, which highlights the exponential speed-up from replaying with an oracle (bright blue), compared to uniform replay (black); faint lines are min/max values from 10 independent runs.  over uniform choice. Such an oracle is of course not realistic, yet the large gap motivates our search for a practical approach that improves on uniform random replay.  3.2 PRIORITIZING WITH TD-ERROR  The central component of prioritized replay is the criterion by which the importance of each transi- tion is measured. One idealised criterion would be the amount the RL agent can learn from a tran- sition in its current state (expected learning progress). While this measure is not directly accessible, a reasonable proxy is the magnitude of a transition’s TD error δ, which indicates how ‘surprising’ or unexpected the transition is: speciﬁcally, how far the value is from its next-step bootstrap esti- mate (Andre et al., 1998). This is particularly suitable for incremental, online RL algorithms, such as SARSA or Q-learning, that already compute the TD-error and update the parameters in propor- tion to δ. The TD-error can be a poor estimate in some circumstances as well, e.g. when rewards are noisy; see Appendix A for a discussion of alternatives. To demonstrate the potential effectiveness of prioritizing replay by TD error, we compare the uni- form and oracle baselines in the Blind Cliffwalk to a ‘greedy TD-error prioritization’ algorithm. This algorithm stores the last encountered TD error along with each transition in the replay memory. The transition with the largest absolute TD error is replayed from the memory. A Q-learning update is applied to this transition, which updates the weights in proportion to the TD error. New transitions arrive without a known TD-error, so we put them at maximal priority in order to guarantee that all experience is seen at least once. Figure 2 (left), shows that this algorithm results in a substantial reduction in the effort required to solve the Blind Cliffwalk task.2 Implementation: To scale to large memory sizes N, we use a binary heap data structure for the pri- ority queue, for which ﬁnding the maximum priority transition when sampling is O(1) and updating priorities (with the new TD-error after a learning step) is O(log N ). See Appendix B.2.1 for more details.  3.3 STOCHASTIC PRIORITIZATION  However, greedy TD-error prioritization has several issues. First, to avoid expensive sweeps over the entire replay memory, TD errors are only updated for the transitions that are replayed. One consequence is that transitions that have a low TD error on ﬁrst visit may not be replayed for a long time (which means effectively never with a sliding window replay memory). Further, it is sensitive to noise spikes (e.g. when rewards are stochastic), which can be exacerbated by bootstrapping, where  2 Note that a random (or optimistic) initialization of the Q-values is necessary with greedy prioritization. If initializing with zero instead, unrewarded transitions would appear to have zero error initially, be placed at the bottom of the queue, and not be revisited until the error on other transitions drops below numerical precision.  3  12n-1…nR=1Published as a conference paper at ICLR 2016  Figure 2: Median number of updates required for Q-learning to learn the value function on the Blind Cliffwalk example, as a function of the total number of transitions (only a single one of which was successful and saw the non-zero reward). Faint lines are min/max values from 10 random initializations. Black is uniform random replay, cyan uses the hindsight-oracle to select transitions, red and blue use prioritized replay (rank-based and proportional respectively). The results differ by multiple orders of magnitude, thus the need for a log-log plot. In both subplots it is evident that replaying experience in the right order makes an enormous difference to the number of updates required. See Appendix B.1 for details. Left: Tabular representation, greedy prioritization. Right: Linear function approximation, both variants of stochastic prioritization.  approximation errors appear as another source of noise. Finally, greedy prioritization focuses on a small subset of the experience: errors shrink slowly, especially when using function approximation, meaning that the initially high error transitions get replayed frequently. This lack of diversity that makes the system prone to over-ﬁtting. To overcome these issues, we introduce a stochastic sampling method that interpolates between pure greedy prioritization and uniform random sampling. We ensure that the probability of being sampled is monotonic in a transition’s priority, while guaranteeing a non-zero probability even for the lowest-priority transition. Concretely, we deﬁne the probability of sampling transition i as  i(cid:80)  pα k pα k  P (i) =  (1)  where pi > 0 is the priority of transition i. The exponent α determines how much prioritization is used, with α = 0 corresponding to the uniform case. The ﬁrst variant we consider is the direct, proportional prioritization where pi = |δi| + (cid:15), where (cid:15) is a small positive constant that prevents the edge-case of transitions not being revisited once their rank(i), where error is zero. The second variant is an indirect, rank-based prioritization where pi = 1 rank(i) is the rank of transition i when the replay memory is sorted according to |δi|. In this case, P becomes a power-law distribution with exponent α. Both distributions are monotonic in |δ|, but the latter is likely to be more robust, as it is insensitive to outliers. Both variants of stochastic prioritization lead to large speed-ups over the uniform baseline on the Cliffwalk task, as shown on Figure 2 (right). Implementation: To efﬁciently sample from distribution (1), the complexity cannot depend on N. For the rank-based variant, we can approximate the cumulative density function with a piecewise linear function with k segments of equal probability. The segment boundaries can be precomputed (they change only when N or α change). At runtime, we sample a segment, and then sample uni- formly among the transitions within it. This works particularly well in conjunction with a minibatch- based learning algorithm: choose k to be the size of the minibatch, and sample exactly one transition from each segment – this is a form of stratiﬁed sampling that has the added advantage of balanc- ing out the minibatch (there will always be exactly one transition with high magnitude δ, one with medium magnitude, etc). The proportional variant is different, also admits an efﬁcient implementa- tion based on a ‘sum-tree’ data structure (where every node is the sum of its children, with the pri- orities as the leaf nodes), which can be efﬁciently updated and sampled from. See Appendix B.2.1 for more additional details.  4  Published as a conference paper at ICLR 2016  for j = 1 to k do  Observe St, Rt, γt Store transition (St−1, At−1, Rt, γt, St) in H with maximal priority pt = maxi<t pi if t ≡ 0 mod K then  Algorithm 1 Double DQN with proportional prioritization 1: Input: minibatch k, step-size η, replay period K and size N, exponents α and β, budget T . 2: Initialize replay memory H = ∅, ∆ = 0, p1 = 1 3: Observe S0 and choose A0 ∼ πθ(S0) 4: for t = 1 to T do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end for  Sample transition j ∼ P (j) = pα Compute importance-sampling weight wj = (N · P (j)) Compute TD-error δj = Rj + γjQtarget (Sj, arg maxa Q(Sj, a)) − Q(Sj−1, Aj−1) Update transition priority pj ← |δj| Accumulate weight-change ∆ ← ∆ + wj · δj · ∇θQ(Sj−1, Aj−1)  end for Update weights θ ← θ + η · ∆, reset ∆ = 0 From time to time copy weights into target network θtarget ← θ  end if Choose action At ∼ πθ(St)  −β / maxi wi  j /(cid:80)  i pα i  3.4 ANNEALING THE BIAS  The estimation of the expected value with stochastic updates relies on those updates corresponding to the same distribution as its expectation. Prioritized replay introduces bias because it changes this distribution in an uncontrolled fashion, and therefore changes the solution that the estimates will converge to (even if the policy and state distribution are ﬁxed). We can correct this bias by using importance-sampling (IS) weights  (cid:18) 1  N  wi =  ·  1  P (i)  (cid:19)β  that fully compensates for the non-uniform probabilities P (i) if β = 1. These weights can be folded into the Q-learning update by using wiδi instead of δi (this is thus weighted IS, not ordinary IS, see e.g. Mahmood et al., 2014). For stability reasons, we always normalize weights by 1/ maxi wi so that they only scale the update downwards. In typical reinforcement learning scenarios, the unbiased nature of the updates is most important near convergence at the end of training, as the process is highly non-stationary anyway, due to changing policies, state distributions and bootstrap targets; we hypothesize that a small bias can be ignored in this context (see also Figure 12 in the appendix for a case study of full IS correction on Atari). We therefore exploit the ﬂexibility of annealing the amount of importance-sampling correction over time, by deﬁning a schedule on the exponent β that reaches 1 only at the end of learning. In practice, we linearly anneal β from its initial value β0 to 1. Note that the choice of this hyperparameter interacts with choice of prioritization exponent α; increasing both simultaneously prioritizes sampling more aggressively at the same time as correcting for it more strongly. Importance sampling has another beneﬁt when combined with prioritized replay in the context of non-linear function approximation (e.g. deep neural networks): here large steps can be very disrup- tive, because the ﬁrst-order approximation of the gradient is only reliable locally, and have to be pre- vented with a smaller global step-size. In our approach instead, prioritization makes sure high-error transitions are seen many times, while the IS correction reduces the gradient magnitudes (and thus the effective step size in parameter space), and allowing the algorithm follow the curvature of highly non-linear optimization landscapes because the Taylor expansion is constantly re-approximated. We combine our prioritized replay algorithm into a full-scale reinforcement learning agent, based on the state-of-the-art Double DQN algorithm. Our principal modiﬁcation is to replace the uniform random sampling used by Double DQN with our stochastic prioritization and importance sampling methods (see Algorithm 1).  5  Published as a conference paper at ICLR 2016  Figure 3: Difference in normalized score (the gap between random and human is 100%) on 57 games with human starts, comparing Double DQN with and without prioritized replay (rank-based variant in red, proportional in blue), showing substantial improvements in most games. Exact scores are in Table 6. See also Figure 9 where regular DQN is the baseline.  4 ATARI EXPERIMENTS  With all these concepts in place, we now investigate to what extent replay with such prioritized sampling can improve performance in realistic problem domains. For this, we chose the collection of Atari benchmarks (Bellemare et al., 2012) with their end-to-end RL from vision setup, because they are popular and contain diverse sets of challenges, including delayed credit assignment, partial observability, and difﬁcult function approximation (Mnih et al., 2015; van Hasselt et al., 2016). Our hypothesis is that prioritized replay is generally useful, so that it will make learning with experience replay more efﬁcient without requiring careful problem-speciﬁc hyperparameter tuning. We consider two baseline algorithms that use uniform experience replay, namely the version of the DQN algorithm from the Nature paper (Mnih et al., 2015), and its recent extension Double DQN (van Hasselt et al., 2016) that substantially improved the state-of-the-art by reducing the over- estimation bias with Double Q-learning (van Hasselt, 2010). Throughout this paper we use the tuned version of the Double DQN algorithm. For this paper, the most relevant component of these base- lines is the replay mechanism: all experienced transitions are stored in a sliding window memory that retains the last 106 transitions. The algorithm processes minibatches of 32 transitions sampled uniformly from the memory. One minibatch update is done for each 4 new transitions entering the memory, so all experience is replayed 8 times on average. Rewards and TD-errors are clipped to fall within [−1, 1] for stability reasons. We use the identical neural network architecture, learning algorithm, replay memory and evaluation setup as for the baselines (see Appendix B.2). The only difference is the mechanism for sampling transitions from the replay memory, with is now done according to Algorithm 1 instead of uniformly. We compare the baselines to both variants of prioritized replay (rank-based and proportional). Only a single hyperparameter adjustment was necessary compared to the baseline: Given that prior- itized replay picks high-error transitions more often, the typical gradient magnitudes are larger, so we reduced the step-size η by a factor 4 compared to the (Double) DQN setup. For the α and β0 hyperparameters that are introduced by prioritization, we did a coarse grid search (evaluated on a subset of 8 games), and found the sweet spot to be α = 0.7, β0 = 0.5 for the rank-based variant and α = 0.6, β0 = 0.4 for the proportional variant. These choices are trading off aggressiveness with robustness, but it is easy to revert to a behavior closer to the baseline by reducing α and/or increasing β. We produce the results by running each variant with a single hyperparameter setting across all games, as was done for the baselines. Our main evaluation metric is the quality of the best policy,  6  -100%0%100%200%normalizedscoreagentnormalizedscoreDoubleDQNVideo PinballDemon AttackRobotankBreakoutDefenderTutankhamBoxingStargunnerBowlingBank HeistCentipedeQ*bertYars' RevengeFreewayPongPitfall!Montezuma's RevengeBerzerkAmidarKangarooAlienAsteroidsGravitarPrivate EyeSurroundMs. Pac-ManTime PilotKung-Fu MasterFishing DerbySkiingBattlezoneWizard of WorZaxxonVentureSolarisIce HockeyUp'n DownH.E.R.O.Chopper CommandTennisRiver RaidKrullFrostbiteName This GameSeaquestCrazy ClimberBeam RiderEnduroAsterixRoad RunnerPhoenixAssaultSpace InvadersDouble DunkJames Bond 007AtlantisGopherrank-basedproportionalPublished as a conference paper at ICLR 2016  Figure 4: Summary plots of learning speed. Left: median over 57 games of the maximum baseline- normalized score achieved so far. The baseline-normalized score is calculated as in Equation 4 but using the maximum Double DQN score seen across training is used instead of the human score. The equivalence points are highlighted with dashed lines; those are the steps at which the curves reach 100%, (i.e., when the algorithm performs equivalently to Double DQN in terms of median over games). For rank-based and proportional prioritization these are at 47% and 38% of total training time. Right: Similar to the left, but using the mean instead of maximum, which captures cumulative performance rather than peak performance. Here rank-based and proportional prioritization reach the equivalence points at 41% and 43% of total training time, respectively. For the detailed learning curves that these plots summarize, see Figure 7.  in terms of average score per episode, given start states sampled from human traces (as introduced in Nair et al., 2015 and used in van Hasselt et al., 2016, which requires more robustness and gen- eralization as the agent cannot rely on repeating a single memorized trajectory). These results are summarized in Table 1 and Figure 3, but full results and raw scores can be found in Tables 7 and 6 in the Appendix. A secondary metric is the learning speed, which we summarize on Figure 4, with more detailed learning curves on Figures 7 and 8.  DQN  Double DQN (tuned)  baseline rank-based baseline rank-based proportional 128% 551% 42 33 57  106% 355% 41 25 49  48% 122% – 15 49  111% 418% – 30 57  113% 454% 38 33 57  Median Mean > baseline > human # games  Table 1: Summary of normalized scores. See Table 6 in the appendix for full results.  We ﬁnd that adding prioritized replay to DQN leads to a substantial improvement in score on 41 out of 49 games (compare columns 2 and 3 of Table 6 or Figure 9 in the appendix), with the median normalized performance across 49 games increasing from 48% to 106%. Furthermore, we ﬁnd that the boost from prioritized experience replay is complementary to the one from introducing Double Q-learning into DQN: performance increases another notch, leading to the current state-of-the-art on the Atari benchmark (see Figure 3). Compared to Double DQN, the median performance across 57 games increased from 111% to 128%, and the mean performance from 418% to 551% bringing additional games such as River Raid, Seaquest and Surround to a human level for the ﬁrst time, and making large jumps on others (e.g. Gopher, James Bond 007 or Space Invaders). Note that mean performance is not a very reliable metric because a single game (Video Pinball) has a dominant contribution. Prioritizing replay gives a performance boost on almost all games, and on aggregate, learning is twice as fast (see Figures 4 and 8). The learning curves on Figure 7 illustrate that while the two variants of prioritization usually lead to similar results, there are games where one of them remains close to the Double DQN baseline while the other one leads to a big boost, for example Double Dunk or Surround for the rank-based variant, and Alien, Asterix, Enduro, Phoenix or Space Invaders for the proportional variant. Another observation from the learning curves is that compared to the uniform baseline, prioritization is effective at reducing the delay until performance gets off the ground in games that otherwise suffer from such a delay, such as Battlezone, Zaxxon or Frostbite.  7  050100150200training step (1e6)0%20%40%60%80%100%120%140%normalized max score050100150200training step (1e6)0%20%40%60%80%100%120%140%normalized mean scoreuniformrank-basedproportionaluniform DQNPublished as a conference paper at ICLR 2016  5 DISCUSSION  In the head-to-head comparison between rank-based prioritization and proportional prioritization, we expected the rank-based variant to be more robust because it is not affected by outliers nor error magnitudes. Furthermore, its heavy-tail property also guarantees that samples will be diverse, and the stratiﬁed sampling from partitions of different errors will keep the total minibatch gradient at a stable magnitude throughout training. On the other hand, the ranks make the algorithm blind to the relative error scales, which could incur a performance drop when there is structure in the distribution of errors to be exploited, such as in sparse reward scenarios. Perhaps surprisingly, both variants perform similarly in practice; we suspect this is due to the heavy use of clipping (of rewards and TD-errors) in the DQN algorithm, which removes outliers. Monitoring the distribution of TD- errors as a function of time for a number of games (see Figure 10 in the appendix), and found that it becomes close to a heavy-tailed distribution as learning progresses, while still differing substantially across games; this empirically validates the form of Equation 1. Figure 11, in the appendix, shows how this distribution interacts with Equation 1 to produce the effective replay probabilities. While doing this analysis, we stumbled upon another phenomenon (obvious in retrospect), namely that some fraction of the visited transitions are never replayed before they drop out of the sliding window memory, and many more are replayed for the ﬁrst time only long after they are encountered. Also, uniform sampling is implicitly biased toward out-of-date transitions that were generated by a policy that has typically seen hundreds of thousands of updates since. Prioritized replay with its bonus for unseen transitions directly corrects the ﬁrst of these issues, and also tends to help with the second one, as more recent transitions tend to have larger error – this is because old transitions will have had more opportunities to have them corrected, and because novel data tends to be less well predicted by the value function. We hypothesize that deep neural networks interact with prioritized replay in another interesting way. When we distinguish learning the value given a representation (i.e., the top layers) from learning an improved representation (i.e., the bottom layers), then transitions for which the representation is good will quickly reduce their error and then be replayed much less, increasing the learning focus on others where the representation is poor, thus putting more resources into distinguishing aliased states – if the observations and network capacity allow for it.  6 EXTENSIONS  Prioritized Supervised Learning: The analogous approach to prioritized replay in the context of supervised learning is to sample non-uniformly from the dataset, each sample using a priority based on its last-seen error. This can help focus the learning on those samples that can still be learned from, devoting additional resources to the (hard) boundary cases, somewhat similarly to boosting (Galar et al., 2012). Furthermore, if the dataset is imbalanced, we hypothesize that samples from the rare classes will be sampled disproportionately often, because their errors shrink less fast, and the chosen samples from the common classes will be those nearest to the decision boundaries, leading to an effect similar to hard negative mining (Felzenszwalb et al., 2008). To check whether these intuitions hold, we conducted a preliminary experiment on a class-imbalanced variant of the classical MNIST digit classiﬁcation problem (LeCun et al., 1998), where we removed 99% of the samples for digits 0, 1, 2, 3, 4 in the training set, while leaving the test/validation sets untouched (i.e., those retain class balance). We compare two scenarios: in the informed case, we reweight the errors of the impoverished classes artiﬁcially (by a factor 100), in the uninformed scenario, we provide no hint that the test distribution will differ from the training distribution. See Appendix B.3 for the details of the convolutional neural network training setup. Prioritized sampling (uninformed, with α = 1, β = 0) outperforms the uninformed uniform baseline, and approaches the performance of the informed uniform baseline in terms of generalization (see Figure 5); again, prioritized training is also faster in terms of learning speed. Off-policy Replay: Two standard approaches to off-policy RL are rejection sampling and using importance sampling ratios ρ to correct for how likely a transition would have been on-policy. Our approach contains analogues to both these approaches, the replay probability P and the IS-correction w. It appears therefore natural to apply it to off-policy RL, if transitions are available in a replay memory. In particular, we recover weighted IS with w = ρ, α = 0, β = 1 and rejection sampling  8  Published as a conference paper at ICLR 2016  Figure 5: Classiﬁcation errors as a function of supervised learning updates on severely class- imbalanced MNIST. Prioritized sampling improves performance, leading to comparable errors on the test set, and approaching the imbalance-informed performance (median of 3 random initializa- tions). Left: Number of misclassiﬁed test set samples. Right: Test set loss, highlighting overﬁtting.  with p = min(1; ρ), α = 1, β = 0, in the proportional variant. Our experiments indicate that intermediate variants, possibly with annealing or ranking, could be more useful in practice – espe- cially when IS ratios introduce high variance, i.e., when the policy of interest differs substantially from the behavior policy in some states. Of course, off-policy correction is complementary to our prioritization based on expected learning progress, and the same framework can be used for a hybrid prioritization by deﬁning p = ρ · |δ|, or some other sensible trade-off based on both ρ and δ. Feedback for Exploration: An interesting side-effect of prioritized replay is that the total number Mi that a transition will end up being replayed varies widely, and this gives a rough indication of how useful it was to the agent. This potentially valuable signal can be fed back to the exploration strategy that generates the transitions. For example, we could sample exploration hyperparameters (such as the fraction of random actions (cid:15), the Boltzmann temperature, or the amount of of intrin- sic reward to mix in) from a parametrized distribution at the beginning of each episode, monitor the usefulness of the experience via Mi, and update the distribution toward generating more useful experience. Or, in a parallel system like the Gorila agent (Nair et al., 2015), it could guide re- source allocation between a collection of concurrent but heterogeneous ‘actors’, each with different exploration hyperparameters. Prioritized Memories: Considerations that help determine which transitions to replay are likely to also be relevant for determining which memories to store and when to erase them (e.g. when it becomes likely that they will never be replayed anymore). An explicit control over which memo- ries to keep or erase can help reduce the required total memory size, because it reduces redundancy (frequently visited transitions will have low error, so many of them will be dropped), while auto- matically adjusting for what has been learned already (dropping many of the ‘easy’ transitions) and biasing the contents of the memory to where the errors remain high. This is a non-trivial aspect, because memory requirements for DQN are currently dominated by the size of the replay memory, no longer by the size of the neural network. Erasing is a more ﬁnal decision than reducing the replay probability, thus an even stronger emphasis of diversity may be necessary, for example by tracking the age of each transitions and using it to modulate the priority in such a way as to preserve sufﬁ- cient old experience to prevent cycles (related to ‘hall of fame’ ideas in multi-agent literature, Rosin & Belew, 1997). The priority mechanism is also ﬂexible enough to permit integrating experience from other sources, such as from a planner or from human expert trajectories (Guo et al., 2014), since knowing the source can be used to modulate each transition’s priority, e.g. in such a way as to preserve a sufﬁcient fraction of external experience in memory.  7 CONCLUSION  This paper introduced prioritized replay, a method that can make learning from experience replay more efﬁcient. We studied a couple of variants, devised implementations that scale to large replay memories, and found that prioritized replay speeds up learning by a factor 2 and leads to a new state-of-the-art of performance on the Atari benchmark. We laid out further variants and extensions that hold promise, namely for class-imbalanced supervised learning.  9  Published as a conference paper at ICLR 2016  ACKNOWLEDGMENTS  We thank our Deepmind colleagues, in particular Hado van Hasselt, Joseph Modayil, Nicolas Heess, Marc Bellemare, Razvan Pascanu, Dharshan Kumaran, Daan Wierstra, Arthur Guez, Charles Blun- dell, Alex Pritzel, Alex Graves, Balaji Lakshminarayanan, Ziyu Wang, Nando de Freitas, Remi Munos and Geoff Hinton for insightful discussions and feedback.  REFERENCES Andre, David, Friedman, Nir, and Parr, Ronald. Generalized prioritized sweeping. In Advances in  Neural Information Processing Systems. Citeseer, 1998.  Atherton, Laura A, Dupret, David, and Mellor, Jack R. Memory trace replay: the shaping of memory  consolidation by neuromodulation. Trends in neurosciences, 38(9):560–570, 2015.  Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and Bowling, Michael. The arcade learning envi-  ronment: An evaluation platform for general agents. arXiv preprint arXiv:1207.4708, 2012.  Bellemare, Marc G., Ostrovski, Georg, Guez, Arthur, Thomas, Philip S., and Munos, R´emi. In- creasing the action gap: New operators for reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2016. URL http://arxiv.org/abs/1512.04860. Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement. Torch7: A matlab-like environment  for machine learning. In BigLearn, NIPS Workshop, number EPFL-CONF-192376, 2011. Diamond, Jared. Zebras and the Anna Karenina principle. Natural History, 103:4–4, 1994. Felzenszwalb, Pedro, McAllester, David, and Ramanan, Deva. A discriminatively trained, multi- scale, deformable part model. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pp. 1–8. IEEE, 2008.  Foster, David J and Wilson, Matthew A. Reverse replay of behavioural sequences in hippocampal  place cells during the awake state. Nature, 440(7084):680–683, 2006.  Galar, Mikel, Fernandez, Alberto, Barrenechea, Edurne, Bustince, Humberto, and Herrera, Fran- cisco. A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid- based approaches. Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 42(4):463–484, 2012.  Geramifard, Alborz, Doshi, Finale, Redding, Joshua, Roy, Nicholas, and How, Jonathan. Online dis- covery of feature dependencies. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 881–888, 2011.  Guo, Xiaoxiao, Singh, Satinder, Lee, Honglak, Lewis, Richard L, and Wang, Xiaoshi. Deep Learn- ing for Real-Time Atari Game Play Using Ofﬂine Monte-Carlo Tree Search Planning. In Ghahra- mani, Z., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 27, pp. 3338–3346. Curran Associates, Inc., 2014.  Hinton, Geoffrey E. To recognize shapes, ﬁrst learn to generate images. Progress in brain research,  165:535–547, 2007.  Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. CoRR,  abs/1412.6980, 2014.  Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document ISSN 0018-9219. doi:  recognition. Proceedings of the IEEE, 86(11):2278–2324, Nov 1998. 10.1109/5.726791.  LeCun, Yann, Cortes, Corinna, and Burges, Christopher JC. The MNIST database of handwritten  digits, 1998.  Lin, Long-Ji. Self-improving reactive agents based on reinforcement learning, planning and teach-  ing. Machine learning, 8(3-4):293–321, 1992.  Mahmood, A Rupam, van Hasselt, Hado P, and Sutton, Richard S. Weighted importance sampling for off-policy learning with linear function approximation. In Advances in Neural Information Processing Systems, pp. 3014–3022, 2014.  McNamara, Colin G, Tejero-Cantero,  ´Alvaro, Trouche, St´ephanie, Campo-Urriza, Natalia, and Dupret, David. Dopaminergic neurons promote hippocampal reactivation and spatial memory persistence. Nature neuroscience, 2014.  Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Graves, Alex, Antonoglou, Ioannis, Wier- stra, Daan, and Riedmiller, Martin. Playing atari with deep reinforcement learning. arXiv preprint  10  Published as a conference paper at ICLR 2016  arXiv:1312.5602, 2013.  Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wier- stra, Daan, Legg, Shane, and Hassabis, Demis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.  Moore, Andrew W and Atkeson, Christopher G. Prioritized sweeping: Reinforcement learning with  less data and less time. Machine Learning, 13(1):103–130, 1993.  Nair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alcicek, Cagdas, Fearon, Rory, Maria, Alessan- dro De, Panneershelvam, Vedavyas, Suleyman, Mustafa, Beattie, Charles, Petersen, Stig, Legg, Shane, Mnih, Volodymyr, Kavukcuoglu, Koray, and Silver, David. Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296, 2015.  Narasimhan, Karthik, Kulkarni, Tejas, and Barzilay, Regina. Language understanding for text- based games using deep reinforcement learning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2015.  ´Olafsd´ottir, H Freyja, Barry, Caswell, Saleem, Aman B, Hassabis, Demis, and Spiers, Hugo J. Hippocampal place cells construct reward related sequences through unexplored space. Elife, 4: e06063, 2015.  Riedmiller, Martin. Rprop-description and implementation details. 1994. Rosin, Christopher D and Belew, Richard K. New methods for competitive coevolution. Evolution-  ary Computation, 5(1):1–29, 1997.  Schaul, Tom, Zhang, Sixin, and Lecun, Yann. No more pesky learning rates. In Proceedings of the  30th International Conference on Machine Learning (ICML-13), pp. 343–351, 2013.  Schmidhuber, J¨urgen. Curious model-building control systems. In Neural Networks, 1991. 1991  IEEE International Joint Conference on, pp. 1458–1463. IEEE, 1991.  Singer, Annabelle C and Frank, Loren M. Rewarded outcomes enhance reactivation of experience  in the hippocampus. Neuron, 64(6):910–921, 2009.  Stadie, Bradly C, Levine, Sergey, and Abbeel, Pieter. Incentivizing exploration in reinforcement  learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.  Sun, Yi, Ring, Mark, Schmidhuber, J¨urgen, and Gomez, Faustino J. Incremental basis construction from temporal difference error. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 481–488, 2011.  van Hasselt, Hado. Double Q-learning. In Advances in Neural Information Processing Systems, pp.  2613–2621, 2010.  van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep Reinforcement Learning with Double Q- learning. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016. URL http://arxiv.org/abs/1509.06461.  van Seijen, Harm and Sutton, Richard. Planning by prioritized sweeping with small backups. In  Proceedings of The 30th International Conference on Machine Learning, pp. 361–369, 2013.  Wang, Z., de Freitas, N., and Lanctot, M. Dueling network architectures for deep reinforcement  learning. Technical report, 2015. URL http://arxiv.org/abs/1511.06581.  Watkins, Christopher JCH and Dayan, Peter. Q-learning. Machine learning, 8(3-4):279–292, 1992. White, Adam, Modayil, Joseph, and Sutton, Richard S. Surprise and curiosity for big data robotics.  In Workshops at the Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence, 2014.  11  Published as a conference paper at ICLR 2016  A PRIORITIZATION VARIANTS  The absolute TD-error is only one possible proxy for the ideal priority measure of expected learning progress. While it captures the scale of potential improvement, it ignores inherent stochasticity in rewards or transitions, as well as possible limitations from partial observability or FA capacity; in other words, it is problematic when there are unlearnable transitions. In that case, its derivative – which could be approximated by the difference between a transition’s current |δ| and the |δ| when it was last replayed3 – may be more useful. This measure is less immediately available, however, and is inﬂuenced by whatever was replayed in the meanwhile, which increases its variance. In preliminary experiments, we found it did not outperform |δ|, but this may say more about the class of (near-deterministic) environments we investigated, than about the measure itself. An orthogonal variant is to consider the norm of the weight-change induced by replaying a tran- sition – this can be effective if the underlying optimizer employs adaptive step-sizes that reduce gradients in high-noise directions (Schaul et al., 2013; Kingma & Ba, 2014), thus placing the burden of distinguishing between learnable and unlearnable transitions on the optimizer. It is possible to modulate prioritization by not treating positive TD-errors the same than negative ones; we can for example invoke the Anna Karenina principle (Diamond, 1994), interpreted to mean that there are many ways in which a transition can be less good than expected, but only one in which can be better, to introduce an asymmetry and prioritize positive TD-errors more than negative ones of equal magnitude, because the former are more likely to be informative. Such an asymmetry in replay frequency was also observed in rat studies (Singer & Frank, 2009). Again, our preliminary experiments with such variants were inconclusive. The evidence from neuroscience suggest that a prioritization based on episodic return rather than expected learning progress may be useful too Atherton et al. (2015); ´Olafsd´ottir et al. (2015); Foster & Wilson (2006). For this case, we could boost the replay probabilities of entire episodes, instead of transitions, or boost individual transitions by their observed return-to-go (or even their value estimates). For the issue of preserving sufﬁcient diversity (to prevent overﬁtting, premature convergence or im- poverished representations), there are alternative solutions to our choice of introducing stochasticity, for example, the priorities could be modulated by a novelty measure in observation space. Nothing prevents a hybrid approach where some fraction of the elements (of each minibatch) are sampled according to one priority measure and the rest according to another one, introducing additional di- versity. An orthogonal idea is to increase priorities of transitions that have not been replayed for a while, by introducing an explicit staleness bonus that guarantees that every transition is revisited from time to time, with that chance increasing at the same rate as its last-seen TD-error becomes stale. In the simple case where this bonus grows linearly with time, this can be implemented at no additional cost by subtracting a quantity proportional to the global step-count from the new priority on any update.4 In the particular case of RL with bootstrapping from value functions, it is possible to exploit the sequential structure of the replay memory using the following intuition: a transition that led to a large amount of learning (about its outgoing state) has the potential to change the bootstrapping target for all transitions leading into that state, and thus there is more to be learned about these. Of course we know at least one of these, namely the historic predecessor transition, and so boosting its priority makes it more likely to be revisited soon. Similarly to eligibility traces, this lets information trickle backward from a future outcome to the value estimates of the actions and states that led to it. In practice, we add |δ| of the current transition to predecessor transition’s priority, but only if the predecessor transition is not a terminal one. This idea is related to ‘reverse replay’ observed in rodents Foster & Wilson (2006), and to a recent extension of prioritized sweeping (van Seijen & Sutton, 2013).  3Of course, more robust approximations would be a function of the history of all encountered δ values. In particular, one could imagine an RProp-style update (Riedmiller, 1994) to priorities that increase the priority while the signs match, and reduce it whenever consecutive errors (for the same transition) differ in sign.  4 If bootstrapping is used with policy iteration, such that the target values come from separate network (as is the case for DQN), then there is a large increase in uncertainty about the priorities when the target network is updated in the outer iteration. At these points, the staleness bonus is increased in proportion to the number of individual (low-level) updates that happened in-between.  12  Published as a conference paper at ICLR 2016  B EXPERIMENTAL DETAILS  B.1 BLIND CLIFFWALK  For the Blind Cliffwalk experiments (Section 3.1 and following), we use a straight-forward Q- learning (Watkins & Dayan, 1992) setup. The Q-values are represented using either a tabular look-up table, or a linear function approximator, in both cases represented Q(s, a) := θ(cid:62)φ(s, a). For each transition, we compute its TD-error using:  and update the parameters using stochastic gradient ascent:  δt  := Rt + γt max  θ ← θ + η · δt · ∇θQ(cid:12)(cid:12)St−1,At−1  a  = θ + η · δt · φ(St−1, At−1)  Q(St, a) − Q(St−1, At−1)  (2)  (3)  For the linear FA case we use a very simple encoding of state as a 1-hot vector (as for tabular), but concatenated with a constant bias feature of value 1. To make generalization across actions impossible, we alternate which action is ‘right’ and which one is ‘wrong’ for each state. All elements are initialized with small values near zero, θi ∼ N (0, 0.1). We vary the size of the problem (number of states n) from 2 to 16. The discount factor is set to γ = 1 − 1 n which keeps values on approximately the same scale independently of n. This allows us to used a ﬁxed step-size of η = 1 The replay memory is ﬁlled by exhaustively executing all 2n possible sequences of actions until termination (in random order). This guarantees that exactly one sequence will succeed and hit the ﬁnal reward, and all others will fail with zero reward. The replay memory contains all the relevant experience (the total number of transitions is 2n+1 − 2), at the frequency that it would be encountered when acting online with a random behavior policy. Given this, we can in principle learn until convergence by just increasing the amount of computation; here, convergence is deﬁned as a mean-squared error (MSE) between the Q-value estimates and the ground truth below 10−3.  4 in all experiments.  B.2 ATARI EXPERIMENTS  B.2.1  IMPLEMENTATION DETAILS  Prioritizing using a replay memory with N = 106 transitions introduced some performance chal- lenges. Here we describe a number of things we did to minimize additional run-time and memory overhead, extending the discussion in Section 3.  Rank-based prioritization Early experiments with Atari showed that maintaining a sorted data structure of 106 transitions with constantly changing TD-errors dominated running time. Our ﬁnal solution was to store transitions in a priority queue implemented with an array-based binary heap. The heap array was then directly used as an approximation of a sorted array, which is infrequently sorted once every 106 steps to prevent the heap becoming too unbalanced. This is an unconventional use of a binary heap, however our tests on smaller environments showed learning was unaffected compared to using a perfectly sorted array. This is likely due to the last-seen TD-error only being a proxy for the usefulness of a transition and our use of stochastic prioritized sampling. A small im- provement in running time came from avoiding excessive recalculation of partitions for the sampling distribution. We reused the same partition for values of N that are close together and by updating α and β infrequently. Our ﬁnal implementation for rank-based prioritization produced an additional 2%-4% increase in running time and negligible additional memory usage. This could be reduced further in a number of ways, e.g. with a more efﬁcient heap implementation, but it was good enough for our experiments.  Proportional prioritization The ‘sum-tree’ data structure used here is very similar in spirit to the array representation of a binary heap. However, instead of the usual heap property, the value of a parent node is the sum of its children. Leaf nodes store the transition priorities and the internal nodes are intermediate sums, with the parent node containing the sum over all priorities, ptotal. This provides a efﬁcient way of calculating the cumulative sum of priorities, allowing O(log N ) updates and sampling. To sample a minibatch of size k, the range [0, ptotal] is divided equally into k ranges.  13  Published as a conference paper at ICLR 2016  Next, a value is uniformly sampled from each range. Finally the transitions that correspond to each of these sampled values are retrieved from the tree. Overhead is similar to rank-based prioritization. As mentioned in Section 3.4, whenever importance sampling is used, all weights wi were scaled so that maxi wi = 1. We found that this worked better in practice as it kept all weights within a reasonable range, avoiding the possibility of extremely large updates. It is worth mentioning that this normalization interacts with annealing on β: as β approaches 1, the normalization constant grows, which reduces the effective average update in a similar way to annealing the step-size η.  B.2.2 HYPERPARAMETERS  Throughout this paper our baseline was DQN and the tuned version of Double DQN. We tuned hyperparameters over a subset of Atari games: Breakout, Pong, Ms. Pac-Man, Q*bert, Alien, Bat- tlezone, Asterix. Table 2 lists the values tried and Table 3 lists the chosen parameters.  Hyperparameter Range of values  α β η  0, 0.4, 0.5, 0.6, 0.7, 0.8 0, 0.4, 0.5, 0.6, 1 ηbaseline, ηbaseline/2, ηbaseline/4, ηbaseline/8  Table 2: Hyperparameters considered in experiments. Here ηbaseline = 0.00025.  DQN  Double DQN (tuned)  Hyperparameter baseline rank-based baseline rank-based proportional α (priority) 0.6 0.4 → 1 β (IS) η (step-size) ηbaseline/4  0 0 ηbaseline/4 0.00025  0.7 0.5 → 1 ηbaseline/4  0 0 0.00025  0.5 → 0 0  Table 3: Chosen hyperparameters for prioritized variants of DQN. Arrows indicate linear annealing, where the limiting value is reached at the end of training. Note the rank-based variant with DQN as the baseline is an early version without IS. Here, the bias introduced by prioritized replay was instead corrected by annealing α to zero.  B.2.3 EVALUATION  We evaluate our agents using the human starts evaluation method described in (van Hasselt et al., 2016). Human starts evaluation uses start states sampled randomly from human traces. The test evaluation that agents periodically undergo during training uses start states that are randomized by doing a random number of no-ops at the beginning of each episode. Human starts evaluation averages the score over 100 evaluations of 30 minutes of game time. All learning curve plots show scores under the test evaluation and were generated using the same code base, with the same random seed initializations. Table 4 and Table 5 show evaluation method differences and the (cid:15) used in the (cid:15)-greedy policy for each agent during evaluation. The agent evaluated with the human starts evaluation is the best agent found during training as in (van Hasselt et al., 2016).  Evaluation method Frames Emulator time Number of evaluations Agent start point Human starts Test  100 human starts  30 mins 139 mins  108,000 500,000  1 up to 30 random no-ops  Table 4: Evaluation method comparison.  Normalized score is calculated as in (van Hasselt et al., 2016):  scorenormalized =  scoreagent − scorerandom |scorehuman − scorerandom|  (4)  Note the absolute value of the denominator is taken. This only affects Video Pinball where the random score is higher than the human score. Combined with a high agent score, Video Pinball has  14  Published as a conference paper at ICLR 2016  DQN  Double DQN (tuned)  Evaluation method baseline rank-based baseline rank-based proportional 0.001 Human starts Test 0.01  0.001 0.01  0.001 0.01  0.05 0.05  0.01 0.05  Table 5: The (cid:15) used in the (cid:15)-greedy policy for each agent, for each evaluation method.  a large effect on the mean normalized score. We continue to use this so our normalized scores are comparable.  B.3 CLASS-IMBALANCED MNIST  B.3.1 DATASET SETUP  In our supervised learning setting we modiﬁed MNIST to obtain a new training dataset with a signif- icant label imbalance. This new dataset was obtained by considering a small subset of the samples that correspond to the ﬁrst 5 digits (0, 1, 2, 3, 4) and all of the samples that correspond to the remain- ing 5 labels (5, 6, 7, 8, 9). For each of the ﬁrst 5 digits we randomly sampled 1% of the available examples, i.e., 1% of the available 0s, 1% of the available 1s etc. In the resulting dataset there are examples of all 10 different classes but it is highly imbalanced since there are 100 times more ex- amples that correspond to the 5, 6, 7, 8, 9 classes than to the 0, 1, 2, 3, 4 ones. In all our experiments we used the original MNIST test dataset without removing any samples.  The architecture of the feed-forward network used in the Prioritized Supervised Learning experi- ments.  Figure 6:  B.3.2 TRAINING SETUP  In our experiments we used a 4 layer feed-forward neural network with an architecture similar to that of LeNet5 (Lecun et al., 1998). This is a 2 layer convolutional neural network followed by 2 fully connected layers at the top. Each convolutional layer is comprised of a pure convolution followed by a rectiﬁer non-linearity and a subsampling max pooling operation. The two fully-connected layers in the network are also separated by a rectiﬁer non-linearity. The last layer is a softmax which is used to obtain a normalized distribution over the possible labels. The complete architecture is shown in Figure 6, and is implemented using Torch7 (Collobert et al., 2011). The model was trained using stochastic gradient descent with no momentum and a minibatch of size 60. In all our experiments we considered 6 different step-sizes (0.3, 0.1, 0.03, 0.01, 0.003 and 0.001) and for each case presented in this work, we selected the step-size that lead to the best (balanced) validation performance. We used the negative log-likelihood loss criterion and we ran experiments with both the weighted and unweighted version of the loss. In the weighted case the loss of the examples that correspond to the ﬁrst 5 digits (0, 1, 2, 3, 4) was scaled by a factor of a 100 to accommodate the label imbalance in the training set described above.  15  Published as a conference paper at ICLR 2016  Figure 7: Learning curves (in raw score) for Double DQN (uniform baseline, in black), with rank-based prioritized replay (red), proportional prioritization (blue), for all 57 games of the Atari benchmark suite. Each curve corresponds to a single training run over 200 million unique frames, using test evaluation (see Section B.2.3), with a moving average smoothed over 10 points. Learning curves for the original DQN are in gray. See Figure 8 for a more detailed view on a subset of these.  16  01500300045006000Alien0400800120016002000Amidar01000200030004000Assault0600012000180002400030000Asterix400800120016002000Asteroids0150000300000450000600000Atlantis02505007501000Bank Heist08000160002400032000Battlezone0500010000150002000025000Beam Rider03006009001200Berzerk015304560Bowling10050050100Boxing080160240320400Breakout1000200030004000Centipede01500300045006000Chopper Command0300006000090000120000Crazy Climber0500010000150002000025000Defender015000300004500060000Demon Attack2010010Double Dunk05001000150020002500Enduro8040040Fishing Derby08162432Freeway08001600240032004000Frostbite0500010000150002000025000Gopher0100200300400Gravitar0500010000150002000025000H.E.R.O.241680Ice Hockey08001600240032004000James Bond 0070400080001200016000Kangaroo0200040006000800010000Krull08000160002400032000Kung-Fu Master0.00.20.40.60.81.0Montezuma's Revenge08001600240032004000Ms. Pac-Man030006000900012000Name This Game0500010000150002000025000Phoenix10007505002500Pitfall!301501530Pong6004002000200Private Eye0400080001200016000Q*bert040008000120001600020000River Raid015000300004500060000Road Runner015304560Robotank0600012000180002400030000Seaquest3000025000200001500010000Skiing06001200180024003000Solaris01000200030004000Space Invaders015000300004500060000Stargunner8404Surround2520151050Tennis0200040006000800010000Time Pilot04080120160Tutankham0500010000150002000025000Up'n Down050100150200250Venture080000160000240000320000400000Video Pinball0200040006000800010000Wizard of Wor030006000900012000Yars' Revenge025005000750010000Zaxxonuniformrank-basedproportionaluniform DQNPublished as a conference paper at ICLR 2016  Figure 8: Detailed learning curves for rank-based (red) and proportional (blue) prioritization, as compared to the uniform Double DQN baseline (black) on a selection of games. The solid lines are the median scores, and the shaded area denotes the interquartile range across 8 random initializa- tions. The dashed green lines are human scores. While the variability between runs is substantial, there are signiﬁcant differences in ﬁnal achieved score, and also in learning speed.  Figure 9: Difference in normalized score (the gap between random and human is 100%) on 49 games with human starts, comparing DQN with and without rank-based prioritized replay, showing substantial improvements in many games. Exact scores are in Table 6. See also Figure 3 where Double DQN is the baseline.  17  010002000300040005000600070008000rewardAlien0500010000150002000025000300003500040000rewardBattlezone020406080100120140160180200training step (1e6)050001000015000200002500030000350004000045000rewardAsterix020406080100120140160180200training step (1e6)020004000600080001000012000140001600018000rewardQ*berthumanuniformrank-basedproportional-100%0%100%200%normalizedscoreagentnormalizedscoreDQNRobotankAssaultTime PilotCentipedeAmidarMontezuma's RevengeIce HockeyTennisMs. Pac-ManAsteroidsPrivate EyeFrostbiteGravitarVentureFishing DerbyPongH.E.R.O.AlienFreewayBowlingBattlezoneSpace InvadersSeaquestBeam RiderKung-Fu MasterChopper CommandRiver RaidTutankhamZaxxonName This GameWizard of WorQ*bertEnduroBank HeistAsterixUp'n DownDemon AttackJames Bond 007KangarooCrazy ClimberStargunnerBoxingBreakoutGopherRoad RunnerKrullAtlantisDouble DunkVideo PinballPublished as a conference paper at ICLR 2016  Figure 10: Visualization of the last-seen absolute TD-error for all transitions in the replay memory, sorted, for a selection of Atari games. The lines are color-coded by the time during learning, at a resolution of 106 frames, with the coldest colors in the beginning and the warmest toward the end of training. We observe that in some games it starts quite peaked but quickly becomes spread out, following approximately a heavy-tailed distribution. This phenomenon happens for both rank-based prioritized replay (top) and uniform replay (bottom) but is faster for prioritized replay.  Figure 11: Effective replay probability, as a function of absolute TD-error, for the rank-based prioritized replay variant near the start of training. This shows the effect of Equation 1 with α = 0.7 in practice, compared to the uniform baseline (dashed horizontal line). The effect is irregular, but qualitatively similar for a selection of games.  18  262165200training step0.00.20.40.60.81.0rank-based prioritized replayerrorAlienAsterixBattlezoneQ*bert0.00.20.40.60.81.0transition (1e6)0.00.20.40.60.81.0uniform replayerror0.00.20.40.60.81.0transition (1e6)0.00.20.40.60.81.0transition (1e6)0.00.20.40.60.81.0transition (1e6)0.000.250.500.751.00transition error0.00.51.01.52.02.5sample probabilityAlien0.000.250.500.751.00transition errorAsterix0.000.250.500.751.00transition errorBattlezone0.000.250.500.751.00transition errorQ*bertPublished as a conference paper at ICLR 2016  Figure 12: Effect of importance sampling: These learning curves (as in Figure 8) show how rank-based prioritization is affected by full importance-sampling correction (i.e., β = 1, in orange), as compared to the uniform baseline (black, α = 0) and pure, uncorrected prioritized replay (violet, β = 0), on a few selected games. The shaded area corresponds to the interquartile range. The step-size for full IS correction is the same as for uniform replay. For uncorrected prioritized replay, the step-size is reduced by a factor of 4. Compared to uncorrected prioritized replay, importance sampling makes learning less aggressive, leading on the one hand to slower initial learning, but on the other hand to a smaller risk of premature convergence and sometimes better ultimate results. Compared to uniform replay, fully corrected prioritization is on average better.  19  01000200030004000500060007000rewardAlien0500010000150002000025000300003500040000rewardBattlezone020406080100120140160180200training step (1e6)0200040006000800010000120001400016000rewardAsterix020406080100120140160180200training step (1e6)020004000600080001000012000140001600018000rewardQ*bertuniformrank-based, no ISrank-based, full ISPublished as a conference paper at ICLR 2016  Game Alien Amidar Assault Asterix Asteroids Atlantis Bank Heist Battlezone Beam Rider Berzerk Bowling Boxing Breakout Centipede Chopper Command Crazy Climber Defender Demon Attack Double Dunk Enduro Fishing Derby Freeway Frostbite Gopher Gravitar H.E.R.O. Ice Hockey James Bond 007 Kangaroo Krull Kung-Fu Master Montezuma’s Revenge Ms. Pac-Man Name This Game Phoenix Pitfall! Pong Private Eye Q*bert River Raid Road Runner Robotank Seaquest Skiing Solaris Space Invaders Stargunner Surround Tennis Time Pilot Tutankham Up’n Down Venture Video Pinball Wizard of Wor Yars’ Revenge Zaxxon  DQN  Double DQN (tuned)  2%  17% 6%  22% 29% 179%  7% 8% 685% -1% -0% 478% 25% 48% 57%  baseline rank-based baseline rank-based proportional 12% 14% 1641% 431% 2% 4425% 128% 87% 176% 47% 27% 632% 1407% 18% 72% 522% 155% 2256% 1169% 239% 105% 109% 69% 2792% -1% 78% 85% 1038% 384% 653% 151% -0% 11% 200% 474% 5% 110% -1% 93% 128% 850% 815% 97% 38% 2% 693% 580% 58% 132% 176% 17% 313% 22% 7367% 177% 10% 113%  19% 14% 8% 10% 1381% 631% 1276% 303% 123% 226% 2% 1% 2419% 1480% 2335% 129% 139% 137% 75% 63% 72% 210% 80% 117% 33% 22% 40% 20% 31% 15% 5% 641% 676% 665% 246% 1149% 1823% 1397% 1298% 12% 23% 19% 73% 34% 48% 507% 429% 448% 151% 207% 176% 596% 2152% 1888% 390% 669% 981% 2000% -350% 233% 164% 158% 68% 106% 98% 98% 91% 113% 111% 113% 101% 83% 5% 33% 2% 1679% 836% 728% 120% 4% -2% 1% -1% 80% 55% 56% 47% 93% 58% 71% 58% 311% 161% 1172% 94% 458% 98% 339% 421% 283% 1051% 590% 598% 153% 97% 146% 56% 1% 0% 0% 1% 11% 4% 5% 7% 173% 138% 143% 73% 270% 202% 284% 2% -1% 3% 110% 111% 110% 2% -2% 0% 106% 91% 82% 70% 81% 74% 854% 643% 780% 752% 872% 828% 29% 36% 63% 44% -122% 33% 3% -21% -14% 291% 118% 191% 689% 660% 653% 103% 29% 77% 130% 93% 110% 113% 89% 140% 67% 63% 35% 125% 173% 200% 7% 0% 4042% 7221% 5727% 131% 52% 144% 11% 10% 7% 68% 102% 113%  102% -1% 37% 25% 136% 863% 6%  130% 100% 16% 28% 4% -5% -15%  99% 378%  9%  4%  Table 6: Normalized scores on 57 Atari games (random is 0%, human is 100%), from a single training run each, using human starts evaluation (see Section B.2.3). Baselines are from van Hasselt et al. (2016), see Equation 4 for how normalized scores are calculated.  20  Published as a conference paper at ICLR 2016  Game Alien Amidar Assault Asterix Asteroids Atlantis Bank Heist Battlezone Beam Rider Berzerk Bowling Boxing Breakout Centipede Chopper Command Crazy Climber Defender Demon Attack Double Dunk Enduro Fishing Derby Freeway Frostbite Gopher Gravitar H.E.R.O. Ice Hockey James Bond 007 Kangaroo Krull Kung-Fu Master Montezuma’s Revenge Ms. Pac-Man Name This Game Phoenix Pitfall! Pong Private Eye Q*bert River Raid Road Runner Robotank Seaquest Skiing Solaris Space Invaders Stargunner Surround Tennis Time Pilot Tutankham Up’n Down Venture Video Pinball Wizard of Wor Yars’ Revenge Zaxxon  Double DQN (tuned)  DQN Gorila 813.5 189.2 1195.8 3324.7 933.6  21.7  351.0  644.5  876.6  823.7  176.3  0.5 368.5  1193.2 886.0  -0.3 1216.6 3.2 28.8 1448.1  54.0 74.2 313.0 6296.9 3191.8  -5.3 1265.6 3.5 28.4 288.7  16.0 1831.0 9.8 28.9 3510.0  865.9 52.0 72.3 343.0 3489.1 4635.0  1011.1 69.6 73.5 368.9 3853.5 3495.0  rank-b. 1334.7 129.1 6548.9 1745.1  128.3 6371.3 11.8 1540.4 166.9 164.5 7536.0 871.3 36517.3  random human baseline 570.2 133.4 628.9 3332.3 124.5 697.1  rank-b. baseline prop. 1033.4 1191.0 900.5 218.4 98.9 169.1 7748.5 3081.3 6060.8 9199.5 16837.0 22484.5 31907.5 1677.2 1654.0 13463.0 26575.0 76108.0 629166.5 207526.0 319688.0 330647.0 593642.0 816.8 399.4 3560.0 33030.0 17560.0 19938.0 22250.0 24740.0 25520.0 29100.0 3822.1 12041.9 17417.2 31181.3 26172.7 254.6 14961.0 8672.4 1165.6 196.1 2237.5 644.0 65.8 146.5 35.2 41.2 58.0 68.6 9.6 -1.5 25.8 69.6 481.1 1.6 27.9 371.6 303.9 1925.5 10321.9 3773.1 3421.9 2959.4 6685.0 6604.0 644.0 8930.0 3046.0 9337.0 32667.0 50992.0 65451.0 109337.0 113782.0 127512.0 131086.0 20634.0 27510.0 23666.5 21093.5 1965.5 14296.0 208.3 3442.8 12835.2 14880.1 19478.8 69803.4 61277.5 73185.8 -14.4 2.7 -16.0 -21.6 -11.3 1884.4 740.2 -81.8 475.6 71.0 9.2 5.1 -77.1 -2.3 4.6 0.1 25.6 27.9 25.8 10.2 66.4 4202.8 2930.2 157.4 426.6 4373.0 17478.2 15253.0 34858.8 57783.8 250.0 2311.0 2731.8 538.4 245.5 3116.0 216.5 218.0 8963.4 15150.9 14892.5 20889.9 20506.4 1580.3 25839.4 12952.5 -1.0 -3.8 -3.8 -9.7 -1.7 1074.5 348.5 3511.5 33.5 444.0 9053.0 11204.0 12185.0 10241.0 100.0 2739.0 2696.0 1431.0 6363.1 11209.5 1151.9 2109.1 3864.0 7406.5 304.0 20786.8 11875.0 20620.0 20181.0 30207.0 31676.0 31244.0 13.0 42.0 50.0 25.0 4182.0 44.0 197.8 15375.0 1241.3 763.5 1824.6 964.7 8960.3 10497.6 11836.1 1747.8 6796.0 5439.9 8738.5 16107.8 12366.5 16903.6 27430.1 1134.4 6686.2 -14.8 -348.8 5998.9 -427.0 -186.7 19.1 18.9 16.2 -18.0 15.5 18.9 16.7 2598.6 662.8 64169.1 298.2 179.0 670.7 -575.5 7089.8 12740.5 11020.8 183.0 12085.0 4589.8 9944.0 11277.0 5310.3 10205.5 10838.4 11807.2 18184.4 588.3 14382.2 4065.3 200.0 6878.0 9264.0 43079.8 57207.0 43156.0 52264.0 56990.0 55.4 215.5 40425.8 2793.9 10145.9 11848.8 14498.0 25463.7 39096.7 -29404.3 -11490.4 -10169.1 -10852.8 -15287.4 -3686.6 2047.2 11032.6 2238.2 9063.0 182.6 1464.9 1449.7 697.0 9528.0 34081.0 14919.2 58946.0 58365.0 61582.0 51959.0 -0.9 -0.7 -2.0 -2.3 8267.8 7448.0 3273.0 5650.0 5640.0 118.5 33.6 12.7 32.4 8747.7 16626.5 19086.9 12157.4 29443.7 707.2 9896.1 3311.3 523.4 54.0 244.0 18.0 1039.0 20452.0 15641.1 20228.1 112093.4 214925.3 367823.7 295972.8 374886.9 246.0 10431.0 7451.0 5965.1 9501.0  2272.8 3912.1 5.9 -5.3 5963.0 56.9  804.0 4556.0 1476.9 47135.2 475.0 8443.0  -5.3 -2.3 5391.0 96.5  1.9 -7.8 6608.0 92.2  269.5 -0.2 3961.0  84.0 1263.0 9238.5  2755.0 6626.7 5901.0  -193.7 18.7 2202.3  5727.0 4687.4 9474.0  6201.0 6270.6 8593.0  134.6 1696.9  810.0 2628.7  51.0 1865.9  -2.5 573.0  -9.7 -21.4  5.4 -6.7  1183.3  6159.4  6872.8  6796.1  110.0  831.0  138.3  200.5  61.8  21.0  94.0  58.5  51.3  59.1  56.2  2.4  8.9  Table 7: Raw scores obtained on the original 49 Atari games plus 8 additional games where avail- able, evaluated on human starts. Human, random, DQN and tuned Double DQN scores are from van Hasselt et al. (2016). Note that the Gorila results from Nair et al. (2015) used much more data and computation, but the other methods are more directly comparable to each other in this respect.  21  ",
1509.00519,2016,Importance Weighted Autoencoders,"['Importance Weighted Autoencoders\nYuri Burda', 'Ruslan Salakhutdinov', 'Roger Grosse']",https://arxiv.org/pdf/1509.00519,"6 1 0 2     v o N 7         ]  G L . s c [      4 v 9 1 5 0 0  .  9 0 5 1 : v i X r a  Under review as a conference paper at ICLR 2016  IMPORTANCE WEIGHTED AUTOENCODERS  Yuri Burda, Roger Grosse & Ruslan Salakhutdinov Department of Computer Science University of Toronto Toronto, ON, Canada {yburda,rgrosse,rsalakhu}@cs.toronto.edu  ABSTRACT  The variational autoencoder (VAE; Kingma & Welling (2014)) is a recently pro- posed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior dis- tribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simpliﬁed representations which fail to use the network’s entire modeling capacity. We present the importance weighted autoen- coder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to ap- proximate the posterior, giving it increased ﬂexibility to model complex posteri- ors which do not ﬁt the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.  1  INTRODUCITON  In recent years, there has been a renewed focus on learning deep generative models (Hinton et al., 2006; Salakhutdinov & E., 2009; Gregor et al., 2014; Kingma & Welling, 2014; Rezende et al., 2014). A common difﬁculty faced by most approaches is the need to perform posterior inference during training: the log-likelihood gradients for most latent variable models are deﬁned in terms of posterior statistics (e.g. Salakhutdinov & E. (2009); Neal (1992); Gregor et al. (2014)). One approach for dealing with this problem is to train a recognition network alongside the generative model (Dayan et al., 1995). The recognition network aims to predict the posterior distribution over latent variables given the observations, and can often generate a rough approximation much more quickly than generic inference algorithms such as MCMC. The variational autoencoder (VAE; Kingma & Welling (2014); Rezende et al. (2014)) is a recently proposed generative model which pairs a top-down generative network with a bottom-up recognition network. Both networks are jointly trained to maximize a variational lower bound on the data log- likelihood. VAEs have recently been successful at separating style and content (Kingma et al., 2014; Kulkarni et al., 2015) and at learning to “draw” images in a realistic manner (Gregor et al., 2015). VAEs make strong assumptions about the posterior distribution. Typically VAE models assume that the posterior is approximately factorial, and that its parameters can be predicted from the observables through a nonlinear regression. Because they are trained to maximize a variational lower bound on the log-likelihood, they are encouraged to learn representations where these assumptions are satisﬁed, i.e. where the posterior is approximately factorial and predictable with a neural network. While this effect is beneﬁcial, it comes at a cost: constraining the form of the posterior limits the expressive power of the model. This is especially true of the VAE objective, which harshly penalizes approximate posterior samples which are unlikely to explain the data, even if the recognition network puts much of its probability mass on good explanations. In this paper, we introduce the importance weighted autoencoder (IWAE), a generative model which shares the VAE architecture, but which is trained with a tighter log-likelihood lower bound de-  1  Under review as a conference paper at ICLR 2016  rived from importance weighting. The recognition network generates multiple approximate pos- terior samples, and their weights are averaged. As the number of samples is increased, the lower bound approaches the true log-likelihood. The use of multiple samples gives the IWAE additional ﬂexibility to learn generative models whose posterior distributions do not ﬁt the VAE modeling as- sumptions. This approach is related to reweighted wake sleep (Bornschein & Bengio, 2015), but the IWAE is trained using a single uniﬁed objective. Compared with the VAE, our IWAE is able to learn richer representations with more latent dimensions, which translates into signiﬁcantly higher log-likelihoods on density estimation benchmarks.  2 BACKGROUND  In this section, we review the variational autoencoder (VAE) model of Kingma & Welling (2014). In particular, we describe a generalization of the architecture to multiple stochastic hidden layers. We note, however, that Kingma & Welling (2014) used a single stochastic hidden layer, and there are other sensible generalizations to multiple layers, such as the one presented by Rezende et al. (2014). The VAE deﬁnes a generative process in terms of ancestral sampling through a cascade of hidden layers:  p(x|θ) =  p(hL|θ)p(hL−1|hL, θ)··· p(x|h1, θ).  (cid:88)  h1,...,hL  Here, θ is a vector of parameters of the variational autoencoder, and h = {h1, . . . , hL} denotes the stochastic hidden units, or latent variables. The dependence on θ is often suppressed for clarity. For convenience, we deﬁne h0 = x. Each of the terms p(h(cid:96)|h(cid:96)+1) may denote a complicated nonlinear relationship, for instance one computed by a multilayer neural network. However, it is assumed that sampling and probability evaluation are tractable for each p(h(cid:96)|h(cid:96)+1). Note that L denotes the number of stochastic hidden layers; the deterministic layers are not shown explicitly here. We assume the recognition model q(h|x) is deﬁned in terms of an analogous factorization:  q(h|x) = q(h1|x)q(h2|h1)··· q(hL|hL−1),  (1)  (2)  (cid:20) p(x, h)  (cid:21)  (cid:20)  (cid:21)  where sampling and probability evaluation are tractable for each of the terms in the product. In this work, we assume the same families of conditional probability distributions as Kingma & Welling (2014). In particular, the prior p(hL) is ﬁxed to be a zero-mean, unit-variance Gaussian. In general, each of the conditional distributions p(h(cid:96)| h(cid:96)+1) and q(h(cid:96)|h(cid:96)−1) is a Gaussian with diagonal covariance, where the mean and covariance parameters are computed by a deterministic feed-forward neural network. For real-valued observations, p(x|h1) is also deﬁned to be such a Gaussian; for binary observations, it is deﬁned to be a Bernoulli distribution whose mean parameters are computed by a neural network. The VAE is trained to maximize a variational lower bound on the log-likelihood, as derived from Jensen’s Inequality:  = L(x).  ≥ Eq(h|x)  p(x, h) q(h|x)  log p(x) = log Eq(h|x)  log  q(h|x)  (3) Since L(x) = log p(x) − DKL(q(h|x)||p(h|x)), the training procedure is forced to trade off the data log-likelihood log p(x) and the KL divergence from the true posterior. This is beneﬁcial, in that it encourages the model to learn a representation where posterior inference is easy to approximate. If one computes the log-likelihood gradient for the recognition network directly from Eqn. 3, the re- sult is a REINFORCE-like update rule which trains slowly because it does not use the log-likelihood gradients with respect to latent variables (Dayan et al., 1995; Mnih & Gregor, 2014). Instead, Kingma & Welling (2014) proposed a reparameterization of the recognition distribution in terms of auxiliary variables with ﬁxed distributions, such that the samples from the recognition model are a deterministic function of the inputs and auxiliary variables. While they presented the reparameter- ization trick for a variety of distributions, for convenience we discuss the special case of Gaussians, since that is all we require in this work. (The general reparameterization trick can be used with our IWAE as well.) In this paper, the recognition distribution q(h(cid:96)|h(cid:96)−1, θ) always takes the form of a Gaussian N (h(cid:96)|µ(h(cid:96)−1, θ), Σ(h(cid:96)−1, θ)), whose mean and covariance are computed from the the states of  2  Under review as a conference paper at ICLR 2016  the hidden units at the previous layer and the model parameters. This can be alternatively expressed by ﬁrst sampling an auxiliary variable (cid:15)(cid:96) ∼ N (0, I), and then applying the deterministic mapping (4) The joint recognition distribution q(h|x, θ) over all latent variables can be expressed in terms of a deterministic mapping h((cid:15), x, θ), with (cid:15) = ((cid:15)1, . . . , (cid:15)L), by applying Eqn. 4 for each layer in sequence. Since the distribution of (cid:15) does not depend on θ, we can reformulate the gradient of the bound L(x) from Eqn. 3 by pushing the gradient operator inside the expectation:  h(cid:96)((cid:15)(cid:96), h(cid:96)−1, θ) = Σ(h(cid:96)−1, θ)1/2(cid:15)(cid:96) + µ(h(cid:96)−1, θ).  ∇θ log Eh∼q(h|x,θ)  (cid:21)  (cid:20) p(x, h|θ)  q(h|x, θ)  (cid:20) = ∇θE(cid:15)1,...,(cid:15)L∼N (0,I)  = E(cid:15)1,...,(cid:15)L∼N (0,I)  log  ∇θ log  p(x, h((cid:15), x, θ)|θ) q(h((cid:15), x, θ)|x, θ) p(x, h((cid:15), x, θ)|θ) q(h((cid:15), x, θ)|x, θ)  (5)  (6)  .  (cid:21) (cid:21)  (cid:20)  Assuming the mapping h is represented as a deterministic feed-forward neural network, for a ﬁxed (cid:15), the gradient inside the expectation can be computed using standard backpropagation. In practice, one approximates the expectation in Eqn. 6 by generating k samples of (cid:15) and applying the Monte Carlo estimator  1 k  ∇θ log w (x, h((cid:15)i, x, θ), θ)  (7) with w(x, h, θ) = p(x, h|θ)/q(h|x, θ). This is an unbiased estimate of ∇θL(x). We note that the VAE update and the basic REINFORCE-like update are both unbiased estimators of the same gradient, but the VAE update tends to have lower variance in practice because it makes use of the log-likelihood gradients with respect to the latent variables.  i=1  k(cid:88)  3  IMPORTANCE WEIGHTED AUTOENCODER  The VAE objective of Eqn. 3 heavily penalizes approximate posterior samples which fail to explain the observations. This places a strong constraint on the model, since the variational assumptions must be approximately satisﬁed in order to achieve a good lower bound. In particular, the posterior distribution must be approximately factorial and predictable with a feed-forward neural network. This VAE criterion may be too strict; a recognition network which places only a small fraction (e.g. 20%) of its samples in the region of high posterior probability region may still be sufﬁcient for performing accurate inference. If we lower our standards in this way, this may give us additional ﬂexibility to train a generative network whose posterior distributions do not ﬁt the VAE assump- tions. This is the motivation behind our proposed algorithm, the Importance Weighted Autoencoder (IWAE). Our IWAE uses the same architecture as the VAE, with both a generative network and a recognition network. The difference is that it is trained to maximize a different lower bound on log p(x). In particular, we use the following lower bound, corresponding to the k-sample importance weighting estimate of the log-likelihood:  Lk(x) = Eh1,...,hk∼q(h|x)  log  1 k  p(x, hi) q(hi|x)  .  (8)  Here, h1, . . . , hk are sampled independently from the recognition model. The term inside the sum corresponds to the unnormalized importance weights for the joint distribution, which we will denote as wi = p(x, hi)/q(hi|x). This is a lower bound on the marginal log-likelihood, as follows from Jensen’s Inequality and the fact that the average importance weights are an unbiased estimator of p(x):  (cid:34)  (cid:35)  k(cid:88)  i=1  Lk = E  log  1 k  ≤ log E  wi  (cid:35)  (cid:34)  k(cid:88)  i=1  1 k  wi  = log p(x),  (9)  where the expectations are with respect to q(h|x). It is perhaps unintuitive that importance weighting would be a reasonable estimator in high dimen- sions. Observe, however, that the special case of k = 1 is equivalent to the standard VAE objective shown in Eqn. 3. Using more samples can only improve the tightness of the bound:  3  (cid:34)  k(cid:88)  i=1  (cid:35)  Under review as a conference paper at ICLR 2016  Theorem 1. For all k, the lower bounds satisfy  Moreover, if p(h, x)/q(h|x) is bounded, then Lk approaches log p(x) as k goes to inﬁnity.  log p(x) ≥ Lk+1 ≥ Lk.  (10)  Proof. See Appendix A. The bound Lk can be estimated using the straightforward Monte Carlo estimator, where we generate samples from the recognition network and average the importance weights. One might worry about the variance of this estimator, since importance weighting famously suffers from extremely high variance in cases where the proposal and target distributions are not a good match. However, as our estimator is based on the log of the average importance weights, it does not suffer from high variance. This argument is made more precise in Appendix B.  3.1 TRAINING PROCEDURE  To train an IWAE with a stochastic gradient based optimizer, we use an unbiased estimate of the gradient of Lk, deﬁned in Eqn. 8. As with the VAE, we use the reparameterization trick to derive a low-variance upate rule:  ∇θLk(x) = ∇θEh1,...,hk  log  1 k  (cid:34)  (cid:35)  wi  k(cid:88)  i=1  (cid:34)  log  i=1  1 k  w(x, h(x, (cid:15)i, θ), θ)  k(cid:88) k(cid:88)  (cid:35) (cid:35) (cid:35) (cid:102)wi∇θ log w(x, h(x, (cid:15)i, θ), θ)  w(x, h(x, (cid:15)i, θ), θ)  1 k  i=1  ∇θ log  = ∇θE(cid:15)1,...,(cid:15)k (cid:34) (cid:34) k(cid:88)  = E(cid:15)1,...,(cid:15)k  = E(cid:15)1,...,(cid:15)k  (11)  (12)  ,  (13)  where (cid:15)1, . . . , (cid:15)k are the same auxiliary variables as deﬁned in Section 2 for the VAE, wi =  w(x, h(x, (cid:15)i, θ), θ) are the importance weights expressed as a deterministic function, and (cid:102)wi = wi/(cid:80)k  i=1 wi are the normalized importance weights.  i=1  In the context of a gradient-based learning algorithm, we draw k samples from the recognition network (or, equivalently, k sets of auxiliary variables), and use the Monte Carlo estimate of Eqn. 13:  k(cid:88)  (cid:102)wi∇θ log w (x, h((cid:15)i, x, θ), θ) .  (14)  i=1  In the special case of k = 1, the single normalized weight(cid:102)w1 takes the value 1, and one obtains the  VAE update rule. We unpack this update because it does not quite parallel that of the standard VAE.1 The gradient of the log weights decomposes as:  ∇θ log w(x, h(x, (cid:15)i, θ), θ) = ∇θ log p(x, h(x, (cid:15)i, θ)|θ) − ∇θ log q(h(x, (cid:15)i, θ)|x, θ).  (15)  The ﬁrst term encourages the generative model to assign high probability to each h(cid:96) given h(cid:96)+1 (following the convention that x = h0). It also encourages the recognition network to adjust the hidden representations so that the generative network makes better predictions. In the case of a single stochastic layer (i.e. L = 1), the combination of these two effects is equivalent to backpropagation in a stochastic autoencoder. The second term of this update encourages the recognition network to have a spread-out distribution over predictions. This update is averaged over the samples with weight proportional to the importance weights, motivating the name “importance weighted autoencoder.”  1Kingma & Welling (2014) separated out the KL divergence in the bound of Eqn. 3 in order to achieve a simpler and lower-variance update. Unfortunately, no analogous trick applies for k > 1. In principle, the IWAE updates may be higher variance for this reason. However, in our experiments, we observed that the performance of the two update rules was indistinguishable in the case of k = 1.  4  Under review as a conference paper at ICLR 2016  The dominant computational cost in IWAE training is computing the activations and parameter gra- dients needed for ∇θ log w(x, h(x, (cid:15)i, θ), θ). This corresponds to the forward and backward passes in backpropagation. In the basic IWAE implementation, both passes must be done independently for each of the k samples. Therefore, the number of operations scales linearly with k. In our GPU-based implementation, the samples are processed in parallel by replicating each training example k times within a mini-batch. One can greatly reduce the computational cost by adding another form of stochasticity. Speciﬁcally, only the forward pass is needed to compute the importance weights. The sum in Eqn. 14 can be and then computing ∇θ log w(x, h(x, (cid:15)i, θ), θ). This method requires k forward passes and one backward pass per training example. Since the backward pass requires roughly twice as many add- multiply operations as the forward pass, for large k, this trick reduces the number of add-multiply operations by roughly a factor of 3. This comes at the cost of increased variance in the updates, but empirically we have found the tradeoff to be favorable.  stochastically approximated by choosing a single sample (cid:15)i proprtional to its normalized weight(cid:102)wi  4 RELATED WORK  There are several broad families of approaches to training deep generative models. Some models are deﬁned in terms of Boltzmann distributions (Smolensky, 1986; Salakhutdinov & E., 2009). This has the advantage that many of the conditional distributions are tractable, but the inability to sample from the model or compute the partition function has been a major roadblock (Salakhutdinov & Murray, 2008). Other models are deﬁned in terms of belief networks (Neal, 1992; Gregor et al., 2014). These models are tractable to sample from, but the conditional distributions become tangled due to the explaining away effect. One strategy for dealing with intractable posterior inference is to train a recognition network which approximates the posterior. A classic approach was the wake-sleep algorithm, used to train Helmholtz machines (Dayan et al., 1995). The generative model was trained to model the condi- tionals inferred by the recognition net, and the recognition net was trained to explain synthetic data generated by the generative net. Unfortunately, wake-sleep trained the two networks on different ob- jective functions. Deep autoregressive networks (Gregor et al., 2014) consisted of deep generative and recognition networks trained using a single variational lower bound. Neural variational infer- ence and learning (Mnih & Gregor, 2014) is another algorithm for training recognition networks which reduces stochasticity in the updates by training a third network to predict reward baselines in the context of the REINFORCE algorithm (Williams, 1992). Salakhutdinov & Larochelle (2010) used a recognition network to approximate the posterior distribution in deep Boltzmann machines. Variational autoencoders (Kingma & Welling, 2014; Rezende et al., 2014), as described in detail in Section 2, are another combination of generative and recognition networks, trained with the same variational objective as DARN and NVIL. However, in place of REINFORCE, they reduce the variance of the updates through a clever reparameterization of the random choices. The reparame- terization trick is also known as “backprop through a random number generator” (Williams, 1992). One factor distinguishing VAEs from the other models described above is that the model is described in terms of a simple distribution followed by a deterministic mapping, rather than a sequence of stochastic choices. Similar architectures have been proposed which use different training objectives. Generative adversarial networks (Goodfellow et al., 2014) train a generative network and a recog- nition network which act in opposition: the recognition network attempts to distinguish between training examples and generated samples, and the generative model tries to generate samples which fool the recognition network. Maximum mean discrepancy (MMD) networks (Li et al., 2015; Dziu- gaite et al., 2015) attempt to generate samples which match a certain set of statistics of the training data. They can be viewed as a kind of adversarial net where the adversary simply looks at the set of pre-chosen statistics (Dziugaite et al., 2015). In contrast to VAEs, the training criteria for adversarial nets and MMD nets are not based on the data log-likelihood. Other researchers have derived log-probability lower bounds by way of importance sampling. Tang & Salakhutdinov (2013) and Ba et al. (2015) avoided recognition networks entirely, instead perform- ing inference using importance sampling from the prior. Gogate et al. (2007) presented a variety of graphical model inference algorithms based on importance weighting. Reweighted wake-sleep  5  Under review as a conference paper at ICLR 2016  (RWS) of Bornschein & Bengio (2015) is another recognition network approach which combines the original wake-sleep algorithm with updates to the generative network equivalent to gradient as- cent on our bound Lk. However, Bornschein & Bengio (2015) interpret this update as following a biased estimate of ∇θ log p(x), whereas we interpret it as following an unbiased estimate of ∇θLk. The IWAE also differs from RWS in that the generative and recognition networks are trained to maximize a single objective, Lk. By contrast, the q-wake and sleep steps of RWS do not appear to be related to Lk. Finally, the IWAE differs from RWS in that it makes use of the reparameterization trick. Apart from our approach of using multiple approximate posterior samples, another way to improve the ﬂexibility of posterior inference is to use a more sophisticated algorithm than importance sam- pling. Examples of this approach include normalizing ﬂows (Rezende & Mohamed, 2015) and the Hamiltonian variational approximation of Salimans et al. (2015). After the publication of this paper the authors learned that the idea of using an importance weighted lower bound for training variational autoencoders has been independently explored by Laurent Dinh and Vincent Dumoulin, and preliminary results of their work were presented at the 2014 CIFAR NCAP Deep Learning summer school.  5 EXPERIMENTAL RESULTS  We have compared the generative performance of the VAE and IWAE in terms of their held-out log- likelihoods on two density estimation benchmark datasets. We have further investigated a particular issue we have observed with VAEs and IWAEs, namely that they learn latent spaces of signiﬁcantly lower dimensionality than the modeling capacity they are allowed. We tested whether the IWAE training method ameliorates this effect.  5.1 EVALUATION ON DENSITY ESTIMATION  We evaluated the models on two benchmark datasets: MNIST, a dataset of images of handwritten digits (LeCun et al., 1998), and Omniglot, a dataset of handwritten characters in a variety of world alphabets (Lake et al., 2013). In both cases, the observations were binarized 28 × 28 images.2 We used the standard splits of MNIST into 60,000 training and 10,000 test examples, and of Omniglot into 24,345 training and 8,070 test examples. We trained models with two architectures:  1. An architecture with a single stochastic layer h1 with 50 units. In between the observations  and the stochastic layer were two deterministic layers, each with 200 units.  2. An architecture with two stochastic layers h1 and h2, with 100 and 50 units, respectively. In between x and h1 were two deterministic layers with 200 units each. In between h1 and h2 were two deterministic layers with 100 units each.  All deterministic hidden units used the tanh nonlinearity. All stochastic layers used Gaussian dis- tributions with diagonal covariance, with the exception of the visible layer, which used Bernoulli distributions. An exp nonlinearity was applied to the predicted variances of the Gaussian distribu- tions. The network architectures are summarized in Appendix C. All models were initialized with the heuristic of Glorot & Bengio (2010). For optimization, we used Adam (Kingma & Ba, 2015) with parameters β1 = 0.9, β2 = 0.999, (cid:15) = 10−4 and minibaches of size 20. The training proceeded for 3i passes over the data with learning rate of 0.001 · 10−i/7 for i=0 3i = 3280 passes over the data). This learning rate schedule was  i = 0 . . . 7 (for a total of(cid:80)7  chosen based on preliminary experiments training a VAE with one stochastic layer on MNIST.  2Unfortunately, the generative modeling literature is inconsistent about the method of binarization, and different choices can lead to considerably different log-likelihood values. We follow the procedure of Salakhut- dinov & Murray (2008): the binary-valued observations are sampled with expectations equal to the real values in the training set. See Appendix D for an alternative binarization scheme.  6  Under review as a conference paper at ICLR 2016  MNIST  OMNIGLOT  VAE  IWAE  VAE  IWAE  # stoch. layers  1  2  k  1 5 50 1 5 50  NLL  86.76 86.47 86.35 85.33 85.01 84.78  active units  19 20 20 16+5 17+5 17+5  NLL  86.76 85.54 84.78 85.33 83.89 82.90  active units  19 22 25 16+5 21+5 26+7  NLL  108.11 107.62 107.80 107.58 106.31 106.30  active units  28 28 28 28+4 30+5 30+5  NLL  108.11 106.12 104.67 107.56 104.79 103.38  active units  28 34 41 30+5 38+6 44+7  Table 1: Results on density estimation and the number of active latent dimensions. For models with two latent layers, “k1+k2” denotes k1 active units in the ﬁrst layer and k2 in the second layer. The generative performance of IWAEs improved with increasing k, while that of VAEs beneﬁtted only slightly. Two-layer models achieved better generative performance than one-layer models.  For each number of samples k ∈ {1, 5, 50} we trained a VAE with the gradient of L(x) estimted as in Eqn. 7 and an IWAE with the gradient estimated as in Eqn. 14. For each k, the VAE and the IWAE were trained for approximately the same length of time. All log-likelihood values were estimated as the mean of L5000 on the test set. Hence, the reported values are stochastic lower bounds on the true value, but are likely to be more accurate than the lower bounds used for training. The log-likelihood results are reported in Table 1. Our VAE results are comparable to those previ- ously reported in the literature. We observe that training a VAE with k > 1 helped only slightly. By contrast, using multiple samples improved the IWAE results considerably on both datasets. Note that the two algorithms are identical for k = 1, so the results ought to match up to random variability. On MNIST, IWAE with two stochastic layers and k = 50 achieves a log-likelihood of -82.90 on the permutation-invariant model on this dataset. By comparison, deep belief networks achieved log- likelihood of approximately -84.55 nats (Murray & Salakhutdinov, 2009), and deep autoregressive networks achieved log-likelihood of -84.13 nats (Gregor et al., 2014). Gregor et al. (2015), who exploited spatial structure, achieved a log-likelihood of -80.97. We did not ﬁnd overﬁtting to be a serious issue for either the VAE or the IWAE: in both cases, the training log-likelihood was 0.62 to 0.79 nats higher than the test log-likelihood. We present samples from our models in Appendix E. For the OMNIGLOT dataset, the best performing IWAE has log-likelihood of -103.38 nats, which is slightly worse than the log-likelihood of -100.46 nats achieved by a Restricted Boltzmann Machine with 500 hidden units trained with persistent contrastive divergence (Burda et al., 2015). RBMs trained with centering or FANG methods achieve a similar performance of around -100 nats (Grosse & Salakhudinov, 2015). The training log-likelihood for the models we trained was 2.39 to 2.65 nats higher than the test log-likelihood.  5.2 LATENT SPACE REPRESENTATION  We have observed that both VAEs and IWAEs tend to learn latent representations with effective dimensions far below their capacity. Our next set of experiments aimed to quantify this effect and determine whether the IWAE objective ameliorates this effect. If a latent dimension encodes useful information about the data, we would expect its distribution to change depending on the observations. Based on this intuition, we measured activity of a latent dimension u using the statistic Au = Covx if Au > 10−2. We have observed two pieces of evidence that this criterion is both well-deﬁned and meaningful:  (cid:0)Eu∼q(u|x)[u](cid:1). We deﬁned the dimension u to be active  1. The distribution of Au for a trained model consisted of two widely separated modes, as  shown in Appendix C.  7  Under review as a conference paper at ICLR 2016  Experiment 1 Experiment 2  trained as  VAE  IWAE, k = 50  First stage NLL  active units  trained as  NLL  active units  Second stage  86.76 84.78  19 25  IWAE, k = 50  VAE  84.88 86.02  22 23  Table 2: Results of continuing to train a VAE model with the IWAE objective, and vice versa. Training the VAE with the IWAE objective increased the latent dimension and test log-likelihood, while training the IWAE with the VAE objective had the opposite effect.  2. To conﬁrm that the inactive dimensions were indeed insigniﬁcant to the predictions, we evaluated all models with the inactive dimensions removed. In all cases, this changed the test log-likelihood by less than 0.06 nats.  In Table 1, we report the numbers of active units for all conditions. In all conditions, the number of active dimensions was far less than the total number of dimensions. Adding more latent dimensions did not increase the number of active dimensions. Interestingly, in the two-layer models, the second layer used very little of its modeling capacity: the number of active dimensions was always less than 10. In all cases with k > 1, the IWAE learned more latent dimensions than the VAE. Since this coincided with higher log-likelihood values, we speculate that a larger number of active dimensions reﬂects a richer latent representation. Superﬁcially, the phenomenon of inactive dimensions appears similar to the problem of “units dying out” in neural networks and latent variable models, an effect which is often ascribed to difﬁculties in optimization. For example, if a unit is inactive, it may never receive a meaningful gradient signal because of a plateau in the optimization landscape. In such cases, the problem may be avoided through a better initialization. To determine whether the inactive units resulted from an optimization issue or a modeling issue, we took the best-performing VAE and IWAE models from Table 1, and continued training the VAE model using the IWAE objective and vice versa. In both cases, the model was trained for an additional 37 passes over the data with a learning rate of 10−4. The results are shown in Table 2. We found that continuing to train the VAE with the IWAE objective increased the number of active dimensions and the test log-likelihood, while continuing to train the IWAE with the VAE objective did the opposite. The fact that training with the VAE objective actively reduces both the number of active dimensions and the log-likelihood strongly suggests that inactivation of the latent dimensions is driven by the objective functions rather than by optimization issues. On the other hand, optimization also appears to play a role, as the results in Table 2 are not quite identical to those in Table 1.  6 CONCLUSION  In this paper, we presented the importance weighted autoencoder, a variant on the VAE trained by maximizing a tighter log-likelihood lower bound derived from importance weighting. We showed empirically that IWAEs learn richer latent representations and achieve better generative performance than VAEs with equivalent architectures and training time. We believe this method may improve the ﬂexibility of other generative models currently trained with the VAE objective.  7 ACKNOWLEDGEMENTS  This research was supported by NSERC, the Fields Institute, and Samsung.  REFERENCES Ba, J. L., Mnih, V., and Kavukcuoglu, K. Multiple object recognition with visual attention. In International  Conference on Learning Representations, 2015.  Bornschein, J. and Bengio, Y. Reweighted wake-sleep. International Conference on Learning Representations,  2015.  Burda, Y., Grosse, R. B., and Salakhutdinov, R. Accurate and conservative estimates of MRF log-likelihood  using reverse annealing. Artiﬁcial Intelligence and Statistics, pp. 102–110, 2015.  8  Under review as a conference paper at ICLR 2016  Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. The Helmholtz machine. Neural Computation, 7:  889–904, 1995.  Dziugaite, K. G., Roy, D. M., and Ghahramani, Z. Training generative neural networks via maximum mean  discrepancy optimization. In Uncertainty in Artiﬁcial Intelligence, 2015.  Glorot, X. and Bengio, Y. Understanding the difﬁculty of training deep feedforward neural networks.  Artiﬁcial Intelligence and Statistics, pp. 249–256, 2010.  In  Gogate, V., Bidyuk, B., and Dechter, R. Studies in lower bounding probability of evidence using the Markov  inequality. In Uncertainty in Artiﬁcial Intelligence, 2007.  Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio,  Y. Generative adversarial nets. In Neural Information Processing Systems, 2014.  Gregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. Deep autoregressive networks. International  Conference on Machine Learning, 2014.  Gregor, K., Danihelka, I., Graves, A., Rezende, D. J., and Wierstra, D. DRAW: A recurrent neural network for  image generation. In International Conference on Machine Learning, pp. 1462–1471, 2015.  Grosse, R. and Salakhudinov, R. Scaling up natural gradient by sparsely factorizing the inverse ﬁsher matrix.  In International Conference on Machine Learning, 2015.  Hinton, G. E., Osindero, S., and Teh, Y. A fast learning algorithm for deep belief nets. Neural Computation,  2006.  Kingma, D. and Ba, J. L. Adam: A method for stochastic optimization.  Learning Representations, 2015.  In International Conference on  Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes.  Representations, 2014.  International Conference on Learning  Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. Semi-supervised learning with deep generative  models. In Neural Information Processing Systems, 2014.  Kulkarni, T. D., Whitney, W., Kohli, P., and Tenenbaum, J. B. Deep convolutional inverse graphics network.  arXiv:1503.03167, 2015.  Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. One-shot learning by inverting a compositional causal  process. In Neural Information Processing Systems, 2013.  Larochelle, H., Murray I. The neural autoregressive distribution estimator. In Artiﬁcial Intelligence and Statis-  tics, pp. 29–37, 2011.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, 1998.  Li, Y., Swersky, K., and Zemel, R. Generative moment matching networks. In International Conference on  Machine Learning, pp. 1718–1727, 2015.  Mnih, A. and Gregor, K. Neural variational inference and learning in belief networks. In International Confer-  ence on Machine Learning, pp. 1791–1799, 2014.  Murray, I. and Salakhutdinov, R. Evaluating probabilities under high-dimensional latent variable models. In  Neural Information Processing Systems, pp. 1137–1144, 2009.  Neal, R. M. Connectionist learning of belief networks. Artiﬁcial Intelligence, 1992.  Rezende, D. J. and Mohamed, S. Variational inference with normalizing ﬂows. In International Conference on  Machine Learning, pp. 1530–1538, 2015.  Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep  generative models. International Conference on Machine Learning, pp. 1278–1286, 2014.  Salakhutdinov, R. and E., Hinton G. Deep Boltzmann machines. In Neural Information Processing Systems,  2009.  Salakhutdinov, R. and Larochelle, H. Efﬁcient learning of deep Boltzmann machines. In Artiﬁcial Intelligence  and Statistics, 2010.  9  Under review as a conference paper at ICLR 2016  Salakhutdinov, R. and Murray, I. On the quantitative analysis of deep belief networks. In International Con-  ference on Machine Learning, 2008.  Salimans, T., Kingma, D. P., and Welling, M. Markov chain Monte Carlo and variational inference: bridging  the gap. In International Conference on Machine Learning, pp. 1218–1226, 2015.  Smolensky, P. Information processing in dynamical systems: foundations of harmony theory. In Rumelhart, D. E. and McClelland, J. L. (eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition. MIT Press, 1986.  Tang, Y. and Salakhutdinov, R. Learning stochastic feedforward neural networks.  Processing Systems, 2013.  In Neural Information  Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Ma-  chine Learning, 8:229–256, 1992.  APPENDIX A Proof of Theorem 1. We need to show the following facts about the log-likelihood lower bound Lk:  1. log p(x) ≥ Lk, 2. Lk ≥ Lm for k ≥ m, 3. log p(x) = limk→∞ Lk, assuming p(h, x)/q(h|x) is bounded.  We prove each in turn:  1. It follows from Jensen’s inequality that  (cid:34)  k(cid:88)  i=1  Lk = E  log  1 k  p(x, hi) q(hi|x)  (cid:35)  ≤ log E  (cid:34)  k(cid:88)  i=1  1 k  (cid:35)  p(x, hi) q(hi|x)  2. Let I ⊂ {1, . . . , k} with |I| = m be a uniformly distributed subset of distinct indices from =  {1, . . . , k}. We will use the following simple observation: EI={i1,...,im}  m  a1+...+ak  k  for any sequence of numbers a1, . . . , ak.  Using this observation and Jensen’s inequality, we get  = log p(x)  (16)  (cid:104) ai1 +...+aim  (cid:105)  Lk = Eh1,...,hk  = Eh1,...,hk  ≥ Eh1,...,hk  = Eh1,...,hm  i=1  1 k  log  k(cid:88)  p(x, hi) q(hi|x)  (cid:35) (cid:34) log EI={i1,...,im}  1 log EI={i1,...,im} (cid:35) (cid:34) m(cid:88) (cid:80)k  p(x, hi) q(hi|x)  1 m  p(x,hi)  1 m  log  i=1  m     m(cid:88) m(cid:88)  j=1  j=1  p(x, hij ) q(hij|x)  p(x, hij ) q(hij|x)  = Lm  (17)  (18)  (19)  (20)  3. Consider the random variable Mk = 1 k  i=1  it follows from the strong law of large numbers that Mk converges to Eq(hi|x) p(x) almost surely. Hence Lk = E log[Mk] converges to log p(x) as k → ∞.  (cid:105) (cid:104) p(x,hi) q(hi|x) . If p(h, x)/q(h|x) is bounded, then  q(hi|x)  =  10  Under review as a conference paper at ICLR 2016  APPENDIX B  It is well known that the variance of an unnormalized importance sampling based estimator can be extremely large, or even inﬁnite, if the proposal distribution is not well matched to the target distribution. Here we argue that the Monte Carlo estimator of Lk, described in Section 3, does not suffer from large variance. More precisely, we bound the mean absolute deviation (MAD). While this does not directly bound the variance, it would be surprising if an estimator had small MAD yet extremely large variance. Suppose we have a strictly positive unbiased estimator ˆZ of a positive quantity Z, and we wish to use log ˆZ as an estimator of log Z. By Jensen’s inequality, this is a biased estimator, i.e. E[log ˆZ] ≤ log Z. Denote the bias as δ = log Z − E[log ˆZ]. We start with the observation that log ˆZ is unlikely to overestimate log Z by very much, as can be shown with Markov’s Inequality:  Pr(log ˆZ > log Z + b) ≤ e−b.  Let (X)+ denote max(X, 0). We now use the above facts to bound the MAD:  E(cid:104)(cid:12)(cid:12)(cid:12)log ˆZ − E[log ˆZ]  (cid:12)(cid:12)(cid:12)(cid:105)  = 2E  log ˆZ − E[log ˆZ]  (cid:20)(cid:16) (cid:20)(cid:16) (cid:20)(cid:16) (cid:20)(cid:16) (cid:90) ∞ (cid:90) ∞  0  (cid:17)  +  (cid:21)  (cid:16)  (cid:17) (cid:17)  +  (cid:21)  +  +  + 2δ  (cid:17)  = 2E  log ˆZ − log Z + log Z − E[log ˆZ]  ≤ 2E  log ˆZ − log Z  log Z − E[log ˆZ]  = 2E  log ˆZ − log Z  (cid:16)  = 2  ≤ 2  e−tdt + 2δ  log ˆZ − log Z > t  Pr  dt + 2δ  (cid:21)  (cid:17)  +  (cid:17)  (cid:21)  +  (21)  (22)  (23)  (24)  (25)  (26)  (27)  Here, (22) is a general formula for the MAD, (26) uses the formula E[Y ] =(cid:82) ∞  (28) 0 Pr(Y > t) dt for a nonnegative random variable Y , and (27) applies the bound (21). Hence, the MAD is bounded by 2 + 2δ. In the context of IWAE, δ corresponds to the gap between Lk and log p(x).  = 2 + 2δ  0  APPENDIX C  NETWORK ARCHITECTURES  Here is a summary of the network architectures used in the experiments: q(h1|x) = N (h1|µq,1, diag(σq,1))  lin+tanh  lin+tanh  200d  x  200d  lin  lin+exp  q(h2|h1) = N (h2|µq,2, diag(σq,2))  p(h1|h2) = N (h1|µp,1, diag(σp,1))  h1  h2  lin+tanh  100d  lin+tanh  100d  lin  lin+exp  lin+tanh  100d  lin+tanh  100d  lin  lin+exp  µq,1 σq,1  µq,2 σq,2  µp,1 σp,1  p(x|h1) = Bernoulli(x|µp,0)  lin+tanh  h1  200d  lin+tanh  200d  lin+sigm  µp,0  11  Under review as a conference paper at ICLR 2016  DISTRIBUTION OF ACTIVITY STATISTIC  In Section 5.2, we deﬁned the activity statistic Au = Covx of 10−2 for determining if a unit is active. One justiﬁcation for this is that the distribution of this statistic consisted of two widely separated modes in every case we looked at. Here is the histogram of log Au for a VAE with one stochastic layer:  (cid:0)Eu∼q(u|x)[u](cid:1), and chose a threshold  VISUALIZATION OF POSTERIOR DISTRIBUTIONS  We show some examples of true and approximate posteriors for VAE and IWAE models trained with two latent dimensions. Heat maps show true posterior distributions for 6 training examples, and the pictures in the bottom row show the examples and their reconstruction from samples from q(h|x). Left: VAE. Middle: IWAE, with k = 5. Right: IWAE, with k = 50. The IWAE prefers less regular posteriors and more spread out posterior predictions.  12  8765432101log variance of µ024681012number of unitsUnder review as a conference paper at ICLR 2016  APPENDIX D  RESULTS FOR A FIXED MNIST BINARIZATION  Several previous works have used a ﬁxed binarization of the MNIST dataset deﬁned by Larochelle (2011). We repeated our experiments training the models on the 50000 examples from the training dataset, and evaluating them on the 10000 examples from the test dataset. Otherwise we used the same training procedure and hyperparameters as in the experiments in the main part of the paper. The results in table 3 indicate that the conclusions about the relative merits of VAEs and IWAEs are unchanged in the new experimental setup. In this setup we noticed signiﬁcantly larger amounts of overﬁtting.  VAE  IWAE  # stoch. layers  1  2  k  1 5 50 1 5 50  NLL  88.71 88.83 89.05 88.08 87.63 87.86  active units  19 19 20 16+5 17+5 17+6  NLL  88.71 87.63 87.10 88.08 86.17 85.32  active units  19 22 24 16+5 21+5 24+7  Table 3: Results on density estimation and the number of active latent dimensions on the ﬁxed binarization MNIST dataset. For models with two latent layers, “k1 + k2” denotes k1 active units in the ﬁrst layer and k2 in the second layer. The generative performance of IWAEs improved with increasing k, while that of VAEs beneﬁtted only slightly. Two-layer models achieved better generative performance than one-layer models.  APPENDIX E  SAMPLES  13  Under review as a conference paper at ICLR 2016  Table 4: Random samples from VAE (left column) and IWAE with k = 50 (right column) models. Row 1: models with one stochastic layer. Row 2: models with two stochastic layers. Samples are represented as the means of the corresponding Bernoulli distributions.  14  ",
1511.06455,2016,Variationally Auto-Encoded Deep Gaussian Processes,"['Variationally Auto-Encoded Deep Gaussian Processes\nZhenwen Dai', 'Andreas Damianou', 'Javier Gonzalez', 'Neil Lawrence']",https://arxiv.org/pdf/1511.06455,"6 1 0 2     b e F 9 2         ]  G L . s c [      2 v 5 5 4 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  VARIATIONAL AUTO-ENCODED DEEP GAUSSIAN PRO- CESSES  Zhenwen Dai, Andreas Damianou, Javier Gonz´alez & Neil Lawrence Department of Computer Science, University of Shefﬁeld, UK {z.dai, andreas.damianou, j.h.gonzalez, n.lawrence}@sheffield.ac.uk  ABSTRACT  We develop a scalable deep non-parametric generative model by augmenting deep Gaussian processes with a recognition model. Inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron. The key aspect of this reformula- tion is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size. We derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks. We show the efﬁcacy of the method on a variety of challenges including deep unsupervised learning and deep Bayesian optimization.  1  INTRODUCTION  Probabilistic directed generative models are ﬂexible tools that have recently captured the attention of the the Deep Learning community (Uria et al., 2014; Mnih & Gregor, 2014; Kingma & Welling, 2013; Rezende et al., 2014). These models have the ability to produce samples able to mimic the learned data and they allow principled assessment of the uncertainty in the predictions. These prop- erties are crucial to successfully addressing challenges such as uncertainty quantiﬁcation or data imputation and allow the ideas of deep learning to be extended to related machine learning ﬁelds such as probabilistic numerics. The main challenge is that exact inference on directed nonlinear probabilistic models is typically in- tractable due to the required marginalisation of the latent components. This has lead to the develop- ment of probabilistic generative models based on neural networks (Kingma & Welling, 2013; Mnih & Gregor, 2014; Rezende et al., 2014), in which probabilistic distributions are deﬁned for the input and output of individual layers. Efﬁcient approximated inference methods have been developed in this context based on stochastic variational inference or stochastic back-propagation. However, a question that remains open is how to properly regularize the model parameters. Techniques such as dropout have been used to avoid over-ﬁtting (Hinton et al., 2012). Alternatively, Bayesian inference offers a mathematically grounded framework for regularization. Blundell et al. (2015) show that Bayesian (variational) inference outperforms dropout. Kingma et al. (2015); Gal & Ghahramani (2015) have shown that dropout itself can be reformulated in the variational inference context. In this work, we develop a new scalable Bayesian non-parametric generative model. We focus on a deep Gaussian processes (DGP) that we augment by means of a recognition model, a multi-layer perceptron (MLP) between the latent representation of layers of the DGP. This allows us to simplify the inference and to avoid the challenge of initializing variational parameters. In addition, although DGP have been used only in small scale data so far, we show how it is possible to scale these models by means of new formulation of the lower bound that allows to distribute most of the computation. The main contributions of this work are: i) a novel extension to DGPs by means of a recognition model that we call Variational Auto-Encoded deep Gaussian process (VAE-DGP), ii) a derivation of the distributed variational lower bound of the model and iii) a demonstration of the utility of the model on several mainstream deep learning datasets.  1  Published as a conference paper at ICLR 2016  f1 ∼ GP  f2 ∼ GP  f3 ∼ GP  Y  X1  X2  X3  Figure 1: A deep Gaussian process with two hidden layers.  Figure 2: Samples from a deep GP showing the generation of features. The upper plot shows the successive non-linear warping of a two-dimensional input space. The red circles correspond to speciﬁc locations in the input space for which a feature (a “loop”) is created in layer 1. As can be seen, as we traverse the hierarchy towards the right, this feature is maintained in the next layers and is potentially further transformed.  2 DEEP GAUSSIAN PROCESSES  Gaussian processes provide ﬂexible, non-parametric, probabilistic approaches to function estima- tion. However, their tractability comes at a price: they can only represent a restricted class of functions. Indeed, even though sophisticated deﬁnitions and combinations of covariance functions can lead to powerful models (Durrande et al., 2011; G¨onen & Alpaydin, 2011; Hensman et al., 2013; Duvenaud et al., 2013; Wilson & Adams, 2013), the assumption about joint normal distribu- tion of instantiations of the latent function remains; this limits the applicability of the models. One line of recent research to address this limitation focused on function composition (Snelson et al., 2004; Calandra et al., 2014). Inspired by deep neural networks, a deep Gaussian process instead employs process composition (Lawrence & Moore, 2007; Damianou et al., 2011; L´azaro-Gredilla, 2012; Damianou & Lawrence, 2013; Hensman & Lawrence, 2014). A deep GP is a deep directed graphical model that consists of multiple layers of latent variables and employs Gaussian processes to govern the mapping between consecutive layers (Lawrence & Moore, 2007; Damianou, 2015). Observed outputs are placed in the down-most layer and observed inputs (if any) are placed in the upper-most layer, as illustrated in Figure 1. More formally, consider a set of data Y ∈ RN×D with N datapoints and D dimensions. A deep GP then deﬁnes L layers of latent variables, {Xl}L  l=1, Xl ∈ RN×Ql through the following nested noise model deﬁnition:  (cid:15)1 ∼ N (0, σ2 1I) (cid:15)l ∼ N (0, σ2 l I),  l = 2 . . . L  Y = f1(X1) + (cid:15)1, Xl−1 = fl(Xl) + (cid:15)l,  (1) (2) where the functions fl are drawn from Gaussian processes with covariance functions kl, i.e. fl(x) ∼ GP(0, kl(x, x(cid:48))). In the unsupervised case, the top hidden layer is assigned a unit Gaussian as a fairly uninformative prior which also provides soft regularization, i.e. XL ∼ N (0, I). In the supervised learning scenario, the inputs of the top hidden layer is observed and govern its hidden outputs. The expressive power of a deep GP is signiﬁcantly greater than that of a standard GP, because the successive warping of latent variables through the hierarchy allows for modeling non-stationarities and sophisticated, non-parametric functional “features” (see Figure 2). Similarly to how a GP is the limit of an inﬁnitely wide neural network, a deep GP is the limit where the parametric function composition of a deep neural network turns into a process composition. Speciﬁcally, a deep neural network can be written as:  g(x) = V(cid:62)  (3) where W, U and V are parameter matrices and φ(·) denotes an activation function. By non- parametrically treating the stacked function composition g(h) = V(cid:62)φ(Uh) as process composition we obtain the deep GP deﬁnition of Equation 2.  L φL(WL−1φL−1(. . . W2φ1(U1x))),  2  Published as a conference paper at ICLR 2016  f1 ∼ GP  f2 ∼ GP  f3 ∼ GP  Y  X1  X2  X3  {g3(µ(n)  2 )}N  n=1  {g2(µ(n)  1 )}N  n=1  {g1(y(n))}N  n=1  Figure 3: A deep Gaussian process with three hidden layers and back-constraints.  2.1 VARIATIONAL INFERENCE  In a standard GP model, inference is performed by analytically integrating out the latent function f. In the DGP case, the latent variables have to additionally be integrated out, to obtain the marginal likelihood of DGPs over the observed data: p(Y|X1)  p(Xl−1|Xl)p(XL)dX1 . . . dXL.  L(cid:89)  p(Y) =  (cid:90)  (4)  l=2  The above marginal likelihood and the following derivation aims at unsupervised learning prob- lems, however, it is straight-forward to extend the formulation to supervised scenario by assuming observed XL. Bayesian inference in DGPs involves optimizing the model hyper-parameters with respect to the marginal likelihood and inferring the posterior distributions of latent variables for training/testing data. The exact inference of DGPs is intractable due to the intractable integral in (4). Approximated inference techniques such as variational inference and EP have been developed (Damianou & Lawrence, 2013; Bui et al., 2015). By taking a variational approach, i.e. assuming a variational posterior distribution of latent variables, q({Xl}L l=1 q(Xl), a lower bound of the log marginal distribution can be derived as  l=1) =(cid:81)L  l=1  l=1  L =  Fl +  H(q(Xl)) − KL (q(XL)(cid:107) p(XL)) ,  (5) where F1 = (cid:104)log p(Y|X1)(cid:105)q(X1) and Fl = (cid:104)log p(Xl−1|Xl)(cid:105)q(Xl−1)q(Xl) , l = 2 . . . L, are known as free energy for individual layers. H(q(Xl)) denotes the entropy of the variational distribu- tion q(Xl) and KL (q(XL)(cid:107) p(XL)) denotes the Kullback-Leibler divergence between q(XL) and p(XL). According to the model deﬁnition, both p(Y|X1) and p(Xl−1|Xl) are Gaussian pro- q(Xl) =(cid:81)N cesses. The variational distribution of Xl is typically parameterized as a Gaussian distribution  n=1 N (x(n)l|µ(n)l, Σ(n)l).  L(cid:88)  L−1(cid:88)  3 VARIATIONAL AUTO-ENCODED MODEL  Damianou & Lawrence (2013) provides a tractable variational inference method for DGP by de- riving a closed-form lower bound of the marginal likelihood. While successfully demonstrating strengths of DGP, the experiments that they show are limited to very small scales (hundreds of data- points). The limitation on scalability is mostly due to the computational expensive covariance matrix inversion and the large number of variational parameters (growing linearly with the size of data). To scale up DGP to handle large datasets, we propose a new deep generative model, by augmenting DGP with a variationally auto-encoded inference mechanism. We refer to this inference mecha- nism as a recognition model (see Figure 3). A recognition model provides us with a mechanism for constraining the variational posterior distributions of latent variables. Instead of representing vari- ational posteriors as individual variational parameters, which become a big burden to optimization, we deﬁne them as a transformation of observed data. This allows us to reduce the number of pa- rameters for optimization (which no longer grow linearly with the size of data) and to perform fast inference at test time. A similar constraint mechanism has been referred to as a “back-constraint” in the GP literature. Lawrence & Qui˜nonero Candela (2006) constrained the latent inputs of a GP with a parametric model to enforce local distance preservation in the inputs; Ek et al. (2008) followed the same approach for constraining the latent space with information from additional views of the data. Our formulation differs from the above in that we rather constrain a whole latent posterior  3  Published as a conference paper at ICLR 2016  (a)  (b)  Figure 4: (a) The learned 2D latent space of one layer DGP and one layer VAE-DGP from the same initialization on a subset of MNIST with noisy background1. A subset of four digits (0, 1, 2, 3) with 10,000 datapoints (2,500 per digit) are taken from the dataset. The left and right ﬁgures show the learned 2D latent space on the training data from the same initialization by DGP and VAE-DGP respectively. The recognition model in VAE-DGP helps to avoid local optima and results in a better latent space. (b) The train and test log-likelihood of one layer VAE-DGP with different sizes of recognition models trained on a subset of MNIST (digit “0” with 1000 for training and 1000 for test). The number of units in the one hidden layer recognition model (MLP) is varied from 1 to 200.  distribution through the variational parameters. Damianou & Lawrence (2015) also constrained the posterior, but this was achieved using a direct speciﬁc parameterization for that distribution, making this back-constraint grow with the number of inputs. Another difference to the previous approaches is that we consider deep hierarchies of latent spaces and, consequently, of recognition models. Our constraint mechanism is more similar to that of other variationally auto-encoded models, such as (Salakhutdinov & Hinton, 2008; Snoek et al., 2012a; Kingma & Welling, 2013; Mnih & Gregor, 2014; Rezende et al., 2014). The main differences with our work is that are that we have a Bayesian non-parametric generative model and a closed-form variational lower bound. This enables us to be Bayesian when inferring the generative distribution and avoids sampling from variational posterior distributions. Speciﬁcally, for the observed layer, the posterior mean of the variational distribution is deﬁned as a transformation of the observed data:  (6) where the transformation function g1 is parameterized by a multi-layer perceptron (MLP). Similarly, for the hidden layers, the posterior mean is deﬁned as a transformation of the posterior mean from the lower layer:  1 = g1(y(n)),  µ(n)  (l−1)).  µ(n)l = gl(µ(n)  (7) Note that all the transformation functions are deterministic, therefore, the posterior mean of all the hidden layers can be viewed as direct transformations of the observed data, i.e. µ(n)l = gl(. . . g1(y(n))). We use the hyperbolic tangent activation function for all the MLPs. The poste- rior variances Σ(n)l are assumed to be diagonal and the same across all the datapoints. The closed-form variational lower bound allows us to apply sophisticated gradient optimization methods such as L-BFGS. It avoids the problem of initializing and optimizing a large number of vari- ational parameters. The initialization of variational parameters are converted into the initialization of neural network parameters, which has been well studied in deep learning literature. Furthermore, with the reparameterization, the variational parameters are moved coherently during optimization through the changes of neural network mapping. This helps the model avoid local optima and ap- proach better solutions. Figure 4a shows an example of the learned 2D latent space of one layer (shallow) DGP and VAE-DGP from the same initialization. Clearly, the recognition model in VAE- DGP helps move the datapoints to a better solution. Note that the recognition model serves as a (deterministic) reparameterization of variational parameters. Therefore, the parameters of MLP are  1The dataset is downloaded from the link: http://www.iro.umontreal.ca/˜lisa/twiki/  bin/view.cgi/Public/BackgroundCorrelation.  4  2101221012DGP (L=-1.01e+06)01232101221012VAE-DGP (L=-9.07e+05)0123Published as a conference paper at ICLR 2016  the variational parameters of our model. As automatically “regularized” by Bayesian inference, a overly complicated cognition model will not cause the generative model to overﬁt. This allows us to freely choose a powerful enough recognition model (see Fig. 4b for an example2). Computationally, the recognition model re-parameterization resolves the linear growing of the num- ber of variational parameters with respect to the size of data. Based on this formulation, we develop a distributed variational inference approach, which is described in detail in the following section.  4 DISTRIBUTED VARIATIONAL INFERENCE  The exact evaluation of the variational lower bound in Equation (5) is still intractable due to the expectation in the free energy terms. A variational approximation technique developed for Bayesian Gaussian Process Latent variable Model (BGPLVM) (Titsias & Lawrence, 2010) can be applied to obtain a lower bound of these free energy terms. Taking the observed layer as an example, by introducing noise-free observations F1 ∈ RN×D, a set of auxiliary variable namely inducing variable U1 ∈ RM×D and a set of variational parameter namely inducing inputs Z1 ∈ RM×Q1, the conditional distribution is reformulated as  p(Y|X1) =  p(Y|F1)p(F1|U1, X1)p(U1)dF1dU1,  (8)  (cid:90)  where each row of U1 represents an inducing variable which is associated with the inducing input at the same row of Z1. Assuming a particular form of the variational distribution of F1 and U1: q(F1, U1|X1) = p(F1|U1, X1)q(U1), the free energy of the observed layer can be lower bounded by  F1 ≥ (cid:104)log p(Y|F1) − KL (q(U1)(cid:107) p(U1))(cid:105)p(F1|U1,X1)q(U1)q(X1) .  (9)  As shown by Titsias & Lawrence (2010), this lower bound can be formulated in closed-form for kernels like linear, exponentiated quadratic. For other kernels, it can be computed approximately by using the techniques such as Gaussian quadrature. Note that the optimal value of q(U1) can be derived in closed-form by setting its gradient ∂L/∂q(U1) to zero, therefore, the only variational parameters that we need to optimize for the observed layer are q(X1) and Z1. For the hidden layers, the variational posterior distributions are slightly different, because the posterior of inducing variables depend on the output variable of that layer. For the l-th hid- den layer, the variational posterior distribution is, therefore, deﬁned as q(Fl, Ul|Xl−1, Xl) = p(Fl|Ul, Xl)q(Ul|Xl−1). Similar to the observed layer, a lower bound of the free energy can be derived as:  Fl ≥ (cid:104)log p(Xl−1|Fl) − KL (q(Ul|Xl−1)(cid:107) p(Ul))(cid:105)p(Fl|Ul,Xl)q(Ul|Xl−1)q(Xl−1)q(Xl) .  (10)  With Equation (5) and (9-10), a closed-form variational lower bound of the log marginal likelihood is deﬁned. The computation of the lower bounds of free energy terms is expensive. This limits the scalability of the original DGP. Fortunately, with the introduced auxiliary variables and the recognition model, most of the computation is distributable in a data-parallelism fashion. We exploit this fact and derive a distributed formulation of the lower bound. This allows us to scale up our inference method to large data. Speciﬁcally, the lower bound of the free energy consists of a few terms (explained below) that depend on the size of data: Tr(Y(cid:62)Y), Tr(Λ−1 1 YY(cid:62)Ψ1), ψ1 and Φ1. All of them can be formulated as a sum of intermediate results from individual datapoints:  1 Ψ(cid:62)  Tr(Y(cid:62)Y) =  (y(n))(cid:62)y(n),  N(cid:88) Λ−1  n=1  1  (cid:32) N(cid:88)  n=1  Tr(Λ1Ψ(cid:62)  1 YY(cid:62)Ψ1) = Tr  1 (y(n))(cid:62) Ψ(n)  (cid:33)(cid:62) ,  1 (y(n))(cid:62) Ψ(n)  (cid:33)(cid:32) N(cid:88)  n=1  2Note that the shown training and test log-likelihood are not directly comparable. The shown train log- likelihood is the lower bound in Equation 5 divided by the size of data. The shown test log-likelihood is an approximation: 1  N∗ (L(Y∗, Y) − L(Y)).  5  Published as a conference paper at ICLR 2016  (a)  (b)  (c)  Figure 5: (a) The samples generated from VAE-DGP trained on the combination of Frey faces and Yale faces (Frey-Yale). (b) Imputation from the test set of Frey-Yale. (c) Imputation from the test set of SVHN. The gray color indicates the missing area. The 1st column shows the input images, the 2nd column show the imputed images and 3rd column shows the original full images.  F1U1  (cid:11)  KF1U1  Φ1 = (cid:10)K(cid:62)  where KF1F1, KU1U1 are the covariance matrices of F1 and U1 respectively, KF1U1 is the cross- covariance matrix between F1 and U1, and ψ1 = Tr((cid:104)KF1F1(cid:105)q(X1)), Ψ1 = (cid:104)KF1U1(cid:105)q(X1) and q(X1), and Λ1 = KU1U1 + Φ1. This enables data-parallelism by dis- tributing the computation that depends on individual datapoints and only collecting the intermediate results that do not scale with the size of data. Gal et al. (2014) and Dai et al. (2014) exploit a similar formulation for distributing the computation of BGPLVM, however, in their formulations, the gra- dients of variational parameters that depend on individual datapoints have to be collected centrally. Such collection severely limits the scalability of the model. For hidden layers, the free energy terms are slightly different. Their data-dependent terms ad- ditionally involve the expectation with respect to the variational distribution of output variables: Ψl), ψl and Φl. The ﬁrst term can be  Tr((cid:10)X(cid:62)  (cid:11)  l−1Xl−1  q(Xl−1)  naturally reformulated as a sum across datapoints:  l−1  (cid:11)  (cid:11)  q(Xl−1)  q(Xl−1)) =  ), Tr(Λl−1Ψl(cid:62)(cid:10)Xl−1X(cid:62) N(cid:88) Tr((cid:10)X(cid:62) (cid:11) Λl−1  q(Xl−1) Ψl) = Tr(cid:0)Λl−1(cid:0)Ψl(cid:62)R(cid:62) (cid:32) N(cid:88)  l−1 (l−1))(cid:62)], Al−1 = diag(α(1) l−1, . . . , α(N )  (µ(n)  q(Xl−1)  + Tr  n=1  For the second term, we can rewrite(cid:10)Xl−1X(cid:62)  l−1Xl−1  (l−1))(cid:62), . . . , (µ(N ) [(µ(1) us to formulate it into a distributable form: Tr(Λl−1Ψl(cid:62) (cid:104)Xl−1Xl−1(cid:105)(cid:62)  l−1)(cid:62)µ(n) = R(cid:62) l−1 ) and α(n)  l−1 + Tr(Σ(n)  l−1).  (11)  l−1Rl−1+Al−1Al−1, where Rl−1 = 2 . This enables  l−1 = Tr(Σ(n)  l−1) 1  (cid:1) (Rl−1Ψl)(cid:1) (cid:33)(cid:32) N(cid:88)  l−1  Ψl(n)α(n) l−1  (cid:33)(cid:62) .  Ψl(n)α(n) l−1  n=1  n=1  (12) With the above formulations, we obtain distributable a variational lower bound. For optimization, the gradients of all the model and variational parameters can be derived with respect to the lower bound. As the variational distributions q({Xl}L l=1) are computed according to the recognition model, the gradients of q({Xl}L l=1) are back-propagated (through the recognition model), which allows to com- pute the gradients of its the parameters.  5 EXPERIMENTS  As a probabilistic generative model, VAE-DGP is applicable to a range of different tasks such as data generation, data imputation, etc. In this section we evaluate our model in a variety of problems and compare it with the alternatives in the in the literature.  6  Published as a conference paper at ICLR 2016  5.1 UNSUPERVISED LEARNING  Model DBN  Stacked CAE Deep GSN  Adversarial nets  GMMN+AE VAE-DGP (5)  VAE-DGP (10-50) VAE-DGP (5-20-50)  MNIST 138±2 121 ± 1.6 214 ± 1.1 225 ± 2 282 ± 2 301.67 674.86 723.65  Figure 6: Samples of imputation on the test sets. The gray color indicates the missing area. The 1st column shows the input im- ages, the 2nd column show the imputed im- ages and 3rd column shows the original full images.  Table 1: Log-likelihood for the MNIST test data with different models. The baselines are DBN and Stacked CAE (Bengio et al., 2013), Deep GSN (Bengio et al., 2014), Adversarial nets (Goodfellow et al., 2014) and GMMN+AE (Li et al., 2015). We ﬁrst apply to our model to the combination of Frey faces and Yale faces (Frey-Yale). The Frey faces contains 1956 20 × 28 frames taken from a video clip. The Yale faces contains 2414 images, which are resized to 20 × 28. We take the last 200 frames from the Frey faces and 300 images randomly from Yale faces as the test set and use the rest for training. The intensity of the original gray-scale images are normalized to [0, 1]. The applied VAE-DGP has two hidden layers (a 2D top hidden layer and a 20D middle hidden layer). The exponentiated quadratic kernel is used for all the layers with 100 inducing points. All the MLPs in the recognition model have two hidden layers with widths (500-300). As a generative model, we can draw samples from the learned model by sampling ﬁrst from the prior distribution of the top hidden layer (a 2D unit Gaussian distribution in this case) and layer-wise downwards. The generated images are shown in Figure 5a. To evaluate the ability of our model learning the data distribution, we train the VAE-DGP on MNIST (LeCun et al., 1998). We use the whole training set for learning, which consists of 60,000 28 × 28 images. The intensity of the original gray-scale images are normalized to [0, 1]. We train our model with three different model settings (one, two and three hidden layers). The trained models are evaluated by the log-likelihood of the test set3, which consists of 10,000 images. The results are shown in Table 1 along with some baseline performances taken from the literature. The numbers in the parenthesis indicate the dimensionality of hidden layers from top to bottom. The exponentiated quadratic kernel are used for all the layers with 300 inducing points. All the MLPs in the recognition model has two hidden layers with width (500-300). All our models are trained as a whole from randomly initialized recognition model.  5.2 DATA IMPUTATION  We demonstrate the model’s ability to impute missing data by showing half of images on the test set. We use the learned VAE-DGP to impute the other half of the images. this is challenging problem because there might be ambiguities in the answers. For instance, by showing the right half of a digit “8”, the answers “3” and “8” are both reasonable. We show the imputation performance for the test images in Frey-Yale and MNIST in Fig. 5b and Fig. 6 respectively. We also apply VAE-DGP to the street view house number dataset (SVHN) (Netzer et al., 2011). We use three hidden layers with the dimensionality of latent space from top to bottom (5-30-500). The top two hidden layers use the exponentiated quadratic kernel and the observed layer uses the linear kernel with 500 inducing points. The learned model is used for imputing the images in the test set (see Fig. 5c).  N∗ log p(Y∗|Y), where Y∗ is the test data and Y is the training data. As the true test log-likelihood is intractable, we approximate it as 1  3As a non-parametric model, the test log-likelihood of VAE-DGP is formulated as 1 N∗ (L(Y∗, Y) − L(Y)).  7  Published as a conference paper at ICLR 2016  Model VEA-DGP GP Lin. Reg.  Abalone  825.31 ± 64.35 888.96 ± 78.22 917.31 ± 53.76  Model VEA-DGP GP Lin. Reg.  Creep  575.39 ± 29.10 602.11 ± 29.59 1865.76 ± 23.36  Figure 7: Bayesian optimization experiments for the Branin function using a standard GP and our VEA-DGP.  Table 2: MSE obtained from our VEA-DGP, standard GP and linear the Abalone and Creep benchmarks.  regression for  5.3 SUPERVISED LEARNING AND BAYESIAN OPTIMIZATION  In this section we consider two supervised learning problem instances: regression and Bayesian optimization (BO) (Osborne, 2010; Snoek et al., 2012b). We demonstrate the utility of VEA-DGP in these settings by evaluating its performance in terms of predictive accuracy and predictive un- certainty quantiﬁcation. For these experiments we use a VEA-DGP with one hidden layer (and one observed inputs layer) and exponentiated quadratic covariance functions. Furthermore, we in- corporate the deep GP modiﬁcation of Duvenaud et al. (2014) so that the observed input layer has an additional connection to the output layer. Duvenaud et al. (2014) showed that this modiﬁca- tion increases the general stability of the method. Since the sample size of the data considered for supervised learning is relatively small, we do not use the recognition model to back-constrain the variational distributions. In the regression experiments we use the Abalone dataset (4177 1-dimensional outputs and 8−dimensional inputs) from UCI and the Creep dataset (2066 1-dimensional outputs and 30−dimensional inputs) from (Cole et al., 2000). A typical split for this data is to use 1000 (Abalone) and 800 (Creep) instaces for training. We used 100 inducing inputs for each layer and performed 4 runs with different random splits. We summarize the results in Table 2. Next, we show how the VAE-DGP can be used in the context of probabilistic numerics, in particular for Bayesian optimization (BO) (Osborne, 2010; Snoek et al., 2012b). In BO, the goal is to ﬁnd xmin = arg minX f (x) for X ⊂ RQ where a limited number of evaluations are available. Typically, a GP is used to ﬁt the available data, as a surrogate model. The GP is iteratively updated with new function evaluations and used to build an acquisition function able to guide the collection of new observations of f. This is done by balancing exploration (regions with large uncertainty) and exploitation (regions with a low mean). In BO, the model is a crucial element of the process: it should be able to express complex classes of functions and to provide coherent estimates of the function uncertainty. In this experiment we use the non-stationary Branin function4 to compare the performance of standard GPs and the VEA-DPP in the context of BO. We used the popular expected improvement (Jones et al., 1998) acquisition function and we ran 10 replicates of the experiment using different initializations, each kicking-off optimization with 3 points randomly selected from the functions’ domain X . In each replicate we iteratively collected 20 evaluations of the functions. In the VEA-DPP we used 30 inducing points. Figure 7 shows that using the VEA-DPP as a surrogate model results in a gain, especially in the ﬁrst steps of the optmization. This is due to ability of VEA- DPP to deal with the non-stationary components of the function and to model a much richer class of distributions (e.g. multi-modal) in the output layer (as opposed to the standard GP which assumes joint Gaussianity in the outputs).  4See http://www.sfu.ca/˜ssurjano/optimization.html for details. The default domain is  in the experiments.  8  5101520Iteration0246810Best found valueBraninGPVAE-DGPPublished as a conference paper at ICLR 2016  6 CONCLUSION  We have proposed a new deep non-parametric generative model. Although general enough to be used in supervised and unsupervised problems, we especially highlighted its usefulness in the latter scenario, a case which is known to be a major challenge for current deep machine learning ap- proaches. Our model is based on a deep Gaussian process, which we extended with a layer-wise parameterization through multilayer perceptrons, signiﬁcantly simplifying optimization. Addition- ally, we developed a new formulation of the lower bound that allows for distributed computations. Overall, our approach is able to perform Bayesian inference using large datasets and compete with current alternatives. Future developments include the regularization of the perceptron weights, to re- formulate the current setup for the context of multi-view problems and to incorporate convolutional structures into the objective function. acknowledgement. The authors thank the ﬁnancial support of RADIANT (EU FP7-HEALTH Project Ref 305626), BBSRC Project No BB/K011197/1 and WYSIWYD (EU FP7-ICT Project Ref 612139).  REFERENCES Bengio, Yoshua, Mesnil, Gr´egoire, Dauphin, Yann, and Rifai, Salah. Better Mixing via Deep Representations.  In International Conference on Machine Learning, 2013.  Bengio, Yoshua, Laufer, Eric, Alain, Guillaume, and Yosinski, Jason. Deep Generative Stochastic Networks  Trainable by Backprop. In International Conference on Machine Learning, 2014.  Blundell, Charles, Cornebise, Julien, Kavukcuoglu, Koray, and Wierstra, Daan. Weight Uncertainty in Neural  Networks. In International Conference on Machine Learning, 2015.  Bui, Thang D., Hern´andez-Lobato, Jos´e Miguel, Li, Yingzhen, Hern´andez-Lobato, Daniel, and Turner, Richard E. Training Deep Gaussian Processes using Stochastic Expectation Propagation and Probabilis- tic Backpropagation. In Workshop on Advances in Approximate Bayesian Inference, NIPS, 2015.  Calandra, Roberto, Peters, Jan, Rasmussen, Carl Edward, and Deisenroth, Marc Peter. Manifold Gaussian  processes for regression. Technical report, 2014.  Cole, D, Martin-Moran, C, Sheard, AG, Bhadeshia, HKDH, and MacKay, DJC. Modelling creep rupture  strength of ferritic steel welds. Science and Technology of Welding & Joining, 5(2):81–89, 2000.  Dai, Zhenwen, Damianou, Andreas, Hensman, James, and Lawrence, Neil. Gaussian process models with  parallelization and GPU acceleration, 2014.  Damianou, Andreas. Deep Gaussian processes and variational propagation of uncertainty. PhD Thesis, Uni-  versity of Shefﬁeld, 2015.  Damianou, Andreas and Lawrence, Neil. Semi-described and semi-supervised learning with Gaussian pro-  cesses. In 31st Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2015.  Damianou, Andreas and Lawrence, Neil D. Deep Gaussian processes. In Carvalho, Carlos and Ravikumar, Pradeep (eds.), Proceedings of the Sixteenth International Workshop on Artiﬁcial Intelligence and Statistics, volume 31, pp. 207–215, AZ, USA, 4 2013. JMLR W&CP 31.  Damianou, Andreas, Titsias, Michalis K., and Lawrence, Neil D. Variational Gaussian process dynamical In Bartlett, Peter, Peirrera, Fernando, Williams, Chris, and Lafferty, John (eds.), Advances in  systems. Neural Information Processing Systems, volume 24, Cambridge, MA, 2011. MIT Press.  Dasgupta, Sanjoy and McAllester, David (eds.). Proceedings of the 30th International Conference on Ma- chine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of JMLR Proceedings, 2013. JMLR.org.  Durrande, N, Ginsbourger, D, and Roustant, O. Additive kernels for Gaussian process modeling. Technical  report, 2011.  Duvenaud, David, Lloyd, James Robert, Grosse, Roger, Tenenbaum, Joshua B., and Ghahramani, Zoubin. In Dasgupta &  Structure discovery in nonparametric regression through compositional kernel search. McAllester (2013), pp. 1166–1174.  9  Published as a conference paper at ICLR 2016  Duvenaud, David, Rippel, Oren, Adams, Ryan, and Ghahramani, Zoubin. Avoiding pathologies in very deep networks. In Kaski, Sami and Corander, Jukka (eds.), Proceedings of the Seventeenth International Work- shop on Artiﬁcial Intelligence and Statistics, volume 33, Iceland, 2014. JMLR W&CP 33.  Ek, Carl Henrik, Rihan, Jon, Torr, Philip, Rogez, Gregory, and Lawrence, Neil D. Ambiguity modeling in latent spaces. In Popescu-Belis, Andrei and Stiefelhagen, Rainer (eds.), Machine Learning for Multimodal Interaction (MLMI 2008), LNCS, pp. 62–73. Springer-Verlag, 28–30 June 2008.  Gal, Yarin and Ghahramani, Zoubin. Dropout as a Bayesian Approximation: Representing Model Uncertainty  in Deep Learning. arXiv:1506.02142, 2015.  Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl E. Distributed Variational Inference in Sparse Gaussian In Advances in Neural Information Processing System,  Process Regression and Latent Variable Models. 2014.  G¨onen, Mehmet and Alpaydin, Ethem. Multiple kernel learning algorithms. Journal of Machine Learning  Research, 12:2211–2268, Jul 2011.  Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua. Generative Adversarial Networks. In Advances in Neural Information Process- ing Systems, 2014.  Hensman, James and Lawrence, Neil D. Nested variational compression in deep Gaussian processes. Technical  report, University of Shefﬁeld, 2014.  Hensman, James, Lawrence, Neil D., and Rattray, Magnus. Hierarchical Bayesian modelling of gene expression time series across irregularly sampled replicates and clusters. BMC Bioinformatics, 14(252), 2013. doi: doi:10.1186/1471-2105-14-252.  Hinton, Geoffrey E., Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-  proving neural networks by preventing co-adaptation of feature detectors. arXiv: 1207.0580, 2012.  Jones, Donald R., Schonlau, Matthias, and Welch, William J. Efﬁcient global optimization of expensive black-  box functions. Journal of Global Optimization, 13(4):455–492, 1998.  Kingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. In ICLR, 2013.  Kingma, Diederik P., Salimans, Tim, and Welling, Max. Variational Dropout and the Local Reparameterization  Trick. In Advances in Neural Information Processing System, 2015.  Lawrence, Neil D. and Moore, Andrew J. Hierarchical Gaussian process latent variable models. In Ghahramani, Zoubin (ed.), Proceedings of the International Conference in Machine Learning, volume 24, pp. 481–488. Omnipress, 2007. ISBN 1-59593-793-3.  Lawrence, Neil D. and Qui˜nonero Candela, Joaquin. Local distance preservation in the GP-LVM through back In Cohen, William and Moore, Andrew (eds.), Proceedings of the International Conference ISBN 1-59593-383-2. doi: 10.1145/  constraints. in Machine Learning, volume 23, pp. 513–520. Omnipress, 2006. 1143844.1143909.  L´azaro-Gredilla, Miguel. Bayesian warped Gaussian processes. In Bartlett, Peter L., Pereira, Fernando C. N., Burges, Christopher J. C., Bottou, L´eon, and Weinberger, Kilian Q. (eds.), Advances in Neural Information Processing Systems, volume 25, Cambridge, MA, 2012.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, November 1998.  Li, Yujia, Swersky, Kevin, and Zemel, Richard. Generative Moment Matching Networks.  Conference on Machine Learning, 2015.  Mnih, A. and Gregor, K. Neural Variational Inference and Learning in Belief Networks.  Conference on Machine Learning, 2014.  In International  In International  Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Reading Digits in Natural Images with Unsupervised Feature Learning. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.  Osborne, Michael. Bayesian Gaussian Processes for Sequential Prediction, Optimisation and Quadrature. PhD  thesis, University of Oxford, 2010.  10  Published as a conference paper at ICLR 2016  Rezende, D J, Mohamed, S, and Wierstra, D. Stochastic backpropagation and approximate inference in deep  generative models. In International Conference on Machine Learning, 2014.  Salakhutdinov, Ruslan and Hinton, Geoffrey. Using Deep Belief Nets to Learn Covariance Kernels for Gaussian  Processes. In Advances in Neural Information Processing Systems, volume 20, 2008.  Snelson, Edward, Rasmussen, Carl Edward, and Ghahramani, Zoubin. Warped Gaussian processes. In Thrun, Sebastian, Saul, Lawrence, and Sch¨olkopf, Bernhard (eds.), Advances in Neural Information Processing Systems, volume 16, Cambridge, MA, 2004. MIT Press.  Snoek, Jasper, Adams, Ryan P., and Larochelle, Hugo. Nonparametric Guidance of Autoencoder Representa-  tions using Label Information. Journal of Machine Learning Research, 13:2567–2588, 2012a.  Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. Practical Bayesian optimization of machine learning  algorithms, pp. 29512959. 2012b.  Titsias, Michalis K. and Lawrence, Neil D. Bayesian Gaussian process latent variable model. In Teh, Yee Whye and Titterington, D. Michael (eds.), Proceedings of the Thirteenth International Workshop on Artiﬁcial Intel- ligence and Statistics, volume 9, pp. 844–851, Chia Laguna Resort, Sardinia, Italy, 13-16 May 2010. JMLR W&CP 9.  Uria, Benigno, Murray, Iain, and Larochelle, Hugo. A Deep and Tractable Density Estimator. In International  Conference on Machine Learning, 2014.  Wilson, Andrew Gordon and Adams, Ryan Prescott. Gaussian process kernels for pattern discovery and ex-  trapolation. In Dasgupta & McAllester (2013), pp. 1067–1075.  11  ",
1511.06744,2016,Training Convolutional Neural Networks with Low-rank Filters for Efficient Image Classification,"['Training Convolutional Neural Networks with Low-rank Filters for Efficient Image Classification\nYani Ioannou', 'Duncan Robertson', 'Jamie Shotton', 'roberto Cipolla', 'Antonio Criminisi', 'Jamie Shotton']",https://arxiv.org/pdf/1511.06744,"6 1 0 2     b e F 7         ]  V C . s c [      3 v 4 4 7 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  TRAINING CNNS WITH LOW-RANK FILTERS FOR EFFICIENT IMAGE CLASSIFICATION  Yani Ioannou1, Duncan Robertson2, Jamie Shotton2, Roberto Cipolla1 & Antonio Criminisi2 1University of Cambridge, 2Microsoft Research {yai20,rc10001}@cam.ac.uk, {a-durobe,jamiesho,antcrim}@microsoft.com  ABSTRACT  We propose a new method for creating computationally efﬁcient convolutional neural networks (CNNs) by using low-rank representations of convolutional ﬁl- ters. Rather than approximating ﬁlters in previously-trained networks with more efﬁcient versions, we learn a set of small basis ﬁlters from scratch; during train- ing, the network learns to combine these basis ﬁlters into more complex ﬁlters that are discriminative for image classiﬁcation. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped ﬁlters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG- 11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model pa- rameters. Applying our method to a near state-of-the-art network for CIFAR, we achieved comparable accuracy with 46% less compute and 55% fewer parameters.  1  INTRODUCTION  Convolutional neural networks (CNNs) have been used increasingly succesfully to solve challeng- ing computer vision problems such as image classiﬁcation (Krizhevsky et al., 2012), object detection (Ren et al., 2015), and human pose estimation (Tompson et al., 2015). However, recent improve- ments in recognition accuracy have come at the expense of increased model size and computational complexity. These costs can be prohibitive for deployment on low-power devices, or for fast analysis of videos and volumetric medical images. One promising aspect of CNNs, from a memory and computational efﬁciency standpoint, is their use of convolutional ﬁlters (a.k.a. kernels). Such ﬁlters usually have limited spatial extent and their learned weights are shared across the image spatial domain to provide translation invari- ance (Fukushima, 1980; LeCun et al., 1998). Thus, as illustrated in Fig. 1, in comparison with fully connected network layers (Fig. 1a), convolutional layers have a much sparser connection structure and use fewer parameters (Fig. 1b). This leads to faster training and test, better generalization, and higher accuracy. This paper focuses on reducing the computational complexity of the convolutional layers of CNNs by further sparsifying their connection structures. Speciﬁcally, we show that by representing con- volutional ﬁlters using a basis space comprising groups of ﬁlters of different spatial dimensions (examples shown in Fig. 1c and d), we can signiﬁcantly reduce the computational complexity of existing state-of-the-art CNNs without compromising classiﬁcation accuracy. Our contributions include a novel method of learning a set of small basis ﬁlters that are combined to represent larger ﬁlters efﬁciently. Rather than approximating previously trained networks, we  1  Published as a conference paper at ICLR 2016  Figure 1: Network connection structure for convolutional layers. An input image is transformed in one layer of an neural network into an output image of same size. Connection weight maps show pairwise dependencies between input and output pixels. In (a), each node is connected to all input pixels. For (b,c,d), output pixels depend only on a subset of input pixels (shared weights are represented by unique colours). Note that sparsity increases from (a) to (d), opening up potentially more efﬁcient implementation.  train networks from scratch and show that our convolutional layer representation can improve both efﬁciency and classiﬁcation accuracy. We further describe how to initialize connection weights ef- fectively for training networks with composite convolutional layers containing groups of differently- shaped ﬁlters, which we found to be of critical importance to our training method.  1.1 RELATED WORK There has been much previous work on increasing the test-time efﬁciency of CNNs. Some promis- ing approaches work by making use of more hardware-efﬁcient representations. For example Gupta et al. (2015) and Vanhoucke et al. (2011) achieve training- and test-time compute savings by further quantization of network weights that were originally represented as 32 bit ﬂoating point numbers. However, more relevant to our work are approaches that depend on new network connection struc- tures, efﬁcient approximations of previously trained networks, and learning low rank ﬁlters.  Efﬁcient Network Connection Structures. There has been shown to be signiﬁcant redundancy in the trained weights of CNNs (Denil et al., 2013). LeCun et al. (1989) suggest a method of prun- ing unimportant connections within networks. However this requires repeated network re-training and may be infeasible for modern, state-of-the-art CNNs requiring weeks of training time. Lin et al. (2013) show that the geometric increase in the number and dimensions of ﬁlters with deeper networks can be managed using low-dimensional embeddings. The same authors show that global average-pooling may be used to decrease model size in networks with fully connected layers. Si- monyan & Zisserman (2014) show that stacked ﬁlters with small spatial dimensions (e.g. 3× 3), can operate on the effective receptive ﬁeld of larger ﬁlters (e.g. 5×5) with less computational complexity. Low-Rank Filter Approximations. Rigamonti et al. (2013) approximate previously trained CNNs with low-rank ﬁlters for the semantic segmentation of curvilinear structures within volumet- ric medical imagery. They discuss two approaches: enforcing an L1-based regularization to learn approximately low rank ﬁlters, which are later truncated to enforce a strict rank, and approximating a set of pre-learned ﬁlters with a tensor-decomposition into many rank-1 ﬁlters. Neither approach learns low rank ﬁlters directly, and indeed the second approach proved the more successful. The work of Jaderberg et al. (2014) also approximates the existing ﬁlters of previously trained net- works. They ﬁnd separable 1D ﬁlters through an optimization minimizing the reconstruction error of the already learned full rank ﬁlters. They achieve a 4.5× speed-up with a loss of accuracy of 1% in a text recognition problem. However since the method is demonstrated only on text recognition, it is not clear how well it would scale to larger data sets or more challenging problems. A key insight  2  Fully connectedlayer structureConvolutional 1 ×3verticalConvolutional 3 ×1horizontalConnection weightsKernelN/AConnection structureInput pixelsOutput pixelsConvolutional 3 ×3 squareInput pixelsOutput pixels10012345678911Input image(zero-padded 3 x4 pixels) 01238910114567Outputfeature map(4 x3)01238910114567Output pixelsInput pixelsInput pixelsOutput pixels(a)(b)(c)(d)Input pixelsOutput pixels10012345678911Published as a conference paper at ICLR 2016  (a) A full rank convolutional layer.  (b) Sequential separable ﬁlters (Jaderberg et al., 2014).  (c) Our method, a learned basis space of ﬁlters that are rectangular in the spatial domain and oriented horizontally and vertically.  (d) Our method, a learned basis space of vertical/horizontal rect- angular ﬁlters and square ﬁlters. Filters of other shapes are also possible.  Figure 2: Methods of using low-rank ﬁlters in CNNs. The activation function is not shown, coming after the last layer in each conﬁguration.  of the paper is that ﬁlters can be represented by low rank approximations not only in the spatial domain but also in the channel domain. Both of these methods show that, at least for their respective applications, low rank approximations of full-rank ﬁlters learned in convolutional networks can increase test time efﬁciency signiﬁcantly. However, being approximations of pre-trained networks, they are unlikely to improve test accuracy, and can only increase the computational requirements during training.  Learning Separable Filters. Mamalet & Garcia (2012) propose training networks with separable ﬁlters on the task of digit recognition with the MNIST dataset. They train networks with sequential convolutional layers of horizontal and vertical 1D ﬁlters, achieving a speed-up factor of 1.6×, but with a relative increase in test error of 13% (1.45% v.s. 1.28%).Our approach generalizes this, al- lowing both horizontal and vertical 1D ﬁlters (and other shapes too) at the same layer and avoiding issues with ordering. We also demonstrate a decrease in error and on more challenging datasets.  2 USING LOW-RANK FILTERS IN CNNS  2.1 CONVOLUTIONAL FILTERS The convolutional layers of a CNN produce output ‘images’ (usually called feature maps) by con- volving input images with one or more learned ﬁlters. In a typical convolutional layer, as illustrated  3  …HWcdHWd filterswhc*1…WHcm filtersHW…mdHWd filterscwm**1h1WHcm filtersHW…mdHWd filterscw1m1**…1h…c1WHm filtersHW…mdHWd filtersm1**………cPublished as a conference paper at ICLR 2016  in Fig. 2a, a c-channel input image of size H × W pixels is convolved with d ﬁlters of size h× w× c to create a d-channel output image. Each ﬁlter is represented by hwc independent weights. There- fore the computational complexity for the convolution of the ﬁlter with a c-channel input image is O(dwhc) (per pixel in the output feature map). In what follows, we describe schemes for modifying the architecture of the convolutional layers so as to reduce computational complexity. The idea is to replace full-rank convolutional layers with modiﬁed versions that represent the same number of ﬁlters by linear combinations of basis vectors, i.e. as lower rank representations of the full rank originals.  2.2 SEQUENTIAL SEPARABLE FILTERS An existing scheme for reducing the computational complexity of convolutional layers (Jaderberg et al., 2014) is to replace each one with a sequence of two regular convolutional layers but with ﬁlters that are rectangular in the spatial domain, as shown in Fig 2b. The ﬁrst convolutional layer has m ﬁlters of size w×1×c, producing an output feature map with m channels. The second convolutional layer has d ﬁlters of size 1× h× m, producing an output feature map with d channels. By this means the full rank original convolutional ﬁlter bank is represented by a low rank approximation formed from a linear combination of a set of separable w × h basis ﬁlters. The computational complexity of this scheme is O(mcw) for the ﬁrst layer of horizontal ﬁlters and O(dmh) for the second layer of vertical ﬁlters, with a total of O(m(cw + dh)). Note that Jaderberg et al. (2014) use this scheme to approximate existing full rank ﬁlters belonging to previously trained networks using a retrospective ﬁtting step. In this work, by contrast, we train networks containing convolutional layers with this architecture from scratch. In effect, we learn the separable basis ﬁlters and their combination weights simultaneously during network training.  2.3 FILTERS AS LINEAR COMBINATIONS OF BASES In this work we introduce another scheme for reducing convolutional layer complexity. This works by representing convolutional ﬁlters as linear combinations of basis ﬁlters as illustrated in Fig. 2c. This scheme uses composite layers comprising several sets of ﬁlters where the ﬁlters in each set have different spatial dimensions (see Fig. 5). The outputs of these basis ﬁlters may be combined in a subsequent layer containing ﬁlters with spatial dimensions 1 × 1. This is illustrated in Fig. 2c. Here, our composite layer contains horizontal w × 1 and vertical 1 × h ﬁlters, the outputs of which are concatenated in the channel dimension, resulting in an intermediate m-channel feature map. These ﬁlter responses are then linearly combined by the next layer of d 1 × 1 ﬁlters to give a d-channel output feature map. In this case, the ﬁlters are applied on the input feature map with c channels and followed by a set of m 1×1 ﬁlters over the m output channels of the basis ﬁlters. If the number of horizontal and vertical ﬁlters is the same, the computational complexity is O(m(wc/2 + hc/2 + d)). Interestingly, the conﬁguration of Fig. 2c gives rise to linear combinations of horizontal and vertical ﬁlters that are cross-shaped in the spatial domain. This is illustrated in Fig. 3 for ﬁlters learned in the ﬁrst convolutional layer of the‘vgg-gmp-lr-join’ model that is described in the Results section when it is trained using ILSVRC dataset. Note that, in general, more than two different sizes of basis ﬁlter might be used in the composite layer. For example, Fig. 2d shows a combination of three sets of ﬁlters with spatial dimensions w × 1, 1× h, and w × h. Also note that an interesting option is to omit the 1× 1 linear combination layer and instead allow the connection weights in a subsequent network layer to learn to combine the basis ﬁlters of the preceding layer (despite any intermediate non-linearity, e.g. ReLUs). This possibility is explored in practice in the Results section. In that our method uses a combination of ﬁlters in a composite layer, it is similar to the ‘GoogLeNet’ of Szegedy et al. (2014) which uses ‘inception’ modules comprising several (square) ﬁlters of dif- ferent sizes ranging from 1×1 to 5×5. In our case, however, we are implicitly learning linear combinations of less computationally expensive ﬁlters with different orientations (e.g. 3×1 and 1×3 ﬁlters), rather than combinations of ﬁlters of different sizes. Amongst networks with similar com- putational requirements, GoogLeNet is one of the most accurate for large scale image classiﬁcation tasks (see Fig. 4), partly due to the use of heterogeneous ﬁlters in the inception modules, but also the use of low-dimensional embeddings and global pooling.  4  Published as a conference paper at ICLR 2016  (a) 3 × 1 ﬁlters.  (b) 1 × 3 ﬁlters.  (c) Learned linear combinations.  Figure 3: Learned Cross-Shaped Filters. The cross-shaped ﬁlters (c) learned as weighted linear combination of (b) 1 × 3 and (c) 3 × 1 basis ﬁlters in the ﬁrst convolutional layer of the the ‘vgg- gmp-lr-join’ model trained using the ILSVRC dataset.  3 TRAINING CNNS WITH MIXED-SHAPE LOW-RANK FILTERS  To determine the standard deviations to be used for weight initialization, we use an approach similar to that described by Glorot & Bengio (2010) (with the adaptation described by He et al. (2015) for layers followed by a ReLU). In Appendix A, we show the details of our derivation, generalizing the approach of He et al. (2015) to the initialization of ‘composite’ layers comprising several groups of ﬁlters of different spatial dimensions (see Appendix A, Fig. 5). This is one of the main contributions of this work. We ﬁnd that a composite layer of heterogeneously-shaped ﬁlter groups, where each ﬁlter group i has  w[i]h[i]d[i] outgoing connections should be initialized as if it is a single layer with ˆn =(cid:80) w[i]h[i]d[i].  Thus in the case of a ReLU non-linearity, we ﬁnd that such a composite layer should be initialized with a zero-mean Gaussian distribution with standard deviation:  (cid:115)  2(cid:80) w[i]h[i]d[i]  .  (1)  σ =  4 RESULTS AND COMPARISONS  To validate our approach, we show that we can replace the ﬁlters used in existing state-of-the-art network architectures with low-rank representations as described above to reduce computational complexity without reducing accuracy. Here we characterize the computational complexity of a CNN using the number of multiply accumulate operations required for a forward pass (which de- pends on the size of the ﬁlters in each convolutional layer as well as the input image size and stride). However, we have observed strong correlation between multiply-accumulate counts and runtime for both CPU and GPU implementations of the networks described here (as shown in Appendix B, Fig. 6). Note that the Caffe timings differ more for the initial convolutional layers where the input sizes are much smaller (3-channels) as BLAS is less efﬁcient for the relatively small matrices being multiplied.  Methodology. We augment our training set with randomly cropped and mirrored images, but do not use any scale or photometric augmentation, or over-sampling. This allows us to compare the efﬁciency of different network architectures without having to factor in the computational cost of the various augmentation methods used elsewhere. During training, for every model except GoogLeNet, we adjust the learning rate according to the schedule γt = γ0(1 + γ0λt)−1, where γ0, γt and λ are the initial learning rate, learning rate at iteration t, and weight decay respectively (Bottou, 2012).  5  Published as a conference paper at ICLR 2016  When the validation accuracy levels off we manually reduce the learning rate by further factors of 10 until the validation accuracy no longer increases. Unless otherwise indicated, aside from changing the standard deviation of the normally distributed weight initialization, as explained in §3, we used the standard hyper-parameters for each given model. Our results use no test-time augmentation.  4.1 VGG-11 ARCHITECTURES FOR ILSVRC OBJECT CLASSIFICATION AND MIT PLACES  SCENE CLASSIFICATION  We evaluated classiﬁcation accuracy of the VGG-11 based architectures using two datasets, Ima- geNet Large Scale Visual Recognition Challenge 2012 (‘ILSVRC’) and MIT Places. The ILSVRC dataset comprises 1.2M training images of 1000 object classes, commonly evaluated by top-1 and top-5 accuracy on the 50K image validation set. The MIT Places dataset comprises 2.4M training images from 205 scene classes, evaluated with top-1 and top-5 accuracy on the 20K image validation set. VGG-11 (‘VGG-A’) is an 11-layer convolutional network introduced by Simonyan & Zisserman (2014). It is in the same family of network architectures used by Simonyan & Zisserman (2014); He et al. (2015) to obtain the state-of-the-art accuracy for ILSVRC, but uses fewer convolutional layers and therefore ﬁts on a single GPU during training. During training of our VGG-11 based models, we used the standard hyperparameters as detailed by Simonyan & Zisserman (2014) and the initialization of He et al. (2015). In what follows, we compare the accuracy of a number of different network architectures detailed in Appendix E, Table 6. Results for ILSVRC are given in Table 1, and plotted in Fig. 4. Results for MIT Places are given in Table 2, and plotted in Fig. 9.  Baseline (Global Max Pooling). Compared to the version of the network described in (Simonyan & Zisserman, 2014), we use a variant that replaces the ﬁnal 2 × 2 max pooling layer before the ﬁrst fully connected layer with a global max pooling operation, similar to the global average pooling used by Lin et al. (2013); Szegedy et al. (2014). We evaluated the accuracy of the baseline VGG-11 network with global max-pooling (vgg-gmp) and without (vgg-11) on the two datasets. We trained these networks at stride 1 on the ILSVRC dataset and at stride 2 on the larger MIT Places dataset. This globally max-pooled variant of VGG-11 uses over 75% fewer parameters than the original network and gives consistently better accuracy – almost 3 percentage points lower top-5 error on ILSVRC than the baseline VGG-11 network on ILSVRC (see Table 1). We used this network as the baseline for the rest of our experiments. Separable Filters. To evaluate the separable ﬁlter approach described in §2.2 (illustrated in Fig. 2b), we replaced each convolutional layer in VGG-11 with a sequence of two layers, the ﬁrst containing horizontally oriented 1 × 3 ﬁlters and the second containing vertically oriented 3 × 1 ﬁl- ters (vgg-gmp-sf). These ﬁlters applied in sequence represent 3× 3 kernels using a low dimensional basis space. Unlike Jaderberg et al. (2014), we trained this network from scratch instead of ap- proximating the full-rank ﬁlters in a previously trained network. Compared to the original VGG-11 network, the separable ﬁlter version requires approximately 14% less compute. Results are shown in Table 1 for ILSVRC and Table 2 for MIT Places. Accuracy for this network is approx. 0.8% lower than that of the baseline vgg-11-gmp network for ILSVRC and broadly comparable for MIT Places. This approach does not give such a signiﬁcant reduction in computational complexity as what follows, but it is nonetheless interesting that separable ﬁlters are capable of achieving quite high classiﬁcation accuracy on such challenging tasks.  Simple Horizontal/Vertical Basis. To demonstrate the efﬁcacy of the simple low rank ﬁlter rep- resentation illustrated in Fig. 2c, we created a new network architecture (vgg-gmp-lr-join) by re- placing each of the convolutional layers in VGG-11 (original ﬁlter dimensions were 3 × 3) with a sequence of two layers. The ﬁrst layer comprises half 1×3 ﬁlters and half 3×1 ﬁlters whilst the sec- ond layer comprises the same number of 1 × 1 ﬁlters. The resulting network is approximately 49% faster than the original and yet it gives broadly comparable accuracy (within 1 percentage point) for both the ILSVRC and MIT Places datasets.  Full-Rank Mixture. An interesting question concerns the impact on accuracy of combining a small proportion of 33 ﬁlters with the 13 and 31 lters used in vgg-gmp-lr-join. To answer this ques-  6  Published as a conference paper at ICLR 2016  tion, we trained a network, vgg-gmp-lr-join-wfull, with a mixture of 25% 3 × 3 and 75% 1 × 3 and 3 × 1 ﬁlters, while preserving the total number of ﬁlters of the baseline network (as illus- trated in Fig. 2d). This network was signiﬁcantly more accurate than both ‘vgg-gmp-lr-join’ and the baseline, with a top-5 center crop accuracy of 89.7% on ILSVRC, with a computational savings of approx. 16% over our baseline. We note that the accuracy is approx. 1 percentage point higher than GoogLeNet.  Implicitly Learned Combinations. In addition, we try a network similar to vgg-gmp-lr-join but without the 1×1 convolutional layer (as shown in Fig. 2c) used to sum the contributions of 3×1 and 1×3 ﬁlters (vgg-gmp-lr). Interestingly, because of the elimination of the extra 1×1 layers, this gives an additional compute saving such that this model is is only 1/3rd of the compute of our baseline, with no reduction in accuracy. This seems to be a consequence of the fact that the subsequent convolutional layer is itself capable of learning effective combinations of ﬁlter responses even after the intermediate ReLU non-linearity. We also trained such a network with double the number of convolutional ﬁlters (vgg-gmp-lr-2x), i.e. with an equal number of 1 × 3 and 3 × 1 ﬁlters, or 2c ﬁlters as shown in Fig. 2c. We found this to increase accuracy further (88.9% Top-5 on ILSVRC) while still being approximately 58% faster than our baseline network.  Low-Dimensional Embeddings. We attempted to reduce the computational complexity of our ‘gmp-lr’ network further in the vgg-gmp-lr-lde network by using a stride of 2 in the ﬁrst convolu- tional layer, and adding low-dimensional embeddings, as in Lin et al. (2013); Szegedy et al. (2014). We reduced the number of output channels by half after each convolutional layer using 1× 1 convo- lutional layers, as detailed in Appendix E, Table 6. While this reduces computation signiﬁcantly, by approx. 86% compared to our baseline, we saw a decrease in top-5 accuracy on ILSVRC of 1.2 per- centage points. We do note however, that this network remains 2.5 percentage points more accurate than the original VGG-11 network, but is 87% faster.  Network vgg-11 gmp gmp-sf gmp-lr-join-wfull gmp-lr-join gmp-lr-2x gmp-lr gmp-lr-lde  Stride Multiple-Acc. ×109 7.61 7.51 6.53 6.34 3.85 3.14 2.52 1.02  1 1 1 1 1 1 1 2  Param. ×107 13.29 3.22 2.97 3.72 2.73 3.13 2.61 2.64  Top-1 Acc. Top-5 Acc. 0.862 0.887 0.879 0.897 0.880 0.889 0.880 0.875  0.649 0.685 0.673 0.704 0.675 0.693 0.676 0.667  Table 1: VGG ILSVRC Results. Accuracy, multiply-accumulate count, and number of parameters for the baseline VGG-11 network (both with and without global max pooling) and more efﬁcient versions created by the methods described in this paper.  Network gmp gmp-sf gmp-lr-join gmp-lr  Stride Multiple-Acc. ×108 18.77 16.57 9.64 6.30  2 2 2 2  Param. ×107 3.22 13.03 2.73 2.61  Top-1 Acc. Top-5 Acc. 0.830 0.824 0.821 0.825  0.526 0.517 0.512 0.520  Table 2: MIT Places Results. Accuracy, multiply-accumulate operations, and number of parameters for the baseline ‘vgg-11-gmp’ network, separable ﬁlter network as described by Jaderberg et al. (2014), and more efﬁcient models created by the methods described in this paper. All networks were trained at stride 2 for the MIT Places dataset.  7  Published as a conference paper at ICLR 2016  r o r r E 5 - p o T  14% 13% 12% 11% 10%  Baseline Networks  Our Results  gmp-lr-lde  gmp-lr  gmp-lr-join  gmp-lr-2x  vgg-11  gmp  gmp-sf  gmp-lr-join-wfull  1  2  3 Multiply-Accumulate Operations  4  5  6  7  8·109  Figure 4: VGG ILSVRC Results. Multiply-accumulate operations v.s. top-5 error for VGG-derived models on ILSVRC object classiﬁcation dataset, the most efﬁcient networks are closer to the origin. Our models are signiﬁcantly faster than the baseline network, in the case of ‘gmp-lr-2x’ by a factor of almost 60%, while slightly lowering error. Note that the ‘gmp-lr’ and ‘gmp-lr-join’ networks have the same accuracy, showing that an explicit linear combination layer may be unnecessary.  4.2 GOOGLENET FOR ILSVRC OBJECT CLASSIFICATION GoogLeNet, introduced by Szegedy et al. (2014), is the most efﬁcient network for ILSVRC, getting close to state-of-the-art results with a fraction of the compute and model size of even VGG-11. The GoogLeNet inception module is a composite layer of 5 homogeneously-shaped ﬁlters, 1 × 1, 3 × 3, 5 × 5, and the output of a 3x3 average pooling operations. All of these are concatenated and used as input for successive layers. For the googlenet-lr network, within only the inception modules we replaced each the 3 × 3 ﬁlters with low-rank 3 × 1 and 1 × 3 ﬁlters, and replaced the layer of 5 × 5 ﬁlters with a set of low-rank 5 × 1 and 1 × 5 ﬁlters. For the googlenet-lr-conv1 network, we similarly replaced the ﬁrst and second layer convolutional layers with 7 × 1 / 1 × 7 and 3 × 1 / 1 × 3 layers respectively. Results are shown in Table 3. Due to the intermediate losses used for training, which contain the only fully-connected layers in GoogLeNet, test time model size is signiﬁcantly smaller than training time model size. Table 3 also reports test time model size. The low-rank network delivers comparable classiﬁcation accuracy using 26% less compute. No other networks produce comparable accuracy within an order of magnitude of compute. We note that although the Caffe pre-trained GoogLeNet model (Jia et al., 2014) has a top-5 accuracy of 0.889, our training of the same network using the given model deﬁnition, including the hyper-parameters and training schedule, but a different random initialization had a top-5 accuracy of 0.883.  Network GoogLeNet lr lr-conv1  Multiple-Acc. ×109 1.59 1.18 0.84  Test Param. ×106 5.97 3.50 3.42  Top-1 Acc. Top-5 Acc. 0.883 0.880 0.870  0.677 0.673 0.659  Table 3: GoogLeNet ILSVRC Results. Accuracy, multiply-accumulate count, and number of pa- rameters for the baseline GoogLeNet network and more efﬁcient versions created by the methods described in this paper.  4.3 NETWORK-IN-NETWORK FOR CIFAR-10 OBJECT CLASSIFICATION The CIFAR-10 dataset consists of 60,000 32 × 32 images in 10 classes, with 6000 images per class. This is split into standard sets of 50,000 training images, and 10,000 test images (Krizhevsky, 2009). As a baseline for the CIFAR-10 dataset, we used the Network in Network architecture (Lin et al., 2013), which has a published test-set error of 8.81%. We also used random crops during training, with which the network has an error of 8.1%. Like most state of the art CIFAR results, this was with ZCA pre-processed training and test data (Goodfellow et al., 2013), training time mirror augmentation and random sub-crops. The results of our CIFAR experiments are listed in Table 4 and plotted in Fig. 11.  8  Published as a conference paper at ICLR 2016  Network Multiple-Acc. ×108 NiN 1.93 nin-c3 1.43 nin-c3-lr 1.04  Param. ×105 Accuracy 0.9188 0.9186 0.9178  9.67 7.74 4.38  Table 4: Network-in-Network CIFAR-10 Results. Accuracy, multiply-accumulate operations, and number of parameters for the baseline Network-in-Network model and more efﬁcient versions created by the methods described in this paper.  This architecture uses 5 × 5 ﬁlters in some layers. We found that we could replace all of these with 3 × 3 ﬁlters, with comparable accuracy. As suggested by Simonyan & Zisserman (2014), stacked 3 × 3 ﬁlters have the effective receptive ﬁeld of larger ﬁlters with less computational complexity. In this nin-c3 network, we replaced the ﬁrst convolutional layer with one 3 × 3 layer, and the second convolutional layer with two 3 × 3 layers. This network is 26% faster than the standard NiN model, with only 54% of the model parameters. Using our low-rank ﬁlters in this network, we trained the nin-c3-lr network, which is of similar accuracy (91.8% v.s. 91.9%) but is approximately 54% of the original network’s computational complexity, with only 45% of the model parameters.  5 DISCUSSION  It is somewhat surprising that networks based on learning ﬁlters with less representational ability are able to do as well, or better, than CNNs with full k × k ﬁlters on the task of image classiﬁcation. However, a lot of interesting small-scale image structure is well-characterized by low-rank ﬁlters, e.g. edges and gradients. Our experiments training a separable (rank-1) model (‘vgg-gmp-sf’) on ILSVRC and MIT Places show surprisingly high accuracy on what are considered challenging prob- lems – approx. 88% top-5 accuracy on ILSVRC – but not enough to obtain comparable accuracies to the models on which they are based. Given that most discriminative ﬁlters learned for image classiﬁcation appear to be low-rank, we instead structure our architectures with a set of basis ﬁlters in the way illustrated in Fig. 2d. This allows our networks to learn the most effective combinations of complex (e.g. k×k) and simple (e.g. 1 × k, k × 1) ﬁlters. Furthermore, in restricting how many complex spatial ﬁlters may be learned, this architecture prevents over-ﬁtting, and helps improve generalization. Even in our models where we do not use square k × k ﬁlters, we obtain comparable accuracies to the baseline model, since the rank-2 cross-shaped ﬁlters effectively learned as a combination of 3× 1 and 1× 3 ﬁlters are capable of representing more complex local pixel relations than rank-1 ﬁlters.  6 CONCLUSION  This paper has presented a method to train convolutional neural networks from scratch using low- rank ﬁlters. This is made possible by a new way of initializing the networks weights which takes into consideration the presence of differently shape ﬁlters in composite layers. Validation on image classiﬁcation in three popular datasets conﬁrms similar or higher accuracy than state of the art, with much greater computational efﬁciency. Recent advances in state-of-the-art accuracy with CNNs for image classiﬁcation have come at the cost of increasingly large and computational complex models. We believe our results to show that learning computationally efﬁcient models with fewer, more relevant parameters, can prevent over- ﬁtting, increase generalization and thus also increase accuracy.  FUTURE WORK This paper has addressed the spatial extents of convolutional ﬁlters in CNNs, however the channel extents also exhibit some redundancy, as highlighted by Jaderberg et al. (2014), and exploited in the form of low-dimensional embeddings by Lin et al. (2013); Szegedy et al. (2014). We intend to further explore how our methods can be extended to learn and combine even smaller basis ﬁlters and ﬁlters with more diverse shapes.  9  Published as a conference paper at ICLR 2016  REFERENCES  Bottou, L´eon. Stochastic gradient descent tricks.  Springer, 2012.  In Neural Networks: Tricks of the Trade, pp. 421–436.  Denil, Misha, Shakibi, Babak, Dinh, Laurent, de Freitas, Nando, et al. Predicting parameters in deep learning.  In Advances in Neural Information Processing Systems, pp. 2148–2156, 2013.  Fukushima, Kunihiko. Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern  Recognition Unaffected by Shift in Position. Biological Cybernetics, 36:193–202, 1980.  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural networks.  In International conference on artiﬁcial intelligence and statistics, pp. 249–256, 2010.  Goodfellow, Ian, Warde-farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout Net- works. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 1319– 1327, 2013.  Gupta, Suyog, Agrawal, Ankur, Gopalakrishnan, Kailash, and Narayanan, Pritish. Deep Learning with Limited  Numerical Precision, February 2015.  He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and 0001, Jian Sun. Delving Deep into Rectiﬁers: Surpassing  Human-Level Performance on ImageNet Classiﬁcation. CoRR, abs/1502.01852, 2015.  Hochreiter, Sepp, Bengio, Yoshua, Frasconi, Paolo, and Schmidhuber, J¨urgen. Gradient Flow in Recurrent  Nets: the Difﬁculty of Learning Long-Term Dependencies, 2001.  Jaderberg, Max, Vedaldi, Andrea, and Zisserman, Andrew. Speeding up Convolutional Neural Networks with  Low Rank Expansions. CoRR, abs/1405.3866, 2014.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093, 2014.  Krizhevsky, Alex. Learning Multiple Layers of Features from Tiny Images. Technical report, University of  Toronto, 2009.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. ImageNet Classiﬁcation with Deep Convolutional Neural Networks. In Pereira, F., Burges, C.J.C., Bottou, L., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 25, pp. 1097–1105. Curran Associates, Inc., 2012.  LeCun, Yann, Denker, John S, Solla, Sara A, Howard, Richard E, and Jackel, Lawrence D. Optimal brain  damage. In NIPs, volume 89, 1989.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-Based Learning Applied to Doc-  ument Recognition. In Proceedings of the IEEE, volume 86, pp. 2278–2324, 1998.  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network In Network. CoRR, abs/1312.4400, 2013. Mamalet, Franck and Garcia, Christophe. Simplifying convnets for fast learning. In Artiﬁcial Neural Networks  and Machine Learning–ICANN 2012, pp. 58–65. Springer, 2012.  Ren, Shaoqing, He, Kaiming, Girshick, Ross, Zhang, Xiangyu, and Sun, Jian. Object Detection Networks on  Convolutional Feature Maps. arXiv preprint arXiv:1504.06066, 2015.  Rigamonti, Roberto, Sironi, Amos, Lepetit, Vincent, and Fua, Pascal. Learning Separable Filters. In CVPR,  pp. 2754–2761. IEEE, 2013.  Simonyan, Karen and Zisserman, Andrew. Very Deep Convolutional Networks for Large-Scale Image Recog-  nition. CoRR, abs/1409.1556, 2014.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going Deeper with Convolutions. CoRR, abs/1409.4842, 2014.  Tompson, Jonathan, Goroshin, Ross, Jain, Arjun, LeCun, Yann, and Bregler, Christoph. Efﬁcient Object Lo-  calization Using Convolutional Networks. June 2015.  Vanhoucke, Vincent, Senior, Andrew, and Mao, Mark Z. Improving the speed of neural networks on CPUs. In  Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, volume 1, 2011.  Xu, Li, Ren, Jimmy S, Liu, Ce, and Jia, Jiaya. Deep convolutional neural network for image deconvolution. In  Advances in Neural Information Processing Systems, pp. 1790–1798, 2014.  10  Published as a conference paper at ICLR 2016  APPENDICES A INITIALIZING CNNS WITH MIXED-SHAPE LOW-RANK FILTERS  At the start of training, network weights are initialized at random using samples drawn from a Gaussian distribution with a standard deviation parameter speciﬁed separately for each layer. We found that the setting of these parameters was critical to the success of network training and difﬁcult to get right, particularly because published parameter settings used elsewhere were not suitable for our new network architectures. With unsuitable weight initialization, training may fail due to exploding gradients, where back propagated gradients grow so large as to cause numeric overﬂow, or vanishing gradients where back propagated gradients grow so small that their effect is dwarfed by that of weight decay such that loss does not decrease during training (Hochreiter et al., 2001). To determine the standard deviations to be used for weight initialization, we use an approach similar to that described by Glorot & Bengio (2010) (with the adaptation described by He et al. (2015) for layers followed by a ReLU). Their approach works by ensuring that the magnitudes of back- propagated gradients remain approximately the same throughout the network. Otherwise, if the gradients were inappropriately scaled by some factor (e.g. β) then the ﬁnal back-propagated signal would be scaled by a potentially much larger factor (βL after L layers). In what follows, we adopt notation similar to that of He et al. (2015), and follow their derivation of the appropriate standard deviation for weight initialization. However, we also generalize their approach to the initialization of ‘composite’ layers comprising several groups of ﬁlters of different spatial dimensions (see Fig. 5). This is one of the main contributions of this work.  Forward Propagation. The response of the lth convolutional layer can be represented as  yl = Wlxl + bl.  (2) Here yl is a d × 1 vector representing a pixel in the output feature map and xl is a whc × 1 vector that represents a w × h subregion of the c-channel input feature map. Wl is the d × n weight matrix, where d is the number of ﬁlters and n is the size of a ﬁlter, i.e. n = whc for a ﬁlter with spatial dimensions w× h operating on an input feature map of c channels, and bl is the bias. Finally xl = f (yl−1) is the output of the previous layer passed through an activation function f (e.g. the application of a ReLU to each element of yl−1).  Backward Propagation. During back-propagation, the gradient of a convolutional layer is com- puted as  ∆xl = ˆWl∆yl,  (3) where ∆xl and ∆yl denote the derivatives of loss L with respect to input and output pixels. ∆xl is a c × 1 vector of gradients with respect to the channels of a single pixel in the input feature map and ∆y represents h × w pixels in d channels of the output feature map. ˆWl is a c × ˆn matrix where the ﬁlter weights are arranged in the right order for back-propagation, and ˆn = whd. Note that ˆWl can be simply reshaped from W(cid:62) l . Also note that the elements of ∆yl correspond to pixels in the output image that had a forwards dependency on the input image pixel corresponding to ∆x. In back propagation, each element ∆yl of ∆yl is related to an element ∆xl+1 of some ∆xl+1 (i.e. a back-propagated gradient in the next layer) by the derivative of the activation function f:  ∆yl = f(cid:48)(yl)∆xl+1,  (4)  where f(cid:48) is the derivative of the activation function.  Weight Initialization. Now let ∆yl, ∆xl and wl be scalar random variables that describe the distribution of elements in ∆yl, ∆xl and ˆWl respectively. Then, assuming f(cid:48)(yl) and ∆xl+1 are independent,  (5) For the ReLU case, f(cid:48)(yl) is zero or one with equal probability. Like Glorot & Bengio (2010), we assume that wl and ∆yl are independent of each other. Thus, equation 3 implies that ∆xl has zero mean for all l, when wl is initialized by a distribution that is symmetric around zero. Thus we have  E[∆yl] = E[f(cid:48)(yl)]E[∆xl+1].  11  Published as a conference paper at ICLR 2016  Figure 5: A composite layer. Composite layers convolve an input feature map with N groups of convolutional ﬁlters of several different spatial dimensions. Here the ith group has d[i] ﬁlters with spatial dimension w[i] × h[i]. The outputs are concatenated to create a d channel output feature map. Composite layers require careful weight initialization to avoid vanishing/exploding gradients during training.  2 E[∆xl+1] = 0 and also E[(∆yl)2] = Var[∆yl] = 1  2 Var[∆xl+1]. Now, since each E[∆yl] = 1 element of ∆xl is a summation of ˆn products of elements of ˆWl and elements of ∆yl, we can compute the variance of the gradients in equation 3:  Var[∆xl] = ˆnVar[wl]Var[∆yl]  =  1 2  ˆnVar[wl]Var[∆xl+1].  (6)  To avoid scaling the gradients in the convolutional layers (to avoid exploding or vanishing gradients), we set the ratio between these variances to 1:  1 2  ˆnVar[wl] = 1.  (7)  This leads to the result of He et al. (2015), in that a layer with ˆnl connections followed by a ReLU activation function should be initialized with a zero-mean Gaussian distribution with standard devi-  ation(cid:112)2/ˆnl.  Weight Initialisation in Composite Layers. The initialization scheme described above assumes that the layer comprises ﬁlters of spatial dimension w× h. Now we extend this scheme to composite convolutional layers containing N groups of ﬁlters of different spatial dimensions w[i] × h[i] (where superscript [i] denotes the group index and with i ∈ {1, . . . , N}). Now the layer response is the concatenation of the responses of each group of ﬁlters:  As before yl is a d × 1 vector representing the response at one pixel of the output feature map. Now each x[i] is a w[i]h[i]c× 1 vector that represents a different shaped w[i] × h[i] sub-region of the input l × ˆn[i] weight matrix, where d is the number of ﬁlters and ˆn[i] is feature map. Each W[i] l the size of a ﬁlter, i.e. ˆn[i] = w[i]h[i]c[i] for a ﬁlter of spatial dimension w[i] × h[i] operating on an input feature map of cl = dl−1 channels.  is the c[i]  (8)   W[1]  l x[1] W[2] l x[2] . . . W[N ] l x[N ]  l  l  l   + bl.  yl =  12  d1concatd[1]d[2]d[N]…dh[1]w[1]w[2]h[2]w[N]h[N]d[1]filtersd[2]filtersd[N]filterssplitccWH*……**……ccWHWHWHWHPublished as a conference paper at ICLR 2016  s n o i t a r e p O  . c c A - y l p i t l u M  0  ·108  3  2  1  M.A. CPU GPU  600  400  200  ) s  m  (  i  e m T U P C  5  4  3  2  1  ) s  m  (  i  e m T U P G  c o n v 3 x-2  c o n v 3 x-1  c o n v 2 x-1  c o n v 1 y-1  c o n v 3 y-1  c o n v 2 y-1  c o n v 1 x-1 Figure 6: Multiply-Accumulate Operations and Caffe CPU/GPU Timings. For the forward pass of each convolutional layer in the ‘vgg-gmp-lr’ network. Caffe CPU and GPU timings were well correlated with multiply-accumulate operations for most layers.  c o n v 3 y-2  c o n v 4 x-1  c o n v 4 y-1  c o n v 4 x-2  c o n v 4 y-2  c o n v 5 x-1  c o n v 5 y-1  c o n v 5 x-2  c o n v 5 y-2  0  0  During back propagation, the gradient of the composite convolutional layer is computed as a sum- mation of the contributions from each group of ﬁlters:  l ∆y[1]  ∆xl = ˆW[1]  (9) where now ∆y[i] represents w[i] × h[i] pixels in d[i] channels of the output feature map. Each ˆW[i] is a cl × ˆn[i] matrix of weights arranged appropriately for back propagation. Again, note that each ˆW[i]  l can be simply reshaped from W[i]  l + ˆW[2]  l ∆y[N ]  l ∆y[2]  .  ,  l  l  l  l + ··· + ˆW[N ]  As before, each element of ∆yl is a sum over ˆn products between elements of ˆW[i] of ∆y[i]  l and here ˆn is given by:  l and elements  ˆn =  w[i]h[i]d[i].  (10)  (cid:88)  In the case of a ReLU non-linearity, this leads to a zero-mean Gaussian distribution with standard deviation:  (cid:115)  σ =  2(cid:80) w[i]h[i]d[i]  .  (11)  In conclusion, a composite layer of heterogeneously-shaped ﬁlter groups, where each ﬁlter group i has w[i]h[i]d[i] outgoing connections should be initialized as if it is a single layer with ˆn =  (cid:80) w[i]h[i]d[i].  B MULTIPLY-ACCUMULATE OPERATIONS AND CAFFE CPU/GPU TIMINGS.  We have characterized the computational complexity of a CNN using the number of multiply ac- cumulate operations required for a forward pass (which depends on the size of the ﬁlters in each convolutional layer as well as the input image size and stride), to give as close as possible to a hardware and implementation independent evaluation the computational complexity of our method. However, we have observed strong correlation between multiply-accumulate counts and runtime for both CPU and GPU implementations of the networks described here (as shown in Fig. 6). Note that the Caffe timings differ more for the initial convolutional layers where the input sizes are much smaller (3-channels), and BLAS is less efﬁcient for the relatively small matrices being multiplied.  C COMPARING WITH STATE OF THE ART NETWORKS FOR ILSVRC  Figures 7 and 8 compare published top-5 ILSVRC validation error v.s. multiply-accumulate opera- tions and number of model parameters (respectively) for several state-of-the-art networks (Simonyan & Zisserman, 2014; Szegedy et al., 2014; He et al., 2015). The error rates for these networks are only reported as obtained with different combinations of computationally expensive training and  13  Published as a conference paper at ICLR 2016  test time augmentation methods, including scale, photometric, ensembles (multi-model), and multi- view/dense oversampling. This can makes it difﬁcult to compare model architectures, especially with respect to computational requirements. State-of-the-art networks, such as MSRA-C, VGG-19 and oversampled GoogLeNet are orders of magnitude larger in computational complexity than our networks. From Fig. 7, where the multiply- accumulate operations are plotted on a log scale, increasing the model size and/or computational complexity of test-time augmentation of CNNs appears to have diminishing returns for decreasing validation error. Our models without training or test time augmentation show comparable accuracy to networks such as VGG-13 with training and test time augmentation, while having far less com- putational complexity and model size. In particular, the ‘googlenet-lr’ model has a much smaller test-time model size than any network of comparable accuracy.  Network msra-c msra-b msra-a vgg-19 vgg-16 (D) vgg-16 (C) vgg-13 vgg-11 googlenet 10x googlenet 144x  Multiply-Acc. ×109 53.46 23.22 19.06 19.63 15.47 11.77 11.31 7.61 1.59 1.59  Test M.A. w/ Aug. ×109 107.17 46.54 38.20 39.30 30.97 23.57 22.64 15.24 15.91 229.11  Param. ×107 33.06 18.33 17.80 14.37 13.84 13.36 13.30 13.29 1.34 1.34  Top-5 Acc. 0.943 0.937 0.935 0.910 0.912 0.906 0.901 0.895 0.909 0.921  Table 5: State of the Art Single Models with Extra Augmentation. Top-5 ILSVRC validation accuracy, single view and augmented test-time multiply-accumulate (M.A.) count, and number of parameters for various state of the art models with various training and test-time augmentation meth- ods. A multi-model ensemble of MSRA-C is the current state of the art network.  D PLOTS OF RESULTS  Following are several plots of results, which for reasons of space consideration are not in the main section of the paper. These include the results for VGG-derived models on MIT Places (Fig. 9), GoogLeNet-derived models on ILSVRC (Fig. 10), and ﬁnally the results for Network-in-Network- derived models on CIFAR-10 (Fig. 11).  E VGG-DERIVED MODEL TABLE Table 6 shows the architectual details of the VGG-11 derived models used in §4.1.  14  Published as a conference paper at ICLR 2016  14%  13%  12%  11%  10%  9%  8%  7%  6%  r o r r E 5 - p o T  vgg-11  vgg-gmp-lr-lde googlenet-lr  googlenet  vgg-gmp-lr-64  vgg-gmp-lr-join-wfull  vgg-11  vgg-13 vgg-16 (C)  googlenet 10x  vgg-19  vgg-16 (D)  msra-a  msra-b  googlenet 144x  1010  log10(Multiply-Accumulate Operations)  msra-c  1011  Our Results  Crop & Mirror Aug. Extra Augmentation  109  Figure 7: Computational Complexity of Single State-of-the-Art ILSVRC Models. Test-time multiply-accumulate operations v.s. top-5 error on state of the art networks using a single model. Note the difference in accuracy and computational complexity for VGG-11 model with/without extra augmentation. Our ‘vgg-gmp-lr-join-wfull’ model without extra augmentation is more accurate than VGG-11 with extra augmentation, and is much less computationally complex.  14%  13%  12%  11%  10%  9%  8%  7%  6%  r o r r E 5 - p o T  googlenet-lr  googlenet  vgg-gmp-lr-lde  vgg-gmp-lr-64  vgg-gmp-lr-join-wfull  googlenet 10x  googlenet 144x  vgg-11  vgg-11  vgg-13 vgg-16 (C)  vgg-19 vgg-16 (D)  Our Results  Crop & Mirror Aug. Extra Augmentation  msra-a msra-b  msra-c  106  107 108 log10(Number of Parameters)  109  Figure 8: Number of Parameters of State-of-the-Art ILSVRC Models. Test time parameters v.s. top-5 error for state of the art models. The main factor in reduced model size is the use of global pooling or lack of fully-connected layers. Note that our ‘googlenet-lr’ model is almost an order of magnitude smaller than any other network of comparable accuracy.  15  Published as a conference paper at ICLR 2016  20%  19%  18%  17%  r o r r E 5 - p o T  16%  0.4  gmp-lr  gmp-lr-join  Baseline Networks  Our Results  gmp-sf  gmp  0.6  0.8  1  1.2  1.4  1.6  1.8  Multiply-Accumulate Operations  2 ·109  Figure 9: MIT Places Results. Multiply-accumulate operations v.s. top-5 error for VGG-derived models on MIT Places scene classiﬁcation dataset.  r o r r E 5 - p o T  14% 13% 12% 11% 10%  lr-conv1  Baseline  Our Results  lr  GoogLeNet  0.8  0.9  1  1.1  1.2  1.3  1.4  1.5  Multiply-Accumulate Operations  1.6 ·109  Figure 10: GoogLeNet ILSVRC Results. Multiply-accumulate operations v.s. top-5 error for GoogLeNet-derived models on ILSVRC object classiﬁcation dataset.  r o r r E  9%  8%  7%  Baseline Networks  Our Results  nin-c3-lr  nin-c3  NiN  1  1.1  1.2  1.4  1.3 Multiply-Accumulate Operations  1.5  1.6  1.7  1.8  1.9  2·108  Figure 11: Network-in-Network CIFAR-10 Results. Multiply-accumulate operations v.s. error for Network-in-Network derived models on CIFAR-10 object classiﬁcation dataset.  16  Layer conv1  VGG-11  3×3, 64  GMP GMP-SF 1×3, 64 3×1, 64  GMP-LR  3×1, 32 (cid:107) 1×3, 32  GMP-LR-2X  3×1, 64 (cid:107) 1×3, 64  GMP-LR-JOIN GMP-LR-LDE  3×1, 32 (cid:107) 1×3, 32  1×1, 64  1×1, 32  1 7  1×3, 128 3×1, 128  1×3, 256 3×1, 256  1×3, 256 3×1, 256  1×3, 512 3×1, 512  1×3, 512 3×1, 512  1×3, 512 3×1, 512  1×3, 512 3×1, 512  conv2  3×3, 128  3×3, 256  3×3, 256  3×3, 512  3×3, 512  3×3, 512  3×3, 512  2×2 maxpool, /2 72 × 512 × 4096  conv3  conv4  conv5  fc6  fc7  fc8  3×1, 64 (cid:107) 1×3, 64  3×1, 128 (cid:107) 1×3, 128  ReLU  2×2 maxpool, /2  3×1, 128 (cid:107) 1×3, 128  ReLU  2×2 maxpool, /2  3×1, 256 (cid:107) 1×3, 256  3×1, 128 (cid:107) 1×3, 128  ReLU  3×1, 256 (cid:107) 1×3, 256  3×1, 256 (cid:107) 1×3, 256  3×1, 512 (cid:107) 1×3, 512  ReLU  2×2 maxpool, /2  3×1, 256 (cid:107) 1×3, 256  ReLU  3×1, 512 (cid:107) 1×3, 512  3×1, 256 (cid:107) 1×3, 256  3×1, 512 (cid:107) 1×3, 512  ReLU  2×2 maxpool, /2  3×1, 256 (cid:107) 1×3, 256  ReLU  3×1, 512 (cid:107) 1×3, 512  3×1, 64 (cid:107) 1×3, 64  1×1, 128  1×1, 64  3×1, 128 (cid:107) 1×3, 128  1×1, 256  1×1, 128  3×1, 128 (cid:107) 1×3, 128  1×1, 256  1×1, 128  3×1, 256 (cid:107) 1×3, 256  1×1, 512  1×1, 256  3×1, 256 (cid:107) 1×3, 256  1×1, 512  1×1, 256  3×1, 256 (cid:107) 1×3, 256  1×1, 512  1×1, 256  3×1, 256 (cid:107) 1×3, 256  1×1, 512  1×1, 256  ReLU  global maxpool 512 × 4096  ReLU  4096 × 4096  ReLU  4096 × 1000  softmax  P u b l i s h e d  a s  a  c o n f e r e n c e  p a p e r  a t  I  C L R 2 0 1 6  GMP-LR-JOIN-WFULL 3×1, 24 (cid:107) 1×3, 24 (cid:107) 3×3, 16  1×1, 64  3×1, 48 (cid:107) 1×3, 48 (cid:107) 3×3, 32  1×1, 128  3×1, 96 (cid:107) 1×3, 96 (cid:107) 3×3, 64  1×1, 256  3×1, 96 (cid:107) 1×3, 96 (cid:107) 3×3, 64  1×1, 256  3×1, 192 (cid:107) 1×3, 192 (cid:107) 3×3, 128  1×1, 512  3×1, 192 (cid:107) 1×3, 192 (cid:107) 3×3, 128  1×1, 512  3×1, 192 (cid:107) 1×3, 192 (cid:107) 3×3, 128  1×1, 512  3×1, 192 (cid:107) 1×3, 192 (cid:107) 3×3, 128  1×1, 512  Table 6: VGG Model Architectures. Here “3×3, 32” denotes 32 3×3 ﬁlters, “/2” denotes stride 2, fc denotes fully-connected, and (cid:107) denotes a concatenation within a composite layer.  ",
1511.06068,2016,Reducing Overfitting in Deep Networks by Decorrelating Representations,"['Reducing Overfitting in Deep Networks by Decorrelating Representations\nMichael Cogswell', 'Faruk Ahmed', 'Ross Girshick', 'Larry Zitnick', 'Dhruv Batra']",https://arxiv.org/pdf/1511.06068,"6 1 0 2     n u J    0 1      ]  G L . s c [      4 v 8 6 0 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  REDUCING OVERFITTING IN DEEP NETWORKS BY DECORRELATING REPRESENTATIONS  Michael Cogswell Virginia Tech Blacksburg, VA cogswell@vt.edu  Faruk Ahmed Université de Montréal Montréal, Quebec, Canada faruk.ahmed@umontreal.ca  Ross Girshick Facebook AI Research (FAIR) Seattle, WA rbg@fb.com  Larry Zitnick Microsoft Research Seattle, WA larryz@microsoft.com  Dhruv Batra Virginia Tech Blacksburg, VA dbatra@vt.edu  ABSTRACT  One major challenge in training Deep Neural Networks is preventing overﬁt- ting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overﬁtting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to signiﬁcantly reduced overﬁtting (as indicated by the difference between train and val performance), and better generalization. Our regularizer en- courages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overﬁtting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.  1  INTRODUCTION  Deep Neural Networks (DNNs) have recently achieved remarkable success on a wide range of tasks – e.g., image classiﬁcation on ImageNet (Krizhevsky et al., 2012), scene recognition on MIT Places (Zhou et al., 2014), image captioning with MS COCO (Lin et al., 2014b; Vinyals et al., 2015; Chen & Zitnick, 2015), and visual question answering (Antol et al., 2015). One signiﬁcant reason for improvement of these methods over their predecessors has to do with scale. Faster computers coupled with optimization improvements such Batch Normalization, Adaptive SGD, and ReLus let us quickly train wider and deep networks. Access to large annotated datasets and regularizers such as Dropout has provided signiﬁcant reduction in the amount of overﬁtting in these large networks, thus enabling the performance we see today. In this paper, we focus on the problem of overﬁtting, which is observed when a high capacity model (such as a DNN) performs very well on training data but poorly on held out data. Even when trained on large annotated datasets (such as ImageNet (Deng et al., 2009) or Places (Zhou et al., 2014), containing millions of labelled images), deep networks are susceptible to overﬁtting. This problem is further exacerbated when moving to new domains and tasks – since DNNs tend not to generalize with a few examples, each new task tends to require curating and annotating a new large dataset. While there has been some success with transfer learning (Girshick et al., 2014; Donahue et al., 2014; Yosinski et al., 2014), networks still overﬁt. A promising alternative to creating even larger datasets is to apply different forms of regularization to the network while training to avoid overﬁtting. These methods include regularizing the norm of the weights (Tikhonov, 1943), Lasso (Tibshirani, 1996), Dropout (Srivastava et al., 2014), Drop- Connect (Wan et al., 2013), Maxout (Goodfellow et al., 2013), etc. One particular regularizer of interest to DNNs is Dropout (Srivastava et al., 2014), which attempts to prevent co-adaptation of neuron activations. Co-adaptation occurs when two or more hidden units rely on one another to perform some function which helps ﬁt training data, thus becoming highly  1  Published as a conference paper at ICLR 2016  correlated. Co-adaptation is reduced by Dropout using an approximate model averaging technique that sets a randomly selected set of activations to zero at training time. Srivastava et al. (2014) show that this has a regularizing effect, leading to increased generalization and sparser, less correlated features. Notice that this is without explicitly encouraging decorrelation in hidden activations. To further investigate the relationship between hidden activation correlations and overﬁtting, we show in Fig. 1 two quantities from a CNN trained for image classiﬁcation on CIFAR100 (Krizhevsky & Hinton, 2009) – (1) the amount of overﬁtting in the model (as measured by the gap between train and val accuracy), and (2) the amount of correlation in hidden activations (as measured by the Frobenius norm of the sample cross-covariance matrix computed from vectors of hidden activations; details in Section 2). Both these quantities of interest are reported as a function of amount of training data (x-axis) and with/without Dropout (left/right subplot). As expected, both increased training data and Dropout have a regularizing effect and lead to reduced overﬁtting. The ﬁgure also shows an interesting novel trend – as the amount of overﬁtting reduces, so does the degree of correlation in hidden activations. In essence, overﬁtting and co-adaptation seem to be correlated. The open question of course is – is the relationship causal?  (a) Without Dropout  (b) With Dropout  Figure 1: Two principal ways to prevent overﬁtting in deep models are to train with more data (x axis) and to train with Dropout (right plot). As expected, both of these decrease validation error (left axis), but they also hap- pen to decrease hidden activation cross-covariance (right axis). We investigate whether explicitly minimizing cross-covariance can lead to reduced overﬁtting.  This leads to the principal questions of this paper – Is it possible to bias networks towards decorre- lated representations by directly reducing correlation between hidden units? And do such decorre- lated representations generalize better? Overview and Contributions. The goal of this paper is to learn DNNs with decorrelated activations and study the effect of this decorrelation on their generalization performance. Towards this end, we propose a fairly natural loss called DeCov, which explicitly encourages decorrelation between the activations in a deep neural network. This loss requires no additional supervision, so it can be added to any existing network. In addition to the link discussed above, our motivation also comes from the classical literature on bagging and ensemble averaging (Hansen & Salamon, 1990; Perrone & Cooper, 1993; Breiman, 1996), which suggests that decorrelated ensembles perform better than correlated ones. Our experiments encompass a range of datasets (MNIST (LeCun et al., 1995), CI- FAR10/100 (Krizhevsky & Hinton, 2009), ImageNet (Deng et al., 2009)), and different kinds of network architectures (Caffe implementations of LeNet (LeCun et al., 1995), AlexNet (Krizhevsky et al., 2012), and Network in Network (Lin et al., 2014a)). All cases suggest that DeCov acts as a novel and useful regularizer. 2 APPROACH: DECO V LOSS To express our notion of redundant or co-adapated features, we impose a loss on the activations of a chosen hidden layer. In a manner similar to Dropout, our proposed Decov loss may be applied to a single layer or multiple layers in a network. For simplicity, let us focus on a single layer. Let hn ∈ Rd denote the activations at the chosen hidden layer, where n ∈ {1, . . . , N} indexes one example from a batch of size N. The covariances between all pairs of activations i and j form a matrix C:  Ci,j =  1 N  i − µi)(hn  j − µj)  (hn  (1)  where µi = 1 N  n hn  i is the sample mean of activation i over the batch.  (cid:80)  (cid:88)  n  2  500010000150002000025000300003500040000NumberofTrainingExamples0.600.650.700.750.80TrainAccuracy-ValidationAccuracy2e+072e+072e+073e+074e+074e+074e+075e+076e+076e+07TotalCross-Covariance(DeCov)OverﬁttingandCross-CovariancewithoutDropoutTrain-ValGap(overﬁtting)Cross-Covariance500010000150002000025000300003500040000NumberofTrainingExamples0.450.500.550.600.650.700.750.80TrainAccuracy-ValidationAccuracy4e+046e+048e+041e+051e+051e+052e+052e+052e+05TotalCross-Covariance(DeCov)OverﬁttingandCross-CovariancewithDropoutTrain-ValGap(overﬁtting)Cross-CovariancePublished as a conference paper at ICLR 2016  2  (cid:1)  (cid:0)(cid:107)C(cid:107)2  F − (cid:107)diag(C)(cid:107)2  We want to minimize covariance between different features, which corresponds to penalizing the norm of C. However, the diagonal of C contains the variance of each hidden activation and we have no reason to require the dynamic range of activations to be small, so we subtract this term from the matrix norm to get our ﬁnal DeCov loss LDeCov = 1 2  (2) where (cid:107) · (cid:107)F is the frobenius norm, and the diag(·) operator extracts the main diagonal of a matrix into a vector. In our experiments, subtracting the diagonal made little difference for small networks, but led to increased stability for larger networks. Perhaps the best quality of this loss is that it requires no supervision, so it can be added to any set of activations. In a manner similar to Dropout, our experiments typically apply Decov loss to fully connected layers towards the deep end of a network (e.g., fc6 and fc7 for AlexNet). However, note that Decov affects all parameters up to the layer where it is applied (and not just the parameters in the speciﬁc layer). At ﬁrst glance, one seeming peculiarity about this loss is that its global minimum can be found by setting all weights for h to 0. This is similar to an L2 regularizer in that both encourage weights to tend toward 0, but one important difference between these two regularizers is that LDeCov depends on input data and is not a function purely of a weight vector like one might ﬁnd in a classical regularizer such as L2 or L1. To understand this further, consider the gradient of the loss with respect to a particular activation a for a particular example m ∂LDeCov ∂hm a  a − µa)(hn  j − µj)  (cid:88)  (cid:88)  (cid:34)  (cid:35)  1 N  1 N  (hn  (3)  =  j(cid:54)=a  n  Let us denote the rightmost term in this expression by I(j, m) = (hm This term is large (in absolute value) when feature j is discriminative for example m w.r.t. the mean of the batch. If j were not discriminative for m then hm j would be close to µj. Hence, we can consider I as an “importance” term, corresponding to a notion of how signiﬁcant feature j is for example m. Also notice that the term on the left in the gradient expression is simply the covariance between feature a and feature j. Thus, the gradient can be re-written as Ca,j · I(j, m).  (cid:88)  (4)  =  j − µj). (hm j − µj).  ∂LDeCov ∂hm a  1 N  j(cid:54)=a  Interpretation. Intuitively, the covariance term can be thought of as measuring (linear) redundancy: features a and j are redundant if they vary together. Thus, the DeCov loss tries to prevent features from being redundant, but redundancy is weighted by importance (I). Speciﬁcally, a feature j contributes towards a large gradient of feature a on example m if j is important for m and correlated with a. This means important features correlated with a (e.g., j) contribute to a large gradient of a . A feature which ﬁres only in specialized situations (e.g., a cat’s a, suppressing the activation hm ear) will likely be nearly identical or noisy for most other examples (e.g., non-cats) and will not contribute towards gradients of other specialized features. 3 RELATED WORK Redundancy Based Representations. The idea of using low redundancy to learn representations has been around for decades. In an early attempt to model human perception, Barlow (1961) lists 3 possible learning principles, the 3rd being the notion that representations should not be redundant. Later work continued to investigate this intuition in the context of unsupervised feature learning. Three objectives emerged, each of which formalize the notion differently. (1) An information the- oretic view is expressed by Linsker (1988). The main idea is to maximize information gained by predicting the next representation/layer between input and output. (2) The closest objective to ours is cross-correlation (not cross-covariance), which appears in (Bengio & Bergstra, 2009) and com- plements a temporal coherence objective. It also appears in (Pearlmutter & Hinton, 1986) where it complements an objective which encourages units to capture higher order input statistics. (3) Finally, redundancy minimization is realized through predictability minimization in (Schmidhuber, 1992) for the purpose of learning factorial codes (representations whose units are independent). This objective says that one unit should not be predictable given all of the others in its layer as input.  3  Published as a conference paper at ICLR 2016  All of these works focus on unsupervised feature learning and do not experiment with supervised models. Furthermore, these early pioneering works were limited by data and evaluated small net- works without many of the modern design choices and features (e.g. ReLus, Dropout, SGD instead of Hebb’s update rule, batch-normalization, etc.). We propose redundancy minimization for a new purpose (regularization), evaluate it using modern techniques such as end-to-end learning using SGD with respect to a supervised objective, and do this in the context of harder challenges presented by modern datasets. To the best of our knowledge, such a setting has not been considered before.  Correlation/Covariance Losses in Other Settings. Other works have used similar penalties, but in different settings and to different effects. Deep Canonical Correlation Analysis (Deep CCA) (An- drew et al., 2013) and Correlational Neural Networks (CorrNets) (Chandar et al., 2015) apply a similar loss which maximizes correlation, unlike our minimization of cross-covariance. Both meth- ods are used to learn better features in the presence of multiple views or modalities. They embed inputs to a common space and maximize correlation between aligned pairs. Another idea similar to ours is that of Cheung et al. (2014), which aims to discover and disentangle hidden factors. The goal is to separate supervised factors of variation (e.g., class of MNIST digits) from unsupervised factors of variation (e.g., handwriting style). In order to achieve this goal, they impose a covariance (not correlation) loss between (1) the softmax outputs of a neural network trained to recognize digits and (2) a hidden representation which is used in conjunction with (1) to reconstruct the input (via an auto-encoder). These two works suggest that correlation losses signiﬁcantly impact learned representations in the context of modern networks. One key difference between these two approaches and ours is that while their formulations decorrelate (Cheung et al., 2014) and disregard (Andrew et al., 2013; Chan- dar et al., 2015) parts of different representations, our approach tries to decorrelate parts of the same representation. Moreover, the ultimate goals are different. Unlike these approaches, our goal is sim- ply to improve supervised classiﬁcation performance by reducing overﬁtting, and not to reconstruct the original data. Dropout and Batch Normalization. Two recent approaches to regularization in deep neural net- works are Dropout (Srivastava et al., 2014) and to some extent Batch Normalization (Ioffe & Szegedy, 2015). Dropout aligns with our intuition and goals more closely as it aims to improve classiﬁcation performance by reducing co-adaptation of activations. On the other hand, Batch Nor- malization focuses on faster optimization by reducing internal co-variate shift, which is the constant variation of a layer’s input as it learns. Some Batch Normalization results indicate it could act as a regularizer, but this has not been exhaustively veriﬁed yet. Our approach is similar to Batch Nor- malization due to its use of mini-batch statistics. 4 EXPERIMENTS We begin with a synthetic dual “modality” experiment, which serves as a testbed for measuring improvement due to decorrelation. Next, we use an autoencoder (as in Srivastava et al. (2014)) to contrast DeCov and Dropout. Finally, we use a variety of experiments to report Image Classiﬁcation performance on CIFAR10/100 and ImageNet, noticing signiﬁcant improvement in all cases. Note that we set the Dropout rate to 0.5 as suggested by Srivastava et al. (2014).  4.1 DUAL MODALITY EXPERIMENTS WITH MNIST: PREDICTING SIDE-BY-SIDE DIGITS We propose a synthetic dual “modality” task on MNIST – simultaneously predict the class labels for two digits placed adjacent in an image. We created a dataset where each example consists of two MNIST digit images horizontally concatenated and separated by 16 black pixels (to prevent interference between feature maps in the ﬁrst layers). Fig. 2 shows a few examples. The important detail of this experiment is the particular bias we inject into the distribution of left and right digits. Let  P (l) = 0.1 and P (r|l) =  if l ∈ {0, . . . , 4} and r ∈ {0, . . . , 4} if l ∈ {0, . . . , 4} and r ∈ {5, . . . , 9} if l ∈ {5, . . . , 9}  .  (5)  To generate one example we ﬁrst sample the left digit using P (l) then the right using P (r|l). As shown in Appendix A, we can compute the conditional entropies of one digit given the other to get H(l|r) = 2.0868 and H(r|l) = 1.9360. Since H(l|r) > H(r|l), the left digit is more informative of the right than the right is of the left. There is no cross-digit signal at test time, so features for  0  0.2 0.1  4  Published as a conference paper at ICLR 2016  Figure 2: We consider the task of simultaneously predicting two MNIST digits placed side by side. By biasing right digits more than left digits at train time, we create a controlled scenario with the type of problem we expect DeCov to solve.  the right and left digits should be completely decorrelated to generalize, but learned features will have some correlation between left and right. Intuitively, DeCov should help generalization in this scenario. Our experiments support this. We use Caffe’s (Jia, 2013) reference version of LeNet (LeCun et al., 1995). It has two convolution layers, each followed by pooling, then a fully connected layer with 500 hidden units which are shared between the two softmax layers. We apply DeCov and/or Dropout to the 500 hidden units of the fully connected layer.  Left Digit  Right Digit  test  train  train  train - test  no yes yes no  train - test 99.98 ± 0.01 97.94 ± 0.18 2.05 ± 0.19 100.00 ± 0.00 96.75 ± 0.24 3.25 ± 0.24 97.39 ± 0.20 2.61 ± 0.20 99.99 ± 0.00 98.45 ± 0.04 1.54 ± 0.04 99.97 ± 0.01 98.59 ± 0.12 1.38 ± 0.12 97.81 ± 0.07 2.18 ± 0.06 99.99 ± 0.00 98.74 ± 0.03 1.25 ± 0.04 97.99 ± 0.12 2.00 ± 0.12  DeCov Dropout no no yes yes weight decay Table 1: MNIST side by side results. As expected, biasing right digits at train time so that they are weakly informed by left digits leads to lower performance on an unbiased test set. More importantly, DeCov pro- vides greater improvements over the baselines on the right, conﬁrming that it leads to better features when decorrelation is extremely likely to improve performance.  99.99 ± 0.00 99.99 ± 0.00 99.99 ± 0.00  97.86  99.97  96.21  99.97  2.11  3.76  test  Results. Table 1 reports the accuracy of left and right digit classiﬁers. Our injected dataset bias can be clearly seen in the lower test accuracy and higher train-test gap of the right classiﬁer, indi- cating that all of our networks incorporate the train time bias into their predictions. We report mean accuracies across 4 trials, along with the standard deviation. We also compare the effect of Dropout. The main result is that the gaps between the performance of DeCov and the baselines are larger for the biased right digit (e.g., right digit test accuracy shows a ∼0.6% improvement when switching from Dropout-alone to DeCov-alone while the improvement for left digits is just ∼0.3%). This suggests that the baselines pick up on the false bias and that DeCov does the best job of correcting for it. DeCov also improves generalization for both classiﬁers since test accuracy is higher in the bottom two rows and the train - test gap is lower in those rows. Combining Dropout with our DeCov loss hurts slightly, but we note that the error bars overlap in some cases, so this is not a statistically signiﬁcant difference. One skeptical hypothesis is that the DeCov loss is simply enforcing something akin to an L2 penalty on the weights. The experiments with DeCov and Dropout already use an L2 penalty, so this is unlikely, but a grid search over weights on this term shows it makes little difference. The best accuracies are reported in the last row of Table 1.  5  Published as a conference paper at ICLR 2016  (a) Baseline with train MSE = 1.47 and test MSE = 1.47  (b) DeCov with train MSE = 0.98 and test MSE = .98  (c) Dropout with train MSE = 3.08 and test MSE = 3.03  Figure 3: Weights learned by the ﬁrst layer of a 2 layer autoencoder are reshaped into images and visualized for a model with no DeCov or Dropout (Fig. 3a), a model with DeCov (Fig. 3b), and a model with Dropout (Fig. 3c).  4.2 MNIST AUTOENCODER  weights by sampling from U [−(cid:113) 3  To offer a more qualitative point of comparison, we visualized learned features using the 2 layer autoencoder experiment from (Srivastava et al., 2014) (section 7). In this experiment an autoencoder is trained on raw pixels of single MNIST digits using an encoder with 1 layer of 256 ReLU units and a decoder (untied weights) that produces 784 (28× 28) ReLU outputs. Fig. 3 shows the weights learned by the autoencoder (reshaped to align with the input image) and mean-square reconstruction errors. Weight initialization turned out to be an important factor for the visualizations.  Initializing all n ] (based on Glorot & Bengio (2010); as implemented in Caffe) led to visualizations as seen in Srivastava et al. (2014) (the baseline looks like noise), but sampling weights from a Gaussian with mean 0 and standard deviation 0.001 led to baseline visual- izations with faint digit outlines. The latter initialization was used in Fig. 3. One take-away is that MSE is signiﬁcantly lower for DeCov than others. However, the key take- away is the qualitative difference between representations learned with Dropout and those learned with DeCov. Recall from Section 1 that Dropout reduces cross-covariance while DeCov explicitly minimizes it. Despite this intuitive similarity, the two lead to different learned representations.  (cid:113) 3  n ,  4.3  IMAGE CLASSIFICATION  4.3.1 CIFAR10  CIFAR10 contains 60,000 32x32 images sorted into 10 distinct categories (Krizhevsky & Hinton, 2009). We training on the 50,000 given training examples and testing on the 10,000 speciﬁed test samples. Hyper-parameters (loss weights for DeCov and weight decay) are chosen by grid search on the standard train/val split. We use Caffe’s quick CIFAR10 architecture, which has 3 convolutional layers followed by a fully connected layer with 64 hidden units and a softmax layer. The hidden fully connected layer is not followed by a non-linearity. The DeCov loss is added only to the 64 hidden units in the hidden fully connected layer. All reported results are average performance over 4 trials with the standard deviation indicated alongside.  DeCov no no yes yes  Dropout no yes yes no  weight decay  train  100.0 ± 0.00 99.10 ± 0.17 87.78 ± 0.08 88.78 ± 0.23  100.0  test  75.24 ± 0.27 77.45 ± 0.21 79.75 ± 0.17 79.72 ± 0.14  75.29  train - test 24.77 ± 0.27 21.65 ± 0.22 8.04 ± 0.16 9.06 ± 0.22  24.71  Table 2: CIFAR10 Classiﬁcation. We can see that DeCov with Dropout leads to the highest test performance and the lowest train-test gap.  6  Published as a conference paper at ICLR 2016  Results. In Table 2, we again observe signiﬁcant improvements when using the DeCov loss – there is a ∼4.5% improvement in test accuracy (over no regularization). Moreover, the DeCov loss reduces the gap between train and val accuracies by ∼15% (without Dropout) and ∼16% (with Dropout)! Comparing the four combinations, we see that using DeCov alone provides a larger improvement than using Dropout. Using both DeCov and Dropout further improves the generalization (as mea- sured by the gap in train and test accuracies), but the improvement in absolute test performance does not seem statistically signiﬁcant. We again test if L2 weight decay can provide similar improvements and ﬁnd once again that the best setting gives little improvement over the baseline. One promise of regularization is the ability to train larger networks, so we increase the size of our CIFAR10 network. We add another fully connected layer to the network used in the previous experiment, double the number of ﬁlters in each convolutional layer, and double the number of units in the fully connected layers. This larger network performs better than the smaller version – all accuracies are higher than corresponding entries in Table 2. However, there are the stronger indications of overﬁtting in this network – speciﬁcally, the train accuracies are much higher than test accuracies (when compared to the previous network). Table 3 shows the results. We observe similar trends as the previous experiment – there are signiﬁcant gains from using DeCov alone compared to Dropout alone, and there is a further slight improvement in combining both. Using Dropout alone gives a ∼1.5% boost in test accuracy, while using DeCov alone provides a ∼4% increase in test accuracy. Using both yields roughly the same test performance, but the trainval and test gap is further reduced.  DeCov Dropout no no yes yes  no yes yes no  (train+val)  100.00 100.00 96.76 98.15  test 77.38 79.93 81.68 81.63  (train+val) - test  22.62 20.07 15.08 16.52  Table 3: CIFAR10 Classiﬁcation with a bigger version of the base network  4.3.2 CIFAR100  To scale up our experiments, we move to CIFAR100 (Krizhevsky & Hinton, 2009). We use the same architecture as the base architecture for CIFAR10 and hold out the last 10,000 of the 50,000 train examples for validation. Table 4 shows that Dropout alone highest higher test performance than DeCov alone, but DeCov leads to a smaller train-test gap. Using both regularizers not only achieves the highest test accuracy, but also the smallest train-test gap (∼34% smaller than using neither regularizer). This suggests that the two regularizers may have complementary effects.  DeCov no no yes yes  Dropout no yes yes no  train 99.77 87.35 72.53 77.92  test 38.52 43.55 45.10 40.34  train - test  61.25 43.80 27.43 37.58  Table 4: CIFAR100 Classiﬁcation Accuracies  One more problem comes with the question of how to weight the DeCov loss. All of our experiments use grid search to pick this hyper-parameter. The optimal weight varies across datasets, but we have found consistency across variations in architecture. We varied both the DeCov weight and the number of hidden units in the fully connected layer to which DeCov is applied, training a new network for each setting. The best DeCov weight (0.1) is consistent for a range of hidden activation sizes in this dataset, though it is different in other experiments.  4.3.3  IMAGENET  Now we explore results for networks trained for ImageNet classiﬁcation, starting by applying DeCov to fc6 and fc7 in AlexNet (Krizhevsky et al., 2012). The last 50,000 of the ILSVRC 2012 train images are held out for validation. Our implementation comes from Caffe. In particular, it  7  Published as a conference paper at ICLR 2016  uses a ﬁxed schedule that multiplies the learning rate by 1/10 every 100,000 iterations (see jumps in Fig. 4). We do not use early stopping and do not perform color augmentation. In Fig. 4 we notice that when neither of the two regularizers – Dropout or DeCov– are applied (blue line), the network overﬁts (it even gets 100% train accuracy), and the DeCov loss (hidden activation redundancy) is higher than with any other combination of the regularizers. Applying either of the regularizers also causes a synchronous drop in both losses. Explicitly minimizing the DeCov loss naturally leads to much lower DeCov losses, and we notice that this coincides with signiﬁcantly reduced overﬁtting. Interestingly, Dropout results in relatively lower DeCov loss too, even when DeCov is not optimized for. This is further indication of the link between redundant activations and overﬁtting.  Figure 4: Cross Entropy and DeCov losses over the course of training AlexNet with 256x256 images. Note that the DeCov val curves are hidden by the train curves. Interestingly, DeCov is reduced even by Dropout, though not nearly as much as when it is explicitly minimized.  Fig. 5 shows accuracies across different image resolutions we used to train AlexNet. AlexNet is typically trained with 256x256 images, but training with smaller images is faster 1 and reduces the number of parameters in the network. Smaller images (we use 128x128, 160x160, 192x192, and 224x224) lead to smaller feature maps output by pool5, so the dense connection between pool5 and fc6 has fewer parameters, the model has less capacity, and it’s less likely to overﬁt. For example, images scaled to 256x256 (taking 227x227 crops 2) lead to a weight matrix with 38 million param- eters while 128x128 images (with 99x99 crops) result in a 4 million parameter matrix. Generally, accuracies (left plots) and the train-val gap (right plots) have a slight positve slope, conﬁrming that performance and overﬁtting increase with resolution and model capacity. Note that the DeCov loss weight was tuned using grid search at each resolution both with and without Dropout. We see that Dropout alone (green) usually has the best val accuracy, which is slightly higher than the two losses combined (purple) and a couple points higher than DeCov alone (red) at higher resolutions. At the lowest resolution Dropout alone is tied with DeCov alone. Dropout also reduces overﬁtting more than DeCov, though both independently reduce overﬁtting by a large margin – from 59.35% to 14.7% in the case of DeCov @ 128x128. Finally, we test our new regularizer on ILSVRC 2012 with one more architecture – the Network in Network (Lin et al., 2014a).3 This architecture is fully convolutional: it contains 4 convolutional layers, with 96, 256, 384, and 1024 feature maps, respectively. Between each of these layers and after the last are two convolutional layers which have 1x1 kernels, which further process each feature map output by the main convolutional layers before being fed into the next layer. To produce 1000 softmax activations, 1000 feature maps are averaged over spatial locations to produce one feature vector. We applied DeCov to these average pooled feature vectors. Interestingly, this architecture has much less overﬁtting than AlexNet. However, adding a DeCov loss still decreases overﬁtting substantially and improves validation accuracy. There is a small boost in performance on validation accuracies and a signiﬁcant decrease of ∼3% (for top 1) and ∼2% (for top 5) in the train - val gap.  1 Using CuDNNv3, AlexNet with 128x128 inputs takes 103ms averaged over 50 runs to compute a forward  and backward pass. For 256x256 images this time is 449ms.  2At train time crops are sampled and mirrored randomly. At test time only the center 227x227 crop is used. 3This is the model provided in the Caffe Model Zoo: https://gist.github.com/mavenlin/  d802a5849de39225bcc6  8  050000100000150000200000250000300000350000400000450000Iterations01234567CrossEntropyCrossEntropyLossvsTrainingIterations050000100000150000200000250000300000350000400000450000Iterations10010110210310410510610710810910101011DeCovDeCovLossvsTrainingIterationstrainnoDeCov,noDropoutvalnoDeCov,noDropouttrainnoDeCov,yesDropoutvalnoDeCov,yesDropouttrainyesDeCov,noDropoutvalyesDeCov,noDropouttrainyesDeCov,yesDropoutvalyesDeCov,yesDropoutPublished as a conference paper at ICLR 2016  Figure 5: ImageNet classiﬁcation performance using AlexNet. Plots on the left show training and validation (ILSVRC 2012 validation set) accuracy at different resolutions. Note how all curves have a much lower train- val gap than the (blue) baseline.  no yes yes no  DeCov Dropout no no yes yes DeCov Dropout no no yes yes  no yes yes no  ILSVRC 2012 train top 1  ILSVRC 2012 val top 1  train - val  71.68 71.32 68.28 68.33  58.67 58.95 59.08 58.85  13.01 12.37 9.20 9.48  ILSVRC 2012 train top 5  ILSVRC 2012 val top 5  train - val  89.91 89.63 87.99 87.88  81.18 81.53 81.94 81.57  8.73 8.10 6.05 6.05  Table 5: ImageNet Classiﬁcation Accuracies with Network in Network  5 DISCUSSION AND CONCLUSION Fine-tuning. In the experiments we presented, networks were always trained from scratch, but we also tried ﬁne-tuning networks in different scenarios. During our ImageNet experiments we ﬁne-tuned both the Network in Network and AlexNet architectures initialized with parameters that weren’t trained with a DeCov loss, but were trained with Dropout. In both cases performance either stayed where it was at ﬁne-tuning initialization or it decreased slightly (within statistical sig- niﬁcance). We found similar results when ﬁne-tuning for other tasks like attribute classiﬁcation (ﬁne-tuning AlexNet) and object detection (Fast RCNN (Girshick, 2015)). This, along with some cases where combining Dropout and DeCov decreases performance slightly suggest that the DeCov loss may possibly be acting adversarially to activations learned by Dropout. Fine-tuning with DeCov is an interesting direction for future work. Trends. All of our experiments strongly indicate two clear trends:  1. DeCov reduces overﬁtting as measured by the gap between train and test performance. 2. DeCov acts as a regularizer: performance with DeCov is always better than performance  without either DeCov or Dropout.  9  120140160180200220240260ImageResolution30405060708090100Top1AccuracyTop1Accuracy120140160180200220240260ImageResolution0102030405060Top1Train-ValAccuracyGapTop1Gap120140160180200220240260ImageResolution556065707580859095100Top5AccuracyTop5Accuracy120140160180200220240260ImageResolution51015202530354045Top5Train-ValAccuracyGapTop5GaptrainnoDeCov,noDropoutvalnoDeCov,noDropouttrainnoDeCov,yesDropoutvalnoDeCov,yesDropouttrainyesDeCov,noDropoutvalyesDeCov,noDropouttrainyesDeCov,yesDropoutvalyesDeCov,yesDropoutPublished as a conference paper at ICLR 2016  To be clear, the results do not support that Dropout can be completely replaced by DeCov, but simply that in a number of scenarios DeCov is a useful alternative and their combination almost always works the best. Our loss clearly has desirable regularization properties at the expense of one extra hyper-parameter to tune. In this work, we proposed a new DeCov loss which explicitly penalizes the covariance between the activations in the same layer of a neural network in an unsupervised fashion. This loss acts as a strong regularizer for deep neural networks, where overﬁtting is a major problem and Dropout has been required to get large models to generalize well. We show that DeCov competes well against Dropout over a range of experiments which investigate different scales, datasets and architectures. Acknowledgements. This work was supported in part by the following awards to DB: National Science Foundation CAREER award, Army Research Ofﬁce YIP award, Ofﬁce of Naval Research grant N00014-14-1-0679, AWS in Education Research Grant, and GPU support by NVIDIA. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor.  REFERENCES Andrew, Galen, Arora, Raman, Bilmes, Jeff, and Livescu, Karen. Deep canonical correlation analysis.  Proceedings of the 30th International Conference on Machine Learning, pp. 1247–1255, 2013. 4  In  Antol, Stanislaw, Agrawal, Aishwarya, Lu, Jiasen, Mitchell, Margaret, Batra, Dhruv, Zitnick, C. Lawrence, and Parikh, Devi. Vqa: Visual question answering. In International Conference on Computer Vision (ICCV), 2015. 1  Barlow, Horace B. Possible principles underlying the transformations of sensory messages. 1961. 3  Bengio, Yoshua and Bergstra, James S. Slow, decorrelated features for pretraining complex cell-like networks.  In Advances in neural information processing systems, pp. 99–107, 2009. 3  Breiman, Leo. Bagging predictors. Machine learning, 24(2):123–140, 1996. 2  Chandar, Sarath, Khapra, Mitesh M, Larochelle, Hugo, and Ravindran, Balaraman. Correlational neural net-  works. arXiv preprint arXiv:1504.07225, 2015. 4  Chen, Xinlei and Zitnick, C Lawrence. Mind’s eye: A recurrent visual representation for image caption gener- ation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2422–2431, 2015. 1  Cheung, Brian, Livezey, Jesse A., Bansal, Arjun K., and Olshausen, Bruno A. Discovering hidden factors of variation in deep networks. Proceedings of the International Conference on Learning Representations (ICLR), abs/1412.6583, 2014. URL http://arxiv.org/abs/1412.6583. 4  Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: A Large-Scale Hierarchical Image  Database. In CVPR, 2009. 1, 2  Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. Decaf: A deep convolutional activation feature for generic visual recognition. In Proceedings of the Inter- national Conference on Machine Learning (ICML), 2014. 1  Girshick, Ross. Fast r-cnn. In International Conference on Computer Vision (ICCV), 2015. 9  Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik, Jagannath. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 580–587. IEEE, 2014. 1  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural networks.  In International conference on artiﬁcial intelligence and statistics, pp. 249–256, 2010. 6  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout net-  works. Proceedings of the International Conference on Learning Representations (ICLR), 2013. 1  Hansen, Lars Kai and Salamon, Peter. Neural network ensembles. IEEE transactions on pattern analysis and  machine intelligence, 12(10):993–1001, 1990. 2  10  Published as a conference paper at ICLR 2016  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 448–456, 2015. URL http://jmlr.org/proceedings/ papers/v37/ioffe15.html. 4  Jia, Yangqing. Caffe: An open source convolutional architecture for fast feature embedding. http:  //caffe.berkeleyvision.org/, 2013. 5  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images. Computer  Science Department, University of Toronto, Tech. Rep, 1(4):7, 2009. 2, 6, 7  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Imagenet classiﬁcation with deep convolutional neural networks. In NIPS, 2012. URL http://books.nips.cc/papers/files/nips25/NIPS2012_ 0534.pdf. 1, 2, 7  LeCun, Yann, Jackel, LD, Bottou, L, Brunot, A, Cortes, C, Denker, JS, Drucker, H, Guyon, I, Muller, UA, Sackinger, E, et al. Comparison of learning algorithms for handwritten digit recognition. In International conference on artiﬁcial neural networks, volume 60, pp. 53–60, 1995. 2, 5  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. Proceedings of the International Conference  on Learning Representations (ICLR), 2014a. 2, 8  Lin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Dollï£¡r, Piotr,  and Zitnick, C. Lawrence. Microsoft COCO: Common objects in context, 2014b. 1  Linsker, Ralph. Self-organization in a perceptual network. Computer, 21(3):105–117, 1988. 3  Pearlmutter, Barak A and Hinton, Geoffrey. G-maximization: An unsupervised learning procedure for discov- ering regularities. In AIP conference proceedings, volume 151, pp. 333–338. American Institute of Physics, 1986. 3  Perrone, Michael P. and Cooper, Leaon N. When networks disagree: Ensemble methods for hybrid neural  networks. In Tech Report, pp. 126–142. Chapman and Hall, 1993. 2  Schmidhuber, Jürgen. Learning factorial codes by predictability minimization. Neural Computation, 4(6):  863–879, 1992. 3  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15 (1):1929–1958, 2014. 1, 2, 4, 6  Tibshirani, Robert. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,  Series B, 58:267 – 288, 1996. 1  Tikhonov, Andrey Nikolayevich. On the stability of inverse problems. Dokl. Akad. Nauk SSSR, pp. 195–198,  1943. 1  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption generator. In Computer Vision and Pattern Recognition, 2015. URL http://arxiv.org/abs/1411. 4555. 1  Wan, Li, Zeiler, Matthew, Zhang, Sixin, Cun, Yann L, and Fergus, Rob. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 1058–1066, 2013. 1  Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod. How transferable are features in deep neural  networks? In Advances in Neural Information Processing Systems, pp. 3320–3328, 2014. 1  Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., and Oliva, A. Learning deep features for scene recognition  using places database. In NIPS, 2014. 1  11  Published as a conference paper at ICLR 2016  APPENDICES  A DETAILS OF THE BIAS IN THE MNIST EXPERIMENT Recall that in Section 4.1 we generate biased pairs of MNIST digits by deﬁning  P (l) = 0.1 and P (r|l) =  if l ∈ {0, . . . , 4} and r ∈ {0, . . . , 4} if l ∈ {0, . . . , 4} and r ∈ {5, . . . , 9} if l ∈ {5, . . . , 9}  (6)  and sampling left then right digits. To show that this creates a larger bias on the right than on the left, we show there is more uncertainty about left digits given right ones than right ones given left ones. That is, we show the conditional entropy H(l|r) is greater than H(r|l). To compute the conditional entropies, we ﬁrst derive  (cid:88)  P (r) =  P (r|l)P (l) =  (cid:26)0.05 if r ∈ {0, . . . , 4}  0.15 if r ∈ {5, . . . , 9}  and  l  P (l|r) =  P (r|l)P (l)  P (r)  =  H(l|r) = −(cid:88) H(r|l) = −(cid:88)  r  if l ∈ {0, . . . , 4} and r ∈ {0, . . . , 4} if l ∈ {0, . . . , 4} and r ∈ {5, . . . , 9} if l ∈ {5, . . . , 9} and r ∈ {0, . . . , 4} if l ∈ {5, . . . , 9} and r ∈ {5, . . . , 9}  P (l|r) log P (l|r) ≈ 2.0868  P (r|l) log P (r|l) ≈ 1.9560  Using the convention 0 log 0 = 0, we can now compute  .  (7)  (8)  (9)  (10)  (11)  0  0.2 0.1    0 2 15 3 15 1 15  P (r)  P (l)  (cid:88) (cid:88)  l  l  r  12  ",
1511.07386,2016,Pushing the Boundaries of Boundary Detection using Deep Learning,['Pushing the Boundaries of Boundary Detection using Deep Learning\nIasonas Kokkinos'],https://arxiv.org/pdf/1511.07386,"6 1 0 2     n a J    2 2      ]  V C . s c [      2 v 6 8 3 7 0  .  1 1 5 1 : v i X r a  Under review as a conference paper at ICLR 2016  PUSHING THE BOUNDARIES OF BOUNDARY DETEC- TION USING DEEP LEARNING  Iasonas Kokkinos Center for Visual Computing CentraleSup´elec and INRIA Chatenay-Malabry, 92095, France {iasonas.kokkinos}@ecp.fr  ABSTRACT  In this work we show that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection. Our contributions consist ﬁrstly in combining a careful design of the loss for boundary detection training, a multi-resolution architecture and training with ex- ternal data to improve the detection accuracy of the current state of the art. When measured on the standard Berkeley Segmentation Dataset, we improve theopti- mal dataset scale F-measure from 0.780 to 0.808 - while human performance is at 0.803. We further improve performance to 0.813 by combining deep learning with grouping, integrating the Normalized Cuts technique within a deep network. We also examine the potential of our boundary detector in conjunction with the task of semantic segmentation and demonstrate clear improvements over state-of- the-art systems. Our detector is fully integrated in the popular Caffe framework and processes a 320x420 image in less than a second.  1  INTRODUCTION  Over the past three years Deep Convolutional Neural Networks (DCNNs) LeCun et al. (1998) have delivered compelling results in high-level vision tasks, such as image classiﬁcation (Krizhevsky et al., 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014; Szegedy et al., 2014; Papandreou et al., 2015b) or object detection (Girshick et al., 2014). Recent works have also shown that DCNNs can equally well apply to pixel-level labelling tasks, including semantic segmentation (Long et al., 2014; Chen et al., 2015) or normal estimation (Eigen et al., 2014). A convenient component of such works is that the inherently convolutional nature of DCNNs allows for simple and efﬁcient ‘fully convolutional’ implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015). Our focus on this work is the low-level task of boundary detection, which is one of the cornerstone problems of computer vision. Segmentation can be considered to be an ill-posed problem, and multiple solutions can be considered plausible, depending on the task at hand - for instance when playing chess we think of a checker board in terms of 64 regions, but when carrying it we treat it as a single object. This is reﬂected in the inconsistency of human segmentations, illustrated in Fig. 1. As detailed in Arbelaez et al. (2011) we can ‘benchmark’ humans against each other, by comparing every annotator to the ‘committee’ formed by the rest: if a user provides details that no committee member has provided these count as false positives, while if a user misses details provided by a committee member, these count as misses. Aggregating this information over different annotators yields the recall and precision of humans, which are in turn summarized in terms of their f-measure, namely their geometric mean. When evaluated on the test set of Berkeley Segmentation Dataset (BSD) humans have an F-measure of 0.803, which is indicative of the difﬁculty of the task. This difﬁculty may be substantially diminished if we consider segmentation as an intermediate to a speciﬁc task, such as object detection; it has been shown for instance in Zhu et al. (2015) that when asking users to provide a label to every region the F-measure of human annotators rapidly increases  1  Under review as a conference paper at ICLR 2016  Figure 1: Ground-truth segmentations provided by different annotators for an image from the BSD dataset, and associated boundary maps. The evident lack of agreement among humans is reﬂected in a low F-measure of human annotators on the task, F = 0.803. Our system delivers F = 0.813.  from 0.8 to 0.9. Still, when considering the segmentation problem in its largest generality, namely as a mid-level task serving detection, tracking, counting, or even grasping and touching, the ambiguity of the labelling most naturally increases. Despite the inherent difﬁculty of the problem, progress in boundary detection has been consistently narrowing the gap between human and machine performance, as measured Our system yields a higher F-measure than humans: when using a common threshold for the whole dataset (Optimal Dataset Scale -ODS) our system’s F-measure equals F = 0.813, while when an oracle sets the threshold per image (Optimal Image Scale -OIS) we obtain F = 0.8308. As in all works following the introduction of human-annotated datasets (Konishi et al., 2003; Martin et al., 2004), e.g. (Dollar et al., 2006; Arbelaez et al., 2011; Ren, 2008; Kokkinos, 2010a; Ren & Bo, 2012; Doll´ar & Zitnick, 2015), we use machine learning to optimize the performance of our boundary detector. Recent works (Bertasius et al., 2015; Kivinen et al., 2014; Hwang & Liu, 2015) have shown hat DCNNs yield substantial improvements over ﬂat classiﬁers; the Holistic Edge Detection approach of Xie & Tu (2015) recently achieved dramatic improvements over the previous state-of-the-art, from an F-measure of 0.75 to 0.78, while keeping computation efﬁcient, requiring 0.4 seconds on the GPU; additional dataset augmentation yielded an F-measure of 0.79. In this work we make contributions in three fronts: ﬁrstly we improve the deep learning algorithms used for boundary detection, secondly we incorporate classical ideas from grouping into the problem and thirdly we exploit our detector to improve the higher-level tasks of semantic segmentation and region proposal generation. We detail these three advances in the following three sections.  2 HED AND DSN TRAINING  We start from a brief presentation of the ‘Holistic Edge Detection’ (HED) work of Xie & Tu (2015) as it serves as a starting point for our work. HED uses ‘Deep Supervised Network’ (DSN) (Lee et al., 2015) training to ﬁne-tune the VGG network for the task of boundary detection, illustrated in Fig. 2. The principle behind DSN can be loosely understood as classiﬁer stacking adapted to deep learning and turns out to be practically very successful: if a multi-layer architecture is optimized for a given task, one can anticipate better results by informing each layer about the ﬁnal objective, rather than relying on the ﬁnal layer to back-propagate the information to its predecessors. This was shown to systematically improve convergence and test performance, both in generic detection tasks (Lee et al., 2015) and in particular in the context of boundary detection (Xie & Tu, 2015). In particular, using the notation of Xie & Tu (2015), we have a training set S = (Xn, Yn), n = 1, . . . , N with Xn being the input image and Yn = {y(n) j ∈ {0, 1} being the predicted labels (we will drop the n subscript for brevity). We consider a multi-layer network, represented in terms of the union of its individual layer pa- rameters, W, to which we append a set of per-layer ‘side’ parameters w(1), . . . w(M ). These side parameters aim at steering the intermediate layers of the network to extract features that are useful for the classiﬁcation task even when used on their own.  , j = 1, . . . ,|Xn|}, y(n)  j  2  Under review as a conference paper at ICLR 2016  Figure 2: HED/DSN training architecture: every intermediate layer of a DCNN (shown blue) is pro- cessed by a side layer (shown in orange) which is penalized by a loss function L. The intermediate results are combined in a late fusion stage, which is again trained with the same loss function.  The objective function of DSN/HED is phrased as:  M(cid:88)  m=1  (cid:88)  j∈Y  . =  Lside(W, w) =  αmlm(W, w(m)),  (1)  where lm are the side-layer losses on the side output of the m-th layer and αm indicates the impor- tance of the different side layer losses - e.g. setting αm = 0, m < M, which amounts to standard training with a single loss at the top. In HED lm is a class-balanced cross-entropy loss: lm(W, w(m)) =−β  log P (yj = 1|X; W, w(m))−(1 − β)  logP (yj = 0|X; W, w(m)) (2)  (cid:88)  (cid:88)  j∈Y+  j∈Y−  wˆyj S(ˆyj, sm  j ),  (3)  where Eq. 2 Y+, Y− are the positive and negative training sample indices respectively, and β is a design parameter set to mitigate the substantially larger number of negative samples in images. The probabilities in Eq. 2 are obtained in terms of a sigmoidal function operating on the inner product j = (cid:104)w(m), fj(cid:105) between the side layer parameters w(m) and the features fj of the DCNN at sm position j, P (yj = 1|X; W, w(m)) = σ(sm j ). In Eq. 3 we rewrite Eq. 2 in a more general form where we sum over the whole image domain and use the ground truth label ˆyj to indicate which weight and which of the two loss terms is used per pixel j. An additional processing step of HED is a late fusion stage where the side outputs are combined into a ﬁnal classiﬁcation score. This is very meaningful for the task of boundary detection, as it exploits the multi-scale information extracted by the different processing layers of the DCNN. In particular, denoting by Sm the ﬁeld of values predicted by the m-th side-layer, these are linearly combined into m=1 hmSm; a fusion loss is used to learn the weights h by calibrating the  a ﬁnal score, Sf s =(cid:80)M  relative importance of the different side-layers when forming the ﬁnal prediction:  Lf use(W, w, h) =  wˆyj S(ˆyj,  hmsm j )  (cid:88)  j∈Y  M(cid:88)  m=1  The overall objective function of HED is written as follows:  LHED(W, w, h) = Lside(W, w) + Lf use(W, w, h)  and is optimized using common Stochastic Gradient Descent training with momentum.  (4)  (5)  3  IMPROVED DEEP BOUNDARY DETECTION TRAINING  Having outlined the HED framework, we now turn to our contributions, consisting in (i) Multiple Instance Learning for boundary detection (ii) Graduated Deep Supervision (iii) Multi-Scale training, as well as introducing external data. The improvements due to these contributions are summarized in Table. 1, where we report our ODS- and OIS-based F-measures on the BSD test set, alongside with the average precision (AP). We compare to our own HED-type baseline that yields a performance marginally below that of the original HED system of Xie & Tu (2015); the latest system of Xie & Tu (2015) has an improved F- measure of F = 0.79, due to additional dataset augmentation, which we have not performed yet. We anticipate that this could further boost our already substantially better performance of F = 0.813. Further comparisons can be found in Table. 2.  3  Under review as a conference paper at ICLR 2016  Image Pyramid Tied CNN outputs  Scale fusion  NCuts & boundaries  Final outputs  Figure 3: Overview of the main computation stages in our system: an input image is processed at three different scales in order to obtain multi-scale information. The the three scales are fused and sent as input to the Normalized Cuts algorithm, that delivers eigenvectors (we show the ﬁrst three of eight dimensions as an RGB image) and the resulting ‘Spectral Boundaries’. The latter are fused with the original boundary map, nonmaximum suppressed, and optionally thresholded (bottom row). All stages are implemented in Caffe, requiring less than a second on an Nvidia Titan GPU.  3.1 DEALING WITH ANNOTATION INCONSISTENCIES  The ﬁrst of our contributions aims at dealing with the inconsistency of human annotations in the BSD, illustrated in Fig. 4. As can be seen, even if the two annotators agree about the semantics (a tiger in water), they may not place the boundaries at a common location. This makes it challenging to deﬁne ‘positive’ and ‘negative’ training samples in the vincinity of boundaries. This problem has already been acknowledged in the literature; for instance Sironi et al. (2015) turn boundary detection into a regression problem, by explicitly manipulating the ground truth to become smoother - which however may come at the cost of localization accuracy. In Xie & Tu (2015) a heuristic that was used was to only consider a pixel as positive if it is annotated consistently by more than three annotators. It is however unclear why other pixels should be labelled as negatives. Our approach builds on Kokkinos (2010a), where Multiple Instance Learning (MIL) (Dietterich et al., 1997) is used to accommodate orientation inconsistencies during the learning of an orientation- sensitive boundary detector. That work was aimed at learning orientation-sensitive classiﬁers in the presence of orientation ambiguity in the annotations - we take a similar approach in order to deal with positional ambiguity in the annotations while learning a position-sensitive detector. Standard, ‘single instance’ learning assumes training samples come in feature-label pairs -or, as in HED above, every pixel is either a boundary or not. Instead, MIL takes as a training sample a set  Figure 4: Location uncertainty of human annotations in the BSD dataset: even if annotators agree on the semantics, the boundaries positions remain uncertain. As shown by the blue circle, the precise position is unclear, while as shown by the orange ellipse, even the overall boundary shape may vary.  4  Under review as a conference paper at ICLR 2016  Method Baseline MIL ODS 0.7863 0.8083 OIS AP 0.802  0.7781 0.7961 0.804  G-DSN M-Scale 0.7892 0.8033 0.8196 0.8106 0.789 0.8483  VOC 0.8086 0.8268 0.861  Grouping 0.8134 0.8308 0.866  Table 1: Improvements obtained in this work over our own reproduction of a HED-type baseline : each column corresponds to a Section (MIL: 3.1, G-DSN: 3.2, Multi-Scale: 3.3, VOC: 3.4, Group- ing: 4). Each improvement builds on top of the directly previous one. As performance indexes we report the ‘Optimal Dataset Scale’ (ODS) F-measure (using a ﬁxed detector threshold for whole dataset), the ‘Optimal Image Scale’ (OIS) F-measure (using an oracle-based, image-dependent threshold), and ‘Average Precision’ (AP).  of features (‘bag’) and its label. A bag should be labelled positive if at least one of its features is classiﬁed as positive, and negative otherwise. In particular, since human annotations come with some positional uncertainty, the standard evalu- ation protocol of Martin et al. (2004) allows for some slack in the predicted position of a pixel (a ﬁxed fraction of the image diagonal). One therefore does not need to label every positive pixel as a positive, but rather give a large score to a pixel in its vicinity - and to be more precise, a set of pixels in the line perpendicular to its orientation. This set of pixels forms the bag associated to every positive pixel annotation. A pixel is declared negative if it is not contained in any positive bag. More speciﬁcally, we associate every ground-truth boundary position j with a set of Nj positions and an associated feature bag, Xj = {Xj,1, . . . , Xj,Nj}. These positions are estimated by identifying the image positions that (i) lie closer to i than any other ground-truth pixel and (ii) have a distance below a threshold d. For each feature Xj,k of the j-th bag our classiﬁer provides a probability pj,k of it being positive, exactly as described inEq. 2 but now the decision is taken by maximizing over instance probabilities:  The cost function now writes:  pXj = P (yj = 1|Xj) = max (cid:88)  (cid:88)  k∈[1,...Nj ]  wˆyj S(−1, sm  j ) +  pj,k  lm(W, w(m)) =  j∈Y−  wˆyj S(1, max j∈Bi  sm j )  j∈Y+  (6)  (7)  where Bi is the ‘bag’ of pixel indices associated with sample i; this allows positive samples to select the neighbours that most support them while forcing all negatives to be negative. In terms of optimization, the max operation in Eq. 6 is not differentiable, but we can use a subdifferential of pj:  ∂pj =  dpj,k∗  df (Xj,k∗ )  , where k∗ = arg max  k  pj,k.  (8)  The ‘MIL’ column of Table. 1 reports improvements over the baseline obtained by setting the dis- tance, d to 1; setting d = 2 yields similar improvements.  3.2 GRADUATED DSN TRAINING  The two terms in the objective function of HED, Eq. 5:  L(W, w, h) = Lside(W, w) + Lf use(W, w, h)  (9)  play a complementary role: the ﬁrst, side-layer, terms force the intermediate layers to be discrim- inative and also extract some preliminary classiﬁcation information; the second, fusion-layer, term calibrates the importance of the intermediate classiﬁcations delivered by the side-layers. As discussed in Lee et al. (2015), DSN can be understood as simplifying the associated learning problem in terms of optimization. But once the network parameters are in the right regime, we can discard any simpliﬁcations that were required to get us there. This is a strategy used in the classical Graduated Non-Convexity technique (Blake & Zisserman, 1987), and here we show that it also helps improve DSN when applied to boundary detection.  5  Under review as a conference paper at ICLR 2016  Figure 5: Network architecture used for multi-resolution HED training: three differently scaled versions of the input image are provided as inputs to three FCNN networks that share weights - their multi-resolution outputs are fused in a late fusion stage, extending DSN to multi-resolution training.  For this we modify the training objective by associating the ‘side’ term with a temporally decreasing weight while keeping the second term’s weight ﬁxed:  L(t)(W, w, h) = (1 − t T  )Lside(W, w) + Lf use(W, w, h),  where t is the current training epoch and T is the total number of epochs. Our training criterion starts from DSN, where every intermediate layer is trained for classiﬁcation, and eventually leads to a skip-layer architecture, where the early layers are handled exclusively by the ﬁnal fusion criterion. By the end the fusion-layer can use the side-layers at will, without the compromises needed to keep the side losses low. The improvements are reported in the G-DSN column of Table. 1.  3.3 MULTI-RESOLUTION ARCHITECTURE  The authors of HED use ‘Deep Supervised Network’ (DSN) (Lee et al., 2015) training to ﬁne- tune the VGG network for the task of boundary detection, illustrated in Fig. 2. However, image boundaries reside in multiple image resolutions (Witkin, 1983) and it has repeatedly been shown in Doll´ar & that fusing information from multiple resolutions improves boundary detection, e.g. Zitnick (2015); Arbelaez et al. (2011). Even though the authors of HED use information from multiple scales by fusing the outputs of many layers, multi-resolution detection can still help. We ﬁrst observed that simply averaging the results of the network applied to differently scaled versions of the image improved performance substantially, but then turned to a more accurate way of doing the multi-resolution detection. As illustrated in Fig. 2, we consider a DSN-type multi- resolution architecture with tied weights, meaning that layers that operate at different resolutions share weights with each other. Parameter sharing across layers both accelerates convergence and also avoids over-ﬁtting. We initialize the weights from a single-resolution architecture and ﬁne-tune with a smaller set of iterations. In order to capture ﬁne-level boundaries the top-resolution image is an upsampled version of the original - e.g. for a 381 × 421 image from the BSD dataset we use a 577 × 865 upsampled version, from which we compute a three-level pyramid by downsampling by a factor of 2 and 4. The multi-resolution results are fused through an additional fusion layer that combines the fused results of the individual resolutions. The improvements are reported in the S = 3 column of Table. 1.  3.4 TRAINING WITH EXTERNAL DATA  Even though HED uses the pre-trained VGG network as initialization, dataset augmentation was reported to give substantial improvements. The authors in Xie & Tu (2015) originally used 32 geometric transformations (16 rotations and ﬂipping) of the 300 images used in the BSD trainval set, resulting in a total of roughly 10000 training images - in a recent version the authors consider two additional transformations are considered, resulting in roughly 30000 training images and pushing performance from F = 0.78 to F = 0.79.  6  Under review as a conference paper at ICLR 2016  We have not used these additional scalings in our experiments due to time constraints, but have considered the use of boundaries from the VOC Context dataset (Mottaghi et al., 2014), where all objects and ‘stuff’ present in the scene are manually segmented. Our sole modiﬁcation to those boundaries has been to label the interiors of houses as ‘don’t care’ regions that are ignored by the loss, since all of the windows, doors, or balconies that are missed by the annotators seemed to us as being legitimate boundaries. We only apply ﬂipping to these images, resulting in roughly 20000 images, which are appended to the 10000 images we had originally used. As can be seen from the ‘VOC’ column of Table. 1, this yields a substantial improvement.  4 USING GROUPING IN A DEEP ARCHITECTURE  The combination of the techniques outlined above already help boundary detection outperform hu- mans on the task of boundary detection - but still do not use any grouping information when deliv- ering the probability of having boundaries. The boundary detector only implicitly exploits grouping cues such as closedness or continuity that can often yield improvements in the high-precision regime (Zhu et al., 2007; Kokkinos, 2010b). To capture such information we use the Normalized Cuts (NCuts) technique of Shi & Malik (1997); Arbelaez et al. (2011). We treat the image as a weighted graph, where nodes corresponding to pixels and weights correspond to low-level afﬁnity between pixels measured in terms of the Intervening Contour cue (Shi & Malik, 1997), where the contours are now estimated by our boundary detector. The NCut technique considers a relaxation of the discrete normalized cut optimization problem, which results in a generalized eigenvector problem (Shi & Malik, 1997):  (D − W )v = λDv,  (10)  where D is the graph degree matrix and W is the afﬁnity. The solutions to this generalized eigenvec- tor problem can be understood (Belkin & Niyogi, 2001) as euclidean embeddings of the inter-node distances - so nodes that have similar embeddings are likely to belong together and vice versa. One of the main impediments to the application of this technique has been computation time, re- quiring roughly 60 seconds on the CPU for a 321 × 481 image for 10 eigenvectors. Even though accelerations exist, e.g. Cour et al. (2005), we found it simpler to harness the computational power of GPUs and integrate the Damascene system of (Catanzaro et al., 2009) with the Caffe deep learn- ing framework. The implementation of (Catanzaro et al., 2009) provides a GPU implementation of the Lanczos solver for the generalized eigenvector problem of Eq. 10 that is two orders of magni- tude faster than the CPU-based algorithm. When integrated with our boundary detector Damascene yields 8 eigenvectors for a 577× 865 image in less that 0.2 seconds. It is also straightforward to use a downsampled version of the boundary map to yield further accelerations. These embeddings can be used for boundary detection in terms of their directional derivatives, in order to provide some ‘global’ evidence for the presence of a boundary, known as the ‘spectral prob- ability of boundary’ cue (Arbelaez et al., 2011). In particular, as in (Arbelaez et al., 2011), we obtain a new boundary map in terms of a linear combination between the posterior probabilities delivered by our multi-resolution network and the spectral boundary magnitude. This further improves the performance of our detector, yielding an F-measure of 0.813, which is substantially better than our earlier performance of 0.807, and humans, who operate at 0.803. We anticipate that adding a few processing layers can further improve performance. We summarize the impact of the different steps described above in Fig. 6 - starting from a baseline (that performs slightly worse than the HED system of Xie & Tu (2015) we have introduced a series of changes that resulted in a system that performs boundary detection with an F-measure that exceeds that of humans. When compared to the current state-of-the-art method of Xie & Tu (2015) our method clearly dominates in terms of all typical performance measures, as shown in Table 2. Indicative qualitative results are included in the supplemental material.  7  Under review as a conference paper at ICLR 2016  Figure 6: Impact of the different improvements described in Section 2: starting from a baseline that performs only slightly worse than the HED system of (Xie & Tu, 2015) we end up with a detector that largely surpasses human F-measure, illustrated in terms of green isocontours. On the right we zoom into the high-F measure regime.  ODS Method 0.726 gPb-owt-ucm (Arbelaez et al., 2011) SE-Var (Doll´ar & Zitnick, 2015) 0.746 DeepNets (Kivinen et al., 2014) 0.738 N4-Fields (Ganin & Lempitsky, 2014) 0.753 DeepEdge (Bertasius et al., 2015) 0.753 0.756 CSCNN (Hwang & Liu, 2015) 0.756 DeepContour (Shen et al., 2015) 0.790 HED-fusion (Xie & Tu, 2015) 0.788 HED-late merging (Xie & Tu, 2015) Ours (DCNN + sPb) 0.8134  OIS 0.757 0.767 0.759 0.769 0.772 0.775 0.773 0.808 0.808 0.8308  AP 0.696 0.803 0.758 0.784 0.807 0.798 0.797 0.811 0.840 0.866  Table 2: Comparison to the state-of-the-art in boundary detection, including the latest version of HED, trained with its most recent dataset augmentation (Xie & Tu, 2015). We clearly outperform HED across all performance measures, while keeping the speed above 1 frame per second.  5 SYNERGY WITH SEMANTIC SEGMENTATION  Having pushed the performance of boundary detection to a good level, we now turn to seeing how it can be explored in the context of the higher level task of semantic segmentation. 1 Since our model is fully-convolutional we can easily combine it with the recent line of works around FCNN-based semantic segmentation(Long et al., 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). These have delivered excellent results, and in particular the use of the Dense Conditional Random Field (DenseCRF) of Kr¨ahenb¨uhl & Koltun (2011) by Chen et al. (2015); Papandreou et al. (2015a); Zheng et al. (2015), has enhanced the discriminative power of FCNNs with local evidence gathered by the image intensity. Following Chen et al. (2015) we deﬁne the CRF distribution as:  (cid:16) − E(x)  (cid:17)  P (x) =  1 Z  exp  (cid:88)  (cid:88)  i  ij  , E(x) =  φi(xi) +  θij(xi, xj).  (11)  In Eq. 11 φi(xi) = where x is the pixel-label assignment and E(x) is the energy function. − log P (xi) with P (xi) being the CNN-based probability of assigning label j to pixel i, and θij(xi, xj) is a bilateral ﬁlter-like image-based pairwise potential between i and j:  (cid:16) − |pi − pj|2  2σ2 α  (cid:17)  − |Ii − Ij|2  2σ2 β  (cid:16) − |pi − pj|2  (cid:17)  2σ2 γ  θij(xi, xj) = w1 exp  + w2 exp  .  (12)  1In an earlier version of this Arxiv report we had considered the combination with object proposals using the system of Kr¨ahenb¨uhl & Koltun (2015) and reported large improvements. This was due to an erroneous calculation of the baseline. After ﬁxing the error there are still some improvements, but they are not large enough to be considered substantial.  8  00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91RecallPrecision  [F=.8027] Human[F=.8134] Grouping[F=.8086] VOC data[F=.8033] Multi−res[F=.7893] G−DSN[F=.7875] MIL[F=.7781] Baseline0.60.70.80.910.60.70.80.91RecallPrecision  [F=.8027] Human[F=.8134] Grouping[F=.8086] VOC data[F=.8033] Multi−res[F=.7893] G−DSN[F=.7875] MIL[F=.7781] BaselineUnder review as a conference paper at ICLR 2016  The ﬁrst kernel in Eq. 12 depends on both pixel positions (denoted as p) and pixel color intensities (denoted as I), while the second kernel only depends on pixel positions - the hyper-parameters σα, σβ and σγ control the Gaussian kernels. Mean-ﬁeld Inference for this form of pairwise terms can be efﬁciently implemented with high-dimensional ﬁltering (Adams et al., 2010). Our modiﬁcations are very simple: ﬁrstly, we adapt the multi-resolution architecture outlined in the previous section to semantic segmentation. Using multi-resolution processing with tied-weights and performing late score fusion yielded substantially better results than using a single-resolution network: as shown in Table. 3 when combining the multi-scale network’s output with DenseCRF inference, performance increases from 72.7 (single-scale counterpart of Chen et al. (2015)) or 73.9 (skip-layer multi-scale counterpart of Chen et al. (2015)) to 74.8 (our multi-scale) in mean accuracy. Secondly, we integrate the boundary information extracted by our detector into the DenseCRF by using the eigenvectors computed by normalized Cuts to augment the RGB color features of Eq. 12, thereby conveying boundary-based proximity into DenseCRF inference. In particular we augment the dimensionality of Ii in Eq. 12 from 3 to 6, by concatenating the 3 eigenvectors delivered by NCuts with the RGB values. We observe that introducing the Normalized Cut eigenvectors into DenseCRF inference yields a clear improvement over an already high-performing system (from 74.8 to 75.4), while a small additional improvement was obtained we performing graph-cut inference with pairwise terms that depend on the boundary strength (from 75.4 to 75.7). Further improvements can be anticipated though an end-to-end training using the recursive CNN framework of Zheng et al. (2015) as in the currently leading works - we will explore this in future work. Indicative qualitative results are included in the supplemental material.  Method Adelaide-Context-CNN-CRF-COCO (Lin et al., 2015) CUHK-DPN-COCO (Liu et al., 2015) Adelaide-Context-CNN-CRF-COCO (Lin et al., 2015) MSRA-BoxSup (Dai et al., 2015) Oxford-TVG-CRF-RNN-COCO (Zheng et al., 2015) DeepLab-MSc-CRF-LF-COCO-CJ (Chen et al., 2015) DeepLab-CRF-COCO-LF(Chen et al., 2015) Multi-Scale DeepLab Multi-Scale DeepLab-CRF Multi-Scale DeepLab-CRF-Embeddings Multi-Scale DeepLab-CRF-Embeddings-GraphCuts  mAP %  77.8 77.5 77.2 75.2 74.7 73.9 72.7 72.1 74.8 75.4 75.7  Table 3: Mean Average Precision performance on the VOC 2012 Semantic Segmentation test set; our results are in the last four rows. We start from a novel multi-resolution variant of DeepLab and consider the gain of introducing normalized cut eigenvectors into DenseCRF inference, as well as adding a more classical boundary-sensitive GraphCut post-processing stage on top.  6 CONCLUSION  We have proposed a method to substantially improve deep learning-based boundary detection per- formance. Our system is fully integrated in the Caffe framework and operates in less than one second per frame. Its F-measure, as measured on the standard BSD dataset is higher than that of humans. We anticipate that further improvements can be gained through a joint treatment of other low-level cues, such as symmetry (Tsogkas & Kokkinos, 2012) or surface orientation, and depth (Eigen & Fergus, 2014). We also intend to further explore the merit of our detector in the context of high- level tasks, such as object detection and recognition.  7 ACKNOWLEDGEMENTS  This work was supported by FP7-RECONFIG and equipment donated by NVIDIA. I thank the authors of Xie & Tu (2015) for inspiration, Alp Guler for illustrations and tables, Kostas Pa- pazafeiropoulos for help with porting Damascene to Caffe, George Papandreou for guidance on Caffe and Pierre-Andr´e Savalle for teaching me to handle prototxt ﬁles like a professional seduser.  9  Under review as a conference paper at ICLR 2016  8 SUPPLEMENTAL MATERIAL  We provide below qualitative results on images from the Pascal VOC test set.  (a)  (b)  (c)  (d)  (e)  Figure 7: Indicative results on the PASCAL VOC 2012 test set: for each image we show in (b) the ﬁnal estimate of the probability of boundary, in (c) three leading eigenvectors delivered by Normal- ized Cuts (d) the semantic segmentation that would be obtained by our multi-scale DCNN variant of DeepLab, prior to DenseCRF inference and (e) the improved result obtained by combining Dense- CRF inference with the normalized Cut embeddings and the image boundaries.  10  Under review as a conference paper at ICLR 2016  REFERENCES Adams, Andrew, Baek, Jongmin, and Davis, Myers Abraham. Fast high-dimensional ﬁltering using the permu-  tohedral lattice. In Computer Graphics Forum, 2010.  Arbelaez, Pablo, Maire, Michael, Fowlkes, Charless, and Malik, Jitendra. Contour detection and hierarchical  image segmentation. PAMI, 2011.  Belkin, Mikhail and Niyogi, Partha. Laplacian eigenmaps and spectral techniques for embedding and cluster-  ing. In NIPS, 2001.  Bertasius, Gedas, Shi, Jianbo, and Torresani, Lorenzo. Deepedge: A multi-scale bifurcated deep network for  top-down contour detection. In Proc. CVPR, 2015.  Blake, Andrew and Zisserman, Andrew. Visual Reconstruction. MIT Press, 1987.  Catanzaro, Bryan C., Su, Bor-Yiing, Sundaram, Narayanan, Lee, Yunsup, Murphy, Mark, and Keutzer, Kurt.  Efﬁcient, high-quality image contour detection. In Proc. ICCV, 2009.  Chen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Semantic  image segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2015.  Cour, Timoth´ee, B´en´ezit, Florence, and Shi, Jianbo. Spectral segmentation with multiscale graph decomposi-  tion. In Proc. CVPR, 2005.  Dai, Jifeng, He, Kaiming, and Sun, Jian. Boxsup: Exploiting bounding boxes to supervise convolutional  networks for semantic segmentation. arXiv preprint arXiv:1503.01640, 2015.  Dietterich, Thomas G., Lathrop, Richard H., and Lozano-perez, Tomas. Solving the multiple-instance problem  with axis-parallel rectangles. Artiﬁcial Intelligence, 89:31–71, 1997.  Dollar, P., Tu, Z., and Belongie, S. Supervised Learning of Edges and Object Boundaries. In Proc. CVPR,  2006.  Doll´ar, Piotr and Zitnick, C. Lawrence. Fast edge detection using structured forests. PAMI, 37(8):1558–1570,  2015.  Eigen, David and Fergus, Rob. Predicting depth, surface normals and semantic labels with a common multi-  scale convolutional architecture. arXiv:1411.4734, 2014.  Eigen, David, Puhrsch, Christian, and Fergus, Rob. Depth map prediction from a single image using a multi-  scale deep network. In NIPS, 2014.  Ganin, Yaroslav and Lempitsky, Victor. Nˆ 4-ﬁelds: Neural network nearest neighbor ﬁelds for image trans-  forms. In Computer Vision–ACCV 2014, pp. 536–551. Springer, 2014.  Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik, Jitendra. Rich feature hierarchies for accurate object  detection and semantic segmentation. In CVPR, 2014.  Hwang, J.-J. and Liu, T.-L. Pixel-wise deep learning for contour detection. In ICLR, 2015.  Kivinen, Jyri J., Williams, Christopher K. I., and Heess, Nicolas. Visual boundary prediction: A deep neural  prediction network and quality dissection. In AISTATS, 2014.  Kokkinos, Iasonas. Boundary detection using f-measure-, ﬁlter- and feature- (f3) boost. In ECCV, 2010a. Kokkinos, Iasonas. Highly accurate boundary detection and grouping. In Proc. CVPR, 2010b.  Konishi, S., Yuille, A., Coughlan, J., and Zhu, S.-C. Statistical edge detection: Learning and evaluating edge  cues. IEEE Trans. PAMI, 25(1):57–74, 2003.  Kr¨ahenb¨uhl, Philipp and Koltun, Vladlen. Efﬁcient inference in fully connected crfs with gaussian edge poten-  tials. In NIPS, 2011.  Kr¨ahenb¨uhl, Philipp and Koltun, Vladlen. Learning to propose objects. In Proc. CVPR, 2015.  Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural net-  works. In NIPS, 2013.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. In  Proc. IEEE, 1998.  11  Under review as a conference paper at ICLR 2016  Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick W., Zhang, Zhengyou, and Tu, Zhuowen. Deeply-supervised  nets. In Proc. AISTATS, 2015.  Lin, Guosheng, Shen, Chunhua, Reid, Ian, et al. Efﬁcient piecewise training of deep structured models for  semantic segmentation. arXiv preprint arXiv:1504.01013, 2015.  Liu, Ziwei, Li, Xiaoxiao, Luo, Ping, Loy, Chen Change, and Tang, Xiaoou. Semantic image segmentation via  deep parsing network. arXiv preprint arXiv:1509.02634, 2015.  Long, Jonathan, Shelhamer, Evan, and Darrell, Trevor. Fully convolutional networks for semantic segmenta-  tion. CoRR, abs/1411.4038, 2014. URL http://arxiv.org/abs/1411.4038.  Martin, D., Fowlkes, C., and Malik, J. Learning to detect natural image boundaries using local brightness,  color, and texture cues. IEEE Trans. PAMI, 26(5):530–549, 2004.  Mottaghi, Roozbeh, Chen, Xianjie, Liu, Xiaobai, Cho, Nam-Gyu, Lee, Seong-Whan, Fidler, Sanja, Urtasun, Raquel, and Yuille, Alan. The role of context for object detection and semantic segmentation in the wild. In Proc. CVPR, 2014.  Oquab, Maxime, Bottou, L´eon, Laptev, Ivan, and Sivic, Josef.  Is object localization for free? - weakly-  supervised learning with convolutional neural networks. In Proc. CVPR, 2015.  Papandreou, George, Chen, Liang-Chieh, Murphy, Kevin, and Yuille, Alan L. Weakly- and semi-supervised  learning of a DCNN for semantic image segmentation. In Proc. ICCV, 2015a.  Papandreou, George, Kokkinos, Iasonas, and Savalle, Pierre-Andr´e. Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection. In Proc. CVPR, 2015b.  Ren, Xiaofeng. Multiscale helps boundary detection. In ECCV, 2008.  Ren, Xiaofeng and Bo, Liefeng. Discriminatively trained sparse code gradients for contour detection. In NIPS,  2012.  Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Yann. Overfeat: Integrated recogni-  tion, localization and detection using convolutional networks. In ICLR, 2014.  Shen, Wei, Wang, Xinggang, Wang, Yan, Bai, Xiang, and Zhang, Zhijiang. Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3982–3991, 2015.  Shi, Jianbo and Malik, Jitendra. Normalized cuts and image segmentation. In Proc. CVPR, 1997.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition.  arXiv:1409.1556, 2014.  Sironi, A., Turetken, E., Lepetit, V., and Fua., P. Multiscale centerline detection. PAMI, 2015.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du- mitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. arXiv:1409.4842, 2014.  Tsogkas, Stavros and Kokkinos, Iasonas. Learning-based symmetry detection in natural images.  ECCV, 2012.  In Proc.  Witkin, A.P. Scale-space ﬁltering. In Proc. Int. Joint Conf. on Artiﬁcial Intel., pp. 1019–1022, 1983.  Xie, Saining and Tu, Zhuowen. Holistically-nested edge detection. In Proc. ICCV, 2015.  Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du, Dalong, In Proc.  Huang, Chang, and Torr, Philip H. S. Conditional random ﬁelds as recurrent neural networks. ICCV, 2015.  Zhu, Qihui, Song, Gang, and Shi, Jianbo. Untangling cycles for contour grouping. In Proc. CVPR, 2007.  Zhu, Yan, Tian, Yuandong, Mexatas, Dimitris, and Doll´ar, Piotr. Semantic amodal segmentation. CoRR,  abs/1509.01329, 2015. URL http://arxiv.org/abs/1509.01329.  12  ",
1509.06664,2016,Reasoning about Entailment with Neural Attention,"['Reasoning about Entailment with Neural Attention\nTim Rocktäschel', 'Edward Grefenstette', 'Karl Moritz Hermann', 'Tomáš Kočiský', 'Phil Blunsom']",https://arxiv.org/pdf/1509.06664,"6 1 0 2    r a  M 1         ] L C . s c [      4 v 4 6 6 6 0  .  9 0 5 1 : v i X r a  Published as a conference paper at ICLR 2016  REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION  Tim Rockt¨aschel University College London t.rocktaschel@cs.ucl.ac.uk  Edward Grefenstette & Karl Moritz Hermann Google DeepMind {etg,kmh}@google.com  Tom´aˇs Koˇcisk´y & Phil Blunsom Google DeepMind & University of Oxford {tkocisky,pblunsom}@google.com  ABSTRACT  While most approaches to automatically recognizing entailment relations have used classiﬁers employing hand engineered features derived from complex nat- ural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classiﬁers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for en- tailment failed to outperform such a simple similarity classiﬁer. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classiﬁer with engineered features by a substantial margin. It is the ﬁrst generic end-to-end differentiable system that achieves state-of-the-art accuracy on a tex- tual entailment dataset.  1  INTRODUCTION  The ability to determine the semantic relationship between two sentences is an integral part of ma- chines that understand and reason with natural language. Recognizing textual entailment (RTE) is the task of determining whether two natural language sentences are (i) contradicting each other, (ii) not related, or whether (iii) the ﬁrst sentence (called premise) entails the second sentence (called hypothesis). This task is important since many natural language processing (NLP) problems, such as information extraction, relation extraction, text summarization or machine translation, rely on it explicitly or implicitly and could beneﬁt from more accurate RTE systems (Dagan et al., 2006). State-of-the-art systems for RTE so far relied heavily on engineered NLP pipelines, extensive manual creation of features, as well as various external resources and specialized subcomponents such as negation detection (e.g. Lai and Hockenmaier, 2014; Jimenez et al., 2014; Zhao et al., 2014; Beltagy et al., 2015). Despite the success of neural networks for paraphrase detection (e.g. Socher et al., 2011; Hu et al., 2014; Yin and Sch¨utze, 2015), end-to-end differentiable neural architectures failed to get close to acceptable performance for RTE due to the lack of large high-quality datasets. An end-to-end differentiable solution to RTE is desirable, since it avoids speciﬁc assumptions about the underlying language. In particular, there is no need for language features like part-of-speech tags or dependency parses. Furthermore, a generic sequence-to-sequence solution allows to extend the concept of capturing entailment across any sequential data, not only natural language. Recently, Bowman et al. (2015) published the Stanford Natural Language Inference (SNLI) cor- pus accompanied by a neural network with long short-term memory units (LSTM, Hochreiter and Schmidhuber, 1997), which achieves an accuracy of 77.6% for RTE on this dataset. It is the ﬁrst time a generic neural model without hand-crafted features got close to the accuracy of a simple lexicalized classiﬁer with engineered features for RTE. This can be explained by the high quality  1  Published as a conference paper at ICLR 2016  and size of SNLI compared to the two orders of magnitude smaller and partly synthetic datasets so far used to evaluate RTE systems. Bowman et al.’s LSTM encodes the premise and hypothesis as dense ﬁxed-length vectors whose concatenation is subsequently used in a multi-layer perceptron (MLP) for classiﬁcation. In contrast, we are proposing an attentive neural network that is capable of reasoning over entailments of pairs of words and phrases by processing the hypothesis conditioned on the premise. Our contributions are threefold: (i) We present a neural model based on LSTMs that reads two sen- tences in one go to determine entailment, as opposed to mapping each sentence independently into a semantic space (§2.2), (ii) We extend this model with a neural word-by-word attention mechanism to encourage reasoning over entailments of pairs of words and phrases (§2.4), and (iii) We provide a detailed qualitative analysis of neural attention for RTE (§4.1). Our benchmark LSTM achieves an accuracy of 80.9% on SNLI, outperforming a simple lexicalized classiﬁer tailored to RTE by 2.7 percentage points. An extension with word-by-word neural attention surpasses this strong bench- mark LSTM result by 2.6 percentage points, setting a new state-of-the-art accuracy of 83.5% for recognizing entailment on SNLI.  2 METHODS In this section we discuss LSTMs (§2.1) and describe how they can be applied to RTE (§2.2). We introduce an extension of an LSTM for RTE with neural attention (§2.3) and word-by-word attention (§2.4). Finally, we show how such attentive models can easily be used for attending both ways: over the premise conditioned on the hypothesis and over the hypothesis conditioned on the premise (§2.5).  2.1 LSTMS  Recurrent neural networks (RNNs) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) have been successfully applied to a wide range of NLP tasks, such as machine translation (Sutskever et al., 2014), constituency parsing (Vinyals et al., 2014), language modeling (Zaremba et al., 2014) and recently RTE (Bowman et al., 2015). LSTMs encompass memory cells that can store information for a long period of time, as well as three types of gates that control the ﬂow of information into and out of these cells: input gates (Eq. 2), forget gates (Eq. 3) and output gates (Eq. 4). Given an input vector xt at time step t, the previous output ht−1 and cell state ct−1, an LSTM with hidden size k computes the next output ht and cell state ct as  (1)  (2) (3)  ot = σ(WoH + bo) ct = ft (cid:12) ct−1 + it (cid:12) tanh(WcH + bc) ht = ot (cid:12) tanh(ct)  (4) (5) (6)  where Wi, Wf , Wo, Wc ∈ R2k×k are trained matrices and bi, bf , bo, bc ∈ Rk trained biases that parameterize the gates and transformations of the input, σ denotes the element-wise application of the sigmoid function and (cid:12) the element-wise multiplication of two vectors.  2.2 RECOGNIZING TEXTUAL ENTAILMENT WITH LSTMS  LSTMs can readily be used for RTE by independently encoding the premise and hypothesis as dense vectors and taking their concatenation as input to an MLP classiﬁer (Bowman et al., 2015). This demonstrates that LSTMs can learn semantically rich sentence representations that are suitable for determining textual entailment.  2.2.1 CONDITIONAL ENCODING  In contrast to learning sentence representations, we are interested in neural models that read both sentences to determine entailment, thereby reasoning over entailments of pairs of words and phrases. Figure 1 shows the high-level structure of this model. The premise (left) is read by an LSTM. A sec- ond LSTM with different parameters is reading a delimiter and the hypothesis (right), but its memory  2  (cid:21)  (cid:20) xt  ht−1  H =  it = σ(WiH + bi) ft = σ(Wf H + bf )  Published as a conference paper at ICLR 2016  Figure 1: Recognizing textual entailment using (A) conditional encoding via two LSTMs, one over the premise and one over the hypothesis conditioned on the representation of the premise (c5), (B) attention only based on the last output vector (h9) or (C) word-by-word attention based on all output vectors of the hypothesis (h7, h8 and h9).  state is initialized with the last cell state of the previous LSTM (c5 in the example), i.e. it is condi- tioned on the representation that the ﬁrst LSTM built for the premise (A). We use word2vec vectors (Mikolov et al., 2013) as word representations, which we do not optimize during training. Out-of- vocabulary words in the training set are randomly initialized by sampling values uniformly from (−0.05, 0.05) and optimized during training.1 Out-of-vocabulary words encountered at inference time on the validation and test corpus are set to ﬁxed random vectors. By not tuning representations of words for which we have word2vec vectors, we ensure that at inference time their representation stays close to unseen similar words for which we have word2vec embeddings. We use a linear layer to project word vectors to the dimensionality of the hidden size of the LSTM, yielding input vectors xi. Finally, for classiﬁcation we use a softmax layer over the output of a non-linear projection of the last output vector (h9 in the example) into the target space of the three classes (ENTAILMENT, NEUTRAL or CONTRADICTION), and train using the cross-entropy loss.  2.3 ATTENTION  Attentive neural networks have recently demonstrated success in a wide range of tasks ranging from handwriting synthesis (Graves, 2013), digit classiﬁcation (Mnih et al., 2014), machine translation (Bahdanau et al., 2015), image captioning (Xu et al., 2015), speech recognition (Chorowski et al., 2015) and sentence summarization (Rush et al., 2015), to geometric reasoning (Vinyals et al., 2015). The idea is to allow the model to attend over past output vectors (see Figure 1 B), thereby mitigating the LSTM’s cell state bottleneck. More precisely, an LSTM with attention for RTE does not need to capture the whole semantics of the premise in its cell state. Instead, it is sufﬁcient to output vectors while reading the premise and accumulating a representation in the cell state that informs the second LSTM which of the output vectors of the premise it needs to attend over to determine the RTE class. Let Y ∈ Rk×L be a matrix consisting of output vectors [h1 ··· hL] that the ﬁrst LSTM produced when reading the L words of the premise, where k is a hyperparameter denoting the size of em- beddings and hidden layers. Furthermore, let eL ∈ RL be a vector of 1s and hN be the last output vector after the premise and hypothesis were processed by the two LSTMs respectively. The atten- tion mechanism will produce a vector α of attention weights and a weighted representation r of the premise via  M = tanh(WyY + WhhN ⊗ eL) α = softmax(wT M) r = YαT  M ∈ Rk×L α ∈ RL r ∈ Rk  (7) (8) (9)  1We found 12.1k words in SNLI for which we could not obtain word2vec embeddings, resulting in 3.65M  tunable parameters.  3  x1c1h1x2c2h2x3c3h3x4c4h4x5c5h5x6c6h6x7c7h7x8c8h8x9c9h9Aweddingpartytakingpictures::SomeonegotmarriedPremiseHypothesis(A)ConditionalEncoding(C)Word-by-wordAttention(B)AttentionPublished as a conference paper at ICLR 2016  where Wy, Wh ∈ Rk×k are trained projection matrices, w ∈ Rk is a trained parameter vector and wT denotes its transpose. Note that the outer product WhhN ⊗ eL is repeating the linearly trans- formed hN as many times as there are words in the premise (i.e. L times). Hence, the intermediate attention representation mi (ith column vector in M) of the ith word in the premise is obtained from a non-linear combination of the premise’s output vector hi (ith column vector in Y) and the transformed hN . The attention weight for the ith word in the premise is the result of a weighted combination (parameterized by w) of values in mi. The ﬁnal sentence-pair representation is obtained from a non-linear combination of the attention- weighted representation r of the premise and the last output vector hN using ∗ ∈ Rk h  = tanh(Wpr + WxhN )  (10)  ∗ h  where Wp, Wx ∈ Rk×k are trained projection matrices.  2.4 WORD-BY-WORD ATTENTION  For determining whether one sentence entails another it can be a good strategy to check for en- tailment or contradiction of individual word- and phrase-pairs. To encourage such behavior we employ neural word-by-word attention similar to Bahdanau et al. (2015), Hermann et al. (2015) and Rush et al. (2015). The difference is that we do not use attention to generate words, but to obtain a sentence-pair encoding from ﬁne-grained reasoning via soft-alignment of words and phrases in the premise and hypothesis. In our case, this amounts to attending over the ﬁrst LSTM’s output vectors of the premise while the second LSTM processes the hypothesis one word at a time, thus generating attention weight-vectors αt over all output vectors of the premise for every word xt with t ∈ (L + 1, N ) in the hypothesis (Figure 1 C). This can be modeled as follows:  t + tanh(Wtrt−1)  Mt = tanh(WyY + (Whht + Wrrt−1) ⊗ eL) αt = softmax(wT Mt) rt = YαT  (11) (12) (13) Note that rt is dependent on the previous attention representation rt−1 to inform the model about what was attended over in the previous step (see Eq. 11 and 13). As in the previous section, the ﬁnal sentence-pair representation is obtained from a non-linear com- bination of the last attention-weighted representation of the premise (here based on the last word of the hypothesis) rN and the last output vector using = tanh(WprN + WxhN )  Mt ∈ Rk×L αt ∈ RL rt ∈ Rk  ∗ ∈ Rk h  (14)  ∗ h  2.5 TWO-WAY ATTENTION  Inspired by bidirectional LSTMs that read a sequence and its reverse for improved encoding (Graves and Schmidhuber, 2005), we introduce two-way attention for RTE. The idea is to use the same model (i.e. same structure and weights) to attend over the premise conditioned on the hypothesis, as well as to attend over the hypothesis conditioned on the premise, by simply swapping the two sequences. This produces two sentence-pair representations that we concatenate for classiﬁcation.  3 EXPERIMENTS  We conduct experiments on the Stanford Natural Language Inference corpus (SNLI, Bowman et al., 2015). This corpus is two orders of magnitude larger than other existing RTE corpora such as Sentences Involving Compositional Knowledge (SICK, Marelli et al., 2014). Furthermore, a large part of training examples in SICK were generated heuristically from other examples. In contrast, all sentence-pairs in SNLI stem from human annotators. The size and quality of SNLI make it a suitable resource for training neural architectures such as the ones proposed in this paper. We use ADAM (Kingma and Ba, 2015) for optimization with a ﬁrst momentum coefﬁcient of 0.9 and a second momentum coefﬁcient of 0.999.2 For every model we perform a small grid search  2Standard conﬁguration recommended by Kingma and Ba.  4  Published as a conference paper at ICLR 2016  Table 1: Results on the SNLI corpus.  Model Lexicalized classiﬁer (Bowman et al., 2015) LSTM (Bowman et al., 2015) Conditional encoding, shared Conditional encoding, shared Conditional encoding Attention Attention, two-way Word-by-word attention Word-by-word attention, two-way  |θ|W+M -  |θ|M k - - 100 ≈ 10M 221k 111k 100 252k 159 116 252k 242k 100 242k 100 252k 100 100 252k  3.8M 3.9M 3.9M 3.9M 3.9M 3.9M 3.9M  Train Dev 99.7 84.4 83.7 84.4 83.5 85.4 86.5 85.3 86.6  - - 81.9 83.0 82.1 83.2 83.0 83.7 83.6  Test 78.2 77.6 80.9 81.4 80.9 82.3 82.4 83.5 83.2  over combinations of the initial learning rate [1E-4, 3E-4, 1E-3], dropout3 [0.0, 0.1, 0.2] and (cid:96)2- regularization strength [0.0, 1E-4, 3E-4, 1E-3]. Subsequently, we take the best conﬁguration based on performance on the validation set, and evaluate only that conﬁguration on the test set.  4 RESULTS AND DISCUSSION  Results on the SNLI corpus are summarized in Table 1. The total number of model parameters, including tunable word representations, is denoted by |θ|W+M (without word representations |θ|M). To ensure a comparable number of parameters to Bowman et al.’s model that encodes the premise and hypothesis independently using one LSTM, we also run experiments for conditional encoding where the parameters of both LSTMs are shared (“Conditional encoding, shared” with k = 100) as opposed to using two independent LSTMs. In addition, we compare our attentive models to two benchmark LSTMs whose hidden sizes were chosen so that they have at least as many parameters as the attentive models. Since we are not tuning word vectors for which we have word2vec embeddings, the total number of parameters |θ|W+M of our models is considerably smaller. We also compare our models against the benchmark lexicalized classiﬁer used by Bowman et al., which constructs features from the BLEU score between the premise and hypothesis, length difference, word overlap, uni- and bigrams, part-of-speech tags, as well as cross uni- and bigrams.  Conditional Encoding We found that processing the hypothesis conditioned on the premise in- stead of encoding each sentence independently gives an improvement of 3.3 percentage points in accuracy over Bowman et al.’s LSTM. We argue this is due to information being able to ﬂow from the part of the model that processes the premise to the part that processes the hypothesis. Specif- ically, the model does not waste capacity on encoding the hypothesis (in fact it does not need to encode the hypothesis at all), but can read the hypothesis in a more focused way by checking words and phrases for contradictions and entailments based on the semantic representation of the premise. One interpretation is that the LSTM is approximating a ﬁnite-state automaton for RTE (cf. Angeli and Manning, 2014). Another difference to Bowman et al.’s model is that we are using word2vec instead of GloVe for word representations and, more importantly, do not ﬁne-tune these word em- beddings. The drop in accuracy from train to test set is less severe for our models, which suggest that ﬁne-tuning word embeddings could be a cause of overﬁtting. Our LSTM outperforms a simple lexicalized classiﬁer by 2.7 percentage points. To the best of our knowledge, this is the ﬁrst instance of a neural end-to-end differentiable model to achieve state-of- the-art performance on a textual entailment dataset.  Attention By incorporating an attention mechanism we found a 0.9 percentage point improvement over a single LSTM with a hidden size of 159, and a 1.4 percentage point increase over a benchmark model that uses two LSTMs for conditional encoding (one for the premise and one for the hypothesis conditioned on the representation of the premise). The attention model produces output vectors  3As in Zaremba et al. (2014), we apply dropout only on the inputs and outputs of the network.  5  Published as a conference paper at ICLR 2016  (a)  (c)  (b)  (d)  Figure 2: Attention visualizations.  summarizing contextual information of the premise that is useful to attend over later when reading the hypothesis. Therefore, when reading the premise, the model does not have to build up a semantic representation of the whole premise, but instead a representation that helps attending over the right output vectors when processing the hypothesis. In contrast, the output vectors of the premise are not used by the benchmark LSTMs. Thus, these models have to build up a representation of the whole premise and carry it over through the cell state to the part that processes the hypothesis—a bottleneck that can be overcome to some degree by using attention.  Word-by-word Attention Enabling the model to attend over output vectors of the premise for ev- ery word in the hypothesis yields another 1.2 percentage point improvement compared to attending based only on the last output vector of the premise. We argue that this is due to the model being able to check for entailment or contradiction of individual words and phrases in the hypothesis, and demonstrate this effect in the qualitative analysis below.  Two-way Attention Allowing the model to also attend over the hypothesis based on the premise does not seem to improve performance for RTE. We suspect that this is due to entailment being an asymmetric relation. Hence, using the same LSTM to encode the hypothesis (in one direction) and the premise (in the other direction) might lead to noise in the training signal. This could be addressed by training different LSTMs at the cost of doubling the number of model parameters.  4.1 QUALITATIVE ANALYSIS  It is instructive to analyze which output representations the model is attending over when deciding the class of an RTE example. Note that interpretations based on attention weights have to be taken with care since the model is not forced to solely rely on representations obtained from attention (see hN in Eq. 10 and 14). In the following we visualize and discuss the attention patterns of the presented attentive models. For each attentive model we hand-picked examples from ten randomly drawn samples of the validation set.  Attention Figure 2 shows to what extent the attentive model focuses on contextual representations of the premise after both LSTMs processed the premise and hypothesis respectively. Note how the model pays attention to output vectors of words that are semantically coherent with the premise (“riding” and “rides”, “animal” and “camel”, 2a) or in contradiction, as caused by a single word (“blue” vs. “pink”, 2b) or multiple words (“swim” and “lake” vs. “frolicking” and “grass”, 2c). Interestingly, the model shows contextual understanding by not attending over “yellow”, the color  6  Published as a conference paper at ICLR 2016  (a)  (b)  (c)  (d)  (e)  (f)  (g)  Figure 3: Word-by-word attention visualizations.  7  Published as a conference paper at ICLR 2016  of the toy, but “pink”, the color of the coat. However, for more involved examples with longer premises we found that attention is more uniformly distributed (2d). This suggests that conditioning attention only on the last output has limitations when multiple words need to be considered for deciding the RTE class.  Word-by-word Attention Visualizations of word-by-word attention are depicted in Figure 3. We found that word-by-word attention can easily detect if the hypothesis is simply a reordering of words in the premise (3a). Furthermore, it is able to resolve synonyms (“airplane” and “aircraft”, 3c) and capable of matching multi-word expressions to single words (“garbage can” to “trashcan”, 3b). It is also noteworthy that irrelevant parts of the premise, such as words capturing little meaning or whole uninformative relative clauses, are correctly neglected for determining entailment (“which also has a rope leading out of it”, 3b). Word-by-word attention seems to also work well when words in the premise and hypothesis are connected via deeper semantics or common-sense knowledge (“snow” can be found “outside” and a “mother” is an “adult”, 3e and 3g). Furthermore, the model is able to resolve one-to-many relation- ships (“kids” to “boy” and “girl”, 3d) Attention can fail, for example when the two sentences and their words are entirely unrelated (3f). In such cases, the model seems to back up to attending over function words, and the sentence-pair representation is likely dominated by the last output vector (hN ) instead of the attention-weighted representation (see Eq. 14).  5 CONCLUSION  In this paper, we show how the state-of-the-art in recognizing textual entailment on a large, human- curated and annotated corpus, can be improved with general end-to-end differentiable models. Our results demonstrate that LSTM recurrent neural networks that read pairs of sequences to produce a ﬁnal representation from which a simple classiﬁer predicts entailment, outperform both a neural baseline as well as a classiﬁer with hand-engineered features. Extending these models with attention over the premise provides further improvements to the predictive abilities of the system, resulting in a new state-of-the-art accuracy for recognizing entailment on the Stanford Natural Language Inference corpus. The models presented here are general sequence models, requiring no appeal to Natural Language- speciﬁc processing beyond tokenization, and are therefore a suitable target for transfer learning through pre-training the recurrent systems on other corpora, and conversely, applying the models trained on this corpus to other entailment tasks. Future work will focus on such transfer learning tasks, as well as scaling the methods presented here to larger units of text (e.g. paragraphs and entire documents) using hierarchical attention mechanisms. Additionally, it would be worthwhile exploring how other, more structured forms of attention (e.g. Graves et al., 2014; Sukhbaatar et al., 2015), or other forms of differentiable memory (e.g. Grefenstette et al., 2015; Joulin and Mikolov, 2015) could help improve performance on RTE over the neural models presented in this paper. Furthermore, we aim to investigate the application of these generic models to non-natural language sequential entailment problems.  ACKNOWLEDGEMENTS  We thank Nando de Freitas, Samuel Bowman, Jonathan Berant, Danqi Chen, Christopher Manning, and the anonymous ICLR reviewers for their helpful comments on drafts of this paper.  REFERENCES Gabor Angeli and Christopher D Manning. Naturalli: Natural logic inference for common sense reasoning. In  EMNLP, 2014.  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to  align and translate. In ICLR, 2015.  Islam Beltagy, Stephen Roller, Pengxiang Cheng, Katrin Erk, and Raymond J. Mooney. Representing meaning  with a combination of logical form and vectors. arXiv preprint arXiv:1505.06816, 2015.  8  Published as a conference paper at ICLR 2016  Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus  for learning natural language inference. In EMNLP, 2015.  Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attention-based  models for speech recognition. In NIPS, 2015.  Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine Learning Challenges. Lecture Notes in Computer Science, volume 3944, pages 177–190. Springer, 2006.  Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. Alex Graves and J¨urgen Schmidhuber. Framewise phoneme classiﬁcation with bidirectional lstm and other  neural network architectures. Neural Networks, 18(5):602–610, 2005.  Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with  unbounded memory. In NIPS, 2015.  Karl Moritz Hermann, Tom´aˇs Koˇcisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,  and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, 2015.  Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,  1997.  Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. Convolutional neural network architectures for  matching natural language sentences. In NIPS, pages 2042–2050, 2014.  Sergio Jimenez, George Duenas, Julia Baquero, Alexander Gelbukh, Av Juan Dios B´atiz, and Av Mendiz´abal. Unal-nlp: Combining soft cardinality features for semantic textual similarity, relatedness and entailment. In SemEval 2014, page 732, 2014.  Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. arXiv  preprint arXiv:1503.01007, 2015.  Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. Alice Lai and Julia Hockenmaier. SemEval 2014, page 329, 2014.  Illinois-lh: A denotational and distributional approach to semantics.  In  Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In SemEval 2014, 2014.  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words  and phrases and their compositionality. In NIPS, pages 3111–3119, 2013.  Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In NIPS, pages  2204–2212, 2014.  Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence sum-  marization. In EMNLP, 2015.  Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng. Dynamic pooling  and unfolding recursive autoencoders for paraphrase detection. In NIPS, pages 801–809, 2011.  Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. arXiv  preprint arXiv:1503.08895, 2015.  Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In NIPS,  pages 3104–3112, 2014.  Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as a  foreign language. In NIPS, 2014.  Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In NIPS, 2015. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.  Wenpeng Yin and Hinrich Sch¨utze. Convolutional neural network for paraphrase identiﬁcation. In NAACL-  HLT, pages 901–911, 2015.  Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint  arXiv:1409.2329, 2014.  Jiang Zhao, Tian Tian Zhu, and Man Lan. Ecnu: One stone two birds: Ensemble of heterogenous measures for  semantic relatedness and textual entailment. In SemEval 2014, page 271, 2014.  9  ",
1511.06067,2016,Convolutional Neural Networks With Low-rank Regularization,"['Convolutional Neural Networks With Low-rank Regularization\nCheng Tai', 'Tong Xiao', 'Yi Zhang', 'Xiaogang Wang', 'Weinan E']",https://arxiv.org/pdf/1511.06067,"6 1 0 2     b e F 4 1         ]  G L . s c [      3 v 7 6 0 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  CONVOLUTIONAL NEURAL NETWORKS WITH LOW- RANK REGULARIZATION  Cheng Tai1, Tong Xiao2, Yi Zhang3, Xiaogang Wang2, Weinan E1 1The Program in Applied and Computational Mathematics, Princeton University 2Department of Electronic Engineering, The Chinese University of Hong Kong 3Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor {chengt,weinan}@math.princeton.edu; yeezhang@umich.edu {xiaotong,xgwang}@ee.cuhk.edu.hk  ABSTRACT  Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problem- atic for deploying these models on mobile devices. Recently, tensor decomposi- tions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolu- tion kernels. The algorithm ﬁnds the exact global optimizer of the decomposi- tion and is more effective than iterative methods. Based on the decomposition, we further propose a new method for training low-rank constrained CNNs from scratch. Interestingly, while achieving a signiﬁcant speedup, sometimes the low- rank constrained CNNs delivers signiﬁcantly better performance than their non- constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank NIN model achieves 91.31% accuracy (without data augmentation), which also im- proves upon state-of-the-art result. We evaluated the proposed method on CIFAR- 10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet, NIN, VGG and GoogleNet with success. For example, the forward time of VGG- 16 is reduced by half while the performance is still comparable. Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speed- ing up large CNNs.  1  INTRODUCTION  Over the course of three years, CNNs have revolutionized computer vision, setting new performance standards in many important applications, see e.g., Krizhevsky et al. (2012); Farabet et al. (2013); Long et al. (2014). The breakthrough has been made possible by the abundance of training data, the deployment of new computational hardware (most notably, GPUs and CPU clusters) and large models. These models typically require a huge number of parameters (107 ∼ 109) to achieve state- of-the-art performance, and may take weeks to train even with high-end GPUs. On the other hand, there is a growing interest in deploying CNNs to low-end mobile devices. On such processors, the computational cost of applying the model becomes problematic, let alone training one, especially when real-time operation is needed. Storage of millions of parameters also complicates the deploy- ment. Modern CNNs would ﬁnd many more applications if both the computational cost and the storage requirement could be signiﬁcantly reduced. There are only a few recent works for speeding up CNNs. Denton et al. (2014) proposed some low-rank approximation and clustering schemes for the convolutional kernels. They achieved 2x speedup for a single convolutional layer with 1% drop in classiﬁcation accuracy. Jaderberg et al. (2014) suggested using different tensor decomposition schemes, reporting a 4.5x speedup with 1% drop in accuracy in a text recognition application. Lebedev et al. (2014) further explored the use of CP decomposition to approximate the convolutional kernels. Vanhoucke et al. (2011) showed that using 8-bit quantization of the parameters can result in signiﬁcant speedup with minimal loss of accuracy. This method can be used in conjunction with low-rank approximations to achieve further speedup.  1  Published as a conference paper at ICLR 2016  As convolution operations constitute the bulk of all computations in CNNs, simplifying the convo- lution layer would have a direct impact on the overall speedup. The convolution kernels in a typical CNN is a 4D tensor. The key observation is that there might be a signiﬁcant amount of redundancy in the tensor. Ideas based on tensor decomposition seem to be a particularly promising way to remove the redundancy as suggested by some previous works. In this paper, we further develop the tensor decomposition idea. Our method is based on Jaderberg et al. (2014), but has several signiﬁcant improvements. The contributions are summarized as follows: • A new algorithm for computing the low-rank tensor decomposition. Low-rank tensor de- compositions are non-convex problems and difﬁcult to compute in general, Jaderberg et al. (2014) use iterative schemes to get an approximate local solution. But we ﬁnd that the par- ticular form of low-rank decomposition in (Jaderberg et al., 2014) has an exact closed form solution which is the global optimum. Hence we obtain the best data-independent approx- imation. Furthermore, computing the exact solution is much more effective than iterative schemes. As the tensor decomposition is the most important step in approximating CNNs, being able to obtain an exact solution efﬁciently thus provides great advantages.  • A new method for training low-rank constrained CNNs from scratch. Most previous works only focus on improving testing time computation cost. This is achieved by approximating and ﬁne-tuning a pre-trained network. Based on the low-rank tensor decomposition, we ﬁnd that the convolutional kernels can be parameterized in a way that naturally enforces the low- rank constraint. Networks parameterized in this low-rank constrained manner have more layers than their non-constrained counterparts. While it is widely observed that deeper networks are harder to train, we are able to train very deep low-rank constrained CNNs with more than 30 layers with the help of a recent training technique called batch normalization Ioffe & Szegedy (2015).  • Evaluation on large networks. Previous experiments in Jaderberg et al. (2014) and Denton et al. (2014) give some promises of the effectiveness of low-rank approximations. But these methods have not been tested extensively for large models and generic datasets. Moreover, as iterative methods are used to ﬁnd the approximation, bad local minima may hurt perfor- mance. In this paper, we test the proposed method for various state-of-the-art CNN models, including NIN (Lin et al., 2013), AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2014) and GoogleNet (Szegedy et al., 2014). The datasets used include CIFAR- 10 and ILSVRC12. We achieved signiﬁcant speedups for these models with comparable or even better performance. Success on a variety of CNN models give strong evidence that low-rank tensor decomposition can be a very useful tool for simplifying and improving deep CNNs.  Our numerical experiments show that signiﬁcant speedup can be achieved with minimal loss of performance, which is consistent with previously reported results. Surprisingly, while all previous efforts report a slight decrease or no change in performance, we found a signiﬁcant increase of classiﬁcation accuracy in some cases. In particular, on the CIFAR-10 dataset, we achieve 91.31% classiﬁcation accuracy (without data augmentation) with the low-rank NIN model, which improves upon not only the original NIN but also upon state-of-the-art results on this dataset. We are not aware of signiﬁcant improvements with low-rank approximations being reported in the previous literature. The rest of the paper is organized as follows. We discuss some related work in section 2. We then introduce our decomposition scheme in section 3. Results with typical networks including AlexNet, NIN, VGG and GoogleNet on CIFAR10 and ILSVRC12 datasets are reported in section 4. We conclude with the summary and discussion in Section 5.  2 RELATED WORK  Using low-rank ﬁlters to accelerate convolution has a long history. Classic examples include high dimensional DCT and wavelet systems constructed from 1D wavelets using tensor products. In the context of dictionary learning, learning separable 1D ﬁlters was suggested by Rigamonti et al. (2013).  2  Published as a conference paper at ICLR 2016  More speciﬁc to CNNs, there are two works that are most related to ours: Jaderberg et al. (2014); Lebedev et al. (2014). For Jaderberg et al. (2014), in addition to the improvements summarized in the previous section, there is another difference in the approximation stage. In Jaderberg et al. (2014), the network is approximated layer by layer. After one layer is approximated by the low- rank ﬁlters, the parameters of that layer are ﬁxed, and the layers above are ﬁne-tuned based on a reconstruction error criterion. Our scheme ﬁne-tunes the entire network simultaneously using a discriminative criterion. While Jaderberg et al. (2014) reported that discriminative ﬁne-tuning was inefﬁcient for their scheme, we found that it works very well in our case. In Lebedev et al. (2014), CP decomposition of the kernel tensors is proposed. Lebedev et al. (2014) used non-linear least squares to compute the CP decomposition. It is also based on the tensor de- composition idea, but our decomposition is based on a different scheme and has some numerical advantages. For the CP decomposition, ﬁnding the best low-rank approximation is an ill-posed problem, and the best rank-K approximation may not exist in the general case, regardless the choice of norm (de Silva & Lim, 2008). But for the proposed scheme, the decomposition always exists, and we have an exact closed form solution for the decomposition. In principle, both the CP de- composition scheme and the proposed scheme can be used to train CNNs from scratch. In the CP decomposition, one convolutional layer is replaced with four convolutional layers. Although the effective depth of the network remains the same, it makes optimization much harder as the gradients of the inserted layers are prone to explosion. Because of this, application of this scheme to larger and deeper models is still problematic due to numerical issues. Lastly, different from both, we consider more and much larger models, which is more challenging. Thus our results provide strong evidence that low-rank approximations can be applicable to a variety of state-of-the-art models.  3 METHOD  In line with the method in Jaderberg et al. (2014), the proposed tensor decomposition scheme is based on a conceptually simple idea: replace the 4D convolutional kernel with two consecutive kernels with a lower rank. In the following, we introduce the details of the decomposition and the algorithms of using the decomposition to approximate a pre-trained network and to train a new one.  3.1 APPROXIMATION OF A PRE-TRAINED CNN Formally, a convolutional kernel in a CNN is a 4D tensor W ∈ RN×d×d×C, where N, C are the numbers of the output and input feature maps respectively and d is the spatial kernel size. We also view W as an 3D ﬁlter array and use notation Wn ∈ Rd×d×C to represent the n-th ﬁlter. Let Z ∈ RX×Y ×C be the input feature map. The output feature map F = (F1,··· ,FN ) is deﬁned as  Fn(x, y) =  Z c(x(cid:48), y(cid:48))W c  n(x − x(cid:48), y − y(cid:48)),  C(cid:88)  X(cid:88)  Y(cid:88)  i=1  x(cid:48)=1  y(cid:48)=1  where the superscript is the index of the channels. The goal is to ﬁnd an approximation ˜W of W that facilitates more efﬁcient computation while maintaining the classiﬁcation accuracy of the CNN. We propose the following scheme:  ˜W c  n =  Hk n(V c  k)T ,  (1)  where K is a hyper-parameter controlling the rank, H ∈ RN×1×d×K is the horizontal ﬁlter, V ∈ RK×d×1×C is the vertical ﬁlter (we have slightly abused the notations to make them concise, Hk and V c With this form, the convolution becomes:  k are both vectors in Rd). Both H and V are learnable parameters.  n  C(cid:88)  K(cid:88)  K(cid:88)  (cid:32) C(cid:88)  (cid:33)  ˜Wn ∗ Z =  Hk n(V c  k)T ∗ Z c =  Hk n ∗  V c k ∗ Z c  (2)  c=1  k=1  k=1  c=1  3  K(cid:88)  k=1  Published as a conference paper at ICLR 2016  (a)  (b)  (c)  Figure 1: (a) Filters in the ﬁrst layer in AlexNet. (b) Low-rank approximation using the proposed schemes with K = 8, corresponding to 3.67× speedup for this layer. Note the low-rank approx- imation captures most of the information, including the directionality of the original ﬁlters. (c) Low-rank ﬁlters trained from scratch with K = 8.  The intuition behind this approximation scheme is to exploit the redundancy that exist both in the spatial dimensions and across channels. Note the convolutions in the above equation are all one dimensional in space. We can estimate the reduction in computation with this scheme. Direct convolution by deﬁnition requires O(d2N CXY ) operations. In the above scheme, the computational cost associated with the vertical ﬁlters is O(dKCXY ) and with horizontal ﬁlters O(dN KXY ), giving a total computational cost of O(dK(N + C)XY ). Acceleration can be achieved if we choose K < dN C N +C . In principle, if C (cid:28) N, which is typical in the ﬁrst layer of a CNN, the acceleration is about d times. We learn the approximating parameters H and V by a two-step strategy. In the ﬁrst step, we approx- imate the convolution kernel W in each layer by minimizing (cid:107) ˜W − W(cid:107)F (index of the layers are omitted for notation simplicity). Note that this step can be done in parallel as there is no inter-layer dependence. Then we ﬁne-tune the whole CNN based on the discriminative criterion of restoring classiﬁcation accuracy.  3.2 ALGORITHM  Based on the approximation criterion introduced in the previous section, the objective function to be minimized is:  (P 1)  E1(H,V) :=  n(V c Hk  k)T(cid:107)2 F .  (3)  (cid:88)  n − K(cid:88)  (cid:107)W c  n,c  k=1  This minimization problem has a closed form solution. This is summarized in the following theo- rem and the proof can be found in the appendix. The theorem gives us an efﬁcient algorithm for computing the exact decomposition. Theorem 1. Deﬁne the following bijection that maps a tensor to a matrix T : RC×d×d×N (cid:55)→ RCd×dN , tensor element (i1, i2, i3, i4) maps to (j1, j2), where  Deﬁne W := T [W]. Let W = U DQT be the Singular Value Decomposition (SVD) of W . Let  j1 = (i1 − 1)d + i2,  j2 = (i4 − 1)d + i3.  ˆV c k(j) = U(c−1)d+j,k ˆHk n(j) = Q(n−1)d+j,k  (cid:112)Dk,k (cid:112)Dk,k,  (4)  then ( ˆH, ˆV) is a solution to (P 1). Because of this Theorem, we call the ﬁlters (H,V) low-rank constrained ﬁlters. Note that the solu- tion to (P 1) is not unique. Indeed, if (H,V) is a solution, then (αH, 1/αV) is also a solution for  4  Published as a conference paper at ICLR 2016  Figure 2: The proposed parametrization for low-rank regularization. Left: The original convolu- tional layer. Right: low-rank constraint convolutional layer with rank-K.  any α (cid:54)= 0, but these solutions are equivalent in our application. An illustration of the closed-form approximation is shown in Figure 1. A different criterion which uses the data distribution is proposed in Denton et al. (2014). But mini- mization for this criterion is NP-hard. The proof is also included in the appendix. The algorithm provided by the above theorem is extremely fast. In our experiments, it completes in less than 1 second for most modern CNNs (AlexNet, VGG, GoogLeNet), as they have small convolutional kernels. Iterative algorithms (Denton et al. (2014); Jaderberg et al. (2014) take much longer, especially with the data-dependent criterion. In addition, iterative algorithms often lead to bad local minimum, which leads to inferior performance even after ﬁne-tuning. The proposed algorithm solves this issue, as it directly provides the global minimum, which is the best data- independent approximation. Numerical demonstrations are given in section 4.  3.3 TRAINING LOW-RANK CONSTRAINED CNN FROM SCRATCH  Using the above scheme to train a new CNN from scratch is conceptually straightforward. Simply parametrize the convolutional to be of the form in (1), and the rest is not very different from training a non-constrained CNN. Here H and V are the trainable parameters. As each convolutional layer is parametrized as the composition of two convolutional layers, the resulting CNN has more layers than the original one. Although the effective depth of the new CNN is not increased, the addi- tional layers make numerical optimization much more challenging due to exploding and vanishing gradients, especially for large networks. To handle this problem, we use a recent technique called Batch Normalization (BN) (Ioffe & Szegedy, 2015). BN transform normalizes the activations of the internal hidden units, hence it can be an effective way to deal with the exploding or vanishing gradients. It is reported in Ioffe & Szegedy (2015) that deeper networks can be trained with BN successfully, and larger learning rates can be used. Empirically, we ﬁnd BN effective in learning the low-rank constrained networks. An illustration of transformation of a original convolutional layer into a low-rank constraint one is in Figure 2. More details can be found in the numerical experiments section.  4 EXPERIMENTS  In this section, we evaluate the proposed scheme on the CIFAR-10 and the ILSVRC12 datasets with several CNN models.  4.1 CIFAR-10  CIFAR-10 dataset is small by today’s standard, but it is a good testbed for new ideas. We deploy two models as baseline models; one is a customized CNN and the other is the NIN model. We compare their performance with their corresponding low-rank constrained versions. All models on this dataset are learned from scratch.  5  Published as a conference paper at ICLR 2016  Table 1: Network structure for CIFAR-10  Layer name  conv1 conv2 conv3 fc1 fc2  CNN  5 × 5 × 192 5 × 5 × 128 5 × 5 × 256  Low-rank CNN  K1 = 12 K2 = 64 K3 = 128  2304 × 512 512 × 10  Layer name  conv1 conv2,3 conv4 conv5,6 conv7 conv8,9  Low-rank NIN  5 × 5 × 192 5 × 5 × 192  NIN K1 = 10 1 × 1 × 160, 1 × 1 × 96 K2 = 51 1 × 1 × 192, 1 × 1 × 192 1 × 1 × 192, 1 × 1 × 10  3 × 3 × 192  Table 2: CIFAR-10 performance  METHOD  CNN (ours) Low-rank CNN (ours) CNN + Dropout (ours) Low-rank CNN + Dropout (ours) NIN (ours) Low-rank NIN (ours) CNN + Maxout (Goodfellow et al., 2013) NIN (Lin et al., 2013) CNN (Srivastava et al., 2014) NIN + APL units (Agostinelli et al., 2014)  WITHOUT AUG. WITH AUG. 15.12% 14.50% 13.90% 13.81% 10.12% 8.69% 11.68% 10.41% 12.61% 9.59%  12.62% 13.10% 12.29% 11.41% 8.19% 6.98% 9.38% 8.81% - 7.51%  SPEEDUP 1× 2.9× 0.96× 2.8× 1× 1.5× - - - -  The conﬁgurations of the baseline models and their low-rank counterparts are outlined in Table 1. We substitute every single convolutional layer in the baseline models with two convolutional layers with parameter K introduced in the previous section. All other speciﬁcations of the network pairs are the same. Rectiﬁed Linear Unit (ReLU) is applied to every layer except for the last one. Our implementation of the NIN model is slightly different from the one introduced in Lin et al. (2013). We did not replace the 3 × 3 convolutional layer because this layer only constitutes a small fraction of the total execution time. Hence the efﬁciency gain of factorizing this layer is small. The networks are trained with back propagation to optimize the multinomial logistic regression objective. The batch size is 100. The learning learning rate is initially set to 0.01 and decreases by a factor of 10 every time the validation error stops decreasing. Some models have dropout units with probability 0.25 inserted after every ReLU. For exact speciﬁcations of the parameters, the reader may check https://github.com/chengtaipu/lowrankcnn. We evaluated the performance of the models both with and without data augmentation. With data augmentation, the images are ﬂipped horizontally with probability 0.5 and translated in both directions by at most 1 pixel. Otherwise, we only subtract the mean of the images and normalize each channel. The results are listed in Table 2. The performance of the low-rank constrained versions of both networks are better than the baseline networks, with and without data augmentation. Notably, the low-rank NIN model outperforms the baseline NIN model by more than 1%. And as far as we know, this is also better than previously published results. We then study how the empirical performance and speedup change as we vary the rank K. We choose the CNN+Dropout as baseline model with data augmentation described above. The results are listed in Table 3. The number of parameters in the network can be reduced by a large factor, especially for the second and third layers. Up to 7× speedup for a speciﬁc layer and 2-3× speedup for the whole network can be achieved. In practice, it is difﬁcult for the speedup to match the theoretical gains based on the number of operations, which is roughly proportional to the reduction of parameters. The actual gain also depends on the software and hardware optimization strategies of convolutions. Our results in Table 3 are based on Nvidia Titan GPUs and Torch 7 with cudnn backend. Interestingly, even with signiﬁcant reductions in the number of parameters, the performance does not decrease much. Most of the networks listed in Table 3 even outperform the baseline model.  6  Published as a conference paper at ICLR 2016  Table 3: Speedup and performance change. Performance change is relative to the baseline CNN+Dropout model with accuracy 87.71%.  LAYER K1 K2 K3  First  Second  Third  4 8 12 12 12 12 12 12 12 12 12 12 12 12 12  64 64 64 8 16 32 64 128 256 64 64 64 64 64 64  256 256 256 256 256 256 256 256 256 8 16 32 64 128 256  ACCURACY CHANGE +0.69% +0.85% +0.94% -0.02% +0.50% +0.89% +0.94% +1.32% +1.40% -2.25% +0.21% +0.19% +0.19% +0.94% +1.75%  SPEEDUP (LAYER) 1.20× 1.13× 1.05× 7.13× 6.76× 6.13× 3.72× 2.38× 1.25× 6.98 × 6.89× 5.82× 3.74× 2.38× 1.31×  SPEEDUP (NET) 2.91× 2.87× 2.85× 3.21× 3.21× 3.13× 2.86× 2.58× 1.92× 3.11× 3.11× 3.10× 2.96× 2.86× 2.30×  REDUCTIONS (WEIGHTS) 3.5× 1.8× 1.2× 47.5× 23.8× 12.0× 6.0× 3.0× 1.5× 52.5× 26.4× 13.3× 6.7× 3.3× 1.7×  Applying the low-rank constraints for all convolutional layers, the total number of parameters in the convolutional layers can be reduced by a large factor without degrading much performance. For example, with K1 = 12, K2 = 16 and K3 = 32, the parameters in the convolutional kernels are reduced by 91% and the relative performance is +0.25%. Nevertheless, the parameters in the fully connected layers still occupy a large fraction. This limits the overall compression ability of the low-rank constraint. There are some very recent works focusing on reducing the parameters in the fully connected layers (Novikov et al., 2015), combining these techniques with the proposed scheme will be explored in future research.  4.2  ILSVRC12  ILSVRC12 (Russakovsky et al., 2015) is a well-known large-scale benchmark dataset for image classiﬁcation. We adopt three famous CNN models, AlexNet (Krizhevsky et al., 2012) (CaffeNet (Jia et al., 2014) as an variant), VGG-16 (Simonyan & Zisserman, 2014), and GoogLeNet (Szegedy et al., 2014) (BN-Inception (Ioffe & Szegedy, 2015) as an variant) as our baselines. The CaffeNet and VGG-16 are directly downloaded from Caffe’s model zoo and then ﬁne-tuned on the training set until convergence, while the BN-Inception model is trained from scratch by ourselves. The introduced low-rank decomposition is applied to each convolutional layer that has kernel size greater than 1 × 1. Input images are ﬁrst warped to 256 × 256 and then cropped to 227 × 227 or 224 × 224 for different models. We use the single center crop during the testing stage, and evaluate the performance by the top-5 accuracy on the validation set. Detailed training parameters are available at https://github.com/chengtaipu/lowrankcnn. As before, the hyper-parameter K controls the trade-off between the speedup factor and the classi- ﬁcation performance of the low-rank models. Therefore, we ﬁrst study its effect for each layer, and then use the information to conﬁgure the whole low-rank model for better overall performance. We decompose a speciﬁc layer with a different K each time, while keeping the parameters of all the other layers ﬁxed. The performance after ﬁne-tuning with respect to the theoretical layer speedup is demonstrated in Figure 3. In general, we choose for each layer the value of K that most accel- erates the forward computation while does not hurt the performance signiﬁcantly (< 1%). A more automatic way for choosing K is based on Eigengap, such that the ﬁrst K eigenvectors account for 95% of the variations. This is similar to choosing the number of principal components in PCA. The detailed low-rank model structures are listed in Table 4. The proposed closed form solution provides the optimal data-independent initialization to the low- rank model. As indicated in Figure 4, there is a performance gap between the low-rank models and  7  Published as a conference paper at ICLR 2016  Figure 3: The performance w.r.t. the theoretical layer speedup. Only the conv1-conv5 layers of the AlexNet are shown.  Figure 4: The performance w.r.t. the ﬁne-tuning epoch when using the proposed closed form so- lution as initialization.  Table 4: Low-rank models for ILSVRC12. For VGG-16, each convolution module contains two or three sub-convolutional layers. For GoogLeNet, each inception module contains one 3 × 3 and two consecutive 3 × 3 convolutional layers. Their corresponding Ks are shown in a cell for brevity.  (a) AlexNet Layer K 8 conv1 40 conv2 conv3 60 100 conv4 200 conv5  (b) VGG-16 K 5, 24 48, 48  64, 128, 160 192, 192, 256 320, 320, 320  Layer conv1 conv2 conv3 conv4 conv5  Layer conv1 conv2  inception(3a) inception(3b) inception(3c) inception(4a)  (c) GoogLeNet K 8 48  Layer  inception(4b) inception(4c) inception(4d) inception(4e) inception(5a) inception(5b)  32, 32, 48 32, 32, 48 80, 32, 48 32, 64, 80  K  64, 64, 80 64, 64, 64 64, 96, 96 64, 128, 160 128, 96, 128 128, 96, 128  their baselines at the beginning, but the performance is restored after ﬁne-tuning. It is claimed in Denton et al. (2014) that data-dependent criterion leads to better performance, we found that this is true upon approximation, but after ﬁne-tuning, the difference between the two criteria is negligible (< 0.1%). At last, we compare the low-rank models with their baselines from the perspective of classiﬁcation performance, as well as the time and space consumption. The results are summarized in Table 5. We can see that all the low-rank models achieve comparable performances. Those initialized with closed form weights approximation (cf. approximation rows in Table 5) are slightly inferior to their baselines. While the low-rank AlexNet trained from scratch with BN could achieve even better performance. This observation again reveals that the low-rank CNN structure could have better discriminative power and generalization ability. On the other hand, both the running time and the number of parameters are consistently reduced. Note that the large gaps between the theoretical and the actual speedup are mainly due to the CNN implementations, and the current BN operations signiﬁcantly slow down the forward computation. This suggests room for accelerating the low-rank models by designing speciﬁc numerical algorithms.  5 DISCUSSION  In this paper, we explored using tensor decomposition techniques to speedup convolutional neural networks. We have introduced a new algorithm for computing the low-rank tensor decomposition and a new method for training low-rank constrained CNNs from scratch. The proposed method is evaluated on a variety of modern CNNs, including AlexNet, NIN, VGG, GoogleNet with success. This gives a strong evidence that low-rank tensor decomposition can be a generic tool for speeding up large CNNs.  8  24816Theoretical layer forward speedup-6%-5%-4%-3%-2%-1%0%1%Val. top-5 accuracy gainconv1conv2conv3conv4conv50510152025The training epoch-20%-15%-10%-5%0%Val. top-5 accuracy gainAlexNet approxVGG-16 approxGoogLeNet approxAlexNet from scratchPublished as a conference paper at ICLR 2016  Table 5: Comparisons between the low-rank models and their baselines. The theoretical speedup and weights reduction are computed concerning only the convolutional layers to be decomposed. While the actual speedup is based on the forward computation time of the whole net.  METHOD  TOP-5 VAL. ACCURACY  THEORETICAL  SPEEDUP  ACTUAL SPEEDUP  WEIGHTS REDUCTION  AlexNet (original) Low-rank (cf. approximation) Low-rank (from scratch with BN) VGG-16 (original) Low-rank (cf. approximation) GoogLeNet (original) Low-rank (cf. approximation)  80.03% 79.66% 80.56% 90.60% 90.31% 92.21% 91.79%  1× 5.27× 5.24× 1× 3.10× 1× 2.89×  1× 1.82× 1.09× 1× 2.05× 1× 1.20×  1× 5.00× 4.94× 1× 2.75× 1× 2.84×  On the the other hand, the interesting fact that the low-rank constrained CNNs sometimes outperform their non-constrained counterparts points to two things. One is the local minima issue. Although the expressive power of low-rank constrained CNNs is strictly smaller than that of the non-constrained one, we have observed in some cases that the former have smaller training error. This seems to suggest the low-rank form helps the CNNs begin with a better initialization and settles at a better local minimum. The other issue is over-ﬁtting. This is shown by the observation that in many cases the constrained model has higher training error but generalizes better. Overall, this suggests room for improvement in both the numerical algorithms and the regularizations of the CNN models.  ACKNOWLEDGMENTS  This work is supported in part by the 973 project 2015CB856000 of the Chinese Ministry of Science and Technology and the DOE grant DE-SC0009248.  REFERENCES Agostinelli, Forest, Hoffman, Matthew, Sadowski, Peter, and Baldi, Pierre. Learning activation  functions to improve deep neural networks. arXiv preprint arXiv:1412.6830, 2014.  de Silva, Vin and Lim, Lek-Heng. Tensor Rank and the Ill-Posedness of the Best Low-Rank Ap- proximation Problem. SIAM Journal on Matrix Analysis and Applications, 30(3):1084–1127, September 2008.  Denton, Emily L, Zaremba, Wojciech, Bruna, Joan, LeCun, Yann, and Fergus, Rob. Exploiting  linear structure within convolutional networks for efﬁcient evaluation. In NIPS, 2014.  Farabet, Clement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical  features for scene labeling. TPAMI, 35(8):1915–1929, 2013.  Gillis, Nicolas and Glineur, Franc¸ois. Low-rank matrix approximation with weights or missing data  is np-hard. SIAM Journal on Matrix Analysis and Applications, 32(4):1149–1165, 2011.  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.  Maxout networks. arXiv preprint arXiv:1302.4389, 2013.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by  reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.  Jaderberg, Max, Vedaldi, Andrea, and Zisserman, Andrew. Speeding up convolutional neural net-  works with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  9  Published as a conference paper at ICLR 2016  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, 2012.  Lebedev, Vadim, Ganin, Yaroslav, Rakhuba, Maksim, Oseledets, Ivan, and Lempitsky, Victor. Speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. arXiv preprint arXiv:1412.6553, 2014.  Lin, M., Chen, Q., and Yan, S. Network In Network. ArXiv e-prints, December 2013. Long, Jonathan, Shelhamer, Evan, and Darrell, Trevor. Fully convolutional networks for semantic  segmentation. arXiv preprint arXiv:1411.4038, 2014.  Novikov, A., Podoprikhin, D., Osokin, A., and Vetrov, D. Tensorizing Neural Networks. ArXiv  e-prints, September 2015.  Rigamonti, Roberto, Sironi, Amos, Lepetit, Vincent, and Fua, Pascal. Learning separable ﬁlters. In  CVPR, 2013.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge. IJCV, pp. 1–42, April 2015.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. arXiv preprint arXiv:1409.1556, 2014.  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan.  Dropout: A simple way to prevent neural networks from overﬁtting. In ICML, 2014.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.  Vanhoucke, Vincent, Senior, Andrew, and Mao, Mark Z. Improving the speed of neural networks  on cpus. In Deep Learning and Unsupervised Feature Learning, NIPS Workshop, 2011.  APPENDIX  PROOF OF THEOREM 1  Proof. Consider the following minimization problem:  Let (H∗,V∗) be a solution to (P1), then we can construct a solution to (P 2) as follows:  (P 2)  E2( ˜W ) := (cid:107) ˜W − W(cid:107)2 subject to Rank( ˜W ) ≤ K.  F  ˜W =  1 ,Hk  2 ,··· ,Hk  N  (cid:3) .  K(cid:88)  k=1  (cid:2)Hk    V 1 kV 2 k... V C  k  Because of the separability of the Frobenius norm,  Moreover, as Rank( ˜W ) ≤ K, hence ˜W is feasible for (P 2). We have E2(W ∗) ≤ E1(H∗,V∗) = E2( ˜W ),  E1(H∗,V∗) = E2( ˜W ).  where W ∗ is any solution to (P 2). On the other hand, let W ∗ be a solution to (P 2), then we construct a solution ( ˆH, ˆV) to (P 1) as (4). Hence  Together with (6),  We have proved ( ˆH, ˆV) is a solution to (P 1).  E1(H∗,V∗) ≤ E1( ˆH, ˆV).  E1( ˆH, ˆV) = E2(W∗) = E1(H∗,V∗).  (5)  (6)  (7)  10  Published as a conference paper at ICLR 2016  HARDNESS OF THE DATA-DEPENDENT APPROXIMATION  Using the data-dependent criterion, the minimization problem is:  E(H,V) :=  Hk n(V c  k)T ∗ Z c i (cid:107)2 F .  (8)  For ﬁxed stride s, deﬁne the linear map Pm : RX×Y (cid:55)→ Rd×d, Pm(z) samples the m-th d× d patch from z ∈ RX×Y followed by ﬂipping the patch horizontally and vertically. Then  Let  c  Zim =  M(cid:88)  N(cid:88)  i=1  n=1  (cid:88)  (cid:107)W c  n ∗ Z c  n, PmZ c  i (cid:105)2.  c=1  k=1  F =  n(cid:107)2  (cid:104)W c  (cid:107)W c  C(cid:88)  i − K(cid:88) n ∗ Z c (cid:88)  ⊗ (1, 1,··· , 1) PmZ 1 (cid:125) (cid:123)(cid:122) (cid:124) ... (cid:88)  PmZ 2  PmZ c  m,c  N  i  i  i  .  E( ˜W ) :=  (cid:107)(W − ˜W ) ◦ Zim(cid:107)2  F  subject to  i,m  Rank( ˜W ) ≤ K,  Similar as in Criterion 1, the approximation problem is equivalent to the following minimization program:  (9)  (10)  where ◦ is the Hadamard product. This is a weighted low-rank approximation problem:  (cid:88)  E( ˜W ) :=  Gij(Wij − ˜Wij)2  where G =(cid:80)  im Zim ◦ Zim.  subject to  ij  Rank( ˜W ) ≤ K,  Although it appears very similar to the problem in Criterion 1, which has a closed form solution, this is much more difﬁcult to solve except for a few special cases. (E.g., when the weight matrix is identity or has rank one.) In fact, it can be proved that this problem is NP-hard (Gillis & Glineur, 2011).  11  ",
1511.03643,2016,Unifying distillation and privileged information,"['Unifying distillation and privileged information\nDavid Lopez-Paz', 'Leon Bottou', 'Bernhard Schölkopf', 'Vladimir Vapnik']",https://arxiv.org/pdf/1511.03643,"6 1 0 2     b e F 6 2         ] L M  . t a t s [      3 v 3 4 6 3 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  UNIFYING DISTILLATION AND PRIVILEGED INFORMATION  David Lopez-Paz Facebook AI Research, Paris, France∗ dlp@fb.com  L´eon Bottou Facebook AI Research, New York, USA leon@bottou.org  Bernhard Sch¨olkopf Max Planck Insitute for Intelligent Systems, T¨ubingen, Germany bs@tuebingen.mpg.de  Vladimir Vapnik Facebook AI Research and Columbia University, New York, USA vladimir.vapnik@gmail.com  ABSTRACT  Distillation (Hinton et al., 2015) and privileged information (Vapnik & Izmailov, 2015) are two techniques that enable machines to learn from other machines. This paper uniﬁes the two into generalized distillation, a framework to learn from mul- tiple machines and data representations. We provide theoretical and causal insight about the inner workings of generalized distillation, extend it to unsupervised, semisupervised and multitask learning scenarios, and illustrate its efﬁcacy on a variety of numerical simulations on both synthetic and real-world data.  1  INTRODUCTION  Humans learn much faster than machines. Vapnik & Izmailov (2015) illustrate this discrepancy with the Japanese proverb  better than a thousand days of diligent study is one day with a great teacher.  Motivated by this insight, the authors incorporate an “intelligent teacher” into machine learning. Their solution is to consider training data formed by a collection of triplets n, yn)} ∼ P n(x, x(cid:63), y).  1, y1), . . . , (xn, x(cid:63)  {(x1, x(cid:63)  Here, each (xi, yi) is a feature-label pair, and the novel element x(cid:63) i is additional information about the example (xi, yi) provided by an intelligent teacher, such as to support the learning process. Unfortunately, the learning machine will not have access to the teacher explanations x(cid:63) i at test time. Thus, the framework of learning using privileged information (Vapnik & Vashist, 2009; Vapnik & Izmailov, 2015) studies how to leverage these explanations x(cid:63) i at training time, to build a classiﬁer for test time that outperforms those built on the regular features xi alone. As an example, xi could be the image of a biopsy, x(cid:63) i the medical report of an oncologist when inspecting the image, and yi a binary label indicating whether the tissue shown in the image is cancerous or healthy. The previous exposition ﬁnds a mathematical justiﬁcation in VC theory (Vapnik, 1998), which char- acterizes the speed at which machines learn using two ingredients: the capacity or ﬂexibility of the ∗The majority of this work was done while DLP was afﬁliated to the Max Planck Institute for Intelligent  Systems and the University of Cambridge.  1  Published as a conference paper at ICLR 2016  (cid:18)(cid:18)|F|VC − log δ  (cid:19)α(cid:19)  machine, and the amount of data that we use to train it. Consider a binary classiﬁer f belonging to a function class F with ﬁnite VC-Dimension |F|VC. Then, with probability 1 − δ, the expected error R(f ) is upper bounded by  R(f ) ≤ Rn(f ) + O  ,  n  (1) 2 ≤ α ≤ 1. For difﬁcult (non-separable) where Rn(f ) is the training error over n data, and 1 2, which translates into machines learning at a slow rate of O(n−1/2). problems the exponent is α = 1 On the other hand, for easy (separable) problems, i.e., those on which the machine f makes no training errors, the exponent is α = 1, which translates into machines learning at a fast rate of O(n−1). The difference between these two rates is huge: the O(n−1) learning rate potentially only requires 1000 examples to achieve the accuracy for which the O(n−1/2) learning rate needs 106 examples. So, given a student who learns from a ﬁxed amount of data n and a function class F, a good teacher can try to ease the problem at hand by accelerating the learning rate from O(n−1/2) to O(n−1). Vapnik’s learning using privileged information is one example of what we call machines-teaching- machines: the paradigm where machines learn from other machines, in addition to training data. Another seemingly unrelated example is distillation (Hinton et al., 2015),1 where a simple ma- chine learns a complex task by imitating the solution of a ﬂexible machine. In a wider context, the machines-teaching-machines paradigm is one step toward the deﬁnition of machine reasoning of Bottou (2014), “the algebraic manipulation of previously acquired knowledge to answer a new question”. In fact, many recent state-of-the-art systems compose data and supervision from multi- ple sources, such as object recognizers reusing convolutional neural network features (Oquab et al., 2014), and natural language processing systems operating on vector word representations extracted from unsupervised text corpora (Mikolov et al., 2013). In the following, we frame Hinton’s distillation and Vapnik’s privileged information as two instances of the same machines-teaching-machines paradigm, termed generalized distillation. The analysis of generalized distillation sheds light to applications in semi-supervised learning, domain adaptation, transfer learning, Universum learning (Weston et al., 2006), reinforcement learning, and curriculum learning (Bengio et al., 2009); some of them discussed in our numerical simulations.  2 DISTILLATION  We focus on c-class classiﬁcation, although the same ideas apply to regression. Consider the data  (2) Here, ∆c is the set of c-dimensional probability vectors. Using (2), we are interested in learning the representation  i=1 ∼ P n(x, y), xi ∈ Rd, yi ∈ ∆c.  {(xi, yi)}n  (3) where Ft is a class of functions from Rd to Rc, the function σ : Rc → ∆c is the softmax operation  ft = arg min  f∈Ft  i=1  (cid:96)(yi, σ(f (xi))) + Ω((cid:107)f(cid:107)),  1 n  n(cid:88)  for all 1 ≤ k ≤ c, the function (cid:96) : ∆c × ∆c → R+ is the cross-entropy loss  σ(z)k =  ezk(cid:80)c (cid:96)(y, ˆy) = − c(cid:88)  ,  j=1 ezj  yk log ˆyk,  and Ω : R → R is an increasing function which serves as a regularizer. When learning from real world data such as high-resolution images, ft is often an ensemble of large deep convolutional neural networks (LeCun et al., 1998a). The computational cost of predicting  k=1  1Distillation relates to model compression (Burges & Sch¨olkopf, 1997; Buciluˇa et al., 2006; Ba & Caruana,  2014). We will adopt the term distillation throughout this manuscript.  2  Published as a conference paper at ICLR 2016  new examples at test time using these ensembles is often prohibitive for production systems. For this reason, Hinton et al. (2015) propose to distill the learned representation ft ∈ Ft into  (1 − λ)(cid:96)(yi, σ(f (xi))) + λ(cid:96)(si, σ(f (xi)))  ,  (4)  n(cid:88)  (cid:104)  i=1  fs = arg min  f∈Fs  1 n  (cid:105)  where  si = σ(ft(xi)/T ) ∈ ∆c  (5) are the soft predictions from ft about the training data, and Fs is a function class simpler than Ft. The temperature parameter T > 0 controls how much do we want to soften or smooth the class- probability predictions from ft, and the imitation parameter λ ∈ [0, 1] balances the importance between imitating the soft predictions si and predicting the true hard labels yi. Higher temperatures lead to softer class-probability predictions si. In turn, softer class-probability predictions reveal label dependencies which would be otherwise hidden as extremely large or small numbers. After distillation, we can use the simpler fs ∈ Fs for faster prediction at test time.  3 VAPNIK’S PRIVILEGED INFORMATION  We now turn back to Vapnik’s problem of learning in the company of an intelligent teacher, as introduced in Section 1. The question at hand is: How can we leverage the privileged information i to build a better classiﬁer for test time? One na¨ıve way to proceed would be to estimate the x(cid:63) privileged representation x(cid:63) i from the regular representation xi, and then use the union of regular and estimated privileged representations as our test-time feature space. But this may be a cumbersome endeavour: in the example of biopsy images xi and medical reports x(cid:63) i , it is reasonable to believe that predicting reports from images is more complicated than classifying the images into cancerous or healthy. Alternatively, we propose to use distillation to extract useful knowledge from privileged information. The proposal is as follows. First, learn a teacher function ft ∈ Ft by solving (3) using the data {(x(cid:63) i )/T ), for all 1 ≤ i ≤ n and some temperature parameter T > 0. Third, distill ft ∈ Ft into fs ∈ Fs by solving (4) using both the hard labeled data {(xi, yi)}n  i=1. Second, compute the teacher soft labels si = σ(ft(x(cid:63)  i=1 and the softly labeled data {(xi, si)}n  i , yi)}n  i=1.  3.1 COMPARISON TO PRIOR WORK  Vapnik & Vashist (2009); Vapnik & Izmailov (2015) offer two strategies to learn using privileged in- formation: similarity control and knowledge transfer. Let us brieﬂy compare them to our distillation- based proposal. The motivation behind similarity control is that SVM classiﬁcation is separable after we correct for the slack values ξi, which measure the degree of misclassiﬁcation of training data points xi (Vapnik & Vashist, 2009). Since separable classiﬁcation admits O(n−1) fast learning rates, it would be ideal to have a teacher that could supply slack values to us. Unluckily, it seems quixotic to aspire for a teacher able to provide with abstract ﬂoating point number slack values. Perhaps it is more realistic to assume instead that the teacher can provide with some rich, high-level representation useful to estimate the sought-after slack values. This reasoning crystallizes into the SVM+ objective function from (Vapnik & Vashist, 2009):  n(cid:88) (cid:123)(cid:122) i (cid:105) + b(cid:63) is the teacher cor- where fi := (cid:104)w, xi(cid:105) + b is the decision boundary at xi, and f (cid:63) recting function at the same location. The SVM+ objective function matches the objective function of non-separable SVM when we replace the correcting functions f (cid:63) i with the slacks ξi. Thus, skilled teachers provide with privileged information x(cid:63) i highly informative about the slack values ξi. Such privileged information allows for simple correcting functions f (cid:63) i , and the easy estimation of these correcting functions is a proxy to O(n−1) fast learning rates. Technically, this amounts to saying  αi − n(cid:88) (cid:123)(cid:122)  (αi + βi − C)f (cid:63)  L(w, w(cid:63), b, b(cid:63), α, β) =  := (cid:104)w(cid:63), x(cid:63)  (cid:107)w(cid:63)(cid:107)2 +  separable SVM objective  corrections from teacher  (cid:107)w(cid:107)2 +  n(cid:88)  αiyifi  (cid:124)  (cid:125)  (cid:124)  (cid:125)  (6)  γ 2  1 2  i=1  i=1  i=1  +  ,  i  i  3  Published as a conference paper at ICLR 2016  j , x(cid:63)  j=1 α(cid:63)  i ))}n  i , yi)}n  j , x(cid:63)) on the input-output pairs {(x(cid:63)  i=1 and fs ∈ Fs. Since the representation x(cid:63)  (cid:80)m α ∈ Rm to obtain the ﬁnal student function fs(x) = (cid:80)m  that a teacher is helpful whenever the capacity of her correcting functions is much smaller than the capacity of the student decision boundary. In knowledge transfer (Vapnik & Izmailov, 2015) the teacher ﬁts a function ft(x(cid:63)) = i=1 and ft ∈ Ft, to ﬁnd the best reduced set j k(cid:63)(u(cid:63) of prototype or basis points {u(cid:63) j}m j=1. Second, the student ﬁts one function gj per set of input-output i=1, for all 1 ≤ j ≤ m. Third, the student ﬁts a new vector of coefﬁcients pairs {(xi, k(cid:63)(u(cid:63) j=1 αjgj(x), using the input-output pairs {(xi, yi)}n i is intelligent, we assume that the function class Ft has small capacity, and thus allows for accurate estimation under small sample sizes. Distillation differs from similarity control in three ways. First, distillation is not restricted to SVMs. Second, while the SVM+ solution contains twice the amount of parameters than the original SVM, the user can choose a priori the amount of parameters in the distilled classiﬁer. Third, SVM+ learns the teacher correcting function and the student decision boundary simultaneously, but distillation proceeds sequentially: ﬁrst with the teacher, then with the student. On the other hand, knowledge transfer is closer in spirit to distillation, but the two techniques differ: while similarity control relies on a student that purely imitates the hidden representation of a low-rank kernel machine, distillation is a trade-off between imitating soft predictions and hard labels, using arbitrary learning algorithms. The framework of learning using privileged information enjoys theoretical analysis (Pechyony & Vapnik, 2010) and multiple applications, including ranking (Sharmanska et al., 2013), computer vision (Sharmanska et al., 2014; Lopez-Paz et al., 2014), clustering (Feyereisl & Aickelin, 2012), metric learning (Fouad et al., 2013), Gaussian process classiﬁcation (Hern´andez-Lobato et al., 2014), and ﬁnance (Ribeiro et al., 2010). Lapin et al. (2014) show that learning using privileged information is a particular instance of importance weighting.  4 GENERALIZED DISTILLATION  We now have all the necessary background to describe generalized distillation. To this end, consider the data {(xi, x(cid:63)  i=1. Then, the process of generalized distillation is as follows:  i , yi)}n  1. Learn teacher ft ∈ Ft using the input-output pairs {(x(cid:63) 2. Compute teacher soft labels {σ(ft(x(cid:63) 3. Learn student fs ∈ Fs using the input-output pairs {(xi, yi)}n  i )/T )}n  i , yi)}n  imitation parameter λ ∈ [0, 1].2  i=1, using temperature parameter T > 0.  i=1, {(xi, si)}n  i=1, Eq. 4, and  i=1 and Eq. 3.  i = xi for all 1 ≤ i ≤ n We say that generalized distillation reduces to Hinton’s distillation if x(cid:63) and |Fs|C (cid:28) |Ft|C, where | · |C is an appropriate function class capacity measure. Conversely, we say that generalized distillation reduces to Vapnik’s learning using privileged information if x(cid:63) i is a privileged description of xi, and |Fs|C (cid:29) |Ft|C. This comparison reveals a subtle difference between Hinton’s distillation and Vapnik’s privileged information. In Hinton’s distillation, Ft is ﬂexible, for the teacher to exploit her general purpose i = xi to learn intricate patterns from large amounts of labeled data. In Vapnik’s representation x(cid:63) (cid:54)= xi to privileged information, Ft is simple, for the teacher to exploit her rich representation x(cid:63) learn intricate patterns from small amounts of labeled data. The space of privileged information is thus a specialized space, one of “metaphoric language”. In our running example of biopsy images, the space of medical reports is much more specialized than the space of pixels, since the space of pixels can also describe buildings, animals, and other unrelated concepts. In any case, the teacher must develop a language that effectively communicates information to help the student come up with better representations. The teacher may do so by incorporating invariances, or biasing them towards being robust with respect to the kind of distribution shifts that the teacher may expect at test time. In general, having a teacher is one opportunity to learn characteristics about the decision boundary which are not contained in the training sample, in analogy to a good Bayesian prior.  i  2Note that these three steps could be combined into a joint end-to-end optimization problem. For simplicity,  our numerical simulations will take each of these three steps sequentially.  4  Published as a conference paper at ICLR 2016  4.1 WHY DOES GENERALIZED DISTILLATION WORK? Recall our three actors: the student function fs ∈ Fs, the teacher function ft ∈ Ft, and the real target function of interest to both the student and the teacher, f ∈ F. For simplicity, consider pure distillation (set the imitation parameter to λ = 1). Furthermore, we will place some assumptions about how the student, teacher, and true function interplay when learning from n data. First, assume that the student may learn the true function at a slow rate  R(fs) − R(f ) ≤ O  + εs,  where the O(·) term is the estimation error, and εs is the approximation error of the student function class Fs with respect to f ∈ F. Second, assume that the better representation of the teacher allows her to learn at the fast rate  R(ft) − R(f ) ≤ O  + εt,  where εt is the approximation error of the teacher function class Ft with respect to f ∈ F. Finally, assume that when the student learns from the teacher, she does so at the rate  R(fs) − R(ft) ≤ O  + εl,  where εl is the approximation error of the student function class Fs with respect to ft ∈ Ft, and 2 ≤ α ≤ 1. Then, the rate at which the student learns the true function f admits the alternative 1 expression  R(fs) − R(f ) = R(fs) − R(ft) + R(ft) − R(f )  n  (cid:19)  (cid:18)|Fs|C√ (cid:18)|Ft|C (cid:19) (cid:18)|Fs|C (cid:19)  n  nα  ≤ O  ≤ O  + εl + O  + εt  nα  (cid:19)  (cid:18)|Fs|C (cid:18)|Fs|C + |Ft|C (cid:19)  nα  + εl + εt ≤ O  (cid:18)|Ft|C  (cid:19)  + εl + εt,  n  (cid:19) (cid:18)|Fs|C√  n  (cid:19)  + εs  (cid:18)|Fs|C + |Ft|C  O  nα  where the last inequality follows because α ≤ 1. Thus, the question at hand is to argue, for a given learning problem, if the inequality  holds. The inequality highlights that the beneﬁts of learning with a teacher arise due to i) the capacity of the teacher being small, ii) the approximation error of the teacher being smaller than the approxi- 2. Remarkably, these factors mation error of the student, and iii) the coefﬁcient α being greater than 1 embody the assumptions of privileged information from Vapnik & Izmailov (2015). The inequality is also reasonable under the main assumption in (Hinton et al., 2015), which is εs (cid:29) εt + εl. More- over, the inequality highlights that the teacher is most helpful in low data regimes, such as small datasets, Bayesian optimization, reinforcement learning, domain adaptation, transfer learning, or in the initial stages of online and reinforcement learning. 2 case” is a general situation, since soft labels (dense vectors with a We believe that the “α > 1 real number of information per class) contain more information than hard labels (one-hot-encoding vectors with one bit of information per class) per example, and should allow for faster learning. This additional information, also understood as label uncertainty, relates to the acceleration in SVM+ due to the knowledge of slack values. Since a good teacher smoothes the decision boundary and instructs the student to fail on difﬁcult examples, the student can focus on the remaining body of data. Although this translates into the unambitious “whatever my teacher could not do, I will not do”, the imitation parameter λ ∈ [0, 1] in (4) allows to follow this rule safely, and fall back to regular learning if necessary.  4.2 EXTENSIONS  Semi-supervised learning We now extend generalized distillation to the situation where examples lack regular features, privileged features, labels, or a combination of the three. In the following, we  5  Published as a conference paper at ICLR 2016  denote missing elements by (cid:3). For instance, the example (xi, (cid:3), yi) has no privileged features, and i , (cid:3)) is missing its label. Using this convention, we introduce the clean subset the example (xi, x(cid:63) notation  c(S) = {v : v ∈ S, vi (cid:54)= (cid:3) ∀i}.  Then, semi-supervised generalized distillation walks the same three steps as generalized distillation, enumerated at the beginning of Section 4, but uses the appropriate clean subsets instead of the whole data. For example, the semi-supervised extension of distillation allows the teacher to prepare soft labels for all the unlabeled data c({(xi, x(cid:63) i=1). These additional soft-labels are additional information available to the student to learn the teacher representation ft. i }n Learning with the Universum The unlabeled data c({xi, x(cid:63) i=1) can belong to one of the classes of interest, or be Universum data (Weston et al., 2006; Chapelle et al., 2007). Universum data may have labels: in this case, one can exploit these additional labels by i) training a teacher that distinguishes amongst all classes (those of interest and those from the Universum), ii) computing soft class-probabilities only for the classes of interest, and iii) distilling these soft probabilities into a student function.  i )}n  Learning from multiple tasks Generalized distillation applies to some domain adaptation, trans- fer learning, or multitask learning scenarios. On the one hand, if the multiple tasks share the same labels yi but differ in their input modalities, the input modalities from the source tasks are privileged information. On the other hand, if the multiple tasks share the same input modalities xi but differ in their labels, the labels from the source tasks are privileged information. In both cases, the regular student representation is the input modality from the target task.  Curriculum and reinforcement learning We conjecture that the uncertainty in the teacher soft predictions can be used as a mechanism to rank the difﬁculty of training examples, and use these ranks for curriculum learning (Bengio et al., 2009). Furthermore, distillation resembles imitation, a technique that learning agents could exploit in reinforcement learning environments.  4.3 A CAUSAL PERSPECTIVE ON GENERALIZED DISTILLATION  The assumption of independence of cause and mechanisms states that “the probability distribution of a cause is often independent from the process mapping this cause into its effects” (Sch¨olkopf et al., 2012). Under this assumption, for instance, causal learning problems —i.e., those where the features cause the labels— do not beneﬁt from semi-supervised learning, since by the independence assumption, the marginal distribution of the features contains no information about the function mapping features to labels. Conversely, anticausal learning problems —those where the labels cause the features— may beneﬁt from semi-supervised learning. Causal implications also arise in generalized distillation. First, if the privileged features x(cid:63) i only add information about the marginal distribution of the regular features xi, the teacher should be able to help only in anticausal learning problems. Second, if the teacher provides additional information about the conditional distribution of the labels yi given the inputs xi, it should also help in the causal setting. We will conﬁrm this hypothesis in the next section.  5 NUMERICAL SIMULATIONS  We now present some experiments to illustrate when the distillation of privileged information is effective, and when it is not. The necessary Python code to replicate all the following experiments is available at http://github.com/lopezpaz. We start with four synthetic experiments, designed to minimize modeling assumptions and to illus- trate different prototypical types of privileged information. These are simulations of logistic regres- sion models repeated over 100 random partitions, where we use ntr = 200 samples for training, and nte = 10, 000 samples for testing. The dimensionality of the regular features xi is d = 50, and the involved separating hyperplanes α ∈ Rd follow the distribution N (0, Id). For each experiment, we report the test accuracy when i) using the teacher explanations x(cid:63) i at both train and test time, ii) using the regular features xi at both train and test time, and iii) distilling the teacher explanations into the student classiﬁer with λ = T = 1.  6  Published as a conference paper at ICLR 2016  1. Clean labels as privileged information. We sample triplets (xi, x(cid:63)  i , yi) from:  xi ∼ N (0, Id) i ← (cid:104)α, xi(cid:105) x(cid:63) εi ∼ N (0, 1) yi ← I((x(cid:63)  i + εi) > 0).  i is the exact distance to the decision boundary for each xi, but Here, each teacher explanation x(cid:63) the data labels yi are corrupt. This setup aligns with the assumptions about slacks in the similarity control framework of Vapnik & Vashist (2009). We obtained a privileged test classiﬁcation accuracy of 96 ± 0%, a regular test classiﬁcation accuracy of 88 ± 1%, and a distilled test classiﬁcation accuracy of 95 ± 1%. This illustrates that distillation of privileged information is an effective mean to detect outliers in label space.  i , yi) from:  2. Clean features as privileged information We sample triplets (xi, x(cid:63)  i ∼ N (0, Id) x(cid:63) εi ∼ N (0, Id) xi ← x(cid:63) i + ε yi ← I ((cid:104)α, x(cid:63) In this setup, the teacher explanations x(cid:63) i are clean versions of the regular features xi available at test time. We obtained a privileged test classiﬁcation accuracy of 90 ± 1%, a regular test classiﬁcation accuracy of 68 ± 1%, and a distilled test classiﬁcation accuracy of 70 ± 1%. This improvement i are independent from is not statistically signiﬁcant. This is because the intelligent explanations x(cid:63) the noise εi polluting the regular features xi. Therefore, there exists no additional information transferable from the teacher to the student.  i (cid:105) > 0) .  3. Relevant features as privileged information We sample triplets (xi, x(cid:63)  i , yi) from:  where the set J, with |J| = 3, is a subset of the variable indices {1, . . . , d} chosen at random but common for all samples. In another words, the teacher explanations indicate the values of the variables relevant for classiﬁcation, which translates into a reduction of the dimensionality of the data that we have to learn from. We obtained a privileged test classiﬁcation accuracy of 98 ± 0%, a regular test classiﬁcation accuracy of 89±1%, and a distilled test classiﬁcation accuracy of 97±1%. This illustrates that distillation on privileged information is an effective tool for feature selection.  4. Sample-dependent relevant features as privileged information Sample triplets  xi ∼ N (0, Id) i ← xi,J x(cid:63) yi ← I((cid:104)αJ , x(cid:63)  i (cid:105) > 0),  xi ∼ N (0, Id) i ← xi,Ji x(cid:63) yi ← I((cid:104)αJi, x(cid:63)  i (cid:105) > 0),  where the sets Ji, with |Ji| = 3 for all i, are a subset of the variable indices {1, . . . , d} chosen at random for each sample x(cid:63) i . One interpretation of such model is the one of bounding boxes in computer vision: each high-dimensional vector xi would be an image, and each teacher explanation i would be the pixels inside a bounding box locating the concept of interest (Sharmanska et al., x(cid:63) 2013). We obtained a privileged test classiﬁcation accuracy of 96 ± 2%, a regular test classiﬁcation accuracy of 55 ± 3%, and a distilled test classiﬁcation accuracy of 0.56 ± 4%. Note that although the classiﬁcation is linear in x(cid:63), this is not the case in terms of x. Therefore, although we have misspeciﬁed the function class Fs for this problem, the distillation approach did not deteriorate the ﬁnal performance. The previous four experiments set up causal learning problems. In the second experiment, the privileged features x(cid:63) i add no information about the target function mapping the regular features to the labels, so the causal hypothesis from Section 4.3 justiﬁes the lack of improvement. The ﬁrst and third experiments provide privileged information that adds information about the target function, and therefore is beneﬁcial to distill this information. The fourth example illustrates that the privileged features adding information about the target function is not a sufﬁcient condition for improvement.  7  Published as a conference paper at ICLR 2016  Figure 1: Results on MNIST for 300 samples (left) and 500 samples (right).  Figure 2: Results on CIFAR 10 (left) and SARCOS (right).  5. MNIST handwritten digit image classiﬁcation The privileged features are the original 28x28 pixels MNIST handwritten digit images (LeCun et al., 1998b), and the regular features are the same images downscaled to 7x7 pixels. We use 300 or 500 samples to train both the teacher and the student, and test their accuracies at multiple levels of temperature and imitation on the full test set. Both student and teacher are neural networks of composed by two hidden layers of 20 rectiﬁer linear units and a softmax output layer (the same networks are used in the remaining experiments). Figure 1 summarizes the results of this experiment, where we see a signiﬁcant improvement in classiﬁcation accuracy when distilling the privileged information, with respect to using the regular features alone. As expected, the beneﬁts of distillation diminished as we further increased the sample size.  6. Semisupervised learning We explore the semisupervised capabilities of generalized distil- lation on the CIFAR10 dataset (Krizhevsky, 2009). Here, the privileged features are the original 32x32 pixels CIFAR10 color images, and the regular features are the same images when polluted with additive Gaussian noise. We provide labels for 300 images, and unlabeled privileged and reg- ular features for the rest of the training set. Thus, the teacher trains on 300 images, but computes the soft labels for the whole training set of 50, 000 images. The student then learns by distilling the 300 original hard labels and the 50, 000 soft predictions. As seen in Figure 2, the soft labeling of unlabeled data results in a signiﬁcant improvement with respect to pure student supervised clas- siﬁcation. Distillation on the 300 labeled samples did not improve the student performance. This illustrates the importance of semisupervised distillation in this data. We believe that the drops in performance for some distillation temperatures are due to the lack of a proper weighting between labeled and unlabeled data in (4).  7. Multitask learning The SARCOS dataset (Vijayakumar, 2000) characterizes the 7 joint torques of a robotic arm given 21 real-valued features. Thus, this is a multitask learning problem, formed by 7 regression tasks. We learn a teacher on 300 samples to predict each of the 7 torques given the other 6, and then distill this knowledge into a student who uses as her regular input space the 21 real-valued features. Figure 2 illustrates the performance improvement in mean squared error when using generalized distillation to address the multitask learning problem. When distilling at the proper temperature, distillation allowed the student to match her teacher performance.  8  0.00.20.40.60.81.0imitationparameterλ0.40.50.60.70.80.9classiﬁcationaccuracy0.00.20.40.60.81.0imitationparameterλ0.40.50.60.70.80.9classiﬁcationaccuracyteacherstudentT=1T=2T=5T=10T=20T=500.00.20.40.60.81.0imitationparameterλ0.100.110.120.130.140.150.160.17classiﬁcationaccuracy0.00.20.40.60.81.0imitationparameterλ1.01.21.41.61.8meansquarederrorPublished as a conference paper at ICLR 2016  ACKNOWLEDGMENTS  We thank discussions with R. Nishihara, R. Izmailov, I. Tolstikhin, and C. J. Simon-Gabriel.  REFERENCES Ba, Jimmy and Caruana, Rich. Do deep nets really need to be deep? In NIPS, 2014.  Bengio, Yoshua, Louradour, J´erˆome, Collobert, Ronan, and Weston, Jason. Curriculum learning. In  ICML, 2009.  Bottou, L´eon. From machine learning to machine reasoning. Machine learning, 94(2):133–149,  2014.  Buciluˇa, Cristian, Caruana, Rich, and Niculescu-Mizil, Alexandru. Model compression. In KDD,  2006.  Burges, Christopher and Sch¨olkopf, Bernhard. Improving the accuracy and speed of support vector  learning machines. In NIPS, 1997.  Chapelle, Olivier, Agarwal, Alekh, Sinz, Fabian H, and Sch¨olkopf, Bernhard. An analysis of infer-  ence with the Universum. In NIPS, 2007.  Feyereisl, Jan and Aickelin, Uwe. Privileged information for data clustering. Information Sciences,  194:4–23, 2012.  Fouad, Shereen, Tino, Peter, Raychaudhury, Somak, and Schneider, Petra. Incorporating privileged information through metric learning. Neural Networks and Learning Systems, 24(7):1086–1098, 2013.  Hern´andez-Lobato, Daniel, Sharmanska, Viktoriia, Kersting, Kristian, Lampert, Christoph H, and Quadrianto, Novi. Mind the nuisance: Gaussian process classiﬁcation using privileged noise. In NIPS, 2014.  Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distilling the knowledge in a neural network.  arXiv, 2015.  Krizhevsky, Alex. The CIFAR-10 and CIFAR-100 datasets, 2009. URL http://www.cs.  toronto.edu/˜kriz/cifar.html.  Lapin, Maksim, Hein, Matthias, and Schiele, Bernt. Learning using privileged information: Svm+  and weighted svm. Neural Networks, 53:95–108, 2014.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998a.  LeCun, Yann, Cortes, Corinna, and Burges, Christopher JC. The MNIST database of handwritten  digits, 1998b. URL http://yann.lecun.com/exdb/mnist/.  Lopez-Paz, David, Sra, Suvrit, Smola, Alex, Ghahramani, Zoubin, and Sch¨olkopf, Bernhard. Ran-  domized nonlinear component analysis. In ICML, 2014.  Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efﬁcient estimation of word repre-  sentations in vector space. arXiv, 2013.  Oquab, Maxime, Bottou, Leon, Laptev, Ivan, and Sivic, Josef. Learning and transferring mid-level  image representations using convolutional neural networks. In CVPR, pp. 1717–1724, 2014.  Pechyony, Dmitry and Vapnik, Vladimir. On the theory of learning with privileged information. In  NIPS, 2010.  Ribeiro, Bernardete, Silva, Catarina, Vieira, Armando, Gaspar-Cunha, Ant´onio, and das Neves,  Jo˜ao C. Financial distress model prediction using SVM+. In IJCNN. IEEE, 2010.  9  Published as a conference paper at ICLR 2016  Sch¨olkopf, Bernhard, Janzing, Dominik, Peters, Jonas, Sgouritsa, Eleni, Zhang, Kun, and Mooij,  Joris. On causal and anticausal learning. ICML, 2012.  Sharmanska, Viktoriia, Quadrianto, Novi, and Lampert, Christoph H. Learning to rank using privi-  leged information. In ICCV, 2013.  Sharmanska, Viktoriia, Quadrianto, Novi, and Lampert, Christoph H. Learning to transfer privileged  information. arXiv, 2014.  Vapnik, Vladimir. Statistical learning theory. Wiley New York, 1998.  Vapnik, Vladimir and Izmailov, Rauf. Learning using privileged information: Similarity control and  knowledge transfer. JMLR, 16:2023–2049, 2015.  Vapnik, Vladimir and Vashist, Akshay. A new learning paradigm: Learning using privileged infor-  mation. Neural Networks, 22(5):544–557, 2009.  Vijayakumar, Sethu. The SARCOS dataset, 2000. URL http://www.gaussianprocess.  org/gpml/data/.  Weston, Jason, Collobert, Ronan, Sinz, Fabian, Bottou, L´eon, and Vapnik, Vladimir. Inference with  the Universum. In ICML, 2006.  10  ",
1511.05879,2016,Particular object retrieval with integral max-pooling of CNN activations,"['Particular object retrieval with integral max-pooling of CNN activations [code]\nGiorgos Tolias', 'Ronan Sicre', 'Hervé Jégou']",https://arxiv.org/pdf/1511.05879,"6 1 0 2     b e F 4 2         ]  V C . s c [      2 v 9 7 8 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  PARTICULAR OBJECT RETRIEVAL WITH INTEGRAL MAX-POOLING OF CNN ACTIVATIONS  Giorgos Tolias ∗ Center for Machine Perception FEE CTU Prague  Ronan Sicre Irisa Rennes  Herv´e J´egou Facebook AI Research  ABSTRACT  Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperform- ing pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build com- pact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efﬁciently local- ize matching objects. The resulting bounding box is ﬁnally used for image re- ranking. As a result, this paper signiﬁcantly improves existing CNN-based recog- nition pipeline: We report for the ﬁrst time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets.  1  INTRODUCTION  CONTENT based image retrieval has received a sustained attention over the last decade, leading to  mature systems for tasks like visual instance retrieval. Current state-of-the-art approaches are derived from the Bag-of-Words model of Sivic & Zisserman (2003) and mainly owe their success to locally invariant features (Lowe, 2004) and large visual codebooks (Philbin et al., 2007). These methods are typically composed of an initial ﬁltering stage where all database images are ranked in terms of similarity to a query image and a second re-ranking stage, which reﬁnes the search results of the top-ranked elements. The ﬁltering stage is improved in several ways, such as incorpo- rating weak geometric information (J´egou et al., 2010), employing compact approximations of the local descriptors (J´egou et al., 2010), or learning smart codebooks (Mikulik et al., 2013; Avrithis & Kalantidis, 2012). In such cases, local descriptors are individually matched and selective matching functions (Tolias et al., 2015; Tao et al., 2014) improve the search quality. Geometric matching mod- els (Philbin et al., 2007; Avrithis & Tolias, 2014) are typically applied in a pairwise manner during the re-ranking stage of a short-list of images. Query expansion approaches signiﬁcantly increase the performance (Chum et al., 2011), at the cost of larger query times. The recent advances achieved by Convolutional Neural Networks (CNN) and the use of intermediate layer activations as feature vectors (Donahue et al., 2013) create opportunities for representations that are competitive for image or particular object retrieval, and not only classiﬁcation tasks. Several works have already investigated this research direction, such as global or local representations based on either fully connected (Babenko et al., 2014; Gong et al., 2014) or convolutional layers (Razavian et al., 2014b; Azizpour et al., 2014; Babenko & Lempitsky, 2015). The performance of CNN-based features has rapidly improved to the point of competing and even outperforming pre-CNN works that aggregate local features (J´egou et al., 2012; Radenovi´c et al., 2015). In particular, activations of convolutional layers followed by a global max-pooling operation (Azizpour et al., 2014) produce highly competitive compact image representations. One limitation is that such approaches are not compatible with the geometric-aware models involved in the ﬁnal re-ranking stages.  ∗Research partially conducted while G. Tolias and H. J´egou were at Inria. We would like to thank Florent  Perronnin for his valuable feedback. This work was partly supported by MSMT LL1303 ERC-CZ grant.  1  Published as a conference paper at ICLR 2016  Figure 1: Query objects (left) and the corresponding localization in another image (right) are shown. We visualize the patches that contribute the highest to the image similarity score. Displayed patches correspond to the receptive ﬁeld of CNN activations. Object localization is displayed in magenta, while different colors are used for patches in correspondence.  This work revisits both ﬁltering and re-ranking stages with CNN-based features. We make the three following contributions.  • First, we propose a compact image representation derived from the convolutional layer activations that encodes multiple image regions without the need to re-feed multiple inputs to the network, in spirit of recent Fast-RCNN (Girshick, 2015) and Faster-RCNN (Ren et al., 2015) methods but here targeting particular object retrieval. The underlying primitive representation is used in all stages (initial retrieval and re-ranking). • Second, we employ the generalized mean (Doll´ar et al., 2009) to enable the use of integral images along with max-pooling. This efﬁcient method is exploited for particular object localization (see Figure 1) directly in the 2D maps of CNN activations. • Third, our localization approach is used for image re-ranking and leads us to deﬁne a simple  yet effective query expansion method.  These approaches are complementary and, when combined, produce for the ﬁrst time a system which compete on the Oxford and Paris building benchmarks with state-of-the-art re-ranking approaches based on local features. Our approach outperforms by a large margin previous methods based on CNN, while being more efﬁcient in practice.  2 RELATED WORK CNN based representation. A typical CNN consists of several convolutional layers, followed by fully connected layers and ends with a softmax layer producing a distribution over the training classes. Instead of using this inherent classiﬁer, one can consider the activations of the intermediate layers to train a classiﬁer. In particular, the activations of the fully connected layers have been shown to be very effective and capable of adaptation to various domains (Oquab et al., 2014), such as scene recognition (Donahue et al., 2013; Sicre & Jurie, 2015), object detection (Iandola et al., 2014), and semantic segmentation (Girshick et al., 2014). In the case of image retrieval, fully connected layers are used as global descriptors followed by dimensionality reduction (Babenko et al., 2014). They are also employed as region descriptors to be compared to database descriptors (Razavian et al., 2014a) or aggregated in a VLAD manner (Gong et al., 2014). Recent works derive visual representations from the activations of the convolutional layers. This is achieved either by stacking activations (Girshick et al., 2014) or by performing spatial max- pooling (Azizpour et al., 2014) or sum-pooling (Babenko & Lempitsky, 2015) for each feature chan- nel. According to Azizpour et al. (2014) such representation offers better generalization properties for test data that are far from the source (training) data. Noticeably, higher performance in particular object or scene retrieval is obtained by using convolutional layers rather than fully connected ones. The very recent work of Babenko & Lempitsky (2015) shows that sum-pooling performs better than max-pooling when the image representation is whitened. In addition to be a costly choice, we will show that this is not optimal in our context of object localization (see Section 8). Finally, Kalantidis et al. (2015) propose spatial and feature channel weighting that signiﬁcantly improves performance. Their approach is complementary to what we propose for the ﬁltering and the re-ranking stage. Recent examples utilize information from fully connected layers to perform generic object detec- tion (Iandola et al., 2014; Papandreou et al., 2014). Such approaches are prohibitive for the re- ranking purposes of large scale image retrieval. They have high computational cost and the inherent features are not optimal for particular object matching.  2  Published as a conference paper at ICLR 2016  Figure 2: We visualize the receptive ﬁelds related to the 5 MAC components that contribute the most to the image similarity. Each displayed receptive ﬁeld corresponds to the maximum response of a feature channel. A different color is used for each feature channel, while different feature channels are shown for each image pair.  Localization. In the recent years, the sliding window principle has been quite successful for many object localization methods. Due to the large number of possible windows, exhaustive search is extremely costly. However, integral images (Viola & Jones, 2001) offer a constant cost solution to the evaluation of a single region. This attractive alternative is applicable for feature vectors constructed via a sum-pooling operation. A globally optimal solution is given by Efﬁcient Subwindow Search (ESS) of Lampert et al. (2009), who use branch-and-bound search to avoid exhaustive search. Their work employs integral images, which are also used in later improvements of ESS (An et al., 2009). An et al. (2009) formalize local- ization as a maximum sub-array problem and similarly to Chen et al. (2013) they employ Bentley’s algorithm (Bentley, 1999). Integral images facilitate the evaluation of many region candidates (Ui- jlings et al., 2013) based on VLAD or Fisher vectors (Van de Sande et al., 2014). All aforementioned approaches take advantage of integral images due to the inherent sum-pooling operation in the given representation. In this paper, we extend integral images to perform max-pooling over CNN acti- vation maps, which is shown to be a better choice for describing regions (as opposed to the entire image). Several object localization techniques have been proposed in the context of image retrieval as well. Lampert (2009) propose a two layer branch-and-bound method that alternates between regions and images. Integral images offer a signiﬁcant speed-up in the work of Lin & Brandt (2010) to perform localization through Bag-of-Words. The overall idea bears similarities with our work. However, we differentiate by employing CNN-based representation with max-pooling. Some approaches (Tao et al., 2014; Shen et al., 2014) individually index local features for localization. In our case, the localization method is built on top of a compact representation, initially used for the ﬁltering stage. Finally, Arandjelovic & Zisserman (2013) propose a localization strategy based on VLAD, where similarity is computed for multiple image regions, giving a more precise localization via regression.  3 BACKGROUND  We consider a pre-trained CNN and discard all the fully connected layers. Given an input image I of size WI × HI, the activations (responses) of a convolutional layer form a 3D tensor of W × H × K dimensions, where K is the number of output feature channels, i.e. multi-dimensional ﬁlters. The spatial resolution W × H depends on the network architecture, the layer examined, and the input image resolution. We assume that Rectiﬁed Linear Units (ReLU) are applied as a last step, guaranteeing that all elements are non-negative. We represent this 3D tensor of responses as a set of 2D feature channel responses X = {Xi}, i = 1 . . . K, where Xi is the 2D tensor representing the responses of the ith feature channel over the set Ω of valid spatial locations, and Xi(p) is the response at a particular position p. Therefore, the feature vector constructed by a spatial max-pooling over all locations (Azizpour et al., 2014) is given by  fΩ = [fΩ,1 . . . fΩ,i . . . fΩ,K](cid:62), with fΩ,i = max p∈Ω  Xi(p).  (1)  Maximum activations of convolutions (MAC). Two images are compared with the cosine similar- ity of the K-dimensional vectors produced as described above. This representation, referred to as MAC, does not encode the location of the activations (unlike activations of fully connected layers),  3  Published as a conference paper at ICLR 2016  due to the max-pooling operated over a single region of size W × H. It encodes the maximum “local” response of each of the convolutional ﬁlters and is therefore translation invariant. In all the following, we consider the last convolutional layer of the examined networks. Figure 2 visualizes the patches that contribute the most to the image similarity. They correspond either to the same object part or similar parts due to repeated structures. We extract MAC from input images of any resolution or aspect ratio by simply subtracting the mean pixel value (Iandola et al., 2014) from the input images. No crop or change of aspect ratio is required (Azizpour et al., 2014). The max pooling operation that is performed over a single cell offers translation invariance to the resulting representation. This is in contrast to representation derived from the fully connected layers that requires objects to be aligned. In our case, we assume that objects are up-right and we simply beneﬁt from the rotation tolerance provided by the CNN due to the training data used. The same stands for the tolerance to scale changes.  4 ENCODING REGIONS INTO SHORT VECTORS  This section describes how we exploit the activations of the CNN convolutional layers to derive representations for image regions. Region vectors are aggregated to produce a short signature used in the ﬁltering stage of image retrieval. Region feature vector. The feature vector fΩ described in Section 3 is a representation for the whole image I. Now, we consider a rectangular region R ⊆ Ω = [1, W ] × [1, H], and deﬁne the regional feature vector  fR = [fR,1 . . . fR,i . . . fR,K](cid:62)  (2)  where fR,i = maxp∈R Xi(p) is the maximum activation of the ith channel on the considered region. The regions R are deﬁned on the space Ω of all valid positions for the considered feature map (and not on the input image plane). A region of size 1 corresponds to a feature vector consisting of a single activation at a particular location. We are now able to construct a representation for multiple regions without re-feeding additional input to the CNN, similarly to recent RNN variants (Ren et al., 2015; Girshick, 2015), which drastically reduces the processing cost. Now assume a linear mapping of a given region R back to the original image. The proposed region vector captures a larger image region than the back-projected one, due to the large receptive ﬁeld. A similar effect occurs in the context of object detection (Iandola et al., 2014), where fully connected layers are applied in a sliding window fashion. R-MAC: regional maximum activation of convolutions. We now consider a set of R regions of different sizes. The structure of the regions is similar to the one proposed by Razavian et al. (2014b), but we deﬁne them on the CNN response maps and not on the original image. We sample square regions at L different scales. At the largest scale (l = 1), the region size is determined to be as large as possible, i.e., its height and width are both equal to min(W, H). The regions are sampled uniformly such that the overlap between consecutive regions is as close as possible to 40%. Remark that the aspect ratio of the original image has an inﬂuence on the number m of regions that we extract (1 region only if the input image is square). At every other scale l we uniformly sample l × (l + m − 1) regions of width 2 min(W, H)/(l + 1), as illustrated in Figure 3 (left). Then we calculate the feature vector associated with each region, and post-process it with (cid:96)2- normalization, PCA-whitening (J´egou & Chum, 2012) and (cid:96)2-normalization. We combine the col- lection of regional feature vectors into a single image vector by summing them and (cid:96)2-normalizing in the end. This choice keeps the dimensionality low which is equal to the number of feature channels. However, we show in our experiments that the resulting representation, referred to as R-MAC, of- fers a signiﬁcant better performance than the corresponding MAC with same dimensionality. Note, the aggregation of the region vectors can be seen as a simple kernel that cross matches all possible regions, including across different scale.  4  Published as a conference paper at ICLR 2016  Figure 3: Left: Sample regions extracted at 3 different scales (l = 1 . . . 3). We show the top-left region of each scale (gray colored region) and its neighboring regions towards each direction (dashed borders). We depict the centers of all regions with a cross. Middle: Approximation error of the maximum value versus the size of the response set for different values of exponent α. Measurements are performed on 10 randomly selected images by evaluating all possible regions. The responses for this set of images take values in [0, 151]. Right: Empirical distribution of the cosine similarity value between the exact vector fR and its approximation ˜fR. Measurements are collected by constructing the exact and approximate vectors of all possible regions on 10 randomly sampled images.  5 OBJECT LOCALIZATION  In this section we propose an extension of integral images to perform approximate max-pooling over a set X of 2D feature channel response maps, which provide a rough yet efﬁcient localization to our CNN-based method. Approximate integral max-pooling. Noticing that the responses Xi are non-negative, we exploit the generalized mean (Doll´ar et al., 2009) to approximate each feature value fR,i associated with a given region R by the estimate  ˜fR,i =  Xi(p)α  ≈ max  p∈R Xi(p) = fR,i,  (3)  where the parameter α > 1 is such that ˜fi → fi when α → +∞. Figure 3 (middle) shows the average approximation error |˜fR,i − fR,i| estimated over several image regions. We report the approximation error as a function of the size of the corresponding response set on which the maximum value is computed. The various sizes of response sets are an outcome of using all possible regions. A high value of the exponent α leads to a better approximation, while applying on more elements makes the approximation less precise. By approximating the maximum in this manner, we can now use integral images (Viola & Jones, 2001) to approximate the regional feature vector fR deﬁned on any rectangular region R. For each channel, we construct the integral image of the 2D tensor whose value at position p is equal to Xi(p)α, p ∈ R. Then, the sum of Equation (3) is simply given by the sum of 4 terms (Viola & Jones, 2001). This allow us to efﬁciently compute max-pooling for many regions and therefore to construct the corresponding feature vectors. This is in contrast to the explicit construction of many regions with representation derived from fully connected layers, which is prohibitive due to the need to resize/crop and re-feed each region to the network. We evaluate the approximation quality by measuring the cosine similarity between the exact vector and its approximate counterpart. The distribution of this similarity is presented in Figure 3 (right) and is measured on all possible regions of 10 randomly selected images. The proposed approxima- tion is very precise even for moderate values of α. We set α equal to 10 in all of our experiments. Window detection. Let us now assume that there is another image Q depicting a single object, i.e. cropped via a bounding box deﬁning the object of interest. We denote as q the corresponding MAC feature vector. The 2D region, deﬁned on the CNN activations X of image I, that maximizes the similarity to q is computed as  ˆR = arg maxR⊆Ω  ˜f(cid:62) Rq (cid:107)˜fR(cid:107)(cid:107)q(cid:107) .  5  (4)  (cid:88)  p∈R   1  α  02004006000246810|Xi||˜fR,i−fR,i|α=5α=10α=20α=500.980.991˜f(cid:62)RfR/((cid:107)˜fR(cid:107)(cid:107)fR(cid:107))α=5α=10Published as a conference paper at ICLR 2016  , H HI  The region ˆR maximizing the similarity is mapped back to the original image I with a precision of ( W ) pixels, providing a rough localization of the object depicted in Q. The corresponding WI similarity does not take into account all the visual content of image I and is therefore free from the inﬂuence of background clutter. The brute-force detection of the optimal region by exhaustive search is expensive, as the number of possible regions is in O(W 2H 2). In preliminary tests, we have evaluated a globally optimal solution based on branch and bound search, as in ESS (Lampert et al., 2009). The necessary bounds are trivially derived for our representation. The search is not signiﬁcantly sped up in our case: The maxima are not distinct enough and a large number of regions are considered, while the overhead of maintaining the priority queue is high. AML: approximate max-pooling localization. Instead, we restrict the number of regions that we evaluate and locally reﬁne the best ones with simple heuristics. Candidate regions are uniformly sampled with a search step equal to t. In addition, regions having an aspect ratio larger than s times that of the query region are discarded. The parameters of the best region are reﬁned in a coordinate descent manner, while allowing a maximum change of 3 units. The reﬁnement process is repeated up to 5 times. Experiments show that the overlap of the detected region to the optimal one is high.  6 RETRIEVAL, LOCALIZATION AND RE-RANKING  Initial retrieval. The MAC or R-MAC feature vector is computed for all databases images. Simi- larly, at query time we process the query image and extract the corresponding feature vector. During the ﬁltering stage we directly evaluate cosine similarity between the query and all the database vec- tors. Therefore, we obtain the initial ranking based on the similarity of MAC or R-MAC vectors. Re-ranking. We consider a second re-ranking stage, as typically performed in spatial veriﬁca- tion (Philbin et al., 2007) with local features. A short-list of N top-ranked images is considered and AML, as described in Section 5, is applied on pairs of query and database images. Note that the query is now represented by the MAC vector, since this is used in AML, while the database image is represented by X . For each re-ranked image we obtain a score given by the region that maxi- mizes the similarity to the query. This similarity is used to re-rank the elements of the short-list. Furthermore, a rough localization of the query object is available. Remarks: At the ﬁltering stage, whitened MAC (whitening as described in Section 8) or R-MAC can be used, while the localization procedure employs similarity with respect to (cid:96)2-normalized MAC. However, once the query object is localized, then, similarity between the query and the detected region is computed via whitened MAC or R-MAC, depending on the chosen ﬁltering method. This similarity score is used to perform re-ranking. The required representation is constructed on query time only for the detected region and is acquired efﬁciently with integral images. Query expansion (QE). Re-ranking brings positive images at the very top ranked positions. Then, we collect the 5 top-ranked images, merge them with the query vector, and compute their mean. Finally, the similarity to this mean vector is adopted to re-rank once more the top N images.  IMPLEMENTATION DETAILS  7 We observe that thresholding the response values of X which are larger than 128 (0.001% of all re- sponses) and mapping each value to the closest smaller integer (ﬂoor operation) leads to insigniﬁcant losses. This allows the computation of α-th power with a lookup table and speeds-up the construc- tion of integral images. Moreover, we approximate the α-th root of Equation (3) by performing binary search on the same lookup table of α-th power. This process allows the optimal window search to be more efﬁcient. The response maps represented by X are sparse (Agrawal et al., 2014). In particular, using the network of Krizhevsky et al. (2012) on Oxford buildings dataset (Philbin et al., 2007) results in 81% of response values being zero, which is convenient for storage purpose. We further decrease the memory requirements by uniformly quantizing the responses into 8 values. This results in more elements mapped to the same value. Therefore, we store the positions of non-zeros values with delta coding and use only 1 byte per non-zero element. Note that an image of resolution equal to 1024 × 768 corresponds to feature channel response maps of size 30 × 22 using the same network. Finally, an image requires around 32 kB of memory. At re-ranking time we construct one integral image at a time and use double precision (8 bytes) for its elements.  6  Published as a conference paper at ICLR 2016  Table 1: Left: Comparison between the exhaustive sliding window and our alternative of window sampling and reﬁnement. We report the average IoU w.r.t. the globally optimal window and the average percentage of windows evaluated w.r.t. to the exhaustive search (noted by %W). Measure- ments are conducted on all pairs of Oxford5k query images and their corresponding positive images. Right: Performance (mAP) of MAC and R-MAC on Oxford5k. Resol. corresponds to the input im- age resolution (maximum dimension).  Search step t  1 2 3 4 5  Aspect ratio change threshold s  1.1  1.5  2.0  IoU %W IoU %W IoU %W 46.3 81.8 3.6 79.9 78.7 0.8 0.3 77.0 75.8 0.2  8.9 88.7 27.5 93.7 2.0 86.6 0.5 83.8 0.2 81.2 0.5 83.6 0.2 81.5 0.1 79.5 0.1 79.0 0.1 80.9  8 EXPERIMENTS  Network Resol. MAC  AlexNet  VGG16  1024 724 1024 724  44.9 44.8 55.2 52.2  R-MAC  L=1 L=2 L=3 L=4 47.9 54.6 56.1 55.6 48.4 54.4 54.3 52.6 57.3 64.5 66.9 67.4 54.8 58.0 60.9 60.3  This section presents the results of our compact representation for image retrieval, evaluate the localization accuracy AML, and ﬁnally employ it for retrieval re-ranking. Experimental setup. We evaluate the proposed methods on Oxford Buildings dataset (Philbin et al., 2007) and Paris dataset (Philbin et al., 2008), which are composed of 5063 and 6412 images, respectively. We refer to these datasets as Oxford5k and Paris6k. We additionally use 100k Flickr images (Philbin et al., 2007) to compose Oxford105k and Paris106k, respectively. A distractor set of 1 million images from Flickr (J´egou et al., 2010) is additionally used to go at larger scale. Re- trieval performance is measured in terms of mean Average Precision (mAP). We follow the standard protocol and use the bounding boxes deﬁned on the query images1. These bounding boxes are also employed to evaluate localization accuracy. PCA is learned on Paris6k when testing on Oxford5k and vice versa. In order to be fair, we directly compare our results only to previous methods that do not perform learning on the test set. The focus of our work is not to train a CNN, but to extract visual descriptors from its convolutional layers. We use networks widely used in the literature: AlexNet by Krizhevsky et al. (2012) and the very deep network (VGG16) by Simonyan & Zisserman (2014). We choose VGG16 instead of VGG19 because we observe that the latter does not always attain better performance while it has higher feature extraction cost. Our representation is extracted from the last pooling layer, which has 256 feature channels for AlexNet and 512 for VGG16. MatConvNet (Vedaldi & Lenc, 2014) is used to extract the features. Localization accuracy. To evaluate the accuracy of AML, we employ pairs of Oxford5k query images and their corresponding positive images. We ﬁrst perform exhaustive search to detect the globally optimal window. Then, we apply our speeded-up detector that evaluates fewer regions and in the end reﬁnes the best one. In both cases the approximate max-pooling is used for each window evaluation. We report Intersection over Union (IoU) with the optimal window and the percentage of windows evaluated compared to the exhaustive case. Results are shown in Table 1 (left). We provide a large speed-up while maintaining a high overlap with the optimal detection. Recall that our purpose is to apply this detector for fast re-ranking. Measuring IoU provides evidence for localization accuracy, however we observed that it does not directly impact retrieval performance. We ﬁnally set s = 1.1 and t = 3 for re-ranking usage. In order to evaluate the localization accuracy with respect to ground-truth annotation we cross- match all 5 query images that exist per building. One of them is used as a query (cropped bounding box), while for the other we compare the detected region to the ground-truth annotation. Exhaustive evaluation achieves an IoU equal to 52.6% (52.9%) and the speeded-up approach achieves 51.3% (51.4%) on Oxford5k (Paris6k) datasets. The accuracy loss is limited, while the localization is approximately 180 times faster. AML provides a rough localization at low computational cost. Such  1The query regions are cropped and then used as input to the CNN.  7  Published as a conference paper at ICLR 2016  Figure 4: Performance of retrieval with re-ranking by AML versus number of re-ranked images on Oxford105k and Oxford5k combined with 1M distractor images.  Table 2: Performance comparison with state of the art. We report results for compact vector repre- sentations (left) and for retrieval approaches employing geometry, re-ranking, query expansion, or vector approximations (right). D = dimensionality. Our approaches are identiﬁed with bullets •. D Method 1024 56.0 J´egou & Zisserman (2014) 43.3 128 J´egou & Zisserman (2014) 128 55.7 Babenko et al. (2014) 53.3 67.0 256 Razavian et al. (2014b) Babenko & Lempitsky (2015) 256 53.1 R-MAC • 56.1 72.9 256 R-MAC • 66.9 83.0 512  Oxf5k Par6k Oxf105k Par106k 82.7 80.5 Chum et al. (2011) Danfeng et al. (2011) 81.4 80.3 84.9 82.4 Mikulik et al. (2013) 75.2 74.1 Shen et al. (2014) 77.8 - Tao et al. (2014) 80.4 77.0 Tolias et al. (2015) R-MAC+AML+QE • 77.3 86.5  Oxf5k Par6k Oxf105k Par106k Method  50.2 35.3 52.3 48.9 50.1 47.0 61.6  76.7 76.7 79.5 72.9  75.0 73.2  60.1 75.7  79.8  71.0  77.3  - - - - -  - - -  - - -  -  -  -  a setup results in an average re-ranking query time of 2.9 sec using AlexNet, when re-ranking 1000 images with a single threaded implementation. Retrieval and re-ranking. We evaluate retrieval performance using MAC and R-MAC compact representations. The MAC vectors are (cid:96)2-normalized, PCA-whitened and (cid:96)2-normalized once more, while the corresponding processing of the R-MAC is as described in Section 4. Table 1 (right) presents the results on Oxford5k. We evaluate different input image resolutions and observe that the original image size (1024) provides higher performance. Note that MAC is similar to the one proposed by Azizpour et al. (2014), however their process remains constrained by standard input size and aspect ratio. The proposed R-MAC gives a large performance improvement at no extra cost, as both feature vectors have exactly the same dimensionality. Regions of different scales are aggregated together, meaning that L = 3 combines regions at scales l = 1, l = 2, and l = 3. We set L = 3 in the following. In order to decompose the components of R-MAC, we construct R-MAC by aggregating only regions of l = 3. It achieves mAP equal to 63.0 on Oxford5k with VGG16. Aggregating both regions of l = 2 and l = 3 improves to 65.4. Finally, adding l = 1 (original R-MAC) performs 66.9 (see Table 1 right). Filtering time is 12 ms on average for Oxford105k. Next, we employ AML for image re-ranking and conduct performance evaluation on Oxford105k by re-ranking up to 1000 images. The performance is consistently improved as shown in Figure 4. R-MAC brings a larger beneﬁt and VGG16 performs better than AlexNet. Query expansion, as described in Section 6, improves the performance at low extra cost, since similarity is re-computed only for the re-ranked short-list. Finally, we carry out experiment at larger scale with 1M distractor images and present results in Figure 4. AML improves the performance by 13% mAP. Examples of ranking using MAC and re-ranking using AML are presented in Figure 5. Recall that we only provide a rough object localization, since our main goal is to obtain improved image similarity. Furthermore, the provided localization is accurate enough for re-ranking. Comparison to the state of the art. We compare the proposed methods to state-of-the-art per- formance of compact representations and approaches based on local features that perform precise  8  02004006008001,00030405060NmAPOxford105k,MACAlex+AMLAlex+AML+QEVGG+AMLVGG+AML+QE02004006008001,0003040506070NmAPOxford105k,VGGnetworkMAC+AMLMAC+AML+QER-MAC+AMLR-MAC+AML+QE02004006008001,000405060NmAPOxford5k+1M,VGGnetworkMAC+AML+QER-MAC+AML+QEPublished as a conference paper at ICLR 2016  Query  1 → 1  21 → 2  19 → 3  13 → 4  3 → 5  25 → 6  2 → 7  8 → 8  Query  1 → 1  4 → 2 220 → 3  52 → 4  15 → 5  212 → 6  10 → 7 159 → 8 26 → 9  860 → 10  Query  120 → 1  118 → 2  753 → 3  467 → 4  631 → 5  82 → 6  594 → 7  Query  1 → 1  2 → 2  7 → 3  5 → 4  3 → 5  4 → 6  8 → 7  6 → 8  43 → 9  9 → 10  Figure 5: Examples of top retrieved images before (top) and after (bottom) re-ranking with AML. On the left we show the query image and depict the bounding box in blue color. When re-ranking is used, we present the top ranked images and report for each image its initial and ﬁnal ranking. The localization window is shown in magenta, while positive/negative/junk images are depicted with green/red/yellow border.  descriptor matching, re-ranking or query expansion. Results are shown in Table 22. AlexNex and VGG16 are used to produce the 256D and 512D vectors for R-MAC, respectively. Regarding the compact representations, our short-sized R-MAC outperforms all other approaches. The better per- formance on Paris is inherited by the nature of the pre-trained networks; the baseline MAC with VGG achieves 55.2 on Oxford5k and 74.7 on Paris6k. Unlike previous description schemes derived from CNN layers, our approach compete with the best approaches based on local features for geometric matching and query expansion. Our AML can even outperform them: while our results are lower on Oxford, we achieve the best performance on Paris and, to the best of our knowledge, outperform all published results on this benchmark. Higher scores on Paris6k are reported by Arandjelovic & Zisserman (2012) (91.0) and by Zhong et al. (2015) (91.5). These are achieved by learning the codebook on Paris6k itself and by performing pre-processing of the indexed dataset. Discussion about other CNN-based approaches. Razavian et al. (2014b) propose to perform re- gion cross-matching and accumulate the maximum similarity per query region. We evaluate this cross-matching process on the collection of regional vectors used in R-MAC; we simply skip the ﬁnal aggregation process and keep the regional vectors individually. The cross-matching achieves 75.2% mAP on Oxford5k as a ﬁltering stage, while re-ranking with AML on top of this acts in a complementary way and increases the performance up to 78.1%. However, cross-matching has  2Small differences of scores compared to the ﬁrst version of the manuscript on arxiv are due to a slightly  different evaluation protocol used before. Now, the evaluation protocol is the standard one for these datasets.  9  Published as a conference paper at ICLR 2016  two drawbacks. Firstly, the region vectors have to be stored individually and increase the mem- ory requirements by a factor of |R|, where |R| is the number of extracted regions. Secondly, the complexity cost is linear in the number of indexed images and quite high since it requires to com- pute |R|2 (e.g. 1024 (Razavian et al., 2014b)) inner products per image. The work of Razavian et al. (2014b) follows a non-standard evaluation protocol by enlarging the provided query bounding boxes. In addition, the cost of their feature extraction is extremely high since they feed 32 images of resolution 576 × 576 to the CNN. The recent work of Xie et al. (2015) is quite similar to theirs and is applied on both retrieval and classiﬁcation. Babenko & Lempitsky (2015) show that global sum-pooling on convolutional layer activations is better than max-pooling when the ﬁnal image vectors are PCA-whitened. When whitening is not employed, then the latter is better. In the context of object localization we efﬁciently evaluate a large number of candidate regions on query time with AML. Performing whitening on each candidate re- gion vector signiﬁcantly increases the cost and is prohibitive for this task. We switch max-pooling to sum-pooling for both our proposed R-MAC and AML and test performance. Note that sum-pooling is a special case of our integral max-pooling with α = 1. Switching to sum-pooling makes R-MAC perform 69.8 and R-MAC +AML +QE perform 76.9 on Paris106k. These scores are directly com- parable to our scores in Table 2 and reveal that our choice is consistently better in all cases within our pipeline.  9 CONCLUSIONS  In this work, we re-visit both ﬁltering and re-ranking retrieval stages by employing CNN activations of convolutional layers. Our compact vector representation encodes several image regions with sim- ple aggregation method and is shown to outperform state-of-the-art competitors. Our localization increases the performance of the retrieval system that is initially based on a compact representa- tion. The same CNN information adopted during the ﬁltering stage is employed for re-ranking as well. Our approach competes with state-of-the-art methods that employ costly geometric matching or query expansion and we achieve the highest performance on Paris dataset, and provided a much better performance than existing approaches built upon CNN features. A very recent work (Arand- jelovic et al., 2015) shows how MAC performance is improved by end-to-end ﬁne tunning where the objective is based on MAC similarity.  REFERENCES Agrawal, Pulkit, Girshick, Ross, and Malik, Jitendra. Analyzing the performance of multilayer  neural networks for object recognition. In ECCV, 2014.  An, Senjian, Peursum, Patrick, Liu, Wanquan, and Venkatesh, Svetha. Efﬁcient algorithms for  subwindow search in object detection and localization. In CVPR, 2009.  Arandjelovic, Relja and Zisserman, Andrew. Three things everyone should know to improve object  retrieval. In CVPR, Jun. 2012.  Arandjelovic, Relja and Zisserman, Andrew. All about VLAD. In CVPR, Jun. 2013.  Arandjelovic, Relja, Gronat, Petr, Torii, Akihiko, Pajdla, Tomas, and Sivic, Josef. Netvlad: Cnn  architecture for weakly supervised place recognition. In arXiv, 2015.  Avrithis, Yannis and Kalantidis, Yannis. Approximate gaussian mixtures for large scale vocabular-  ies. In ECCV, 2012.  Avrithis, Yannis and Tolias, Giorgos. Hough pyramid matching: Speeded-up geometry re-ranking  for large scale image retrieval. IJCV, 2014.  Azizpour, Hossein, Razavian, Ali Sharif, Sullivan, Josephine, Maki, Atsuto, and Carlsson, Stefan.  From generic to speciﬁc deep representations for visual recognition. In arXiv, 2014.  Babenko, Artem and Lempitsky, Victor. Aggregating deep convolutional features for image retrieval.  In ICCV, 2015.  10  Published as a conference paper at ICLR 2016  Babenko, Artem, Slesarev, Anton, Chigorin, Alexandr, and Lempitsky, Victor. Neural codes for  image retrieval. In ECCV, Sep. 2014.  Bentley, Joe. Programming Pearls, 2/E. Addison-Wesley Professional, 1999.  Chen, Qiang, Song, Zheng, Feris, Rogerio, Datta, Amitava, Cao, Liangliang, Huang, Zhongyang, and Yan, Shuicheng. Efﬁcient maximum appearance search for large-scale object detection. In CVPR, 2013.  Chum, Ondrej, Mikulik, A., Perdoch, M., and Matas, J. Total recall II: Query expansion revisited.  In CVPR, Jun. 2011.  Danfeng, Qin, Gammeter, S., Bossard, L., Quack, T., and Gool, L. Van. Hello neighbor: Accurate  object retrieval with k-reciprocal nearest neighbors. In CVPR, 2011.  Doll´ar, Piotr, Tu, Zhuowen, Perona, Pietro, and Belongie, Serge.  BMVC, 2009.  Integral channel features.  In  Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. Decaf: A deep convolutional activation feature for generic visual recognition. In arXiv, 2013.  Girshick, Ross. Fast r-cnn. In arXiv, 2015.  Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik, Jitendra. Rich feature hierarchies for  accurate object detection and semantic segmentation. In CVPR, 2014.  Gong, Yunchao, Wang, Liwei, Guo, Ruiqi, and Lazebnik, Svetlana. Multi-scale orderless pooling  of deep convolutional activation features. In ECCV, 2014.  Iandola, Forrest, Moskewicz, Matt, Karayev, Sergey, Girshick, Ross, Darrell, Trevor, and Keutzer,  Kurt. Densenet: Implementing efﬁcient convnet descriptor pyramids. In arxiv, 2014.  J´egou, Herv´e and Chum, Ondrej. Negative evidences and co-occurences in image retrieval: The  beneﬁt of PCA and whitening. In ECCV, Oct. 2012.  J´egou, Herv´e and Zisserman, Andrew. Triangulation embedding and democratic aggregation for  image search. In CVPR, 2014.  J´egou, Herv´e, Douze, Matthijs, and Schmid, Cordelia. Improving bag-of-features for large scale  image search. IJCV, 87(3), Feb. 2010.  J´egou, Herv´e, Perronnin, Florent, Douze, Matthijs, S´anchez, Jorge, P´erez, Patrick, and Schmid,  Cordelia. Aggregating local descriptors into compact codes. Trans. PAMI, Sep. 2012.  Kalantidis, Yannis, Mellina, Clayton, and Osindero, Simon. Cross-dimensional weighting for ag-  gregated deep convolutional features. In arXiv preprint arXiv:1512.04065, 2015.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, 2012.  Lampert, Christoph H. Detecting objects in large image collections and videos by efﬁcient subimage  retrieval. In ICCV, 2009.  Lampert, Christoph H, Blaschko, Matthew B, and Hofmann, Thomas. Efﬁcient subwindow search:  A branch and bound framework for object localization. Trans. PAMI, 31(12):2129–2142, 2009.  Lin, Zhe and Brandt, Jonathan. A local bag-of-features model for large-scale object retrieval. In  ECCV, 2010.  Lowe, David. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, Nov.  2004.  Mikulik, Andrej, Perdoch, Michal, Chum, Ondˇrej, and Matas, Jiˇr´ı. Learning vocabularies over a ﬁne  quantization. IJCV, 103(1), 2013.  11  Published as a conference paper at ICLR 2016  Oquab, Maxime, Bottou, Leon, Laptev, Ivan, and Sivic, Josef. Learning and transferring mid-level  image representations using convolutional neural networks. In CVPR, 2014.  Papandreou, George, Kokkinos, Iasonas, and Savalle, Pierre-Andr´e. Untangling local and global de- formations in deep convolutional networks for image classiﬁcation and sliding window detection. In arXiv, 2014.  Philbin, James, Chum, Ondrej, Isard, Michael, Sivic, Josef, and Zisserman, Andrew. Object retrieval  with large vocabularies and fast spatial matching. In CVPR, Jun. 2007.  Philbin, James, Chum, Ondrej, Isard, Michael, Sivic, Josef, and Zisserman, Andrew. Lost in quanti- zation: Improving particular object retrieval in large scale image databases. In CVPR, Jun. 2008.  Radenovi´c, Filip, Jegou, Herve, and Chum, Ondrej. Multiple measurements and joint dimensionality  reduction for large scale image search with short vectors. In ICMR, 2015.  Razavian, Ali Sharif, Azizpour, Hossein, Sullivan, Josephine, and Carlsson, Stefan. CNN features  off-the-shelf: An astounding baseline for recognition. In CVPRW, 2014a.  Razavian, Ali Sharif, Sullivan, Josephine, Maki, Atsuto, and Carlsson, Stefan. A baseline for visual  instance retrieval with deep convolutional networks. In arXiv, 2014b.  Ren, Shaoqing, He, Kaiming, Girshick, Ross, and Sun, Jian. Faster r-cnn: Towards real-time object  detection with region proposal networks. In arXiv, 2015.  Shen, Xiaohui, Lin, Zhe, Brandt, Jonathan, and Wu, Ying. Spatially-constrained similarity measure  for large-scale object retrieval. Trans. PAMI, 36(6):1229–1241, 2014.  Sicre, Ronan and Jurie, Frdric. Discriminative part model for visual recognition. CVIU, 141:28 –  37, 2015. ISSN 1077-3142.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. In arXiv, 2014.  Sivic, Josef and Zisserman, Andrew. Video Google: A text retrieval approach to object matching in  videos. In ICCV, 2003.  Tao, Ran, Gavves, Efstratios, Snoek, Cees GM, and Smeulders, Arnold WM. Locality in generic  instance search from one example. In CVPR, 2014.  Tolias, Giorgos, Avrithis, Yannis, and J´egou, Herv´e. Image search with selective match kernels:  aggregation across single and multiple images. IJCV, 2015.  Uijlings, Jasper, Van de Sande, Koen, Gevers, Theo, and Smeulders, Arnold. Selective search for  object recognition. IJCV, 104(2):154–171, 2013.  Van de Sande, Koen EA, Snoek, Cees GM, and Smeulders, Arnold WM. Fisher and VLAD with  ﬂair. In CVPR, 2014.  Vedaldi, Andrea and Lenc, Karel. Matconvnet-convolutional neural networks for matlab. In arXiv,  2014.  Viola, Paul and Jones, Michael. Robust real-time object detection. IJCV, 4:34–47, 2001.  Xie, Lingxi, Tian, Q, Hong, R, and Zhang, B. Image classiﬁcation and retrieval are one. In ICMR,  2015.  Zhong, Zhiyuan, Zhu, Jianke, and Hoi, Steven CH. Fast object retrieval using direct spatial match-  ing. IEEE Trans. on Multimedia, 17(8):1391–1397, 2015.  12  ",
1511.06422,2016,All you need is a good init,"['All you need is a good init [code]\nDmytro Mishkin', 'Jiri Matas']",https://arxiv.org/pdf/1511.06422,"6 1 0 2     b e F 9 1         ]  G L . s c [      7 v 2 2 4 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  ALL YOU NEED IS A GOOD INIT  Dmytro Mishkin, Jiri Matas  Center for Machine Perception Czech Technical University in Prague Czech Republic {mishkdmy,matas}@cmp.felk.cvut.cz  ABSTRACT  Layer-sequential unit-variance (LSUV) initialization – a simple method for weight initialization for deep net learning – is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the ﬁrst to the ﬁnal layer, normaliz- ing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed speciﬁcally for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.  1  INTRODUCTION  Deep nets have demonstrated impressive results on a number of computer vision and natural lan- guage processing problems. At present, state-of-the-art results in image classiﬁcation (Simonyan & Zisserman (2015); Szegedy et al. (2015)) and speech recognition (Sercu et al. (2015)), etc., have been achieved with very deep (≥ 16 layer) CNNs. Thin deep nets are of particular interest, since they are accurate and at the same inference-time efﬁcient (Romero et al. (2015)). One of the main obstacles preventing the wide adoption of very deep nets is the absence of a general, repeatable and efﬁcient procedure for their end-to-end training. For example, VGGNet (Simonyan & Zisserman (2015)) was optimized by a four stage procedure that started by training a network with moderate depth, adding progressively more layers. Romero et al. (2015) stated that deep and thin networks are very hard to train by backpropagation if deeper than ﬁve layers, especially with uniform initialization. On the other hand, He et al. (2015) showed that it is possible to train the VGGNet in a single optimization run if the network weights are initialized with a speciﬁc ReLU-aware initialization. The He et al. (2015) procedure generalizes to the ReLU non-linearity the idea of ﬁlter-size dependent initialization, introduced for the linear case by (Glorot & Bengio (2010)). Batch normalization (Ioffe & Szegedy (2015)), a technique that inserts layers into the the deep net that transform the output for the batch to be zero mean unit variance, has successfully facilitated training of the twenty-two layer GoogLeNet (Szegedy et al. (2015)). However, batch normalization adds a 30% computational overhead to each iteration. The main contribution of the paper is a proposal of a simple initialization procedure that, in con- nection with standard stochastic gradient descent (SGD), leads to state-of-the-art thin and very deep neural nets1. The result highlights the importance of initialization in very deep nets. We review the history of CNN initialization in Section 2, which is followed by a detailed description of the novel initialization method in Section 3. The method is experimentally validated in Section 4.  1The code allowing to reproduce the experiments is available at  https://github.com/ducha-aiki/LSUVinit  1  Published as a conference paper at ICLR 2016  Figure 1: Relative magnitude of weight updates as a function of the training iteration for different weight initialization scaling after ortho-normalization. The values in the range 0.1% .. 1% lead to convergence, larger to divergence, for smaller, the network can hardly leave the initial state. Sub- graphs show results for different non-linearities – ReLU (top left), VLReLU (top right), hyperbolic tangent (bottom left) and Maxout (bottom right).  2  INITIALIZATION IN NEURAL NETWORKS  After the success of CNNs in IVSRC 2012 (Krizhevsky et al. (2012)), initialization with Gaussian noise with mean equal to zero and standard deviation set to 0.01 and adding bias equal to one for some layers become very popular. But, as mentioned before, it is not possible to train very deep network from scratch with it (Simonyan & Zisserman (2015)). The problem is caused by the activation (and/or) gradient magnitude in ﬁnal layers (He et al. (2015)). If each layer, not properly initialized, scales input by k, the ﬁnal scale would be kL, where L is a number of layers. Values of k > 1 lead to extremely large values of output layers, k < 1 leads to a diminishing signal and gradient. Glorot & Bengio (2010) proposed a formula for estimating the standard deviation on the basis of the number of input and output channels of the layers under assumption of no non-linearity between layers. Despite invalidity of the assumption, Glorot initialization works well in many applications. He et al. (2015) extended this formula to the ReLU (Glorot et al. (2011)) non-linearity and showed its superior performance for ReLU-based nets. Figure 1 shows why scaling is important. Large weights lead to divergence via updates larger than the initial values, small initial weights do not allow the network to learn since the updates are of the order of 0.0001% per iteration. The optimal 2 by He et al. scaling for ReLU-net is around 1.4, which is in line with the theoretically derived  √  2  0123456789# Iteration10-810-710-610-510-410-310-210-1100101102Relative magnitude of expected weight updatesReLU0.40.60.81.01.21.41.61.80123456789# Iteration10-810-710-610-510-410-310-210-1100101102Relative magnitude of expected weight updatesVLReLU0.40.60.81.01.21.41.61.80123456789# Iteration10-810-710-610-510-410-310-210-1100101102Relative magnitude of expected weight updatesTanH0.40.60.81.01.21.41.61.80123456789# Iteration10-810-710-610-510-410-310-210-1100101102Relative magnitude of expected weight updatesMaxout0.40.60.81.01.21.41.61.8Published as a conference paper at ICLR 2016  (2015). Sussillo & Abbott (2014) proposed the so called Random walk initialization, RWI, which keeps constant the log of the norms of the backpropagated errors. In our experiments, we have not been able to obtain good results with our implementation of RWI, that is why this method is not evaluated in experimental section. Hinton et al. (2014) and Romero et al. (2015) take another approach to initialization and formulate training as mimicking teacher network predictions (so called knowledge distillation) and internal representations (so called Hints initialization) rather than minimizing the softmax loss. Srivastava et al. (2015) proposed a LSTM-inspired gating scheme to control information and gradi- ent ﬂow through the network. They trained a 1000-layers MLP network on MNIST. Basically, this kind of networks implicitly learns the depth needed for the given task. Independently, Saxe et al. (2014) showed that orthonormal matrix initialization works much better for linear networks than Gaussian noise, which is only approximate orthogonal. It also work for networks with non-linearities. The approach of layer-wise pre-training (Bengio et al. (2007)) which is still useful for multi-layer- perceptron, is not popular for training discriminative convolution networks.  3 LAYER-SEQUENTIAL UNIT-VARIANCE INITIALIZATION  To the best of our knowledge, there have been no attempts to generalize Glorot & Bengio (2010) formulas to non-linearities other than ReLU, such as tanh, maxout, etc. Also, the formula does not cover max-pooling, local normalization layers Krizhevsky et al. (2012) and other types of layers which inﬂuences activations variance. Instead of theoretical derivation for all possible layer types, or doing extensive parameters search as in Figure 1, we propose a data-driven weights initialization. We thus extend the orthonormal initialization Saxe et al. (2014) to an iterative procedure, described in Algorithm 1. Saxe et al. (2014) could be implemented in two steps. First, ﬁll the weights with Gaussian noise with unit variance. Second, decompose them to orthonormal basis with QR or SVD- decomposition and replace weights with one of the components. The LSUV process then estimates output variance of each convolution and inner product layer and scales the weight to make variance equal to one. The inﬂuence of selected mini-batch size on estimated variance is negligible in wide margins, see Appendix. The proposed scheme can be viewed as an orthonormal initialization combined with batch normal- ization performed only on the ﬁrst mini-batch. The similarity to batch normalization is the unit variance normalization procedure, while initial ortho-normalization of weights matrices efﬁciently de-correlates layer activations, which is not done in Ioffe & Szegedy (2015). Experiments show that such normalization is sufﬁcient and computationally highly efﬁcient in comparison with full batch normalization. The LSUV algorithm is summarized in Algorithm 1. The single parameter Tolvar inﬂuences conver- gence of the initialization procedure, not the properties of the trained network. Its value does not noticeably inﬂuence the performance in a broad range of 0.01 to 0.1. Because of data variations, it is often not possible to normalize variance with the desired precision. To eliminate the possibility of  Algorithm 1 Layer-sequential unit-variance orthogonal initialization. L – convolution or full- connected layer, WL - its weights, BL - its output blob., Tolvar - variance tolerance, Ti – current trial, Tmax – max number of trials.  Pre-initialize network with orthonormal matrices as in Saxe et al. (2014) for each layer L do  while |Var(BL) − 1.0| ≥ Tolvar and (Ti < Tmax) do  do Forward pass with a mini-batch calculate Var(BL)  WL = WL /(cid:112)Var(BL)  end while  end for  3  Published as a conference paper at ICLR 2016  an inﬁnite loop, we restricted number of trials to Tmax. However, in experiments described in paper, the Tmax was never reached. The desired variance was achieved in 1-5 iterations. We tested a variant LSUV initialization which was normalizing input activations of the each layer in- stead of output ones. Normalizing the input or output is identical for standard feed-forward nets, but normalizing input is much more complicated for networks with maxout (Goodfellow et al. (2013)) or for networks like GoogLeNet (Szegedy et al. (2015)) which use the output of multiple layers as input. Input normalization brought no improvement of results when tested against the LSUV Algorithm 1, LSUV was also tested with pre-initialization of weights with Gaussian noise instead of orthonormal matrices. The Gaussian initialization led to small, but consistent, decrease in performance.  4 EXPERIMENTAL VALIDATION  Here we show that very deep and thin nets could be trained in a single stage. Network architectures are exactly as proposed by Romero et al. (2015). The architectures are presented in Table 1.  Table 1: FitNets Romero et al. (2015) network architecture used in experiments. Non-linearity: Maxout with 2 linear pieces in convolution layers, Maxout with 5 linear pieces in fully-connected.  FitNet-1  250K param conv 3x3x16 conv 3x3x16 conv 3x3x16  pool 2x2  conv 3x3x32 conv 3x3x32 conv 3x3x32  pool 2x2  conv 3x3x48 conv 3x3x48 conv 3x3x64  FitNet-4  2.5M param conv 3x3x32 conv 3x3x32 conv 3x3x32 conv 3x3x48 conv 3x3x48  pool 2x2  conv 3x3x80 conv 3x3x80 conv 3x3x80 conv 3x3x80 conv 3x3x80  pool 2x2  conv 3x3x128 conv 3x3x128 conv 3x3x128 conv 3x3x128 conv 3x3x128  FitNet-MNIST  30K param conv 3x3x16 conv 3x3x16  pool 4x4, stride2  conv 3x3x16 conv 3x3x16  FitResNet-4 2.5M param conv 3x3x32  conv 3x3x32 →sum conv 3x3x48 →ssum  conv 3x3x48  conv 3x3x48  pool 2x2  conv 3x3x80  conv 3x3x80 →sum conv 3x3x80→sum  conv 3x3x80  conv 3x3x80  pool 2x2  conv 3x3x128  pool 4x4, stride2 conv 3x3x12 conv 3x3x128 →sum conv 3x3x12 conv 3x3x128 →sum  conv 3x3x128  pool 8x8 (global) pool 8x8 (global)  fc-500  fc-500  softmax-10(100)  softmax-10(100)  softmax-10(100)  conv 3x3x128  pool 8x8 (global)  fc-500  pool 2x2  softmax-10  4.1 MNIST  First, as a ”sanity check”, we performed an experiment on the MNIST dataset (Lecun et al. (1998)). It consists of 60,000 28x28 grayscale images of handwritten digits 0 to 9. We selected the FitNet- MNIST architecture (see Table 1) of Romero et al. (2015) and trained it with the proposed initial- ization strategy, without data augmentation. Recognition results are shown in Table 2, right block. LSUV outperforms orthonormal initialization and both LSUV and orthonormal outperform Hints initialization Romero et al. (2015). The error rates of the Deeply-Supervised Nets (DSN, Lee et al. (2015)) and maxout networks Goodfellow et al. (2013), the current state-of-art, are provided for reference. Since the widely cited DSN error rate of 0.39%, the state-of-the-art (until recently) was obtained after replacing the softmax classiﬁer with SVM, we do the same and also observe improved results (line FitNet-LSUV-SVM in Table 2).  4  Published as a conference paper at ICLR 2016  4.2 CIFAR-10/100  We validated the proposed initialization LSUV strategy on the CIFAR-10/100 (Krizhevsky (2009)) dataset. It contains 60,000 32x32 RGB images, which are divided into 10 and 100 classes, respec- tively. The FitNets are trained with the stochastic gradient descent with momentum set to 0.9, the initial learning rate set to 0.01 and reduced by a factor of 10 after the 100th, 150th and 200th epoch, ﬁnishing at 230th epoch. Srivastava et al. (2015) and Romero et al. (2015) trained their networks for 500 epochs. Of course, training time is a trade-off dependent on the desired accuracy; one could train a slightly less accurate network much faster. Like in the MNIST experiment, LSUV and orthonormal initialized nets outperformed Hints-trained Fitnets, leading to the new state-of-art when using commonly used augmentation – mirroring and random shifts. The gain on the ﬁne-grained CIFAR-100 is much larger than on CIFAR-10. Also, note that FitNets with LSUV initialization outperform even much larger networks like Large-All- CNN Springenberg et al. (2014) and Fractional Max-pooling Graham (2014a) trained with afﬁne and color dataset augmentation on CIFAR-100. The results of LSUV are virtually identical to the orthonormal initialization.  Table 2: Network performance comparison on the MNIST and CIFAR-10/100 datasets. Results marked ’†’ were obtained with the RMSProp optimizer Tieleman & Hinton (2012).  Accuracy on CIFAR-10/100, with data augmentation CIFAR-10, [%] CIFAR-100,[%] 70.04 (72.34†) 93.94 70.44 (72.30†) 93.78 64.96 91.61 92.46 68.09 66.29 92.75 65.43 92.03 64.32 91.19 65.46 90.62 93.25 71.14  Network Fitnet4-LSUV Fitnet4-OrthoInit Fitnet4-Hints Fitnet4-Highway ALL-CNN DSN NiN maxout MIN  Extreme data augmentation  95.59 Large ALL-CNN Fractional MP (1 test) 95.50 Fractional MP (12 tests) 96.53  n/a 68.55 73.61  Error on MNIST w/o data augmentation  Network  layers params Error, %  10 HighWay-16 6 FitNet-Hints 6 FitNet-Ortho FitNet-LSUV 6 FitNet-Ortho-SVM 6 FitNet-LSUV-SVM 6  FitNet-like networks 39K 30K 30K 30K 30K 30K State-of-art-networks  0.57 0.51 0.48 0.48 0.43 0.38  DSN-Softmax DSN-SVM HighWay-32 maxout MIN 2  3 3 10 3 9  350K 0.51 350K 0.39 151K 0.45 420K 0.45 447K 0.24  5 ANALYSIS OF EMPIRICAL RESULTS  5.1  INITIALIZATION STRATEGIES AND NON-LINEARITIES  For the FitNet-1 architecture, we have not experienced any difﬁculties training the network with any of the activation functions (ReLU, maxout, tanh), optimizers (SGD, RMSProp) or initialization (Xavier, MSRA, Ortho, LSUV), unlike the uniform initialization used in Romero et al. (2015). The most probable cause is that CNNs tolerate a wide variety of mediocre initialization, only the learning time increases. The differences in the ﬁnal accuracy between the different initialization methods for the FitNet-1 architecture is rather small and are therefore not presented here. The FitNet-4 architecture is much more difﬁcult to optimize and thus we focus on it in the experi- ments presented in this section. We have explored the initializations with different activation functions in very deep networks. More speciﬁcally, ReLU, hyperbolic tangent, sigmoid, maxout and the VLReLU – very leaky ReLU (Gra- ham (2014c)) – a variant of leaky ReLU ( Maas et al. (2013), with a large value of the negative slope  2 When preparing this submission we have found recent unreviewed paper MIN Chang & Chen (2015) paper, which uses a sophisticated combination of batch normalization, maxout and network-in-network non- linearities and establishes a new state-of-art on MNIST.  5  Published as a conference paper at ICLR 2016  3:  The  compatibility  Table initialization. Dataset: CIFAR-10. Architecture: FitNet4, 2.5M params for maxout net, 1.2M for the rest, 17 layers. The n/c symbol stands for “failed to converge”; n/c† – after extensive trials, we managed to train a maxout-net with MSRA initialization with very small learning rate and gradient clipping, see Figure 2. The experiment is marked n/c as training time was excessive and parameters non-standard.  activation  functions  and  of  Init method LSUV OrthoNorm OrthoNorm-MSRA scaled Xavier MSRA  maxout ReLU VLReLU tanh 93.94 89.28 93.78 89.48 – – 89.82 91.75 n/c† 89.54  92.11 91.74 91.93 90.63 90.91  92.97 92.40 93.09 92.27 92.43  Sigmoid n/c n/c n/c n/c n/c  0.333, instead of the originally proposed 0.01) which is popular in Kaggle competitions Dieleman (2015), Graham (2014b)). Testing was performed on CIFAR-10 and results are in Table 3 and Figure 2. Performance of orthonormal-based methods is superior to the scaled Gaussian-noise approaches for all tested types of activation functions, except tanh. Proposed LSUV strategy outperforms orthonormal initialization by smaller margin, but still consistently (see Table 3). All the methods failed to train sigmoid-based very deep network. Figure 2 shows that LSUV method not only leads to better generalization error, but also converges faster for all tested activation functions, except tanh. We have also tested how the different initializations work ”out-of-the-box” with the Residual net training He et al. (2015); a residual net won the ILSVRC-2015 challenge. The original paper pro- posed different implementations of residual learning. We adopted the simplest one, showed in Ta- ble 1, FitResNet-4. The output of each even convolutional layer is summed with the output of the previous non-linearity layer and then fed into the next non-linearity. Results are shown in Ta- ble 4. LSUV is the only initialization algorithm which leads nets to convergence with all tested non-linearities without any additional tuning, except, again, sigmoid. It is worth nothing that the residual training improves results for ReLU and maxout, but does not help tanh-based network.  Table 4: The performance of activation functions and initialization in the Residual learning setup He et al. (2015), FitResNet-4 from Table 1.The n/c symbol stands for “failed to converge”;  94.16  Init method maxout ReLU VLReLU tanh 89.17 LSUV 89.31 OrthoNorm n/c 89.62 n/c Xavier MSRA n/c 88.59  93.36 n/c 93.34 n/c  92.82 91.42 92.48 n/c  Sigmoid n/c n/c n/c n/c  5.2 COMPARISON TO BATCH NORMALIZATION (BN)  LSUV procedure could be viewed as batch normalization of layer output done only before the start of training. Therefore, it is natural to compare LSUV against a batch-normalized network, initialized with the standard method.  5.2.1 WHERE TO PUT BN – BEFORE OR AFTER NON-LINEARITY?  It is not clear from the paper Ioffe & Szegedy (2015) where to put the batch-normalization layer – before input of each layer as stated in Section 3.1, or before non-linearity, as stated in section 3.2, so we have conducted an experiment with FitNet4 on CIFAR-10 to clarify this. Results are shown in Table 5. Exact numbers vary from run to run, but in the most cases, batch normalization put after non-linearity performs better. In the next experiment we compare BN-FitNet4, initialized with Xavier and LSUV-initialized Fit- Net4. Batch-normalization reduces training time in terms of needed number of iterations, but each it-  6  Published as a conference paper at ICLR 2016  Figure 2: CIFAR-10 accuracy of FitNet-4 with different activation functions. Note that graphs are cropped at 0.4 accuracy. Highway19 is the network from Srivastava et al. (2015).  Table 5: CIFAR-10 accuracy of batch-normalized FitNet4. Comparison of batch normalization put before and after non-linearity.  Non-linearity Where to put BN After 89.22 92.58 92.98  Before 88.10 92.60 92.30  TanH ReLU Maxout  eration becomes slower because of extra computations. The accuracy versus wall-clock-time graphs are shown in Figure 3. LSUV-initialized network is as good as batch-normalized one. However, we are not claiming that batch normalization can always be replaced by proper initializa- tion, especially in large datasets like ImageNet.  5.3  IMAGENET TRAINING  We trained CaffeNet (Jia et al. (2014)) and GoogLeNet (Szegedy et al. (2015)) on the ImageNet- 1000 dataset( Russakovsky et al. (2015)) with the original initialization and LSUV. CaffeNet is a variant of AlexNet with the nearly identical performance, where the order of pooling and normal- ization layers is switched to reduce the memory footprint. LSUV initialization reduces the starting ﬂat-loss time from 0.5 epochs to 0.05 for CaffeNet, and starts to converge faster, but it is overtaken by a standard CaffeNet at the 30-th epoch (see Figure 4) and its ﬁnal precision is 1.3% lower. We have no explanation for this empirical phenomenon. On the contrary, the LSUV-initialized GoogLeNet learns faster than hen then original one and shows better test accuracy all the time – see Figure 5. The ﬁnal accuracy is 0.680 vs. 0.672 respectively.  5.4 TIMINGS  A signiﬁcant part of LSUV initialization is SVD-decomposition of the weight matrices, e.g. for the fc6 layer of CaffeNet, an SVD of a 9216x4096 matrix is required. The computational overhead on top of generating almost instantly the scaled random Gaussian samples is shown in Table 6. In the slowest case – CaffeNet – LSUV initialization takes 3.5 minutes, which is negligible in comparison the training time.  7  20406080100120140Epoch0.40.50.60.70.80.9Test accuracyMaxout-HWLSUVMSRAOrthoNormXavierhighway-1920406080100120140Epoch0.40.50.60.70.80.9Test accuracyReLULSUVMSRAOrthoNormXavierOrthoNormMSRA20406080100120140Epoch0.40.50.60.70.80.9Test accuracyVLReLULSUVMSRAOrthoNormXavierOrthoNormMSRA20406080100120140Epoch0.40.50.60.70.80.9Test accuracyTanHLSUVMSRAOrthoNormXavierPublished as a conference paper at ICLR 2016  Figure 3: CIFAR-10 accuracy of FitNet-4 LSUV and batch normalized (BN) networks as function of wall-clock time. BN-half stands for half the number of iterations in each step.  Figure 4: CaffeNet training on ILSVRC-2012 dataset with LSUV and original Krizhevsky et al. (2012) initialization. Training loss (left) and validation accuracy (right). Top – ﬁrst epoch, middle – ﬁrst 10 epochs, bottom – full training.  8  0200040006000800010000Seconds0.40.50.60.70.80.9Test accuracyMaxout-BNMaxout-LSUVMaxout-BNMaxout-BN-half0100020003000400050006000Seconds0.40.50.60.70.80.9Test accuracyReLU-BNReLU-LSUVReLU-BNReLU-BN-half0100020003000400050006000Seconds0.40.50.60.70.80.9Test accuracyVLReLU-BNVLReLU-LSUVVLReLU-BNVLReLU-BN-half0100020003000400050006000Seconds0.40.50.60.70.80.9Test accuracyTanH-BNTanH-LSUVTanH-BNTanH-BN-half0.00.20.40.60.81.0Epoch4.04.55.05.56.06.57.07.5Training lossLSUVoriginal0.00.20.40.60.81.0Epoch0.000.020.040.060.080.100.120.14Test accuracyLSUVoriginal0246810Epoch234567Training lossLSUVoriginal0246810Epoch0.000.050.100.150.200.250.300.350.40Test accuracyLSUVoriginal10203040506070Epoch1.01.52.02.53.03.54.0Training lossLSUVoriginal10203040506070Epoch0.300.350.400.450.500.550.60Test accuracyLSUVoriginalPublished as a conference paper at ICLR 2016  Figure 5: GoogLeNet training on ILSVRC-2012 dataset with LSUV and reference Jia et al. (2014) BVLC initializations. Training loss (left) and validation accuracy (right). Top – ﬁrst epoch, middle – ﬁrst ten epochs, bottom – full training  Table 6: Time needed for network initialization  on top of random Gaussian (seconds).  Network  FitNet4 CaffeNet GoogLeNet  Init  OrthoNorm LSUV 4 210 60  1 188 24  6 CONCLUSIONS  LSUV, layer sequential uniform variance, a simple strategy for weight initialization for deep net learning, is proposed. We have showed that the LSUV initialization, described fully in six lines of pseudocode, is as good as complex learning schemes which need, for instance, auxiliary nets. The LSUV initialization allows learning of very deep nets via standard SGD, is fast, and leads to (near) state-of-the-art results on MNIST, CIFAR, ImageNet datasets, outperforming the sophisti- cated systems designed speciﬁcally for very deep nets such as FitNets( Romero et al. (2015)) and Highway( Srivastava et al. (2015)). The proposed initialization works well with different activation functions. Our experiments conﬁrm the ﬁnding of Romero et al. (2015) that very thin, thus fast and low in parameters, but deep networks obtain comparable or even better performance than wider, but shallower ones.  ACKNOWLEDGMENTS  The authors were supported by The Czech Science Foundation Project GACR P103/12/G084 and CTU student grant SGS15/155/OHK3/2T/13.  9  0.00.20.40.60.81.0Epoch67891011Training lossLSUVReference_BVLC0.00.20.40.60.81.0Epoch0.000.020.040.060.080.100.120.14Test accuracyLSUVReference_BVLC0246810Epoch4567891011Training lossLSUVReference_BVLC0246810Epoch0.000.050.100.150.200.250.300.350.400.45Test accuracyLSUVReference_BVLC10203040506070Epoch234567Training lossLSUVReference_BVLC10203040506070Epoch0.300.350.400.450.500.550.600.650.70Test accuracyLSUVReference_BVLCPublished as a conference paper at ICLR 2016  REFERENCES Bengio, Yoshua, Lamblin, Pascal, Popovici, Dan, and Larochelle, Hugo. Greedy layer-wise training of deep networks. In Sch¨olkopf, B., Platt, J.C., and Hoffman, T. (eds.), Advances in Neural Infor- mation Processing Systems 19, pp. 153–160. MIT Press, 2007. URL http://papers.nips. cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf.  Chang, J.-R. and Chen, Y.-S. Batch-normalized Maxout Network in Network. ArXiv e-prints,  November 2015. URL http://arxiv.org/abs/1511.02583.  Dieleman, Sander. Classifying plankton with deep neural networks, 2015. URL http://  benanne.github.io/2015/03/17/plankton.html.  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neu- In In Proceedings of the International Conference on Artiﬁcial Intelligence and  ral networks. Statistics (AISTATS10). Society for Artiﬁcial Intelligence and Statistics, 2010.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectiﬁer neural networks. In Gordon, Geoffrey J. and Dunson, David B. (eds.), Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS-11), volume 15, pp. 315–323. Jour- nal of Machine Learning Research - Workshop and Conference Proceedings, 2011. URL http: //www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf.  Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron C., and Bengio, Yoshua. Maxout networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1319–1327, 2013. URL http://jmlr. org/proceedings/papers/v28/goodfellow13.html.  Graham, Ben. Fractional Max-Pooling. ArXiv e-prints, December 2014a. URL http://arxiv.  org/abs/1412.6071.  Ben.  Graham, URL train-you-very-own-deep-convolutional-network.  Train 2014b. https://www.kaggle.com/c/cifar-10/forums/t/10493/  convolutional  network,  deep  very  own  you  Graham, Ben. Spatially-sparse convolutional neural networks. ArXiv e-prints, September 2014c.  He, K., Zhang, X., Ren, S., and Sun, J. Deep Residual Learning for Image Recognition. ArXiv  e-prints, December 2015. URL http://arxiv.org/abs/1512/03385.  He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers: Sur- In International Conference on  passing human-level performance on imagenet classiﬁcation. Computer Vision (ICCV), 2015. URL http://arxiv.org/abs/1502.01852.  Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distilling the Knowledge in a Neural Network. In  Proceedings of Deep Learning and Representation Learning Workshop: NIPS 2014, 2014.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Blei, David and Bach, Francis (eds.), Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 448–456. JMLR Workshop and Conference Proceedings, 2015. URL http://jmlr.org/proceedings/papers/v37/ ioffe15.pdf.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Krizhevsky, Alex. Learning Multiple Layers of Features from Tiny Images. Master’s thesis, 2009. URL http://www.cs.toronto.edu/˜{}kriz/learning-features-2009-TR. pdf.  10  Published as a conference paper at ICLR 2016  Krizhevsky, Alex, Sutskever,  Ilya, and Hinton, Geoffrey E.  Imagenet classiﬁcation with In Pereira, F., Burges, C.J.C., Bottou, L., and deep convolutional neural networks. Information Processing Systems 25, pp. Weinberger, K.Q. 1097–1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/ 4824-imagenet-classification-with-deep-convolutional-neural-networks. pdf.  (eds.), Advances in Neural  Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document ISSN 0018-9219. doi:  recognition. Proceedings of the IEEE, 86(11):2278–2324, Nov 1998. 10.1109/5.726791.  Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick W., Zhang, Zhengyou, and Tu, Zhuowen. Deeply- supervised nets. In Proceedings of the Eighteenth International Conference on Artiﬁcial Intelli- gence and Statistics, AISTATS 2015, San Diego, California, USA, May 9-12, 2015, 2015. URL http://jmlr.org/proceedings/papers/v38/lee15a.html.  Maas, Andrew L, Hannun, Awni Y, and Ng, Andrew Y. Rectiﬁer nonlinearities improve neural  network acoustic models. Proc. ICML, 30, 2013.  Romero, Adriana, Ballas, Nicolas, Kahou, Samira Ebrahimi, Chassang, Antoine, Gatta, Carlo, and Bengio, Yoshua. Fitnets: Hints for thin deep nets. In Proceedings of ICLR, May 2015. URL http://arxiv.org/abs/1412.6550.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, International Journal of Computer Li. Vision (IJCV), pp. 1–42, April 2015. doi: 10.1007/s11263-015-0816-y.  ImageNet Large Scale Visual Recognition Challenge.  Saxe, Andrew M., McClelland, James L., and Ganguli, Surya. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In Proceedings of ICLR, 2014. URL http: //arxiv.org/abs/1312.6120.  Sercu, T., Puhrsch, C., Kingsbury, B., and LeCun, Y. Very Deep Multilingual Convolutional Neural Networks for LVCSR. ArXiv e-prints, September 2015. URL http://arxiv.org/abs/ 1509/08967.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale visual recognition. In Proceedings of ICLR, May 2015. URL http://arxiv.org/abs/1409. 1556.  Springenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. Striving for Simplicity: The All Convolutional Net. In Proceedings of ICLR Workshop, December 2014. URL http://arxiv. org/abs/1412.6806.  Srivastava, Rupesh Kumar, Greff, Klaus, and Schmidhuber, Jrgen. Training Very Deep Networks.  In Proceedings of NIPS, 2015. URL http://arxiv.org/abs/1507.06228.  Sussillo, David and Abbott, L. F. Random Walk Initialization for Training Very Deep Feedforward Networks. ArXiv e-prints, December 2014. URL http://arxiv.org/abs/1412.6558.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. In CVPR 2015, 2015. URL http://arxiv.org/abs/1409.4842.  Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5: RMSProp – Divide the gradient by a running average of its recent magnitude. In COURSERA: Neural Networks for Machine Learning. 2012.  11  Published as a conference paper at ICLR 2016  A TECHNICAL DETAILS  A.1  INFLUENCE OF MINI-BATCH SIZE TO LSUV INITIALIZATION  We have selected tanh activation as one, where LSUV initialization shows the worst performance and tested the inﬂuence of mini-batch size to training process. Note, that training mini-batch is the same for all initializations, the only difference is mini-batch used for variance estimation. One can see from Table 7 that there is no difference between small or large mini-batch, except extreme cases, where only two sample are used.  Table 7: FitNet4 TanH ﬁnal performance on CIFAR-10. Dependence on LSUV mini-batch size  Batch size for LSUV 2 Final accuracy, [%]  89.27  16 89.30  32 89.30  128 89.28  1024 89.31  A.2 LSUV WEIGHT STANDARD DEVIATIONS IN DIFFERENT NETWORKS  Tables 8 and 9 show the standard deviations of the ﬁlter weights, found by the LSUV procedure and by other initialization schemes.  Table 8: Standard deviations of the weights per layer for different initializations, FitNet4, CIFAR10, ReLU  Layer conv11 conv12 conv13 conv14 conv15 conv21 conv22 conv23 conv24 conv25 conv26 conv31 conv32 conv33 conv34 conv35 conv36 ip1  LSUV OrthoNorm MSRA Xavier 0.191 0.383 0.091 0.059 0.059 0.083 0.059 0.076 0.048 0.068 0.037 0.036 0.048 0.037 0.037 0.061 0.037 0.052 0.037 0.067 0.055 0.037 0.037 0.034 0.029 0.044 0.029 0.042 0.029 0.041 0.040 0.029 0.029 0.043 0.048 0.088  0.175 0.058 0.058 0.058 0.048 0.048 0.037 0.037 0.037 0.037 0.037 0.037 0.029 0.029 0.029 0.029 0.029 0.044  0.265 0.082 0.083 0.083 0.060 0.052 0.052 0.052 0.052 0.052 0.052 0.052 0.041 0.041 0.041 0.041 0.041 0.124  A.3 GRADIENTS  To check how the activation variance normalization inﬂuences the variance of the gradient, we mea- sure the average variance of the gradient at all layers after 10 mini-batches. The variance is close to 10−9 for all convolutional layers. It is much more stable than for the reference methods, except MSRA; see Table 10.  12  Published as a conference paper at ICLR 2016  Table 9: Standard deviations of the weights per layer for different non-linearities, found by LSUV, FitNet4, CIFAR10  Layer conv11 conv12 conv13 conv14 conv15 conv21 conv22 conv23 conv24 conv25 conv26 conv31 conv32 conv33 conv34 conv35 conv36 ip1  TanH ReLU VLReLU Maxout 0.386 0.118 0.102 0.101 0.081 0.065 0.064 0.060 0.058 0.061 0.063 0.054 0.052 0.051 0.050 0.051 0.051 0.084  0.384 0.084 0.075 0.080 0.065 0.037 0.047 0.049 0.049 0.043 0.052 0.037 0.037 0.042 0.038 0.039 0.037 0.044  0.383 0.058 0.063 0.065 0.044 0.034 0.040 0.032 0.041 0.040 0.037 0.027 0.031 0.033 0.028 0.030 0.033 0.038  0.388 0.083 0.096 0.082 0.064 0.044 0.055 0.055 0.064 0.061 0.049 0.032 0.049 0.048 0.047 0.047 0.040 0.044  Table 10: Variance of the initial gradients per layer, different initializations, FitNet4, ReLU  Layer conv11 conv12 conv13 conv14 conv15 conv21 conv22 conv23 conv24 conv25 conv26 conv31 conv32 conv33 conv34 conv35 conv36 ip1 var(ip1)/var(conv11)  LSUV 4.87E-10 5.07E-10 4.36E-10 3.21E-10 3.85E-10 1.25E-09 1.15E-09 1.19E-09 9.12E-10 7.45E-10 8.21E-10 3.06E-09 2.57E-09 2.40E-09 2.19E-09 1.94E-09 2.31E-09 1.24E-07 255  MSRA 9.42E-09 9.62E-09 1.07E-08 7.03E-09 6.57E-09 9.11E-09 9.73E-09 1.07E-08 1.07E-08 1.09E-08 1.15E-08 1.92E-08 2.01E-08 1.99E-08 2.25E-08 2.57E-08 2.97E-08 1.95E-07 20  OrthoInit 5.67E-15 1.17E-14 2.30E-14 2.95E-14 6.71E-14 1.95E-13 3.79E-13 8.18E-13 1.79E-12 4.04E-12 8.36E-12 2.65E-11 5.95E-11 1.21E-10 2.64E-10 5.89E-10 1.32E-09 6.91E-08 12198922  Xavier 2.30E-14 4.85E-14 9.94E-14 1.35E-13 3.10E-13 8.00E-13 1.56E-12 3.28E-12 6.69E-12 1.36E-11 2.99E-11 1.05E-10 2.28E-10 4.69E-10 1.01E-09 2.27E-09 5.57E-09 7.31E-08 3176821  13  ",
1506.05011,2016,Bayesian Representation Learning with Oracle Constraints,"['Bayesian Representation Learning with Oracle Constraints\nTheofanis Karaletsos', 'Serge Belongie', 'Gunnar Rätsch']",https://arxiv.org/pdf/1506.05011,"6 1 0 2    r a  M 1         ] L M  . t a t s [      4 v 1 1 0 5 0  .  6 0 5 1 : v i X r a  Published as a conference paper at ICLR 2016  BAYESIAN REPRESENTATION LEARNING WITH ORACLE CONSTRAINTS  Theofanis Karaletsos Computational Biology, Sloan Kettering Institute 1275 York Avenue, New York, USA Theofanis.Karaletsos@ratschlab.org  Serge Belongie Cornell Tech 111 8th Avenue #302, New York, USA sjb344@cornell.edu  Gunnar R¨atsch Computational Biology, Sloan Kettering Institute 1275 York Avenue, New York, USA Gunnar.Ratsch@ratschlab.org  ABSTRACT  Representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. Recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human percep- tion excels at understanding semantics but cannot always be expressed in terms of labels. Thus, oracles or human-in-the-loop systems, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similar- ity function encoded in human perception. In this work we propose to combine generative unsupervised feature learning with a probabilistic treatment of oracle information like triplets in order to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models of the observations. We use a fast variational algorithm to learn the joint model and demonstrate applicabil- ity to a well-known image dataset. We show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. In addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables.  1  INTRODUCTION  Machine Learning excels in its ability to model large quantities of data with layered non-linear feature-learning systems for purposes such as classiﬁcation and understanding of images, scenes, videos, text and more structured objects. Commonly, many successes are owed due to excessive availability of labels coupled with supervised learning. In other successful cases, the structure of the data is being used as a means to hard-code wiring for models, for instance modeling video using slowness and convolutions in images. Oftentimes, especially in the case of perception, a) the real structure of the data generating process is unknown and hard to explicitly model well, or b) large amounts of accurate labels are hard to come by or may even be inadequate for knowledge representation. One way to incoporate further information is to query oracles like crowds to gather cheap labels or to collect auxiliary information like similarity constraints accoring to undeﬁned perceptual biases the crowd may be aware of. While labeling may be noisy or inadequate to represent knowledge, similarity constraints present a robust way to encode implicit information about various properties of stimuli. We propose to take advantage of auxiliary (implicit) information provided by one or more oracles as a means to learn ﬂexible graphical models with latent variables. Examples of oracles include (human) crowds or implicit structural knowledge about the data, such as structural or multi-modal constraints without access to explicit features, which are encoded as triplet constraints (see Sec- tion 2.1). Critically, we consider the oracle similarity constraints as implicit observations generated through an unknown process which we include in our model in order to capture subtle knowledge  1  Published as a conference paper at ICLR 2016  about similarity from the oracle(s). This key idea helps shape explicit, interpretable latent spaces that exceed the performance of purely unsupervised learning and can be applied in cases where labels are sparse or undesirable. These latent spaces can also be used to explicitly inspect the implicit knowl- edge passed on by the oracle to the model. Our goal is to infer a latent factor model which learns jointly from triplets and observed data and transfers implicit biases encoded in the triplets into an explicit latent space that captures the semantics of the triplet-generating process better than simple density estimation (see Figure 4). We provide a detailed review of related work in Section 3 where we explain the relationship of our model in the context of other triplet-loss based metric learning approaches and generative models . We ﬁrst describe the two key contributions needed to perform the described task. In Section 2.1 we introduce a novel probabilistic generative model of oracle observations. We extend this with a principled approach for multi-query oracles using masked subspaces in Section 2.2. The second key contribution is described in Section 2.4, where we propose a principled approach combining the probabilistic oracle model with a graphical model performing nonlinear feature learning in order to transfer the implicit triplet knowledge into an explicit parametric model. In Section 2.5 we introduce a fast variational inference algorithm to learn posterior latent spaces respecting the observations of data and constraints. Finally, in Sections 4 and 5, we present experimental results for benchmarking the proposed approaches, illustrating their properties and discussing the beneﬁts they confer over competing approaches.  2 METHODS Let x ∈ RN×D denote N observations with D dimensions. We deﬁne latent variable z ∈ RN×H corresponding to H-dimensional latent representations of datapoints x. 2.1 PROBABILISTIC MODELING OF ORACLE TRIPLETS We consider an unknown (dis)-similarity function sQ(Φ(xi), Φ(xj)) that computes the distance between two objects xi and xj with respect to a query Q based on semantic information associated with these two objects. We consider z = Φ(x) to be the internal conceptual representation the oracle uses to apply similarity function sQ(·,·) for an unobserved feature space Φ. In addition, we consider the case where we can not directly observe the similarity function, but were we only observe orderings over similarities of zi to zj and zl, i.e., either sQ(zi, zj) is greater (equal) or smaller than sQ(zi, zl). We deﬁne the set of all oracle triplets related to query Q as:  TQ = {(i, j, l) | sQ(zi, zj) > sQ(zi, zl)}.  (1) We do not have access to the exhaustive set TQ, but can sample K-times from it using the oracle to yield a ﬁnite sample TQK = {tk}K An illustrative example of this process is human perceptual judgement of similarities, which heavily relies on internal representations and abstracted concepts to evaluate similarities over purely using raw low level image statistics. A frequently used oracle is the crowd. Systems like Amazon Me- chanical Turk are used to obtain triplet samples to explore the human perceptual prior as an oracle. However, oracles also naturally arize from data structure, such as temporal or spatial orderings or other known semantic structure. Another type of oracle is access to privileged information, which are extra features only implicitly available through the triplets, such as sentiments associated with visual features. A shared property of all these oracles is that they provide weak natural constraints on similarity without explicitly quantifying it. We model the likelihood ti,j,l of a triplet being contained TQ as a draw from a Bernoulli distribution over the states True and False parametrized using a softmax-function. If we consider  k=1.  (cid:90)  p(ti,j,l) =  p(ti,j,l|zi, zj, zl)p(zi)p(zj)p(zk)dzidzjdzk,  this gives the following likelihood:  z  with  Da,b =  p(ti,j,l) = Ber(ti,j,l) =  H(cid:88)  a,b = − H(cid:88)  (cid:104)  Dh  (cid:16)  e−Di,j  e−Di,j + e−Di,l  JS  p(zh  a)||p(zh b )  (cid:17)(cid:105)  .  h=1  h=1  2  (2)  (3)  (4)  Published as a conference paper at ICLR 2016  (cid:17)  (cid:16)  (cid:17) b )||pq(zh)  (cid:16) (cid:17) b ) and KL(q||p) =(cid:82)  a)||p(zh b )  p(zh  (cid:16)  a)||pq(zh) p(z) )dz.  z  + 1  = 1  p(zh  p(zh  2KL  with pq(zh) = 1  2KL q(z)log( q(z)  and JS 1 2 p(zh We denote the likelihood in (3) as BER in the rest of the paper. Since the Jensen Shannon divergence above is commonly intractable, we discuss alternatives used in our experiments in Supplement B. A subtlety of the acquisition process for the triplets is that oracles are not asked to provide distances, but just binary statements over similarity rankings based on the prompted question. Thus, any triplet which fulﬁlls the statement made by the deﬁnition is valid. We model this relaxation into a truncated and refer to it as TBER later.  Bernoulli likelihood as p(ti,j,l) =  (cid:26)1  2 p(zh  a) +  if Ber(ti,j,l) ≥ 0.5 if Ber(ti,j,l) < 0.5  Ber(ti,j,l)  2.2 MASKED ORACLE MODELS  Frequently, oracle information can be conﬂicting, especially when using multiple oracles. For in- stance, Consider colored geometric shapes, where we have a red triangle, a red circle and a blue cir- cle. If we ask one oracle to compare shapes with a set of triplets and we furthermore ask another ora- cle to compare colors, we may get conﬂicting oracle constraints. The circles are more similar shapes while the red circle and triangle have more similar colors. The generated oracle con- straints cannot easily be jointly fulﬁlled with uniform global constraints. We extend the presented model of oracle observations by incorporating masks over the dimensions of the latent space, which weigh/select dimen- sions on which the oracle constraints must hold. Since latent variables typically are of higher dimension than 1, this approach aims at learning semantic latent subspaces where speciﬁc variables encode features which cor- respond to semantic information associated with an oracle. These subspaces can in gen- eral be entirely private to an oracle question, or may share information with multiple ques- tions. This leads to a compartmentalization of the semantic representation. In Figure 2 we give another conceptual illustration and example that we also use in Results. Formally, for a set of H-dimensional latent variables z we deﬁne a corresponding H-dimensional global mask-variable mQ which is shared between all samples and is speciﬁc to a question/oracle Q. Using these masks, we adapt (4) to yield the masked oracle model:  Figure 2: Conceptually, observing oracle information from multiple oracles/questions allows an otherwise fully un- supervised and unstructured model (left) to identify a se- mantically compartmentalized generative process by using masked subspaces (right). In Results we model images of illuminated faces and show that by using oracle obser- vations the latent space factorizes automatically into sub- spaces related to different semantic aspects.  We deﬁne learned masks by mh = σ(bh), where bh ∼ N (0, 1) and σ denotes the sigmoid function.  h=1  Dm  di,j =  mhDh  i,j.  (5)  2.3 VARIATIONAL BELIEF NETWORKS Apart from modeling oracle triplets deﬁned over latent representations z ∈ RH, we are interested in modeling observations x ∈ RD well. We learn a graphical model to maximize p(x) using H-dimensional latent variables z. The latent variables can be drawn from any exponential fam- ily distribution p(z), but simplifying cases for inference and learning exist for many continuous distributions. We can write the model as follows:  p(x; θ) =  pθ(x|z)p(z)dz,  (6)  with pθ(x|z) := f (x; z, θ) being an exponential family likelihood with parameters given as a func- tion of z. Also, f is a function parametrized by θ (for instance, a multi-layer perceptron). Here, we focus on the Gaussian distribution as a prior p(z) with dimensions H, but note that with small adap- tations to the inference procedure other distributions are feasible. In this case we predict D means  z  3  H(cid:88)  (cid:90)  ...............AzimuthElevationIdentityShared/NoiseJoint Shared Latent SpaceOracle ObservationsPublished as a conference paper at ICLR 2016  z for each dimension d given the state of latent variables z using µz,d = fθ,µ,d(z)  µz and variances σ2 and log σz,d = fθ,σ,d(z). A good estimator for learning the parameters of such a model by assuming an approximate condi- tional posterior qφ(z|x) was suggested in Kingma & Welling (2013); Rezende et al. (2014); Mnih & Gregor (2014), all of which can be understood as instances of doubly stochastic variational inference. The estimator forms a variational lower bound Wainwright & Jordan (2008); Jordan et al. (1999) to the marginal likelihood. Performing coordinate ascent with respect to variational parameters θ & φ corresponds to minimizing the divergence between the true and the approximate posterior: log pθ(x(i)) ≥ L(θ, φ; x(i)) = −KL(qφ(z)||pθ(z)) + Eqφ(z)[log pθ(x(i)|z)].  (7)  In the following, we will refer to this fully unsupervised model as VAE.  2.4  JOINT MODEL OF TRIPLETS AND OBSERVATIONS: ORACLE-PRIORITIZED BELIEF NETWORK  After having established an observation model for triplets t and an observation model for x, we can proceed to introduce the full generative process for a joint model over both observables. Instead of relying on a supervised model taking observations x as input, we prefer using a generative approach that models the joint density of both data and triplets to provide an unsupervised model which requires only input data and samples from an oracle to be trained. The advantage is that the generative latent variable encoding does not throw away in- formation about the observations, leading to models which explicitly need to capture la- tent factors generating the data. When ob- serving oracle-samples, learning leads to ex- plicit factors of the information from both the oracle and the observations. We use a belief network as introduced in Section 2.3 to model observations x and connect the latent variables with an oracle observation-term for triplets t introduced in Section 2.1. Triplets require multiple sam- ples from the prior to be drawn, as they are deﬁned over multiple objects jointly. Simi- lar to the inference model in the Siamese net- work (Chopra et al., 2005), this necessitates multiple instances of the model with shared parameters to work in coordination to gener- ate a triplet. We sketch the generative model in Figure 4. For our proposed joint-model that we will refer to as OPBN, we consider N datapoints and K triplets deﬁned over them:  Figure 4: Shown is the proposed joint model. It models observations x and triplets from an oracle t as observed (shaded) variables. The latent space with variables z causes the shaded variables and thus captures the information nec- essary for modeling both.  K(cid:89)  (cid:2)p(tk|zki, zkj , zkl )(cid:3) dz  (8)  p(x, t; θ) =  [p(zn)pθ(xn|zn)]  (cid:90)  N(cid:89)  n  z  k  The generative process according to this model is:  1. sample zi, zj, zl from prior: z ∼ N (0, 1); 2. for each z, sample observation x using nonlinear likelihood, e.g. p(x|z) = N (x; µz, σ2 z); 3. for each set {i, j, l}, sample triplet ti,j,l ∼ p(t|zi, zj, zl).  Triplets tie together multiple datapoints and capture their dependencies through the latent repre- sentations. This has the effect of attaching higher-order potentials to the latent space, which the model uses for regularization and guidance. It is noteworthy that learning consists of maximizing the marginal likelihood p(x, t) by integrating out the latent z’s. This directly maximizes the evi- dence coming from the oracle and the observations, while maintaining ﬂexibility for the model used in-between. This model balances a reconstruction cost for the datapoints, the generative cost for the triplets and the prior on the latent variables when generating samples.  4  Z1X1X2X3Z2Z3tp(t|z)θθθφφφPublished as a conference paper at ICLR 2016  2.5 LEARNING USING FAST VARIATIONAL INFERENCE Our goal is to maximize the marginal likelihood of the evidence, logpθ(x, t) in order to learn a good mapping capturing the dependencies between observations x and triplets t. This involves integrating out the latent variables which is in general analytically intractable in highly ﬂexible model classes. In order to perform efﬁcient learning and inference in the model given by (8) we resort to approximate inference methods and employ doubly stochastic Variational Inference (Kingma & Welling, 2013; Rezende et al., 2014; Titsias & L´azaro-Gredilla, 2014). Variational Inference (Jordan et al., 1999; Wainwright & Jordan, 2008) requires approximate distributions q(z) over the posterior of the latent variables. We use amortized inference by employing an inference network to learn a conditional variational distribution qφ(z|x) with an MLP parametrized by φ. The inference model predicts the variational approximation to the posterior latent variables per input data point. The evidence lower bound (ELBO) looks as follows: log pθ(x, t) =L(θ, φ; x, t) + KL(q(z)||p(z|x, t)) ≥ L(θ, φ; x, t)  (cid:104)Eq(z)[log pθ(xn|zn)] (cid:105)  (cid:104)Eq(z)[log p(tk|zkijl )] (cid:105)  + Ek  = − En  KL(q(zn)||pθ(z))  + En  (cid:104)  (cid:105)  L  L  l=1  l=1  (cid:105)  (cid:105)  (cid:105)  (cid:104)  h, σξ  + Ek  + En  L(cid:80)  L(cid:80)  (cid:104) 1  (cid:104) 1  [logp(tk|zkl  [logpθ(xn|zl)]  KL(qφ(zn|xn)||pθ(z))  where kijl acts as an index on matrix z selecting the corresponding datapoints. Theoretically, per- forming coordinate ascend on this lower bound is sufﬁcient to infer the parameters of the model θ and inference network φ. However, the expectations over latent variables q(z) present in the ELBO are intractable. We resort to the reparametrization trick (Kingma & Welling, 2013; Rezende et al., 2014; Titsias & L´azaro-Gredilla, 2014) and perform doubly stochastic variational inference by drawing L unbiased samples zl from these expectations using the identity zl = µφ + λφ · (cid:15)l, where {µφ, λφ} are predicted variational parameters using the inference network and (cid:15)l ∼ N (0, 1) are unbiased samples from a unit Gaussian. The differentiable new bound L(θ, φ; x, t) then takes the shape: −En . ijl)] On this new objective we can now perform gradient-based learning by following ∇L(θ, φ; x, t) with respect to global variational parameters φ, θ. We perform stochastic gradient descent by drawing minibatches with Nb datapoints and Kb triplets each time. When learning masks m as in Section 2.2, we infer posterior distribution of the masks given ob- served data p(m|D). We use variational inference analogously to learn an approximate distribution q(mh; ξ) = N (µξ h) by adding the KL loss for the masks to the ELBO while taking into account the state of the mask variable in the triplet likelihood. Upon close inspection we detect that the components qφ(z|x) and pθ(x|z) form a variational au- toencoder where the parameters have distilled the triplet information. This also clariﬁes where the transfer of implicit information from the triplets to the learned parametric model happens. In simple terms, the formulation of the model forces the inference network to learn encodings respecting the triplets and the model pθ(x|z) decodings which account for that shared information. 3 RELATIONSHIP TO OTHER WORK Similarity-based learning, for instance via crowdsourcing, has been tackled in various ways in the community before. Notably, crowd-kernels are inferred and used for various vision tasks using Van Der Maaten & Weinberger (2012) which assumes a ﬁxed Student-t structure to produce an embed- ding using similarity constraints from a crowd, but does not learn an adaptive latent representation of the input features. In Chechik et al. (2010), a metric respecting the particular distances in similarity is learned. This differs from the case we are studying, as it assumes that speciﬁc distances or simi- larities are observed, which is hard to ask of a weak oracle. In Tamuz et al. (2011), a probabilistic treatment for triplets is introduced and an adaptive crowd kernel is learned without speciﬁc visual features in mind. While we also adopt a probabilistic treatment of triplets, we will learn an adaptive feature representation comparing images from the crowd as well. Flexible nonlinear models have been employed in a variety of situations to learn representations for data. A key result in relation to this work is the Siamese network (Chopra et al., 2005), which uses discriminatively learned features and reﬁnes them using a loss attached to the encodings of multiply winged networks over the compared images. A similar version was later also developed which just uses the oracle triplets as supervision instead of reﬁning a supervised version of the features, which is a setting we also consider (Hadsell et al., 2006). Similar approaches have been used in Wang et al. (2014); Schroff et al. (2015), where usage of supervised features with crowd-inferred simi- larities boosts performance in face veriﬁcation and more generic ﬁne-grained visual categorization  5  Published as a conference paper at ICLR 2016  tasks. The key difference to our work is two-fold: we focus on a probabilistic encoder-decoder ap- proach, where features are learned from images without labels and image information is not thrown away. Feature learning is guided additionally by an oracle and we introduce a probabilistic gener- ative model which provides a joint model of all these components and their interactions, including semantic masking. This forces our model to learn explicit latent factors which capture the knowl- edge from the oracle rather than learning to be invariant to it and thus constitutes a harder and more comprehensive task. Bayesian generative models have been proposed before for crowd-sourcing tasks (Liu et al., 2012). Our model differs in that we introduce latent variables generating the observations on which we eval- uate triplet constraints, which are the observation from the oracle. Our setup better facilitates implicit knowledge transfer via posterior regularization. Generative models in representation learning have recently made rapid progress using variational inference (Kingma & Welling, 2013; Rezende et al., 2014; Mnih & Gregor, 2014). These techniques allow fast learning of directed graphical models and have been a major stepping stone in combining deep learning with graphical models. We brieﬂy review variational autoencoders in Section 2.3. Notably, in Kingma et al. (2014) these approaches are used to achieve state-of-the art results in semi-supervised learning with explicit labels. We iden- tify that as a related setting to ours: using an oracle we can obtain weak implicit supervision in the form of similarity constraints over a sparse subset of the data and generalize from that, while most of the data is not subject to oracle constraints. Furthermore, we similarly can learn functionally compact subspaces which have semantic roles during generation. In Cheung et al. (2014), deep generative models are used with functional constraints on the latent space to increase speciﬁcity of latent variables, which is a goal we share but tackle using the oracle-information as a model-based semantic regularizer. Disentangling information and structuring models semantically is a theme in more recent work (Reed et al., 2014), (Kulkarni et al., 2015). Constraints on latent variables mod- els in an otherwise unsupervised setting have also found early usage in the context of Gaussian Processes (Lawrence & Qui˜nonero-Candela, 2006) using backconstraints. Using side-knowledge as a regularizer for the posterior over latent variables has been explored in other settings for simpler latent variable models in Ganchev et al. (2010) and we take inspiration from that work. An interesting link also exists between our formulation of the triplet likelihood using the Jensen Shannon divergence and generative adversarial networks (Goodfellow et al., 2014). The Bernoulli likelihood we employ using a softmax can conceptually be adapted to use a classiﬁer to match the framework from Goodfellow et al. (2014). Finally, an intuitive connection also exists with Vapnik’s privileged learning framework (Vapnik & Vashist, 2009) where in a supervised setting improved classiﬁers can be learned if privileged infor- mation in the form of additional features is present during training time. Borrowing terminology, we consider the similarity constraints to be a sparse privilege conveyed by an oracle of unobserved structure and aim at learning a student model which improves understanding of the data. Our gen- erative interpretation of this setting ultimately leads to our approach of learning a pseudo-causal inverse model of the data guided by oracle information by modeling factors of variation, instead of learning invariances as in Hadsell et al. (2006); Chopra et al. (2005).  4 RESULTS  The aim of this section is to illustrate the key properties of our proposed algorithms. We start by de- scribing the dataset and preliminaries in Section 4.1. In the ﬁrst part (Section 4.2), we quantitatively compare our methods against other baseline and state-of-the-art methods. In the second part (Sec- tion 4.3), we illustrate how our model variant with masks factorizes the latent spaces into distinct semantic units.  4.1 PRELIMINARIES We use a relatively small dataset that is, however, well-suited to illustrate the features of the algo- rithm and facilitates the interpretation of the factorized latent spaces: the Yale Faces dataset (Lee et al., 2005). The version we used comprises of 2, 414 images from 38 individuals under different light conditions. We split it into 300 test images and 2, 114 training images. The images were taken under controlled conditions using a lighting rig which allows for light sources to be varied in spe- ciﬁc ways. The azimuth and elevation of the light in relation with the depicted face were changed with values between −130 to +130 degrees and −40 to 90 degrees, respectively. The resulting im- ages have dramatic variability in appearance due to shading, apart from variability in identity of the depicted person.  6  Published as a conference paper at ICLR 2016  We proceeded with a series of oracle simulations. Particularity, we simulate three different questions upon presenting it with random triplets of images which we will then use the evaluations below. The questions we used were the following:  1. Who has the most similar identity? (”Identity”) 2. Where is the light condition most similar in terms of azimuth? (”Azimuth”) 3. Where is the light condition most similar in terms of elevation? (”Elevation”).  While the ﬁrst question is similar to a typical classiﬁcation setting, answering it accurately may actually require the ability to understand light variation well. Question 2 & 3 concern complex qualities of the images related to visual physics. The used Yale faces dataset provides metadata for all images and we can use this metadata to simulate a number of triplets for each oracle. 4.2 COMPARISON WITH BASELINE AND STATE-OF-THE-ART METHODS For evaluation we aim at a more quantitative understanding of what our and other models are good at and where they have limitations. For this we consider the metric learning network analogous to Hadsell et al. (2006) and the a purely unsupervised variational autoencoder (see Section 2.3) as comparators. We refer to them as MetricL and VAE, respectively. VAE is independent of oracle triplets in all experiments since it works entirely unsupervised. We use the learned representations of each model to assess the quality with respect to different evaluation measures. In particular, we use the representations to predict the identity of the face, the azimuth degree and the elevation degree (we provide classiﬁcation error and RMSD for the degrees). The latter is to assess how well the models capture physical properties of the images. All evaluations are done on held-out test data using a logistic regression model. In addition, we measure how well each model is able to predict triplets on test data, i.e., predict whether triplet i, j, l or i, l, j is true. We provide further information for model details and experimental setup in Appendix A. The results reveal that OPBN and its variants are on average the best-performing method. MetricL effectively learns a classiﬁer in this setting. In Table 1a we see that the generative model competes with the metric learning method in terms of classiﬁcation when being informed about identity from the oracle, while maintaining low error rates for tasks related to image physics. It outperforms VAE on classiﬁcation accuracy, at no loss to image understanding. We believe this shows that it incorporates oracle knowledge to shape alternative latent spaces compared to VAE. In Table 1b we test how well a model can incorporate more subtle oracle knowledge. We inform models only using a light condition oracle (azimuth). As expected, the metric learning performance collapses on all tasks except on the targeted oracle-task. VAE maintains the same performance since it is agnostic to oracle information. OPBN on the other hand maintains good performance on all tasks, beneﬁts in predicting light conditions over the unsupervised VAE. In a more complex setting, we also give the models oracle-information from all available questions jointly and test their performance. As we show in Table 2, again OPBN is the best-performing method on average. Metric learning approach cannot incorporate the variability of the available information usefully when using just a few triplets (data not shown), but in the setting we report using 100000 triplets it achieves good performance on classiﬁcation and elevation prediction. The main difference in performance between VAE and OPBN is in classiﬁcation and the ability to pre- dict triplets, which correlates with our observation that only about 50% of training triplets would be satisﬁed with the VAE approach. We see, that OPBN learns an equally competitive, but clearly different latent space than VAE and captures the semantics of the oracle better by being more pre- dictive on unseen triplets on test data. However, the beneﬁts of the oracles when incorporating multiple queries are underwhelming in comparison to single oracles. To address this issue, we Oracle/Method Identity Azimuth Elevation Triplet prediction  MetricL VAE OPBN 18.7 16.4 10.2 6.60  Oracle/Method Identity Azimuth Elevation Triplet prediction  MetricL VAE OPBN  18.2 20.4 10.5 34.00  18.2 20.4 10.5 34.00  9.0 36.3 20.0 1.25  9.0 20.6 10.4 6.42  70.0 13.5 18.0 3.4  (a) Model trained with the identity oracle.  (b) Model trained with the azimuth oracle.  Table 1: Comparison of Metric Learning Networks (MetricL), Variational Autoencoders (VAE) and our pro- posed model without masks (OPBN). We train the model with 100,000 triplets from the identity oracle (left) and the azimuth oracle (right). Best results results are in bold face. Second best results italic. We observe that OPBN predicts all properties reasonably well, while MetricL only for the task it is trained for. VAE works well on predicting lighting conditions. For more details see main text.  7  Published as a conference paper at ICLR 2016  MetricL VAE OPBN OPBN Mask OPBN Mask 500k  Oracle/Method Identity Azimuth Elevation Triplet prediction  10.2 27.1 9.1 27.1∗  18.2 20.4 10.5 34.0∗  9.7 19.8 10.0 5.7  9.0 13.8 6.5 5.2  7.3 14.1 6.0 4.2  Table 2: Comparison of Metric learning networks (MetricL), Variational Autoencoders (VAE) and three variants of our model (OPBN in Figure 1, OPBN with masks and OPBN with masks and 500,000 triplets instead of 100,000 triplets per oracle). We use triplets from the three oracles Identity, Azimuth and Elevation. We observe that OBPN with masks performs much better on all evaluations. Using more triplets leads to further improvements. Numbers marked with ∗ should be considered with care since MetricL and VAE are not aware of differences between oracles. See main text for more details.  add an experiment using Masked-OPBN. We observe that masked OPBN exhibits greatly improved quantitative performance on all tasks and yields representations which are more predictive of the image-properties, the class and the held out triplets than the other models. To further test this capac- ity after seeing that OPBN stalls in its improved performance, we add an extra experiment with 5 times the triplets for Masked OPBN. We observe that masking allows the model to continue improv- ing as more triplets are added. OPBN with masks thus shows the greatest promise to incorporate heterogeneous information from oracles into its latent spaces.  4.3 MASKING OPBN LEADS TO FACTORIZATION OF LATENT SPACES  We observed that the masked version of OPBN shows greatly improved ability to learn from com- plex oracles with multiple heterogeneous queries compared to other discussed approaches (see Fig- ure 2). For a model with otherwise equal paramet- ric capacity and the same fundamental inference machinery, this constitutes a surprising observa- tion. In this section, we will illustrate the effects of the learned masks and how they contribute to performance improvements. In the non-masked models triplet likelihoods are global. By learning local likelihoods via mask- ing, or subspaces, we allow the model to decide which parts of the space it uses per query. Varia- tional compression leads to solutions with the least possible amounts of used variables. In Figure 6 we show a fully learned mask for the Yale Faces model for each query. Learning jointly from all queries leads to a factorization of the latent space into task speciﬁc and partially shared latent vari- ables. We also observed a strong quantitative foot- print from usage of these masks: performance on all predictive tasks for models of otherwise equal capacity improves across the board (see Table 2), leading to models which capture light conditions and class better jointly. We consider this an effect of knowledge transfer from the oracle/crowd, allowing the model to identify semantic latent variable systems which do not only strive for high likelihoods on pixels, but also help model oracle triplets. We also observe that models with masks improve dramatically with availability of more triplets. The Bayesian objective helps compress the latent spaces into semantic variables. In order to inspect the latent spaces induced by the masks, we embed the respective subspaces of the latent variable encoding using t-SNE in Figure 7. It reveals that we learn ﬁne-grained class clusters when using the identity subspace, and continuous and smooth embeddings for azimuth and elevation pointing to the understanding of the model of the continuous and nature of light placements in the images. These results point to the fact that the oracle-informed model is able to learn the semantics of light placement and of facial structure in dedicated subspaces, increasing the semanticness of the learned space signiﬁcantly. This helps identify semantic variables which were explicitly never observed, as sketched earlier in Figure 2. We ﬁnally present an example of using masks to sample synthesized images in Figure 8. This illustrates the controlled transfer of subtle imaging-physics properties from one image to the next using the model.  Figure 6: We show the oracle-speciﬁc masks over the latent space when learning from multiple oracles at once. It is evident, that the model learns to virtu- ally switch off different dimensions per question and learns to factorize the latent space into compartmen- talized, task-relevant subspaces without ever explic- itly receiving supervision, for instance, about how to factorize light from a face class.  8  510152025303540455000.51Identity subspace510152025303540455000.51Azimuth subspace510152025303540455000.51Elevation subspaceLatent dimensionsWeightWeightWeightPublished as a conference paper at ICLR 2016  Figure 7: t-SNE visualizations of the latent subspaces that were identiﬁed in the model with shown weights in Figure 6. For visualization, we only use the dimensions with weight greater than 0.2 for each oracle. We observe that the identity subspace clearly separates faces from different persons (left), the azimuth degree (middle) and elevation degree (right) of light exposure.  Figure 8: We illustrate the factorization of latent spaces when us- ing masked models in the following. We take two images from the training-set, A and B, and project them into latent space. We use the encoding for face given by the identity mask from image B and combine it with the latent features given by the azimuth mask ap- plied to the encoding of image A. The resulting image is a blend of two, as expected, and approximates the facial features of image B, especially the mouth region and facial shape. The blended im- age furthermore exhibits light properties similar to image A. We ﬁnally show an unobserved test image which shows face B under light conditions A for comparison. We see that the facial transfer is not perfect, as the eyebrows are still taken from image A and the skin shading is a blend of both images. These are fair mistakes, since eyebrows are frequently shaded or mixed up with to light conditions in this dataset. We expect this to improve on bigger datasets and when using more oracle samples.  5 DISCUSSION We have introduced a joint unsupervised generative model over observations and triplet-constraints as given by an oracle. Our contributions are ﬁrst a fully probabilistic treatment of triplets and latent variable models in a joint unsupervised setting using variational belief networks. We show how this joint learning allows for implicit knowledge from an oracle, such as a human crowd, to be transferred to a rich parametric model, resulting in improved classiﬁcation scores, improved ability to predict triplets and more interpretability of the crowd biases. This can be a useful framework to encode expert knowledge in probabilistic reasoning systems when the exact model is unknown or labels are hard to obtain. Second, we introduce information theoretic distance measures for triplets generaliz- ing the commonly used Euclidian distances. We furthermore introduce the notion of question spe- ciﬁc masks in latent space to force the model to identify interpretable features of relevance for each speciﬁc type of oracle constraint, enabling the model to learn from multiple types of questions at once and boosting performance further. Our approach using variational inference and a triplet likeli- hood is not limited to belief networks, thus it will be interesting to use the framework in conjunction with other ﬂexible probabilistic models such as Gaussian Processes and inﬁnite partition models. We highlight the fact that using our framework no supervised pre-training of features is needed, as it can learn problem speciﬁc nonlinear feature-spaces adapted to the available information. We showed that our approach compares favorably with state-of-the-art metric learning models and fully unsupervised method in a generic application using feedforward networks. Our model is trivially extendable with convolutional and de-convolutional networks to be used on high-dimensional data. It will be interesting to combine the learning approach with more structure in temporal or spatially constrained models and encode other relationships like topological or unobserved constraints, such as taste of food in images. On the oracle side, future work regarding more accurate crowd-modeling for different bias and noise regimes are promising in conjunction with use-cases such as amazon mechanical turk. Our model is also particularly amenable to active learning for probing the ora- cles optimally. Finally, we wish to mention the potential for this framework to assist perceptual applications where biases of the human visual system can be studied assisted by generative models.  9  Identity subspaceAzimuth subspace08121620242832364Identity -120-60-300306090120-90Azimuth degree-30 Elevation subspace0153045607590-15Elevation degreeLight from Image AFace from Image BSynthetic: Face B Light AOriginal: Face B Light AVSPublished as a conference paper at ICLR 2016  REFERENCES Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Chechik, Gal, Sharma, Varun, Shalit, Uri, and Bengio, Samy. Large scale online learning of image  similarity through ranking. The Journal of Machine Learning Research, 11:1109–1135, 2010.  Cheung, Brian, Livezey, Jesse A, Bansal, Arjun K, and Olshausen, Bruno A. Discovering hidden  factors of variation in deep networks. arXiv preprint arXiv:1412.6583, 2014.  Chopra, Sumit, Hadsell, Raia, and LeCun, Yann. Learning a similarity metric discriminatively, with application to face veriﬁcation. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pp. 539–546. IEEE, 2005.  Ganchev, Kuzman, Grac¸a, Joao, Gillenwater, Jennifer, and Taskar, Ben. Posterior regularization for structured latent variable models. The Journal of Machine Learning Research, 11:2001–2049, 2010.  Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sher- jil, Courville, Aaron, and Bengio, Yoshua. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672–2680, 2014.  Graves, Alex.  Generating sequences with recurrent neural networks.  arXiv:1308.0850, 2013.  arXiv preprint  Hadsell, Raia, Chopra, Sumit, and LeCun, Yann. Dimensionality reduction by learning an invariant mapping. In Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pp. 1735–1742. IEEE, 2006.  Hershey, John R, Olsen, Peder, et al. Approximating the kullback leibler divergence between gaus- In Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE  sian mixture models. International Conference on, volume 4, pp. IV–317. IEEE, 2007.  Jordan, Michael I, Ghahramani, Zoubin, Jaakkola, Tommi S, and Saul, Lawrence K. An introduction  to variational methods for graphical models. Machine learning, 37(2):183–233, 1999.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980, 2014.  Kingma, Diederik P and Welling, Max. Auto-encoding variational bayes.  arXiv:1312.6114, 2013.  arXiv preprint  Kingma, Diederik P, Mohamed, Shakir, Rezende, Danilo Jimenez, and Welling, Max. Semi- supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581–3589, 2014.  Kulkarni, Tejas D, Whitney, Will, Kohli, Pushmeet, and Tenenbaum, Joshua B. Deep convolutional  inverse graphics network. arXiv preprint arXiv:1503.03167, 2015.  Lawrence, Neil D and Qui˜nonero-Candela, Joaquin. Local distance preservation in the gp-lvm through back constraints. In Proceedings of the 23rd international conference on Machine learn- ing, pp. 513–520. ACM, 2006.  Lee, Kuang-Chih, Ho, Jeffrey, and Kriegman, David. Acquiring linear subspaces for face recogni- tion under variable lighting. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 27(5):684–698, 2005.  Liu, Qiang, Peng, Jian, and Ihler, Alex T. Variational inference for crowdsourcing. In Advances in  Neural Information Processing Systems, pp. 692–700, 2012.  Mnih, Andriy and Gregor, Karol. Neural variational inference and learning in belief networks. arXiv  preprint arXiv:1402.0030, 2014.  10  Published as a conference paper at ICLR 2016  Reed, Scott, Sohn, Kihyuk, Zhang, Yuting, and Lee, Honglak. Learning to disentangle factors of variation with manifold interaction. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 1431–1439, 2014.  Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and  approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.  Schroff, Florian, Kalenichenko, Dmitry, and Philbin, James. Facenet: A uniﬁed embedding for face  recognition and clustering. arXiv preprint arXiv:1503.03832, 2015.  Tamuz, Omer, Liu, Ce, Belongie, Serge, Shamir, Ohad, and Kalai, Adam Tauman. Adaptively  learning the crowd kernel. arXiv preprint arXiv:1105.1033, 2011.  Titsias, Michalis and L´azaro-Gredilla, Miguel. Doubly stochastic variational bayes for non- conjugate inference. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 1971–1979, 2014.  Van Der Maaten, Laurens and Weinberger, Kilian. Stochastic triplet embedding. In Machine Learn- ing for Signal Processing (MLSP), 2012 IEEE International Workshop on, pp. 1–6. IEEE, 2012.  Vapnik, Vladimir and Vashist, Akshay. A new learning paradigm: Learning using privileged infor-  mation. Neural Networks, 22(5):544–557, 2009.  Wainwright, Martin J and Jordan, Michael I. Graphical models, exponential families, and variational  inference. Foundations and Trends R(cid:13) in Machine Learning, 1(1-2):1–305, 2008.  Wang, Jiang, Song, Yang, Leung, Thomas, Rosenberg, Chuck, Wang, Jingbin, Philbin, James, Chen, Bo, and Wu, Ying. Learning ﬁne-grained image similarity with deep ranking. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 1386–1393. IEEE, 2014.  11  Published as a conference paper at ICLR 2016  A APPENDIX A: EXPERIMENTAL SETTINGS  Here we can give more experimental details. In all experiments we used diagonal Normal distributions as priors for the latent space and rmsProp with momentum (Graves, 2013) or ADAM (Kingma & Ba, 2014) as an optimizer. All experiments were run on Graphics Processing Units (GPUs) using a theano (Bergstra et al., 2010) implementation and did not take more than a few hours each. We can simulate an oracle for each question by using the annotations provided with the dataset. For question one, we sample from the label distribution checking for a match to produce answers to the triplets generated. For question two and three we resort to sampling from the relative distances of target angles to given angles to produce the triplet information. We ﬁnally generate 3 different sim- ulated oracles, OracleID, OracleAz and OracleAll which correspond to asking just the ﬁrst question, just the second or all three mixed. We sample 100,000 triplets for each oracle-question at random (meaning that the third simulation has 300000 triplets). We repeat the process 3 times to account for sampling bias and report the means of the reruns on our experiments. In an extra illustrative example combining all three oracles, we samples 500,000 triplets per question for a total of 1,500,000 triplets to inform the model. While these numbers sound high, we note that there is a large combinatorial space of possible triplets to be explored. We proceed to learn fully unsupervised models of these images using an architecture with 200 hid- den deterministic units and 50 latent variables. The deterministic layers use tanh nonlinearities. We set up the analogous MetricL model without a generative path as a supervised learning model optimizing the triplet embeddings given images with a euclidian loss function.  B APPENDIX B: DETAILS FOR ORACLE-LIKELIHOOD  In order to compute the likelihood for the triplet likelihood, we need to calculate an expensive diver- gence term D using an information theoretic quantity, the Jensen Shannon Divergence as deﬁned in Section 2.1. In practice, this term is typically intractable analytically since it involves a KL diver- gence involving over a mixture over two possibly disjoint distributions. In order to evaluate this KL divergence, exhaustive sampling methods need to be used. In order to avoid expensive sampling steps during training, we explore approximations to the term D. In the presented experiments we used:  H(cid:88)  a,b = − H(cid:88)  Dh  h=1  h=1  (cid:20) 1  2  (cid:16)  Da,b =  (cid:17)  (cid:16)  KL  p(zh  a)||p(zh b )  +  1 2  KL  p(zh  b )||p(zh a)  (cid:17)(cid:21)  .  This approximation is inaccurate globally, but empirically is fast and yields better results than the KL divergence or a eucilidian distance and becomes accurate in the limit of closeby distributions. Clearly, using the full JS is beneﬁcial to the model and yields stronger posterior regularization allowing to learn more efﬁciently from triplets, especially in combination with full covariance latent spaces. An overview of previous approximations related to the JS is given in (Hershey et al., 2007). We have tried previously known Monte Carlo-based approximations and explore novel deterministic approximations to this term and expect to show empirical performance in an update to this paper and in follow-up work.  C APPENDIX C: FURTHER YALE FACES SAMPLES  We trained a model on Yale Faces with 400 hidden units (units chosen until likelihoods stopped improving) and used it similarly to the masked experiments in the main paper. We use the space in the supplement to show a few more samples in Figure 9 and do a form of image algebra by adding components of various images together.  12  Published as a conference paper at ICLR 2016  Figure 9: Top We select a trainings image of a face and select the latent encoding corresponding to its identify. Middle We select three training images of another face under different light conditions and select the light variables according to the mask. Bottom We synthesize new images with the face and light images clamped to the observations and see that noisy faces are generated which look like the top face and have light conditions like the middle one.  D APPENDIX D: YALE FACES TRIPLET VARIATIONS  We sample another batch of 100000 triplets for the Yale dataset per query and rerun OPBN-Masked with a varying number of triplets to clarify the effect. We show results in Table 3, where it is evident that all queries improve as we add triplets. We want to note that these numbers are based on a single sampling of triplets and thus are subject to sampling noise. By chance, more or less good triplets may be contained in the set.  Table 3: Comparison of Model metrics on Yale between with varying triplet numbers.  ORACLEALL TRIPLET PREDICTION CLASSIFICATION AZIMUTH RMSD ELEVATION RMSD  100 35.95 19.00 19.59 9.59  1000 28.88 15.66 18.73 10.99  10000 22.95 12.66 17.02 7.75  100000 5.56 8.66 15.50 6.37  13  Face()()()()+Light=Published as a conference paper at ICLR 2016  Figure 10: We show the generated training data. Left the original image Right progressively rotated version of the original, depending on position on trajectory.  E APPENDIX E: MNIST EXPERIMENTS  We generated a perturbed version of the MNIST dataset in order to show other settings where the proposed approach can be used interestingly. In normal MNIST, each letter is generated depending on its class. By eyeballing, style variations can be seen, but they are not captured in the meta-data in order to be used for evaluation. We proceed to take 10000 MNIST digits of equal proportions from each class and rotate them by 5 progressively increasing positive angles. This creates the effect of pushing the digits to fall over towards the right side in a trajectory, as shown in Figure 10. The questions we can ask simulated oracles here are the following:  1. Which image of a set is part of the same trajectory? This question is related to both identity  and instantiation (style) of a variable.  2. Which images have similar/dissimilar angles/timepoints? 3. Which images have similar labels?  We chose not to include order into the labels, but similarity in digit-images could also be deﬁned by the value of a digit which could be used for reasoning tasks such as performing mathematical operations with inferred values in an ordered manifold. In our setting we assume similarity is implied by the same label and dissimilarity else. In future work we plan to exploit semantic oracles which understand order for reasoning-related tasks. In our experiment using 2 deterministic layers with 800 and 400 hidden units with tanh nonlinearities and 50 latent variables we ﬁnd OPBN masked to outperform VAE, see Table 4. The mask variables also manage to factorize the latent space sharply into variables for each query in the setting of 100000 triplets. We also observed that when using less triplets the second query suffered the most in performance, whoch makes intuitive sense since learning a rotation is a harder task than learning to match labels. In terms of ﬁnal performance we observe that OPBN can strongly reduce the predictive errors for the tasks, although the two synthetic tasks are actually quite hard.  Table 4: Comparison of Model metrics on MNIST between VAE and OPBN masked with 100, 1000 and 100000 triplets.  VAE ORACLEALL 36.51 TRIPLET PREDICTION CLASSIFICATION 24.19 ROTATION ANGLE RMSD 18.87  OPBNMASKED-100 OPBNMASKED-1000 OPBNMASKED-100000 16.45 10.68 14.28  34.32 22.61 18.7  34.35 23.05 18.4  14  Published as a conference paper at ICLR 2016  Table 5: Comparison of Model metrics on Yale Faces with Identity crowd.  ORACLEID (cid:15) = 0 AZIMUTH RMSD ELEVATION RMSD CLASSIFICATION% TRIPLET PREDICTION %  METRICL OPBN 20.99 10.13 7.2 5.3  57 24.3 14 5.0  Table 6: Comparison of Model metrics on Yale Faces with Azimuth crowd  ORACLEAZ (cid:15) = 0 TRIPLET PREDICTION % AZIMUTH RMSD ELEVATION RMSD CLASSIFICATION %  10.8 18  METRICL OPBN 18.1 19.8 11.7 7.9  18.59 40.7  F APPENDIX F: ROBUSTNESS TO ORACLE-NOISE  Here we train Metric Learning and OPBN with 2,000 triplets and 2,000 datapoints in a variety of oracle settings. Oracles are perturbed by noise (cid:15), meaning that a fraction equal to (cid:15) of the triplets are ﬂipped and thus wrong. The experiment illustrates the robustness the generative aspect of ghe model gives it, whereas it is evident that metric learning approaches lose more performance since they cannot beneﬁt from modeling observations directly. We observe that OPBN learns signiﬁcantly better representations to predict azimuth, elevation and the classiﬁcation label. OPBN performs similarly good in terms of triplet prediction.  Table 7: Comparison of Model metrics on Yale Faces with All-oracle.  ORACLEALL (cid:15) = 0 TRIPLET PREDICTION % 38.7 21.18 AZIMUTH RMSD ELEVATION RMSD 11.57 CLASSIFICATION %  VAE METRICL OPBN OBPN-MASKED 9.7 16.37 7.14 13.6  18.4 21.77 10.23 12.6  40.42 14.75  17  33  25  15  Published as a conference paper at ICLR 2016  Table 8: Comparison of Model metrics on Yale Faces for noise noise robustness when using MetricL.  METRICL-ALL TRIPLET PREDICTION % AZIMUTH RMSD ELEVATION RMSD CLASSIFICATION %  (cid:15) = 0  67  40.42 14.75  75  (cid:15) = 0.2 59.9 35.32 18.62  71  (cid:15) = 0.4  52  41.42 21.6 71  Table 9: Comparison of Model metrics on Yale Faces for noise robustness when using OPBN.  (cid:15) = 0.4  52  41.42 21.6 71  (cid:15) = 0  67  40.42 14.75  75  (cid:15) = 0.2 59.9 35.32 18.62  71  OPBN (cid:15) = 0 TRIPLET PREDICTION % 18.4 AZIMUTH RMSD 21.77 10.23 ELEVATION RMSD CLASSIFICATION % 92.6  (cid:15) = 0.2 37.6 22.9 10.89 89.3  (cid:15) = 0.4 46.2 23.75 10.72 89.1  METRICL-ALL TRIPLET PREDICTION % AZIMUTH RMSD ELEVATION RMSD CLASSIFICATION %  16  ",
1511.04834,2016,Neural Programmer: Inducing Latent Programs with Gradient Descent,"['Neural Programmer: Inducing Latent Programs with Gradient Descent\nArvind Neelakantan', 'Quoc Le', 'Ilya Sutskever']",https://arxiv.org/pdf/1511.04834,"6 1 0 2     g u A 4         ]  G L . s c [      3 v 4 3 8 4 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  NEURAL PROGRAMMER: INDUCING LATENT  PROGRAMS WITH GRADIENT DESCENT  Arvind Neelakantan∗ University of Massachusetts Amherst arvind@cs.umass.edu  Quoc V. Le Google Brain qvl@google.com  Ilya Sutskever Google Brain ilyasu@google.com  ABSTRACT  Deep neural networks have achieved impressive supervised classiﬁcation perfor- mance in many tasks including image recognition, speech recognition, and se- quence to sequence learning. However, this success has not been translated to ap- plications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we pro- pose Neural Programmer, a neural network augmented with a small set of basic arithmetic and logic operations that can be trained end-to-end using backpropaga- tion. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We ﬁnd that training the model is difﬁ- cult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent net- works and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.  1  INTRODUCTION  The past few years have seen the tremendous success of deep neural networks (DNNs) in a variety of supervised classiﬁcation tasks starting with image recognition (Krizhevsky et al., 2012) and speech recognition (Hinton et al., 2012) where the DNNs act on a ﬁxed-length input and output. More recently, this success has been translated into applications that involve a variable-length sequence as input and/or output such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2014), image captioning (Vinyals et al., 2015; Xu et al., 2015), conversational model- ing (Shang et al., 2015; Vinyals & Le, 2015), end-to-end Q&A (Sukhbaatar et al., 2015; Peng et al., 2015; Hermann et al., 2015), and end-to-end speech recognition (Graves & Jaitly, 2014; Hannun et al., 2014; Chan et al., 2015; Bahdanau et al., 2015). While these results strongly indicate that DNN models are capable of learning the fuzzy underlying patterns in the data, they have not had similar impact in applications that involve crisp reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, Joulin & Mikolov (2015) show that recurrent neural networks (RNNs) fail at the task of adding two binary numbers even when the result has less than 10 bits. This makes existing DNN models unsuitable for downstream applications that require complex reasoning, e.g., natural language question answering. For example, to answer the question “how many states border Texas?” (see Zettlemoyer & Collins (2005)), the algorithm has to perform an act of counting in a table which is something that a neural network is not yet good at.  ∗Work done during an internship at Google.  1  Published as a conference paper at ICLR 2016  A fairly common method for solving these problems is program induction where the goal is to ﬁnd a program (in SQL or some high-level languages) that can correctly solve the task. An application of these models is in semantic parsing where the task is to build a natural language interface to a structured database (Zelle & Mooney, 1996). This problem is often formulated as mapping a natural language question to an executable query. A drawback of existing methods in semantic parsing is that they are difﬁcult to train and require a great deal of human supervision. As the space over programs is non-smooth, it is difﬁcult to apply simple gradient descent; most often, gradient descent is augmented with a complex search procedure, such as sampling (Liang et al., 2010). To further simplify training, the algorithmic de- signers have to manually add more supervision signals to the models in the form of annotation of the complete program for every question (Zettlemoyer & Collins, 2005) or a domain-speciﬁc grammar (Liang et al., 2011). For example, designing grammars that contain rules to associate lexical items to the correct operations, e.g., the word “largest” to the operation “argmax”, or to produce syntactically valid programs, e.g., disallow the program >= dog. The role of hand-crafted grammars is crucial in semantic parsing yet also limits its general applicability to many different domains. In a recent work by Wang et al. (2015) to build semantic parsers for 7 domains, the authors hand engineer a separate grammar for each domain. The goal of this work is to develop a model that does not require substantial human supervision and is broadly applicable across different domains, data sources and natural languages. We propose Neural Programmer (Figure 1), a neural network augmented with a small set of basic arithmetic and logic operations that can be trained end-to-end using backpropagation. In our formulation, the neural network can run several steps using a recurrent neural network. At each step, it can select a segment in the data source and a particular operation to apply to that segment. The neural network propagates these outputs forward at every step to form the ﬁnal, more complicated output. Using the target output, we can adjust the network to select the right data segments and operations, thereby inducing the correct program. Key to our approach is that the selection process (for the data source and operations) is done in a differentiable fashion (i.e., soft selection or attention), so that the whole neural network can be trained jointly by gradient descent. At test time, we replace soft selection with hard selection.  Figure 1: The architecture of Neural Programmer, a neural network augmented with arithmetic and logic operations. The controller selects the operation and the data segment. The memory stores the output of the operations applied to the data segments and the previous actions taken by the controller. The controller runs for several steps thereby inducing compositional programs that are more complex than the built-in operations. The dotted line indicates that the controller uses information in the memory to make decisions in the next time step.  By combining neural network with mathematical operations, we can utilize both the fuzzy pattern matching capabilities of deep networks and the crisp algorithmic power of traditional programmable computers. This approach of using an augmented logic and arithmetic component is reminiscent of the idea of using an ALU (arithmetic and logic unit) in a conventional computer (Von Neumann, 1945). It is loosely related to the symbolic numerical processing abilities exhibited in the intrapari- etal sulcus (IPS) area of the brain (Piazza et al., 2004; Cantlon et al., 2006; Kucian et al., 2006; Fias et al., 2007; Dastjerdi et al., 2013). Our work is also inspired by the success of the soft attention mechanism (Bahdanau et al., 2014) and its application in learning a neural network to control an additional memory component (Graves et al., 2014; Sukhbaatar et al., 2015).  2  ControllerSoftSelectionArithmetic and logic operationsMemoryDataOutputApplyInputt = 1, 2, …, TTimestep tPublished as a conference paper at ICLR 2016  Neural Programmer has two attractive properties. First, it learns from a weak supervision signal which is the result of execution of the correct program. It does not require the expensive annotation of the correct program for the training examples. The human supervision effort is in the form of question, data source and answer triples. Second, Neural Programmer does not require additional rules to guide the program search, making it a general framework. With Neural Programmer, the algorithmic designer only deﬁnes a list of basic operations which requires lesser human effort than in previous program induction techniques. We experiment with a synthetic table-comprehension dataset, consisting of questions with a wide range of difﬁculty levels. Examples of natural language translated queries include “print elements in column H whose ﬁeld in column C is greater than 50 and ﬁeld in column E is less than 20?” or “what is the difference between sum of elements in column A and number of rows in the table?”. We ﬁnd that LSTM recurrent networks (Hochreiter & Schmidhuber, 1997) and LSTM models with attention (Bahdanau et al., 2014) do not work well. Neural Programmer, however, can completely solve this task or achieve greater than 99% accuracy on most cases by inducing the required latent program. We ﬁnd that training the model is difﬁcult, but it can be greatly improved by injecting random Gaussian noise to the gradient (Welling & Teh, 2011; Neelakantan et al., 2016) which enhances the generalization ability of the Neural Programmer.  2 NEURAL PROGRAMMER  Even though our model is quite general, in this paper, we apply Neural Programmer to the task of question answering on tables, a task that has not been previously attempted by neural networks. In our implementation for this task, Neural Programmer is run for a total of T time steps chosen in advance to induce compositional programs of up to T operations. The model consists of four modules:  • A question Recurrent Neural Network (RNN) to process the input question, • A selector to assign two probability distributions at every step, one over the set of operations • A list of operations that the model can apply and, • A history RNN to remember the previous operations and data segments selected by the  and the other over the data segments,  model till the current time step.  These four modules are also shown in Figure 2. The history RNN combined with the selector module functions as the controller in this case. Information about each component is discussed in the next sections.  Figure 2: An implementation of Neural Programmer for the task of question answering on tables. The output of the model at time step t is obtained by applying the operations on the data segments weighted by their probabilities. The ﬁnal output of the model is the output at time step T . The dotted line indicates the input to the history RNN at step t+1.  Apart from the list of operations, all the other modules are learned using gradient descent on a training set consisting of triples, where each triple has a question, a data source and an answer. We  3  RNN stepht-1Input at step t ctHistory RNNData Source; []Input atstep t+1Timestep tFinalOutput=OutputTt = 1, 2, …, T      hcolqQuestion RNNOutputt =Op on data weighted by softmaxSoftmaxSoftmaxhopOp SelectorCol SelectorhtOperations     ApplyPublished as a conference paper at ICLR 2016  assume that the data source is in the form of a table, table ∈ RM×C, containing M rows and C columns (M and C can vary amongst examples). The data segments in our experiments are the columns, where each column also has a column name.  2.1 QUESTION MODULE  The question module converts the question tokens to a distributed representation. In the basic version of our model, we use a simple RNN (Werbos, 1990) parameterized by W question and the last hidden state of the RNN is used as the question representation (Figure 3).  Figure 3: The question module to process the input question. q = zq denotes the question represen- tation used by Neural Programmer. Consider an input question containing Q words {w1, w2, . . . , wQ}, the question module performs the following computations:  zi = tanh(W question [zi−1; V (wi)]),∀i = 1, 2, . . . , Q  where V (wi) ∈ Rd represents the embedded representation of the word wi, [a; b] ∈ R2d represents the concatenation of two vectors a, b ∈ Rd, W question ∈ Rd×2d is the recurrent matrix of the question RNN, tanh is the element-wise non-linearity function and zQ ∈ Rd is the representation of the question. We set z0 to [0]d. We pre-process the question by removing numbers from it and storing the numbers in a separate list. Along with the numbers we store the word that appeared to the left of it in the question which is useful to compute the pivot values for the comparison operations described in Section 2.3. For tasks that involve longer questions, we use a bidirectional RNN since we ﬁnd that a simple unidirectional RNN has trouble remembering the beginning of the question. When the bidirectional RNN is used, the question representation is obtained by concatenating the last hidden states of the two-ends of the bidirectional RNNs. The question representation is denoted by q.  2.2 SELECTOR  The selector produces two probability distributions at every time step t (t = 1, 2, . . . , T ): one probablity distribution over the set of operations and another probability distribution over the set of columns. The inputs to the selector are the question representation (q ∈ Rd) from the question module and the output of the history RNN (described in Section 2.4) at time step t (ht ∈ Rd) which stores information about the operations and columns selected by the model up to the previous step. Each operation is represented using a d-dimensional vector. Let the number of operations be O and let U ∈ RO×D be the matrix storing the representations of the operations. Operation Selection is performed by:  αop t = softmax (U tanh(W op[q; ht]))  t ∈ [0, 1]O over the set of operations (Figure 4).  where W op ∈ Rd×2d is the parameter matrix of the operation selector that produces the probability distribution αop The selector also produces a probability distribution over the columns at every time step. We obtain vector representations for the column names using the parameters in the question module (Section 2.1) by word embedding or an RNN phrase embedding. Let P ∈ RC×D be the matrix storing the representations of the column names. Data Selection is performed by:  4  Question RNNlast RNN hidden statew1V(w1) z1w2V(w2) z2 = tanh(Wquestion [z1; V(w2)])wqV(wq) q=zq……Published as a conference paper at ICLR 2016  Figure 4: Operation selection at time step t where the selector assigns a probability distribution over the set of operations.  αcol t = softmax (P tanh(W col[q; ht]))  where W col ∈ Rd×2d is the parameter matrix of the column selector that produces the probability distribution αcol  t ∈ [0, 1]C over the set of columns (Figure 5).  Figure 5: Data selection at time step t where the selector assigns a probability distribution over the set of columns.  2.3 OPERATIONS  Neural Programmer currently supports two types of outputs: a) a scalar output, and b) a list of items selected from the table (i.e., table lookup).1 The ﬁrst type of output is for questions of type “Sum of elements in column C” while the second type of output is for questions of type “Print elements in column A that are greater than 50.” To facilitate this, the model maintains two kinds of out- put variables at every step t, scalar answert ∈ R and lookup answert ∈ [0, 1]M×C. The output lookup answert (i , j ) stores the probability that the element (i, j) in the table is part of the out- put. The ﬁnal output of the model is scalar answerT or lookup answerT depending on whichever of the two is updated after T time steps. Apart from the two output variables, the model main- tains an additional variable row selectt ∈ [0, 1]M that is updated at every time step. The variables row selectt [i ](∀i = 1, 2, . . . , M) maintain the probability of selecting row i and allows the model to dynamically select a subset of rows within a column. The output is initialized to zero while the row select variable is initialized to [1]M . Key to Neural Programmer is the built-in operations, which have access to the outputs of the model at every time step before the current time step t, the operations have access to (scalar answer i, lookup answer i),∀i = 1, 2, . . . , t − 1. This enables the model to build powerful compositional programs. It is important to design the operations such that they can work with probabilistic row and column selection so that the model is differentiable. Table 1 shows the list of operations built into the model along with their deﬁnitions. The reset operation can be selected any number of times which when required allows the model to induce programs whose complexity is less than T steps.  i.e.,  1It is trivial to extend the model to support general text responses by adding a decoder RNN to generate text  sentences.  5  RNN stepht ht-1Input at step t ctHistory RNNqQuestion RNNhop =tanh(Wop [q; ht])Op SelectorOp: 1Op: 2 Op: VOperations……Softmax…………Timestep tt = 1, 2, …, TRNN stepht ht-1Input at step t ctHistory RNNqQuestion RNNTimestep tt = 1, 2, …, TCol:1Col:C      ….Data SourceSoftmax……Col Selector       hcol = tanh(Wcol [q; ht])Published as a conference paper at ICLR 2016  M(cid:80) M(cid:80)  i=1  Type  Operation  Deﬁnition  Aggregate  Sum  sumt [j] =  row selectt−1 [i] ∗ table[i][j],∀j = 1, 2, . . . , C  i=1  row selectt−1 [i]  Arithmetic Comparison  Count Difference Greater Lesser And Or assign Reset  countt = diﬀt = scalar output t−3 − scalar output t−1 gt[i][j] = table[i][j] > pivotg ,∀(i, j), i = 1, . . . , M, j = 1, . . . , C lt[i][j] = table[i][j ] < pivotl ,∀(i, j), i = 1, . . . , M, j = 1, . . . , C and t[i] = min(row selectt−1 [i], row selectt−2 [i]),∀i = 1, 2, . . . , M or t[i] = max(row selectt−1 [i], row selectt−2 [i]),∀i = 1, 2, . . . , M assignt[i][j] = row selectt−1 [i],∀(i, j)i = 1, 2, . . . , M, j = 1, 2, . . . , C resett [i] = 1,∀i = 1, 2, . . . , M  Logic Assign Lookup Reset Table 1: List of operations along with their deﬁnitions at time step t, table ∈ RM×C is the data source in the form of a table and row selectt ∈ [0, 1]M functions as a row selector.  While the deﬁnitions of the operations are fairly straightforward, comparison operations greater and lesser require a pivot value as input (refer Table 1), which appears in the question. Let qn1, qn2, . . . , qnN be the numbers that appear in the question. For every comparison operation (greater and lesser), we compute its pivot value by adding up all the numbers in the question each of them weighted with the probabilities assigned to it computed using the hidden vector at position to the left of the number,2 and the operation’s embedding vector. More precisely:  βop = softmax (ZU (op))  pivotop =  βop(i)qni  N(cid:88)  i=1  where U (op) ∈ Rd is the vector representation of operation op (op ∈ {greater, lesser}) and Z ∈ RN×d is the matrix storing the hidden vectors of the question RNN at positions to the left of the occurrence of the numbers. By overloading the deﬁnition of αop t (j) denote the probability assigned by the selector to operation x (x ∈ {sum, count, difference, greater, lesser, and, or, assign, reset}) and column j (∀j = 1, 2, . . . , C) at time step t respectively. Figure 6 show how the output and row selector variables are computed. The output and row selector variables at a step is obtained by additively combining the output of the individual operations on the different data segments weighted with their corresponding probabilities assigned by the model.  t (x) and αcol  t and αcol  , let αop  t  Figure 6: The output and row selector variables are obtained by applying the operations on the data segments and additively combining their outputs weighted using the probabilities assigned by the selector.  2This choice is made to reﬂect the common case in English where the pivot number is usually mentioned  after the operation but it is trivial to extend to use hidden vectors both in the left and the right of the number.  6  Data SourceTimestep tt = 1, 2, …, TOperations     ApplySelectorSoftmaxSoftmaxrow_selectt-1row_selectt-2scalar_answert-1scalar_answert-2scalar_answert-3row_selecttscalar_answertlookup_answertPublished as a conference paper at ICLR 2016  More formally, the output variables are given by:  scalar answert = αop  t (count)countt + αop  t (difference)diﬀt +  C(cid:88)  t (j)αop αcol  t (sum)sumt [j ],  t (assign)assignt [i][j],∀(i, j)i = 1, 2, . . . , M, j = 1, 2, . . . , C  j=1  t (j)αop lookup answert [i][j] = αcol The row selector variable is given by: row selectt [i ] = αop  t (and)andt [i] + αop  C(cid:88)  t (or)ort [i] + αop  t (reset)resett [i]+  t (j)(αop αcol  t (greater)gt[i][j] + αop  t (lesser)lt[i][j]),∀i = 1, . . . , M  j=1  It is important to note that other operations like equal to, max, min, not etc. can be built into this model easily.  2.3.1 HANDLING TEXT ENTRIES  So far, our disscusion has been only concerned with tables that have numeric entries. In this section we describe how Neural Programmer handles text entries in the input table. We assume a column can contain either numeric or text entries. An example query is “what is the sum of elements in column B whose ﬁeld in column C is word:1 and ﬁeld in column A is word:7?”. In other words, the query is looking for text entries in the column that match speciﬁed words in the questions. To answer these queries, we add a text match operation that updates the row selector variable appropriately. In our implementation, the parameters for vector representations of the column’s text entries are shared with the question module. The text match operation uses a two-stage soft attention mechanism, back and forth from the text entries to question module. In the following, we explain its implementation in detail. Let T C1, T C2, . . . , T CK be the set of columns that each have M text entries and A ∈ M × K × d store the vector representations of the text entries. In the ﬁrst stage, the question representation coarsely selects the appropriate text entries through the sigmoid operation. Concretely, coarse se- lection, B, is given by the sigmoid of dot product between vector representations for text entries, A, and question representation, q:  (cid:32) d(cid:88)  (cid:33)  B[m][k] = sigmoid  A[m][k][p] · q[p]  ∀(m, k) m = 1, . . . , M, k = 1, . . . , K  p=1  To obtain question-speciﬁc column representations, D, we use B as weighting factors to compute the weighted average of the vector representations of the text entries in that column:  M(cid:88)  m=1  1 M  D[k][p] =  (B[m][k] · A[m][k][p]) ∀(k, p) k = 1, . . . , K, p = 1, . . . , d  To allow different words in the question to be matched to the corresponding columns (e.g., match word:1 in column C and match word:7 in column A for question “what is the sum of elements in column B whose ﬁeld in column C is word:1 and ﬁeld in column A is word:7?’), we add the column name representations (described in Section 2.2), P , to D to obtain column representations E. This make the representation also sensitive to the column name. In the second stage, we use E to compute an attention over the hidden states of the question RNN to get attention vector G for each column of the input table. More concretely, we compute the dot product between E and the hidden states of the question RNN to obtain scalar values. We then  7  Published as a conference paper at ICLR 2016  pass them through softmax to obtain weighting factors for each hidden state. G is the weighted combination of the hidden states of the question RNN. Finally, text match selection is done by:  text match[m][k] = sigmoid  A[m][k][p] · G[k][p]  ∀(m, k) m = 1, . . . , M, k = 1, . . . , K  (cid:33)  Without loss of generality, let the ﬁrst K (K ∈ [0, 1, . . . , C]) columns out of C columns of the table contain text entries while the remaining contain numeric entries. The row selector variable now is given by:  row selectt [i ] = αop  t (and)andt [i] + αop  t (or)ort [i] + αop  t (reset)resett [i]+  t (j)(αop αcol  t (greater)gt[i][j] + αop  t (lesser)lt[i][j])+  (cid:32) d(cid:88)  p=1  C(cid:88) K(cid:88)  j=K+1  t (j)(αop αcol  t (text match)text match t[i][j],∀i = 1, . . . , M  j=1  The two-stage mechanism is required since in our experiments we ﬁnd that simply averaging the vector representations fails to make the representation of the column speciﬁc enough to the question. Unless otherwise stated, our experiments are with input tables whose entries are only numeric and in that case the model does not contain the text match operation.  2.4 HISTORY RNN  The history RNN keeps track of the previous operations and columns selected by the selector module so that the model can induce compositional programs. This information is encoded in the hidden vector of the history RNN at time step t, ht ∈ Rd. This helps the selector module to induce the probability distributions over the operations and columns by taking into account the previous actions selected by the model. Figure 7 shows details of this component.  Figure 7: The history RNN which helps in remembering the previous operations and data segments selected by the model. The dotted line indicates the input to the history RNN at step t+1.  The input to the history RNN at time step t, ct ∈ R2d is obtained by concatenating the weighted representations of operations and column names with their corresponding probability distribution produced by the selector at step t − 1. More precisely:  ct = [(αop  t−1)T U ; (αcol  t−1)T P ]  The hidden state of the history RNN at step t is computed as:  ht = tanh(W history [ct; ht−1]),∀i = 1, 2, . . . , Q  where W history ∈ Rd×3d is the recurrent matrix of the history RNN, and ht ∈ Rd is the current representation of the history. The history vector at time t = 1, h1 is set to [0]d.  8  RNN stepht-1Input at step t ctHistory RNNData Source; []Input atstep t+1Timestep tt = 1, 2, …, T      hcolqQuestion RNNSoftmaxSoftmaxhophtOperationsWeighted sum of col vectors Weighted sum of op vectors Published as a conference paper at ICLR 2016  2.5 TRAINING OBJECTIVE  The parameters of the model include the parameters of the question RNN, W question, parameters of the history RNN, W history, word embeddings V (.), operation embeddings U, operation selector and column selector matrices, W op and W col respectively. During training, depending on whether the answer is a scalar or a lookup from the table we have two different loss functions. When the answer is a scalar, we use Huber loss (Huber, 1964) given by:  Lscalar(scalar answerT , y) =  (cid:26) 1 2 a2, if a ≤ δ δa − 1  2 δ2, otherwise  where a = |scalar answer T − y| is the absolute difference between the predicted and true answer, and δ is the Huber constant treated as a model hyper-parameter. In our experiments, we ﬁnd that using square loss makes training unstable while using the absolute loss makes the optimization difﬁcult near the non-differentiable point. When the answer is a list of items selected from the table, we convert the answer to y ∈ {0, 1}M×C, where y[i, j] indicates whether the element (i, j) is part of the output. In this case we use log-loss over the set of elements in the table given by:  Llookup(lookup answer T , y) = − 1 M C  M(cid:88)  (cid:18) C(cid:88)  i=1  j=1  y[i, j] log(lookup answer T [i, j])+  (cid:19) (1 − y[i, j]) log(1 − lookup answer T [i, j])  The training objective of the model is given by:  (cid:18)  N(cid:88)  k=1  L =  1 N  [nk == T rue]L(k)  scalar + [nk == F alse]λL(k)  lookup  (cid:19)  scalar and L(k)  where N is the number of training examples, L(k) lookup are the scalar and lookup loss on kth example, nk is a boolean random variable which is set to True when the kth example’s answer is a scalar and set to False when the answer is a lookup, and λ is a hyper-parameter of the model that allows to weight the two loss functions appropriately. At inference time, we replace the three softmax layers in the model with the conventional maximum (hardmax) operation and the ﬁnal output of the model is either scalar answerT or lookup answerT , depending on whichever among them is updated after T time steps. Algorithm 1 gives a high-level view of Neural Programmer during inference.  3 EXPERIMENTS  Neural Programmer is faced with many challenges, speciﬁcally: 1) can the model learn the param- eters of the different modules with delayed supervision after T steps? 2) can it exhibit composi- tionality by generalizing to unseen questions? and 3) can the question module handle the variability and ambiguity of natural language? In our experiments, we mainly focus on answering the ﬁrst two questions using synthetic data. Our reason for using synthetic data is that it is easier to understand a new model with a synthetic dataset. We can generate the data in a large quantity, whereas the biggest real-word semantic parsing datasets we know of contains only about 14k training examples (Pasu- pat & Liang, 2015) which is very small by neural network standards. In one of our experiments, we introduce simple word-level variability to simulate one aspect of the difﬁculties in dealing with natural language input.  3.1 DATA  We generate question, table and answer triples using a synthetic grammar. Tables 4 and 5 (see Ap- pendix) shows examples of question templates from the synthetic grammar for single and multiple  9  Published as a conference paper at ICLR 2016  Algorithm 1 High-level view of Neural Programmer during its inference stage for an input example. 1: Input: table ∈ RM×C and question 2: Initialize: scalar answer 0 = 0, lookup answer 0 = 0M×C, row select 0 = 1M , history vector 3: Preprocessing: Remove numbers from question and store them in a list along with the words  at time t = 0, h0 = 0d and input to history RNN at time t = 0, c0 = 02d that appear to the left of it. The tokens in the input question are {w1, w2, . . . , wQ}.  4: Question Module: Run question RNN on the preprocessed question to get question represen-  tation q and list of hidden states z1, z2, . . . , zQ  5: Pivot numbers: pivotg and pivotl are computed using hidden states from question RNN and  operation representations U  6: for t = 1, 2, . . . , T do 7: 8: 9: 10:  Compute history vector ht by passing input ct to the history RNN Operation selection using q, ht and operation representations U Data selection on table using q, ht and column representations V Update scalar answert, lookup answert and row select t using the selected operation and  Compute input to the history RNN at time t + 1, ct+1  11: 12: end for 13: Output: scalar answer T or lookup answer T depending on whichever of the two is updated  column  at step T  columns respectively. The elements in the table are uniformly randomly sampled from [-100, 100] and [-200, 200] during training and test time respectively. The number of rows is sampled randomly from [30, 100] in training while during prediction the number of rows is 120. Each question in the test set is unique, i.e., it is generated from a distinct template. We use the following settings: Single Column: We ﬁrst perform experiments with a single column that enables 23 different ques- tion templates which can be answered using 4 time steps. Many Columns: We increase the difﬁculty by experimenting with multiple columns (max columns = 3, 5 or 10). During training, the number of columns is randomly sampled from (1, max columns) and at test time every question had the maximum number of columns used during training. Variability: To simulate one aspect of the difﬁculties in dealing with natural language input, we consider multiple ways to refer to the same operation (Tables 6 and 7). Text Match: Now we consider cases where some columns in the input table contain text entries. We use a small vocabulary of 10 words and ﬁll the column by uniformly randomly sampling from them. In our ﬁrst experiment with text entries, the table always contains two columns, one with text and other with numeric entries (Table 8). In the next experiment, each example can have up to 3 columns containing numeric entries and up to 2 columns containing text entries during training. At test time, all the examples contain 3 columns with numeric entries and 2 columns with text entries.  3.2 MODELS  In the following, we benchmark the performance of Neural Programmer on various versions of the table-comprehension dataset. We slowly increase the difﬁculty of the task by changing the table properties (more columns, mixed numeric and text entries) and question properties (word variabil- ity). After that we discuss a comparison between Neural Programmer, LSTM, and LSTM with Attention.  3.2.1 NEURAL PROGRAMMER  We use 4 time steps in our experiments (T = 4). Neural Programmer is trained with mini-batch stochastic gradient descent with Adam optimizer (Kingma & Ba, 2014). The parameters are ini- tialized uniformly randomly within the range [-0.1, 0.1]. In all experiments, we set the mini-batch size to 50, dimensionality d to 256, the initial learning rate and the momentum hyper-parameters of Adam to their default values (Kingma & Ba, 2014). We found that it is extremely useful to add random Gaussian noise to our gradients at every training step. This acts as a regularizer to the model  10  Published as a conference paper at ICLR 2016  and allows it to actively explore more programs. We use a schedule inspired from Welling & Teh (2011), where at every step we sample a Gaussian of 0 mean and variance= curr step−0.55. To prevent exploding gradients, we perform gradient clipping by scaling the gradient when the norm exceeds a threshold (Graves, 2013). The threshold value is picked from [1, 5, 50]. We tune the (cid:15) hyper-parameter in Adam from [1e-6, 1e-8], the Huber constant δ from [10, 25, 50] and λ (weight between two losses) from [25, 50, 75, 100] using grid search. While performing experiments with multiple random restarts we ﬁnd that the performance of the model is stable with respect to (cid:15) and gradient clipping threshold but we have to tune δ and λ for the different random seeds.  Type Single Column 3 Columns 5 Columns 10 Columns Word Variability on 1 Column Word Variability on 5 Columns Text Match on 2 Columns Text Match on 5 Columns  No. of Test Question Templates Accuracy % seen test 23 307 1231 7900 1368 24000 1125 14600  100.0 99.02 99.11 99.13 96.49 88.99 99.11 98.03  100 100 98.62 62.44 100 31.31 97.42 31.02  Table 2: Summary of the performance of Neural Programmer on various versions of the synthetic table-comprehension task. The prediction of the model is considered correct if it is equal to the correct answer up to the ﬁrst decimal place. The last column indicates the percentage of question templates in the test set that are observed during training. The unseen question templates generate questions containing sequences of words that the model has never seen before. The model can generalize to unseen question templates which is evident in the 10-columns, word variability on 5-columns and text match on 5 columns experiments. This indicates that Neural Programmer is a powerful compositional model since solving unseen question templates requires performing a sequence of actions that it has never done during training.  The training set consists of 50, 000 triples in all our experiments. Table 2 shows the performance of Neural Programmer on synthetic data experiments. In single column experiments, the model answers all questions correctly which we manually verify by inspecting the programs induced by the model. In many columns experiments with 5 columns, we use a bidirectional RNN and for 10 columns we additionally perform attention (Bahdanau et al., 2014) on the question at every time step using the history vector. The model is able to generalize to unseen question templates which are a considerable fraction in our ten columns experiment. This can also be seen in the word variability experiment with 5 columns and text match experiment with 5 columns where more than two-thirds of the test set contains question templates that are unseen during training. This indicates that Neural Programmer is a powerful compositional model since solving unseen question templates requires inducing programs that do not appear during training. Almost all the errors made by the model were on questions that require the difference operation to be used. Table 3 shows examples of how the model selects the operation and column at every time step for three test questions.  Figure 8 shows an example of the effect of adding random noise to the gradients in our experiment with 5 columns.  3.2.2 COMPARISON TO LSTM AND LSTM WITH ATTENTION  We apply a three-layer sequence-to-sequence LSTM recurrent network model (Hochreiter & Schmidhuber, 1997; Sutskever et al., 2014) and LSTM model with attention (Bahdanau et al., 2014). We explore multiple attention heads (1, 5, 10) and try two cases, placing the input table before and after the question. We consider a simpler version of the single column dataset with only questions that have scalar answers. The number of elements in the column is uniformly randomly sampled  11  Published as a conference paper at ICLR 2016  Question  greater 50.32 C and lesser 20.21 E sum H What is the sum of numbers in column H  whose ﬁeld in column C is greater than 50.32 and ﬁeld in Column E is lesser than 20.21. lesser -80.97 D or greater 12.57 B print F  Print elements in column F  whose ﬁeld in column D is lesser than -80.97 or ﬁeld in Column B is greater than 12.57.  sum A diff count  What is the difference  between sum of elements in column A and number of rows  t  1 2 3 4 1 2 3 4 1 2 3 4  Selected  Op  Selected Column  pivotg  pivotl  Greater Lesser And Sum Lesser Greater  Or  Assign Sum Reset Count Diff  C E - H D B - F A - - -  50.32  20.21  12.57  -80.97  -1  -1  Row select  g1 l2  and3 [0]M l1 g2 or3 [0]M [0]M [1]M [0]M [0]M  Table 3: Example outputs from the model for T = 4 time steps on three questions in the test set. We show the synthetically generated question along with its natural language translation. For each question, the model takes 4 steps and at each step selects an operation and a column. The pivot numbers for the comparison operations are computed before performing the 4 steps. We show the selected columns in cases during which the selected operation acts on a particular column.  Figure 8: The effect of adding random noise to the gradients versus not adding it in our experiment with 5 columns when all hyper-parameters are the same. The models trained with noise generalizes almost always better.  from [4, 7] while the elements are sampled from [−10, 10]. The best accuracy using these models is close to 80% in spite of relatively easier questions and supplying fresh training examples at every step. When the scale of the input numbers is changed to [−50, 50] at test time, the accuracy drops to 30%. Neural Programmer solves this task and achieves 100% accuracy using 50, 000 training examples. Since hardmax operation is used at test time, the answers (or the program induced) from Neural Programmer is invariant to the scale of numbers and the length of the input.  4 RELATED WORK  Program induction has been studied in the context of semantic parsing (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Liang et al., 2011) in natural language processing. Pasupat & Liang (2015) develop a semantic parser with a hand engineered grammar for question answering on tables with natural language questions. Methods such as Piantadosi et al. (2008); Eisenstein et al. (2009); Clarke et al. (2010) learn a compositional semantic model without hand engineered compositional grammar, but still requiring a hand labeled lexical mapping of words to the operations. Poon (2013) develop an unsupervised method for semantic parsing, which requires many pre-processing steps  12  050100150200250300No. of epochs100015002000250030003500Train LossTrain Loss: Noise Vs. No Noiseno noisenoise050100150200250300No. of epochs020406080100Test AccuracyTest Accuracy: Noise Vs. No Noiseno noisenoisePublished as a conference paper at ICLR 2016  including dependency parsing and mapping from words to operations. Liang et al. (2010) propose an hierarchical Bayesian approach to learn simple programs. There has been some early work in using neural networks for learning context free grammar (Das et al., 1992a;b; Zeng et al., 1994) and context sensitive grammar (Steijvers, 1996; Gers & Schmid- huber, 2001) for small problems. Neelakantan et al. (2015); Lin et al. (2015) learn simple Horn clauses in a large knowledge base using RNNs. Neural networks have also been used for Q&A on datasets that do not require complicated arithmetic and logic reasoning (Bordes et al., 2014; Iyyer et al., 2014; Sukhbaatar et al., 2015; Peng et al., 2015; Hermann et al., 2015). While there has been lot of work in augmenting neural networks with additional memory (Das et al., 1992a; Schmidhu- ber, 1993; Hochreiter & Schmidhuber, 1997; Graves et al., 2014; Weston et al., 2015; Kumar et al., 2015; Joulin & Mikolov, 2015), we are not aware of any other work that augments a neural network with a set of operations to enhance complex reasoning capabilities. After our work was submitted to ArXiv, Neural Programmer-Interpreters (Reed & Freitas, 2016), a method that learns to induce programs with supervision of the entire program was proposed. This was followed by Neural Enquirer (Yin et al., 2015), which similar to our work tackles the problem of synthetic table QA. However, their method achieves perfect accuracy only when given supervision of the entire program. Later, dynamic neural module network (Andreas et al., 2016) was proposed for question answering which uses syntactic supervision in the form of dependency trees.  5 CONCLUSIONS  We develop Neural Programmer, a neural network model augmented with a small set of arithmetic and logic operations to perform complex arithmetic and logic reasoning. The model can be trained in an end-to-end fashion using backpropagation to induce programs requiring much lesser sophisticated human supervision than prior work. It is a general model for program induction broadly applicable across different domains, data sources and languages. Our experiments indicate that the model is capable of learning with delayed supervision and exhibits powerful compositionality.  Acknowledgements We sincerely thank Greg Corrado, Andrew Dai, Jeff Dean, Shixiang Gu, Andrew McCallum, and Luke Vilnis for their suggestions and the Google Brain team for the support.  REFERENCES Andreas, Jacob, Rohrbach, Marcus, Darrell, Trevor, and Klein, Dan. Learning to compose neural  networks for question answering. ArXiv, 2016.  Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly  learning to align and translate. ICLR, 2014.  Bahdanau, Dzmitry, Chorowski,  End-to-end attention-based large vocabulary speech recognition.  Jan, Serdyuk, Dmitriy, Brakel, Philemon, and Bengio, arXiv preprint  Yoshua. arxiv:1508.04395, 2015.  Bordes, Antoine, Chopra, Sumit, and Weston, Jason. Question answering with subgraph embed-  dings. In EMNLP, 2014.  Cantlon, Jessica F., Brannon, Elizabeth M., Carter, Elizabeth J., and Pelphrey, Kevin A. Functional  imaging of numerical processing in adults and 4-y-old children. PLoS Biology, 2006.  Chan, William, Jaitly, Navdeep, Le, Quoc V., and Vinyals, Oriol. Listen, attend and spell. arXiv  preprint arxiv:1508.01211, 2015.  Clarke, James, Goldwasser, Dan, Chang, Ming-Wei, and Roth, Dan. Driving semantic parsing from  the world’s response. In CoNLL, 2010.  Das, Sreerupa, Giles, C. Lee, and zheng Sun, Guo. Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. In CogSci, 1992a. Das, Sreerupa, Giles, C. Lee, and zheng Sun, Guo. Using prior knowledge in an NNPDA to learn  context-free languages. In NIPS, 1992b.  13  Published as a conference paper at ICLR 2016  Dastjerdi, Mohammad, Ozker, Muge, Foster, Brett L, Rangarajan, Vinitha, and Parvizi, Josef. Nu- merical processing in the human parietal cortex during experimental and natural conditions. Na- ture communications, 4, 2013.  Eisenstein, Jacob, Clarke, James, Goldwasser, Dan, and Roth, Dan. Reading to learn: Constructing  features from semantic abstracts. In EMNLP, 2009.  Fias, Wim, Lammertyn, Jan, Caessens, Bernie, and Orban, Guy A. Processing of abstract ordinal knowledge in the horizontal segment of the intraparietal sulcus. The Journal of Neuroscience, 2007.  Gers, Felix A. and Schmidhuber, J¨urgen. LSTM recurrent networks learn simple context free and  context sensitive languages. IEEE Transactions on Neural Networks, 2001.  Graves, Alex.  Generating sequences with recurrent neural networks.  arxiv:1308.0850, 2013.  arXiv preprint  Graves, Alex and Jaitly, Navdeep. Towards end-to-end speech recognition with recurrent neural  networks. In ICML, 2014.  Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural Turing Machines.  arxiv:1410.5401, 2014.  arXiv preprint  Hannun, Awni Y., Case, Carl, Casper, Jared, Catanzaro, Bryan C., Diamos, Greg, Elsen, Erich, Prenger, Ryan, Satheesh, Sanjeev, Sengupta, Shubho, Coates, Adam, and Ng, Andrew Y. Deep Speech: Scaling up end-to-end speech recognition. arXiv preprint arxiv:1412.5567, 2014.  Hermann, Karl Moritz, Kocisk´y, Tom´as, Grefenstette, Edward, Espeholt, Lasse, Kay, Will, Suley-  man, Mustafa, and Blunsom, Phil. Teaching machines to read and comprehend. NIPS, 2015.  Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George, rahman Mohamed, Abdel, Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara, and Kingsbury, Brian. Deep neural networks for acoustic modeling in speech recognition. Signal Processing Magazine, 2012.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural Computation, 1997.  Huber, Peter. Robust estimation of a location parameter. In The Annals of Mathematical Statistics,  1964.  Iyyer, Mohit, Boyd-Graber, Jordan L., Claudino, Leonardo Max Batista, Socher, Richard, and III, Hal Daum´e. A neural network for factoid question answering over paragraphs. In EMNLP, 2014.  Joulin, Armand and Mikolov, Tomas. Inferring algorithmic patterns with stack-augmented recurrent  nets. NIPS, 2015.  Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. ICLR, 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, 2012.  Kucian, Karin, Loenneker, Thomas, Dietrich, Thomas, Dosch, Mengia, Martin, Ernst, and Von Aster, Michael. Impaired neural networks for approximate calculation in dyscalculic chil- dren: a functional mri study. Behavioral and Brain Functions, 2006.  Kumar, Ankit, Irsoy, Ozan, Su, Jonathan, Bradbury, James, English, Robert, Pierce, Brian, On- druska, Peter, Gulrajani, Ishaan, and Socher, Richard. Ask me anything: Dynamic memory net- works for natural language processing. ArXiv, 2015.  Liang, Percy, Jordan, Michael I., and Klein, Dan. Learning programs: A hierarchical Bayesian  approach. In ICML, 2010.  Liang, Percy, Jordan, Michael I., and Klein, Dan. Learning dependency-based compositional se-  mantics. In ACL, 2011.  14  Published as a conference paper at ICLR 2016  Lin, Yankai, Liu, Zhiyuan, Luan, Huan-Bo, Sun, Maosong, Rao, Siwei, and Liu, Song. Modeling  relation paths for representation learning of knowledge bases. In EMNLP, 2015.  Luong, Thang, Sutskever, Ilya, Le, Quoc V., Vinyals, Oriol, and Zaremba, Wojciech. Addressing  the rare word problem in neural machine translation. ACL, 2014.  Neelakantan, Arvind, Roth, Benjamin, and McCallum, Andrew. Compositional vector space models  for knowledge base completion. In ACL, 2015.  Neelakantan, Arvind, Vilnis, Luke, Le, Quoc V., Sutskever, Ilya, Kaiser, Lukasz, Kurach, Karol, ICLR  and Martens, James. Adding gradient noise improves learning for very deep networks. Workshop, 2016.  Pasupat, Panupong and Liang, Percy. Compositional semantic parsing on semi-structured tables. In  ACL, 2015.  Peng, Baolin, Lu, Zhengdong, Li, Hang, and Wong, Kam-Fai. Towards neural network-based rea-  soning. arXiv preprint arxiv:1508.05508, 2015.  Piantadosi, Steven T., Goodman, N.D., Ellis, B.A., and Tenenbaum, J.B. A Bayesian model of the  acquisition of compositional semantics. In CogSci, 2008.  Piazza, Manuela, Izard, Veronique, Pinel, Philippe, Le Bihan, Denis, and Dehaene, Stanislas. Tuning  curves for approximate numerosity in the human intraparietal sulcus. Neuron, 2004.  Poon, Hoifung. Grounded unsupervised semantic parsing. In ACL, 2013.  Reed, Scott and Freitas, Nando De. Neural programmer-interpreters. ICLR, 2016.  Schmidhuber, J. A self-referentialweight matrix. In ICANN, 1993.  Shang, Lifeng, Lu, Zhengdogn, and Li, Hang. Neural responding machine for short-text conversa-  tion. arXiv preprint arXiv:1503.02364, 2015.  Steijvers, Mark. A recurrent network that performs a context-sensitive prediction task. In CogSci,  1996.  Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and Fergus, Rob. End-to-end memory net-  works. arXiv preprint arXiv:1503.08895, 2015.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural net-  works. In NIPS, 2014.  Vinyals, Oriol and Le, Quoc V. A neural conversational model. ICML DL Workshop, 2015.  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural  image caption generator. In CVPR, 2015.  Von Neumann, John. First draft of a report on the EDVAC. Technical report, 1945.  Wang, Yushi, Berant, Jonathan, and Liang, Percy. Building a semantic parser overnight. In ACL,  2015.  Welling, Max and Teh, Yee Whye. Bayesian learning via stochastic gradient Langevin dynamics. In  ICML, 2011.  Werbos, P. Backpropagation through time: what does it do and how to do it. In Proceedings of  IEEE, 1990.  Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Memory Networks. 2015.  Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun, Courville, Aaron C., Salakhutdinov, Ruslan, Zemel, Richard S., and Bengio, Yoshua. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.  15  Published as a conference paper at ICLR 2016  Yin, Pengcheng, Lu, Zhengdong, Li, Hang, and Kao, Ben. Neural enquirer: Learning to query tables  with natural language. ArXiv, 2015.  Zelle, John M. and Mooney, Raymond J. Learning to parse database queries using inductive logic  programming. In AAAI/IAAI, 1996.  Zeng, Z., Goodman, R., and Smyth, P. Discrete recurrent neural networks for grammatical inference.  IEEE Transactions on Neural Networks, 1994.  Zettlemoyer, Luke S. and Collins, Michael. Learning to map sentences to logical form: Structured  classiﬁcation with probabilistic categorial grammars. In UAI, 2005.  16  Published as a conference paper at ICLR 2016  APPENDIX  sum count print greater [number] sum lesser [number] sum greater [number] count lesser [number] count greater [number] print lesser [number] print greater [number1] and lesser [number2] sum lesser [number1] and greater [number2] sum greater [number1] or lesser [number2] sum lesser [number1] or greater [number2] sum greater [number1] and lesser [number2] count lesser [number1] and greater [number2] count greater [number1] or lesser [number2] count lesser [number1] or greater [number2] count greater [number1] and lesser [number2] print lesser [number1] and greater [number2] print greater [number1] or lesser [number2] print lesser [number1] or greater [number2] print sum diff count count diff sum  Table 4: 23 question templates for single column experiment. We have four categories of questions: 1) simple aggregation (sum, count) 2) comparison (greater, lesser) 3) logic (and, or) and, 4) arith- metic (diff). We ﬁrst sample the categories uniformly randomly and each program within a category is equally likely. In the word variability experiment with 5 columns we sampled from the set of all programs uniformly randomly since greater than 90% of the test questions were unseen during training using the other procedure.  greater [number1] A and lesser [number2] A sum A greater [number1] B and lesser [number2] B sum B greater [number1] A and lesser [number2] A sum B greater [number1] A and lesser [number2] B sum A greater [number1] B and lesser [number2] A sum A greater [number1] A and lesser [number2] B sum B greater [number1] B and lesser [number2] B sum A greater [number1] B and lesser [number2] B sum A  Table 5: 8 question templates of type “greater [number1] and lesser [number2] sum” when there are 2 columns.  sum count greater lesser assign difference  sum, total, total of, sum of count, count of, how many greater, greater than, bigger, bigger than, larger, larger than lesser, lesser than, smaller, smaller than, under print, display, show difference, difference between  Table 6: Word variability, multiple ways to refer to the same operation.  17  Published as a conference paper at ICLR 2016  greater [number] sum greater [number] total greater [number] total of greater [number] sum of greater than [number] sum greater than [number] total greater than [number] total of greater than [number] sum of bigger [number] sum bigger [number] total bigger [number] total of bigger [number] sum of bigger than [number] sum bigger than [number] total bigger than [number] total of bigger than [number] sum of larger [number] sum larger [number] total larger [number] total of larger [number] sum of larger than [number] sum larger than [number] total larger than [number] total of larger than [number] sum of  Table 7: 24 questions templates for questions of type “greater [number] sum” in the single column word variability experiment.  word:0 A sum B word:1 A sum B word:2 A sum B word:3 A sum B word:4 A sum B word:5 A sum B word:6 A sum B word:7 A sum B word:8 A sum B word:9 A sum B  Table 8: 10 questions templates for questions of type “[word] A sum B” in the two columns text match experiment.  18  ",
1511.06051,2016,SparkNet: Training Deep Networks in Spark,"['SparkNet: Training Deep Networks in Spark\nPhilipp Moritz', 'Robert Nishihara', 'Ion Stoica', 'Michael Jordan']",https://arxiv.org/pdf/1511.06051,"6 1 0 2     b e F 8 2         ] L M  . t a t s [      4 v 1 5 0 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  SPARKNET: TRAINING DEEP NETWORKS IN SPARK  Philipp Moritz∗, Robert Nishihara∗, Ion Stoica, Michael I. Jordan Electrical Engineering and Computer Science University of California Berkeley, CA 94720, USA {pcmoritz,rkn,istoica,jordan}@eecs.berkeley.edu  ABSTRACT  Training deep networks is a time-consuming process, with networks for ob- ject recognition often requiring multiple days to train. For this reason, lever- aging the resources of a cluster to speed up training is an important area of work. However, widely-popular batch-processing computational frameworks like MapReduce and Spark were not designed to support the asynchronous and communication-intensive workloads of existing distributed deep learning systems. We introduce SparkNet, a framework for training deep networks in Spark. Our im- plementation includes a convenient interface for reading data from Spark RDDs, a Scala interface to the Caffe deep learning framework, and a lightweight multi- dimensional tensor library. Using a simple parallelization scheme for stochastic gradient descent, SparkNet scales well with the cluster size and tolerates very high-latency communication. Furthermore, it is easy to deploy and use with no parameter tuning, and it is compatible with existing Caffe models. We quantify the dependence of the speedup obtained by SparkNet on the number of machines, the communication frequency, and the cluster’s communication overhead, and we benchmark our system’s performance on the ImageNet dataset.  1  INTRODUCTION  Deep learning has advanced the state of the art in a number of application domains. Many of the recent advances involve ﬁtting large models (often several hundreds megabytes) to larger datasets (often hundreds of gigabytes). Given the scale of these optimization problems, training can be time- consuming, often requiring multiple days on a single GPU using stochastic gradient descent (SGD). For this reason, much effort has been devoted to leveraging the computational resources of a cluster to speed up the training of deep networks (and more generally to perform distributed optimization). Many attempts to speed up the training of deep networks rely on asynchronous, lock-free optimiza- tion (Dean et al., 2012; Chilimbi et al., 2014). This paradigm uses the parameter server model (Li et al., 2014; Ho et al., 2013), in which one or more master nodes hold the latest model parameters in memory and serve them to worker nodes upon request. The nodes then compute gradients with respect to these parameters on a minibatch drawn from the local data shard. These gradients are shipped back to the server, which updates the model parameters. At the same time, batch-processing frameworks enjoy widespread usage and have been gaining in popularity. Beginning with MapReduce (Dean & Ghemawat, 2008), a number of frameworks for distributed computing have emerged to make it easier to write distributed programs that leverage the resources of a cluster (Zaharia et al., 2010; Isard et al., 2007; Murray et al., 2013). These frameworks have greatly simpliﬁed many large-scale data analytics tasks. However, state-of-the-art deep learning systems rely on custom implementations to facilitate their asynchronous, communication-intensive workloads. One reason is that popular batch-processing frameworks (Dean & Ghemawat, 2008; Zaharia et al., 2010) are not designed to support the workloads of existing deep learning systems. SparkNet implements a scalable, distributed algorithm for training deep networks that lends itself to batch computational frameworks such as MapReduce and Spark and works well out-of-the-box in bandwidth-limited environments.  ∗Both authors contributed equally.  1  Published as a conference paper at ICLR 2016  Figure 1: This ﬁgure depicts the SparkNet architecture.  The beneﬁts of integrating model training with existing batch frameworks are numerous. Much of the difﬁculty of applying machine learning has to do with obtaining, cleaning, and processing data as well as deploying models and serving predictions. For this reason, it is convenient to integrate model training with the existing data-processing pipelines that have been engineered in today’s distributed computational environments. Furthermore, this approach allows data to be kept in memory from start to ﬁnish, whereas a segmented approach requires writing to disk between operations. If a user wishes to train a deep network on the output of a SQL query or on the output of a graph computation and to feed the resulting predictions into a distributed visualization tool, this can be done conveniently within a single computational framework. We emphasize that the hardware requirements of our approach are minimal. Whereas many ap- proaches to the distributed training of deep networks involve heavy communication (often com- municating multiple gradient vectors for every minibatch), our approach gracefully handles the bandwidth-limited setting while also taking advantage of clusters with low-latency communication. For this reason, we can easily deploy our algorithm on clusters that are not optimized for com- munication. Our implementation works well out-of-the box on a ﬁve-node EC2 cluster in which broadcasting and collecting model parameters (several hundred megabytes per worker) takes on the order of 20 seconds, and performing a single minibatch gradient computation requires about 2 sec- onds (for AlexNet). We achieve this by providing a simple algorithm for parallelizing SGD that involves minimal communication and lends itself to straightforward implementation in batch com- putational frameworks. Our goal is not to outperform custom computational frameworks but rather to propose a system that can be easily implemented in popular batch frameworks and that performs nearly as well as what can be accomplished with specialized frameworks.  2  IMPLEMENTATION  Here we describe our implementation of SparkNet. SparkNet builds on Apache Spark (Zaharia et al., 2010) and the Caffe deep learning library (Jia et al., 2014). In addition, we use Java Native Access  class Net {  def Net(netParams: NetParams): Net def setTrainingData(data: Iterator[(NDArray,Int)]) def setValidationData(data: Iterator[(NDArray,Int)]) def train(numSteps: Int) def test(numSteps: Int): Float def setWeights(weights: WeightCollection) def getWeights(): WeightCollection  }  Listing 1: SparkNet API  2  Published as a conference paper at ICLR 2016  val netParams = NetParams(  RDDLayer(""data"", shape=List(batchsize, 1, 28, 28)), RDDLayer(""label"", shape=List(batchsize, 1)), ConvLayer(""conv1"", List(""data""), kernel=(5,5), numFilters=20), PoolLayer(""pool1"", List(""conv1""), pool=Max, kernel=(2,2), stride=(2,2)), ConvLayer(""conv2"", List(""pool1""), kernel=(5,5), numFilters=50), PoolLayer(""pool2"", List(""conv2""), pool=Max, kernel=(2,2), stride=(2,2)), LinearLayer(""ip1"", List(""pool2""), numOutputs=500), ActivationLayer(""relu1"", List(""ip1""), activation=ReLU), LinearLayer(""ip2"", List(""relu1""), numOutputs=10), SoftmaxWithLoss(""loss"", List(""ip2"", ""label""))  )  Listing 2: Example network speciﬁcation in SparkNet  var trainData = loadData(...) var trainData = preprocess(trainData).cache() var nets = trainData.foreachPartition(data => {  var net = Net(netParams) net.setTrainingData(data) net)  var weights = initialWeights(...) for (i <- 1 to 1000) {  var broadcastWeights = broadcast(weights) nets.map(net => net.setWeights(broadcastWeights.value)) weights = nets.map(net => {  net.train(50) net.getWeights()}).mean() // an average of WeightCollection objects  }  Listing 3: Distributed training example  for accessing Caffe data and weights natively from Scala, and we use the Java implementation of Google Protocol Buffers to allow the dynamic construction of Caffe networks at runtime. The Net class wraps Caffe and exposes a simple API containing the methods shown in Listing 1. The NetParams type speciﬁes a network architecture, and the WeightCollection type is a map from layer names to lists of weights. It allows the manipulation of network components and the storage of weights and outputs for individual layers. To facilitate manipulation of data and weights without copying memory from Caffe, we implement the NDArray class, which is a lightweight multi-dimensional tensor library. One beneﬁt of building on Caffe is that any existing Caffe model deﬁnition or solver ﬁle is automatically compatible with SparkNet. There is a large community developing Caffe models and extensions, and these can easily be used in SparkNet. By building on top of Spark, we inherit the advantages of modern batch computational frameworks. These include the high-throughput loading and preprocessing of data and the ability to keep data in memory between operations. In Listing 2, we give an example of how network architectures can be speciﬁed in SparkNet. In addition, model speciﬁcations or weights can be loaded directly from Caffe ﬁles. An example sketch of code that uses our API to perform distributed training is given in Listing 3.  2.1 PARALLELIZING SGD  To perform well in bandwidth-limited environments, we recommend a parallelization scheme for SGD that requires minimal communication. This approach is not speciﬁc to SGD. Indeed, SparkNet works out of the box with any Caffe solver.  3  Published as a conference paper at ICLR 2016  The parallelization scheme is described in Listing 3. Spark consists of a single master node and a number of worker nodes. The data is split among the Spark workers. In every iteration, the Spark master broadcasts the model parameters to each worker. Each worker then runs SGD on the model with its subset of data for a ﬁxed number of iterations τ (we use τ = 50 in Listing 3) or for a ﬁxed length of time, after which the resulting model parameters on each worker are sent to the master and averaged to form the new model parameters. We recommend initializing the network by running SGD for a small number of iterations on the master. A similar and more sophisticated approach to parallelizing SGD with minimal communication overhead is discussed in Zhang et al. (2015). The standard approach to parallelizing each gradient computation requires broadcasting and collect- ing model parameters (hundreds of megabytes per worker and gigabytes in total) after every SGD update, which occurs tens of thousands of times during training. On our EC2 cluster, each broadcast and collection takes about twenty seconds, putting a bound on the speedup that can be expected using this approach without better hardware or without partitioning models across machines. Our approach broadcasts and collects the parameters a factor of τ times less for the same number of iterations. In our experiments, we set τ = 50, but other values seem to work about as well. We note that Caffe supports parallelism across multiple GPUs within a single node. This is not a competing form of parallelism but rather a complementary one. In some of our experiments, we use Caffe to handle parallelism within a single node, and we use the parallelization scheme described in Listing 3 to handle parallelism across nodes.  3 EXPERIMENTS  In Section 3.2, we will benchmark the performance of SparkNet and measure the speedup that our system obtains relative to training on a single node. However, the outcomes of those experiments depend on a number of different factors. In addition to τ (the number of iterations between syn- chronizations) and K (the number of machines in our cluster), they depend on the communication overhead in our cluster S. In Section 3.1, we ﬁnd it instructive to measure the speedup in the ideal- ized case of zero communication overhead (S = 0). This idealized model gives us an upper bound on the maximum speedup that we could hope to obtain in a real-world cluster, and it allows us to build a model for the speedup as a function of S (the overhead is easily measured in practice).  3.1 THEORETICAL CONSIDERATIONS  Before benchmarking our system, we determine the maximum possible speedup that could be ob- tained in principle in a cluster with no communication overhead. We determine the dependence of this speedup on the parameters τ (the number of iterations between synchronizations) and K (the number of machines in our cluster).  3.1.1 LIMITATIONS OF NAIVE PARALLELIZATION  To begin with, we consider the theoretical limitations of a naive parallelism scheme which paral- lelizes SGD by distributing each minibatch computation over multiple machines (see Figure 2b). Let Na(b) be the number of serial iterations of SGD required to obtain an accuracy of a when train- ing with a batch size of b (when we say accuracy, we are referring to test accuracy). Suppose that computing the gradient over a batch of size b requires C(b) units of time. Then the running time required to achieve an accuracy of a with serial training is  Na(b)C(b).  (1) A naive parallelization scheme attempts to distribute the computation at each iteration by dividing each minibatch between the K machines, computing the gradients separately, and aggregating the results on one node. Under this scheme, the cost of the computation done on a single node in a single iteration is C(b/K) and satisﬁes C(b/K) ≥ C(b)/K (the cost is sublinear in the batch size). In a system with no communication overhead and no overhead for summing the gradients, this approach could in principle achieve an accuracy of a in time Na(b)C(b)/K. This represents a linear speedup in the number of machines (for values of K up to the batch size b). In practice, there are several important considerations. First, for the approximation C(b/K) ≈ C(b)/K to hold, K must be much smaller than b, limiting the number of machines we can use to  4  Published as a conference paper at ICLR 2016  effectively parallelize the minibatch computation. One might imagine circumventing this limitation by using a larger batch size b. Unfortunately, the beneﬁt of using larger batches is relatively modest. As the batch size b increases, Na(b) does not decrease enough to justify the use of a very large value of b. Furthermore, the beneﬁts of this approach depend greatly on the degree of communication overhead. If aggregating the gradients and broadcasting the model parameters requires S units of time, then the time required by this approach is at least C(b)/K + S per iteration and Na(b)(C(b)/K + S) to achieve an accuracy of a. Therefore, the maximum achievable speedup is C(b)/(C(b)/K + S) ≤ C(b)/S. We may expect S to increase modestly as K increases, but we suppress this effect here.  3.1.2 LIMITATIONS OF SPARKNET PARALLELIZATION  The performance of the naive parallelization scheme is easily understood because its behavior is equivalent to that of the serial algorithm. In contrast, SparkNet uses a parallelization scheme that is not equivalent to serial SGD (described in Section 2.1), and so its analysis is more complex. SparkNet’s parallelization scheme proceeds in rounds (see Figure 2c). In each round, each machine runs SGD for τ iterations with batch size b. Between rounds, the models on the workers are gathered together on the master, averaged, and broadcast to the workers. We use Ma(b, K, τ ) to denote the number of rounds required to achieve an accuracy of a. The number of parallel iterations of SGD under SparkNet’s parallelization scheme required to achieve an accuracy of a is then τ Ma(b, K, τ ), and the wallclock time is  (τ C(b) + S)Ma(b, K, τ ),  (2)  where S is the time required to gather and broadcast model parameters. To measure the sensitivity of SparkNet’s parallelization scheme to the parameters τ and K, we consider a grid of values of K and τ. For each pair of parameters, we run SparkNet using a modiﬁed version of AlexNet on a subset of ImageNet (the ﬁrst 100 classes each with approximately 1000 data points) for a total of 20000 parallel iterations. For each of these training runs, we compute the ratio τ Ma(b, K, τ )/Na(b). This is the speedup achieved relative to training on a single machine when S = 0. In Figure 3, we plot a heatmap of the speedup given by the SparkNet parallelization scheme under different values of τ and K. Figure 3 exhibits several trends. The top row of the heatmap corresponds to the case K = 1, where we use only one worker. Since we do not have multiple workers to synchronize when K = 1, the number of iterations τ between synchronizations does not matter, so all of the squares in the top row of the grid should behave similarly and should exhibit a speedup factor of 1 (up to randomness in the optimization). The rightmost column of each heatmap corresponds to the case τ = 1, where we synchronize after every iteration of SGD. This is equivalent to running serial SGD with a batch size of Kb, where b is the batchsize on each worker (in these experiments we use b = 100). In this column, the speedup should increase sublinearly with K. We note that it is slightly surprising that the speedup does not increase monotonically from left to right as τ decreases. Intuitively, we might expect more synchronization to be strictly better (recall we are disregarding the overhead due to synchronization). However, our experiments suggest that modest delays between synchronizations can be beneﬁcial. This experiment capture the speedup that we can expect from the SparkNet parallelization scheme in the case of zero communication overhead (the numbers are dataset speciﬁc, but the trends are of interest). Having measured these numbers, it is straightforward to compute the speedup that we can expect as a function of the communication overhead. In Figure 4, we plot the speedup expected both from naive parallelization and from SparkNet on a ﬁve-node cluster as a function of S (normalized so that C(b) = 1). As expected, naive paral- lelization gives a maximum speedup of 5 (on a ﬁve-node cluster) when there is zero communication overhead (note that our plot does not go all the way to S = 0), and it gives no speedup when the communication overhead is comparable to or greater than the cost of a minibatch computation. In contrast, SparkNet gives a relatively consistent speedup even when the communication overhead is 100 times the cost of a minibatch computation.  5  Published as a conference paper at ICLR 2016  (a) This ﬁgure depicts a serial run of SGD. Each block corresponds to a single SGD update with batch size b. The quantity Na(b) is the number of iterations required to achieve an accuracy of a.  (b) This ﬁgure depicts a parallel run of SGD on K = 4 machines under a naive parallelization scheme. At each iteration, each batch of size b is divided among the K machines, the gradients over the subsets are computed separately on each machine, the updates are aggregated, and the new model is broadcast to the workers. Algorithmically, this approach is exactly equivalent to the serial run of SGD in Figure 2a and so the number of iterations required to achieve an accuracy of a is the same value Na(b).  (c) This ﬁgure depicts a parallel run of SGD on K = 4 machines under SparkNet’s parallelization scheme. At each step, each machine runs SGD with batch size b for τ iterations, after which the models are aggregated, averaged, and broadcast to the workers. The quantity Ma(b, K, τ ) is the number of rounds (of τ iterations) required to obtain an accuracy of a. The total number of parallel iterations of SGD under SparkNet’s parallelization scheme required to obtain an accuracy of a is then τ Ma(b, K, τ ).  Figure 2: Computational models for different parallelization schemes.  The speedup given by the naive parallelization scheme can be computed exactly and is given by C(b)/(C(b)/K +S). This formula is essentially Amdahl’s law. Note that when S ≥ C(b), the naive parallelization scheme is slower than the computation on a single machine. The speedup obtained by SparkNet is Na(b)C(b)/[(τ C(b) + S)Ma(b, K, τ )] for a speciﬁc value of τ. The numerator is the time required by serial SGD to achieve an accuracy of a from Equation 1, and the denominator is the time required by SparkNet to achieve the same accuracy from Equation 2. Choosing the optimal value of τ gives us a speedup of maxτ Na(b)C(b)/[(τ C(b) + S)Ma(b, K, τ )]. In practice, choosing τ is not a difﬁcult problem. The ratio Na(b)/(τ Ma(b, K, τ )) (the speedup when S = 0) degrades  6  Published as a conference paper at ICLR 2016  Figure 3: This ﬁgure shows the speedup τ Ma(b, τ, K)/Na(b) given by SparkNet’s parallelization scheme relative to training on a single machine to obtain an accuracy of a = 20%. Each grid square corresponds to a different choice of K and τ. We show the speedup in the zero communication overhead setting. This experiment uses a modiﬁed version of AlexNet on a subset of ImageNet (100 classes each with approximately 1000 images). Note that these numbers are dataset speciﬁc. Nevertheless, the trends they capture are of interest.  Figure 4: This ﬁgure shows the speedups obtained by the naive parallelization scheme and by SparkNet as a function of the cluster’s communication overhead (normalized so that C(b) = 1). We consider K = 5. The data for this plot applies to training a modiﬁed version of AlexNet on a subset of ImageNet (approximately 1000 images for each of the ﬁrst 100 classes). The speedup obtained by the naive parallelization scheme is C(b)/(C(b)/K + S). The speedup obtained by SparkNet is Na(b)C(b)/[(τ C(b) + S)Ma(b, K, τ )] for a speciﬁc value of τ. The numerator is the time required by serial SGD to achieve an accuracy of a, and the denominator is the time required by SparkNet to achieve the same accuracy (see Equation 1 and Equation 2). For the optimal value of τ, the speedup is maxτ Na(b)C(b)/[(τ C(b) + S)Ma(b, K, τ )]. To plot the SparkNet speedup curve, we maximize over the set of values τ ∈ {1, 2, 5, 10, 25, 100, 500, 1000, 2500} and use the values Ma(b, K, τ ) and Na(b) from the experiments in the ﬁfth row of Figure 3. In our experiments, we have S ≈ 20s and C(b) ≈ 2s.  slowly as τ increases, so it sufﬁces to choose τ to be a small multiple of S (say 5S) so that the algorithm spends only a fraction of its time in communication. When plotting the SparkNet speedup in Figure 4, we do not maximize over all positive integer values of τ but rather over the set τ ∈ {1, 2, 5, 10, 25, 100, 500, 1000, 2500}, and we use the values  7  250010005001002510521τ654321K1.11.71.71.82.62.83.02.42.01.11.61.91.92.43.03.02.41.91.21.71.81.92.22.42.62.11.91.31.41.92.12.12.12.22.61.71.41.61.61.81.62.02.01.81.61.21.11.31.21.31.21.10.81.3speeduptoaccuracy20%1.001.251.501.752.002.252.502.753.0010−210−1100101102103communicationoverheadS012345speedupNaiveSparkNetNoSpeedupPublished as a conference paper at ICLR 2016  Figure 5: This ﬁgure shows the performance of SparkNet on a 3-node, 5-node, and 10-node cluster, where each node has 1 GPU. In these experiments, we use τ = 50. The baseline was obtained by running Caffe on a single GPU with no communication. The experiments are performed on ImageNet using AlexNet.  Figure 6: This ﬁgure shows the performance of SparkNet on a 3-node cluster and on a 6-node cluster, where each node has 4 GPUs. In these experiments, we use τ = 50. The baseline uses Caffe on a single node with 4 GPUs and no communication overhead. The experiments are performed on ImageNet using GoogLeNet.  of Na(b) and Ma(b, K, τ ) corresponding to the ﬁfth row of Figure 3. Including more values of τ would only increase the SparkNet speedup. The distributed training of deep networks is typically thought of as a communication-intensive procedure. However, Figure 4 demonstrates the value of SparkNet’s parallelization scheme even in the most bandwidth-limited settings. The naive parallelization scheme may appear to be a straw man. However, it is a frequently-used ap- proach to parallelizing SGD (Noel et al., 2015; Iandola et al., 2015), especially when asynchronous updates are not an option (as in computational frameworks like MapReduce and Spark).  3.2 TRAINING BENCHMARKS  To explore the scaling behavior of our algorithm and implementation, we perform experiments on EC2 using clusters of g2.8xlarge nodes. Each node has four NVIDIA GRID GPUs and 60GB memory. We train the default Caffe model of AlexNet (Krizhevsky et al., 2012) on the ImageNet dataset (Russakovsky et al., 2015). We run SparkNet with K = 3, 5, and 10 and plot the results in Figure 5. For comparison, we also run Caffe on the same cluster with a single GPU and no communication overhead to obtain the K = 1 plot. These experiments use only a single GPU on each node. To measure the speedup, we compare the wall-clock time required to obtain an accuracy of 45%. With 1 GPU and no communication overhead, this takes 55.6 hours. With 3, 5, and 10 GPUs, SparkNet takes 22.9, 14.5, and 12.8 hours, giving speedups of 2.4, 3.8, and 4.4. We also train the default Caffe model of GoogLeNet (Szegedy et al., 2015) on ImageNet. We run SparkNet with K = 3 and K = 6 and plot the results in Figure 6. In these experiments, we use Caffe’s multi-GPU support to take advantage of all four GPUs within each node, and we use SparkNet’s parallelization scheme to handle parallelism across nodes. For comparison, we train Caffe on a single node with four GPUs and no communication overhead. To measure the speedup, we compare the wall-clock time required to obtain an accuracy of 40%. Relative to the baseline of Caffe with four GPUs, SparkNet on 3 and 6 nodes gives speedups of 2.7 and 3.2. Note that this is on top of the speedup of roughly 3.5 that Caffe with four GPUs gets over Caffe with one GPU, so the speedups that SparkNet obtains over Caffe on a single GPU are roughly 9.4 and 11.2. Furthermore, we explore the dependence of the parallelization scheme described in Section 2.1 on the parameter τ which determines the number of iterations of SGD that each worker does before synchronizing with the other workers. These results are shown in Figure 7. Note that in the presence of stragglers, it sufﬁces to replace the ﬁxed number of iterations τ with a ﬁxed length of time, but in our experimental setup, the timing was sufﬁciently consistent and stragglers did not arise. The single GPU experiment in Figure 5 was trained on a single GPU node with no communication overhead.  8  05101520hours051015202530354045accuracyCaffeSparkNet 3 nodeSparkNet 5 nodeSparkNet 10 node020406080100120hours0102030405060accuracyCaffe 4 GPUSparkNet 3 node 4 GPUSparkNet 6 node 4 GPUPublished as a conference paper at ICLR 2016  Figure 7: This ﬁgure shows the dependence of the parallelization scheme described in Section 2.1 on τ. Each experiment was run with K = 5 workers. This ﬁgure shows that good performance can be achieved without collecting and broadcasting the model after every SGD update.  4 RELATED WORK  Much work has been done to build distributed frameworks for training deep networks. Coates et al. (2013) build a model-parallel system for training deep networks on a GPU cluster using MPI over Inﬁniband. Dean et al. (2012) build DistBelief, a distributed system capable of training deep net- works on thousands of machines using stochastic and batch optimization procedures. In particular, they highlight asynchronous SGD and batch L-BFGS. Distbelief exploits both data parallelism and model parallelism. Chilimbi et al. (2014) build Project Adam, a system for training deep networks on hundreds of machines using asynchronous SGD. Li et al. (2014); Ho et al. (2013) build parameter servers to exploit model and data parallelism, and though their systems are better suited to sparse gradient updates, they could very well be applied to the distributed training of deep networks. More recently, Abadi et al. (2015) build TensorFlow, a sophisticated system for training deep networks and more generally for specifying computation graphs and performing automatic differentiation. Iandola et al. (2015) build FireCaffe, a data-parallel system that achieves impressive scaling using naive parallelization in the high-performance computing setting. They minimize communication overhead by using a tree reduce for aggregating gradients in a supercomputer with Cray Gemini interconnects. These custom systems have numerous advantages including high performance, ﬁne-grained control over scheduling and task placement, and the ability to take advantage of low-latency communication between machines. On the other hand, due to their demanding communication requirements, they are unlikely to exhibit the same scaling on an EC2 cluster. Furthermore, due to their nature as custom systems, they lack the beneﬁts of tight integration with general-purpose computational frameworks such as Spark. For some of these systems, preprocessing must be done separately by a MapReduce style framework, and data is written to disk between segments of the pipeline. With SparkNet, preprocessing and training are both done in Spark. Training a machine learning model such as a deep network is often one step of many in real-world data analytics pipelines (Sparks et al., 2015). Obtaining, cleaning, and preprocessing the data are often expensive operations, as is transferring data between systems. Training data for a machine learning model may be derived from a streaming source, from a SQL query, or from a graph com- putation. A user wishing to train a deep network in a custom system on the output of a SQL query would need a separate SQL engine. In SparkNet, training a deep network on the output of a SQL query, or a graph computation, or a streaming data source is straightforward due to its general pur- pose nature and its support for SQL, graph computations, and data streams (Armbrust et al., 2015; Gonzalez et al., 2014; Zaharia et al., 2013). Some attempts have been made to train deep networks in general-purpose computational frame- works, however, existing work typically hinges on extremely low-latency intra-cluster communica-  9  0246810hours051015202530354045accuracy20 iterations50 iterations100 iterations150 iterationsPublished as a conference paper at ICLR 2016  tion. Noel et al. (2015) train deep networks in Spark on top of YARN using SGD and leverage cluster resources to parallelize the computation of the gradient over each minibatch. To achieve competitive performance, they use remote direct memory accesses over Inﬁniband to exchange model parameters quickly between GPUs. In contrast, SparkNet tolerates low-bandwidth intra-cluster communication and works out of the box on Amazon EC2. A separate line of work addresses speeding up the training of deep networks using single-machine parallelism. For example, Caffe con Troll (Abuzaid et al., 2015) modiﬁes Caffe to leverage both CPU and GPU resources within a single node. These approaches are compatible with SparkNet and the two can be used in conjunction. Many popular computational frameworks provide support for training machine learning models (Meng et al., 2015) such as linear models and matrix factorization models. However, due to the demanding communication requirements and the larger scale of many deep learning problems, these libraries have not been extended to include deep networks. Various authors have studied the theory of averaging separate runs of SGD. In the bandwidth-limited setting, Zinkevich et al. (2010) analyze a simple algorithm for convex optimization that is easily implemented in the MapReduce framework and can tolerate high-latency communication between machines. Zhang et al. (2015) deﬁne a parallelization scheme that penalizes divergences between parallel workers, and they provide an analysis in the convex case. Zhang & Jordan (2015) pro- pose a general abstraction for parallelizing stochastic optimization algorithms along with a Spark implementation.  5 DISCUSSION  We have described an approach to distributing the training of deep networks in communication- limited environments that lends itself to an implementation in batch computational frameworks like MapReduce and Spark. We provide SparkNet, an easy-to-use deep learning implementation for Spark that is based on Caffe and enables the easy parallelization of existing Caffe models with minimal modiﬁcation. As machine learning increasingly depends on larger and larger datasets, integration with a fast and general engine for big data processing such as Spark allows researchers and practitioners to draw from a rich ecosystem of tools to develop and deploy their models. They can build models that incorporate features from a variety of data sources like images on a distributed ﬁle system, results from a SQL query or graph database query, or streaming data sources. Using a smaller version of the ImageNet benchmark we quantify the speedup achieved by SparkNet as a function of the size of the cluster, the communication frequency, and the cluster’s communi- cation overhead. We demonstrate that our approach is effective even in highly bandwidth-limited settings. On the full ImageNet benchmark we showed that our system achieves a sizable speedup over a single node experiment even with few GPUs. The code for SparkNet is available at https://github.com/amplab/SparkNet. We invite contributions and hope that the project will help bring a diverse set of deep learning applications to the Spark community.  ACKNOWLEDGMENTS  We would like to thank Cyprien Noel, Andy Feng, Tomer Kaftan, Evan Sparks, and Shivaram Venkataraman for valuable advice. This research is supported in part by NSF grant number DGE- 1106400. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award SN10040 DE-SC0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple, Blue Goji, Bosch, Cisco, Cray, Cloudera, EMC2, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Informatica, Intel, Microsoft, NetApp, Pivotal, Samsung, Schlumberger, Splunk, Virdata and VMware.  10  Published as a conference paper at ICLR 2016  REFERENCES Abadi, Mart´ın, Agarwal, Ashish, Barham, Paul, et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorﬂow.org.  Abuzaid, Firas, Hadjis, Stefan, Zhang, Ce, and R´e, Christopher. Caffe con Troll: Shallow ideas to  speed up deep learning. arXiv preprint arXiv:1504.04343, 2015.  Armbrust, Michael, Xin, Reynold S, Lian, Cheng, Huai, Yin, Liu, Davies, Bradley, Joseph K, Meng, Xiangrui, Kaftan, Tomer, Franklin, Michael J, Ghodsi, Ali, et al. Spark SQL: Relational data In Proceedings of the 2015 ACM SIGMOD International Conference on processing in Spark. Management of Data, pp. 1383–1394. ACM, 2015.  Chilimbi, Trishul, Suzue, Yutaka, Apacible, Johnson, and Kalyanaraman, Karthik. Project Adam: Building an efﬁcient and scalable deep learning training system. In 11th USENIX Symposium on Operating Systems Design and Implementation, pp. 571–582, 2014.  Coates, Adam, Huval, Brody, Wang, Tao, Wu, David, Catanzaro, Bryan, and Andrew, Ng. Deep learning with cots hpc systems. In Proceedings of the 30th International Conference on Machine Learning, pp. 1337–1345, 2013.  Dean, Jeffrey and Ghemawat, Sanjay. MapReduce: simpliﬁed data processing on large clusters.  Communications of the ACM, 51(1):107–113, 2008.  Dean, Jeffrey, Corrado, Greg, Monga, Rajat, Chen, Kai, Devin, Matthieu, Mao, Mark, Ranzato, Marc’Aurelio, Senior, Andrew, Tucker, Paul, Yang, Ke, Le, Quoc V., and Ng, Andrew Y. Large In Advances in Neural Information Processing Systems, pp. scale distributed deep networks. 1223–1231, 2012.  Gonzalez, Joseph E, Xin, Reynold S, Dave, Ankur, Crankshaw, Daniel, Franklin, Michael J, and Stoica, Ion. Graphx: Graph processing in a distributed dataﬂow framework. In Proceedings of OSDI, pp. 599–613, 2014.  Ho, Qirong, Cipar, James, Cui, Henggang, Lee, Seunghak, Kim, Jin Kyu, Gibbons, Phillip B, Gib- son, Garth A, Ganger, Greg, and Xing, Eric P. More effective distributed ML via a stale syn- chronous parallel parameter server. In Advances in Neural Information Processing Systems, pp. 1223–1231, 2013.  Iandola, Forrest N, Ashraf, Khalid, Moskewicz, Mattthew W, and Keutzer, Kurt. FireCaffe: near-linear acceleration of deep neural network training on compute clusters. arXiv preprint arXiv:1511.00175, 2015.  Isard, Michael, Budiu, Mihai, Yu, Yuan, Birrell, Andrew, and Fetterly, Dennis. Dryad: Distributed data-parallel programs from sequential building blocks. In Proceedings of the 2nd ACM SIGOP- S/EuroSys European Conference on Computer Systems, pp. 59–72, 2007.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- In Proceedings of the ACM International Conference on Multimedia, pp. 675–678. bedding. ACM, 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097–1105, 2012.  Li, Mu, Andersen, David G, Park, Jun Woo, Smola, Alexander J, Ahmed, Amr, Josifovski, Vanja, Long, James, Shekita, Eugene J, and Su, Bor-Yiing. Scaling distributed machine learning with the parameter server. In 11th USENIX Symposium on Operating Systems Design and Implementation, pp. 583–598, 2014.  Meng, Xiangrui, Bradley, Joseph, Yavuz, Burak, Sparks, Evan, Venkataraman, Shivaram, Liu, Davies, Freeman, Jeremy, Tsai, DB, Amde, Manish, Owen, Sean, et al. MLlib: Machine learning in Apache Spark. arXiv preprint arXiv:1505.06807, 2015.  11  Published as a conference paper at ICLR 2016  Murray, Derek G, McSherry, Frank, Isaacs, Rebecca, Isard, Michael, Barham, Paul, and Abadi, Mart´ın. Naiad: a timely dataﬂow system. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, pp. 439–455. ACM, 2013.  Noel, Cyprien, Shi, Jun, and Feng, Andy. Large scale distributed deep learning on Hadoop clusters, 2015. URL http://yahoohadoop.tumblr.com/post/129872361846/ large-scale-distributed-deep-learning-on-hadoop.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, International Journal of Computer Li. Vision, pp. 1–42, 2015.  ImageNet Large Scale Visual Recognition Challenge.  Sparks, Evan R., Venkataraman, Shivaram, Kaftan, Tomer, Franklin, Michael, and Recht, Benjamin.  KeystoneML: End-to-end machine learning pipelines at scale. 2015.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. In Computer Vision and Pattern Recognition, 2015.  Zaharia, Matei, Chowdhury, Mosharaf, Franklin, Michael J, Shenker, Scott, and Stoica, Ion. Spark: cluster computing with working sets. In Proceedings of the 2nd USENIX conference on Hot topics in cloud computing, volume 10, pp. 10, 2010.  Zaharia, Matei, Das, Tathagata, Li, Haoyuan, Hunter, Timothy, Shenker, Scott, and Stoica, Ion. Discretized streams: Fault-tolerant streaming computation at scale. In Proceedings of the Twenty- Fourth ACM Symposium on Operating Systems Principles, pp. 423–438. ACM, 2013.  Zhang, Sixin, Choromanska, Anna E, and LeCun, Yann. Deep learning with elastic averaging SGD.  In Advances in Neural Information Processing Systems, pp. 685–693, 2015.  Zhang, Yuchen and Jordan, Michael I. Splash: User-friendly programming interface for parallelizing  stochastic algorithms. arXiv preprint arXiv:1506.07552, 2015.  Zinkevich, Martin, Weimer, Markus, Li, Lihong, and Smola, Alex J. Parallelized stochastic gradient  descent. In Advances in Neural Information Processing Systems, pp. 2595–2603, 2010.  12  ",
1511.06390,2016,Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks,['Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks\nJost Tobias Springenberg'],https://arxiv.org/pdf/1511.06390,"6 1 0 2    r p A 0 3         ] L M  . t a t s [      2 v 0 9 3 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  UNSUPERVISED AND SEMI-SUPERVISED LEARNING WITH CATEGORICAL GENERATIVE ADVERSARIAL NETWORKS  Jost Tobias Springenberg University of Freiburg 79110 Freiburg, Germany springj@cs.uni-freiburg.de  ABSTRACT  In this paper we present a method for learning a discriminative classiﬁer from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classiﬁer to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classiﬁcation against an optimal adversary. We empirically evaluate our method – which we dub categorical generative adversarial networks (or CatGAN) – on synthetic data as well as on challenging image classiﬁcation tasks, demonstrating the robustness of the learned classiﬁers. We further qualitatively assess the ﬁdelity of samples generated by the adversarial generator that is learned alongside the discriminative classiﬁer, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).  1  INTRODUCTION  Learning non-linear classiﬁers from unlabeled or only partially labeled data is a long standing prob- lem in machine learning. The premise behind learning from unlabeled data is that the structure present in the training examples contains information that can be used to infer the unknown labels. That is, in unsupervised learning we assume that the input distribution p(x) contains information about p(y | x) – where y ∈ {1, . . . , K} denotes the unknown label. By utilizing both labeled and unlabeled examples from the data distribution one hopes to learn a representation that captures this shared structure. Such a representation might, subsequently, help classiﬁers trained using only a few labeled examples to generalize to parts of the data distribution that it would otherwise have no information about. Additionally, unsupervised categorization of data is an often sought-after tool for discovering groups in datasets with unknown class structure. This task has traditionally been formalized as a cluster assignment problem, for which a large num- ber of well studied algorithms can be employed. These can be separated into two types: (1) gen- erative clustering methods such as Gaussian mixture models, k-means, and density estimation al- gorithms, which directly try to model the data distribution p(x) (or its geometric properties); (2) discriminative clustering methods such as maximum margin clustering (MMC) (Xu et al., 2005) or regularized information maximization (RIM) (Krause et al., 2010), which aim to directly group the unlabeled data into well separated categories through some classiﬁcation mechanism without explic- itly modeling p(x). While the latter methods more directly correspond to our goal of learning class separations (rather than class exemplars or centroids), they can easily overﬁt to spurious correlations in the data; especially when combined with powerful non-linear classiﬁers such as neural networks. More recently, the neural networks community has explored a large variety of methods for un- supervised and semi-supervised learning tasks. These methods typically involve either training a generative model – parameterized, for example, by deep Boltzmann machines (e.g. Salakhutdinov & Hinton (2009), Goodfellow et al. (2013)) or by feed-forward neural networks (e.g. Bengio et al.  1  Published as a conference paper at ICLR 2016  (2014), Kingma et al. (2014)) –, or training autoencoder networks (e.g. Hinton & Salakhutdinov (2006), Vincent et al. (2008)). Because they model the data distribution explicitly through recon- struction of input examples, all of these models are related to generative clustering methods, and are typically only used for pre-training a classiﬁcation network. One problem with such reconstruction- based learning methods is that, by construction, they try to learn representations which preserve all information present in the input examples. This goal of perfect reconstruction is often directly opposed to the goal of learning a classiﬁer which is to model p(y|x) and hence to only preserve information necessary to predict the class label (and become invariant to unimportant details) The idea of the categorical generative adversarial networks (CatGAN) framework that we develop in this paper then is to combine both the generative and the discriminative perspective. In particular, we learn discriminative neural network classiﬁers D that maximize mutual information between the inputs x and the labels y (as predicted through the conditional distribution p(y|x, D)) for a number of K unknown categories. To aid these classiﬁers in their task of discovering categories that generalize well to unseen data, we enforce robustness of the classiﬁer to examples produced by an adversarial generative model, which tries to trick the classiﬁer into accepting bogus input examples. The rest of the paper is organized as follows: Before introducing our new objective, we brieﬂy review the generative adversarial networks framework in Section 2. We then derive the CatGAN objective as a direct extension of the GAN framework, followed by experiments on synthetic data, MNIST (LeCun et al., 1989) and CIFAR-10 (Krizhevsky & Hinton, 2009).  2 GENERATIVE ADVERSARIAL NETWORKS  Recently, Goodfellow et al. (2014) introduced the generative adversarial networks (GAN) frame- work. They trained generative models through an objective function that implements a two-player zero sum game between a discriminator D – a function aiming to tell apart real from fake input data – and a generator G – a function that is optimized to generate input data (from noise) that “fools” the discriminator. The “game” that the generator and the discriminator play can then be intuitively described as follows. In each step the generator produces an example from random noise that has the potential to fool the discriminator. The discriminator is then presented a few real data examples, together with the examples produced by the generator, and its task is to classify them as “real” or “fake”. Afterwards, the discriminator is rewarded for correct classiﬁcations and the generator for generating examples that did fool the discriminator. Both models are then updated and the next cycle of the game begins. This process can be formalized as follows. Let X = {x1, . . . xN} be a dataset of provided “real” inputs with dimensionality I (i.e. x ∈ RI). Let D denote the mentioned discriminative function and G denote the generator function. That is, G maps random vectors z ∈ RZ to generated inputs ˜x = G(z) and we assume D to predict the probability of example x being present in the dataset X : p(y = 1 | x, D) =  1  1+e−D(x) . The GAN objective is then given as Ex∼X  log p(y = 1 | x, D)  + Ez∼P (z)  (cid:104)  log(cid:0)1 − p(y = 1 | G(z), D)(cid:1)(cid:105)  ,  (cid:104)  (cid:105)  (1)  min  G  max  D  where P (z) is an arbitrary noise distribution which – without loss of generality – we assume to be the uniform distribution P (zi) = U(0, 1) for the remainder of this paper. If both the generator and the discriminator are differentiable functions (such as deep neural networks) then they can be trained by alternating stochastic gradient descent (SGD) steps on the objective functions from Equation (1), effectively implementing the two player game described above.  3 CATEGORICAL GENERATIVE ADVERSARIAL NETWORKS (CATGANS)  Building on the foundations from Section 2 we will now derive the categorical generative adversarial networks (CatGAN) objective for unsupervised and semi-supervised learning. For the derivation we ﬁrst restrict ourselves to the unsupervised setting, which can be obtained by generalizing the GAN framework to multiple classes – a limitation that we remove by considering semi-supervised learning in Section 3.3. It should be noted that we could have equivalently derived the CatGAN model starting from the perspective of regularized information maximization (RIM) – as described in the appendix – with an equivalent outcome.  2  Published as a conference paper at ICLR 2016  to a conditional probability distribution over categories; that is(cid:80)K  3.1 PROBLEM SETTING As before, let X = {x1, . . . xN} be a dataset of unlabeled examples. We consider the problem of unsupervisedly learning a discriminative classiﬁer D from X , such that D classiﬁes the data into an a priori chosen number of categories (or classes) K. Further, we require D(x) to give rise k=1 p(y = k|x, D) = 1. The goal of learning then is to train a probabilistic classiﬁer D whose class assignments satisfy some goodness of ﬁt measures. Notably, since the true class distribution over examples is not known we have to resort to an intermediary measure for judging classiﬁer performance, rather than just minimizing, e.g., the negative log likelihood. Speciﬁcally, we will, in the following, always prefer D for which the conditional class distribution p(y|x, D) for a given example x has high certainty and for which the marginal class distribution p(y|D) is close to some prior distribution P (y) for all k. We will henceforth always assume a uniform prior over classes, that is we expect that the amount of examples per class in X is the same for all k: ∀k, k(cid:48) ∈ K : p(y = k|D) = p(y = k(cid:48)|D) 1 A ﬁrst observation about this problem is that it can naturally be considered as a “soft” or probabilistic cluster assignment task. It could thus, in principle, be solved by probabilistic clustering algorithms such as regularized information maximization (RIM) (Krause et al., 2010), or the related entropy minimization (Grandvalet & Bengio, 2005), or the early work on unsupervised classiﬁcation with phantom targets by Bridle et al. (1992). All of these methods are prone to overﬁtting to spurious correlations in the data 2, a problem that we aim to mitigate by pairing the discriminator with an adversarial generative model to whose examples it must become robust. We note in passing, that our method can be understood as a robust extension of RIM – in which the adversary provides an adaptive regularization mechanism. This relationship is made explicit in Section B in the appendix. A somewhat obvious, yet important, second observation that can be made is that the standard GAN objective cannot directly be used to solve the described problem. The reason for this is that while optimization of Equation (1) does result in a discriminative classiﬁer D – which must capture the statistics of the provided input data – this classiﬁer is only useful for determining whether or not a given example x belongs to X . In principle, we could hope that a classiﬁer which can model the data distribution might also learn a feature representation (e.g. in case of neural networks the hidden representation in the last layer of D) useful for extracting classes in a second step; for example via discriminative clustering. It is, however, instructive to realize that the means by which the function D performs the binary classiﬁcation task – of discriminating real from fake examples – are not restricted in the GAN framework and hence the classiﬁer will focus mainly on input features which are not yet correctly modeled by the generator. In turn, these features need not necessarily align with our concept of classes into which we want to separate the data. They could, in the worst case, be detecting noise in the data that stems from the generator. Despite these issues there is a principled, yet simple, way of extending the GAN framework such that the discriminator can be used for multi-class classiﬁcation. To motivate this, let us consider a change in protocol to the two player game behind the GAN framework (which we will formalize in the next section): Instead of asking D to predict the probability of x belonging to X we can require D to assign all examples to one of K categories (or classes), while staying uncertain of class assignments for samples from the generative model G – which we expect will help make the classiﬁer robust. Analogously, we can change the problem posed to the generator from “generate samples that belong to the dataset” to “generate samples that belong to precisely one out of K classes”. If we succeeded at training such a classiﬁer-generator pair – and simultaneously ensured that the discovered K classes coincide with the classiﬁcation problem we are interested in (e.g. D satisﬁes the goodness of ﬁt criteria outlined above) – we would have a general purpose formulation for training a classiﬁer from unlabeled data.  3.2 CATGAN OBJECTIVE  As outlined above, the optimization problem that we want to solve differs from the standard GAN formulation from Eq. (1) in one key aspect: instead of learning a binary discriminative function, we  1We discuss the possibility of using different priors in our framework in the appendix of this paper. 2In preliminary experiments we noticed that the MNIST dataset can, for example, be nicely separated into  ten classes by creating 2-3 classes for common noise patterns and collapsing together several “real” classes.  3  Published as a conference paper at ICLR 2016  Figure 1: Visualization of the information ﬂow through the generator (in green) and discriminator (in violet) neural networks (left). A sketch of the three parts (i) - (iii) of the objective function LD for the discriminator (right). To obtain certain predictions the discriminator minimizes the entropy of p(y|x, D), leading to a peaked conditional class distribution. To obtain uncertain predictions for generated samples the the entropy of p(y|G(z), D) is maximized which, in the limit, would result in a uniform distribution. Finally, maximizing the marginal class entropy over all data-points leads to uniform usage of all classes.  aim to learn a discriminator that separates the data into K categories by assigning a label y to each example x. Formally, we deﬁne the discriminator D(x) for this setting as a differentiable function predicting logits for K classes: D(x) ∈ RK. The probability of example x belonging to one of the K mutually exclusive classes is then given through a softmax assignment based on the discriminator output:  p(y = k|x, D) =  .  (2)  (cid:80)K  eDk(x) k=1 eDk(x)  As in the standard GAN formulation we deﬁne the generator G(z) to be a function mapping random noise z ∈ RZ to generated samples ˜x ∈ RI:  ˜x = G(z), with z ∼ P (z),  (3)  where P (z) again denotes an arbitrary noise distribution. For the purpose of this paper both D and G are always parameterized as multi-layer neural networks with either linear or sigmoid output. As informally described in Section 3.1, the goodness of ﬁt criteria – in combination with the idea that we want to use a generative model to regularize our classiﬁer – directly dictate three requirements that a learned discriminator should fulﬁll, and two requirements that the generator should fulﬁll. We repeat these here before turning them into a learnable objective function (a visualization of the requirements is shown in Figure 1). Discriminator perspective. The requirements to the discriminator are that it should (i) be certain of class assignment for samples from D, (ii) be uncertain of assignment for generated samples, and (iii) use all classes equally 3. Generator perspective. The requirements to the generator are that it should (i) generate samples with highly certain class assignments, and (ii) equally distribute samples across all K classes. We will now address each of these requirements in turn – framing them as maximization or mini- mization problems of class probabilities – beginning with the perspective of the discriminator. Note that without additional (label) information about the K classes we cannot directly specify which class probability p(y = k|x, D) should be maximized to meet requirement (i) for any given x. We can, nonetheless, formally capture the intuition behind this requirement through information theo- retic measures on the predicted class distribution. The most direct measure that can be applied to this problem is the Shannon entropy H, which is deﬁned as the expected value of the information car- ried by a sample from a given distribution. Intuitively, if we want the class distribution p(y | x, D) conditioned on example x to be highly peaked – i.e. D should be certain of the class assignment – we want the information content H[p(y | x, D)] of a sample from it to be low, since any draw from  3Since we assume a uniform prior P (y) over classes.  4  Published as a conference paper at ICLR 2016  (4)  (5)  said distribution should almost always result in the same class. If we, on the other hand, want the conditional class distribution to be ﬂat (highly uncertain) for examples that do not belong to X – but instead come from the generator – we can maximize the entropy H[p(y | G(z), D)], which, at the optimum, will result in a uniform conditional distribution over classes and fulﬁll requirement (ii). Concretely, we can deﬁne the empirical estimate of the conditional entropy over examples from X as  (cid:104)  H(cid:2)p(y | x, D)(cid:3)(cid:105)  Ex∼X  N(cid:88) N(cid:88)  i=1  H(cid:2)p(y | xi, D)(cid:3) − K(cid:88)  i=1  k=1  =  =  1 N  1 N  p(y = k | xi, D) log p(y = k | xi, D).  The empirical estimate of the conditional entropy over samples from the generator can be expressed as the expectation of H[p(y | G(z), D)] over the prior distribution P (z) for the noise vectors z, which we can further approximate through Monte-Carlo sampling yielding  (cid:104)  H(cid:2)p(y | D(z), D)(cid:3)(cid:105) ≈ 1  M(cid:88)  i=1  M  Ez∼P (z)  H(cid:2)p(y | G(zi), D)(cid:3), with zi ∼ P (z),  and where M denotes the number of independently drawn samples (which we simply set equal to N). To meet the third requirement that all classes should be used equally – corresponding to a uniform marginal distribution – we can maximize the entropy of the marginal class distribution as measured empirically based on X and samples from G:  (cid:104) (cid:104)  HX  HG  p(y | D)  p(y | D)  (cid:105) (cid:105) ≈ H  = H  (cid:104) 1 (cid:104) 1  N  N(cid:88) M(cid:88)  i=1  M  i=1  p(y | xi, D)  ,  (cid:105)  (cid:105)  p(y | G(zi), D)  , with zi ∼ P (z).  (6)  The second of these entropies can readily be used to deﬁne the maximization problem that needs to be satisﬁed for the requirement (ii) imposed on the generator. Satisfying the condition (i) from the generator perspective then ﬁnally amounts to minimizing rather than maximizing Equation (5). Combining the deﬁnition from Equations (4,5,6) we can deﬁne the CatGAN objective for the dis- criminator, which we refer to with LD, and for the generator, which we refer to with LG as  (cid:105) − Ex∼X  (cid:104) H(cid:2)p(y | x, D)(cid:3)(cid:105) (cid:104) H(cid:2)p(y | G(z), D)(cid:3)(cid:105)  ,  + Ez∼P (z)  + Ez∼P (z)  (cid:104) H(cid:2)p(y | G(z), D)(cid:3)(cid:105)  ,  (7)  LD = max LG = min  D  HX −HG  G  p(y | D)  p(cid:0)y | D(cid:1)(cid:105)  (cid:104) (cid:104)  where H denotes the empirical entropy as deﬁned above and we chose to deﬁne the objective for the generator LG as a minimization problem to make the analogy to Equation (1) apparent. This formu- lation satisﬁes all requirements outlined above and has a simple information theoretic interpretation: Taken together the ﬁrst two terms in LD are an estimate of the mutual information between the data distribution and the predicted class distribution – which the discriminator wants to maximize while minimizing information it encodes about G(z). Analogously, the ﬁrst two terms in LG esti- mate the mutual information between the distribution of generated samples and the predicted class distribution. Since we are interested in optimizing the objectives from Equation (7) on large datasets we would like both LG and LD to be amenable to to optimization via mini-batch stochastic gradient descent on batches XB of data – with size B (cid:28) N – drawn independently from X . The conditional entropy terms in Equation (7) both only consist of sums over per example entropies, and can thus trivially however, contain sums either over the whole dataset X or over a large set of samples from G within the entropy calculation and therefore cannot be split into “per-batch” terms. If the number of cate- gories K that the discriminator needs to predict is much smaller than the batch size B, a simple ﬁx to this problem is to estimate the marginal class distributions over the B examples in the random p(y | xi, D)]. For HG[p(y | D)] we can, similarly, mini-batch only: HX [p(y | D)] ≈ H[ 1  be adapted for batch-wise computation. The marginal entropies HX [p(y | D)] and HG[p(cid:0)y | D(cid:1)],  (cid:80)  B  x∈XB  5  Published as a conference paper at ICLR 2016  calculate an estimate using B samples only – instead of using M = N samples. We note that while this approximation is reasonable for the problems we consider (for which K <= 10 and B = 100) it will be problematic for scenarios in which we expect a large number of categories. In such a setting one would have to estimate the marginal class distribution over multiple batches (or periodically evaluate it on a larger number of examples).  3.3 EXTENSION TO SEMI-SUPERVISED LEARNING  We will now consider adapting the formulation from Section 3.2 to the semi-supervised setting. Let X L = {(x1, y1), (xL, yL)} be a set of L labeled examples, with label vectors yi ∈ RK in one-hot encoding, that are provided in addition to the N unlabeled examples contained in X . These additional examples can be incorporated into the objectives from Equation (7) by calculating a cross-entropy term between the predicted conditional distribution p(y | x, D) and the true label distribution of examples from X L (instead of the entropy term H used for unlabeled examples). The cross-entropy for a labeled data pair (x, y) is given as  CE(cid:2)y, p(y | x, D)(cid:3) = − K(cid:88)  yi log p(y = yi | x, D).  (8)  i=1  (cid:104)  The semi-supervised CatGAN problem is then given through the two objectives LL criminator) and LL G (for the generator) with p(y | D) HX +λE(x,y)∼X L  (cid:105) − Ex∼X CE(cid:2)y, p(y | x, D)(cid:3)(cid:105)  H(cid:2)p(y | x, D)(cid:3)(cid:105)  H(cid:2)p(y | G(z), D)(cid:3)(cid:105)  LL D = max  + Ez∼P (z)  (cid:104)  (cid:104)  (cid:104)  D  ,  D (for the dis-  (9)  where λ is a cost weighting term and where LL  G is the same as in Equation (7): LL  G = LG.  3.4  IMPLEMENTATION DETAILS  In our experiments both the generator and the discriminator are always parameterized through neu- ral networks. The details of architectural choices for each considered benchmark are given in the appendix, while we only cover major design choices in this section. GANs are known to be hard to train due to several unfortunate circumstances. First, the formulation from Equation (1) can become unstable if the discriminator learns too quickly (in which case the loss for the generator saturates). Second, the generator might get stuck generating one mode of the data or it may start wildly switching between generating different modes during training. We therefore take two measures to stabilize training. First, we use batch normalization (Ioffe & Szegedy, 2015) in all layers of the discriminator and all but the last layer (the layer producing generated examples ˜x) of the generator. This helps bound the activations in each layer and we empirically found it to prevent mode switching of the generator as well as to increase generalization capabilities of the discriminator in the few labels case. Additionally, we regularize the discriminator by applying noise to its hidden layers. While we did ﬁnd dropout (Hinton et al., 2012) to be effective for this purpose, we found Gaussian noise added to the batch normalized hidden activations to yield slightly better performance. We suspect that this is mainly due to the fact that dropout noise can severely affect mean and variance computation during batch-normalization – whereas Gaussian noise on the activations for which to compute these statistics is a natural assumption.  4 EMPIRICAL EVALUATION  The results of our empirical evaluation are given in Tables 1, 2 and 3. As can be seen, our method is competitive to the state of the art on almost all datasets. It is only slightly outperformed by the Ladder network utilizing denoising costs in each layer of the neural network.  4.1 CLUSTERING WITH CATGANS  Since categorization of unlabeled data is inherently linked to clustering we performed a ﬁrst set of experiments on common synthetic datasets that are often used to evaluate clustering algorithms. We  6  Published as a conference paper at ICLR 2016  Figure 2: Comparison between k-means (left), RIM (middle) and CatGAN (rightmost three) – with neural networks – on the “circles” dataset with K = 2. Blue and green denote class assignments to the two different classes. For CatGAN we visualize class assignments – both on the dataset and on a larger region of the input domain – and generated samples. Best viewed in color.  PI-MNIST test error (%) with n labeled examples  Algorithm MTC (Rifai et al., 2011) PEA (Bachman et al., 2014) PEA+ (Bachman et al., 2014) VAE+SVM (Kingma et al., 2014) SS-VAE (Kingma et al., 2014) Ladder Γ-model (Rasmus et al., 2015) Ladder full (Rasmus et al., 2015) RIM + NN GAN + SVM CatGAN (unsupervised) CatGAN (semi-supervised)  n = 100 12.03 10.79 5.21  11.82 (± 0.25) 3.33 (± 0.14) 4.34 (± 2.31) 1.13 (± 0.04) 16.19 (± 3.45) 28.71 (± 7.41) 1.91 (± 0.1)  9.7  n = 1000  100 2.33 2.67  4.24 (± 0.07) 2.4 (± 0.02) 1.71 (± 0.07) 1.00 (± 0.06) 10.41 (± 0.89) 13.21 (± 1.28) 1.73 (± 0.18)  0.96  0.79 (± 0.05)  All 0.81 1.08  - -  -  0.91  Table 1: Classiﬁcation error, in percent, for the permutation invariant MNIST problem with a re- duced number of labels. Results are averaged over 10 different sets of labeled examples.  compare the CatGAN algorithm with standard k-means clustering and RIM with neural networks as discriminative models, which amounts to removing the generator from the CatGAN model and adding (cid:96)2 regularization (see Section B in the appendix for an explanation). We considered three standard synthetic datasets – with feature dimensionality two, thus x ∈ R2 – for which we assumed the optimal number of clusters K do be known: the “two moons” dataset (which contains two clusters), the “circles” arrangement (again containing two clusters) and a simple dataset with three isotropic Gaussian blobs of data. In Figure 2 we show the results of that experiment for the “circles” dataset (plots for the other two experiments are relegated to Figures 4-6 in the appendix due to space constraints). In summary, the simple clustering assignment with three data blobs is solved by all algorithms. For the two more difﬁcult examples both k-means and RIM fail to “correctly” identify the clusters: (1) k-means fails due to the euclidean distance measure it employs to evaluate distances between data points and cluster centers, (2) in RIM the objective function only speciﬁes that the deep network has to separate the data into two equal classes, without any geometric constraints 4. In the CatGAN model, on the other hand, the discriminator has to place its decision boundaries such that it can easily detect a non-optimal adversarial generator which seems to coincide with the correct cluster assignment. Additionally, the generator quickly learns to generate the datasets in all cases.  4.2 UNSUPERVISED AND SEMI-SUPERVISED LEARNING OF IMAGE FEATURES  We next evaluate the capabilities of the CatGAN model on two image recognition datasets. We performed experiments using fully connected and convolutional networks on MNIST (LeCun et al., 1989) and CIFAR-10 (Krizhevsky & Hinton, 2009). We either used the full set of labeled exam- ples or a reduced set of labeled examples and kept the remaining examples for semi-supervised or unsupervised learning.  4We tried to rectify this by adding regularization (we tried both (cid:96)2 regularization and adding Gaussian noise)  but that did not yield any improvement  7  k-meansRIM + NNdata + class assignmentCatGANgenerated examplesdecision boundariesPublished as a conference paper at ICLR 2016  Algorithm EmbedCNN (Weston et al., 2012) SWWAE (Zhao et al., 2015) Small-CNN (Rasmus et al., 2015) Conv-Ladder Γ-model (Rasmus et al., 2015) RIM + CNN Conv-GAN + SVM Conv-CatGAN (unsupervised) Conv-CatGAN (semi-supervised)  7.75  n = 100 8.71 ±0.34 6.43 (± 0.84) 0.86 (± 0.41) 10.75 (± 2.25) 15.43 (± 1.72) 1.39 (± 0.28)  4.27  All -  0.71 0.36  -  0.53 9.64  0.48  MNIST test error (%) with n labeled examples  Table 2: Classiﬁcation error, in percent, for different learning methods in combination with convo- lutional neural networks (CNNs) with a reduced number of labels.  Algorithm View-Invariant k-means Hui (2013) Exemplar-CNN (Dosovitskiy et al., 2014) Conv-Ladder Γ-model (Rasmus et al., 2015) Conv-CatGAN (semi-supervised)  CIFAR-10 test error (%) with n labeled examples n = 4000 27.4 (± 0.7) 23.4 (± 0.2) 20.09 (± 0.46) 19.58 (± 0.58)  All 18.1 15.7 9.27 9.38  Table 3: Classiﬁcation error for different methods on the CIFAR-10 dataset (without data augmen- tation) for the full dataset and a reduced set of 400 labeled examples per class.  We performed experiments using two setups: (1) using a subset of labeled examples we optimized the semi-supervised objective from Equation (7), and (2) using no labeled examples we optimized the unsupervised objective from Equation (9) with K = 20 “pseudo” categories. In setup (2) learn- ing was followed by a category matching step. In this second step we simply looked at 100 examples from a validation set (we always kept 10000 examples from the training set for validation) for which we assume the correct labeling to be known, and assigned each pseudo category yk to be indicative of one of the true classes ci ∈ {1 . . . 10}. Speciﬁcally we assign yk to the class i for which the count of examples that were classiﬁed as yk and belonged to ci was maximal. This setup hence bears some similarity to one-shot learning approaches from the literature (see e.g. Fei-Fei et al. (2006) for an application to computer vision). Since no learning is involved in the actual matching step we – somewhat colloquially – refer to this setup as half-shot learning. The results for the experiment on the permutation invariant MNIST (PI-MNIST) task are listed in Table 1. The table also lists state-of-the-art results for this benchmark as well as two baselines: a version of our algorithm where the generator is removed – but all other pieces stay in place – which we call RIM + NN due to the relationship between our algorithm and RIM; and the discriminator stemming from a standard GAN paired with an SVM trained based on features from it 5. While both the RIM and GAN training objectives do produce features that are useful for classifying digits, their performance is far worse than the best published result for this setting. The semi- supervised CatGAN, on the other hand, comes close to the best results, works remarkably well even with only 100 labeled examples, and is only outperformed by the Ladder network with a specially designed denoising objective in each layer. Perhaps more surprisingly the half-shot learning proce- dure described above results in a classiﬁer that achieves 9.7% error without the need for any label information during training. Finally, we performed experiments with convolutional discriminator networks and de- convolutional (Zeiler et al., 2011) generator networks (using the same up-sampling procedure from Dosovitskiy et al. (2015)) on MNIST and CIFAR-10. As before, details on the network architec- tures are given in the appendix. The results are given in Table 2 and 3 and are qualitatively similar to the PI-MNIST results; notably the unsupervised CatGAN again performs very well, achieving a classiﬁcation error of 4.27. The discriminator trained with the semi-supervised CatGAN objective performed well on both tasks, matching the state of the art on CIFAR-10 with reduced labels.  5Speciﬁcally, we ﬁrst train a generator-discriminator using the standard GAN objective and then extract the  last layer features from the discriminator on the available labeled examples, and use them to train an SVM.  8  Published as a conference paper at ICLR 2016  Figure 3: Exemplary images produced by a generator trained using the semi-supervised CatGAN objective. We show samples for a generator trained on MNIST (left) CIFAR-10 (right).  4.3 EVALUATION OF THE GENERATIVE MODEL  Finally, we qualitatively evaluate the capabilities of the generative model. We trained an unsuper- vised CatGAN on MNIST, LFW and CIFAR-10 and plot samples generated by these models in Figure 3. As an additional quantitative evaluation we compared the unsupervised CatGAN model trained on MNIST with other generative models based on the log likelihood of generated samples (as measured through a Parzen-window estimator). The full results of this evaluation are given in Table 6 in the appendix. In brief: The CatGAN model performs comparable to the best existing al- gorithms, achieving a log-likelihood of 237± 6 on MNIST; in comparison, Goodfellow et al. (2014) report 225 ± 2 for GANs. We note, however, that this does not necessarily mean that the CatGAN model is superior as comparing generative models with respect to log-likelihood measured by a Parzen-window estimate can be misleading (see Theis et al. (2015) for a recent in-depth discussion).  5 RELATION TO PRIOR WORK  As highlighted in the introduction our method is related to, and stands on the shoulders of, a large body of literature on unsupervised and semi-supervised category discovery with machine learning methods. While a comprehensive review of these methods is out of the scope for this paper we want to point out a few interesting connections. First, as already discussed, the idea of minimizing entropy of a classiﬁer on unlabeled data has been considered several times already in the literature (Bridle et al., 1992; Grandvalet & Bengio, 2005; Krause et al., 2010), and our objective function falls back to the regularized information max- imization from Krause et al. (2010) when the generator is removed and the classiﬁer is additionally (cid:96)2 regularized6. Several researchers have recently also reported successes for unsupervised learning with pseudo-tasks, such as self-supervised labeling a set of unlabeled training examples (Lee, 2013), learning to recognize pseudo-classes obtained through data augmentation (Dosovitskiy et al., 2014) and learning with pseudo-ensembles (Bachman et al., 2014), in which a set of models (with shared parameters) are trained such they agree on their predictions, as measured through e.g. cross-entropy. While on ﬁrst glance these appear only weakly related, they are strongly connected to entropy min- imization as, for example, concisely explained in Bachman et al. (2014). From the generative modeling perspective, our model is a direct descendant of the generative ad- versarial networks framework (Goodfellow et al., 2014). Several extensions to this framework have been developed recently, including conditioning on a set of variables (Gauthier, 2014; Mirza & Osindero, 2014) and hierarchical generation using Laplacian pyramids (Denton et al., 2015). These are orthogonal to the methods developed in this paper and a combination of, for example, CatGANs with more advanced generator architectures is an interesting avenue for future work.  6 CONCLUSION  We have presented categorical generative adversarial networks, a framework for robust unsuper- vised and semi-supervised learning. Our method combines neural network classiﬁers with an ad- versarial generative model that regularizes a discriminatively trained classiﬁer. We found the pro- posed method to yield classiﬁcation performance that is competitive with state-of-the-art results for semi-supervised learning for image classiﬁcation and further conﬁrmed that the generator, which is learned alongside the classiﬁer, is capable of generating images of high visual ﬁdelity.  6We note that we did not ﬁnd (cid:96)2 regularization to help in our experiments.  9  Published as a conference paper at ICLR 2016  ACKNOWLEDGMENTS  The author would like to thank Alexey Dosovitskiy, Alec Radford, Manuel Watter, Joschka Boedecker and Martin Riedmiller for extremely helpful discussions on the contents of this manuscript. Further, huge thanks go to Alec Radford and the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and Lasagne (Dieleman et al., 2015) for sharing research code. This work was funded by the the German Research Foundation (DFG) within the priority program “Au- tonomous learning” (SPP1597).  REFERENCES Bachman, Phil, Alsharif, Ouais, and Precup, Doina. Learning with pseudo-ensembles. In Advances in Neural Information Processing Systems (NIPS) 27, pp. 3365–3373. Curran Associates, Inc., 2014.  Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Bengio, Yoshua, Thibodeau-Laufer, Eric, and Yosinski, Jason. Deep generative stochastic networks trainable by backprop. In Proceedings of the 31st International Conference on Machine Learning (ICML), 2014.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), 2010.  Bridle, John S., Heading, Anthony J. R., and MacKay, David J. C. Unsupervised classiﬁers, mutual information and phantom targets. In Advances in Neural Information Processing Systems (NIPS) 4. MIT Press, 1992.  Denton, Emily, Chintala, Soumith, Szlam, Arthur, and Fergus, Rob. Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in Neural Information Processing Systems (NIPS) 28, 2015.  Dieleman, Sander, Schlter, Jan, Raffel, Colin, Olson, Eben, Sønderby, Søren Kaae, Nouri, Daniel, Maturana, Daniel, Thoma, Martin, Battenberg, Eric, Kelly, Jack, Fauw, Jeffrey De, Heilman, Michael, and et al. Lasagne: First release., August 2015. URL http://dx.doi.org/10. 5281/zenodo.27878.  Dosovitskiy, A., Springenberg, J. T., and Brox, T. Learning to generate chairs with convolutional In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  neural networks. 2015.  Dosovitskiy, Alexey, Springenberg, Jost Tobias, Riedmiller, Martin, and Brox, Thomas. Discrimi- native unsupervised feature learning with convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS) 27. Curran Associates, Inc., 2014.  Ester, Martin, Kriegel, Hans-Peter, Sander, Jrg, and Xu, Xiaowei. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proc. of 2nd International Conference on Knowledge Discovery and Data Mining (KDD), 1996.  Fei-Fei, L., Fergus, R., and Perona. One-shot learning of object categories. IEEE Transactions on  Pattern Analysis Machine Intelligence, 28:594–611, April 2006.  Funk, Simon. SMORMS3 - blog entry: RMSprop loses to SMORMS3 - beware the epsilon!  http://sifter.org/ simon/journal/20150420.html, 2015.  Gauthier, Jon. Conditional generative adversarial networks for face generation. Class Project for  Stanford CS231N, 2014.  10  Published as a conference paper at ICLR 2016  Goodfellow, Ian, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Multi-prediction deep boltz- mann machines. In Advances in Neural Information Processing Systems (NIPS) 26. Curran As- sociates, Inc., 2013.  Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sher- jil, Courville, Aaron, and Bengio, Yoshua. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS) 27. Curran Associates, Inc., 2014.  Grandvalet, Yves and Bengio, Yoshua. Semi-supervised learning by entropy minimization.  Advances in Neural Information Processing Systems (NIPS) 17. MIT Press, 2005.  In  Hinton, G E and Salakhutdinov, R R. Reducing the dimensionality of data with neural networks.  Science, 313(5786):504–507, July 2006.  Hinton, Geoffrey E., Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Rus- Improving neural networks by preventing co-adaptation of feature detectors. CoRR,  lan R. abs/1207.0580v3, 2012. URL http://arxiv.org/abs/1207.0580v3.  Huang, Gary B., Ramesh, Manu, Berg, Tamara, and Learned-Miller, Erik. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007.  Hui, Ka Y. Direct modeling of complex invariances for visual object features. In Proceedings of the 30th International Conference on Machine Learning (ICML). JMLR Workshop and Conference Proceedings, 2013.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning (ICML). JMLR Proceedings, 2015.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. In International  Conference on Learning Representations (ICLR), 2015.  Kingma, Diederik P, Mohamed, Shakir, Jimenez Rezende, Danilo, and Welling, Max. Semi- supervised learning with deep generative models. In Advances in Neural Information Processing Systems (NIPS) 27. Curran Associates, Inc., 2014.  Krause, Andreas, Perona, Pietro, and Gomes, Ryan G. Discriminative clustering by regularized information maximization. In Advances in Neural Information Processing Systems (NIPS) 23. MIT Press, 2010.  Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Master’s  thesis, Department of Computer Science, University of Toronto, 2009.  LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541– 551, 1989.  Lee, Dong-Hyun. Pseudo-label : The simple and efﬁcient semi-supervised learning method for deep  neural networks. In Workshop on Challenges in Representation Learning, ICML, 2013.  Li, Yujia, Swersky, Kevin, and Zemel, Richard S. Generative moment matching networks.  Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.  In  Mirza, Mehdi and Osindero, Simon. Conditional generative adversarial nets. CoRR, abs/1411.1784,  2014. URL http://arxiv.org/abs/1411.1784.  Osendorfer, Christian, Soyer, Hubert, and van der Smagt, Patrick.  Image super-resolution with fast approximate convolutional sparse coding. In ICONIP, Lecture Notes in Computer Science. Springer International Publishing, 2014.  Rasmus, Antti, Valpola, Harri, Honkala, Mikko, Berglund, Mathias, and Raiko, Tapani. Semi- supervised learning with ladder network. In Advances in Neural Information Processing Systems (NIPS) 28, 2015.  11  Published as a conference paper at ICLR 2016  Rifai, Salah, Dauphin, Yann N, Vincent, Pascal, Bengio, Yoshua, and Muller, Xavier. The manifold In Advances in Neural Information Processing Systems (NIPS) 24. Curran  tangent classiﬁer. Associates, Inc., 2011.  Salakhutdinov, Ruslan and Hinton, Geoffrey. Deep Boltzmann machines. In Proceedings of the  International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2009.  Schaul, Tom, Zhang, Sixin, and LeCun, Yann. No More Pesky Learning Rates. In International  Conference on Machine Learning (ICML), 2013.  Springenberg, Jost Tobias, Dosovitskiy, Alexey, Brox, Thomas, and Riedmiller, Martin. Striving for  simplicity: The all convolutional net. In arXiv:1412.6806, 2015.  Theis, Lucas, van den Oord, A¨aron, and Bethge, Matthias. A note on the evaluation of generative  models. CoRR, abs/1511.01844, 2015. URL http://arxiv.org/abs/1511.01844.  Tieleman, T. and Hinton, G. Lecture 6.5—RmsProp: Divide the gradient by a running average of its  recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Extracting and composing robust features with denoising autoencoders. In Proceedings of the Twenty-ﬁfth International Conference on Machine Learning (ICML), 2008.  Weston, J., Ratle, F., Mobahi, H., and Collobert, R. Deep learning via semi-supervised embedding. In Montavon, G., Orr, G., and Muller, K-R. (eds.), Neural Networks: Tricks of the Trade. Springer, 2012.  Xu, Linli, Neufeld, James, Larson, Bryce, and Schuurmans, Dale. Maximum margin clustering. In  Advances in Neural Information Processing Systems (NIPS) 17. MIT Press, 2005.  Zeiler, Matthew D., Taylor, Graham W., and Fergus, Rob. Adaptive deconvolutional networks for mid and high level feature learning. In IEEE International Conference on Computer Vision, ICCV, pp. 2018–2025, 2011.  Zhao, Junbo, Mathieu, Michael, Goroshin, Ross, and Lecun, Yann. Stacked what-where auto-  encoders. CoRR, abs/1506.02351, 2015. URL http://arxiv.org/abs/1506.02351.  APPENDIX  A ON THE RELATION BETWEEN CATGAN AND GAN  In this section we will make the relation between the CatGAN objective from Equation (7) in the main paper and the GAN objective given by Equation (1) more directl apparent. Starting from the CatGAN objective let us consider the case K = 1. In this case the conditional probabilities should model binary dependent variables (and are thus no longer multinomial). The “correct” choice for the discriminative model is a logistic classiﬁer with output D(x) ∈ R with conditional probability p(y = 1 | x, D) given as p(y = 1 | x, D) = eD(x) 1+e−D(x) . Using this deﬁnition The discriminator loss LD from Equation (7) can be expanded to give  eD(x)+1 =  1  L1 D = max  D  = max  D  (cid:104) (cid:104)  H(cid:2)p(y | G(z), D)(cid:3)(cid:105) H(cid:2)p(y | x, D)(cid:3)(cid:105) (cid:104) (cid:105) (cid:104) − pG(z) log pG(z) − (1 − pG(z)) log(1 − pG(z)) (cid:105)  px log px + (1 − px) log(1 − px)  + Ez∼P (z)  −Ex∼X Ex∼X +Ez∼P (z)  (10)  ,  where we introduced the short notation px = p(y = 1 | x, D), pG(z) = p(y = 1 | G(z), D) and dropped the entropy term HX concerning the empirical class distribution as we only consider one class and hence the classes are equally distributed by deﬁnition. Equation (10) now is similar to the GAN objective but pushes the conditional probability for samples from X to 0 or 1  p(y | D)  (cid:104)  (cid:105)  12  Published as a conference paper at ICLR 2016  1 | x, D) we can replace the entropy H(cid:2)p(y | x, D)(cid:3) with the cross-entropy CE(cid:2)1, p(y | x, D)(cid:3)  and the probability for generated samples towards 0.5. To obtain a classiﬁer which predicts p(y =  yielding  (cid:104)  (cid:105)  L1 D = max  D  Ex∼X +Ez∼P (z)  log p(y = 1 | x, D)  (cid:104) − pG(z) log pG(z) − (1 − pG(z)) log(1 − pG(z)) (cid:105)  (11)  ,  which is equivalent to the discriminative part of the GAN formulation except for the fact that op- timization of Equation (11) will result in examples from the generator being pushed towards the decision boundary of p(y = 1 | G(z), D) = 0.5 rather than p(y = 1 | G(z), D) = 0. An equivalent derivation can be made for the generator objective LG leading to a symmetric objective – just as in the GAN formulation.  B ON THE RELATION BETWEEN CATGAN AND RIM  In this section we re-derive CatGAN as an extension to the RIM framework from Krause et al. (2010). As in the main paper we will restrict ourselves to the unsupervised setting but an extension to the semi-supervised setting is straight-forward. The idea behind RIM is to train a discriminative classiﬁer, which we will suggestively call D, from unlabeled data. The objective that is maximized for this purpose is the mutual information between the data distribution and the predicted class labels, which can be formalized as  (cid:105) − Ex∼X  (cid:104)  H(cid:2)p(y | x, D)(cid:3)(cid:105) − γR(D),  LRIM = max  D  HX  p(y | D)  (12)  (cid:104)  where the entropy terms are deﬁned as in the main paper and R(D) is a regularization term acting on the discriminative model. In Krause et al. (2010) D was chosen as a logistic regression classiﬁer and R(D) consisted of (cid:96)2 regularization on the discriminator weights. If we instantiate D to be a neural network we obtain the baseline RIM + NN which we considered in our experiments. To connect the RIM objective to the CatGAN formulation from Equation (7) we can set let R(D) = −Ez∼P (z) , that is we let R(D) measure the negative entropy of samples from the generator. With this setting we achieve equivalence between LRIM and LD. If we now also train the generator G alongside the discriminator D using the objective LG we arrive at the CatGAN formulation.  H(cid:2)p(y | G(z), D)(cid:3)(cid:105)  (cid:104)  C ON DIFFERENT PRIORS FOR THE EMPIRICAL CLASS DISTRIBUTION  In the main paper we always assumed a uniform prior over classes, that is we enforced that the amount of examples per class in X is the same for all k: ∀k, k(cid:48) ∈ K : p(y = k(cid:48)|D) = p(y = k(cid:48)|D). This was achieved by maximizing the entropy of the class distribution HX If this prior assumption is not valid our method could be extended to different prior distributions P (y) similar to how RIM can be adapted (see Section 5.2 of Krause et al. (2010)). This becomes easy to see ny noticing the relationship between the Entropy and the KL divergence: HX = log(K) − KL(p(y | D)(cid:107)U ) where U denotes the discrete uniform distribution. We can thus simply drop the constant term log(K) and use −KL(p(y | D)(cid:107)U) directly, allowing us to replace U with an arbitrary prior P (y) – as long as we can differentiate through the computation of the KL divergence (or estimate it via sampling).  p(y | D)  p(y | D)  (cid:104)  (cid:105)  .  (cid:104)  (cid:105)  D DETAILED EXPLANATION OF THE TRAINING PROCEDURE As mentioned in the main Paper we perform training by alternating optimization steps on LD and LG. More speciﬁcally, we use batch size B = 100 in all experiments and approximate the expecta- tions in Equation (7) and Equation (9) using 100 random examples from X , X L and the generator G(z) respectively. We then do one gradient ascent step on the objective for the discriminator fol- lowed by one gradient descent step on the objective for the generator. We also added noise to all  13  Published as a conference paper at ICLR 2016  layers as mentioned in the main paper. Since adding noise to the network can result in instabilities in the computation of the entropy terms from our objective (due to small values inside the loga- rithms which are multiplied with non-negative probabilities) we added noise only to the terms not appearing inside logarithms. That is we effectively replace H[p(y | x, D)] with the cross-entropy CE[p(y | x, D), p(y | x, ˆD)], where ˆD is the network with added noise and additionally truncate probabilities to be bigger than 1e − 4. During our evaluation we experimented with Adam (Kingma & Ba, 2015) for adapting learning rates but settled for a hybrid between (Schaul et al., 2013) and rmsprop (Tieleman & Hinton, 2012), called SMORMS3 (Funk, 2015) which we found slightly eas- ier to use as it only has one free parameter – a maximum learning rate – which we did always set to 0.001.  D.1 DETAILS ON NETWORK ARCHITECTURES  D.1.1 SYNTHETIC BENCHMARKS  For the synthetic benchmarks we used neural networks with three hidden layers, containing 100 leaky rectiﬁed linear units each (leak rate 0.1), both for the discriminator and the generator (where applicable). Batch normalization was used in all layers (with added Gaussian noise with standard deviation 0.05) and the dimensionality of the noise vectors z for the CatGAN model was chosen to be 10 for. Note that while such large networks are most certainly an “overkill” for the considered benchmarks, we did chose these settings to ensure that learning was easily possible. We also exper- imented with smaller networks but did not ﬁnd them to result in better decision boundaries or more stable learning.  Table 4: The discriminator and generator CNNs used for MNIST.  discriminator D Input 28 × 28 Gray image 5 × 5 conv. 32 lReLU 3 × 3 max-pool, stride 2 3 × 3 conv. 64 lReLU 3 × 3 conv. 64 lReLU 3 × 3 max-pool, stride 2 3 × 3 conv. 128 lReLU 1 × 1 conv. 10 lReLU 128 fc lReLU 10-way softmax  Model  generator G Input z ∈ R128 8 × 8 × 96 fc lReLU 2 × 2 perforated up-sampling 5 × 5 conv. 64 lReLU 2 × 2 perforated up-sampling 5 × 5 conv. 64 lReLU 5 × 5 conv. 1 lReLU  Table 5: The discriminator and generator CNNs used for CIFAR-10.  generator G Input 32 × 32 RGB image 3 × 3 conv. 96 lReLU 3 × 3 conv. 96 lReLU 3 × 3 conv. 96 lReLU 2 × 2 max-pool, stride 2 3 × 3 conv. 192 lReLU 3 × 3 conv. 192 lReLU 3 × 3 conv. 192 lReLU 3 × 3 max-pool, stride 2 3 × 3 conv. 192 lReLU 1 × 1 conv. 192 lReLU 1 × 1 conv. 10 lReLU global average 10-way softmax  Model  discriminator D Input z ∈ R128 8 × 8 × 192 fc lReLU  2 × 2 perforated up-sampling 5 × 5 conv. 96 lReLU 5 × 5 conv. 96 lReLU 2 × 2 perforated up-sampling 5 × 5 conv. 96 lReLU 5 × 5 conv. 1 lReLU  14  Published as a conference paper at ICLR 2016  D.1.2 PERMUTATION INVARIANT MNIST  For the permutation invariant MNIST task we used fully connected generator and discriminator networks with leaky rectiﬁed linearities (and a leak rate of 0.1). For the discriminator we used the same architecture as in Rasmus et al. (2015), consisting of a network with 5 hidden layers (with sizes 1000, 500, 250, 250, 250 respectively). Batch normalization was applied to each of these layers and Gaussian noise was added to the batch normalized responses as well as the pixels of the input images (with a standard deviation of 0.3). The generator for this task consisted of a network with three hidden layers (with hidden sizes 500, 500, 1000) respectively. The output of this network was of size 784 = 28 × 28, producing pixel images, and used a sigmoid nonlinearity. The noise dimensionality for vectors z was chosen as Z = 128 and the cost weighting factor λ was simply set to λ = 1. Note that on MNIST the classiﬁer quickly learns to classify the few labeled examples leading to a vanishing supervised cost term; in a sense the labeled examples serve more as a “class initialization” in these experiments. We note that we found many different architectures to work well for this benchmark and merely settled on the described settings to keep our results somewhat comparable to the results from Rasmus et al. (2015).  D.1.3 CNNS FOR MNIST AND CIFAR-10  Full details regarding the CNN architectures used both for the generator and the discriminator are given in Table 4 for MNIST and in Table 5 for CIFAR-10. They are similar to the models from Rasmus et al. (2015) who, in turn, derived them from the best models found by Springenberg et al. (2015). In the Table ReLU denotes rectiﬁed linear units, lReLU denotes leaky rectiﬁed linear units (with leak rate 0.1), fc stands for a fully connected layer, conv for a convolutional layer and per- forated up-sampling denotes the deconvolution approach derived in Dosovitskiy et al. (2015) and Osendorfer et al. (2014).  E ADDITIONAL EXPERIMENTS  E.1 QUANTITATIVE EVALUATION OF THE GENERATIVE MODEL  Table 6 shows the sample log-likelihood for samples from an unsupervised CatGAN model. The CatGAN model performs comparable to the best existing algorithms; except for GMMN + AE which does not generate images directly but generates hidden layer activations of an AE that then reconstructs an image. As noted in the main paper we however want to caution the reader comparing generative models with respect to log-likelihood as measured by a Parzen-window estimate can be misleading (see Theis et al. (2015) for a recent in-depth discussion).  Algorithm GMMN (Li et al., 2015) GSN (Bengio et al., 2014) GAN (Goodfellow et al., 2014) CatGAN GMMN + AE (Li et al., 2015)  Log-likelihood  147 ± 2 214 ± 1 225 ± 2 237 ± 6 282 ± 2  Table 6: Comparison between different generative models on MNIST.  E.2 ADDITIONAL PLOTS FOR EXPERIMENTS ON SYNTHETIC DATA  In Figure 4, 4 and 6 we show the results of training k-means, RIM and CatGAN models on the three synthetic datasets from the main paper. Only the CatGAN model “correctly” clusters the data and, as an aside, also produces a generative model capable of generating data points that are almost indistinguishable from those present in the dataset. It should be mentioned that there exist clustering algorithms – such as DBSCAN (Ester et al., 1996) or spectral clustering methods – which can correctly identify the clusters in the datasets by making additional assumptions on the data distribution.  15  Published as a conference paper at ICLR 2016  Figure 4: Comparison between k-means, RIM and CatGAN – with neural networks – on the “blobs” dataset, with K = 3. In the decision boundary plots cyan denotes points whose class assignment is close to chance level (∀k : p(y = k, x, D) < 0.55). Note that the class identity is not known a priori as all models are trained unsupervisedly (hence the different color/class assignments for different models).  E.3 ADDITIONAL VISUALIZATIONS OF SAMPLES FROM THE GENERATIVE MODEL  We depict additional samples from an unsupervised CatGAN model trained on MNIST and Labeled Faces in the Wild (LFW)(Huang et al., 2007) in Figures 7 and 8. The architecture for the MNIST model is the same as in the semi-supervised experiments and the architecture for LFW is the same as for the CIFAR-10 experiments.  16  k-meansRIM + NNdata + class assignmentCatGANgenerated examplesdecision boundariesPublished as a conference paper at ICLR 2016  Figure 5: Comparison between k-means, RIM and CatGAN – with neural networks – on the “two moons” dataset, with K = 2. In the decision boundary plots cyan denotes points whose class assignment is close to chance level (∀k : p(y = k, x, D) < 0.55). Note that the class identity is not known a priori as all models are trained unsupervisedly (hence the different color/class assignments for different models).  17  k-meansRIM + NNdata + class assignmentCatGANgenerated examplesdecision boundariesPublished as a conference paper at ICLR 2016  Figure 6: Comparison between k-means, RIM and CatGAN – with neural networks – on the “circles” dataset. This ﬁgure complements Figure 2 from the main paper.  18  k-meansRIM + NNdata + class assignmentCatGANgenerated examplesdecision boundariesPublished as a conference paper at ICLR 2016  Figure 7: Samples generated by the generator neural network G for a CatGAN model trained on the MNIST dataset.  19  Published as a conference paper at ICLR 2016  Figure 8: Samples generated by the generator neural network G for a CatGAN model trained on cropped images from the LFW dataset.  20  ",
1511.05176,2016,MuProp: Unbiased Backpropagation For Stochastic Neural Networks,"['MuProp: Unbiased Backpropagation For Stochastic Neural Networks\nShixiang Gu', 'Sergey Levine', 'Ilya Sutskever', 'Andriy Mnih']",https://arxiv.org/pdf/1511.05176,"6 1 0 2     b e F 5 2         ]  G L . s c [      3 v 6 7 1 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  MUPROP: STOCHASTIC NEURAL NETWORKS  UNBIASED BACKPROPAGATION FOR  Shixiang Gu1 2, Sergey Levine3, Ilya Sutskever3, and Andriy Mnih4 1 University of Cambridge 2 MPI for Intelligent Systems, T¨ubingen, Germany 3 Google Brain 4 Google DeepMind {shanegu,slevine,ilyasu,amnih}@google.com  ABSTRACT  Deep neural networks are powerful parametric models that can be trained efﬁ- ciently using the backpropagation algorithm. Stochastic neural networks com- bine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropaga- tion is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains difﬁ- cult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio esti- mator by reducing its variance using a control variate based on the ﬁrst-order Tay- lor expansion of a mean-ﬁeld network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbi- ased and well behaved. Our experiments on structured output prediction and dis- crete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difﬁcult tasks.  1  INTRODUCTION  Deep neural networks (Krizhevsky et al., 2012; Hinton et al., 2012; Sutskever et al., 2014) are re- sponsible for numerous state-of-the-art results in a variety of domains, including computer vision, speech recognition, and natural language processing. The cornerstone of their success has been the simple and scalable backpropagation algorithm (Rumelhart et al., 1986). Backpropagation provides an efﬁcient way of computing the derivatives of the error with respect to the model parameters. The key to the applicability of backpropagation is to utilize deterministic, differentiable activation func- tions, such as sigmoidal and softmax units, to make the model differentiable. However, in certain real-world scenarios, it is more suitable to learn a model that can carry out a sequence of stochastic operations internally, in order to represent a complex stochastic process. Such stochastic networks are studied in policy gradient reinforcement learning methods (Williams, 1992; Weaver & Tao, 2001; Peters & Schaal, 2006), probabilistic latent variable models for structured prediction, unsupervised learning of generative models (Tang & Salakhutdinov, 2013; Kingma & Welling, 2014), and most recently, attention and memory networks (Mnih et al., 2014; Zaremba & Sutskever, 2015). The versatility of stochastic neural networks motivates research into more effective algorithms for training them. Models with continuous latent variables and simple approximate posteriors can al- ready be trained efﬁciently using the variational lower bound along with the reparameterization trick, which makes it possible to train both the model and the inference network using backpropagation (Kingma & Welling, 2014; Rezende et al., 2014). Training models with discrete latent variable dis- tributions, such as Bernoulli or multinomial, is considerably more difﬁcult. Unbiased estimators based on the likelihood-ratio method tend to be signiﬁcantly less effective than biased estimators, such as the straight-through method (Bengio et al., 2013; Raiko et al., 2015) and the estimator pro- posed by Gregor et al. (2014). We hypothesize that this is due to the fact that, unlike the biased estimators, the unbiased ones do not take advantage of the gradient information provided by the backpropagation algorithm. However, the biased estimators are heuristic and not well understood, which means that it is difﬁcult to enumerate the situations in which these estimators will work well.  1  Published as a conference paper at ICLR 2016  We posit that an effective method for training stochastic neural networks should take advantage of the highly efﬁcient backpropagation algorithm, while still providing the convergence guarantees of an unbiased estimator. To that end, we derive MuProp, an unbiased gradient estimator for deep stochastic neural networks that is based on backpropagation. To the best of our knowledge, it is the ﬁrst unbiased estimator that can handle both continuous and discrete stochastic variables while taking advantage of analytic gra- dient information. MuProp’s simple and general formulation allows a straightforward derivation of unbiased gradient estimators for arbitrary stochastic computational graphs – directed acyclic graph (DAG) with a mix of stochastic and deterministic computational nodes (see, e.g., Schulman et al. (2015)). While the algorithm is applicable to both continuous and discrete distributions, we used only discrete models in our experiments, since the reparameterization trick (Kingma & Welling, 2014) already provides an effective method for handling continuous variables. We present exper- imental results for training neural networks with discrete Bernoulli and multinomial variables for both supervised and unsupervised learning tasks. With these models, which are notoriously difﬁ- cult to train, biased methods often signiﬁcantly outperform the unbiased ones (Dayan et al., 1995; Gregor et al., 2014; Raiko et al., 2015), except in certain cases (Mnih & Gregor, 2014). Our results indicate that MuProp’s performance is more consistent and often superior to that of the competing estimators. It is the ﬁrst time that a well-grounded, unbiased estimator consistently performs as well or better than the biased gradient estimators across a range of difﬁcult tasks.  2 RELATED WORK  Probabilistic latent variable models described by neural networks date back to pioneering early work on sigmoidal belief networks and Helmholtz machines (Neal, 1992; Dayan et al., 1995). However, their adoption has been hindered by the lack of training methods that are both efﬁcient and theoreti- cally sound. Algorithms based on Markov chain Monte Carlo (Neal, 1992) and mean-ﬁeld inference (Saul et al., 1996) are theoretically well-grounded, but do not scale well to large models. The wake- sleep algorithm (Hinton et al., 1995) scales to large models, but does not optimize a well deﬁned- objective function, and therefore does not provide convergence guarantees. Recently, several new scalable methods have been proposed for training such models. A number of these algorithms use likelihood ratio estimators with variance reduction techniques (Ranganath et al., 2014; Mnih & Gre- gor, 2014; Gregor et al., 2014), inspired by methods from reinforcement learning (Williams, 1992; Weaver & Tao, 2001; Peters & Schaal, 2006). Another group of algorithms speciﬁcally addresses structured predictions tasks involving binary latent variables. Instead of performing inference using an inference network, these algorithms either use importance sampling to reweight samples from the prior (Tang & Salakhutdinov, 2013; Raiko et al., 2015), or rely on heuristics for approximate backpropagation through the stochastic units (Bengio et al., 2013; Raiko et al., 2015; Gregor et al., 2014).  2.1 LIKELIHOOD-RATIO GRADIENT ESTIMATION  Consider a simple stochastic system with discrete random variable x whose probability is given by pθ(x), and a loss function f (x). This formulation subsumes the training objectives of popular gen- erative models such as sigmoid belief networks, with or without inference networks. The objective of training is to minimize the expected cost L(θ) = Epθ(x)[f (x)]. The gradient g = ∂L/∂θ is usually intractable to compute exactly, and must therefore be estimated using an estimator ˆg(x), i.e.  i=1 ˆg(xi)/m, where xi ∼ pθ(x) and m is the number of Monte Carlo samples.  g ≈(cid:80)m  The likelihood-ratio (LR) estimator, of which the popular REINFORCE algorithm is a special case (Williams, 1992; Peters & Schaal, 2006), provides a convenient method for estimating this gradient, and serves as the basis for all of the unbiased estimators discussed in this paper, including MuProp. The LR method only requires that pθ(x) is differentiable with respect to θ:  g(LR) = ∇θEpθ(x)[f (x)] =  ∇θpθ(x)f (x)  (cid:88)  =  x  pθ(x)∇θ log pθ(x) · f (x) = Epθ(x)[∇θ log pθ · f (x)]  (1)  (cid:88)  x  ˆg(LR) = ∇θ log pθ(x) · f (x) where x ∼ pθ  2  Published as a conference paper at ICLR 2016  In its basic form, this estimator is unbiased, but tends to have high variance, and variance reduction techniques are typically required to make it practical (Williams, 1992; Peters & Schaal, 2006). A major deﬁciency of the LR estimator is that it fails to utilize information about the derivative of the cost function, unlike the most successful biased estimators. We will show that MuProp, which combines the derivative information with the LR estimator, can outperform these biased estimators, as well as the standard LR method.  2.2 VARIANCE REDUCTION WITH THE LIKELIHOOD-RATIO ESTIMATOR  High variance of LR estimators can make convergence very slow or impossible. This is further exacerbated by the fact that LR estimators for minibatch training of neural networks typically use m = 1, in order to maximize the variety of examples seen by the network (Kingma & Welling, 2014; Gregor et al., 2014; Rezende et al., 2014; Mnih & Gregor, 2014). The simplest way to reduce variance is to increase m, but this is computationally expensive. Efﬁcient and effective variance reduction techniques are therefore crucial for the LR estimator to be practical. The derivation of MuProp in Section 3 uses a variance reduction technique known as control variates (Paisley et al., 2012). The main idea is to subtract an analytically tractable term from the LR esti- mate in order to reduce the variance of the Monte Carlo estimate, and then add back the analytical expectation of this term to recover an unbiased estimator:  is always zero in expectation, since(cid:80)  Epθ(x)[∇θ log pθ(x) · f (x)] = Epθ(x)[∇θ log pθ(x) · (f (x) − b − h(x))] + µ (cid:80)  In this example, b + h(x) is a control variate, which is also known as a sample-dependent baseline. The expectation of the baseline needs to be added to the expression to make the resulting estimator unbiased. If the baseline is constant (i.e. h(x) = 0), its contribution to the gradient estimate x pθ(x) = ∇θ1 = 0. Otherwise, we must compute the expectation of the baseline (b + h(x))∇ log pθ(x) analytically, as µ = Epθ(x)[∇θ log pθ(x) · h(x)]. In our experiments, we utilize three types of variance reduction techniques for the estimators that include the same term as Eq. 2. While these techniques, proposed in a similar context in (Mnih & Gregor, 2014), do not result in the optimal variance reduction, they work well enough to make LR estimators useful in practice.1 For the precise details of these variance reduction techniques, please refer to Mnih & Gregor (2014). For reference, the techniques are:  x pθ(x)∇θ log pθ(x) = (cid:80)  x ∇θpθ(x) = ∇θ  (2)  • Centering the learning signal (C), which involves subtracting from the learning signal its  moving average (corresponding to b in Eq. 2).  • Input-dependent baseline (IDB), which allows baseline b to depend on the input or the sample from the previous layer (x0). This baseline is a neural network parameterized by ψ, and is trained jointly with the model to minimize the expected square of the centered learning signal Epθ(x|x0)[(l(x) − b − Bψ(x0))2].  • Variance normalization (VN), which keeps track of the moving average of the signal v). Unlike the other two techniques,  variance v, and divides the learning signal by max(1, this does not correspond to a baseline, and is a type of adaptive gradient.  √  3 MUPROP  The MuProp estimator has two components: a deterministic term ˆgM F , which is computed by back- propagation through a mean-ﬁeld network that we describe in Section 3.1, and a LR term ˆgR, which accounts for the residuals to produce an unbiased gradient estimate. In the following, let µx(θ) be Epθ(x)[x] and ¯x be any value that does not depend on x. The key idea of MuProp is summarized in the following equation:  ˆg(µ) = ∇θ log pθ(x) · [f (x) − f (¯x) − f(cid:48)(¯x)(x − ¯x)]  + f(cid:48)(¯x)∇θµx(θ)  (cid:125)  (cid:124)  (cid:123)(cid:122)  ˆgM F  (cid:125)  (3)  (cid:123)(cid:122)  ˆgR  (cid:124)  1Optimal baselines are somewhat involved, as they take into account the magnitude of ∇θ log pθ(x) and are  different for each parameter (Weaver & Tao, 2001).  3  Published as a conference paper at ICLR 2016  f (s)  s  x  h  θ  ¯f  ¯s  ¯x  ¯h  θ  ∂ ¯f ∂ ¯s  ∂ ¯f ∂ ¯x  ∂ ¯f ∂¯h  f (s)  ¯f (x)  ¯f (h)  s  x  h  θ  ¯s(x)  ¯s  ∂ ¯f ∂ ¯s  ¯x(h)  ∂ ¯f ∂ ¯x  ∂ ¯f ∂¯h  ¯f  ¯s  ¯x  ¯h  Figure 1: Standard MuProp (left) and MuProp with rollout (right). Circles indicate stochastic nodes. Blue, black, and red arrows indicate stochastic forward pass, deterministic mean-ﬁeld forward pass, and gradient ﬂow in backward pass respectively. ¯f indicates mean-ﬁeld evaluation of f.  where x ∼ pθ, and ˆg(µ) is an unbiased estimate of the gradient. The derivation follows directly from the baseline technique described in Section 2.2: MuProp uses a control variate that corresponds to the ﬁrst-order Taylor expansion of f around some ﬁxed value ¯x, i.e., h(x) = f (¯x) + f(cid:48)(¯x)(x − ¯x). The idea of using Taylor expansion as a baseline is also explored by (Paisley et al., 2012) and (Gregor et al., 2014); however, (Paisley et al., 2012) does not explore the estimator in the context of arbitrary stochastic computational graphs, and (Gregor et al., 2014) chooses a different form of Taylor expansion that makes the estimator biased. The latter approach also does not generalize to multi-layer graphs. Our contribution with MuProp is to extend the idea of Taylor expansions as baselines and make them applicable to arbitrary stochastic computational graphs. We will describe our derivation and design choices, as well as possible extensions to the standard form of MuProp that can enhance the statistical efﬁciency of the method. In the appendix, we also present a simple algorithm to compute the MuProp estimator using automatic differentiation, which makes it easy to integrate into any existing automatic differentiation library.  3.1 GENERALIZED BACKPROPAGATION WITH MEAN-FIELD NETWORKS  It is not always possible to directly apply Eq. 3 to an arbitrary computation graph because of the f(cid:48)(¯x) terms. If the computation graph includes a discrete sampling operation as in Figure 1, then one cannot directly deﬁne continuous gradient through such an operation. As a solution, we use a deterministic mean-ﬁeld network, which ignores the sampling operations in the original graph and propagates the mean values instead of the samples. Figure 1 shows how the backward pass computes the gradient with respect to θ. MuProp computes the Taylor expansion using the mean-ﬁeld network, which is fully differentiable, and uses these terms to reduce the variance. While the choice to use the mean-ﬁeld network seems arbitrary, we show that a proper recursive derivation of Eq. 3 for deep networks naturally leads to recursive Taylor expansions around the mean functions. Furthermore, if ¯x coincides with the mean-ﬁeld forward pass, the gradient estimator simpliﬁes to Eq. 3. The full derivation is given in the appendix.  4 COMPARISON WITH OTHER GRADIENT ESTIMATORS  While MuProp is derived from the LR estimator, which is also unbiased, it is closely related to two biased estimators commonly used to train stochastic binary networks. In this section we will describe these two estimators, the Straight-Through estimator and the 1/2 estimator. To simplify notation, we describe the estimators assuming one stochastic variable x. The provided estimator expressions however generalize easily to arbitrary stochastic computational graphs.2 The estimators mentioned above are summarized in Table 1 for completeness.  2Only the LR and MuProp estimators have a principled extension to deep networks. The other methods can  be extended to deep networks heuristically.  4  Published as a conference paper at ICLR 2016  We do not consider the importance-sampling based approaches (Raiko et al., 2015; Tang & Salakhut- dinov, 2013; Bornschein & Bengio, 2015; Burda et al., 2015) as these can be interpreted as optimiz- ing a different objective (Burda et al., 2015) that requires sampling the latent variables multiple times. The resulting estimators are more specialized than the ones considered in this paper, which are agnostic to the objective being optimized. As a result, MuProp and the other general estimators can be applied to the objective optimized by the importance-sampling based methods. We leave exploring this direction as future work.  4.1 THE STRAIGHT-THROUGH ESTIMATOR  The straight-through estimator (ST) (Bengio et al., 2013; Raiko et al., 2015) is a biased but low- variance estimator, devised primarily for binary stochastic neurons3. The idea is to backpropagate through the thresholding function as if it were the identity function. The estimator is given below:  ˆg(ST ) = f(cid:48)(x)∇θµx(θ)  (4)  The estimator resembles the ˆgM F term in our estimator in Eq. 3, since for a stochastic binary net- work the activation function is the mean function. The difference is that this estimator depends on sampled xi during backpropagation, while our formulation backpropagates the gradient through the mean-ﬁeld activations. Despite the heuristic derivation, this simple biased estimator works well in practice, most likely due to its overall similarity to the backpropagation algorithm. However, we show that MuProp signiﬁcantly outperforms ST estimators on certain tasks, which suggests that unbiased estimators are more reliable.  4.2 THE 1/2 ESTIMATOR  Gregor et al. (2014) proposed a biased estimator for the Deep AutoRegressive Network (DARN), which we refer to as the “1/2” estimator due to its particular choice of the baseline. Like the Straight- Through estimator, the 1/2 estimator is also specialized for stochastic binary networks. For models with only one stochastic unit, the 1/2 estimator corresponds to using the Taylor expansion around the sample x, evaluated at a ﬁxed point ¯x, as the baseline, such that h(x) = f (x) + f(cid:48)(x)(¯x − x). Since the expectation over h(x) cannot be computed analytically and ¯x cannot be chosen to make the baseline mean 0 for any arbitrary function f, this estimator is biased. Critically, the 1/2 estimator is derived for models with only one unit but is applied to large models by treating each unit as the sole unit of the model. Furthermore, the 1/2 estimator is not well-justiﬁed in the multi-layer case. However, backpropagating the gradient estimator derived in the single-layer case is shown to work well for stochastic binary neurons. The basic form of the estimator is given by  ˆg(1/2) = ∇θ log pθ(x) · (f(cid:48)(x)T · (x − ¯x))  (5)  If x consists of binary random variables, the expression can be further simpliﬁed by using ¯x = 1/2 (see (Gregor et al., 2014) for justiﬁcation); however, for non-quadratic f, the estimator is biased:  ˆg(1/2) =  f(cid:48)(x)∇θµx(θ)  2pθ(x)  (6)  If x consists of multinomial random variables with k categories, no sensible ¯x has been proposed. Since 1/2 estimator is only tested on binary variables, we experimented with three choices of ¯x: 1/2, 1/k, and µx(θ). Let µx(θ) = softmax(θ) be the mean of the multinomial units (θ ∈ Rk), and x ∈ Rk is one-hot encoding of the categorical values. Then the 1/2 estimator for the multinomial case becomes  (f(cid:48)(x)T · (x − ¯x))(xT · ∇θµx(θ))  ˆg(1/2) =  µx(θ)  (7)  3The ST estimator was proposed by Hinton (2012) and named in Bengio et al. (2013). The ST estimator can include the derivative of the sigmoid or not. Raiko et al. (2015) derive the version with the derivative of the sigmoid using another interpretation and differentiate it from the original ST estimator in their work. For simplicity, here we always assume that the ST estimator includes the derivative of the sigmoid, which is often essential for achieving the best performance (Raiko et al., 2015).  5  Published as a conference paper at ICLR 2016  Estimator Unbiased? Require baseline? Use Cost Gradient?  LR ST 1/2  MuProp  Yes No No Yes  Yes No No Yes  No Yes Yes Yes  Table 1: Summary of Gradient Estimators for Stochastic Discrete Variables  (a) Test negative log-likelihood (NLL)  (b) Test samples  Figure 2: Results for MNIST imputation dataset. MuProp is the only unbiased estimator that can compete with the biased estimators such as ST and 1/2.  5 EXPERIMENTS  We compare the LR, ST, and 1/2 estimators with the MuProp estimator on tasks that use a diverse set of network architectures. For the LR and the MuProp estimators, ‘-C’, ‘-VN’, and ‘-IDB’ indicate constant mean baseline, variance normalization, and input-dependent baseline respectively. The ﬁrst task does not make use of an inference network and involves direct optimization of an approximation to the expected objective. The second task involves training a sigmoid belief network jointly with an inference network by maximizing the variational lower bound on the intractable log-likelihood. MuProp performs consistently well at both tasks.  5.1 STRUCTURED OUTPUT PREDICTION  (cid:2)log(cid:0) 1  In this experiment, we follow the setup proposed by Raiko et al. (2015). The two tasks are to pre- dict the lower half of an MNIST digit given the top half, and to predict multiple facial expressions from an average face using Toronto Face dataset (TFD); the output distribution in both tasks exhibits complex multi-modality. For MNIST, the output pixels are binarized using the same protocol as in (Raiko et al., 2015). Given an input x, an output y, and stochastic hidden variables h, the objective is to maximize Ehi∼pθ(h|x) hood objective (Raiko et al., 2015; Burda et al., 2015). We use m = 1 for training, and m = 100 for validation and testing. For MNIST, a ﬁxed learning rate is chosen from {0.003, 0.001, .., 0.00003}, and the best test result is reported for each method. For the TFD dataset, the learning rate is chosen from the same list, but each learning rate is 10 times smaller. We used a momentum of 0.9 and minibatches of size 100. The input-dependent baseline (IDB) of Mnih & Gregor (2014) uses both the input and output as its input and has a single hidden-layer of 100 Tanh units.  i=1 pθ(y|hi)(cid:1)(cid:3), an importance-sampled estimate of the likeli- (cid:80)m  m  Estimator  MNIST 392-200-200-392  MNIST 392-200-200-200-392  TFD 2034-200-200-2034  LR-C-VN-IDB MuProp-C MuProp-C-VN  63.2 100.6 38.2  57.7 62.7 29.6  56.7 57.2 28.2  ST 56.1 54.4 27.7  1/2 57.2 55.8 27.6  Table 2: Test negative log-likelihood on MNIST 392-200-200-392 model.  6  050100150200250step(x104)5560657075808590NLLTestNegativeLog-likelihoodLR-C-VNLR-C-VN-IDBST1/2MuProp-CMuProp-C-VNPublished as a conference paper at ICLR 2016  Figure 3: Test negative variational lower-bound on 200-200-784 SBN model (left) and 200×10-784 categorical model (right). MuProp outperforms ST and 1/2 and converges faster than LR.  Figure 2a shows the test cost versus the number of parameter update steps of a typical run, while Table 2 summarizes the test performance for the different estimators. Convergence of the LR es- timators is signiﬁcantly slower than that of MuProp, ST, and 1/2. In fact, MuProp with simple mean subtraction is consistently better than LR with all of the variance reduction techniques. For MuProp, adding input-dependent baselines does not lead to any improvement over a simple constant mean baseline and variance normalization, and it is interesting to observe that MuProp’s sample- dependent-baseline reduces the variance signiﬁcantly more than the input-dependent baseline that also utilizes the output y. MuProp is slightly worse than ST and 1/2, and this performance gap can be explained by the fact that standard MuProp only uses a single-trunk mean-ﬁeld pass of the network, making the “mean” values at higher layers less correlated with the true conditional distri- bution. Section 6 discusses extensions to address this problem, at the cost of more computation or training an auxiliary network. Our results show that MuProp signiﬁcantly outperforms LR and closely matches the performance of ST and 1/2 in terms of both convergence speed and ﬁnal accuracy, and is the only unbiased estimator that can compete with the biased ST and 1/2 estimators on this task.  5.2 VARIATIONAL TRAINING OF GENERATIVE MODELS  In the second set of experiments, we apply MuProp to variational training of generative models. The auto-encoding variational Bayes framework (Kingma & Welling, 2014), along with similar methods, allows powerful generative models to be trained efﬁciently by replacing slow iterative inference algorithms with fast feedforward approximate inference networks. The inference networks, which map observations to samples from the variational posterior, are trained jointly with the model by maximizing a common objective. This objective is a variational lower bound on the marginal log- likelihood, and it is straight-forward to show that it is a stochastic computation graph with particular choice of cost. When the variational distributions over the latent variables have a speciﬁc form, such as conditional Gaussian, such models can be trained easily by using the reparameterization trick to backpropagate the gradient through the inference network (Kingma & Welling, 2014). However, Gaussian distribu- tions are not appropriate for all types of data or all models. If we opt to use discrete latent variables, we would have to choose between gradient estimators that are biased but have low variance (Gregor et al., 2014) or are unbiased but higher variance (Mnih & Gregor, 2014). As MuProp is unbiased and has relatively low variance due to its use of backpropagation for gradient estimation, we expect it to be particularly well-suited for training such models. We will concentrate on training layered belief networks with either Bernoulli or multinomial latent variables. The model in question is equivalent to a sigmoid belief network if it does not have au- toregressive connections, or to fDARN if it has autoregressive connections Gregor et al. (2014). The multinomial model uses 200 latent variables with 10 categories each (k = 10), and thus is referred to as the 200×10 model. We applied the models to the binarized MNIST dataset, which consists of 28×28 images of hand-written digits, and is commonly used for evaluating generative models. We trained the models using stochastic gradient descent using momentum of 0.9. The learning rate was  7  0100200300400500step(x104)100110120130140150NLLTestNegativeLog-likelihoodLR-CLR-C-IDBLR-C-VN-IDBST1/2MuProp-C02004006008001000step(x104)110120130140150160NLLTestNegativeLog-likelihoodLR-CLR-C-IDBLR-C-VN-IDBST1/2MuProp-CPublished as a conference paper at ICLR 2016  selected from {0.003, 0.001, .., 0.000003}, and the best test score is reported. For the 1/2 estimator in the categorical model, we found that ¯x=1/2 or 1/k worked best, and used the latter value for the results reported. As we used the same experimental setup and SBN/fDARN models, our results are directly comparable to those obtained using neural variational inference and learning (NVIL) in (Mnih & Gregor, 2014). Note that LR-C-VN-IDB is our implementation of NVIL.  Estimator  SBN 200-784  SBN 200-200-784  SBN 200-200-200-784 Categorical (200×10)-784  fDARN 200-784  LR-C-VN-IDB MuProp-C MuProp-C-IDB  113.5 99.7 97.5 92.1 107.7  113.6 100.4 101.1 92.9 108.6  113.1 100.4 98.6 92.7 107.8  ST 129.4 119.9 116.7 110.2 114.5  1/2 112.4 103.5 99.3 94.2 120.5  Table 3: Negative variational lower-bound on MNIST test data  Figure 3 shows sample training curves for typical runs, and Table 3 summarizes the results. These re- sults convincingly show that MuProp outperforms all the competing estimators. While the variance- reduced LR estimator (NVIL) performs very well on this task and is on par with MuProp on the ﬁnal variational lower-bound, MuProp converges signiﬁcantly faster (e.g. for the categorical model, about 3-4 times faster). This suggests that MuProp (with only mean subtraction) still has signif- icantly less variance than LR on this task, just as in Section 5.1. For the ST and 1/2 estimators, the story is more interesting. First, their ﬁnal variational lower-bounds are signiﬁcantly worse than MuProp or LR. More importantly, they exhibit little consistency in performance. On the SBN mod- els, 1/2 typically outperforms ST, while on the categorical model, the reverse is true. This result, along with the ﬂuctuations observed in the categorical model training curve for 1/2, suggests that such biased estimators can be unreliable for more complex models, and that their performance can vary drastically based on the cost function and model architecture. MuProp, on the other hand, is guaranteed to improve the desired objective because it is unbiased, while having much less variance than LR due to the use of the gradient information from backpropagation.  6 DISCUSSION  In this paper, we presented MuProp, which is an unbiased estimator of derivatives in stochastic computational graphs that combines the statistical efﬁciency of backpropagation with the correctness of a likelihood ratio method. MuProp has a number of natural extensions. First, we might consider using other functions for the baseline rather than just the Taylor expansion, which could be learned in a manner that resembles Q-learning (Watkins & Dayan, 1992) and target propagation (Lee et al., 2015). In reinforcement learning, ﬁtted Q-functions obtained by estimating the expected return of a given policy πθ sum- marize all future costs, and a good Q-function can greatly simplify the temporal credit assignment problem. Combining MuProp with such ﬁtted Q-functions could greatly reduce the variance of the estimator and make it better suited for very deep computational graphs, such as long recurrent neural networks and applications in reinforcement learning. The second extension is to make ¯x depend on samples of its parent nodes, as illustrated by rollout procedure in Figure 1. This could substantially improve performance on deeper networks, where the value from a single-trunk mean-ﬁeld pass may diverge signiﬁcantly from any samples drawn with a fully stochastic pass. By drawing ¯x using mean-ﬁeld passes originating at sampled values from preceding layers would prevent such divergence, though at additional computational cost, since the number of mean-ﬁeld passes would depend on the depth n of the network, for a total of O(n2) partial passes through the network. Intuitively, the single mean-ﬁeld “chain” would turn into a “tree,” with a sampled trunk and a different mean-ﬁeld branch at each layer.  ACKNOWLEDGMENTS  We sincerely thank Jascha Solh-dickstein, Laurent Dinh, Ben Poole, and Quoc Le for helpful dis- cussions and the Google Brain team for the support.  8  Published as a conference paper at ICLR 2016  REFERENCES Bengio, Yoshua, L´eonard, Nicholas, and Courville, Aaron. Estimating or propagating gradients through  stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.  Bornschein, J¨org and Bengio, Yoshua. Reweighted wake-sleep. ICLR, 2015.  Burda, Yuri, Grosse, Roger, and Salakhutdinov, Ruslan. Importance weighted autoencoders. arXiv preprint  arXiv:1509.00519, 2015.  Dayan, Peter, Hinton, Geoffrey E, Neal, Radford M, and Zemel, Richard S. The helmholtz machine. Neural  computation, 7(5):889–904, 1995.  Gregor, Karol, Danihelka, Ivo, Mnih, Andriy, Blundell, Charles, and Wierstra, Daan. Deep autoregressive  networks. In ICML, 2014.  Hinton, Geoffrey. Neural networks for machine learning. Coursera, video lectures, 2012.  Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82–97, 2012.  Hinton, Geoffrey E, Dayan, Peter, Frey, Brendan J, and Neal, Radford M. The ”wake-sleep” algorithm for  unsupervised neural networks. Science, 268(5214):1158–1161, 1995.  Kingma, Diederik P and Welling, Max. Auto-encoding variational bayes. ICLR, 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convolutional  neural networks. In NIPS, pp. 1097–1105, 2012.  Lee, Dong-Hyun, Zhang, Saizheng, Fischer, Asja, and Bengio, Yoshua. Difference target propagation.  Machine Learning and Knowledge Discovery in Databases, pp. 498–515. Springer, 2015.  In  Mnih, Andriy and Gregor, Karol. Neural variational inference and learning in belief networks. In ICML, 2014.  Mnih, Volodymyr, Heess, Nicolas, Graves, Alex, et al. Recurrent models of visual attention. In NIPS, 2014.  Neal, Radford M. Connectionist learning of belief networks. Artiﬁcial intelligence, 56(1):71–113, 1992.  Paisley, John, Blei, David, and Jordan, Michael. Variational bayesian inference with stochastic search. ICML,  2012.  Peters, Jan and Schaal, Stefan. Policy gradient methods for robotics. In Intelligent Robots and Systems, 2006  IEEE/RSJ International Conference on, pp. 2219–2225. IEEE, 2006.  Raiko, Tapani, Berglund, Mathias, Alain, Guillaume, and Dinh, Laurent. Techniques for learning binary  stochastic feedforward neural networks. ICLR, 2015.  Ranganath, Rajesh, Gerrish, Sean, and Blei, David M. Black box variational inference. AISTATS, 2014.  Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and approximate  inference in deep generative models. ICML, 2014.  Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning representations by back-  propagating errors. Nature, 323:533–536, 1986.  Saul, Lawrence K, Jaakkola, Tommi, and Jordan, Michael I. Mean ﬁeld theory for sigmoid belief networks.  Journal of artiﬁcial intelligence research, 4(1):61–76, 1996.  Schulman, John, Heess, Nicolas, Weber, Theophane, and Abbeel, Pieter. Gradient estimation using stochastic  computation graphs. NIPS, 2015.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc. Sequence to sequence learning with neural networks. In NIPS,  2014.  Tang, Yichuan and Salakhutdinov, Ruslan. Learning stochastic feedforward neural networks. In NIPS, 2013.  Watkins, Christopher JCH and Dayan, Peter. Q-learning. Machine learning, 8(3-4):279–292, 1992.  Weaver, Lex and Tao, Nigel. The optimal reward baseline for gradient-based reinforcement learning. In UAI,  pp. 538–545. Morgan Kaufmann Publishers Inc., 2001.  9  Published as a conference paper at ICLR 2016  Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement learning.  Machine learning, 8(3-4):229–256, 1992.  Zaremba, Wojciech and Sutskever, Ilya. Reinforcement learning neural turing machines. arXiv preprint  arXiv:1505.00521, 2015.  10  Published as a conference paper at ICLR 2016  APPENDIX  7 RECURSIVE DERIVATION OF MUPROP  i=1 p(xi|xi−1, θi−1) = (cid:81)n  show derivation assuming that p(x1, ..., xn|x0, θ) = (cid:81)n  Given a model with multiple layers of discrete random variables x1...xn and input x0, the gen- eral loss function can be written as L(x0, θ) = Ep(x1,...,xn|x0,θ)[fθ(x0:n)]. For simplicity, we i=1 pi and f (x1:n, θ) = f (xn, θn), where θ = {θ0, ..., θn}; however, the derivation is easy to extend to any directed acyclic graph. We assume that there is a continuous and differentiable mean function for each p(xi|xi−1, θi−1), denoted by µi(xi−1, θi−1) = Ep(xi|xi−1,θi−1)[xi]. The derivation is based on the recursive ﬁrst-order Taylor expansion around some arbitrary ﬁxed points ¯x1:n, which we detail later. We use µ(cid:48)  i and f(cid:48) to denote ∂µi }i=0:n. The ﬁnal expression for We ﬁrst derive the gradient for ∂L ∂θn the estimator resembles the classical backpropagation formulation, and allows efﬁcient computation through forward and backward passes. We intentionally avoid the standard matrix algebra notation to provide clean derivations, as the extension is straight-forward. = Ep1:n [∇θn f (xn, θn)]  . and then for { ∂L  and ∂f ∂xn  ∂θn−1  ∂xi−1  and  (8)  ∂θi  ∂L  ∂L ∂θn  ∂L  ∂θn−1  Epn [f (xn, θn)]]  = Ep1:n−1 [∇θn−1 = Ep1:n−1 [Epn [∇θn−1 log pn · f (xn, θn)]] = Ep1:n−1 [Epn [∇θn−1 log pn · [f (xn, θn) − f (¯xn, θn) − f(cid:48)(¯xn, θn) · (xn − ¯xn)]]  + Epn [∇θn−1 log pn · [f (¯xn, θn) + f(cid:48)(¯xn, θn) · (xn − ¯xn)]]] Epn [xN ]]  = Ep1:n−1 [Epn [∇θn−1 log pn · Rn] + f(cid:48)(¯xn, θn)∇θn−1 = Ep1:n−1 [Epn [∇θn−1 log pn · Rn] + f(cid:48)(¯xn, θn)∇θn−1µn(xn−1, θn−1)]  (9)  where we deﬁne the residuals Ri = µi+1(xi, θi) − µi+1(¯xi, θi) − µ(cid:48) µn+1(xn, θn) = f (xn, θn). Applying the same derivation recursively, we get:  i+1(¯xi, θi) · (xi − ¯xi) and let  ∂L ∂θi  = Ep1:n [∇θi log pi+1 · Rn] + f(cid:48)(¯xn, θn) · Ep1:n−1[∇θi log pi+1 · µn(xn−1, θn−1)] = Ep1:n [∇θi log pi+1 · Rn] + f(cid:48)(¯xn, θn)[Ep1:n−1[∇θi log pi+1 · Rn−1] + µ(cid:48) = Ep1:n [∇θi log pi+1 · (  n(¯xn−1, θn−1)[Ep1:n−2[∇θi log pi+1 · Rn−2] + ... + µ(cid:48)  µ(cid:48) j(¯xj−1, θj−1))]  n+1(cid:89)  n(cid:88)  Rk  i+2(¯xi+1, θi+1) · ∇θiµi+1(xi, θi)]]...]  n+1(cid:89)  + Ep1:i[(  k=i+1  j=k+2  j(¯xj−1, θj−1)) · ∇θiµi+1(xi, θi)] µ(cid:48)  j=i+2  where we slightly abuse the notation and assume(cid:81)n+1  j=n+2(µ(cid:48)  j) = 1. The estimator expression is:  (10)  n(cid:88)  n+1(cid:89)  n+1(cid:89)  ˆ∂L ∂θi (µ)  = ∇θi log pi+1 · (  Rk  µ(cid:48) j(¯xj−1, θj−1)) + (  k=i+1  j=k+2  j=i+2  j(¯xj−1, θj−1)) · ∇θiµi+1(xi, θi) µ(cid:48)  (11)  The formulation assumes a set of ﬁxed points ¯x1:n, whose only requirements are that ¯xi cannot depend on xi. If we choose ¯x1:n as given by deterministic mean-ﬁeld forward pass, the gradient estimator simpliﬁes and recovers Eq. 3.  11  Published as a conference paper at ICLR 2016  8 MUPROP WITH AUTOMATIC DIFFERENTIATION  We present a simple algorithm to compute MuProp gradient, taking advantage of the automatic dif- ferentiation functionalities. Algorithm 1 assumes that the automatic differentiation library provides two functionalities: Gradients(cost, inputs) which computes derivatives of cost with respect to inputs, and StopGradient(x) which returns a node with equal value with x but stops gradient when Gradients is called. It assumes that the graph consists of n stochastic nodes {xi}i=1:n and loss function f which can be the sum of multiple loss functions at different parts of the graph. PARENTSxi denotes the stochastic parental nodes of xi. ForwardPass builds the sym- bolic graph whose outputs can be differentiated using Gradients. When stochastic=true, each stochastic node samples the value and wraps the value using StopGradient such that no gradient is propagated through the stochastic operation. stochastic=false builds the mean-ﬁeld network that is fully differentiable. The algorithm also assumes it uses m = 1 sample, and in general the run time of the algorithm is 1 deterministic pass + m stochastic passes through the network. In practice, we augment the algorithm with the variance reduction techniques described in Section 2.2. Extensions discussed in Section 6 are also easy to include.  Algorithm 1 Compute MuProp Gradient Estimator Require: Input: x0, Parameters: θ  ¯x1:n, f (¯x1:n) ← ForwardPass(x0, stochastic=false) {f(cid:48)(¯xi)}i=1:n ← Gradients(f (¯x1:n), ¯x1:n) x1:n, µ1:n, f (x1:n),{log pθ(xi|PARENTSxi)}i=1:n ← ForwardPass(x0, stochastic=true) ci ← log pθ(xi|PARENTSxi)StopGradient(f (x1:n) − f (¯x1:n) − f(cid:48)(¯xi)T · (xi − ¯xi)) + µT · StopGradient(f(cid:48)(¯xi))  c ← f (x1:n) +(cid:80)n  ˆg(µ) ← Gradients(c, θ)  i=1 ci  12  ",
1511.06606,2016,Data Representation and Compression Using Linear-Programming Approximations,"['Data Representation and Compression Using Linear-Programming Approximations\nHristo Paskov', 'John Mitchell', 'Trevor Hastie']",https://arxiv.org/pdf/1511.06606,"6 1 0 2     y a M 3         ]  G L . s c [      5 v 6 0 6 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  DATA REPRESENTATION AND COMPRESSION USING LINEAR-PROGRAMMING APPROXIMATIONS  Hristo S. Paskov Computer Science Department Stanford University hpaskov@stanford.edu  John C. Mitchell Computer Science Department Stanford University jcm@stanford.edu  Trevor J. Hastie Statistics Department Stanford University hastie@stanford.edu  ABSTRACT  We propose ‘Dracula’, a new framework for unsupervised feature selection from sequential data such as text. Dracula learns a dictionary of n-grams that efﬁciently compresses a given corpus and recursively compresses its own dictionary; in ef- fect, Dracula is a ‘deep’ extension of Compressive Feature Learning. It requires solving a binary linear program that may be relaxed to a linear program. Both problems exhibit considerable structure, their solution paths are well behaved, and we identify parameters which control the depth and diversity of the dictionary. We also discuss how to derive features from the compressed documents and show that while certain unregularized linear models are invariant to the structure of the com- pressed dictionary, this structure may be used to regularize learning. Experiments are presented that demonstrate the efﬁcacy of Dracula’s features.  1  INTRODUCTION  At the core of any successful machine learning problem is a good feature representation that high- lights salient properties in the data and acts as an effective interface to the statistical model used for inference. This paper focuses on using classical ideas from compression to derive useful fea- ture representations for sequential data such as text. The basic tenets of compression abound in machine learning: the minimum description length principle can be used to justify regularization as well as various model selection criteria (Gabrilovich & Markovitch (2004)), while for unsupervised problems deep autoencoders (Salakhutdinov (2009)) and the classical K-means algorithm both seek a parsimonious description of data. Meanwhile, off-the-shelf compressors, such as LZ-77 (Ziv & Lempel (1977)), have been successfully applied to natural language problems as kernels that com- pute pairwise document similarities (Bratko et al. (2006)). We propose a new framework, Dracula, so called because it simultaneously ﬁnds a useful data representation and compression using linear-programming approximations of the criterion that mo- tivates dictionary-based compressors like LZ-77 (Ziv & Lempel (1977)). Dracula ﬁnds an explicit feature representation for the documents in a corpus by learning a dictionary of n-grams that is used to losslessly compress the corpus. It then recursively compresses the dictionary. This recursion makes Dracula a deep extension of Compressive Feature Learning (CFL) (Paskov et al. (2013)) that can ﬁnd exponentially smaller representations and promotes similar n-grams to enter the dictionary. As noted in (Paskov et al. (2013)), feature representations derived from off-the-shelf compressors are inferior because the algorithms used are sensitive to document order; both Dracula and CFL are invariant to document order. Our framework is expressed as a binary linear program (BLP) that can viewed as a linear program (LP) over a sufﬁciently constrained polyhedron or relaxed to an LP by relaxing the integrality con- straints. This is a notable departure from traditional deep learners (Salakhutdinov (2009); Socher & Manning (2013); LeCun et al. (2006)), which are formulated as non-convex, non-linear optimiza- tion problems. This structure makes it possible to analyze Dracula in view of well known techniques from convex analysis (e.g. the KKT conditions), polyhedral combinatorics, and graph theory. For example, we show that Dracula is easily parameterized to control the depth and diversity of its dictionary and that its solutions are well behaved as its parameters vary.  1  Published as a conference paper at ICLR 2016  This paper introduces Dracula in section 2 and discusses some of its problem structure and computa- tional properties, including its NP-Completeness. Section 3 uses Dracula’s polyhedral interpretation to explore the compressed representations it ﬁnds as its storage cost model varies. It also discusses how to extract features directly from a compression and how to integrate dictionary structure into the features. Finally, section 4 provides empirical evidence that deep compression ﬁnds hierarchical structure in data that is useful for learning and compression, and section 5 concludes.  2 DRACULA  This section introduces Dracula by showing how to extend CFL to a deep architecture that com- presses its own dictionary elements. We also show how to interpret any Dracula solution as a di- rected acyclic graph (DAG) that makes precise the notion of depth and provides useful statistical insights. Finally, we prove that Dracula is NP-Complete and discuss linear relaxation schemes. Notation Throughout this paper Σ is a ﬁxed ﬁnite alphabet and C = {D1, . . . , DN} is a ﬁxed document corpus with each Dk ∈ C a string Dk = ck i ∈ Σ. An n-gram is any substring of some Dk and S is the set of all n-grams in the document corpus, including the original documents. For any s ∈ S a pointer p is a triple p = (s, l ∈ {1, . . . ,|s|}, z ∈ S) indicating that z = sl . . . sl+|z|−1. We say that p uses z at location l in s. Let P be the set of all valid pointers and for any P ⊂ P we use P (s) = {p ∈ P|p = (s, l, z)} to select pointers whose ﬁrst element is s, e.g. P = ∪s∈SP(s). Moreover, P uses z ∈ S if there is some p ∈ P using z, and P reconstructs s ∈ S if every location in s is covered by at least one pointer, i.e. ∪(s,l,v)∈P (s){l, . . . , l + |v| − 1} = {1, . . . ,|s|}. Conceptually, s is recovered from P by iterating through the (s, l, v) ∈ P and ”pasting” a copy of v into location l of a blank string. It will be helpful to deﬁne PC = ∪s∈CP(s) to be the set of pointers that can only be used to reconstruct the corpus.  j of characters ck  1 . . . ck  2.1 CFL CFL represents document corpus C by storing a dictionary S ⊂ S, a set of n-grams, along with a pointer set P ⊂ PC that only uses dictionary n-grams and losslessly reconstructs each of the docu- ments in C. Importantly, CFL stores the dictionary directly in plaintext. The overall representation is chosen to minimize its total storage cost for a given storage cost model that speciﬁes ds, the cost of including n-gram s ∈ S in the dictionary, as well as cp, the cost of including pointer p ∈ PC in the pointer set. Selecting an optimal CFL representation may thus be expressed as  subject to P reconstructs Dk ∀Dk ∈ C; P only uses s ∈ S.  ds  (1)  (cid:88)  p∈P  (cid:88)  s∈S  minimize S⊂S,P⊂PC  cp +  This optimization problem naturally decomposes into subproblems by observing that when the dic- tionary is ﬁxed, selecting the optimal pointer set decouples into |C| separate problems of optimally reconstructing each corpus document. We thus deﬁne the reconstruction module for document Dk ∈ C, which takes as input a dictionary S and outputs the minimum cost of reconstructing Dk with pointers that only use strings in S. Note that speciﬁc pointers and dictionary strings can be disallowed by setting their respective costs to ∞. For example setting ds = ∞ for all s ∈ S longer than a certain length limits the size of dictionary n-grams. Of course, in practice, any variables with inﬁnite costs are simply disregarded. The reconstruction module can be expressed as a BLP by associating with every pointer p ∈ P(Dk) a binary indicator variable wp ∈ {0, 1} whereby wp = 1 indicates that p is included in the optimal pointer set for Dk. We similarly use binary variables ts ∈ {0, 1} to indicate that s ∈ S is included in the dictionary. Since there is a one-to-one correspondence between pointer sets (dictionaries) and w ∈ {0, 1}|P(Dk)| (t ∈ {0, 1}|S|), the vector storing the wp (ts), we will directly refer to these vectors as pointer sets (dictionaries). Lossless reconstruction is encoded by the constraint X Dk w ≥ 1 where X Dk ∈ {0, 1}|Dk|×|P(Dk)| is a binary matrix indicating the indices of Dk that each pointer can reconstruct. In particular, for every p = (Dk, l, z) ∈ P(Dk), column X Dk is all zeros except for a contiguous sequence of 1’s in indices l, . . . , l + |z| − 1. Control of which pointers may be used (based on the dictionary) is achieved by the constraint w ≤ V Dk t where V Dk ∈ {0, 1}|P(Dk)|×|S| contains a row for every pointer indicating the string it uses. In particular,  p  2  Published as a conference paper at ICLR 2016  for every p = (Dk, l, z), V Dk may now be expressed as  RDk (t; c) = minimize  w∈{0,1}|P(Dk )|  p,z = 1 is the only non-zero entry in the row pertaining to p. The BLP  wpcp  subject to X Dk w ≥ 1; w ≤ V Dk t.  (2)  The optimization problem corresponding to an optimal CFL representation may now be written as a BLP by sharing the dictionary variable t among the reconstruction modules for all documents in C:  minimize t∈{0,1}|S|  RDk (t, c) +  tsds  (3)  (cid:88)  s∈S  p∈P(Dk)  (cid:88)  (cid:88)  Dk∈C  2.2 ADDING DEPTH WITH DRACULA  The simplicity of CFL’s dictionary storage scheme is a fundamental shortcoming that is demon- strated by the string aa . . . a consisting of the character a replicated 22n times. Let the cost of using any pointer be cp = 1 and the cost of storing any dictionary n-gram be its length, i.e. ds = |s|. The best CFL can do is to store a single dictionary element of length 2n and repeat it 2n times, incurring a total storage cost of 2n+1. In contrast, a “deep” compression scheme that recursively compresses its own dictionary by allowing dictionary strings to be represented using pointers attains exponential space savings relative to CFL. In particular, the deep scheme constructs dictionary strings of length 2, 4, . . . , 22n−1 recursively and incurs a total storage cost of 4n 1. Dracula extends CFL precisely in this hierarchical manner by allowing dictionary strings to be ex- pressed as a combination of characters and pointers from shorter dictionary strings. CFL thus cor- responds to a shallow special case of Dracula which only uses characters to reconstruct dictionary n-grams. This depth allows Dracula to leverage similarities among the dictionary strings to obtain further compression of the data. It also establishes a hierarchy among dictionary strings that allows us to interpret Dracula’s representations as a directed acyclic graph (DAG) that makes precise the notion of representation depth. Formally, a Dracula compression (compression for brevity) of corpus C is a triple D = (S ⊂ S, P ⊂ PC, ˆP ⊂ P) consisting of dictionary, a pointer set P that reconstructs the documents in C, and a pointer set ˆP that reconstructs every dictionary string in S. As with CFL, any pointers in P may only use strings in S. However, a pointer p ∈ ˆP reconstructing a dictionary string s ∈ S is valid if it uses a unigram (irrespective of whether the unigram is in S) or a proper substring of s that is in S. This is necessary because unigrams take on the special role of characters for dictionary strings. They are the atomic units of any dictionary, so the character set Σ is assumed to be globally known for dictionary reconstruction. In contrast, document pointers are not allowed to use characters and may only use a unigram if it is present in S; this ensures that all strings used to reconstruct the corpus are included in the dictionary for use as features. Finding an optimal Dracula representation may also be expressed as a BLP through simple mod- iﬁcations of CFL’s objective function. In essence, the potential dictionary strings in S are treated like documents that only need to be reconstructed if they are used by some pointer. We extend the storage cost model to specify costs cp for all pointers p ∈ PC used for document reconstruction as well as costs ˆcp for all pointers p ∈ P used for dictionary reconstruction. In keeping with the aformentioned restrictions we assume that ˆcp = ∞ if p = (s, 1, s) illegally tries to use s to recon- struct s and s is not a unigram. The dictionary cost ds is now interpreted as the “overhead” cost of including s ∈ S in the dictionary without regard to how it is reconstructed; CFL uses the ds to also encode the cost of storing s in plaintext (e.g. reconstructing it only with characters). Finally, we introduce dictionary reconstruction modules as analogs to the (document) reconstruction modules for dictionary strings: the reconstruction module for s ∈ S takes as input a dictionary and outputs the cheapest valid reconstruction of s if s needs to be reconstructed. This can be written as the BLP  ˆRs(t; ˆc) = minimize w∈{0,1}|P(s)|  wpˆcp  subject to X sw ≥ ts1; w ≤ ˆV st.  (4)  (cid:88)  p∈P(s)  1Note that the recursive model is allowed to use pointers in the dictionary and therefore selects from a larger pointer set than CFL. Care must be taken to ensure that the comparison is fair since the “size” of a compression is determing by the storage cost model and we could “cheat” by setting all dictionary pointer costs to 0. Setting all pointer costs to 1 ensures fairness.  3  Published as a conference paper at ICLR 2016  Here X s is analogously deﬁned as in equation (4) and ˆV s is analogous to V s in equation (4) except that it does not contain any rows for unigram pointers. With this setup in mind, the optimization problem corresponding to an optimal Dracula representation may be written as the BLP  (cid:88)  Dk∈C  (cid:104)  (cid:88)  s∈S  (cid:105)  minimize t∈{0,1}|S|  RDk (t, c) +  tsds + ˆRs(t; ˆc)  (5)  Finally, any compression can be interpreted graphically as, and is equivalent to, a DAG whose vertices correspond to members of Σ, S, or C and whose labeled edge set is determined by the pointers: for every (s, l, z) ∈ P or ˆP there is a directed edge from z to s with label l. Note that D deﬁnes a multi-graph since there may be multiple edges between nodes. Figure 1 shows the graph corresponding to a simple compression. As this graph encodes all of the information stored by D, and vice versa, we will at times treat D directly as a graph. Since D has no cycles, we can organize its vertices into layers akin to those formed by deep neural networks and with connections determined by the pointer set: layer 0 consists only of characters (i.e. there is a node for every character in Σ), layer 1 consists of all dictionary n-grams constructed solely from characters, higher levels pertain to longer dictionary n-grams, and the highest level consists of the document corpus C. While there are multiple ways to organize the intermediate layers, a simple stratiﬁcation is obtained by placing s ∈ S into layer i only if ˆP (s) uses a string in layer i−1 and no strings in layers i+1, . . . . We note that our architecture differs from most conventional deep learning architectures which tend to focus on pairwise layer connections – we allow arbitrary connections to higher layers.  Figure 1: Compression of “aabaabaax” using a 3-layered dictionary. Layer 0 consists of characters; layers 1 and 2 are dictionary n-grams. There are three kinds of pointers: character to dictionary n-gram (dashed blue lines), dictionary n-gram to (longer) dictionary n-gram (solid blue line), and dictionary n-gram to document (double red lines).  2.3 COMPUTATIONAL HARDNESS AND RELAXATION  The document and dictionary reconstruction modules RDk / ˆRs are the basic building blocks of Dracula; when dictionary t is ﬁxed, solving equation (5) is tantamount to solving the reconstruction modules separately. The discussion in the Appendix section A.1 shows that for a ﬁxed binary t, solving RDk or ˆRs is easy because of the structure of the constraint matrices X Dk /X s. In fact, this problem is equivalent to a min-cost ﬂow problem. Similarly, if the pointer sets are known for each document or dictionary string then it is easy to ﬁnd the corresponding dictionary t by checking which strings are used (in linear time relative to the number of pointers). One would hope that the easiness of Dracula’s subproblems leads to an easy overall learning problem. However, learning the dictionary and pointer sets simultaneously makes this problem hard: Dracula is NP- Complete. In particular, it requires solving a binary LP (which are NP-Complete in general) and it generalizes CFL which is itself NP-Complete (Paskov et al. (2014)) (see section 3.1.1 for how to restrict representations to be shallow). We thus turn to solving Dracula approximately via its LP relaxation. This is obtained by replacing all binary constraints in equations (2),(4),(5) with interval constraints [0, 1]. We let QC denote this LP’s constraint polyhedron and note that it is a subset of the unit hypercube. Importantly, we may also interpret the original problem in equation (5) as an LP over a polyhedron Q whose vertices are always binary and hence always has binary basic solutions. Here Q2 is the convex hull of all (binary) Dracula solutions and Q ⊂ QC; all valid Dracula solutions may be obtained from the linear relaxation. In fact, the Chv´atal-Gomory theorem (Schrijver (2003)) shows that we may “prune”  2Note that unlike QC, this polyhedron is likely to be difﬁcult to describe succinctly unless P = N P .  4  Published as a conference paper at ICLR 2016  QC into Q by adding additional constraints. We describe additional constraints in the Appendix section A.1.1 that leverage insights from sufﬁx trees to prune QC into a tighter approximation Q(cid:48) C ⊂ QC of Q. Remarkably, when applied to natural language data, these constraints allowed Gurobi (Gurobi Optimization (2015)) to quickly ﬁnd optimal binary solutions. While we did not use these binary solutions in our learning experiments, they warrant further investigation. As the pointer and dictionary costs vary, the resulting problems will vary in difﬁculty as measured by the gap between the objectives of the LP and binary solutions. When the costs force either t or the wDk /ws to be binary, our earlier reasoning shows that the entire solution will lie on a binary vertex of QC that is necessarily optimal for the corresponding BLP and the gap will be 0. This reasoning also shows how to round any continuous solution into a binary one by leveraging the easiness of the individual subproblems. First set all non-zero entries in t to 1, then reconstruct the documents and dictionary using this dictionary to yield binary pointers, and ﬁnally ﬁnd the minimum cost dictionary based on which strings are used in the pointers.  3 LEARNING WITH COMPRESSED FEATURES  This section explores the feature representations and compressions that can be obtained from Drac- ula. Central to our discussion is the observation of section 2.3 that all compressions obtained from Dracula are the vertices of a polyhedron. Each of these vertices can be obtained as the optimal com- pression for an appropriate storage cost model3, so we take a dual perspective in which we vary the storage costs to characterize which vertices exist and how they relate to one another. The ﬁrst part of this section shows how to “walk” around the surface of Dracula’s polyhedron and it highlights some “landmark” compressions that are encountered, including ones that lead to classical bag-of-n-grams features. Our discussion applies to both, the binary and relaxed, versions of Dracula since the former can viewed as an LP over a polyhedron Q with only binary vertices. The second part of this section shows how to incorporate dictionary structure into features via a dictionary diffusion process. We derive features from a compression in a bag-of-n-grams (BoN) manner by counting the number of pointers that use each dictionary string or character. It will be useful to explicitly distinguish between strings and characters when computing our representations and we will use squares brackets to denote the character inside a unigram, e.g. [c] . Recall that given a compression D = (S, P, ˆP ), a unigram pointer in P (used to reconstruct a document) is interpreted as a string whereas a unigram pointer in ˆP is interpreted as a character. We refer to any z ∈ S ∪ Σ as a feature and associate with every document Dk ∈ C or dictionary string s ∈ S a BoN feature vector xDk , xs ∈ Z|S|+|Σ| , respectively. Entry xDk z = |{p ∈ P (s)| p = (Dk, l, z)}|, and will necessarily have xDk z = 0 for all z ∈ Σ. Dictionary strings are treated analogously with the caveat that if p = (s, l, z) ∈ ˆP uses a unigram, p counts towards the character entry xDk  counts the number of pointers that use z to reconstruct Dk, i.e. xDk  +  z  [z] , not xDk z .  3.1 DRACULA’S SOLUTION PATH  Exploring Dracula’s compressions is tantamount to varying the dictionary and pointer costs supplied to Dracula. When these costs can be expressed as continuous functions of a parameter λ ∈ [0, 1], i.e. ∀s ∈ S, p ∈ PC, ˆp ∈ P the cost functions ds(λ), cp(λ), ˆc ˆp(λ) are continuous, the optimal solution sets vary in a predictable manner around the surface of Dracula’s constraint polyhedron Q or the polyhedron of its relaxation QC. We use F (Q) to denote the set of faces of polyhedron Q (including Q), and take the dimension of a face to be the dimension of its afﬁne hull. We prove the following theorem in the Appendix section A.3: Theorem 1. Let Q ⊂ Rd be a bounded polyhedron with nonempty interior and b : [0, 1] → Rd a continuous function. Then for some N ∈ Z+∪{∞} there exists a countable partition Γ = {γi}N i=0 of [0, 1] with corresponding faces Fi ∈ F (Q) satisfying Fi (cid:54)= Fi+1 and Fi∩Fi+1 (cid:54)= ∅. For all α ∈ γi, the solution set of the LP constrained by Q and using cost vector b(α) is Fi = arg minx∈Q xT b(α). Moreover, Fi never has the same dimension as Fi+1 and the boundary between γi, γi+1 is )[ iff dim Fi < dim Fi+1 and ]( otherwise.  3The storage costs pertaining to each vertex form a polyhedral cone, see (Ziegler (1995)) for details.  5  Published as a conference paper at ICLR 2016  (a)  (b)  (c)  Figure 2: Part (a) shows a nonlinear projection of a subset of Dracula’s constraint polyhedron Q in which every vertex corresponds to a distinct compression of “xaxabxabxacxac”. Part (b) is the pro- jection’s polar; its faces delineate the (linear) costs for which each vertex in (a) is optimal. The red/ purple/ blue line in (b) demonstrates a continuous family of costs. All red (blue) costs are uniquely minimized by the vertex in (a) highlighted in red (blue), respectively; (c) shows the corresponding compressions. Purple costs lie on the edge between the faces containing the red and blue lines and are minimized by any convex combination of the vertices highlighted in (a).  This theorem generalizes the notion of a continuous solution path typically seen in the context of regularization (e.g. the Lasso) to the LP setting where unique solutions are piecewise constant and transitions occur by going through values of λ for which the solution set is not unique. For instance, suppose that vertex v0 is uniquely optimal for some λ0 ∈ [0, 1), another vertex v1 is uniquely optimal for a λ0 < λ1 ≤ 1, and no other vertices are optimal in (λ0, λ1). Then Theorem 1 shows that v0 and v1 must be connected by a face (typically an edge) and there must be some λ ∈ (λ0, λ1) for which this face is optimal. As such, varying Dracula’s cost function continuously ensures that the solution set for the binary or relaxed problem will not suddenly “jump” from one vertex to the next; it must go through an intermediary connecting face. This behavior is depicted in Figure 2 on a nonlinear projection of Dracula’s constraint polyhedron for the string “xaxabxabxacxac”. It is worthwhile to note that determining the exact value of λ for which the face connecting v0 and v1 is optimal is unrealistic in practice, so transitions may appear abrupt. While it is possible to smooth this behavior by adding a strongly convex term to the objective (e.g. an L2 penalty), the important insight provided by this theorem is that the trajectory of the solution path depends entirely on the combinatorial structure of Q or QC. This structure is characterized by the face lattice4 of the polyhedron and it shows which vertices are connected via edges, 2-faces, . . . , facets. It limits, for example, the set of vertices reachable from v0 when the costs vary continuously and ensure that transitions take place only along edges 5. This predictable behavior is desirable when ﬁne tuning the compression for a learning task, akin to how one might tune the regularization parameter of a Lasso, and it is not possible to show in general for non-convex functions.  4We leave it as an open problem to analytically characterize Dracula’s face lattice. 5Restricting transitions only to edges is possible with probability 1 by adding a small amount of Gaussian  noise to c.  6  Published as a conference paper at ICLR 2016  We now provide a simple linear cost scheme that has globally predictable effects on the dictionary. For all s ∈ S, p ∈ PC, ˆp ∈ P we set ds = τ, cp = 1, ˆc ˆp = αλ if ˆp uses as unigram (i.e. is a character), and ˆc ˆp = λ otherwise. We constrain τ, λ ≥ 0 and α ∈ [0, 1]. In words, all document pointer costs are 1, all dictionary costs τ, and dictionary pointer costs are λ if they use a string and αλ if they use a character. The effects these parameters have on the compression may be understood by varying a single parameter and holding all others constant: Varying τ controls the minimum frequency with which s ∈ S must be used before it enters the dictionary; if few pointers use s it is cheaper to construct s “in place” using shorter n-grams. Long n-grams appear less frequently so ↑ τ biases the dictionary towards shorter n-grams. Varying λ has a similar effect to τ in that it becomes more expensive to construct s as λ increases, so the overall cost of dictionary membership increases. The effect is more nuanced, however, since the manner in which s is constructed also matters; s is more likely to enter the dictionary if it shares long substrings with existing dictionary strings. This suggests a kind of grouping effect whereby groups of strings that share many substrings are likely to enter together. Varying α controls the Dracula’s propensity to use characters in place of pointers in the dictionary and thereby directly modulates dictionary depth. When α < 1 K for K = 2, 3, . . . , all dictionary n-grams of length at most K are constructed entirely from characters.  3.1.1 LANDMARKS ON DRACULA’S POLYHEDRON  While Dracula’s representations are typically deep and space saving, it is important to note that valid Dracula solutions include all of CFL’s solutions as well as a set of fully redundant representations that use as many pointers as possible. The BoN features computed from these “space maximizing” compressions yield the traditional BoN features containing all n-grams up to a maximum length K. A cost scheme that includes all pointers using all n-grams up to length K is obtained by setting all costs to be negative, except for ts = ∞ for all s ∈ S where |s| > K (to disallow these strings). The optimal compression then includes all pointers with negative cost and each document position is reconstructed K times. Moreover, it is possible to restrict representations to be valid CFL solutions by disallowing all non-unigram pointers for dictionary reconstruction, i.e. by setting ˆcp = ∞ if p is not a single character string.  3.2 DICTIONARY DIFFUSION  We now discuss how to incorporate dictionary information from a compression D = (S, P, ˆP ) into the BoN features for each corpus document. It will be convenient to store the BoN feature vectors xDk for each document as rows in a feature matrix X ∈ Z|C|×(|S|+|Σ|) and the BoN feature vectors xs for each dictionary string as rows in a feature matrix G ∈ Z(|S|+|Σ|)×(|S|+|Σ|). We also include rows of all 0’s for every character in Σ to make G a square matrix for mathematical convenience. Graphically, this procedure transforms D into a simpler DAG, DR, by collapsing all multi-edges into single edges and labeling the resulting edges with an appropriate xs z. For any two features s, z, we say that s is higher (lower) order than z if it is a successor (predecessor) of z in D. Once our feature extraction process throws away positional information in the pointers higher order features capture more information than their lower order constituents since the presence of an s ∈ S formed by concatenating features z1 . . . zm indicates the order in which the zi appear and not just that they appear. Conversely, since each zi appears in the same locations as s (and typically many others), we can obtain better estimates for coefﬁcients associated with zi than for the coefﬁcient of s. If the learning problem does not require the information speciﬁed by s we pay an unnecessary cost in variance by using this feature over the more frequent zi. In view of this reasoning, feature matrix X captures the highest order information about the docu- ments but overlooks the features’ lower order n-grams (that are indirectly used to reconstruct docu- ments). This latter information is provided by the dictionary’s structure in G and can be incorporated by a graph diffusion process that propagates the counts of s in each document to its constituent zi, which propagate these counts to the lower order features used to construct them, and so on. This process stops once we reach the characters comprising s since they are atomic. We can express this s xs spreads xDk s zi, the number of times each zi  information ﬂow in terms of G by noting that the product GT xDk =(cid:80)  to each of the zi used to reconstruct s by multiplying xDk  s with xs  s∈S∪Σ xDk  7  Published as a conference paper at ICLR 2016  feature matrix ˆX = XH where H = I +(cid:80)∞  units of ﬂow to each parent zi, and is directly used in s. Graphically, node s in DR sends xDk zi, the strength of the edge connecting zi to s. Performing this ﬂow is modulated in proportion to xs zi to the features this procedure a second time, i.e. multiplying GT (GT xDk ), further spreads xDk used to reconstruct zi, modulated in proportion to their usage. Iterating this procedure deﬁnes a new n=1 Gn spreads the top level xDk to the entire graph6. We can interpret the effect of the dictionary diffusion process in view of two equivalent regularized learning problems that learn coefﬁcients β, η ∈ R|S∪Σ| for every feature in S ∪ Σ by solving  s xs  s  minimize β∈R|S∪Σ| ≡ minimize η∈R|S∪Σ|  L( ˆXβ) + λR(β) L(Xη) + λR ((I − G)η) .  (6)  We assume that L is a convex loss (that may implicitly encode any labels), R is a convex regular- ization penalty that attains its minimum at β = 0, and that a minimizer β∗ exists. Note that adding an unpenalized offset does not affect our analysis. The two problems are equivalent because H is deﬁned in terms of a convergent Neumann series and, in particular, H = (I − G)−1 is invertible. We may switch from one problem to the other by setting β = H−1η or η = Hβ. When λ = 0 the two problems reduce to estimating β/η for unregularized models that only differ in the features they use, ˆX or X respectively. The equivalence of the problems shows, however, that using ˆX in place of X has no effect on the models as their predictions are always the same. Indeed, if β∗ is optimal for the ﬁrst problem then η∗ = Hβ∗ is optimal for the second and for any z ∈ R|S∪Σ|, the predictions zT η∗ = (zT H)β∗ are the same. Unregularized linear models – including generalized linear models – are therefore invariant to the dictionary reconstruction scheme and only depend on the document feature counts xDk, i.e. how documents are reconstructed. When λ > 0, using ˆX in place of X results in a kind of graph Laplacian regularizer that encourages ηs to be close to ηT xs. One interpretation of this is effect is that ηs acts a “label” for s: we use its feature representation to make a prediction for what ηs should be and penalize the model for any deviations. A complementary line of reasoning uses the collapsed DAG DR to show that (6) favors lower order features. Associated with every node s ∈ S ∪ Σ is a ﬂow ηs and node z sends ηz units of ﬂow to each of its children s. This ﬂow is attenuated (or ampliﬁed) by xs z, the strength of the edge connecting z to s. In turn, s adds its incoming ﬂows and sends out ηs units of ﬂow to its children; each document’s prediction is given by the sum of its incoming ﬂows. Here R acts a kind of “ﬂow conservation” penalty that penalizes nodes for sending out a different amount of ﬂow than they receive and the lowest order nodes (characters) are penalized for any ﬂow. From this viewpoint it follows that the model prefers to disrupt the ﬂow conservation of lower order nodes whenever they sufﬁciently decrease the loss since they inﬂuence the largest number documents. Higher order nodes inﬂuence fewer documents than their lower order constituents and act as high frequency components.  4 EXPERIMENTS  This section presents experiments comparing traditional BoN features with features derived from Dracula and CFL. Our primary goal is investigate whether deep compression can provide better fea- tures for learning than shallow compression or the traditional “fully redundant” BoN representation (using all n-grams up to a maximum length). Since any of these representations can be obtained from Dracula using an appropriate cost scheme, positive evidence for the deep compression implies Dracula is uncovering hierarchical structure which is simultaneously useful for compression and learning. We also provide a measure of compressed size that counts the number of pointers used by each representation, i.e. the result of evaluating each compression with a “common sense” space ob- jective where all costs are 1. We use Top to indicate BoN features counting only document pointers (X in previous section), Flat for dictionary diffusion features (i.e. ˆX), CFL for BoN features from CFL, and All for traditional BoN features using all n-grams considered by Dracula.  6This sum converges because G corresponds to a ﬁnite DAG so it can be permuted to a strictly lower  triangular matrix so that  n→∞Gn = 0. See Appendix section A.2 for weighted variations. lim  8  Published as a conference paper at ICLR 2016  Figure 3: Proteins represented using the 4th and 5th singular vectors of Top features from Dracula.  Table 1: Bacteria Identiﬁcation Accuracy using Protein Data  SVD Rank  5  10  15  20  All CFL Top  59.5 89.7 87.5  77.7 85.0 91.2  83.3 76.9 89.0  77.6 74.5 83.3  All  81.1 74.0 84.3  # Pointers 4.54×105 2.69×104 1.76×104  We used Gurobi (Gurobi Optimization (2015)) to solve the reﬁned LP relaxation of Dracula for all of our experiments. While Gurobi can solve impressively large LP’s, encoding Dracula for a general- purpose solver is inefﬁcient and limited the scale of our experiments. Dedicated algorithms that utilize problem structure, such as the network ﬂow interpretation of the reconstruction modules, are the subject of a follow-up paper and will allow Dracula to scale to large-scale datasets. We limited our parameter tuning to the dictionary pointer cost λ (discussed in the solution path section) as this had the largest effect on performance. Experiments were performed with τ = 0, α = 1, a maximum n-gram length, and only on n-grams that appear at least twice in each corpus.  Protein Data We ran Dracula using 7-grams and λ = 1 on 131 protein sequences that are labeled with the kingdom and phylum of their organism of origin (pro). Bacterial proteins (73) dominate this dataset, 68 of which evenly come from Actinobacteria (A) and Fermicutes (F). The ﬁrst 5 singular values (SV’s) of the Top features show a clear separation from the remaining SV’s and Figure 3 plots the proteins when represented by their 4th and 5th principle components. They are labeled by kingdom and, in more interesting cases, by phylum. Note the clear separation of the kingdoms, the two main bacterial phyla, and the cluster of plants separated from the other eukaryotes. Table 1 shows the average accuracy of two binary classiﬁcation tasks in which bacteria are positive and we hold out either phylum A or F, along with other randomly sampled phyla for negative cases, as a testing set. We compare All features to Top features from Dracula and CFL using an (cid:96)2-regularized SVM with C = 1. Since there are many more features than training examples we plot the effect of using the top K principle components of each feature matrix. Flat features did not help and performance strictly decreased if we limited the n-gram length for All features, indicating that long n-grams contain essential information. Both compression criteria perform well, but using a deep dictionary seems to help as Dracula’s proﬁle is more stable than CFL’s.  Stylometry We extracted 100 sentences from each of the training and testing splits of the Reuters dataset (Liu) for 10 authors, i.e. 2, 000 total sentences, and replaced their words with part-of-speech tags. The goal of this task is to predict the author of a given set of writing samples (that all come from the same author). We make predictions by representing each author by the centroid of her 100 training sentences, averaging together the unknown writing samples, and reporting the nearest author centroid to the sample centroid. We ran Dracula on this representation with 10-grams and normalized centroids by their (cid:96)1 norm and features by their standard deviation. Table 2 compares the performance of All features to Top features derived from various λ’s for various testing sentence sample sizes. We report the average of 1, 000 trials, where each trial tested every author once and  9  Published as a conference paper at ICLR 2016  Table 2: Author Identiﬁcation Accuracy  # Samples  5  10  25  50  75  All CFL λ = 20 Top λ = 1 Top λ = 10 Top λ = 20  36.0 39.6 35.1 39.6 37.7  47.9 50.5 46.2 51.0 49.4  67.9 73.8 68.6 75.0 73.8  80.6 87.5 85.3 88.9 91.5  86.4 91.4 93.7 93.7 97.8  # Pointers 5.01×105 3.33×104 2.39×104 3.00×104 3.32×104  Table 3: Sentiment Classiﬁcation Accuracy  λ: MNL # Pointers Top  Flat  n-gram Len. NB All  SVM (cid:96)1 All  SVM (cid:96)2 All  0.25 0.5 1 2 5  4.02 3.78 3.19 2.51 1.96  1.79×105 1.75×105 1.71×105 1.71×105 1.86×105  73.9 75.1 76.6 78.0 78.0  78.2 78.8 78.2 78.1 78.0  5 4 3 2 1  77.9 77.9 78.4 78.8 78.0  76.6 76.8 77.0 77.2 76.3  76.9 77.0 77.2 77.5 76.5  randomly selected a set of sample sentences from the testing split sentences. As in the protein data, neither Flat nor shorter n-gram features helped, indicating that higher order features contain vital information. CFL with λ = 20 strictly dominated every other CFL representation and is the only one included for brevity. Dracula with λ = 10 or λ = 20 shows a clear separation from the other schemes, indicating that the deep compression ﬁnds useful structure.  Sentiment Prediction We use a dataset of 10, 662 movie review sentences (Pang & Lee (2005)) labeled as having positive or negative sentiment. Bigrams achieve state-of-the-art accuracy on this dataset and unigrams perform nearly as well (Wang & Manning (2012)), so enough information is stored in low order n-grams that the variance from longer n-grams hurts prediction. We ran Dracula using 5-grams to highlight the utility of Flat features, which focus the classiﬁer onto lower order features. Following (Wang & Manning (2012)), Table 3 compares the 10-fold CV accuracy of a multinomial na¨ıve-Bayes (NB) classiﬁer using Top or Flat features with one using all n-grams up to a maximum length. The dictionary diffusion process successfully highlights relevant low or- der features and allows the Flat representation to be competitive with bigrams (the expected best performer). The table also plots the mean n-gram length (MNL) used by document pointers as a function of λ. The MNL decreases as λ increases and this eventually pushes the Top features to behave like a mix of bigrams and unigrams. Finally, we also show the performance of (cid:96)2 or (cid:96)1- regularized support vector machines for which we tuned the regularization parameter to minimize CV error (to avoid issues with parameter tuning). It is known that NB performs surprisingly well relative to SVMs on a variety of sentiment prediction tasks, so the dropoff in performance is ex- pected. Both SVMs achieve their best accuracy with bigrams; the regularizers are unable to fully remove the spurious features introduced by using overly long n-grams. In contrast, Flat achieves its best performance with larger MNLs which suggests that Dracula performs a different kind of feature selection than is possible with direct (cid:96)1/(cid:96)2 regularization. Moreover, tuning λ combines fea- ture selection with NB or any kind of classiﬁer, irrespective of whether it natively performs feature selection.  5 CONCLUSION  We have introduced a novel dictionary-based compression framework for feature selection from sequential data such as text. Dracula extends CFL, which ﬁnds a shallow dictionary of n-grams with which to compress a document corpus, by applying the compression recursively to the dictionary. It thereby learns a deep representation of the dictionary n-grams and document corpus. Experiments  10  Published as a conference paper at ICLR 2016  with biological, stylometric, and natural language data conﬁrm the usefulness of features derived from Dracula, suggesting that deep compression uncovers relevant structure in the data. A variety of extensions are possible, the most immediate of which is the design of an algorithm that takes advantage of the problem structure in Dracula. We have identiﬁed the basic subproblems com- prising Dracula, as well as essential structure in these subproblems, that can be leveraged to scale the compression to large datasets. Ultimately, we hope to use Dracula to explore large and fundamental datasets, such as the human genome, and to investigate the kinds of structures it uncovers.  ACKNOWLEDGEMENTS Dedicated to Ivan i Kalinka Hand(cid:25)ievi (Ivan and Kalinka Handjievi). Funding provided by the Air Force Ofﬁce of Scientiﬁc Research and the National Science Foundation.  REFERENCES Protein classiﬁcation benchmark collection. http://hydra.icgeb.trieste.it/benchmark/index.php?page=00.  Bertsimas, Dimitris and Weismantel, Robert. Optimization over integers. Athena Scientiﬁc, 2005.  ISBN 978-0-97591-462-5.  Bratko, Andrej, Filipiˇc, Bogdan, Cormack, Gordon V., Lynam, Thomas R., and Zupan, Blaˇz. Spam  ﬁltering using statistical data compression models. JMLR, 7:2673–2698, 2006.  Gabrilovich, Evgeniy and Markovitch, Shaul. Text categorization with many redundant features:  Using aggressive feature selection to make SVMs competitive with C4.5. In ICML, 2004.  Gurobi Optimization, Inc. Gurobi optimizer reference manual, 2015. URL http://www.  gurobi.com.  Gusﬁeld, Dan. Algorithms on Strings, Trees, and Sequences - Computer Science and Computational  Biology. Cambridge University Press, 1997. ISBN 0-521-58519-8.  LeCun, Yann, Chopra, Sumit, Hadsell, Raia, Ranzato, Marc’Aurelio, and Huang, Fu-Jie. A tutorial on energy-based learning. In Bakir, G., Hofman, T., Sch¨olkopf, B., Smola, A., and Taskar, B. (eds.), Predicting Structured Data. MIT Press, 2006.  Liu, Zhi. Reuter 50 50 data set.  Lu, Shu and Robinson, Stephen M. Normal fans of polyhedral convex sets. Set-Valued Analysis, 16  (2-3):281–305, 2008.  Pang, Bo and Lee, Lillian. Seeing stars: Exploiting class relationships for sentiment categorization In Proceedings of the 43rd Annual Meeting on Association for  with respect to rating scales. Computational Linguistics, pp. 115–124. Association for Computational Linguistics, 2005.  Paskov, Hristo, West, Robert, Mitchell, John, and Hastie, Trevor. Compressive feature learning. In  NIPS, 2013.  Paskov, Hristo, Mitchell, John, and Hastie, Trevor. An efﬁcient algorithm for large scale compressive  feature learning. In AISTATS, 2014.  Salakhutdinov, Ruslan. Learning deep generative models, 2009.  Schrijver, A. Combinatorial Optimization - Polyhedra and Efﬁciency. Springer, 2003.  Socher, Richard and Manning, Christopher D. Deep learning for NLP (without magic). In Confer- ence of the North American Chapter of the Association of Computational Linguistics, Proceed- ings, pp. 1–3, 2013.  Wang, Sida and Manning, Christopher D. Baselines and bigrams: Simple, good sentiment and topic classiﬁcation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pp. 90–94. Association for Computational Linguistics, 2012.  11  Published as a conference paper at ICLR 2016  Ziegler, Gunter M. Lectures on Polytopes. Graduate texts in mathematics. Springer-Verlag, 1995.  Ziv, Jacob and Lempel, Abraham. A universal algorithm for sequential data compression. TIT, 23  (3):337–343, 1977.  A APPENDIX  A.1 RECONSTRUCTION MODULES  The reconstruction modules RDk / ˆRs are the basic building blocks of Dracula; when t is ﬁxed solving (5) is tantamount to solving the reconstruction modules separately. These simple BLPs have a number of properties that result in computational savings because of the structure of the constraint matrix X Dk /X s. In order to simplify notation we deﬁne  Ts(t, v; b, V ) = minimize w∈{0,1}|P(s)|  wpbp  subject to X sw ≥ v1; w ≤ V t.  (7)  (cid:88)  p∈P(s)  Using TDk (t, 1; c, V Dk ) = RDk (t; c) and Ts(t, ts; ˆc, ˆV s) = ˆRs(t; ˆc) results in the document or dictionary reconstruction modules. Now note that every column in X s is all zeros except for a con- tiguous sequence of ones so that X s is an interval matrix and therefore totally unimodular (TUM). Deﬁne T c  s to be the LP relaxation of Ts obtained by replacing the integrality constraints: subject to X sw ≥ v1; w ≤ V t.  (cid:88)  wpbp  s (t, v; b, V ) = minimize T c w∈[0,1]|P(s)|  (8)  p∈P(s)  Aside from X, the remaining constraints on w are bound constraints. It follows from (Bertsimas & Weismantel (2005)) that T c Proposition 1. If the arguments t, v are integral, then all basic solutions of T c  s is an LP over a integral polyhedron so we may conclude that  s (t, v; b, V ) are binary.  Indeed, a simple dynamic program discussed in (Paskov et al. (2013)) solves Ts efﬁciently. Our second property reformulates T c s by transforming the constraint matrix X s into a simpler form. The resulting matrix has at most 2 non-zero entries per column instead of up to |s| non-zero entries per column in X s. This form is more efﬁcient to work with when solving the LP and it shows s is equivalent to a min-cost ﬂow problem over an appropriately deﬁned graph. Deﬁne Q ∈ that T c {0,±1}|s|×|s| be the full rank lower triangular matrix with entries Qs (i+1)i = 1 and 0 elsewhere (and Qs|s||s| = 1). The interval structure of X s implies that column i of Z s = QsX s is all zeros except for Z s ij = 1 and k > j is the ﬁrst ik = 0 after the sequences of ones (if such a k exists). By introducing non-negative row in which X s slack variables for the X sw ≥ v1 constraint, i.e. writing X sw = v1 + ξ, and noting that Qs1 = e1, where e1 is all zeros except for a 1 as its ﬁrst entry, we arrive at:  ik = 1 where j is the ﬁrst row in which X s  ii = −Qs  ij = −Z s  s (t, v; b, V ) = minimize T c  w,ξ  wpbp  p∈P(s)  subject to Z sw − Qsξ = ve1, 0 ≤ w ≤ V t, 0 ≤ ξ.  (9)  The matrix Ψ = [Z s| − Qs] has special structure since every column has at most one 1 and at most one −1. This allows us to interpret Ψ as the incidence matrix of a directed graph if we add source and sink nodes with which to ﬁll all columns out so that they have exactly one 1 and one −1. T c may then be interpreted as a min-cost ﬂow problem.  s  A.1.1 POLYHEDRAL REFINEMENT  We now show how to tighten Dracula’s LP relaxation by adding additional constraints to QC to shrink it closer to Q. If every time we see a string s it is followed by the character α (in a given corpus), the strings s and sα belong to the same equivalence class; the presence of sα conveys the same information as the presence of s. Importantly, the theory of sufﬁx trees shows that all substrings  12  (cid:88)  Published as a conference paper at ICLR 2016  (cid:80) s∈ε ts ≤ 1 to the LP relaxation to tighten QC will not remove any binary solutions.  of a document corpus can be grouped into at most 2n−1 equivalence classes (Gusﬁeld (1997)) where n is the word count of the corpus. We always take equivalence classes to be inclusion-wise maximal sets and say that equivalence class ε ⊂ S appears at a location if any (i.e. all) of its members appear at that location. We prove the following theorem below. This theorem veriﬁes common sense and implies that, when the pointer costs do not favor any particular string in ε, adding the constraint Theorem 2. Let Ω denote the set of all equivalence classes in corpus C and suppose that all costs are non-negative and ∀ε ∈ Ω,∀z ∈ S,∀s, x ∈ ε, the dictionary costs ds = dx are equal, the pointer costs cz q ) whenever pointers p = q = (l, h) refer to the same location and use the same string (or character) h. Then there is an optimal compression D = (S, P ˆP ) in which S contains at most one member of ε.  q) are equal when p = (l, s) and q = (l, x), and cs  p = ˆcx  p = cx  p = cz  p = ˆcz  q ( ˆcs  q (ˆcz  q  p = cDk  Proof. Suppose that the conditions for theorem 1 hold, let ε be an equivalence class, let D = (S, P, ˆP ) be an optimal compression, and suppose for the sake of contradiction that s1, s2 ∈ ε are both included in the optimal dictionary. Without loss of generality we assume that |s1| < |s2|. Consider ﬁrst document pointer p which uses s1 for document Dk. By assumption there is another so we are indifferent in our choice. pointer q which uses s2 in the same location and cDk We thereby may replace all document pointers that use s1 with equivalent ones that use s2 without changing the objective value. Consider next the usage of s1 to construct higher order dictionary elements. We must be careful here since if some dictionary element s3 is in the optimal dictionary S and can be expressed as s3 = zs1 for some string z then we may not use s2 in place of s1 since it would lead to a different dictionary string. The key step here is to realize that s3 must belong to the same equivalence class as string zs2 and we can use zs2 in place of s3 in all documents. If s3 is itself used to construct higher order dictionary elements, we can apply the same argument for s2 to zs2 in an inductive manner. Eventually, since our text is ﬁnite, we will reach the highest order strings in the dictionary, none of whose equivalence class peers construct any other dictionary n-grams. Our earlier argument shows that we can simply take the longest of the highest order n-grams that belong to the same equivalence class. Going back to s3, we note that our assumptions imply that the cost of constructing zs2 is identical to the cost of constructing s3 so we may safely replace s3 with zs2. The only remaining place where s1 may be used now is to construct s2. However, our assumptions imply that the cost of constructing s1 “in place” when constructing s2 is the same. By eliminating s1 we therefore never can do worse, and we may strictly improve the objective if ts1 > 0 or s1 is used to construct s2 and its pointer cost is non-zero. QED.  A.2 WEIGHTED DIFFUSION When G is generated from the relaxation of Dracula and t ∈ (0, 1]|S| are the dictionary coefﬁcients, any s ∈ S with ts < 1 will have Gsz ≤ ts∀z ∈ S. In order to prevent overly attenuating the diffusion we may wish to normalize row s in G by t−1 for consistency. We note that a variety of other weightings are also possible to different effects. For example, weighting G by a scalar ρ ≥ 0 attenuates or enhances the entire diffusion process and mitigates or enhances the effect of features the farther away they are from directly constructing any feature directly used in the documents.  s  A.3 PROOF OF PATH THEOREM  theorem of linear programming states that for any c ∈ Rd, S(c, Q) ≡ The fundamental arg minx∈Q xT c(α) ∈ F (Q) since Q has non-empty interior and is therefore non-empty. We will use a construction known as the normal fan of Q, denoted by N (Q), that partitions Rd into a ﬁnite set of polyhedral cones pertaining to (linear) objectives for which each face in F (Q) is the solution set. We begin with some helpful deﬁnitions.  A partition P ⊂ 2X of a set X is any collection of sets satisfying(cid:83)  p∈P p = X and ∀p, q ∈ P p (cid:54)= q implies p∩q = ∅. The relative interior of a convex set X ⊂ Rd, denoted by relint X, is the interior of X with respect to its afﬁne hull. Formally, relint X = {x ∈ X | ∃ε > 0, B(x, ε) ∩ affX ⊂ X}. The following deﬁnition is taken from (Lu & Robinson (2008)): A fan is a ﬁnite set of nonempty polyhedral convex cones in Rd, N = {N1, N2, . . . , Nm}, satisfying:  13  Published as a conference paper at ICLR 2016  1. any nonempty face of any cone in N is also in N , 2. any nonempty intersection of any two cones in N is a face of both cones.  This deﬁnition leads to the following lemma, which is adapted from (Lu & Robinson (2008)):  Lemma 1. Let N be a fan in Rd and S =(cid:83)  N∈N N the union of its cones.  1. If two cones N1, N2 ∈ N satisfy (relintN1) ∩ N2 (cid:54)= ∅ then N1 ⊂ N2,  2. The relative interiors of the cones in N partition S, i.e.(cid:83)  N∈N relintN = S.  Lemma 1 is subtle but important as it contains a key geometric insight that allow us to prove our theorem. Next, let Q ⊂ Rd be a bounded polyhedron with vertex set V and nonempty interior, i.e. whose afﬁne hull is d-dimensional. For any face F ∈ F (Q) deﬁne V (F ) = F ∩ V to  be the vertices of F and NF = (cid:8)y ∈ Rd | ∀x ∈ F,∀z ∈ Q, yT x ≤ yT z(cid:9) to be the normal cone (cid:8)y ∈ Rd | ∀x ∈ V (F ),∀z ∈ V, yT x ≤ yT z(cid:9). The normal fan for Q, N (Q) = {NF}F∈F (Q), is  to F . That NF is a (pointed) polyhedral cone follows from noting that it can be equivalently expressed as a ﬁnite collection of linear constraints involving the vertices of F and Q: NF =  deﬁned to be the set of all normal cones for faces of Q. Noting that Q is bounded and therefore has a recession cone of {0}, the following Lemma is implied by Proposition 1 and Corollary 1 of (Lu & Robinson (2008)): Lemma 2. Let N (Q) be the normal fan of a bounded polyhedron Q with non-empty interior in R. Then  1. N (Q) is a fan, 2. for any nonempty faces F1, F2 ∈ F (Q), F1 ⊂ F2 iff NF1 ⊃ NF2,  3. (cid:83) 4. every nonempty face F ∈ F (Q) satisﬁes relintNF =(cid:8)y ∈ Rd | F = S(y, Q)(cid:9).  F∈F (Q) relintNF = Rd,  We will also makes use of the following two results. The ﬁrst is implied by Theorem 2.7, Corollary 2.14, and Problem 7.1 in Ziegler (1995): Lemma 3. Let Q ⊂ Rd be a bounded polyhedron with nonempty interior, F ∈ F (Q), and NF the normal cone to F . Then dim F + dim NF = d. The second Lemma states a kind of neighborliness for the cones in N (Q): Lemma 4. Let Q ⊂ Rd be a bounded polyhedron with nonempty interior. For any N ∈ N (Q) and x ∈ relint N there exists a δ > 0 such that for any y ∈ B(x, δ) there is a N(cid:48) ∈ N (Q) with y ∈ relint N(cid:48) and N ⊂ N(cid:48).  Proof. Let N ∈ N (Q) and x ∈ N be given. We say that N(cid:48) ∈ N (Q) occurs within δ (for δ > 0) if there is some y ∈ B(x, δ) with y ∈ relint N(cid:48). Now suppose that there is an N(cid:48) ∈ N (Q) that occurs within δ for all δ > 0. Since N(cid:48) is a closed convex cone it must be that x ∈ N(cid:48) so we may conclude from Lemma 1 that N ⊂ N(cid:48). Next, let M be the set of cones in N (Q) which do not contain N and suppose that for all δ > 0 there is some N(cid:48) ∈ M that occurs within δ. Since |M| is ﬁnite, this is only possible if there is a cone N(cid:48) ∈ M that occurs within δ for all δ > 0. However, this leads to a contradiction since N(cid:48) must contain N so the Lemma follows.  We are now ready to prove our main theorem which is restated below with S(c, Q) = arg minx∈Q xT c(α) for simplicity. Theorem 3. Let Q ⊂ Rd be a bounded polyhedron with nonempty interior and c : [0, 1] → Rd a continuous function. Then for some N ∈ Z+ ∪ {∞} there exists a countable partition Γ = {γi}N of [0, 1] with corresponding faces Fi ∈ F (Q) satisfying Fi (cid:54)= Fi+1 and Fi ∩ Fi+1 (cid:54)= ∅ and Fi = S(c(α), Q)∀α ∈ γi. Moreover, Fi never has the same dimension as Fi+1 and the boundary between γi, γi+1 is )[ iff dim Fi < dim Fi+1 and ]( otherwise.  i=0  14  Published as a conference paper at ICLR 2016  i  (cid:9)Nk  i ∈ F (Q) satisfying F k  i = S(c(α), Q)∀α ∈ γk i .  (cid:8)x ∈ [0, 1] | dim Nf (x) ≥ k(cid:9) to be the set of all arguments to c whose normal cone is at least k- sis that for some Nk ∈ Z+ ∪ {∞} there exists a countable partition Γk = (cid:8)γk  Proof. For ease of notation let f (x) = S(c(x), Q) and for k = 0, . . . , d deﬁne ωk = dimensional. Moreover, for any x ∈ [0, 1] deﬁne σ(x) = {y ∈ [0, x] | ∀z ∈ [y, x], f (x) = f (z)} ∪ {y ∈ [x, 1] | ∀z ∈ [x, y], f (x) = f (z)} to be the largest contiguous set containing x over which f remains constant and let m(x) = inf σ(x) and M (x) = sup σ(x) be its inﬁnimum and supremem, respectively. The proof follows by induction on k = d, d − 1, . . . , 0 with the inductive hypothe- i=0 of ωk with  There are two cases to consider. Thus,  corresponding faces F k Base case (k = d): Let x ∈ ωd so that σ(x) ⊂ ωd. Since Nf (x) is d-dimensional, int Nf (x) = relint Nf (x) so continuity of c implies that σ(x) is a (non-empty) open interval with m(x) < M (x). It follows that Γk = {σ(x) | x ∈ ωd} deﬁnes a partition of ωd into a set of open intervals. Each in- terval contains (an inﬁnite number) of rational numbers, and we see that Γk is countable by assigning to each interval a rational number that it contains. Inductive step: Let x ∈ ωk\ωk+1. If m(x) < M (x) then (m(x), M (x)) ⊂ σ(x) contains a rational number. o = {σ(x) | x ∈ ωk\ωk+1, m(x) < M (x)} is countable. Otherwise, if m(x) = x = M (x) then by Lemma 4 there is a δ > 0 such that if y ∈ B(x, δ) then Nf (x) ⊂ NS(y,Q). Conti- nuity of c implies that there is a ε > 0 for which c((x − ε, x + ε)) ⊂ B(x, δ) and hence (x − ε, x + ε)\{x} ⊂ ωk+1. Assigning to x any rational number in (x − ε, x + ε) and letting c = {σ(x) | x ∈ ωk\ωk+1, m(x) = M (x)}, we may appeal to the inductive hypothesis to con- Γk c ∪ Γk+1 is a ﬁnite union of countable sets and clude that Γk therefore countable. Since ω0 = [0, 1] we have shown that Γ = Γ0 is a countable partition of [0, 1] into intervals over which f is constant. Now consider two consecutive intervals γi, γi+1 ∈ Γ and let M be the supre- mum of γi. If M /∈ γi then since cone NFi is closed, c(M ) ∈ NFi. Since c(M ) ∈ relint NFi+1 by assumption, it follows that NFi+1 is a proper subset of NFi and hence that Fi is a proper subset of Fi+1. Otherwise, if M ∈ γi then the continuity of c and Lemma 4 imply that NFi is a proper subset of NFi+1 so Fi+1 is a proper subset of Fi. In either case Fi ∩ Fi+1 (cid:54)= ∅ and Lemma 3 implies the dimensionality result of our Theorem.  c is countable. Finally, Γk = Γk  the set Γk  o ∪ Γk  15  ",
1511.05077,2016,Diversity Networks,"['Diversity Networks\nZelda Mariet', 'Suvrit Sra']",https://arxiv.org/pdf/1511.05077,"7 1 0 2    r p A 8 1         ]  G L . s c [      6 v 7 7 0 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  Diversity Networks: Neural Network Compression Using Deter- minantal Point Processes  Zelda Mariet and Suvrit Sra Massachusetts Institute of Technology Cambridge, MA 02139, USA zelda@csail.mit.edu,suvrit@mit.edu  Abstract  We introduce Divnet, a ﬂexible technique for learning networks with di- verse neurons. Divnet models neuronal diversity by placing a Determi- nantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Di- vnet oﬀers a more principled, ﬂexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables eﬀec- tive auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.  1  Introduction  Training neural networks requires setting several hyper-parameters to adequate values: num- ber of hidden layers, number of neurons per hidden layer, learning rate, momentum, dropout rate, etc. Although tuning such hyper-parameters via parameter search has been recently investigated by Maclaurin et al. (2015), doing so remains extremely costly, which makes it imperative to develop more eﬃcient techniques.  Of the many hyper-parameters, those that determine the network’s architecture are among the hardest to tune, especially because changing them during training is more diﬃcult than adjusting more dynamic parameters such as the learning rate or momentum. Typically, the architecture parameters are set once and for all before training begins. Thus, assigning them correctly is paramount: if the network is too small, it will not learn well; if it is too large, it may take signiﬁcantly longer to train while running the risk of overﬁtting. Networks are therefore usually trained with more parameters than necessary, and pruned once the training is complete. This paper introduces Divnet, a new technique for reducing the size of a network. Divnet decreases the amount of redundancy in a neural network (and hence its size) in two steps: ﬁrst, it samples a diverse subset of neurons; then, it merges the remaining neurons with the ones previously selected. Speciﬁcally, Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) (Hough et al., 2006) over neurons in a layer, which is then used to select a subset of diverse neurons. Subsequently, Divnet “fuses” information from the dropped neurons into the selected ones through a reweighting procedure. Together, these steps reduce network size (and act as implicit regularization), without requiring any further training or signiﬁcantly hurting performance. Divnet is fast and runs in time negligible compared to the network’s prior training time. Moreover, it is agnostic to other network parameters such as activation functions, number of hidden layers, and learning rates.  1  Published as a conference paper at ICLR 2016  For simplicity, we describe and analyze Divnet for feed-forward neural networks, however Divnet is not limited to this setting. Indeed, since Divnet operates on a layer fully connected to the following one in a network’s hierarchy, it applies equally well to other architectures with fully connected layers. For example, it can be applied without any further modiﬁcation to Deep Belief Nets and to the fully-connected layers in Convolutional Neural Networks. As these layers are typically responsible for the large majority of the CNNs’ memory footprint (Yang et al., 2014), Divnet is particularly well suited for such networks.  Contributions. The key contributions of this paper are the following:  – Introduction of DPPs as a ﬂexible, powerful tool for modeling layerwise neuronal diversity (§2.1). Speciﬁcally, we present a practical method for creating DPPs over neurons, which enables diversity promoting sampling and thereby leads to smaller network sizes.  – A simple but crucial “fusing” step that minimizes the adverse eﬀects of removing neurons. Speciﬁcally, we introduce a reweighting procedure for a neuron’s connections that transfers the contributions of the pruned neurons to the ones that are retained (§2.2).  The combination of both ideas is called Divnet. We perform several experiments to validate Divnet and compare to previous neuron pruning approaches, which Divnet consistently outperforms. Notably, Divnet’s reweighting strategy beneﬁts other pruning approaches.  Related work. Due to their large number of parameters, deep neural networks typically have a heavy memory footprint. Moreover, in many deep neural network models parameters show a signiﬁcant amount of redundancy (Denil et al., 2013). Consequently, there has been signiﬁcant interest in developing techniques for reducing a network’s size without penalizing its performance.  A common approach to reducing the number of parameters is to remove connections between layers. In (LeCun et al., 1990; Hassibi et al., 1993), connections are deleted using informa- tion drawn from the Hessian of the network’s error function. Sainath et al. (2013) reduce the number of parameters by analyzing the weight matrices, and applying low-rank factorization to the ﬁnal weight layer. Han et al. (2015) remove connections with weights smaller than a given threshold before retraining the network. These methods focus on deleting parameters whose removal inﬂuences the network the least, while Divnet seeks diversity and merges similar neurons; these methods can thus be used in conjunction with ours. Although meth- ods such as (LeCun et al., 1990) that remove connections between layers may also delete neurons from the network by removing all of their outgoing or incoming connections, it is likely that the overall impact on the size of the network will be lesser than approaches such as Divnet that remove entire neurons: indeed, removing a neuron decreases the number of rows or columns of the weight matrices connecting the neuron’s layer to both the previous and following layers.  Convolutional Neural Networks (LeCun et al., 1998) replace fully-connected layers with convolution and subsampling layers, which signiﬁcantly decreases the number of parameters. However, as CNNs still maintain fully-connected layers, they also beneﬁt from Divnet.  Closer to our approach of reducing the number of hidden neurons is (He et al., 2014), which evaluates each hidden neuron’s importance and deletes neurons with the smaller importance scores. In (Srinivas and Babu, 2015), a neuron is pruned when its weights are similar to those of another neuron in its layer, leading to a weight update procedure that is somewhat similar in idea (albeit simpler) to our reweighting step: where (Srinivas and Babu, 2015) removes neurons with equal or similar weights, we consider the more complicated task of merging neurons that, as a group, perform redundant calculations based on their activations.  Other recent approaches consider network compression without pruning: in (Hinton et al., 2015), a new, smaller network is trained on the outputs of the large network; Chen et al. (2015) use hashing to reduce the size of the weight matrices by forcing all connections within the same hash bucket to have the same weight. Courbariaux et al. (2014) and Gupta et al. (2015) show that networks can be trained and run using limited precision values to store the network parameters, thus reducing the overall memory footprint.  2  Published as a conference paper at ICLR 2016  We emphasize that Divnet’s focus on neuronal diversity is orthogonal and complementary to prior network compression techniques. Consequently, Divnet can be combined, in most cases trivially, with previous approaches to memory footprint reduction.  2 Diversity and redundancy reduction  In this section we introduce our technique for modeling neuronal diversity more formally. Let T denote the training data, (cid:96) a layer of n(cid:96) neurons, aij the activation of the i-th neuron on input tj, and vi = (ai1, . . . , ain(cid:96))(cid:62) the activation vector of the i-th neuron obtained by feeding the training data through the network. To enforce diversity in layer (cid:96), we must determine which neurons are computing redundant information and remove them. Doing so requires ﬁnding a maximal subset of (linearly) independent activation vectors in a layer and retaining only the corresponding neurons. In practice, however, the number of items in the training set (or the number of batches) can be much larger than the number of neurons in a layer, so the activation vectors v1, . . . , vn(cid:96) are likely linearly independent. Merely selecting by the maximal subset may thus lead to a trivial solution that selects all neurons.  Reducing redundancy therefore requires a more careful approach to sampling. We propose to select a subset of neurons whose activation patterns are diverse while contributing to the network’s overall computation (i.e., their activations are not saturated at 0). We achieve this diverse selection by formulating the neuron selection task as sampling from a Determinantal Point Process (DPP). We describe the details below.  2.1 Neuronal diversity via Determinantal Point Processes  DPPs are probability measures over subsets of a ground set of items. Originally introduced to model the repulsive behavior of fermions (Macchi, 1975), they have since been used fruitfully in machine-learning (Kulesza and Taskar, 2012). Interestingly, they have also been recently applied to modeling inter-neuron inhibitions in neural spiking behavior in the rat hippocampus (Snoek et al., 2013).  DPPs present an elegant mathematical technique to model diversity: the probability mass associated to each subset is proportional to the determinant (hence the name) of a DPP kernel matrix. The determinant encodes negative associations between variables, and thus DPPs tend to assign higher probability mass to diverse subsets (corresponding to diverse submatrices of the DPP kernel). Formally, a ground set of N items Y = {1, . . . , N} and a probability P : 2Y → [0, 1] such that  P(Y ) =  det(LY ) det(L + I)  ,  (1)  where L is a N -by-N positive deﬁnite matrix, form a DPP. L is called the DPP kernel ; here, LY indicates the |Y | × |Y | principal submatrix of L indexed by the elements of Y . The key ingredient that remains to be speciﬁed is the DPP kernel, which we now describe.  2.1.1 Constructing the DPP kernel  There are numerous potential choices for the DPP kernel. We found that experimentally a well-tuned Gaussian RBF kernel provided a good balance between simplicity and quality: for instance, it provides much better results that simple linear kernels (obtained via the outer product of the activation vectors) and is easier to use than more complex Gaussian RBF kernels with additional parameters. A more thorough evaluation of kernel choice is future work. Recall that layer (cid:96) has activations v1, . . . , vn(cid:96) . Using these, we ﬁrst create an n(cid:96) × n(cid:96) kernel L(cid:48) with bandwidth parameter β by setting  (2) To ensure strict positive deﬁniteness of the kernel matrix L(cid:48), we add a small diagonal perturbation εI to L(cid:48) (ε = 0.01). The choice of the bandwidth parameter could be done by  ij = exp(−β(cid:107)vi − vj(cid:107)2) L(cid:48)  1 ≤ i, j ≤ n(cid:96).  3  Published as a conference paper at ICLR 2016  cross-validation, but that would greatly increase the training cost. Therefore, we use the ﬁxed choice β = 10/|T |, which was experimentally seen to work well. Finally, in order to limit rounding errors, we introduce a ﬁnal scaling operation: suppose we wish to obtain a desired size, say k, of sampled subsets (in which case we are said to be using a k-DPP (Kulesza and Taskar, 2011)). To that end, we can scale the kernel L(cid:48) + εI by a factor γ, so that its expected sample size becomes k. For a DPP with kernel L, the expected sample size is given by (Kulesza and Taskar, 2012, Eq. 34):  Therefore, we scale the kernel to γ(L(cid:48) + εI) with γ such that  E[|Y |] = Tr(L(I + L)  −1).  γ =  k  n(cid:96) − k  · n(cid:96) − k(cid:48) k(cid:48)  ,  where k(cid:48) is the expected sample size for the kernel L(cid:48) + εI. Finally, generating and then sampling from L = γ(L(cid:48) + εI) has O(n3 In our experiments, this sampling cost was negligible compared with the cost of training. For networks with very large hidden layers, one can avoiding the n3 (cid:96) cost by using more scalable sampling techniques (Li et al., 2015; Kang, 2013).  (cid:96)|T |) cost.  (cid:96) + n2  2.2 Fusing redundant neurons  Simply excising the neurons that are not sampled by the DPP drastically alters the neuron inputs to the next layer. Intuitively, since activations of neurons marked redundant are not arbitrary, throwing them away is wasteful. Ideally we should preserve the total information of a given layer, which suggests that we should “fuse” the information from unselected neurons into the selected ones. We achieve this via a reweighting procedure as outlined below.  layer (indexed by j, 1 ≤ j ≤ n(cid:96)+1):  Without loss of generality, let neurons 1 through k be the ones sampled by the DPP and v1, . . . , vk their corresponding activation vectors. Let wij be the weights connecting the i-th  neuron (1 ≤ i ≤ k) in the current layer to the j-th neuron in the next layer; let (cid:101)wij = δij +wij minimize the diﬀerence in inputs to neurons in the subsequent layer before ((cid:80) denote the updated weights after merging the contributions from the removed neurons. We seek to minimize the impact of removing n(cid:96) − k neurons from layer (cid:96). To that end, we and after ((cid:80) i=1≤k (cid:101)wijvi) DPP pruning. That is, we wish to solve for all neurons in the next wijvi) (cid:13)(cid:13)(cid:13)(cid:13)(cid:88)k i=1 (cid:101)wijvi −(cid:88)n(cid:96) Eq. 3 is minimized when (cid:80)  i>k wijvi onto the linear space generated by {v1, . . . , vk}. Thus, to minimize Eq. 3, we obtain the coeﬃcients αij that for j > k minimize  δijvi −(cid:88)n(cid:96)  min(cid:101)wij∈R  (cid:13)(cid:13)(cid:13)(cid:13)2  wijvi  wijvi  i≤n(cid:96)  i=k+1  (3)  i=1  .  i=1  = min δij∈R  (cid:13)(cid:13)(cid:13)(cid:13)(cid:88)k i≤k δijvi is the projection of (cid:80) (cid:13)(cid:13)(cid:13)(cid:13)2 (cid:88)n(cid:96)  (cid:13)(cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)(cid:13)vj −(cid:88)k ∀i, 1 ≤ i ≤ k, (cid:101)wij = wij +  αijvi  i=1  r=k+1  and then update the weights by setting  αirwrj  (4)  Using ordinary least squares to obtain α, the reweighting procedure runs in O(|T |n2  (cid:96) + n3  (cid:96) ).  4  Published as a conference paper at ICLR 2016  3 Experimental results  To quantify the performance of our algorithm, we present below the results of experi- ments1 on common datasets for neural network evaluation: MNIST (LeCun and Cortes, 2010), MNIST ROT (Larochelle et al., 2007) and CIFAR-10 (Krizhevsky, 2009).  All networks were trained up until a certain training error threshold, using softmax activa- tion on the output layer and sigmoids on other layers; see Table 1 for more details. In all following plots, error bars represent standard deviations.  Table 1: Overview of the sets of networks used in the experiments. We train each class of networks until the ﬁrst iteration of backprop for which the training error reaches a predeﬁned threshold.  Dataset MNIST  MNIST ROT CIFAR-10  Instances Trained up until  5 5 5  < 1% error < 1% error < 50% error  Architecture  784 - 500 - 500 - 10 784 - 500 - 500 - 10  3072 - 1000 - 1000 - 1000 - 10  3.1 Pruning and reweighting analysis  To validate our claims on the beneﬁts of using DPPs and fusing neurons, we compare these steps separately and also simultaneously against random pruning, where a ﬁxed number of neurons are chosen uniformly at random from a layer and then removed, with and without our fusing step. We present performance results on the test data; of course, both DPP selection and reweighting are based solely on information drawn from the training data.  Figure 1 visualizes neuron activations in the ﬁrst hidden layer of a network trained on the MNIST dataset. Each column in the plotted heat maps represents the activation of a neuron on instances of digits 0 through 9. Figure 1a shows the activations of the 50 neurons sampled using a k-DPP (k = 50) deﬁned over the ﬁrst hidden layer, whereas Figure 1b shows the activations of the ﬁrst 50 neurons of the same layer. Figure 1b contains multiple similar columns: for example, there are 3 entirely green columns, corresponding to three neurons that saturate to 1 on each of the 10 instances. In contrast, the DPP samples neurons with diverse activations, and Figure 1a shows no similar redundancy. Figures 2 through 4 illustrate the impact of each step of Divnet separately. Figure 2 shows the impact of pruning on test error using DPP pruning and random pruning (in which a ﬁxed number of neurons are selected uniformly at random and removed from the network). DPP- pruned networks have consistently better training and test errors than networks pruned at random for the same ﬁnal size. As expected, the more neurons are maintained, the less the error suﬀers from the pruning procedure; however, the pruning is in both cases destructive, and is seen to signiﬁcantly increase the error rate.  This phenomenon can be mitigated by our reweighting procedure, as shown in Figure 3. By fusing and reweighting neurons after pruning, the training and test errors are consid- erably reduced, even when 90% of the layer’s neurons are removed. Pruning also reduces variability of the results: the standard deviation for the results of the reweighted networks is signiﬁcantly smaller than for the non-reweighted networks, and may be thus seen as a way to regularize neural networks. Finally, Figure 4 illustrates the performance of Divnet (DPP pruning and reweighting) compared to random pruning with reweighting. Although Divnet’s performance is ulti- mately better, the reweighting procedure also dramatically beneﬁts the networks that were pruned randomly.  We also ran these experiments on networks for shrinking the second layer while maintaining the ﬁrst layer intact. The results are similar, and may be found in Appendix A. Notably, we found that the gap between Divnet and random pruning’s performances was much wider  1Run in MATLAB, based on the code from DeepLearnToolBox (https://github.com/ rasmusbergpalm/DeepLearnToolbox) and Alex Kulesza’s code for DPPs (http://web.eecs.umich. edu/~kulesza/), on a Linux Mint system with 16GB of RAM and an i7-4710HQ CPU @ 2.50GHz.  5  Published as a conference paper at ICLR 2016  (a) 50 neurons sampled via DPP from the ﬁrst hidden layer  (b) First 50 neurons of the ﬁrst hidden layer  Figure 1: Heat map of the activation of subsets of 50 neurons for one instance of each class of the MNIST dataset. The rows correspond to digits 0 through 9. Each column corresponds to the activation values of one neuron in the network’s ﬁrst layer on images of digits 0 through 9. Green values are activations close to 1, red values are activations close to 0.  when pruning the last layer. We believe this is due to the connections to the output layer being learned much faster, thus letting a small, diverse subset of neurons (hence well suited to DPP sampling) in the last hidden layer take over the majority of the computational eﬀort.  3.2 Performance analysis  Much attention has been given to reducing the size of neural networks in order to reduce memory consumption. When using neural nets locally on devices with limited memory, it is crucial that their memory footprint be as small as possible.  Node importance-based pruning (henceforth “importance pruning”) is one of the most in- tuitive ways to cut down on network size. Introduced to deep networks by He et al. (2014), this method removes the neurons whose calculations impact the network the least. Among the three solutions to estimating a neuron’s importance discussed in He et al. (2014), the sum the output weights of each neuron (the ‘onorm’ function) provided the best results:  (cid:88)n(cid:96)  onorm(ni) :=  1  n(cid:96)+1  |w(cid:96)+1  ij  |.  j=1  Figure 5 compares the test data error of the networks after being pruned using importance pruning that uses onorm as a measure of relevance against Divnet. Since importance prun- ing deletes neurons that contribute the least to the next layer’s computations, it performs well up to a certain point; however, when pruning a signiﬁcant amount of neurons, this pruning procedure even removes neurons performing essential calculations, hurting the net- work’s performance signiﬁcantly. However, since Divnet fuses redundant neurons, instead of merely deleting them its resulting network delivers much better performance even with very large amounts of pruning. In order to illustrate numerically the impact of Divnet on network performance, Table 2 shows network training and test errors (between 0 and 1) under various compression rates obtained with Divnet, without additional retraining (that is, the pruned network is not retrained to re-optimize its weights).  6  01234567890123456789Published as a conference paper at ICLR 2016  (a) MNIST dataset  (b) MNIST ROT dataset  Figure 2: Comparison of random and k-DPP pruning procedures.  (a) MNIST dataset  (b) MNIST ROT dataset  Figure 3: Comparison of Divnet (k-DPP + reweighting) to simple k-DPP pruning.  (a) MNIST dataset  (b) MNIST ROT dataset  Figure 4: Comparison of random and k-DPP pruning when both are followed by reweighting.  Table 2: Training and test error for diﬀerent percentages of remaining neurons (mean ± standard deviation). Initially, MNIST and MNIST ROT nets have 1000 hidden neurons, and CIFAR-10 have 3000.  Remaining hidden neurons training error  MNIST  MNIST ROT  CIFAR-10  test error  training error  test error  training error  test error  50%  25%  10%  75%  100 %  0.01 ±0.001 0.76 ± 0.06 0.28 ± 0.12 0.15 ± 0.04 0.06 ± 0.04 0.76 ± 0.07 0.29 ± 0.12 0.17 ± 0.05 0.07 ± 0.03 0.03 ± 0.002 0.74 ± 0.08 0.54 ± 0.09 0.34 ± 0.06 0.20 ± 0.03 0.01 ± 0.003 0.73 ± 0.09 0.49 ± 0.11 0.25 ± 0.07 0.06 ± 0.03 0.15 ±0.008 0.84 ± 0.05 0.61 ± 0.01 0.52 ± 0.01 0.50 ± 0.01 0.49 ± 0.004 0.85 ± 0.05 0.62 ± 0.02 0.54 ± 0.01 0.52 ± 0.01 0.51 ± 0.005  7  10020030040050000.20.40.60.81numberofneuronsinﬁrsthiddenlayertesterrorrandompruningk-DPPpruning10020030040050000.20.40.60.81numberofneuronsinﬁrsthiddenlayertesterrorrandompruningk-DPPpruning10020030040050000.20.40.60.81numberofneuronsinﬁrsthiddenlayertesterrork-DPPpruningk-DPP+reweighting10020030040050000.20.40.60.81numberofneuronsinﬁrsthiddenlayertesterrork-DPPpruningk-DPP+reweighting10020030040050000.20.40.60.81numberofneuronsinﬁrsthiddenlayertesterrorrandompruning+reweightingk-DPP+reweighting10020030040050000.20.40.60.81numberofneuronsinﬁrsthiddenlayertesterrorrandompruning+reweightingk-DPP+reweightingPublished as a conference paper at ICLR 2016  (a) MNIST dataset  (b) MNIST ROT dataset  Figure 5: Comparison of random pruning, importance pruning, and Divnet’s impact on the network’s performance after decreasing the number of neurons in the ﬁrst hidden layer of a network.  3.3 Discussion and Remarks ◦ In all experiments, sampling and reweighting ran several orders of magnitude faster than training; reweighting required signiﬁcantly more time than sampling. If Divnet must be further sped up, a fraction of the training set can be used instead of the entire set, at the possible cost of subsequent network performance. ◦ When using DPPs with a Gaussian RBF kernel, sampled neurons need not have linearly independent activation vectors: not only is the DPP sampling probabilistic, the kernel itself is not scale invariant. Indeed, for two collinear but unequal activation vectors, the corresponding coeﬃcient in the kernel will not be 1 (or γ with the L ← γL update). ◦ In our work, we selected a subset of neurons by sampling once from the DPP. Alternatively, one could sample a ﬁxed amount of times, using the subset with the highest likelihood (i.e., largest det(LY )), or greedily approximate the mode of the DPP distribution. ◦ Our reweighting procedure beneﬁts competing pruning methods as well (see Figure 4). ◦ We also investigated DPP sampling for pruning concurrently with training iterations, hoping that this might allow us to detect superﬂuous neurons before convergence, and thus reduce training time. However, we observed that in this case DPP pruning, with or without reweighting, did not oﬀer a signiﬁcant advantage over random pruning. ◦ Consistently over all datasets and networks, the expected sample size from the kernel L(cid:48) was much smaller for the last hidden layer than for other layers. We hypothesize that this is caused by the connections to the output layer being learned faster than the others, allowing a small subset of neurons to take over the majority of the computational eﬀort.  4 Future work and conclusion  Divnet leverages similarities between the behaviors of neurons in a layer to detect redundant parameters and merge them, thereby enforcing neuronal diversity within each hidden layer. Using Divnet, large, redundant networks can be shrunk to much smaller structures without impacting their performance and without requiring further training. We believe that the performance proﬁle of Divnet will remain similar even when scaling to larger scale datasets and networks, and hope to include results on bigger problems (e.g., Imagenet (Russakovsky et al., 2015)) in the future.  Many hyper-parameters can be tuned by a user as per need include: the number of remaining neurons per layer can be ﬁxed manually; the precision of the reweighting and the sampling procedure can be tuned by choosing how many training instances are used to generate the DPP kernel and the reweighting coeﬃcients, creating a trade-oﬀ between accuracy, memory management, and computational time. Although Divnet requires the user to select the size of the ﬁnal network, we believe that a method where no parameter explicitly needs to be tuned is worth investigating. The fact that DPPs can be augmented to also reﬂect  8  10020030040050000.20.40.60.81sizeofﬁrsthiddenlayertesterrorrandomimportancepruningDIVNET10020030040050000.20.40.60.81sizeofﬁrsthiddenlayertesterrorrandomimportancepruningDIVNETPublished as a conference paper at ICLR 2016  diﬀerent distributions over the sampled set sizes (Kulesza and Taskar, 2012, §5.1.1) might be leveraged to remove the burden of choosing the layer’s size from the user. Importantly, Divnet is agnostic to most parameters of the network, as it only requires knowledge of the activation vectors. Consequently, Divnet can be easily used jointly with other pruning/memory management methods to reduce size. Further, the reweighting pro- cedure is agnostic to how the pruning is done, as shown in our experiments. Furthermore, the principles behind Divnet can theoretically also be leveraged in non fully- connected settings. For example, the same diversifying approach may also be applicable to ﬁlters in CNNs: if a layer of the CNN is connected to a simple, feed-forward layer – such as the S4 layer in LeCun et al. (1998) – by viewing each ﬁlter’s activation values as an vector and applying Divnet on the resulting activation matrix, one may be able to remove entire ﬁlters from the network, thus signiﬁcantly reducing CNN’s memory footprint.  Finally, we believe that investigating DPP pruning with diﬀerent kernels, such as kernels invariant to the scaling of the activation vectors, or even kernels learned from data, may provide insight into which interactions between neurons of a layer contain the information necessary for obtaining good representations and accurate classiﬁcation. This marks an interesting line of future investigation, both for training and representation learning.  Acknowledgments  This work is partly supported by NSF grant: IIS-1409802.  References  W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen. Compressing neural networks  with the hashing trick. CoRR, abs/1504.04788, 2015.  M. Courbariaux, Y. Bengio, and J. David. Low precision arithmetic for deep learning. CoRR,  abs/1412.7024, 2014.  M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep  learning. CoRR, abs/1306.0543, 2013.  S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan. Deep learning with limited numerical  precision. CoRR, abs/1502.02551, 2015.  S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both weights and connections for eﬃcient  neural networks. CoRR, abs/1506.02626, 2015.  B. Hassibi, D. G. Stork, and S. C. R. Com. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in Neural Information Processing Systems 5, pages 164–171. Morgan Kaufmann, 1993.  T. He, Y. Fan, Y. Qian, T. Tan, and K. Yu. Reshaping deep neural network for fast decoding by node-pruning. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 245–249, May 2014.  G. E. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. CoRR,  abs/1503.02531, 2015.  J. B. Hough, M. Krishnapur, Y. Peres, and B. Vir´ag. Determinantal processes and independence.  Probability Surveys, 3(206–229):9, 2006.  B. Kang. Fast determinantal point process sampling with application to clustering. In Advances in  Neural Information Processing Systems 26, pages 2319–2327. Curran Associates, Inc., 2013. A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. A. Kulesza and B. Taskar. k-DPPs: Fixed-size determinantal point processes. In Proceedings of  the 28th International Conference on Machine Learning, 2011.  A. Kulesza and B. Taskar. Determinantal point processes for machine learning. Foundations and  Trends in Machine Learning, 5(2–3), 2012.  H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th International Conference on Machine Learning, ICML ’07, pages 473–480, 2007.  Y. LeCun and C. Cortes. MNIST handwritten digit database, 2010. URL http://yann.lecun.  com/exdb/mnist/.  Y. LeCun, J. S. Denker, and S. A. Solla. Optimal brain damage. In Advances in Neural Information  Processing Systems, pages 598–605. Morgan Kaufmann, 1990.  Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document  recognition. In Proceedings of the IEEE, pages 2278–2324, 1998.  9  Published as a conference paper at ICLR 2016  C. Li, S. Jegelka, and S. Sra. Eﬃcient sampling for k-determinantal point processes. preprint, 2015.  URL http://arxiv.org/abs/1509.01618.  O. Macchi. The coincidence approach to stochastic point processes. Advances in Applied Probability,  7(1), 1975.  D. Maclaurin, D. Duvenaud, and R. P. Adams. Gradient-based hyperparameter optimization In Proceedings of the 32nd International Conference on Machine  through reversible learning. Learning, July 2015.  O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.  T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, pages 6655– 6659. IEEE, 2013.  J. Snoek, R. Zemel, and R. P. Adams. A determinantal point process latent variable model for inhibition in neural spiking data. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 1932– 1940. Curran Associates, Inc., 2013.  S. Srinivas and R. V. Babu. Data-free parameter pruning for deep neural networks. CoRR,  abs/1507.06149, 2015. URL http://arxiv.org/abs/1507.06149.  Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola, L. Song, and Z. Wang. Deep fried  convnets. CoRR, abs/1412.7149, 2014.  10  Published as a conference paper at ICLR 2016  A Pruning the second layer  (a) MNIST dataset  (b) MNIST ROT dataset  Figure 6: Comparison of random and k-DPP pruning procedures.  (a) MNIST dataset  (b) MNIST ROT dataset  Figure 7: Comparison of Divnet to simple k-DPP pruning.  (a) MNIST dataset  (b) MNIST ROT dataset  Figure 8: Comparison of random and k-DPP pruning when both are followed by reweighting.  11  10020030040050000.20.40.60.81numberofneuronsinsecondhiddenlayertesterrorrandompruningk-DPPpruning10020030040050000.20.40.60.81numberofneuronsinsecondhiddenlayertesterrorrandompruningk-DPPpruning10020030040050000.20.40.60.81numberofneuronsinsecondhiddenlayertesterrork-DPPpruningk-DPP+reweighting10020030040050000.20.40.60.81numberofneuronsinsecondhiddenlayertesterrork-DPPpruningk-DPP+reweighting10020030040050000.20.40.60.81numberofneuronsinsecondhiddenlayertesterrorrandompruning+reweightingk-DPP+reweighting10020030040050000.20.40.60.81numberofneuronsinsecondhiddenlayertesterrorrandompruning+reweightingk-DPP+reweightingPublished as a conference paper at ICLR 2016  B Influence of the β parameter on network size and error  Figure 9: Inﬂuence of β on training error (using the networks trained on MNIST). The dotted lines show min and max errors.  Figure 10: Inﬂuence of β on the number of neurons that remain after pruning networks trained on MNIST (when pruning non-parametrically, using a DPP instead of a k-DPP.)  12  10020030040050010−210−1numberofneuronsinﬁrsthiddenlayertrainingerrorβ=10−1β=10−2β=10−3β=10−4-10-510-410-310-210-1100Numberofhiddenneurons050100150200250300Published as a conference paper at ICLR 2016  C Comparison of Divnet to importance-based pruning and  random pruning on the CIFAR-10 dataset  (a) Training error on CIFAR-10 dataset  (b) Test error on CIFAR-10 dataset  Figure 11: Comparison of random pruning, importance pruning, and Divnet’s impact on the network’s performance after decreasing the number of parameters in the network.  13  02004006008001,0000.40.60.81sizeofﬁrsthiddenlayertrainingerrorrandomimportancepruningDIVNET02004006008001,0000.40.60.81sizeofﬁrsthiddenlayertesterrorrandomimportancepruningDIVNET",
1511.04143,2016,Deep Reinforcement Learning in Parameterized Action Space,"['Deep Reinforcement Learning in Parameterized Action Space [code] [data]\nMatthew Hausknecht', 'Peter Stone']",https://arxiv.org/pdf/1511.04143,"6 1 0 2     b e F 6 1         ] I  A . s c [      4 v 3 4 1 4 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  DEEP REINFORCEMENT LEARNING IN PARAMETER- IZED ACTION SPACE  Matthew Hausknecht Department of Computer Science University of Texas at Austin mhauskn@cs.utexas.edu  Peter Stone Department of Computer Science University of Texas at Austin pstone@cs.utexas.edu  ABSTRACT  Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring con- tinuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To ﬁll this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agents can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.  1  INTRODUCTION  This paper extends the Deep Deterministic Policy Gradients (DDPG) algorithm (Lillicrap et al., 2015) into a parameterized action space. We document a modiﬁcation to the published version of the DDPG algorithm: namely bounding action space gradients. We found this modiﬁcation necessary for stable learning in this domain and will likely be valuable for future practitioners attempting to learn in continuous, bounded action spaces. We demonstrate reliable learning, from scratch, of RoboCup soccer policies capable of goal scoring. These policies operate on a low-level continuous state space and a parameterized-continuous action space. Using a single reward function, the agents learn to locate and approach the ball, dribble to the goal, and score on an empty goal. The best learned agent proves more reliable at scoring goals, though slower, than the hand-coded 2012 RoboCup champion. RoboCup 2D Half-Field-Offense (HFO) is a research platform for exploring single agent learning, multi-agent learning, and adhoc teamwork. HFO features a low-level continuous state space and parameterized-continuous action space. Speciﬁcally, the parameterized action space requires the agent to ﬁrst select the type of action it wishes to perform from a discrete list of high level actions and then specify the continuous parameters to accompany that action. This parameterization introduces structure not found in a purely continuous action space. The rest of this paper is organized as follows: the HFO domain is presented in Section 2. Sec- tion 3 presents background on deep continuous reinforcement learning including detailed actor and critic updates. Section 5 presents a method of bounding action space gradients. Section 6 covers experiments and results. Finally, related work is presented in Section 8 followed by conclusions.  2 HALF FIELD OFFENSE DOMAIN  RoboCup is an international robot soccer competition that promotes research in AI and robotics. Within RoboCup, the 2D simulation league works with an abstraction of soccer wherein the players, the ball, and the ﬁeld are all 2-dimensional objects. However, for the researcher looking to quickly prototype and evaluate different algorithms, the full soccer task presents a cumbersome prospect: full games are lengthy, have high variance in their outcome, and demand specialized handling of rules such as free kicks and offsides.  1  Published as a conference paper at ICLR 2016  The Half Field Offense domain abstracts away the difﬁculties of full RoboCup and exposes the experimenter only to core decision-making logic, and to focus on the most challenging part of a RoboCup 2D game: scoring and defending goals. In HFO, each agent receives its own state sensa- tions and must independently select its own actions. HFO is naturally characterized as an episodic multi-agent POMDP because of the sequential partial observations and actions on the part of the agents and the well-deﬁned episodes which culminate in either a goal being scored or the ball leav- ing the play area. To begin each episode, the agent and ball are positioned randomly on the offensive half of the ﬁeld. The episode ends when a goal is scored, the ball leaves the ﬁeld, or 500 timesteps pass. Example videos of Half Field Offense games may be viewed at: https://vid.me/sNev https://vid.me/JQTw https://vid.me/1b5D. The following subsections introduce the low-level state and action space used by agents in this domain.  2.1 STATE SPACE  The agent uses a low-level, egocentric viewpoint encoded using 58 continuously-valued features. These features are derived through Helios-Agent2D’s (Akiyama, 2010) world model and provide angles and distances to various on-ﬁeld objects of importance such as the ball, the goal, and the other players. Figure 1 depicts the perceptions of the agent. The most relevant features include: Agent’s position, velocity, and orientation, and stamina; Indicator if the agent is able to kick; Angles and distances to the following objects: Ball, Goal, Field-Corners, Penalty-Box-Corners, Teammates, and Opponents. A full list of state features may be found at https://github.com/mhauskn/ HFO/blob/master/doc/manual.pdf.  (a) State Space  (b) Helios Champion  Figure 1: Left: HFO State Representation uses a low-level, egocentric viewpoint providing features such as distances and angles to objects of interest like the ball, goal posts, corners of the ﬁeld, and opponents. Right: Helios handcoded policy scores on a goalie. This 2012 champion agent forms a natural (albeit difﬁcult) baseline of comparison.  2.2 ACTION SPACE  Half Field Offense features a low-level, parameterized action space. There are four mutually- exclusive discrete actions: Dash, Turn, Tackle, and Kick. At each timestep the agent must select one of these four to execute. Each action has 1-2 continuously-valued parameters which must also be speciﬁed. An agent must select both the discrete action it wishes to execute as well as the contin- uously valued parameters required by that action. The full set of parameterized actions is: Dash(power, direction): Moves in the indicated direction with a scalar power in [0, 100]. Move- ment is faster forward than sideways or backwards. Turn(direction): Turns to indicated direction. Tackle(direction): Contests the ball by moving in the indicated direction. This action is only useful when playing against an opponent. Kick(power, direction): Kicks the ball in the indicated direction with a scalar power in [0, 100]. All directions are parameterized in the range of [−180, 180] degrees.  2  Published as a conference paper at ICLR 2016  2.3 REWARD SIGNAL  True rewards in the HFO domain come from winning full games. However, such a reward signal is far too sparse for learning agents to gain traction. Instead we introduce a hand-crafted reward signal with four components: Move To Ball Reward provides a scalar reward proportional to the change in distance between the agent and the ball d(a, b). An additional reward Ikick of 1 is given the ﬁrst time each episode the agent is close enough to kick the ball. Kick To Goal Reward is proportional to the change in distance between the ball and the center of the goal d(b, g). An additional reward is given for scoring a goal Igoal. A weighted sum of these components results in a single reward that ﬁrst guides the agent close enough to kick the ball, then rewards for kicking towards goal, and ﬁnally for scoring. It was necessary to provide a higher gain for the kick-to-goal component of the reward because immediately following each kick, the move-to-ball component produces negative rewards as the ball moves away from the agent. The overall reward is as follows:  t + 3(cid:0)dt−1(b, g) − dt(b, g)(cid:1) + 5Igoal  t  (1)  rt = dt−1(a, b) − dt(a, b) + Ikick  It is disappointing that reward engineering is necessary. However, the exploration task proves far too difﬁcult to ever gain traction on a reward that consists only of scoring goals, because acting randomly is exceedingly unlikely to yield even a single goal in any reasonable amount of time. An interesting direction for future work is to ﬁnd better ways of exploring large state spaces. One recent approach in this direction, Stadie et al. (2015) assigned exploration bonuses based on a model of system dynamics.  3 BACKGROUND: DEEP REINFORCEMENT LEARNING  Deep neural networks are adept general purpose function approximators that have been most widely used in supervised learning tasks. Recently, however they have been applied to reinforcement learn- ing problems, giving rise to the ﬁeld of deep reinforcement learning. This ﬁeld seeks to combine the advances in deep neural networks with reinforcement learning algorithms to create agents ca- pable of acting intelligently in complex environments. This section presents background in deep reinforcement learning in continuous action spaces. The notation closely follows that of Lillicrap et al. (2015). Deep, model-free RL in discrete action spaces can be performed using the Deep Q-Learning method introduced by Mnih et al. (2015) which employs a single deep network to estimate the value function of each discrete action and, when acting, selects the maximally valued output for a given state input. Several variants of DQN have been explored. Narasimhan et al. (2015) used decaying traces, Hausknecht & Stone (2015) investigated LSTM recurrency, and van Hasselt et al. (2015) explored double Q-Learning. These networks work well in continuous state spaces but do not function in continuous action spaces because the output nodes of the network, while continuous, are trained to output Q-Value estimates rather than continuous actions. An Actor/Critic architecture (Sutton & Barto, 1998) provides one solution to this problem by de- coupling the value learning and the action selection. Represented using two deep neural networks, the actor network outputs continuous actions while the critic estimates the value function. The actor network µ, parameterized by θµ, takes as input a state s and outputs a continuous action a. The critic network Q, parameterized by θQ, takes as input a state s and action a and outputs a scalar Q-Value Q(s, a). Figure 2 shows Critic and Actor networks. Updates to the critic network are largely unchanged from the standard temporal difference update used originally in Q-Learning (Watkins & Dayan, 1992) and later by DQN:  Q(s, a) = Q(s, a) + α(cid:0)r + γ max  Adapting this equation to the neural network setting described above results in minimizing a loss function deﬁned as follows:  (cid:16)  Q(s, a|θQ) −(cid:0)r + γ max  LQ(s, a|θQ) =  a(cid:48) Q(s(cid:48), a(cid:48)) − Q(s, a)(cid:1) a(cid:48) Q(s(cid:48), a(cid:48)|θQ)(cid:1)(cid:17)2  3  (2)  (3)  Published as a conference paper at ICLR 2016  (4)  (5)  However, in continuous action spaces, this equation is no longer tractable as it involves maximizing over next-state actions a(cid:48). Instead we ask the actor network to provide a next-state action a(cid:48) = µ(s(cid:48)|θµ). This yields a critic loss with the following form:  (cid:16)  Q(s, a|θQ) −(cid:0)r + γQ(s(cid:48), µ(s(cid:48)|θµ)(cid:48)|θQ)(cid:1)(cid:17)2  LQ(s, a|θQ) =  The value function of the critic can be learned by gradient descent on this loss function with respect to θQ. However, the accuracy of this value function is highly inﬂuenced by the quality of the actor’s policy, since the actor determines the next-state action a(cid:48) in the update target. The critic’s knowledge of action values is then harnessed to learn a better policy for the actor. Given a sample state, the goal of the actor is to minimize the difference between its current output a and the optimal action in that state a∗.  Lµ(s|θµ) =(cid:0)a − a∗(cid:1)2  =(cid:0)µ(s|θQ) − a∗(cid:1)2  The critic may be used to provide estimates of the quality of different actions but naively estimating a∗ would involve maximizing the critic’s output over all possible actions: a∗ ≈ arg maxa Q(s, a|θQ). Instead of seeking a global maximum, the critic network can provide gra- dients which indicate directions of change, in action space, that lead to higher estimated Q-Values: ∇aQ(s, a|θQ). To obtain these gradients requires a single backward pass over the critic network, much faster than solving an optimization problem in continuous action space. Note that these gra- dients are not the common gradients with respect to parameters. Instead these are gradients with respect to inputs, ﬁrst used in this way by NFQCA (Hafner & Riedmiller, 2011). To update the actor network, these gradients are placed at the actor’s output layer (in lieu of targets) and then back- propagated through the network. For a given state, the actor is run forward to produce an action that the critic evaluates, and the resulting gradients may be used to update the actor:  ∇θµµ(s) = ∇aQ(s, a|θQ)∇θµ µ(s|θµ)  (6)  Alternatively one may think about these updates as simply interlinking the actor and critic networks: On the forward pass, the actor’s output is passed forward into the critic and evaluated. Next, the estimated Q-Value is backpropagated through the critic, producing gradients ∇aQ that indicate how the action should change in order to increase the Q-Value. On the backwards pass, these gradients ﬂow from the critic through the actor. An update is then performed only over the actor’s parameters. Figure 2 shows an example of this update.  3.1 STABLE UPDATES  Updates to the critic rely on the assumption that the actor’s policy is a good proxy for the optimal policy. Updates to the actor rest on the assumption that the critic’s gradients, or suggested directions for policy improvement, are valid when tested in the environment. It should come as no surprise that several techniques are necessary to make this learning process stable and convergent. Because the critic’s policy Q(s, a|θQ) inﬂuences both the actor and critic updates, errors in the critic’s policy can create destructive feedback resulting in divergence of the actor, critic, or both. To resolve this problem Mnih et al. (2015) introduce a Target-Q-Network Q(cid:48), a replica of the critic network that changes on a slower time scale than the critic. This target network is used to generate next state targets for the critic update (Equation 4). Similarly a Target-Actor-Network µ(cid:48) combats quick changes in the actor’s policy. The second stabilizing inﬂuence is a replay memory D, a FIFO queue consisting of the agent’s latest experiences (typically one million). Updating from mini-batches of experience sampled uniformly from this memory reduces bias compared to updating exclusively from the most recent experiences.  4  Published as a conference paper at ICLR 2016  Figure 2: Actor-Critic architecture (left): actor and critic networks may be interlinked, allowing activations to ﬂow forwards from the actor to the critic and gradients to ﬂow backwards from the critic to the actor. The gradients coming from the critic indicate directions of improvement in the continuous action space and are used to train the actor network without explicit targets. Actor Update (right): Backwards pass generates critic gradients ∇aQ(s, a|θQ) w.r.t. the action. These gradients are back-propagated through the actor resulting in gradients w.r.t. parameters ∇θµ which are used to update the actor. Critic gradients w.r.t. parameters ∇θQ are ignored during the actor update.  Employing these two techniques the critic loss in Equation 4 and actor update in Equation 5 can be stably re-expressed as follows:  LQ(θQ) = E(st,at,rt,st+1)∼D  ∇θµµ = Est∼D  Q(st, at) −(cid:0)rt + γQ(cid:48)(st+1, µ(cid:48)(st+1))(cid:1)(cid:17)2(cid:21)  (cid:20)(cid:16) (cid:20) ∇aQ(st, a|θQ)∇θµµ(st)|a=µ(st)  (cid:21)  (7)  (8)  Finally, these updates are applied to the respective networks, where α is a per-parameter step size de- termined by the gradient descent algorithm. Additionally, the target-actor and target-critic networks are updated to smoothly track the actor and critic using a factor τ (cid:28) 1:  θQ = θQ + α∇θQ LQ(θQ) θµ = θµ + α∇θµµ θQ(cid:48) θµ(cid:48)  = τ θQ + (1 − τ )θQ(cid:48) = τ θµ + (1 − τ )θµ(cid:48)  (9)  One ﬁnal component is an adaptive learning rate method such as ADADELTA (Zeiler, 2012), RM- SPROP (Tieleman & Hinton, 2012), or ADAM (Kingma & Ba, 2014).  3.2 NETWORK ARCHITECTURE  Shown in Figure 2, both the actor and critic employ the same architecture: The 58 state inputs are processed by four fully connected layers consisting of 1024-512-256-128 units respectively. Each fully connected layer is followed by a rectiﬁed linear (ReLU) activation function with negative slope 10−2. Weights of the fully connected layers use Gaussian initialization with a standard deviation of 10−2. Connected to the ﬁnal inner product layer are two linear output layers: one for the four  5  State4 Actions6 Parameters1024ReLU256ReLU512ReLU128ReLUQ-Value1024ReLU256ReLU512ReLU128ReLUActorCriticState(cid:7512)θμ4 Actions6 Parameters(cid:7512)θQQ-ValueActorCritic(cid:7512)aQ(s,a)Published as a conference paper at ICLR 2016  discrete actions and another for the six parameters accompanying these actions. In addition to the 58 state features, the critic also takes as input the four discrete actions and six action parameters. It outputs a single scalar Q-value. We use the ADAM solver with both actor and critic learning rate set to 10−3. Target networks track the actor and critic using a τ = 10−4. Complete source code for our agent is available at https://github.com/mhauskn/dqn-hfo and for the HFO domain at https://github.com/mhauskn/HFO/. Having introduced the background of deep reinforcement learning in continuous action space, we now present the parameterized action space.  4 PARAMETERIZED ACTION SPACE ARCHITECTURE  Following notation in (Masson & Konidaris, 2015), a Parameterized Action Space Markov Decision Process (PAMDP) is deﬁned by a set of discrete actions Ad = {a1, a2, . . . , ak}. Each discrete action a ∈ Ad features ma continuous parameters {pa } ∈ Rma. Actions are represented ). Thus the overall action space A = ∪a∈Ad (a, pa by tuples (a, pa ). ma is A = In Half Field Offense, the complete parameterized action space (Section 2.2) 3 )∪ (Tackle, ptackle (Dash, pdash ). The actor network in Figure 2 factors the action space into one output layer for discrete actions (Dash, Turn, Tackle, Kick) and another for all six continuous parameters (pdash  )∪ (Kick, pkick  )∪ (Turn, pturn  1, . . . , pa  1, . . . , pa  1, . . . , pa  , pdash  , pdash  , pkick  , pkick  , pkick  , pturn  ma  ma  ).  1  2  4  5  6  3 , ptackle  4  1  2  5  6  4.1 ACTION SELECTION AND EXPLORATION  ma  1, . . . , pa  Using the factored action space, deterministic action selection proceeds as follows: At each timestep, the actor network outputs values for each of the four discrete actions as well as six continuous parameters. The discrete action is chosen to be the maximally valued output a = max(Dash, Turn, Tackle, Kick) and paired with associated parameters from the parameter output layer (a, pa ). Thus the actor network simultaneously chooses which discrete action to execute and how to parameterize that action. During training, the critic network receives, as input, the values of the output nodes of all four discrete actions and all six action parameters. We do not indicate to the critic which discrete action was actually applied in the HFO environment or which continuous parameters are associated with that discrete action. Similarly, when updating the actor, the critic provides gradients for all four discrete actions and all six continuous parameters. While it may seem that the critic is lacking crucial information about the structure of the action space, our experimental results in Section 6 demonstrate that the critic learns to provide gradients to the correct parameters of each discrete action. Exploration in continuous action space differs from discrete space. We adapt (cid:15)-greedy exploration to parameterized action space: with probability (cid:15), a random discrete action a ∈ Ad is selected and the } are sampled using a uniform random distribution. associated continuous parameters {pa Experimentally, we anneal (cid:15) from 1.0 to 0.1 over the ﬁrst 10, 000 updates. Lillicrap et al. (2015) demonstrate that Ornstein-Uhlenbeck exploration is also successful in continuous action space.  1, . . . , pa  ma  5 BOUNDED PARAMETER SPACE LEARNING  The Half Field Offense domain bounds the range of each continuous parameter. Parameters indicat- ing direction (e.g. Turn and Kick direction) are bounded in [−180, 180] and parameters for power (e.g. Kick and Dash power) are bounded in [0, 100]. Without enforcing these bounds, after a few hundred updates, we observed continuous parameters routinely exceeding the bounds. If updates were permitted to continue, parameters would quickly trend towards astronomically large values. This problem stems from the critic providing gradients that encourage the actor network to continue increasing a parameter that already exceeds bounds. We explore three approaches for preserving parameters in their intended ranges:  6  Published as a conference paper at ICLR 2016  Zeroing Gradients: Perhaps the simplest approach is to examine the critic’s gradients for each parameter and zero the gradients that suggest increasing/decreasing the value of a parameter that is already at the upper/lower limit of its range:  (cid:26)∇p  0  ∇p =  if pmin < p < pmax otherwise  (10)  Where ∇p indicates the critic’s gradient with respect to parameter p, (e.g. ∇pQ(st, a|θQ)) and pmin, pmax, p indicate respectively the minimum bound, maximum bound, and current activation of that parameter. Squashing Gradients: A squashing function such as the hyperbolic tangent (tanh) is used to bound the activation of each parameter. Subsequently, the parameters are re-scaled into their intended ranges. This approach has the advantage of not requiring manual gradient tinkering, but presents issues if the squashing function saturates. Inverting Gradients: This approach captures the best aspects of the zeroing and squashing gradi- ents, while minimizing the drawbacks. Gradients are downscaled as the parameter approaches the boundaries of its range and are inverted if the parameter exceeds the value range. This approach actively keeps parameters within bounds while avoiding problems of saturation. For example, if the critic continually recommends increasing a parameter, it will converge to the parameter’s upper bound. If the critic then decides to decrease that parameter, it will decrease immediately. In contrast, a squashing function would be saturated at the upper bound of the range and require many updates to decrease. Mathematically, the inverted gradient approach may be expressed as follows:  (cid:26)(pmax − p)/(pmax − pmin)  (p − pmin)/(pmax − pmin)  ∇p = ∇p ·  if ∇p suggests increasing p otherwise  (11)  It should be noted that these approaches are not speciﬁc to HFO or parameterized action space. Any domain featuring a bounded-continuous action space will require a similar approach for enforcing bounds. All three approaches are empirically evaluated the next section.  6 RESULTS  We evaluate the zeroing, squashing, and inverting gradient approaches in the parameterized HFO domain on the task of approaching the ball and scoring a goal. For each approach, we independently train two agents. All agents are trained for 3 million iterations, approximately 20,000 episodes of play. Training each agent took three days on a NVidia Titan-X GPU. Of the three approaches, only the inverting gradient shows robust learning. Indeed both inverting gradient agents learned to reliably approach the ball and score goals. None of the other four agents using the squashing or zeroing gradients were able to reliably approach the ball or score. Further analysis of the squashing gradient approach reveals that parameters stayed within their bounds, but squashing functions quickly became saturated. The resulting agents take the same discrete action with the same maximum/minimum parameters each timestep. Given the observed proclivity of the critic’s gradients to push parameters towards ever larger/small values, it is no sur- prise that squashing function quickly become saturated and never recover. Further analysis of the zeroing gradient approach reveals two problems: 1) parameters still overﬂow their bounds and 2) instability: While the gradient zeroing approach negates any direct attempts to increase a parameter p beyond its bounds, we hypothesize the ﬁrst problem stems from gradients applied to other parameters pi (cid:54)= p which inadvertently allow parameter p to overﬂow. Empirically, we observed learned networks attempting to dash with a power of 120, more than the maximum of 100. It is reasonable for a critic network to encourage the actor to dash faster. Unstable learning was observed in one of the two zeroing gradient agents. This instability is well captured in the Q-Values and critic losses shown in Figure 3. It’s not clear why this agent became unstable, but the remaining stable agent showed clear results of not learning.  7  Published as a conference paper at ICLR 2016  These results highlight the necessity of non-saturating functions that effectively enforce action bounds. The approach of inverting gradients was observed to respect parameter boundaries (ob- served dash power reaches 98.8 out of 100) without saturating. As a result, the critic was able to effectively shape the actor’s policy. Further evaluation of the reliability and quality of the inverting- gradient policies is presented in the next section.  (a) Inverting Gradients  (b) Zeroing Gradients  (c) Squashing Gradients  Figure 3: Analysis of gradient bounding strategies: The left/middle/right columns respectively correspond to the inverting/zeroing/squashing gradients approaches to handling bounded continuous actions. First row depicts learning curves showing overall task performance: Only the inverting gra- dient approach succeeds in learning the soccer task. Second row shows average Q-Values produced by the critic throughout the entire learning process: Inverting gradient approach shows smoothly increasing Q-Values. The zeroing approach shows astronomically high Q-Values indicating insta- bility in the critic. The squashing approach shows stable Q-Values that accurately reﬂect the actor’s performance. Third row shows the average loss experienced during a critic update (Equation 7): As more reward is experienced critic loss is expected to rise as past actions are seen as increasingly sub-optimal. Inverting gradients shows growing critic loss with outliers accounting for the rapid increase nearing the right edge of the graph. Zeroing gradients approach shows unstably large loss. Squashing gradients never discovers much reward and loss stays near zero.  8  50001000015000Episode024681012Reward50001000015000Episode024681012Reward50001000015000Episode024681012Reward0100020003000Critic Iteration (x1000)0246Average Critic Q-Value0100020003000Critic Iteration (x1000)0100000200000300000Average Critic Q­Value0100020003000Critic Iteration (x1000)0.50.00.5Average Critic Q­Value0100020003000Critic Iteration (x1000)050000100000150000200000Average Critic Loss0100020003000Critic Iteration (x1000)0.00.51.01.52.0Average Critic Loss1e80100020003000Critic Iteration (*1000)0.00000.00050.00100.00150.0020Average Critic LossPublished as a conference paper at ICLR 2016  7 SOCCER EVALUATION  We further evaluate the inverting gradient agents by comparing them to an expert agent indepen- dently created by the Helios RoboCup-2D team. This agent won the 2012 RoboCup-2D world championship and source code was subsequently released (Akiyama, 2010). Thus, this hand-coded policy represents an extremely competent player and a high performance bar. As an additional baseline we compare to a SARSA learning agent. State-Action-Reward-State- Action (SARSA) is an algorithm for model-free on-policy Reinforcement Learning Sutton & Barto (1998). The SARSA agent learns in a simpliﬁed version of HFO featuring high-level discrete actions for moving, dribbling, and shooting the ball. As input it is given continuous features that including the distance and angle to the goal center. Tile coding Sutton & Barto (1998) is used to discretize the state space. Experiences collected by playing the game are then used to bootstrap a value function. To show that the deep reinforcement learning process is reliable, in additional to the previous two inverting-gradient agents we independently train another ﬁve inverting-gradient agents, for a total of seven agents DDPG1−7. All seven agents learned to score goals. Comparing against the Helios’ champion agent, each of the learned agents is evaluated for 100 episodes on how quickly and reliably it can score. Six of seven DDPG agents outperform the SARSA baseline, and remarkably, three of the seven DDPG agents score more reliably than Helios’ champion agent. Occasional failures of the Helios agent result from noise in the action space, which occasionally causes missed kicks. In contrast, DDPG agents learn to take extra time to score each goal, and become more accurate as a result. This extra time is reasonable considering DDPG is rewarded only for scoring and experiences no real pressure to score more quickly. We are encouraged to see that deep reinforcement learning can produce agents competitive with and even exceeding an expert handcoded agent.  Scoring Avg. Steps Percent  to Goal  Helios’ Champion  SARSA DDPG1 DDPG2 DDPG3 DDPG4 DDPG5 DDPG6 DDPG7  .962 .81 1 .99 .98 .96 .94 .84 .80  72.0 70.7 108.0 107.1 104.8 112.3 119.1 113.2 118.2  (a) Learning Curve  (b) Evaluation Performance  Figure 4: Left: Scatter plot of learning curves of DDPG-agents with Lowess curve. Three dis- tinct phases of learning may be seen: the agents ﬁrst get small rewards for approaching the ball (episode 1500), then learn to kick the ball towards the goal (episodes 2,000 - 8,000), and start scoring goals around episode 10,000. Right: DDPG-agents score nearly as reliably as ex- pert baseline, but take longer to do so. A video of DDPG1’s policy may be viewed at https: //youtu.be/Ln0Cl-jE_40.  8 RELATED WORK  RoboCup 2D soccer has a rich history of learning. In one of the earliest examples, Andre & Teller (1999) used Genetic Programming to evolve policies for RoboCup 2D Soccer. By using a sequence of reward functions, they ﬁrst encourage the players to approach the ball, kick the ball, score a goal, and ﬁnally to win the game. Similarly, our work features players whose policies are entirely trained and have no hand-coded components. Our work differs by using a gradient-based learning method paired with using reinforcement learning rather than evolution.  9  50001000015000Episode024681012RewardPublished as a conference paper at ICLR 2016  Masson & Konidaris (2015) present a parameterized-action MDP formulation and approaches for model-free reinforcement learning in such environments. The core of this approach uses a parame- terized policy for choosing which discrete action to select and another policy for selecting continu- ous parameters for that action. Given a ﬁxed policy for parameter selection, they use Q-Learning to optimize the policy discrete action selection. Next, they ﬁx the policy for discrete action selection and use a policy search method to optimize the parameter selection. Alternating these two learn- ing phases yields convergence to either a local or global optimum depending on whether the policy search procedure can guarantee optimality. In contrast, our approach to learning in parameterized action space features a parameterized actor that learns both discrete actions and parameters and a parameterized critic that learns only the action-value function. Instead of relying on an external pol- icy search procedure, we are able to directly query the critic for gradients. Finally, we parameterize our policies using deep neural networks rather than linear function approximation. Deep networks offer no theoretical convergence guarantees, but have a strong record of empirical success. Experimentally, Masson & Konidaris (2015) examine a simpliﬁed abstraction of RoboCup 2D soc- cer which co-locates the agent and ball at the start of every trial and features a smaller action space consisting only of parameterized kick actions. However, they do examine the more difﬁcult task of scoring on a keeper. Since their domain is hand-crafted and closed-source, it’s hard to estimate how difﬁcult their task is compared to the goal scoring task in our paper. Competitive RoboCup agents are primarily handcoded but may feature components that are learned or optimized. MacAlpine et al. (2015) employed the layered-learning framework to incrementally learn a series of interdependent behaviors. Perhaps the best example of comprehensively integrating learning is the Brainstormers who, in competition, use a neural network to make a large portion of decisions spanning low level skills through high level strategy (Riedmiller et al., 2009; Riedmiller & Gabel, 2007). However their work was done prior to the advent of deep reinforcement learning, and thus required more constrained, focused training environments for each of their skills. In contrast, our study learns to approach the ball, kick towards the goal, and score, all within the context of a single, monolithic policy. Deep learning methods have proven useful in various control domains. As previously mentioned DQN (Mnih et al., 2015) and DDPG (Lillicrap et al., 2015) provide great starting points for learning in discrete and continuous action spaces. Additionally, Levine et al. (2015) demonstrates the ability of deep learning paired with guided policy search to learn manipulation policies on a physical robot. The high requirement for data (in the form of experience) is a hurdle for applying deep reinforcement learning directly onto robotic platforms. Our work differs by examining an action space with latent structure and parameterized-continuous actions.  9 FUTURE WORK  The harder task of scoring on a goalie is left for future work. Additionally, the RoboCup domain presents many opportunities for multi-agent collaboration both in an adhoc-teamwork setting (in which a single learning agent must collaborate with unknown teammates) and true multi-agent set- tings (in which multiple learning agents must collaborate). Challenges in multi-agent learning in the RoboCup domain have been examined by prior work (Kalyanakrishnan et al., 2007) and solutions may translate into the deep reinforcement learning settings as well. Progress in this direction could eventually result in a team of deep reinforcement learning soccer players. Another interesting possibility is utilizing the critic’s gradients with respect to state inputs ∇sQ(s, a|θQ). These gradients indicate directions of improvement in state space. An agent with a forward model may be able to exploit these gradients to transition into states which the critic ﬁnds more favorable. Recent developments in model-based deep reinforcement learning (Oh et al., 2015) show that detailed next state models are possible.  10 CONCLUSION  This paper has presented an agent trained exclusively with deep reinforcement learning which learns from scratch how to approach the ball, kick the ball to goal, and score. The best learned agent scores goals more reliably than a handcoded expert policy. Our work does not address more challenging  10  Published as a conference paper at ICLR 2016  tasks such as scoring on a goalie or cooperating with a team, but still represents a step towards fully learning complex RoboCup agents. More generally we have demonstrated the capability of deep reinforcement learning in parameterized action space. To make this possible, we extended the DDPG algorithm (Lillicrap et al., 2015), by presenting an analyzing a novel approach for bounding the action space gradients suggested by the Critic. This extension is not speciﬁc to the HFO domain and will likely prove useful for any continuous, bounded action space.  ACKNOWLEDGMENTS  The authors wish to thank Yilun Chen. This work has taken place in the Learning Agents Research Group (LARG) at the Artiﬁcial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by grants from the National Science Foundation (CNS-1330072, CNS- 1305287), ONR (21C184-01), AFRL (FA8750-14-1-0070), AFOSR (FA9550-14-1-0087), and Yu- jin Robot. Additional support from the Texas Advanced Computing Center, and Nvidia Corporation.  REFERENCES Akiyama, Hidehisa. Agent2d base code, 2010.  Andre, David and Teller, Astro. Evolving Team Darwin United. Lecture Notes in Computer URL http://link.springer-ny.  Science, 1604:346, 1999. com/link/service/series/0558/bibs/1604/16040346.htm;http: //link.springer-ny.com/link/service/series/0558/papers/1604/ 16040346.pdf.  ISSN 0302-9743.  Hafner, Roland and Riedmiller, Martin. Reinforcement learning in feedback control. Machine Learning, 84(1-2):137–169, 2011. ISSN 0885-6125. doi: 10.1007/s10994-011-5235-x. URL http://dx.doi.org/10.1007/s10994-011-5235-x.  Hausknecht, Matthew J. and Stone, Peter. Deep recurrent q-learning for partially observable mdps.  CoRR, abs/1507.06527, 2015. URL http://arxiv.org/abs/1507.06527.  Kalyanakrishnan, Shivaram, Liu, Yaxin, and Stone, Peter. Half ﬁeld offense in RoboCup soccer: A multiagent reinforcement learning case study. In Lakemeyer, Gerhard, Sklar, Elizabeth, Sorenti, Domenico, and Takahashi, Tomoichi (eds.), RoboCup-2006: Robot Soccer World Cup X, volume 4434 of Lecture Notes in Artiﬁcial Intelligence, pp. 72–85. Springer Verlag, Berlin, 2007. ISBN 978-3-540-74023-0.  Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. CoRR,  abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.  Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel, Pieter. End-to-end training of deep visuomotor policies. CoRR, abs/1504.00702, 2015. URL http://arxiv.org/abs/1504. 00702.  Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.  Continuous control with deep reinforcement learning. ArXiv e-prints, September 2015.  MacAlpine, Patrick, Depinet, Mike, and Stone, Peter. UT Austin Villa 2014: RoboCup 3D simula- tion league champion via overlapping layered learning. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence (AAAI), January 2015.  Masson, Warwick and Konidaris, George. Reinforcement learning with parameterized actions.  CoRR, abs/1509.01644, 2015. URL http://arxiv.org/abs/1509.01644.  Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A., Veness, Joel, Bellemare, Marc G., Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K., Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis, Demis. Human-level control through deep re- inforcement learning. Nature, 518(7540):529–533, February 2015. ISSN 0028-0836. doi: 10.1038/nature14236. URL http://dx.doi.org/10.1038/nature14236.  11  Published as a conference paper at ICLR 2016  Narasimhan, Karthik, Kulkarni, Tejas, and Barzilay, Regina. Language understanding for text- based games using deep reinforcement learning. CoRR, abs/1506.08941, 2015. URL http: //arxiv.org/abs/1506.08941.  Oh, Junhyuk, Guo, Xiaoxiao, Lee, Honglak, Lewis, Richard L., and Singh, Satinder P. Action- conditional video prediction using deep networks in atari games. CoRR, abs/1507.08750, 2015. URL http://arxiv.org/abs/1507.08750.  Riedmiller, Martin, Gabel, Thomas, Hafner, Roland, and Lange, Sascha. Reinforcement learning ISSN 0929-5593. doi: 10.1007/  for robot soccer. Autonomous Robots, 27(1):55–73, 2009. s10514-009-9120-4. URL http://dx.doi.org/10.1007/s10514-009-9120-4.  Riedmiller, Martin A. and Gabel, Thomas. On experiences in a complex and competitive gaming domain: Reinforcement learning meets robocup. ISBN 1- 4244-0709-5. URL http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp? punumber=4219012.  In CIG, pp. 17–23. IEEE, 2007.  Stadie, Bradly C., Levine, Sergey, and Abbeel, Pieter. Incentivizing exploration in reinforcement learning with deep predictive models. CoRR, abs/1507.00814, 2015. URL http://arxiv. org/abs/1507.00814.  Sutton, Richard S. and Barto, Andrew G. Reinforcement Learning: An Introduction. MIT Press, ISBN 0262193981. URL http://www.cs.ualberta.ca/%7Esutton/book/  1998. ebook/the-book.html.  Tieleman, T. and Hinton, G. Lecture 6.5—RmsProp: Divide the gradient by a running average of its  recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep reinforcement learning with double q-  learning. CoRR, abs/1509.06461, 2015. URL http://arxiv.org/abs/1509.06461.  Watkins, Christopher J. C. H. and Dayan, Peter. Q-learning. Machine Learning, 8(3-4):279–292, 1992. doi: 10.1023/A:1022676722315. URL http://jmvidal.cse.sc.edu/library/ watkins92a.pdf.  Zeiler, Matthew D. ADADELTA: An adaptive learning rate method. CoRR, abs/1212.5701, 2012.  URL http://arxiv.org/abs/1212.5701.  12  ",
1511.07404,2016,Learning VIsual Predictive  Models of Physics for Playing Billiards,"['Learning VIsual Predictive  Models of Physics for Playing Billiards\nKaterina Fragkiadaki', 'Pulkit Agrawal', 'Sergey Levine', 'Jitendra Malik']",https://arxiv.org/pdf/1511.07404,"Under review as a conference paper at ICLR 2016  LEARNING VISUAL PREDICTIVE MODELS OF PHYSICS FOR PLAYING BILLIARDS  Katerina Fragkiadaki∗ Pulkit Agrawal∗ Electrical Engineering and Computer Science University of California, Berkeley (katef,pulkitag,svlevine,malik)@berkeley.edu  Sergey Levine  Jitendra Malik  6 1 0 2     n a J    9 1      ]  V C . s c [      3 v 4 0 4 7 0  .  1 1 5 1 : v i X r a  ABSTRACT  The ability to plan and execute goal speciﬁc actions in varied and unseen envi- ronments settings is a central requirement of intelligent agents. In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations (“visual imagination”). Our models directly process raw visual input, and use a novel object-centric prediction formulation based on vi- sual glimpses centered on objects (ﬁxations) to enforce translational invariance of the learned physical laws. The agent trains itself through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that were previously never en- countered by the agent. We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball.  1  INTRODUCTION  Imagine a hypothetical person who has never encountered the game of billiards. While this person may not be very adept at playing the game, he would still be capable of inferring the direction in which the cue ball needs to be hit to displace the target ball to a desired location. How can this person make such an inference without any prior billiards-speciﬁc experience? One explanation is that humans are aware of the laws of physics, and a strategy for playing billiards can be inferred from knowledge about dynamics of bouncing objects. However, humans do not appear to consciously solve Newton’s equations of motion, but rather have an intuitive understanding of how their actions affect the world. In the speciﬁc example of billiards, humans can imagine the trajectory that the ball would follow when a force is applied, and how the trajectory of ball would change when it hits the side of the billiards table or another ball. We term models that can enable the agents to visually anticipate the future states of the world as visual predictive models of physics. A visual predictive model of physics equips an agent with the ability to generate potential future states of the world in response to an action without actually performing that action (“visual imagi- nation”). Such visual imagination can be thought of as running an internal simulation of the external world. By running multiple internal simulations to imagine the effects of different actions, the agent can perform planning, choosing the action with the best outcome and executing it in the real world. The idea of using internal models for planning actions is well known in the control literature (Mayne, 2014). However, the question of how such models can be learned from raw visual input has received comparatively little attention, particularly in situations where the external world can change signiﬁcantly, requiring generalization to a variety of environments and situations. Previous methods have addressed the question of learning models, including visual models, of the agent’s own body (Watter et al., 2015; Lillicrap et al., 2015). However, when performing actions in complex environments, models of both the agent and the external world are required. The external world can exhibit considerably more variation than the agent itself, and therefore such models must generalize more broadly. This makes problem of modelling the environment substantially harder than modelling the agent itself.  ∗equal contribution  1  Under review as a conference paper at ICLR 2016  Figure 1: Frame-centric versus object-centric prediction. Left: Frame-centric model predicts takes as input the the image of the the entire billiards and forces applied by the agent to make pre- dictions about the future. Right: Object-centric model predicts the future states of the world by individually modeling the temporal evolution of each the L objects in the world. In the billiards world, predicting the future velocities (ul t+h, h ∈ [1, 20], l ∈ [1, L]) of each of the L balls is sufﬁ- cient for generating the future world states. Please see section 3 for more details.  The complexities associated with modeling the external world may be elucidated through an exam- ple. Consider the family of worlds composed of moving balls on a 2D table (i.e. moving-ball world). This family contains diverse worlds that can be generated by varying factors such as the number of balls, the table geometries, ball sizes, the colors of balls and walls, and the forces applied to push the balls. Because the number of objects can change across worlds, it is not possible to explicitly deﬁne a single state space for these worlds. For the purpose of modeling, an implicit state space must be learnt directly from visual inputs. In addition to this combinatorial structure, differences in geometry and nonlinear phenomena such as collisions result in considerable complexity. Similar to the real world, in the moving-ball world, an agent must perform actions in novel conditions it has never encountered before. Although moving-ball worlds are relatively constrained and syn- thetic, the diversity of such worlds can be manipulated using a small number of factors. This makes them a good case study for systematically evaluating and comparing the performance of different model-based action selection methods under variation in external conditions (i.e. generalization). Both the real world and the synthetic moving-ball worlds also contain regularities that allow learn- ing generalizable models in the face of extensive variation, such as the translational invariance of physical laws. The main contribution of this work is a ﬁrst step towards learning dynamical model of the external world directly from visual inputs that can handle combinatorial structure and exploits translation invariance in dynamics. We propose an object-centric (OC) prediction approach, illus- trated in Figure 1), that predicts the future states of the world by individually modeling the temporal evolution of each object from object-centric glimpses. The object-centric (OC) approach naturally incorporates translation invariance and model sharing across different worlds and object instances. We use a simulated billiards-playing domain where the agent can push balls on a 2D billiard table with varying geometry as a working example to evaluate our approach. We show that our agent learns a model of the billiards world that can be used to effectively simulate the movements of balls and consequently plan actions without requiring any goal-speciﬁc supervision. Our agent successfully predicts forces required to displace the ball to a desired location on the billiards table and to hit another moving ball.  2 PREVIOUS WORK  Vision-based control Due to the recent success of deep neural networks for learning feature rep- resentations that can handle the complexity of visual input Krizhevsky et al. (2012), there has been considerable interest in utilizing this capability for learning to control dynamical systems directly from visual input. Methods that directly learn policies for prediction actions from visual inputs have been successfully used to learn to play Atari games Mnih et al. (2013) and control a real robot for a predeﬁned set of manipulation tasks Levine et al. (2015). However, these methods do not attempt to model how visual observations will evolve in response to agent’s actions. This makes it difﬁcult to repurpose the learned policies for new tasks.  2  next	  frame	  rendering	  neural	  	  network	  visual	  stream	  next	  frame	  force	  ﬁeld	  stream	  neural	  	  network	  F`1···F`tu`t+1displacements	  visual	  stream	  centered	  on	  the	  lth	  object	  	  force	  stream	  on	  the	  lth	  object	  8`21···Lnext	  frame	  Under review as a conference paper at ICLR 2016  Another body of work (Kietzmann & Riedmiller, 2009; Lange et al., 2012) has attempted to build models that transform raw sensory observations into a low-dimensional feature space that is better suited for reinforcement learning and control. More recently works such as (Wahlstr¨om et al., 2015; Watter et al., 2015) have shown successful results on relatively simple domains of manipulating a synthetic two degree of freedom robotic arm or controlling an inverted pendulum in simulation. However, training and testing environments in these works were exactly the same. In contrast, our work shows that vision based model predictive control can be used in scenarios where the test environments are substantially different from training environments.  Models of physics and model based control (Hamrick et al., 2011) provided evidence that hu- man judgement of how dynamical systems evolve in future can be explained by the hypothesis that humans use internal models of physics. (Jordan & Rumelhart, 1992; Wolpert et al., 1995; Haruno et al., 2001; Todorov & Ghahramani, 2003) proposed using internal models of the external world for planning actions. However these works have either been theoretical or have striven to explain senso- rimotor learning in humans. To the best of our knowledge we are the ﬁrst work that strives to build an internal model of the external world purely from visual data and use it for planning novel actions. (Oh et al., 2015) successfully predict future frames in Atari game videos and train a Q-controller for playing Atari games using the predicted frames. Training a Q-controller requires task speciﬁc supervision whereas our work explores whether effective dynamical models for action planning can be learnt without requiring any task speciﬁc supervision.  Learning Physics from Images and Videos Works of Wu et al. (2015); Bhat et al. (2002); Brubaker et al. (2009); Mottaghi et al. (2015) propose methods for estimating the parameters of Newtonian equations from images and videos. As laws of physics governing the dynamics of balls and walls on a billiards table are well understood, it is possible to use these laws instead of learning a predictive model for planning actions. However, there are different dynamic models that control ball-ball collisions, ball-wall collisions and the movement of ball in free space. Therefore, if these known dynamical model are to be used, then a system for detecting different event types would be required for selecting the appropriate dynamics model at different time points. In contrast, our approach avoids hand designing such event detectors and switches and provides a more general and scalable solution even in the case of billiards.  Video prediction (Michalski et al., 2014; Sutskever et al., 2008) learn models capable of gen- erating images of bouncing balls. However, these models are not shown to generalize to novel environments. Further these works donot include any notion of an agent or its inﬂuence on the envi- ronment. (Boots et al., 2014) proposes model for predicting the future visual appearance of a robotic arm, but the method is only shown to work when the same object in the same visual environment is considered. Further it is not obvious how the non-parametric approach would scale with large datasets. In contrast our approach generalized to novel environments and can scale easily with large amounts of data.  Motion prediction for Visual Tracking In Computer Vision, object trackers use a wide variety of predictive models, from simple constant velocity models, to linear dynamical systems and their variants (Urtasun et al., 2006), HMMs (Brand et al., 1997; Ghahramani & Jordan, 1997), and other models. Standard smoothers or ﬁlters, such as Kalman ﬁlters (Weng et al., 2006), usually operate on Cartesian coordinates, rather than the visual content of the targets, and in this way discard useful information that may be present in the images. Finally, methods for 3D tracking of Kyriazis et al. (2011); Salzmann & Urtasun (2011) use Physics simulators to constrain the search space during data association.  3 LEARNING PREDICTIVE VISUAL MODELS  We consider an agent observing an interacting with dynamic moving-ball worlds consisting of mul- tiple balls and multiple walls. We also refer to these worlds as billiard worlds. The agent interacts with the world by applying forces to change the velocities of the balls. In the real world, the envi- ronment of an agent is not ﬁxed, and the agent can ﬁnd itself in environments that it has not seen before. To explore this kind of generalization, we train our predictive model in a variety of billiards  3  Under review as a conference paper at ICLR 2016  Figure 2: Network architecture. At each time step t, for each object, the network is provided with the previous four glimpses centered on the object’s position, as well as the agent’s applied forces t ) and the hidden states of the LSTM units from the previous time step. The output is Ft = (F x ball displacements ut+k = (∆xt+k, ∆yt+k) for k = 1··· h in the next h frames.  t , F y  environments, which involve different numbers of balls and different wall geometries, and then test the learnt model in previously unseen settings. In the case of moving-ball world, it is sufﬁcient to predict the displacement of the ball during the next time step to generate the visual of the world in the future. Therefore, instead of directly pre- dicting image pixels, we predict each object’s current and future velocity given a sequence of visual glimpses centered at the object (visual ﬁxation) and the forces applied to it. We assume that during training the agent can perfectly track the objects. This assumption is a mild one because not only tracking is a well studied problem but also because there is evidence in the child development literature that very young infants can redirect their gaze to keep an object in focus by anticipating its motion (i.e. smooth pursuit) (Hofsten & Rosander, 1997). The early development of smooth pursuit suggests that it is important for further development of visual and motor faculties of a human infant.  3.1 MODEL DETAILS  Our network architecture is illustrated in ﬁgure 2. The input to the model is a stack of 4 images comprised of the current and previous 3 glimpses of the ﬁxated object and the exercised force on the object at the current time step. The model predicts the velocity of the object at each of the h time steps in the future. We use h = 20. The same model is applied to all the objects in the world. Our network uses an AlexNet style architecture (Krizhevsky et al., 2012) to extract visual features. The ﬁrst layer (conv1) is adapted to process a stack of 4 frames. Layers 2 and 3 have the same architecture as that of AlexNet. Layer 4 (conv4) is composed of 128 convolution kernels of size 3 × 3. The output of conv4 is rescaled to match the value range of the applied forces, then is concatenated with the current force and is passed into a fully connected (encoder) layer. Two layers of LSTM units operate on the output of the encoder to model long range temporal dynamics. Then, the output is decoded to predicted velocities. The model is trained by minimizing the Euclidean loss between ground-truth and predicted object velocities for h time steps in the future. The ground-truth velocities are known because we assume object tracking. The loss is mathematically expressed as:  L =  wk(cid:107)˜ut+k − ut+k(cid:107)2  2  (1)  where ut+k, ˜ut+k represent ground-truth and predicted velocities at the kth time step in the future respectively. Velocities are expressed in cartesian coordinates. We weigh the loss in a manner that  4  h(cid:88)  k=1  Under review as a conference paper at ICLR 2016  Figure 3: Predicting ball motion under interactions and agent forces. Dots denote predictions of our model and circles the ground-truth future positions for the ﬁxated ball. Color indicates length of the prediction horizon, blue denotes close and red far in the future. Our model accurately predicts collisions under a wide variety of table conﬁgurations.  1  errors in predictions at a shorter time horizon are penalized more than predictions at a longer time 4 ). We use the publicly horizon. This weighting is achieved using penalty weights wk = exp(−k available Caffe package for training our model. For model learning, we generate sequences of ball motions in a randomly sampled world conﬁg- uration. As shown in Figure 3, we experimented both with rectangular and non-rectangular wall geometries. For rectangular walls, a single sample of the world was generated by randomly choos- ing the size of the walls, location of the balls and the forces applied on the balls from a predeﬁned range. The length of each sequence was sampled from the range [20, 200]. The length of the walls was sampled from a range of [300 pixels, 550 pixels]. Balls were of radius 25 pixels and uniform density. Force direction was uniformly sampled and the force magnitude was sampled from the range [30K Newtons, 80K Newtons]. Forces were only applied on the ﬁrst frame. The size of visual glimpses is 600x600 pixels. The objects can move up to 10 pixels in each time step and therefore in 20 time steps they can cover distances up to 200 pixels. For training, we pre-generated 10K such sequences. We constructed minibatches by choosing 50 random subsets of 20 consequent frames from this pre-generated dataset. Weights in layers conv2 and conv3 were initialized from the weights of Alexnet that was trained for performing image clas- siﬁcation on Imagenet (Krizhevsky et al., 2012). Weights in other layers are randomly initialized.  4 MODEL EVALUATION  First we report evaluations on random worlds sampled from the same distribution as the training data. Next, we report evaluations on worlds sampled from a different distribution of world conﬁg- urations to study the generalization of the proposed approach. Error in the angle and magnitude of the predicted velocities were used as performance metrics. We compared the performance of the proposed object centric (OC) model with a constant velocity (CV) and frame centric (FC) model. The constant velocity model predicts the velocity of a ball for all the future frames to be same as the ground truth velocity at the previous time step. The ball changes the velocity only when it strikes another ball or hits a wall. As collisions are relatively infrequent, the constant velocity model is a good baseline. We ﬁrst trained a model on the family of rectangular worlds consisting of 1 ball only. The results of this evaluation are reported in Table 1. We used average error across all the frames and the error  5  Under review as a conference paper at ICLR 2016  Time  t+1 t+5 t+20  CV  3.0o/0.00 11.8o/0.01 45.3o/0.01  Overall Error  FC  6.2o/0.04 8.7o/0.05 16.3o/0.09  OC  5.1o/0.03 7.2o/0.04 14.8o/0.09  Error Near Collisions  CV  23.2o/0.00 56.6o/0.05 123.0o/0.04  FC  11.4o/0.06 21.1o/0.12 54.8o/0.20  OC  9.8o/0.04 17.9o/0.10 54.8o/0.20  Table 1: Quantitative evaluation of the prediction model reported as error in the magnitude and angle of the velocity. The average error across all frames (i.e. Overall Error) and error averaged only in frames that were within [-4, 4] frames of a frame depicting collision (i.e. Error Near Collisions) are reported for Constant Velocity (CV) model, Frame Centric (FC) and Object Centric (OC) models. The errors are reported as ao/b, where a is the mean of angular error in degrees and b is the relative error in the magnitude of the predicted velocity. The constant velocity model predicts the velocity of a ball for all the future frames to be same as the ground truth velocity at the previous time step.  Figure 4: Performance comparison of frame centric (FC) and object centric (OC) models. Perfor- mance is measured as the angular error near collisions. Dashed and solid lines show the angular error over a horizon of 20 time steps (h = 20) for FC and OC models respectively. (Left) Angular errors for models trained and tested on 1, 2 and 3 ball worlds respectively. The performance of the models only degrades slightly with increasing number of balls. Generally, the OC model is more accurate than the FC model. (Right) Comparing the performance the FC model against the OC model when models trained on 2 and 3 balls are tested on worlds containing a larger number of balls. The naming convention nB-on-mB indicates models trained on n-ball worlds and tested on m-ball worlds. The generalization of the proposed OC model is signiﬁcantly better than the FC model.  averaged across frames only near the collisions as the error metrics for measuring performance. As balls move in linear trajectories except for time of collision, accurately predicting the velocities after a collision event is of speciﬁc interest. Results in Table 1 show that the object centric (OC) model is better than frame centric model (FC) model and much better than the constant velocity model. These results show that object centric modelling leads to better learning.  How well does our model scale with increasing number of balls? For studying this, we trained models on families of world consisting of 2 and 3 balls respectively. We used the learnt 1-ball model to initialize the training of the 2-ball model, which in turn was used to initialize the training of the 3- ball model. We found this curriculum learning approach to outperform models trained from scratch. The 2 and 3-ball models were evaluated on worlds separate from training set that consisted of 2 and 3 ball respectively. The angular errors measured near collisions (for h = 1 to 20) are shown Figure 4. The performance of our model degrades only by small amounts as the number of balls increase. Also, in general the OC model performs better than the FC model. We also trained and tested our models on non-rectangular walls. Qualitative visualizations of ground truth and predicted ball trajectories are show in ﬁgure 3. The ﬁgure shows that our model accurately predicts the velocities of balls after collisions in varied environments. This result indicates that our models are not constrained to any speciﬁc environment and have learnt something about the dynamics of balls and their collisions.  6  05101520Number of time steps in the future (h)01020304050607080Angular Error (degrees)1 Ball2 Balls3 Balls05101520Number of time steps in the future (h)1020304050607080Angular Error (degrees)2B-on-3B3B-on-4B3B-on-6BUnder review as a conference paper at ICLR 2016  Figure 5: Generating visual imaginations: The visual image of the world at current and past 3 time steps and the applied forces are used to predict the velocity of every ball. The visual image of the world at the next time step is rendered by translating the balls by their predicted velocities. This process is repeated iteratively to generate a sequence of visuals of the future world states.  4.1 EVALUATING GENERALIZATION  The results reported in the previous section show generalization to worlds sampled from the same distribution as the training set. In addition to this, we also tested our models on worlds substantially different from the worlds in the training set. Figure 7 shows that our model can generalize to much larger wall conﬁgurations than those used in the training. The wall lengths in the training set were between 300-550 pixels, whereas the the wall lengths in the testing set were sampled from the range of 800-1200 pixels. This shows that our models can generalize to different wall geometries. Figure 4 (right) shows that models trained on 2 and 3-ball worlds perform well when tested on 3, 4 and 6-ball worlds. This shows that our models can generalize to worlds with larger number of balls without requiring any additional training. The results in the ﬁgure also show that proposed OC model generalizes substantially better than the FC model.  5 GENERATING VISUAL IMAGINATIONS  As our models can accurately predict ball dynamics in the future, we can use these models to gen- erate visual imaginations of the future. The procedure we use for generating visual imaginations is illustrated in ﬁgure 5. Given a conﬁguration of the world and applied forces, the visual image of the world at the time step t + 1 is generated by translating each ball by amount to equal to its predicted velocity (˜ut) at time t. This generated image forms the input to the model for generating the visual image of the world at time step t+2. This process is repeated iteratively to generate visual imaginations of the future world states. Some examples of visual imaginations by our model are shown in ﬁgure 6. Our model learns to anticipate collisions early in time. Predicting trajectory reversals at collisions is not possible using methods Kalman ﬁlter based methods that are extensively used in object tracking(Welch & Bishop, 1995). Comparison with ground truth trajectories reveals that our models are not perfect and in some cases accumulation of errors can produce imagined trajectories that are different from the ground truth trajectories (for instance see the ﬁrst column in ﬁgure 6). Even in the cases when predicted trajectories do not exactly match up with the ground truth trajectories, the visual imaginations are consistent with the dynamics of balls and collisions. Figure 7 shows visual imaginations by our model in environments that are much larger than the environments used in the training set. Notice that the glimpse size is considerably smaller than the size of the environment. With glimpses of this size, visual inputs when the ball is not close to any of the walls are uninformative because such visual inputs merely comprise of a ball present in center of white background. In such scenarios, our model is able to make accurate predictions of velocity due  7  (,)(,)(,)(,)neural	  	  network	  neural	  	  network	  neural	  	  network	  neural	  	  network	  u1t+1u2t+1u3t+1u4t+1frame	  at	  .me	  t	  frame	  at	  .me	  t+1	  (F1t,F2t,F3t,F4t)F1tF2tF3tF4tforces	  	  at	  .me	  t	  forces	  	  at	  .me	  t+1	  (0,0,0,0)Under review as a conference paper at ICLR 2016  Figure 6: Visual Imaginations generated by our model. (Top) shows the trajectory of the ball gener- ated by our model by iteratively rendering the next world image using the predicted ball velocities and feeding this rendered image as input to the model. The imagined ball trajectory is color coded to reﬂect the progression of time. Change in color from dark to light blue indicates moving forward in time. (Bottom) shows the respective ground truth trajectories. Force on the ball is applied at the ﬁrst time step and the force vector is shown by the orange arrow. The ﬁgure shows that our model learns the dynamics of balls and collisions and is successfully able to image collision events.  Figure 7: Visual imaginations of our model in very large environments. Accurate predictions of the ball trajectory are made despite the fact that visual glimpses (shown in yellow) are mostly un- informative (they only contain a ball in center of white background). This is made possible by the long term memory in LSTMs. Without LSTMs, the ball exhibits non natural motion and reverses its direction in the middle of the arena.  to the long-range LSTM memory of the past. Without LSTM units, we noticed that imagined ball trajectory exhibited unexpected reversal in directions and other errors. For more examples, please see accompanying video for imaginations in two and three ball worlds.  6 USING PREDICTIVE VISUAL MODELS FOR ACTION PLANNING  We used the learnt predictive models for planning actions to achieve goals which the agent has never received any direct supervision. We ﬁrst show results on a relatively simple task of planning the force required to push the desired ball to a desired location. Next, we show results on a more challenging task of planning the force required to push the desired ball to hit a second moving ball. Figure 8 illustrates the method of action planning. Given a target state, the optimal force is found by running multiple simulations (i.e. visual imaginations) of the world after applying different forces. The optimal force is the one that produces the world state that is closest to the target state. In order to verify the accuracy of this method, we use the predicted force from our model as input to our physics engine to generate its actual (rather than the imagined) outcome and compare the resulting states against the goal states. In practice, instead of exhaustively searching for all forces we use CMA-ES method (Hansen & Ostermeier, 2001) for determining the optimal force.  8  Under review as a conference paper at ICLR 2016  Figure 8: Illustration of the method for determining the force required to push the cue ball (the ball with the arrow) to a target location (shown in green). The agent runs multiple simulations of the world (i.e. visual imagination) by applying different forces (shown by red arrows). Each grey box shows the results of simulations performed by the agent. The position of the ball closest to the target location is shown in pink in each of the simulations. The agent chooses the force that leads to a ball location that is closest to the target location (simulation in yellow box).  Method  Oracle Random Ours (FC-Model) Ours (OC-Model)  Hit Accuracy  < 10 pixels < 25 pixels < 50 pixels  95% 3% 15% 30%  100% 14% 39% 56%  100% 23% 60% 85%  Table 2: Hit accuracy of our approach for pushing the ball to a desired target location. The hit accuracy was measured as the number of trials for which the closest point on the ball’s trajectory was within p pixels of the target. Accuracy has been reported for p={10, 25, 50} pixels. The random baseline was constructed by randomly choosing a force and the oracle used the ground truth physics simulator for selecting the optimal actions.  Table 2 reports the hit accuracy of our system in pushing the ball to a desired location. The hit accuracy was measured as the number of trials for which the closest point on the ball’s trajectory was within p pixels of the target. With an accuracy of 56% our model is able to push the ball within 25 pixels (the size of the arena was between 300-550 pixels in size) of the target location as compared to the oracle which is successful 100% times. The OC model signiﬁcantly outperforms the FC model. The oracle was constructed by using the ground truth physics simulator for making predictions and used the same mechanism for action selection as described above. Qualitative results of our methods are best seen in the accompanying video. We will include quantitative evaluation of more complex actions in the next revision of the paper.  7 DISCUSSION AND CONCLUSION  We have presented an object-centric prediction approach that exploits translation invariance in dy- namics of physical systems to learn a dynamical model of the world directly from visual inputs. We show that the model generalizes to environments never encountered during training and can be used for planning actions in novel environments without the requirement of task-speciﬁc supervision. Using our method in complex real world settings requires more nuanced mechanisms for creating visual renderings. We are investigating multiple directions like creating imaginations in a latent abstract feature space, or using visual exemplars as proxies of per frame visual renderings. We are also exploring different mechanisms for improving predictions using error denoising and alternate loss functions. We believe that the direction of learning to predict the effect of agent’s actions on the world directly from visual inputs is an important direction for enabling robots to act in previously unseen environments. Our work makes a small step in this direction.  9  Under review as a conference paper at ICLR 2016  REFERENCES Bhat, Kiran S., Seitz, Steven M., and Popovic, Jovan. Computing the physical parameters of rigid- body motion from video. In Heyden, Anders, Sparr, Gunnar, Nielsen, Mads, and Johansen, Peter (eds.), ECCV (1), volume 2350 of Lecture Notes in Computer Science, pp. 551–565. Springer, 2002. ISBN 3-540-43745-2. URL http://dblp.uni-trier.de/db/conf/ eccv/eccv2002-1.html#BhatSP02.  Boots, Byron, Byravan, Arunkumar, and Fox, Dieter. Learning predictive models of a depth camera & manipulator from raw execution traces. In 2014 IEEE International Conference on Robotics and Automation, ICRA 2014, Hong Kong, China, May 31 - June 7, 2014, pp. 4021–4028, 2014. doi: 10.1109/ICRA.2014.6907443. URL http://dx.doi.org/10.1109/ICRA.2014. 6907443.  Brand, M., Oliver, N., and Pentland, A. Coupled hidden markov models for complex action recog-  nition. In CVPR, 1997.  Brubaker, Marcus A., Sigal, L., and Fleet, D.J. Estimating contact dynamics. In Computer Vision, 2009 IEEE 12th International Conference on, pp. 2389–2396, Sept 2009. doi: 10.1109/ICCV. 2009.5459407.  Ghahramani, Zoubin and Jordan, Michael I. Factorial hidden markov models. Mach. Learn., 29,  1997.  Hamrick, Jessica, Battaglia, Peter, and Tenenbaum, Joshua B. Internal physics models guide prob- abilistic judgments about object dynamics. In Proceedings of the 33rd annual conference of the cognitive science society, pp. 1545–1550. Cognitive Science Society Austin, TX, 2011.  Hansen, Nikolaus and Ostermeier, Andreas. Completely derandomized self-adaptation in evolution  strategies. Evolutionary computation, 9(2):159–195, 2001.  Haruno, Masahiko, Wolpert, David H, and Kawato, Mitsuo. Mosaic model for sensorimotor learning  and control. Neural computation, 13(10):2201–2220, 2001.  Hofsten, Claes Von and Rosander, Kerstin. Development of smooth pursuit tracking in young in- fants. Vision Research, 37(13):1799 – 1810, 1997. ISSN 0042-6989. doi: http://dx.doi.org/ 10.1016/S0042-6989(96)00332-X. URL http://www.sciencedirect.com/science/ article/pii/S004269899600332X.  Jordan, Michael I and Rumelhart, David E. Forward models: Supervised learning with a distal  teacher. Cognitive science, 16(3):307–354, 1992.  Kietzmann, Tim C. and Riedmiller, Martin A. The neuro slot car racer: Reinforcement learning In International Conference on Machine Learning and Applications, in a real world setting. ICMLA 2009, Miami Beach, Florida, USA, December 13-15, 2009, pp. 311–316, 2009. doi: 10.1109/ICMLA.2009.15. URL http://dx.doi.org/10.1109/ICMLA.2009.15.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS. 2012.  Kyriazis, Nikolaos, Oikonomidis, Iason, and Argyros, Antonis A. Binding computer vision to physics based simulation: The case study of a bouncing ball. In British Machine Vision Con- ference, BMVC 2011, Dundee, UK, August 29 - September 2, 2011. Proceedings, pp. 1–11, 2011. doi: 10.5244/C.25.43. URL http://dx.doi.org/10.5244/C.25.43.  Lange, Sascha, Riedmiller, Martin A., and Voigtl¨ander, Arne. Autonomous reinforcement learning on raw visual input data in a real world application. In The 2012 International Joint Conference on Neural Networks (IJCNN), Brisbane, Australia, June 10-15, 2012, pp. 1–8, 2012. doi: 10.1109/ IJCNN.2012.6252823. URL http://dx.doi.org/10.1109/IJCNN.2012.6252823.  Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel, Pieter. End-to-end training of deep visuomotor policies. CoRR, abs/1504.00702, 2015. URL http://arxiv.org/abs/1504. 00702.  10  Under review as a conference paper at ICLR 2016  Lillicrap, Timothy P., Hunt, Jonathan J., Pritzel, Alexander, Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David, and Wierstra, Daan. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015.  Mayne, David Q. Model predictive control: Recent developments and future promise. Automat- ica, 50(12):2967 – 2986, 2014. ISSN 0005-1098. doi: http://dx.doi.org/10.1016/j.automatica. 2014.10.128. URL http://www.sciencedirect.com/science/article/pii/ S0005109814005160.  Michalski, Vincent, Memisevic, Roland,  ing deep temporal dependencies with recurrent grammar in Neural formation Processing Canada, 5549-modeling-deep-temporal-dependencies-with-recurrent-grammar-cells.  Model- In Advances In- 2014, Montreal, Quebec, URL http://papers.nips.cc/paper/  Information Processing Systems 27:  Annual Conference on Neural  and Konda, Kishore Reddy.  2014, December  1925–1933,  Systems  cells””.  2014.  8-13  pp.  Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Graves, Alex, Antonoglou, Ioannis, Wier- stra, Daan, and Riedmiller, Martin A. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.  Mottaghi, Roozbeh, Bagherinezhad, Hessam, Rastegari, Mohammad, and Farhadi, Ali. Newtonian image understanding: Unfolding the dynamics of objects in static images. CoRR, abs/1511.04048, 2015. URL http://arxiv.org/abs/1511.04048.  Oh, Junhyuk, Guo, Xiaoxiao, Lee, Honglak, Lewis, Richard, and Singh, Satinder. Action- arXiv preprint  in atari games.  conditional video prediction using deep networks arXiv:1507.08750, 2015.  Salzmann, Mathieu and Urtasun, Raquel. Physically-based motion models for 3d tracking: A convex formulation. In Metaxas, Dimitris N., Quan, Long, Sanfeliu, Alberto, and Gool, Luc J. Van (eds.), ICCV, pp. 2064–2071. IEEE, 2011. ISBN 978-1-4577-1101-5. URL http://dblp. uni-trier.de/db/conf/iccv/iccv2011.html#SalzmannU11.  Sutskever, Ilya, Hinton, Geoffrey E., and Taylor, Graham W. The recurrent temporal restricted  boltzmann machine. In NIPS, 2008.  Todorov, Emanuel and Ghahramani, Zoubin. Unsupervised learning of sensory-motor primitives. In Engineering in Medicine and Biology Society, 2003. Proceedings of the 25th Annual International Conference of the IEEE, volume 2, pp. 1750–1753. IEEE, 2003.  Urtasun, Raquel, Fleet, David J., and Fua, Pascal. 3d people tracking with gaussian process dynam-  ical models. In CVPR, 2006.  Wahlstr¨om, Niklas, Sch¨on, Thomas B., and Deisenroth, Marc Peter. From pixels to torques: Policy learning with deep dynamical models. CoRR, abs/1502.02251, 2015. URL http://arxiv. org/abs/1502.02251.  Watter, Manuel, Springenberg, Jost Tobias, Boedecker, Joschka, and Riedmiller, Martin A. Em- bed to control: A locally linear latent dynamics model for control from raw images. CoRR, abs/1506.07365, 2015. URL http://arxiv.org/abs/1506.07365.  Welch, Greg and Bishop, Gary. An introduction to the kalman ﬁlter. Technical report, Chapel Hill,  NC, USA, 1995.  Weng, Shiuh-Ku, Kuo, Chung-Ming, and Tu, Shu-Kang. Video object tracking using adap- tive kalman ﬁlter. ISSN 1047- 3203. doi: 10.1016/j.jvcir.2006.03.004. URL http://dx.doi.org/10.1016/j.jvcir. 2006.03.004.  J. Vis. Comun. Image Represent., 17(6):1190–1208, 2006.  Wolpert, Daniel M, Ghahramani, Zoubin, and Jordan, Michael I. An internal model for sensorimotor  integration. Science-AAAS-Weekly Paper Edition, 269(5232):1880–1882, 1995.  11  Under review as a conference paper at ICLR 2016  Wu, Jiajun, Yildirim, Ilker, Lim, Joseph J, Freeman, Bill, and Tenenbaum, Josh. Galileo: Perceiving physical object properties by integrating a physics engine with deep learn- In Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R., and ing. Information Processing Systems 28, pp. 127– Garnett, R. 135. Curran Associates, URL http://papers.nips.cc/paper/ 5780-galileo-perceiving-physical-object-properties-by-integrating-a-physics-engine-with-deep-learning. pdf.  (eds.), Advances in Neural  2015.  Inc.,  12  ",
1502.05698,2016,Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks,"['Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks [code] [data]\nJason Weston', 'Antoine Bordes', 'Sumit Chopra', 'Sasha Rush', 'Bart van Merrienboer', 'Armand Joulin', 'Tomas Mikolov']",https://arxiv.org/pdf/1502.05698,"5 1 0 2   c e D 1 3         ] I  A . s c [      0 1 v 8 9 6 5 0  .  2 0 5 1 : v i X r a  Under review as a conference paper at ICLR 2016  TOWARDS AI-COMPLETE QUESTION ANSWERING: A SET OF PREREQUISITE TOY TASKS  Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merri¨enboer, Armand Joulin & Tomas Mikolov Facebook AI Research 770 Broadway New York, USA {jase,abordes,spchopra,tmikolov,sashar,bartvm}@fb.com  ABSTRACT  One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the use- fulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.  1  INTRODUCTION  There is a rich history of the use of synthetic tasks in machine learning, from the XOR problem which helped motivate neural networks (Minsky & Papert, 1969; Rumelhart et al., 1985), to circle and ring datasets that helped motivate some of the most well-known clustering and semi-supervised learning algorithms (Ng et al., 2002; Zhu et al., 2003), Mackey Glass equations for time series (M¨uller et al., 1997), and so on – in fact some of the well known UCI datasets (Bache & Lichman, 2013) are synthetic as well (e.g., waveform). Recent work continues this trend. For example, in the area of developing learning algorithms with a memory component synthetic datasets were used to help develop both the Neural Turing Machine of Graves et al. (2014) and the Memory Networks of Weston et al. (2014), the latter of which is relevant to this work.  One of the reasons for the interest in synthetic data is that it can be easier to develop new techniques using it. It is well known that working with large amounts of real data (“big data”) tends to lead researchers to simpler models as “simple models and a lot of data trump more elaborate models based on less data” (Halevy et al., 2009). For example, N -grams for language modeling work well relative to existing competing methods, but are far from being a model that truly understands text. As researchers we can become stuck in local minima in algorithm space; development of synthetic data is one way to try and break out of that.  In this work we propose a framework and a set of synthetic tasks for the goal of helping to develop learning algorithms for text understanding and reasoning. While it is relatively difﬁcult to auto- matically evaluate the performance of an agent in general dialogue – a long term-goal of AI – it is relatively easy to evaluate responses to input questions, i.e., the task of question answering (QA). Question answering is incredibly broad: more or less any task one can think of can be cast into this setup. This enables us to propose a wide ranging set of different tasks, that test different capabilities of learning algorithms, under a common framework.  Our tasks are built with a uniﬁed underlying simulation of a physical world, akin to a classic text adventure game (Montfort, 2005) whereby actors move around manipulating objects and interacting  1  Under review as a conference paper at ICLR 2016  with each other. As the simulation runs, grounded text and question answer pairs are simultaneously generated. Our goal is to categorize different kinds of questions into skill sets, which become our tasks. Our hope is that the analysis of performance on these tasks will help expose weaknesses of current models and help motivate new algorithm designs that alleviate these weaknesses. We further envision this as a feedback loop where new tasks can then be designed in response, perhaps in an adversarial fashion, in order to break the new models.  The tasks we design are detailed in Section 3, and the simulation used to generate them in Section 4. In Section 5 we give benchmark results of standard methods on our tasks, and analyse their successes and failures. In order to exemplify the kind of feedback loop between algorithm development and task development we envision, in Section A we propose a set of improvements to the recent Memory Network method, which has shown to give promising performance in QA. We show our proposed approach does indeed give improved performance on some tasks, but is still unable to solve some of them, which we consider as open problems.  2 RELATED WORK  Several projects targeting language understanding using QA-based strategies have recently emerged. Unlike tasks like dialogue or summarization, QA is easy to evaluate (especially in true/false or multiple choice scenarios) and hence makes it an appealing research avenue. The difﬁculty lies in the deﬁnition of questions: they must be unambiguously answerable by adult humans (or children), but still require some thinking. The Allen Institute for AI’s ﬂagship project ARISTO1 is organized around a collection of QA tasks derived from increasingly difﬁcult science exams, at the 4th, 8th, and 12th grade levels. Richardson et al. (2013) proposed the MCTest2 a set of 660 stories and associated questions intended for research on the machine comprehension of text. Each question requires the reader to understand different aspects of the story.  These two initiatives go in a promising direction but interpreting the results on these benchmarks remain complicated. Indeed, no system has yet been able to fully solve the proposed tasks and since many sub-tasks need to be solved to answer any of their questions (coreference, deduction, use of common-sense, etc.), it is difﬁcult to clearly identify capabilities and limitations of these systems and hence to propose improvements and modiﬁcations. As a result, conclusions drawn from these projects are not much clearer than that coming from more traditional works on QA over large-scale Knowledge Bases (Berant et al., 2013; Fader et al., 2014). Besides, the best performing systems are based on hand-crafted patterns and features, and/or statistics acquired on very large corpora. It is difﬁcult to argue that such systems actually understand language and are not simply light upgrades of traditional information extraction methods (Yao et al., 2014). The system of Berant et al. (2014) is more evolved since it builds a structured representation of a text and of a question to answer. Despite its potential this method remains highly domain speciﬁc and relies on a lot of prior knowledge.  Based on these observations, we chose to conceive a collection of much simpler QA tasks, with the main objective that failure or success of a system on any of them can unequivocally provide feedback on its capabilities. In that, we are close to the Winograd Schema Challenge Levesque et al. (2011), which is organized around simple statements followed by a single binary choice question such as: “Joan made sure to thank Susan for all the help she had received. Who had received the help? Joan or Susan?”. In this challenge, and our tasks, it is straightforward to interpret results. Yet, where the Winograd Challenge is mostly centered around evaluating if systems can acquire and make use of background knowledge that is not expressed in the words of the statement, our tasks are self-contained and are more diverse. By self-contained we mean our tasks come with both training data and evaluation data, rather than just the latter as in the case of ARISTO and the Winograd Challenge. MCTest has a train/test split but the training set is likely too small to capture all the reasoning needed to do well on the test set. In our setup one can assess the amount of training examples needed to perform well (which can be increased as desired) and commonsense knowledge and reasoning required for the test set should be contained in the training set. In terms of diversity, some of our tasks are related to existing setups but we also propose many additional ones; tasks 8 and 9 are inspired by previous work on lambda dependency-based compositional semantics (Liang et al., 2013; Liang, 2013) for instance. For us, each task checks one skill that the system must  1 2  http://allenai.org/aristo.html  http://research.microsoft.com/mct  2  Under review as a conference paper at ICLR 2016  have and we postulate that performing well on all of them is a prerequisite for any system aiming at full text understanding and reasoning.  3 THE TASKS  Principles Our main idea is to provide a set of tasks, in a similar way to how software testing is built in computer science. Ideally each task is a “leaf” test case, as independent from oth- ers as possible, and tests in the simplest way possible one aspect of intended behavior. Subse- quent (“non-leaf”) tests can build on these by testing combinations as well. The tasks are pub- licly available at http://fb.ai/babi. Source code to generate the tasks is available at https://github.com/facebook/bAbI-tasks.  Each task provides a set of training and test data, with the intention that a successful model performs well on test data. Following Weston et al. (2014), the supervision in the training set is given by the true answers to questions, and the set of relevant statements for answering a given question, which may or may not be used by the learner. We set up the tasks so that correct answers are limited to a single word (Q: Where is Mark? A: bathroom), or else a list of words (Q: What is Mark holding?) as evaluation is then clear-cut, and is measured simply as right or wrong.  All of the tasks are noiseless and a human able to read that language can potentially achieve 100% accuracy. We tried to choose tasks that are natural to a human: they are based on simple usual situ- ations and no background in areas such as formal semantics, machine learning, logic or knowledge representation is required for an adult to solve them.  The data itself is produced using a simple simulation of characters and objects moving around and interacting in locations, described in Section 4. The simulation allows us to generate data in many different scenarios where the true labels are known by grounding to the simulation. For each task, we describe it by giving a small sample of the dataset including statements, questions and the true labels (in red) in Tables 1 and 2.  Single Supporting Fact Task 1 consists of questions where a previously given single supporting fact, potentially amongst a set of other irrelevant facts, provides the answer. We ﬁrst test one of the simplest cases of this, by asking for the location of a person, e.g. “Mary travelled to the ofﬁce. Where is Mary?”. This kind of task was already employed in Weston et al. (2014). It can be considered the simplest case of some real world QA datasets such as in Fader et al. (2013).  Two or Three Supporting Facts A harder task is to answer questions where two supporting state- ments have to be chained to answer the question, as in task 2, where to answer the question “Where is the football?” one has to combine information from two sentences “John is in the playground” and “John picked up the football”. Again, this kind of task was already used in Weston et al. (2014). Similarly, one can make a task with three supporting facts, given in task 3, whereby the ﬁrst three statements are all required to answer the question “Where was the apple before the kitchen?”.  Two or Three Argument Relations To answer questions the ability to differentiate and recognize subjects and objects is crucial. In task 4 we consider the extreme case where sentences feature re- ordered words, i.e. a bag-of-words will not work. For example, the questions “What is north of the bedroom?” and “What is the bedroom north of?” have exactly the same words, but a different order, with different answers. A step further, sometimes one needs to differentiate three separate arguments. Task 5 involves statements like “Jeff was given the milk by Bill” and then queries who is the giver, receiver or which object is involved.  Yes/No Questions Task 6 tests, on some of the simplest questions possible (speciﬁcally, ones with a single supporting fact), the ability of a model to answer true/false type questions like “Is John in the playground?”.  Counting and Lists/Sets Task 7 tests the ability of the QA system to perform simple counting operations, by asking about the number of objects with a certain property, e.g. “How many objects is Daniel holding?”. Similarly, task 8 tests the ability to produce a set of single word answers in the form of a list, e.g. “What is Daniel holding?”. These tasks can be seen as QA tasks related to basic database search operations.  3  Under review as a conference paper at ICLR 2016  Table 1: Sample statements and questions from tasks 1 to 10.  Task 1: Single Supporting Fact Mary went to the bathroom. John moved to the hallway. Mary travelled to the ofﬁce. Where is Mary? A:ofﬁce  Task 2: Two Supporting Facts John is in the playground. John picked up the football. Bob went to the kitchen. Where is the football? A:playground  Task 3: Three Supporting Facts John picked up the apple. John went to the ofﬁce. John went to the kitchen. John dropped the apple. Where was the apple before the kitchen? A:ofﬁce  Task 4: Two Argument Relations The ofﬁce is north of the bedroom. The bedroom is north of the bathroom. The kitchen is west of the garden. What is north of the bedroom? A: ofﬁce What is the bedroom north of? A: bathroom  Task 5: Three Argument Relations Mary gave the cake to Fred. Fred gave the cake to Bill. Jeff was given the milk by Bill. Who gave the cake to Fred? A: Mary Who did Fred give the cake to? A: Bill  Task 6: Yes/No Questions John moved to the playground. Daniel went to the bathroom. John went back to the hallway. Is John in the playground? A:no Is Daniel in the bathroom? A:yes  Task 7: Counting Daniel picked up the football. Daniel dropped the football. Daniel got the milk. Daniel took the apple. How many objects is Daniel holding? A: two  Task 8: Lists/Sets Daniel picks up the football. Daniel drops the newspaper. Daniel picks up the milk. John took the apple. What is Daniel holding? milk, football  Task 9: Simple Negation Sandra travelled to the ofﬁce. Fred is no longer in the ofﬁce. Is Fred in the ofﬁce? A:no Is Sandra in the ofﬁce? A:yes  Task 10: Indeﬁnite Knowledge John is either in the classroom or the playground. Sandra is in the garden. Is John in the classroom? A:maybe Is John in the ofﬁce? A:no  Simple Negation and Indeﬁnite Knowledge Tasks 9 and 10 test slightly more complex natural language constructs. Task 9 tests one of the simplest forms of negation, that of supporting facts that imply a statement is false e.g. “Fred is no longer in the ofﬁce” rather than “Fred travelled to the ofﬁce”. (In this case, task 6 (yes/no questions) is a prerequisite to the task.) Task 10 tests if we can model statements that describe possibilities rather than certainties, e.g. “John is either in the classroom or the playground.”, where in that case the answer is “maybe” to the question “Is John in the classroom?”.  Basic Coreference, Conjunctions and Compound Coreference Task 11 tests the simplest type of coreference, that of detecting the nearest referent, e.g. “Daniel was in the kitchen. Then he went to the studio.”. Real-world data typically addresses this as a labeling problem and studies more sophisticated phenomena (Soon et al., 2001), whereas we evaluate it as in all our other tasks as a question answering problem. Task 12 (conjunctions) tests referring to multiple subjects in a single statement, e.g. “Mary and Jeff went to the kitchen.”. Task 13 tests coreference in the case where the pronoun can refer to multiple actors, e.g. “Daniel and Sandra journeyed to the ofﬁce. Then they went to the garden”.  Time Reasoning While our tasks so far have included time implicitly in the order of the state- ments, task 14 tests understanding the use of time expressions within the statements, e.g. “In the afternoon Julie went to the park. Yesterday Julie was at school.”, followed by questions about the order of events such as “Where was Julie before the park?”. Real-world datasets address the task of evaluating time expressions typically as a labeling, rather than a QA task, see e.g. UzZaman et al. (2012).  Basic Deduction and Induction Task 15 tests basic deduction via inheritance of properties, e.g. “Sheep are afraid of wolves. Gertrude is a sheep. What is Gertrude afraid of?”. Task 16 similarly  4  Under review as a conference paper at ICLR 2016  Table 2: Sample statements and questions from tasks 11 to 20.  Task 11: Basic Coreference Daniel was in the kitchen. Then he went to the studio. Sandra was in the ofﬁce. Where is Daniel? A:studio  Task 13: Compound Coreference Daniel and Sandra journeyed to the ofﬁce. Then they went to the garden. Sandra and John travelled to the kitchen. After that they moved to the hallway. Where is Daniel? A: garden  Task 15: Basic Deduction Sheep are afraid of wolves. Cats are afraid of dogs. Mice are afraid of cats. Gertrude is a sheep. What is Gertrude afraid of? A:wolves  Task 12: Conjunction Mary and Jeff went to the kitchen. Then Jeff went to the park. Where is Mary? A: kitchen Where is Jeff? A: park  Task 14: Time Reasoning  In the afternoon Julie went to the park. Yesterday Julie was at school. Julie went to the cinema this evening. Where did Julie go after the park? A:cinema Where was Julie before the park? A:school  Task 16: Basic Induction Lily is a swan. Lily is white. Bernhard is green. Greg is a swan. What color is Greg? A:white  Task 17: Positional Reasoning The triangle is to the right of the blue square. The red square is on top of the blue square. The red sphere is to the right of the blue square. Is the red sphere to the right of the blue square? A:yes Is the red square to the left of the triangle? A:yes  Task 18: Size Reasoning The football ﬁts in the suitcase. The suitcase ﬁts in the cupboard. The box is smaller than the football. Will the box ﬁt in the suitcase? A:yes Will the cupboard ﬁt in the box? A:no  Task 19: Path Finding The kitchen is north of the hallway. The bathroom is west of the bedroom. The den is east of the hallway. The ofﬁce is south of the bedroom. How do you go from den to kitchen? A: west, north How do you go from ofﬁce to bathroom? A: north, west  Task 20: Agent’s Motivations John is hungry. John goes to the kitchen. John grabbed the apple there. Daniel is hungry. Where does Daniel go? A:kitchen Why did John go to the kitchen? A:hungry  tests basic induction via inheritance of properties. A full analysis of induction and deduction is clearly beyond the scope of this work, and future tasks should analyse further, deeper aspects.  Positional and Size Reasoning Task 17 tests spatial reasoning, one of many components of the classical SHRDLU system (Winograd, 1972) by asking questions about the relative positions of colored blocks. Task 18 requires reasoning about the relative size of objects and is inspired by the commonsense reasoning examples in the Winograd schema challenge (Levesque et al., 2011).  Path Finding The goal of task 19 is to ﬁnd the path between locations: given the description of various locations, it asks: how do you get from one to another? This is related to the work of Chen & Mooney (2011) and effectively involves a search problem.  Agent’s Motivations Finally, task 20 questions, in the simplest way possible, why an agent per- forms an action. It addresses the case of actors being in a given state (hungry, thirsty, tired, . . . ) and the actions they then take, e.g. it should learn that hungry people might go to the kitchen, and so on.  As already stated, these tasks are meant to foster the development and understanding of machine learning algorithms. A single model should be evaluated across all the tasks (not tuning per task) and then the same model should be tested on additional real-world tasks.  In our data release, in addition to providing the above 20 tasks in English, we also provide them (i) in Hindi; and (ii) with shufﬂed English words so they are no longer readable by humans. A good learning algorithm should perform similarly on all three, which would likely not be the case for a method using external resources, a setting intended to mimic a learner being ﬁrst presented a language and having to learn from scratch.  5  Under review as a conference paper at ICLR 2016  4 SIMULATION  All our tasks are generated with a simulation which behaves like a classic text adventure game. The idea is that generating text within this simulation allows us to ground the language used into a coherent and controlled (artiﬁcial) world. Our simulation follows those of Bordes et al. (2010); Weston et al. (2014) but is somewhat more complex.  The simulated world is composed of entities of various types (locations, objects, persons. etc.) and of various actions that operate on these entities. Entities have internal states: their location, whether they carry objects on top or inside them (e.g., tables and boxes), the mental state of actors (e.g. hungry), as well as properties such as size, color, and edibility. For locations, the nearby places that are connected (e.g. what lies to the east, or above) are encoded. For actors, a set of pre-speciﬁed rules per actor can also be speciﬁed to control their behavior, e.g. if they are hungry they may try to ﬁnd food. Random valid actions can also be executed if no rule is set, e.g. walking around randomly.  The actions an actor can execute in the simulation consist of the following: go <location>, get <object>, get <object1> from <object2>, put <object1> in/on <object2>, give <object> to <actor>, drop <object>, set <entitity> <state>, look, inventory and examine <object>. A set of universal constraints is imposed on those actions to enforce coherence in the simulation. For example an actor cannot get something that they or someone else already has, they cannot go to a place that is not connected to the current location, cannot drop something they do not already have, and so on. Using the underlying actions, rules for actors, and their constraints, deﬁnes how actors act. For each task we limit the actions needed for that task, e.g. task 1 only needs go whereas task 2 uses go, get and drop. If we write the commands down this gives us a very simple “story” which is executable by the simulation, e.g., joe go playground; bob go ofﬁce; joe get football. This example corresponds to task 2. The system can then ask questions about the state of the simulation e.g., where john?, where football? and so on. It is easy to calculate the true answers for these questions as we have access to the underlying world.  To produce more natural looking text with lexical variety from statements and questions we employ a simple automated grammar. Each verb is assigned a set of synonyms, e.g., the simulation command get is replaced with either picked up, got, grabbed or took, and drop is replaced with either dropped, left, discarded or put down. Similarly, each object and actor can have a set of replacement synonyms as well, e.g. replacing Daniel with he in task 11. Adverbs are crucial for some tasks such as the time reasoning task 14.  There are a great many aspects of language not yet modeled. For example, all sentences are so far relatively short and contain little nesting. Further, the entities and the vocabulary size is small (150 words, and typically 4 actors, 6 locations and 3 objects used per task). The hope is that deﬁning a set of well deﬁned tasks will help evaluate models in a controlled way within the simulated environment, which is hard to do with real data. That is, these tasks are not a substitute for real data, but should complement them, especially when developing and analysing algorithms.  5 EXPERIMENTS  We compared the following methods on our tasks (on the English dataset): (i) an N - gram classiﬁer baseline, (ii) LSTMs (long short term memory Recurrent Neural Networks) (Hochreiter & Schmidhuber, 1997), (iii) Memory Networks (MemNNs) (Weston et al., 2014), (iv) some extensions of Memory Networks we will detail; and (v) a structured SVM that incorporates external labeled data from existing NLP tasks. These models belong to three separate tracks. Weakly supervised models are only given question answer pairs at training time, whereas strong supervision provides the set of supporting facts at training time (but not testing time) as well. Strongly super- vised ones give accuracy upper bounds for weakly supervised models, i.e. the performance should be superior given the same model class. Methods in the last external resources track can use labeled data from other sources rather than just the training set provided, e.g. coreference and semantic role labeling tasks, as well as strong supervision. For each task we use 1000 questions for training, and 1000 for testing, and report the test accuracy. We consider a task successfully passed if ≥ 95% accuracy is obtained3.  3The choice of 95% (and 1000 training examples) is arbitrary.  6  Under review as a conference paper at ICLR 2016  Table 3: Test accuracy (%) on our 20 Tasks for various methods (1000 training examples each). Our proposed extensions to MemNNs are in columns 5-9: with adaptive memory (AM), N-grams (NG), nonlinear matching function (NL), and combinations thereof. Bold numbers indicate tasks where our extensions achieve ≥ 95% accuracy but the original MemNN model of Weston et al. (2014) did not. The last two columns (10-11) give extra analysis of the MemNN method. Column 10 gives the amount of training data for each task needed to AM + NG + NL obtain ≥ 95% accuracy, or FAIL if this is not achievable with 1000 training examples. The ﬁnal column gives the accuracy when training on all data at once, rather than separately.  Weakly  Supervised  Uses External  Resources  Strong Supervision  (using supporting facts)  M SV features Structured OREF+SRL  C  N-gram Classiﬁer  LSTM  36 2 7 50 20 49 52 40 62 45 29 9 26 19 20 43 46 52 0 76 34  50 20 20 61 70 48 49 45 64 44 72 74 94 27 21 23 51 52 8 91 49  99 74 17 98 83 99 69 70 100 99 100 96 99 99 96 24 61 62 49 95 79  (2014) N N Mem al. et Weston 100 100 20 71 83 47 68 77 65 59 100 100 100 99 74 27 54 57 0 100 75  ORY N N MEM Mem ADAPTIVE  100 100 100 69 83 52 78 90 71 57 100 100 100 100 73 100 46 50 9 100 79  TASK  1 - Single Supporting Fact 2 - Two Supporting Facts 3 - Three Supporting Facts 4 - Two Arg. Relations 5 - Three Arg. Relations 6 - Yes/No Questions 7 - Counting 8 - Lists/Sets 9 - Simple Negation 10 - Indeﬁnite Knowledge 11 - Basic Coreference 12 - Conjunction 13 - Compound Coref. 14 - Time Reasoning 15 - Basic Deduction 16 - Basic Induction 17 - Positional Reasoning 18 - Size Reasoning 19 - Path Finding 20 - Agent’s Motivations Mean Performance  N N  N-GRAMS  Mem  + AM 100 100 99 100 86 53 86 88 63 54 100 100 100 99 100 100 49 74 3 100 83  NLINEAR  Mem  N N O N + AM 100 100 100 73 86 100 83 94 100 97 100 100 100 100 77 100 57 54 15 100 87  95 ≥ req. ex. of No.  250 ex. 500 ex. 500 ex. 500 ex. 1000 ex. 500 ex. FAIL FAIL 500 ex. 1000 ex. 250 ex. 250 ex. 250 ex. 500 ex. 100 ex. 100 ex. FAIL 1000 ex. FAIL 250 ex. 100  Mem  N NL N + G N + AM 100 100 100 100 98 100 85 91 100 98 100 100 100 99 100 100 65 95 36 100 93  Training MultiTask  100 100 98 80 99 100 86 93 100 98 100 100 100 99 100 94 72 93 19 100 92  Methods The N -gram classiﬁer baseline is inspired by the baselines in Richardson et al. (2013) but applied to the case of producing a 1-word answer rather than a multiple choice question: we construct a bag-of-N -grams for all sentences in the story that share at least one word with the question, and then learn a linear classiﬁer to predict the answer using those features4. LSTMs are a popular method for sequence prediction (Sutskever et al., 2014) and outperform stan- dard RNNs (Recurrent Neural Networks) for tasks similar to ours (Weston et al., 2014). They work by reading the story until the point they reach a question and then have to output an answer. Note that they are weakly supervised by answers only, and are hence at a disadvantage compared to strongly supervised methods or methods that use external resources.  MemNNs (Weston et al., 2014) are a recently proposed class of models that have been shown to perform well at QA. They work by a “controller” neural network performing inference over the stored memories that consist of the previous statements in the story. The original proposed model performs 2 hops of inference: ﬁnding the ﬁrst supporting fact with the maximum match score with the question, and then the second supporting fact with the maximum match score with both the question and the ﬁrst fact that was found. The matching function consists of mapping the bag-of- words for the question and facts into an embedding space by summing word embeddings. The word embeddings are learnt using strong supervision to optimize the QA task. After ﬁnding supporting facts, a ﬁnal ranking is performed to rank possible responses (answer words) given those facts. We also consider some extensions to this model:  • Adaptive memories performing a variable number of hops rather than 2, the model is trained to predict a hop or the special “STOP” class. A similar procedure can be applied to output multiple tokens as well.  4Constructing N-grams from all sentences rather than using the ﬁltered set gave worse results.  7  Under review as a conference paper at ICLR 2016  • N-grams We tried using a bag of 3-grams rather than a bag-of-words to represent the text.  In both cases the ﬁrst step of the MemNN is to convert these into vectorial embeddings.  • Nonlinearity We apply a classical 2-layer neural network with tanh nonlinearity in the  matching function.  More details of these variants is given in Sec A of the appendix.  Finally, we built a classical cascade NLP system baseline using a structured support vector ma- chine (SVM), which incorporates coreference resolution and semantic role labeling (SRL) prepro- cessing steps, which are themselves trained on large amounts of costly labeled data. The Stanford coreference system (Raghunathan et al., 2010) and the SENNA semantic role labeling (SRL) system (Collobert et al., 2011) are used to build features for the input to the SVM, trained with strong super- vision to ﬁnd the supporting facts, e.g. features based on words, word pairs, and the SRL verb and verb-argument pairs. After ﬁnding the supporting facts, we build a similar structured SVM for the response stage, with features tuned for that goal as well. More details are in Sec. B of the appendix.  Learning rates and other hyperparameters for all methods are chosen using the training set. The summary of our experimental results on the tasks is given in Table 3. We give results for each of the 20 tasks separately, as well as mean performance and number of failed tasks in the ﬁnal two rows.  Results Standard MemNNs generally outperform the N -gram and LSTM baselines, which is con- sistent with the results in Weston et al. (2014). However they still “fail” at a number of tasks; that is, test accuracy is less than 95%. Some of these failures are expected due to insufﬁcient modeling power as described in more detail in Sec. A.1, e.g. k = 2 facts, single word answers and bag-of- words do not succeed on tasks 3, 4, 5, 7, 8 and 18. However, there were also failures on tasks we did not at ﬁrst expect, for example yes/no questions (6) and indeﬁnite knowledge (10). Given hindsight, we realize that the linear scoring function of standard MemNNs cannot model the match between query, supporting fact and a yes/no answer as this requires three-way interactions.  Columns 5-9 of Table 3 give the results for our MemNN extensions: adaptive memories (AM), N -grams (NG) and nonlinearities (NL), plus combinations thereof. The adaptive approach gives a straight-forward improvement in tasks 3 and 16 because they both require more than two supporting facts, and also gives (small) improvements in 8 and 19 because they require multi-word outputs (but still remain difﬁcult). We hence use the AM model in combination with all our other extensions in the subsequent experiments.  MemNNs with N -gram modeling yield clear improvements when word order matters, e.g. tasks 4 and 15. However, N -grams do not seem to be a substitute for nonlinearities in the embedding function as the NL model outperforms N -grams on average, especially in the yes/no (6) and indef- inite tasks (10), as explained before. On the other hand, the NL method cannot model word order and so fails e.g., on task 4. The obvious step is thus to combine these complimentary approaches: indeed AM+NG+NL (column 9) gives improved results over both, with a total of 9 tasks that have been upgraded from failure to success compared to the original MemNN model.  The structured SVM, despite having access to external resources, does not perform better, still fail- ing at 9 tasks. It does perform better than vanilla MemNNs (without extensions) on tasks 6, 9 and 10 where the hand-built feature conjunctions capture the necessary nonlinearities. However, com- pared to MemNN (AM+NG+NL) it seems to do signiﬁcantly worse on tasks requiring three (and sometimes, two) supporting facts (e.g. tasks 3, 16 and 2) presumably as ranking over so many possi- bilities introduces more mistakes. However, its non-greedy search does seem to help on other tasks, such as path ﬁnding (task 19) where search is very important. Since it relies on external resources speciﬁcally designed for English, it is unsure that it would perform as well on other languages, like Hindi, where such external resources might be of worse quality.  The ﬁnal two columns (10-11) give further analysis of the AM+NG+NL MemNN method. The second to last column (10) shows the minimum number of training examples required to achieve ≥ 95% accuracy, or FAIL if this is not achieved with 1000 examples. This is important as it is not only desirable to perform well on a task, but also using the fewest number of examples (to generalize well, quickly). Most succeeding tasks require 100-500 examples. Task 8 requires 5000 examples and 7 requires 10000, hence they are labeled as FAIL. The latter task can presumably be solved by adding all the times an object is picked up, and subtracting the times it is dropped, which seems  8  Under review as a conference paper at ICLR 2016  possible for an MemNN, but it does not do perfectly. Two tasks, positional reasoning 17 and path ﬁnding 19 cannot be solved even with 10000 examples, it seems those (and indeed more advanced forms of induction and deduction, which we plan to build) require a general search algorithm to be built into the inference procedure, which MemNN (and the other approaches tried) are lacking.  The last column shows the performance of AM+NG+NL MemNNs when training on all the tasks jointly, rather than just on a single one. The performance is generally encouragingly similar, showing such a model can learn many aspects of text understanding and reasoning simultaneously. The main issues are that these models still fail on several of the tasks, and use a far stronger form of supervision (using supporting facts) than is typically realistic.  6 DISCUSSION  A prerequisite set We developed a set of tasks that we believe are a prerequisite to full language understanding and reasoning. While any learner that can solve these tasks is not necessarily close to full reasoning, if a learner fails on any of our tasks then there are likely real-world tasks that it will fail on too (i.e., real-world tasks that require the same kind of reasoning). Even if the situations and the language of the tasks are artiﬁcial, we believe that the mechanisms required to learn how to solve them are part of the key towards text understanding and reasoning.  A ﬂexible framework This set of tasks is not a deﬁnitive set. The purpose of a simulation-based approach is to provide ﬂexibility and control of the tasks’ construction. We grounded the tasks into language because it is then easier to understand the usefulness of the tasks and to interpret their results. However, our primary goal is to ﬁnd models able to learn to detect and combine patterns in symbolic sequences. One might even want to decrease the intrinsic difﬁculty by removing any lexical variability and ambiguity and reason only over bare symbols, stripped down from their lin- guistic meaning. One could also decorrelate the long-term memory from the reasoning capabilities of systems by, for instance, arranging the supporting facts closer to the questions. In the opposing view, one could instead want to transform the tasks into more realistic stories using annotators or more complex grammars. The set of 20 tasks presented here is a subset of what can be achieved with a simulation. We chose them because they offer a variety of skills that we would like a text reasoning model to have, but we hope researchers from the community will develop more tasks of varying complexity in order to develop and analyze models that try to solve them. Transfer learning across tasks is also a very important goal, beyond the scope of this paper. We have thus made the simulator and code for the tasks publicly available for those purposes.  Testing learning methods Our tasks are designed as a test-bed for learning methods: we provide training and test sets because we intend to evaluate the capability of models to discover how to reason from patterns hidden within them. It could be tempting to hand-code solutions for them or to use existing large-scale QA systems like Cyc (Curtis et al., 2005). They might succeed at solving them, even if our structured SVM results (a cascaded NLP system with hand-built features) show that this is not straightforward; however this is not the tasks’ purpose since those approaches would not be learning to solve them. Our experiments show that some existing machine learning methods are successful on some of the tasks, in particular Memory Networks, for which we introduced some useful extensions (in Sec. A). However, those models still fail on several of the tasks, and use a far stronger form of supervision (using supporting facts) than is typically realistic.  These datasets are not yet solved. Future research should aim to minimize the amount of required supervision, as well as the number of training examples needed to solve a new task, to move closer to the task transfer capabilities of humans. That is, in the weakly supervised case with only 1000 training examples or less there is no known general (i.e. non-hand engineered) method that solves the tasks. Further, importantly, our hope is that a feedback loop of developing more challenging tasks, and then algorithms that can solve them, leads us to fruitful research directions.  Note that these tasks are not a substitute for real data, but should complement them, especially when developing and analysing algorithms. There are many complementary real-world datasets, see for example Hermann et al. (2015); Bordes et al. (2015); Hill et al. (2015). That is, even if a method works well on our 20 tasks, it should be shown to be useful on real data as well.  9  Under review as a conference paper at ICLR 2016  Impact Since being online, the bAbI tasks have already directly inﬂuenced the development of several promising new algorithms, including weakly supervised end-to-end Memory Networks (MemN2N) of Sukhbaatar et al. (2015), Dynamic Memory Networks of Kumar et al. (2015), and the Neural Reasoner (Peng et al., 2015). MemN2N has since been shown to perform well on some real-world tasks (Hill et al., 2015).  REFERENCES Bache, K.  and Lichman, M.  UCI machine  learning repository,  2013.  URL  http://archive.ics.uci.edu/ml.  Berant, Jonathan, Chou, Andrew, Frostig, Roy, and Liang, Percy. Semantic parsing on freebase from  question-answer pairs. In EMNLP, pp. 1533–1544, 2013.  Berant, Jonathan, Srikumar, Vivek, Chen, Pei-Chun, Huang, Brad, Manning, Christopher D, Van- der Linden, Abby, Harding, Brittany, and Clark, Peter. Modeling biological processes for reading comprehension. In Proc. EMNLP, 2014.  Bordes, Antoine, Usunier, Nicolas, Collobert, Ronan, and Weston, Jason. Towards understanding  situated natural language. In AISTATS, 2010.  Bordes, Antoine, Usunier, Nicolas, Chopra, Sumit, and Weston, Jason. Large-scale simple question  answering with memory networks. arXiv preprint arXiv:1506.02075, 2015.  Chen, David L and Mooney, Raymond J. Learning to interpret natural language navigation instruc-  tions from observations. San Francisco, CA, pp. 859–865, 2011.  Collobert, Ronan, Weston, Jason, Bottou, L´eon, Karlen, Michael, Kavukcuoglu, Koray, and Kuksa, Pavel. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537, 2011.  Curtis, Jon, Matthews, Gavin, and Baxter, David. On the effective use of cyc in a question answering system. In IJCAI Workshop on Knowledge and Reasoning for Answering Questions, pp. 61–70, 2005.  Fader, Anthony, Zettlemoyer, Luke, and Etzioni, Oren. Paraphrase-driven learning for open question  answering. In ACL, pp. 1608–1618, 2013.  Fader, Anthony, Zettlemoyer, Luke, and Etzioni, Oren. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1156–1165. ACM, 2014.  Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural turing machines.  arXiv:1410.5401, 2014.  arXiv preprint  Halevy, Alon, Norvig, Peter, and Pereira, Fernando. The unreasonable effectiveness of data. Intelli-  gent Systems, IEEE, 24(2):8–12, 2009.  Hermann, Karl Moritz, Koˇcisk´y, Tom´aˇs, Grefenstette, Edward, Espeholt, Lasse, Kay, Will, Teaching machines to read and compre- URL  In Advances in Neural Information Processing Systems (NIPS), 2015.  Suleyman, Mustafa, and Blunsom, Phil. hend. http://arxiv.org/abs/1506.03340.  Hill, Felix, Bordes, Antoine, Chopra, Sumit, and Weston, Jason. The goldilocks principle: Reading children’s books with explicit memory representation s. arXiv preprint arXiv:1511.02301, 2015.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Kumar, Ankit, Irsoy, Ozan, Su, Jonathan, Bradbury, James, English, Robert, Pierce, ˜Brian, On- druska, Peter, Gulrajani, Ishaan, and Socher, Richard. Ask me anything: Dynamic memory net- works for natural language processing. http://arxiv.org/abs/1506.07285, 2015.  10  Under review as a conference paper at ICLR 2016  Levesque, Hector J, Davis, Ernest, and Morgenstern, Leora. The winograd schema challenge. In  AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, 2011.  Liang, Percy. Lambda dependency-based compositional semantics. arXiv preprint arXiv:1309.4408,  2013.  Liang, Percy, Jordan, Michael I, and Klein, Dan. Learning dependency-based compositional seman-  tics. Computational Linguistics, 39(2):389–446, 2013.  Minsky, Marvin and Papert, Seymour. Perceptron: an introduction to computational geometry. The  MIT Press, Cambridge, expanded edition, 19:88, 1969.  Montfort, Nick. Twisty Little Passages: an approach to interactive ﬁction. Mit Press, 2005.  M¨uller, K-R, Smola, Alex J, R¨atsch, Gunnar, Sch¨olkopf, Bernhard, Kohlmorgen, Jens, and Vapnik, Vladimir. Predicting time series with support vector machines. In Artiﬁcial Neural NetworksI- CANN’97, pp. 999–1004. Springer, 1997.  Ng, Andrew Y, Jordan, Michael I, Weiss, Yair, et al. On spectral clustering: Analysis and an algo-  rithm. Advances in neural information processing systems, 2:849–856, 2002.  Peng, Baolin, Lu, Zhengdong, Li, Hang, and Wong, Kam-Fai. Towards neural network-based rea-  soning. arXiv preprint arXiv:1508.05508, 2015.  Raghunathan, Karthik, Lee, Heeyoung, Rangarajan, Sudarshan, Chambers, Nathanael, Surdeanu, Mihai, Jurafsky, Dan, and Manning, Christopher. A multi-pass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 492–501. Association for Computational Linguistics, 2010.  Richardson, Matthew, Burges, Christopher JC, and Renshaw, Erin. Mctest: A challenge dataset for  the open-domain machine comprehension of text. In EMNLP, pp. 193–203, 2013.  Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning internal representations  by error propagation. Technical report, DTIC Document, 1985.  Soon, Wee Meng, Ng, Hwee Tou, and Lim, Daniel Chung Yong. A machine learning approach to  coreference resolution of noun phrases. Computational linguistics, 27(4):521–544, 2001.  Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and Fergus, Rob. End-to-end memory net-  works. Proceedings of NIPS, 2015.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural net-  works. In Advances in Neural Information Processing Systems, pp. 3104–3112, 2014.  UzZaman, Naushad, Llorens, Hector, Allen, James, Derczynski, Leon, Verhagen, Marc, and Puste- jovsky, James. Tempeval-3: Evaluating events, time expressions, and temporal relations. arXiv preprint arXiv:1206.5333, 2012.  Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Memory networks. CoRR, abs/1410.3916,  2014.  Winograd, Terry. Understanding natural language. Cognitive psychology, 3(1):1–191, 1972.  Yao, Xuchen, Berant, Jonathan, and Van Durme, Benjamin. Freebase qa: Information extraction or  semantic parsing? ACL 2014, pp. 82, 2014.  Yu, Mo, Gormley, Matthew R, and Dredze, Mark. Factor-based compositional embedding models.  NIPS 2014 workshop on Learning Semantics, 2014.  Zhu, Xiaojin, Ghahramani, Zoubin, Lafferty, John, et al. Semi-supervised learning using gaussian  ﬁelds and harmonic functions. In ICML, volume 3, pp. 912–919, 2003.  11  Under review as a conference paper at ICLR 2016  A EXTENSIONS TO MEMORY NETWORKS  Memory Networks Weston et al. (2014) are a promising class of models, shown to perform well at QA, that we can apply to our tasks. They consist of a memory m (an array of objects indexed by mi) and four potentially learnable components I, G, O and R that are executed given an input:  I: (input feature map) – convert input sentence x to an internal feature representation I(x). G: (generalization) – update the current memory state m given the new input: mi =  G(mi, I(x), m), ∀i.  O: (output feature map) – compute output o given the new input and the memory: o =  O(I(x), m).  R: (response) – ﬁnally, decode output features o to give the ﬁnal textual response to the user:  r = R(o).  Potentially, component I can make use of standard pre-processing, e.g., parsing and entity resolu- tion, but the simplest form is to do no processing at all. The simplest form of G is store the new incoming example in an empty memory slot, and leave the rest of the memory untouched. Thus, in Weston et al. (2014) the actual implementation used is exactly this simple form, where the bulk of the work is in the O and R components. The former is responsible for reading from memory and performing inference, e.g., calculating what are the relevant memories to answer a question, and the latter for producing the actual wording of the answer given O. The O module produces output features by ﬁnding k supporting memories given x. They use k = 2. For k = 1 the highest scoring supporting memory is retrieved with:  o1 = O1(x, m) = arg max i=1,...,N  sO(x, mi)  (1)  where sO is a function that scores the match between the pair of sentences x and mi. For the case k = 2 they then ﬁnd a second supporting memory given the ﬁrst found in the previous iteration:  o2 = O2(q, m) = arg max i=1,...,N  sO([x, mo1], mi)  (2)  where the candidate supporting memory mi is now scored with respect to both the original input and the ﬁrst supporting memory, where square brackets denote a list. The ﬁnal output o is [x, mo1, mo2], which is input to the module R. Finally, R needs to produce a textual response r. While the authors also consider Recurrent Neural Networks (RNNs), their standard setup limits responses to be a single word (out of all the words seen by the model) by ranking them:  r = R(q, w) = argmaxw∈W sR([x, mo1, mo2], w)  where W is the set of all words in the dictionary, and sR is a function that scores the match. The scoring functions sO and sR have the same form, that of an embedding model:  s(x, y) = Φx(x)⊤U ⊤U Φy(y).  (3)  (4)  where U is a n × D matrix where D is the number of features and n is the embedding dimension. The role of Φx and Φy is to map the original text to the D-dimensional feature space. They choose a bag of words representation, and D = 3|W | for sO, i.e., every word in the dictionary has three different representations: one for Φy(.) and two for Φx(.) depending on whether the words of the input arguments are from the actual input x or from the supporting memories so that they can be modeled differently.  They consider various extensions of their model, in particular modeling write time and modeling unseen words. Here we only discuss the former which we also use. In order for the model to work on QA tasks over stories it needs to know which order the sentences were uttered which is not available in the model directly. They thus add extra write time extra features to SO which take on the value 0 or 1 indicating which sentence is older than another being compared, and compare triples of pairs of sentences and the question itself. Training is carried out by stochastic gradient descent using supervision from both the question answer pairs and the supporting memories (to select o1 and o2). See Weston et al. (2014) for more details.  12  Under review as a conference paper at ICLR 2016  A.1 SHORTCOMINGS OF THE EXISTING MEMNNS  The Memory Networks models deﬁned in (Weston et al., 2014) are one possible technique to try on our tasks, however there are several tasks which they are likely to fail on:  • They model sentences with a bag of words so are likely to fail on tasks such as the 2-  argument (task 4) and 3-argument (task 5) relation problems.  • They perform only two max operations (k = 2) so they cannot handle questions involving  more than two supporting facts such as tasks 3 and 7.  • Unless a RNN is employed in the R module, they are unable to provide multiple answers in the standard setting using eq. (3). This is required for the list (8) and path ﬁnding (19) tasks.  We therefore propose improvements to their model in the following section.  A.2  IMPROVING MEMORY NETWORKS  A.2.1 ADAPTIVE MEMORIES (AND RESPONSES)  We consider a variable number of supporting facts that is automatically adapted dependent on the question being asked. To do this we consider scoring a special fact m∅. Computation of supporting memories then becomes:  i = 1 oi = O(x, m) while oi 6= m∅ do  i ← i + 1 oi = O([x, mo1 , . . . , moi−1 ], m)  end while  That is, we keep predicting supporting facts i, conditioning at each step on the previously found facts, until m∅ is predicted at which point we stop. m∅ has its own unique embedding vector, which is also learned. In practice we still impose a hard maximum number of loops in our experiments to avoid fail cases where the computation never stops (in our experiments we use a limit of 10).  Multiple Answers We use a similar trick for the response module as well in order to output multi- ple words. That is, we add a special word w∅ to the dictionary and predict word wi on each iteration i conditional on the previous words, i.e., wi = R([x, mo1 , . . . , m|o|, wi, . . . , wi−1], w), until we predict w∅.  A.2.2 NONLINEAR SENTENCE MODELING  There are several ways of modeling sentences that go beyond a bag-of-words, and we explore three variants here. The simplest is a bag-of-N-grams, we consider N = 1, 2 and 3 in the bag. The main disadvantage of such a method is that the dictionary grows rapidly with N . We therefore consider an alternative neural network approach, which we call a multilinear map. Each word in a sentence is binned into one of Psz positions with p(i, l) = ⌈(iPsz)/l)⌉ where i is the position of the word in a sentence of length l, and for each position we employ a n × n matrix Pp(i,l). We then model the matching score with:  s(q, d) = E(q) · E(d); E(x) = tanh( X  Pp(i,l)Φx(xi)⊤U )  (5)  i=1,...,l  whereby we apply a linear map for each word dependent on its position, followed by a tanh non- linearity on the sum of mappings. Note that this is related to the model of (Yu et al., 2014) who consider tags rather than positions. While the results of this method are not shown in the main paper due to space restrictions, it performs similarly well to N -grams to and may be useful in real-world cases where N -grams cause the dictionary to be too large. Comparing to Table 3 MemNN with adaptive memories (AM) + multilinear obtains a mean performance of 93, the same as MemNNs with AM+NG+NL (i.e., using N-grams instead).  13  Under review as a conference paper at ICLR 2016  Finally, to assess the performance of nonlinear maps that do not model word position at all we also consider the following nonlinear embedding:  (6) where W is a n × n matrix. This is similar to a classical two-layer neural network, but applied to both sides q and d of s(q, d). We also consider the straight-forward combination of bag-of-N -grams followed by this nonlinearity.  E(x) = tanh(W tanh(Φx(x)⊤U )).  B BASELINE USING EXTERNAL RESOURCES  We also built a classical cascade NLP system baseline using a structured SVM, which incorpo- rates coreference resolution and semantic role labeling preprocessing steps, which are themselves trained on large amounts of costly labeled data. We ﬁrst run the Stanford coreference system (Raghunathan et al., 2010) on the stories and each mention is then replaced with the ﬁrst mention of its entity class. Second, the SENNA semantic role labeling system (SRL) (Collobert et al., 2011) is run, and we collect the set of arguments for each verb. We then deﬁne a ranking task for ﬁnding the supporting facts (trained using strong supervision):  o1, o2, o3 = arg max  o∈O  SO(x, fo1, fo2, fo3; Θ)  where given the question x we ﬁnd at most three supporting facts with indices oi from the set of facts f in the story (we also consider selecting an “empty fact” for the case of less than three), and SO is a linear scoring function with parameters Θ. Computing the argmax requires doing exhaustive search, unlike e.g. the MemNN method which is greedy. For scalability, we thus prune the set of possible matches by requiring that facts share one common non-determiner word with each other match or with x. SO is constructed as a set of indicator features. For simplicity each of the features only looks at pairs of sentences, i.e. SO(x, fo1, fo2, fo3; Θ) = Θ ∗ (g(x, fo1 ), g(x, fo2), g(x, fo3 ), g(fo1, fo2), g(fo2, fo3), g(fo1, fo3)). The feature function g is made up of the following feature types, shown here for g(fo1, fo2): (1) Word pairs: One indicator variable for each pair of words in fo1 and fo2. (2) Pair distance: Indicator for the distance between the sentence, i.e. o1 − o2. (3) Pair order: Indicator for the order of the sentence, i.e. o1 > o2. (4) SRL Verb Pair: Indicator variables for each pair of SRL verbs in fo1 and fo2. (5) SRL Verb-Arg Pair: Indicator variables for each pair of SRL arguments in fo1, fo2 and their corresponding verbs. After ﬁnding the supporting facts, we build a similar structured SVM for the response stage, also with features tuned for that goal: Words – indicator for each word in x, Word Pairs – indicator for each pair of words in x and supporting facts, and similar SRL Verb and SRL Verb-Arg Pair features as before.  Results are given in Table 3. The structured SVM, despite having access to external resources, does not perform better than MemNNs overall, still failing at 9 tasks. It does perform well on tasks 6, 9 and 10where the hand-built feature conjunctions capture the necessary nonlinearities that the original MemNNs do not. However, it seems to do signiﬁcantly worse on tasks requiring three (and sometimes, two) supporting facts (e.g. tasks 3, 16 and 2) presumably as ranking over so many possibilities introduces more mistakes. However, its non-greedy search does seem to help on other tasks, such as path ﬁnding (task 19) where search is very important.  14  ",
1511.06931,2016,Evaluating Prerequisite Qualities for Learning End-to-end Dialog Systems,"['Evaluating Prerequisite Qualities for Learning End-to-end Dialog Systems [data]\nJesse Dodge', 'Andreea Gane', 'Xiang Zhang', 'Antoine Bordes', 'Sumit Chopra', 'Alexander Miller', 'Arthur Szlam', 'Jason Weston']",https://arxiv.org/pdf/1511.06931,"6 1 0 2    r p A 9 1         ] L C . s c [      6 v 1 3 9 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  EVALUATING PREREQUISITE QUALITIES FOR LEARN- ING END-TO-END DIALOG SYSTEMS  Jesse Dodge∗, Andreea Gane∗, Xiang Zhang∗, Antoine Bordes, Sumit Chopra, Alexander H. Miller, Arthur Szlam & Jason Weston Facebook AI Research 770 Broadway New York, USA {jessedodge,agane,xiangz,abordes,spchopra,ahm,aszlam,jase}@fb.com  ABSTRACT  A long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks (Weston et al., 2015b) which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and ﬁnally to per- form on natural dialogs from Reddit. We provide a dataset covering ∼75k movie entities and with ∼3.5M training examples. We present results of various models on these tasks, and evaluate their performance.  1  INTRODUCTION  With the recent employment of Recurrent Neural Networks (RNNs) and the large quantities of con- versational data available on websites like Twitter or Reddit, a new type of dialog system is emerg- ing. Such end-to-end dialog systems (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.g. in Henderson (2015)). These methods are trained to imitate user-user conversations and do not need any hand-coding of attributes and labels for dia- log states and goals like state tracking methods do. Being trained on large corpora, they are robust to many language variations and seem to mimic human conversations to some extent.  In spite of their ﬂexibility and representational power, these neural network based methods lack pertinent goal-oriented frameworks to validate their performance. Indeed, traditional systems have a wide range of well deﬁned evaluation paradigms and benchmarks that measure their ability to track user states and/or to reach user-deﬁned goals (Walker et al., 1997; Paek, 2001; Griol et al., 2008; Williams et al., 2013). Recent end-to-end models, on the other hand, rely either on very few human scores (Vinyals & Le, 2015), crowdsourcing (Ritter et al., 2011; Shang et al., 2015) or machine translation metrics like BLEU (Sordoni et al., 2015) to judge the quality of the generated language only. This is problematic because these evaluations do not assess if end-to-end systems can conduct dialog to achieve pre-deﬁned objectives, but simply whether they can generate correct language that could ﬁt in the context of the dialog; in other words, they quantify their chit-chatting abilities.  ∗The ﬁrst three authors contributed equally.  1  Published as a conference paper at ICLR 2016  To ﬁll in this gap, this paper proposes a collection of four tasks designed to evaluate different pre- requisite qualities of end-to-end dialog systems. Focusing on the movie domain, we propose to test if systems are able to jointly perform: (1) question-answering (QA), (2) recommendation, (3) a mix of recommendation and QA and (4) general dialog about the topic, which we call chit-chat. All four tasks have been chosen because they test basic capabilities we expect a dialog system performing insightful movie recommendation should have while evaluation on each of them can be well deﬁned without the need of human-in-the-loop (e.g. via Wizard-of-Oz strategies (Whittaker et al., 2002)). Our ultimate goal is to validate if a single model can solve the four tasks at once, which we assert is a pre-requisite for an end-to-end dialog system supposed to act as a movie recommendation assistant, and by extension a general dialog agent as well. At the same time we advocate developing methods that make no special engineering for this domain, and hence should generalize to learning on tasks and data from other domains easily.  to the bAbI tasks which test basic capabilities of story understanding systems In contrast (Weston et al., 2015b), the tasks have been created using large-scale real-world sources (OMDb1, MovieLens2 and Reddit3). Overall, the dataset covers ∼75k movie entities (movie, actor, director, genre, etc.) with ∼3.5M training examples: even if the dataset is restricted to a single domain, it is large and allows a great variety of discussions, language and user goals. We evaluate on these tasks the performance of various neural network models that can potentially create end-to-end dialogs, ranging from simple supervised embedding models (Bai et al., 2009), RNNs with Long Short-Term Memory (LSTMs) (Hochreiter & Schmidhuber, 1997), and attention-based models, in particular Memory Networks (Sukhbaatar et al., 2015). To validate the quality of our results, we also apply our best performing model, Memory Networks, in other conditions by comparing it on the Ubuntu Dialog Corpus (Lowe et al., 2015) against baselines trained by the authors of the corpus. We show that they outperform all baselines by a wide margin.  2 THE MOVIE DIALOG DATASET  We introduce a set of four tasks to test the ability of end-to-end dialog systems, focusing on the domain of movies and movie related entities. They aim to test ﬁve abilities which we postulate as being key towards a fully functional general dialog system (i.e., not speciﬁc to movies per se):  • QA Dataset: Tests the ability to answer factoid questions that can be answered without  relation to previous dialog. The context consists of the question only.  • Recommendation Dataset: Tests the ability to provide personalized responses to the user  via recommendations (in this case, of movies) rather than universal facts as above.  • QA+Recommendation Dataset: Tests the ability of maintaining short dialogs involving  both factoid and personalized content where conversational state has to be maintained.  • Reddit Dataset: Tests the ability to identify most likely replies in discussions on Reddit. • Joint Dataset: All our tasks are dialogs. They can be combined into a single dataset,  testing the ability of an end-to-end model to perform well at all skills at once.  Sample input contexts and target replies from the tasks are given in Tables 1-4. The datasets are available at: http://fb.ai/babi.  2.1 QUESTION ANSWERING (QA)  The ﬁrst task we build is to test whether a dialog agent is capable of answering simple factual questions. The dataset was built from the Open Movie Database (OMDb)4 which contains metadata about movies. The subset we consider contains ∼15k movies, ∼10k actors and ∼6k directors. We also matched these movies to the MovieLens dataset5 to attribute tags to each movie. We build a knowledge base (KB) directly from the combined data, stored as triples such as (THE DARK HORSE,  1http://en.omdb.org 2http://movielens.org 3http://reddit.com/r/movie 4Downloaded from http://beforethecode.com/projects/omdb/download.aspx. 5http://grouplens.org/datasets/movielens/  2  Published as a conference paper at ICLR 2016  STARRED ACTOR, BETTE DAVIS) and (MOONRAKER, HAS TAG, JAMES BOND), with 8 different relation types involving director, writer, actor, release date, genre, tags, rating and imdb votes.  We distinguish 11 classes of question, corresponding to different kinds of edges in our KB: actor to movie (“What movies did Michael J Fox star in?”), movie to actors (“Who starred in Back to The Future?”), movie to director, director to movie, movie to writer, writer to movie, movie to tags, tag to movie, movie to year, movie to genre and movie to language. For each question type there is a set of possible answers. Using SimpleQuestions, an existing open-domain question answering dataset based on Freebase (Bordes et al., 2015) we identiﬁed the subset of questions posed by those human annotators that covered our question types. We expanded this set to cover all of our KB by substituting the actual entities in those questions to also apply them to other questions, e.g. if the original question written by an annotator was “What movies did Michael J Fox star in?”, we created a pattern “What movies did [@actor] star in?” which we substitute for any other actors in our set, and repeat this for all annotations. We split the questions into training, development and test sets with ∼96k, 10k and 10k examples, respectively.  Task 1: Factoid Question Answering (QA)  What movies are about open source? Revolution OS Ruggero Raimondi appears in which movies? Carmen What movies did Darren McGavin star in? Billy Madison, The Night Stalker, Mrs. Pollifax-Spy Can you name a ﬁlm directed by Stuart Ortiz? Grave Encounters Who directed the ﬁlm White Elephant? Pablo Trapero What is the genre of the ﬁlm Dial M for Murder? Thriller, Crime What language is Whity in? German  Table 1: Sample input contexts and target replies (in red) from Task 1.  To simplify evaluation rather than requiring the generation of sentences containing the answers, we simply ask a model to output a list, which is ranked as the possible set of answers. We then use standard ranking metrics to evaluate the list, making the results easy to interpret. Our main results report the hits@1 metric (i.e. is the top answer correct); other metrics are given in the appendix.  2.2 RECOMMENDATION DATASET  Not all questions about movies in dialogs have an objective answer, independent of the person asking; indeed much of human dialog is based on opinons and personalized responses. One of the simplest dialogs of this type to evaluate is that of recommendation, where we can utilize existing data resources. We again employ the MovieLens dataset which features a user × item matrix of movie ratings, rated from 1 to 5. We ﬁltered the set of movies to be the same set as in the QA task and additionally only kept movies that had at least 2 ratings, giving around ∼ 11k movies. To use this data for evaluating dialog, we then use it to generate dialog exchanges. We ﬁrst select a user at random; this will be the user who is participating in the dialog, and then sample 1-8 movies that the user has rated 5. We then form a statement intended to express the user’s feelings about these movies, according to a ﬁxed set of natural language templates, one of which is selected randomly. See Table 2 for some examples. From the remaining set of movies the same user gave a rating of 5, we select one to be the answer.  Task 2: Recommendation  Schindler’s List, The Fugitive, Apocalypse Now, Pulp Fiction, and The Godfather are ﬁlms I really liked. Can you suggest a ﬁlm? The Hunt for Red October  Some movies I like are Heat, Kids, Fight Club, Shaun of the Dead, The Avengers, Skyfall, and Jurassic Park. Can you suggest something else I might like? Ocean’s Eleven  Table 2: Sample input contexts and target replies (in red) from Task 2.  There are ∼110k users in the training, ∼1k users in the development set and ∼1k for test. We follow the procedure above sampling users with replacement and generate 1M training examples and 10k development and test set examples, respectively. To evaluate the performance of a model, just as in the ﬁrst task, we evaluate a ranked list of answers. In our main results we measure hits@100, i.e. 1  3  Published as a conference paper at ICLR 2016  if the provided answer is in the top 100, and 0 otherwise, rather than hits@1 as this task is harder than the last.  Note that we expect absolute hits@k numbers to be lower for this task than for QA due to incom- plete labeling (“missing ratings”): in recommendation there is no exact right answer, and it is not surprising the actual single true label is not always at the top position, i.e. the top predictions of the model may be good as well, but we do not have their labels. One can thus view the ranking metric as a kind of lower bound on performance of actually labeling all the predictions using human annotations, which would be time consuming and no longer automatic, and hence undesirable for algorithm development. This is standard in recommendation, see e.g. Cremonesi et al. (2010).  2.3 QA+RECOMMENDATION DIALOG  The tasks presented so far only involve questions followed by responses, with no context from pre- vious dialog. This task aims at evaluating responses in the context of multiple previous exchanges, while remaining straightforward enough that evaluation and analysis are still tractable. We hence combine the question answering and recommendation tasks from before in a multi-response dialog, where dialogs consist of 3 exchanges (3 turns from each participant).  The ﬁrst exchange requires a recommendation similar to Task 1 except that they also specify what genre or topic they are interested in, e.g. “I’m looking for a Music movie”, where the answer might be “School of Rock”, as in the example of Table 3.  In the second exchange, given the model’s response (movie suggestion), the user asks a factoid question about that suggestion, e.g. “What else is that about?”, “Who stars in that?” and so on. This question refer back to the previous dialog, making context important.  In the third exchange, the user asks for a alternative recommendation, and provides extra information about their tastes, e.g. “I like Tim Burton movies more”. Again, context of the last two exchanges should help for best performance.  Task 3: QA + Recommendation Dialog  I loved Billy Madison, My Neighbor Totoro, Blades of Glory, Bio-Dome, Clue, and Happy Gilmore. I’m looking for a Music movie. School of Rock What else is that about? Music, Musical, Jack Black, school, teacher, Richard Linklater, rock, guitar I like rock and roll movies more. Do you know anything else? Little Richard  Tombstone, Legends of the Fall, Braveheart, The Net, Outbreak, and French Kiss are ﬁlms I really liked. I’m looking for a Fantasy movie. Jumanji Who directed that? Joe Johnston I like Tim Burton movies more. Do you know anything else? Big Fish  Table 3: Sample input contexts and target replies (in red) from Task 3.  We thus generate 1M examples of such 6 line dialogs (3 turns from each participant) for training, and ∼10k for development and testing respectively. We can evaluate the performance of models across all the lines of dialog (e.g., all ∼30k responses from the test set), but also only on the 1st (Recommendation), 2nd (QA) or 3rd exchange (Similarity) for a more ﬁne-grained analysis. We again use a ranking metric (here, hits@10), just as in our previous tasks.  2.4 REDDIT DISCUSSION  Our fourth task is to predict responses in movie discussions using real conversation data taken di- rectly from Reddit, a website where registered community members can submit content in various areas of interest, called “subreddits”. We selected the movie subreddit6 to match our other tasks. The original discussion data is potentially between multiple participants. To simplify the setup, we ﬂatten this to appear as two participants (parent and comment), just as in our other tasks. In this way we collected ∼1M dialogs, of which 10k are reserved for a development set, and another 10k for the  6https://www.reddit.com/r/movies,  from https://www.reddit.com/r/datasets/comments/3bxlg7.  selecting  the  dataset  available  at  4  Published as a conference paper at ICLR 2016  test set. Of the dialogs, ∼76% involve a single exchange, ∼17% have at least two exchanges, and 7% have at least three exchanges (the longest exchange is length 50).  Task 4: Reddit Discussion  I think the Terminator movies really suck, I mean the ﬁrst one was kinda ok, but after that they got really cheesy. Even the second one which people somehow think is great. And after that... forgeddabotit. C’mon the second one was still pretty cool.. Arny was still so badass, as was Sararah Connor’s character.. and the way they blended real action and effects was perhaps the last of its kind...  Table 4: Sample input contexts and target replies (in red) from Task 4.  To evaluate the performance of models, we again separate the problem of evaluating the quality of a response from that of language generation by considering a ranking setup, in line with other recent works (Sordoni et al., 2015). We proceed as follows: we select a further 10k comments for the development set and another 10k for the test set which have not appeared elsewhere in our dataset, and use these as potential candidates for ranking during evaluation. For each exchange, given the input context, we rank 10001 possible candidates: the true response given in the dataset, plus the 10k “negative” candidates just described. The model has to rank the true response as high as possible. Similar to recommendation as described before we do not expect absolute hits@k performance to be as high as for QA due to incomplete labeling. As with Task 3, we can evaluate on all the data, or only on the 1st, 2nd or 3rd exchange, and so on. We also identiﬁed the subset of the test set where there is an entity match with at least two entities from Tasks 1-3, where one of the entities appears in the input, and the other in the response: this subset serves to evaluate the impact of using a knowledge base for conducting such a dialog.  2.5  JOINT TASK  Finally, we consider a task made of the combination of all four of the previous ones. At both training and test time examples consist of exchanges from any of the datasets, sampled at random, whereby the conversation is ‘reset’ at each sample, so that the context history only ever includes exchanges from the current conversation.  We consider this to be the most important task, as it tests whether a model can not only produce chit-chat (Task 4) but also can provide meaningful answers during dialog (Tasks 1-3). On the other hand, the point of delineating the separate tasks is to evaluate exactly which types of dialog a model is succeeding at or not. That all the datasets are in the same domain is crucial to testing the ability of models at performing well on all tasks jointly. If the domains were different, then the vocabularies would be trivially non-overlapping, allowing to learn effectively separate models inside a single one.  2.6 RELATION TO EXISTING EVALUATION FRAMEWORKS  Traditional dialog systems consist of two main modules: (1) a dialog state tracking component that tracks what has happened in a dialog, incorporating into a pre-deﬁned explicit state structure sys- tem outputs, user utterances, context from previous turns, and other external information, and (2) a response generator. Evaluation of the dialog state tracking stage is well deﬁned since the PAR- ADISE framework (Walker et al., 1997) and subsequent initiatives (Paek, 2001; Griol et al., 2008), including recent competitons (Williams et al., 2013; Henderson et al., 2014) as well as situated vari- ants (Rojas-Barahona et al., 2012). However, they require ﬁne grained data annotations in terms of labeling internal dialog state and precisely deﬁned user intent (goals). As a result, they do not really scale to large domains and dialogs with high variability in terms of language. Because of language ambiguity and variation, evaluation of the response generation step is complicated and usually relies on human judgement (Walker et al., 2003).  End-to-end dialog systems do not rely on explicit internal state and hence do not have state tracking modules, they directly generate responses given user utterances and dialog context and hence can not be evaluated using state tracking test-beds. Unfortunately, as for response generator modules, their evaluation is ill-deﬁned as it is difﬁcult to objectively rate at scale the ﬁt of returned responses. Most existing work (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) chose to use human ratings, which does not easily scale. Sordoni et al. (2015) also use the BLEU score to compare to actual user utterances but this is not a completely satisfying measure of success,  5  Published as a conference paper at ICLR 2016  especially when used in a chit-chat setting where there are no clear goals and hence measures of success. Lowe et al. (2015) use a similar ranking evaluation to ours, but only in a chit-chat setting.  Our approach of providing a collection of tasks to be jointly solved is related to the evaluation framework of the bAbI tasks (Weston et al., 2015a) and of the collection of sequence prediction tasks of Joulin & Mikolov (2015). However, unlike them, our Tasks 1-3 are much closer to real dialog, being built from human-written text, and with Task 4 actually involving real dialog from Reddit. The design of our tasks is such that all test one or more key characteristics a dialog system should have but also that an unambiguous answer is expected after each dialog act. In that sense, it follows the the notion of dialog evaluation by a reference answer introduced in (Hirschman et al., 1990). The application of movie recommender systems is connected to that of TV program suggestion proposed by Ramachandran et al. (2014), except that we frame it so that we can generate systematic evaluation from it, where they only rely on human judgement at small scale.  3 MODELS  3.1 MEMORY NETWORKS  Memory Networks (Weston et al., 2015c; Sukhbaatar et al., 2015) are a recent class of models that perform language understanding by incorporaring a memory component that potentially includes both long-term memory (e.g., to remember facts about the world) and short-term context (e.g., the last few turns of dialog). They have only been evaluated in a few setups: question answering (Bordes et al., 2015), language modeling (Sukhbaatar et al., 2015; Hill et al., 2015), and language understanding on the bAbI tasks (Weston et al., 2015a), but not so far on dialog tasks such as ours.  We employ the MemN2N architecture of Sukhbaatar et al. (2015) in our experiments, with some additional modiﬁcations to construct both long-term and short-term context memories. At any given time step we are given as input the history of the current conversation: messages from the user cu i at i from the model itself at the corresponding time steps, time step i and the corresponding responses cr t and the model has to respond. i = 1, . . . , t − 1. At the current time t we are only given the input cu  Retrieving long-term memories For each word in the last N messages we perform a hash lookup to return all long-term memories (sentences) from a database that also contain that word. Words above a certain frequency cutoff can be ignored to avoid sentences that only share syntax or unim- portant words. We employ the movie knowledge base of Sec. 2.1 for our long-term memories, but potentially any text dataset could be used. See Figure 5 for an example of this process.  Attention over memories The sentences hj, j = 1, . . . , H returned from the hashing step plus the messages from the current conversation form the memory of the Memory Network7:  x = (cu  1 , . . . , cu  t−1, cr  1, . . . , cr  t−1, h1, . . . , hH ).  t is embedded using a matrix A of size d × V where d is the embedding The last user input cu dimension and V is the size of the vocabulary, giving u = Acu t . Each memory xi is embedded using the same matrix, giving mi = Axi. The match between the input and the memories is then computed by taking the inner product followed by a softmax: pi = Softmax(u⊤mi) giving a probability vector over the memories. The output memory representation is then constructed with o = R Pi pimi where R is a d×d rotation matrix8. The memory output is then added to the original input q = o+cu t . This procedure can then be stacked in what is called multiple “hops” of attention over the memory.  Generating the ﬁnal prediction The ﬁnal prediction is ˆa = Softmax(q⊤W y1, . . . , q⊤W yC ) where there are C candidate responses in y, and W is of dimension V × d. For Tasks 1-3 the candidates are the set of words in the vocabulary, which are ranked for ﬁnal evaluation, whereas for Task 4 the candidates are target respones (sentences).  then deﬁned as:  The whole model is trained using stochastic gradient descent by minimizing a standard cross-entropy loss between ˆa and the true label a.  7We also add time features to each memory to denote their position following (Sukhbaatar et al., 2015). 8Optionally, different dictionaries can be used for inputs, memories and outputs instead of being shared.  6  Published as a conference paper at ICLR 2016  Long-Term Memories hi  1  Short-Term cu Memories cr Input  cu  1  2  Output  y  Shaolin Soccer directed by Stephen Chow Shaolin Soccer written by Stephen Chow Shaolin Soccer starred actors Stephen Chow Shaolin Soccer release year 2001 Shaolin Soccer has genre comedy Shaolin Soccer has tags martial arts, kung fu soccer, stephen chow Kung Fu Hustle directed by Stephen Chow Kung Fu Hustle written by Stephen Chow Kung Fu Hustle starred actors Stephen Chow Kung Fu Hustle has genre comedy action Kung Fu Hustle has imdb votes famous Kung Fu Hustle has tags comedy, action, martial arts, kung fu, china, soccer, hong kong, stephen chow The God of Cookery directed by Stephen Chow The God of Cookery written by Stephen Chow The God of Cookery starred actors Stephen Chow The God of Cookery has tags hong kong Stephen Chow From Beijing with Love directed by Stephen Chow From Beijing with Love written by Stephen Chow From Beijing with Love starred actors Stephen Chow, Anita Yuen  . . . <and more> . . .  1) I’m looking a fun comedy to watch tonight, any ideas? 2) Have you seen Shaolin Soccer? That was zany and great.. really funny but in a whacky way. 3) Yes! Shaolin Soccer and Kung Fu Hustle are so good I really need to ﬁnd some more Stephen Chow ﬁlms I feel like there is more awesomeness out there that I haven’t discovered yet ... 4) God of Cookery is pretty great, one of his mid 90’s hong kong martial art comedies.  Table 5: Memory Network long-term and short-term memories. Blue underlined text indicates those words that hashed into the knowledge base to recall sentences from the long-term memory. Those, along with the recent short-term context (lines labeled 1 and 2) are used as input memories to the Memory Network along with the input (labeled 3). The desired goal is to output dialog line 4.  3.2 SUPERVISED EMBEDDING MODELS  While one of the major uses of word embedding models is to learn unsupervised embeddings over large unlabeled datasets such as in Word2Vec (Mikolov et al., 2013) there are also very effective word embedding models for training supervised models when labeled data is available. The sim- plest approach which works suprisingly well is to sum the word embeddings of the input and the target independently and then compare them with a similarity metric such as inner product or co- sine similarity. A ranking loss is used to ensure the correct targets are ranked higher than any other targets. Several variants of this approach exist. For matching two documents supervised semantic indexing (SSI) was shown to be superior to unsupervised latent semantic indexing (LSI) (Bai et al., 2009). Similar methods were shown to outperform SVD for recommendation (Weston et al., 2013). However, we do not expect this method to work as well on question answering tasks, as all the memorization must occur in the individual word embeddings, which was shown to perform poorly in (Bordes et al., 2014). For example, consider asking the question “who was born in Paris?” and re- quiring the word embedding for Paris to effectively contain all the pertinent information. However, for rarer items requiring less storage, performance may not be as degraded. In general we believe this is a surprisingly strong baseline that is often neglected in evaluations. Our implementation corresponds to a Memory Network with no attention over memory.  3.3 RECURRENT LANGUAGE MODELS  Recurrent Neural Networks (RNNs) have proven successful at several tasks involving natural language, language modeling (Mikolov et al., 2011), and have been applied recently to dialog (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). LSTMs are not known however for tasks such as QA or item recommendation, and so we expect them to ﬁnd our datasets challenging.  There are a large number of variants of RNNs, including Long-Short Term Memory activa- tion units (LSTMs) (Hochreiter & Schmidhuber, 1997), bidirectional LSTMs (Graves et al., 2012), seq2seq models (Sutskever et al., 2014), RNNs that take into account the document context (Mikolov & Zweig, 2012) and RNNs that perform attention over their input in various different ways (Bahdanau et al., 2015; Hermann et al., 2015; Rush et al., 2015). Evaluating all these variants  7  Published as a conference paper at ICLR 2016  is beyond the scope of this work and we instead use standard LSTMs as our baseline method9. How- ever, we note that LSTMs with attention have many properties in common with Memory Networks if the attention is applied over the same memory setup.  3.4 QUESTION ANSWERING SYSTEMS  For the particular case of Task 1 we can apply existing question answering systems. There has been a recent surge in interest in such systems that try to answer a question posed in natural language by converting it into a database search over a knowledge base (Berant & Liang, 2014; Kwiatkowski et al., 2013; Fader et al., 2014), which is a setup natural for our QA task also. How- ever, such systems cannot easily solve any of our other tasks, for example our recommendation Task 2 does not involve looking up a factoid answer in a database. Nevertheless, this allows us to compare the performance of end-to-end systems performant on all our tasks to a standard QA benchmark. We chose the method of Bordes et al. (2014)10 as our baseline. This system learns em- beddings that match questions to database entries, and then ranks the set of entries, and has been shown to achieve good performance on the WEBQUESTIONS benchmark (Berant et al., 2013).  3.5 SINGULAR VALUE DECOMPOSITION  Singular Value Decomposition (SVD) is a standard benchmark for recommendation, being at the core of the best ensemble results in the Netﬂix challenge, see Koren & Bell (2011) for a review. However, it has been shown to be outperformed by other ﬂavors of matrix factorization, in particular by using a ranking loss rather than squared loss (Weston et al., 2013) which we will compare to (cf. sec 3.2), as well as improvements like SVD++ (Koren, 2008). Collaborative ﬁltering methods are applicable to Task 2, but cannot easily be used for any of the other tasks. Even for Task 2, while our dialog models use textual input, as shown in Table 2, SVD requires a user × item matrix, so for this baseline we preprocessed the text to assign each entity an ID, and throw away all other text. In contrast, the end-to-end dialog models have to learn to process the text as part of the task.  3.6  INFORMATION RETRIEVAL MODELS  To select candidate responses a standard baseline is nearest neighbour information retrieval (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015). Two simple vari- ants are often tried: given an input message, either (i) ﬁnd the most similar message in the (training) dataset and output the response from that exchange; or (ii) ﬁnd the most similar response to the input directly. In both cases the standard measure of similarity is tf-idf weighted cosine similarity between the bags of words. Note that that the Supervised Embedding Models of Sec. 3.2 effectively implement the same kind of model (ii) but with a learnt similarity measure. It has been shown pre- viously that method (ii) performs better (Ritter et al., 2011), and our initial IR experiments showed the same result. Note that while (non-learning) IR systems can also be applied to other tasks such as QA (Kolomiyets & Moens, 2011) they require signiﬁcant tuning to do so. Here we stick to a vanilla vector space model and hence only apply an IR baseline to Task 4.  4 RESULTS  Our main results across all the models and tasks are given in Table 4. Supervised Embeddings and Memory Networks are tested in two settings: trained and tested on all tasks separately, or jointly on the combined Task 5. Other methods are only evaluated on independent tasks. In all cases, parameter search was performed on the development sets; parameter choices are provided in the appendix.  Answering Factual Questions Memory Networks and the baseline QA system are the two meth- ods that have an explicit long-term memory via access to the knowledge base (KB). On the task of answering factual questions where the answers are contained in the KB, they outperform the other methods convincingly, with LSTMS being particularly poor. The latter is not unexpected as that method is good at language modeling, not question answering, see e.g. Weston et al. (2015b). The  9We used the code available at: https://github.com/facebook/SCRNNs 10We used the ‘Path Representation’ for the knowledge base, as described in Sec. 3.1 of Bordes et al. (2014).  8  Published as a conference paper at ICLR 2016  METHODS QA SYSTEM (BORDES ET AL., 2014) SVD IR LSTM SUPERVISED EMBEDDINGS MEMN2N JOINT SUPERVISED EMBEDDINGS JOINT MEMN2N  QA TASK (HITS@1)  RECS TASK (HITS@100)  QA+RECS TASK REDDIT TASK (HITS@10)  (HITS@10)  90.7 N/A N/A 6.5 50.9 79.3 43.6 83.5  N/A 19.2 N/A 27.1 29.2 28.6 28.1 26.5  N/A N/A N/A 19.9 65.9 81.7 58.9 78.9  N/A N/A 23.7 11.8 27.6 29.2 14.5 26.6  Table 6: Test results across all tasks. Of those methods tested, supervised embeddings, LSTMs and MemN2N are easily applicable to all tasks. The other methods are standard benchmarks for individual tasks. The ﬁnal two rows are models trained on the Combined Task, of all tasks at once. Evaluation uses the hits@k metric (in percent) with the value of k given in the second row.  baseline QA system, which is designed for this task, is superior to Memory Networks, indicating there is still room for improvement in that model. On the other hand, the latter’s much more general design allows it to perform well on our other dialog tasks, whereas the former is task speciﬁc.  Making Recommendations In this task a long-term memory does not bring any improvement, with LSTMs, Supervised Embeddings and Memory Networks all performing similarly, and all out- performing the SVD baseline. Here, we conjecture LSTMs can perform well because it looks much more like a language modeling task, i.e. the input is a sequence of similar recommendations.  Using Dialog History In both QA+Recommendations (Task 3) and Reddit (Task 4) Memory Net- works outperform Supervised Embeddings due to their better use of context. This can be seen by breaking down the results by length of context: in the ﬁrst response they perform similarly, but Memory Networks show a relative improvement on the second and third responses, see Tables 9 and 10 in the appendix. Note that these improvements come from the short term memory (dialog history), not from the use of the KB, as we show Memory Networks results without access to the KB and they perform similarly. We believe the QA performance in these cases is not hindered by the lack of a KB because we ask questions based on fewer relations than in Task 1 and it is easier to store the knowledge directly in the word embeddings. The baseline IR model in Task 4 bene- ﬁts from context too, it is compared with and without in Table 10. LSTMs perform poorly: the posts in Reddit are quite long and the memory of the LSTM is relatively short, as pointed out by Sordoni et al. (2015). In that work they employed a linear reranker that used LSTM prediction as features to better effect. Testing more powerful recurrent networks such as LSTMs with attention on these benchmarks remains as future work (although the latter is related to Memory Networks, which we do report).  Joint Learning A truly end-to-end dialog system has to be good at all the skills in Tasks 1-4 (and more besides, i.e. this is necessary, but not sufﬁcient). We thus report results on our Combined Task for Supervised Embeddings and Memory Networks. Supervised Embeddings still have the same failings as before on Tasks 1 and 3, but now seem to perform even more poorly due to the difﬁculty of encoding all the necessary skills in the word embeddings, so e.g., they now do signiﬁcantly worse on Task 4. This is despite us trying word embeddings of up to 2000 dimensions. Memory Networks fare better, having only a slight loss in performance on Tasks 2-4 and a slight gain in Task 1. In their case, the modeling power is not only in the word embeddings, but also in the attention over the long-term and short-term memory, so it does not need as much capacity in the word embeddings. However, the best achievable models would presumably have some improvement from training across all the tasks, not a loss, and would perform at least as well as all the individual task baselines (i.e. in this case, perform better at Task 1).  5 UBUNTU DIALOGUE CORPUS RESULTS  As no other authors have yet published results on our new benchmark, to validate the quality of our results we also apply our best performing model in other conditions by comparing it on the Ubuntu Dialog Corpus (Lowe et al., 2015). In particular, this also allows us to compare to more sophisticated  9  Published as a conference paper at ICLR 2016  METHODS IR† RNN† LSTM† MEMN2N 1-HOP MEMN2N 2-HOPS MEMN2N 3-HOPS MEMN2N 4-HOPS  VALIDATION (HITS@1)  TEST  (HITS@1)  N/A N/A N/A 57.23 64.28 64.31 64.01  48.81 37.91 55.22 56.25 63.51 63.72 62.82  Table 7: Ubuntu Dialog Corpus results. The evaluation is retrieval-based, similar to that of Reddit (Task 4). For each dialog, the correct answer is mixed among 10 random candidates; Hits@1 (in %) are reported. Methods with † have been ran by Lowe et al. (2015).  LSTMs models that are trained discriminatively using metric learning, as well as additional baseline methods all trained by the authors. The Ubuntu Dialog Corpus contains almost 1M dialogs of more than 7 turns on average (900k dialogs for training, 20k for validation and 20k for testing), and 100M million words. The corpus was scraped from the Ubuntu IRC channel logs where users ask questions about issues they are having with Ubuntu and get answers by other users. Most chats can involve more than two users but a series of heuristics to disentangle them into dyadic dialogs was used.  The evaluation is similar to that of Reddit (Task 4): each correct answer has to be retrieved among a set of 10, mixed with 9 randomly chosen candidate utterances. We report the Hits@1 in Table 7.11 We used the same MemN2N architecture as before. all models were selected using validation accu- racy. On this dataset, which has longer dialogs than those from the Movie Dialog Corpus, we can see that running more hops on the memory with the MemN2N improves performance: the 1-hop model performs similarly to the LSTM but with 2-hops and more we can gain more than a +8% increase over the previous best reported model. Using even more hops still improves over 1-hop but not much over 2-hops.  6 CONCLUSION  We have presented a new set of benchmark tasks designed to evaluate end-to-end dialog systems. The movie dialog dataset measures how well such models can perform at both goal driven dialog, of both objective and subjective goals thanks to evaluation metrics on question answering and rec- ommendation tasks, and at less goal driven chit-chat. A true end-to-end model should perform well at all these tasks, being a necessary but not sufﬁcient condition for a fully functional dialog agent.  We showed that some end-to-end neural networks models can perform reasonably across all tasks compared to standard per-task baselines. Speciﬁcally, Memory Networks that incorporate short and long term memory can utilize local context and knowledge bases of facts to boost performance. We believe this is promising because we showed these same architectures also perform well on a separate dialog task, the Ubuntu Dialog Corpus, and have been shown previously to work well on the synthetic but challenging bAbI tasks of Weston et al. (2015a), and have no special engineering for the tasks or domain. However, some limitations remain, in particular they do not perform as well as stand-alone QA systems for QA, and performance is also degraded rather than improved when training on all four tasks at once. Future work should try to overcome these problems.  While our dataset focused on movies, there is nothing speciﬁc to the task design which could not be transferred immediately to other domains, for example sports, music, restaurants, and so on. Future work should create new tasks in this and other domains to ensure that models are ﬁrstly not overtuned for these goals, and secondly to test further skills – and to motivate the development of algorithms to be skillful at them.  REFERENCES  Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly  learning to align and translate. ICLR 2015, 2015.  11Results for the baselines from (Lowe et al., 2015) differ to that from the v3 of the arxiv paper, because the  corpus has been updated since then. All results in Table 7 use the latest version of the corpus.  10  Published as a conference paper at ICLR 2016  Bai, Bing, Weston, Jason, Grangier, David, Collobert, Ronan, Sadamasa, Kunihiko, Qi, Yanjun, Chapelle, Olivier, and Weinberger, Kilian. Supervised semantic indexing. In Proceedings of the 18th ACM conference on Information and knowledge management, pp. 187–196. ACM, 2009.  Berant, Jonathan and Liang, Percy. Semantic parsing via paraphrasing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL’14), Baltimore, USA, 2014.  Berant, Jonathan, Chou, Andrew, Frostig, Roy, and Liang, Percy. Semantic parsing on freebase from  question-answer pairs. In EMNLP, pp. 1533–1544, 2013.  Bordes, Antoine, Chopra, Sumit, and Weston, Jason. Question answering with subgraph embed-  dings. In Proc. EMNLP, 2014.  Bordes, Antoine, Usunier, Nicolas, Chopra, Sumit, and Weston, Jason. Large-scale simple question  answering with memory networks. arXiv preprint arXiv:1506.02075, 2015.  Cremonesi, Paolo, Koren, Yehuda, and Turrin, Roberto. Performance of recommender algorithms on top-n recommendation tasks. In Proceedings of the fourth ACM conference on Recommender systems, pp. 39–46. ACM, 2010.  Fader, Anthony, Zettlemoyer, Luke, and Etzioni, Oren. Open question answering over curated and extracted knowledge bases. In Proceedings of 20th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD’14), New York City, USA, 2014. ACM.  Graves, Alex et al. Supervised sequence labelling with recurrent neural networks, volume 385.  Springer, 2012.  Griol, David, Hurtado, Llu´ıs F, Segarra, Encarna, and Sanchis, Emilio. A statistical approach to  spoken dialog systems design and evaluation. Speech Communication, 50(8):666–682, 2008.  Henderson, Matthew. Machine learning for dialog state tracking: A review. In Proceedings of The  First International Workshop on Machine Learning in Spoken Language Processing, 2015.  Henderson, Matthew, Thomson, Blaise, and Williams, Jason. The second dialog state tracking challenge. In 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pp. 263, 2014.  Hermann, Karl Moritz, Koˇcisk´y, Tom´aˇs, Grefenstette, Edward, Espeholt, Lasse, Kay, Will, Teaching machines to read and compre- URL  In Advances in Neural Information Processing Systems (NIPS), 2015.  Suleyman, Mustafa, and Blunsom, Phil. hend. http://arxiv.org/abs/1506.03340.  Hill, Felix, Bordes, Antoine, Chopra, Sumit, and Weston, Jason. The goldilocks principle: Reading children’s books with explicit memory representations. arXiv preprint arXiv:1511.02301, 2015.  Hirschman, Lynette, Dahl, Deborah A, McKay, Donald P, Norton, Lewis M, and Linebarger, Mar- cia C. Beyond class a: A proposal for automatic evaluation of discourse. Technical report, DTIC Document, 1990.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Isbell, Charles Lee, Kearns, Michael, Kormann, Dave, Singh, Satinder, and Stone, Peter. Cobot in  lambdamoo: A social statistics agent. In AAAI/IAAI, pp. 36–41, 2000.  Jafarpour, Sina, Burges, Christopher JC, and Ritter, Alan. Filter, rank, and transfer the knowledge:  Learning to chat. Advances in Ranking, 10, 2010.  Joulin, Armand and Mikolov, Tomas. Inferring algorithmic patterns with stack-augmented recurrent  nets. arXiv preprint: 1503.01007, 2015.  Kolomiyets, Oleksandr and Moens, Marie-Francine. A survey on question answering technology  from an information retrieval perspective. Information Sciences, 181(24):5412–5434, 2011.  11  Published as a conference paper at ICLR 2016  Koren, Yehuda. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 426–434. ACM, 2008.  Koren, Yehuda and Bell, Robert. Advances in collaborative ﬁltering.  handbook, pp. 145–186. Springer, 2011.  In Recommender systems  Kwiatkowski, Tom, Choi, Eunsol, Artzi, Yoav, and Zettlemoyer, Luke. Scaling semantic parsers with on-the-ﬂy ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP’13), Seattle, USA, October 2013.  Lowe, Ryan, Pow, Nissan, Serban, Iulian, and Pineau, Joelle. The ubuntu dialogue corpus: arXiv preprint  A large dataset for research in unstructured multi-turn dialogue systems. arXiv:1506.08909, 2015.  Mikolov, Tomas and Zweig, Geoffrey. Context dependent recurrent neural network language model.  In SLT, pp. 234–239, 2012.  Mikolov, Tom´aˇs, Kombrink, Stefan, Burget, Luk´aˇs, ˇCernock`y, Jan Honza, and Khudanpur, San- jeev. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pp. 5528–5531. IEEE, 2011.  Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efﬁcient estimation of word repre-  sentations in vector space. arXiv:1301.3781, 2013.  Narasimhan, Karthik, Kulkarni, Tejas, and Barzilay, Regina. Language understanding for text-based  games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.  Paek, Tim. Empirical methods for evaluating dialog systems. In Proceedings of the workshop on Evaluation for Language and Dialogue Systems-Volume 9, pp. 2. Association for Computational Linguistics, 2001.  Ramachandran, Deepak, Yeh, Peter Z, Jarrold, William, Douglas, Benjamin, Ratnaparkhi, Adwait, Provine, Ronald, Mendel, Jeremy, and Emﬁeld, Adam. An end-to-end dialog system for tv pro- gram discovery. In Spoken Language Technology Workshop (SLT), 2014 IEEE, pp. 602–607. IEEE, 2014.  Ritter, Alan, Cherry, Colin, and Dolan, William B. Data-driven response generation in social media. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 583–593. Association for Computational Linguistics, 2011.  Rojas-Barahona, Lina M, Lorenzo, Alejandra, and Gardent, Claire. An end-to-end evaluation of two situated dialog systems. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pp. 10–19. Association for Computational Linguistics, 2012.  Rush, Alexander M, Chopra, Sumit, and Weston, Jason. A neural attention model for abstractive  sentence summarization. Proceedings of EMNLP, 2015.  Shang, Lifeng, Lu, Zhengdong, and Li, Hang. Neural responding machine for short-text conversa-  tion. arXiv preprint arXiv:1503.02364, 2015.  Sordoni, Alessandro, Galley, Michel, Auli, Michael, Brockett, Chris, Ji, Yangfeng, Mitchell, Mar- garet, Nie, Jian-Yun, Gao, Jianfeng, and Dolan, Bill. A neural network approach to context- sensitive generation of conversational responses. Proceedings of NAACL, 2015.  Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and Fergus, Rob. End-to-end memory net-  works. Proceedings of NIPS, 2015.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural net-  works. In Advances in neural information processing systems, pp. 3104–3112, 2014.  Vinyals, Oriol and Le, Quoc. A neural conversational model. arXiv preprint arXiv:1506.05869,  2015.  12  Published as a conference paper at ICLR 2016  Walker, Marilyn A, Litman, Diane J, Kamm, Candace A, and Abella, Alicia. Paradise: A framework for evaluating spoken dialogue agents. In Proceedings of the eighth conference on European chap- ter of the Association for Computational Linguistics, pp. 271–280. Association for Computational Linguistics, 1997.  Walker, Marilyn A, Prasad, Rashmi, and Stent, Amanda. A trainable generator for recommendations  in multimodal dialog. In INTERSPEECH, 2003.  Weston, J., Bordes, A., Chopra, S., and Mikolov, T. Towards AI-complete question answering: A  set of prerequisite toy tasks. arXiv preprint: 1502.05698, 2015a.  Weston, Jason, Yee, Hector, and Weiss, Ron J. Learning to rank recommendations with the k-order statistic loss. In Proceedings of the 7th ACM conference on Recommender systems, pp. 245–248. ACM, 2013.  Weston, Jason, Bordes, Antoine, Chopra, Sumit, and Mikolov, Tomas. Towards ai-complete question  answering: a set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015b.  Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Memory networks. Proceedings of ICLR,  2015c.  Whittaker, Steve, Walker, Marilyn A, and Moore, Johanna D. Fish or fowl: A wizard of oz evaluation  of dialogue strategies in the restaurant domain. In LREC, 2002.  Williams, Jason, Raux, Antoine, Ramachandran, Deepak, and Black, Alan. The dialog state tracking  challenge. In Proceedings of the SIGDIAL 2013 Conference, pp. 404–413, 2013.  13  Published as a conference paper at ICLR 2016  A FURTHER EXPERIMENTAL DETAILS  Dictionary For all models we built a dictionary using all the known entities in the KB (e.g. “Bruce Willis” and “Die Hard” are single dictionary elements). This allows us to output a single symbol for QA and Recommendation in order to predict an entity, rather than having to construct the answer out of words, making training and evaluation of the task simpler. The rest of the dictionary is built of unigrams that are not covered by our entity dictionary, where we removed other words (but not entities) with frequency less than 5. Overall this gives a dictionary of size 189472, which includes 75542 entities. All entries and texts were lower-cased. Our text parser to convert to the dictionary representation is then very simple: it goes left to right, consuming the largest n-gram at each step.  Memory Networks For most of the tasks the optimal number of hops was 1, except for Task 3 where 2 or 3 hops outperform 1. See Table 9 and the parameter choices in Sec. B. For the joint task (Task 5), to achieve best performance we increased the capacity compared to the individual task models by using different dictionaries for the input, memory and output layers, see Sec. B. Additionally, we pre-trained the weights by training without the long-term memory for speed.  Supervised Embedding Models We tried two ﬂavors of supervised embedding model: (i) a model f (x, y) = x⊤U ⊤U y (“single dictionary model”); and (ii) a model f (x, y) = x⊤U ⊤V y (“two dictionary model”). That is, the latter has two sets of word embeddings depending on whether the word is in the input+context, or the label. The input and context are concatenated together to form a bag of words in either case. It turns out method (i) works better on Tasks 1 and 4, and method (ii) works better on Tasks 2 & 3. Some of the reasons why that is so are easy to understand: on Tasks 2 and 3 (recommendations) a single dictionary model favors predicting the same movies that are already in the input context, which are never correct. However, it appears that on Tasks 1 and 4 the two dictionary model appears to overﬁt to some degree. This partially explains why the model overall is worse on the joint dataset (Task 5). See Sec. B for more details.  LSTMs LSTMs performed poorly on Task 4 and we spent some time trying to improve these re- sults. Despite the perplexity looking reasonable (∼96 on the training set, and ∼105 on the validation set) after training for ∼6 days, we still obtain poor results distinguishing between candidates. We also tried Seq2Seq models (without attention or metric learning) and did not obtain improvements. Part of the problem is that posts in Reddit vary from very short (a few words) to very long (several paragraphs) and one natural procedure to try – computing the probability of those sequences seeded by the input – gives very unbalanced results, and tends to select the shorter ones, ending up with worse than random performance. Further, computationally the whole procedure is then very slow compared to all other methods tested. Memory Networks and supervised embeddings need to com- pute the inner product between embedded inputs and outputs, and hence the the candidates can be embedded once and cached for the whole test set. This trick is not applicable to the method described above rendering it much slower. To deal with the speed issue one can use our supervised embedding model as a ﬁrst step, and then only reranking the top 100 results with the LSTM to make it tractable, however performance is still poor as mentioned. We obtained improved results by instead adopting the approach of Narasimhan et al. (2015): we take the representation for a dialog message as the average embedding over the hidden states as the symbols are consumed (at each step of the recur- rence). We also note that Lowe et al. (2015) report good results (on a different dataset, the Ubuntu Corpus) by training an additional metric learner on top of an LSTM representation, which we have not tried. However, we do compare that approach to Memory Networks on that corpus in Section 5.  Information Retrieval Aside from the models described in the main paper, we also experimented with a hybrid relevance feedback approach: ﬁnd the most similar message in the history, add the response to the query (with a certain weight) and then score candidate responses with the combined input. However, the relevance feedback model did not help: as we increase the feedback parameter (how much to use the retrieved response) the model only degrades, see Table 10 for the performance adding with a weight of 0.5.  B OPTIMAL HYPER-PARAMETER VALUES  Hyperparameters of all learning models have been set using grid search on the validation set. The main hyperparameters are embedding dimension d, learning rate λ, number of dictionaries w, num-  14  Published as a conference paper at ICLR 2016  ber of hops K for MemNNs and unfolding depth blen for LSTMs. All models are implemented in the Torch library (see torch.ch).  Task 1 (QA)  • QA System of Bordes et al. (2014): λ = 0.001, d = 50. • Supervised Embedding Model: λ = 0.05, d = 50, w = 1. • MemN2N: λ = 0.005, d = 50, w = 1, K = 1. • LSTM: λ = 0.001, d = 100, blen = 10.  Task 2 (Recomendation)  • SVD: d = 50. • Supervised Embedding Model: λ = 0.005, d = 200, w = 2. • MemN2N: λ = 0.01, d = 1000, w = 1, K = 1. • LSTM: λ = 0.01, d = 100, blen = 10.  Task 3 (QA+Recommendation)  • Supervised Embedding Model: λ = 0.005, d = 1000, w = 2. • MemN2N: λ = 0.001, d = 50, w = 1, K = 3. • LSTM: λ = 0.001, d = 100, blen = 10.  Task 4 (Reddit)  • Supervised Embedding Model: λ = 0.1, d = 1000, w = 1. • MemN2N: λ = 0.01, d = 1000, w = 1, K = 1. • LSTM: λ = 0.01, d = 512, blen = 15.  Joint Task We chose hyperparameters by taking the mean performance over the four tasks, after scaling each task by the best performing model on that task on the development set in order to normalize the metrics.  • Supervised Embedding Model: λ = 0.01, d = 1000, w = 2. • MemN2N: λ = 0.005, d = 1000, w = 3.  Ubuntu Dialog Corpus Hyperparameters of the MemN2N have been set using grid search on the validation set. We report the best models with K = 1, 2, 3, 4 in the paper; other hyperparameters were λ = 0.001, d = 256.  15  Published as a conference paper at ICLR 2016  C FURTHER DETAILED RESULTS  C.1 BREAKDOWN OF TASK 1 (QA) RESULTS BY QUESTION TYPE  TASK WRITER TO MOVIE TAG TO MOVIE MOVIE TO YEAR MOVIE TO WRITER MOVIE TO TAGS MOVIE TO LANGUAGE MOVIE TO GENRE MOVIE TO DIRECTOR MOVIE TO ACTORS DIRECTOR TO MOVIE ACTOR TO MOVIE TOTAL  QA SYSTEM OF  BORDES ET AL. (2014) H@1 98.7 71.8 89.8 88.8 84.5 94.6 93.0 88.2 88.5 98.3 98.9 90.7  H@10 98.7 71.8 89.8 89.5 85.3 94.8 93.5 88.2 88.5 98.3 98.9 91.0  SUPERVISED EMBEDDINGS H@10 H@1 90.8 77.3 96.1 53.4 3.4 25.4 93.6 61.7 92.0 36.8 84.7 45.2 95.0 46.4 90.1 52.3 64.5 95.2 93.8 61.4 89.4 79.0 50.9 82.97  MEMN2N  H@1 77.6 61.4 87.3 73.5 79.9 90.1 92.5 78.3 68.4 71.5 76.7 78.9  H@10 95.5 88.6 92.1 84.1 95.1 97.6 99.4 87.1 87.2 91.0 96.7 91.8  Table 8: QA task test performance per question type (h@1 / h@10 metrics).  C.2 BREAKDOWN OF TASK 3 (QA+RECOMMENDATION) RESULTS BY RESPONSE TYPE  RESPONSE 1 RESPONSE 2 RESPONSE 3 (SIMILAR)  METHODS SUPERVISED EMBEDDINGS LSTM MEMN2N (1 HOP) MEMN2N (2 HOPS) MEMN2N (3 HOPS) MEMN2N (3 HOPS, -KB)  WHOLE TEST SET  56.0 19.9 70.5 76.8 75.4 75.9  (RECS) 56.7 35.3 47.0 53.4 52.6 54.3  (QA) 76.2 14.3 89.2 90.1 90.0 85.0  38.8 9.2 76.5 88.6 84.2 91.5  Table 9: QA+Recommendation task test results (h@10 metric). The last row shows MemN2N without access to a long-term memory (KB).  C.3 BREAKDOWN OF TASK 4 (REDDIT) RESULTS BY RESPONSE TYPE  METHODS IR (QUERY+CONTEXT) IR (QUERY) IR (QUERY) RF=0.05 SUPERVISED EMBEDDINGS MEMN2N (-KB) MEMN2N  WHOLE TEST SET MATCHED RESPONSE 1 RESPONSE 2 RESPONSE 3+  ENTITY  23.7 23.1 19.2 27.6 29.6 29.2  49.0 48.3 40.8 54.1 57.0 56.4  21.1 21.1 18.3 24.8 25.6 25.4  26.4 25.7 21.2 30.4 34.2 32.9  30.0 27.9 21.4 33.1 37.2 37.0  Table 10: Reddit task test results (h@10 metric). MEMN2N (-KB) is the Memory Network model without access to the knowledge base.  16  ",
1511.06410,2016,Better Computer Go Player with Neural Network and Long-term Prediction,"['Better Computer Go Player with Neural Network and Long-term Prediction\nYuandong Tian', 'Yan Zhu']",https://arxiv.org/pdf/1511.06410,"6 1 0 2     b e F 9 2         ]  G L . s c [      3 v 0 1 4 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  BETTER COMPUTER GO PLAYER WITH NEURAL NET- WORK AND LONG-TERM PREDICTION  Yuandong Tian Facebook AI Research Menlo Park, CA 94025 yuandong@fb.com  Yan Zhu Rutgers University Facebook AI Research yz328@cs.rutgers.edu  ABSTRACT  Competing with top human players in the ancient game of Go has been a long- term goal of artiﬁcial intelligence. Go’s high branching factor makes traditional search techniques ineffective, even on leading-edge hardware, and Go’s evaluation function could change drastically with one stone change. Recent works [Maddi- son et al. (2015); Clark & Storkey (2015)] show that search is not strictly nec- essary for machine Go players. A pure pattern-matching approach, based on a Deep Convolutional Neural Network (DCNN) that predicts the next move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source Go en- gines such as Pachi [Baudis & Gailly (2012)] if its search budget is limited. We extend this idea in our bot named darkforest, which relies on a DCNN designed for long-term predictions. Darkforest substantially improves the win rate for pattern- matching approaches against MCTS-based approaches, even with looser search budgets. Against human players, the newest versions, darkfores2, achieve a sta- ble 3d level on KGS Go Server as a ranked bot, a substantial improvement upon the estimated 4k-5k ranks for DCNN reported in Clark & Storkey (2015) based on games against other machine players. Adding MCTS to darkfores2 creates a much stronger player named darkfmcts3: with 5000 rollouts, it beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyS- tone) except for AlphaGo [Silver et al. (2016)]; with 110k rollouts, it won the 3rd place in January KGS Go Tournament.  1  INTRODUCTION  For a long time, computer Go is considered to be a grand challenge in artiﬁcial intelligence. Fig. 1 shows a simple illustration of the game of Go. Two players, black and white, place stones at inter- sections in turn on a 19x19 board (Fig. 1(a)). Black plays ﬁrst on an empty board. A 4-connected component of the same color is called a group. The liberties of a group is the number of its neigh- boring empty intersections (Fig. 1(b)). A group is captured if its liberties is zero. The goal of the game is to control more territory than the opponent (Fig. 1(c)). Fig. 1(d)) shows the Go rating sys- tem, ranging from kyu level (beginner to decent amateur, 30k-1k) to dan level (advanced amateur, 1d-7d) and to professional levels (1p-9p) [Silver (2009)]. Go is difﬁcult due to its high branching factors (typically on the order of hundred on a 19x19 board) and subtle board situations that are sensitive to small changes (adding/removing one stone could alter the life/death situation of a large group of stone and thus completely changes the ﬁnal score). A combination of the two implies that the only solution is to use massive search that requires a prohibitive amount of resources, which is not attainable with cutting-edge hardware. Fortunately, recent works [Maddison et al. (2015); Clark & Storkey (2015)] in Computer Go have shown that the Go board situation could be deciphered with Deep Convolutional Neural Network (DCNN). They can predict the next move that a human would play 55.2% of the time. How- ever, whether this accuracy leads to a strong Go AI is not yet well understood. It is possible that DCNN correctly predicts most regular plays by looking at the correlation of local patterns, but still fails to predict the critical one or two moves and loses the game. Indeed, a DCNN-based player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesv´ari (2006)], let alone commercial ones.  1  Published as a conference paper at ICLR 2016  Figure 1: A simple illustrations on Go rules and rating system. Images are from Internet.  Figure 2: Some special situations in Go. (a) Ko. After black captures white stone by playing at a, white is prohibited to capture back immediately by playing at b to prevent repetition of game state. (b) Ko ﬁght. Black captures white at 1, white cannot capture back. Instead, white can plays at 2, threatening the three black stones (called Ko threat). If black plays at 3 to connect, white can then win back the Ko. (c) Ladder. Black plays at 1, threatening to capture the white stone at circle. White escapes but eventually gets captured at the border. Each time after black plays, white’s liberties shrink from 2 to 1. Images from Sensei’s Library (http://senseis.xmp.net/).  In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 72.6%, Fuego: 12.5% vs 89.7%) than current state-of-the-art DCNN-based player [Maddi- son et al. (2015)]. In addition, our search-less bot darkfores2 played ranked game on KGS Go Server and achieves stable 3d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. Combining DCNN with MCTS, our hybrid bot darkfmcts3 addresses such issues. With 5000 rollouts, it beats Pachi with 10k rollouts in all 250 games (100% win rate); with 75k rollouts it achieves a stable 5d level in KGS Go server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone); with 110k rollouts, it won the 3rd place in January KGS Go Tournament. Recently, DeepMind’s AlphaGo [Silver et al. (2016)] defeated European Go Champion Fan Hui with 5-0, showing the strong power of DCNN-based bot.  2 METHOD  Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al. (1994); Enzenberger (1996)]. Recent progress [Maddison et al. (2015); Clark & Storkey (2015)] uses Deep Convolutional Neural Network (DCNN) for move prediction, and shows substantial im- provement over shallow networks or linear function approximators based on manually designed features or simple patterns extracted from previous games [Silver (2009)]. In this paper, we train a DCNN that predicts the next k moves given the current board situation as an input. We treat the 19 × 19 board as a 19 × 19 image with multiple channels. Each channel encodes a different aspect of board information, e.g., liberties (Fig. 1(b)). Compared to previous works, we use a more compact feature set and predict long-term moves, and show that they lead to a substantial performance boost in terms of win rate against open source engines.  2  30k	  1k	  1d	  7d	  1p	  9p	  (b)(a)(d)B	  W	  (c)(a)	(b)	(c)	Published as a conference paper at ICLR 2016  Name  our/opponent liberties  Ko (See Fig. 2(a))  our/opponent stones/empty  our/opponent history  opponent rank  border  position mask  our/opponent territory  Type binary binary binary real binary binary real binary  standard  extended  Description  true if the group has 1, 2 and ≥ 3 liberties true if it is a Ko location (illegal move)  -  how long our/opponent stone is placed  All true if opponent is at that rank  true if at border  exp(−.5 ∗ distance2) to the board center true if the location is closer to us/opponent.  #planes  6 1 3 2 9 1 1 2  Table 1: Features extracted from the current board situation as the input of the network. Note that extended feature set also includes standard set. As a result, standard set has 21 channels while extended one has 25 channels.  Figure 3: Our network structure (d = 12, w = 384). The input is the current board situation (with history information), the output is to predict next k moves.  2.1 FEATURE CHANNELS Table 1 shows the features extracted from the current board situation. Each feature is a binary 19×19 map except for history information and position mask, which are real numbers in [0, 1]. History is encoded as exp(−t ∗ 0.1), where t is how long the stone has been placed. The exponential temporal decay is meant to enable the network to focus on the recent battle. Position mark is deﬁned as exp(− 1 2 l2), where l2 is the squared L2 distance to the board center. It is used to encode the relative position of each intersection. There are two differences between our features and those in Maddison et al. (2015). First, we use relative coding (our/opponent) for almost all the features. In contrast, the features in Maddison et al. (2015) are largely player-agnostic. Second, our feature set is simpler and compact (25 vs. 36 input planes), in particular, free from one step forward simulation. In comparison, Maddison et al. (2015) uses such features like liberties after the move, captures after the move, etc. We use a similar way to encode rank in 9 planes as in Maddison et al. (2015). That is, all kyu-players have all nine planes zero, 1d players has their ﬁrst plane all-1, 2d players have their second plane all-1, etc. For 9d and professional players, all the planes are ﬁlled with 1.  2.2 NETWORK ARCHITECTURE  Fig. 3 shows the architecture of the network for our best model. We use a 12-layered (d = 12) full convolutional network. Each convolution layer is followed by a ReLU nonlinearity. Except for the ﬁrst layer, all layers use the same width w = 384. No weight sharing is used. We do not use pooling since they negatively affect the performance. Instead of using two softmax outputs [Maddison et al. (2015)] to predict black and white moves, we only use one softmax layer to predict the next move, reducing the number of parameters.  2.3 LONG TERM PLANNING  Predicting only the immediate next move limits the information received by the lower layers. In- stead, we predict next k moves (self and opponent, alternatively) from the current board situation. Each move is a separate softmax output. The motivation is two-fold. First, we want our network to focus on a strategic plan rather than the immediate next move. Second, with multiple softmax  3  25 feature planesConv layer92 channels5 × 5 kernelConv layers x 10384 channels3 × 3 kernelConv layerk maps3 × 3 kernelk parallel softmaxx	  10	  Our next move (next-1)Opponent move (next-2)Our counter move (next-3)Current boardPublished as a conference paper at ICLR 2016  outputs, we expect to have more supervisions to train the network. Table 2 computes the ratio of average gradient L2 norm (over the ﬁrst 9 epochs, ﬁrst 1000 mini-batches removed) between 1-step and 3-step predictions at each convolutional layer. As expected, the gradient magnitudes of the top layers (layers closer to softmax) are higher in 3-step prediction. However, the gradient magnitudes of the lower layers are approximately the same, showing that the lower gradients are canceled out in 3-step prediction, presumably leaving only the most important gradient for training. Empirically, DCNN trained with 3 steps gives high win rate than that with 1 step.  layer  gradient norm ratio  conv1 1.18  conv3 1.20  conv5 1.37  conv7 1.46  conv9 1.98  conv11 2.41  Table 2: Comparison in gradient L2 norm between 1-step prediction and 3-step prediction network.  2.4 TRAINING  When training, we use 16 CPU threads to prepare the minibatch, each simulating 300 random se- lected games from the dataset. In each minibatch, for each thread, randomly select one game out of 300, simulate one step according to the game record, and extract features and next k moves as the input/output pair in the batch. If the game has ended (or fewer than k moves are left), we randomly pick one (with replacement) from the training set and continue. The batch size is 256. We use data augmentation with rotation at 90-degree intervals and horizontal/vertical ﬂipping. For each board situation, data augmentation could generate up to 8 different situations. Before training, we randomly initialize games into different stages. This ensures that each batch contains situations corresponding to different stages of games. Without this, the network will quickly overﬁt and get trapped into poor local minima. Because of our training style, it is not clear when the training set has been thoroughly processed once. Therefore, we just deﬁne an epoch as 10,000 mini-batches. Unlike Maddison et al. (2015) that uses asynchronous stochastic gradient descent, we just use vanilla SGD on 4 NVidia K40m GPUs in a single machine to train the entire network (for some models we use 3 GPUs with 255 as the batch size). Each epoch lasts about 5 to 6 hours. The learning rate is initially 0.05 and then divided by 5 when convergence stalls. Typically, the model starts to converge within one epoch and shows good performance after 50-60 epochs (around two weeks). Other than simplest DCNN model, we also tried training with ResNet [He et al. (2015)] which recently gives state-of-the-art performance in image classiﬁcation. We also tried using additional targets, such as predicting the endgame territories given the current board status. Both gives faster convergence. Recurrent Neural Network is also tried but gives worse performance.  2.5 MONTE CARLO TREE SEARCH  From the experiments, we clearly show that DCNN is tactically weak due to the lack of search. Search is a way to explore the solution space conditioned on the current board situation, and build a non-parametric local model for the game. The local model is more ﬂexible than the global model learned from massive training data and more adapted to the current situation. The state-of-the-art approach in computer Go is Monte-Carlo Tree Search (MCTS). Fig. 4 shows its basic principle. Combining DCNN with MCTS requires nontrivial engineering efforts because each rollout of MCTS is way much faster than DCNN evaluation. Therefore, these two must run in parallel with frequent communications. Our basic implementation of MCTS gives 16k rollouts per second (for 16 threads on a machine with Intel Xeon CPU E5-2680 v2 at 2.80GHz) while it typically takes 0.2s for DCNN to give board evaluations of a batch size of 128 with 4 GPUs. There are two ways to address this problem. In asynchronized implementation used in Maddison et al. (2015), MCTS sends the newly expanded node to DCNN but is not blocked by DCNN evalu- ation. MCTS will use its own tree policy until DCNN evaluation arrives and updates moves. This gives high rollout rate, but there is a time lag for the DCNN evaluation to take effect, and it is not clear how many board situations have been evaluated for a given number of MCTS rollouts. In syn- chronized implementation, MCTS will wait until DCNN evaluates the board situation of a leaf node, and then expands the leaf. Default policy may run before/after DCNN evaluation. This is much slower but guarantees that each node is expanded using DCNN’s high-quality suggestions.  4  Published as a conference paper at ICLR 2016  Figure 4: A brief illustration of MCTS with DCNN. (a) A game tree. For each node, the statistics m/n indicates that from the node, n games are emulated, out of which m are won by black. Root represents the current game state. (b) A new rollout starting from the root. It picks a move from the current state using tree policy and advances to the next game state, until it picks the a new move and expand a new leaf. From the leaf, we run default policy until the game ends (black wins in the illustration). At the same time, the leaf status is sent to a DCNN server for evaluation. For synchronized implementation, this new node is available for tree policy after the evaluation is returned. (c) The statistics along the trajectory of the tree policy is updated accordingly.  In our experiments, we evaluate the synchronized case, which achieves 84.8% win rate against its raw DCNN player with only 1000 rollouts. Note that our implementation is not directly comparable to the asynchronized version in Maddison et al. (2015), achieving 86.7% with 100k rollouts. In their recent AlphaGo system [Silver et al. (2016)], a fast CPU-based tree policy is used before DCNN evaluation arrives. Combined with their default policy that gives a move suggestion in 2µ s with 24.2% accuracyon a Tygem dataset, the CPU-only system already achieves 3d level. We also try implementing it using similar local patterns and achieves slight higher accuracy (25.1% evaluated on GoGoD) with 4-5 µ s per move. However, when combined with our system, its performance is not as good as Pachi’s rule-based default policy, showing that top-1 accuracy may not be a sensitive metric to use. Indeed, some moves are much more critical than others.  3 EXPERIMENTS  3.1 SETUP We use the public KGS dataset (∼170k games), which is used in Maddison et al. (2015). We use all games before 2012 as the training set and 2013-2015 games as the test set. This leads to 144,748 games for training and 26,814 games for testing. We also use GoGoD dataset1 (∼80k games), which is also used in Clark & Storkey (2015). 75,172 games are used for training and 2,592 for testing. For evaluation, our model competes with GnuGo, Pachi [Baudis & Gailly (2012)] and Fuego [En- zenberger et al. (2010)]. We use GnuGo 3.8 level 10, Pachi 11.99 (Genjo-devel) with the pattern ﬁles, and Fuego 1.1 throughout our experiments.  3.2 MOVE PREDICTION  Table 3 shows the performance comparison for move prediction. For models that predict the next k moves, we only evaluate their prediction accuracy for the immediate next move, i.e., the ﬁrst move the model predicts.  Maddison et al. (2015)  d=12,w=384  d=12,w=512  d=16,w=512  d=17,w=512  55.2  57.1  57.3  56.6  56.4  Table 3: Comparison of Top-1 accuracies for immediate move predictions using standard features. d is the model depth while w is the number of ﬁlters at convolutional layers (except the ﬁrst layer).  With our training framework, we are able to achieve slightly higher Top-1 prediction accuracy of im- mediate next move (after hundreds of epochs) compared to Maddison et al. (2015). Note that using  1We used GoGoD 2015 summer version, purchased from http://www.gogod.co.uk. We skip ancient  games and only use game records after 1800 AD.  5  2/10	2/10	2/10	1/1	20/30	10/18	9/10	10/12	1/8	22/40	1/1	2/10	2/10	1/1	20/30	10/18	9/10	10/12	1/8	Synced DCNN server22/40	2/10	1/1	21/31	11/19	10/11	10/12	1/8	23/41	1/1	(a)(b)(c)Tree  policy	Default  policy	Published as a conference paper at ICLR 2016  Figure 5: Top-1 accuracy of the immediate move prediction with k = 1, 2 and 3 next move predic- tions.  Figure 6: Evolution of win rate versus Pachi 10k. Each win rate is computed from 300 games.  standard or extended features seem to have marginal gains (Fig. 5). For the remaining experiments, we thus use d = 12 and w = 384, as shown in Fig. 3.  3.3 WIN RATE  Although the improvement in move prediction accuracy is small, the improvement in play strength, in terms of win rate, is much larger. Fig. 6 shows the improvement of win rate over time. Our DCNN trained with 2 or 3 steps is about 10%− 15% (in absolute difference) better than DCNN trained with 1 step. More steps show diminishing returns. On the other hand, the win rate of the standard feature set is comparable to the extended one. Table 4 shows that win rate of our approach is substantially higher than that of previous works. We also train a smaller model with w = 144 whose number of parameters are comparable to Maddison et al. (2015). Our smaller model achieves 43.3% in 300 games against Pachi 10k when Pachi’s pondering is on (keep searching when the opponent plays), and 55.7% when it is off. In contrast, Maddison et al. (2015) reports 47.4% and does not mention pondering status. Darkforest AI Bots. We build three bots from the trained models. Our ﬁrst bot darkforest is trained using standard features, 1 step prediction on KGS dataset. The second bot darkfores1 is trained using extended features, 3 step prediction on GoGoD dataset. Both bots are trained with constant learning rate 0.05. Based on darkfores1, we ﬁne-tuned the learning rate to create an even stronger DCNN player, darkfores2. Note that ﬁne-tuning KGS model achieves comparable strength. Table 4 shows their strengths against open source engines. It seems that despite the fact that GoGoD is smaller, our model can be trained faster with better performance, presumably because GoGoD contains professional games, while KGS games are from amateurs and hence a bit noisy. Win rates among the three bots (Table 5) are consistent with their performances against open source engines.  3We also test darkfores2 against Fuego under this setting, and its win rate is 93% ± 1%.  6  epoch020406080Top-1 accuracy0.40.450.50.550.6feature type: standardnstep=1nstep=2nstep=3epoch020406080Top-1 accuracy0.40.450.50.550.6feature type: extendednstep=1nstep=2nstep=3epoch203040506070winrate against Pachi 10k0.50.60.70.80.91feature type: standardnstep=1nstep=2nstep=3epoch203040506070winrate against Pachi 10k0.50.60.70.80.91feature type: extendednstep=1nstep=2nstep=3Published as a conference paper at ICLR 2016  GnuGo (level 10)  Pachi 10k  Pachi 100k  Fuego 10k  Fuego 100k  Clark & Storkey (2015) Maddison et al. (2015)  darkforest darkfores1 darkfores2  91.0 97.2  98.0 ± 1.0 99.7 ± 0.3 100 ± 0.0  -  47.4  71.5 ± 2.1 88.7 ± 2.1 94.3 ± 1.7  -  11.0  27.3 ± 3.0 59.0 ± 3.3 72.6 ± 1.9  14.0  23.3  84.5 ± 1.5 93.2 ± 1.5 98.5 ± 0.1  12.5  56.7 ± 2.5 78.0 ± 1.7 89.7 ± 2.1  Table 4: Win rate comparison against open source engines between our model and previous works. For each setting, 3 groups of 100 games are played. We report the average win rate and standard deviation computed from group averages. All the game experiments mentioned in this paper use komi 7.5 and Chinese rules. Pondering (keep searching when the opponent is thinking) in Pachi and Fuego are on. Note that in Clark & Storkey (2015), they control the time per move as 10 sec/move on 2x 1.6 GHz cores, instead of ﬁxing the rollout number.3  Move sampled from Top-5  darkforest  darkfores1  darkfores2  Move sampled from Top-300  darkforest  darkfores1  darkfores2  darkforest darkfores1 darkfores2  49% 70% 85%  27% 47% 55%  17% 36% 48%  49% 69% 80%  25% 48% 59%  15% 34% 47%  Table 5: Win rate among three bots. Each pair plays 100 games. Rows play black. Moves drawn from the DCNN softmax probability.  We also compare darkforest with a public DCNN model4. To create diverse games, moves are sampled according to DCNN softmax probability. We played two sets of 100 games with 100% and 99% win rate. Darkforest always wins if sampling from top-1/top-5 moves. Performance against humans. We put our bots onto KGS Go server and check their performance against humans over ﬁve months period. Darkforest became publicly available on Aug 31, 2015. Since then it has played about 2000 games. Recently we also release the improved version dark- fores1 on Nov 2, 2015, and darkfores2 after ICLR deadline. All bots become ranked since late November 2015. To score the endgame board situations, we randomly run 1000 trials of default policy to ﬁnd the dead stones, followed by standard Tromp-Taylor scoring. If all 1000 trials show losing by 10+ points, they resign. All the three pure DCNN bots are quite popular on KGS Go server, playing around 100 games a day. Once ranked, darkforest achieves 1k-1d and darkfores1 is on strong 2d level, showing the strength of next 3 predictions, consistent with the estimations using free games played in KGS (See Table 6). The ﬁne-tuned version, darkfores2, is on stable 3d level, a very impressive result as pure DCNN models. It even beats a 6d 3 games in a row. We notice that once we open handicap games, their win rates become higher. This is a major improvement upon DCNN developed in Clark & Storkey (2015) that holds 4k-5k level, estimated by playing against Go engines. Fig. 7 shows one example game between darkfores1 and a KGS 1d human player. Overall, our bots have a very good understanding of global board situations, and tend to play “good shapes” but occasionally fail under local basic life/death situations, e.g., not making a large con- nected group alive during capturing race, or failing to make two eyes. Rarely they lost in ladder capture, a special case in Go (Fig. 2(c)) in which one keeps chasing the opponent’s stones until the board’s border and kill them. Apparently the network failed to capture the ladder pattern due to its rarity in actual games. To handle this, a separate simple search module is added.  darkforest  darkfores1  win rate win/total win rate win/total  <10k 10k - 6k 100% 98.3% 78/78 473/481 100% 99.1% 17/17 218/220  5k - 2k 90.5% 620/685 97.2% 345/355  1k  1d  2d  3d  70.4% 55.8% 50.0% 47.1% 57/81 32/68 87.0% 82.5% 69.0% 62.1% 60/69 18/29  63/113  24/48  47/57  20/29  unranked 86.4% 561/649 91.1% 357/392  Table 6: Performance breakdown against different level of players on KGS Go server.  4From http://physik.de/net.tgz by Detlef Schmicker. He released the model in Computer-Go fo- rum. See http://computer-go.org/pipermail/computer-go/2015-April/007573.html  7  Published as a conference paper at ICLR 2016  Figure 7: (Left): Example game between darkfores1 (white) and a KGS 1d player gugun (black). Move 274 shows the understanding of Ko ﬁght in darkfores1. Black has to react in the lower left corner to reinforce the group. In trade, white wins the Ko ﬁght on the top right corner. The result of this game is white win by 7.5 (komi 7.5, Chinese rule). (Right): Example game between darkforest (white) and darkforest+MCTS (1000 rollout). Darkforest resigned. For concise illustration, we truncated the game when the estimated win rate by MCTS exceeds 0.9.  3.4 COMBINATION WITH MONTE CARLO TREE SEARCH (MCTS)  We build a standard MCTS framework and study the performance of DCNN+MCTS. Tree policy: Moves are ﬁrst sorted by DCNN conﬁdences, and then picked in order until the accumulated proba- bility exceeds 0.8, or the maximum number of top moves are reached. Then we use UCT [Browne et al. (2012)] to select moves for tree expansion. Note that DCNN conﬁdences are not used in UCT. Noise uniformly distributed in [0, σ] is added to the win rate to enforce that search threads quickly diverge and not locked into the same node waiting for DCNN evaluation. This speeds up the tree search tremendously (σ = 0.05 thoroughout the experiments). Default policy: Following Pachi’s implementation [Baudis & Gailly (2012)], we use 3x3 patterns, opponent atari points, detection of nakade points and avoidance of self-atari for default policy. Note that Pachi’s complete default pol- icy yields slightly better performance. Due to the non-deterministic nature of multi-threading, the game between DCNN+MCTS and DCNN is always different for each trial. Versus Pure DCNN. In Table 7, Darkforest+MCTS gives the highest performance boost over dark- forest, while boost on darkfores1 and darkfores2 is smaller5. This indicates that MCTS mitigates the weakness of DCNN, in particular for the weaker engine. Another interesting observation is that performance becomes higher if we consider fewer top moves in the search. This shows that (1) the top moves from DCNN are really high quality moves and (2) MCTS works better if it digs deep into promising paths. Interestingly, while MCTS with top-2 gives even better performance against pure DCNN, its performance is worse on our KGS version. Finally, setting the minimal number of choices to be more than 1, hurts the performance tremendously. Versus Pachi 10k. With only 1000 rollouts, the win rate improvement over pure DCNN model is huge, in particular for weak models. 5000 rollouts make the performance even better. In particular, darkforest2+MCTS overwhelms Pachi 10k with 100% win rate. Note that for these experiments, Pachi’s pondering is turned off (not search during the opponent round). Comparison with previous works. In comparison, an asynchronized version is used in Maddison et al. (2015) that achieves 86.7% with 100k rollouts, with faster CPU and GPU (Intel Xeon E5-2643  5We ﬁxed a few bugs from verson 1 of the paper and the baseline win rate versus Pachi/Fuego increases.  8  1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152172182192202212222232242252262272282292302312322342352372402422432442452462472482492502512522532542552562572582592602612622632642652662672682692712722732742751014910265216213233229236230238231239229241227270173276272277P278P2015-11-12 (KGS Go Server)W+13.5(komi: 7.5)gugun (1d)darkfores1 (?)Powered by TCPDF (www.tcpdf.org)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916116316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724925125225325425525625725825926026126226326426526626726826916014616215720619520750248228250105269end ()B+Resign(komi: 7.5)go_player_v2_mcts (?)go_player_v2 (?)Powered by TCPDF (www.tcpdf.org)Published as a conference paper at ICLR 2016  darkforest+MCTS  darkfores1+MCTS  darkfores2+MCTS  Vs pure DCNN (1000rl/top-20) Vs pure DCNN (1000rl/top-5) Vs pure DCNN (1000rl/top-3) Vs pure DCNN (5000rl/top-5)  Vs Pachi 10k (pure DCNN baseline)  84.8% 89.6% 91.6% 96.8% 71.5%  74.0% 76.4% 89.6% 94.3% 88.7%  62.8% 68.4% 79.2% 82.3% 94.3%  Vs Pachi 10k (1000rl/top-20) Vs Pachi 10k (1000rl/top-5) Vs Pachi 10k (1000rl/top-3) Vs Pachi 10k (5000/top-5)  91.2% (+19.7%) 88.4% (+16.9%) 95.2% (+23.7%)  98.4%  92.0% (+3.3%) 94.4% (+5.7%) 98.4% (+9.7%)  99.6%  95.2% (+0.9%) 97.6% (+3.3%) 99.2% (+4.9%)  100.0%  Table 7: Win rate of DCNN+MCTS against pure DCNN (200 games) and Pachi 10k (250 games).  v2 at 3.50GHz and GeForce GTX Titan Black). The two numbers are not directly comparable since (1) in asynchronized implementations, the number of game states sent to DCNN for evaluation is unknown during 100k rollouts, and (2) Table 7 shows that stronger DCNN model beneﬁts less when combined with MCTS. Section 2.5 gives a detailed comparison between the two implementations. Evaluation on KGS Go server. The distributed version, named darkfmcts3 in KGS Go Server, use darkfores2 as the underlying DCNN model, runs 75, 000 rollouts on 2048 threads and produces a move every 13 seconds with one Intel Xeon E5-2680 v2 at 2.80GHz and 44 NVidia K40m GPUs. It uses top-3 predictions in the ﬁrst 140 moves and switched to top-5 afterwards so that MCTS could have more choices. Pondering is used. Dynamic komi is used only for high handicap games (≥H5). darkfmcts3 now holds a stable KGS 5d level, on par with the top Go AIs except for AlphaGo [Silver et al. (2016)], has beaten Zen19 once and hold 1win/1lose against a Korean 6p professional player with 4 handicaps. A version with 110k rollouts and 64 GPUs has won the 3rd place in January KGS Computer Go tournament, where Zen and DolBaram took the 1st and 2nd place6, and Abacus [Graf & Platzner (2015)] took the 4th. With 5000 rollouts the bot can be run on a single machine with 4 GPUs with 8.8s per move. Weakness. Despite the involvement of MCTS, a few weakness remains. (1) The top-3/5 moves of DCNN might not contain a critical local move to save/kill the local self/enemy group so local tactics remain weak. Sometimes the bot plays tenuki (“move elsewhere”) pointlessly when a tight local battle is needed. (2) DCNN tends to give high conﬁdences for ko moves even they are useless. This enables DCNN to play single ko ﬁghts decently, by following the pattern of playing the ko, playing ko threats and playing the ko again. But it also gets confused in the presence of double ko. (3) When the bot is losing, it plays bad moves like other MCTS bots and loses more.  4 CONCLUSION AND FUTURE WORK  In this paper, we have substantially improved the performance of DCNN-based Go AI, extensively evaluated it against both open source engines and strong amateur human players, and shown its potentials if combined with Monte-Carlo Tree Search (MCTS). Ideally, we want to construct a system that combines both pattern matching and search, and can be trained jointly in an online fashion. Pattern matching with DCNN is good at global board reading, but might fail to capture special local situations. On the other hand, search is excellent in model- ing arbitrary situations, by building a local non-parametric model for the current state, only when the computation cost is affordable. One paradigm is to update DCNN weights (i.e., Policy Gradi- ent [Sutton et al. (1999)]) after MCTS completes and chooses a different best move than DCNN’s proposal. To increase the signal bandwidth, we could also update weights using all the board situa- tions along the trajectory of the best move. Alternatively, we could update the weights when MCTS is running. Actor-Critics algorithms [Konda & Tsitsiklis (1999)] can also be used to train two mod- els simultaneously, one to predict the next move (actor) and the other to evaluate the current board situation (critic). Finally, local tactics training (e.g., Life/Death practice) focuses on local board situation with fewer variations, which DCNN approaches should beneﬁt from like human players.  Acknowledgement We thank Tudor Bosman for building distributed systems, Rob Fergus and Keith Adams for constructive suggestions, and Vincent Cheung for engineering help.  6darkfmcts3 lost a won game to Zen due to a time management bug, otherwise it would have won 1st place.  9  Published as a conference paper at ICLR 2016  REFERENCES Baudis, Petr and Gailly, Jean-loup. Pachi: State of the art open source go program. pp. 2438, 2012.  Browne, Cameron B, Powley, Edward, Whitehouse, Daniel, Lucas, Simon M, Cowling, Peter, Rohlf- shagen, Philipp, Tavener, Stephen, Perez, Diego, Samothrakis, Spyridon, Colton, Simon, et al. A survey of monte carlo tree search methods. Computational Intelligence and AI in Games, IEEE Transactions on, 4(1):1–43, 2012.  Clark, Christopher and Storkey, Amos. Training deep convolutional neural networks to play go. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1766– 1774, 2015.  Enzenberger, Markus. The integration of a priori knowledge into a go playing neural network. URL:  http://www. markus-enzenberger. de/neurogo. html, 1996.  Enzenberger, Markus, M¨uller, Martin, Arneson, Broderick, and Segal, Richard. Fuegoan open- source framework for board games and go engine based on monte carlo tree search. Computa- tional Intelligence and AI in Games, IEEE Transactions on, 2(4):259–270, 2010.  Graf, Tobias and Platzner, Marco. Adaptive playouts in monte-carlo tree search with policy-gradient  reinforcement learning. In Advances in Computer Games, pp. 1–11. Springer, 2015.  He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image  recognition. arXiv preprint arXiv:1512.03385, 2015.  Kocsis, Levente and Szepesv´ari, Csaba. Bandit based monte-carlo planning. In Machine Learning:  ECML 2006, pp. 282–293. Springer, 2006.  Konda, Vijay R and Tsitsiklis, John N. Actor-critic algorithms. In NIPS, volume 13, pp. 1008–1014,  1999.  Maddison, Chris J, Huang, Aja, Sutskever, Ilya, and Silver, David. Move evaluation in go using  deep convolutional neural networks. 2015.  Richards, Norman, Moriarty, David E, and Miikkulainen, Risto. Evolving neural networks to play  go. Applied Intelligence, 8(1):85–96, 1998.  Schraudolph, Nicol N, Dayan, Peter, and Sejnowski, Terrence J. Temporal difference learning of position evaluation in the game of go. Advances in Neural Information Processing Systems, pp. 817–817, 1994.  Silver, David. Reinforcement learning and simulation-based search. Doctor of philosophy, Univer-  sity of Alberta, 2009.  Silver, David, Huang, Aja, Maddison, Chris J., Guez, Arthur, Sifre, Laurent, van den Driessche, George, Schrittwieser, Julian, Antonoglou, Ioannis, Panneershelvam, Veda, Lanctot, Marc, Diele- man, Sander, Grewe, Dominik, Nham, John, Kalchbrenner, Nal, Sutskever, Ilya, Lillicrap, Tim- othy, Leach, Madeleine, Kavukcuoglu, Koray, Graepel, Thore, and Hassabis, Demis. Mastering the game of go with deep neural networks and tree search. Nature, 2016.  Sutskever, Ilya and Nair, Vinod. Mimicking go experts with convolutional neural networks.  Artiﬁcial Neural Networks-ICANN 2008, pp. 101–110. Springer, 2008.  In  Sutton, Richard S, McAllester, David A, Singh, Satinder P, Mansour, Yishay, et al. Policy gradient methods for reinforcement learning with function approximation. In NIPS, volume 99, pp. 1057– 1063. Citeseer, 1999.  10  ",
1507.00677,2016,Distributional Smoothing with Virtual Adversarial Training,"['Distributional Smoothing with Virtual Adversarial Training [code]\nTakeru Miyato', 'Shin-ichi Maeda', 'Masanori Koyama', 'Ken Nakae', 'Shin Ishii']",https://arxiv.org/pdf/1507.00677,"6 1 0 2     n u J    1 1      ] L M  . t a t s [      9 v 7 7 6 0 0  .  7 0 5 1 : v i X r a  Published as a conference paper at ICLR 2016  DISTRIBUTIONAL SMOOTHING WITH VIRTUAL ADVERSARIAL TRAINING  Takeru Miyato1, Shin-ichi Maeda1, Masanori Koyama1, Ken Nakae1 & Shin Ishii2 Graduate School of Informatics Kyoto University Yoshidahonmachi 36-1, Sakyo, Kyoto, Japan 1{miyato-t,ichi,koyama-m,nakae-k}@sys.i.kyoto-u.ac.jp 2ishii@i.kyoto-u.ac.jp  ABSTRACT  We propose local distributional smoothness (LDS), a new notion of smoothness for statistical model that can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS based regularization as virtual adversarial training (VAT). The LDS of a model at an input datapoint is deﬁned as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural network, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and conﬁrmed our method’s superior perfor- mance over the current state of the art semi-supervised method applied to these datasets.  1  INTRODUCTION  Overﬁtting is a serious problem in supervised training of classiﬁcation and regression functions. When the number of training samples is ﬁnite, training error computed from empirical distribution of the training samples is bound to be different from the test error, which is the expectation of the log-likelihood with respect to the true underlying probability measure (Akaike, 1998; Watanabe, 2009). One popular countermeasure against overﬁtting is addition of a regularization term to the objective function. The introduction of the regularization term makes the optimal parameter for the objective function to be less dependent on the likelihood term. In Bayesian framework, the regularization term corresponds to the logarithm of the prior distribution of the parameters, which deﬁnes the preference of the model distribution. Of course, there is no universally good model distribution, and the good model distribution should be chosen dependent on the problem we tackle. Still yet, our experiences often dictate that the outputs of good models should be smooth with respect to inputs. For example, images and time series occurring in nature tend to be smooth with respect to the space and time (Wahba, 1990). We would therefore invent a novel regularization term called local distributional smoothness (LDS), which rewards the smoothness of the model distribution with respect to the input around every input datapoint, We deﬁne LDS as the negative of the sensitivity of the model distribution p(y|x, θ) with respect to the perturbation of x, measured in the sense of KL divergence. The objective function based on this regularization term is therefore the log likelihood of the dataset augmented with the sum of LDS computed at every input datapoint in the dataset. Because LDS is measuring the local smoothness of the model distribution itself, the regularization term is parametrization invariant. More precisely, our  1  Published as a conference paper at ICLR 2016  regularized objective function T satisﬁes the natural property that, if the θ∗ = arg maxθ T (θ), then f (θ∗) = arg maxθ T (f (θ)) for any diffeomorphism f. Therefore, regardless of the parametrization, the optimal model distribution trained with LDS regularization is unique. This is a property that is not enjoyed by the popular Lq regularization (Friedman et al., 2001). Also, for sophisticated models like deep neural network (Bengio, 2009; LeCun et al., 2015), it is not a simple task to assess the effect of the Lq regularization term on the topology of the model distribution. Our work is closely related to adversarial training (Goodfellow et al., 2015). At each step of the train- ing, Goodfellow et al. identiﬁed for each pair of the observed input x and its label y the direction of the input perturbation to which the classiﬁer’s label assignment of x is most sensitive. Goodfel- low et al. then penalized the model’s sensitivity with respect to the perturbation in the adversarial direction. On the other hand, our LDS deﬁned without label information. Using the language of adversarial training, LDS at each point is therefore measuring the robustness of the model against the perturbation in local and ‘virtual’ adversarial direction. We therefore refer to our regularization method as virtual adversarial training (VAT). Because LDS does not require the label information, VAT is also applicable to semi-supervised learning. This is not the case for adversarial training. Furthermore, with the second order Taylor expansion of the LDS and an application of power method, we made it possible to approximate the gradient of the LDS efﬁciently. The approxi- mated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. We summarize the advantages of our method below:  • Applicability to both supervised and semi-supervised training. • At most two hyperparameters. • Parametrization invariant formulation. The performance of the method is invariant under • Low computational cost. For Neural network in particular, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations.  reparatrization of the model.  When we applied the VAT to the supervised and semi-supervised learning of the permutation invari- ant task for the MNIST dataset, our method outperformed all the contemporary methods other than the state of the art method (Rasmus et al., 2015) that uses a highly advanced generative model based method. We also applied our method for semi-supervised learning of permutation invariant task for the SVHN and NORB dataset, and conﬁrmed our method’s superior performance over the current state of the art semi-supervised method applied to these datasets.  2 METHODS  2.1 FORMALIZATION OF LOCAL DISTRIBUTIONAL SMOOTHNESS  We begin with the formal deﬁnition of the local distributional smoothness. Let us ﬁx θ for now, suppose the input space (cid:60)I, the output space Q, and a training samples  D = {(x(n), y(n))|x(n) ∈ (cid:60)I , y(n) ∈ Q, n = 1,··· , N},  and consider the problem of using D to train the model distribution p(y|x, θ) parametrized by θ. Let KL[p||q] denote the KL divergence between the distributions p and q. Also, with the hyperparameter (cid:15) > 0, we deﬁne  ∆KL(r, x(n), θ) ≡ KL[p(y|x(n), θ)(cid:107)p(y|x(n) + r, θ)]  v-adv ≡ arg max r(n)  r  {∆KL(r, x(n), θ); (cid:107)r(cid:107)2 ≤ (cid:15)}.  (1) (2)  v-adv as the virtual adversarial perturbation. We deﬁne the local distribu-  From now on, we refer to r(n) tional smoothing (LDS) of the model distribution at x(n) by LDS(x(n), θ) ≡ −∆KL(r(n)  (3) v-adv is the direction to which the model distribution p(y|x(n), θ) is most sensitive in the In a way, this is a KL divergence analogue of the gradient ∇x of the  Note r(n) sense of KL divergence.  v-adv, x(n), θ).  2  Published as a conference paper at ICLR 2016  model distribution with respect to the input, and perturbation of x in this direction wrecks the local smoothness of p(y|x(n), θ) at x(n) in a most dire way. The smaller the value of ∆KL(r(n) v-adv, x(n), θ) at x(n), the smoother the p(y|x(n), θ) at x. Our goal is to improve the smoothness of the model in the neighborhood of all the observed inputs. Formulating this goal based on the LDS, we obtain the following objective function,  N(cid:88)  n=1  1 N  N(cid:88)  n=1  log p(y(n)|x(n), θ) + λ  1 N  LDS(x(n), θ).  (4)  adv ≡ arg min v-adv, x(n), θ) in (3) with log p(y(n)|x(n) + r(n)  We call the training based on (4) the virtual adversarial training (VAT). By the construction, VAT is {p(y(n)|x(n) + parametrized by the hyperparameters λ > 0 and (cid:15) > 0. If we deﬁne r(n) r, θ),(cid:107)r(cid:107)p ≤ (cid:15)} and replace −∆KL(r(n) adv, θ), we obtain the objective function of the adversarial training (Goodfellow et al., 2015). Perturbation of x(n) in the direction of r(n) adv can most severely damage the probability that the model correctly assigns the label y(n) to x(n). As opposed to r(n) v-adv on x(n) does not require the correct label y(n). This property allows us to apply the VAT to semi-supervised learning. LDS is a deﬁnition meant to be applied to any model with distribution that are smooth with respect to x. For instance, for a linear regression model p(y|x, θ) = N (θTx, σ2) the LDS becomes  adv, the deﬁnition of r(n)  r  LDS(x, θ) = − 1  2σ2 (cid:15)2(cid:107)θ(cid:107)2  2,  and this is the same as the form of L2 regularization. This does not, however, mean that L2 regular- ization and LDS is equivalent for linear models. It is not difﬁcult to see that, when we reparametrize the model as p(y|x, θ) = N (θ3Tx, σ2), we obtain LDS(x, θ3) ∝ −(cid:15)2(cid:107)θ3(cid:107)2 2. For a logistic regression model p(y = 1|x, θ) = σ(θTx) = (1 + exp(−θTx))−1, we obtain  2, not −(cid:15)2(cid:107)θ(cid:107)2  σ(θTx)(cid:0)1 − σ(θTx)(cid:1)(cid:15)2(cid:107)θ(cid:107)2  2  LDS(x, θ) ∼= − 1  2  with the second-order Taylor approximation with respect to θT r.  2.2 EFFICIENT EVALUATION OF LDS AND ITS DERIVATIVE WITH RESPECT TO θ  Once r(n) v-adv is computed, the evaluation of the LDS is simply the computation of the KL divergence between the model distributions p(y|x(n), θ) and p(y|x(n) + r(n) v-adv, θ). When p(y|x(n), θ) can be approximated with well known exponential family, this computation is straightforward. For exam- ple, one can use Gaussian approximation for many cases of NNs. In what follows, we discuss the efﬁcient computation of r(n)  v-adv, for which there is no evident approximation.  2.2.1 EVALUATION OF rv-adv We assume that p(y|x, θ) is differentiable with respect to θ and x almost everywhere. Because ∆KL(r, x, θ) takes minimum value at r = 0, the differentiability assumption dictates that its ﬁrst derivative ∇r∆KL(r, x, θ)|r=0 is zero. Therefore we can take the second-order Taylor approxima- tion as  (5) where H(x, θ) is a Hessian matrix given by H(x, θ) ≡ ∇∇r∆KL(r, x, θ)|r=0. Under this approxi- mation rv-adv emerges as the ﬁrst dominant eigenvector of H(x, θ), u(x, θ), of magnitude (cid:15),  rT H(x, θ)r,  ∆KL(r, x, θ) ∼=  rv-adv(x, θ) ∼= arg max  {rT H(x, θ)r; (cid:107)r(cid:107)2 ≤ (cid:15)}  1 2  r  (6) where ¯· denotes an operator acting on arbitrary non-zero vector v that returns a unit vector in the direction of v as ¯v. Hereafter, we denote H(x, θ) and u(x, θ) as H and u, respectively.  = (cid:15)u(x, θ),  3  Published as a conference paper at ICLR 2016  The eigenvectors of the Hessian H(x, θ) require O(I 3) computational time, which becomes unfea- sibly large for high dimensional input space. We therefore resort to power iteration method (Golub & Van der Vorst, 2000) and ﬁnite difference method to approximate rv-adv. Let d be a randomly sampled unit vector. As long as d is not perpendicular to the dominant eigenvector u, the iterative calculation of  (7) will make the d converge to u. We need to do this without the direct computation of H, however. Hd can be approximated by ﬁnite difference 1  d ← Hd  Hd ∼=  ∇r∆KL(r, x, θ)|r=ξd − ∇r∆KL(r, x, θ)|r=0 ∇r∆KL(r, x, θ)|r=ξd  ξ  (8) with ξ (cid:54)= 0. In the computation above, we used the fact ∇r∆KL(r, x, θ)|r=0 = 0 again. In summary, we can approximate rv-adv with the repeated application of the following update:  =  ξ  ,  d ← ∇r∆KL(r, x, θ)|r=ξd.  (9) The approximation improves monotonically with the iteration times of the power method, Ip. Most notably, the value ∇r∆KL(r + ξd, x, θ)|r=0 can be computed easily by the back propa- gation method in the case of neural networks. We denote the approximated r(n) v-adv = GenVAP(θ, x(n), (cid:15), Ip, ξ) (See Algorithm 1), and denote the LDS computed with ˜r(n)  v-adv as ˜r(n)  v-adv as  (cid:103)LDS(x(n), θ) ≡ −∆KL(˜r(n)  v-adv, x(n), θ).  (10)  Algorithm 1 Generation of ˜r(n)  v-adv  Function GenVAP(θ,x(n),(cid:15),Ip,ξ)  1. Initialize d ∈ RI by a random unit vector. 2. Repeat For i in 1 . . . Ip (Perform Ip-times power method)  d ← ∇r∆KL(r, x(n), θ)|r=ξd  3. Return (cid:15)d  with respect to θ at θ = ˆθ, or  (cid:103)LDS(x(n), θ)  ∂ ∂θ  2.2.2 EVALUATION OF DERIVATIVE OF APPROXIMATED LDS W.R.T θ  Let ˆθ be the current value of θ in the algorithm. It remains to evaluate the derivative of (cid:103)LDS(x(n), θ)  (cid:12)(cid:12)(cid:12)(cid:12)θ=ˆθ  = − ∂ ∂θ  KL(cid:2)p(cid:0)y|x(n), θ(cid:1)(cid:107)p(cid:0)y|x(n) + ˜r(n)  v-adv, θ(cid:1)(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)θ=ˆθ  (11) By the deﬁnition, ˜rv-adv depends on θ. Our numerical experiments, however, indicates that ∇θrv-adv is quite volatile with respect to θ, and we could not make effective regularization when we used the  numerical evaluation of ∇θ ˜rv-adv in ∇θ(cid:103)LDS(x(n), θ). We have therefore followed the work of fact achieved better generalization performance and higher (cid:103)LDS(x(n), θ). We also replaced the ﬁrst  Goodfellow et al. (2015) and ignored the derivative of ˜rv-adv with respect to θ. This modiﬁcation in  .  θ in the KL term of (11) with ˆθ and computed  KL(cid:2)p(cid:0)y|x(n), ˆθ(cid:1)(cid:107)p(cid:0)y|x(n) + ˜r(n)  v-adv, θ(cid:1)(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)θ=ˆθ  − ∂ ∂θ  .  (12)  The stochastic gradient descent based on (4) with (12) was able to achieve even better generalization performance. From now on, we refer to the training of the regularized likelihood (4) based on (12) as virtual adversarial training (VAT).  1For many models including neural networks, Hd can be computed exactly (Pearlmutter, 1994). We forgo this computation in this very paper, however, because the implementation of his procedure is not straightforward in some of the standard deeplearning frameworks (e.g. Caffe (Jia et al., 2014), Chainer (Tokui et al., 2015)).  4  Published as a conference paper at ICLR 2016  2.3 COMPUTATIONAL COST OF COMPUTING THE GRADIENT OF LDS  We would like to also comment on the computational cost required for (12). We will restrict our discussion to the case with Ip = 1 in (7) , because one iteration of power method was sufﬁcient for computing accurate Hd and increasing Ip did not have much effect in all our experiments. With the efﬁcient computation of LDS we presented above, we only need to compute ∇r∆KL(r + ξd, x(n), θ) in order to compute r(n) v-adv is decided, we compute the gradient of the LDS with respect to the parameter with (12). For neural network, the steps that we listed above require only two forward propagations and two back propagations. In semi-supervised training, we would also need to evaluate the probability distribution p(y|x(n), θ) in (1) for unlabeled samples. As long as we use the same dataset to compute the likelihood term and the LDS term, this requires only one additional forward propagation. Overall, especially for neural network, we need no more than three pairs of forward propagation and back propagation to compute the derivative approximated LDS with (12).  v-adv. Once r(n)  3 EXPERIMENTS  All the computations were conducted with Theano (Bergstra et al., 2010; Bastien et al., 2012). Reproducing code is uploaded on https://github.com/takerum/vat. Throughout the ex- periments on our proposed method, we used a ﬁxed value of λ = 1, and we also used a ﬁxed value of Ip = 1 except for the experiments of synthetic datasets.  3.1 SUPERVISED LEARNING FOR THE BINARY CLASSIFICATION OF SYNTHETIC DATASET  (a) Moons dataset  (b) Circles dataset  Figure 1: Visualization of the synthetic datasets. Each panel depicts the total of 16 training data points. Red circles stand for the samples with label 1 and blue triangles stand for the samples with label 0. Samples with each label are prepared uniformly from the light-colored trajectory.  We created two synthetic datasets by generating multiple points uniformly over two trajectories on (cid:60)2 as shown in Figure 1 and linearly embedding them into 100 dimensional vector space. The datasets are called (a) ‘Moons’ dataset and (b) ‘Circles’ dataset based on the shapes of the two trajectories, respectively. Each dataset consists of 16 training samples and 1000 test samples. Because the number of the samples is very small relative to the input dimension, maximum likelhood estimation (MLE) is vul- nerable to overﬁtting problem on these datasets. The set of hyperparameters in each regularization method and the other detailed experimental settings are described in Appendix A.1. We repeated the experiments 50 times with different samples of training and test sets, and reported the average of the 50 test performances. Our classiﬁer was a neural network (NN) with one hidden layer consisting of 100 hidden units. We used ReLU (Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011) activation function for hid- den units, and used softmax activation function for all the output units. The regularization methods we compared against the VAT on this dataset include L2 regularization (L2-reg), dropout (Srivas- tava et al., 2014), adversarial training(Adv), and random perturbation training (RP). The random perturbation training is a modiﬁed version of the VAT in which we replaced rv-adv with an (cid:15) sized unit vector uniformly sampled from I dimensional unit sphere. We compared random perturbation training with VAT in order to highlight the importance of choosing the appropriate direction of the  5  21012210122101221012Published as a conference paper at ICLR 2016  perturbation. As for the adversarial training, we followed Goodfellow et al. (2015) and determined the size of the perturbation r in terms of both L∞ norm and L2 norm.  the training and test samples with (cid:15) = 0.5 and Ip = 5.  Figure 2 compares the learning process between the VAT and the MLE. Panels (a.I) and (b.I) show  Figure 2: Comparison of transitions of average LDS(I) and error rate(II) between MLE and VAT  implemented with (cid:15) = 0.5 and Ip = 1. Average (cid:103)LDS showed in (a.I) and (b.I) were evaluated on the transitions of the average (cid:103)LDS, while panels (a.II) and (b.II) show the transitions of the error rate, on both training and test set. The average (cid:103)LDS is nearly zero at the beginning, because the models are initially close to uniform distribution around each inputs. The average (cid:103)LDS then decreases This difference suggests that a high sustained value of (cid:103)LDS is beneﬁcial in alleviating the overﬁtting  slowly for the VAT, and falls rapidly for the MLE. Although the training error eventually drops to zero for both methods, the ﬁnal test error of the VAT is signiﬁcantly lower than that of the MLE.  and in decreasing the test error.  Figure 3: Contours of p(y = 1|x, θ) drawn by NNs (with ReLU activation) trained with various regularization methods for a single dataset of ‘Moons’. A black line represents the contour of value 0.5. Red circles represent the data points with label 1, and blue triangles represent the data points  with label 0. The value above each panel correspond to average (cid:103)LDS value. Average (cid:103)LDS evaluated  on the training set with (cid:15) = 0.5 and Ip = 5 is shown at the top of each panel.  Figure 4: Contours of p(y = 1|x, θ) drawn by NNs trained with various regularization methods for a single dataset of ‘Circles’. The rest of the details follow the caption of the Figure 3.  Figures 3 and 4 show the contour plot of the model distributions for the binary classiﬁcation prob- lems of ‘Moons’ and ‘Circles’ trained with the best set of hyper parameters. The value above each  panel correspond to average (cid:103)LDS value. We see from the ﬁgures that NN without regularization  (MLE) and NN with L2 regularization are drawing decisively wrong decision boundary. The de- cision boundary drawn by dropout for ‘Circles’ is convincing, but the dropout’s decision boundary for ‘Moons’ does not coincide with our intention. The opposite can be said for the random pertur- bation training. Only adversarial training and VAT are consistently yielding the intended decision  6  (a) Moons(b) Circles(a) MLE(b) L2 regularization(c) Dropout   (d) Random        perturbation(e) Adversarial      training (L2)(f) VAT (ours)(a) MLE(b) L2 regularization(c) Dropout   (d) Random        perturbation(e) Adversarial      training (L2)(f) VAT (ours)Published as a conference paper at ICLR 2016  boundaries for both datasets. VAT is drawing appropriate decision boundary by imposing local smoothness regularization around each data point. This does not mean, however, that the large  value of LDS immediately implies good decision boundary. By its very deﬁnition, (cid:103)LDS tends to disfavor abrupt change of the likelihood around training datapoint. Large value of (cid:103)LDS therefore forces large relative margin around the decision boundary. One can achieve large value of (cid:103)LDS with  L2 regularization, dropout and random perturbation training with appropriate choice of hyperpa- rameters by smoothing the model distribution globally. This, indeed, comes at the cost of accuracy, however. Figure 5 summarizes the average test errors of six regularization methods with the best set  (a) Moons  (b) Circles  Figure 5: Comparison of the test error rate for (a) ‘Moons’ and (b) ‘Circles’  of hyperparameters. Adversarial training and VAT achieved much lower test errors than the other regularization methods. Surprisingly, the performance of the VAT did not change much with the value of Ip. We see that Ip = 1 sufﬁces for our dataset.  3.2 SUPERVISED LEARNING FOR THE CLASSIFICATION OF THE MNIST DATASET  Next, we tested the performance of our regularization method on the MNIST dataset, which consists of 28 × 28 pixel images of handwritten digits and their corresponding labels. The input dimen- sion is therefore 28 × 28 = 784 and each label is one of the numerals from 0 to 9. We split the original 60,000 training samples into 50,000 training samples and 10,000 validation samples, and used the latter of which to tune the hyperparameters. We applied our methods to the training of 2 types of NNs with different numbers of layers, 2 and 4. As for the number of hidden units, we used (1200, 600) and (1200, 600, 300, 150) respectively. The ReLU activation function and batch normalization technique (Ioffe & Szegedy, 2015) were used for all the NNs. The detailed settings of the experiments are described in Appendix A.2. For each regularization method, we used the set of hyperparameters that achieved the best perfor- mance on the validation data to train the NN on all training samples. We applied the trained networks to the test set and recorded their test errors. We repeated this procedure 10 times with different seeds for the weight initialization, and reported the average test error values. Table 1 summarizes the test error obtained by our regularization method (VAT) and the other regular- ization methods. VAT performed better than all the contemporary methods except Ladder network, which is highly advanced generative model based method.  3.3 SEMI-SUPERVISED LEARNING FOR THE CLASSIFICATION OF THE BENCHMARK DATASETS  Recall that our deﬁnition of LDS (Eq.(3)) at any point x is independent of the label information y. This in particular means that we can apply the VAT to semi-supervised learning tasks. We would like to emphasize that this is a property not enjoyed by adversarial training and dropout. We applied VAT to semi-supervised learning tasks in the permutation invariant setting for three datasets: MNIST, SVHN, and NORB. In this section we describe the detail of our setup for the semi-supervised learning of the MNIST dataset, and leave the further details of the experiments in the Appendix A.3 and A.4. We experimented with four sizes of labeled training samples Nl = {100, 600, 1000, 3000} and observed the effect of Nl on the test error. We used the validation set of ﬁxed size 1000, and used all the training samples excluding the validation set and the labeled to train the NNs. That is, when  7  MLEL2-regDropoutAdv(L∞)Adv(L2)RPVAT,Ip=1Ip=2Ip=40246810121416Error (%)MLEL2-regDropoutAdv(L∞)Adv(L2)RPVAT,Ip=1Ip=2Ip=40246810Error (%)Published as a conference paper at ICLR 2016  Table 1: Test errors of 10-class supervised learning for the permutation invariant MNIST task. Stars * indicate the methods that are dependent on generative models or pre-training. Test errors in the upper panel are the ones reported in the literature, and test errors in the bottom panel are the outputs of our implementation.  Method SVM (gaussian kernel) Gaussian dropout (Srivastava et al., 2014) Maxout Networks (Goodfellow et al., 2013) *MTC (Rifai et al., 2011) *DBM (Srivastava et al., 2014) Adversarial training (Goodfellow et al., 2015) *Ladder network (Rasmus et al., 2015) Plain NN (MLE) Random perturbation training Adversarial training (with L∞ norm constraint) Adversarial training (with L2 norm constraint) VAT (ours)  Test error (%)  1.40 0.95 0.94 0.81 0.79 0.782  1.11 0.843 0.788 0.708  0.57±0.02  0.637±0.046  Nl = 100, the unlabeled training set had the size of 60,000 − 100 − 1,000 = 58,900. For each choice of the hyperparameters, we repeated the experiment 10 times with different set. As for the architecture of NNs, we used ReLU based NNs with two hidden layers with the number of hidden units (1200, 1200). Batch normalization was implemented as well. Table 2a summarizes the results for the permutation invariant MNIST task. All the methods other than SVM and plain NN (MLE) are semi-supervised learning methods. For the MNIST dataset, VAT outperformed all the contemporary methods other than Ladder network (Rasmus et al., 2015), which is current state of the art. Table 2b and Table 2c summarizes the the results for SVHN and NORB respectively. Our method strongly outperforms the current state of the art semi-supervised learning method applied to these datasets.  4 DISCUSSION AND RELATED WORKS  Our VAT was motivated by the adversarial training (Goodfellow et al., 2015). Adversarial training and VAT are similar in that they both use the local input-output relationship to smooth the model distribution in the corresponding neighborhood. In contrast, L2 regularization does not use the local input-output relationship, and cannot introduce local smoothness to the model distribution. Increasing of the regularization constant in L2 regularization can only intensify the global smoothing of the distribution, which results in higher training and generalization error. The adversarial training aims to train the model while keeping the average of the following value high:  r  min  {log p(y(n)|x(n) + r, θ);(cid:107)r(cid:107)p ≤ (cid:15)}.  (13) This makes the likelihood evaluated at the n-th labeled data point robust against (cid:15)−perturbation applied to the input in its adversarial direction. PEA (Bachman et al., 2014), on the other hand, used the model’s sensitivity to random perturbation on input and hidden layers in their construction of the regularization function. PEA is similar to the random perturbation training of our experiment in that it aims to make the model distribution robust against random perturbation. Our VAT, however, outperforms PEA and random perturbation training. This fact is particularly indicative of the importance of the role of the hessian H in VAT (Eq.(5)). Because PEA and random perturbation training attempts to smooth the distribution into any arbitrary direction at every step of the update, it tends to make the variance of the loss function large in unnecessary dimensions. On the other hand, VAT always projects the perturbation in the principal direction of H, which is literally the principal direction into which the distribution is sensitive. Deep contractive network by Gu and Rigazio (Gu & Rigazio, 2015) takes still another approach to smooth the model distribution. Gu et al. introduced a penalty term based on the Frobenius norm of the Jacobian of the neural network’s output y = f (x, θ) with respect to x. Instead of computing the computationally expensive full Jacobian, they approximated the Jabobian by the sum of the  8  Published as a conference paper at ICLR 2016  Table 2: Test errors of semi-supervised learning for the permutation invariant task for MNIST, SVHN and NORB. All values are the averages over random partitioning of the dataset. Stars * indicate the methods that are dependent on generative models or pre-training.  Method  (a) MNIST  Nl  SVM (Weston et al., 2012) TSVM (Weston et al., 2012) EmbedNN (Weston et al., 2012) *MTC (Rifai et al., 2011) PEA (Bachman et al., 2014) *PEA (Bachman et al., 2014) *DG (Kingma et al., 2014) *Ladder network (Rasmus et al., 2015) Plain NN (MLE) VAT (ours)  Test error(%) 1000 600 7.77 8.85 5.38 6.16 5.73 5.97 5.13 3.64 2.23 2.44 2.64 2.87 2.40 2.59 0.84 7.25 1.36  9.16 1.39  100 23.44 16.81 16.9 12.0 10.79 5.21 3.33 1.06 21.98 2.33  3000 4.21 3.45 3.59 2.57 1.91 2.30 2.18  4.32 1.25  (b) SVHN  Method  TSVM (Kingma et al., 2014) *DG,M1+TSVM (Kingma et al., 2014) *DG,M1+M2 (Kingma et al., 2014) SVM (Gaussian kernel) Plain NN (MLE) VAT (ours)  (c) NORB  Method  TSVM (Kingma et al., 2014) *DG,M1+TSVM (Kingma et al., 2014) SVM (Gaussian kernel) Plain NN (MLE) VAT (ours)  Test error(%) 1000 Nl 66.55 55.33 36.02 63.28 43.21 24.63  Test error(%) 1000 Nl 26.00 18.79 23.62 20.00 9.88  Frobenius norm of the Jacobian over every adjacent pairs of hidden layers. The deep contractive network was, however, unable to signiﬁcantly decrease the test error. Ladder network (Rasmus et al., 2015) is a method that uses layer-wise denoising autoencoder. Their method is currently the best method in both supervised and semi-supervised learning for permutation invariant MNIST task. Ladder network seems to be conducting a variation of manifold learning that extracts the knowledge of the local distribution of the inputs. Still another classic way to include the information about the the input distribution is to use local smoothing of the data like Vicinal Risk Minimization (VRM) (Chapelle et al., 2001). VAT, on the other hand, only uses the property of the conditional distribution p(y|x, θ), giving no consideration to the the generative process p(x|θ) nor to the full joint distribution p(y, x|θ). In this aspect, VAT can be complementary to the methods that explicitly model the input distribution. We might be able to improve VAT further by introducing the notion of manifold learning into its framework.  5 CONCLUSION  Our experiments based on the synthetic datasets and the three real world dataset, MNIST, SVHN and NORB indicate that the VAT is an effective method for both supervised and semi-supervised learning. For the MNIST dataset, VAT outperformed all contemporary methods other than Ladder network, which is the current state of the art. VAT also outperformed current state of the art semi- supervised learning method for SVHN and NORB as well. We would also like to emphasize the simplicity of the method. With our approximation of LDS, VAT can be computed with relatively small computational cost. Also, models that relies heavily on generative models are dependent on many hyperparameters. VAT, on the other hand, has only two hyperparamaters, (cid:15) and λ. In fact, our experiments worked sufﬁciently well with the optimization of one hyperparameter (cid:15) while ﬁxing λ = 1.  9  Published as a conference paper at ICLR 2016  REFERENCES Akaike, Hirotugu. Information theory and an extension of the maximum likelihood principle. In  Selected Papers of Hirotugu Akaike, pp. 199–213. Springer, 1998.  Bachman, Phil, Alsharif, Ouais, and Precup, Doina. Learning with pseudo-ensembles. In Advances  in Neural Information Processing Systems, 2014.  Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Workshop on Deep Learning and Unsupervised Feature Learning at Neural Information Process- ing Systems, 2012.  Bengio, Yoshua. Learning deep architectures for ai. Foundations and trends R(cid:13) in Machine Learning,  2(1):1–127, 2009.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Chapelle, Olivier, Weston, Jason, Bottou, L´eon, and Vapnik, Vladimir. Vicinal risk minimization.  In Advances in Neural Information Processing Systems, 2001.  Coates, Adam, Ng, Andrew Y, and Lee, Honglak. An analysis of single-layer networks in unsuper- vised feature learning. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 215–223, 2011.  Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. The elements of statistical learning.  Springer series in statistics Springer, Berlin, 2001.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectiﬁer neural networks. In  International Conference on Artiﬁcial Intelligence and Statistics, pp. 315–323, 2011.  Golub, Gene H and Van der Vorst, Henk A. Eigenvalue computation in the 20th century. Journal of  Computational and Applied Mathematics, 123(1):35–65, 2000.  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.  Maxout networks. In International Conference on Machine Learning, 2013.  Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Christian. Explaining and harnessing adversarial  examples. In International Conference on Learning Representation, 2015.  Gu, Shixiang and Rigazio, Luca. Towards deep neural network architectures robust to adversarial  examples. In International Conference on Learning Representation, 2015.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by  reducing internal covariate shift. In International Conference on Machine Learning, 2015.  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th Interna- tional Conference on, pp. 2146–2153. IEEE, 2009.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- In Proceedings of the ACM International Conference on Multimedia, pp. 675–678. bedding. ACM, 2014.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. In International  Conference on Learning Representation, 2015.  Kingma, Diederik, Mohamed, Shakir, Rezende, Danilo Jimenez, and Welling, Max.  Semi- supervised learning with deep generative models. In Advances in Neural Information Processing Systems, 2014.  10  Published as a conference paper at ICLR 2016  LeCun, Yann, Bengio, Yoshua, and Hinton, Geoffrey. Deep learning. Nature, 521(7553):436–444,  2015.  Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines.  In International Conference on Machine Learning, 2010.  Pearlmutter, Barak A. Fast exact multiplication by the hessian. Neural computation, 6(1):147–160,  1994.  Rasmus, Antti, Valpola, Harri, Honkala, Mikko, Berglund, Mathias, and Raiko, Tapani. Semi-  supervised learning with ladder network. arXiv preprint arXiv:1507.02672, 2015.  Rifai, Salah, Dauphin, Yann N, Vincent, Pascal, Bengio, Yoshua, and Muller, Xavier. The manifold  tangent classiﬁer. In Advances in Neural Information Processing Systems, 2011.  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15(1):1929–1958, 2014.  Tokui, Seiya, Oono, Kenta, Hido, Shohei, and Clayton, Justin. Chainer: a next-generation open source framework for deep learning. In Workshop on Machine Learning Systems at Neural Infor- mation Processing Systems, 2015.  Wahba, Grace. Spline models for observational data. Siam, 1990.  Watanabe, Sumio. Algebraic geometry and statistical learning theory. Cambridge University Press,  2009.  Weston, Jason, Ratle, Fr´ed´eric, Mobahi, Hossein, and Collobert, Ronan. Deep learning via semi-  supervised embedding. In Neural Networks: Tricks of the Trade, pp. 639–655. Springer, 2012.  A APPENDIX:DETAILS OF EXPERIMENTAL SETTINGS  A.1 SUPERVISED BINARY CLASSIFICATION FOR SYNTHETIC DATASETS  We provide more details of experimental settings on synthetic datasets. We list the search space for the hyperparameters below:  • L2 regularization: regularization coefﬁcient λ = {1e − 4,··· , 200} • Dropout (only for the input layer): dropout rate p(z) = {0.05,··· , 0.95} • Random perturbation training: (cid:15) = {0.2,··· , 4.0} • Adversarial training (with L∞ norm constraint): (cid:15) = {0.01,··· , 0.2} • Adversarial training (with L2 norm constraint): (cid:15) = {0.1,··· , 2.0} • VAT: (cid:15) = {0.1,··· , 2.0}  All experiments with random perturbation training, adversarial training and VAT were conducted with λ = 1. As for the training we used stochastic gradient descent (SGD) with a moment method. When J(θ) is the objective function, the moment method augments the simple update in the SGD with a term dependent on the previous update ∆θi−1:  ∆θi = µi∆θi−1 + (1 − µi)γi  (14) In the expression above, µi ∈ [0, 1) stands for the strength of the momentum, and γi stands for the learning rate. In our experiment, we used µi = 0.9, and exponentially decreasing γi with rate 0.995. As for the choice of γ1, we used 1.0. We trained the NNs with 1,000 parameter updates.  J(θ).  ∂ ∂θ  A.2 SUPERVISED CLASSIFICATION FOR THE MNIST DATASET  We provide more details of experimental settings on supervised classiﬁcation for the MNIST dataset. Following lists summarizes the ranges from which we searched for the best hyperparameters of each regularization method:  • Random perturbation training: (cid:15) = {5.0,··· , 15.0}  11  Published as a conference paper at ICLR 2016  • Adversarial training (with L∞ norm constraint): (cid:15) = {0.05,··· , 0.1} • Adversarial training (with L2 norm constraint): (cid:15) = {1.0,··· , 3.0} • VAT: (cid:15) = {1.0,··· , 3.0}, Ip = 1  All experiments were conducted with λ = 1. The training was conducted by mini-batch SGD based on ADAM (Kingma & Ba, 2015). We chose the mini-batch size of 100, and used the default values of Kingma & Ba (2015) for the tunable parameters of ADAM. We trained the NNs with 50,000 parameter updates. As for the base learning rate in validation, we selected the initial value of 0.002 and adopted the schedule of exponential decay with rate 0.9 per 500 updates. After the hyperparameter determination, we trained the NNs over 60,000 parameter updates. For the learning coefﬁcient, we used the initial value of 0.002 and adopted the schedule of exponential decay with rate 0.9 per 600 updates.  A.3 SEMI-SUPERVISED CLASSIFICATION FOR THE MNIST DATASET  We provide more details of experimental settings on semi-supervised classiﬁcation for the MNIST dataset. We searched for the best hyperparameter (cid:15) from {0.2, 0.3, 0.4} in the Nl = 100 case. Best (cid:15) was selected from {1.5, 2.0, 2.5} for all other cases. All experiments were conducted with λ = 1 and Ip = 1. For the optimization method, we again used ADAM-based minibatch SGD with the same hyperparameter values as those in the supervised setting. We note that, in the computation of ADAM, the likelihood term can be computed from labeled data only. We therefore used two separate minibatches at each step: one minibatch of size 100 from labeled samples for the computation of the likelihood term, and another minibatch of size 250 from both labeled and unlabeled samples for computing the regularization term. We trained the NNs over 50,000 parameter updates. For the learning rate, we used the initial value of 0.002 and adopted the schedule of exponential decay with rate 0.9 per 500 updates.  A.4 SEMI-SUPERVISED CLASSIFICATION FOR THE SVHN AND NORB DATASET  We provide the details of the numerical experiment we conducted for the SVHN and NORB dataset. The SVHN dataset consists of 32 × 32 × 3 pixel RGB images of housing numbers and their cor- responding labels (0-9), and the number of training samples and test samples within the dataset are 73,257 and 26,032, respectively. To simplify the experiment, we down sampled the images from 32 × 32 × 3 to 16 × 16 × 3. We vectorized each image to 768 dimensional vector, and applied whitening (Coates et al., 2011) to the dataset. We reserved 1000 dataset for validation. From the rest, we used 1000 dataset as labeled dataset in semi-supervised training. We repeated the experi- ment 10 times with different choice of labeled dataset and validation dataset. The NORB dataset consists of 2 × 96 × 96 pixel gray images of 50 different objects and their cor- responding labels (cars, trucks, planes, animals, humans). The number of training samples and test samples constituting the dataset are 24,300. We downsampled the images from 2 × 96 × 96 to 2 × 32 × 32. We vectorized each image to 2048 dimensional vector and applied whitening. We reserved 1000 dataset for validation. From the rest, we used 1000 dataset as labeled dataset in semi- supervised training. We repeated the experiment 10 times with different choice of labeled dataset and validation dataset. For both SVHN and NORB, we used neural network with the number of hidden nodes given by (1200, 600, 300, 150, 150). In our setup, these two dataset preferred deeper network than the net- work we used for the MNIST dataset. We used ReLU for the activation function. As for the hyper- parameters (cid:15), we conducted grid search over the range {1.0, 1.5,··· , 4.5, 5.0}, and we used λ = 1. For power iteration, we used Ip = 1. We used ADAM based minibach SGD with the same hyperparameter values as the MNIST. We chose minibatch size of 100. Thus one epoch for SVHN completes with (cid:100)73, 257/100(cid:101) = 753 rounds of minibatches. For the learning rate, we used the initial value of 0.002 with exponential decay of rate 0.9 per epoch. We trained NNs with 100 epochs.  12  ",
1511.06114,2016,Multi-task Sequence to Sequence Learning,"['Multi-task Sequence to Sequence Learning\nMinh-Thang Luong', 'Quoc Le', 'Ilya Sutskever', 'Oriol Vinyals', 'Lukasz Kaiser']",https://arxiv.org/pdf/1511.06114,"6 1 0 2    r a  M 1         ]  G L . s c [      4 v 4 1 1 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  MULTI-TASK SEQUENCE TO SEQUENCE LEARNING  Minh-Thang Luong∗, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser Google Brain lmthang@stanford.edu,{qvl,ilyasu,vinyals,lukaszkaiser}@google.com  ABSTRACT  Sequence to sequence learning has recently emerged as a new paradigm in super- vised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the one- to-many setting – where the encoder is shared between several tasks such as ma- chine translation and syntactic parsing, (b) the many-to-one setting – useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting – where multiple encoders and de- coders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Further- more, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.  1  INTRODUCTION  Multi-task learning (MTL) is an important machine learning paradigm that aims at improving the generalization performance of a task using other related tasks. Such framework has been widely studied by Thrun (1996); Caruana (1997); Evgeniou & Pontil (2004); Ando & Zhang (2005); Argyriou et al. (2007); Kumar & III (2012), among many others. In the context of deep neural net- works, MTL has been applied successfully to various problems ranging from language (Liu et al., 2015), to vision (Donahue et al., 2014), and speech (Heigold et al., 2013; Huang et al., 2013).  Recently, sequence to sequence (seq2seq) learning, proposed by Kalchbrenner & Blunsom (2013), Sutskever et al. (2014), and Cho et al. (2014), emerges as an effective paradigm for dealing with variable-length inputs and outputs. seq2seq learning, at its core, uses recurrent neural networks to map variable-length input sequences to variable-length output sequences. While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application – ma- chine translation – (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015), but also image caption generation (Vinyals et al., 2015b), and con- stituency parsing (Vinyals et al., 2015a).  Despite the popularity of multi-task learning and sequence to sequence learning, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. (2015) which applies a seq2seq models for machine translation, where the goal is to translate from one language to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach – for tasks that can have an encoder in common, such as translation and parsing; this applies to the multi-target translation setting in (Dong et al., 2015) as well, (b) the many-to-one approach – useful for multi- source translation or tasks in which only the decoder can be easily shared, such as translation and image captioning, and lastly, (c) the many-to-many approach – which share multiple encoders and decoders through which we study the effect of unsupervised learning in translation. We show that syntactic parsing and image caption generation improves the translation quality between English  ∗Minh-Thang Luong is also a student at Stanford University.  1  Published as a conference paper at ICLR 2016  Figure 1: Sequence to sequence learning examples – (left) machine translation (Sutskever et al., 2014) and (right) constituent parsing (Vinyals et al., 2015a).  and German by up to +1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. We also explore two unsupervised learning objectives, sequence autoencoders (Dai & Le, 2015) and skip-thought vectors (Kiros et al., 2015), and reveal their interesting properties in the MTL setting: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.  2 SEQUENCE TO SEQUENCE LEARNING  Sequence to sequence learning (seq2seq) aims to directly model the conditional probability p(y|x) of mapping an input sequence, x1, . . . , xn, into an output sequence, y1, . . . , ym. It accomplishes such goal through the encoder-decoder framework proposed by Sutskever et al. (2014) and Cho et al. (2014). As illustrated in Figure 1, the encoder computes a representation s for each input sequence. Based on that input representation, the decoder generates an output sequence, one unit at a time, and hence, decomposes the conditional probability as:  log p(y|x) = X  m  j=1  log p (yj|y<j , x, s)  (1)  A natural model for sequential data is the recurrent neural network (RNN), which is used by most of the recent seq2seq work. These work, however, differ in terms of: (a) architecture – from unidirec- tional, to bidirectional, and deep multi-layer RNNs; and (b) RNN type – which are long-short term memory (LSTM) (Hochreiter & Schmidhuber, 1997) and the gated recurrent unit (Cho et al., 2014).  Another important difference between seq2seq work lies in what constitutes the input represen- tation s. The early seq2seq work (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015b; Vinyals et al., 2015b) uses only the last encoder state to initialize the decoder and sets s = [ ] in Eq. (1). Recently, Bahdanau et al. (2015) proposes an attention mechanism, a way to provide seq2seq models with a random access memory, to handle long input sequences. This is accomplished by setting s in Eq. (1) to be the set of encoder hidden states already computed. On the decoder side, at each time step, the attention mechanism will decide how much information to retrieve from that memory by learning where to focus, i.e., computing the alignment weights for all input positions. Recent work such as (Xu et al., 2015; Jean et al., 2015a; Luong et al., 2015a; Vinyals et al., 2015a) has found that it is crucial to empower seq2seq models with the attention mechanism.  3 MULTI-TASK SEQUENCE-TO-SEQUENCE LEARNING  We generalize the work of Dong et al. (2015) to the multi-task sequence-to-sequence learning set- ting that includes the tasks of machine translation (MT), constituency parsing, and image caption generation. Depending which tasks involved, we propose to categorize multi-task seq2seq learning into three general settings. In addition, we will discuss the unsupervised learning tasks considered as well as the learning process.  3.1 ONE-TO-MANY SETTING  This scheme involves one encoder and multiple decoders for tasks in which the encoder can be shared, as illustrated in Figure 2. The input to each task is a sequence of English words. A separate decoder is used to generate each sequence of output units which can be either (a) a sequence of tags  2  Published as a conference paper at ICLR 2016  German (translation)  English  Tags (parsing)  English (unsupervised)  Figure 2: One-to-many Setting – one encoder, multiple decoders. This scheme is useful for either multi-target translation as in Dong et al. (2015) or between different tasks. Here, English and Ger- man imply sequences of words in the respective languages. The α values give the proportions of parameter updates that are allocated for the different tasks.  for constituency parsing as used in (Vinyals et al., 2015a), (b) a sequence of German words for ma- chine translation (Luong et al., 2015a), and (c) the same sequence of English words for autoencoders or a related sequence of English words for the skip-thought objective (Kiros et al., 2015).  3.2 MANY-TO-ONE SETTING  This scheme is the opposite of the one-to-many setting. As illustrated in Figure 3, it consists of mul- tiple encoders and one decoder. This is useful for tasks in which only the decoder can be shared, for example, when our tasks include machine translation and image caption generation (Vinyals et al., 2015b). In addition, from a machine translation perspective, this setting can beneﬁt from a large amount of monolingual data on the target side, which is a standard practice in machine translation system and has also been explored for neural MT by Gulcehre et al. (2015).  German (translation)  Image (captioning)  English  English (unsupervised)  Figure 3: Many-to-one setting – multiple encoders, one decoder. This scheme is handy for tasks in which only the decoders can be shared.  3.3 MANY-TO-MANY SETTING  Lastly, as the name describes, this category is the most general one, consisting of multiple encoders and multiple decoders. We will explore this scheme in a translation setting that involves sharing multiple encoders and multiple decoders. In addition to the machine translation task, we will include two unsupervised objectives over the source and target languages as illustrated in Figure 4.  3.4 UNSUPERVISED LEARNING TASKS  Our very ﬁrst unsupervised learning task involves learning autoencoders from monolingual corpora, which has recently been applied to sequence to sequence learning (Dai & Le, 2015). However, in Dai & Le (2015)’s work, the authors only experiment with pretraining and then ﬁnetuning, but not joint training which can be viewed as a form of multi-task learning (MTL). As such, we are very interested in knowing whether the same trend extends to our MTL settings.  Additionally, we investigate the use of the skip-thought vectors (Kiros et al., 2015) in the context of our MTL framework. Skip-thought vectors are trained by training sequence to sequence models on pairs of consecutive sentences, which makes the skip-thought objective a natural seq2seq learning candidate. A minor technical difﬁculty with skip-thought objective is that the training data must  3  Published as a conference paper at ICLR 2016  German (translation)  English  English (unsupervised)  German (unsupervised)  Figure 4: Many-to-many setting – multiple encoders, multiple decoders. We consider this scheme in a limited context of machine translation to utilize the large monolingual corpora in both the source and the target languages. Here, we consider a single translation task and two unsupervised autoencoder tasks.  consist of ordered sentences, e.g., paragraphs. Unfortunately, in many applications that include machine translation, we only have sentence-level data where the sentences are unordered. To address that, we split each sentence into two halves; we then use one half to predict the other half.  3.5 LEARNING  Dong et al. (2015) adopted an alternating training approach, where they optimize each task for a ﬁxed number of parameter updates (or mini-batches) before switching to the next task (which is a different language pair). In our setting, our tasks are more diverse and contain different amounts of training data. As a result, we allocate different numbers of parameter updates for each task, which are expressed with the mixing ratio values αi (for each task i). Each parameter update consists of training data from one task only. When switching between tasks, we select randomly a new task i with probability  αi  .  Pj αj  Our convention is that the ﬁrst task is the reference task with α1 = 1.0 and the number of training parameter updates for that task is prespeciﬁed to be N . A typical task i will then be trained for αi ·N α1 parameter updates. Such convention makes it easier for us to fairly compare the same reference task in a single-task setting which has also been trained for exactly N parameter updates.  When sharing an encoder or a decoder, we share both the recurrent connections and the correspond- ing embeddings.  4 EXPERIMENTS  We evaluate the multi-task learning setup on a wide variety of sequence-to-sequence tasks: con- stituency parsing, image caption generation, machine translation, and a number of unsupervised learning as summarized in Table 1.  4.1 DATA  Our experiments are centered around the translation task, where we aim to determine whether other tasks can improve translation and vice versa. We use the WMT’15 data (Bojar et al., 2015) for the English⇆German translation problem. Following Luong et al. (2015a), we use the 50K most frequent words for each language from the training corpus.1 These vocabularies are then shared with other tasks, except for parsing in which the target “language” has a vocabulary of 104 tags. We use newstest2013 (3000 sentences) as a validation set to select our hyperparameters, e.g., mixing coefﬁcients. For testing, to be comparable with existing results in (Luong et al., 2015a), we use the ﬁltered newstest2014 (2737 sentences)2 for the English→German translation task and newstest2015 (2169 sentences)3 for the German→English task. See the summary in Table 1.  1The corpus has already been tokenized using the default tokenizer from Moses. Words not in these vocab-  ularies are represented by the token <unk>.  2http://statmt.org/wmt14/test-filtered.tgz 3http://statmt.org/wmt15/test.tgz  4  Published as a conference paper at ICLR 2016  Task English→German Translation German→English Translation English unsupervised German unsupervised Penn Tree Bank Parsing High-Conﬁdence Corpus Parsing Image Captioning  Test Train Valid Size Size Size 3003 4.5M 3000 4.5M 3000 2169 12.1M Details in text 13.8M 2416 1700 40K 11.0M 1700 2416 596K 4115  -  Vocab Size  Train Source Target Epoch 50K 50K 50K 50K 50K 50K  50K 50K 50K 50K 104 104 50K  12 12 6 6 40 6 10  -  Finetune  Start Cycle  8 8 4 4 20 4 5  1 1 0.5 0.5 4 0.5 1  Table 1: Data & Training Details – Information about the different datasets used in this work. For each task, we display the following statistics: (a) the number of training examples, (b) the sizes of the vocabulary, (c) the number of training epochs, and (d) details on when and how frequent we halve the learning rates (ﬁnetuning).  For the unsupervised tasks, we use the English and German monolingual corpora from WMT’15.4 Since in our experiments, unsupervised tasks are always coupled with translation tasks, we use the same validation and test sets as the accompanied translation tasks.  For constituency parsing, we experiment with two types of corpora:  1. a small corpus – the widely used Penn Tree Bank (PTB) dataset (Marcus et al., 1993) and, 2. a large corpus – the high-conﬁdence (HC) parse trees provided by Vinyals et al. (2015a).  The two parsing tasks, however, are evaluated on the same validation (section 22) and test (sec- tion 23) sets from the PTB data. Note also that the parse trees have been linearized following Vinyals et al. (2015a). Lastly, for image caption generation, we use a dataset of image and caption pairs provided by Vinyals et al. (2015b).  4.2 TRAINING DETAILS  In all experiments, following Sutskever et al. (2014) and Luong et al. (2015b), we train deep LSTM models as follows: (a) we use 4 LSTM layers each of which has 1000-dimensional cells and embed- dings,5 (b) parameters are uniformly initialized in [-0.06, 0.06], (c) we use a mini-batch size of 128, (d) dropout is applied with probability of 0.2 over vertical connections (Pham et al., 2014), (e) we use SGD with a ﬁxed learning rate of 0.7, (f) input sequences are reversed, and lastly, (g) we use a simple ﬁnetuning schedule – after x epochs, we halve the learning rate every y epochs. The values x and y are referred as ﬁnetune start and ﬁnetune cycle in Table 1 together with the number of training epochs per task.  As described in Section 3, for each multi-task experiment, we need to choose one task to be the refer- ence task (which corresponds to α1 = 1). The choice of the reference task helps specify the number of training epochs and the ﬁnetune start/cycle values which we also when training that reference task alone for fair comparison. To make sure our ﬁndings are reliable, we run each experimental conﬁguration twice and report the average performance in the format mean (stddev).  4.3 RESULTS  We explore several multi-task learning scenarios by combining a large task (machine translation) with: (a) a small task – Penn Tree Bank (PTB) parsing, (b) a medium-sized task – image caption generation, (c) another large task – parsing on the high-conﬁdence (HC) corpus, and (d) lastly, unsupervised tasks, such as autoencoders and skip-thought vectors. In terms of evaluation metrics, we report both validation and test perplexities for all tasks. Additionally, we also compute test BLEU scores (Papineni et al., 2002) for the translation task.  4The training sizes reported for the unsupervised tasks are only 10% of the original WMT’15 monolingual corpora which we randomly sample from. Such reduced sizes are for faster training time and already about three times larger than that of the parallel data. We consider using all the monolingual data in future work.  5For image caption generation, we use 1024 dimensions, which is also the size of the image embeddings.  5  Published as a conference paper at ICLR 2016  4.3.1 LARGE TASKS WITH SMALL TASKS  In this setting, we want to understand if a small task such as PTB parsing can help improve the performance of a large task such as translation. Since the parsing task maps from a sequence of English words to a sequence of parsing tags (Vinyals et al., 2015a), only the encoder can be shared with an English→German translation task. As a result, this is a one-to-many MTL scenario (§3.1). To our surprise, the results in Table 2 suggest that by adding a very small number of parsing mini- batches (with mixing ratio 0.01, i.e., one parsing mini-batch per 100 translation mini-batches), we can improve the translation quality substantially. More concretely, our best multi-task model yields a gain of +1.5 BLEU points over the single-task baseline. It is worth pointing out that as shown in Table 2, our single-task baseline is very strong, even better than the equivalent non-attention model reported in (Luong et al., 2015a). Larger mixing coefﬁcients, however, overﬁt the small PTB corpus; hence, achieve smaller gains in translation quality.  For parsing, as Vinyals et al. (2015a) have shown that attention is crucial to achieve good parsing performance when training on the small PTB corpus, we do not set a high bar for our attention-free systems in this setup (better performances are reported in Section 4.3.3). Nevertheless, the parsing results in Table 2 indicate that MTL is also beneﬁcial for parsing, yielding an improvement of up to +8.9 F1 points over the baseline.6 It would be interesting to study how MTL can be useful with the presence of the attention mechanism, which we leave for future work.  Task (Luong et al., 2015a)  Translation PTB Parsing  Valid ppl  Translation Test ppl  Test BLEU  Parsing Test F1  8.1 Our single-task systems  -  14.0  8.8 (0.3)  8.3 (0.2)  14.3 (0.3)  -  -  - Our multi-task systems  -  -  43.3 (1.7)  Translation + PTB Parsing (1x) Translation + PTB Parsing (0.1x) Translation + PTB Parsing (0.01x)  8.5 (0.0) 8.3 (0.1) 8.2 (0.2)  8.2 (0.0) 7.9 (0.0) 7.7 (0.2)  14.7 (0.1) 15.1 (0.0) 15.8 (0.4)  54.5 (0.4) 55.2 (0.0) 39.8 (2.7)  Table 2: English→German WMT’14 translation & Penn Tree Bank parsing results – shown are perplexities (ppl), BLEU scores, and parsing F1 for various systems. For muli-task models, reference tasks are in italic with the mixing ratio in parentheses. Our results are averaged over two runs in the format mean (stddev). Best results are highlighted in boldface.  4.3.2 LARGE TASKS WITH MEDIUM TASKS  We investigate whether the same pattern carries over to a medium task such as image caption gen- eration. Since the image caption generation task maps images to a sequence of English words (Vinyals et al., 2015b; Xu et al., 2015), only the decoder can be shared with a German→English translation task. Hence, this setting falls under the many-to-one MTL setting (§3.2). The results in Table 3 show the same trend we observed before, that is, by training on another task for a very small fraction of time, the model improves its performance on its main task. Speciﬁcally, with 5 parameter updates for image caption generation per 100 updates for translation (so the mixing ratio of 0.05), we obtain a gain of +0.7 BLEU scores over a strong single-task baseline. Our baseline is almost a BLEU point better than the equivalent non-attention model reported in Luong et al. (2015a).  4.3.3 LARGE TASKS WITH LARGE TASKS  Our ﬁrst set of experiments is almost the same as the one-to-many setting in Section 4.3.1 which combines translation, as the reference task, with parsing. The only difference is in terms of parsing  6While perplexities correlate well with BLEU scores as shown in (Luong et al., 2015b), we observe empir- ically in Section 4.3.3 that parsing perplexities are only reliable if it is less than 1.3. Hence, we omit parsing perplexities in Table 2 for clarity. The parsing test perplexities (averaged over two runs) for the last four rows in Table 2 are 1.95, 3.05, 2.14, and 1.66. Valid perplexities are similar.  6  Published as a conference paper at ICLR 2016  Task (Luong et al., 2015a)  Translation Captioning  Translation + Captioning (1x) Translation + Captioning (0.1x) Translation + Captioning (0.05x) Translation + Captioning (0.01x)  Valid ppl  Translation  Test ppl  -  14.3 Our single-task systems  Test BLEU  16.9  11.0 (0.0)  12.5 (0.2)  17.8 (0.1)  Captioning Valid ppl  -  -  -  - Our multi-task systems 14.0  11.9  10.5 (0.4) 10.3 (0.1) 10.6 (0.0)  12.1 (0.4) 11.8 (0.0) 12.3 (0.1)  -  30.8 (1.3)  16.7  18.0 (0.6) 18.5 (0.0) 18.1 (0.4)  43.3  28.4 (0.3) 30.1 (0.3) 35.2 (1.4)  Table 3: German→English WMT’15 translation & captioning results – shown are perplexities (ppl) and BLEU scores for various tasks with similar format as in Table 2. Reference tasks are in italic with mixing ratios in parentheses. The average results of 2 runs are in mean (stddev) format.  data. Instead of using the small Penn Tree Bank corpus, we consider a large parsing resource, the high-conﬁdence (HC) corpus, which is provided by Vinyals et al. (2015a). As highlighted in Table 4, the trend is consistent; MTL helps boost translation quality by up to +0.9 BLEU points.  Task (Luong et al., 2015a)  Valid ppl  - Our systems  Translation Test ppl  Test BLEU  8.1  14.0  Translation Translation + HC Parsing (1x) Translation + HC Parsing (0.1x) Translation + HC Parsing (0.05x)  8.8 (0.3) 8.5 (0.0) 8.2 (0.3) 8.4 (0.0)  8.3 (0.2) 8.1 (0.1) 7.7 (0.2) 8.0 (0.1)  14.3 (0.3) 15.0 (0.6) 15.2 (0.6) 14.8 (0.2)  Table 4: English→German WMT’14 translation – shown are perplexities (ppl) and BLEU scores of various translation models. Our multi-task systems combine translation and parsing on the high- conﬁdence corpus together. Mixing ratios are in parentheses and the average results over 2 runs are in mean (stddev) format. Best results are bolded.  The second set of experiments shifts the attention to parsing by having it as the reference task. We show in Table 5 results that combine parsing with either (a) the English autoencoder task or (b) the English→German translation task. Our models are compared against the best attention-based systems in (Vinyals et al., 2015a), including the state-of-the-art result of 92.8 F1.  Before discussing the multi-task results, we note a few interesting observations. First, very small parsing perplexities, close to 1.1, can be achieved with large training data.7 Second, our baseline system can obtain a very competitive F1 score of 92.2, rivaling Vinyals et al. (2015a)’s systems. This is rather surprising since our models do not use any attention mechanism. A closer look into these models reveal that there seems to be an architectural difference: Vinyals et al. (2015a) use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings. This further supports ﬁndings in (Jozefowicz et al., 2016) that larger networks matter for sequence models.  For the multi-task results, while autoencoder does not seem to help parsing, translation does. At the mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 F1 over the baseline and with 92.4 F1, our multi-task system is on par with the best single system reported in (Vinyals et al., 2015a). Furthermore, by ensembling 6 different multi-task models (trained with the translation task at mixing ratios of 0.1, 0.05, and 0.01), we are able to establish a new state-of-the-art result in English constituent parsing with 93.0 F1 score.  7Training solely on the small Penn Tree Bank corpus can only reduce the perplexity to at most 1.6, as evidenced by poor parsing results in Table 2. At the same time, these parsing perplexities are much smaller than what can be achieved by a translation task. This is because parsing only has 104 tags in the target vocabulary compared to 50K words in the translation case. Note that 1.0 is the theoretical lower bound.  7  Published as a conference paper at ICLR 2016  Task  LSTM+A (Vinyals et al., 2015a) LSTM+A+E (Vinyals et al., 2015a) Our systems  HC Parsing HC Parsing + Autoencoder (1x) HC Parsing + Autoencoder (0.1x) HC Parsing + Autoencoder (0.01x) HC Parsing + Translation (1x) HC Parsing + Translation (0.1x) HC Parsing + Translation (0.05x) HC Parsing + Translation (0.01x) Ensemble of 6 multi-task systems  Parsing  Valid ppl  Test F1 92.5 92.8  - -  1.12/1.12 1.12/1.12 1.12/1.12 1.12/1.13 1.12/1.13 1.13/1.13 1.11/1.12 1.12/1.12  -  92.2 (0.1) 92.1 (0.1) 92.1 (0.1) 92.0 (0.1) 91.5 (0.2) 92.0 (0.2) 92.4 (0.1) 92.2 (0.0)  93.0  Table 5: Large-Corpus parsing results – shown are perplexities (ppl) and F1 scores for various parsing models. Mixing ratios are in parentheses and the average results over 2 runs are in mean (stddev) format. We show the individual perplexities for all runs due to small differences among them. For Vinyals et al. (2015a)’s parsing results, LSTM+A represents a single LSTM with atten- tion, whereas LSTM+A+E indicates an ensemble of 5 systems. Important results are bolded.  4.3.4 MULTI-TASKS AND UNSUPERVISED LEARNING  Our main focus in this section is to determine whether unsupervised learning can help improve translation. Speciﬁcally, we follow the many-to-many approach described in Section 3.3 to couple the German→English translation task with two unsupervised learning tasks on monolingual corpora, one per language. The results in Tables 6 show a similar trend as before, a small amount of other tasks, in this case the autoencoder objective with mixing coefﬁcient 0.05, improves the translation quality by +0.5 BLEU scores. However, as we train more on the autoencoder task, i.e. with larger mixing ratios, the translation performance gets worse.  Task (Luong et al., 2015a)  Translation  Valid ppl  -  Translation Test ppl  14.3  Test BLEU  16.9  Our single-task systems  11.0 (0.0)  12.5 (0.2)  17.8 (0.1)  German Test ppl  English Test ppl  -  -  -  -  Translation + autoencoders (1.0x) Translation + autoencoders (0.1x) Translation + autoencoders (0.05x)  Our multi-task systems with Autoencoders 16.0 17.7  12.3 11.4  13.9 12.7  10.9 (0.1)  12.0 (0.0)  18.3 (0.4)  Our multi-task systems with Skip-thought Vectors  Translation + skip-thought (1x) Translation + skip-thought (0.1x) Translation + skip-thought (0.01x)  10.4 (0.1) 10.7 (0.0) 11.0 (0.1)  10.8 (0.1) 11.4 (0.2) 12.2 (0.0)  17.3 (0.2) 17.8 (0.4) 17.8 (0.3)  1.01 1.13  1.40 (0.01)  2.10 1.44  2.38 (0.39)  36.9 (0.1) 52.8 (0.3) 76.3 (0.8)  31.5 (0.4) 53.7 (0.4) 142.4 (2.7)  Table 6: German→English WMT’15 translation & unsupervised learning results – shown are perplexities for translation and unsupervised learning tasks. We experiment with both autoencoders and skip-thought vectors for the unsupervised objectives. Numbers in mean (stddev) format are the average results of 2 runs; others are for 1 run only.  Skip-thought objectives, on the other hand, behave differently. If we merely look at the perplexity metric, the results are very encouraging: with more skip-thought data, we perform better consistently across both the translation and the unsupervised tasks. However, when computing the BLEU scores, the translation quality degrades as we increase the mixing coefﬁcients. We anticipate that this is due to the fact that the skip-thought objective changes the nature of the translation task when using one half of a sentence to predict the other half. It is not a problem for the autoencoder objectives, however, since one can think of autoencoding a sentence as translating into the same language.  8  Published as a conference paper at ICLR 2016  We believe these ﬁndings pose interesting challenges in the quest towards better unsupervised objec- tives, which should satisfy the following criteria: (a) a desirable objective should be compatible with the supervised task in focus, e.g., autoencoders can be viewed as a special case of translation, and (b) with more unsupervised data, both intrinsic and extrinsic metrics should be improved; skip-thought objectives satisfy this criterion in terms of the intrinsic metric but not the extrinsic one.  5 CONCLUSION  In this paper, we showed that multi-task learning (MTL) can improve the performance of the attention-free sequence to sequence model of (Sutskever et al., 2014). We found it surprising that training on syntactic parsing and image caption data improved our translation performance, given that these datasets are orders of magnitude smaller than typical translation datasets. Furthermore, we have established a new state-of-the-art result in constituent parsing with an ensemble of multi-task models. We also show that the two unsupervised learning objectives, autoencoder and skip-thought, behave differently in the MTL context involving translation. We hope that these interesting ﬁndings will motivate future work in utilizing unsupervised data for sequence to sequence learning. A crit- icism of our work is that our sequence to sequence models do not employ the attention mechanism (Bahdanau et al., 2015). We leave the exploration of MTL with attention for future work.  ACKNOWLEDGMENTS  We thank Chris Manning for helpful feedback on the paper and members of the Google Brain team for thoughtful discussions and insights.  REFERENCES Ando, Rie Kubota and Zhang, Tong. A framework for learning predictive structures from multiple  tasks and unlabeled data. JMLR, 6:1817–1853, 2005.  Argyriou, Andreas, Evgeniou, Theodoros, and Pontil, Massimiliano. Multi-task feature learning. In  NIPS, 2007.  Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly  learning to align and translate. In ICLR, 2015.  Bojar, Ondˇrej, Chatterjee, Rajen, Federmann, Christian, Haddow, Barry, Huck, Matthias, Hokamp, Chris, Koehn, Philipp, Logacheva, Varvara, Monz, Christof, Negri, Matteo, Post, Matt, Scarton, Carolina, Specia, Lucia, and Turchi, Marco. Findings of the 2015 workshop on statistical machine translation. In WMT, 2015.  Caruana, Rich. Multitask learning. Machine Learning, 28(1):41–75, 1997.  Cho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, 2014.  Dai, Andrew M. and Le, Quoc V. Semi-supervised sequence learning. In NIPS, 2015.  Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell,  Trevor. DeCAF: A deep convolutional activation feature for generic visual recognition, 2014.  Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng. Multi-task learning for  multiple language translation. In ACL, 2015.  Evgeniou, Theodoros and Pontil, Massimiliano. Regularized multi–task learning.  2004.  In SIGKDD,  Gulcehre, Caglar, Firat, Orhan, Xu, Kelvin, Cho, Kyunghyun, Barrault, Loic, Lin, Huei-Chi, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. On using monolingual corpora in neural machine translation. arXiv preprint arXiv:1503.03535, 2015.  9  Published as a conference paper at ICLR 2016  Heigold, Georg, Vanhoucke, Vincent, Senior, Alan, Nguyen, Patrick, Ranzato, Marc’Aurelio, Devin, Matthieu, and Dean, Jeffrey. Multilingual acoustic models using distributed deep neural networks. In ICASSP, 2013.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural Computation, 9(8):  1735–1780, 1997.  Huang, Jui-Ting, Li, Jinyu, Yu, Dong, Deng, Li, and Gong, Yifan. Cross-language knowledge  transfer using multilingual deep neural network with shared hidden layers. In ICASSP, 2013.  Jean, S´ebastien, Cho, Kyunghyun, Memisevic, Roland, and Bengio, Yoshua. On using very large  target vocabulary for neural machine translation. In ACL, 2015a.  Jean, S´ebastien, Firat, Orhan, Cho, Kyunghyun, Memisevic, Roland, and Bengio, Yoshua. Montreal  neural machine translation systems for WMT’15. In WMT, 2015b.  Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu, Y. Exploring the limits of language  modeling. arXiv preprint arXiv:1602.02410, 2016.  Kalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. In EMNLP, 2013.  Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan, Zemel, Richard S., Torralba, Antonio, Urtasun,  Raquel, and Fidler, Sanja. Skip-thought vectors. In NIPS, 2015.  Kumar, Abhishek and III, Hal Daum´e. Learning task grouping and overlap in multi-task learning.  In ICML, 2012.  Liu, Xiaodong, Gao, Jianfeng, He, Xiaodong, Deng, Li, Duh, Kevin, and Wang, Ye-Yi. Represen- tation learning using multi-task deep neural networks for semantic classiﬁcation and information retrieval. In NAACL, 2015.  Luong, Minh-Thang and Manning, Christopher D. Stanford neural machine translation systems for  spoken language domain. In IWSLT, 2015.  Luong, Minh-Thang, Pham, Hieu, and Manning, Christopher D. Effective approaches to attention-  based neural machine translation. In EMNLP, 2015a.  Luong, Minh-Thang, Sutskever, Ilya, Le, Quoc V., Vinyals, Oriol, and Zaremba, Wojciech. Ad-  dressing the rare word problem in neural machine translation. In ACL, 2015b.  Marcus, Mitchell P., Marcinkiewicz, Mary Ann, and Santorini, Beatrice. Building a large annotated  corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330, 1993.  Papineni, Kishore, Roukos, Salim, Ward, Todd, and jing Zhu, Wei. Bleu: a method for automatic  evaluation of machine translation. In ACL, 2002.  Pham, Vu, Bluche, Th´eodore, Kermorvant, Christopher, and Louradour, J´erˆome. Dropout improves recurrent neural networks for handwriting recognition. In Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on, pp. 285–290. IEEE, 2014.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural net-  works. In NIPS, 2014.  Thrun, Sebastian. Is learning the n-th thing any easier than learning the ﬁrst? In NIPS, 1996.  Vinyals, Oriol, Kaiser, Lukasz, Koo, Terry, Petrov, Slav, Sutskever, Ilya, and Hinton, Geoffrey.  Grammar as a foreign language. In NIPS, 2015a.  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural  image caption generator. In CVPR, 2015b.  Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun, Courville, Aaron C., Salakhutdinov, Ruslan, Zemel, Richard S., and Bengio, Yoshua. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.  10  ",
1511.04581,2016,A Test of Relative Similarity for Model Selection in Generative Models,"['A Test of Relative Similarity for Model Selection in Generative Models\nEugene Belilovsky', 'Wacha Bounliphone', 'Matthew Blaschko', 'Ioannis Antonoglou', 'Arthur Gretton']",https://arxiv.org/pdf/1511.04581,"6 1 0 2     b e F 5 1         ] L M  . t a t s [      4 v 1 8 5 4 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  A TEST OF RELATIVE SIMILARITY FOR MODEL SELECTION IN GENERATIVE MODELS  Wacha Bounliphone,12∗ Eugene Belilovsky,12∗ & Matthew B. Blaschko2 1CentraleSup´elec & Inria Saclay, Universit´e Paris-Saclay, 92295 Chˆatenay-Malabry, France 2ESAT-PSI, KU Leuven, Kasteelpark Arenberg 10, 3001 Leuven, Belgium {wacha.bounliphone,eugene.belilovsky}@inria.fr matthew.blaschko@esat.kuleuven.be  Ioannis Antonoglou Google Deepmind 5 New Street Square London EC4A 3TW, UK ioannisa@google.com  Arthur Gretton Gatsby Computational Neuroscience Unit University College London 25 Howland Street London W1T 4JG, UK arthur.gretton@gmail.com  ABSTRACT  Probabilistic generative models provide a powerful framework for representing data that avoids the expense of manual annotation typically needed by discrimi- native approaches. Model selection in this generative setting can be challenging, however, particularly when likelihoods are not easily accessible. To address this issue, we introduce a statistical test of relative similarity, which is used to de- termine which of two models generates samples that are signiﬁcantly closer to a real-world reference dataset of interest. We use as our test statistic the differ- ence in maximum mean discrepancies (MMDs) between the reference dataset and each model dataset, and derive a powerful, low-variance test based on the joint asymptotic distribution of the MMDs between each reference-model pair. In ex- periments on deep generative models, including the variational auto-encoder and generative moment matching network, the tests provide a meaningful ranking of model performance as a function of parameter and training settings.  1  INTRODUCTION  Generative models based on deep learning techniques aim to provide sophisticated and accurate models of data, without expensive manual annotation (Bengio, 2009; Kingma et al., 2014). This is especially of interest as deep networks tend to require comparatively large training samples to achieve a good result (Krizhevsky et al., 2012). Model selection within this class of techniques can be a challenge, however. First, likelihoods can be difﬁcult to compute for some families of recently proposed models based on deep learning (Goodfellow et al., 2014; Li et al., 2015b). The current best method to evaluate such models is based on Parzen-window estimates of the log likelihood (Goodfellow et al., 2014, Section 5). Second, if we are given two models with similar likelihoods, we typically do not have a computationally inexpensive hypothesis test to determine whether one likelihood is signiﬁcantly higher than the other. Permutation testing or other generic strategies are often computationally prohibitive, bearing in mind the relatively high computational requirements of deep networks (Krizhevsky et al., 2012). In this work, we provide an alternative strategy for model selection, based on a novel, non-parametric hypothesis test of relative similarity. We treat the two trained networks being compared as generative models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test whether the ﬁrst candidate model generates samples signiﬁcantly closer to a reference validation set. The null hypothesis is that the ordering is reversed, and the second candidate model is closer to  ∗These authors contributed equally to this work  1  Published as a conference paper at ICLR 2016  the reference (further, both samples are assumed to remain distinct from the reference, as will be the case for any sufﬁciently complex modeling problem). Our model selection criterion is based on the maximum mean discrepancy (MMD) (Gretton et al., 2006; 2012a), which represents the distance between embeddings of empirical distributions in a re- producing kernel Hilbert space (RKHS). The maximum mean discrepancy is a metric on the space of probability distirbutions when a characteristic kernel is used (Fukumizu et al., 2008; Sriperumbudur et al., 2010), meaning that the distribution embeddings are unique for each probability measure. Recently, the MMD has been used in training generative models adversarially, (Li et al., 2015b; Dziugaite et al., 2015), where the MMD measures the distance of the generated samples to some reference target set; it has been used for statistical model criticism (Lloyd and Ghahramani, 2015); and to minimize the effect of nuisance variables on learned representations (Louizos et al., 2016). Rather than train a single model using the MMD distance to a reference distribution, our goal in this work is to evaluate the relative performance of two models, by testing whether one generates samples signiﬁcantly closer to the reference distribution than the other. This extends the applicability of the MMD to problems of model selection and evaluation. Key to this result is a novel expression for the joint asymptotic distribution of two correlated MMDs (between samples generated from each model, and samples from the reference distribution). Li et al. (2015a) have derived the joint distribution of a speciﬁc MMD estimator under the assumption that the distributions are equal. By contrast, we derive the case in which the distributions are unequal, as is expected due to irreducible model error. We provide a detailed introduction to the MMD and its associated notation in Section 2. We derive the joint asymptotic distribution of the MMDs in Section 3: this uses similar ideas to the relative dependence test in Bounliphone et al. (2015), with the additional complexity due to there being three independent samples to deal with, rather than a single joint sample. We formulate a hypothesis test of relative similarity, to determine whether the difference in MMDs is statistically signiﬁcant. Our ﬁrst test benchmark is on a synthetic data for which the ground truth is known (Section 4), where we verify that the test performs correctly under the null and the alternative. Finally, in Section 5, we demonstrate the performance of our test over a broad selection of model comparison problems in the deep learning setting, by evaluating relative similarity of pairs of model outputs to a validation set over a range of training regimes and settings. Our benchmark models in- clude the variational auto-encoder (Kingma and Welling, 2014) and the generative moment matching network (Li et al., 2015b). We ﬁrst demonstrate that the test performs as expected in scenarios where the same model is trained with different training set sizes, and the relative ordering of model per- formance is known. We then ﬁx the training set size and change various architectural parameters of these networks, showing which models are signiﬁcantly preferred with our test. We validate the rankings returned by the test using a separate set of data for which we compute alternate metrics for assessing the models, such as classiﬁcation accuracy and likelihood.  2 BACKGROUND MATERIAL  In comparing samples from distributions, we use the Maximum Mean Discrepancy (MMD) (Gretton et al., 2006; 2012a). We brieﬂy review this statistic and its asymptotic behaviour for a single pair of samples. Deﬁnition 1. (Gretton et al., 2012a, Deﬁnition 2: Maximum Mean Discrepancy (MMD)) Let F be an RKHS, with the continuous feature mapping ϕ(x) ∈ F from each x ∈ X , such that the inner product between the features is given by the kernel function k(x, x(cid:48)) := (cid:104)φ(x), φ(x(cid:48))(cid:105). Then the squared population MMD is  MMD2(F, Px, Py) = Ex,x(cid:48) [k(x, x(cid:48))] − 2 Ex,y [k(x, y)] + Ey,y(cid:48) [k(y, y(cid:48))] .  (1)  The following theorem describes an unbiased quadratic-time estimate of the MMD, and its asymp- totic distribution when Px and Py are different. Theorem 1. (Gretton et al., 2012a, Lemma 6 & Corollary 16: Unbiased empirical estimate & u(F, Xm, Yn)) Deﬁne observations Xm := {x1, ..., xm} and Asymptotic distribution of MMD2  2  Published as a conference paper at ICLR 2016  Yn := {y1, ..., yn} independently and identically distributed (i.i.d.) from Px and Py, respectively. An unbiased empirical estimate of MMD2(F, Px, Py) is a sum of two U-statistics and a sample average,  MMD2  u(F, Xm, Yn) =  k(xi, xj) +  1  n(n − 1)  k(yi, yj)  (2)  n(cid:88)  n(cid:88)  i=1  j(cid:54)=i  m(cid:88)  j(cid:54)=i  m(cid:88) n(cid:88)  i=1  1  m(cid:88)  m(m − 1)  − 2 mn  i=1  j=1  k(xi, yj).  Let V := (v1, ..., vm) be m i.i.d. random variables, where v := (x, y) ∼ Px × Py. When m = n, an unbiased empirical estimate of MMD2(F, Px, Py) is 1  m(cid:88)  MMD2  u(F, Xm, Ym) =  (3)  m(m − 1)  i(cid:54)=j  h(vi, vj)  with h(vi, vj) = k(xi, xj) + k(yi, yj) − k(xi, yj) − k(xj, yi). We assume E(h2) < ∞. When Px (cid:54)= Py, MMD2 u(F, X, Y ) converges in distribution to a Gaussian according to √  m(cid:0)MMD2 u(F, Xm, Yn) − MMD2(F, Px, Py)(cid:1) D−→ N(cid:0)0, σ2 XY = 4(cid:0)Ev1 [(Ev2h(v1, v2))2] − [(Ev1,v2 h(v1, v2))2](cid:1)  where  (cid:1)  (4)  (5)  XY  σ2 √ m. uniformly at rate 1/  A two-sample test may be constructed using the MMD as a test statistic, however when Px = Py the statistic is degenerate, and the asymptotic distribution is a weighted sum of χ2 variables (which can have inﬁnitely many terms, (Gretton et al., 2012a)). By contrast, our problem setting is to determine with high signiﬁcance whether a target distribution Px is closer to one of two candidate distributions Py, Pz, based on two empirical estimates of the u(F, Xm, Yn) as well MMD and their variances. This requires us to characterize σ2 u(F, Xm, Zr) (the as the covariance of two dependent estimates, MMD2 dependence arises from the shared sample Xm). Fortunately, degeneracy does not arise if we assume Py, Pz are each distinct from Px. In the next section, we obtain the joint asymptotic distribution of two dependent MMD statistics. We demonstrate how this joint distribution can be empirically estimated, and use the resulting parametric form to construct a computationally efﬁcient and powerful hypothesis test for relative similarity.  u(F, Xm, Yn) and MMD2  XY for MMD2  3  JOINT ASYMPTOTIC DISTRIBUTION OF TWO CORRELATED MMDS AND A RESULTING TEST STATISTIC  In this section, we derive our statistical test for relative similarity as measured by MMD. In order to maximize the statistical efﬁciency of the test, we will reuse samples from the reference distribution, denoted by Px, to compute the MMD estimates with two candidate distributions Py and Pz. We u(F, Xm, Zr), and as the data sample consider two MMD estimates MMD2 Xm is identical between them, these estimates will be correlated. We therefore ﬁrst derive the joint asymptotic distribution of these two metrics and use this to construct a statistical test. Theorem 2. We assume that Px (cid:54)= Py, Px (cid:54)= Pz, E(k(xi, xj)) < ∞, E(k(yi, yj)) < ∞ and E(k(xi, yj)) < ∞, then √  (cid:18)MMD2(F, Px, Py) (cid:19)(cid:19)  u(F, Xm, Yn) and MMD2  (cid:18)(cid:18)MMD2  (cid:18)(cid:18)0 (cid:19)  (cid:18) σ2  σXY XZ  (cid:19)(cid:19)  (cid:19) u(F, Xm, Yn) u(F, Xm, Zr)  −  0  ,  XY  σXY XZ  σ2  XZ  MMD2(F, Px, Pz)  m  MMD2  d−→ N  (6)  We substitute the kernel MMD deﬁnition from Equation (2), expand the terms in the expectation, and determine their empirical estimates in order to compute the variances in practice. The proof and additional details of the following derivations are given in Appendix A.  3  Published as a conference paper at ICLR 2016  An empirical estimate of σXY XZ in Equation (6), neglecting higher order terms, can be computed in O(m2):  (cid:18)  (cid:19)2  σXY XZ ≈  (7)  1  1  (cid:18) m(m − 1)2 eT ˜Kxx ˜Kxxe − (cid:18) − (cid:18) 1  m(m − 1)n  m(m − 1)r  −  1  eT ˜KxxKxze −  eT ˜KxxKxye −  +  mnr  eT KyxKxze − 1  m2nr  eT ˜Kxxe  1  m(m − 1) 1  m2(m − 1)r  1  m2(m − 1)n  eT KxyeeT Kxze  (cid:19)  (cid:19) (cid:19)  eT ˜KxxeeT Kxze  eT ˜KxxeeT Kxze  where e is a vector of 1s with appropriate size, while ˜Kxx, Kxy and Kxz refer to the kernel matri- ces, with ˜K indicating that the diagonal entries have been set to zero (cf. Appendix A). Similarly, Equation (5) is constructed as in Equation (7). Based on the empirical distribution from Equation (6), we now describe a statistical test to solve the following problem: Problem 1 (Relative similarity test). Let Px, Py, x and y be deﬁned as above, z be an independent random variables with distribution Pz. Given observations Xm := {x1, ..., xm}, Yn := {y1, ..., yn} and Zr := {z1, ..., zr} i.i.d. from Px, Py and Pz respectively such that Px (cid:54)= Py, Px (cid:54)= Pz, we test the hypothesis that Px is closer to Pz than Py i.e. we test the null hypothesis H0: MMD(F, Px, Py) ≤ MMD(F, Px, Pz) versus the alternative hypothesis H1: MMD(F, Px, Py) > MMD(F, Px, Pz) at a given signiﬁcance level α u(F, Xm, Yn) − MMD2  u(F, Xm, Zr) is used to compute the p-value p for The test statistic MMD2 the standard normal distribution. The test statistic is obtained by rotating the joint distribution (cf. Eq. 6) by π 4 about the origin, and integrating the resulting projection on the ﬁrst axis, in a √ manner similar to Bounliphone et al. (2015). Denote the asymptotically normal distribution of u(F, Xm, Zr)]T as N (µ, Σ). The resulting distribution from ro-  tating by π/4 and projecting onto the primary axis is N(cid:0)[Rµ]1, [RΣRT ]11  (cid:1) where  m[MMD2  u(F, Xm, Yn); MMD2 √  [Rµ]1 =  (MMD2  [RΣRT ]11 =  XY + σ2  u(F, Xm, Yn) − MMD2 XZ − 2σXY XZ)  u(F, Xm, Zr))  with R is the rotation by π/4. Then, the p-values for testing H0 versus H1 are u(F, Xm, Zr)  u(F, Xm, Yn) − MMD2  p ≤ Φ  XY + σ2  XZ − 2σXY XZ  (cid:112)σ2  (cid:33)  2 2 1 (σ2 2  (cid:32) − MMD2  (8)  (9)  (10)  where Φ is the CDF of a standard normal distribution. We have made code for performing the test is available.1  4 EXPERIMENTAL VALIDATION OF THE RELATIVE MMD TEST  We verify the validity of the hypothesis test described above using a synthetic data set in which we can directly control the relative similarity between distributions. We constructed three Gaussian dis- tributions as illustrated in Figure 1. These Gaussian distributions are speciﬁed with different means so that we can control the degree of relative similarity between them. The question is whether the similarity between X and Z is greater than the similarity between X and Y . In these experiments, we used a Gaussian kernel with bandwidth selected as the median pairwise distance between data points, and we ﬁxed µY = [−20,−20], µZ = [20, 20] and varied µX such that µX = (1− γ)µY + γµZ, for 41 regularly spaced values of γ ∈ [0.1, 0.9] (avoiding the degenerate cases Px = Py or Px = Pz).  1Code and examples can be found at https://github.com/eugenium/MMD  4  Published as a conference paper at ICLR 2016  u(F, X, Y ) is almost equal to MMD2  Figure 3 shows the p-values of the relative similarity test for different distribution. When γ is u(F, X, Z), the p-values varying around 0.5, i.e., when MMD2 quickly transition from 1 to 0, indicating strong discrimination of the test. In Figure 2, we compare the power of our test to the power of a naive test when the reference sample is split in two, and the MMDs have no covariance: clearly, the latter simple approach does worse than ours (a similar comparison in testing relative dependence returned the same advantage for a test based on the joint distribution; see Bounliphone et al. (2015, Section 3)). Figure 4 shows an empirical scatter plot of the pairs of MMD statistics along with a 2σ iso-curve of the estimated distribution, demonstrating that the parametric Gaussian distribution is well calibrated to the empirical values. Futhermore, we validate our derived formulas using simulations in Appendix B, where we show the p-values have the correct distribution under the null.  s t s e t  e h t  f o  r e w o P  γ  Figure 2: Comparison of the power of the pro- posed method to an independent test analogous to Bounliphone et al. (2015, Section 3) as a function of γ.  Figure 1: Illustration of  the synthetic dataset where X, Y and Z are respectively Gaussian distributed with mean µX = [0, 0]T , µY = [−20,−20]T , µZ = [20, 20]T and with variance ( 1 0  0 1 ).  s e u l a v - p  γ  m = 1000  Figure 3: We ﬁxed µY = [−5,−5], µZ = [5, 5] and varied µX such that µX = (1 − γ)µY + γµZ, for 41 regularly spaced values of γ ∈ [0.1, 0.9] versus p-values for 100 repeated tests.  Figure 4: The empirical scatter plot of the joint MMD statistics with m = 1000 for 200 repeated tests, along with the 2σ iso-curve of the an- alytical Gaussian distribution estimated by Equation (6). The analytical distribution closely matches the empirical scatter plot, verifying the correctness of the variances.  5 MODEL SELECTION FOR DEEP UNSUPERVISED NEURAL NETWORKS  An important potential application of the Relative MMD can be found in recent work on unsu- pervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015b; Goodfellow et al., 2014). As noted by several authors, the evaluation of generative models is a challenging open prob- lem (Li et al., 2015b; Goodfellow et al., 2014), and the distributions of samples from these models are very complex and difﬁcult to evaluate. The relative MMD performance can be used to compare different model settings, or even model families, in a statistically valid framework. To compare two models using our test, we generate samples from both, and compare these to a set of real target data samples that were not used to train either model. In the experiments in the sequel we focus on the recently introduced variational auto-encoder (VAE) (Kingma and Welling, 2014) and the generative moment matching networks (GMMN) (Li et al.,  5  −40−2002040−30−20−100102030Illustration of the synthetic dataset  X:µX:[00]Y:µY:[-20-20]Z:µZ:[2020]0.50.550.60.650.700.20.40.60.81  dependent testindependent test00.20.40.60.8100.20.40.60.811.741.7451.751.7551.761.7651.741.7451.751.7551.761.765Published as a conference paper at ICLR 2016  (a)  (b)  Figure 5: (a) Variational auto-encoder reference model. We have 400 hidden nodes (both encoder and decoder) and 20 latent variables in the reference model for our experiments. (b) Auto-Encoder + GMMN reference model. The auto-encoder (indicated in orange) is trained separately and has 1024 and 32 hidden nodes in decode and encode hidden layers. The GMMN has 10 variables generated by the prior, and the hidden layers have 64, 256, 256, 1024 nodes in each layer respectively. In both networks red arrows indicate the data ﬂow during sampling  2015b). The former trains an encoder and decoder network jointly minimizing a regularized vari- ational lower bound (Kingma and Welling, 2014). While the latter class of models is purely gen- erative minimizing an MMD based objective, this model works best when coupled with a separate auto-encoder which reduces the dimensionality of the data. An architectural schematic for both classes of models is provided in Fig. 5. Both these models can be trained using standard backprop- agation (Rumelhart et al., 1988). Using the latent variable prior we can directly sample the data distribution of these models without using MCMC procedures (Hinton et al., 2006; Salakhutdinov and Hinton, 2009). We use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling, 2014; Goodfellow et al., 2014). We ﬁrst demonstrate the effectiveness of our test in a setting where we have a theoretical basis for expecting superiority of one unsupervised model versus another. Speciﬁcally, we use a setup where more training samples were used to create one model versus the other. We ﬁnd that the Relative MMD framework agrees with the expected results (models trained with more data generalize better). We then demonstrate how the Relative MMD can be used in evaluating network architecture choices, and we show that our test strongly agrees with other established metrics, but in contrast can provide signiﬁcance results using just the validation data while other methods may require an additional test set. Several practical matters must be considered when applying the Relative MMD test. The selection of kernel can affect the quality of results, particularly more suitable kernels can give a faster conver- gence. In this work we extend the logic of the median heuristic (Gretton et al., 2012b) for bandwidth selection by computing the median pairwise distance between samples from Px and Py and aver- aging that with the median pairwise distance between samples from Px and Pz, which helps to maximize the difference between the two MMD statistics. Although the derivations for the variance of our statistic hold for all cases, the estimates require asymptotic arguments and thus a sufﬁciently large n. Selecting the kernel bandwidth in an appropriate range can therefore substantially increase the power of the test at a ﬁxed sample size. While we observed the median heuristic to work well in our experiments, there are cases where alternative choices of kernel can provide greater power: for instance, the kernel can be chosen to maximize the expected test power on a held-out dataset (Gretton et al., 2012b).  6  Decoder Hidden LayerInputEncoder Hidden LayerLatent VariablesData Space SamplesGMMN Hidden Layer 3PriorGMMN Hidden Layer 1GMMN Hidden Layer 2Latent Space SamplesGMMN Hidden Layer 4Encoder HIdden Layer 1Encoder Hidden Layer 2Decoder Hidden Layer 1Decoder Hidden Layer 2InputData Space SamplesPublished as a conference paper at ICLR 2016  5.1 VARIATIONAL AUTO-ENCODER SAMPLE SIZE AND ARCHITECTURE EXPERIMENTS  We use the architecture from Kingma and Welling (2014) with a hidden layer at both the encoder and decoder and a latent variable layer as shown in Figure 5a. We use sigmoidal activation for the hidden layers of encoder and decoder. For the FreyFace data, we use a Gaussian prior on the latent space and data space. For MNIST, we used a Bernoulli prior for the data space. We ﬁx the training set size of the second auto-encoder to 300 images for the FreyFace data and 1500 images for the MNIST data. We vary the number of training samples for the ﬁrst auto-encoder. We then generate samples from both auto-encoders and compare them using Relative MMD to a held out set of data. We use 1500 FreyFace samples as the target in Relative MMD and 15000 images from MNIST. Since a single sample of the data might lead to better generalization performance by chance, we repeat this experiment multiple times and record whether the relative similarity test indicated a network is preferred or if it failed to reject the null hypothesis. The results are shown in Figure 6 which demonstrates that we are closely following the expected model preferences. Additionally for MNIST we use another separate set of supervised training and test data. We encode this data using both auto-encoders and use logistic regression to obtain a classiﬁcation accuracy. The indicated accuracies closely match the results of the relative similarity test, further validating the test.  (a)  (c)  (b)  Figure 6: We show the effect of (a) varying the train- ing set size of one auto-encoder trained on MNIST data. (c) As a secondary valida- tion we compute the classiﬁcation accu- racy of MNIST on a separate train/test set encoded using encoder 1 and encoder 2. (b) We then show the effect of varying the training set size of one auto-encoder using the FreyFace data. We note that due to the size of the FreyFace dataset, we limit the range of ratios used. From this ﬁgure we see that the results of the relative similarity test match our expectation: more data pro- duces models which more closely match the true distribution.  We consider model selection between networks using different architectures. We train two encoders, one a ﬁxed reference model (400 hidden units and 20 latent variables), and the other varying as speciﬁed in Table 1. 25000 images from the MNIST data set were used for training. We use another 20000 images as the target data in Relative MMD. Finally, we use a set of 10000 training and 10000 test images for a supervised task experiment. We use the labels in the MNIST data and perform training and classiﬁcation using an (cid:96)2-regularized logistic regression on the encoded features. In addition we use the supervised task test data to evaluate the variational lower bound of the data under the two models (Kingma and Welling, 2014). We show the result of this experiment in Table 1. For each comparison we take a different subset of training data which helps demonstrate the variation in  7  0.30.50.80.91.01.52.02.53.04.0Ratio Training Samples for Encoder 2 and Training Samples Encoder 10510152025Number of Times Hypothesis SelectedRelative MMD Decisions (25 trials)Encoder 2 Closer to DataEncoder 1 Closer to DataNo Conclusion1.01.52.02.53.04.05.06.07.0Ratio Training Samples for Encoder 2 and Training Samples Encoder 1020406080100Number of Times Hypothesis SelectedSelected Hypothesis (100 runs)Encoder 2 Closer to DataEncoder 1 Closer to DataNo Conclusion0.30.50.80.91.01.52.02.53.04.0Ratio Training Samples for Encoder 2 and Training Samples Encoder 10.890.900.910.920.930.940.95AccuracyAverage Accuracy Using Autoencoder Features (25 trials)Encoder 2 AccuracyEncoder 1 AccuracyUnencoded AccuracyPublished as a conference paper at ICLR 2016  lower bound and accuracy when re-training the reference architecture. We use a signiﬁcance value of 5% and indicate when the test favors one auto-encoder over another or fails to reject the null hypothesis. We ﬁnd that Relative MMD evaluation of the models closely matches performance on the supervised task and the test set variational lower bound.  Hidden Latent Result VAE 1 200 200 400 800 800  VAE 1 RelativeMMD VAE 1 5 20 50 20 50  Favor VAE 2 Favor VAE 2 Favor VAE 1 Favor VAE 1 Favor VAE 1  Accuracy (%) Accuracy (%) Lower Bound Lower Bound 92.8 ± 0.3 92.6 ± 0.3 94.6 ± 0.2 94.8 ± 0.2 94.2 ± 0.3  VAE 2 94.7 ± 0.2 94.5 ± 0.2 94.0 ± 0.2 93.9 ± 0.2 94.5 ± 0.2  VAE 2 -97 -105 -123.44 -115 -103  VAE 1 -126 -115 -99.6 -111 -101  Table 1: We compare several variational auto encoder (VAE) architectural choices for the number of hidden units in both decoder and encoder and the number of latent variables for the VAE. The reference encoder, denoted encoder 2, has 400 hidden units and 20 latent variables. We denote the competing architectural models as encoder 1. We vary the number of hidden nodes in both the decoder and encoder and the number of latent variables. Our test closely follows the performance difference of the auto-encoder on a supervised task (MNIST digit classiﬁcation) as well as the variational lower bound on a withheld set of data. The data used for evaluating the Accuracy and Lower Bound is separate from that used to train the auto-encoders and for the hypothesis test.  5.2 GENERATIVE MOMENT MATCHING NETWORKS ARCHITECTURE EXPERIMENTS  We demonstrate our hypothesis test on a different class of deep generative models called Generative Moment Matching Networks (GMMN) Li et al. (2015b). This recently introduced model has shown competitive performance in terms of test set likelihood on the MNIST data. Furthermore the training of this model is based on the MMD criterion. Li et al. (2015b) proposes to use that model along with an auto-encoder, which is the setup we employ in this work. Here a standard auto-encoder model is trained on the data to obtain a low dimensional representation, then a GMMN network is trained on the latent representations (Figure 5). We use the relative similarity test to evaluate various architectural choices in this new class of mod- els. We start from the baseline model speciﬁed in Li et al. (2015b) and associated software. The details of the reference model are speciﬁed in Figure 5. We vary the number of auto-encoder hidden layers (1 to 4), generative model layers(1, 4, or 5), the number of network nodes (all or 50% of the reference model), and use of drop-out on the auto- encoder. We use the same training set of 55000, validation set of 5000 and test set of 10000 as in (Li et al., 2015b; Goodfellow et al., 2014). In total we train 48 models. We use these to compare 4 simpliﬁed binary network architecture choices using the Relative MMD: using dropout on the auto-encoder, few (1) or more (4 or 5) GMMN layers, few (1 or 2) or more (3 or 4) auto-encoder layers, and the number of network nodes. We use our test to compare these model settings using the validation set as the target in the relative similarity test, and samples from the models as the two sources. To validate our results we compare it to likelihoods computed on the test set. The results are shown in Table 2. We see that the likelihood results computed on a separate test set follow the conclusions obtained from MMD on the validation set. Particularly, we ﬁnd that using fewer hidden layers for the GMMN and more hidden nodes generally produces better models.  5.3 DISCUSSION  In these experiments we have seen that the Relative MMD test can be used to compare deep gen- erative models obtaining judgements aligned with other metrics. Comparisons to other metrics are important for verifying our test is sensible, but it can occlude the fact that MMD is a valid eval- uation technique on its own. When evaluating only sample generating models where likelihood computation is not possible, MMD is an appropriate and tractable metric to consider in addition to Parzen-Window log likelihoods and visual appearance of the samples. In several ways it is poten- tially more appropriate than Parzen-windows as it allows one to consider directly the discrepancy between the test data samples and the model samples while allowing for signiﬁcance results. In such a situation, comparing the performance of several models using the MMD against a single set of test  8  Published as a conference paper at ICLR 2016  Experimental Condition (A/B) Dropout/No Dropout More/Fewer GMMN Layers More/Fewer Nodes More/Fewer AE layers  Inconclusive  RelativeMMD Preference Avg Likelihood A Avg Likelihood B B A 76.76 ± 42.83 −9.01 ± 55.43 199 360 249.6 ± 8.07 393 −73.99 ± 40.96 105 125.2 ± 43.4 −57 ± 49.57 450 113 25.96 ± 55.85 41.78 ± 44.07 324 231  17 14 13 21  Table 2: For each experimental condition (e.g. dropout or no dropout) we show the number of times the Relative MMD prefers models in group 1 or 2 and number of inconclusive tests. We use the validation set as the target data for Relative MMD. An average likelihood for the MNIST test set for each group is shown with error bars. We can see that the MMD choices are in agreement with likelihood evaluations. Particularly we identify that models with fewer GMMN layers and models with more nodes have more favourable samples, which is conﬁrmed by the likelihood results.  samples, the Relative MMD test can provide an automatic signiﬁcance value without expensive cross-validation procedures. Gaussian kernels are closely related to Parzen-window estimates, thus computing an MMD in this case can be considered related to comparing Parzen window log-likelihoods. The MMD gives sev- eral advantages, however. First, the asymptotics of MMD are quite different to Parzen-windows, since the Parzen-window bandwidth shrinks as n grows. Asymptotics of relative tests with shrink- ing bandwidth are unknown: even for two samples this is challenging (Krishnamurthy et al., 2015). Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and Rafsky, 1979; Hall and Tajvidi, 2002). This is because the tests above rely on graph edge counting or nearest neighbor-type statistics, and null distributions are obtained via combinatorial arguments which are not easily extended from two to three samples. MMD is a U-statistic, hence its asymptotic behavior is much more easily generalised to multiple dependent statistics. There are two primary advantages of the MMD over the variational lower bound, where it is known (Kingma and Welling, 2014): ﬁrst, we have a characterization of the asymptotic behavior, which allows us to determine when the difference in performance is signiﬁcant; second, comparing two lower bounds produced from two different models is unreliable, as we do not know how conservative either lower bound is.  6 CONCLUSION  We have described a novel non-parametric statistical hypothesis test for relative similarity based on the Maximum Mean Discrepancy. The test is consistent, and the computation time is quadratic. Our proposed test statistic is theoretically justiﬁed for the task of comparing samples from arbitrary distributions as it can be shown to converge to a quantity which compares all moments of the two pairs of distributions. We evaluate test performance on synthetic data, where the degree of similarity can be controlled. Our experimental results on model selection for deep generative networks show that Relative MMD can be a useful approach to comparing such models. There is a strong correspondence between the test resuts and the expected likelihood, prediction accuracy, and variational lower bounds on the models tested. Moreover, our test has the advantage over these alternatives of providing guarantees of statistical signiﬁcance to its conclusions. This suggests that the relative similarity test will be useful in evaluating hypotheses about network architectures, for example that AE-GMMN models may generalize better when fewer layers are used in the generative model. Code for our method is available.2  ACKNOWLEDGMENTS  We thank Joel Veness for helpful comments. This work is partially funded by Internal Funds KU Leuven, ERC Grant 259112, FP7-MC-CIG 334380, the Royal Academy of Engineering through the Newton Alumni Scheme, and DIGITEO 2013-0788D-SOPRANO. WB is supported by a Centrale- Sup´elec fellowship.  2 https://github.com/eugenium/MMD  9  Published as a conference paper at ICLR 2016  REFERENCES Y. Bengio. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2(1):  1–127, 2009.  Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks trainable by backprop. In Proceedings of the 31st International Conference on Machine Learning, 2014.  W. Bounliphone, A. Gretton, A. Tenenhaus, and M. B. Blaschko. A low variance consistent test of relative dependency. In F. Bach and D. Blei, editors, Proceedings of The 32nd International Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings, pages 20–29, 2015.  G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via maximum  mean discrepancy optimization. In Conference on Uncertainty in Artiﬁcial Intelligence, 2015.  J. H. Friedman and L. C. Rafsky. Multivariate generalizations of the Wald-Wolfowitz and Smirnov  two-sample tests. The Annals of Statistics, pages 697–717, 1979.  K. Fukumizu, A. Gretton, X. Sun, and B. Sch¨olkopf. Kernel measures of conditional dependence.  pages 489–496, Cambridge, MA, 2008. MIT Press.  I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672–2680, 2014.  A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. J. Smola. A kernel method for the two-sample-problem. In Advances in neural information processing systems, pages 513–520, 2006.  A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, and A. Smola. A kernel two-sample test.  The Journal of Machine Learning Research, 13(1):723–773, 2012a.  A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, K. Fukumizu, and B. K. Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1205–1213. 2012b.  P. Hall and N. Tajvidi. Permutation tests for equality of distributions in high-dimensional settings.  Biometrika, 89(2):359–374, 2002.  G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural  computation, 18(7):1527–1554, 2006.  W. Hoeffding. A class of statistics with asymptotically normal distribution. The annals of mathe-  matical statistics, pages 293–325, 1948.  D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on  Learning Representations, 2014.  D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pages 3581–3589, 2014.  A. Krishnamurthy, K. Kandasamy, B. P´oczos, and L. A. Wasserman. On estimating L2  2 divergence. In Proceedings of the Eighteenth International Conference on Artiﬁcial Intelligence and Statistics, 2015.  A. Krizhevsky, I. Sutskever, and G. E. Hinton.  Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.  H. Larochelle and I. Murray. The neural autoregressive distribution estimator. Journal of Machine  Learning Research, 15:29–37, 2011.  10  Published as a conference paper at ICLR 2016  Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document  recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  S. Li, Y. Xie, H. Dai, and L. Song. M-statistic for kernel change-point detection. In Advances in  Neural Information Processing Systems, pages 3348–3356, 2015a.  Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In International Confer-  ence on Machine Learning, pages 1718–1727, 2015b.  J. R. Lloyd and Z. Ghahramani. Statistical model criticism using kernel two sample tests. In Ad-  vances in Neural Information Processing Systems, 2015.  C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair auto encoder. In  International Conference on Learning Representations, 2016.  P. R. Rosenbaum. An exact distribution-free test comparing two multivariate distributions based on adjacency. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(4): 515–530, 2005.  D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. In J. A. Anderson and E. Rosenfeld, editors, Neurocomputing: Foundations of Research, pages 696–699. MIT Press, Cambridge, MA, USA, 1988.  R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In International Conference on  Artiﬁcial Intelligence and Statistics, pages 448–455, 2009.  R. J. Serﬂing. Approximation theorems of mathematical statistics, volume 162. John Wiley & Sons,  2009.  B. K. Sriperumbudur, A. Gretton, K. Fukumizu, G. R. Lanckriet, and B. Sch¨olkopf. Hilbert space embeddings and metrics on probability measures. Journal of Machine Learning Research, 11: 1517–1561, 2010.  A DETAILED DERIVATIONS OF THE TEST VARIANCE AND COVARIANCE  The variance and the covariance for a U-statistic is described in Hoeffding (1948, Eq. 5.13) and Serﬂing (2009, Chap. 5). Let V := (v1, ..., vm) be m iid random variables where v := (x, y) ∼ Px × Py. An unbiased estimator of MMD2(F, Px, Py) is  MMD2  u(F, Xm, Yn) =  1  m(m − 1)  h(vi, vj)  i(cid:54)=j with h(vi, vj) = k(xi, xj) + k(yi, yj) − k(xi, yj) − k(xj, yi). Similarly, let W := (w1, ..., wm) be m iid random variables where w := (x, z) ∼ Px × Pz. An unbiased estimator of MMD2(F, Px, Pz) is  m(cid:88)  m(cid:88)  MMD2  u(F, Xm, Zr) =  1  m(m − 1)  g(wi, wj)  i(cid:54)=j with g(wi, wj) = k(xi, xj) + k(zi, zj) − k(xi, zj) − k(xj, zi) Then the variance/covariance for a U-statistic with a kernel of order 2 is given by  V ar(MMD2  u) =  4(m − 2) m(m − 1)  ζ1 +  2  m(m − 1)  ζ2  Equation (13) with neglecting higher terms can be written as  V ar(MMD2  u) =  4(m − 2) m(m − 1)  ζ1 + O(m−2)  (11)  (12)  (13)  (14)  where for the variance term, ζ1 = Var [Ev1 [h(v1, V2)]] and for the covariance term ζ1 = Var [Ev1,w1 [h(v1, V2)g(w1, W2)]].  11  Published as a conference paper at ICLR 2016  Notation [ ˜Kxx]ij = [Kxx]ij for all i (cid:54)= j and [ ˜Kxx(cid:48)]ij = 0 for j = i. Same for ˜Kyy and ˜Kzz. We will also make use of the fact that k(xi, xj) = (cid:104)φ(xi), φ(xj)(cid:105) for an appropriately chosen inner product, and function φ. We then denote  (cid:90)  φ(x)dPx.  (15)  µx :=  A.1 VARIANCE OF MMD  We note many terms in expansion of the squares above cancel out due to independence. For example Ex1,y1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)] = 0. We can thus simplify to the following expression for ζ1  (Ex2,y2 [h(x1, y1)])2(cid:105) −(cid:0)MMD2(F, PX , PY )(cid:1)2 (cid:104) (cid:2)((cid:104)φ(x1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105))2(cid:3) −(cid:0)MMD2(F, PX , PY )(cid:1)2 (cid:2)(cid:104)φ(x1), µx(cid:105)2 + 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(y1), µy(cid:105) − 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)  (17) (18)  (16)  ζ1 = Ex1,y1  = Ex1,y1  = Ex1,y1  − 2(cid:104)φ(x1), µx(cid:105)(cid:104)φ(y1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105)2 − 2(cid:104)φ(y1), µy(cid:105)(cid:104)φ(x1), µy(cid:105) − 2(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105) + (cid:104)φ(x1), µy(cid:105)2 + 2(cid:104)φ(x1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)  + (cid:104)φ(y1), µx(cid:105)2(cid:3) −(cid:0)MMD2(F, PX , PY )(cid:1)2  = Ex1 [(cid:104)φ(x1), µx(cid:105)2] − Ex1[(cid:104)φ(x1), µx(cid:105)]2  − 2(Ex1[(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)]) + Ey1 [(cid:104)φ(y1), µy(cid:105)2] − Ey1 [(cid:104)φ(y1), µy(cid:105)]2 − 2(Ey1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)] − Ey1[(cid:104)φ(y1), µy(cid:105)]Ey1[(cid:104)φ(y1), µx(cid:105)]) + Ex1[(cid:104)φ(x1), µy(cid:105)2] − Ex1 [(cid:104)φ(x1), µy(cid:105)]2 + Ey1 [(cid:104)φ(y1), µx(cid:105)2] − Ey1 [(cid:104)φ(y1), µx(cid:105)]2  (19)  Substituting empirical expectations over the data sample for the population expectations in Eq. (19) gives  (cid:18)  (cid:19)2  eT ˜Kxxe  ζ1 ≈  1  (cid:18)  m(m − 1)2 eT ˜Kxx ˜Kxxe − m(m − 1)n 1  − 2  1  1  m(m − 1) eT ˜KxxKxye −  +  (cid:18) n(n − 1)2 eT ˜Kyy ˜Kyye − − 2 (cid:18) 1  n(n − 1)m eT KyxKxye − 2  eT ˜KyyKyxe −  +  1  1  n2m  eT ˜KxxeeT Kxye  (cid:18)  1  m2(m − 1)n 1  eT ˜Kyye  (cid:19)2  n(n − 1) 1  (cid:19)2  n2(n − 1)m  eT ˜KyyeeT Kxye  eT Kxye  +  1  m2n  eT KxyKyxe  (cid:19)  (20)  (cid:19)  nm  12  Published as a conference paper at ICLR 2016  Derivation of the ﬁrst term for example  Ex1[(cid:104)x1, µx(cid:105)2] ≈ 1 m  (cid:104)φ(xi),  m(cid:88)  i=1  φ(xj)(cid:105)(cid:104)φ(xi),  1  m − 1  m(cid:88)  k=1 k(cid:54)=i  φ(xk)(cid:105)  (21)  m(cid:88) m(cid:88)  j=1 j(cid:54)=i  1  m − 1  m(cid:88)  m(cid:88)  i=1  j=1 j(cid:54)=i  k=1 k(cid:54)=i  1  m(m − 1)2  =  =  1  m(m − 1)2 eT ˜Kxx ˜Kxxe  k(xi, xj)k(xi, xk)  A.2 COVARIANCE OF MMD  We note many terms in expansion of the squares above cancel out due to independence. For example Ex1,z1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(z1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ez1 [(cid:104)φ(z1), µz(cid:105)] = 0. We can thus simplify to the following expression for ζ1  ζ1 = Ex1,y1,z1 [Ex2,y2,z2 [h(x1, y1)g(x1, z1)]] −(cid:0)MMD2(F, PX , PY ) MMD2(F, PX , PZ)(cid:1)  (22)  (23)  = Ex1,y1,z1[((cid:104)φ(x1), µx(cid:105) + (cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)φ(x1), µy)(cid:105))  ((cid:104)φ(x1), µx)(cid:105) + (cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105))]  − MMD2(F, PX , PY ) MMD2(F, PX , PZ)  (cid:2)(cid:104)φ(x1), µx(cid:105)2(cid:3) − Ex1 [(cid:104)φ(x1), µx(cid:105)]2  = Ex1  1  (cid:18)  − (Ex1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)]) − (Ex1 [(cid:104)φ(x1), µx(cid:105)(cid:104)φ(x1), µy(cid:105)] − Ex1 [(cid:104)φ(x1), µx(cid:105)] Ex1 [(cid:104)φ(x1), µy(cid:105)]) + Ex1 [(cid:104)φ(x1), µy(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)] (cid:18) m(m − 1)2 eT ˜Kxx ˜Kxxe − (cid:18) − (cid:18) 1  m(m − 1) 1  eT ˜KxxKxye −  eT ˜KxxKxze −  m2(m − 1)n  eT ˜KxxeeT Kxze  eT ˜KxxeeT Kxze  m2(m − 1)r  m(m − 1)n  m(m − 1)r  (cid:19) (cid:19)  eT ˜Kxxe  (cid:19)2  (cid:19)  −  1  1  1  1  +  mnr  eT KyxKxze − 1  m2nr  eT KxyeeT Kxze  ≈  A.3 DERIVATION OF THE VARIANCE OF THE DIFFERENCE OF TWO MMD STATISTICS  In this section we propose an alternate strategy of deriving directly the variance of a u-statistic of the difference of MMDs with a joint variable. This formulation agrees with the derivation of the covariance matrix and subsequent projection, and provides extra insights. Let D := (d1, ..., dm) be m iid random variables where d := (x, y, z) ∼ Px × Py × Pz. Then the difference of the unbiased estimators of MMD2(F, Px, Py) and MMD2(F, Px, Pz) is given by  MMD2  u(F, x, y) − MMD2  u(F, x, z) =  1  m(m − 1)  f (di, dj)  (24)  with f, the kernel of MMD2(F, Px, Py) − MMD2(F, Px, Pz) of order 2 as follows  f (d1, d2) = (k(x1, x2) + k(y1, y2) − k(x1, y2) − k(x2, y1))  − (k(x1, x2) + k(z1, z2) − k(x1, z2) − k(x2, z1))  = (k(y1, y2) − k(x1, y2) − k(x2, y1)) − (k(z1, z2) − k(x1, z2) − k(x2, z1))  (25)  m(cid:88)  i(cid:54)=j  13  Published as a conference paper at ICLR 2016  Equation (24) is a U-statistic and thus we can apply Equation (14) to obtain its variance. We ﬁrst note  Ed1(f (d1, d2)) :=(cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105)  − ((cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)µx, φ(z1)(cid:105))  Ed1,d2(f (d1, d2)) := MMD2(F, Px, Py) − MMD2(F, Px, Pz)  We are now ready to derive the dominant leading term,ζ1, in the variance expression (14).  Term ζ1 ζ1 : = Var(Ed1 (f (d1, d2)))  = Ex1,y1,z1[((cid:104)φ(y1), µy(cid:105) − (cid:104)φ(x1), µy(cid:105) − (cid:104)µx, φ(y1)(cid:105) − ((cid:104)φ(z1), µz(cid:105) − (cid:104)φ(x1), µz(cid:105) − (cid:104)µx, φ(z1)(cid:105))2]  − (MMD2(F, Px, Py) − MMD2(F, Px, Pz))2  (29) We note many terms in expansion of the squares above cancel out due to independence. For example Ey1,z1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(z1), µz(cid:105)] − Ey1[(cid:104)φ(y1), µy(cid:105)] Ez1 [(cid:104)φ(z1), µz(cid:105)] = 0. We can thus simplify to the following expression for ζ1 ζ1 = Ey1 [(cid:104)φ(y1), µy(cid:105)2] − Ey1[(cid:104)φ(y1), µy(cid:105)]2  (30)  (26) (27)  (28)  + Ex1[(cid:104)φ(x1), µy(cid:105)2] − Ex1[(cid:104)φ(x1), µy(cid:105)]2 + Ey1 [(cid:104)µx, φ(y1)(cid:105)2] − Ey1[(cid:104)µx, φ(y1)(cid:105)]2 + Ez1 [(cid:104)φ(z1), µz(cid:105)2] − Ez1[(cid:104)φ(z1), µz(cid:105)]2 + Ex1[(cid:104)φ(x1), µz(cid:105)2] − Ex1[(cid:104)φ(x1), µz(cid:105)]2 + Ez1 [(cid:104)µx, φ(z1)(cid:105)2] − Ez1 [(cid:104)µx, φ(z1)(cid:105)]2 − 2(Ey1[(cid:104)φ(y1), µy(cid:105)(cid:104)µx, φ(y1)(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ey1[(cid:104)µx, φ(y1)(cid:105)]) − 2(Ex1 [(cid:104)φ(x1), µy(cid:105)(cid:104)φ(x1), µz(cid:105)] − Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)]) − 2(Ez1[(cid:104)φ(z1), µz(cid:105)(cid:104)µx, φ(z1)(cid:105)] − Ez1 [(cid:104)φ(z1), µz(cid:105)] Ez1[(cid:104)µx, φ(z1)(cid:105)])  We can empirically approximate these terms as follows:  (cid:19)2  (31)  ζ1 ≈  (cid:19)2  1  n(n − 1)  eT ˜Kyye  (cid:19)2 (cid:19)2  (cid:19)2 (cid:19)2  1  1  +  n2m  eT K T  (cid:18) n(n − 1)2 eT ˜Kyy ˜Kyye − (cid:18) 1 xyKxye − (cid:18) 1 1 nm2 eT KxyK T r(r − 1)2 eT ˜Kzz ˜Kzze − (cid:18) 1 (cid:18) 1 1 rm2 eT KxzK T 1  xye −  xze −  +  +  +  1  rm  nm  nm  (cid:18)  +  r2m  eT K T  xzKxze − 1  (cid:18) (cid:18) 1 (cid:18)  n(n − 1)m eT ˜K T  nmr 1  r(r − 1)m  − 2  − 2  − 2  xyKxze − 1 nm xze −  eT ˜KzzK T  rm  eT ˜KyyKyxe −  eT Kxye  eT Kxye  eT Kxze  eT Kxze  1  r(r − 1)  eT ˜Kzze  1  (cid:19)  eT ˜Kyye × 1 nm  n(n − 1) eT Kxye × 1 rm eT ˜Kyye × 1 nm  n(n − 1)  eT Kxze  1  (cid:19)  eT Kxye  (cid:19)  eT Kxye  14  Published as a conference paper at ICLR 2016  A.4 EQUALITY OF DERIVATIONS  XY + σXZ − 2σXY XZ = Ey1 σ2  In this section, we prove that Equation (9) is equal to the variance of the difference of 2 MMD2(F, Px, Py) and MMD2(F, Px, Pz).  (32)  (cid:2)(cid:104)φ(y1), µy(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µy(cid:105)]2 (cid:2)(cid:104)φ(z1), µy(cid:105)2(cid:3) − Ez1 [(cid:104)φ(z1), µy(cid:105)]2 (cid:2)(cid:104)φ(x1), µy(cid:105)2(cid:3) − Ex1 [(cid:104)φ(x1), µy(cid:105)]2 (cid:2)(cid:104)φ(y1), µz(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µz(cid:105)]2 (cid:2)(cid:104)φ(y1), µx(cid:105)2(cid:3) − Ey1 [(cid:104)φ(y1), µx(cid:105)]2 (cid:2)(cid:104)φ(z1), µx(cid:105)2(cid:3) − Ez1 [(cid:104)φ(z1), µx(cid:105)]2  + Ez1 − 2 (Ey1 [(cid:104)φ(y1), µy(cid:105)(cid:104)φ(y1), µx(cid:105)] − Ey1 [(cid:104)φ(y1), µy(cid:105)] Ey1 [(cid:104)φ(y1), µx(cid:105)]) − 2 (Ez1 [(cid:104)φ(z1), µz(cid:105)(cid:104)φ(z1), µx(cid:105)] − Ez1 [(cid:104)φ(z1), µz(cid:105)] Ez1 [(cid:104)φ(z1), µx(cid:105)]) + Ex1 + Ey1 + Ey1 + Ez1 − 2 (Ex1 [(cid:104)φ(x1), µy(cid:105)] Ex1 [(cid:104)φ(x1), µz(cid:105)])  We have shown that Equation (9) is equal to Equation (31).  B CALIBRATION OF THE TEST  We show here that our derived test is well calibrated. A calibrated test should output a uniform distribution of p-values when the two MMD distances are equal. The empirical distributions of p-values for various sets of Px, Py and Pz are given in Figure 8. Similarly, for a given signiﬁcance level α, the false positive rate should be equal to α. The empirical false positive rates for varying α are shown in Figure 7 further demonstrating the proper calibration of the test.  y c n e u q e r F  p-values  Figure 7: Calibration of the rela-  tive similarity test  15  00.20.40.60.8100.20.40.60.81Published as a conference paper at ICLR 2016  (a) Illustration of the synthetic data with different  means for X, Y and Z.  (b) Uniform histogram of p-values  (c) Illustration of the synthetic data with different  means and orientations for X, Y and Z.  (d) Uniform histogram of p-values  (e) Illustration of the synthetic data with different ori-  entations for X, Y and Z.  (f) Uniform histogram of p-values  Figure 8: Calibration of the relative similarity test  16  −40−2002040−30−20−100102030Illustration of the synthetic dataset  X:µX:[00],αX=0Y:µY:[-20-20],αY=0Z:µZ:[2020],αZ=000.20.40.60.81020406080Histogram of pvalues for the relative similarity test−30−20−10010203040−25−20−15−10−50510152025Illustration of the synthetic dataset  X:µX:[00],αX=0Y:µY:[-20-20],αY=π/2Z:µZ:[2020],αZ=π/200.20.40.60.81020406080Histogram of pvalues for the relative similarity test−15−10−5051015−15−10−5051015Illustration of the synthetic dataset  X:µX:[00],αX=0Y:µY:[00],αY=π/2Z:µZ:[00],αZ=π/200.20.40.60.810102030405060Histogram of pvalues for the relative similarity test",
1511.06530,2016,Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications,"['Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications\nYong-Deok Kim', 'Eunhyeok Park', 'Sungjoo Yoo', 'Taelim Choi', 'Lu Yang', 'Dongjun Shin']",https://arxiv.org/pdf/1511.06530,"6 1 0 2     b e F 4 2         ]  V C . s c [      2 v 0 3 5 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  COMPRESSION OF DEEP CONVOLUTIONAL NEURAL NETWORKS FOR FAST AND LOW POWER MOBILE AP- PLICATIONS  Yong-Deok Kim1, Eunhyeok Park2, Sungjoo Yoo2, Taelim Choi1, Lu Yang1 & Dongjun Shin1  1Software R&D Center, Device Solutions, Samsung Electronics, South Korea {yd.mlg.kim, tl.choi, lu2014.yang, d.j.shin}@samsung.com  2Department of Computer Science and Engineering, Seoul National University, South Korea {canusglow, sungjoo.yoo}@gmail.com  ABSTRACT  Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as Ima- geNet classiﬁcation on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the en- tire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian ma- trix factorization, (2) Tucker decomposition on kernel tensor, and (3) ﬁne-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGG- S, GoogLeNet, and VGG-16) on the smartphone. Signiﬁcant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in ac- curacy. In addition, we address the important implementation level issue on 1 × 1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.  1  INTRODUCTION  Deployment of convolutional neural networks (CNNs) for computer vision tasks on mobile devices is gaining more and more attention. On mobile applications, it is typically assumed that training is performed on the server and test or inference is executed on the mobile devices. One of the most critical issues in mobile applications of CNNs is that mobile devices have strict constraints in terms of computing power, battery, and memory capacity. Thus, it is imperative to obtain CNNs tailored to the limited resources of mobile devices. Deep neural networks are known to be over-parameterized, which facilitates convergence to good local minima of the loss function during training (Hinton et al., 2012; Denil et al., 2013). To im- prove test-time performance on mobile devices, such redundancy can be removed from the trained networks without noticeable impact on accuracy. Recently, there are several studies to apply low- rank approximations to compress CNNs by exploiting redundancy (Jaderberg et al., 2014; Denton et al., 2014; Lebedev et al., 2015). Such compressions typically focus on convolution layers since they dominate total computation cost especially in deep neural networks (Simonyan & Zisserman, 2015; Szegedy et al., 2015). Existing methods, though effective in reducing the computation cost of a single convolutional layer, introduce a new challenge called whole network compression which aims at compressing the entire network. Whole network compression: It is nontrivial to compress whole and very deep CNNs for complex tasks such as ImageNet classiﬁcation. Recently, Zhang et al. (2015b;a) showed that entire convo- lutional layers can be accelerated with “asymmetric (3d)” decomposition. In addition, they also presented the effective rank selection and optimization method. Although their proposed decom-  1  Published as a conference paper at ICLR 2016  position of layers can be easily implemented in popular development tools (e.g. Caffe, Torch, and Theano), the rank selection and optimization parts still require because they consist of multiple steps and depend on the output of previous layers. In this paper, we present much simpler but still power- ful whole network compression scheme which takes entire convolutional and fully-connected layers into account. Contribution: This paper makes the following major contributions.  steps: (1) rank selection, (2) low-rank tensor decomposition, and (3) ﬁne-tuning.  • We propose a one-shot whole network compression scheme which consists of simple three • In the proposed scheme, Tucker decomposition (Tucker, 1966) with the rank determined by a global analytic solution of variational Bayesian matrix factorization (VBMF) (Nakajima et al., 2012) is applied on each kernel tensor. Note that we simply minimize the recon- struction error of linear kernel tensors instead of non-linear responses. Under the Tucker decomposition, the accumulated loss of accuracy can be sufﬁciently recovered by using ﬁne-tuning with ImageNet training dataset. • Each step of our scheme can be easily implemented using publicly available tools, (Naka- jima, 2015) for VBMF, (Bader et al., 2015) for Tucker decomposition, and Caffe for ﬁne- tuning. • We evaluate various compressed CNNs (AlexNet, VGG-S,GoogLeNet, and VGG-16) on both Titan X and smartphone. Signiﬁcant reduction in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. • By analysing power consumption over time, we observe interesting behaviours of 1 × 1 convolution which is the key operation in our compressed model as well as in inception module of GoogLeNet. Although the 1 × 1 convolution is mathematically simple opera- tion, it is considered to lack in cache efﬁciency, hence it is the root cause of gap between theoretical and practical speed up ratios.  This paper is organized as follows. Section 2 reviews related work. Section 3 explains our proposed scheme. Section 4 gives experimental results. Section 5 summarizes the paper.  2 RELATED WORK  2.1 CNN COMPRESSION  CNN usually consists of convolutional layers and fully-connected layers which dominate compu- tation cost and memory consumption respectively. After Denil et al. (2013) showed the possibility of removing the redundancy of neural networks, several CNN compression techniques have been proposed. A recent study (Denton et al., 2014) showed that the weight matrix of a fully-connected layer can be compressed by applying truncated singular value decomposition (SVD) without signif- icant drop in the prediction accuracy. More recently, various methods based on vector quantization (Gong et al., 2014), hashing techniques (Chen et al., 2015), circulant projection (Cheng et al., 2015), and tensor train decomposition (Novikov et al., 2015) were proposed and showed better compres- sion capability than SVD. To speed up the convolutional layers, several methods based on low-rank decomposition of convolutional kernel tensor were proposed (Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2015), but they compress only single or a few layers. Concurrent with our work, Zhang et al. (2015b) presented “asymmetric (3d) decomposition” to accelerate the entire convolutional layers, where the original D × D convolution is decomposed to D × 1, 1 × D, and 1 × 1 convolution. In addition, they also present a rank selection method based on PCA accumulated energy and an optimization method which minimizes the reconstruction error of non-linear responses. In the extended version (Zhang et al., 2015a), the additional ﬁne- tuning of entire network was considered for further improvement. Compared with these works, our proposed scheme is different in that (1) Tucker decomposition is adopted to compress the entire convolutional and fully-connected layers, (2) the kernel tensor reconstruction error is minimized instead of non-linear response, (3) a global analytic solution of VBMF (Nakajima et al., 2012) is applied to determine the rank of each layer, and (4) a single run of ﬁne-tuning is performed to account for the accumulation of errors.  2  Published as a conference paper at ICLR 2016  Figure 1: Mode-1 (top left), mode-2 (top right), and mode-3 (bottom left) matricization of the 3-way tensor. They are constructed by concatenation of frontal, horizontal, and vertical slices, respectively. (Bottom right): Illustration of 3-way Tucker decomposition. The original tensor X of size I1×I2×I3 is decomposed to the product of the core tensor S of size J1 × J2 × J3 and factor matrices A(1), A(2), and A(3).  A pruning approach (Han et al., 2015b;a) also aims at reducing the total amount of parameters and operations in the entire network. Pruning based approaches can give signiﬁcant reductions in parameter size and computation workload. However, it is challenging to achieve runtime speed-up with conventional GPU implementation as mentioned in (Han et al., 2015a). Orthogonal to model level compression, implementation level approaches were also proposed. The FFT method was used to speed-up convolution (Mathieu et al., 2013). In (Vanhoucke et al., 2011), CPU code optimizations to speed-up the execution of CNN are extensively explored.  2.2 TENSOR DECOMPOSITION  A tensor is a multi-way array of data. For example, a vector is 1-way tensor and a matrix is 2- way tensor. Two of the most popular tensor decomposition models are CANDECOMP/PARAFAC model (Carroll & Chang, 1970; Harshman & Lundy, 1994; Shashua & Hazan, 2005) and Tucker model (Tucker, 1966; De Lathauwer et al., 2000; Kim & Choi, 2007). In this paper, we exten- sively use Tucker model for whole network compression. Tucker decomposition is a higher order extension of the singular value decomposition (SVD) of matrix, in the perspective of computing the orthonormal spaces associated with the different modes of a tensor. It simultaneously analyzes mode-n matricizations of the original tensor, and merges them with the core tensor as illustrated in Fig. 1. In our whole network compression scheme, we apply Tucker-2 decomposition, which is also known as GLRAM (Ye, 2005), from the second convolutional layer to the ﬁrst fully connected layers. For the other layers, we apply Tucker-1 decomposition, which is equivalent to SVD. For more information on the tensor decomposition, the reader is referred to the survey paper (Kolda & Bader, 2009).  3 PROPOSED METHOD  Fig. 2 illustrates our one-shot whole network compression scheme which consists of three steps: (1) rank selection; (2) Tucker decomposition; (3) ﬁne-tuning. In the ﬁrst step, we analyze principal subspace of mode-3 and mode-4 matricization of each layer’s kernel tensor with global analytic variational Bayesian matrix factorization. Then we apply Tucker decomposition on each layer’s kernel tensor with previously determined rank. Finally, we ﬁne-tune the entire network with standard back-propagation.  3  Published as a conference paper at ICLR 2016  Figure 2: Our one-shot whole network compression scheme consists of (1) rank selection with VBMF; (2) Tucker decomposition on kernel tensor; (3) ﬁne-tuning of entire network. Note that Tucker-2 decomposition is applied from the second convolutional layer to the ﬁrst fully connected layers, and Tucker-1 decomposition to the other layers.  3.1 TUCKER DECOMPOSITION ON KERNEL TENSOR Convolution kernel tensor: In CNNs, the convolution operation maps an input (source) tensor X of size H × W × S into output (target) tensor Y of size H(cid:48) × W (cid:48) × T using the following linear mapping:  Yh(cid:48),w(cid:48),t =  Ki,j,s,t Xhi,wj ,s,  D(cid:88)  D(cid:88)  S(cid:88)  i=1  j=1  s=1  hi = (h(cid:48) − 1)∆ + i − P and wj = (w(cid:48) − 1)∆ + j − P,  (1) where K is a 4-way kernel tensor of size D × D × S × T , ∆ is stride, and P is zero-padding size. Tucker Decomposition: The rank-(R1, R2, R3, R4) Tucker decomposition of 4-way kernel tensor K has the form:  R1(cid:88)  R2(cid:88)  R3(cid:88)  R4(cid:88)  r1=1  r2=1  r3=1  r4=1  Ki,j,s,t =  C(cid:48)  r1,r2,r3,r4  U (1) i,r1  U (2) j,r2  U (3) s,r3  U (4) t,r4,  where C(cid:48) is a core tensor of size R1 × R2 × R3 × R4 and U (1), U (2), U (3), and U (4) are factor matrices of sizes D × R1, D × R2, S × R3, and T × R4, respectively. In the Tucker decomposition, every mode does not have to be decomposed. For example, we do not decompose mode-1 and mode-2 which are associated with spatial dimensions because they are al- ready quite small (D is typically 3 or 5). Under this variant called Tucker-2 decomposition (Tucker, 1966), the kernel tensor is decomposed to:  Ki,j,s,t =  (2) where C is a core tensor of size D × D × R3 × R4. After substituting (2) into (1), performing rearrangements and grouping summands, we obtain the following three consecutive expressions for the approximate evaluation of the convolution (1):  r3=1  r4=1  s,r3  U (4) t,r4,  Ci,j,r3,r4 U (3)  R3(cid:88)  R4(cid:88)  Zh,w,r3 =  Z(cid:48) h(cid:48),w(cid:48),r4  =  Yh(cid:48),w(cid:48),t =  Xh,w,s,  U (3) s,r3  D(cid:88)  R3(cid:88)  Ci,j,r3,r4 Zhi,wj ,r3,  j=1  r3=1  t,r4 Z(cid:48) U (4)  h(cid:48),w(cid:48),r4  ,  (3)  (4)  (5)  s=1  S(cid:88) D(cid:88) R4(cid:88)  i=1  r4=1  4  Published as a conference paper at ICLR 2016  Figure 3: Tucker-2 decompositions for speeding-up a convolution. Each transparent box corre- sponds to 3-way tensor X , Z, Z(cid:48), and Y in (3-5), with two frontal sides corresponding to spatial dimensions. Arrows represent linear mappings and illustrate how scalar values on the right are com- puted. Yellow tube, red box, and blue tube correspond to 1 × 1, D × D, and 1 × 1 convolution in (3), (4), and (5) respectively.  where Z and Z(cid:48) are intermediate tensors of sizes H × W × R3 and H(cid:48) × W (cid:48) × R4, respectively. 1 × 1 convolution: As illustrated in Fig. 3, computing Z from X in (3) as well as Y from Z(cid:48) in (5) is 1 × 1 convolutions that essentially perform pixel-wise linear re-combination of input maps. It is introduced in network-in-network (Lin et al., 2014) and extensively used in inception module of GoogLeNet (Szegedy et al., 2015). Note that computing (3) is similar to inception module in the sense that D × D convolution is applied after dimensional reduction with 1 × 1 convolution, but different in the sense that there is no non-linear ReLU function between (3) and (4). In addition, similar to (Zhang et al., 2015b;a), we compute smaller intermediate output tensor Z(cid:48) in (4) and then recover its size in (5). The Tucker-2 decomposition naturally integrates two compression techniques. Complexity analysis: The convolution operation in (1) requires D2ST parameters and D2ST H(cid:48)W (cid:48) multiplication-addition operations. With Tucker decomposition, compression ratio M and speed-up ratio E are given by:  M =  D2ST  SR3 + D2R3R4 + T R4  and E =  D2ST H(cid:48)W (cid:48)  SR3HW + D2R3R4H(cid:48)W (cid:48) + T R4H(cid:48)W (cid:48) ,  and these are bounded by ST /R3R4. Tucker vs CP: Recently, CP decomposition is applied to approximate the convolution layers of CNNs for ImageNet which consist of 8 layers (Denton et al., 2014; Lebedev et al., 2015). However it cannot be applied to the entire layers and the instability issue of low-rank CP decomposition is reported (De Silva & Lim, 2008; Lebedev et al., 2015). On the other hand, our kernel tensor ap- proximation with Tucker decomposition can be successfully applied to the entire layers of AlexNet, VGG-S, GoogLeNet, and VGG-16  3.2 RANK SELECTION WITH GLOBAL ANALYTIC VBMF  The rank-(R3, R4) are very important hyper-parameters which control the trade-off between per- formance (memory, speed, energy) improvement and accuracy loss. Instead of selecting the rank- (R3, R4) by time consuming trial-and-error, we considered data-driven one-shot decision via empir- ical Bayes (MacKay, 1992) with automatic relevance determination (ARD) prior (Tipping, 2001). At the ﬁrst time, we designed probabilistic Tucker model which is similar to (Mørup & Hansen, 2009), and applied empirical variational Bayesian learning. However, the rank selection results were severely unreliable because they heavily depend on (1) initial condition, (2) noise variance estimation policy, and (3) threshold setting for pruning. For this reason, we decided to use a sub- optimal but highly reproducible approach. We employed recently developed global analytic solutions for variational Bayesian matrix factoriza- tion (VBMF) (Nakajima et al., 2013). The global analytic VBMF is a very promising tool because it can automatically ﬁnd noise variance, rank and even provide theoretical condition for perfect rank recovery (Nakajima et al., 2012). We determined the rank R3 and R4 by applying global analytic VBMF on mode-3 matricization (of size S × T D2) and mode-4 matricization (of size T × D2S) of kernel tensor K, respectively.  5  S W H R3 W H R4 9(cid:31) *(cid:31) T 9(cid:31) *(cid:31) D D R3 Published as a conference paper at ICLR 2016  Figure 4: Accuracy of compressed CNNs in ﬁne-tuning.  3.3 FINE-TUNING  Because we minimize the reconstruction error of linear kernel tensors instead of non-linear re- sponses, the accuracy is signiﬁcantly dropped after whole network compression (e.g. more than 50% in the case of AlexNet). However, as shown in Fig. 4, we can easily recover the accuracy by using ﬁne-tuning with ImageNet training dataset. We observed that accuracy is recovered quickly in one epoch. However, more than 10 epochs are required to recover the original accuracy. While (Lebedev et al., 2015; Zhang et al., 2015a) reported difﬁculty on ﬁnding a good SGD learning rate, our single learning rate scheduling rule works well for various compressed CNNs. In our experiment, we set the base learning η = 10−3 and decrease it by a factor of 10 every 5 epochs. Because of GPU memory limitation, we set the batch size: 128, 128, 64, and 32 for AlexNet, VGG-S, GoogLeNet, and VGG-16, respectively. We also tried to train the architecture of the approximated model from scratch on the ImageNet training dataset. At this time, we only tested the Gaussian random initialization and it did not work. We leave the use of other initialization methods (Glorot & Bengio, 2010; He et al., 2015) and batch normalization (Ioffe & Szegedy, 2015) as future work.  4 EXPERIMENTS  We used four representative CNNs, AlexNet, VGG-S, GoogLeNet, and VGG-16, which can be down- loaded on Berkeley’s Caffe model zoo. In the case of inception module of GoogLeNet, we only com- pressed the 3 × 3 convolution kernel which is the main computational part. In the case of VGG-16, we only compressed the convolutional layers as done in (Zhang et al., 2015a). Top-5 single-view accuracy is measured using 50,000 validation images from the ImageNet2012 dataset. We performed experiments on Nvidia Titan X (for ﬁne-tuning and runtime comparison on Caffe+cuDNN2) and a smartphone, Samsung Galaxy S6 (for the comparison of runtime and en- ergy consumption). The application processor of the smartphone (Exynos 7420) is equipped with a mobile GPU, ARM Mali T760. Compared with the GPU used on Titan X, the mobile GPU gives 35 times (6.6TFlops vs 190GFlops) lower computing capability and 13 times (336.5GBps vs 25.6GBps) smaller memory bandwidth. In order to run Caffe models on the mobile GPU, we developed a mobile version of Caffe called S-Caffe (Caffe for Smart mobile devices) where all the Caffe models can run on our target mobile devices (for the moment, Samsung smartphones) without modiﬁcation. We also developed an An- droid App which performs image classiﬁcation by running each of the four CNNs (AlexNet, VGG-S, GoogLeNet , and VGG-16) on the smartphone.  6  Epoch051015Top-5 accuracy75808590η = 10-5η = 10-3η =10-4VGG-16:34.06 → 78.68 ... → 89.40VGG-S: 60.10 → 81.09 ... → 84.19AlexNet:23.39 → 74.74 ... → 78.33GoogleNet: 56.98 → 87.74  ... → 88.66Published as a conference paper at ICLR 2016  We measured the power consumption of whole smartphone which is decomposed into the power consumption of GPU, main memory, and the other components of smartphone, e.g., ARM CPU, display, modem, etc. and give component-level analysis, especially, the power consumption of GPU and main memory (see supplementary material for details of measurement environment). The measurement results of runtime and energy consumption are the average of 50 runs.  4.1 OVERALL RESULTS Table 1 shows the overall results for the three CNNs. Our proposed scheme gives ×5.46/ × 2.67 (AlexNet), ×7.40/ × 4.80 (VGG-S), ×1.28/ × 2.06 (GoogLeNet), and ×1.09/ × 4.93 (VGG- 16) reductions in total weights and FLOPs, respectively. Such reductions offer ×1.42 ∼ ×3.68 (×1.23 ∼ ×2.33) runtime improvements on the smartphone (Titan X). We report the energy con- sumption of mobile GPU and main memory. The smartphone gives larger reduction ratios (e.g., ×3.41 vs. ×2.72 for AlexNet) for energy consumption than runtime. We will give a detailed analy- sis in the following subsection. Comparison with Zhang et al. (2015a)’s method: The accuracy of our compressed VGG-16 is 89.40% for theoretical ×4.93 speed-up, and it is comparable to the 89.6% (88.9%) for theoretical ×4 (×5) speed-up in (Zhang et al., 2015a).  Table 1: Original versus compressed CNNs. Memory, runtime and energy are signiﬁcantly reduced with only minor accuracy drop. We report the time and energy consumption for processing single image in S6 and Titan X. (* compression, S6: Samsung Galaxy S6). S6  Model AlexNet AlexNet* (imp.) VGG-S VGG-S* (imp.) GoogLeNet GoogLeNet* (imp.) VGG-16 VGG-16* (imp.)  Top-5 Weights 61M 80.03 78.33 11M (×5.46) (-1.70) 84.60 84.05 (-0.55) 88.90 88.66 (-0.24) 89.90 89.40 (-0.50)  FLOPs 725M 272M (×2.67) 103M 2640M 14M 549M (×4.80) (×7.40) 6.9M 1566M 760M 4.7M (×2.06) (×1.28)  117ms 43ms (×2.72) 357ms 97ms (×3.68) 273ms 192ms (×1.42) 138M 15484M 1926ms 127M 3139M 576ms (×3.34) (×4.93)  (×1.09)  245mJ 72mJ (×3.41) 825mJ 193mJ (×4.26) 473mJ 296mJ (×1.60) 4757mJ 1346mJ (×3.53)  Titan X 0.54ms 0.30ms (×1.81) 1.86ms 0.92ms (×2.01) 1.83ms 1.48ms (×1.23) 10.67ms 4.58ms (×2.33)  4.2 LAYERWISE ANALYSIS  Tables 2, 3, 4 and 5 1 show the detailed comparisons. Each row has two results (the above one for the original uncompressed CNN and the other one for the compressed CNN), and improvements. For instance, in Table 2, the second convolutional layer having the input and output channel dimensions of 48× 2 and 128× 2 is compressed to give the Tucker-2 ranks of 25× 2 and 59× 2, which reduces the amount of weights from 307K to 91K. After compression, a layer in the compressed network performs three matrix multiplications. We give the details of three matrix multiplications for each of weights, FLOPs, and runtime. For instance, on the smartphone (column S6 in Table 2), the second convolutional layer of compressed AlexNet takes 10.53ms which is decomposed to 0.8ms, 7.43ms and 2.3ms for the three matrix multiplications. In Tables 2, 3, 4 and 5 we have two observations. Observation 1: Given a compressed network, the smartphone tends to give larger performance gain than the Titan X. It is mainly because the mobile GPU on the smartphone lacks in thread-level parallelism. It has 24 times less number of threads (2K vs. 48K in terms of maximum number of threads) than that in Titan X. Compression reduces the amount of weights thereby reducing cache conﬂicts and memory latency. Due to the small thread-level parallelism, the reduced latency has more impact on the performance of threads on the mobile GPU than that on Titan X.  1See supplementary material for Tables 3, 4 and 5  7  Published as a conference paper at ICLR 2016  Table 2: Layerwise analysis on AlexNet. Note that conv2, conv4, and conv5 layer have 2-group structure. (S: input channel dimension, T : output channel dimension, (R3, R4): Tucker-2 rank).  S/R3 3  48 × 2 25 × 2  256 105 192 × 2 49 × 2 192 × 2 40 × 2  256 210  4096  4096  Layer conv1 conv1* (imp.) conv2 conv2* (imp.) conv3 conv3* (imp.) conv4 conv4* (imp.) conv5 conv5* (imp.) fc6 fc6* (imp.) fc7 fc7* (imp.) fc8 fc8* (imp.)  T /R4 Weights  96 26 128 × 2 59 × 2  384 112 192 × 2 46 × 2 128 × 2 34 × 2  4096 584  4096 301  1000 195  FLOPs 35K 105M 11K (×2.92)  36M(=29+7) (×2.92)  307K 224M 91K 67M(=2+54+11)  (×3.37)  (×3.37)  885K 150M 178K (×5.03)  30M(=5+18+7) (×5.03)  664K 112M 77K (×7.10)  13M(=3+7+3) (×7.10) 442K 75.0M 8.2M(=2.6+4.1+1.5) 49K (×9.11) (×9.11) 37.7M 37.7M 6.9M 8.7M(=1.9+4.4+2.4) (×8.03) (×4.86) 16.8M 16.8M 2.4M 2.4M(=1.2+1.2) (×6.80) (×6.80) 4.1M 4.1M 1.0M 1.0M(=0.8+0.2) (×4.12)  (×4.12)  S6 15.05 ms 10.19m(=8.28+1.90) (×1.48) 24.25 ms 10.53ms(=0.80+7.43+2.30) (×2.30) 18.60ms 4.85ms(=1.00+2.72+1.13) (×3.84) 15.17ms 4.29 ms(=1.55+1.89+0.86) (×3.53) 10.78ms 3.44 ms(=1.15+1.61+0.68) (×3.13) 18.94ms 5.07 ms(=0.85+3.12+1.11) (×3.74) 7.75ms 1.02 ms(=0.51+0.51) (×7.61) 2.00ms 0.66ms(=0.44+0.22) (×3.01)  Observation 2: Given the same compression rate, the smartphone tends to exhibit larger perfor- mance gain at fully-connected layers than at convolutional layers. We think it is also due to the reduced cache conﬂicts enabled by network compression as explained above. Especially, in the case of fully-connected layers, the effect of weight reduction can give more signiﬁcant impact because the weights at the fully-connected layers are utilized only once, often called dead-on-arrival (DoA) data. In terms of cache performance, such DoA data are much more harmful than convolution kernel weights (which are reused multiple times). Thus, weight reduction at the fully connected layer can give more signiﬁcant impact on cache performance thereby exhibiting more performance improve- ment than in the case of weight reduction at convolutional layers.  4.3 ENERGY CONSUMPTION ANALYSIS  Fig. 5 compares power consumption on the smartphone. Each network gives the power consumption of GPU and main memory. Note that we enlarged the time axis of compressed networks for a better comparison. We omitted VGG-16 since VGG-16 gives similar trend. The ﬁgure shows that the compression reduces power consumption (Y axis) as well as runtime (X axis), which explains why the reduction in energy consumption is larger than that in runtime in Table 1. Fig. 5 also shows that the GPU power consumption of compressed CNN is smaller than that of uncompressed CNN. We analyze this due to the extensive usage of 1 × 1 convolutions in the compressed CNN. When executing convolutions, we apply optimization techniques such as Caffeinated convolution(Chellapilla et al., 2006). In such a case, in terms of cache efﬁciency, 1 × 1 convolutions are inferior to the other convolutions, e.g., 3 × 3, 5 × 5, etc. since the amount of data reuse is proportional to the total size of convolution kernel. Thus, 1 × 1 convolutions tend to incur more cache misses than the other larger convolutions. Cache misses on the mobile GPU without sufﬁcient thread level parallelism often incur stall cycles, i.e., make GPU cores idle consuming less power, which reduces the power consumption of GPU core during the execution of 1×1 convolution.  8  Published as a conference paper at ICLR 2016  Figure 5: Power consumption over time for each model. (Blue: GPU, Red: main memory).  As mentioned earlier, our proposed method improves cache efﬁciency by reducing the amount of weights. However, 1 × 1 convolutions have negative impacts on cache efﬁciency and GPU core utilization. Fig. 5 shows the combined effects. In the compressed networks, the power consumption of GPU core is reduced by 1 × 1 convolutions and tends to change more frequently due to frequent executions of 1× 1 convolution while, in the case of uncompressed networks, especially for AlexNet and VGG-S, the power consumption of GPU core tends to be stable during the execution of convo- lutional layers. In the case of uncompressed GoogLeNet, the power consumption tends to ﬂuctuate. It is mainly because (1) GoogLeNet consists of many small layers (about 100 building blocks), and (2) 1 × 1 convolutions are heavily utilized. The three compressed networks show similar behavior of frequent ﬂuctuations in power consump- tion mostly due to 1 × 1 convolutions. Fig. 5 also shows that, in the uncompressed networks, fully connected layers incur signiﬁcant amount of power consumption in main memory. It is because the uncompressed networks, especially AlexNet and VGG-S have large numbers (more than tens of mega-bytes) of weights in fully connected layers which incur signiﬁcant amount of memory ac- cesses. As shown in Fig. 5, the proposed scheme reduces the amount of weights at fully connected layers thereby reducing the power consumption in main memory.  5 DISCUSSION  Although we can obtain very promising results with one-shot rank selection, it is not fully investi- gated yet whether the selected rank is really optimal or not. As future work, we will investigate the optimality of our proposed scheme. The 1 × 1 convolution is a key operation in our compressed model as well as in inception module of GoogLeNet. Due to its characteristics, e.g. channel com- pression and computation reduction, we expect that 1 × 1 convolutions will become more and more popular in the future. However, as shown in our experimental results, it lacks in cache efﬁciency. We expect further investigations are required to make best use of 1x1 convolutions. Whole network compression is challenging due to the large design space and associated long design time. In order to address this problem, we propose a one-shot compression scheme which applies a single general low-rank approximation method and a global rank selection method. Our one- shot compression enables fast design and easy implementation with publicly available tools. We evaluated the effectiveness of the proposed scheme on a smartphone and Titan X. The experiments show that the proposed scheme gives, for four CNNs (AlexNet, VGG-S, GoogLeNet, and VGG- 16) average ×2.72 (×3.41), ×3.68 (×4.26), ×1.42 (×1.60), and ×3.34 (×3.53) improvements in runtime (energy consumption) on the smartphone.  9  Time [ms]050100150Power [mW]05001000150020002500AlexNet UncompC1C2C3C4C5F6F7F8Time [ms]0100200300400Power [mW]05001000150020002500VGG UncompTime [ms]050100150200250300Power [mW]05001000150020002500GoogLeNet UncompTime [ms]0102030405060Power [mW]05001000150020002500AlexNet CompTime [ms]020406080100120Power [mW]05001000150020002500VGG CompTime [ms]050100150200250Power [mW]05001000150020002500GoogLeNet CompPublished as a conference paper at ICLR 2016  REFERENCES Bader, Brett W., Kolda, Tamara G., et al. Matlab tensor toolbox version 2.6. Available online,  February 2015. URL http://www.sandia.gov/˜tgkolda/TensorToolbox/.  Carroll, J Douglas and Chang, Jih-Jie. Analysis of individual differences in multidimensional scal- ing via an n-way generalization of eckart-young decomposition. Psychometrika, 35(3):283–319, 1970.  Chellapilla, Kumar, Puri, Sidd, and Simard, Patrice. High performance convolutional neural net- works for document processing. In Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft, 2006.  Chen, Wenlin, Wilson, James T, Tyree, Stephen, Weinberger, Kilian Q, and Chen, Yixin. Compress-  ing neural networks with the hashing trick. arXiv preprint arXiv:1504.04788, 2015.  Cheng, Yu, Yu, Felix X, Feris, Rogerio S, Kumar, Sanjiv, Choudhary, Alok, and Chang, Shih-Fu.  Fast neural networks with circulant projections. arXiv preprint arXiv:1502.03436, 2015.  De Lathauwer, Lieven, De Moor, Bart, and Vandewalle, Joos. A multilinear singular value decom-  position. SIAM journal on Matrix Analysis and Applications, 21(4):1253–1278, 2000.  De Silva, Vin and Lim, Lek-Heng. Tensor rank and the ill-posedness of the best low-rank approxi-  mation problem. SIAM Journal on Matrix Analysis and Applications, 30(3):1084–1127, 2008.  Denil, Misha, Shakibi, Babak, Dinh, Laurent, de Freitas, Nando, et al. Predicting parameters in deep  learning. In Advances in Neural Information Processing Systems, pp. 2148–2156, 2013.  Denton, Emily L, Zaremba, Wojciech, Bruna, Joan, LeCun, Yann, and Fergus, Rob. Exploiting In Advances in Neural  linear structure within convolutional networks for efﬁcient evaluation. Information Processing Systems, pp. 1269–1277, 2014.  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural networks. In International conference on artiﬁcial intelligence and statistics, pp. 249–256, 2010.  Gong, Yunchao, Liu, Liu, Yang, Ming, and Bourdev, Lubomir. Compressing deep convolutional  networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.  Han, Song, Mao, Huizi, and Dally, William J. A deep neural network compression pipeline: Pruning,  quantization, huffman encoding. arXiv preprint arXiv:1510.00149, 2015a.  Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for  efﬁcient neural networks. arXiv preprint arXiv:1506.02626, 2015b.  Harshman, Richard A and Lundy, Margaret E. Parafac: Parallel factor analysis. Computational  Statistics & Data Analysis, 18(1):39–72, 1994.  He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers: Surpass- ing human-level performance on imagenet classiﬁcation. In IEEE International Conference on Computer Vision, 2015.  Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Rus- lan R. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by  reducing internal covariate shift. In International Conference on Machine Learning, 2015.  Jaderberg, M., Vedaldi, A., and Zisserman, A. Speeding up convolutional neural networks with low  rank expansions. In British Machine Vision Conference, 2014.  Kim, Y.-D. and Choi, S. Nonnegative Tucker decomposition. In Proceedings of the IEEE CVPR-  2007 Workshop on Component Analysis Methods, Minneapolis, Minnesota, 2007.  10  Published as a conference paper at ICLR 2016  Kolda, Tamara G and Bader, Brett W. Tensor decompositions and applications. SIAM review, 51(3):  455–500, 2009.  Lebedev, Vadim, Ganin, Yaroslav, Rakhuba, Maksim, Oseledets, Ivan, and Lempitsky, Victor. Speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. In International Conference on Learning Representations, 2015.  Lin, M., Chen, Q., and Yan, S. Network in network.  Representations, 2014.  In International Conference on Learning  MacKay, David JC. Bayesian interpolation. Neural computation, 4(3):415–447, 1992.  Mathieu, Michael, Henaff, Mikael, and LeCun, Yann. Fast training of convolutional networks  through ffts. arXiv preprint arXiv:1312.5851, 2013.  Mørup, Morten and Hansen, Lars Kai. Automatic relevance determination for multi-way models.  Journal of Chemometrics, 23(7-8):352–363, 2009.  Nakajima, Shinichi. Variational Bayesian matrix factorization version 1.02, 2015. URL https:  //sites.google.com/site/shinnkj23/downloads.  Nakajima, Shinichi, Tomioka, Ryota, Sugiyama, Masashi, and Babacan, S Derin. Perfect dimen- sionality recovery by variational bayesian pca. In Advances in Neural Information Processing Systems, pp. 971–979, 2012.  Nakajima, Shinichi, Sugiyama, Masashi, Babacan, S Derin, and Tomioka, Ryota. Global analytic solution of fully-observed variational bayesian matrix factorization. The Journal of Machine Learning Research, 14(1):1–37, 2013.  Novikov, Alexander, Podoprikhin, Dmitry, Osokin, Anton, and Vetrov, Dmitry. Tensorizing neural  networks. arXiv preprint arXiv:1509.06569, 2015.  Shashua, Amnon and Hazan, Tamir. Non-negative tensor factorization with applications to statistics and computer vision. In Proceedings of the 22nd international conference on Machine learning, pp. 792–799. ACM, 2005.  Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recogni-  tion. In International Conference on Learning Representations, 2015.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.  Tipping, Michael E. Sparse bayesian learning and the relevance vector machine. The journal of  machine learning research, 1:211–244, 2001.  Tucker, Ledyard R. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):  279–311, 1966.  Vanhoucke, Vincent, Senior, Andrew, and Mao, Mark Z. Improving the speed of neural networks on cpus. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, volume 1, 2011.  Ye, Jieping. Generalized low rank approximations of matrices. Machine Learning, 61(1-3):167–191,  2005.  Zhang, Xiangyu, Zou, Jianhua, He, Kaiming, and Sun, Jian. Accelerating very deep convolutional  networks for classiﬁcation and detection. arXiv preprint arXiv:1505.06798, 2015a.  Zhang, Xiangyu, Zou, Jianhua, Ming, Xiang, He, Kaiming, and Sun, Jian. Efﬁcient and accurate  approximations of nonlinear convolutional networks. 2015b.  11  Published as a conference paper at ICLR 2016  APPENDICES  A EXPERIMENTAL SETUP  This section describes the details of experimental setup including the measurement system for power consumption and exempliﬁes the measured data.  A.1 MEASUREMENT SYSTEM  Fig. 6 shows the power measurement system. As the ﬁgure shows, it consists of a probe board (left) having a Samsung Galaxy S6 smartphone and power probes and a monitor board (right). The probe board provides 8 probes which are connected to the power pins of application processor (to be introduced below). The power proﬁling monitor samples, for each power probe, the electric current every 0.1ms and gives power consumption data with time stamps.  Figure 6: Power measurement system.  Fig. 7 illustrates the main board of the smartphone (Fig. 7 (a)), the application processor chip package (red rectangle in Fig. 2 (a)) consisting of the application processor and main memory (LPDDR4 DRAM) in the smartphone (Fig. 7 (b)), and a simpliﬁed block diagram of the application processor (Fig. 7 (c)). The power measurement system provides the probes connected to the power pins for mobile GPU (ARM Mali T760 in Fig. 7 (c)) and main memory (LPDDR4 DRAM in Fig. 7 (b)).  A.2 MEASURED DATA EXAMPLE: GoogLeNet CASE  Fig. 8 shows the power consumption data for the uncompressed GoogLeNet. We also identiﬁed the period of each layer, e.g., the ﬁrst convolutional layer (Conv 1 in the ﬁgure), and the ﬁrst Inception module (i3a). As mentioned in our submission, the proﬁle of power consumption shows more fre- quent ﬂuctuations in Inception modules than in the convolutional layers. The ﬁgure also shows that the ﬁrst two convolutional layers (Conv 1 and Conv 2) occupy about 1/4 of total energy consumption while Inception modules consume about 3/4 of total energy consumption.  12  Published as a conference paper at ICLR 2016  Figure 7: Details of mobile application processor and main memory.  Figure 8: Power proﬁle of uncompressed GoogLeNet.  B LAYERWISE ANALYSIS  We report detailed comparison results VGG-S, GoogLeNet, and VGG-16.  13  Published as a conference paper at ICLR 2016  Table 3: Layerwis analysis on VGG-S. (S: input channel dimension, T : output channel dimension, (R3, R4): Tucker-2 rank).  T /R4 Weights  FLOPs 14K 168M 10K 121M(=73+48)  (×1.38)  (×1.38)  S/R3 3  96 48  256 126  512 143  512 120  512 343  4096  4096  Layer conv1 conv1* (imp.) conv2 conv2* (imp.) conv3 conv3* (imp.) conv4 conv4* (imp.) conv5 conv5* (imp.) fc6 fc6* (imp.) fc7 fc7* (imp.) fc8 fc8* (imp.)  96 42  256 89  512 175  512 144  512 120  4096 561  4096 301  1000 195  614K 699M 134K 147M(=6+116+25)  (×4.54)  93M(=9+57+26) (×3.68)  96M(=21+54+21) (×7.10)  (×4.58) 1180K 341M 320K (×3.68) 2359K 682M 332K (×7.10) 2359K 682M 73M(=18+37+18) 252K (×9.34) (×9.34) 75.5M 75.5M 9.4M 15.5M(=6.3+6.9+2.3) (×4.86) (×8.03) 16.8M 16.8M 2.4M 2.4M(=1.2+1.2) (×6.80) (×6.80) 4.1M 4.1M 1.0M 1.0M(=0.8+0.2) (×4.12)  (×4.12)  S6 23.88ms 23.15ms(=14.47+8.68) (×1.03) 74.57ms 18.54ms(=1.32+13.59+3.64) (×4.02) 38.33ms 11.59ms(=1.53+6.82+3.24) (×3.31) 78.43ms 12.23ms(=2.92+6.63+2.78) (×6.37) 78.40ms 10.25ms(=2.76+5.03+2.46) (×7.65) 40.75ms 7.18ms(=1.58+4.61+0.98) (×5.68) 7.68ms 1.26ms(=0.65+0.60) (×6.10) 1.97ms 0.67ms(=0.45+0.22) (×2.92)  14  Published as a conference paper at ICLR 2016  Table 4: Layerwise analysis on GoogLeNet. (S: input channel dimension, T : output channel dimen- sion, (R3, R4): Tucker-2 rank).  Layer conv1 conv1* (imp.) conv2 conv2* (imp.) i3a i3a* (imp.) i3b i3b* (imp.) i4a i4a* (imp.) i4b i4b* (imp.) i4c i4c* (imp.) i4d i4d* (imp.) i4e i4e* (imp.) i5a i5a* (imp.) i5b i5b* (imp.)  S/R3 3  T /R4 64 23  64  96 41  128 42  96 35  112 55  128 63  144 67  160 97  160 91  192 108  192 23  128 41  192 37  208 39  224 75  256 87  288 105  320 131  320 139  384 178  Weights  24K 19M(3+12+4)  60M(=42+18) (×1.94)  FLOPs 9.4K 118M 4.8K (×1.94) 11.1K 347M 4.8K (×4.99) 111K(68%) (×4.55) 221K(57%) 26K (×8.36) 180K(48%) 24K (×7.56) 226K(51%) (×3.76) 295K(58%) (×3.70) 373K(62%) (×3.62) 461K(60%) (×2.68) 461K(44%) 173K (×2.67) 664K(46%) 262K (×2.53)  60M(=42+18) (×4.99) 87M(68%) (×4.55) 173M(57%) 21M(4+11+6) (×8.36) 35M(48%) 5M(1+2+2) (×7.56) 44M(51%) (×3.76) 58M(58%) (×3.70) 73M(62%) (×3.62) 90M(60%) (×2.68) 23M(44%) 8M(1+6+2) (×2.67) 33M(46%) 13M(1+8+3) (×2.53)  103K 20M(2+12+6)  172K 34M(3+22+8)  80K 16M(2+10+4)  60K 12M(1+7+3)  S6 18.96ms 21.76ms(=16.85+4.91) (×0.87) 34.69ms 12.04ms(=1.66+5.95+4.43) (×2.88) 9.39ms 3.70ms=(0.70+2.02+0.98) (×2.54) 17.49ms 4.10ms=(0.89+1.99+1.21) (×4.27) 4.35ms 1.68ms=(0.39+0.79+0.50) (×2.60) 5.39ms 2.65ms=(0.47+1.48+0.70) (×2.03) 6.93ms 3.10ms=(0.52+1.74+0.84) (×2.23) 8.93ms 3.67ms=(0.61+2.03+1.04) (×2.43) 10.90ms 5.45ms=(0.76+3.35+1.34) (×2.00) 3.96ms 2.55ms=(0.41+1.55+0.59) (×1.55) 5.71ms 3.28ms=(0.51+1.95+0.82) (×1.74)  15  Published as a conference paper at ICLR 2016  Table 5: Layerwis analysis on VGG-16. We do not compress the ﬁrst convolutional layer and fully- connected layers as done in Zhang et al. (2015a). The theoretical speed-up raito of convolutional layers and whole layers are ×5.03 and ×4.93 respectively. (S: input channel dimension, T : output channel dimension, (R3, R4): Tucker-2 rank).  Layer C12 C12* (imp.) C21 C21* (imp.) C22 C22* (imp.) C31 C31* (imp.) C32 C32* (imp.) C33 C33* (imp.) C41 C41* (imp.) C42 C42* (imp.) C43 C43* (imp.) C51 C51* (imp.) C52 C52* (imp.) C53 C53* (imp.)  S/R3 64 11  64 22  128 39  128 58  2562 138  256 124  256 148  512 212  512 178  512 185  512 172  512 120  T /R4 Weights  64 18  128 34  128 36  256 117  256 132  256 119  512 194  512 207  512 163  512 164  512 170  512 120  FLOPs 37K 1853M 4K (×10.15)  182M(=35+89+58) (×10.15)  74K 926M 8K 101M(=8+38+55)  (×9.17)  (×9.17) 148K 1851M 22K 279(=63+159+58) (×6.64) (×6.64)  295K 926M 74K (×4.01)  231M(=15+122+94) (×4.01) 590K 1850M 237M(=55+129+53) 76K (×7.81) (×7.81) 590K 1850M 195K 612M(=100+416+96) (×3.03) (×3.03) 1180K 925M 208M(=17+114+78) 265K (×4.45) (×4.45) 2360K 1850M 478M(=85+310+83) 609K (×3.87) (×3.87) 2360K 1850M 342M(=71+205+65) 436K (×5.42) (×5.42) 2360K 463M 452K (×5.22) 2360K 463M 416K (×5.67) 2360K 463M 438K (×5.38)  89M(=19+54+16) (×5.22)  82M(=16+48+17) (×5.67)  86M(=17+52+17) (×5.38)  S6 234.58ms 64.35ms(= 13.42+28.46+20.47) (×3.65) 105.66ms 23.20ms(=3.26+7.82+12.12) (×4.55) 226.29ms 50.66ms(=11.60+27.19+11.86) (×4.47) 93.71ms 29.92ms(=2.96+14.73+12.23) (×3.13) 211.75ms 34.16ms(=7.94+17.89+8.33) (×6.20) 213.31ms 72.66ms(=12.74+47.74+12.19) (×2.94) 98.40ms 23.58ms(=2.54+12.25+8.79) (×4.17) 216.16ms 51.18ms(=9.10+33.22+8.86) (×4.22) 216.34ms 38.85ms(=8.06+23.14+7.66) (×5.57) 57.54ms 13.09ms(=2.80+7.89+2.39) (×4.40) 76.80ms 11.87ms(=2.64+6.82+2.42) (×6.47) 67.69ms 12.16ms(=2.65+7.13+2.38) (×5.57)  16  ",
1511.06939,2016,Session-based recommendations with recurrent neural networks,"['Session-based recommendations with recurrent neural networks [code]\nBalázs Hidasi', 'Alexandros Karatzoglou', 'Linas Baltrunas', 'Domonkos Tikk']",https://arxiv.org/pdf/1511.06939,"6 1 0 2    r a     M 9 2      ]  G L . s c [      4 v 9 3 9 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS  Bal´azs Hidasi ∗ Gravity R&D Inc. Budapest, Hungary balazs.hidasi@gravityrd.com  Alexandros Karatzoglou Telefonica Research Barcelona, Spain alexk@tid.es  Linas Baltrunas † Netﬂix Los Gatos, CA, USA lbaltrunas@netflix.com  Domonkos Tikk Gravity R&D Inc. Budapest, Hungary domonkos.tikk@gravityrd.com  ABSTRACT  We apply recurrent neural networks (RNN) on a new domain, namely recom- mender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netﬂix). In this situation the frequently praised matrix factorization approaches are not accurate. This prob- lem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN- based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modiﬁcations to classic RNNs such as a ranking loss function that make it more viable for this speciﬁc problem. Experimental results on two data-sets show marked improvements over widely used approaches.  1  INTRODUCTION  Session-based recommendation is a relatively unappreciated problem in the machine learning and recommender systems community. Many e-commerce recommender systems (particularly those of small retailers) and most of news and media sites do not typically track the user-id’s of the users that visit their sites over a long period of time. While cookies and browser ﬁngerprinting can provide some level of user recognizability, those technologies are often not reliable enough and moreover raise privacy concerns. Even if tracking is possible, lots of users have only one or two sessions on a smaller e-commerce site, and in certain domains (e.g. classiﬁed sites) the behavior of users often shows session-based traits. Thus subsequent sessions of the same user should be handled independently. Consequently, most session-based recommendation systems deployed for e-commerce are based on relatively simple methods that do not make use of a user proﬁle e.g. item- to-item similarity, co-occurrence, or transition probabilities. While effective, those methods often take only the last click or selection of the user into account ignoring the information of past clicks. The most common methods used in recommender systems are factor models (Koren et al., 2009; Weimer et al., 2007; Hidasi & Tikk, 2012) and neighborhood methods (Sarwar et al., 2001; Ko- ren, 2008). Factor models work by decomposing the sparse user-item interactions matrix to a set of d dimensional vectors one for each item and user in the dataset. The recommendation problem is then treated as a matrix completion/reconstruction problem whereby the latent factor vectors are then used to ﬁll the missing entries by e.g. taking the dot product of the corresponding user–item latent factors. Factor models are hard to apply in session-based recommendation due to the absence  ∗The author spent 3 months at Telefonica Research during the research of this topic. †This work was done while the author was a member of the Telefonica Research group in Barcelona, Spain  1  Published as a conference paper at ICLR 2016  of a user proﬁle. On the other hand, neighborhood methods, which rely on computing similari- ties between items (or users) are based on co-occurrences of items in sessions (or user proﬁles). Neighborhood methods have been used extensively in session-based recommendations. The past few years have seen the tremendous success of deep neural networks in a number of tasks such as image and speech recognition (Russakovsky et al., 2014; Hinton et al., 2012) where unstruc- tured data is processed through several convolutional and standard layers of (usually rectiﬁed linear) units. Sequential data modeling has recently also attracted a lot of attention with various ﬂavors of RNNs being the model of choice for this type of data. Applications of sequence modeling range from test-translation to conversation modeling to image captioning. While RNNs have been applied to the aforementioned domains with remarkable success little atten- tion, has been paid to the area of recommender systems. In this work we argue that RNNs can be applied to session-based recommendation with remarkable results, we deal with the issues that arise when modeling such sparse sequential data and also adapt the RNN models to the recommender setting by introducing a new ranking loss function suited to the task of training these models. The session-based recommendation problem shares some similarities with some NLP-related problems in terms of modeling as long as they both deals with sequences. In the session-based recommenda- tion we can consider the ﬁrst item a user clicks when entering a web-site as the initial input of the RNN, we then would like to query the model based on this initial input for a recommendation. Each consecutive click of the user will then produce an output (a recommendation) that depends on all the previous clicks. Typically the item-set to choose from in recommenders systems can be in the tens of thousands or even hundreds of thousands. Apart from the large size of the item set, another challenge is that click-stream datasets are typically quite large thus training time and scalability are really important. As in most information retrieval and recommendation settings, we are interested in focusing the modeling power on the top-items that the user might be interested in, to this end we use ranking loss function to train the RNNs.  2 RELATED WORK  2.1 SESSION-BASED RECOMMENDATION  Much of the work in the area of recommender systems has focused on models that work when a user identiﬁer is available and a clear user proﬁle can be built. In this setting, matrix factorization methods and neighborhood models have dominated the literature and are also employed on-line. One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user proﬁle is the item-to-item recommendation approach (Sarwar et al., 2001; Linden et al., 2003) in this setting an item to item similarity matrix is precomputed from the available session data, that is items that are often clicked together in sessions are deemed to be similar. This similarity matrix is then simply used during the session to recommend the most similar items to the one the user has currently clicked. While simple, this method has been proven to be effective and is widely employed. While effective, these methods are only taking into account the last click of the user, in effect ignoring the information of the past clicks. A somewhat different approach to session-based recommendation are Markov Decision Processes (MDPs) (Shani et al., 2002). MDPs are models of sequential stochastic decision problems. An MDP is deﬁned as a four-tuple (cid:104)S, A, Rwd, tr(cid:105) where S is the set of states, A is a set of actions Rwd is a reward function and tr is the state-transition function. In recommender systems actions can be equated with recommendations and the simplest MPDs are essentially ﬁrst order Markov chains where the next recommendation can be simply computed on the basis of the transition probability between items. The main issue with applying Markov chains in session-based recommendation is that the state space quickly becomes unmanageable when trying to include all possible sequences of user selections. The extended version of the General Factorization Framework (GFF) (Hidasi & Tikk, 2015) is ca- pable of using session data for recommendations. It models a session by the sum of its events. It uses two kinds of latent representations for items, one represents the item itself, the other is for representing the item as part of a session. The session is then represented as the average of the feature vectors of part-of-a-session item representation. However, this approach does not consider any ordering within the session.  2  Published as a conference paper at ICLR 2016  2.2 DEEP LEARNING IN RECOMMENDERS  One of the ﬁrst related methods in the neural networks literature where the use of Restricted Boltz- mann Machines (RBM) for Collaborative Filtering (Salakhutdinov et al., 2007). In this work an RBM is used to model user-item interaction and perform recommendations. This model has been shown to be one of the best performing Collaborative Filtering models. Deep Models have been used to extract features from unstructured content such as music or images that are then used together with more conventional collaborative ﬁltering models. In Van den Oord et al. (2013) a convolutional deep network is used to extract feature from music ﬁles that are then used in a factor model. More recently Wang et al. (2015) introduced a more generic approach whereby a deep network is used to extract generic content-features from any types of items, these features are then incorporated in a standard collaborative ﬁltering model to enhance the recommendation performance. This approach seems to be particularly useful in settings where there is not sufﬁcient user-item interaction information.  3 RECOMMENDATIONS WITH RNNS  Recurrent Neural Networks have been devised to model variable-length sequence data. The main difference between RNNs and conventional feedforward deep models is the existence of an internal hidden state in the units that compose the network. Standard RNNs update their hidden state h using the following update function:  ht = g(W xt + U ht−1)  (1) Where g is a smooth and bounded function such as a logistic sigmoid function xt is the input of the unit at time t. An RNN outputs a probability distribution over the next element of the sequence, given its current state ht. A Gated Recurrent Unit (GRU) (Cho et al., 2014) is a more elaborate model of an RNN unit that aims at dealing with the vanishing gradient problem. GRU gates essentially learn when and by how much to update the hidden state of the unit. The activation of the GRU is a linear interpolation between the previous activation and the candidate activation ˆht: ht = (1 − zt)ht−1 + zt ˆht  (2)  where the update gate is given by:  zt = σ(Wzxt + Uzht−1)  while the candidate activation function ˆht is computed in a similar manner:  ˆht = tanh (W xt + U (rt (cid:12) ht−1))  and ﬁnaly the reset gate rt is given by:  rt = σ(Wrxt + Urht−1)  3.1 CUSTOMIZING THE GRU MODEL  (3)  (4)  (5)  We used the GRU-based RNN in our models for session-based recommendations. The input of the network is the actual state of the session while the output is the item of the next event in the session. The state of the session can either be the item of the actual event or the events in the session so far. In the former case 1-of-N encoding is used, i.e. the input vector’s length equals to the number of items and only the coordinate corresponding to the active item is one, the others are zeros. The latter setting uses a weighted sum of these representations, in which events are discounted if they have occurred earlier. For the stake of stability, the input vector is then normalized. We expect this to help because it reinforces the memory effect: the reinforcement of very local ordering constraints which are not well captured by the longer memory of RNN. We also experimented with adding an additional embedding layer, but the 1-of-N encoding always performed better. The core of the network is the GRU layer(s) and additional feedforward layers can be added between the last layer and the output. The output is the predicted preference of the items, i.e. the likelihood of being the next in the session for each item. When multiple GRU layers are used, the hidden state of the previous layer is the input of the next one. The input can also be optionally connected  3  Published as a conference paper at ICLR 2016  Figure 1: General architecture of the network. Processing of one event of the event stream at once.  to GRU layers deeper in the network, as we found that this improves performance. See the whole architecture on Figure 1, which depicts the representation of a single event within a time series of events. Since recommender systems are not the primary application area of recurrent neural networks, we modiﬁed the base network to better suit the task. We also considered practical points so that our solution could be possibly applied in a live environment.  3.1.1 SESSION-PARALLEL MINI-BATCHES  RNNs for natural language processing tasks usually use in-sequence mini-batches. For example it is common to use a sliding window over the words of sentences and put these windowed fragments next to each other to form mini-batches. This does not ﬁt our task, because (1) the length of sessions can be very different, even more so than that of sentences: some sessions consist of only 2 events, while others may range over a few hundreds; (2) our goal is to capture how a session evolves over time, so breaking down into fragments would make no sense. Therefore we use session-parallel mini-batches. First, we create an order for the sessions. Then, we use the ﬁrst event of the ﬁrst X sessions to form the input of the ﬁrst mini-batch (the desired output is the second events of our active sessions). The second mini-batch is formed from the second events and so on. If any of the sessions end, the next available session is put in its place. Sessions are assumed to be independent, thus we reset the appropriate hidden state when this switch occurs. See Figure 2 for more details.  Figure 2: Session-parallel mini-batch creation  3.1.2 SAMPLING ON THE OUTPUT  Recommender systems are especially useful when the number of items is large. Even for a medium- sized webshop this is in the range of tens of thousands, but on larger sites it is not rare to have  4  GRU layerFeedforwardlayersGRU layerInput: actualitem, 1-of-N codingEmbeddinglayerGRU layer…Output: scoresonitems(cid:1)(cid:2),(cid:2)(cid:1)(cid:2),(cid:4)(cid:1)(cid:2),(cid:5)(cid:1)(cid:2),(cid:6)(cid:1)(cid:4),(cid:2)(cid:1)(cid:4),(cid:4)(cid:1)(cid:4),(cid:5)(cid:1)(cid:5),(cid:2)(cid:1)(cid:5),(cid:4)(cid:1)(cid:5),(cid:5)(cid:1)(cid:5),(cid:6)(cid:1)(cid:5),(cid:7)(cid:1)(cid:5),(cid:8)(cid:1)(cid:6),(cid:2)(cid:1)(cid:6),(cid:4)(cid:1)(cid:7),(cid:2)(cid:1)(cid:7),(cid:4)(cid:1)(cid:7),(cid:5)Session1Session2Session3Session4Session5…(cid:1)(cid:2),(cid:2)(cid:1)(cid:2),(cid:4)(cid:1)(cid:2),(cid:5)(cid:1)(cid:4),(cid:2)(cid:1)(cid:4),(cid:4)(cid:1)(cid:5),(cid:2)(cid:1)(cid:5),(cid:4)(cid:1)(cid:5),(cid:5)(cid:1)(cid:5),(cid:6)(cid:1)(cid:5),(cid:7)(cid:1)(cid:6),(cid:2)(cid:1)(cid:7),(cid:2)(cid:1)(cid:7),(cid:4)(cid:1)(cid:2),(cid:4)(cid:1)(cid:2),(cid:5)(cid:1)(cid:2),(cid:6)(cid:1)(cid:4),(cid:4)(cid:1)(cid:4),(cid:5)(cid:1)(cid:5),(cid:4)(cid:1)(cid:5),(cid:5)(cid:1)(cid:5),(cid:6)(cid:1)(cid:5),(cid:7)(cid:1)(cid:5),(cid:8)(cid:1)(cid:6),(cid:4)(cid:1)(cid:7),(cid:4)(cid:1)(cid:7),(cid:5)InputOutputMini-batch1Mini-batch2Mini-batch3………………Published as a conference paper at ICLR 2016  hundreds of thousands of items or even a few millions. Calculating a score for each item in each step would make the algorithm scale with the product of the number of items and the number of events. This would be unusable in practice. Therefore we have to sample the output and only compute the score for a small subset of the items. This also entails that only some of the weights will be updated. Besides the desired output, we need to compute scores for some negative examples and modify the weights so that the desired output is highly ranked. The natural interpretation of an arbitrary missing event is that the user did not know about the existence of the item and thus there was no interaction. However there is a low probability that the user did know about the item and chose not to interact, because she disliked the item. The more popular the item, the more probable it is that the user knows about it, thus it is more likely that a missing event expresses dislike. Therefore we should sample items in proportion of their popularity. Instead of generating separate samples for each training example, we use the items from the other training examples of the mini-batch as negative examples. The beneﬁt of this approach is that we can further reduce computational times by skipping the sampling. Additionally, there are also beneﬁts on the implementation side from making the code less complex to faster matrix operations. Meanwhile, this approach is also a popularity-based sampling, because the likelihood of an item being in the other training examples of the mini-batch is proportional to its popularity.  3.1.3 RANKING LOSS  The core of recommender systems is the relevance-based ranking of items. Although the task can also be interpreted as a classiﬁcation task, learning-to-rank approaches (Rendle et al., 2009; Shi et al., 2012; Steck, 2015) generally outperform other approaches. Ranking can be pointwise, pair- wise or listwise. Pointwise ranking estimates the score or the rank of items independently of each other and the loss is deﬁned in a way so that the rank of relevant items should be low. Pairwise rank- ing compares the score or the rank of pairs of a positive and a negative item and the loss enforces that the rank of the positive item should be lower than that of the negative one. Listwise ranking uses the scores and ranks of all items and compares them to the perfect ordering. As it includes sorting, it is usually computationally more expensive and thus not used often. Also, if there is only one relevant item – as in our case – listwise ranking can be solved via pairwise ranking. We included several pointwise and pairwise ranking losses into our solution. We found that point- wise ranking was unstable with this network (see Section 4 for more comments). Pairwise ranking losses on the other hand performed well. We use the following two.  • BPR: Bayesian Personalized Ranking (Rendle et al., 2009) is a matrix factorization method that uses pairwise ranking loss. It compares the score of a positive and a sampled negative item. Here we compare the score of the positive item with several sampled items and use their average as the loss. The loss at a given point in one session is deﬁned as: Ls = − 1 j=1 log (σ (ˆrs,i − ˆrs,j)), where NS is the sample size, ˆrs,k is the score on item k at the given point of the session, i is the desired item (next item in the session) and j are the negative samples.  ·(cid:80)NS  NS  ·(cid:80)NS  • TOP1: This ranking loss was devised by us for this task. It is the regularized approximation of the relative rank of the relevant item. The relative rank of the relevant item is given by j=1 I{ˆrs,j > ˆrs,i}. We approximate I{·} with a sigmoid. Optimizing for this 1 NS would modify parameters so that the score for i would be high. However this is unstable as certain positive items also act as negative examples and thus scores tend to become increasingly higher. To avoid this, we want to force the scores of the negative examples to be around zero. This is a natural expectation towards the scores of negative items. Thus we added a regularization term to the loss. It is important that this term is in the same range as the relative rank and acts similarly to it. The ﬁnal loss function is as follows: Ls = 1 NS  j=1 σ (ˆrs,j − ˆrs,i) + σ(cid:0)ˆr2 ·(cid:80)NS  (cid:1)  s,j  4 EXPERIMENTS  We evaluate the proposed recursive neural network against popular baselines on two datasets.  5  Published as a conference paper at ICLR 2016  The ﬁrst dataset is that of RecSys Challenge 20151. This dataset contains click-streams of an e- commerce site that sometimes end in purchase events. We work with the training set of the challenge and keep only the click events. We ﬁlter out sessions of length 1. The network is trained on ∼ 6 months of data, containing 7,966,257 sessions of 31,637,239 clicks on 37,483 items. We use the sessions of the subsequent day for testing. Each session is assigned to either the training or the test set, we do not split the data mid-session. Because of the nature of collaborative ﬁltering methods, we ﬁlter out clicks from the test set where the item clicked is not in the train set. Sessions of length one are also removed from the test set. After the preprocessing we are left with 15,324 sessions of 71,222 events for the test set. This dataset will be referred to as RSC15. The second dataset is collected from a Youtube-like OTT video service platform. Events of watching a video for at least a certain amount of time were collected. Only certain regions were subject to this collection that lasted for somewhat shorter than 2 months. During this time item-to-item recommendations were provided after each video at the left side of the screen. These were provided by a selection of different algorithms and inﬂuenced the behavior of the users. Preprocessing steps are similar to that of the other dataset with the addition of ﬁltering out very long sessions as they were probably generated by bots. The training data consists of all but the last day of the aforementioned period and has ∼ 3 million sessions of ∼ 13 million watch events on 330 thousand videos. The test set contains the sessions of the last day of the collection period and has ∼ 37 thousand sessions with ∼ 180 thousand watch events. This dataset will be referred to as VIDEO. The evaluation is done by providing the events of a session one-by-one and checking the rank of the item of the next event. The hidden state of the GRU is reset to zero after a session ﬁnishes. Items are ordered in descending order by their score and their position in this list is their rank. With RSC15, all of the 37,483 items of the train set were ranked. However, this would have been impractical with VIDEO, due to the large number of items. There we ranked the desired item against the most popular 30,000 items. This has negligible effect on the evaluations as rarely visited items often get low scores. Also, popularity based pre-ﬁltering is common in practical recommender systems. As recommender systems can only recommend a few items at once, the actual item a user might pick should be amongst the ﬁrst few items of the list. Therefore, our primary evaluation metric is recall@20 that is the proportion of cases having the desired item amongst the top-20 items in all test cases. Recall does not consider the actual rank of the item as long as it is amongst the top-N. This models certain practical scenarios well where there is no highlighting of recommendations and the absolute order does not matter. Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)(Liu et al., 2012; Hidasi & Tikk, 2012). The second metric used in the experiments is MRR@20 (Mean Reciprocal Rank). That is the average of reciprocal ranks of the desired items. The reciprocal rank is set to zero if the rank is above 20. MRR takes into account the rank of the item, which is important in cases where the order of recommendations matter (e.g. the lower ranked items are only visible after scrolling).  4.1 BASELINES  We compare the proposed network to a set of commonly used baselines.  set. Despite its simplicity it is often a strong baseline in certain domains.  • POP: Popularity predictor that always recommends the most popular items of the training • S-POP: This baseline recommends the most popular items of the current session. The rec- ommendation list changes during the session as items gain more events. Ties are broken up using global popularity values. This baseline is strong in domains with high repetitiveness. • Item-KNN: Items similar to the actual item are recommended by this baseline and simi- larity is deﬁned as the cosine similarity between the vector of their sessions, i.e. it is the number of co-occurrences of two items in sessions divided by the square root of the product of the numbers of sessions in which the individual items are occurred. Regularization is also included to avoid coincidental high similarities of rarely visited items. This baseline is one of the most common item-to-item solutions in practical systems, that provides recom- mendations in the “others who viewed this item also viewed these ones” setting. Despite of its simplicity it is usually a strong baseline (Linden et al., 2003; Davidson et al., 2010).  1http://2015.recsyschallenge.com/  6  Published as a conference paper at ICLR 2016  Table 1: Recall@20 and MRR@20 using the baseline methods  Baseline  POP S-POP Item-KNN BPR-MF  RSC15  VIDEO  Recall@20 MRR@20 Recall@20 MRR@20  0.0050 0.2672 0.5065 0.2574  0.0012 0.1775 0.2048 0.0618  0.0499 0.1301 0.5508 0.0692  0.0117 0.0863 0.3381 0.0374  Table 2: Best parametrizations for datasets/loss functions  Dataset Loss TOP1 RSC15 BPR RSC15 Cross-entropy RSC15 TOP1 VIDEO VIDEO BPR Cross-entropy VIDEO  Mini-batch Dropout Learning rate Momentum  50 50 500 50 50 200  0.5 0.2 0 0.4 0.3 0.1  0.01 0.05 0.01 0.05 0.1 0.05  0 0.2 0 0 0 0.3  • BPR-MF: BPR-MF (Rendle et al., 2009) is one of the commonly used matrix factorization methods. It optimizes for a pairwise ranking objective function (see Section 3) via SGD. Matrix factorization cannot be applied directly to session-based recommendations, because the new sessions do not have feature vectors precomputed. However we can overcome this by using the average of item feature vectors of the items that had occurred in the session so far as the user feature vector. In other words we average the similarities of the feature vectors between a recommendable item and the items of the session so far.  Table 1 shows the results for the baselines. The item-KNN approach clearly dominates the other methods.  4.2 PARAMETER & STRUCTURE OPTIMIZATION  We optimized the hyperparameters by running 100 experiments at randomly selected points of the parameter space for each dataset and loss function. The best parametrization was further tuned by individually optimizing each parameter. The number of hidden units was set to 100 in all cases. The best performing parameters were then used with hidden layers of different sizes. The optimization was done on a separate validation set. Then the networks were retrained on the training plus the validation set and evaluated on the ﬁnal test set. The best performing parametrizations are summarized in table 2. Weight matrices were initialized by random numbers drawn uniformly from [−x, x] where x depends on the number of rows and columns of the matrix. We experimented with both rmsprop (Dauphin et al., 2015) and adagrad (Duchi et al., 2011). We found adagrad to give better results. We brieﬂy experimented with other units than GRU. We found both the classic RNN unit and LSTM to perform worse. We tried out several loss functions. Pointwise ranking based losses, such as cross-entropy and MRR optimization (as in Steck (2015)) were usually unstable, even with regularization. For example cross-entropy yielded only 10 and 6 numerically stable networks of the 100 random runs for RSC15 and VIDEO respectively. We assume that this is due to independently trying to achieve high scores for the desired items and the negative push is small for the negative samples. On the other hand pairwise ranking-based losses performed well. We found the ones introduced in Section 3 (BPR and TOP1) to perform the best. Several architectures were examined and a single layer of GRU units was found to be the best performer. Adding addition layers always resulted in worst performance w.r.t. both training loss and recall and MRR measured on the test set. We assume that this is due to the generally short  7  Published as a conference paper at ICLR 2016  Table 3: Recall@20 and MRR@20 for different types of a single layer of GRU, compared to the best baseline (item-KNN). Best results per dataset are highlighted.  Loss / #Units  RSC15  Recall@20  MRR@20  Recall@20  VIDEO  MRR@20  TOP1 100 BPR 100 Cross-entropy 100 TOP1 1000 BPR 1000 Cross-entropy 1000  0.5853 (+15.55%) 0.6069 (+19.82%) 0.6074 (+19.91%) 0.6206 (+22.53%) 0.6322 (+24.82%) 0.5777 (+14.06%)  0.2305 (+12.58%) 0.2407 (+17.54%) 0.2430 (+18.65%) 0.2693 (+31.49%) 0.2467 (+20.47%) 0.2153 (+5.16%)  0.6141 (+11.50%) 0.5999 (+8.92%) 0.6372 (+15.69%) 0.6624 (+20.27%) 0.6311 (+14.58%)  –  0.3511 (+3.84%) 0.3260 (-3.56%) 0.3720 (+10.04%) 0.3891 (+15.08%) 0.3136 (-7.23%)  –  lifespan of the sessions not requiring multiple time scales of different resolutions to be properly represented. However the exact reason of this is unknown as of yet and requires further research. Using embedding of the items gave slightly worse results, therefore we kept the 1-of-N encoding. Also, putting all previous events of the session on the input instead of the preceding one did not result in additional accuracy gain; which is not surprising as GRU – like LSTM – has both long and short term memory. Adding additional feed-forward layers after the GRU layer did not help either. However increasing the size of the GRU layer improved the performance. We also found that it is beneﬁcial to use tanh as the activation function of the output layer.  4.3 RESULTS  Table 3 shows the results of the best performing networks. Cross-entropy for the VIDEO data with 1000 hidden units was numerically unstable and thus we present no results for that scenario. The results are compared to the best baseline (item-KNN). We show results with 100 and 1000 hidden units. The running time depends on the parameters and the dataset. Generally speaking the difference in runtime between the smaller and the larger variant is not too high on a GeForce GTX Titan X GPU and the training of the network can be done in a few hours2. On CPU, the smaller network can be trained in a practically acceptable timeframe. Frequent retraining is often desirable for recommender systems, because new users and items are introduced frequently. The GRU-based approach has substantial gain over the item-KNN in both evaluation metrics on both datasets, even if the number of units is 1003. Increasing the number of units further improves the results for pairwise losses, but the accuracy decreases for cross-entropy. Even though cross-entropy gives better results with 100 hidden units, the pairwise loss variants surpass these results as the number of units increase. Although, increasing the number of units increases the training times, we found that it was not too expensive to move from 100 units to 1000 on GPU. Also, the cross-entropy based loss was found to be numerically unstable as the result of the network individually trying to increase the score for the target items, while the negative push is relatively small for the other items. Therefore we suggest using any of the two pairwise losses. The TOP1 loss performs slightly better on these two datasets, resulting in ∼ 20 − 30% accuracy gain over the best performing baseline.  5 CONCLUSION & FUTURE WORK  In this paper we applied a kind of modern recurrent neural network (GRU) to new application do- main: recommender systems. We chose the task of session based recommendations, because it is a practically important area, but not well researched. We modiﬁed the basic GRU in order to ﬁt the task better by introducing session-parallel mini-batches, mini-batch based output sampling and ranking loss function. We showed that our method can signiﬁcantly outperform popular baselines that are used for this task. We think that our work can be the basis of both deep learning applications in recommender systems and session based recommendations in general.  2Using Theano with ﬁxes for the subtensor operators on GPU. 3Except for using the BPR loss on the VIDEO data and evaluating for MRR.  8  Published as a conference paper at ICLR 2016  Our immediate future work will focus on the more thorough examination of the proposed network. We also plan to train the network on automatically extracted item representation that is built on content of the item itself (e.g. thumbnail, video, text) instead of the current input.  ACKNOWLEDGMENTS  The work leading to these results has received funding from the European Union’s Seventh Frame- work Programme (FP7/2007-2013) under CrowdRec Grant Agreement n◦ 610594.  REFERENCES Cho, Kyunghyun, van Merri¨enboer, Bart, Bahdanau, Dzmitry, and Bengio, Yoshua. On the proper- ties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.  Dauphin, Yann N, de Vries, Harm, Chung, Junyoung, and Bengio, Yoshua. Rmsprop and equi- librated adaptive learning rates for non-convex optimization. arXiv preprint arXiv:1502.04390, 2015.  Davidson, James, Liebald, Benjamin, Liu, Junning, et al. The YouTube video recommendation system. In Recsys’10: ACM Conf. on Recommender Systems, pp. 293–296, 2010. ISBN 978-1- 60558-906-0.  Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning  and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.  Hidasi, B. and Tikk, D. Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback. In ECML-PKDD’12, Part II, number 7524 in LNCS, pp. 67–82. Springer, 2012.  Hidasi, Bal´azs and Tikk, Domonkos. General factorization framework for context-aware recommen- dations. Data Mining and Knowledge Discovery, pp. 1–30, 2015. ISSN 1384-5810. doi: 10.1007/ s10618-015-0417-y. URL http://dx.doi.org/10.1007/s10618-015-0417-y.  Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural net- works for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82–97, 2012.  Koren, Y. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model. In  SIGKDD’08: ACM Int. Conf. on Knowledge Discovery and Data Mining, pp. 426–434, 2008.  Koren, Yehuda, Bell, Robert, and Volinsky, Chris. Matrix factorization techniques for recommender  systems. Computer, 42(8):30–37, 2009.  Linden, G., Smith, B., and York, J. Amazon. com recommendations: Item-to-item collaborative  ﬁltering. Internet Computing, IEEE, 7(1):76–80, 2003.  Liu, Qiwen, Chen, Tianjian, Cai, Jing, and Yu, Dianhai. Enlister: Baidu’s recommender system for the biggest Chinese Q&A website. In RecSys-12: Proc. of the 6th ACM Conf. on Recommender Systems, pp. 285–288, 2012.  Rendle, S., Freudenthaler, C., Gantner, Z., and Schmidt-Thieme, L. BPR: Bayesian personalized ranking from implicit feedback. In UAI’09: 25th Conf. on Uncertainty in Artiﬁcial Intelligence, pp. 452–461, 2009. ISBN 978-0-9749039-5-8.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael S., Berg, Alexander C., and Li, Fei-Fei. Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. URL http://arxiv.org/abs/1409.0575.  9  Published as a conference paper at ICLR 2016  Salakhutdinov, Ruslan, Mnih, Andriy, and Hinton, Geoffrey. Restricted boltzmann machines for collaborative ﬁltering. In Proceedings of the 24th international conference on Machine learning, pp. 791–798. ACM, 2007.  Sarwar, Badrul, Karypis, George, Konstan, Joseph, and Riedl, John. Item-based collaborative ﬁlter- ing recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web, pp. 285–295. ACM, 2001.  Shani, Guy, Brafman, Ronen I, and Heckerman, David. An mdp-based recommender system. In Proceedings of the Eighteenth conference on Uncertainty in artiﬁcial intelligence, pp. 453–460. Morgan Kaufmann Publishers Inc., 2002.  Shi, Yue, Karatzoglou, Alexandros, Baltrunas, Linas, Larson, Martha, Oliver, Nuria, and Hanjalic, Alan. Climf: Learning to maximize reciprocal rank with collaborative less-is-more ﬁltering. In Proceedings of the Sixth ACM Conference on Recommender Systems, RecSys ’12, pp. 139–146, New York, NY, USA, 2012. ACM. ISBN 978-1-4503-1270-7. doi: 10.1145/2365952.2365981. URL http://doi.acm.org/10.1145/2365952.2365981.  Steck, Harald. Gaussian ranking by matrix factorization. In Proceedings of the 9th ACM Confer- ence on Recommender Systems, RecSys ’15, pp. 115–122, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-3692-5. doi: 10.1145/2792838.2800185. URL http://doi.acm.org/ 10.1145/2792838.2800185.  Van den Oord, Aaron, Dieleman, Sander, and Schrauwen, Benjamin. Deep content-based music recommendation. In Advances in Neural Information Processing Systems, pp. 2643–2651, 2013.  Wang, Hao, Wang, Naiyan, and Yeung, Dit-Yan. Collaborative deep learning for recommender In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge  systems. Discovery and Data Mining, KDD ’15, pp. 1235–1244, New York, NY, USA, 2015. ACM.  Weimer, Markus, Karatzoglou, Alexandros, Le, Quoc Viet, and Smola, Alex. Maximum margin ma- trix factorization for collaborative ranking. Advances in neural information processing systems, 2007.  10  ",
1509.02971,2016,Continuous control with deep reinforcement learning,"['Continuous control with deep reinforcement learning\nTimothy Lillicrap', 'Jonathan Hunt', 'Alexander Pritzel', 'Nicolas Heess', 'Tom Erez', 'Yuval Tassa', 'David Silver', 'Daan Wierstra']",https://arxiv.org/pdf/1509.02971,"9 1 0 2    l u J    5      ]  G L . s c [      6 v 1 7 9 2 0  .  9 0 5 1 : v i X r a  Published as a conference paper at ICLR 2016  CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING  Timothy P. Lillicrap∗, Jonathan J. Hunt∗, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver & Daan Wierstra Google Deepmind London, UK {countzero, jjhunt, apritzel, heess, etom, tassa, davidsilver, wierstra} @ google.com  ABSTRACT  We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the de- terministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our al- gorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to ﬁnd policies whose performance is com- petitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel in- puts.  1  INTRODUCTION  One of the primary goals of the ﬁeld of artiﬁcial intelligence is to solve complex tasks from unpro- cessed, high-dimensional, sensory input. Recently, signiﬁcant progress has been made by combin- ing advances in deep learning for sensory processing (Krizhevsky et al., 2012) with reinforcement learning, resulting in the “Deep Q Network” (DQN) algorithm (Mnih et al., 2015) that is capable of human level performance on many Atari video games using unprocessed pixels for input. To do so, deep neural network function approximators were used to estimate the action-value function. However, while DQN solves problems with high-dimensional observation spaces, it can only handle discrete and low-dimensional action spaces. Many tasks of interest, most notably physical control tasks, have continuous (real valued) and high dimensional action spaces. DQN cannot be straight- forwardly applied to continuous domains since it relies on a ﬁnding the action that maximizes the action-value function, which in the continuous valued case requires an iterative optimization process at every step. An obvious approach to adapting deep reinforcement learning methods such as DQN to continuous domains is to to simply discretize the action space. However, this has many limitations, most no- tably the curse of dimensionality: the number of actions increases exponentially with the number of degrees of freedom. For example, a 7 degree of freedom system (as in the human arm) with the coarsest discretization ai ∈ {−k, 0, k} for each joint leads to an action space with dimensionality: 37 = 2187. The situation is even worse for tasks that require ﬁne control of actions as they require a correspondingly ﬁner grained discretization, leading to an explosion of the number of discrete actions. Such large action spaces are difﬁcult to explore efﬁciently, and thus successfully training DQN-like networks in this context is likely intractable. Additionally, naive discretization of action spaces needlessly throws away information about the structure of the action domain, which may be essential for solving many problems. In this work we present a model-free, off-policy actor-critic algorithm using deep function approx- imators that can learn policies in high-dimensional, continuous action spaces. Our work is based  ∗These authors contributed equally.  1  Published as a conference paper at ICLR 2016  on the deterministic policy gradient (DPG) algorithm (Silver et al., 2014) (itself similar to NFQCA (Hafner & Riedmiller, 2011), and similar ideas can be found in (Prokhorov et al., 1997)). However, as we show below, a naive application of this actor-critic method with neural function approximators is unstable for challenging problems. Here we combine the actor-critic approach with insights from the recent success of Deep Q Network (DQN) (Mnih et al., 2013; 2015). Prior to DQN, it was generally believed that learning value functions using large, non-linear function approximators was difﬁcult and unstable. DQN is able to learn value functions using such function approximators in a stable and robust way due to two innovations: 1. the network is trained off-policy with samples from a replay buffer to minimize correlations between samples; 2. the network is trained with a target Q network to give consistent targets during temporal difference backups. In this work we make use of the same ideas, along with batch normalization (Ioffe & Szegedy, 2015), a recent advance in deep learning. In order to evaluate our method we constructed a variety of challenging physical control problems that involve complex multi-joint movements, unstable and rich contact dynamics, and gait behavior. Among these are classic problems such as the cartpole swing-up problem, as well as many new domains. A long-standing challenge of robotic control is to learn an action policy directly from raw sensory input such as video. Accordingly, we place a ﬁxed viewpoint camera in the simulator and attempted all tasks using both low-dimensional observations (e.g. joint angles) and directly from pixels. Our model-free approach which we call Deep DPG (DDPG) can learn competitive policies for all of our tasks using low-dimensional observations (e.g. cartesian coordinates or joint angles) using the same hyper-parameters and network structure. In many cases, we are also able to learn good policies directly from pixels, again keeping hyperparameters and network structure constant 1. A key feature of the approach is its simplicity: it requires only a straightforward actor-critic archi- tecture and learning algorithm with very few “moving parts”, making it easy to implement and scale to more difﬁcult problems and larger networks. For the physical control problems we compare our results to a baseline computed by a planner (Tassa et al., 2012) that has full access to the underly- ing simulated dynamics and its derivatives (see supplementary information). Interestingly, DDPG can sometimes ﬁnd policies that exceed the performance of the planner, in some cases even when learning from pixels (the planner always plans over the underlying low-dimensional state space).  2 BACKGROUND  We consider a standard reinforcement learning setup consisting of an agent interacting with an en- vironment E in discrete timesteps. At each timestep t the agent receives an observation xt, takes an action at and receives a scalar reward rt. In all the environments considered here the actions are real-valued at ∈ IRN . In general, the environment may be partially observed so that the entire history of the observation, action pairs st = (x1, a1, ..., at−1, xt) may be required to describe the state. Here, we assumed the environment is fully-observed so st = xt. An agent’s behavior is deﬁned by a policy, π, which maps states to a probability distribution over the actions π : S → P(A). The environment, E, may also be stochastic. We model it as a Markov decision process with a state space S, action space A = IRN , an initial state distribution p(s1), transition dynamics p(st+1|st, at), and reward function r(st, at).  The return from a state is deﬁned as the sum of discounted future reward Rt =(cid:80)T  i=t γ(i−t)r(si, ai) with a discounting factor γ ∈ [0, 1]. Note that the return depends on the actions chosen, and therefore on the policy π, and may be stochastic. The goal in reinforcement learning is to learn a policy which maximizes the expected return from the start distribution J = Eri,si∼E,ai∼π [R1]. We denote the discounted state visitation distribution for a policy π as ρπ. The action-value function is used in many reinforcement learning algorithms. It describes the ex- pected return after taking an action at in state st and thereafter following policy π:  Qπ(st, at) = Eri≥t,si>t∼E,ai>t∼π [Rt|st, at]  (1)  1You can view a movie of some of the learned policies at https://goo.gl/J4PIAz  2  Published as a conference paper at ICLR 2016  (3)  (4)  (5)  Many approaches in reinforcement learning make use of the recursive relationship known as the Bellman equation:  (cid:2)r(st, at) + γ Eat+1∼π [Qπ(st+1, at+1)](cid:3)  Qπ(st, at) = Ert,st+1∼E  (2) If the target policy is deterministic we can describe it as a function µ : S ← A and avoid the inner expectation:  Qµ(st, at) = Ert,st+1∼E [r(st, at) + γQµ(st+1, µ(st+1))]  The expectation depends only on the environment. This means that it is possible to learn Qµ off- policy, using transitions which are generated from a different stochastic behavior policy β. Q-learning (Watkins & Dayan, 1992), a commonly used off-policy algorithm, uses the greedy policy µ(s) = arg maxa Q(s, a). We consider function approximators parameterized by θQ, which we optimize by minimizing the loss:  L(θQ) = Est∼ρβ ,at∼β,rt∼E  (cid:104)(cid:0)Q(st, at|θQ) − yt  (cid:1)2(cid:105)  where  yt = r(st, at) + γQ(st+1, µ(st+1)|θQ).  While yt is also dependent on θQ, this is typically ignored. The use of large, non-linear function approximators for learning value or action-value functions has often been avoided in the past since theoretical performance guarantees are impossible, and prac- tically learning tends to be unstable. Recently, (Mnih et al., 2013; 2015) adapted the Q-learning algorithm in order to make effective use of large neural networks as function approximators. Their algorithm was able to learn to play Atari games from pixels. In order to scale Q-learning they intro- duced two major changes: the use of a replay buffer, and a separate target network for calculating yt. We employ these in the context of DDPG and explain their implementation in the next section.  3 ALGORITHM  It is not possible to straightforwardly apply Q-learning to continuous action spaces, because in con- tinuous spaces ﬁnding the greedy policy requires an optimization of at at every timestep; this opti- mization is too slow to be practical with large, unconstrained function approximators and nontrivial action spaces. Instead, here we used an actor-critic approach based on the DPG algorithm (Silver et al., 2014). The DPG algorithm maintains a parameterized actor function µ(s|θµ) which speciﬁes the current policy by deterministically mapping states to a speciﬁc action. The critic Q(s, a) is learned using the Bellman equation as in Q-learning. The actor is updated by following the applying the chain rule to the expected return from the start distribution J with respect to the actor parameters:  (cid:2)∇θµQ(s, a|θQ)|s=st,a=µ(st|θµ) (cid:2)∇aQ(s, a|θQ)|s=st,a=µ(st)∇θµµ(s|θµ)|s=st  (cid:3)  (cid:3)  ∇θµJ ≈ Est∼ρβ = Est∼ρβ  (6)  Silver et al. (2014) proved that this is the policy gradient, the gradient of the policy’s performance 2. As with Q learning, introducing non-linear function approximators means that convergence is no longer guaranteed. However, such approximators appear essential in order to learn and generalize on large state spaces. NFQCA (Hafner & Riedmiller, 2011), which uses the same update rules as DPG but with neural network function approximators, uses batch learning for stability, which is intractable for large networks. A minibatch version of NFQCA which does not reset the policy at each update, as would be required to scale to large networks, is equivalent to the original DPG, which we compare to here. Our contribution here is to provide modiﬁcations to DPG, inspired by the success of DQN, which allow it to use neural network function approximators to learn in large state and action spaces online. We refer to our algorithm as Deep DPG (DDPG, Algorithm 1).  2In practice, as in commonly done in policy gradient implementations, we ignored the discount in the state-  visitation distribution ρβ.  3  Published as a conference paper at ICLR 2016  One challenge when using neural networks for reinforcement learning is that most optimization al- gorithms assume that the samples are independently and identically distributed. Obviously, when the samples are generated from exploring sequentially in an environment this assumption no longer holds. Additionally, to make efﬁcient use of hardware optimizations, it is essential to learn in mini- batches, rather than online. As in DQN, we used a replay buffer to address these issues. The replay buffer is a ﬁnite sized cache R. Transitions were sampled from the environment according to the exploration policy and the tuple (st, at, rt, st+1) was stored in the replay buffer. When the replay buffer was full the oldest samples were discarded. At each timestep the actor and critic are updated by sampling a minibatch uniformly from the buffer. Because DDPG is an off-policy algorithm, the replay buffer can be large, allowing the algorithm to beneﬁt from learning across a set of uncorrelated transitions. Directly implementing Q learning (equation 4) with neural networks proved to be unstable in many environments. Since the network Q(s, a|θQ) being updated is also used in calculating the target value (equation 5), the Q update is prone to divergence. Our solution is similar to the target network used in (Mnih et al., 2013) but modiﬁed for actor-critic and using “soft” target updates, rather than directly copying the weights. We create a copy of the actor and critic networks, Q(cid:48)(s, a|θQ(cid:48) ) and µ(cid:48)(s|θµ(cid:48) ) respectively, that are used for calculating the target values. The weights of these target networks are then updated by having them slowly track the learned networks: θ(cid:48) ← τ θ + (1 − τ )θ(cid:48) with τ (cid:28) 1. This means that the target values are constrained to change slowly, greatly improving the stability of learning. This simple change moves the relatively unstable problem of learning the action-value function closer to the case of supervised learning, a problem for which robust solutions exist. We found that having both a target µ(cid:48) and Q(cid:48) was required to have stable targets yi in order to consistently train the critic without divergence. This may slow learning, since the target network delays the propagation of value estimations. However, in practice we found this was greatly outweighed by the stability of learning. When learning from low dimensional feature vector observations, the different components of the observation may have different physical units (for example, positions versus velocities) and the ranges may vary across environments. This can make it difﬁcult for the network to learn effec- tively and may make it difﬁcult to ﬁnd hyper-parameters which generalise across environments with different scales of state values. One approach to this problem is to manually scale the features so they are in similar ranges across environments and units. We address this issue by adapting a recent technique from deep learning called batch normalization (Ioffe & Szegedy, 2015). This technique normalizes each dimension across the samples in a minibatch to have unit mean and variance. In addition, it maintains a run- ning average of the mean and variance to use for normalization during testing (in our case, during exploration or evaluation). In deep networks, it is used to minimize covariance shift during training, by ensuring that each layer receives whitened input. In the low-dimensional case, we used batch normalization on the state input and all layers of the µ network and all layers of the Q network prior to the action input (details of the networks are given in the supplementary material). With batch normalization, we were able to learn effectively across many different tasks with differing types of units, without needing to manually ensure the units were within a set range. A major challenge of learning in continuous action spaces is exploration. An advantage of off- policies algorithms such as DDPG is that we can treat the problem of exploration independently from the learning algorithm. We constructed an exploration policy µ(cid:48) by adding noise sampled from a noise process N to our actor policy  µ(cid:48)(st) = µ(st|θµ  t ) + N  (7) N can be chosen to suit the environment. As detailed in the supplementary materials we used an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) to generate temporally correlated exploration for exploration efﬁciency in physical control problems with inertia (similar use of auto- correlated noise was introduced in (Wawrzy´nski, 2015)).  4 RESULTS  We constructed simulated physical environments of varying levels of difﬁculty to test our algorithm. This included classic reinforcement learning environments such as cartpole, as well as difﬁcult,  4  Published as a conference paper at ICLR 2016  Algorithm 1 DDPG algorithm  Randomly initialize critic network Q(s, a|θQ) and actor µ(s|θµ) with weights θQ and θµ. Initialize target network Q(cid:48) and µ(cid:48) with weights θQ(cid:48) ← θQ, θµ(cid:48) ← θµ Initialize replay buffer R for episode = 1, M do  Initialize a random process N for action exploration Receive initial observation state s1 for t = 1, T do  Select action at = µ(st|θµ) + Nt according to the current policy and exploration noise Execute action at and observe reward rt and observe new state st+1 Store transition (st, at, rt, st+1) in R (cid:80) Sample a random minibatch of N transitions (si, ai, ri, si+1) from R Set yi = ri + γQ(cid:48)(si+1, µ(cid:48)(si+1|θµ(cid:48) i(yi − Q(si, ai|θQ))2 Update critic by minimizing the loss: L = 1 N Update the actor policy using the sampled policy gradient:  )|θQ(cid:48)  )  (cid:88)  ∇aQ(s, a|θQ)|s=si,a=µ(si)∇θµµ(s|θµ)|si  i  θQ(cid:48) ← τ θQ + (1 − τ )θQ(cid:48) θµ(cid:48) ← τ θµ + (1 − τ )θµ(cid:48)  ∇θµ J ≈ 1 N  Update the target networks:  end for  end for  high dimensional tasks such as gripper, tasks involving contacts such as puck striking (canada) and locomotion tasks such as cheetah (Wawrzy´nski, 2009). In all domains but cheetah the actions were torques applied to the actuated joints. These environments were simulated using MuJoCo (Todorov et al., 2012). Figure 1 shows renderings of some of the environments used in the task (the supplementary contains details of the environments and you can view some of the learned policies at https://goo.gl/J4PIAz). In all tasks, we ran experiments using both a low-dimensional state description (such as joint angles and positions) and high-dimensional renderings of the environment. As in DQN (Mnih et al., 2013; 2015), in order to make the problems approximately fully observable in the high dimensional envi- ronment we used action repeats. For each timestep of the agent, we step the simulation 3 timesteps, repeating the agent’s action and rendering each time. Thus the observation reported to the agent contains 9 feature maps (the RGB of each of the 3 renderings) which allows the agent to infer veloc- ities using the differences between frames. The frames were downsampled to 64x64 pixels and the 8-bit RGB values were converted to ﬂoating point scaled to [0, 1]. See supplementary information for details of our network structure and hyperparameters. We evaluated the policy periodically during training by testing it without exploration noise. Figure 2 shows the performance curve for a selection of environments. We also report results with compo- nents of our algorithm (i.e. the target network or batch normalization) removed. In order to perform well across all tasks, both of these additions are necessary. In particular, learning without a target network, as in the original work with DPG, is very poor in many environments. Surprisingly, in some simpler tasks, learning policies from pixels is just as fast as learning using the low-dimensional state descriptor. This may be due to the action repeats making the problem simpler. It may also be that the convolutional layers provide an easily separable representation of state space, which is straightforward for the higher layers to learn on quickly. Table 1 summarizes DDPG’s performance across all of the environments (results are averaged over 5 replicas). We normalized the scores using two baselines. The ﬁrst baseline is the mean return from a naive policy which samples actions from a uniform distribution over the valid action space. The second baseline is iLQG (Todorov & Li, 2005), a planning based solver with full access to the  5  Published as a conference paper at ICLR 2016  underlying physical model and its derivatives. We normalize scores so that the naive policy has a mean score of 0 and iLQG has a mean score of 1. DDPG is able to learn good policies on many of the tasks, and in many cases some of the replicas learn policies which are superior to those found by iLQG, even when learning directly from pixels. It can be challenging to learn accurate value estimates. Q-learning, for example, is prone to over- estimating values (Hasselt, 2010). We examined DDPG’s estimates empirically by comparing the values estimated by Q after training with the true returns seen on test episodes. Figure 3 shows that in simple tasks DDPG estimates returns accurately without systematic biases. For harder tasks the Q estimates are worse, but DDPG is still able learn good policies. To demonstrate the generality of our approach we also include Torcs, a racing game where the actions are acceleration, braking and steering. Torcs has previously been used as a testbed in other policy learning approaches (Koutn´ık et al., 2014b). We used an identical network architecture and learning algorithm hyper-parameters to the physics tasks but altered the noise process for exploration because of the very different time scales involved. On both low-dimensional and from pixels, some replicas were able to learn reasonable policies that are able to complete a circuit around the track though other replicas failed to learn a sensible policy.  Figure 1: Example screenshots of a sample of environments we attempt to solve with DDPG. In order from the left: the cartpole swing-up task, a reaching task, a gasp and move task, a puck-hitting task, a monoped balancing task, two locomotion tasks and Torcs (driving simulator). We tackle all tasks using both low-dimensional feature vector and high-dimensional pixel inputs. Detailed descriptions of the environments are provided in the supplementary. Movies of some of the learned policies are available at https://goo.gl/J4PIAz.  Figure 2: Performance curves for a selection of domains using variants of DPG: original DPG algorithm (minibatch NFQCA) with batch normalization (light grey), with target network (dark grey), with target networks and batch normalization (green), with target networks from pixel-only inputs (blue). Target networks are crucial.  5 RELATED WORK  The original DPG paper evaluated the algorithm with toy problems using tile-coding and linear function approximators. It demonstrated data efﬁciency advantages for off-policy DPG over both on- and off-policy stochastic actor critic. It also solved one more challenging task in which a multi- jointed octopus arm had to strike a target with any part of the limb. However, that paper did not demonstrate scaling the approach to large, high-dimensional observation spaces as we have here. It has often been assumed that standard policy search methods such as those explored in the present work are simply too fragile to scale to difﬁcult problems (Levine et al., 2015). Standard policy search  6  CartPendulum Swing-upCartpole Swing-upFixed ReacherBlockworldGripperPuck ShootingMonoped BalancingMoving GripperCheetahMillion Steps01101100110010011100Normalized Reward1000001111Published as a conference paper at ICLR 2016  Figure 3: Density plot showing estimated Q values versus observed returns sampled from test episodes on 5 replicas. In simple domains such as pendulum and cartpole the Q values are quite accurate. In more complex tasks, the Q estimates are less accurate, but can still be used to learn competent policies. Dotted line indicates unity, units are arbitrary.  Table 1: Performance after training across all environments for at most 2.5 million steps. We report both the average and best observed (across 5 runs). All scores, except Torcs, are normalized so that a random agent receives 0 and a planning algorithm 1; for Torcs we present the raw reward score. We include results from the DDPG algorithn in the low-dimensional (lowd) version of the environment and high-dimensional (pix). For comparision we also include results from the original DPG algorithm with a replay buffer and batch normalization (cntrl).  Rav,lowd Rbest,lowd  Rbest,pix Rav,cntrl Rbest,cntrl  environment blockworld1 blockworld3da  canada canada2d  cart  cartpole  cartpoleBalance  cartpoleParallelDouble cartpoleSerialDouble cartpoleSerialTriple  cheetah  ﬁxedReacher  ﬁxedReacherDouble ﬁxedReacherSingle  gripper  gripperRandom  hardCheetah  hopper  hyq  movingGripper  pendulum reacher  reacherSingle  walker2d  torcs  reacher3daFixedTarget reacher3daRandomTarget  1.156 0.340 0.303 0.400 0.938 0.844 0.951 0.549 0.272 0.736 0.903 0.849 0.924 0.954 0.655 0.618 1.311 0.676 0.416 0.474 0.946 0.720 0.585 0.467 0.981 0.705  1.511 0.705 1.735 0.978 1.336 1.115 1.000 0.900 0.719 0.946 1.206 1.021 0.996 1.000 0.972 0.937 1.990 0.936 0.722 0.936 1.021 0.987 0.943 0.739 1.102 1.573  Rav,pix 0.466 0.889 0.176 -0.285 1.096 0.482 0.335 0.188 0.195 0.412 0.457 0.693 0.872 0.827 0.406 0.082 1.204 0.112 0.234 0.480 0.663 0.194 0.453 0.374 1.000 0.944  -401.911  1.299 2.225 0.688 0.119 1.258 1.138 0.996 0.323 0.642 0.427 0.792 0.981 0.943 0.995 0.790 0.791 1.431 0.924 0.672 0.644 1.055 0.878 0.922 0.735 1.083 1.476  -0.080 -0.139 0.125 -0.045 0.343 0.244 -0.468 0.197 0.143 0.583 -0.008 0.259 0.290 0.620 0.461 0.557 -0.031 0.078 0.198 0.416 0.099 0.231 0.204 -0.046 1.010 0.393  1.260 0.658 1.157 0.701 1.216 0.755 0.528 0.572 0.701 0.942 0.425 0.927 0.995 0.999 0.816 0.808 1.411 0.917 0.618 0.805 0.951 0.953 0.631 0.158 1.083 1.397  -393.385  1840.036  1876.284  -911.034  1961.600  is thought to be difﬁcult because it deals simultaneously with complex environmental dynamics and a complex policy. Indeed, most past work with actor-critic and policy optimization approaches have had difﬁculty scaling up to more challenging problems (Deisenroth et al., 2013). Typically, this is due to instability in learning wherein progress on a problem is either destroyed by subsequent learning updates, or else learning is too slow to be practical. Recent work with model-free policy search has demonstrated that it may not be as fragile as previ- ously supposed. Wawrzy´nski (2009); Wawrzy´nski & Tanwani (2013) has trained stochastic policies  7  PendulumCheetahCartpoleEstimated QReturnPublished as a conference paper at ICLR 2016  in an actor-critic framework with a replay buffer. Concurrent with our work, Balduzzi & Ghifary (2015) extended the DPG algorithm with a “deviator” network which explicitly learns ∂Q/∂a. How- ever, they only train on two low-dimensional domains. Heess et al. (2015) introduced SVG(0) which also uses a Q-critic but learns a stochastic policy. DPG can be considered the deterministic limit of SVG(0). The techniques we described here for scaling DPG are also applicable to stochastic policies by using the reparametrization trick (Heess et al., 2015; Schulman et al., 2015a). Another approach, trust region policy optimization (TRPO) (Schulman et al., 2015b), directly con- structs stochastic neural network policies without decomposing problems into optimal control and supervised phases. This method produces near monotonic improvements in return by making care- fully chosen updates to the policy parameters, constraining updates to prevent the new policy from diverging too far from the existing policy. This approach does not require learning an action-value function, and (perhaps as a result) appears to be signiﬁcantly less data efﬁcient. To combat the challenges of the actor-critic approach, recent work with guided policy search (GPS) algorithms (e.g., (Levine et al., 2015)) decomposes the problem into three phases that are rela- tively easy to solve: ﬁrst, it uses full-state observations to create locally-linear approximations of the dynamics around one or more nominal trajectories, and then uses optimal control to ﬁnd the locally-linear optimal policy along these trajectories; ﬁnally, it uses supervised learning to train a complex, non-linear policy (e.g. a deep neural network) to reproduce the state-to-action mapping of the optimized trajectories. This approach has several beneﬁts, including data efﬁciency, and has been applied successfully to a variety of real-world robotic manipulation tasks using vision. In these tasks GPS uses a similar convolutional policy network to ours with 2 notable exceptions: 1. it uses a spatial softmax to reduce the dimensionality of visual features into a single (x, y) coordinate for each feature map, and 2. the policy also receives direct low-dimensional state information about the conﬁguration of the robot at the ﬁrst fully connected layer in the network. Both likely increase the power and data efﬁciency of the algorithm and could easily be exploited within the DDPG framework. PILCO (Deisenroth & Rasmussen, 2011) uses Gaussian processes to learn a non-parametric, proba- bilistic model of the dynamics. Using this learned model, PILCO calculates analytic policy gradients and achieves impressive data efﬁciency in a number of control problems. However, due to the high computational demand, PILCO is “impractical for high-dimensional problems” (Wahlstr¨om et al., 2015). It seems that deep function approximators are the most promising approach for scaling rein- forcement learning to large, high-dimensional domains. Wahlstr¨om et al. (2015) used a deep dynamical model network along with model predictive control to solve the pendulum swing-up task from pixel input. They trained a differentiable forward model and encoded the goal state into the learned latent space. They use model-predictive control over the learned model to ﬁnd a policy for reaching the target. However, this approach is only applicable to domains with goal states that can be demonstrated to the algorithm. Recently, evolutionary approaches have been used to learn competitive policies for Torcs from pixels using compressed weight parametrizations (Koutn´ık et al., 2014a) or unsupervised learning (Koutn´ık et al., 2014b) to reduce the dimensionality of the evolved weights. It is unclear how well these approaches generalize to other problems.  6 CONCLUSION  The work combines insights from recent advances in deep learning and reinforcement learning, re- sulting in an algorithm that robustly solves challenging problems across a variety of domains with continuous action spaces, even when using raw pixels for observations. As with most reinforcement learning algorithms, the use of non-linear function approximators nulliﬁes any convergence guar- antees; however, our experimental results demonstrate that stable learning without the need for any modiﬁcations between environments. Interestingly, all of our experiments used substantially fewer steps of experience than was used by DQN learning to ﬁnd solutions in the Atari domain. Nearly all of the problems we looked at were solved within 2.5 million steps of experience (and usually far fewer), a factor of 20 fewer steps than  8  Published as a conference paper at ICLR 2016  DQN requires for good Atari solutions. This suggests that, given more simulation time, DDPG may solve even more difﬁcult problems than those considered here. A few limitations to our approach remain. Most notably, as with most model-free reinforcement approaches, DDPG requires a large number of training episodes to ﬁnd solutions. However, we believe that a robust model-free approach may be an important component of larger systems which may attack these limitations (Gl¨ascher et al., 2010).  REFERENCES Balduzzi, David and Ghifary, Muhammad. Compatible value gradients for reinforcement learning  of continuous deep policies. arXiv preprint arXiv:1509.03005, 2015.  Deisenroth, Marc and Rasmussen, Carl E. Pilco: A model-based and data-efﬁcient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML- 11), pp. 465–472, 2011.  Deisenroth, Marc Peter, Neumann, Gerhard, Peters, Jan, et al. A survey on policy search for robotics.  Foundations and Trends in Robotics, 2(1-2):1–142, 2013.  Gl¨ascher, Jan, Daw, Nathaniel, Dayan, Peter, and O’Doherty, John P. States versus rewards: dis- sociable neural prediction error signals underlying model-based and model-free reinforcement learning. Neuron, 66(4):585–595, 2010.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectiﬁer networks. In Proceed- ings of the 14th International Conference on Artiﬁcial Intelligence and Statistics. JMLR W&CP Volume, volume 15, pp. 315–323, 2011.  Hafner, Roland and Riedmiller, Martin. Reinforcement learning in feedback control. Machine  learning, 84(1-2):137–169, 2011.  Hasselt, Hado V. Double q-learning. In Advances in Neural Information Processing Systems, pp.  2613–2621, 2010.  Heess, N., Hunt, J. J, Lillicrap, T. P, and Silver, D. Memory-based control with recurrent neural  networks. NIPS Deep Reinforcement Learning Workshop (arXiv:1512.04455), 2015.  Heess, Nicolas, Wayne, Gregory, Silver, David, Lillicrap, Tim, Erez, Tom, and Tassa, Yuval. Learn- ing continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2926–2934, 2015.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by  reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980, 2014.  Koutn´ık, Jan, Schmidhuber, J¨urgen, and Gomez, Faustino. Evolving deep unsupervised convolu- tional networks for vision-based reinforcement learning. In Proceedings of the 2014 conference on Genetic and evolutionary computation, pp. 541–548. ACM, 2014a.  Koutn´ık, Jan, Schmidhuber, J¨urgen, and Gomez, Faustino. Online evolution of deep convolutional network for vision-based reinforcement learning. In From Animals to Animats 13, pp. 260–269. Springer, 2014b.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel, Pieter. End-to-end training of deep  visuomotor policies. arXiv preprint arXiv:1504.00702, 2015.  9  Published as a conference paper at ICLR 2016  Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Graves, Alex, Antonoglou, Ioannis, Wier- stra, Daan, and Riedmiller, Martin. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.  Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Human- level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.  Prokhorov, Danil V, Wunsch, Donald C, et al. Adaptive critic designs. Neural Networks, IEEE  Transactions on, 8(5):997–1007, 1997.  Schulman, John, Heess, Nicolas, Weber, Theophane, and Abbeel, Pieter. Gradient estimation using stochastic computation graphs. In Advances in Neural Information Processing Systems, pp. 3510– 3522, 2015a.  Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan, Michael I, and Abbeel, Pieter. Trust region  policy optimization. arXiv preprint arXiv:1502.05477, 2015b.  Silver, David, Lever, Guy, Heess, Nicolas, Degris, Thomas, Wierstra, Daan, and Riedmiller, Martin.  Deterministic policy gradient algorithms. In ICML, 2014.  Tassa, Yuval, Erez, Tom, and Todorov, Emanuel. Synthesis and stabilization of complex behaviors through online trajectory optimization. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 4906–4913. IEEE, 2012.  Todorov, Emanuel and Li, Weiwei. A generalized iterative lqg method for locally-optimal feed- back control of constrained nonlinear stochastic systems. In American Control Conference, 2005. Proceedings of the 2005, pp. 300–306. IEEE, 2005.  Todorov, Emanuel, Erez, Tom, and Tassa, Yuval. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026– 5033. IEEE, 2012.  Uhlenbeck, George E and Ornstein, Leonard S. On the theory of the brownian motion. Physical  review, 36(5):823, 1930.  Wahlstr¨om, Niklas, Sch¨on, Thomas B, and Deisenroth, Marc Peter. From pixels to torques: Policy  learning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.  Watkins, Christopher JCH and Dayan, Peter. Q-learning. Machine learning, 8(3-4):279–292, 1992.  Wawrzy´nski, Paweł. Real-time reinforcement learning by sequential actor–critics and experience  replay. Neural Networks, 22(10):1484–1497, 2009.  Wawrzy´nski, Paweł. Control policy with autocorrelated noise in reinforcement learning for robotics.  International Journal of Machine Learning and Computing, 5:91–95, 2015.  Wawrzy´nski, Paweł and Tanwani, Ajay Kumar. Autonomous reinforcement learning with experience  replay. Neural Networks, 41:156–167, 2013.  10  Published as a conference paper at ICLR 2016  Supplementary Information: Continuous control with  deep reinforcement learning  7 EXPERIMENT DETAILS  We used Adam (Kingma & Ba, 2014) for learning the neural network parameters with a learning rate of 10−4 and 10−3 for the actor and critic respectively. For Q we included L2 weight decay of 10−2 and used a discount factor of γ = 0.99. For the soft target updates we used τ = 0.001. The neural networks used the rectiﬁed non-linearity (Glorot et al., 2011) for all hidden layers. The ﬁnal output layer of the actor was a tanh layer, to bound the actions. The low-dimensional networks had 2 hidden layers with 400 and 300 units respectively (≈ 130,000 parameters). Actions were not included until the 2nd hidden layer of Q. When learning from pixels we used 3 convolutional layers (no pooling) with 32 ﬁlters at each layer. This was followed by two fully connected layers with 200 units (≈ 430,000 parameters). The ﬁnal layer weights and biases of both the actor and critic were initialized from a uniform distribution [−3× 10−3, 3× 10−3] and [3× 10−4, 3× 10−4] for the low dimensional and pixel cases respectively. This was to ensure the initial outputs for the policy and value estimates were near zero. The other layers were initialized from uniform distributions [− 1√ f ] where f is the fan-in of the layer. The actions were not included until the fully-connected layers. We trained with minibatch sizes of 64 for the low dimensional problems and 16 on pixels. We used a replay buffer size of 106. For the exploration noise process we used temporally correlated noise in order to explore well in physical environments that have momentum. We used an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) with θ = 0.15 and σ = 0.2. The Ornstein-Uhlenbeck process models the velocity of a Brownian particle with friction, which results in temporally correlated values centered around 0.  f , 1√  8 PLANNING ALGORITHM  Our planner is implemented as a model-predictive controller (Tassa et al., 2012): at every time step we run a single iteration of trajectory optimization (using iLQG, (Todorov & Li, 2005)), starting from the true state of the system. Every single trajectory optimization is planned over a horizon between 250ms and 600ms, and this planning horizon recedes as the simulation of the world unfolds, as is the case in model-predictive control. The iLQG iteration begins with an initial rollout of the previous policy, which determines the nom- inal trajectory. We use repeated samples of simulated dynamics to approximate a linear expansion of the dynamics around every step of the trajectory, as well as a quadratic expansion of the cost function. We use this sequence of locally-linear-quadratic models to integrate the value function backwards in time along the nominal trajectory. This back-pass results in a putative modiﬁcation to the action sequence that will decrease the total cost. We perform a derivative-free line-search over this direction in the space of action sequences by integrating the dynamics forward (the forward- pass), and choose the best trajectory. We store this action sequence in order to warm-start the next iLQG iteration, and execute the ﬁrst action in the simulator. This results in a new state, which is used as the initial state in the next iteration of trajectory optimization.  9 ENVIRONMENT DETAILS  9.1 TORCS ENVIRONMENT  For the Torcs environment we used a reward function which provides a positive reward at each step for the velocity of the car projected along the track direction and a penalty of −1 for collisions. Episodes were terminated if progress was not made along the track after 500 frames.  11  Published as a conference paper at ICLR 2016  9.2 MUJOCO ENVIRONMENTS  For physical control tasks we used reward functions which provide feedback at every step. In all tasks, the reward contained a small action cost. For all tasks that have a static goal state (e.g. pendulum swingup and reaching) we provide a smoothly varying reward based on distance to a goal state, and in some cases an additional positive reward when within a small radius of the target state. For grasping and manipulation tasks we used a reward with a term which encourages movement towards the payload and a second component which encourages moving the payload to the target. In locomotion tasks we reward forward action and penalize hard impacts to encourage smooth rather than hopping gaits (Schulman et al., 2015b). In addition, we used a negative reward and early termination for falls which were determined by simple threshholds on the height and torso angle (in the case of walker2d). Table 2 states the dimensionality of the problems and below is a summary of all the physics envi- ronments.  task name blockworld1 blockworld3da canada canada2d cart cartpole cartpoleBalance cartpoleParallelDouble cartpoleParallelTriple cartpoleSerialDouble cartpoleSerialTriple cheetah ﬁxedReacher ﬁxedReacherDouble ﬁxedReacherSingle gripper gripperRandom hardCheetah hardCheetahNice hopper hyq hyqKick movingGripper movingGripperRandom pendulum reacher reacher3daFixedTarget reacher3daRandomTarget reacherDouble reacherObstacle reacherSingle walker2d  dim(s)  18 31 22 14 2 4 4 6 8 6 8 18 10 8 6 18 18 18 18 14 37 37 22 22 2 10 20 20 6 18 6 18  dim(a)  5 9 7 3 1 1 1 1 1 1 1 6 3 2 1 5 5 6 6 4 12 12 7 7 1 3 7 7 1 5 1 6  dim(o)  43 102 62 29 3 14 14 16 23 14 23 17 23 18 13 43 43 17 17 14 37 37 49 49 3 23 61 61 13 38 13 41  Table 2: Dimensionality of the MuJoCo tasks: the dimensionality of the underlying physics model dim(s), number of action dimensions dim(a) and observation dimensions dim(o).  task name  blockworld1  Brief Description  Agent is required to use an arm with gripper constrained to the 2D plane to grab a falling block and lift it against gravity to a ﬁxed target position.  12  Published as a conference paper at ICLR 2016  blockworld3da  canada  canada2d  cart  cartpole  cartpoleBalance  Agent is required to use a human-like arm with 7-DOF and a simple gripper to grab a block and lift it against gravity to a ﬁxed target posi- tion.  Agent is required to use a 7-DOF arm with hockey-stick like appendage to hit a ball to a target.  Agent is required to use an arm with hockey-stick like appendage to hit a ball initialzed to a random start location to a random target location.  Agent must move a simple mass to rest at 0. The mass begins each trial in random positions and with random velocities.  The classic cart-pole swing-up task. Agent must balance a pole at- tached to a cart by applying forces to the cart alone. The pole starts each episode hanging upside-down.  The classic cart-pole balance task. Agent must balance a pole attached to a cart by applying forces to the cart alone. The pole starts in the upright positions at the beginning of each episode.  cartpoleParallelDouble  Variant on the classic cart-pole. Two poles, both attached to the cart, should be kept upright as much as possible.  cartpoleSerialDouble  cartpoleSerialTriple  cheetah  Variant on the classic cart-pole. Two poles, one attached to the cart and the second attached to the end of the ﬁrst, should be kept upright as much as possible.  Variant on the classic cart-pole. Three poles, one attached to the cart, the second attached to the end of the ﬁrst, and the third attached to the end of the second, should be kept upright as much as possible.  The agent should move forward as quickly as possible with a cheetah- like body that is constrained to the plane. This environment is based very closely on the one introduced by Wawrzy´nski (2009); Wawrzy´nski & Tanwani (2013).  ﬁxedReacher  Agent is required to move a 3-DOF arm to a ﬁxed target position.  ﬁxedReacherDouble  Agent is required to move a 2-DOF arm to a ﬁxed target position.  ﬁxedReacherSingle  Agent is required to move a simple 1-DOF arm to a ﬁxed target position.  gripper  gripperRandom  hardCheetah  hopper  hyq  Agent must use an arm with gripper appendage to grasp an object and manuver the object to a ﬁxed target.  The same task as gripper except that the arm object and target posi- tion are initialized in random locations.  The agent should move forward as quickly as possible with a cheetah- like body that is constrained to the plane. This environment is based very closely on the one introduced by Wawrzy´nski (2009); Wawrzy´nski & Tanwani (2013), but has been made much more difﬁcult by removing the stabalizing joint stiffness from the model.  Agent must balance a multiple degree of freedom monoped to keep it from falling.  Agent is required to keep a quadroped model based on the hyq robot from falling.  13  Published as a conference paper at ICLR 2016  movingGripper  Agent must use an arm with gripper attached to a moveable platform to grasp an object and move it to a ﬁxed target.  movingGripperRandom  The same as the movingGripper environment except that the object po- sition, target position, and arm state are initialized randomly.  pendulum  The classic pendulum swing-up problem. The pendulum should be brought to the upright position and balanced. Torque limits prevent the agent from swinging the pendulum up directly.  reacher3daFixedTarget  Agent is required to move a 7-DOF human-like arm to a ﬁxed target position.  reacher3daRandomTarget Agent is required to move a 7-DOF human-like arm from random start-  ing locations to random target positions.  reacher  reacherSingle  reacherObstacle  walker2d  Agent is required to move a 3-DOF arm from random starting locations to random target positions.  Agent is required to move a simple 1-DOF arm from random starting locations to random target positions.  Agent is required to move a 5-DOF arm around an obstacle to a ran- domized target position.  Agent should move forward as quickly as possible with a bipedal walker constrained to the plane without falling down or pitching the torso too far forward or backward.  14  ",
1511.06644,2016,Recurrent Gaussian Processes,"['Recurrent Gaussian Processes\nCésar Lincoln Mattos', 'Zhenwen Dai', 'Andreas Damianou', 'Jeremy Forth', 'Guilherme Barreto', 'Neil Lawrence']",https://arxiv.org/pdf/1511.06644,"6 1 0 2     b e F 4 2         ]  G L . s c [      6 v 4 4 6 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  RECURRENT GAUSSIAN PROCESSES  C´esar Lincoln C. Mattos1, Zhenwen Dai2, Andreas Damianou3, Jeremy Forth4, Guilherme A. Barreto5 & Neil D. Lawrence6 1,5Federal University of Cear´a, Fortaleza, Cear´a, Brazil 2,3,6University of Shefﬁeld, Shefﬁeld, UK 1cesarlincoln@terra.com.br 2,3{z.dai,andreas.damianou}@sheffield.ac.uk 4jforth@iweng.org 5gbarreto@ufc.br 6N.Lawrence@dcs.sheffield.ac.uk  ABSTRACT  We deﬁne Recurrent Gaussian Processes (RGP) models, a general family of Bayesian nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential data. Similar to Recurrent Neural Networks (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully ex- ploit the Bayesian nature of the RGP model we develop the Recurrent Variational Bayes (REVARB) framework, which enables efﬁcient inference and strong reg- ularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recogni- tion models. We apply our model to the tasks of nonlinear system identiﬁcation and human motion modeling. The promising obtained results indicate that our RGP model maintains its highly ﬂexibility while being able to avoid overﬁtting and being applicable even when larger datasets are not available.  1  INTRODUCTION  The task of learning patterns from sequences is an ongoing challenge for the machine learning com- munity. Recurrent models are able to learn temporal patterns by creating internal memory represen- tations of the data dynamics. A general recurrent model, comprised of external inputs ui, observed outputs yi and hidden states xi, is given by  xi = f (xi−1, ui−1) + (cid:15)x i , yi = g(xi) + (cid:15)y i ,  (1) (2) where i is the instant of observation, f (·) and g(·) are unknown nonlinear functions respectively called transition and observation functions, (cid:15)x yI) are re- spectively Gaussian transition and observation noises, and I is the identity matrix. The recurrent nature of the model is expressed by the state variables xi, which are dependent on their past values, allowing past patterns to have inﬂuence in future outputs. In recurrent parametric models, such as Recurrent Neural Networks (RNN), both the transition and observation functions are modeled with weight matrices W , U, V and nonlinear element-wise activation functions φf (·) and φg(·):  i ∼ N ((cid:15)x  i ∼ N ((cid:15)y  xI) and (cid:15)y  i |0, σ2  i |0, σ2  xi = φf (W (cid:62)xi−1, U(cid:62)ui−1), yi = φg(V (cid:62)xi).  (3) (4)  As argued by Pascanu et al. (2013), the basic recurrent structure in Eq. 3 can be made deep, for example by adding multiple hidden layers comprised of multiple transition functions, where the output of each layer is used as the input of the next one.  1  Published as a conference paper at ICLR 2016  The difﬁculties related to learning dynamical structures from data (Bengio et al., 1994) have moti- vated the proposal of several RNN architectures in the literature, such as time-delay neural networks (Lang et al., 1990), hierarchical RNNs (El Hihi & Bengio, 1996), nonlinear autoregressive with exogenous inputs (NARX) neural networks (Lin et al., 1996), long short-term memory networks (Hochreiter & Schmidhuber, 1997), deep RNNs (Pascanu et al., 2013) and the RNN encoder-decoder (Cho et al., 2014). The usefulness of RNNs has been demonstrated in interesting applications, such as music generation (Boulanger-lewandowski et al., 2012), handwriting synthesis, (Graves, 2013) speech recognition (Graves et al., 2013), and machine translation (Cho et al., 2014). However, one well known limitation of parametric models, such as RNNs, is that they usually require large training datasets to avoid overﬁtting and generalization degradation. In contrast Bayesian nonparametric methods, such as Gaussian Processes (GP) models, often perform well with smaller datasets. In particular GP-based models are able to propagate uncertainty through their different structural components, something which ensures that when data is not present in a particular region of input space the predictions do not become over conﬁdent. The general recurrent Eqs. 1 and 2 have been widely studied in the control and dynamical sys- tem identiﬁcation community as either non-linear auto-regressive models with exogenous inputs (NARX) models or state-space models (SSM). Here we are particularly interested in the Bayesian approach to those models (Peterka, 1981). In this context, several GP-NARX models have been pro- posed in the literature (Murray-Smith et al., 1999; Solak et al., 2003; Kocijan et al., 2005). However, these models do not propagate the past states’ uncertainty through the transition function during the training or prediction phase. Girard et al. (2003); Damianou & Lawrence (2015) rectify this prob- lem. Nevertheless, in all of the above standard NARX approaches the autoregressive structure is performed directly with the observed outputs, which are noisy. A more general alternative to standard NARX models is the use of SSMs. Such structures have been explored recently by the GP community. Frigola et al. (2014) proposed a variational GP-SSM where both the transition and observation functions can have GP priors. Although they present results exclusively for the case where only the transition is modeled by a GP, while the observation has a parametric form. Conversely, Moreover, the inference required an additional smoothing step with, for example, a sequential Monte Carlo algorithm. Svensson et al. (2015) also consider a GP- SSM, but with a reduced-rank structure, and perform inference following a fully Bayesian approach, using a particle MCMC algorithm. All the aforementioned dynamic GP models contain recurrent structures. Each model makes a par- ticular choice for the deﬁnition of the states xi and the algorithm used to perform inference. Because all these GP models incorporate recurrent structures we refer to this general class of models as the Recurrent GP (RGP) family of methods. These are models such as in Eqs (1) and (2) which in- corporate GP priors for the transition and/or observation functions. Inspired by developments in the RNN community we propose a novel RGP model which introduces latent autoregression and is embedded in a new variational inference procedure named Recurrent Variational Bayes (REVARB). Our formulation aims to tackle some issues of past RGP structures. Our algorithm allows the RGP class of models to easily be extended to have deep structures, similar to deep RNNs. Furthermore, we develop an extension which combines the RGP and RNN technologies by reparameterizing the means of REVARB’s variational distributions through a new RNN-based recognition model. This idea results in simpler optimization and faster inference in larger datasets. Recently, Sohl-Dickstein & Kingma (2015) have detailed interesting similarities between the log- likelihood training of RNNs and the variational Bayes training objective in the context of generative models. In the present work we also follow a variational approach with the proposed REVARB framework, but with respect to RGP models. The rest of the paper is structured as follows. In Section 2 we brieﬂy summarize the standard GP for regression. In Section 3 we deﬁne the structure of our proposed RGP model. In Section 4 we describe the REVARB inference method. In Section 5 we present some experiments with REVARB in some challenging applications. We conclude the paper with hints to further work in Section 6.  2  Published as a conference paper at ICLR 2016  2 STANDARD GP MODEL FOR REGRESSION In the GP framework, a multiple input single output nonlinear function f (·) applied to a collection of N examples of D-dimensional inputs X ∈ RN×D is given a multivariate Gaussian prior:  (5) where a zero mean vector was considered, f ∈ RN and K ∈ RN×N , Kij = k(xi, xj), is the covariance matrix, obtained with a covariance (or kernel) function k(·,·), which must generate a semideﬁnite positive matrix K, for example the exponentiated quadratic kernel:  f = f (X) ∼ N (f|0, K),  (cid:34)  D(cid:88)  (cid:35)  k(xi, xj) = σ2  − 1 2  d(xid − xjd)2 w2  f exp D](cid:62) is comprised of the hyperparameters which characterize  d=1  ,  (6)  f , w2  1, . . . , w2  yI) relating the observations y and the  where the vector θ = [σ2 the covariance of the model. If we consider a Gaussian likelihood p(y|f ) = N (y|f , σ2 unknown values f, inference for a new output f∗, given a new input x∗, is calculated by:  p(f∗|y, X, x∗) = N(cid:0)f∗|k∗N (K + σ2  yI)−1y, k∗∗ − k∗N (K + σ2  yI)−1kN∗(cid:1) ,  (7) ∗N and k∗∗ = k(x∗, x∗). The predictive  where k∗N = [k(x∗, x1),··· , k(x∗, xN )], kN∗ = k(cid:62) y. distribution of y∗ is similar to the one in Eq. (7), but the variance is added by σ2 y and be determined The vector of hyperparameters θ can be extended to include the noise variance σ2 with the maximization of the marginal log-likelihood log p(y|X, θ) of the observed data, the so- called evidence of the model. The optimization is guided by the gradients of the evidence with respect to each component of the vector θ. It is worth mentioning that such optimization can be seen as the model selection step of obtaining a plausible GP model from the training data.  3 OUR RECURRENT GP MODEL  We follow an alternative SSM approach where the states have an autoregressive structure. Differ- ently from standard NARX models, the autoregression in our model is performed with latent (non- observed) variables. Thus, given L lag steps and introducing the notation ¯xi = [xi,··· , xi−L+1](cid:62) we have  xi = f (¯xi−1, ¯ui−1) + (cid:15)x i , yi = g(¯xi) + (cid:15)y i ,  (8) (9) where ¯ui−1 = [ui−1,··· , ui−Lu ](cid:62) and Lu is the number of past inputs used. Even if the output of the transition function in Eq. 8 is chosen to be 1-dimensional, it should be noticed that the actual hidden state ¯xi ∈ RL is multidimensional for L > 1. If we have H transition functions, each one comprising a hidden layer, it naturally gives rise to the deep structure  (cid:17)  (cid:17)  ,  + (cid:15)(H+1)  i  ,  ˆx(H+1)  i  f (h) ∼ N(cid:16) f (H+1) ∼ N(cid:16)  (cid:17)  (cid:17)  0, K(h)  f  ,  0, K(H+1)  f  1 ≤ h ≤ H  (10)  (11)  x(h)  ˆx(h)  + (cid:15)(h)  i  where we put GP priors with zero mean and covariance matrix K(h) f (·)(h), the noise in each layer is deﬁned as (cid:15)(h) variables and functions from distinct layers. We also introduce the notation  on the unknown functions h) and the upper index differentiates  f  (cid:105)(cid:62) (cid:105)(cid:62) ¯x(1) i−1, ¯ui−1 (cid:104) i−1, ¯x(h−1) ¯x(h) i x(H) i = i  i ∼ N (0, σ2 (cid:104)(cid:104) (cid:105) (cid:104)(cid:104) (cid:105) (cid:104) , [ui−1,··· , ui−Lu ] i−1,··· , x(1) x(1) i−L (cid:105)(cid:62) i−1,··· , x(h) x(h) , i−L ,··· , x(H)  x(h−1)  i−L+1  =  =  ,  i  , ,··· , x(h−1) i−L+1  (cid:105)(cid:62)  (cid:105)(cid:105)(cid:62)  ,  if h = 1, if 1 < h ≤ H,  if h = H + 1.  (12)  i  i = f (h)(cid:16) yi = f (H+1)(cid:16)   (cid:104) (cid:104)  ¯x(H)  ˆx(h)  i =  3  Published as a conference paper at ICLR 2016  The graphical model for the RGP is presented in Fig. 1, where we kept the general states ¯x(h) to make the recurrent connections more clear. It should be noted that the standard GP-NARX and GP-SSM are also RGPs, but with different states structure.  u  ¯x(1)  ¯x(2)  ···  ¯x(H)  y  Figure 1: RGP graphical model with H hidden layers.  Our RGP model, as deﬁned by Eqs. 10 and 11, can be seen as a special case of the Deep GP framework (Damianou & Lawrence, 2013; Damianou, 2015) where the priors of the latent variables in each hidden layer follow the autoregressive structure of Eq. 12. We emphasize that our model preserves the non-observed states of standard SSMs but avoids the ambiguities of generic multidimensional states by imposing a latent autoregressive structure. In the next section, we explain how this novel RGP model can be trained using the REVARB framework.  4 RECURRENT VARIATIONAL BAYES (REVARB)  z  (cid:17)  , where K(h)  Inference is intractable in our RGP model because we are not able to get analytical forms for the posterior of f (h) or the marginal likelihood of y. In order to tackle such intractabilities, we apply a novel variational approximation scheme named REVARB. REVARB is based on the variational sparse framework proposed by Titsias (2009), thus, we start by including to each layer h a number of M inducing points z(h) ∈ RM evaluated in M pseudo-  is the covariance matrix obtained from ζ(h). Considering a model with H hidden layers and 1-dimensional outputs, the joint distribution of all the variables is given by:  inputs ζ(h) ∈ RD such as that z(h) are extra samples of the GP that models f (h)(·) and p(cid:0)z(h)(cid:1) = z(h)(cid:12)(cid:12)(cid:12)0, K(h) N(cid:16) (cid:16) (cid:32) N(cid:89) (cid:32)H+1(cid:89)  x(h), f (h), z(h)(cid:111)(cid:12)(cid:12)(cid:12)H (cid:110) (cid:17) (cid:12)(cid:12)(cid:12)z(H+1), ˆx(H) (cid:12)(cid:12)(cid:12)f (H+1) (cid:17) (cid:16) (cid:16) (cid:17)(cid:33) z(h)(cid:17)(cid:33)(cid:32) L(cid:89) (cid:16) H(cid:89)  (cid:12)(cid:12)(cid:12)z(h), ˆx(h)  (cid:12)(cid:12)(cid:12)f (h)  y, f (H+1), z(H+1),  (cid:17) H(cid:89)  (cid:17)(cid:33)  f (H+1) i  f (h) i  x(h) i  (cid:16)  (cid:16)  (cid:17)  (cid:16)  (13)  i=L+1  h=1  h=1  yi  =  p  p  p  p  p  p  p  .  z  i  i  i  i  x(h) i  h=1  i=1  h=1  By applying Jensen’s inequality, similar to the standard variational approach, we can obtain a lower bound to the log-marginal likelihood log p(y) (Bishop, 2006):  (cid:90)  (cid:34)  p(cid:0)y, f (H+1), z(H+1),(cid:8)x(h), f (h), z(h)(cid:9)(cid:12)(cid:12)H  h=1  (cid:35)  (cid:1)  log p(y) ≥  Q log  f ,x,z  Q  where Q is the variational distribution. We choose the following factorized expression:  (cid:32) H(cid:89)  x(h)(cid:17)(cid:33)(cid:32)H+1(cid:89) (cid:16)  q  (cid:16)  z(h)(cid:17)(cid:33)(cid:32) N(cid:89)  Q =  q  H+1(cid:89)  (cid:16)  p  f (h) i  (cid:17)(cid:33)  (cid:12)(cid:12)(cid:12)z(h), ˆx(h)  i  h=1  h=1  i=L+1  h=1  4  ,  .  (14)  (15)  Published as a conference paper at ICLR 2016  Considering a mean-ﬁeld approximation, each term is given by  q  x(h)(cid:17) (cid:16) z(h)(cid:17) (cid:16) (cid:12)(cid:12)(cid:12)z(h), ˆx(h) (cid:17) (cid:17)−1  q  i  z(h)  i  i  ,  =  i=1  (cid:17)  x(h) i  , λ(h)  (cid:12)(cid:12)(cid:12)µ(h) N(cid:16) N(cid:89) z(h)(cid:12)(cid:12)(cid:12)m(h), Σ(h)(cid:17) = N(cid:16) (cid:12)(cid:12)(cid:12)(cid:104) (cid:105) (cid:105) (cid:104) = N(cid:16) f − K(h)  f = K(h)  and Σ(h)  a(h) f  f (h) i  Σ(h)  f z  ,  ,  f  i  (cid:16) (cid:16)  p  f (h) i  (16)  (17)  (18)  .  (cid:17)−1(cid:16)  (cid:17)(cid:62)  K(h) f z  (cid:17) (cid:16)  ii  ,  K(h)  z  , m(h) and Σ(h) are variational parameters, K(h)  is the standard kernel is the sparse kernel matrix calculated from the pseudo-inputs ζ(h)  f  K(h)  z  where a(h)  f = K(h) , λ(h)  f z  i  In the above, µ(h) matrix obtained from ˆx(h), K(h) and K(h)  f z = k(ˆx(h), ζ(h)) ∈ RN×M .  z  i  The variational distribution in Eq. 16 indicates that the latent variables x(h) are related to 2N variational parameters. In standard variational GP-SSM, such as in Frigola et al. (2014) we would have a total of 2N D parameters, for D-dimensional states, even for a diagonal covariance matrix in the posterior. Such reduction of parameters in the mean-ﬁeld approximation was enabled by the latent autoregressive structure of our model. Replacing the variational distribution in the Eq. 14 and working the expressions we are able to optimally eliminate the variational parameters m(h) and Σ(h), obtaining the ﬁnal form of the lower bound, presented in the included appendix. We have to compute some statistics that come up in the full bound:  (cid:18)(cid:68) (cid:68) (cid:28)(cid:16)  K(h) f z  K(h)  f  (cid:69)  (cid:69) (cid:17)(cid:62)  q(·)(h)  q(·)(h)  (cid:19) (cid:29)  Ψ(h)  0 = Tr  Ψ(h)  1 =  q(cid:0)x(1)(cid:1) , q(cid:0)x(h)(cid:1) q(cid:0)x(h−1)(cid:1) , q(cid:0)x(H)(cid:1) ,  ⇒ q(·)(h) =  if h = 1, if 1 < h ≤ H, if h = H + 1,  (19)  Ψ(h)  2 =  K(h) f z  where (cid:104)·(cid:105)q(x(h)) means expectation with respect to the distribution q(cid:0)x(h)(cid:1), which itself depends  K(h) f z  q(·)(h)  i  i  and λ(h)  only on the variational parameters µ(h) . All the expectations are tractable for our choice of the exponentiated quadratic covariance function and follow the same expressions presented by ?. The bound can be optimized with the help of analytical gradients with respect to the kernel and variational hyperparameters. The REVARB framework allows for a natural way to approximately propagate the uncertainty dur- ing both training and prediction. For testing, given a new sequence of external inputs, we can calculate the moments of the predictive distribution of each layer by recursively applying the results introduced in Girard et al. (2003), with predictive equations presented in the included appendix.  4.1 SEQUENTIAL RNN-BASED RECOGNITION MODEL  (cid:110)  (cid:111)  i = g(h)(cid:16)  (cid:17)  From Eq. (16) it is obvious that the number of variational parameters in REVARB grows linearly with the number of output samples. This renders optimization challenging in large N scenarios. ,∀h, i using RNNs. To alleviate this problem we propose to constrain the variational means More speciﬁcally, we have:  µ(h) i  µ(h)  ˆx(h) i−1  , where g(x) = V (cid:62)  φLN (WLN−1φLN−1(··· W2φ1(U1x))),  (20) W , U and V are parameter matrices, φ(·) denotes the hyperbolic tangent activation function and LN denotes the depth of the neural network. We refer to this RNN-based constraint as the sequen- tial recognition model. Such model directly captures the transition between the latent representation across time. This provides a constraint over the variational posterior distribution of the RGP that emphasizes free simulation. The recognition model’s inﬂuence is combined with that of the analytic  LN  5  Published as a conference paper at ICLR 2016  lower bound in the same objective optimization function. In this way, we no longer need to optimize the variational means but, instead, only the set of RNN weights, whose number does not increase linearly with N. Importantly, this framework also allows us to kick-off optimization by random initialization of the RNN weights, as opposed to more elaborate initialization schemes. The recog- nition model idea relates to the work of (Kingma & Welling, 2013; Rezende et al., 2014). In our case, however, the recognition model is sequential to agree with the latent structure and its purpose is distinct, because it acts as a constraint in an already analytic variational lower bound. Furthermore, our sequential recognition model acts upon a nonparametric Bayesian model.  5 EXPERIMENTS  In this section we evaluate the performance of our RGP model in the tasks of nonlinear system identiﬁcation and human motion modeling.  5.1 NONLINEAR SYSTEM IDENTIFICATION  We use one artiﬁcial benchmark, presented by Narendra & Li (1996), and two real datasets. The ﬁrst real dataset, named Actuator and described by Sj¨oberg et al. (1995) 1, consists of a hydraulic actuator that controls a robot arm, where the input is the size of the actuator’s valve opening and the output is its oil pressure. The second dataset, named Drives and introduced by Wigren (2010), is comprised by a system with two electric motors that drive a pulley using a ﬂexible belt. The input is the sum of voltages applied to the motors and the output is the speed of the belt. In the case of the artiﬁcial dataset we choose L = Lu = 5 and generate 300 samples for training and 300 samples for testing, using the same inputs described by Narendra & Li (1996). For the real datasets we use L = Lu = 10 and apply the ﬁrst half of the data for training and the second one for testing. The evaluation is done by calculating the root mean squared error (RMSE) of the free simulation on the test data. We emphasize that the predictions are made based only on the test inputs and past predictions. We compare our RGP model with 2 hidden layers, REVARB inference and 100 inducing inputs with two models commonly applied to system identiﬁcation tasks: standard GP-NARX and MLP-NARX. We use the MLP implementation from the MATLAB Neural Network Toolbox with 1 hidden layer. We also include experiments with the LSTM network, although the task itself probably does not re- quire long term dependences. The original LSTM architecture by Hochreiter & Schmidhuber (1997) was chosen, with a network depth of 1 to 3 layers and the number of cells at each layer selected to be up to 2048. LSTM memory length was unlimited, and sequence length was chosen initially to be a multiple of the longest duration memory present in the data generative process, and reduced gradually. During experiments with varying LSTM network conﬁgurations, it became clear that it was possible in most cases to obtain convergence on the training sets, using a carefully chosen net- work model size and hyperparameters. Training was organized around batches, and achieved using a learning rate selected to fall slightly below loop instability, and it was incrementally reduced when instability re-appeared. A batch in this context is the concatenation of ﬁxed length sub-sequences of the temporal data set. Neither gradient limits nor momentum were used. The results are summarized in Tab. 1 and the obtained simulations are illustrated in Fig. 2. The REVARB model was superior in all cases, with large improvements over GP-NARX. Although worse than REVARB, the MLP-NARX model presented good results, specially for the Actuator dataset. The higher RMSE values obtained by the LSTM model is possibly related to the difﬁculties we have encountered when trying to optimize its architecture for this given task.  Table 1: Summary of RMSE values for the free simulation results on system identiﬁcation test data.  Artiﬁcial Drive Actuator  MLP-NARX LSTM GP-NARX REVARB 0.4513 0.2491 0.3680  2.2438 0.4329 0.5170  1.9245 0.4128 1.5488  1.6334 0.4403 0.4621  1Available in the DaISy repository at http://www.iau.dtu.dk/nnbook/systems.html.  6  Published as a conference paper at ICLR 2016  (a) MLP-NARX - Artiﬁcial dataset. (b) MLP-NARX - Drives dataset. (c) MLP-NARX - Actuator dataset.  (d) LSTM - Artiﬁcial dataset.  (e) LSTM - Drives dataset.  (f) LSTM - Actuator dataset.  (g) GP-NARX - Artiﬁcial dataset.  (h) GP-NARX - Drives dataset.  (i) GP-NARX - Actuator dataset.  (j) REVARB - Artiﬁcial dataset.  (k) REVARB - Drives dataset.  (l) REVARB - Actuator dataset.  Figure 2: Free simulation on system identiﬁcation test data.  5.2 HUMAN MOTION MODELING  The motion capture data from the CMU database2 was used to model walking and running motions. Training was performed with the trajectories 1 to 4 (walking) and 17 to 20 (running) from subject 35. The test set is comprised by the trajectories 5 to 8 (walking) and 21 to 24 (running) from the  2Available at http://mocap.cs.cmu.edu/.  7  Published as a conference paper at ICLR 2016  Figure 3: The generated motion with a step function signal, starting with walking (blue), switching to running (red) and switching back to walking (blue).  same subject. The original dataset contains 59 outputs, but 2 are constant, so we remove those and use the remaining 57. In order to perform free simulation in the test set, we include a control input given by the y coordinate of the left toes. Following the previous system identiﬁcation experiments, predictions are made based only on such control input and previous predictions. We normalize the inputs and outputs with zero mean and unitary standard deviation. We evaluate a 2 hidden layer REVARB with 200 inducing inputs, the standard GP-NARX model and a 1 hidden layer MLP with 1000 hidden units. The orders are ﬁxed at L = Lu = 20. Note that the data related to both walking and running is used in the same training step. The latent autoregressive structure of REVARB allow us to train a single model for all outputs. In the case of GP-NARX, we had to train separate models for each output, since training a single model with 57× 20 + 20 = 1160 dimensional regressor vector was not feasible. The mean of the test RMSE values are summarized in Tab. 2. The REVARB model obtained better results than both the other models. We emphasize that REVARB has an additional advantage over GP-NARX because its latent autoregressive structure allows the training of a single mode for all the outputs.  Table 2: Summary of RMSE values for the free simulation results on human motion test data.  MLP-NARX GP-NARX REVARB 0.8600  0.8987  1.2141  5.3 AVATAR CONTROL  We demonstrate the capability of RGP by applying it to synthesize human motions with simple control signals such as the velocity. Such system ideally can be used to generate realistic human motion according to human instruction in virtual environment such as video games. We use the 5 walking and 5 running sequences from CMU motion database and take the average velocity as the control signal. We train a 1 hidden layer REVARB model with the RNN sequential recognition model (two hidden layer 500-200 units). After training, we use the model to synthesize motions with unseen control signals. Figure 3 shows the frames of the generated motion with a step function signal (the training sequences do not contain any switch of motions). The video of this and some more motions are available at https://youtu.be/FuF-uZ83VMw, https://youtu.be/ FR-oeGxV6yY, https://youtu.be/AT0HMtoPgjc.  6 DISCUSSION AND FURTHER WORK  We deﬁned the broad family of Recurrent Gaussian Processes models, which, similarly to RNNs, are able to learn, possibly deep, temporal representations from data. We also proposed a novel RGP model with a latent autoregressive structure where the intractabilities brought by the recurrent GP priors are tackled with a variational approximation approach, resulting in the REVARB framework.  8  Published as a conference paper at ICLR 2016  Furthermore, we extended REVARB with a sequential RNN-based recognition model that simpliﬁes the optimization. We applied REVARB to the tasks of nonlinear system identiﬁcation and human motion modeling. The good results obtained by our model indicate that the latent autoregressive structure and our variational approach were able to better capture the dynamical behavior of the data. In the work Turner & Sahani (2008), the authors present some concerns with respect to the use of mean-ﬁeld approximations within a time-series context, suggesting that such approximation has a hard time propagating uncertainty through time. However, we observed in practice that our proposed REVARB framework is able to better account for uncertainty in the latent space with its autoregres- sive deep structure. This may be because the next layer is able to ‘compensate’ the mean-ﬁeld assumption of the previous layer, accounting for additional (temporal) correlations. Since each la- tent variable xi and, thus, its associated variational parameters, is present in two layers (see Eq. 12), this effect is enabled for all latent variables of the model. A similar observation is made for regular deep GPs by Damianou (2015). The ﬂexibility of GP modeling along with expressive recurrent structures is a theme for further theoretical investigations and practical applications. For instance, we intend to verify if some of the recommendations for deep modeling described by Duvenaud et al. (2014) would be helpful for our RGP model. Finally, we hope that our paper opens up new directions in the study of the parallels between RGPs and RNNs. To this end, we intend to explore the REVARB approach within longer term memory tasks and extend it with non-Gaussian likelihood distributions. Acknowledgements. The authors thank the ﬁnancial support of CAPES, FUNCAP, NUTEC, CNPq (Maresia, grant 309451/2015-9, and Amalia, grant 407185/2013-5), RADIANT (EU FP7-HEALTH Project Ref 305626) and WYSIWYD (EU FP7-ICT Project Ref 612139).  REFERENCES Bengio, Yoshua, Simard, Patrice, and Frasconi, Paolo. Learning long-term dependencies with gradient descent  is difﬁcult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.  Bishop, Christopher M. Pattern recognition and machine learning. Springer, 2006.  Boulanger-lewandowski, Nicolas, Bengio, Yoshua, and Vincent, Pascal. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pp. 1159–1166, 2012.  Cho, Kyunghyun, Van Merri¨enboer, Bart, Gulcehre, Caglar, Bahdanau, Dzmitry, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using rnn encoder-decoder for statistical ma- chine translation. arXiv preprint arXiv:1406.1078, 2014.  Damianou, Andreas. Deep Gaussian processes and variational propagation of uncertainty. PhD Thesis, Uni-  versity of Shefﬁeld, 2015.  Damianou, Andreas and Lawrence, Neil. Deep Gaussian processes. In Proceedings of the Sixteenth Interna-  tional Conference on Artiﬁcial Intelligence and Statistics, pp. 207–215, 2013.  Damianou, Andreas and Lawrence, Neil. Semi-described and semi-supervised learning with Gaussian pro-  cesses. In 31st Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2015.  Duvenaud, David, Rippel, Oren, Adams, Ryan P, and Ghahramani, Zoubin. Avoiding pathologies in very deep  networks. arXiv preprint arXiv:1402.5836, 2014.  El Hihi, Salah and Bengio, Yoshua. Hierarchical recurrent neural networks for long-term dependencies. In  Advances in Neural Information Processing Systems, pp. 493–499, 1996.  Frigola, Roger, Chen, Yutian, and Rasmussen, Carl. Variational Gaussian process state-space models.  Advances in Neural Information Processing Systems 27 (NIPS), pp. 3680–3688, 2014.  In  Girard, A., Rasmussen, CE., Qui˜nonero-Candela, J., and Murray-Smith, R. Multiple-step ahead prediction for non linear dynamic systems: A gaussian process treatment with propagation of the uncertainty. In Advances in Neural Information Processing Systems 15, pp. 529–536. MIT Press, Cambridge, MA, USA, 2003.  9  Published as a conference paper at ICLR 2016  Graves, Alan, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 6645–6649. IEEE, 2013.  Graves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):1735–1780,  1997.  King, Nathaniel J and Lawrence, Neil D. Fast variational inference for gaussian process models through kl-  correction. In Machine Learning: ECML 2006, pp. 270–281. Springer, 2006.  Kingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. In ICLR, 2013.  Kocijan, Juˇs, Girard, Agathe, Banko, Blaˇz, and Murray-Smith, Roderick. Dynamic systems identiﬁcation with  Gaussian processes. Math Comp Model Dyn, 11(4):411–424, 2005.  Lang, Kevin J, Waibel, Alex H, and Hinton, Geoffrey E. A time-delay neural network architecture for isolated  word recognition. Neural networks, 3(1):23–43, 1990.  Lin, Tsungnam, Horne, Bil G, Tiˇno, Peter, and Giles, C Lee. Learning long-term dependencies in narx recurrent  neural networks. Neural Networks, IEEE Transactions on, 7(6):1329–1338, 1996.  Murray-Smith, Roderick, Johansen, Tor A, and Shorten, Robert. On transient dynamics, off-equilibrium be- haviour and identiﬁcation in blended multiple model structures. In European Control Conference (ECC’99), Karlsruhe, BA-14. Springer, 1999.  Narendra, Kumpati S and Li, Sai-Ming. Neural networks in control systems. Mathematical Perspectives on  Neural Networks, pp. 347–394, 1996.  Pascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, and Bengio, Yoshua. How to construct deep recurrent  neural networks. arXiv preprint arXiv:1312.6026, 2013.  Peterka, V. Bayesian approach to system identiﬁcation. Trends and Prog in Syst ident, 1:239–304, 1981.  Rezende, D J, Mohamed, S, and Wierstra, D. Stochastic backpropagation and approximate inference in deep  generative models. In International Conference on Machine Learning, 2014.  Sj¨oberg, Jonas, Zhang, Qinghua, Ljung, Lennart, Benveniste, Albert, Delyon, Bernard, Glorennec, Pierre-Yves, Hjalmarsson, H˚akan, and Juditsky, Anatoli. Nonlinear black-box modeling in system identiﬁcation: a uniﬁed overview. Automatica, 31(12):1691–1724, 1995.  Sohl-Dickstein, Jascha and Kingma, Diederik P. Technical note on equivalence between recurrent neural net-  work time series models and variational bayesian models. arXiv preprint arXiv:1504.08025, 2015.  Solak, Ercan, Murray-Smith, Roderick, Leithead, William E, Leith, Douglas J, and Rasmussen, Carl Edward. Derivative observations in Gaussian process models of dynamic systems. Advances in Neural Information Processing Systems, 16, 2003.  Svensson, Andreas, Solin, Arno, S¨arkk¨a, Simo, and Sch¨on, Thomas B. Computationally efﬁcient bayesian  learning of gaussian process state space models. arXiv preprint arXiv:1506.02267, 2015.  Titsias, Michalis K. Variational learning of inducing variables in sparse gaussian processes. In International  Conference on Artiﬁcial Intelligence and Statistics, pp. 567–574, 2009.  Turner, Richard E and Sahani, Maneesh. Two problems with variational expectation maximisation for time- series models. In Proceedings of the Workshop on Inference and Estimation in Probabilistic Time-Series Models, pp. 107–115, 2008.  Wigren, Torbj¨orn.  Input-output data sets for development and benchmarking in nonlinear  iden- Information Technology, 20:2010–020, Dataset available in http://www.it.uu.se/research/publications/reports/  tiﬁcation. 2010. 2010-020/NonlinearData.zip as DATAPRBS.mat, with input u1 and output z1.  from the department of  Technical Reports  10  Published as a conference paper at ICLR 2016  APPENDIX  REVARB LOWER BOUND  Replacing the deﬁnition of the joint distribution (Eq. 13) and the factorized variational distribution Q (Eq. 15) in the Jensen’s inequality of Eq. 14, we are able to cancel the terms p inside the log:  f (h) i  i  i=L+1  log p(y) ≥ N(cid:88) N(cid:88) (cid:90) − H+1(cid:88) − N(cid:88)  i=L+1  h=1  +  q  f ,x,z  (cid:16)  (cid:90) (cid:90) H(cid:88) (cid:16) z(h)(cid:17) (cid:90) (cid:16) H(cid:88)  f ,x,z  h=1  q  z  p  q  q  x(H)(cid:17) (cid:32) H(cid:89) (cid:16) (cid:17)  h(cid:48)=1  log q  z(H+1)(cid:17) (cid:16) x(h(cid:48))(cid:17)(cid:33) (cid:16) z(h)(cid:17) H+1(cid:88) (cid:17) (cid:16)  h=1  +  q  i  f (H+1) i  (cid:12)(cid:12)(cid:12)z(H+1), ˆx(H) (cid:17) (cid:16) (cid:12)(cid:12)(cid:12)z(h), ˆx(h) z(h)(cid:17) (cid:16) (cid:16) (cid:90) z(h)(cid:17) (cid:16) (cid:16) z(h)(cid:17) (cid:90) (cid:17) L(cid:88) H(cid:88)  f (h) i  (cid:16)  log p  p  q  z  i  q  x(h) i  log q  x(h) i  +  q  x(h) i  log p  (cid:17)  i=L+1  h=1  x  i=1  h=1  x  (cid:16) (cid:16)  (cid:17)  (cid:12)(cid:12)(cid:12)z(h), ˆx(h) (cid:12)(cid:12)(cid:12)f (H+1) (cid:17) (cid:12)(cid:12)(cid:12)f (h) (cid:16)  x(h) i  (cid:17)  i  i  log p  yi  log p  (cid:16)  (cid:17)  ,  x(h) i  (21)  i  give rise to the statistics Ψ(h)  where the integrals are tractable, since all the distributions are Gaussians. The expectations with respect to x(h) Following similar argument of King & Lawrence (2006), we are able to optimally eliminate the variational parameters associated with the inducing points, m(h) and Σ(h) and get to the ﬁnal form of the REVARB lower bound:  2 , deﬁned in Eq. 19.  0 , Ψ(h)  and Ψ(h)  1  log p(y) ≥ − N − L  log 2πσ2  (cid:18)  z  1  2  h=1  H+1(cid:88) (cid:12)(cid:12)(cid:12)K(H+1) (cid:12)(cid:12)(cid:12) − log (cid:32) N(cid:88) (cid:40) H+1)2 y(cid:62)Ψ(H+1) − 1 (cid:12)(cid:12)(cid:12)K(h) (cid:12)(cid:12)(cid:12) − 1 2σ2 h µ(h)(cid:17)(cid:62) (cid:16) (cid:90) (cid:16)  Ψ(h)  (cid:17)  i=L+1  log  2  1  z  q  x(h) i  +  +  +  +  1 2  log  1 2(σ2  H(cid:88)  h=1 1 2  log  +  1 2(σ2 h)2  − N(cid:88)  i=L+1  x(h)  i  z  z  +  +  1 2  H+1  K(H+1)  (cid:12)(cid:12)(cid:12)(cid:12)K(H+1) µ(h)(cid:17)(cid:62) (cid:16)  h − 1 2σ2 (cid:18) (cid:12)(cid:12)(cid:12)(cid:12)K(h) (cid:18) (cid:16)  λ(h) i +  1 σ2 h  1 σ2 h  z +  z +  K(h)  (cid:17)  2  x(h) i  +  log q  Ψ(h)  (cid:18)(cid:16)  (cid:17)−1  K(H+1)  z  Ψ(H+1)  2  (cid:19)(cid:19)  (cid:17)(cid:62) (cid:17)−1  y  Ψ(H+1)  K(h)  z  (cid:19)(cid:33)  Ψ(h)  2  y(cid:62)y + Ψ(H+1)  0  1 σ2  H+1  Ψ(H+1)  2  1 σ2  H+1  Ψ(H+1)  2  − Tr  (cid:12)(cid:12)(cid:12)(cid:12) (cid:19)−1(cid:16) (cid:18)(cid:16)  1  Ψ(h)  µ(h) + Ψ(h)  (cid:12)(cid:12)(cid:12)(cid:12) L(cid:88)  2  i=1  (cid:16)  0 − Tr (cid:17)(cid:62) (cid:17)  Ψ(h)  1  x(h) i  (cid:19)−1(cid:16) (cid:90)  q  (cid:16) = N(cid:16)  x(h)  i  (cid:17)  (cid:17)(cid:41)  .  x(h) i  (cid:17)  log p  µ(h)  (cid:16) (cid:12)(cid:12)(cid:12)µ(h)  (22)  Note that the parameters of the Gaussian priors p variables x(h) ters.  of the initial latent |L i=1 can be optimized along with the variational parameters and kernel hyperparame-  0i , λ(h)  x(h) i  x(h) i  0i  i  11  Published as a conference paper at ICLR 2016  REVARB PREDICTIVE EQUATIONS  Predictions in the REVARB framework are done iteratively, with approximate uncertainty propaga- tion between each layer:  (cid:16) µ(h)∗ = E(cid:110) (cid:16) λ(h)∗ = V(cid:110) (cid:18)(cid:18)(cid:16)  p  p  − Tr  f (h)∗  f (h)∗  =  (cid:12)(cid:12)(cid:12)ˆx(h)∗ (cid:17)(cid:111) (cid:12)(cid:12)(cid:12)ˆx(h)∗ (cid:17)(cid:111) (cid:17)−1 −(cid:16)  =  K(h)  z  h Ψ(h)  2  Ψ(h) 1∗  Ψ(h)  K(h)  B(h)(cid:17)(cid:62)(cid:16) (cid:16) B(h)(cid:17)(cid:62)(cid:18) (cid:16) z + σ−2 (cid:16) (cid:16) (cid:17)  K(H+1)  (cid:17)  z  = N(cid:16)  Ψ(h) 1∗  ,  ,  Ψ(h) 2∗  Ψ(h) 1∗  (cid:17)(cid:62) (cid:17)(cid:62) 2∗ −(cid:16) (cid:17)−1(cid:19) (cid:19) (cid:16) (cid:17)−1(cid:16) (cid:12)(cid:12)(cid:12)µ(h)∗ , λ(h)∗ (cid:17) ˆx(h)∗ , ζ(h)(cid:17)  H+1Ψ(H+1)  z + σ−2  K(h)  x(h)∗  (cid:16)  h  .  2  (cid:19)  B(h) + Ψ(h) 0∗  (23)  (24)  (cid:17)−1(cid:16) (cid:17)(cid:62)  (cid:17)(cid:62)  (cid:16)  (cid:17)  h Ψ(h)  2  Ψ(h)  1  µ(h),  Ψ(H+1)  1  y. The terms  is deﬁned similar to the Eq. 12, B(h) = σ−2 + σ−2  where ˆx(h)∗ for 1 ≤ h ≤ H, and B(H+1) = σ−2 0∗ , Ψ(h) Ψ(h)  1∗ and Ψ(h)  H+1  use the new Gaussian approximation q respectively by K(h)∗ = k ˆx(h)∗ , ˆx(h)∗  x(h)∗ and K(h)∗z = k  (cid:16)  2∗ are computed as in the Eq. 19, but instead of the distributions q and replace K(h)  x(h) we i and K(h) f z  f  12  ",
1411.7676,2016,Modeling Visual Representations:Defining Properties and Deep Approximations,"['Modeling Visual Representations:Defining Properties and Deep Approximations\nStefano Soatto', 'Alessandro Chiuso']",https://arxiv.org/pdf/1411.7676,"6 1 0 2     b e F 9 2         ]  V C . s c [      9 v 6 7 6 7  .  1 1 4 1 : v i X r a  Published as a conference paper at ICLR 2016 VISUAL REPRESENTATIONS: DEFINING PROPERTIES AND DEEP APPROXIMATIONS  Stefano Soatto Department of Computer Science University of California, Los Angeles Los Angeles, CA 90095, USA soatto@ucla.edu  Alessandro Chiuso Dipartimento di Ingegneria dell’Informazione Universit`a di Padova Via Gradenigo 6/b, 35131 Padova, Italy alessandro.chiuso@unipd.it  ABSTRACT  Visual representations are deﬁned in terms of minimal sufﬁcient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufﬁciency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guar- antees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and ap- proximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization.  1  INTRODUCTION  A visual representation is a function of visual data (images) that is “useful” to accomplish visual tasks. Visual tasks are decision or control actions concerning the surrounding environment, or scene, and its properties. Such properties can be geometric (shape, pose), photometric (reﬂectance), dy- namic (motion) and semantic (identities, relations of “objects” within). In addition to such prop- erties, the data also depends on a variety of nuisance variables that are irrelevant to the task. De- pending on the task, they may include unknown characteristics of the sensor (intrinsic calibration), its inter-play with the scene (viewpoint, partial occlusion), and properties of the scene that are not directly of interest (e.g., illumination). We are interested in modeling and analyzing visual representations: How can we measure how “useful” one is? What guidelines or principles should inform its design? Is there such a thing as an optimal representation? If so, can it be computed? Approximated? Learned? We abstract (classes of) visual tasks to questions about the scene. They could be countable semantic queries (e.g., concerning “objects” in the scene and their relations) or continuous control actions (e.g., “in which direction to move next”). In Sect. 2.1 we formalize these questions using well-known concepts, and in Sect. 2.3 we derive an equivalent characterization that will be the starting point for designing, analyzing and learning representations.  1.1 RELATED WORK AND CONTRIBUTIONS  Much of Computer Vision is about computing functions of images that are “useful” for visual tasks. When restricted to subsets of images, local descriptors typically involve statistics of image gradi- ents, computed at various scales and locations, pooled over spatial regions, variously normalized and  1  Published as a conference paper at ICLR 2016  quantized. This process is repeated hierarchically in a convolutional neural network (CNN), with weights inferred from data, leading to representation learning Ranzato et al. (2007); LeCun (2012); Simonyan et al. (2014); Serre et al. (2007); Bouvrie et al. (2009); Susskind et al. (2011); Bengio (2009). There are more methods than we can review here, and empirical comparisons (e.g., Mikola- jczyk et al. (2004)) have recently expanded to include CNNs. Unfortunately, many implementation details and parameters make it hard to draw general conclusions Chatﬁeld et al. (2011). We take a different approach, and derive a formal expression for optimal representations from established principles of sufﬁciency, minimality, invariance. We show how existing descriptors are related to such representations, highlighting the (often tacit) underlying assumptions. Our work relates most closely to Bruna & Mallat (2011); Anselmi et al. (2015) in its aim to construct and analyze representations for classiﬁcation tasks. However, we wish to represent the scene, rather than the image, so occlusion and locality play a key role, as we discuss in Sect. 5. Also Morel & Yu (2011) characterize the invariants to certain nuisance transformations: Our models are more general, involving both the range and the domain of the data, although more restrictive than Tishby et al. (2000) and speciﬁc to visual data. We present an alternate interpretation of pooling, in the context of classical sampling theory, that differs from other analyses Gong et al. (2014); Boureau et al. (2010). Our contributions are to (i) deﬁne minimal sufﬁcient invariant representations and characterize them explicitly (Claim 1); (ii) show that local descriptors currently in use in Computer Vision can approx- imate such representations under very restrictive conditions (Claim 2 and Sec. 3.3); (iii) compute in closed form the minimal sufﬁcient contrast invariant (14) and show how local descriptors relate to it (Rem. 2); show that such local descriptors can be implemented via linear convolutions and rectiﬁed linear units (Sect. 4.3); (iv) explain the practice of “joint normalization” (Rem. 5) and “clamping” (Sect. 3.3.1) as procedures to approximate the sufﬁcient invariant; these practices are seldom ex- plained and yet they have a signiﬁcant impact on performance in empirical tests Kirchner (2015); (v) explain “spatial pooling” in terms of anti-aliasing, or local marginalization with respect to a small-dimensional nuisance group, in convolutional architectures (Sec. 4.1-4.2). In the Appendix we show that an ideal representation, if generatively trained, maximizes the information content of the representation (App. A).  2 CHARACTERIZATION AND PROPERTIES OF REPRESENTATIONS  Because of uncertainty in the mechanisms that generate images, we treat them as realizations of random vectors x (past/training set) and y (future/test set), of high but ﬁnite dimension. The scene they portray is inﬁnitely more complex.1 Even if we restrict it to be a static “model” or “parameter” θ, it is in general inﬁnite-dimensional. Nevertheless, we can ask questions about it. The number of questions (“classes”) K is large but ﬁnite, corresponding to a partition of the space of scenes, represented by samples {θ1, . . . , θK}. A simple model of image formation including scaling and occlusion Dong & Soatto (2014) is known as the Lambert-Ambient, or LA, model.  2.1 DESIDERATA  Among (measurable) functions of past data (statistics), we seek those useful for a class of tasks. Abstracting the task to questions about the scene, “useful” can be measured by uncertainty reduction on the answers, captured by the mutual information between the data x and the object of interest θ. While a representation can be no more informative than the data,2 ideally it should be no less, i.e., a sufﬁcient statistic. It should also be “simpler” than the data itself, ideally minimal. It should also discount the effects of nuisance variables g ∈ G, and ideally be invariant to them. We use a = {x1, . . . , xt}. superscript to denote a collection of t data points (the history up to t, if ordered), xt .  Thus, a representation is any function φ constructed using past data xt that is useful to answer questions about the scene θ given future data y it generates, regardless of nuisance factors g.  1Scenes are made of surfaces supporting reﬂectance functions that interact with illumination, etc. No matter  how many images we already have, even a single static scene can generate inﬁnitely many different ones.  2Data Processing Inequality, page 88 of Shao (1998).  2  Published as a conference paper at ICLR 2016  An optimal representation is a minimal sufﬁcient statistic for a task that is invariant to nuisance factors. In Sec. 2.3 we introduce the SA Likelihood, an optimal representation, and in subsequent sections show how it can be approximated.  2.2 BACKGROUND  The data X is a random variable with samples x, y; the model θ is unknown in the experiment E = {x, θ, pθ(x)} where pθ(x) is the probability density function of X, that depends on the parameter θ, evaluated at the sample x; a statistic T is a function of the sample; it is sufﬁcient (of x for θ) if X | T = τ does not depend on θ;3 it is minimal if it is a function of all other sufﬁcient statistics4. If θ is treated as a random variable and a prior is available, φ is Bayesian sufﬁcient5 if p(θ|φ(xt)) = p(θ|xt). If T is minimal, any smaller6 U entails “information loss.” If θ was a discrete random variable, the information content of T could be measured by uncertainty reduction: H(θ) − H(θ|T (X)), which is the mutual information7 between θ and T and H denotes entropy Cover & Thomas (1991); fur- thermore, T (X) ∈ arg inf φ H(θ|φ(X)), where the inﬁmum is with respect to measurable functions and is in general not unique. Consider a set G of transformations g of the data x, g(x), which we denote simply as gx. A function φG (x) is G-invariant if φG(gx) = φG (x) for all g ∈ G. The sensitivity of φ to G is S = (cid:107) ∂φ(gx) ∂g (cid:107) where φ is assumed to be differentiable. By deﬁnition, an invariant has zero sensitivity to G. . The Likelihood function is L(θ; x) = pθ(x), understood as a function of θ for a ﬁxed sample x, sometimes written as p(x|θ) even though θ is not a random variable. Theorem 3.2 of Pawitan (2001) can be extended to an inﬁnite-dimensional parameter θ (Theorem 6.1 of Bahadur (1954)): Theorem 1 (The likelihood function as a statistic). The likelihood function L(θ; x) is a minimal sufﬁcient statistic of x for θ.  2.3 NUISANCE MANAGEMENT: PROFILE, MARGINAL, AND SAL LIKELIHOODS A nuisance g ∈ G is an unknown “factor” (a random variable, parameter, or transformation) that is not of interest and yet it affects the data. Given pθ(·), when g is treated as a parameter that . = pθ(gy); when it is treated as a random variable, transforms the data via g(y) pθ(y|g)  . = pθ(gy). The proﬁle likelihood  . = gy, then pθ,g(y)  where the nuisance has been “maxed-out” is G-invariant. The marginal likelihood  pθ,G(y)  pθ,g(y)  . = sup g∈G  (cid:90)  pθ(y|G)  . =  pθ(y|g)dP (g)  (1)  (2)  G  is invariant only if dP (g) = dµ(g) is the constant8 measure on G. Both are sufﬁcient invariants, in the sense that they are invariant to G and are minimal sufﬁcient. This counters the common belief that “invariance trades off selectivity.” In Rem. 1 we argue that both can be achieved, at the price of complexity. Computing the proﬁle likelihood in practice requires reducing G to a countable set {g1, . . . , gN} of samples,9 usually at a loss. The tradeoff is a subject of sampling theory, where samples can  Blackwell & Ramamoorthi (1982).  3Deﬁnition 3.1 of Pawitan (2001) or Sec. 6.7 of DeGroot (1989), page 356 4If U is sufﬁcient, then the sigma algebra σ(T ) ⊂ σ(U ), DeGroot (1989), page 368. 5The two are equivalent for discrete random variables, but pathological cases can arise in inﬁnite dimensions 6In the sense of inclusion of sigma algebras, σ(U ) ⊂ σ(T ). 7See Cover & Thomas (1991) eq. 2.28, page 18. 8Base, or Haar measure if G is a group. It can be improper if G is not compact. 9Note that N can be inﬁnite if G is not compact.  3  (cid:90)  Published as a conference paper at ICLR 2016  be generated regularly, independent of the signal being sampled, or adaptively.10 In either case, the occurrence of spurious extrema (“aliases”) can be mitigated by retaining not the value of the function at the samples, pθ,gi(y), but an anti-aliased version consisting of a weighted average around the samples:  (cid:90)  ˆpθ,gi(y)  . =  pθ,gi(gy)w(g)dµ(g)  (3)  for suitable weights w.11 When the prior dP (g) = w(g)dµ(g) is positive and normalized, the previous equation (anti-aliasing) can be interpreted as local marginalization, and is often referred to as mean-pooling. The approximation of the proﬁle likelihood obtained by sampling and anti- aliasing, is called the SA (sampled anti-aliased) likelihood, or SAL:  ˆpθ,G(y) = max  i  ˆpθ,gi(y) = max  i  pθ,gi(gy)dP (g).  (4)  The maximization over the samples in the above equation is often referred to as max-pooling. Claim 1 (The SAL is an optimal representation). Let the joint likelihood pθ,g be smooth with respect to the base measure on G. For any approximation error (cid:15), there exists an integer N = N ((cid:15)) number of samples {gi}N i=1 and a suitable (regular or adaptive) sampling mechanism so that the SAL maxi ˆpθ,gi approximates to within (cid:15) the proﬁle likelihood supg∈G pθ,g, after normalization, in the sense of distributions.  For the case of (conditionally) Gaussian models under the action of the translation group, the claim follows from classical sampling arguments. More generally, an optimal representation is difﬁcult to compute. In the next section, we show a ﬁrst example when this can be done. Remark 1 (Invariance, sensitivity and “selectivity”). It is commonly believed that invariance comes at the cost of discriminative power. This is partly due to the use of the term invariance (or “ap- proximate invariance” or “stability”) to denote sensitivity, and of the term “selectivity” to denote maximal invariance. A function is insensitive to g if small changes in g produce small changes in its value. It is invariant to g if it is constant as a function of g. It is a maximal invariant if equal value implies equivalence up to G (Sect. 4.2 of Shao (1998), page 213). It is common to decrease sensitiv- ity with respect to a transformation g by averaging the function with respect to g, a lossy operation in general. For instance, if the function is the image itself, it can be made insensitive to rotation about its center by averaging rotated versions of it. The result is an image consisting of concentric circles, with much of the informative content of the original image gone, in the sense that it is not possible to reconstruct the original image. Nevertheless, while invertibility is relevant for reconstruction tasks, it is not necessarily relevant for classiﬁcation, so it is possible for the averaging operation to yield a sufﬁcient invariant, albeit not a maximal one.  Thus, one can have invariance while retaining sufﬁciency, albeit generally not maximality: The proﬁle likelihood, or the marginal likelihood with respect to the uniform measure, are sufﬁcient statistics and are (strictly) invariant. The price to have both is complexity, as both are inﬁnite- dimensional in general. However, they can be approximated, which is done by sampling in the SA likelihood.  2.4 A FIRST EXAMPLE: LOCAL REPRESENTATIONS/DESCRIPTORS  Let the task be to decide whether a (future) image y is of a scene θ given a single training image x of it, which we can then assume to be the scene itself: x = θ. Nuisances affecting y are limited  10{gi}N  i=1 are generated by a (deterministic or stochastic) mechanism ψ that depends on the data and respects the structure of G. If G is a group, this is known as a co-variant detector: It is a function ψ that (is Morse i=1 = {g | ∇Gψ(y, g) = 0} that equivary: ∇Gψ(˜gy, gi) = 0 ⇒ in g, i.e., it) has isolated extrema {gi(y)}N ∇Gψ(y, ˜ggi) = 0 for all i and ˜g ∈ G. The samples {gi}N i=1 deﬁne a reference frame in which an invariant can (y)y | ∇Gψ(y, gi) = 0}. be easily constructed in a process known as canonization Soatto (2009): φ(y) 11For regular sampling of stationary signals on the real line, optimal weights for reconstruction can be derived explicitly in terms of spectral characteristics of the signal, as done in classical (Shannon) sampling theory. More in general, the computation of optimal weights for function-valued signals deﬁned on a general group G for tasks other than reconstruction, is an open problem. Recent results Chen & Edelsbrunner (2011) show that diffusion on the location-scale group typically reduce the incidence of topological features such as aliases in the ﬁltered signal. Thus, low-pass ﬁltering such as (generalized) Gaussian smoothing as done here, can have anti-aliasing effects.  = {g .  −1 i  4  Published as a conference paper at ICLR 2016  to translation parallel to the image plane, scaling, and changes in pixel values that do not alter relative order.12 Under these (admittedly restrictive) conditions, the SAL can be easily computed and corresponds to known “descriptors.” Note that, by deﬁnition, x and y must be generated by the same scene θ for them to “correspond.” SIFT Lowe (2004) performs canonization10 of local similarity transformations via adaptive sam- pling of the planar translation-scale group (extrema of the difference-of-Gaussians operator in space and scale), and planar rotation (extrema of the magnitude of the oriented gradient). Alternatively, locations, scales and rotation can be sampled regularly, as in “dense SIFT.” Regardless, on the do- main determined by each sample, gi, SIFT computes a weighted histogram of gradient orientations. Spatial regularization anti-aliases translation; histogram regularization anti-aliases orientation; scale anti-aliasing, however, is not performed, an omission corrected in DSP-SIFT Dong & Soatto (2015). In formulas, if α(y) = ∠∇y ∈ S1 is a direction, θ = xi is the image restricted to a region determined by the reference frame gi, centered at (ui, vi) ∈ R2 axis-aligned and with size si > 0, we have13  φxi(y) =  κσ(ui − ˜u, vi − ˜v)κ(cid:15)(∠∇x(˜u, ˜v), α(y))(cid:107)∇x(˜u, ˜v)(cid:107)Esi(σ)d˜ud˜vdσ  (5) and φsift(α) = [φx11(y), . . . , φx44(y)] is a sampling on a 4×4 grid, with each sample assumed inde- pendent and α = ∠∇y quantized into 8 levels. Variants of SIFT such as HOG differ in the number and location of the samples, the regions where the histograms are accumulated and normalized. Here κσ and κ(cid:15) are Parzen kernels (bilinear in SIFT; Gaussian in DSP-SIFT) with parameter σ, (cid:15) > 0 and Es is an exponential prior on scales. Additional properties of SIFT and its variants are discussed in Sect. 3.3, as a consequence of which the above approximates the SAL for translation-scale and contrast transformation groups. Claim 2 (DSP-SIFT). The continuous extension of DSP-SIFT Dong & Soatto (2015) (5) is an anti- aliased sample of the proﬁle likelihood (4) for G = SE(2)×R+×H the group of planar similarities transformations and local contrast transformations, when the underlying scene θ = xi has locally stationary and ergodic radiance, and the noise is assumed Gaussian IID with variance proportional to the norm of the gradient.  (cid:90)  The proof follows from a characterization of the maximal invariant to contrast transformations de- scribed in Sect. 3.3. Out-of-plane rotations induce a scene-shape-dependent deformation of the domain of the image that cannot be determined from a single training image, as we discuss in Sect. 3. When interpreting local descriptors as samples of the SAL, they are usually assumed independent, an assumption lifted in Sect. 4 in the context of convolutional architectures.  3 A MORE REALISTIC INSTANTIATION  Relative motion between a non-planar scene and the viewer generally triggers occlusion phenomena. These call for the representation to be local. Intrinsic variability of a non-static scene must also be taken into account in the representation. In this section we describe the approximation of the SAL under more realistic assumptions than those implied by local, single-view descriptors such as SIFT.  3.1 OCCLUSION, CLUTTER AND “RECEPTIVE FIELDS” The data y has many components, y = {y1, . . . , yMy }, only an unknown subset of which from the scene or object of interest θ (the “visible” set V ⊂ D = {1, . . . , My}). The rest come from clutter, 12The planar translation-scale group can be taken as a very crude approximation of the transformation in- duced on the image plane by spatial translation. Contrast transformations (monotonic continuous transforma- tions of the range of the image) can be interpreted as crude approximations of changes of illumination. 13Here gy(ui, vi) = y(ui + u, vi + v) for the translation group and gy(ui, vi) = y (σui + u, σvi + v)) for translation-scale. In general, gy − x (cid:54)= y − g−1x, unless gy − x = 0. If px(y(u)) = q(y(u)− x(u)) for some . q is a density function for the random variable y(u), in general q(gy(u)− x(u)) (cid:54)= q(y(u)− g−1x(u)), unless the process y is G-stationary independent and identically distributed (IID), in which case px(gy) = pg−1x(y). Note that the marginal density of the gradient of natural images is to a reasonable approximation invariant to the translation-scale group Huang & Mumford (1999).  5  Published as a conference paper at ICLR 2016  V = (cid:83)M  occlusion and other phenomena unrelated to θ, although they may be informative as “context.” We indicate the restriction of y to V as y|V = {yj, j ∈ V }. Since the visible set is not known, proﬁling pθ,G(y) = maxi,V ∈P(D) pθ,gi(y|V ) requires searching over the power set P(D). To make computation tractable, we can restrict the visible set V to be the union of “receptive ﬁelds” Vj, that can be obtained by transforming14 a “base region” B0 (“unit ball,” for instance a square patch of pixels with an arbitrary “base size,” say 10 × 10) with group elements gj ∈ G, Vj = gjB0: . j=1 gjB0 where the number of receptive ﬁelds M (cid:28) My. Thus V is determined by the j=1 of receptive ﬁelds that are “active,” which are unknown  reference frames (group elements) {gj}M a-priori. Alternatively, we can marginalize V by computing, for every class (represented by a hidden variable θk as discussed next) and every receptive ﬁeld (determined by gj as above), conditional densities pθ(y|θk, gj) that can be averaged with respect to weights wjk, trained to select (if sparse along j) and combine (by averaging along k) local templates via  pθ(y|θk, gj)wjk  (6)  (cid:88)  j,k  where the weights or “ﬁlters” wjk, if positive and normalized,15 are interpreted as probabilities wjk = pθ(θk, gj). To make this marginalization tractable, we write the ﬁrst term as |θk, gj)  |θk, gj)pθ(y|V c  ) ∝ p(y|Vj  pθ(y|Vj  , y|V c  (7)  j  where we have assumed that the second factor is constant, thus ignoring photometric context beyond the receptive ﬁelds, e.g., due to mutual illumination. Under these assumptions, we have  j  |θk, gj) = p(y|Vj (cid:88)  pθ,gi(y) =  p(y|Vj  |θk, gigj)pθ,gi(gjθk)  (8)  j,k  where the ﬁrst term in the sum is known as a “feature map” and we have assumed that both gi, gj ∈ G for simplicity, with gij = gigj. The order of operations (deformation by gi and selection by gj) is arbitrary, so we assume that the selection by gj is applied ﬁrst, and then the nuisance-induced deformation gi, so pθ,gi(gjy) ∝ pθ,e(gijy), where e is the identity of the group G.  3.2  INTRA-CLASS VARIABILITY AND DEFORMABLE TEMPLATES  For category-level recognition, the parameter space can be divided into K classes, allowing vari- ability of θ within each class. Endowing the parameter space with a distribution p(θ|k) requires deﬁning a probability density in the space of shapes, reﬂectance functions etc. Alternatively one can capture the variability θ induces on the data. For any scene θk from class k, one can consider a single image generated by it xk ∼ pθk (x) as a “template” from which any other datum from the same class can be obtained by the (transitive) action of a group gk ∈ G. Thus if y ∼ pθk (y) with θk ∼ p(θ|k), which we indicate with y ∼ p(y|k), then we assume that there exists a gk such that y = g−1  k xk, so  p(y|k) =  =  p(y|xk, gk)dP (xk, gk|θk)dP (θk|k) p(gky|θk)dP (gk|θk)  (9)  (cid:90) (cid:90)  where we have used the fact that p(y|xk, gk, k) = δ(y − g−1 k xk) and that only one sample of θk is given, so all variability is represented by gk and xk conditionally on θk.16 For this approach to work, gk has to be sufﬁciently complex to allow xk to “reach” every datum y generated by an element17 of  14The action of a group g on a set B ⊂ D is deﬁned as gB ⊂ D such that g(y|B ) = y|gB. 15Note that current convolutional architectures rectify and normalize the feature maps, not the ﬁlters. How-  ever, learned biases, as well as rectiﬁcation and normalization at each layer, may partly compensate for it.  16In the last expression we assumed that xk and gk are conditionally independent given θk, i.e., that the  image formation process (noise) and deformation are independent once the scene θk is given.  17The group has to act transitively on xk. For instance, in Grenander (1993) gk was chosen to belong to the  entire (inﬁnite-dimensional) group of domain diffeomorphisms.  6  Published as a conference paper at ICLR 2016  j,k  (cid:88) (cid:88) (cid:88) (cid:124)  j,k  j,k  (cid:90) (cid:90)  the class. Fortunately, the density on a complex group can be reduced to a joint density on GM , the mutual conﬁguration of the receptive ﬁelds, as we will show. The restriction of gk to the domain of the receptive ﬁeld Vj = gjB0 is indicated by {gkj}M j=1, deﬁned by gkj x = gkx ∀ x ∈ Vj. Then, we can consider the global group nuisance gi, the selector of receptive ﬁelds gj and the local restriction of the intra-class group gk, assumed (d(gk, e)) small, as all belonging to the same group G, for instance afﬁne or similarity transformations of the domain and range space of the data. Starting from (8), neglecting gi for the moment, we have18 |θk, gj)pθ(θk, gj)  pθ(y) =  (10)  p(y|Vj  (11)  (12)  =  GM  |θk, gkj )dP ({gkj}|θk)pθ(θk, gj)  p(y|Vj  and bringing back the global nuisance gi,  pθ,G(y) = max  i  GM  p(gikj y|Vj  |θk)dPG({gkj}|θk)pθ(θk, gj) (cid:123)(cid:122) (cid:125)  pθ(giy)  where the measure in the last equation is made invariant to gi ∈ G. The feature maps p(gigkj y|Vj |θk) represent the photometric component of the likelihood. The geometric component is the relative conﬁguration of receptive ﬁelds {gkj}, which is class-dependent but G-invariant. The inner inte- gral corresponds to “mean pooling” and the maximization to “max pooling.” The sum over j, k marginalizes the local classes θk, or “parts” and selects them to compose the hypothesis θ. To summarize, gi are the samples of the nuisance group in (4); gj are the local reference frames that deﬁne each receptive ﬁeld in (8); gk are the global deformations that deﬁne the variability induced by a class k on a template in (9). The latter are in general far more complex than the former, but their restriction to each receptive ﬁeld, gkj , can be approximated by an afﬁne or similarity transformation and hence composed with gi and gj. Note that (11) can be interpreted as a model of a three-layer neural network: The visible layer, where |θk, gkj ) live, and an output layer that, after y lives, a hidden layer, where the feature maps p(y|Vj rectiﬁcation and normalization, yields an approximation of the likelihood pθ(y). Invariance to G can be obtained via a fourth layer outputting pθ,G(y) by max-pooling third-layer outputs pθ(giy) for different gi in (12).  3.3 CONTRAST INVARIANCE  Contrast is a monotonic continuous transformation of the (range space of the) data, which can be used to model changes due to illumination. It is well-known that the curvature of the level sets of the image is a maximal invariant Alvarez et al. (1993). Since it is everywhere orthogonal to the level sets, the gradient orientation is also a maximal contrast invariant. Here we compute a contrast invariant by marginalizing the norm of the gradient of the test image (thus retaining its orientation) in the likelihood function of a training image. Since the action of contrast transformations is spatially independent, in the absence of other nuisances we assume that the gradient of the test image y can be thought of as a noisy version of the gradient of the training image x, i.e.,  (13) and compute the density of y given x marginalized with respect to contrast transformations H of y. Theorem 2 (Contrast-invariant sufﬁcient statistic). The likelihood of a training image x at a given pixel, given a test image y, marginalized with respect to contrast transformations of the latter, is  ∇y ∼ N (∇x, (cid:15)2)  18Here we condition on the restrictions gkj of gk on the receptive ﬁelds Vj so that, by deﬁnition,  p(y|Vj  |θk, gj, gk) = p(y|Vj  |θk, gkj ).  7  Published as a conference paper at ICLR 2016  (cid:18)  (cid:19)  M  (14)  given by  px(y|H)  where, if we call Ψ(a) then  . = 1√ 2π  = p(∠∇y|∇x) = .  exp  1√ 2π(cid:15)2  (cid:82) a −∞ e− 1 2 τ 2  M =  − (m)2 2(cid:15)2√ (cid:15)e 2π  2(cid:15)2 sin2(∠∇y − ∠∇x)(cid:107)∇x(cid:107)2 − 1 (cid:16)− m  (cid:17)  .  + m − mΨ  (cid:15)  dτ for any a ∈ R, and m  = cos(∠∇y − ∠∇x)(cid:107)∇x(cid:107), .  (15)  The expression in (14) is, therefore, a minimal sufﬁcient statistic of y that is invariant to contrast transformations.  Figure 1: SIFT integrand (18) (red) vs. marginalized likelihood (14) (blue) computed for a random patch on α ∈ [−π, π] (left), and on a regular sub-sampling of 8 orientations (right). Several random tests are shown as mean and error-bars corresponding to three standard deviations across trials.  Remark 2 (Relation to SIFT). Compared to (14), SIFT (i) neglects the normalization factor M√ (ii) replaces the kernel  2π(cid:15)  ,  ˜κ(cid:15)(α)  . = exp  with a bilinear one κ(cid:15) deﬁned by  κ(cid:15)(α)  (cid:19)  (cid:18) 1  (cid:19)  2(cid:15)2 sin2(α)  (cid:39) exp  2(cid:15)2 α2  (cid:18) 1 (cid:26) α+(cid:15)  . =  (cid:15)2 (cid:15)−α (cid:15)2  α ∈ [−(cid:15), 0] α ∈ [0, −(cid:15)]  (cid:18) 1  (cid:19)  and, ﬁnally, (iii) multiplies the result by the norm of the gradient, obtaining the sift integrand  φsift(∠∇y|∇x) = κ(cid:15)(∠∇y − ∠∇x)(cid:107)∇x(cid:107) To see this, calling α = ∠∇y − ∠∇x and β = (cid:107)∇x(cid:107) > 0, notice that  κ(cid:15)(α)β = κ(cid:15)β(αβ) (cid:39) exp  2(cid:15)2β2 α2β2  (cid:39) ˜κ(cid:15)(α)  where the left-hand side is (18) and the right-hand side is (14) or (23). We make no claim that this approximation is good, it just happens to be the choice made by SIFT, and the above just highlights the relation to the contrast invariant (14). (cid:15) (cid:39) 0 holds uniformly (in α) pro- Remark 3 (Uninformative training images). It is possible that m (cid:15) (cid:28) 1, i.e., , if the modulus of the gradient ∇x is very small as compared to the standard vided γ deviation (cid:15). Under such circumstances, the training image x is essentially constant (“ﬂat”), and the conditional density p(α|∇x) becomes uniform  p(α|∇x) (cid:39) =  2(cid:15)2 γ2(1−(cid:104)∇y,∇x(cid:105)2) (cid:15)√ − 1 1√ 2π(cid:15)2 e (cid:15)√ 1√ 2π(cid:15)2 2π 2(cid:15)2 γ2(1−(cid:104)∇y,∇x(cid:105)2) (cid:39) 1 when γ − 1  = 1 2π  2π  where the approximation holds given that e SIFT (18), that becomes zero when the norm of the gradient goes to zero.  (cid:15) (cid:28) 1. This is unlike  8  (16)  (17)  (18)  (19)  (20)  −3−2−10123012345678910x 10−3−3−2−1012300.10.20.30.40.50.60.70.80.91Published as a conference paper at ICLR 2016  Note that, other than for the gradient, the computations in (14) can be performed point-wise, so for an image or patch with pixel values yi, if αi(y) p(α|∇x) =  = ∠∇yi, we can write .  (cid:89)  p(αi|∇xi).  (21)  i  We often omit reference to contrast transformations H in px(y|H), when the argument α makes it clear we are referring to a contrast invariant. The width of the kernel (cid:15) is a design (regularization) parameter. Remark 4 (Invariance for x). Note that (21) is invariant to contrast transformations of y, but not of x. This should not be surprising, since high-contrast training patches should yield tests with high conﬁdence, unlike low-contrast patches. However, when the training set contains instances that are subject to contrast changes, such variability must be managed. To eliminate the dependency on (cid:107)∇x(cid:107), consider a model where the noise is proportional to the norm of the gradient:  (22)  (cid:19)  where ˜(cid:15)((cid:107)∇x(cid:107)) = (cid:15)(cid:107)∇x(cid:107). Under this noise model, the sufﬁcient contrast invariant (14) becomes  M  exp  1√ 2π(cid:15)2  px(y|H)  = p(∠∇y|∇x) = .  2(cid:15)2 sin2(∠∇y − ∠∇x) − 1  (23) and M has the same expression (15) but with m = cos(∠∇y − ∠∇x). Thus a simple approach to managing contrast variability of x in addition to y is to use the above expression in lieu of (14). Remark 5 (Joint normalization). If we consider only afﬁne contrast transformations ax + b where a, b are assumed to be constant on a patch V which contains all the cells Ci where the descriptors are computed19 it is clear that to recapture invariance w.r.t. the scale factor a it is necessary and sufﬁcient that p(∠∇y|∇x(vi)) = p(∠∇y|a∇x(vi)), ∀vi ∈ V . We shall now illustrate how this invariance can be achieved. Assume that data generating model (13) is replaced by the distribution-dependent model (cid:107)∇x(cid:107)2px(∇x)d∇x  (cid:15)2(px) = σ2Ex(cid:107)∇x(cid:107)2 = σ2  ∇y ∼ N (∇x, (cid:15)2(px))  (cid:90)  (24)  ∇y ∼ N(cid:0)∇x, ˜(cid:15)2(cid:1) (cid:18)  where the noise variance (cid:15)2 depends linearly on the average squared gradient norm (w.r.t. the distribution px(∇x)); σ2 is ﬁxed constant. The resulting marginal distribution for the gradient orientation becomes  (cid:19)  (cid:107)∇x(cid:107)2 Ex(cid:107)∇x(cid:107)2  exp  (cid:18) − 1 2σ2 sin2(∠∇y − ∠∇x) √Ex(cid:107)∇x(cid:107)2 (cid:107)∇x(cid:107) − ( ¯m)2 2σ2√ σe 2π  (cid:16)− ¯m  + ¯m − ¯mΨ  (cid:17)  σ  ,  .  ¯M =  ¯M  (25)  (26)  ¯px(y|H)  = ¯p(∠∇y|∇x) = .  1√ 2πσ2 = cos(∠∇y − ∠∇x) .  where, deﬁning ¯m  Equation (25) is clearly invariant to afﬁne transformations of the image values x(v) → ax(v) + b, ∀v ∈ V .20 It is a trivial calculation to show that using ¯p(∠∇y|ρ) in lieu of p(∠∇y|ρ), the result is invariant w.r.t afﬁne transformations. To obtain a sampled version of this normalization the expected squared gradient norm can be re- placed with the sample average on the training patch  Npix(cid:88)  i=1  ˆρ2 . =  1  Npix  (cid:107)∇x(vi)(cid:107)2  19SIFT divides each patch into a 4 × 4 grid of cells Ci, i = 1, .., 16 20Note that an afﬁne transformation on the image values x(v) → ax(v) + b, ∀v ∈ V , induces a scale a px(ρ/a) and therefore the average squared  transformation on the distribution px(∇x) so that pax+b(ρ) = 1 gradient is scaled by a2, i.e. Eax+b(cid:107)ρ(cid:107)2 = a2Ex(cid:107)ρ(cid:107)2.  9  Published as a conference paper at ICLR 2016  so that (24) becomes,  Npix(cid:88)  ∇y ∼ N (∇x, (cid:15)2(V ))  (27) where vi ∈ V , i = 1, .., Npix are the pixel locations in the training patch V . This procedure is known as “joint normalization”, and is simply equivalent to normalizing the patch in pre-processing by dividing by the average gradient norm.  (cid:15)2(V ) = σ2 ˆρ2 = σ2 1 Npix  (cid:107)∇x(vi)(cid:107)2  i=1  3.3.1 CLAMPING GRADIENT ORIENTATION HISTOGRAMS  (cid:82) min{φsift(α|x), τ} S1 min{φsift(α|x), τ}dα  Local descriptors such as SIFT commonly apply a “clamping” procedure to modify a (discretized, spatially-pooled, un-normalized) density φsift of the form (18), by clipping it at a certain threshold τ, and re-normalizing:  φclamp(α|x) =  (28) where α = ∠∇y ∈ S1 is typically discretized into 8 bins and τ is chosen as a percentage of the maximum, for instance τ = 0.2 ∗ maxα φsift(α|x). Although clamping has a dramatic effect on performance, with high sensitivity to the choice of threshold, it is seldom explained, other than as a procedure to “reduce the inﬂuence of large gradient magnitudes” Lowe (2004). Here, we show empirically that (18) becomes closer to (14) after clamping for certain choices of threshold τ and sufﬁciently coarse binning. Fig. 2 shows that, without clamping, (18) is considerably more peaked than (14) and has thinner tails. After clamping, however, the approximation improves, and for coarse binning and threshold between around 20% and 30% the two are very similar.  Figure 2: Clamping effects: For α ∈ [−π, π] (abscissa), the top shows the marginalized likelihood p(α|∇x) (14) (blue), the SIFT integrand (18) (solid red), and its clamped version (28) (dashed red) for thresholds ranging from 50% to 10% of its maximum. The bottom shows the same discretized to 8 orientation bins. The clamping approximation is sensible only for coarse binning, and heavily inﬂuenced by the choice of threshold. For an 8-bin histogram, the best approximation is typically achieved with clamping threshold between 10% and 30% of the maximum; note that Lowe (2004) empirically chose 20%.  3.4 ROTATION INVARIANCE  Canonization Soatto (2009) is particularly well suited to deal with planar rotation, since it is possible to design co-variant detectors with few isolated extrema. An example is the local maximum of the norm of the gradient along the direction α = ˆαl(x). Invariance to G = SO(2) can be achieved by retaining the samples {pθ(α|ˆαl)}L l=1. When no consistent (sometimes referred to as “stable”) reference ˆαl can be found, it means that there is no co-variant functional with isolated extrema with respect to the rotation group, which means that the data is already invariant to rotation. Note that, again, planar rotations can affect both the training image x and the test image y. In some cases, a consistent reference (canonical element) is available. For instance, for geo-referenced scenes L = 1, and the projection of the gravity vector onto the image plane , ˆα, provides a canonical reference unless the two are orthogonal:  pθ(α|G) = pθ(α|ˆα).  (29)  4 DEEP CONVOLUTIONAL ARCHITECTURES  In this section we study the approximation of (12) implemented by convolutional architectures. Starting from (12) for a particular class and a ﬁnite number of receptive ﬁelds we notice that, since  10  −3−2−1012300.0010.0020.0030.0040.0050.0060.0070.0080.0090.01  ExactSIFTSIFT clamped at 50%−3−2−1012300.0010.0020.0030.0040.0050.0060.0070.0080.0090.01  ExactSIFTSIFT clamped at 40%−3−2−1012300.0010.0020.0030.0040.0050.0060.0070.0080.0090.01  ExactSIFTSIFT clamped at 30%−3−2−1012300.0010.0020.0030.0040.0050.0060.0070.0080.0090.01  ExactSIFTSIFT clamped at 20%−3−2−1012300.0010.0020.0030.0040.0050.0060.0070.0080.0090.01  ExactSIFTSIFT clamped at 10%−3−2−1012300.10.20.30.40.50.60.70.8  ExactSIFTSIFT clamped at 50%−3−2−1012300.10.20.30.40.50.60.70.8  ExactSIFTSIFT clamped at 40%−3−2−1012300.10.20.30.40.50.60.70.8  ExactSIFTSIFT clamped at 30%−3−2−1012300.10.20.30.40.50.60.70.8  ExactSIFTSIFT clamped at 20%−3−2−1012300.10.20.30.40.50.60.70.8  ExactSIFTSIFT clamped at 10%Published as a conference paper at ICLR 2016  the “true scene” θ and the nuisances g are unknown, we cannot factor the likelihood pθ,g(y) into a product of pθ,gj , which would correspond to a “bag-of-words” discarding the dependencies in dPG({gj}|θ). Convolutional architectures (CNNs) promise to capture such dependencies by hier- archical decomposition into progressively larger receptive ﬁelds. Each “layer” is a collection of separator (hidden) variables (nodes) that make lower layers (approximately) conditionally indepen- dent.  4.1 STACKING SIMPLIFIES NUISANCE MARGINALIZATION  We show this in several steps. We ﬁrst argue that managing the group of diffeomorphisms can be accomplished by independently managing small diffeomorphisms in each layer. We use marginal- ization, but a similar argument can be constructed for max-out or the SA likelihood. Then, we leverage on the local restrictions induced by receptive ﬁelds, to deal with occlusion, and argue that such small diffeomorphisms can be reduced locally to a simpler group (afﬁne, similarity, location- scale or even translations, the most common choice in convolutional architectures). Then global marginalization of diffeomorphisms can be accomplished by local marginalization of the reduced group in each layer. The following lemma establishes that global diffeomorphisms can be approxi- mated by the composition of small diffeomorphisms. Lemma 1. Let g ∈ G, g : D → D be an orientation-preserving diffeomorphism of a compact subset D of the real plane, e ∈ G the identity, and d(e, g) the “size” of g. Then for any (cid:15) > 0 there exists an N < ∞ and g1, . . . , gN such that g = g1 ◦ g2 ··· ◦ gN and d(e, gi) < (cid:15) ∀ i = 1, . . . , N. Now for two layers, let g = g1 ◦ g2, with g1, g2 ∼ p(g) drawn independently from a prior on G. Then p(g|g1, g2) = δ(g − g1 ◦ g2) (or a non-degenerate distribution if gi are approximated by elements of the reduced group). Then let θ1 = g2θ, or more generally let θ1 be deﬁned such that θ1 ⊥ g1 | θ, g2. We then have  pG (y|θ) =  p(y|θ, g)dP (g) =  p(y|θ1, g1, g)dP (θ1, g1, g2|θ, g)dP (g) =  (30)  (cid:90)  =  p(y|θ1, g1)dP (g1|θ)p(θ1|θ, g2)dP (g2|θ)dθ1  (31) where we have also used the fact that y ⊥ θ | θ1. Once the separator variable θ1 is reduced to a number K1 of ﬁlters, we have  (cid:90) (cid:90)  p(y|θ1  k, g1)dP (g1|θ)  p(θ1  pG(y|θ1  k)pG(θ1  k|θ)  (32)  k=1  k=1  in either case, by extending this construction to L = N layers, we can see that marginalization of each layer can be performed with respect to (conditionally) independent diffeomorphisms that can be chosen to be small per the Lemma above. Claim 3. Marginalization of the likelihood with respect to an arbitrary diffeomorphism g ∈ G can be achieved by introducing layers of hidden variables θl l = 1, . . . , L and independently marginal- izing small diffeomorphisms gl ∈ G at each layer.  The next step is to restrict the marginalization to each receptive ﬁeld, at which point it can be approximated by a reduced subgroup, or the (linear) generators.  (cid:90)  k|θ, g2)dP (g2|θ) (cid:39) K1(cid:88)  pG (y|θ) (cid:39) K1(cid:88)  (cid:90)  4.2 HIERARCHICAL DECOMPOSITION OF THE LIKELIHOOD  Let  . =  p(y|θ, g)dP (g)  (33)  be the marginal likelihood with respect to some prior on G and introduce a layer of “separator variables” θ1 and group actions g, deﬁned such that y ⊥ θ | (θ1, g1). This can always be done by choosing θ1 = y; we will address non-trivial choices later. In either case, forgoing the subscript G,  (cid:90)  G  pG (y|θ) (cid:90)  p(y|θ) =  p(y|θ1, g1)dP (θ1, g1|θ).  (34)  11  Published as a conference paper at ICLR 2016  K1  1, . . . , θ1 K1  If θ1 and g1 take a ﬁnite number K1 and L1 of values {θ1 }, then the above reduces to a sum over k = 1, . . . , K1 and (cid:96)1 = 1, . . . L1; the conditional likelihoods j ), . . . , p(y|θ1 j )} are the feature maps. If y has dimensions N × M and the group {p(y|θ1 1, g1 , g1 j are taken to be pixel wise translations across the image plane, so that L1 = N × M, actions g1 the feature maps p(y|θ1, g1) can be represented as a tensor with dimensions N × M × K1. One can repeat the procedure for new separator variables that take K2 possible values, and group actions g2 that take L2 = N1 × M1 values; the ﬁlters θ2 must be supported on the entire feature maps p(y|θ1, g1) (i.e., take values in N1 × M1 × K1) for the sum over k = 1, . . . , K1 to implement the marginalization above  } (ﬁlters) and {g1  1, . . . , g1 L1  p(y|θ) =  p(y|θ1  k, g1 (cid:96)1  )p(θ1  k, g1 (cid:96)1  |θ2 j , g2 (cid:96)2  )  p(θ2  j , g2 (cid:96)2  |θ).  (35)  (cid:96)2=1  j=1  (cid:96)1=1  k=1  The sum is implemented in convolutional networks by the use of translation invariant ﬁlters:  1. At the ﬁrst layer the support of θ1 is a small fraction of N × M and g1 acts on y so that21  L2(cid:88)  K2(cid:88)  (cid:34) L1(cid:88)  K1(cid:88)  (cid:35)  k, g1 (cid:96)1  y|θ1  p(y|θ1  ) = p(g1 (cid:96)1  k) = p(y|Vj|θ1 k). 2. At the second layer the ﬁlter p(θ1 k, g1 (cid:96)1 |θ2 j )  ber of group actions g1 (cid:96)1 k, g2 ) = p(θ1 p(θ1 (cid:96)2  |θ2 j , g2 (cid:96)2  k, g1 (cid:96)1  g1 (cid:96)1  ) is nonzero for a ﬁnite (and small) num- and also satisﬁes the shift invariant (convolutional) property  |θ2 j , g2 (cid:96)2  The third dimension of the ﬁlters is the number of feature maps in the previous layer.  4.3 APPROXIMATION OF THE FIRST LAYER  Each node in the ﬁrst layer computes a local representation (5) using parent node(s) as a “scene.” This relates to existing convolutional architectures where nodes compute the response of a rectiﬁed linear unit (ReLu) to a ﬁlter bank. For simplicity we restrict G to the translation group, thus reducing (5) to SIFT, but the arguments apply to similarities. A ReLu response at (u, v) to an oriented ﬁlter bank G with scale σ and orientation α is given by R+(α, u, v, σ) = max(0,G(u, v; σ, α) ∗ x(u, v)). Let N (u, v; σ) be a Gaussian, centered in (u, v) with isotropic variance σI, ∇N (u, v; σ) = [ ∂N ∂v (u, v; σ)], r(α) = [cos α sin α]T . Then G(u, v; σ, α) = ∇N (u, v; σ)r(α) is a directional ﬁlter with principal orientation α and scale . σ. Omitting rectiﬁcation for now, the response of an image to a ﬁlter bank obtained by varying α ∈ [−π, π], at each location (u, v) and for all scales σ is obtained as  ∂u (u, v; σ) ∂N  R(α, u, v, σ) = G(u, v; σ, α) ∗ x(u, v) = N (u, v; σ) ∗ ∇x(u, v)r(α)  = N (u, v; σ) ∗(cid:10) ∇x(u, v) (cid:90)  (cid:107)∇x(u, v)(cid:107) , r(α)(cid:11)(cid:107)∇x(u, v)(cid:107)  N (u − ˜u, v − ˜v; σ)κ(∠∇x(˜u, ˜v), α)(cid:107)∇x(˜u, ˜v)(cid:107)d˜ud˜v  =  (36)  (37)  (38)  where κ, the cosine function, has to be rectiﬁed for the above to approximate a histogram, κ+(α) = max(0, cos α) which yields SIFT. Unfortunately, in general the latter does not equal max(0,G ∗ x) for one cannot simply move the maximum inside the integral. However, under conditions on x, which are typically satisﬁed by natural images, this is the case. Claim 4. Let G be positive, smooth and have a small effective support σ < ∞. I.e., ∀ (cid:15)1, (cid:15)2 ∃ σ | vol(G(˜u, ˜v; σ, α) ≥ (cid:15)1) < (cid:15)2. Let x have a sparse and continuous gradient ﬁeld, so that for every α the domain of x can be partitioned in three (multiply-connected) regions D+(α), D−(α) and the remainder (the complement of their union), where the projection of the gradient in the di- rection α is, respectively, positive, negative, and negligible, and d(α) > 0 the minimum distance  21There is a non-trivial approximation here, namely that context is neglected when assuming that the likeli- k) depends only on the restriction of y to the receptive ﬁeld Vj; see also Section 3.1 and equation  (cid:96)1 y|θ1  hood p(g1 (7).  12  Published as a conference paper at ICLR 2016  between the regions D+ and D−. Then, provided that σ ≤ minα d(α), we have that  max(0,G(u, v; σ, α) ∗ x(u, v))  (cid:39)  N (u − ˜u, v − ˜v; σ)κ+(∠∇x(˜u, ˜v), α)(cid:107)∇x(˜u, ˜v)(cid:107)d˜ud˜v  (cid:124)  (cid:123)(cid:122)  ReLu  (cid:123)(cid:122)  sift  (cid:90) (cid:124)  (cid:125)  (cid:125)  (39)  4.4 STACKING INFORMATION A local hierarchical architecture allows approximating the SA likelihood pθ,G(·) by reducing nui- sance management to local marginalization and max-out of simple group transformations. The SA likelihood pθ,G(y) is an optimal representation for any query on θ given data y. For instance, for classiﬁcation, the representation pθ,G(y) is itself the classiﬁer (discriminant). Thus, if we could compute an optimal classiﬁer, the representation would be the identity; vice-versa, if we could com- pute the optimal representation, the classiﬁer would be a threshold. In practice, one restricts the family of classiﬁers – for instance to soft-max, or SVM, or linear – leaving the job of feeding the most informative statistic to the classiﬁer. In a hierarchical architecture, this is the feature maps in the last layer. This is equivalent to neglecting the global dependency pθ,G(y|θL) on θ at the last layer. The information loss inherent in this choice is the loss of assuming that θL are independent (whereas they are only independent conditioned on θ). An optimal representation with restricted complexity L < ∞, therefore, maximizes the indepen- dence of the components of θL, or equivalently the independence of the components of y given θL. Using those results, one can show that the information content of a representation ((47) in App. A) grows with the number of layers L.  5 DISCUSSION  For the likelihood interpretation of a CNN put forward here to make sense, training should be per- formed generatively, so ﬁxing the class θk one could sample (hallucinate) future images y from the class. However neither the architecture not the training of current CNN incorporate mechanisms to enforce the statistics of natural images. In this paper we emphasize the role of the task in the representation: If nothing is known about the task, nothing interesting can be said about the representation, and the only optimal one is the trivial one that stores all the data. This is because the task could end up being a query about the value of a particular pixel in a particular image. Nevertheless, there may be many different tasks that share the same representation by virtue of the fact that they share the same nuisances. In fact, the task affects what are nuisance factors, and the nuisance factors affect the design and learning of the representation. For some complex tasks, writing the likelihood function may be prohibitively complex, but some classes of nuisances such as changes of illumination or occlusions, are common to many tasks. Note that, by deﬁnition, a nuisance is not informative. Certain transformations that are nuisances for some tasks may be informative for others (and therefore would not be called nuisances). For instance, viewpoint is a nuisance in object detection tasks, as we want to detect objects regardless of where they are. It is, of course, not a nuisance for navigation, as we must control our pose relative to the surrounding environment. In some cases, nuisance and intrinsic variability can be entangled, as for the case of intra-class deformations and viewpoint-induced deformations in object categorization. Nevertheless, the deformation would be informative if it was known or measured, but since it is not, it must be marginalized. Our framework does not require nuisances to have the structure of a group. For instance, occlusions do not. Invariance and sensitivity are still deﬁned, as a statistic is invariant if it is constant with respect to variations of the nuisance. What is not deﬁned is the notion of maximal invariance, that requires the orbit structure. However, in our theory maximal invariance is not the focus. Instead, sufﬁcient invariance is. The literature on the topic of representation is vast and growing. We focus on visual representations, where several have been active. Anselmi et al. (2015) have developed a theory of representation aim- ing at approximating maximal invariants, which restricts nuisances to (locally) compact groups and  13  Published as a conference paper at ICLR 2016  therefore do not explicitly handle occlusions. Both frameworks achieve invariance at the expense of discriminative power, whereas in our framework both can be attained at the cost of complexity. Patel et al. (2015), that appeared after earlier drafts of this manuscript were made public, instead of of starting from principles and showing that they lead to a particular kind of computational architec- ture, instead assume a particular architecture and interpret it probabilistically, similarly to Ver Steeg & Galstyan (2015) that uses total correlation as a proxy of information, which is related to our App. A. However, there the representation is deﬁned absent a task, so the analysis does not account for the role of nuisance factors. In particular, Anselmi et al. (2015) deﬁne a G-invariant representation µ of a (single) image I as being selective if µ(I) = µ(I(cid:48)) ⇒ I ∼ I(cid:48) for all I, I(cid:48), i.e., if it is a maximal invariant. But while equivalence to the data up to the action of the nuisance group is critical for reconstruction, it is not necessary for other tasks. Furthermore, for non-group nuisances, such as occlusions, a maximal invariant cannot be constructed. Instead, given a task, we replace maximality with sufﬁciency for the task, and deﬁne at the outset an optimal representation to be a minimal sufﬁcient invariant statistic, or “sufﬁcient invariant,” approximated by the SAL Likelihood. The construction in Anselmi et al. (2015) guarantees maximality for compact groups. Similarly, Sundaramoorthi et al. (2009) have shown that maximal invariants can be constructed even for diffeomorphisms, which are inﬁnite- dimensional and non-compact. In practice, however, occlusions and scaling/quantization break the group structure, and therefore a different approach is needed that relies on sufﬁciency, not maximal- ity, as we proposed here. To relate our approach to Anselmi et al. (2015), we note that the orbit probability of Def. 4.2 is given by  (40) and is used to induce a probability on I, via P (I)[A] = P ({g | gI ∈ A}). On the other hand, we deﬁne the minimal sufﬁcient invariant statistic as the marginalized likelihood  ρI (A) = P ({g | gI ∈ A})  pθ,G(y)  . =  pθ,g(y)dP (g)  (41)  where y is a (future) image, and θ is the scene. If we consider the scene to be comprised of a set of images A = θ, and the future image y = I, then we see that the OP is a marginalized likelihood where dP (g) = dµ(g) is the Haar measure, and pθ,g(y) = δ(gy ∩ θ). Thus, substitutions G ← G, θ ← A, y ← I yield  (42) for the particular choice of Haar measure and impulsive density pθ,g. The TP representation can also be understood as a marginalized likelihood, as Ψ(I)[A] is the G-marginalized likelihood of I given A when using the uniform prior and an impulsive conditional pA,g(I):  P (I)[A] = pθ,G(y)  Ψ(I)[A] =  p(gI|A)dµ(g).  (43)  Finally, our treatment of representations is not biologically motivated, in the sense that we sim- ply deﬁne optimal representations from ﬁrst principles, without regards for whether they are im- plementable with biological hardware. However, we have established connections with both local descriptors and deep neural networks, that were derived using biological inspiration.  ACKNOWLEDGMENTS  Work supported by FA9550-15-1-0229, ARO W911NF-15-1-0564, ONR N00014-15-1-2261, and MIUR RBFR12M3AC “Learning meets time.” We are appreciative of discussions with Tomaso Poggio, Lorenzo Rosasco, Stephane Mallat, and Andrea Censi.  REFERENCES Alvarez, L., Guichard, F., Lions, P. L., and Morel, J. M. Axioms and fundamental equations of  image processing. Arch. Rational Mechanics, 123, 1993. 7  Anselmi, F., Rosasco, L., and Poggio, T. On invariance and selectivity in representation learning.  arXiv preprint arXiv:1503.05938, 2015. 2, 13, 14  14  (cid:90)  (cid:90)  G  Published as a conference paper at ICLR 2016  Bahadur, R. R. Sufﬁciency and statistical decision functions. Annals of Mathematical Statistics, 25  (3):423–462, 1954. 3  Bengio, Y. Learning deep architectures for ai. Foundations and trends R(cid:13) in Machine Learning, 2  (1):1–127, 2009. 2  Blackwell, D. and Ramamoorthi, R.V. A bayes but not classically sufﬁcient statistic. The Annals of  Statistics, 10(3):1025–1026, 1982. 3  Boureau, Y.-L., Ponce, J., and LeCun, Y. A theoretical analysis of feature pooling in visual recogni-  tion. In Proc. of Intl Conf. on Mach. Learning, pp. 111–118, 2010. 2  Bouvrie, J. V., Rosasco, L., and Poggio, T. On invariance in hierarchical models.  162–170, 2009. 2  In NIPS, pp.  Bruna, J. and Mallat, S. Classiﬁcation with scattering operators. In Proc. IEEE Conf. on Comp.  Vision and Pattern Recogn., 2011. 2  Chatﬁeld, K., Lempitsky, V., Vedaldi, A., and Zisserman, A. The devil is in the details: an evaluation  of recent feature encoding methods. 2011. 2  Chen, C. and Edelsbrunner, H. Diffusion runs low on persistence fast. In ICCV, pp. 423–430, 2011.  4  Cover, T. M. and Thomas, J. A. Elements of Information Theory. Wiley, 1991. 3  Creutzig, F., Globerson, A., and Tishby, N. Past-future information bottleneck in dynamical systems.  Phys. Rev. Lett. E, 79(4):19251–19255, 2009. 17  DeGroot, M. H. Probability and statistics. Addison-Wesley, 1989. 3  Dong, J. and Soatto, S. Machine Learning for Computer Vision, chapter Visual Correspondence, the Lambert-Ambient Shape Space and the Systematic Design of Feature Descriptors. R. Cipolla, S. Battiato, G.-M. Farinella (Eds), Springer Verlag, 2014. 2  Dong, J. and Soatto, S. Domain size pooling in local descriptors: Dsp-sift. In Proc. IEEE Conf. on  Comp. Vision and Pattern Recogn., 2015. 5  Fraser, D. and Naderi, A. Minimal sufﬁcient statistics emerge from the observed likelihood function.  International Journal of Statistical Science, 6:55–61, 2007. 17  Gong, Y., Wang, L., Guo, R., and Lazebnik, S. Multi-scale orderless pooling of deep convolutional  activation features. arXiv preprint arXiv:1403.1840, 2014. 2  Grenander, U. General Pattern Theory. Oxford University Press, 1993. 6  Hinkley, D. Predictive likelihood. The Annals of Statistics, pp. 718–728, 1979. 19  Huang, J. and Mumford, D. Statistics of natural images and models. In Proc. CVPR, pp. 541–547,  1999. 5  Kirchner, M. R. Automatic thresholding of SIFT descriptors. Technical Report, 2015. 2  LeCun, Y. Learning invariant feature hierarchies. In ECCV, pp. 496–505, 2012. 2  Lowe, D. G. Distinctive image features from scale-invariant keypoints. IJCV, 2(60):91–110, 2004.  5, 10  Mikolajczyk, K., Tuytelaars, T., Schmid, C., Zisserman, A., Matas, J., Schaffalitzky, F., Kadir, T.,  and Gool, L. Van. A comparison of afﬁne region detectors. IJCV, 1(60):63–86, 2004. 2  Morel, J. M. and Yu, G. Is sift scale invariant? Inverse Problems and Imaging, 5(1):115–136, 2011.  2  Patel, A., Nguyen, T., and Baraniuk, R. A probabilistic theory of deep learning. arXiv preprint  arXiv:1504.00641, 2015. 14  15  Published as a conference paper at ICLR 2016  Pawitan, Y. In all likelihood: Statistical modeling and inference using likelihood. Oxford, 2001. 3,  19  Ranzato, M., Huang, F. J., Boureau, Y.-L., and LeCun, Y. Unsupervised learning of invariant feature  hierarchies with applications to object recognition. In CVPR, pp. 1–8, 2007. 2  Serre, T., Oliva, A., and Poggio, T. A feedforward architecture accounts for rapid categorization.  Proceedings of the National Academy of Sciences, 104(15):6424–6429, 2007. 2  Shao, J. Mathematical Statistics. Springer Verlag, 1998. 2, 4  Simonyan, K., Vedaldi, A., and Zisserman, A. Learning local feature descriptors using convex  optimisation. IEEE Trans. Pattern Anal. Mach. Intell., 2(4), 2014. 2  Soatto, S. Actionable information in vision. In Proc. of the Intl. Conf. on Comp. Vision, October  2009. 4, 10, 17, 18  Sundaramoorthi, G., Petersen, P., Varadarajan, V. S., and Soatto, S. On the set of images modulo viewpoint and contrast changes. In Proc. IEEE Conf. on Comp. Vision and Pattern Recogn., June 2009. 14  Susskind, J., Memisevic, R., Hinton, G. E., and Pollefeys, M. Modeling the joint density of two In Proc. IEEE Conf. on Comp. Vision and Pattern  images under a variety of transformations. Recogn., pp. 2793–2800, 2011. 2  Tishby, N., Pereira, F. C., and Bialek, W. The information bottleneck method.  Allerton Conf., 2000. 2  In Proc. of the  Ver Steeg, G. and Galstyan, A. Maximally informative hierarchical representations of high-  dimensional data. in depth, 13:14, 2015. 14  16  Published as a conference paper at ICLR 2016  A QUANTIFYING THE INFORMATION CONTENT OF A REPRESENTATION In general we do not know the likelihood, but we have a collection of samples xt ∼ pθ,gt(x), each generated with some nuisance gt, which we can use to infer, or “learn” an approximation of the SAL likelihood Fraser & Naderi (2007).  = pθ(·) (cid:39) ˆp φθ(·) . φθ,G(·) = pθ,G(·) (cid:39) ˆp . = φθ,G(y)φθ(xt) (cid:39) ˆp .  (·) (y)ˆp  Xt,G  Xt,G  Xt (·), X t ∼ pθ(·)  Xt (xt) ∝ ˆp  xt,G  (empirical likelihood)  (44) (45) (proﬁle likelihood) (y) (learned representation)(46)  φθ,G(y, xt)  Depending on modeling choices made, including the number of samples N, the sampling mecha- nism, and the priors for local marginalization, the resulting representation will be “lossy” compared to the data. Next we quantify such a loss.  A.1  INFORMATIVE CONTENT OF A REPRESENTATION  . = H(φθ,G(xt)) where φθ,G is a  The scene θ is in general inﬁnite-dimensional. Thus, the informative content of the data cannot be directly quantiﬁed by mutual information I(θ; xt). In fact, H(θ) = ∞ and therefore, no matter how many (ﬁnite) data xt we have, I(θ; xt) = H(θ) − H(θ|xt) = ∞ − ∞ is not deﬁned. Similarly, I(θ; φ(xt)) is undeﬁned and therefore mutual information cannot be used directly to measure the informative content of a representation, or to infer the most informative statistic. The notion of Actionable Information Soatto (2009) as H(xt) maximal G-invariant, can be used to bypass the computation of H(θ): Writing formally I(θ; φθ,G(y)) = H(φθ,G(y)) − H(φθ,G(y)|θ) = H(xt) − H(φθ,G(y)|θ)  (47) we see that, if we were given the scene θ, we could generate the data y, under the action of some nuisance g, up to the residual modeling uncertainty, which is assumed white and zero-mean (lest the mean and correlations of the residual can be included in the model). Similarly, we can generate a maximal invariant φθ,G(y) up to a residual with constant entropy; therefore, the statistic which maximizes I(θ; φθ,G(y)) is the one that maximizes the ﬁrst term in (47), H(φθ,G(y)). This formal argument allows deﬁning the most informative statistic as the one that maximizes Actionable In- = H(xt), bypassing the computation of the entropy . formation, ˆφθ,G = arg maxφθ,G of the “scene” θ. Note that the task still inﬂuences the information content of a representation, by deﬁning what are the nuisances G, which in turn affect the computation of actionable information. An alternative approach is to measure the informative content of a representation bypassing consid- eration of the scene is described next. Deﬁnition 1 (Informative Content of a Representation). The information a statistic φ of xt conveys on θ is the information it conveys on a task T (e.g., a question on the scene θ), regardless of nuisances g ∈ G:  H(φθ,G(xt))  I(gT ; φ(xt)) = H(gT ) − H(gT|φ(xt)) ∀ g ∈ G  (48)  If the task is reconstruction (or prediction) T = y, where past data xt and future data y are gen- erated by the same scene θ, then the deﬁnition above relates to the past-future mutual information Creutzig et al. (2009) except for the role of the nuisance g. The following claim shows that an ideal representation, as previously deﬁned in terms of minimal sufﬁcient invariant statistic, maximizes information. Claim 5. Let past data xt and future data y, used to accomplish a task T , be generated by the same scene θ. Then the representation φθ,G maximizes the informative content of a representation. The next claim relates a representation to Actionable Information. Claim 6. If φ maximizes Actionable Information, it also maximizes the informative content of a representation.  Note that maximizing Actionable Information is a stronger condition than maximizing the infor- mation content of a representation. Since Actionable Information concerns maximal invariance,  17  Published as a conference paper at ICLR 2016  sufﬁciency is automatically implied, and the only role the task plays is the deﬁnition of the nuisance group G. This is limiting, since we want to handle nuisances that do not have the structure of a group, and therefore Deﬁnition 1 affords more ﬂexibility than Soatto (2009). The next two claims characterize the maximal properties of the proﬁle likelihood. We ﬁrst recall that the marginalized likelihood is invariant only if marginalization is done with respect to the base (Haar) measure, and in general it is not a maximal invariant, as one can show easily with a counter-example (e.g., a uniform density). On the other hand, the proﬁle likelihood is by construction invariant regardless of the distribution on G, but is also – in general – not maximal. However, it is maximal under general conditions on the likelihood. To see this, consider pθ(·) to be given; for a free variable . x, it can be written as a map q: pθ(x) = q(θ, x). For a ﬁxed x, q is a function of θ. If q is constant along x (the level curves are straight lines), then in general q(θ, y) = q(θ, x) for all θ does not imply y = x. Indeed, y can be arbitrarily different from x. Even if q is non-degenerate (non-constant along certain directions), but presents symmetries, it is possible for different y (cid:54)= x to yield the same q(θ, y) = q(θ, x) for all θ. However, under generic conditions q(θ, y) = q(θ, x) for all θ implies y = x. Now consider pθ,G(·) to be given; for a free variable x the map can be written using a function q such that pθ,G(x) = ming q(θ, gx). Note that pθ,G is, by construction, invariant to G. Also note that, following the same argument as above, the invariant is not maximal, for it is possible for pθ,G(x) = pθ,G(y) for all θ and yet x and y are not equivalent (equal up to a constant g: y = gx). However, if the function q is generic, then the invariant is maximal. In fact, let ˆg(θ, x) = arg min q(θ, gx), so that pθ,G(x) = q(θ, ˆg(θ, x)x). If we now have q(θ, ˆg(θ, y)y) = q(θ, ˆg(θ, x)x) for all θ, then we can conclude, based on the argument above, that ˆg(θ, x)x = ˆg(θ, y)y. Since x and y are ﬁxed, and the equality is for all θ, we can conclude that ˆg(θ, y)−1ˆg(θ, x) is independent of θ. That allows us to conclude the following. Claim 7 (Maximality of the proﬁle likelihood). If the density pθ(x) is generic with respect to x, then pθ,G(·) is a maximal G-invariant. Since we do not have control on the function pθ,G(·), which is instead in general constructed from data, it is legitimate to ask what happens when the generic condition above is not satisﬁed. For- tunately, distributions that yield non-maximal invariants can be ruled out as uninformative at the outset: Claim 8 (Non-maximality and non-informativeness). If q is such that, for any x (cid:54)= y we have q(θ, ˆg(θ, x)x) = q(θ, ˆg(θ, y)y) for all θ, then qθ,G(·) is uninformative. This follows from the deﬁnition of information, for any statistic T . As we have pointed out before, what matters is not that the invariant be maximal, but that it be sufﬁcient. As anticipated in Rem. 1, we can achieve invariance with no sacriﬁce of discriminative power, albeit at the cost of complexity.  B PROOFS  Theorem 1  Proof. Pick any θ0, deﬁne T (x) lemma with h(x) = L(θ0; x).  Theorem 2  = L(·;x) .  L(θ0;x) and f (T (x), θ)  . = L(θ;x)  L(θ0;x) and apply the factorization  ∇y(cid:107)∇y(cid:107) the normalized gradient of y, and similarly for x; Φ maps it to  Proof. We denote with ∇y polar coordinates (α, ρ) = Φ(∇y) and (β, γ) = Φ(∇x), where = ∠∇x γ .  = (cid:107)∇y(cid:107) β .  = ∠∇y .  . =  α  ρ  The conditional density of ∇y given ∇x takes the polar form  = (cid:107)∇x(cid:107). .  p(ρ, α|∇x) = p(∇y|∇x)∇y=Φ−1(ρ,α)ρ − 1 2(cid:15)2 (cid:107)∇y−∇x(cid:107)2  p(∇y|∇x) = 1  2π(cid:15)2 e  .  (49)  18  Published as a conference paper at ICLR 2016  Deﬁning (∇x)i to be the i-th component of ∇x, (49) can be expanded as  p(ρ, α|∇x) = ρ  1 2π(cid:15)2 e  − 1 2(cid:15)2 [(ρ cos(α)−(∇x)1)2+(ρ sin(α)−(∇x)2)2]  (50)  and the exponent is (ρ cos(α) − (∇x)1)2 + (ρ sin(α) − (∇x)2)2 = ρ2 − 2ρ((∇x)1 cos α + (∇x)2 sin α) + (cid:107)∇x(cid:107)2  = (cid:0)ρ − γ(cid:104)∇y,∇x(cid:105)(cid:1)2 (cid:90) ∞  + γ2(cid:0)1 − (cid:104)∇y,∇x(cid:105)2(cid:1)  (51)  We are now interested in the marginal of (50) with respect to ρ, i.e., p(ρ, α|∇x) dρ.  p(α|∇x) =  where we can isolate the factor that does not depend on ρ,  p(α|∇x) =  1√ 2π(cid:15)2  e  − 1 2(cid:15)2 γ2(1−(cid:104)∇y,∇x(cid:105)2)  0  (cid:90) ∞ (cid:124)  0  1√ 2π(cid:15)2  e  − 1 2(cid:15)2 (ρ−γ(cid:104)∇y,∇x(cid:105))2  (cid:123)(cid:122)  M  (52)  (53)  .  ρdρ  (cid:125)  The bracketed term M is the integral on the interval [0,∞) of a Gaussian density with mean m . = γ(cid:104)∇y,∇x(cid:105) = cos(∠∇y − ∠∇x)(cid:107)∇x(cid:107) and variance (cid:15)2; it can be rewritten, using the change of variable ξ ((cid:15)ξ + m) dξ which can be integrated by parts to yield  = (ρ − m)/(cid:15), as(cid:82) ∞  2 ξ2 e 1  .  −m/(cid:15)  1√ 2π  M =  2  m2  − 1 (cid:15)2√ (cid:15)e 2π  + m  (cid:16) (cid:18)  1 − Ψ  (cid:16)− m −(cid:107)∇x(cid:107)2 − m2  (cid:17)(cid:17) (cid:19)  (cid:15)  2(cid:15)2  and therefore  p(α|∇x) =  1√ 2π(cid:15)2  exp  M  (54)  which, once written explicitly in terms of x and y, yields (14)-(15).  Claim 5  Proof. Since pθ,G(y, xt) is sufﬁcient for θ, and it factorizes into φθ,G(y)φθ(xt), then φθ(xt) is sufﬁcient of xt for θ. By the factorization theorem (Theorem 3.1 of Pawitan (2001)), there exist functions fθ and ψ such that φθ(xt) ∝ fθ(ψ(xt)), i.e., the likelihood depends on the data only through the function ψ(·). This latter function is what is more commonly known as the sufﬁcient statistic, which in particular has the property that p(θ|xt) = p(θ|ψ(xt)). However, if φθ is sufﬁcient for θ, it is also sufﬁcient for future data generated from θ. Formally,  pG(y|xt) =  pG(y|xt, θ)p(θ|xt)dP (θ) =  (55) which shows that ψ minimizes the uncertainty of y for any g ∈ G since the right-hand-side is G-invariant. The right-hand side above is the predictive likelihood Hinkley (1979), which must therefore be proportional to ˜fy(ψ(xt)) for some ˜f and the same ψ, also by the factorization theorem.  pG(y|θ)p(θ|ψ(xt))dP (θ) = pG(y|ψ(xt))  (cid:90)  (cid:90)  Claim 4  Proof. (Sketch) The integral in (38) can be split into 3 components, one of which omitted, leaving the positive component integrated on D+, the negative component on D−. If the distance between these two is greater than σ, however, the components are disjoint, so for each (u, v) and α, only the the positive or the negative component are non-zero, and since N and (cid:107)∇x(cid:107) are both positive, and the sign is constant, rectiﬁcation inside or outside the integral is equivalent. When σ > d there is an error in the approximation, that can be bounded as a function of σ, the minimum distance and the maximum gradient component.  19  Published as a conference paper at ICLR 2016  Figure 3: D+(0) (green) and D−(0), the positive and negative responses to a gradient ﬁlter in the horizontal direction. The black region is their complement, which separates them.  Figure 4: Rectiﬁed cosine (blue) and its powers, compared to a Gaussian kernel (red). While the two are distinctly different for (cid:15) = 1, as the power/dispersion decreases, the latter approximates the former. The plot shows (cid:15) = 1, 1/5, 1/9 for the cosine, and 1/5, 1/9, 1/13 for the Gaussian.  A more general kernel could be considered, with a parameter (cid:15) that controls the decay, or width, of the kernel, κ(cid:15)(α). For instance, κ(cid:15)(α) = κ(α) 1 (cid:15) , with the default value being (cid:15) = 1. An alternative is to deﬁne κ to be an angular Gaussian with dispersion parameter (cid:15), which is constrained to be positive and therefore does not need rectiﬁcation. Although the angular Gaussian is quite different from the cosine kernel for (cid:15) = 1, it approximates it as (cid:15) decreases (Fig. 4). A corollary of the above is that the visible layer of a CNN computes the SAL Likelihood of the ﬁrst hidden layer. The interpretation of SIFT as a likelihood function given the test image y can be confusing, as ordinarily it is interpreted as a “feature vector” associated to the training image x, and compared with other feature vectors using the Euclidean distance. In a likelihood interpretation, x is used to compute the likelihood function, and y is used to evaluate it. So, there is no descriptor built for y. The same interpretational difference applies to convolutional architectures. If interpreted as a likelihood, which would require generative learning, one would compute the likelihood of different hypotheses given the test data. Instead, currently the test data is fed to the network just as training data were, thus generating features maps, that are then compared (discriminatively) by a classiﬁer.  20  −4−3−2−10123400.0020.0040.0060.0080.010.0120.014",
1511.07069,2016,Auxiliary Image Regularization for Deep CNNs with Noisy Labels,"['Auxiliary Image Regularization for Deep CNNs with Noisy Labels\nSamaneh Azadi', 'Jiashi Feng', 'Stefanie Jegelka', 'Trevor Darrell']",https://arxiv.org/pdf/1511.07069,"6 1 0 2    r a  M 2         ]  V C . s c [      2 v 9 6 0 7 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  AUXILIARY IMAGE REGULARIZATION FOR DEEP CNNS WITH NOISY LABELS  Samaneh Azadi1, Jiashi Feng1,2, Stefanie Jegelka3 & Trevor Darrell1 1 Department of EECS, University of California, Berkeley 2 Department of ECE, National University of Singapore 3 Department of EECS, Massachusetts Institute of Technology {sazadi,trevor}@eecs.berkeley.edu elefjia@nus.edu.sg stefje@csail.mit.edu  ABSTRACT  Precisely-labeled data sets with sufﬁcient amount of samples are very important for training deep convolutional neural networks (CNNs). However, many of the available real-world data sets contain erroneously labeled samples and those errors substantially hinder the learning of very accurate CNN models. In this work, we consider the problem of training a deep CNN model for image classiﬁcation with mislabeled training samples – an issue that is common in real image data sets with tags supplied by amateur users. To solve this problem, we propose an auxiliary image regularization technique, optimized by the stochastic Alternating Direction Method of Multipliers (ADMM) algorithm, that automatically exploits the mutual context information among training images and encourages the model to select reliable images to robustify the learning process. Comprehensive experiments on benchmark data sets clearly demonstrate our proposed regularized CNN model is resistant to label noise in training data.  1  INTRODUCTION  Deep Convolutional Neural Network (CNN) models have seen great success in solving general ob- ject recognition problems (Krizhevsky et al., 2012). However, due to their extremely huge parameter space, the performance of deep models relies heavily on the availability of a sufﬁciently large num- ber of training examples. In practice, collecting images as well as their accurate annotations at a large scale is usually tedious and expensive. On the other hand, there are millions of freely available images with user-supplied tags that can be easily collected from the web. Being able to exploit this rich resource seems promising for learning a deep classiﬁcation model. The labels of these web images, however, tend to be much more noisy, and hence challenging to learn from. In this work, we consider the problem of image classiﬁcation in this challenging scenario where annotations of the training examples are noisy. In particular, we are interested in how to learn a deep CNN model that is able to produce robust image representations and classiﬁcation results in presence of noisy supervision. Deep CNN models have been recognized to be sensitive to sample and label noise in recent works (Sukhbaatar & Fergus, 2014; Nguyen et al., 2014). Thus, several methods (Sukhbaatar & Fergus, 2014; Xiao et al., 2015) have been developed to alleviate the negative effect of noisy labels in learning the deep models. However, most of the existing methods only consider modeling a ﬁxed category-level label confusion distribution and cannot alleviate the effect of noise in representation learning for each sample. We propose a novel auxiliary image regularizer (AIR) to address this issue of deception of training annotations. Intuitively, the proposed regularizer exploits the structure of the data and automatically retrieves useful auxiliary examples to collaboratively facilitate training of the classiﬁcation model. Here, structure of the data means the nonlinear manifold structure underlying images from multiple categories learned from a well-trained deep model on another data set D(cid:48). To some extent, the AIR regularizer can be deemed as seeking some “nearest neighbors” within the training examples to reg-  1  Published as a conference paper at ICLR 2016  Figure 1: Exemplar description of the regularized deep CNN model. We impose the proposed auxiliary example regularization on the top layer of the deep CNN model. Image representations from 5 different categories of Imagenet7k data set, learned from a well-trained deep CNN model, are visualized by t-SNE embedding (Van der Maaten & Hinton, 2008). The noisy categories are shown in different colors, circles demonstrate images with clean labels in the learned manifold and triangles display images with corrupted annotations. Example images are clustered according to their ground- truth labels. The border color indicates noise-contaminated labels of images. Our auxiliary image regularizer (AIR) gives higher weights to the important image representations while seeking for a structured sparsity pattern. AIR encourages the impact of some images to go to zero (especially those mislabeled images whose representations associate them with a set of images incorporated with a different label). It in fact pursues some “nearest neighbors” within the training examples to regularize the ﬁtting of a deep CNN model to noisy samples. As the result, the groups (and the corresponding features) deemed not relevant will have small effect on classiﬁcation. The blue solid line is the hypothetical boundary classiﬁer suggested by AIR to classify images in the blue category versus non-blue categories while the dashed line corresponds to the hypothetical SVM classiﬁer. Here, AIR disregards images with blue noisy labels (triangles) while SVM considers those examples in training the classiﬁer.  ularize the ﬁtting of a deep CNN model to noisy samples and improve its classiﬁcation performance in presence of noise. Robustifying classiﬁcation models via regularization is a common practice to enhance robustness of the models. Popular regularizers include Tikhonov regularization ((cid:96)2-norm) and the (cid:96)1-norm on the model parameters (Tibshirani, 1996). However, an effective regularizer for training a deep CNN model is still absent, especially for handling the above learning problem with faulty labels. To the best of our knowledge, this work is among the ﬁrst to introduce an effective regularizer for deep CNN models to handle label noise. Inspired by related work in natural language processing (Yogatama & Smith, 2014), we use a group sparse norm to automatically select auxiliary images. Figure 1 shows an exemplar overview of our model. In contrast to previous works imposing the regularization on the model parameter, we propose to construct groups of input image features and apply the group sparse regularizer on the response maps. Imposing such group sparsity regularization on the classiﬁer response enables it to actively select the relevant and useful features, which gives higher learning weights to the infor- mative groups in the classiﬁcation task and forces the weights of irrelevant or noisy groups toward zero. The activated auxiliary images implicitly provide guiding information for training the deep models. We solve the associated optimization problem via ADMM (Glowinski & Marroco, 1975;  2  Published as a conference paper at ICLR 2016  Gabay & Mercier, 1976; Lions & Mercier, 1979), recently popularized by Boyd et al. (2011). In particular, we use the stochastic ADMM method of Ouyang et al. (2013) and Azadi & Sra (2014) on this large-scale problem. We demonstrate the effect of AIR on image classiﬁcation via deep CNNs, where we synthetically corrupt the training annotations. We investigate how the proposed method identiﬁes informative images and ﬁlters out noisy ones among the candidate auxiliary images. Going one step further, we then explore how the proposed method improves learning of image classiﬁcation from user-supplied tags and handles the inherent noise in these tags. Comprehensive experiments on benchmark data sets, shown in Section 4, clearly demonstrate the effectiveness of our proposed method for the large- scale image classiﬁcation task.  1.1 RELATED WORK  A large body of existing work proposes to employ sparsity-inducing (Tibshirani, 1996) or group- sparsity inducing (Friedman et al., 2010; Zou & Hastie, 2005) norms for effective model regular- ization and better model selection, with applications to dictionary learning (Mairal et al., 2009) and image representation learning (Yang et al., 2009; Bach et al., 2012), to name a few examples from the ﬁeld of computer vision. However, most focus on imposing a structured prior on the model parameters. The idea of exploiting the group sparsity structure within raw data was recently pro- posed in (Yogatama & Smith, 2014), to solve text recognition problems, by exploiting the intrinsic structure among sentences in a document. However, as far as we know, none of existing works have investigated how to exploit the structural information among data for a deep model, as we address here, and we are among the ﬁrst to propose such a regularized deep model based on auxiliary data. In this work, we are particularly interested in learning from noisy labeled image data, where a limited number of training examples are supplied with clean labels. Among the most recent contributions, Sukhbaatar et al. (2015) solve this problem by learning the noise distribution through an extra noise layer added to the deep model,while Xiao et al. (2015) explore this problem from a probabilistic graphical model point of view and train a classiﬁer in an end-to-end learning procedure. Izadinia et al. (2014) introduce a robust logistic regression method for classiﬁcation with user-supplied tags. Feng et al. (2014) deal with arbitrary outliers in the data through a robust logistic regression method by estimating the parameters in a linear programming scheme. Different from those existing works, we automatically exploit the contextual information from useful auxiliary images via a new regular- ization on deep CNN models.  2 AUXILIARY IMAGE REGULARIZER  2.1 PROBLEM SETUP Suppose we are given a set of training images D = {(x1, y1), . . . , (xn, yn)} ⊂ Rp ×C. Some train- ing samples in D have noisy annotations. We do not assume any speciﬁc distribution on the noise in yi. Indeed, we only assume the number of noisy labels does not exceed the number of correct labels. This signiﬁcantly relaxes the assumptions imposed in previous works (Sukhbaatar & Fergus, 2014; Xiao et al., 2015). For instance, Sukhbaatar & Fergus (2014) require the noise to follow a ﬁxed con- fusion distribution. Our goal is to learn a deep CNN model M for the data set D while we have ac- n(cid:48))} ⊂ Rp×C(cid:48) cess to a deep network pre-trained on an independent set D(cid:48) = {(x(cid:48) containing sufﬁciently many accurately annotated examples in categories C(cid:48). We use this pre-trained network to produce representations of images in the main set D. We employ the popular AlexNet architecture (Krizhevsky et al., 2012) to build our CNN model. The top layer which accounts for classiﬁcation is parameterized by w, and we apply the usual empirical risk minimization method to learn such parameter:  1), . . . , (x(cid:48)  n(cid:48), y(cid:48)  1, y(cid:48)  (cid:40)  (cid:41)  n(cid:88)  i=1  w = argmin  L(w; D) :=  w  (cid:96)(w; xi, yi)  ,  where (cid:96)(·; xi, yi) is the classiﬁcation loss on an individual training example in set D. Adding a regularizer to the loss function is a common approach in solving large-scale classiﬁcation problems  3  Published as a conference paper at ICLR 2016  to prevent overﬁtting and train a more accurate and generalizable classiﬁer:  ˆw = argmin  L(w; D) + Ω(w).  w  (1)  Popular regularizers Ω(w) include Ω(w) = λ(cid:107)w(cid:107)2  2, and Ω(w) = λ(cid:107)w(cid:107)1.  2.2 AUXILIARY IMAGE REGULARIZER  In this work, beyond imposing a prior structure of the model w, we propose a novel regularizer to exploit the data structure within D to handle the sample noise, which is deﬁned as:  where (cid:107) · (cid:107)g denotes the group norm deﬁned as (cid:107)v(cid:107)g :=(cid:80)  Ωaux(w; D) = (cid:107)F w(cid:107)g,  j∈G λj(cid:107)vj(cid:107)2. In the group norm, G is an index set of all groups/partitions within the vector v, (λj)j∈G are some positive weights, and vj denotes the sub-vector indexed by group j. The norm (cid:107)v(cid:107)g induces a group sparsity that encourages the coefﬁcients outside a small number of groups to be zero. We do not impose the group norm on the parameter w directly. Instead, we encourage the unlabeled response of the deep model on the learned representation of image data set D to be group-wisely sparse, such that only a small number of images will contribute to learning of the model, while other non-relevant images are ﬁltered out. These image representations are extracted from the model pre-trained on the well-labeled data set D(cid:48). Therefore, the regularization draws information from features trained on available data D(cid:48). Each image is a “group” of active features. The regularizer enforces the features of the “good” and “stable” images to be used for model learning, while noisy additional activations will be disregarded. The active images are the ones that are categorized well in the feature space obtained from the deep model. For a new image, the subset of active features close to those stable images in the learned manifold will be weighted most highly (hence “neighbor regularization”). Towards this target, the matrix F is constructed as F (cid:62) = [X1, X2,··· , Xn] and (F w) = [X1w, X2w,··· , Xnw](cid:62). Here, Xi is a diagonal matrix consisting of the features (such as the outputs of the fc-7 layer) of image i representing the i-th group in the group norm regularization setting. The group norm enforces the resulting response vector F w to be sparse, namely only a small number of images are active. These active images are auxiliary examples for learning that are automatically identiﬁed by the model. They contribute additional information to model learning. In multi-class classiﬁcation, F w is a matrix and the group norm regularizer is deﬁned as the sum of all group sparsity norms of each column. Our proposed auxiliary image regularized (AIR) model is then deﬁned as:  w = arg min L(w; D) + Ωaux(w; D).  Our goal is to classify an independent target data set D that has corrupted labels. A deep Convolutional Neural Network (CNN) is parameterized layer-wisely by w(1), . . . , w((cid:96)), and in this work we only impose the auxiliary regularizer on the top (last) layer of the CNN. The objective function we are going to work with for training a deep CNN model is  L(w(1), . . . , w((cid:96)); D) + Ωaux(w((cid:96)); D).  A popular loss function used for image classiﬁcation with CNNs is the following cross-entropy loss with softmax:  L(w; D) = − n(cid:88)  |C|(cid:88)  i=1  j=1  1{yi = j} log   exp{w((cid:96)) (cid:80)|C| xi} (cid:62) h=1 exp{w((cid:96))  (cid:62)  h  j  xi}   ,  (2)  with the output feature xi being a function of w(1), . . . , w((cid:96)−1) and the raw input image. Here, 1 is an indicator function.  4  Published as a conference paper at ICLR 2016  3 OPTIMIZATION WITH STOCHASTIC ADMM  The standard backprogapation technique via stochastic gradient descent cannot handle the non- smooth regularization term Ωaux well and presents slow convergence rate. We here demonstrate how the loss function in Section 2.2 can be optimized via ADMM. In this section, we use w to denote the parameter of the top layer w((cid:96)). After introducing an auxiliary variable v = F w, our optimization problem becomes:  min v,w  L(w; x, y) + λ1(cid:107)w(cid:107)2  2 +  λg(cid:107)vg(cid:107)2,  (cid:88)  j∈G  s.t.  v = F w,  (3) The matrix F ∈ Rnp×p is a sparse matrix with only one non-zero entry in each row, which cor- responds to an entry in the feature vector, and the ﬁrst p rows of F correspond to the features of the ﬁrst sample, indicated as X1 in Section 2.2. In other words, the matrix F gives a weight to the parameter value wi (1 ≤ i ≤ p) within each group that is equal to the i-th entry in the feature vec- tor. Feature values indicate the activity of corresponding visual feature of an image in the network model. Each term in the group regularizer is normalized by the size of the group through weights λg such that all of the groups have the same overall effect in the regularizer. The approximated Augmented Lagrangian (Ouyang et al., 2013) for the objective function in Eq. (3) with u being the Lagrange variable is:  min v,w  L(wk; x, y) + (cid:104)gk, w(cid:105) + λ1(cid:107)w(cid:107)2  2 +  +(cid:104)u, v − F w(cid:105) +  (cid:107)v − F w(cid:107)2  2 +  ρk 2  λg(cid:107)vg(cid:107)2  (cid:88)  j∈G 1  2ηk+1  (cid:107)w − wk(cid:107)2,  (4)  where L(w; x, y) is replaced with its ﬁrst order approximation at wk, with k indicating the number of iteration. Here, gk is the gradient of L(w; x, y) at the k-th iteration over a mini-batch of training samples. We set ηk equal to 2/(k + 2) according to Azadi & Sra (2014). Then applying Stochastic Alternating Direction Method of Multipliers (SADMM) (Ouyang et al., 2013) followed by a non- uniform averaging step to give higher weights to the recent updates, inspired by Azadi & Sra (2014), gives the following alternative updates of the variables w, v, u:  wk+1 = argmin  w  L(wk; x, y) + (cid:104)gk, w(cid:105) + λ1(cid:107)w(cid:107)2  2  +  ρk 2  (cid:88)  (cid:107)F w − (vk + uk/ρk)(cid:107)2 λg(cid:107)vj(cid:107)2 + ρk vk+1 = argmin 2 uk+1 = u + ρk(vk+1 − F wk+1) ρk+1 = βρk  j∈G  v  1  (cid:107)w − wk(cid:107)2  2ηk+1  2 + (cid:107)v − (F wk+1 − uk/ρk)(cid:107)2  2  (5)  We use an adaptive ρ by increasing its value by a factor of β in each iteration up to a ﬁxed maximum value ρmax. w can be updated in a closed form solution as:  wk+1 = (ρkF (cid:62)F +  I  ηk+1  )−1(−gk − λ1 2  wk + ρkF (cid:62)vk + F (cid:62)uk +  1  ηk+1  wk)  The update of v has a closed-form solution as the proximal operator for the (cid:96)1/2-norm (group soft- thresholding) (Bach et al., 2011): g = prox2,λg/ρk vk+1  (cid:0)Fgwk − uk g /ρ(cid:1) ,  (6)  where soft-thresholding operator for group norm is deﬁned as  (cid:40)  prox2,α(z) =  if (cid:107)z(cid:107)2 ≤ α z, otherwise.  0, (cid:107)z(cid:107)2−α (cid:107)z(cid:107)2  5  Published as a conference paper at ICLR 2016  Since the update of vi is independent of the update of any other vj when i (cid:54)= j, i, j ∈ G, all vi can be updated in parallel to further reduce computational cost. Thereafter, we apply a non-uniform averaging step on w as:  ¯wk = (1 − θk) ¯wk−1 + θkwk,  where θk = 2/(k + 2); similar updates apply for ¯vk and ¯uk.  4 EXPERIMENTS ON DEEP CNN MODEL  In this section, we explore the robustness added to the classiﬁer by the proposed auxiliary regularizer and evaluate the performance of the regularized CNN model proposed in Section 2.2 on different benchmark data sets. First, we examine the effect of the auxiliary regularizer when noisy labels are added to clean data sets manually. Second, we investigate its inﬂuence on the robustness of the model trained on a freely-available user-tagged data set. In all of our experiments, we use the AlexNet CNN model pre-trained on the ISLVRC2012 data set (Jia et al., 2014) and ﬁne-tune its last layer on the target data set D. Furthermore, we set λ1 in Eq. (3) to a very small number, λg equal to 10 over the length of each feature vector, and ﬁne- tune the other set of hyper-parameters (batch size, β ∈ {1.1, 1.3, 1.5} , and ρmax) in the SADMM updates of Eq. (5) on the cross-validation set for each experiment. The initial value for ρ is 10 in all experiments. We cross-validate the regularization parameter C for our SVM baseline from the set of {1, 10, 100}. We deﬁne our loss function to be a softmax and apply the AIR regularizer on the top layer in the CNN model.  4.1 EXPERIMENTS WITH SYNTHETIC NOISY LABELS  First, we conduct image classiﬁcation on a subset of the ImageNet7k data set (Deng et al., 2010). We use a pre-trained AlexNet CNN model and ﬁne-tune its last layer on randomly selected 50 classes from ImageNet7k data set as the leaf categories of animal each of which contains 100 samples. We randomly ﬂipped half of the labels among all 50 categories. This is exactly the problem described in Section 2.1. We perform similar experiment on the MNIST data with 10 categories of handwritten digits con- taining 7 × 104 samples in total. We use a confusion matrix Q to deﬁne the distribution of noisy labels among the 10 categories. We followed the same settings as in Sukhbaatar et al. (2015) to determine the probability of changing label i to label j for all i, j ∈ [1, 10] as qij. Different levels of noise are applied by setting the diagonal values of the 10 × 10 matrix Q equal to the noise level and normalizing the distribution accordingly. We empirically investigate the robustness gained from applying the proposed AIR regularize com- pared to a linear SVM classiﬁer as well as Robust Logistic Regression (RoLR) (Feng et al., 2014) which assumes a constant fraction of outliers – The SVM and RoLR classiﬁers are used on the last layer of a similar network. The results, shown in Table 1, demonstrate that AIR offers signiﬁcant improvements over the performance of deep CNN plus SVM or RoLR. On the ImageNet7k data, the performance gain is as large as 8% compared to the SVM. We also trained the deep CNN with AIR on the CIFAR-10 data which contains 10 different cate- gories each with 5000 training samples. We used the same confusion matrix Q as explained above and exactly the same as the matrix deﬁned in Sukhbaatar et al. (2015) to randomly corrupt the labels. Since the size of images in CIFAR-10 and MNIST data sets is small (32 × 32 and 28 × 28 respec- tively), we re-sized images to 256×256, and then cropped them to 227×227 and thereafter extracted their fc7 features from the pre-trained model explained above. We tested the robustness of the model on different batches of CIFAR-10 with various noise levels. In Figure 2, we compare the accuracy of the AIR model to that of an SVM with (cid:96)2 regularization, Sukhbaatar et al. (2015)’s deep model that adds a noise layer to the CudaConv network, and the CudaConv network. CudaConv refers to the network with three convolutional layers with similar model architecture and hyper-parameter settings used in Sukhbaatar et al. (2015) given by Krizhevsky.  6  Published as a conference paper at ICLR 2016  Table 1: Comparison of classiﬁcation accuracy achieved by the deep models with different regular- izers: “ft-last-AIR” as our proposed AIR regularizer, “ft-last-SVM” as SVM with (cid:96)2 regularization, and “ft-last-RoLR” as the Robust Logistic Regression method, all applied on ﬁne-tuning the last layer. All models were pre-trained on the ILSVRC2012 data set and ﬁne tuned on the target data set.  Regularizer  data set  ImageNet7k MNIST AlexNet+ft-last-AIR 61.5% 92.37% AlexNet+ft-last-SVM 53.44% 90.93% AlexNet+ft-last-RoLR 58.71% 90.89%  Figure 2: Comparison of different deep models on CIFAR-10: “ft-last-AIR” as our proposed AIR regularizer, and “ft-last-SVM” as SVM with (cid:96)2 regularization applied on the last layer of Alexnet. “CudaConv+learned-Q” indicates the end-to-end model proposed by Sukhbaatar et al. (2015) where they add a noise layer to the “CudaConv” model.  Figure 2 illustrates that the SVM model suffers as the number of incorrect labels grows, whereas AIR remains robust even with large amounts of corruption. Moreover, the performance of both CudaConv+learned-Q and CudaConv depends heavily on the number of training samples, while we can reach the same classiﬁcation accuracy with signiﬁcantly fewer number of data points and the same noise level. To show the beneﬁt of stochastic ADMM in solving our proposed regularized optimization problem in comparison with stochastic gradient descent (SGD), we re-run the same experiment on CIFAR-10 data set but train the model with SGD in the presence of 50% label noise. This experiment results in an accuracy of 66% compared to 72% accuracy achieved by stochastic ADMM following the same settings discussed in section 3. We also trained AlexNet with (cid:96)2 regularizer in an end-to-end scheme on CIFAR-10 with 50% in- correct labels where the weights of the last layer are initialized with the learned weights from AlexNet+ft-last-SVM and AlexNet+ft-last-AIR. Classiﬁcation accuracy obtained from these dif- ferent initializations is 78% and 79%, respectively. It shows that even implementing the proposed AIR regularizer in the last layer will improve the accuracy of the whole deep model that is trained end-to-end. The AIR regularizer automatically incorporates inherent structure in the image data by forcing the weights of noisy groups to decrease toward zero and giving higher learning weights to the sta- ble groups. We illustrate this characteristic of AIR in Figure 3 by comparing the distribution of activations of noisy-labeled images and the distribution of the activations of clean images in the Imagenet7k experiment along different learning iterations. Here “activation” refers to the (cid:96)2-norm of the weights associated with each of the groups, i.e., (cid:107)vg(cid:107)2 in Eq. (3). The distributions of these two sets of activations highly overlap in the ﬁrst iteration and they gradually get more and more  7  12345Number of Training Data (x10k)50556065707580Accuracy(%)30% Incorrect Labels12345Number of Training Data (x10k)5055606570758040% Incorrect Labels12345Number of Training Data (x10k)5055606570758050% Incorrect LabelsAlexNet+ft-last-AIRAlexNet+ft-last-SVMCudaConv+learned-QCudaConvPublished as a conference paper at ICLR 2016  Figure 3: Distribution of activations given to true and noisy images along different learning itera- tions. Images belong the Imagenet7k experiment. Distributions on the left hand side of the plot refer to AIR and the one in the right is related to SVM.  (a) Auxiliary Image Regularizer (AIR)  (b) (cid:96)2-Regularized SVM  Figure 4: Images with clean labels have blue borders and images with noisy labels are surrounded by red borders. Images are ranked based on their activities found by AIR (right) or SVM (left) from the top row to the bottom. Images are from a category with synset ID “n01317916” from the 50 animal categories in the ImageNet7k data set.  distinct from each other revealing the ability of the auxiliary regularizer in ﬁnding the images with noisy annotations. We manually compute the same activation scores per image for SVM by setting = XgwSVM and compare the corresponding distribution in the right plot of Figure 3. Acti- vSVM g vation scores for images with both clean and noisy labels, learned from SVM, overlap notably after equal number of epochs of training with AIR.  4.2 EXPERIMENTS WITH REAL NOISY LABELS  In this section, we examine the performance of deep image classiﬁer on images with user-supplied tags from publicly available multi-label NUS-WIDE-LITE data set (Chua et al., July 8-10, 2009) as a subset of large Flickr data set. This data set contains 81 different tags with the total number of  8  0.00.20.40 Epoch0.00.20.41 Epoch0.00.20.4AlexNet+ft-last-AIR4 Epochs0.00.20.412 Epochs0.00.20.450 Epochs0.00.10.20.3AlexNet+ft-last-SVM50 EpochsImages with Noisy labelsImages with True labelsPublished as a conference paper at ICLR 2016  Figure 5: Comparison of overall precision and recall for different methods on NUS-WIDE-LITE data set. Numbers on each plot represent the number of top labels used to calculate the corre- sponding precision and recall. mAPL and mAPI for each method has also been reported beside its corresponding curve.  55615 samples divided into two equal-sized train and test sets. After ignoring the subset of training samples which are not annotated by any tags, we have 20033 samples in the training set. We train the classiﬁer on the user-tagged images of training data and evaluate the performance on the test set with ground-truth labels. We followed the same experimental settings as explained in Section 4. We compare the performance of deep model with different classiﬁers applied to the last layer of AlexNet. In Figure 5, we plot averaged-per-image precision and recall values when we assign n highest-ranked predictions to each image. As a secondary metric, we compare AIR’s performance with the baselines in terms of Mean Average Precision (mAP ) (Li et al., 2015) which does not depend on the top rankings, but on the full rankings for each image. We use mAPL to measure the quality of image ranking per label and mAPI to measure the quality of tag ranking per image. Robustness to noisy user-tags obtained from the auxiliary regularizer is signiﬁcant as shown in Figure 5. A few sample images and their top-5 predictions by both AIR and SVM regularizers are represented in Figure 6.  4.3 VISUALIZATION OF SELECTED AUXILIARY IMAGES  Finally, we try to understand whether the model indeed has the expected ability of retrieving in- formative images during the training process. To this end, we visualize the automatically selected auxiliary images in Figure 4, left plot. The ﬁgure refers to the experiments on the Imagenet7k data set with noisy labels with the same settings explained in Section 4.1, and displays the images whose corresponding groups were active (selected during the optimization), and also the images that were ﬁltered out (suppressed). The images in this ﬁgure are ranked based on their activation scores, ex- pecting clean images to have higher ranks and appearing on the top rows of the plot. Similar to Section 4.1, we rank images based on their activation scores obtained from SVM learned weights on the right hand side of Figure 4. Indeed, the ﬁgure shows that AIR forces the weights of noisy or non-informative images to zero and encourages the model to select clear and informative images in the training procedure much more accurately than SVM. This also explains why our proposed model is robust to the noisy-labeled training examples, as shown in the previous experiments.  4.4 SCALABILITY OF THE AUXILIARY IMAGE REGULARIZER  To reduce memory requirement for matrix F on large data sets, we can randomly select a small number of groups to be considered in AIR regularizer. When large number of data points are avail- able, randomly ignoring the groups in regularization of the response will not substantially affect the learning process that is inﬂuenced by the distribution of informative images in the feature space. To  9  0.10.20.30.40.50.60.70.80.91.0Recall0.00.10.20.30.40.50.60.7Precision123451020123451020123451020mAPI=57:2%mAPL=35:6%mAPI=46:6%mAPL=34:9%mAPI=41:7%mAPL=33:8%AlexNet+ft-last-AIRAlexNet+ft-last-SVMAlexNet+ft-last-RoLRPublished as a conference paper at ICLR 2016  Figure 6: Instance results for top-5 predictions for a few examples of NUS-WIDE-LITE data set by AIR and SVM. We illustrate true predictions (compared with ground-truth annotations) in blue, pre- dictions non-overlapping with the ground-truth labels in red, and predictions looking as reasonable annotations but not in the ground-truth labels in green.  verify this point, we repeat the experiment on CIFAR-10 data set with 50% synthetic noise level but with only 1% of the groups used in the regularization term. This signiﬁcant memory reduction will only drop the accuracy by 0.5% from 72.3% to 71.8%. This experiment shows sampling from the groups does not reduce the ﬁnal classiﬁcation accuracy considerably in the case of large data sets but saves the memory cost signiﬁcantly.  5 SUMMARY AND FUTURE WORK  We introduced a new regularizer that uses overlapping group norms for deep CNN models to im- prove image classiﬁcation accuracy when training labels are noisy. This regularizer is adaptive, in that it automatically incorporates inherent structure in the image data. Our experiments demon- strated that the regularized model performs well for both synthetic and real noisy labels: it leads to a substantial enhancement in performance on the benchmark data sets when compared with standard models. In the future, we will explore the effect of AIR on robustifying the classiﬁer in an end-to- end scheme where the error information from the auxiliary regularizer will back-propagate through the inner layers of deep model to produce robust image representations.  10  Published as a conference paper at ICLR 2016  REFERENCES Azadi, Samaneh and Sra, Suvrit. Towards an optimal stochastic alternating direction method of multipliers. In Proceedings of the 31st International Conference on Machine Learning, pp. 620– 628, 2014.  Bach, Francis, Jenatton, Rodolphe, Mairal, Julien, Obozinski, Guillaume, et al. Convex optimization  with sparsity-inducing norms. Optimization for Machine Learning, pp. 19–53, 2011.  Bach, Francis, Jenatton, Rodolphe, Mairal, Julien, and Obozinski, Guillaume. Optimization with sparsity-inducing penalties. Foundations and Trends R(cid:13) in Machine Learning, 4(1):1–106, 2012. Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and Eckstein, Jonathan. Distributed opti- mization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Machine Learning, 3(1):1–122, 2011.  Chua, Tat-Seng, Tang, Jinhui, Hong, Richang, Li, Haojie, Luo, Zhiping, and Zheng, Yan-Tao. Nus- wide: A real-world web image database from national university of singapore. In Proc. of ACM Conf. on Image and Video Retrieval (CIVR’09), Santorini, Greece., July 8-10, 2009.  Deng, Jia, Berg, Alexander C, Li, Kai, and Fei-Fei, Li. What does classifying more than 10,000  image categories tell us? In Computer Vision–ECCV 2010, pp. 71–84. Springer, 2010.  Feng, Jiashi, Xu, Huan, Mannor, Shie, and Yan, Shuicheng. Robust logistic regression and classiﬁ-  cation. In Advances in Neural Information Processing Systems, pp. 253–261, 2014.  Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. A note on the group lasso and a sparse  group lasso. arXiv preprint arXiv:1001.0736, 2010.  Gabay, Daniel and Mercier, Bertrand. A dual algorithm for the solution of nonlinear variational problems via ﬁnite element approximation. Computers & Mathematics with Applications, 2(1): 17–40, 1976.  Glowinski, Roland and Marroco, A. Sur l’approximation, par elements ﬁnis d’ordre un, et la resolu- tion, par penalisation-dualite d’une classe de problemes de dirichlet non lineaires. ESAIM: Math- ematical Modelling and Numerical Analysis-Mod´elisation Math´ematique et Analyse Num´erique, 9(R2):41–76, 1975.  Izadinia, Hamid, Farhadi, Ali, Hertzmann, Aaron, and Hoffman, Matthew D. Image classiﬁcation  and retrieval from user-supplied tags. arXiv preprint arXiv:1411.6909, 2014.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Krizhevsky, Alex. cuda-convnet. URL https://code.google.com/p/cuda-convnet/.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Li, Xirong, Uricchio, Tiberio, Ballan, Lamberto, Bertini, Marco, Cees, Snoek, and Bimbo, Al- berto Del. Socializing the semantic gap: A comparative survey on image tag assignment, reﬁne- ment and retrieval. http://arxiv.org/pdf/1503.08248v2.pdf, 2015.  Lions, Pierre-Louis and Mercier, Bertrand. Splitting algorithms for the sum of two nonlinear opera-  tors. SIAM Journal on Numerical Analysis, 16(6):964–979, 1979.  Mairal, Julien, Ponce, Jean, Sapiro, Guillermo, Zisserman, Andrew, and Bach, Francis R. Super- vised dictionary learning. In Advances in neural information processing systems, pp. 1033–1040, 2009.  Nguyen, Anh, Yosinski, Jason, and Clune, Jeff. Deep neural networks are easily fooled: High  conﬁdence predictions for unrecognizable images. arXiv preprint arXiv:1412.1897, 2014.  11  Published as a conference paper at ICLR 2016  Ouyang, Hua, He, Niao, Tran, Long, and Gray, Alexander. Stochastic alternating direction method of multipliers. In Proceedings of the 30th International Conference on Machine Learning, pp. 80–88, 2013.  Sukhbaatar, Sainbayar and Fergus, Rob. Learning from noisy labels with deep neural networks.  arXiv preprint arXiv:1406.2080, 2014.  Sukhbaatar, Sainbayar, Bruna, Joan, Paluri, Manohar, Bourdev, Lubomir, and Fergus, Rob. Training  convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2015.  Tibshirani, Robert. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical  Society. Series B (Methodological), pp. 267–288, 1996.  Van der Maaten, Laurens and Hinton, Geoffrey. Visualizing data using t-sne. Journal of Machine  Learning Research, 9(2579-2605):85, 2008.  Xiao, Tong, Xia, Tian, Yang, Yi, Huang, Chang, and Wang, Xiaogang. Learning from massive noisy labeled data for image classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2691–2699, 2015.  Yang, Jianchao, Yu, Kai, Gong, Yihong, and Huang, Thomas. Linear spatial pyramid matching using sparse coding for image classiﬁcation. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 1794–1801. IEEE, 2009.  Yogatama, Dani and Smith, Noah. Making the most of bag of words: Sentence regularization with alternating direction method of multipliers. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 656–664, 2014.  Zou, Hui and Hastie, Trevor. Regularization and variable selection via the elastic net. Journal of the  Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005.  12  ",
1511.06295,2016,Policy Distillation,"['Policy Distillation\nAndrei Rusu', 'Sergio Gomez', 'Caglar Gulcehre', 'Guillaume Desjardins', 'James Kirkpatrick', 'Razvan Pascanu', 'Volodymyr Mnih', 'Koray Kavukcuoglu', 'Raia Hadsell']",https://arxiv.org/pdf/1511.06295,"6 1 0 2     n a J    7      ]  G L . s c [      2 v 5 9 2 6 0  .  1 1 5 1 : v i X r a  Under review as a conference paper at ICLR 2016  POLICY DISTILLATION  Andrei A. Rusu, Sergio G´omez Colmenarejo, C¸ a˘glar G¨ulc¸ehre∗, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu & Raia Hadsell Google DeepMind London, UK {andreirusu, sergomez, gdesjardins, kirkpatrick, razp, vmnih, korayk, raia}@google.com, gulcehrc@iro.umontreal.ca  ABSTRACT  Policies for complex visual tasks have been successfully learned with deep rein- forcement learning, using an approach called deep Q-networks (DQN), but rela- tively large (task-speciﬁc) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy dis- tillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramati- cally smaller and more efﬁcient. Furthermore, the same method can be used to consolidate multiple task-speciﬁc policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.  1  INTRODUCTION  Recently, advances in deep reinforcement learning have shown that policies can be encoded through end-to-end learning from reward signals, and that these pixel-to-action policies can deliver super- human performance on many challenging tasks (Mnih et al., 2015). The deep Q-network (DQN) algorithm interacts with an environment, receiving pixel observations and rewards. At each step, an agent chooses the action that maximizes its predicted cumulative reward, and a convolutional network is trained to approximate the optimal action-value function. The DQN algorithm requires long training times to train on a single task. In this paper, we introduce policy distillation for transferring one or more action policies from Q-networks to an untrained network. The method has multiple advantages: network size can be compressed by up to 15 times without degradation in performance; multiple expert policies can be combined into a single multi-task policy that can outperform the original experts; and ﬁnally it can be applied as a real-time, online learning process by continually distilling the best policy to a target network, thus efﬁciently tracking the evolving Q-learning policy. The contribution of this work is to describe and discuss the policy distillation approach and to demonstrate results on (a) single game distillation, (b) single game distillation with highly compressed models, (c) multi-game distillation, and (d) online distillation. Distillation was ﬁrst presented as an efﬁcient means for supervised model compression (Bucila et al., 2006), and it has since been extended to the problem of creating a single network from an ensemble model (Hinton et al., 2014). It also shows merit as an optimization method that acts to stabilize learning over large datasets or in dynamic domains (Shalev-Shwartz, 2014). It uses supervised regression to train a target network to produce the same output distribution as the original network, often using a less peaked, or ‘softened’ target distribution. We show that distillation can also be used in the context of reinforcement learning (RL), a signiﬁcant discovery that belies the commonly held belief that supervised learning cannot generalize to sequential prediction tasks (Barto and Dietterich, 2004). Distillation has been traditionally applied to networks whose outputs represent class probabilities. In reinforcement learning, however, the neural network encodes action values, which are real-valued and unbounded and whose scale depends on the expected future rewards in the game. They are  ∗While interning at Google DeepMind. Other afﬁliation: Universit´e de Montr´eal, Montr´eal, Canada.  1  Under review as a conference paper at ICLR 2016  be blurred and non-discriminative when multiple actions have similar consequences but sharp and discriminative when actions are important. These traits make distillation difﬁcult to apply.  2 PREVIOUS WORK  This work is related to four different research areas: model compression using distillation, deep reinforcement learning, multi-task learning and imitation learning. The concept of model compres- sion through training a student network using the outputs of a teacher network was ﬁrst suggested by Bucila et al. (2006), who proposed it as a means of compressing a large ensemble model into a single network. In an extension of this work, Ba and Caruana (2014) used compression to transfer knowledge from a deep network to a shallow network. Other authors applied the same concept in somewhat different ways: Liang et al. (2008) proposed an approach for training a fast logistic re- gression model using data labeled by a slower structured-output CRF model; Menke and Martinez (2009) used model transfer as a regularization technique. Hinton et al. (2014) introduced the term distillation and suggested raising the temperature of the softmax distribution in order to transfer more knowledge from teacher to student network. Distillation has since been applied in various ways (Li et al., 2014; Romero et al., 2014; Chan et al., 2015; Wang et al., 2015; Tang et al., 2015), however it has not been applied to sequential prediction or reinforcement learning problems. In reinforcement learning, several approaches have been proposed to learn a policy by regression to a teacher’s signal, which is often referred to as imitation learning. Often, the teacher signal comes from a model-based algorithm, for example in regret-based approximate policy iteration (Lazaric et al., 2010) or by using Monte Carlo tree search as an oracle Guo et al. (2014). In the latter case it was shown that superhuman Atari scores could be achieved by regressing to the policy suggested by a UCT (Kocsis and Szepesv´ari, 2006) algorithm. This work is related to ours, but it requires a model of the game which has access to the true state, rather than learning directly from observations. The classiﬁcation-based policy iteration (CAPI) framework (Farahmand et al., 2012) is another approach to imitation learning which does not require a model-based teacher. It is possbile to view a single iteration of CAPI as policy distillation using a particular loss function (i.e. weighing classiﬁcation of actions by the action gap). Another algorithm to tackle imitation learning is DAGGER Ross et al. (2010). In DAGGER the student policy generates some of the training trajectories, whereas in this work the trajectories are entirely produced by the teacher policy. Multi-task learning (Caruana, 1997) is often described as a method for improving generalization performance by leveraging a fairly limited number of similar tasks as a shared source of inductive bias. Typically, such tasks need to be deﬁned on the same input distribution. Although Atari games share a common input modality, their images are very diverse and do not share a common statistical basis (as opposed to natural images), making multi-task learning much more difﬁcult. We show that model compression and distillation can alleviate such issues.  3 APPROACH  Before describing policy distillation, we will ﬁrst give a brief review of deep Q-learning, since DQN serves as both the baseline for performance comparisons as well as the teacher for the policy distillation. Note that the proposed method is not tied to DQN and can be applied to models trained using other RL algorithms. After the DQN summary, we will describe policy distillation for single and multiple tasks.  3.1 DEEP Q-LEARNING  DQN is a state-of-the-art model-free approach to reinforcement learning using deep networks, in environments with discrete action choices, which has achieved super-human performance on a large collection of diverse Atari 2600 games (Mnih et al., 2015). In deep Q-learning, a neural net- work is optimized to predict the average discounted future return of each possible action given a small number of consecutive observations. The action with the highest predicted return is cho- sen by the agent. Thus, given an environment E whose interface at timestep i comprises actions ai ∈ A = {1, ..., K}, observations xi ∈ Rd, and rewards ri ∈ R, we deﬁne a sequence t(cid:48)=t γt(cid:48)−trt.  st = x1, a1, x2, a2, ..., at−1, xt and a future return at time t with discount γ: Rt =(cid:80)T  2  Under review as a conference paper at ICLR 2016  (cid:20)(cid:16)  (cid:17)2(cid:21)  .  The Q function gives the maximum expected return after seeing sequence s and choosing action a: Q∗(s, a) = maxπ E[Rt|st = s, at = a, π], where π is an action policy, a mapping from sequences to actions. In order to train a convolutional neural net to approximate Q∗(s, a), DQN minimizes the following loss, using samples (s, a, r, s(cid:48)) drawn from a replay memory:  Li(θi) = E(s,a,r,s(cid:48))∼U (D)  r + γ max  a(cid:48) Q(s(cid:48), a(cid:48); θ−  i ) − Q(s, a; θi)  The use of a replay memory to decorrelate samples is a critical element of DQN, as is the use of a target network, an older version of the parameters (θ− i ). Both mechanisms help to stabilize learning.  3.2 SINGLE-GAME POLICY DISTILLATION  Distillation is a method to transfer knowledge from a teacher model T to a student model S. The distillation targets from a classiﬁcation network are typically obtained by passing the weighted sums of the last network layer through a softmax function. Figure 1 illustrates this with examples from two Atari games, and Figure 2(a) depicts the distillation process. In order to transfer more of the knowledge of the network, the teacher outputs can be softened by passing the network output through a relaxed (higher temperature) softmax than the one that was used for training. For a selected temperature τ, the new teacher outputs are thus given by softmax( qT τ ), where qT is the vector of Q-values of T . These can be learned by S using regression.  Figure 1: Example frames from two Atari games, with the Q-values output by DQN (top) and the distillation targets after softmax (middle). For Pong, the two frames only differ by a few pixels yet the Q-values are different. In the Space Invaders example, the input frames are very different yet the Q-values are very similar. In both games the softmax sharpens the targets, making it easier for the student to learn.  In the case of transferring a Q-function rather than a classiﬁer, however, predicting Q-values of all actions given the observation is a difﬁcult regression task. For one, the scale of the Q-values may be hard to learn because it is not bounded and can be quite unstable. Further, it is computationally challenging in general to compute the action values of a ﬁxed policy because it implies solving the Q-value evaluation problem. On the other hand, training S to predict only the single best action is also problematic, since there may be multiple actions with similar Q-values. In all cases To test this intuition we consider three methods of policy distillation from T to S. we assume that the teacher T has been used to generate a dataset DT = {(si, qi)}N i=0, where each sample consists of a short observation sequence si and a vector qi of unnormalized Q-values with one value per action. The ﬁrst method uses only the highest valued action from the teacher, ai,best = argmax(qi), and the student model is trained with a negative log likelihood loss (NLL) to predict the same action:  LNLL(DT , θS) = −  log P (ai = ai,best|xi, θS)  |D|(cid:88)  i=1  3  1.20	1.25	1.30	1.35	Teacher	Q-values	No-op	Up	Down	0.00	0.50	1.00	Dist-KL	Targets	10.00	10.50	11.00	Teacher	Q-values	No-op	Fire	Right	Le9	Right	+	Fire	Le9	+	Fire	0.00	0.50	1.00	Dist-KL	Targets	Under review as a conference paper at ICLR 2016  In the second case, we train using a mean-squared-error loss (MSE). The advantage of this objective is that it preserves the full set of action-values in the resulting student model. In this loss, qT and qS are the vectors of Q-values from the teacher and student networks respectively.  LM SE(DT , θS) =  ||qT  i − qS  i ||2 2.  |D|(cid:88)  i=1  In the third case, we adopt the distillation setup of Hinton et al. (2014) and use the Kullback-Leibler divergence (KL) with temperature τ:  LKL(DT , θS) =  softmax(  qT i τ  ) ln  i  softmax( qT τ ) softmax(qS i )  |D|(cid:88)  i=1  In the traditional classiﬁcation setting, the output distribution of qT is very peaked, so softening the distribution by raising the temperature of the softmax allows more of the secondary knowledge to be transferred to the student. In the case of policy distillation, however, the outputs of the teacher are not a distribution, rather they are the expected future discounted reward of each possible action. Rather than soften these targets, we expect that we may need to make them sharper.  3.3 MULTI-TASK POLICY DISTILLATION  Figure 2: (a) Single-task data collection and policy distillation. The DQN agent periodically adds gameplay to the replay memory while the student network is trained. (b) Multi-task data collection and policy distillation.  The approach for multi-task policy distillation, illustrated in Figure 2(b), is straightforward. We use n DQN single-game experts, each trained separately. These agents produce inputs and targets, just as with single-game distillation, and the data is stored in separate memory buffers. The distillation agent then learns from the n data stores sequentially, switching to a different one every episode. Since different tasks often have different action sets, a separate output layer (called the controller layer) is trained for each task and the id of the task is used to switch to the correct output during both training and evaluation. We also experiment with both the KL and NLL distillation loss functions for multi-task learning. An important contribution of this work is to compare the performance of multi-task DQN agents with multi-task distillation agents. For multi-task DQN, the approach is similar to single-game learning: the network is optimized to predict the average discounted return of each possible action given a small number of consecutive observations. Like in multi-task distillation, the game is switched every episode, separate replay memory buffers are maintained for each task, and training is evenly interleaved between all tasks. The game label is used to switch between different output layers as in multi-task DQN, thus enabling a different output layer, or controller, for each game. With this architecture in place, the multi-task DQN loss function remains identical to single-task learning. Even with the separate controllers, multi-game DQN learning is extremely challenging for Atari games and DQN generally fails to reach full single-game performance on the games. We believe this is due to interference between the different policies, different reward scaling, and the inherent instability of learning value functions. Policy distillation may offer a means of combining multiple policies into a single network without the damaging interference and scaling problems. Since policies are compressed and reﬁned during the distillation process, we surmise that they may also be more effectively combined into a sin- gle network. Also, policies are inherently lower variance than value functions, which should help performance and stability (Greensmith et al., 2004).  4  Online Data CollectionSupervised Policy TrainingDQN (Teacher)TargetOutputPolicy Net (Student)PredictedOutputInput State &Game LabelSupervisionDistillation LossReplay MemoryInput States,Game Labels &Target OutputsOnline Data CollectionSupervised Policy TrainingReplay Memory NReplay Memory 2Pong DQN (Teacher 1)TargetOutputPolicy Net (Student)PredictedOutputInput States,Game Labels &Target OutputsInput State &Game LabelSupervisionDistillation LossReplay Memory 1Freeway DQN (Teacher 2)Qbert DQN (Teacher N)[…][…]Under review as a conference paper at ICLR 2016  4 RESULTS AND DISCUSSION  A brief overview of the training and evaluation setup is given below; complete details are in Ap- pendix A.  4.1 TRAINING AND EVALUATION  Single task policy distillation is a process of data generation by the teacher network (a trained DQN agent) and supervised training by the student network, as illustrated in Figure 2(a). For each game we trained a separate DQN agent, as reported in Mnih et al. (2015). Each agent was subsequently ﬁxed (no Q-learning) and used as a teacher for a single student policy network. The DQN teacher’s outputs (Q-values for all actions) alongside the inputs (images) were held in a buffer. We employed a similar training procedure for multi-task policy distillation, as shown in Figure 2(b). The network used to train the DQN agents is described in (Mnih et al., 2015). The same network was used for the student, except for the compression experiments which scaled down the number of units in each layer. A larger network (four times more parameters, with an additional fully connected layer) was used to train on multi-task distillation with 10 games. The multi-task networks had a separate MLP output (controller) layer for each task. See AppendixA for full details of training procedure and networks. Because many Atari games are highly deterministic, a learner could potentially memorize and re- produce action sequences from a few starting points. To rigorously test the generalization capability of both DQN teachers and distilled agents we followed the evaluation techniques introduced by Nair et al. (2015) and adopted by subsequent research (van Hasselt et al.), in which professional human expert play was used to generate starting states for each game. Any points accumulated by the human expert until that time were discarded, thus the agent scores are not directly comparable to null-op evaluations previously reported. Since the agent is not in control of the distribution over starting states, nor do we generate any training data using human trajectories, we assert that high scores imply good levels of generalization. Ten popular Atari games were selected and ﬁxed before starting this research. These particular games were chosen in order to sample the diverse levels of DQN performance seen on the full collection, from super-human play (e.g. Breakout, Space Invaders) to below human level (Q*bert, Ms.Pacman).  4.2 SINGLE-GAME POLICY DISTILLATION RESULTS  In this section we show that the Kullback-Leibler (KL) cost function leads to the best-performing student agents, and that these distilled agents outperform their DQN teachers on most games. Table 1 compares the effectiveness of different policy distillation cost functions in terms of generalization performance on four Atari games, while keeping the same network architecture as DQN. Only four games were used for this experiment in order to establish parameters for the loss functions which are then ﬁxed across other experiments (which use ten games). Note that the evaluation uses human starting points to robustly test generalization (see Section 4.1).  Table 1: Comparison of learning criteria used for policy distillation from DQN teachers to students with identi- cal network architectures: MSE (mean squared error), NLL (negative log likelihood), and KL (Kullback-Leibler divergence). Best relative scores are outlined in bold.  DQN score 303.9 25.8 16.2 4589.8  Breakout Freeway Pong Q*bert  Dist-KL  Dist-NLL  Dist-MSE score %DQN score %DQN score %DQN 102.9 25.7 15.3 5607.3  235.9 26.2 15.4 6773.5  287.8 26.7 16.3 7112.8  94.7 103.5 100.9 155.0  33.9 99.4 94.4 122.2  77.6 101.4 94.9 147.6  Students trained with a MSE loss performed worse than KL or NLL, even though we are successfully minimizing the squared error. This is not surprising considering that greedy action choices can be made based on very small differences in Q-values, which receive low weight in the MSE cost. Such  5  Under review as a conference paper at ICLR 2016  situations are not uncommon in Atari games. Mean discounted future returns are very similar in a large number of states when coupled with control at a ﬁne temporal resolution or very sparse rewards. This is an intrinsic property of Q-functions, which, coupled with residual errors of non- linear function approximation during DQN training, make MSE a poor choice of loss function for policy distillation. At the other end of the spectrum, using the NLL loss assumes that a single action choice is correct at any point in time, which is not wrong in principle, since any optimal policy is always deterministic if rewards are not stochastic. However, without an optimal teacher, minimizing the NLL could amplify the noise inherent in the teacher’s learning process. Passing Q-values through a softmax function with a temperature parameter and minimizing the KL divergence cost strikes a convenient balance between these two extremes. We determine empirically that a low temperature τ = 0.01 is best suited for distillation in this domain. Given the performance of the KL loss, we did not experiment with other possibilities, such as combining the NLL and MSE criteria.  4.3 POLICY DISTILLATION WITH MODEL COMPRESSION  In the single game setting, we also explore model compression through distillation. DQN networks are relatively large, in part due to optimization problems such as local minima that are alleviated by overcomplete models. It is also due to Q-learning, which comprises many consecutive steps of value iteration and policy improvement, thus requiring that the same deep network must represent a whole sequence of different policies before convergence. In practice DQN beneﬁts considerably from increased network capacity, but it is likely that the ﬁnal policy does not require all, or indeed, most of this capacity. We evaluate single-game distilled agents and DQN teachers using 10 different Atari games, using student networks that were signiﬁcantly smaller (25%, 7%, and 4% of the DQN network param- eters). The distilled agents which are four times smaller than DQN (Dist-KL-net1, 428,000 pa- rameters) actually outperform DQN, as shown in Figure 3. Distilled agents with 15 times fewer parameters perform on par with their DQN teachers. Even the smallest distilled agent (Dist-KL- net3, 62,000 parameters) achieves a mean of 84%. Details of the networks are given in Appendix A.  Figure 3: Scores and sizes of distilled agents, both relative to their respective DQN teachers. We report the geometric mean over 10 Atari games, with error bars showing the 95% conﬁdence interval. A detailed results table is given in Appendix B  These results suggest that DQN could beneﬁt from a reduced capacity model or regularization. How- ever, it has been found that training a smaller DQN agent results in considerably lower performance across games (Mnih et al., 2013). We speculate that training a larger network accelerates the policy iteration cycle (Sutton and Barto, 1998) of DQN. Intuitively, once DQN performs actions resulting in a high empirical return, it is essential that the values of the novel trajectory are quickly estimated. A learner with limited capacity can be very inefﬁcient at exploiting such potential, because high returns are often present in a minor fraction of its interactions with the environment. Hence, strong regularization could hinder the discovery of better policies with DQN.  6  100%	25%	7%	4%	100%	108%	102%	84%	0%	20%	40%	60%	80%	100%	120%	Teachers	(DQN)	Dist-KL-Net1	Dist-KL-Net2	Dist-KL-Net3	Percent	of	Teacher	(DQN)	Score	Agents	(diﬀerent	network	sizes)	Rela=ve	Size	Rela=ve	Score	Under review as a conference paper at ICLR 2016  Table 2: Performance of a distilled multi-task agent on 10 Atari games. The agent is a single network that achieves 89.3% of the generalization score of 10 single-task DQN teachers, computed as a geometric mean.  Beamrider Breakout Enduro Freeway Ms.Pacman Pong Q*bert Seaquest Space Invaders Riverraid Geometric Mean  DQN score 8672.4 303.9 475.6 25.8 763.5 16.2 4589.8 2793.3 1449.7 4065.3  Multi-Dist-KL score % DQN 4789.0 216.0 613.0 26.6 681.8 16.1 6098.3 4320.7 461.1 4326.8  55.2 71.1 128.9 102.9 89.3 99.6 132.9 154.7 31.8 106.4 89.3  4.4 MULTI-GAME POLICY DISTILLATION RESULTS  Figure 4: Performance of multi-task agents with identical network architecture and size, relative to respective single-task DQN teachers. A detailed results table is given in Appendix B  We train a multi-task DQN agent using the standard DQN algorithm applied to interleaved expe- rience from three games (Multi-DQN), and compare it against distilled agents (Multi-Dist) which were trained using targets from three different single-game DQN teachers (see Figure 4). All three agents are using an identical multi-controller architecture of comparable size to a single teacher net- work. About 90% of parameters are shared, with only 3 small MLP “controllers” on top which are task speciﬁc and allow for different action sets between different games. The multi-task DQN agent learns the three tasks to 83.5% of single-task DQN performance (see Figure 4). In contrast, both distilled agents perform better than their DQN teachers, with mean scores of 105.1% for Multi- Dist-NLL and 116.9% for Multi-Dist-KL. There is a ceiling effect on Freeway and Pong, since the single-task DQN teachers are virtually optimal, but we do see a considerable improvement on Q*bert, with as much as 50% higher scores for Multi-Dist-KL. We use the same approach to distill 10 Atari games into a single student network that is four times larger than a single DQN. As can be seen from Table 2, this is quite successful, with three of the games achieving much higher scores than the teacher and an overall relative performance of 89.3%. We don’t offer a comparison to a jointly trained, 10 game DQN agent, as was done for the three game set, because in our preliminary experiments DQN failed to reach higher-than-chance performance on most of the games. This highlights the challenges of multi-task reinforcement learning and supports our ﬁndings on the three game set (Figure 4).  4.5 ONLINE POLICY DISTILLATION RESULTS  As a ﬁnal contribution, we investigated online policy distillation, where the student must track the DQN teacher during Q-learning. It was not obvious whether this effort would be successful, since it  7  90%	103%	102%	74%	91%	104%	87%	124%	150%	0%	25%	50%	75%	100%	125%	150%	Mul+-DQN	Mul+-Dist-NLL	Mul+-Dist-KL	Percent	of	Teacher	(DQN)	Score	Agents	(same	network	size)	Freeway	Pong	Q*bert	Under review as a conference paper at ICLR 2016  Figure 5: Online Policy Distillation during DQN learning (pale blue) on Q*bert. The current best DQN policy to date (green) is distilled into a new network during DQN training. Showing online distillation experiments with 2 initial random seeds and the same learning rate across runs.  has been observed that the DQN policy changes dramatically throughout training as different parts of the game are explored and mastered. To test this, DQN was trained normally and the network was periodically saved if it attained a new high score. This network was then used by the distillation learner until updated by a higher DQN score. The results of this experiment are shown in Figure 5. The learning curves show the high-variance DQN score, the best-so-far score, and the score reached by two distillation agents initialized with different seeds. The distilled agent is much more stable than the DQN teacher and achieves similar or equal performance on all games (see Appendix C for additional examples of online distillation).  5 DISCUSSION  In this work we have applied distillation to policy learnt in deep Q-networks. This procedure has been used for three distinct purposes: (1) to compress policies learnt on single games in smaller models, (2) to build agents that are capable of playing multiple games, (3) to improve the stability of the DQN algorithm by distilling online the policy of the best performing agent. We have shown that in the RL setting, special care must be taken to chose the correct loss function for distillation and have observed that the best results are obtained by weighing action classiﬁcation by a soft-max of the action-gap, similarly to what is suggested by the CAPI framework Farahmand et al. (2012). Our results show that distillation can be applied to reinforcement learning, even without using an iterative approach and without allowing the student network to control the data distribution it is trained on. The fact that the distilled policy can yield better results than the teacher conﬁrms the growing body of evidence that distillation is a general principle for model regularization.  REFERENCES Jimmy Ba and Rich Caruana. Do deep nets really need to be deep?  Processing Systems (NIPS), pages 2654–2662. Curran Associates, Inc., 2014.  In Advances in Neural Information  A. G. Barto and T. G. Dietterich. Handbook of learning and approximate dynamic programming. Wiley-IEEE  Press, 2004.  Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, pages 535–541.  ACM, 2006.  Rich Caruana. Multitask learning. Mach. Learn., 28(1):41–75, July 1997.  William Chan, Nan Rosemary Ke, and Ian Lane. Transferring knowledge from a rnn to a dnn. arXiv preprint  arXiv:1504.01483, 2015.  Amir-massoud Farahmand, Doina Precup, and Mohammad Ghavamzadeh. Generalized classiﬁcation-based approximate policy iteration. In Tenth European Workshop on Reinforcement Learning (EWRL), volume 2, 2012.  8  0.00.20.40.60.81.01.2steps1e8010002000300040005000600070008000game scoreDQNBest-DQNOnline-Dist-seed=1Online-Dist-seed=2QbertUnder review as a conference paper at ICLR 2016  Philip J. Fleming and John J. Wallace. How not to lie with statistics: The correct way to summarize benchmark  results. Commun. ACM, 29(3):218–221, March 1986.  Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates  in reinforcement learning. Journal of Machine Learning Research (JMLR), pages 1471–1530, 2004.  Xiaoxiao Guo, Satinder P. Singh, Honglak Lee, Richard L. Lewis, and Xiaoshi Wang. Deep learning for real- In Advances in Neural Information  time atari game play using ofﬂine monte-carlo tree search planning. Processing Systems (NIPS), pages 3338–3346, 2014.  G. Hinton, O. Vinyals, and J. Dean. Distilling the Knowledge in a Neural Network. Deep Learning and  Representation Learning Workshop, NIPS, 2014.  Levente Kocsis and Csaba Szepesv´ari. Bandit based monte-carlo planning. In Machine Learning: ECML 2006,  pages 282–293. Springer, 2006.  Alessandro Lazaric, Mohammad Ghavamzadeh, and R´emi Munos. Analysis of a classiﬁcation-based policy iteration algorithm. In ICML-27th International Conference on Machine Learning, pages 607–614. Omni- press, 2010.  Jinyu Li, Rui Zhao, Jui-Ting Huang, and Yifan Gong. Learning small-size dnn with output-distribution-based  criteria. In Proc. Interspeech, 2014.  Percy Liang, Hal Daum III, and Dan Klein. Structure compilation: trading structure for features. In Proceedings  of International Conference on Machine Learning (ICML), 2008.  Joshua Menke and Tony Martinez.  Improving supervised learning by adapting the problem to the learner.  International Journal of Neural Systems, 19(01):1–9, 2009.  Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. Deep Learning Workshop, NIPS, 2013.  Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hass- abis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 02 2015.  Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Ko- ray Kavukcuoglu, and David Silver. Massively parallel methods for deep reinforcement learning. CoRR, abs/1507.04296, 2015.  Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio.  Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.  St´ephane Ross, Geoffrey J Gordon, and J Andrew Bagnell. A reduction of imitation learning and structured  prediction to no-regret online learning. arXiv preprint arXiv:1011.0686, 2010.  S. Shalev-Shwartz. SelﬁeBoost: A Boosting Algorithm for Deep Learning. ArXiv e-prints, November 2014.  Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA,  USA, 1st edition, 1998.  Zhiyuan Tang, Dong Wang, Yiqiao Pan, and Zhiyong Zhang. Knowledge transfer pre-training. arXiv preprint  arXiv:1506.02256, 2015.  T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent  magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-sne. Journal of Machine  Learning Research (JMLR), 2008.  Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In  Proceedings of the AAAI Conference on Artiﬁcial Intelligence.  Dong Wang, Chao Liu, Zhiyuan Tang, Zhiyong Zhang, and Mengyuan Zhao. Recurrent neural network training  with dark knowledge transfer. arXiv preprint arXiv:1505.04630, 2015.  9  Under review as a conference paper at ICLR 2016  A EXPERIMENTAL DETAILS  Policy Distillation Training Procedure Online data collection during policy distillation was performed under similar conditions to agent evaluation in Mnih et al. (2015). The DQN agent plays a random number of null-ops (up to 30) to initialize the episode, then acts greedily with respect to its Q-function, except for 5% of actions, which are chosen uniformly at random. Episodes can last up to 30 minutes of real-time play, or 108,000 frames. The small percentage of random actions leads to diverse game trajectories, which improves coverage of a game’s state space. We recorded the DQN teacher’s outputs (Q-values for all valid actions) and inputs (emulator frames) into a replay memory with a capacity of 10 hours of real-time gameplay (540,000 control steps at 15Hz). At the end of each new hour of teacher gameplay added to the replay memory we performed 10,000 minibatch updates on the student network. We used the RmsProp (Tieleman and Hinton, 2012) variation of minibatch stochastic gradient descent to train student networks. Results were robust for primary learning rates between 1.0e−4 and 1.0e−3, with maximum learning rates between 1.0e−3 and 1.0e−1. We chose hyper-parameters using preliminary experiments on 4 games. The reported results consumed 500 hours of teacher gameplay to train each student, less than 50% of the amount that was used to train each DQN teacher. Using modern GPUs we can refresh the replay memory and train the students much faster than real-time, with typical convergence in a few days. With multi-task students we used separate replay memories for each game, with the same capacity of 10 hours, and the respective DQN teachers took turns adding data. After one hour of gameplay the student is trained with 10,000 minibatch updates (each minibatch is drawn from a randomly chosen single game memory). The same 500 hour budget of gameplay was used for all but the largest network, which used 34,000 hours of experience over 10 games. Distillation Targets Using DQN outputs we have deﬁned three types of training targets that correspond to the three distillation loss functions discussed in Section3. First, the teacher’s Q-values for all actions were used directly as supervised targets; thus, training the student consisted of minimizing the mean squared error (MSE) between the student’s and teacher’s outputs for each input state. Second, we used only the teacher’s highest valued action as a one-hot target. Naturally, we minimized the negative log likelihood (NLL) loss. Finally, we passed Q-values through a softmax function whose temperature (τ = 0.01) was selected empirically from [1.0, 0.1, 0.01, 0.001]. The resulting probabilities were used as targets by minimizing the Kullback-Leibler (KL) divergence between these “sharpened” action probabilities and the corresponding output distribution pre- dicted by the student policy. We went on to use the KL cost function for a majority of reported experiments, with a ﬁxed hyper-parameter value. This choice was based on experiments described in subsection 4.2 which were performed on 4 out of the 10 games considered. Network Architectures Details of the architectures used by DQN and single-task distilled agents are given in table A1. Rectiﬁer non-linearities were added between each two consecutive layers. We used one unit for each valid action in the output layer, which was linear. A ﬁnal softmax operation was performed when distilling with NLL and KL loss functions.  Table A1: Network architectures and parameter counts of models used for single-task compression experiments.  Agent Teacher (DQN) Dist-KL-net1 Dist-KL-net2 Dist-KL-net3  Input Conv. 1 Conv. 2 Conv. 3  4 4 4 4  32 16 16 16  64 32 16 16  64 32 16 16  F.C. 1 512 256 128 64  Output up to 18 up to 18 up to 18 up to 18  Parameters 1,693,362 427,874 113,346 61,954  For compression experiments we scaled down the number of units in each layer without changing the basic architecture. The vast majority of saved parameters were in the fully connected layer on top of the convolutional stack. The distinct characteristic of all multi-task experiments was the use of different MLP “controller” networks for each game, on top of shared representations. The speciﬁc details of these architectures are given in table A2. All results reported on 3 games used identical models of similar size with a single DQN teacher. A network 4 times larger than a teacher was trained using multi-task distillation on 10 games. Agent Evaluation Professional human expert play was used to generate starting states for each game by sam- pling 100 random positions which occurred in the ﬁrst 20% of each episode’s length. Agents are allowed to act for 30 minutes of real-time gameplay, or 108,000 frames, and they use a high value of (cid:15) equal to 5%. We also do not compute a generalization score until the agent’s training process has ended. Evaluating the performance of a multi-task agent is not trivial. Since each game has a different reward structure and somewhat arbitrary choice of reward scale, it is meaningless to compute an arithmetic mean of scores across games. Therefore, DQN generalization scores (published previously (van Hasselt et al.; Nair et al., 2015)) are  10  Under review as a conference paper at ICLR 2016  Table A2: Network architectures and parameter counts of models used for multi-task distillation experiments.  Agent One Teacher (DQN) Multi-DQN/Dist (3 games) Multi-Dist-KL (10 games)  Input Conv. 1 Conv. 2 Conv. 3  4 4 4  32 32 64  64 64 64  64 64 64  F.C. 1 512 512 1500  F.C. 2 n/a  Output up to 18  128 (x3) 128 (x10)  up to 18 (x3) up to 18 (x10)  Parameters 1,693,362 1,882,668 6,756,721  taken as a reference point and student scores are reported as a relative percentage. This way, performance on multiple games can be measured using the geometric mean (Fleming and Wallace, 1986).  B SUPPORTING TABLES FOR POLICY DISTILLATION FIGURES  Table B1: Performance of single-task compressed networks on 10 Atari games. Best relative scores are outlined in bold.  DQN score 8672.4 303.9 475.6 25.8 763.5 16.2 4589.8 4065.3 2793.3 1449.7  Beamrider Breakout Enduro Freeway Ms.Pacman Pong Q*bert Riverraid Seaquest Space Invaders Geometric Mean  Dist-KL-net2  Dist-KL-net3  Dist-KL-net1 score % DQN score % DQN score % DQN 7552.8 321.0 677.9 26.7 782.5 16.3 5947.3 4442.7 3986.6 1140.0  7393.3 298.2 672.2 26.7 659.9 16.8 5994.0 4175.3 4567.1 716.1  6521.2 238.8 556.7 26.7 734.3 15.7 4952.3 3417.9 3838.3 302.3  87.1 105.6 142.5 103.5 102.5 100.6 129.6 109.3 142.7 78.6 108.3  85.3 98.1 141.3 103.5 86.4 103.7 130.6 102.7 163.5 49.4 101.7  75.2 78.6 117.1 103.5 96.2 96.9 107.9 84.1 137.4 20.9 83.9  Table B2: Performance of multi-task distilled agents on 3 Atari games. Best relative scores are outlined in bold.  DQN score 25.8 16.2 4589.8  Freeway Pong Q*bert Geometric Mean  Multi-Dist-NLL  Multi-DQN score % DQN score % DQN score % DQN 23.3 12.0 3987.3  26.5 14.8 5678.0  26.3 16.9 6890.3  Multi-Dist-KL  102.0 104.4 150.1 116.9  102.7 91.4 123.7 105.1  90.3 74.1 86.9 83.5  11  Under review as a conference paper at ICLR 2016  C ADDITIONAL RESULTS USING ONLINE POLICY DISTILLATION  Figure C1: Online Policy Distillation during DQN learning (pale blue). The current best DQN policy to date (green) is distilled into a new network during DQN training. Showing online distillation experiments on 2 games, with 2 initial random seeds and the same learning rate across all runs.  D VISUALIZATION OF REPRESENTATION OVER 10 ATARI GAMES  Figure D2: Shared embeddings learned using Multi-Dist-KL on data from 10 Atari games. Showing t-SNE vi- sualizations (van der Maaten and Hinton, 2008) of activations in the ﬁrst convolutional and last fully connected layers.  A visual interpretation of the representation learned by the multi-task distillation is given in Figure D2, where t-SNE embeddings of network activations from 10 different games are plotted with distinct colors. The embed- ding on the left suggests that statistics of low level representations (layer 1) may be game-speciﬁc, although  12  0.00.20.40.60.81.0steps1e8201001020game scoreDQNBest-DQNOnline-Dist-seed=1Online-Dist-seed=2Pong0.00.20.40.60.81.0steps1e8050100150200250300350game scoreDQNBest-DQNOnline-Dist-seed=1Online-Dist-seed=2Breakoutt-SNE embedding of conv. layer 1 activationst-SNE embedding of last shared layer activationsAtari Games:+ Breamrider+ Q*bert+ Enduro+ Breakout+ Pong+ Seaquest+ Freeway+ Ms. Pacman+ Space Invaders+ RiverraidUnder review as a conference paper at ICLR 2016  this may be due to the diversity of the inputs. The second embedding characterizes shared representations at the ﬁnal network layer. While activations are still game-speciﬁc, we observe higher within-game variance of representations, which probably reﬂect output statistics.  13  ",
1511.06392,2016,Neural Random-Access Machines,"['Neural Random-Access Machines\nKarol Kurach', 'Marcin Andrychowicz', 'Ilya Sutskever']",https://arxiv.org/pdf/1511.06392,"6 1 0 2     b e F 9         ]  G L . s c [      3 v 2 9 3 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  NEURAL RANDOM-ACCESS MACHINES  Karol Kurach∗ & Marcin Andrychowicz∗ & Ilya Sutskever Google {kkurach,marcina,ilyasu}@google.com  ABSTRACT  In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whose so- lutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. More- over, memory access during inference can be done in a constant time under some assumptions.  1  INTRODUCTION  Deep learning is successful for two reasons. First, deep neural networks are able to represent the “right” kind of functions; second, deep neural networks are trainable. Deep neural networks can be potentially improved if they get deeper and have fewer parameters, while maintaining train- ability. By doing so, we move closer towards a practical implementation of Solomonoff induc- tion (Solomonoff, 1964). The ﬁrst model that we know of that attempted to train extremely deep networks with a large memory and few parameters is the Neural Turing Machine (NTM) (Graves et al., 2014) — a computationally universal deep neural network that is trainable with backprop- agation. Other models with this property include variants of Stack-Augmented recurrent neural networks (Joulin & Mikolov, 2015; Grefenstette et al., 2015), and the Grid-LSTM (Kalchbrenner et al., 2015)—of which the Grid-LSTM has achieved the greatest success on both synthetic and real tasks. The key characteristic of these models is that their depth, the size of their short term memory, and their number of parameters are no longer confounded and can be altered independently — which stands in contrast to models like the LSTM (Hochreiter & Schmidhuber, 1997), whose number of parameters grows quadratically with the size of their short term memory. A fundamental operation of modern computers is pointer manipulation and dereferencing. In this work, we investigate a model class that we name the Neural Random-Access Machine (NRAM), which is a neural network that has, as primitive operations, the ability to manipulate, store in mem- ory, and dereference pointers into its working memory. By providing our model with dereferencing as a primitive, it becomes possible to train models on problems whose solutions require pointer manipulation and chasing. Although all computationally universal neural networks are equivalent, which means that the NRAM model does not have a representational advantage over other models if they are given a sufﬁcient number of computational steps, in practice, the number of timesteps that a given model has is highly limited, as extremely deep models are very difﬁcult to train. As a result, the model’s core primitives have a strong effect on the set of functions that can be feasibly learned in practice, similarly to the way in which the choice of a programming language strongly affects the functions that can be implemented with an extremely small amount of code. Finally, the usefulness of computationally-universal neural networks depends entirely on the ability of backpropagation to ﬁnd good settings of their parameters. Indeed, it is trivial to deﬁne the “op- timal” hypothesis class (Solomonoff, 1964), but the problem of ﬁnding the best (or even a good)  ∗Equal contribution.  1  Published as a conference paper at ICLR 2016  function in that class is intractable. Our work puts the backpropagation algorithm to another test, where the model is extremely deep and intricate. In our experiments, we evaluate our model on several algorithmic problems whose solutions required pointer manipulation and chasing. These problems include algorithms on a linked-list and a binary tree. While we were able to achieve encouraging results on these problems, we found that standard optimization algorithms struggle with these extremely deep and nonlinear models. We believe that advances in optimization methods will likely lead to better results.  2 RELATED WORK  There has been a signiﬁcant interest in the problem of learning algorithms in the past few years. The most relevant recent paper is Neural Turing Machines (NTMs) (Graves et al., 2014). It was the ﬁrst paper to explicitly suggest the notion that it is worth training a computationally universal neural network, and achieved encouraging results. A follow-up model that had the goal of learning algorithms was the Stack-Augmented Recurrent Neural Network (Joulin & Mikolov, 2015) This work demonstrated that the Stack-Augmented RNN can generalize to long problem instances from short problem instances. A related model is the Reinforcement Learning Neural Turing Machine (Zaremba & Sutskever, 2015), which attempted to use reinforcement learning techniques to train a discrete-continuous hybrid model. The memory network (Weston et al., 2014) is an early model that attempted to explicitly separate the memory from computation in a neural network model. The followup work of Sukhbaatar et al. (2015) combined the memory network with the soft attention mechanism, which allowed it to be trained with less supervision. The Grid-LSTM (Kalchbrenner et al., 2015) is a highly interesting extension of LSTM, which allows to use LSTM cells for both deep and sequential computation. It achieves excellent results on both synthetic, algorithmic problems and on real tasks, such as language modelling, machine translation, and object recognition. The Pointer Network (Vinyals et al., 2015) is somewhat different from the above models in that it does not have a writable memory — it is more similar to the attention model of Bahdanau et al. (2014) in this regard. Despite not having a memory, this model was able to solve a number of difﬁ- cult algorithmic problems that include the convex hull and the approximate 2D travelling salesman problem (TSP). Finally, it is important to mention the attention model of Bahdanau et al. (2014). Although this work is not explicitly aimed at learning algorithms, it is by far the most practical model that has an “algorithmic bent”. Indeed, this model has proven to be highly versatile, and variants of this model have achieved state-of-the-art results on machine translation (Luong et al., 2015), speech recognition (Chan et al., 2015), and syntactic parsing (Vinyals et al., 2014), without the use of almost any domain-speciﬁc tuning.  3 MODEL  In this section we describe the NRAM model. We start with a description of the simpliﬁed version of our model which does not use an external memory and then explain how to augment it with a variable-size random-access memory. The core part of the model is a neural controller, which acts as a “processor”. The controller can be a feedforward neural network or an LSTM, and it is the only trainable part of the model. The model contains R registers, each of which holds an integer value. To make our model trainable with gradient descent, we made it fully differentiable. Hence, each register represents an integer value with a distribution over the set {0, 1, . . . , M − 1}, for some constant M. We do not assume that these distributions have any special form — they are simply stored as vectors p ∈ RM satisfying i pi = 1. The controller does not have direct access to the registers; it can interact with them using a number of prespeciﬁed modules (gates), such as integer addition or equality test.  pi ≥ 0 and(cid:80)  2  Published as a conference paper at ICLR 2016  Let’s denote the modules m1, m2, . . . , mQ, where each module is a function:  mi : {0, 1, . . . , M − 1} × {0, 1, . . . , M − 1} → {0, 1, . . . , M − 1}.  On a high level, the model performs a sequence of timesteps, each of which consists of the following substeps:  1. The controller gets some inputs depending on the values of the registers (the controller’s  inputs are described in Sec. 3.1).  2. The controller updates its internal state (if the controller is an LSTM). 3. The controller outputs the description of a “fuzzy circuit” with inputs r1, . . . , rR, gates  m1, . . . , mQ and R outputs.  4. The values of the registers are overwritten with the outputs of the circuit.  More precisely, each circuit is created as follows. The inputs for the module mi are chosen by the controller from the set {r1, . . . , rR, o1, . . . , oi−1}, where:  • rj is the value stored in the j-th register at the current timestep, and • oj is the output of the module mj at the current timestep.  for each 1 ≤ i ≤ Q the controller chooses weighted averages of the values  Hence, {r1, . . . , rR, o1, . . . , oi−1} which are given as inputs to the module. Therefore,  (cid:0)(r1, . . . , rR, o1, . . . , oi−1)T softmax(ai), (r1, . . . , rR, o1, . . . , oi−1)T softmax(bi)(cid:1) ,  oi = mi where the vectors ai, bi ∈ RR+i−1 are produced by the controller (Fig. 1).  (1)  outputs of previous modules  registers  r1  . . .  rR  o1  . . . oi−1  LSTM  ai  bi  s-m  s-m  (cid:104)·,·(cid:105)  (cid:104)·,·(cid:105)  mi  oi  Figure 1: The execution of the module mi. Gates s-m represent the softmax function and (cid:104)·,·(cid:105) denotes inner product. See Eq. 1 for details.  Recall that the variables rj represent probability distributions and therefore the inputs to mi, be- ing weighted averages of probability distributions, are also probability distributions. Thus, as the modules mi are originally deﬁned for integer inputs and outputs, we must extend their domain to probability distributions as inputs, which can be done in a natural way (and make their output also be a probability distribution):  ∀0≤c<M P (mi(A, B) = c) =  P(A = a)P(B = b)[mi(a, b) = c].  (2)  (cid:88)  0≤a,b<M  After the modules have produced their outputs, the controller decides which of the values {r1, . . . , rR, o1, . . . , oQ} should be stored in the registers. In detail, the controller outputs the vec- tors ci ∈ RR+Q for 1 ≤ i ≤ R and the values of the registers are updated (simultaneously) using the formula:  ri := (r1, . . . , rR, o1, . . . , oQ)T softmax(ci).  (3)  3  Published as a conference paper at ICLR 2016  3.1 CONTROLLER’S INPUTS  Recall that at the beginning of each timestep the controller receives some inputs, and it is an im- portant design decision to decide where should these inputs come from. A naive approach is to use the values of the registers as inputs to the controller. However, the values of the registers are probability distributions and are stored as vectors p ∈ RM . If the entire distributions were given as inputs to the controller then the number of the model’s parameters would depend on M. This would be undesirable because, as will be explained in the next section, the value M is linked to the size of an external random-access memory tape and hence it would prevent the model from generalizing to different memory sizes. Hence, for each 1 ≤ i ≤ R the controller receives, as input, only one scalar from each register, namely P(ri = 0) — the probability that the value in the register is equal 0. This solution has an additional advantage, namely it limits the amount of information available to the controller and forces it to rely on the modules instead of trying to solve the problem on its own. Notice that this information is sufﬁcient to get the exact value of ri if ri ∈ {0, 1}, which is the case whenever ri is an output of a ,,boolean” module, e.g. the inequality test module mi(a, b) = [a < b].  3.2 MEMORY TAPE  One could use the model described so far for learning sequence-to-sequence transformations by initializing the registers with the input sequence, and training the model to produce the desired output sequence in its registers after a given number of timesteps. The disadvantage of such model is that it would be completely unable to generalize to longer sequences, because the length of the sequence that the model can process is equal to the number of its registers, which is constant. Therefore, we extend the model with a variable-size memory tape, which consists of M memory cells, each of which stores a distribution over the set {0, 1, . . . , M −1}. Notice that each distribution stored in a memory cell or a register can be interpreted as a fuzzy address in the memory and used as a fuzzy pointer. We will hence identify integers in the set {0, 1, . . . , M − 1} with pointers to the memory. Therefore, the value in each memory cell may be interpreted as an integer or as a pointer. The exact state of the memory can be described by a matrix M ∈ RM M , where the value Mi,j is the probability that the i-th cell holds the value j. The model interacts with the memory tape solely using two special modules:  • READ module: this module takes as the input a pointer1 and returns the value stored under the given address in the memory. This operation is extended to fuzzy pointers similarly to Eq. 2. More precisely, if p is a vector representing the probability distribution of the input (i.e. pi is the probability that the input pointer points to the i-th cell) then the module returns the value MT p.  • WRITE module: this module takes as the input a pointer p and a value a and stores the value a under the address p in the memory. The fuzzy form of the operation can be effectively expressed using matrix operations 2.  The full architecture of the NRAM model is presented on Fig. 2  3.3  INPUTS AND OUTPUTS HANDLING  The memory tape also serves as an input-output channel — the model’s memory is initialized with the input sequence and the model is expected to produce the output in the memory. Moreover, we use a novel way of deciding how many timesteps should be executed. After each timestep we let the controller decide whether it would like to continue the execution or ﬁnish it, in which case the current state of the memory is treated as the output.  1Formally each module takes two arguments. In this case the second argument is simply ignored. 2The exact formula is M := (J − p)J T · M + paT , where J denotes a (column) vector consisting of M  ones and · denotes coordinate-wise multiplication.  4  Published as a conference paper at ICLR 2016  binarized  LSTM  ﬁnish?  r1 r2 r3 r4r  s r e t s i g e  m1  m3  m2  r1 r2 r3 r4  memory tape  Figure 2: One timestep of the NRAM architecture with R = 4 registers. The LSTM controller gets the ,,binarized” values r1, r2, . . . stored in the registers as inputs and outputs the description of the circuit in the grey box and the probability of ﬁnishing the execution in the current timestep (See Sec. 3.3 for more detail). The weights of the solid thin connections are outputted by the controller. The weights of the solid thick connections are trainable parameters of the model. Some of the modules (i.e. READ and WRITE) may interact with the memory tape (dashed connections).  cution has not been ﬁnished before the timestep t is equal(cid:81)t−1 the output is produced exactly at the timestep t is equal pt = ft ·(cid:81)t−1 produce output in the last step if it has not done it yet, i.e. pT = 1−(cid:80)T−1  More precisely, after the timestep t the controller outputs a scalar ft ∈ [0, 1]3, which denotes the willingness to ﬁnish the execution in the current timestep. Therefore, the probability that the exe- i=1(1 − fi), and the probability that i=1(1 − fi). There is also some maximal allowed number of timesteps T , which is a hyperparameter. The model is forced to i=1 pi regardless of the value  M denote the memory matrix after the timestep t, i.e. M(t)  of producing the correct output, i.e., −(cid:80)T  fT . Let M(t) ∈ RM i,j is the probability that the i-th memory cell holds the value j after the timestep t. For an input-output pair (x, y), where x, y ∈ {0, 1, . . . , M − 1}M we deﬁne the loss of the model as the expected negative log-likelihood assuming that the memory was initialized with the sequence x4. Moreover, for all problems we consider the output sequence is shorter than the memory. Therefore, we compute the loss only over memory cells, which should contain the output.  (cid:16) pt ·(cid:80)M  i=1 log(M(t)  (cid:17)  t=1  i,yi  )  3.4 DISCRETIZATION  Computing the outputs of modules, represented as probability distributions, is a computationally costly operation. For example, computing the output of the READ module takes Θ(M 2) time as it requires the multiplication of the matrix M ∈ RM One may however suspect (and we empirically verify this claim in Sec. 4) that the NRAM model naturally learns solutions in which the distributions of intermediate values have very low entropy. The argument for this hypothesis is that fuzziness in the intermediate values would probably prop- agate to the output and cause a higher value of the cost function. To test this hypothesis we trained the model and then used its discretized version during interference. In the discretized version every module gets as inputs the values from modules (or registers), which are the most probable to produce  M and the vector p ∈ RM .  3In fact, the controller outputs a scalar xi and fi = sigmoid(xi).  4One could also use the negative log-likelihood of the expected output, i.e. −(cid:80)M  (cid:16)(cid:80)T  i=1 log  t=1 pt · M(t)  i,yi  (cid:17)  as the loss function.  5  Published as a conference paper at ICLR 2016  the given input accordingly to the distribution outputted by the controller. More precisely, it corre- sponds to replacing the function softmax in equations (1,3) with the function returning the vector containing 1 on the position of the maximum value in the input and zeros on all other positions. Notice that in the discretized NRAM model each register and memory cell stores an integer from the set {0, 1, . . . , M − 1} and therefore all modules may be executed efﬁciently (assuming that the functions represented by the modules can be efﬁciently computed). In case of a feedforward controller and a small (e.g. ≤ 20) number of registers the interference can be accelerated even further. Recall that the only inputs to the controller are binarized values of the register. Therefore, instead of executing the controller one may simple precompute the (discretized) controller’s output for each conﬁguration of the registers’ binarized values. Such algorithm would enjoy an extremely efﬁcient implementation in machine code.  4 EXPERIMENTS  4.1 TRAINING PROCEDURE  The NRAM model is fully differentiable and we trained it using the Adam optimization algorithm (Kingma & Ba, 2014) with the negative log-likelihood cost function. Notice that we do not use any additional supervised data (such as memory access traces) beyond pure input-output examples. We used multilayer perceptrons (MLPs) with two hidden layers or LSTMs with a hidden layer between input and LSTM cells as controllers. The number of hidden units in each layer was equal. The ReLu nonlinearity (Nair & Hinton, 2010) was used in all experiments. Below are some important techniques that we used in the training:  Curriculum learning As noticed in several papers (Bengio et al., 2009; Zaremba & Sutskever, 2014), curriculum learning is crucial for training deep networks on very complicated problems. We followed the curriculum learning schedule from Zaremba & Sutskever (2014) without any modiﬁ- cations. The details can be found in Appendix B.  Gradient clipping Notice that the depth of the unfolded execution is roughly a product of the number of timesteps and the number of modules. Even for moderately small experiments (e.g. 14 modules and 20 timesteps) this value easily exceeds a few hundreds. In networks of such depth, the gradients can often “explode” (Bengio et al., 1994), what makes training by backpropagation much harder. We noticed that the gradients w.r.t. the intermediate values inside the backpropagation were so large, that they sometimes led to an overﬂow in single-precision ﬂoating-point arithmetic. Therefore, we clipped the gradients w.r.t. the activations, within the execution of the backpropaga- tion algorithm. More precisely, each coordinate is separately cropped into the range [−C1, C1] for some constant C1. Before updating parameters, we also globally rescale the whole gradient vector, so that its L2 norm is not bigger than some constant value C2.  Noise We added random Gaussian noise to the computed gradients after the backpropagation step. The variance of this noise decays exponentially during the training. The details can be found in Neelakantan et al. (2015).  Enforcing Distribution Constraints For very deep networks, a small error in one place can prop- agate to a huge error in some other place. This was the case with our pointers: they are probability distributions over memory cells and they should sum up to 1. However, after a number of operations are applied, they can accumulate error as a result of inaccurate ﬂoating-point arithmetic. We have a special layer which is responsible for rescaling all values (multiplying by the inverse of their sum), to make sure they always represent a probability distribution. We add this layer to our model in a few critical places (eg. after the softmax operation)5.  5We do not however backpropagate through these renormalizing operations, i.e. during the backward pass  we simply assume that they are identities.  6  Published as a conference paper at ICLR 2016  Entropy While searching for a solution, the network can ﬁx the pointer distribution on some particular value. This is advantageous at the end of training, because ideally we would like to be able to discretize the model. However, if this happens at the begin of the training, it could force the network to stay in a local minimum, with a small chance of moving the probability mass to some other value. To address this problem, we encourage the network to explore the space of solutions by adding an ”entropy bonus”, that decreases over time. More precisely, for every distribution outputted by the controller, we subtract from the cost function the entropy of the distribution multiplied by some coefﬁcient, which decreases exponentially during the training.  Limiting the values of logarithms There are two places in our model where the logarithms are computed — in the cost function and in the entropy computation. Inputs to whose logarithms can be very small numbers, which may cause very big values of the cost function or even overﬂows in ﬂoating-point arithmetic. To prevent this phenomenon we use log(max(x, (cid:15))) instead of log(x) for some small hyperparameter (cid:15) whenever a logarithm is computed.  4.2 TASKS  We now describe the tasks used in our experiments. For every task, the input is given to the network in the memory tape, and the network’s goal is to modify the memory according to the task’s speciﬁ- cation. We allow the network to modify the original input. The ﬁnal error for a test case is computed as c m, where c is the number of correctly written cells, and m represents the total number of cells that should be modiﬁed. Due to limited space, we describe the tasks only brieﬂy here. The detailed memory layout of inputs and outputs can be found in the Appendix A.  1. Access Given a value k and an array A, return A[k]. 2. Increment Given an array, increment all its elements by 1. 3. Copy Given an array and a pointer to the destination, copy all elements from the array to  the given location.  4. Reverse Given an array and a pointer to the destination, copy all elements from the array  in reversed order.  5. Swap Given two pointers p, q and an array A, swap elements A[p] and A[q]. 6. Permutation Given two arrays of n elements: P (contains a permutation of numbers  1, . . . , n) and A (contains random elements), permutate A according to P .  7. ListK Given a pointer to the head of a linked list and a number k, ﬁnd the value of the k-th  element on the list.  8. ListSearch Given a pointer to the head of a linked list and a value v to ﬁnd return a pointer  to the ﬁrst node on the list with the value v.  9. Merge Given pointers to 2 sorted arrays A and B, merge them. 10. WalkBST Given a pointer to the root of a Binary Search Tree, and a path to be traversed  (sequence of left/right steps), return the element at the end of the path.  4.3 MODULES  In all of our experiments we used the same sequence of 14 modules: READ (described in Sec. 3.2), ZERO(a, b) = 0, ONE(a, b) = 1, TWO(a, b) = 2, INC(a, b) = (a+1) mod M, ADD(a, b) = (a+b) mod M, SUB(a, b) = (a− b) mod M, DEC(a, b) = (a− 1) mod M, LESS-THAN(a, b) = [a < b], LESS-OR-EQUAL-THAN(a, b) = [a ≤ b], EQUALITY-TEST(a, b) = [a = b], MIN(a, b) = min(a, b), MAX(a, b) = max(a, b), WRITE (described in Sec. 3.2). We also considered settings in which the module sequence is repeated many times, e.g. there are 28 modules, where modules number 1. and 15. are READ, modules number 2. and 16. are ZERO and so on. The number of repetitions is a hyperparameter.  7  Published as a conference paper at ICLR 2016  Train Complexity  Train error Generalization  Discretization  Task Access  Increment  Copy Reverse Swap  Permutation  ListK  ListSearch  Merge  WalkBST  len(A) ≤ 20 len(A) ≤ 15 len(A) ≤ 15 len(A) ≤ 15 len(A) ≤ 20 len(A) ≤ 6 len(list) ≤ 10 len(list) ≤ 6 size(tree) ≤ 10  len(A) + len(B) ≤ 10  0 0 0 0 0 0 0 0 1% 0.3%  perfect perfect perfect perfect perfect  strong weak weak strong  almost perfect  perfect perfect perfect perfect perfect perfect  hurts performance hurts performance hurts performance hurts performance  Table 1: Results of the experiments. The perfect generalization error means that the tested problem had error 0 for complexity up to 50. Exact generalization errors are presented in Fig. 3 The perfect discretization means that the discretized version of the model produced exactly the same outputs as the original model on all test cases.  Figure 3: Generalization errors for hard tasks. The Permutation and ListSearch problems were trained only up to complexity 6. The remaining problems were trained up to complexity 10. The horizontal axis denotes the maximal task complexity, i.e., x = 20 denotes results with complexity sampled uniformly from the interval [1, 20].  4.4 RESULTS  Overall, we were able to ﬁnd parameters that achieved an error 0 for all problems except Merge and WalkBST (where we got an error of ≤ 1%). As described in 4.2, our metric is an accuracy on the memory cells that should be modiﬁed. To compute it, we take the continuous memory state produced by our network, then discretize it (every cell will contain the value with the highest probability), and ﬁnally compare with the expected output. The results of the experiments are summarized in Table 1. Below we describe our results on all 10 tasks in more detail. We divide them into 2 categories: ”easy” and ”hard” tasks. Easy tasks is a category of tasks that achieved low error scores for many sets of parameters and we did not have to spend much time trying to tune them. First 5 problems from our task list belong to this category. Hard tasks, on the other hand, are problems that often trained to low error rate only in a very small number of cases, eg. 1 out of 100.  4.4.1 EASY TASKS  This category includes the following problems: Access, Increment, Copy, Reverse, Swap. For all of them we were able to ﬁnd many sets of hyperparameters that achieved error 0, or close to it without much effort.  8  1015202530Max task complexity0.000.050.100.150.200.250.300.350.400.45Test errorMergeWalkBSTListKListSearchPermutationPublished as a conference paper at ICLR 2016  Step  1 2 3 4 5 6 7 8 9 10 11  0 6 6 6 6 6 6 6 6 6 6 6  1 2 2 2 2 2 2 2 2 2 2 2  2 10 10 10 10 10 10 10 10 10 10 10  3 6 6 6 6 6 6 6 6 6 6 6  4 8 8 8 8 8 8 8 8 8 8 8  5 9 9 9 9 9 9 9 9 9 9 9  6 0 0 2 2 2 2 2 2 2 2 2  7 0 0 0 0 10 10 10 10 10 10 10  8 0 0 0 0 0 0 6 6 6 6 6  9 0 0 0 0 0 0 0 0 8 8 8  10 0 0 0 0 0 0 0 0 0 0 9  11 0 0 0 0 0 0 0 0 0 0 0  r1 0 0 0 0 0 0 0 0 0 0 0  r2 0 5 5 5 5 5 5 5 5 5 5  r3 0 0 1 1 2 2 3 3 4 4 5  r4 0 1 1 2 2 3 3 4 4 5 5  READ p:0 p:1 p:1 p:2 p:2 p:3 p:3 p:4 p:4 p:5 p:5  WRITE p:0 a:6 p:6 a:2 p:6 a:2 p:7 a:10 p:7 a:10 p:8 a:6 p:8 a:6 p:9 a:8 p:9 a:8 p:10 a:9 p:10 a:9  Table 2: State of memory and registers for the Copy problem at the start of every timestep. We also show the arguments given to the READ and WRITE functions in each timestep. The argument “p:” represents the source/destination address and “a:” represents the value to be written (for WRITE). The value 6 at position 0 in the memory is the pointer to the destination array. It is followed by 5 values (gray columns) that should be copied.  We also tested how those solutions generalize to longer input sequences. To do this, for every problem we selected a model that achieved error 0 during the training, and tested it on inputs with lengths up to 506. To perform these tests we also increased the memory size and the number of allowed timesteps. In all cases the model solved the problem perfectly, what shows that it generalizes not only to longer input sequences, but also to different memory sizes and numbers of allowed timesteps. Moreover, the discretized version of the model (see Sec. 3.4 for details) also solves all the problems perfectly. These results show that the NRAM model naturally learns “algorithmic” solutions, which generalize well. We were also interested if the found solutions generalize to sequences of arbitrary length. It is eas- iest to verify in the case of a discretized model with a feedforward controller. That is because then circuits outputted by the controller depend solely on the values of registers, which are integers. We manually analysed circuits for problems Copy and Increment and veriﬁed that found solutions gen- eralize to inputs of arbitrary length, assuming that the number of allowed timesteps is appropriate.  4.4.2 HARD TASKS This category includes: Permutation, ListK, ListSearch, Merge and WalkBST. For all of them we had to perform an extensive random search to ﬁnd a good set of hyperparameters. Usually, most of the parameter combinations were stuck on the starting curriculum level with a high error of 50%− 70%. For the ﬁrst 3 tasks we managed to train the network to achieve er- ror 0. For WalkBST and Merge the training er- rors were 0.3% and 1% respectively. For train- ing those problems we had to introduce addi- tional techniques described in Sec. 4.1. For Permutation, ListK and WalkBST our model generalizes very well and achieves low error rates on inputs at least twice longer than the ones seen during the training. The exact generalization errors are shown in Fig. 3. The only hard problem on which our model discretizes well is Permutation — on this task  Figure 4: The circuit generated at every timestep ≥ 2. The values of the pointer (p) for READ, WRITE and the value to be written (a) for WRITE are presented in Table 2. The modules whose out- puts are not used were removed from the picture.  6Unfortunately we could not test for lengths longer than 50 due to the memory restrictions.  9  r1r1'r2addminr2'writepr4'r3incr4readpr3'aPublished as a conference paper at ICLR 2016  the discretized version of the model produces exactly the same outputs as the original model on all cases tested. For the remaining four problems the discretized version of the models perform very poorly (error rates ≥ 70%). We believe that better results may be obtained by using some techniques encouraging discretization during the training 7. We noticed that the training procedure is very unstable and the error often raises from a few percents to e.g. 70% in just one epoch. Moreover, even if we use the best found set of hyperparameters, the percent of random seeds that converges to error 0 was usually equal about 11%. We observed that the percent of converging seeds is much lower if we do not add noise to the gradient — in this case only about 1% of seeds converge.  4.5 COMPARISON TO EXISTING MODELS  A comparison to other models is challenging because we are the ﬁrst to consider problems with pointers. The NTM can solve tasks like Copy or Reverse, but it suffers from the inability to naturally store a pointer to a ﬁxed location in the memory. This makes it unlikely that it could solve tasks such as ListK, ListSearch or WalkBST since the pointers used in these tasks refer to absolute positions. What distinguishes our model from most of the previous attempts (including NTMs, Memory Net- works, Pointer Networks) is the lack of content-based addressing. It was a deliberate design deci- sion, since this kind of addressing inherently slows down the memory access. In contrast, our model — if discretized — can access the memory in a constant time. The NRAM is also the ﬁrst model that we are aware of employing a differentiable mechanism for deciding when to ﬁnish the computation.  4.6 EXEMPLARY EXECUTION  We present one example execution of our model for the problem Copy. For the example, we use a very small model with 12 memory cells, 4 registers and the standard set of 14 modules. The controller for this model is a feedforward network, and we run it for 11 timesteps. Table 2 contains, for every timestep, the state of the memory and registers at the begin of the timestep. The model can execute different circuits at different timesteps. In particular, we observed that the ﬁrst circuit is slightly different from the rest, since it needs to handle the initialization. Starting from the second step all generated circuits are the same. We present this circuit in Fig. 4. The register r2 is constant and keeps the offset between the destination array and the source array (6 − 1 = 5 in this case). The register r3 is responsible for incrementing the pointer in the source array. Its value is 8, the register used by the READ module. For the WRITE module, it also uses r4 which copied to r4 is shifted by r2. The register r1 is unused. This solution generalizes to sequences of arbitrary length.  5 CONCLUSIONS  In this paper we presented the Neural Random-Access Machine, which can learn to solve problems that require explicit manipulation and dereferencing of pointers. We showed that this model can learn to solve a number of algorithmic problems and generalize well to inputs longer than ones seen during the training. In particular, for some problems it generalizes to inputs of arbitrary length. However, we noticed that the optimization problem resulting from the backpropagating through the execution trace of the program is very challenging for standard optimization techniques. It seems likely that a method that can search in an easier “abstract” space would be more effective at solving such problems.  7One could for example add at later stages of training a penalty proportional to the entropy of the interme-  diate values of registers/memory.  timestep, but then the array is already copied.  8In our case r3 < r2, so the MIN module always outputs the value r3 + 1. It is not satisﬁed in the last  10  Published as a conference paper at ICLR 2016  REFERENCES Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly  learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.  Bengio, Yoshua, Simard, Patrice, and Frasconi, Paolo. Learning long-term dependencies with gra-  dient descent is difﬁcult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.  Bengio, Yoshua, Louradour, J´erˆome, Collobert, Ronan, and Weston, Jason. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41–48. ACM, 2009.  Chan, William, Jaitly, Navdeep, Le, Quoc V, and Vinyals, Oriol. Listen, attend and spell. arXiv  preprint arXiv:1508.01211, 2015.  Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural turing machines.  arXiv:1410.5401, 2014.  arXiv preprint  Grefenstette, Edward, Hermann, Karl Moritz, Suleyman, Mustafa, and Blunsom, Phil. Learning to  transduce with unbounded memory. arXiv preprint arXiv:1506.02516, 2015.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Joulin, Armand and Mikolov, Tomas. Inferring algorithmic patterns with stack-augmented recurrent  nets. arXiv preprint arXiv:1503.01007, 2015.  Kalchbrenner, Nal, Danihelka, Ivo, and Graves, Alex. Grid long short-term memory. arXiv preprint  arXiv:1507.01526, 2015.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980, 2014.  Luong, Minh-Thang, Pham, Hieu, and Manning, Christopher D. Effective approaches to attention-  based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.  Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807– 814, 2010.  Neelakantan, Arvind, Vilnis, Luke, Le, Quoc V, Sutskever, Ilya, Kaiser, Lukasz, Kurach, Karol, and Martens, James. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.  Solomonoff, Ray J. A formal theory of inductive inference. part i. Information and control, 7(1):  1–22, 1964.  Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and Fergus, Rob. End-to-end memory net-  works. arXiv preprint arXiv:1503.08895, 2015.  Vinyals, Oriol, Kaiser, Lukasz, Koo, Terry, Petrov, Slav, Sutskever, Ilya, and Hinton, Geoffrey.  Grammar as a foreign language. arXiv preprint arXiv:1412.7449, 2014.  Vinyals, Oriol, Fortunato, Meire, and Jaitly, Navdeep.  arXiv:1506.03134, 2015.  Pointer networks.  arXiv preprint  Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Memory networks.  arXiv:1410.3916, 2014.  arXiv preprint  Zaremba, Wojciech and Sutskever, Ilya. Learning to execute. arXiv preprint arXiv:1410.4615, 2014. Zaremba, Wojciech and Sutskever, Ilya. Reinforcement learning neural turing machines. arXiv  preprint arXiv:1505.00521, 2015.  11  Published as a conference paper at ICLR 2016  A DETAILED TASKS DESCRIPTIONS  In this section we describe in details the memory layout of inputs and outputs for the tasks used in our experiments. In all descriptions below, big letters represent arrays and small letters represents pointers. N U LL denotes the value 0 and is used to mark the end of an array or a missing next element in a list or a binary tree.  1. Access Given a value k and an array A, return A[k]. Input is given as k, A[0], .., A[n −  Input is given as  increment all its elements by 1.  2. Increment Given an array A,  1], N U LL and the network should replace the ﬁrst memory cell with A[k]. A[0], ..., A[n − 1], N U LL and the expected output is A[0] + 1, ..., A[n − 1] + 1. 3. Copy Given an array and a pointer to the destination, copy all elements from the array to the given location. Input is given as p, A[0], ..., A[n−1] where p points to one element after A[n− 1]. The expected output is A[0], ..., A[n− 1] at positions p, ..., p + n− 1 respectively. 4. Reverse Given an array and a pointer to the destination, copy all elements from the array in reversed order. Input is given as p, A[0], ..., A[n − 1] where p points one element after A[n− 1]. The expected output is A[n− 1], ..., A[0] at positions p, ..., p + n− 1 respectively. Input is given as p, q, A[0], .., A[p], ..., A[q], ..., A[n − 1], 0. The expected modiﬁed array A is: A[0], ..., A[q], ..., A[p], ..., A[n − 1]. 6. Permutation Given two arrays of n elements: P (contains a permutation of numbers 0, . . . , n − 1) and A (contains random elements), permutate A according to P . Input is given as a, P [0], ..., P [n − 1], A[0], ..., A[n − 1], where a is a pointer to the array A. The expected output is A[P [0]], ..., A[P [n − 1]], which should override the array P .  5. Swap Given two pointers p, q and an array A, swap elements A[p] and A[q].  7. ListK Given a pointer to the head of a linked list and a number k, ﬁnd the value of the k-th element on the list. List nodes are represented as two adjacent memory cells: a pointer to the next node and a value. Elements are in random locations in the memory, so that the network needs to follow the pointers to ﬁnd the correct element. Input is given as: head, k, out, ... where head is a pointer to the ﬁrst node on the list, k indicates how many hops are needed and out is a cell where the output should be put.  8. ListSearch Given a pointer to the head of a linked list and a value v to ﬁnd return a pointer to the ﬁrst node on the list with the value v. The list is placed in memory in the same way as in the task ListK. We ﬁll empty memory with “trash” values to prevent the network from “cheating” and just iterating over the whole memory. 9. Merge Given pointers to 2 sorted arrays A and B, and the pointer to the output o, merge the two arrays into one sorted array. The input is given as: a, b, o, A[0], .., A[n − 1], G, B[0], ..., B[m − 1], G, where G is a special guardian value, a and b point to the ﬁrst elements of arrays A and B respectively, and o points to the address after the second G. The n + m element should be written in correct order starting from position o.  10. WalkBST Given a pointer to the root of a Binary Search Tree, and a path to be traversed, return the element at the end of the path. The BST nodes are represented as tripes (v, l, r), where v is the value, and l, r are pointers to the left/right child. The triples are placed randomly in the memory. Input is given as root, out, d1, d2, ..., dk, N U LL, ..., where root points to the root node and out is a slot for the output. The sequence d1...dk, di ∈ {0, 1} represents the path to be traversed: di = 0 means that the network should go to the left child, di = 1 represents going to the right child.  12  Published as a conference paper at ICLR 2016  B DETAILS OF CURRICULUM TRAINING  As noticed in several papers (Bengio et al., 2009; Zaremba & Sutskever, 2014), curriculum learning is crucial for training deep networks on very complicated problems. We followed the curriculum learning schedule from Zaremba & Sutskever (2014) without any modiﬁcations. For each of the tasks we have manually deﬁned a sequence of subtasks with increasing difﬁculty, where the difﬁculty is usually measured by the length of the input sequence. During training the input-output examples are sampled from a distribution that is determined by the current difﬁculty level D. The level is increased (up to some maximal value) whenever the error rate of the model goes below some threshold. Moreover, we ensure that successive increases of D are separated by some number of batches. In more detail, to generate an input-output example we ﬁrst sample a difﬁculty d from a distribution determined by the current level D and then draw the example with the difﬁculty d. The procedure for sampling d is the following:  • with probability 10%: pick d uniformly at random from the set of all possible difﬁculties; • with probability 25%: pick d uniformly from [1, D + e], where e is a sample from a geo- • with probability 65%: set d = D + e, where e is sampled as above.  metric distribution with a success probability 1/2;  Notice that the above procedure guarantees that every difﬁculty d can be picked regardless of the current level D, which has been shown to increase performance Zaremba & Sutskever (2014).  13  Published as a conference paper at ICLR 2016  C EXAMPLE CIRCUITS  Below are presented example circuits generated during training for all simple tasks (except Copy which was presented in the paper). For modules READ and WRITE, the value of the ﬁrst argument (pointer to the address to be read/written) is marked as p. For WRITE, the value to be written is marked as a and the value returned by this module is always 0. For modules LESS-THAN and LESS-OR-EQUAL-THAN the ﬁrst parameter is marked as x and the second one as y. Other modules either have only one parameter or the order of parameters is not important. For all tasks below (except Increment), the circuit generated at timestep 1 is different than circuits generated at steps ≥ 2, which are the same. This is because the ﬁrst circuit needs to handle the initialization. We present only the ”main” circuits generated for timesteps ≥ 2.  C.1 ACCESS  Figure 5: The circuit generated at every timestep ≥ 2 for the task Access.  Step  1 2 3  0 3 3 4  1 1 1 1  2 12 12 12  3 4 4 4  4 7 7 7  5 12 12 12  6 1 1 1  7 13 13 13  8 8 8 8  9 2 2 2  10 1 1 1  11 3 3 3  12 11 11 11  13 11 11 11  14 12 12 12  15 0 0 0  r1 0 3 3  r2 0 0 0  Table 3: Memory for task Access. Only the ﬁrst memory cell is modiﬁed.  14  r1readpr1'incwritealtxr2'minp0yPublished as a conference paper at ICLR 2016  C.2  INCREMENT  Figure 6: The circuit generated at every timestep for the task Increment.  Step  1 2 3 4 5 6 7 8 9 10 11  0 1 2 2 2 2 2 2 2 2 2 2  1 11 11 12 12 12 12 12 12 12 12 12  2 3 3 3 4 4 4 4 4 4 4 4  3 8 8 8 8 9 9 9 9 9 9 9  4 1 1 1 1 1 2 2 2 2 2 2  5 2 2 2 2 2 2 3 3 3 3 3  6 9 9 9 9 9 9 9 10 10 10 10  7 8 8 8 8 8 8 8 8 9 9 9  8 5 5 5 5 5 5 5 5 5 6 6  9 3 3 3 3 3 3 3 3 3 3 4  10 0 0 0 0 0 0 0 0 0 0 0  11 0 0 0 0 0 0 0 0 0 0 0  12 0 0 0 0 0 0 0 0 0 0 0  13 0 0 0 0 0 0 0 0 0 0 0  14 0 0 0 0 0 0 0 0 0 0 0  15 0 0 0 0 0 0 0 0 0 0 0  r1 0 1 2 3 4 5 6 7 8 9 10  r2 0 2 12 4 9 2 3 10 9 6 4  r3 0 2 12 4 9 2 3 10 9 6 4  r4 0 2 12 4 9 2 3 10 9 6 4  r5 0 1 2 3 4 5 6 7 8 9 10  Table 4: Memory for task Increment.  15  r5readpaddwritepincminr5'maxr2'r3'r1'ar4'1Published as a conference paper at ICLR 2016  C.3 REVERSE  Figure 7: The circuit generated at every timestep ≥ 2 for the task Reverse.  Step  1 2 3 4 5 6 7 8 9 10  0 8 8 8 8 8 8 8 8 8 8  1 8 8 8 8 8 8 8 8 8 8  2 1 1 1 1 1 1 1 1 1 1  3 3 3 3 3 3 3 3 3 3 3  4 5 5 5 5 5 5 5 5 5 5  5 1 1 1 1 1 1 1 1 1 1  6 1 1 1 1 1 1 1 1 1 1  7 2 2 2 2 2 2 2 2 2 2  8 0 0 0 0 0 0 0 0 2 2  9 0 0 0 0 0 0 0 1 1 1  10 0 0 0 0 0 0 1 1 1 1  11 0 0 0 0 0 5 5 5 5 5  12 0 0 0 0 3 3 3 3 3 3  13 0 0 0 1 1 1 1 1 1 1  14 0 0 8 8 8 8 8 8 8 8  15 0 0 0 0 0 0 0 0 0 0  r1 0 8 8 8 8 8 8 8 8 8  r2 0 0 1 1 1 1 1 1 1 1  r3 0 1 2 3 4 5 6 7 8 9  r4 0 1 1 1 1 1 1 1 1 1  Table 5: Memory for task Reverse.  16  r1addr1'subdecwritepr3readpincminmaxr3'ar4lexr2'r4'1yPublished as a conference paper at ICLR 2016  C.4 SWAP  For swap we observed that 2 different circuits are generated, one for even timesteps, one for odd timesteps.  Figure 8: The circuit generated at every even timestep for the task Swap.  Figure 9: The circuit generated at every odd timestep ≥ 3 for the task Swap.  Step  1 2 3 4  0 4 5 5 5  1 13 13 13 13  2 6 6 6 6  3 10 10 10 10  4 5 5 12 12  5 4 4 4 4  6 6 6 6 6  7 3 3 3 3  8 7 7 7 7  9 1 1 1 1  10 1 1 1 1  11 11 11 11 11  12 13 13 13 13  13 12 12 12 5  14 0 0 0 0  15 0 0 0 0  r1 0 1 0 5  r2 0 4 13 1  Table 6: Memory for task Swap.  17  r1readpmaxreadpr2'writeamaxaddwriteapr2pr1'r1readpeqmaxr1'subwriteawritealtxr2paddsubyleypr2'0incltxx1y2",
1511.05493,2016,Gated Graph Sequence Neural Networks,"['Gated Graph Sequence Neural Networks\nYujia Li', 'Daniel Tarlow', 'Marc Brockschmidt', 'Richard Zemel', 'CIFAR']",https://arxiv.org/pdf/1511.05493,"7 1 0 2     p e S 2 2         ]  G L . s c [      4 v 3 9 4 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  GATED GRAPH SEQUENCE NEURAL NETWORKS  Yujia Li∗& Richard Zemel Department of Computer Science, University of Toronto Toronto, Canada {yujiali,zemel}@cs.toronto.edu  Marc Brockschmidt & Daniel Tarlow Microsoft Research Cambridge, UK {mabrocks,dtarlow}@microsoft.com  ABSTRACT  Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is pre- vious work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a ﬂexible and broadly useful class of neural net- work models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program veriﬁcation, in which subgraphs need to be described as abstract data structures.  1  INTRODUCTION  Many practical applications build on graph-structured data, and thus we often want to perform ma- chine learning tasks that take graphs as inputs. Standard approaches to the problem include engineer- ing custom features of an input graph, graph kernels (Kashima et al., 2003; Shervashidze et al., 2011), and methods that deﬁne graph features in terms of random walks on graphs (Perozzi et al., 2014). More closely related to our goal in this work are methods that learn features on graphs, including Graph Neural Networks (Gori et al., 2005; Scarselli et al., 2009), spectral networks (Bruna et al., 2013) and recent work on learning graph ﬁngerprints for classiﬁcation tasks on graph representations of chemical molecules (Duvenaud et al., 2015). Our main contribution is an extension of Graph Neural Networks that outputs sequences. Previous work on feature learning for graph-structured inputs has focused on models that produce single outputs such as graph-level classiﬁcations, but many problems with graph inputs require outputting sequences. Examples include paths on a graph, enumerations of graph nodes with desirable properties, or sequences of global classiﬁcations mixed with, for example, a start and end node. We are not aware of existing graph feature learning work suitable for this problem. Our motivating application comes from program veriﬁcation and requires outputting logical formulas, which we formulate as a sequential output problem. A secondary contribution is highlighting that Graph Neural Networks (and further extensions we develop here) are a broadly useful class of neural network model that is applicable to many problems currently facing the ﬁeld. There are two settings for feature learning on graphs: (1) learning a representation of the input graph, and (2) learning representations of the internal state during the process of producing a sequence of outputs. Here, (1) is mostly achieved by previous work on Graph Neural Networks (Scarselli et al., 2009); we make several minor adaptations of this framework, including changing it to use modern practices around Recurrent Neural Networks. (2) is important because we desire outputs from graph- structured problems that are not solely individual classiﬁcations. In these cases, the challenge is how  ∗Work done primarily while author was an intern at Microsoft Research.  1  Published as a conference paper at ICLR 2016  to learn features on the graph that encode the partial output sequence that has already been produced (e.g., the path so far if outputting a path) and that still needs to be produced (e.g., the remaining path). We will show how the GNN framework can be adapted to these settings, leading to a novel graph-based neural network model that we call Gated Graph Sequence Neural Networks (GGS-NNs). We illustrate aspects of this general model in experiments on bAbI tasks (Weston et al., 2015) and graph algorithm learning tasks that illustrate the capabilities of the model. We then present an application to the veriﬁcation of computer programs. When attempting to prove properties such as memory safety (i.e., that there are no null pointer dereferences in a program), a core problem is to ﬁnd mathematical descriptions of the data structures used in a program. Following Brockschmidt et al. (2015), we have phrased this as a machine learning problem where we will learn to map from a set of input graphs, representing the state of memory, to a logical description of the data structures that have been instantiated. Whereas Brockschmidt et al. (2015) relied on a large amount of hand-engineering of features, we show that the system can be replaced with a GGS-NN at no cost in accuracy.  2 GRAPH NEURAL NETWORKS  In this section, we review Graph Neural Networks (GNNs) (Gori et al., 2005; Scarselli et al., 2009) and introduce notation and concepts that will be used throughout. GNNs are a general neural network architecture deﬁned according to a graph structure G = (V,E). Nodes v ∈ V take unique values from 1, . . . ,|V|, and edges are pairs e = (v, v(cid:48)) ∈ V × V. We will focus in this work on directed graphs, so (v, v(cid:48)) represents a directed edge v → v(cid:48), but we note that the framework can easily be adapted to undirected graphs; see Scarselli et al. (2009). The node vector (or node representation or node embedding) for node v is denoted by hv ∈ RD. Graphs may also } contain node labels lv ∈ {1, . . . , LV } for each node v and edge labels or edge types le ∈ {1, . . . , LE for each edge. We will overload notation and let hS = {hv | v ∈ S} when S is a set of nodes, and lS = {le | e ∈ S} when S is a set of edges. The function IN(v) = {v(cid:48) | (v(cid:48), v) ∈ E} returns the set of predecessor nodes v(cid:48) with v(cid:48) → v. Analogously, OUT(v) = {v(cid:48) | (v, v(cid:48)) ∈ E} is the set of successor nodes v(cid:48) with edges v → v(cid:48). The set of all nodes neighboring v is NBR(v) = IN(v) ∪ OUT(v), and the set of all edges incoming to or outgoing from v is CO(v) = {(v(cid:48), v(cid:48)(cid:48)) ∈ E | v = v(cid:48) ∨ v = v(cid:48)(cid:48)}. GNNs map graphs to outputs via two steps. First, there is a propagation step that computes node rep- resentations for each node; second, an output model ov = g(hv, lv) maps from node representations and corresponding labels to an output ov for each v ∈ V. In the notation for g, we leave the depen- dence on parameters implicit, and we will continue to do this throughout. The system is differentiable from end-to-end, so all parameters are learned jointly using gradient-based optimization.  2.1 PROPAGATION MODEL  Here, an iterative procedure propagates node representations. Initial node representations h(1) are v set to arbitrary values, then each node representation is updated following the recurrence below until convergence, where t denotes the timestep:  v = f∗(lv, lCO(v), lNBR(v), h(t−1) h(t)  NBR(v)  ).  Several variants are discussed in Scarselli et al. (2009) including positional graph forms, node-speciﬁc updates, and alternative representations of neighborhoods. Concretely, Scarselli et al. (2009) suggest decomposing f∗(·) to be a sum of per-edge terms:  f∗(lv, lCO(v), lNBR(v), h(t)  NBR(v)  f (lv, l(v(cid:48),v), lv(cid:48), h(t−1)  v(cid:48)  f (lv, l(v,v(cid:48)), lv(cid:48), h(t−1)  v(cid:48)  ),  where f (·) is either a linear function of hv(cid:48) or a neural network. The parameters of f depends on the conﬁguration of labels, e.g. in the following linear case, A and b are learnable parameters,  f (lv, l(v(cid:48),v), lv(cid:48), h(t)  v(cid:48) ) = A(lv,l(v(cid:48) ,v),lv(cid:48) )h(t−1)  v(cid:48)  + b(lv,l(v(cid:48),v),lv(cid:48) ).  2.2 OUTPUT MODEL AND LEARNING  The output model is deﬁned per node and is a differentiable function g(hv, lv) that maps to an output. This is generally a linear or neural network mapping. Scarselli et al. (2009) focus on outputs that are  2  ) +(cid:88)v(cid:48)∈OUT(v)  ) =(cid:88)v(cid:48)∈IN(v)  Published as a conference paper at ICLR 2016  v  v  independent per node, which are implemented by mapping the ﬁnal node representations h(T ) , to an , lv) for each node v ∈ V. To handle graph-level classiﬁcations, they suggest to output ov = g(h(T ) create a dummy “super node” that is connected to all other nodes by a special type of edge. Thus, graph-level regression or classiﬁcation can be handled in the same manner as node-level regression or classiﬁcation. Learning is done via the Almeida-Pineda algorithm (Almeida, 1990; Pineda, 1987), which works by running the propagation to convergence, and then computing gradients based upon the converged solution. This has the advantage of not needing to store intermediate states in order to compute gradients. The disadvantage is that parameters must be constrained so that the propagation step is a contraction map. This is needed to ensure convergence, but it may limit the expressivity of the model. When f (·) is a neural network, this is encouraged using a penalty term on the 1-norm of the network’s Jacobian. See Appendix A for an example that gives the intuition that contraction maps have trouble propagating information across a long range in a graph.  3 GATED GRAPH NEURAL NETWORKS  We now describe Gated Graph Neural Networks (GG-NNs), our adaptation of GNNs that is suitable for non-sequential outputs. We will describe sequential outputs in the next section. The biggest mod- iﬁcation of GNNs is that we use Gated Recurrent Units (Cho et al., 2014) and unroll the recurrence for a ﬁxed number of steps T and use backpropagation through time in order to compute gradients. This requires more memory than the Almeida-Pineda algorithm, but it removes the need to constrain parameters to ensure convergence. We also extend the underlying representations and output model.  3.1 NODE ANNOTATIONS  In GNNs, there is no point in initializing node representations because the contraction map constraint ensures that the ﬁxed point is independent of the initializations. This is no longer the case with GG-NNs, which lets us incorporate node labels as additional inputs. To distinguish these node labels used as inputs from the ones introduced before, we call them node annotations, and use vector x to denote these annotations. To illustrate how the node annotations are used, consider an example task of training a graph neural network to predict whether node t can be reached from node s on a given graph. For this task, there are two problem-related special nodes, s and t. To mark these nodes as special, we give them an initial annotation. The ﬁrst node s gets the annotation xs = [1, 0](cid:62), and the second node t gets the annotation xt = [0, 1](cid:62). All other nodes v have their initial annotation set to xv = [0, 0](cid:62). Intuitively, this marks s as the ﬁrst input argument and t as the second input argument. We then initialize the node state vectors h(1) v using these label vectors by copying xv into the ﬁrst dimensions and padding with extra 0’s to allow hidden states that are larger than the annotation size. In the reachability example, it is easy for the propagation model to learn to propagate the node annota- tion for s to all nodes reachable from s, for example by setting the propagation matrix associated with forward edges to have a 1 in position (0,0). This will cause the ﬁrst dimension of node representation to be copied along forward edges. With this setting of parameters, the propagation step will cause all nodes reachable from s to have their ﬁrst bit of node representation set to 1. The output step classiﬁer can then easily tell whether node t is reachable from s by looking whether some node has nonzero entries in the ﬁrst two dimensions of its representation vector.  3.2 PROPAGATION MODEL  The basic recurrence of the propagation model is  (cid:62)  h(1)  (cid:62) v , 0] v = [x (cid:62) v:  a(t) v = A  (cid:104)  (cid:16)  h(t−1)  1  (cid:62)  . . . h(t−1) v + Uzh(t−1)  |V|  v  Wza(t)  zt v = σ  (cid:62)(cid:105)(cid:62) (cid:17)  + b  (1)  (2)  (3)  3  rt v = σ  Wra(t)  v + Urh(t−1)  v  (cid:16)  (cid:16)  (cid:103)h(t)  v = tanh v = (1 − zt h(t)  Wa(t) v + U v) (cid:12) h(t−1)  v  (cid:16)  (cid:17) v (cid:12)(cid:103)h(t)  v  v .  v (cid:12) h(t−1) rt  + zt  (cid:17)(cid:17)  (4)  (5)  (6)  Published as a conference paper at ICLR 2016  (a)  (b)  (c) A =(cid:2)A(out), A(in)(cid:3)  Figure 1: (a) Example graph. Color denotes edge types. (b) Unrolled one timestep. (c) Parameter tying and sparsity in recurrent matrix. Letters denote edge types with B(cid:48) corresponding to the reverse edge of type B. B and B(cid:48) denote distinct parameters.  The matrix A ∈ RD|V|×2D|V| determines how nodes in the graph communicate with each other. The sparsity structure and parameter tying in A is illustrated in Fig. 1. The sparsity structure corresponds to the edges of the graph, and the parameters in each submatrix are determined by the edge type and direction. Av: ∈ RD|V|×2D are the two columns of blocks in A(out) and A(in) corresponding to node v. Eq. 1 is the initialization step, which copies node annotations into the ﬁrst components of the hidden state and pads the rest with zeros. Eq. 2 is the step that passes information between different nodes of the graph via incoming and outgoing edges with parameters dependent on the edge v ∈ R2D contains activations from edges in both directions. The remaining are type and direction. a(t) GRU-like updates that incorporate information from the other nodes and from the previous timestep to update each node’s hidden state. z and r are the update and reset gates, σ(x) = 1/(1 + e−x) is the logistic sigmoid function, and (cid:12) is element-wise multiplication. We initially experimented with a vanilla recurrent neural network-style update, but in preliminary experiments we found this GRU-like propagation step to be more effective.  3.3 OUTPUT MODELS  There are several types of one-step outputs that we would like to produce in different situations. First, , xv) for each node v ∈ V output node GG-NNs support node selection tasks by making ov = g(h(T ) scores and applying a softmax over node scores. Second, for graph-level outputs, we deﬁne a graph level representation vector as  v  hG = tanh(cid:32)(cid:88)v∈V  σ(cid:16)i(h(T )  v  , xv)(cid:17) (cid:12) tanh(cid:16)j(h(T )  v  , xv)(cid:17)(cid:33) ,  (7)  v  where σ(i(h(T ) current graph-level task. i and j are neural networks that take the concatenation of h(T ) input and outputs real-valued vectors. The tanh functions can also be replaced with the identity.  , xv)) acts as a soft attention mechanism that decides which nodes are relevant to the and xv as  v  4 GATED GRAPH SEQUENCE NEURAL NETWORKS  Here we describe Gated Graph Sequence Neural Networks (GGS-NNs), in which several GG-NNs operate in sequence to produce an output sequence o(1) . . . o(K). For the kth output step, we denote the matrix of node annotations as X (k) = [x(k) ](cid:62) ∈ for predicting o(k) from X (k), and F (k)X for R|V|×LV . We use two GG-NNs F (k) predicting X (k+1) from X (k). X (k+1) can be seen as the states carried over from step k to k + 1. Both F (k) and F (k)X contain a propagation model and an output model. In the propagation models, we denote the matrix of node vectors at the tth propagation step of the kth output step as H(k,t) = ](cid:62) ∈ R|V|×D. As before, in step k, we set H(k,1) by 0-extending X (k) per node. An [h(k,t) overview of the model is shown in Fig. 2. Alternatively, F (k) and F (k)X can share a single propagation model, and just have separate output models. This simpler variant is faster to train and evaluate, and  and F (k)X : F (k)  1 ; . . . ; x(k) |V|  ; . . . ; h(k,t) |V|  o  o  o  o  1  4  3412h(t1)2h(t1)1h(t1)3h(t1)4h(t)4h(t)3h(t)2h(t)1BCCB12341234Outgoing EdgesB’C’C’B’1234Incoming Edges|{z}|{z}Published as a conference paper at ICLR 2016  o(1) F ( 1 ) o F (1)X H(1,1)  Init  X (2)  o(2) F ( 2 ) o F (2)X H(2,1)  X (3)  Init  X (1)  . . .  Figure 2: Architecture of GGS-NN models.  in many cases can achieve similar performance level as the full model. But in cases where the desired propagation behavior for F (k) We introduce a node annotation output model for predicting X (k+1) from H(k,T ). The prediction is done for each node independently using a neural network j(h(k,T ) v ) that takes the concatenation of h(k,T )  , x(k) as input and outputs a vector of real-valued scores:  and F (k)X are different, this variant may not work as well.  and x(k)  o  v  v  v  x(k+1)  v  = σ(cid:16)j(h(k,T )  v  , x(k)  v )(cid:17) .  (8)  There are two settings for training GGS-NNs: specifying all intermediate annotations X (k), or train- ing the full model end-to-end given only X (1), graphs and target sequences. The former can improve performance when we have domain knowledge about speciﬁc intermediate information that should be represented in the internal state of nodes, while the latter is more general. We describe both.  Sequence outputs with observed annotations Consider the task of making a sequence of predic- tions for a graph, where each prediction is only about a part of the graph. In order to ensure we predict an output for each part of the graph exactly once, it sufﬁces to have one bit per node, indicat- ing whether the node has been “explained” so far. In some settings, a small number of annotations are sufﬁcient to capture the state of the output procedure. When this is the case, we may want to directly input this information into the model via labels indicating target intermediate annotations. In some cases, these annotations may be sufﬁcient, in that we can deﬁne a model where the GG-NNs are rendered conditionally independent given the annotations. In this case, at training time, given the annotations X (k) the sequence prediction task decomposes into single step prediction tasks and can be trained as separate GG-NNs. At test time, predicted annotations from one step will be used as input to the next step. This is analogous to training directed graphical models when data is fully observed.  Sequence outputs with latent annotations More generally, when intermediate node annotations X (k) are not available during training, we treat them as hidden units in the network, and train the whole model jointly by backpropagating through the whole sequence.  5 EXPLANATORY APPLICATIONS  In this section we present example applications that concretely illustrate the use of GGS-NNs. We focus on a selection of bAbI artiﬁcial intelligence (AI) tasks (Weston et al., 2015) and two graph algorithm learning tasks.  5.1 BABI TASKS  The bAbI tasks are meant to test reasoning capabilities that AI systems should be capable of. In the bAbI suite, there are 20 tasks that test basic forms of reasoning like deduction, induction, counting, and path-ﬁnding. We have deﬁned a basic transformation procedure that maps bAbI tasks to GG-NNs or GGS-NNs. We use the --symbolic option from the released bAbI code to get stories that just involve sequences of relations between entities, which are then converted into a graph. Each entity is mapped to a node, and each relation is mapped to an edge with edge label given by the relation. The full story is consumed and mapped to a single graph. Questions are marked by eval in the data and are comprised of a question type (e.g., has fear), and some argument (e.g., one or more nodes). The arguments are converted into initial node annotations, with the i-th bit of the i-th argument node’s  5  Published as a conference paper at ICLR 2016  E = [1, 0](cid:62), A gets x(1)  A = [0, 1](cid:62), and for all other nodes v, x(1)  annotation vector set to 1. For example, if the eval line is eval E > A true, then E gets initial annotation x(1) v = [0, 0](cid:62). Question type is 1 (for ‘>’) and output is class 1 (for ‘true’). Some tasks have multiple question types, for example Task 4 which has 4 question types: e, s, w, n. For such tasks we simply train a separate GG- NN for each task. We do not use the strong supervision labels or give the GGS-NNs any intermediate annotations in any experiments. While simple, this transformation does not preserve all information about the story (e.g., it discards temporal order of the inputs), and it does not easily handle ternary and higher order relations (e.g., Yesterday John went to the garden is not easily mapped to a simple edge). We also emphasize that it is a non-trivial task to map general natural language to symbolic form,1 so we could not directly apply this approach to arbitrary natural language. Relaxing these restrictions is left for future work. However, even with this simple transformation, there are a variety of bAbI tasks that can be formu- lated, including Task 19 (Path Finding), which is arguably the hardest task. We provide baselines to show that the symbolic representation does not help RNNs or LSTMs signiﬁcantly, and show that GGS-NNs solve the problem with a small number of training instances. We also develop two new bAbI-like tasks that involve outputting sequences on graphs: shortest paths, and a simple form of Eulerian circuits (on random connected 2-regular graphs). The point of these experiments is to illustrate the capabilities of GGS-NNs across a variety of problems.  Example 1. As an example, below is an instance from the symbolic dataset for bAbI task 15, Basic Deduction.  D is A B is E A has_fear F G is F E has_fear H F has_fear A H has_fear A C is H eval B has_fear eval G has_fear eval C has_fear eval D has_fear  H A A F  Here the ﬁrst 8 lines describe the facts, the GG-NN will use these facts to build a graph. Capital letters are nodes, is and has fear are interpreted as edge labels or edge types. The last 4 lines are 4 questions asked for this input data. has fear in these lines are interpreted as a question type. For this task, in each question only one node is special, e.g. the B in eval B has fear, and we assign a single value 1 to the annotation vector for this special node and 0 to all the other nodes. For RNN and LSTM the data is converted into token sequences like below:  n6 e1 n1 eol n6 e1 n5 eol n1 e1 n2 eol n4 e1 n5 eol n3 e1 n4 eol n3 e1 n5 eol n6 e1 n4 eol q1 n6 n2 ans 1  where n<id> are nodes, e<id> are edges, q<id> are question types, extra tokens eol (end-of- line) and ans (answer) are added to give the RNN & LSTM access to the complete information available in the dataset. The ﬁnal number is the class label.  1Although the bAbI data is quite templatic, so it is straightforward to hand-code a parser that will work for  the bAbI data; the symbolic option removes the need for this.  6  Published as a conference paper at ICLR 2016  Example 2. As a second example, below is an instance from the symbolic dataset for bAbI task 19, Path Finding.  E s A B n C E w F B w E eval path B A w,s  Here the ﬁrst 4 lines describe edges, s, n, w, e (e does not appear in this example) are all different edge types. The last line is a path question, the answer is a sequence of directions w,s, as the path going from B to A is to ﬁrst go west to E then go south to A. The s, n, w, e in the question lines are treated as output classes.  More Training Details. For all tasks in this section, we generate 1000 training examples and 1000 test examples, 50 of the training examples are used for validation. When evaluating model performance, for all bAbI tasks that contain more than one questions in one example, the predictions for different questions were evaluated independently. As there is randomness in the dataset generation process, we generated 10 such datasets for each task, and report the mean and standard deviation of the evaluation performance across the 10 datasets. For all explanatory tasks, we start by training different models on only 50 training examples, and gradually increase the number of training examples to 100, 250, 500, and 950 (50 of the training examples are reserved for validation) until the model’s test accuracy reaches 95% or above, a success by bAbI standard Weston et al. (2015). For each method, we report the minimum number of training examples it needs to reach 95% accuracy along with the accuracy it reaches with that amount of training examples. In all these cases, we unrolled the propagation process for 5 steps. For bAbI task 4, 15, 16, 18, 19, we used GG-NN with the size of node vectors h(t) set to D = 4, D = 5, D = 6, v D = 3 and D = 6 respectively. For all the GGS-NNs in this section we used the simpler variant in which F (k) and F (k)X share a single propagation model. For shortest path and Eulerian circuit tasks, we used D = 20. All models are trained long enough with Adam (Kingma & Ba, 2014), and the validation set is used to choose the best model to evaluate and avoid models that are overﬁtting.  o  5.1.1 SINGLE STEP OUTPUTS  We choose four bAbI tasks that are suited to the restrictions described above and require single step outputs: 4 (Two Argument Relations), 15 (Basic Deduction), 16 (Basic Induction), and 18 (Size Reasoning). For Task 4, 15 and 16, a node selection GG-NN is used. For Task 18 we used a graph- level classiﬁcation version. All the GGNN networks contain less than 600 parameters2. As baselines, we train RNN and LSTM models on the symbolic data in raw sequence form. The RNNs and LSTMs use 50 dimensional embeddings and 50 dimensional hidden layers; they predict a single output at the end of the sequences and the output is treated as a classiﬁcation problem, the loss is cross entropy. The RNNs and LSTMs contain around 5k and 30k parameters, respectively. Test results appear in Table 1. For all tasks GG-NN achieves perfect test accuracy using only 50 training examples, while the RNN/LSTM baselines either use more training examples (Task 4) or fail to solve the tasks (Task 15, 16 and 18). In Table 2, we further break down performance of the baselines for task 4 as the amount of training data varies. While both the RNN and LSTM are able to solve the task almost perfectly, the GG- NN reaches 100% accuracy with much less data.  2For bAbI task 4, we treated ‘e’, ‘s’, ‘w’, ‘n’ as 4 question types and trained one GG-NN for each question type, so strictly speaking for bAbI task 4 our GG-NN model has 4 times the number of parameters of a single GG-NN model. In our experiments we used a GG-NN with 271 parameters for each question type which means 1084 parameters in total.  7  Published as a conference paper at ICLR 2016  Task bAbI Task 4 bAbI Task 15 bAbI Task 16 bAbI Task 18  RNN  97.3±1.9 (250) 48.6±1.9 (950) 33.0±1.9 (950) 88.9±0.9 (950)  LSTM  97.4±2.0 (250) 50.3±1.3 (950) 37.5±0.9 (950) 88.9±0.8 (950)  GG-NN  100.0±0.0 (50) 100.0±0.0 (50) 100.0±0.0 (50) 100.0±0.0 (50)  Table 1: Accuracy in percentage of different models for different tasks. Number in parentheses is number of training examples required to reach shown accuracy.  #Training Examples RNN LSTM  50  76.7±3.8 73.5±5.2  100  90.2±4.0 86.4±3.8  250  97.3±1.9 97.4±2.0  500  98.4±1.3 99.2±0.8  950  99.7±0.4 99.6±0.8  Table 2: Performance breakdown of RNN and LSTM on bAbI task 4 as the amount of training data changes.  5.1.2 SEQUENTIAL OUTPUTS  The bAbI Task 19 (Path Finding) is arguably the hardest task among all bAbI tasks (see e.g., (Sukhbaatar et al., 2015), which reports an accuracy of less than 20% for all methods that do not use the strong supervision). We apply a GGS-NN to this problem, again on the symbolic form of the data (so results are not comparable to those in (Sukhbaatar et al., 2015)). An extra ‘end’ class is added to the end of each output sequence; at test time the network will keep making predictions until it predicts the ‘end’ class. The results for this task are given in Table 3. Both RNN and LSTM fail on this task. However, with only 50 training examples, our GGS-NNs achieve much better test accuracy than RNN and LSTM.  5.2 LEARNING GRAPH ALGORITHMS  Task bAbI Task 19 Shortest Path Eulerian Circuit  RNN  24.7±2.7 (950) 9.7±1.7 (950) 0.3±0.2 (950)  LSTM  28.2±1.3 (950) 10.5±1.2 (950) 0.1±0.2 (950)  71.1±14.7 (50) 100.0± 0.0 (50) 100.0± 0.0 (50)  GGS-NNs 92.5±5.9 (100)  99.0±1.1 (250)  Table 3: Accuracy in percentage of different models for different tasks. The number in parentheses is number of training examples required to reach that level of accuracy.  We further developed two new bAbI-like tasks based on algorithmic problems on graphs: Shortest Paths, and Eulerian Circuits. For the ﬁrst, we generate random graphs and produce a story that lists all edges in the graphs. Questions come from choosing two random nodes A and B and asking for the shortest path (expressed as a sequence of nodes) that connects the two chosen nodes. We constrain the data generation to only produce questions where there is a unique shortest path from A to B of length at least 2. For Eulerian circuits, we generate a random two-regular connected graph and a separate random distractor graph. The question gives two nodes A and B to start the circuit, then the question is to return the Eulerian circuit (again expressed as a sequence of nodes) on the given subgraph that starts by going from A to B. Results are shown in the Table 3. RNN and LSTM fail on both tasks, but GGS-NNs learns to make perfect predictions using only 50 training examples.  6 PROGRAM VERIFICATION WITH GGS-NNS  Our work on GGS-NNs is motivated by a practical application in program veriﬁcation. A crucial step in automatic program veriﬁcation is the inference of program invariants, which approximate the set of program states reachable in an execution. Finding invariants about data structures is an open problem. As an example, consider the simple C function on the right.  8  Published as a conference paper at ICLR 2016  node* concat(node* a, node* b) {  cur = cur->next;  cur->next = b; return a;  if (a == NULL) return b; node* cur = a; while (cur.next != NULL)  To prove that this program indeed concatenates the two lists a and b and that all pointer dereferences are valid, we need to (mathematically) characterize the program’s heap in each iteration of the loop. For this, we use separation logic (O’Hearn et al., 2001; } Reynolds, 2002), which uses inductive predicates to describe abstract data structures. For example, a list segment is deﬁned as ls(x, y) ≡ x = y ∨ ∃v, n.ls(n, y) ∗ x (cid:55)→ {val : v, next : n}, where x (cid:55)→ {val : v, next : n} means that x points to a memory region that contains a structure with val and next ﬁelds whose values are in turn v and n. The ∗ connective is a conjunction as ∧ in Boolean logic, but additionally requires that its operators refer to “separate” parts of the heap. Thus, ls(cur, NULL) implies that cur is either NULL, or that it points to two values v, n on the heap, where n is described by ls again. The formula ∃t.ls(a, cur)∗ ls(cur, NULL)∗ ls(b, t) is an invariant of the loop (i.e., it holds when entering the loop, and after every iteration). Using it, we can prove that no program run will fail due to dereferencing an unallocated memory address (this property is called memory safety) and that the function indeed concatenates two lists using a Hoare-style veriﬁcation scheme (Hoare, 1969). The hardest part of this process is coming up with formulas that describe data structures, and this is where we propose to use machine learning. Given a program, we run it a few times and extract the state of memory (represented as a graph; see below) at relevant program locations, and then predict a separation logic formula. Static program analysis tools (e.g., (Piskac et al., 2014)) can check whether a candidate formula is sufﬁcient to prove the desired properties (e.g., memory safety).  6.1 FORMALIZATION  Representing Heap State as a Graph As inputs we consider directed, possibly cyclic graphs representing the heap of a program. These graphs can be automatically constructed from a program’s memory state. Each graph node v corresponds to an address in memory at which a sequence of pointers v0, . . . , vk is stored (we ignore non-pointer values in this work). Graph edges reﬂect these pointer values, i.e., v has edges labeled with 0, . . . , k that point to nodes v0, . . . , vk, respectively. A subset of nodes are labeled as corresponding to program variables. An example input graph is displayed as “Input” in Fig. 3. In it, the node id (i.e., memory address) is displayed in the node. Edge labels correspond to speciﬁc ﬁelds in the program, e.g., 0 in our example corresponds to the next pointer in our example function from the previous section. For binary trees there are two more types of pointers left and right pointing to the left and right children of a tree node.  Output Representation Our aim is to mathematically describe the shape of the heap. In our model, we restrict ourselves to a syntactically restricted version of separation logic, in which formulas are of the form ∃x1, . . . , xn.a1 ∗ . . . ∗ am, where each atomic formula ai is either ls(x, y) (a list from x to y), tree(x) (a binary tree starting in x), or none(x) (no data structure at x). Existential quantiﬁers are used to give names to heap nodes which are needed to describe a shape, but not labeled by a program variable. For example, to describe a “panhandle list” (a list that ends in a cycle), the ﬁrst list element on the cycle needs to be named. In separation logic, this can be expressed as ∃t.ls(x, t) ∗ ls(t, t).  Data We can generate synthetic (labeled) datasets for this problem. For this, we ﬁx a set of pred- icates such as ls and tree (extensions could consider doubly-linked list segments, multi-trees, . . .) together with their inductive deﬁnitions. Then we enumerate separation logic formulas instantiat- ing our predicates using a given set of program variables. Finally, for each formula, we enumerate heap graphs satisfying that formula. The result is a dataset consisting of pairs of heap graphs and associated formulas that are used by our learning procedures.  6.2 FORMULATION AS GGS-NNS  It is easy to obtain the node annotations for the intermediate prediction steps from the data generation process. So we train a variant of GGS-NN with observed annotations (observed at training time; not test time) to infer formulas from heap graphs. Note that it is also possible to use an unobserved GGS-  9  Published as a conference paper at ICLR 2016  Step  Labeled Graph  Out  Step  Labeled Graph  Out  Input  Line 3/(†)  Line 4-7/(‡)  Line 10 (for b)  b 1 b 1 b 1 b 1  2  2  2  2  0  0  0  0  3  3 t 3 t 3  0  0  0  0  0  0  0  0  0  0  0  0  4  4  4 ∃t.  4 ∃t.  Line 11/((cid:63))  Line 13,14/(♥)  Line 18/(♠)  Line 10 (for t)  b 1 b 1 b 1 b 1  2  2  2  2  0  0  0  0  0  0  0  0  t 3 t 3 t 3 t 3  0  0  0  0  0  0  0  0  4 ∃t.  4 ∃t.ls(b, t)∗  4 ∃t.ls(b, t)∗ 4 ∃t.ls(b, t)∗  Figure 3: Illustration of the ﬁrst 8 steps to predict a separation logic formula from a memory state. Label is-named signiﬁed by variable near node, active by double border, is-explained by white ﬁll.  NN variant and do end-to-end learning. The procedure breaks down the production of a separation logic formula into a sequence of steps. We ﬁrst decide whether to declare existential variables, and if so, choose which node corresponds to the variable. Once we have declared existentials, we iterate over all variable names and produce a separation logic formula describing the data structure rooted at the node corresponding to the current variable. The full algorithm for predicting separation logic formula appears below, as Alg. 1. We use three explicit node annotations, namely is-named (heap node labeled by program variable or declared existentially quantiﬁed variable), active (cf. algorithm) and is-explained (heap node is part of data structure already predicted). Initial node labels can be directly computed from the input graph: “is- named” is on for nodes labeled by program variables, “active” and “is-explained” are always off (done in line 2). The commented lines in the algorithm are implemented using a GG-NN, i.e., Alg. 1 is an instance of our GGS-NN model. An illustration of the beginning of a run of the algorithm is shown in Fig. 3, where each step is related to one line of the algorithm.  Algorithm 1 Separation logic formula prediction procedure Input: Heap graph G with named program variables 1: X ← compute initial labels from G 2: H ← initialize node vectors by 0-extending X 3: while ∃ quantiﬁer needed do t ← fresh variable name 4: v ← pick node 5: X ← turn on “is-named” for v in X 6: print “∃t.” 7: 8: end while 9: for node v(cid:96) with label “is-named” in X do H ← initialize node vectors, turn on “active” label for v(cid:96) in X 10: pred ← pick data structure predicate 11: if pred = ls then 12: 13: 14: 15: 16: 17: 18: 19: end for  (cid:96)end ← pick list end node print “ls((cid:96), (cid:96)end ) ∗” print “pred ((cid:96)) ∗”  end if X ← update node annotations in X  else  (cid:46) Graph-level Classiﬁcation (†) (cid:46) Node Selection (‡)  (cid:46) Graph-level Classiﬁcation ((cid:63)) (cid:46) Node Selection (♥)  (cid:46) Node Annotation (♠)  6.3 MODEL SETUP DETAILS We use the full GGS-NN model where F (k) and F (k)X have separate propagation models. For all the GG-NN components in the GGS-NN pipeline, we unrolled the propagation process for 10 time steps. The GGS-NNs associated with step (†) (deciding wheter more existentially quantiﬁed variable need to be declared) and (‡) (identify which node need to be declared as existentially quantiﬁed)  o  10  Published as a conference paper at ICLR 2016  uses D = 16 dimensional node representations. For all other GGS-NN components, D = 8 is used. Adam (Kingma & Ba, 2014) is used for optimization, the models are trained on minibatches of 20 graphs, and optimized until training error is very low. For the graph-level classiﬁcation tasks, we also artiﬁcially balanced classes to have even number of examples from each class in each minibatch. All the GGS-NN components contain less than 5k parameters and no overﬁtting is observed during training.  6.4 BATCH PREDICTION DETAILS  Vg(t), where Vg(t) maps variable name t to a node in graph g, and og  In practice, a set of heap graphs will be given as input and a single output formula is expected to describe and be consistent with all the input graphs. The different heap graphs can be snapshots of the heap state at different points in the program execution process, or different runs of the same program with different inputs. We call this the “batch prediction” setup contrasting with the single graph prediction described in the main paper. To make batch predictions, we run one GGS-NN for each graph simultaneously. For each prediction step, the outputs of all the GGS-NNs at that step across the batch of graphs are aggregated. For node selection outputs, the common named variables link nodes on different graphs togeter, which is the key for aggregating predictions in a batch. We compute the score for a particular named variable t as ot =(cid:80)g og Vg(t) is the output score for named variable t in graph g. When applying a softmax over all names using ot as scores, this is equivalent to a model that computes p(toselect = t) =(cid:81)g pg(toselect = Vg(t)). or equivalently compute p(class = k) =(cid:81)g pg(class = k). Node annotation outputs are updated  for each graph independently as different graphs have completely different set of nodes. However, when the algorithm tries to update the annotation for one named variable, the nodes associated with that variable in all graphs are updated. During training, all labels for intermediate steps are available to us from the data generation process, so the training process again can be decomposed to single output single graph training. A more complex scenario allowing for nested data structures (e.g., list of lists) was discussed in Brockschmidt et al. (2015). We have also successfully extended the GGS-NN model to this case. More details on this can be found in Appendix C.  For graph-level classiﬁcation outputs, we add up scores of a particular class across the batch of graphs,  6.5 EXPERIMENTS.  For this paper, we produced a dataset of 327 formulas that involves three program variables, with 498 graphs per formula, yielding around 160,000 formula/heap graph combinations. To evaluate, we split the data into training, validation and test sets using a 6:2:2 split on the formulas (i.e., the formulas in the test set were not in the training set). We measure correctness by whether the formula predicted at test time is logically equivalent to the ground truth; equivalence is approximated by canonicalizing names and order of the formulas and then comparing for exact equality. We compared our GGS-NN-based model with a method we developed earlier (Brockschmidt et al., 2015). The earlier approach treats each prediction step as standard classiﬁcation, and requires com- plex, manual, problem-speciﬁc feature engineering, to achieve an accuracy of 89.11%. In contrast, our new model was trained with no feature engineering and very little domain knowledge and achieved an accuracy of 89.96%. An example heap graph and the corresponding separation logic formula found by our GGS-NN model is shown in Fig. 4. This example also involves nested data structures and the batching extension developed in the previous section. We have also successfully used our new model in a program veriﬁcation framework, supplying needed program invariants to a theorem prover to prove correctness of a collection of list-manipulating algorithms such as insertion sort. The following Table 4 lists a set of benchmark list manipulation programs and the separation logic formula invariants found by the GGS-NN model, which were successfully used in a veriﬁcation framework to prove the correctness of corresponding programs.  11  Published as a conference paper at ICLR 2016  Figure 4: A heap graph example that contains two named variables arg1 and arg2, and one iso- lated NULL node (node 1). All the edges to NULL are not shown here for clarity. The numbers on edges indicate different edge types. Our GGS-NN model successfully ﬁnds the right formula ls(arg1, NULL, λt1 → ls(t1, NULL,(cid:62))) ∗ tree(arg2, λt2 → ∃e1.ls(t2, e1,(cid:62)) ∗ ls(e1, e1,(cid:62))).  Program Traverse1 Traverse2 curr (cid:54)= NULL ∗ lst (cid:54)= NULL ∗ ls(lst, curr) ∗ ls(curr, NULL) Concat  Invariant Found ls(lst, curr) ∗ ls(curr, NULL) a (cid:54)= NULL ∗ a (cid:54)= b ∗ b (cid:54)= curr ∗ curr (cid:54)= NULL ∗ls(curr, NULL) ∗ ls(a, curr) ∗ ls(b, NULL) ls(curr, NULL) ∗ ls(lst, curr) ∗ ls(cp, NULL) ls(lst, NULL) curr (cid:54)= NULL ∗ curr (cid:54)= elt ∗ elt (cid:54)= NULL ∗ elt (cid:54)= lst ∗ lst (cid:54)= NULL ∗ls(elt, NULL) ∗ ls(lst, curr) ∗ ls(curr, NULL) curr (cid:54)= NULL ∗ lst (cid:54)= NULL ∗ ls(lst, curr) ∗ ls(curr, NULL)  Copy Dispose Insert  Remove  Table 4: Example list manipulation programs and the separation logic formula invariants the GGS- NN model founds from a set of input graphs. The “(cid:54)=” parts are produced by a deterministic procedure that goes through all the named program variables in all graphs and checks for inequality.  A further extension of the current pipeline has been shown to be able to successfully prove more sophisticated programs like sorting programs and various other list-manipulating programs.  7 RELATED WORK  The most closely related work is GNNs, which we have discussed at length above. Micheli (2009) proposed another closely related model that differs from GNNs mainly in the output model. GNNs have been applied in several domains (Gori et al., 2005; Di Massa et al., 2006; Scarselli et al., 2009; Uwents et al., 2011), but they do not appear to be in widespread use in the ICLR community. Part of our aim here is to publicize GNNs as a useful and interesting neural network variant. An analogy can be drawn between our adaptation from GNNs to GG-NNs, to the work of Domke (2011) and Stoyanov et al. (2011) in the structured prediction setting. There belief propagation (which  12  Published as a conference paper at ICLR 2016  must be run to near convergence to get good gradients) is replaced with truncated belief propagation updates, and then the model is trained so that the truncated iteration produce good results after a ﬁxed number of iterations. Similarly, Recursive Neural Networks (Goller & Kuchler, 1996; Socher et al., 2011) being extended to Tree LSTMs (Tai et al., 2015) is analogous to our using of GRU updates in GG-NNs instead of the standard GNN recurrence with the aim of improving the long-term propagation of information across a graph structure. The general idea expressed in this paper of assembling problem-speciﬁc neural networks as a compo- sition of learned components has a long history, dating back at least to the work of Hinton (1988) on assembling neural networks according to a family tree structure in order to predict relations between people. Similar ideas appear in Hammer & Jain (2004) and Bottou (2014). Graph kernels (Shervashidze et al., 2011; Kashima et al., 2003) can be used for a variety of kernel- based learning tasks with graph-structured inputs, but we are not aware of work that learns the kernels and outputs sequences. Perozzi et al. (2014) convert graphs into sequences by following random walks on the graph then learns node embeddings using sequence-based methods. Sperduti & Starita (1997) map graphs to graph vectors then classify using an output neural network. There are several models that make use of similar propagation of node representations on a graph structure. Bruna et al. (2013) generalize convolutions to graph structures. The difference between their work and GNNs is analogous to the difference between convolutional and recurrent networks. Duvenaud et al. (2015) also consider convolutional like operations on graphs, building a learnable, differentiable variant of a successful graph feature. Lusci et al. (2013) converts an arbitrary undirected graph to a number of different DAGs with different orientations and then propagates node representations inwards towards each root, training an ensemble of models. In all of the above, the focus is on one-step problems. GNNs and our extensions have many of the same desirable properties of pointer networks (Vinyals et al., 2015); when using node selection output layers, nodes from the input can be chosen as outputs. There are two main differences: ﬁrst, in GNNs the graph structure is explicit, which makes the models less general but may provide stronger generalization ability; second, pointer networks require that each node has properties (e.g., a location in space), while GNNs can represent nodes that are deﬁned only by their position in the graph, which makes them more general along a different dimension. GGS-NNs are related to soft alignment and attentional models (e.g., Bahdanau et al. (2014); Kumar et al. (2015); Sukhbaatar et al. (2015)) in two respects: ﬁrst, the graph representation in Eq. 7 uses context to focus attention on which nodes are important to the current decision; second, node annotations in the program veriﬁcation example keep track of which nodes have been explained so far, which gives an explicit mechanism for making sure that each node in the input has been used over the sequence of producing an output.  8 DISCUSSION  What is being learned? It is instructive to consider what is being learned by the GG-NNs. To do so, we can draw analogy between how the bAbI task 15 would be solved via a logical formulation. As an example, consider the subset of lines needed to answer one example on the right. To do logical reasoning, we would need not only a logical encoding of the facts present in the story but also the background world knowledge encoded as inference rules such as  B is E E has_fear H eval B has_fear  is(x, y) ∧ has-fear(y, z) =⇒ has-fear(x, z).  (9)  Our encoding of the tasks simpliﬁes the parsing of the story into graph form, but it does not provide any of the background knowledge. The GG-NN model can be seen as learning this, with results stored in the neural network weights.  Discussion The results in the paper show that GGS-NNs have desirable inductive biases across a range of problems that have some intrinsic graph structure to them, and we believe there to be many more cases where GGS-NNs will be useful. There are, however, some limitations that need to be overcome to make them apply even more broadly. Two limitations that we mentioned previously are that the bAbI task translation does not incorporate temporal order of inputs or ternary and higher order relations. We can imagine several possibilities for lifting these restrictions, such as concatenating a  13  Published as a conference paper at ICLR 2016  series of GG-NNs, where there is one GG-NNs for each edge, and representing higher order relations as factor graphs. A more signiﬁcant challenge is how to handle less structured input representations. For example, in the bAbI tasks it would be desirable not to use the symbolic form of the inputs. One possible approach is to incorporate less structured inputs, and latent vectors, in our GGS-NNs. However, experimentation is needed to ﬁnd the best way of addressing these issues. The current GGS-NNs formulation speciﬁes a question only after all the facts have been consumed. This implies that the network must try to derive all consequences of the seen facts and store all pertinent information to a node within its node representation. This is likely not ideal; it would be preferable to develop methods that take the question as an initial input, and then dynamically derive the facts needed to answer the question. We are optimistic about the further applications of GGS-NNs. We are particularly interested in continuing to develop end-to-end learnable systems that can learn about semantic properties of programs, that can learn more complicated graph algorithms, and in applying these ideas to problems that require reasoning over knowledge bases and databases. More generally, we consider these graph neural networks as representing a step towards a model that can combine structured representations with the powerful algorithms of deep learning, with the aim of taking advantage of known structure while learning and inferring how to reason with and extend these representations.  ACKNOWLEDGEMENTS  We thank Siddharth Krishna, Alex Gaunt, Emine Yilmaz, Milad Shokouhi, and Pushmeet Kohli for useful conversations and Douwe Kiela for comments on an earlier draft of the paper.  REFERENCES Almeida, Luis B. A learning rule for asynchronous perceptrons with feedback in a combinatorial  environment. In Artiﬁcial neural networks, pp. 102–111. IEEE Press, 1990.  Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly  learning to align and translate. CoRR, abs/1409.0473, 2014.  Bottou, L´eon. From machine learning to machine reasoning. Machine learning, 94(2):133–149,  2014.  Brockschmidt, Marc, Chen, Yuxin, Cook, Byron, Kohli, Pushmeet, and Tarlow, Daniel. Learning to decipher the heap for program veriﬁcation. In Workshop on Constructive Machine Learning at the International Conference on Machine Learning (CMLICML), 2015.  Bruna, Joan, Zaremba, Wojciech, Szlam, Arthur, and LeCun, Yann. Spectral networks and locally  connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.  Cho, Kyunghyun, Van Merri¨enboer, Bart, Gulcehre, Caglar, Bahdanau, Dzmitry, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.  Di Massa, Vincenzo, Monfardini, Gabriele, Sarti, Lorenzo, Scarselli, Franco, Maggini, Marco, and Gori, Marco. A comparison between recursive neural networks and graph neural networks. In International Joint Conference on Neural Networks (IJCNN), pp. 778–785. IEEE, 2006.  Domke, Justin. Parameter learning with truncated message-passing. In IEEE Conference on Com-  puter Vision and Pattern Recognition (CVPR), pp. 2937–2943. IEEE, 2011.  Duvenaud, David, Maclaurin, Dougal, Aguilera-Iparraguirre, Jorge, G´omez-Bombarelli, Rafael, Hirzel, Timothy, Aspuru-Guzik, Al´an, and Adams, Ryan P. Convolutional networks on graphs for learning molecular ﬁngerprints. arXiv preprint arXiv:1509.09292, 2015.  Goller, Christoph and Kuchler, Andreas. Learning task-dependent distributed representations by back- propagation through structure. In IEEE International Conference on Neural Networks, volume 1, pp. 347–352. IEEE, 1996.  14  Published as a conference paper at ICLR 2016  Gori, Marco, Monfardini, Gabriele, and Scarselli, Franco. A new model for learning in graph domains. In International Joint Conference onNeural Networks (IJCNN), volume 2, pp. 729–734. IEEE, 2005.  Hammer, Barbara and Jain, Brijnesh J. Neural methods for non-standard data. In European Sympo-  sium on Artiﬁcial Neural Networks (ESANN), 2004.  Hinton, Geoffrey E. Representing part-whole hierarchies in connectionist networks. In Proceedings  of the Tenth Annual Conference of the Cognitive Science Society, pp. 48–54. Erlbaum., 1988.  Hoare, Charles Antony Richard. An axiomatic basis for computer programming. Communications  of the ACM, 12(10):576–580, 1969.  Kashima, Hisashi, Tsuda, Koji, and Inokuchi, Akihiro. Marginalized kernels between labeled graphs. In Proceedings of the International Conference on Machine Learning, volume 3, pp. 321–328, 2003.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980, 2014.  Kumar, Ankit, Irsoy, Ozan, Su, Jonathan, Bradbury, James, English, Robert, Pierce, Brian, Ondruska, Peter, Gulrajani, Ishaan, and Socher, Richard. Ask me anything: Dynamic memory networks for natural language processing. arXiv preprint arXiv:1506.07285, 2015.  Lusci, Alessandro, Pollastri, Gianluca, and Baldi, Pierre. Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules. J Chem Inf Model, 2013.  Micheli, Alessio. Neural network for graphs: A contextual constructive approach. IEEE Transactions  on Neural Networks, 20(3):498–511, 2009.  O’Hearn, Peter, Reynolds, John C., and Yang, Hongseok. Local reasoning about programs that alter data structures. In 15th International Workshop on Computer Science Logic (CSL’01), pp. 1–19, 2001.  Perozzi, Bryan, Al-Rfou, Rami, and Skiena, Steven. Deepwalk: Online learning of social representa- tions. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 701–710. ACM, 2014.  Pineda, Fernando J. Generalization of back-propagation to recurrent neural networks. Physical  review letters, 59(19):2229, 1987.  Piskac, Ruzica, Wies, Thomas, and Zufferey, Damien. GRASShopper - complete heap veriﬁcation In 20st International Conference on Tools and Algorithms for the  with mixed speciﬁcations. Construction and Analysis of Systems (TACAS’14), pp. 124–139, 2014.  Reynolds, John C. Separation logic: A logic for shared mutable data structures.  Symposium on Logic in Computer Science (LICS’02), pp. 55–74, 2002.  In 7th IEEE  Scarselli, Franco, Gori, Marco, Tsoi, Ah Chung, Hagenbuchner, Markus, and Monfardini, Gabriele.  The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009.  Shervashidze, Nino, Schweitzer, Pascal, Van Leeuwen, Erik Jan, Mehlhorn, Kurt, and Borgwardt, Karsten M. Weisfeiler-lehman graph kernels. The Journal of Machine Learning Research, 12: 2539–2561, 2011.  Socher, Richard, Lin, Cliff C, Manning, Chris, and Ng, Andrew Y. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 129–136, 2011.  Sperduti, Alessandro and Starita, Antonina. Supervised neural networks for the classiﬁcation of  structures. IEEE Transactions on Neural Networks, 8(3):714–735, 1997.  15  Published as a conference paper at ICLR 2016  Stoyanov, Veselin, Ropson, Alexander, and Eisner, Jason. Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 725–733, 2011.  Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and Fergus, Rob. End-to-end memory networks.  arXiv preprint arXiv:1503.08895, 2015.  Tai, Kai Sheng, Socher, Richard, and Manning, Christopher D. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.  Uwents, Werner, Monfardini, Gabriele, Blockeel, Hendrik, Gori, Marco, and Scarselli, Franco. Neural networks for relational learning: an experimental comparison. Machine Learning, 82(3):315–349, 2011.  Vinyals, Oriol, Fortunato, Meire, and Jaitly, Navdeep.  arXiv:1506.03134, 2015.  Pointer networks.  arXiv preprint  Weston, Jason, Bordes, Antoine, Chopra, Sumit, and Mikolov, Tomas. Towards ai-complete question  answering: a set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.  16  Published as a conference paper at ICLR 2016  A CONTRACTION MAP EXAMPLE Consider a linear 1-hidden unit cycle-structured GNN with N nodes {1, . . . , N}. For simplicity we ignored all edge labels and node labels, equivalently this is a simple example with LV = 1 and LE = 1. At each timestep t we update hidden states h1, . . . , hN as  (10) i−1 + bi, for each i, where mi and bi are parameters of the propagation model. We use the convention that hj cycles around and refers to hN +j when j ≤ 0. Let h(t) = [h(t)  1 , . . . , h(t)  N ](cid:62),  i = mi · h(t−1) h(t)  0 0 0  0 0 m2 0 0 m3 ...  M =  . . . m1 0 0  ... mN    0 and b = [b1, . . . , bN ](cid:62). We can write the joint update for all i as  0  0  (12) Restrict the update to deﬁne a contraction mapping in the Euclidean metric. This means that there is some ρ < 1 such that for any h, h(cid:48),  h(t) = Mh(t−1) + b = T (h(t−1))  ||T (h) − T (h(cid:48))|| < ρ||h − h(cid:48)||,  or in other words,  ||M(h − h(cid:48))|| < ρ||h − h(cid:48)||.  (14) We can immediately see that this implies that |mi| < ρ for each i by letting h be the elementary vector that is all zero except for a 1 in position i − 1 and letting h(cid:48) be the all zeros vector. Expanding Eq. 10, we get  i−1 + bi−1) + bi i−1 + mibi−1 + bi  i = mi · (mi−1h(t−2) h(t) = mimi−1h(t−2) = mimi−1(mi−2h(t−3) = mimi−1mi−2h(t−3)  i−2 + bi−2) + mibi−1 + bi i−2 + mimi−1bi−2 + mibi−1 + bi.  (15) In the GNN model, node label li controls which values of mi and bi are used during the propagation. Looking at this expansion and noting that mi < ρ for all i, we see that information about labels of . Thus, at least in this simple case, the restriction that T be  ρ(cid:17)δ nodes δ away will decay at a rate of(cid:16) 1  a contraction means that it is not able to maintain long-range dependencies.  A.1 NONLINEAR CASE  The same analysis can be applied to a nonlinear update, i.e.  h(t)  i = σ(cid:16)mi · h(t−1)  i−1 + bi(cid:17) ,  where σ is any nonlinear function. Then T (h) = σ (Mh + b). Let T (h) = [T1(h), ..., TN (h)](cid:62), where Ti(h(t−1)) = h(t) . The contraction map deﬁnition Eq. 13 implies that each entry of the i Jacobian matrix of T is bounded by ρ, i.e.  (17) To see this, consider two vectors h and h(cid:48), where hk = h(cid:48)k,∀k (cid:54)= j and hj + ∆ = h(cid:48)j. The deﬁnition in Eq. 13 implies that for all i,  ||Ti(h) − Ti(h(cid:48))|| ≤ ||T (h) − T (h(cid:48))|| < ρ|∆|.  (18)  ∀i,∀j.  (cid:12)(cid:12)(cid:12)(cid:12)  ∂Ti  ∂hj(cid:12)(cid:12)(cid:12)(cid:12) < ρ,  (11)  (13)  (16)  17  Published as a conference paper at ICLR 2016  Ti(h1, ..., hj−1, hj, hj+1, ..., hN ) − Ti(h1, ..., hj−1, hj + ∆, hj+1, ..., hN )  Therefore  (cid:13)(cid:13)(cid:13)(cid:13) where the left hand side is(cid:13)(cid:13)(cid:13) ∂Ti  When j = i − 1,  Also, because of the special cycle graph structure, for all other js we have ∂Ti ∂hj the update at timestep t, we get  ∆  ∂Ti  ∂hj(cid:13)(cid:13)(cid:13) by deﬁnition as ∆ → 0. ∂hi−1(cid:12)(cid:12)(cid:12)(cid:12) < ρ. (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  could affect h(t) t  ∂h(t) ∂h(t−1)  (cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  < ρ.  i−1  i  (cid:13)(cid:13)(cid:13)(cid:13) < ρ,  (19)  (20)  = 0. Applying this to  (21)  Now let’s see how a change in h(1) 1 structure, we have  . Using the chain rule and the special graph  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  t  ∂h(t) ∂h(1)  1 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  t  2  =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  ∂h(t) ∂h(t−1) t−1 ∂h(t) ∂h(t−1)  · ∂h(t−1) t−1 ∂h(t−2) t−2 ∂h(t−1) t−1 ∂h(t−2) < ρ · ρ··· ρ = ρt−1.  t−1 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  t−2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  ·(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  ··· ∂h(2) ∂h(1)  1 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ···(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  ∂h(2) ∂h(1)  2  t  (22) As ρ < 1, this derivative will approach 0 exponentially fast as t grows. Intuitively, this means that the impact one node has on another node far away will decay exponetially, therefore making it difﬁcult to model long range dependencies.  B WHY ARE RNN AND LSTM SO BAD ON THE SEQUENCE PREDICTION  TASKS?  RNN and LSTM performance on the sequence prediction tasks, i.e. bAbI task 19, shortest path and Eulerian circuit, are very poor compared to single output tasks. The Eulerian circuit task is the one that RNN and LSTM fail most dramatically. A typical training example for this task looks like the following,  3 connected-to 7 7 connected-to 3 1 connected-to 2 2 connected-to 1 5 connected-to 7 7 connected-to 5 0 connected-to 4 4 connected-to 0 1 connected-to 0 0 connected-to 1 8 connected-to 6 6 connected-to 8 3 connected-to 6 6 connected-to 3 5 connected-to 8 8 connected-to 5 4 connected-to 2 2 connected-to 4 eval eulerian-circuit 5 7  5,7,3,6,8  18  Published as a conference paper at ICLR 2016  This describes a graph with two cycles 3-7-5-8-6 and 1-2-4-0, where 3-7-5-8-6 is the target cycle and 1-2-4-0 is a smaller distractor graph. All edges are presented twice in both directions for symmetry. The task is to ﬁnd the cycle that starts with the given two nodes and in the direction from the ﬁrst to the second. The distractor graph is added to increase the difﬁculty of this task, this also makes the output cycle not strictly “Eulerian”. For RNN and LSTM the above training example is further transformed into a sequence of tokens,  n4 e1 n8 eol n8 e1 n4 eol n2 e1 n3 eol n3 e1 n2 eol n6 e1 n8 eol n8 e1 n6 eol n1 e1 n5 eol n5 e1 n1 eol n2 e1 n1 eol n1 e1 n2 eol n9 e1 n7 eol n7 e1 n9 eol n4 e1 n7 eol n7 e1 n4 eol n6 e1 n9 eol n9 e1 n6 eol n5 e1 n3 eol n3 e1 n5 eol q1 n6 n8 ans 6 8 4 7 9  Note the node IDs here are different from the ones in the original symbolic data. The RNN and LSTM read through the whole sequence, and start to predict the ﬁrst output when reading the ans token. Then for each prediction step, the ans token is fed as the input and the target node ID (treated as a class label) is expected as the output. In this current setup, the output of each prediction step is not fed as the input for the next. Our GGS-NN model uses the same setup, where the output of one step is not used as input to the next, only the predicted node annotations X (k) carry over from one step to the next, so the comparison is still fair for RNN and LSTM. Changing both our method and the baselines to make use of previous predictions is left as future work. From this example we can see that the sequences the RNN and LSTM have to handle is quite long, close to 80 tokens before the predictions are made. Some predictions really depend on long range memory, for example the ﬁrst edge (3-7) and ﬁrst a few tokens (n4 e1 n8) in the sequence are needed to make prediction in the third prediction step (3 in the original symbolic data, and 4 in the tokenized RNN data). Keeping long range memory in RNNs is challenging, LSTMs do better than RNNs but still can’t completely solve the problem. Another challenge about this task is the output sequence does not appear in the same order as in the input sequence. In fact, the data has no sequential nature at all, even when the edges are randomly permutated, the target output sequence should not change. The same applies for bAbI task 19 and the shortest path task. GGS-NNs are good at handling this type of “static” data, while RNN and LSTM are not. However future work is needed to determine how best to apply GGS-NNs to temporal sequential data which RNN and LSTM are good at. This is one limitation of the GGS-NNs model which we discussed in Section 8.  C NESTED PREDICTION DETAILS  Data structures like list of lists are nested data structures, in which the val pointer of each node in a data structure points to another data structure. Such data structures can be represented in sep- aration logic by allowing predicates to be nested. For example, a list of lists can be represented as ls(x, y, λt → ls(t, NULL)), where λt → ls(t, NULL) is a lambda expression and says that for each node in the list from x to y, its val pointer t satisﬁes ls(t, NULL). So there is a list from x to y, where each node in that list points to another list. A simple list without nested structures can be represented as ls(x, y,(cid:62)) where (cid:62) represents an empty predicate. Note that unlike the non-nested case where the val pointer always points to NULL, we have to consider the val pointers here in order to describe and handle nested data structures. To make our GGS-NNs able to predict nested formulas, we adapt Alg. 1 to Alg. 2. Where an outer loop goes through each named variable once and generate a nested predicate with the node associated with that variable as the active node. The nested prediction procedure handles prediction similarly as in Alg. 1. Before calling the nested prediction procedure recursively, the node annotation update in line 32 not only annotates nodes in the current structure as “is-explained”, but also annotates nodes linked to via the “val” pointer from all nodes in the current structure as “active”. For the list of lists example, after predicting “ls(x, y,”, the annotation step annotates all nodes in the list from x to y as  19  Published as a conference paper at ICLR 2016  “is-explained” and all nodes the val pointer points to from the list as “active”. This knowledge is not hard coded into the algorithm, the annotation model can learn this behavior from data.  (cid:46) Graph G with named program variables  end for  e ← fresh existentially quantiﬁed variable name v ← pick node X ← turn on “is-named” for v in X print “∃e.”  X ← compute initial labels from G for each variable name var do v(cid:96) ← node associated with var turn on “active” bit for v(cid:96) in X PREDICTNESTEDFORMULA(G, X , var)  Algorithm 2 Nested separation logic formula prediction procedure 1: procedure OUTERLOOP(G) 2: 3: 4: 5: 6: 7: 8: end procedure 9: 10: procedure PREDICTNESTEDFORMULA(G, X , var) H ← initialize node vectors by 0-extending X 11: while ∃ quantiﬁer needed do 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: end if 31: X ← update node annotations in X 32: t ← fresh lambda variable name 33: PREDICTNESTEDFORMULA(G, X , t) 34: print “) ∗” 35: 36: end procedure  (cid:96)end ← pick list end node varend ← get variable name associated with (cid:96)end print “ls(var, varend ,”  end if pred ← pick data structure predicate if pred = ls then  print “tree(var,” print “none(var) ∗” return  end while if var is a lambda variable name then  else if pred = tree then  print “λ var.”  else  (cid:46) Graph-level Classiﬁcation (†) (cid:46) Node Selection (‡)  (cid:46) Graph-level Classiﬁcation ((cid:63)) (cid:46) Node Selection (♥)  (cid:46) Node Annotation (♠)  (cid:46) Recursively predict all nested formulas.  20  ",
1511.05939,2016,Metric Learning with Adaptive Density Discrimination,"['Metric Learning with Adaptive Density Discrimination\nOren Rippel', 'Manohar Paluri', 'Piotr Dollar', 'Lubomir Bourdev']",https://arxiv.org/pdf/1511.05939,"6 1 0 2    r a  M 2         ] L M  . t a t s [      2 v 9 3 9 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  METRIC LEARNING WITH ADAPTIVE DENSITY DISCRIMINATION  Oren Rippel MIT, Facebook AI Research rippel@math.mit.edu  Piotr Dollar Facebook AI Research pdollar@fb.com  Manohar Paluri Facebook AI Research mano@fb.com  Lubomir Bourdev UC Berkeley lubomir.bourdev@gmail.com  ABSTRACT  Distance metric learning (DML) approaches learn a transformation to a represen- tation space where distance is in correspondence with a predeﬁned notion of sim- ilarity. While such models offer a number of compelling beneﬁts, it has been dif- ﬁcult for these to compete with modern classiﬁcation algorithms in performance and even in feature extraction. In this work, we propose a novel approach explicitly designed to address a num- ber of subtle yet important issues which have stymied earlier DML algorithms. It maintains an explicit model of the distributions of the different classes in repre- sentation space. It then employs this knowledge to adaptively assess similarity, and achieve local discrimination by penalizing class distribution overlap. We demonstrate the effectiveness of this idea on several tasks. Our approach achieves state-of-the-art classiﬁcation results on a number of ﬁne-grained visual recognition datasets, surpassing the standard softmax classiﬁer and outperform- ing triplet loss by a relative margin of 30-40%. In terms of computational perfor- mance, it alleviates training inefﬁciencies in the traditional triplet loss, reaching the same error in 5-30 times fewer iterations. Beyond classiﬁcation, we further validate the saliency of the learnt representations via their attribute concentration and hierarchy recovery properties, achieving 10-25% relative gains on the softmax classiﬁer and 25-50% on triplet loss in these tasks.  1  INTRODUCTION  The problem of classiﬁcation is a mainstay task in machine learning, as it provides us with a coherent metric to gauge progress and juxtapose new ideas against existing approaches. To tackle various other tasks beyond categorization, we often require alternative representations of our inputs which provide succinct summaries of relevant characteristics. Here, classiﬁcation algorithms often serve as convenient feature extractors: a very popular approach involves training a network for classiﬁcation on a large dataset, and retaining the outputs of the last layer as inputs transferred to other tasks (Donahue et al., 2013; Sharif Razavian et al., 2014; Qian et al., 2015; Snoek et al., 2015). However, this paradigm exhibits an intrinsic discrepancy: we have no guarantee that our extracted features are suitable for any task but the particular classiﬁcation problem from which they were derived. On the contrary: in our classiﬁcation procedure, we propagate high-dimensional inputs through a complex pipeline, and map each to a single, scalar prediction. That is, we explicitly demand our algorithm to, ultimately, dispose of all information but class label. In the process, we destroy intra- and inter-class variation that would in fact be desirable to maintain in our features. In principle, we have no reason to compromise: we should be able to construct a representation which is amenable to classiﬁcation, while still maintaining more ﬁne-grained information. This philosophy motivates the class of distance metric learning (DML) approaches, which learn a trans- formation to a representation space where distance is in correspondence with a notion of similarity.  1  Published as a conference paper at ICLR 2016  Figure 1: Distance metric learning approaches sculpt a representation space where distance is in correspondence with a notion of similarity. Traditionally, similarity is speciﬁed a-priori and of- ten strictly semantically. In contrast, Magnet Loss adaptively sculpts its representation space by autonomously identifying and respecting intra-class variation and inter-class similarity.  Metric learning offers a number of beneﬁts: for example, it enables zero-shot learning (Mensink et al., 2013; Chopra et al., 2005), visualization of high-dimensional data (van der Maaten & Hinton, 2008), learning invariant maps (Hadsell et al., 2006), and graceful scaling to instances with millions of classes (Schroff et al., 2015). In spite of this, it has been difﬁcult for DML-based approaches to compete with modern classiﬁcation algorithms in performance and even in feature extraction. Admittedly, however, these are two sides of the same coin: a more salient representation should, in theory, enable improved classiﬁcation performance and features for task transfer. In this work, we strive to reconcile this gap. We introduce Magnet Loss, a novel approach explicitly designed to address subtle yet important issues which have hindered the quality of learnt representations and the training efﬁciency of a class of DML approaches. In essence, instead of penalizing individual examples or triplets, it maintains an explicit model of the distributions of the different classes in representation space. It then employs this knowledge to adaptively assess similarity, and achieve discrimination by reducing local distribution overlap. It utilizes clustering techniques to simulta- neously tackle a number of components in model design, from capturing the distributions of the different classes to hard negative mining. For a particular set of assumptions in its conﬁguration, it reduces to the familiar triplet loss (Weinberger & Saul, 2009). We demonstrate the effectiveness of this idea on several tasks. Using a soft k-nearest-cluster metric for evaluation, this approach achieves state-of-the-art classiﬁcation results on a number of ﬁne- grained visual recognition datasets, surpassing the standard softmax classiﬁer and outperforming triplet loss by a relative margin of 30-40%. In terms of computational performance, it alleviates sev- eral training inefﬁciencies in traditional triplet-based approaches, reaching the same error in 5-30 times fewer iterations. Beyond classiﬁcation, we further validate the saliency of the learnt represen- tations via their attribute concentration and hierarchy recovery properties, achieving 10-25% relative gains on the softmax classiﬁer and 25-50% on triplet loss in these tasks.  2 MOTIVATION: CHALLENGES IN DISTANCE METRIC LEARNING  We start by providing an overview of challenges which we believe have been impeding the success of existing distance metric learning approaches. These will motivate our work to follow.  Issue #1: predeﬁned target neighbourhood structure All metric learning approaches must de- ﬁne a relationship between similarity and distance, which prescribes neighbourhood structure. The corresponding training algorithm, then, learns a transformation to a representation space where this property is obeyed. In existing approaches, similarity has been canonically deﬁned a-priori by in- tegrating available supervised knowledge. The most common is semantic, informed by class labels. Finer assignment of neighbourhood structure is enabled with access to additional prior information, such as similarity ranking (Wang et al., 2014) and hierarchical class taxonomy (Verma et al., 2012).  2  Published as a conference paper at ICLR 2016  Figure 2: 2D visualizations of representations attained by training triplet loss, Magnet Loss and a softmax classiﬁer on 10 classes of ImageNet. The different colours correspond to different classes, and the values to density estimates computed from an application of t-SNE (van der Maaten & Hinton, 2008) on the original 1024-dimensional representations. The white dots in the Magnet t-SNE correspond to K = 32 clusters used by Magnet to capture each class. The red arrows retrieve the examples closest to particular clusters (which were learnt autonomously). 1. It can be seen that triplet loss and softmax result in unimodal separation, due to enforcement of semantic similarity. For Magnet Loss, the distributions of the different classes may arbitrarily split, adaptively embracing intra-class variation and inter-class similarity. 2. Green corresponds to manta-rays, blue to sharks, and magenta to gazelles. Magnet Loss captures intra-class variation between (c) and (b) as manta-rays in the deep, and manta- rays with people. It also respects inter-class similarity, allowing shared structure between (c) and (d) as ﬁsh in the deep, and between (a) and (b) as animals with people. See Appendix A for image maps of other t-SNE projections.  In practice, however, the only available supervision is often in the form of class labels. In this case, a ubiquitous solution is to enforce semantic similarity: examples of each class are demanded to be tightly clustered together, far from examples of other classes (for example, Schroff et al. (2015); Norouzi et al. (2012); Globerson & Roweis (2006); Chopra et al. (2005)). However, this collapses intra-class variation and does not embrace shared structure between different classes. Hence, this imposes too strong of a requirement, as each class is assumed to be captured by a single mode. This issue is well-known, and has motivated the notion of local similarity: each example is desig- nated only a small number of target neighbours of the same class (Weinberger & Saul, 2009; Qian et al., 2015; Hadsell et al., 2006). In existing work, these target neighbours are determined prior to training: they are retrieved based on distances in the original input space, and after which are never updated again. Ironically, this is in contradiction with our fundamental assumption which motivated us to pursue a DML approach in the ﬁrst place. Namely, we want to learn a metric because we cannot trust distances in our original input space — but on the other hand deﬁne target similarity using this exact metric that cannot be trusted! Thus, although this approach has the good intentions of encoding similarity into our representation, it harms intra-class variation and inter-class similarity by enforcing unreasonable proximity relationships. Apart from its information preservation rami- ﬁcations, achieving predeﬁned separation requires signiﬁcant effort, which results in inefﬁciencies during training time. Instead, what we ought to do is rather deﬁne similarity as function of distances of our represen- tations — which lie in precisely the space sculpted for metric saliency. Since representations are adjusted continuously during training, it then follows that similarity must be deﬁned adaptively. To that end, we must alternate between updating our representations, and refreshing our model which designates similarity as function of these. Visualizations of representations of different DML ap- proaches can be found in a toy example in Figure 2.  Issue #2: objective formulation Two very popular classes of DML approaches have stemmed from Triplet Loss (Weinberger & Saul, 2009) and Contrastive Loss (Hadsell et al., 2006). The outlined issues apply to both, but for simplicity of exposition we use triplet loss as an example.  3  Published as a conference paper at ICLR 2016  (a) Triplet: before.  (b) Triplet: after.  (c) Magnet: before.  (d) Magnet: after.  Figure 3: The intuition behind triplet loss and Magnet Loss. Triplet loss only considers a single triplet at a time, resulting in reduced performance and training inefﬁciencies. In contrast, in Mag- net Loss, at each iteration an entire local neighbourhood of nearest clusters is retrieved, and their overlaps are penalized. Insight into representation distribution permits adaptive similarity character- ization, local discrimination and a globally consistent optimization procedure.  During its training, triplets consisting of a seed example, a “positive” example similar to the seed and m and r− a “negative” dissimilar example are sampled. Let us denote their representations as rm, r+ m for m = 1, . . . , M. Triplet loss then demands that the difference of distances of the representation of the seed to the negative and to the positive be larger than some pre-assigned margin constant α ∈ R:  Ltriplet (Θ) =  1 M  (cid:110)(cid:13)(cid:13)rm − r−  m  2 −(cid:13)(cid:13)rm − r+ (cid:13)(cid:13)2  m  (cid:13)(cid:13)2  (cid:111)  M(cid:88)  m=1  2 + α  +  ,  (1)  where {·}+ is the hinge function and Θ the parameters of the map to representation space. The representations are often normalized to achieve scale invariance, and negative examples are mined in order to ﬁnd margin violators (for example, Schroff et al. (2015); Norouzi et al. (2012)). Objectives formulated in this spirit exhibit a short-sightedness. Namely, penalizing individual pairs or triplets of examples does not employ sufﬁcient contextual insight of neighbourhood structure, and as such different triplet terms are not necessarily consistent. This hinders both the convergence rate as well as performance of these approaches. Moreover, the cubic growth of the number of triplets renders operation on these computationally inefﬁcient. In contrast to this, it is desirable to instead inform the algorithm of the distributions of the differ- ent classes in representation space and their overlaps, and rather manipulate these in a way that is globally consistent. We elaborate on this in the section below.  3 MAGNET LOSS FOR DISTANCE METRIC LEARNING  We proceed to design a model to mitigate the identiﬁed difﬁculties. Let us for a moment neglect practical considerations, and envision our ideal DML approach. To start, as concluded at the start of Section 2, we are interested to characterize similarity adaptively as function of current representation structure. We would then utilize this knowledge to pursue local separation as opposed to global: we seek to separate between distributions of different classes in representation space, but do not mind if they are interleaved. As such, let us assume that we have knowledge of the representation distribution of each class at any time during training. Our DML algorithm, then, would discover regions of local overlap between different classes, and penalize these to achieve discrimination. Such an approach would liberate us from the unimodality assumption and unreasonable prior tar- get neighbourhood assignments — resulting in a more expressive representation which maintains signiﬁcantly more information. Moreover, employing a loss informed of distributions rather than individual examples would allow for a more coherent training procedure, where the distance metric is adjusted in a way that is globally consistent. To that end, a natural approach would be to employ clustering techniques to capture these distribu- tions in representation space. Namely, for each class, we will maintain an index of clusters, which we will update continuously throughout training. Our objective, then, would jointly manipulate entire clusters — as opposed to individual examples — in the pursuit of local discrimination. This intuition  4  넃넃Published as a conference paper at ICLR 2016  of cluster attraction and repulsion motivates us to name it Magnet Loss. A caricature illustrating the intuition behind this approach can be found in Figure 3. In addition to its advantages from a modeling perspective, a clustering-based approach also facili- tates computation by enabling efﬁcient hard negative mining. That is, we may perform approximate nearest neighbour retrieval in a two-step process, where we ﬁrst retrieve nearest clusters, after which we retrieve examples from these clusters. Finally, as discussed, throughout training we are interested in a more complete characterization of neighbourhood structure. At each iteration, we sample entire local neighbourhoods rather than collections of independent examples (or triplets) as per usual, which signiﬁcantly improves training efﬁciency. We elaborate on this in Section 3.2.  3.1 MODEL FORMULATION  We proceed to quantify the modeling objectives outlined above. Let us assume we have a training set consisting of N input-label pairs D = {xn, yn}N n=1 belonging to C classes. We consider a parametrized map f (·; Θ) which hashes our inputs to representation space, and denote their rep- resentations as rn = f (xn; Θ), n = 1, . . . , N. In this work, we select this transformation as GoogLeNet (Szegedy et al., 2015; Ioffe & Szegedy, 2015), which has been demonstrated to be a powerful CNN architecture; in Section 4 we elaborate on this choice. We assume that, for each class c, we have K cluster assignments I c K obtained via an ap- plication of the K-means algorithm. Note that K may vary across classes, but for simplicity of exposition we ﬁx it as uniform. In Section 3.2, we discuss how to maintain this index. To that end, we assume that these assignments have been chosen to minimize intra-cluster distances. Namely, for each class c, we have  1, . . . ,I c  K(cid:88)  (cid:88)  k=1  r∈I c  k  I c 1, . . . ,I c  K = arg min I c 1 ,...,I c K  (cid:107)r − µc  k(cid:107)2 2 ,  (cid:88)  r∈I c  k  r .  µc  k =  1 |I c k|  We further deﬁne C(r) as the class of representation r, and µ(r) as its assigned cluster center. We proceed to deﬁne our objective as follows:  − log  N(cid:88)  n=1  (cid:80)  L (Θ) =  1 N    c(cid:54)=C(rn)  e  (cid:80)K  2−α  − 1 2σ2 (cid:107)rn−µ(rn)(cid:107)2 2σ2(cid:107)rn−µc k(cid:107)2 − 1 (cid:80) r∈D (cid:107)r − µ(r)(cid:107)2  k=1 e  +  2  where {·}+ is the hinge function, α ∈ R is a scalar, and σ2 = 1 2 is the vari- N−1 ance of all examples away from their respective centers. We note that cluster centers sufﬁciently far from a particular example vanish from its term in the objective. This allows accurately approximat- ing each term with a small number of nearest clusters. A feature of this objective not usually available in standard distance metric learning approach is variance standardization. This renders the objective invariant to the characteristic lengthscale of the problem, and allows the model to gauge its conﬁdence of prediction by comparison of intra- and inter-cluster distances. With this in mind, α is then the desired cluster separation gap, measured in units of variance. In our formulation, we may thus interpret α as a modulator of the probability assigned to an example of a particular class under the distribution of another. We remark that during model design, an alternative objective we considered is the cluster-based analogue of NCA (see Section 3.4): this objective seems to be a natural approach with a clear probabilistic interpretation. However, we found empirically that this objective does not generalize as well, since it only vanishes in the limit of extreme discrimination margins.  5  (2)  (3)  (4)  ˆL (Θ) =  1  M D  (cid:80)M  (cid:80)D d=1 (cid:107)rm  Published as a conference paper at ICLR 2016  3.2 TRAINING PROCEDURE  Component #1: neighbourhood sampling At each iteration, we sample entire local neighbour- hoods rather than a collection of independent examples. Namely, we construct our minibatch in the following way:  1. Sample a seed cluster I1 ∼ pI(·) 2. Retrieve M − 1 nearest impostor clusters I2, . . . , IM of I1 3. For each cluster Im, m = 1, . . . , M, sample D examples xm  1 , . . . xm  D ∼ pIm(·)  The choices of pI(·) and pIm(·), m = 1, . . . , M allow us to adapt to the current distributions of examples in representation space. Namely, in our training, these allow us to speciﬁcally target and reprimand contested neighbourhoods with large cluster overlap. During training, we cache the losses of individual examples, from which we compute the mean loss LI of each cluster I. We then choose pI(I) ∝ LI, and PIm (·) as a uniform distribution. We remark that these choices work well in practice, but have been made arbitrarily and perhaps can be improved. Given our samples, we may proceed to construct a stochastic approximation of our objective:  − log  (cid:80)  M(cid:88)  D(cid:88)  m=1  d=1    +  (5)  − 1 2ˆσ2 (cid:107)rm  e  ˆµ : C( ˆµ)(cid:54)=C(rm  d − ˆµm(cid:107)2 2−α 2ˆσ2(cid:107)rm − 1 (cid:80)D  d ) e  d − ˆµ(cid:107)2  2  where we approximate the cluster means as ˆµm = ˆσ = 1 and the full CNN which gave rise to the representations.  and variance as 2. During training, we backpropagate through this objective,  d − ˆµm(cid:107)2  d=1 rm d  M D−1  m=1  1 D  Component #2: cluster index As mentioned above, we maintain for each class a K-means index which captures its distribution in representation space during training. We initialize each index with K-means++ (Arthur & Vassilvitskii, 2007), and refresh it periodically. To attain the representations to be indexed, we pause training and compute the forward passes of all inputs in the training set. The computational cost of refreshing the cluster index is signiﬁcantly smaller than the cost of training the CNN itself: it is not done frequently, it only requires forward passes of the inputs, and the relative cost of K-means clustering is negligible. It may seem that freezing the training is unnecessarily computationally expensive. Note that we also explored the alternative strategy of caching the representations of each minibatch on-the-ﬂy during training. However, we found that it is critical to maintain the true neighbourhood structure where the representations are all computed in the same stage of learning. We empirically observed that since the representation space is changing continuously during training, indexing examples whose representations were computed in different times resulted in incorrect inference of neighbourhood structure, which in turn irreparably damaged nearest impostor assessment.  Improvement of training efﬁciency The proposed approach offers a number of beneﬁts which compound to considerably enhance training efﬁciency, as can be seen empirically in Section 4.1. First, one of the main criticisms of triplet-based approaches is the cubic growth of the number of triplets. Manipulating entire clusters of examples, on the other hand, signiﬁcantly improves this complexity, as this requires far fewer pairwise distance evaluations. Second, operating on entire cluster neighbourhoods also permits information recycling: we may jointly separate all clusters from one another at once, whereas an approach based on independent sampling would require far more repetitions of the same examples. Finally, penalizing clusters of points away from one another leads to a more coherent adjustment of each point, whereas different triplet terms may not necessarily be consistent with one another.  3.3 EVALUATION PROCEDURE  The evaluation procedure is consistent with the objective formulation: we assign the label of each example xn as function of its representation’s softmax similarities to its L closest clusters, say  6  Published as a conference paper at ICLR 2016  µ1, . . . , µL. More precisely, we choose label c∗  n as  (cid:80)  c∗ n = arg max  c=1,...,C  (cid:80)L  − 1 2σ2 (cid:107)rn−µl(cid:107)2  2  µl : C(µl)=c e − 1 2σ2 (cid:107)rn−µl(cid:107)2  2  l=1 e  ,  (6)  where σ is a running average of stochastic estimates ˆσ computed during training. This can be thought of as “k-nearest-cluster” (kNC), a variant of a soft kNN classiﬁer. This has the added beneﬁt of reducing the complexity of nearest neighbour evaluation from being a function of the number of examples to the number of clusters. Here, the lengthscale σ autonomously charac- terizes local neighbourhood radius, and as such implies how to sensibly choose L. In general, we found that performance improves monotonically with L, as the soft classiﬁcation is able to make use of additional neighbourhood information. At some point, however, retrieving additional nearest neighbours is clearly of no further utility, since these are much farther away than the lengthscale deﬁned by σ. In practice we use L = 128 for all experiments in this work.  3.4 RELATION TO EXISTING MODELS  Triplet Loss Our objective proposed in Equation 4 has the nice property that it reduces to the familiar triplet loss under a particular set of assumptions. Speciﬁcally, let us assume that we approx- imate each neighbourhood with a single impostor cluster, i.e, M = 2. Let us further assume that we approximate the seed cluster with merely D = 2 samples, and the impostor cluster with one. We further simplify by ignoring the variance normalization. Our objective then exactly reduces to triplet loss for a pair of triplets “symmetrized” for the two positive examples:  (cid:110)(cid:13)(cid:13)r1  2(cid:88)  d=1  ˆL (Θ) =  d − r1  2−d  (cid:13)(cid:13)2 2 −(cid:13)(cid:13)r1  d − r2  1  (cid:13)(cid:13)2  (cid:111)  2 + α  +  .  (7)  Neighbourhood Components Analysis Neighbourhood Components Analysis (NCA) and its ex- tensions (Goldberger et al., 2004; Salakhutdinov & Hinton, 2007; Min et al., 2010) have been de- signed in a similar spirit to Magnet Loss. The NCA objective is given by  (cid:80)  N(cid:88)  n=1  LNCA (Θ) =  1 N  − log  (cid:80)N n(cid:48) : C(rn(cid:48) )=C(rn) e n(cid:48)=1 e−(cid:107)rn−r(cid:48)  −(cid:107)rn−r(cid:48) n(cid:107)2  2  n(cid:107)2  2  .  (8)  However, this formulation does not address a number of concerns both in modeling and implementa- tion. As an example, it does not touch on minibatch sampling in large datasets. Even if we maintain a nearest neighbour index, if we na¨ıvely retrieve the nearest neighbours for each example, they are all going to be of different classes with high probability.  Nearest Class Mean Nearest Class Mean (Mensink et al., 2013) is cleverly designed for scalable c(x)=c x, c = 1, . . . , C of the examples in DML. In this approach, the mean vectors µc = 1|C| their raw input form are computed and ﬁxed for each class. A linear transformation W is then learned to maximize the softmax distance of each example to the cluster center of its class:  LNCM (W) =  1 N  − log  (cid:80)C  e  −(cid:107)Wxn−Wµc(xn )(cid:107)2 c=1 e−(cid:107)Wxn−Wµc(cid:107)2  2  2  N(cid:88)  n=1  .  (9)  The authors further generalize this to Nearest Class Multiple Centroids (NCMC), where for each class, K centroids are computed with K-means. Magnet shares many ideas with NCMC, but these approaches differ in a number of important ways. For NCMC, the centroids are computed on the raw inputs and are ﬁxed prior to training, rather than updated continuously on a learnt representation. It is also not clear how to extend this to more expressive transformations (such as CNNs) to represen- tation space, but this step is required in order to enjoy the success of deep learning approaches in a DML setting.  7  (cid:80)  Published as a conference paper at ICLR 2016  Approach Angelova & Long Gavves et al. Xie et al. Gavves et al. Qian et al. Softmax Triplet Magnet  Error 51.7% 49.9% 43.0% 43.0% 30.9% 26.6% 35.8% 24.9%  Approach Angelova & Zhu Angelova & Long Murray & Perronnin Sharif Razavian et al. Qian et al. Softmax Triplet Magnet  Error 23.3% 19.6% 15.4% 13.2% 11.6% 11.2% 17.0% 8.6%  Approach Angelova & Zhu Parkhi et al. Angelova & Long Murray & Perronnin Qian et al. Softmax Triplet Magnet  Error 49.2% 46.0% 44.6% 43.2% 19.6% 11.3% 13.5% 10.6%  (a) Stanford Dogs.  (b) Oxford 102 Flowers.  (c) Oxford-IIIT Pet.  Approach Error 14.1% Softmax 26.8% Triplet Magnet 15.9%  Approach Error@1 Error@5 15.0% Softmax 23.4% Triplet Magnet 7.8%  30.9% 44.6% 28.6%  (d) ImageNet Attributes.  (e) Different metrics on Stanford Dogs.  (f) Hierarchy recovery on ImageNet Attributes.  Figure 4: (a)-(d) Comparison of test set errors of various state-of-the-art approaches on different ﬁne-grained visual categorization datasets. The bottom three results for each table were all attained by applying different objectives on exactly the same architecture. (e) Evaluation of test errors on the Stanford Dogs dataset under different metrics. (f) We explore whether each algorithm is able to recover a latent class hierarchy, provided only coarse superclasses. We collapse random pairs of classes of ImageNet Attributes onto the same label. We then train on the corrupted labels, and report test errors on the original classes.  4 EXPERIMENTS We run all experiments on a cluster of Tesla K40M GPU’s. All parametrized maps f (·; Θ) to rep- resentation space are chosen as GoogLeNet with batch normalization (Ioffe & Szegedy, 2015). We add an additional fully-connected layer to map to a representation space of dimension 1024. We ﬁnd that it is useful to warm-start any DML optimization with weights of a partly-trained a standard softmax classiﬁer. It is important to not use weights of a net trained to completion, as this would result in information dissipation and as such defeat the purpose of pursuing DML in the ﬁrst place. Hence, we initialize all models with the weights of a net trained on ImageNet (Russakovsky et al., 2015) for 3 epochs only. We augment all experiments with random input rescaling of up to 30%, followed by jittering back to the original input size of 224 × 224. At test-time we evaluate an input by averaging the outputs of 16 random samples drawn from this augmentation distribution.  4.1 FINE-GRAINED CLASSIFICATION  We validate the classiﬁcation efﬁcacy of the learnt representations on a number of popular ﬁne- grained visual categorization tasks, including Stanford Dogs (Khosla et al., 2011), Oxford-IIIT Pet (Parkhi et al., 2012) and Oxford 102 Flowers (Nilsback & Zisserman, 2008) datasets. We also include results on ImageNet attributes, a dataset described in Section 4.2. We seek to compare optimal performances of the different model spaces, and so perform hyperpa- rameter search on validation error generated by 3 classes of objectives: a standard softmax classiﬁer, triplet loss, and Magnet Loss. The hyperparameter search setup, including optimal conﬁgurations for each experiment, is speciﬁed in full detail in Appendix B. In general, for Magnet Loss we ob- served empirically that it is beneﬁcial to increase the number of clusters per minibatch to around M = 12 in the cost of reducing the number of retrieved examples per cluster to D = 4. The optimal gap has in general been α ≈ 1, and the value of K varied as function of dataset cardinality.  8  MagnetTripletSoftmax2025303540Testerror28.335.836.724.938.132.9kNNkNCPublished as a conference paper at ICLR 2016  Figure 5: Training curves for various experiments as function of number of iterations. For both triplet and Magnet Loss objectives, the experiment with optimal hyperparameter conﬁguration for each model space is presented. The red diamonds indicate the point in time in which the triplet asymptotic error rate is achieved. It can be observed that Magnet Loss reaches the same error in 5-30 times fewer iterations.  The classiﬁcation results can be found in Table 4. We use soft kNN to evaluate triplet loss error and kNC (see Section 3.3) for Magnet Loss. However, for completeness of comparison, in Figure 4(e) we present evaluations of all learnt representations under both kNN and kNC. It can be observed that Magnet Loss outperforms the traditional triplet loss by a considerable margin. It is also able to surpass the standard softmax classiﬁer in most cases: while the margin is not signiﬁcant, note that the true win here is in terms of learning representations much more suitable for task transfer, as validated in the following subsections. In Figure 5, it can be seen that Magnet Loss reaches the triplet loss asymptotic error rate 5-30 times faster. The prohibitively slow convergence of triplet loss has been well-known in the community. Magnet Loss achieves this speedup as it mitigates some of the training-time inefﬁciencies featured by triplet loss presented throughout Section 2 and the end of Section 3.2. For fairness of comparison, we remark that softmax converges faster than Magnet; however, this comes at the cost of a less informative representation.  4.2 ATTRIBUTE DISTRIBUTION  We expect Magnet to sculpt a more expressive representation, which enables similar examples of different classes to be close together, and dissimilar examples of the same class to be far apart; this can be seen qualitatively in Figure 2. In order to explore this hypothesis quantitatively, after training is complete we examine the attributes of neighbouring examples as a proxy for assessment of similarity. We indeed ﬁnd the distributions of these attributes to be more concentrated for Magnet. We attain attribute labels from the Object Attributes dataset (Russakovsky & Fei-Fei, 2010). This provides 25 attribute annotations for 90 classes of an updated version of ImageNet, with about 25 annotated examples per class. Attributes include visual properties such as “striped”, “brown”, “vegetation” and so on; examples of these can be found in Figure 6(a). Annotations are assigned individually for each input, which allows capturing intra-class variation and inter-class invariance. We train softmax, triplet and Magnet Loss objectives on a curated dataset we refer to as ImageNet Attributes. This dataset contains 116,236 examples, and comprises all examples of each of the 90 ImageNet classes for which any attribute annotations are available: in Appendix C we describe it in detail. We emphasize we do not employ any attribute information during training. At convergence, we measure attribute concentration by computing mean attribute precision as function of neighbour- hood size. Speciﬁcally, for each example and attribute, we compute over different neighbourhood cardinalities the fraction of neighbours also featuring this attribute. This result can be found in Figure 6(d). Magnet Loss outperforms both softmax and triplet losses by a reasonable margin in terms of attribute concentration, with consistent gains of 25-50% over triplet and 10-25% over softmax across neighbourhood sizes. It may seem surprising that softmax surpasses triplet — an approach speciﬁcally crafted for distance metric learning. However, note that while the softmax classiﬁer requires high relative projection onto the hyperplane associated with each class, it leaves some ﬂexibility for information retainment in its high-dimensional nullspace.  9  0k50k100k150kPet2−62−52−42−32−2Trainingerror0k20k40k60kFlowers2−22−10k100k200kDogs2−32−22−10k200k400kImageNetAttributes2−52−42−32−22−1TripletMagnetPublished as a conference paper at ICLR 2016  y r r u F  d e t t o p S  (a) Attribute examples.  (b) Magnet.  (c) Softmax.  (d) Attribute precision.  Figure 6: Attribute concentration properties. (a) Examples of images featuring particular attributes. (b) & (c) The translucent underlying densities correspond to the t-SNE visualizations presented in Figure 2. These are overlaid with distributions of examples featuring the speciﬁed attributes, coloured in orange. Magnet clusters together examples of different classes but with similar at- (d) Mean fraction of neighbours tributes, whereas softmax and triplet loss (not shown) do not. featuring the same attributes as function of neighbourhood cardinality. Magnet consistently outper- forms softmax and triplet across neighbourhood sizes.  Triplet loss, on the other hand, demands separation based on an imprecise assessment of similarity, resulting in poor proximity of similar examples of different classes. Magnet’s attribute concentration can also be observed visually in Figures 6(b) and 6(c), presenting the t-SNE projections from Figure 2 overlaid with attribute distribution. It can be seen qualitatively that the Magnet attributes are concentrated in particular areas of space, irrespective of class.  4.3 HIERARCHY RECOVERY  In this experiment, we are interested to see whether each algorithm is able to recover a latent class hierarchy, provided only coarse superclasses. To test this, we randomly pair all classes of ImageNet Attributes, and collapse each pair under a single label. We then train on the corrupted labels, and check whether the ﬁner-grained class labels may be recovered from the learnt representations. The results can be found in Table 4(f). Magnet is able to identify intra-class representation variation, an essential property for success in this task. Softmax also achieves surprisingly competitive results, suggesting that meaningful variation is nevertheless captured within the nullspace of its last layer. For triplet loss, on the other hand, target neighbourhoods are designated prior to training, and as such it is not able to adaptively discriminate ﬁner structure within superclasses.  5 DISCUSSION AND FUTURE WORK  In this work, we highlighted a number of difﬁculties in a class of DML algorithms, and sought to address them. We validated the effectiveness of our approach under a variety of metrics, ranging from classiﬁcation performance to convergence rate to attribute concentration. In this paper, we anchored in place a number of parameters: we chose the number of clusters K per class as uniform across classes, and refreshed our representation index at a ﬁxed rate. We believe that adaptively varying these during training can enhance performance and facilitate computation. Another interesting line of work would be to replace the density estimation and indexing component with an approach more sophisticated than K-means. One natural candidate would be a tree-based algorithm. This would enable more efﬁcient and more accurate neighbourhood retrieval.  ACKNOWLEDGEMENTS  We are grateful to Laurens van der Maaten, Florent Perronnin, Rob Fergus, Yaniv Taigman and others at Facebook AI Research for meaningful discussions and input. We thank Soumith Chintala,  10  2022242628Neighbourhoodsize0.20.40.60.81.0MeanattributeprecisionSoftmaxTripletMagnetPublished as a conference paper at ICLR 2016  Alexey Spiridonov, Kevin Lee and everyone else at the FAIR Engineering team for unbreaking things at light speed.  APPENDIX A T-SNE IMAGE MAPS FOR TYPICAL MAGNET AND TRIPLET  REPRESENTATION SPACES  Figure 7: Visualization of t-SNE map for a typical Magnet representation. We highlight interesting observations of the distributions of the learnt representations splitting to repsect intra-class variance and inter-class similarity.  11  Published as a conference paper at ICLR 2016  Figure 8: Visualization of t-SNE map for a typical triplet representation with enforcement of se- mantic similarity. Classes with similar examples are far from one another, and no obvious local similarity can be found within individual classes.  12  Published as a conference paper at ICLR 2016  APPENDIX B HYPERPARAMETER TUNING SPECIFICATIONS AND OPTIMAL  CONFIGURATIONS  Here we describe in detail the hyperparameter search setups for the different experiments, and the optimal conﬁguration for each. For all models, we tune optimization hyperparameters consisting of learning rate and its annealing factor which we apply every epoch. We ﬁx the momentum as 0.9 for all experiments. For the smaller datasets, we refresh our index every epoch, and for ImageNet Attributes every 1000 iterations. For Magnet Loss, we additionally tune the separation margin α, the number of nearest clusters per minibatch M, the number of examples per cluster D, and the number of clusters per class K which we take to be the same for all classes (the examples per minibatch M D is upper-bounded by 48 due to memory constraints). Note that we permit the choices M = D = 2, which, as discussed in 3.4, reverts this back to triplet loss: hence, we expect this choice to be discovered if triplet loss is in fact the optimal choice of distance metric learning loss of this class. For triplet loss, we tune the separation margin α, the fraction of nearest impostors retrieved in each minibatch and neighbourhood size retrieved for kNN evaluation. We now specify the optimal hyperparameter conﬁgurations for the different datasets and model spaces, as found empirically via random search. The learning rate annealing factor is marked as “N/A” for smaller datasets, where we do not anneal the learning rate at all.  Model Magnet  Triplet  Hyperparameter Learning rate Annealing factor Gap Global scaling Clusters/class  Learning rate Annealing factor Gap Nearest impostor fraction Neighbourhood size  Pet 0.00184 N/A 7.18 3.52 8  0.000598 N/A 0.304 0.184 128  Flowers 0.0240 N/A 2.43 14.2 1  0.00155 N/A 0.554 0.129 128  Dogs 0.00292 N/A 0.710 3.03 1  0.00293 N/A 0.370 0.00713 128  ImageNet Attributes 0.00459 0.974 0.700 6.42 2  Hierarchy recovery 0.00177 0.988 0.783 2.33 16  0.00807 0.966 0.495 0.0700 128  0.00187 0.995 0.556 0.0424 128  Table 1: Optimal hyperparameter conﬁgurations for the different datasets and model spaces.  APPENDIX C SPECIFICATIONS FOR IMAGENET ATTRIBUTES DATASET  To curate this dataset, we ﬁrst matched the annotated examples in the Object Attributes dataset (Rus- sakovsky & Fei-Fei, 2010) to examples in the training set of ImageNet. The ImageNet Attributes training and validation sets then comprise all examples of all classes for which annotated examples exist. Below we list these classes. n01693334, n01773549, n01773797, n01796340, n01872401, n01873310, n01882714, n01883070, n02071294, n02074367, n02088238, n02088364, n02088466, n02088632, n02090379, n02091134, n02091635, n02092002, n02096294, n02100583, n02100735, n02101556, n02102480, n02104029, n02104365, n02105056, n02105162, n02105251, n02105505, n02106030, n02109047, n02109525, n02110806, n02110958, n02112350, n02115913, n02119789, n02123045, n02123394, n02124075, n02125311, n02128925, n02129165, n02130308, n02326432, n02342885, n02361337, n02391049, n02410509, n02422106, n02422699, n02423022, n02441942, n02442845, n02443114, n02443484, n02444819, n02445715, n02447366, n02480495, n02480855, n02481823, n02483708, n02484975, n02486261, n02486410, n02487347, n02488291, n02488702, n02500267, n02509815, n02536864, n02802426, n02808440, n02910353, n03249569, n03325584, n03721384, n03977966, n03982430, n04118776, n04228054, n04447861, n07615774, n07745940, n07873807, n07875152, n07880968, n11939491, n12267677  REFERENCES Angelova, Anelia and Long, Philip M. Benchmarking large-scale ﬁne-grained categorization. In IEEE Winter Conference on Applications of Computer Vision, Steamboat Springs, CO, USA, March 24-26, 2014, pp. 532–539, 2014. doi: 10.1109/WACV.2014.6836056.  13  Published as a conference paper at ICLR 2016  Angelova, Anelia and Zhu, Shenghuo. Efﬁcient object detection and segmentation for ﬁne-grained recognition. In 2013 IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA, June 23-28, 2013, pp. 811–818, 2013. doi: 10.1109/CVPR.2013.110.  Arthur, David and Vassilvitskii, Sergei. K-means++: The advantages of careful seeding. In Pro- ceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’07, pp. 1027–1035, Philadelphia, PA, USA, 2007. Society for Industrial and Applied Mathematics. ISBN 978-0-898716-24-5.  Chopra, Sumit, Hadsell, Raia, and LeCun, Yann. Learning a similarity metric discriminatively, with application to face veriﬁcation. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) - Volume 1 - Volume 01, CVPR ’05, pp. 539–546, Washington, DC, USA, 2005. IEEE Computer Society. ISBN 0-7695-2372-2. doi: 10.1109/CVPR.2005.202.  Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell,  Trevor. Decaf: A deep convolutional activation feature for generic visual recognition. 2013.  Gavves, E., Fernando, B., Snoek, C. G. M., Smeulders, A. W. M., and Tuytelaars, T. Fine-grained  categorization by alignments. In IEEE International Conference on Computer Vision, 2013.  Gavves, Efstratios, Fernando, Basura, Snoek, CeesG.M., Smeulders, ArnoldW.M., and Tuytelaars, Tinne. Local alignments for ﬁne-grained categorization. International Journal of Computer Vi- sion, 111(2):191–212, 2015. ISSN 0920-5691. doi: 10.1007/s11263-014-0741-5.  Globerson, Amir and Roweis, Sam T. Metric learning by collapsing classes.  In Weiss, Y., Sch¨olkopf, B., and Platt, J.C. (eds.), Advances in Neural Information Processing Sys- tems 18, pp. 451–458. MIT Press, 2006. URL http://papers.nips.cc/paper/ 2947-metric-learning-by-collapsing-classes.pdf.  Goldberger, Jacob, Roweis, Sam, Hinton, Geoff, and Salakhutdinov, Ruslan. Neighbourhood com- ponents analysis. In Advances in Neural Information Processing Systems 17, pp. 513–520. MIT Press, 2004.  Hadsell, Raia, Chopra, Sumit, and Lecun, Yann. Dimensionality reduction by learning an invariant In In Proc. Computer Vision and Pattern Recognition Conference (CVPR06. IEEE  mapping. Press, 2006.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 448–456, 2015.  Khosla, Aditya, Jayadevaprakash, Nityananda, Yao, Bangpeng, and Fei-Fei, Li. Novel dataset for In First Workshop on Fine-Grained Visual Categorization, ﬁne-grained image categorization. IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, June 2011.  Mensink, Thomas, Verbeek, Jakob, Perronnin, Florent, and Csurka, Gabriela. Distance-based image classiﬁcation: Generalizing to new classes at near zero cost. Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2013.  Min, Martin Renqiang, van der Maaten, Laurens, Yuan, Zineng, Bonner, Anthony J., and Zhang, In Proceedings of the 27th International Zhaolei. Deep supervised t-distributed embedding. Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 791–798, 2010.  Murray, Naila and Perronnin, Florent. Generalized max pooling.  In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pp. 2473–2480, 2014. doi: 10.1109/CVPR.2014.317.  Nilsback, M-E. and Zisserman, A. Automated ﬂower classiﬁcation over a large number of classes. In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.  14  Published as a conference paper at ICLR 2016  Norouzi, Mohammad, Fleet, David, and Salakhutdinov, Ruslan R. Hamming distance metric learn- ing. In Pereira, F., Burges, C.J.C., Bottou, L., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 25, pp. 1061–1069. Curran Associates, Inc., 2012.  Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. V. Cats and dogs. In IEEE Conference  on Computer Vision and Pattern Recognition, 2012.  Qian, Qi, Jin, Rong, Zhu, Shenghuo, and Lin, Yuanqing. Fine-grained visual categorization via multi-stage metric learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.  Russakovsky, Olga and Fei-Fei, Li. Attribute learning in large-scale datasets. In European Confer-  ence of Computer Vision (ECCV), International Workshop on Parts and Attributes, 2010.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, International Journal of Computer Li. Vision (IJCV), pp. 1–42, April 2015. doi: 10.1007/s11263-015-0816-y.  ImageNet Large Scale Visual Recognition Challenge.  Salakhutdinov, Ruslan and Hinton, Geoffrey E. Learning a nonlinear embedding by preserving class neighbourhood structure. In Meila, Marina and Shen, Xiaotong (eds.), Proceedings of the Eleventh International Conference on Artiﬁcial Intelligence and Statistics (AISTATS-07), vol- ume 2, pp. 412–419. Journal of Machine Learning Research - Proceedings Track, 2007.  Schroff, Florian, Kalenichenko, Dmitry, and Philbin, James. Facenet: A uniﬁed embedding for face recognition and clustering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.  Sharif Razavian, Ali, Azizpour, Hossein, Sullivan, Josephine, and Carlsson, Stefan. Cnn features In The IEEE Conference on Computer  off-the-shelf: An astounding baseline for recognition. Vision and Pattern Recognition (CVPR) Workshops, June 2014.  Snoek, Jasper, Rippel, Oren, Swersky, Kevin, Kiros, Ryan, Satish, Nadathur, Sundaram, Narayanan, Patwary, Md. Mostofa Ali, Prabhat, and Adams, Ryan P. Scalable bayesian optimization using deep neural networks. In International Conference on Machine Learning, 2015.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. In CVPR 2015, 2015.  van der Maaten, L.J.P. and Hinton, G.E. Visualizing high-dimensional data using t-sne. 2008.  Verma, Nakul, Mahajan, Dhruv, Sellamanickam, Sundararajan, and Nair, Vinod. Learning hier- In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE  archical similarity metrics. Conference on, pp. 2280–2287. IEEE, 2012.  Wang, Jiang, Song, Yang, Leung, Thomas, Rosenberg, Chuck, Wang, Jingbin, Philbin, James, Chen, Bo, and Wu, Ying. Learning ﬁne-grained image similarity with deep ranking. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 1386–1393. IEEE, 2014.  Weinberger, Kilian Q. and Saul, Lawrence K. Distance metric learning for large margin nearest  neighbor classiﬁcation. J. Mach. Learn. Res., 10:207–244, June 2009. ISSN 1532-4435.  Xie, Saining, Yang, Tianbao, Wang, Xiaoyu, and Lin, Yuanqing. Hyper-class augmented and reg- ularized deep learning for ﬁne-grained image classiﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pp. 2645–2654, 2015. doi: 10.1109/CVPR.2015.7298880.  15  ",
1511.05897,2016,Censoring Representations with an Adversary,"['Censoring Representations with an Adversary\nHarrison Edwards', 'Amos Storkey']",https://arxiv.org/pdf/1511.05897,"6 1 0 2    r a  M 4         ]  G L . s c [      3 v 7 9 8 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  CENSORING REPRESENTATIONS WITH AN ADVERSARY  Harrison Edwards & Amos Storkey Department of Informatics University of Edinburgh Edinburgh, UK, EH8 9AB H.L.Edwards@sms.ed.ac.uk, A.Storkey@ed.ac.uk  ABSTRACT  In practice, there are often explicit constraints on what representations or deci- sions are acceptable in an application of machine learning. For example it may be a legal requirement that a decision must not favour a particular group. Alterna- tively it can be that that representation of data must not have identifying informa- tion. We address these two related issues by learning ﬂexible representations that minimize the capability of an adversarial critic. This adversary is trying to pre- dict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the repre- sentation about the sensitive variable. We demonstrate this adversarial approach on two problems: making decisions free from discrimination and removing pri- vate information from images. We formulate the adversarial model as a minimax problem, and optimize that minimax objective using a stochastic gradient alter- nate min-max optimizer. We demonstrate the ability to provide discriminant free representations for standard test problems, and compare with previous state of the art methods for fairness, showing statistically signiﬁcant improvement across most cases. The ﬂexibility of this method is shown via a novel problem: remov- ing annotations from images, from separate training examples of annotated and unannotated images, and with no a priori knowledge of the form of annotation provided to the model.  1  INTRODUCTION  When we apply machine learning techniques many real-world settings, it is not long before we run into the problem of sensitive information. It may be that we want to provide information to a third party, but be sure that third party cannot determine critical sensitive variables. Alternatively it may be that we need to make decisions that do not treat one category differently from another. Two speciﬁc cases of this are image anonymization and fairness respectively.  1.1 FAIRNESS  As more of life’s decisions become automated there is a growing need, for legal and ethical reasons, to have machine learning algorithms that can make fair decisions. A decision is fair if it does not depend upon a sensitive variable such as gender, age or race. One naive approach to achieving a fair decision would be to simply remove the sensitive covariate from the model. But information about the sensitive variable can ‘leak’ back into the decisions made if there is any dependence between it and the other variables. In this work we focus on fair classiﬁers where we want to predict a binary variable Y and be fair with respect to a binary sensitive variable S. Here, fairness means that the decision is not-dependent on (i.e. marginally independent of) the sensitive variable. Previous works, such as those listed in Section 2.2, have tended to develop speciﬁc fair variants of common classiﬁers to solve this problem. Our approach, called adversarial learned fair represen- tations (ALFR), is to learn representations of the data which are concurrently both fair and discrim- inative for the prediction task. These features can then be used with any classiﬁer. To achieve both fair and discriminative properties, we represent this as a dual objective optimization problem that can be cast as a minimax problem. We maintain the ﬂexibility of both the representation and the  1  Published as a conference paper at ICLR 2016  test of fairness, by using deep feed-forward neural networks for each part. There is a deep neural network that is used to produce the representation; that representation is then critiqued by a deep neural adversary who tries to predict the sensitive variable from the representation. In this paper, we introduced the adversarial method ALFR as a minimax problem, describe the op- timization process, and evaluate ALFR on two datasets, Diabetes and Adult. This demonstrates improvement over a related approach Zemel et al. (2013). We also, as an aside, provide the relation- ship between the discrimination of a classiﬁer and the H-divergence, as used in domain adaptation. The relationship of these methods to domain adaptation is interesting: the different cases for the sensitive variable can be thought of as different domains. However, we leave the study of this for another paper.  1.2  IMAGE ANONYMIZATION  There are many notions and problems relating to privacy in the literature. Strict forms of privacy such as that enforced by differential privacy are not always necessary, and more relaxed notions of privacy are beneﬁcial in reducing distortion and making more accurate analysis possible . One particular case of privacy is where certain parts of the data should not be communicated (e.g. someones address or name). However in many settings it is hard to be explicit about exactly what should not be communicated or whether that information is coupled with other measured variables. In this work we consider the concrete case of removing private information from an image, whilst distorting the image as little as possible. Examples of private information include: licence plates on cars in photos and doctors’ annotations on medical images such as X-rays. We suggest a mod- iﬁcation of an autoencoder to remove such private information, and validate this idea by removing surnames from a collection of images of faces. A similar application might be removing logos or watermarks from images. The novelty of our approach is that the model does not need to be trained with aligned input/output examples, rather only examples of inputs and (separately) examples of outputs, labelled as such. For example if the task is to remove text from an image, an aligned input/output pair would be an image containing text, and the same image with the text removed. Unaligned data would simply be images labelled as containing no text, and images labelled as containing text. The former sort of data would often be substantially more difﬁcult to obtain than the latter. Once again we use the same two-neural-network minimax formalism to characterise the problem, and the same stochastic gradient optimization procedure to learn the neural network parameters. The model is applied to the problem of images with and without annotation, to good visual affect. A neural network can no longer distinguish well between annotated and non-annotated images, and the actually annotation itself is obscured.  2 RELATED WORK  2.1 ADVERSARIAL LEARNING  The idea of adversarial learning is that one has a representation R, a dependent variable S and an adversary that tries to predict S from R. The adversary then provides an adaptive measure of depen- dence between R and S which can then be used to learn R so that it minimizes this dependence. The adversarial approach was, to the best of our knowledge, introduced in Schmidhuber (1991) where it was used to learn a representation of data R = (R1, . . . , Rd) where each Ri is both binary and independent of the other Rj. The experiments in this work were on synthetic data, and were later followed up to learn ﬁlters from natural image patches in Schmidhuber et al. (1996) and Schraudolph et al. (1999). They referred to this approach as the principle of predictability minimization. More recently in Goodfellow et al. (2014) the idea of using an adversary to learn a generative model of data was introduced and followed-up by work such as Gauthier (2015), Rudy & Taylor (2014) and Denton et al. (2015). In this setting the representation R is a mixture of data samples and generated samples, and S is a binary variable indicating whether a given sample is from the data or ‘fake’. For discussion on using ‘distinguishability criteria’ to learn generative models see Goodfellow (2014).  2  Published as a conference paper at ICLR 2016  The inspiration for ALFR comes from using adversarial learning to do domain-adaptation in Ganin et al. (2015). In this setting S is a variable indicating the domain, and the idea is to learn a repre- sentation R in which the domains are indistinguishable, motivated by bounds in Ben-David et al. (2010) relating performance on the target domain (the new domain to which we want to adapt) to the dissimilarity of the target and source domains.  2.2 FAIR CLASSIFIERS  Several works have proposed variants of classiﬁers that enforce fairness. These include discrimination-free naive-Bayes (Calders & Verwer (2010)), a regularized version of logistic regres- sion (Kamishima et al. (2011)), and a more recent approach Zafar et al. (2015), where the authors introduce constraints into the objective functions of logistic regression, hinge-loss classiﬁers and support vector machines. Another approach is data massaging, whereby the labels of the training data are changed so that the training data is fair (Kamiran & Calders (2009)), this is similar to the resampling methods often used to tackle class-imbalance problems. An approach more in the spirit of ALFR is learned fair representations (LFR) (Zemel et al. (2013)). In that paper, the authors learn a representation of the data that is a probability distribution over clusters — a form of ‘fair clustering’ — where learning the cluster of a datapoint tells one nothing about the sensitive variable S. The clustering is learned to be fair and also discriminative for the prediction task. Other than LFR, the previous work has focused on enforcing fair decisions, whereas LFR and ALFR aim to get fairness as a side-effect of fair representations. The advantage of the latter approach is that the representations can potentially be reused for different tasks, and there is the possibility of a separation of concerns whereby one party is responsible for making the representations fair, and another party is responsible for making the best predictive model. In contrast to LFR, our approach is more ﬂexible in terms of the kinds of representations it can learn, whereas LFR learns essentially categorical representations. In addition our approach means that the representations can be used with any classiﬁer. Concurrent with this work is the preprint ‘The Variational Fair Autoencoder’ (Louizos et al. (2015)). There are two main differences between this work and theirs. First they use a variational autoen- coder that factorizes the latent variables Z and sensitive variable S. This aspect is complementary and could be incorporated into the adversarial framework. Secondly they use a Maximum Mean Discrepancy (MMD) penalty to reduce dependence of the representation on S, this is a kernel-based alternative to using an adversary. It is not yet clear in what circumstances we should prefer MMD over an adversary, we hope that future work will address this question.  2.3 REMOVING PRIVATE INFORMATION  The problem of removing information from images in a learned manner has not been tackled, to the best of our knowledge, in the machine learning community. However there has been work on detecting when private information has been erroneously left in an image, such as in Korayem et al. (2014) and Erickson et al.  3 FORMALISM: FAIRNESS AND DISCRIMINATION  We consider a binary classiﬁcation task where X ⊂ Rn is the input space, Y = {0, 1} is the label set and S = {0, 1} is a protected variable label set. We are provided with i.i.d examples {(xi, yi, si)}m i=1 drawn from some joint distribution P over corresponding random variables (X, Y, S). The goal of the learning algorithm is to build a classiﬁer η : X → Y that has high accuracy whilst maintaining the property of statistical parity or fairness, that is  P (η(X) = 1|S = 1) = P (η(X) = 1|S = 0).  (1)  3  Published as a conference paper at ICLR 2016  A key statistic we will use to measure statistical parity is the discrimination deﬁned  η(xi)  η(xi)  ydisc =  i:si=0  N0  −  i:si=1  N1  (cid:80)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ,  (cid:80)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  (2)  where N0, N1 are the number of data items where si equal to 0 and 1, respectively. We measure the success of our classiﬁer using the empirical accuracy yacc. Following Zemel et al. (2013) we aim to optimize the difference between discrimination and classi- ﬁcation accuracy  (3) where t ≥ 0, called the delta. In Zemel et al. (2013) they consider the speciﬁc trade off where t = 1, whereas we will evaluate our models across a range of different values for t.  yt,delta = yacc − t · ydisc,  4 CENSORED REPRESENTATIONS  In this section we show how a modiﬁcation of an autoencoder can be used to learn a representation that obscures/removes a sensitive variable. In the general case we have an input variable X, a target variable Y and a binary sensitive variable S. The objective is to learn a representation R = Enc(X) that preserves information about X, is useful for predicting Y and is approximately independent of S. The loss we will use is of the form  L = αC(X, R) + βD(S, R) + γE(Y, R)  (4)  where C is the cost of reconstructing X from R, D is a measure of dependence between R and S and E is the error in predicing Y from R. The scalars α, β, γ ≥ 0 are hyperparameters controlling the weighting of the competing objectives. If we don’t have a speciﬁc prediction task, as in the case where we just want to remove private information from an image, then we don’t need the γ term. On the other hand, if we are not interested in reusing the representation for different predictive tasks then we may set α = 0. We may also want to have α > 0 in the case where we want to learn a transformation of the data X → ˆX that is fair and preserves the semantics of the representation, for example in certain regulated areas the interpretability of the model is paramount. Notice also that only the ﬁnal term depends-upon Y and so there is an opportunity to train in a semi-supervized fashion if the labels are not available for all of the data.  4.1 QUANTIFYING DEPENDENCE  We begin with quantifying the dependence D(S, R) between the representation R and the sensitive variable S. Since S is binary, we can think of this as measuring the difference between two condi- tional distributions: R0 ∼ R|(S = 0) and R1 ∼ R|(S = 1). We will do this by training a classiﬁer to tell them apart, called the adversary. Interested readers may wish to know that this measure of dependence is related to the notion of an H-divergence, as described in Appendix A. In particular if the adversary network Adv : R → [0, 1] trained to discriminate between R0 and R1 has parameters φ, and the encoder Enc : X → R has parameters θ, then we can deﬁne  Dθ,φ(R, S) = E  X,S  S · log (Adv(R)) + (1 − S) · log (1 − Adv(R)) ,  (5)  that is the negative of the standard log-loss for a binary classiﬁer. The adversary’s parameters φ should be chosen to maximize D (and hence to approximately realize the empirical H-divergence ˆdH(A, B)), whilst the representation parameters θ should be chosen to minimize D (and hence to approximately minimize ˆdH(A, B)), so we have a minimax problem:  min  θ  max  φ  Dθ,φ(R, S).  (6)  Of course, this admits a trivial solution by learning a constant representation R, and so we must introduce constraints to learn anything interesting.  4  Published as a conference paper at ICLR 2016  (7)  (8)  4.2 QUANTIFYING C(X,R) We quantify the information retained in R about X by the ability of a decoder Dec : R → X to reconstruct X from R, in particular we use the expected squared error  (cid:107)X − Dec(R)(cid:107)2 2 , where we extend θ to include the parameters in the decoder Dec.  Cθ(X, R) = E  X  4.3 QUANTIFYING E(Y,R)  We quantify how discriminative R is for the prediction task using the log-loss of a classiﬁer or predictor network Pred : R → [0, 1], trained to predict Y from R:  Eθ(R, S) = − E  Y · log (Pred(R)) + (1 − Y ) · log (1 − Pred(R)) ,  X,Y  where we again extend θ to encompass the parameters of the predictor network Pred.  4.4 OPTIMIZATION  There are four elements in the model: the encoder, the decoder, the predictor and the adversary. Each is implemented using a feed-forward neural network, the precize architectures are detailed in Section 5. The costs for these elements, given in equations 8, 5 and 7, are joined together to give the joint loss  This enables the problem of learning censored representation to be cast as the minimax problem  L(θ, φ) = αCθ(X, R) + βDθ,φ(R, S) + γEθ(Y, R)  (9)  min  θ  max  φ  L(θ, φ).  (10)  Expecting to be able to train the adversary in the inner loop to a global optimum is unrealistic. Instead, as described in Goodfellow et al. (2014) we use a heuristic: a variant on stochastic gradient descent where for each minibatch we decide whether to take a gradient step with respect to the actors parameters θ or a negative gradient step with respect to the adversary’s parameters φ. In Goodfellow et al. (2014) they consider simple strict alternation between updating the adversary and actor, we ﬁnd this to be a useful default. We give detailed pseudo-code for strict alternation in Algorithm 1. Note that the gradient steps in Algorithm 1 can easily be replaced with a more powerful optimizer such as the Adam algorithm (Kingma & Ba (2014)). This method is a heuristic in the sense that we do not provide formal guarantees of convergence, but we do know that the solution to the minimax problem is a ﬁxed point of the process. There are a large number of papers using this method (such as those mentioned in 2.1) and getting good results, so there is considerable empirical evidence in its favour. The key issue in this process is that if the adversary is too competent then the gradients will be weak for the actor, whereas if the adversary is too incompetent then the gradients will be uninformative for the actor. We have also considered not updating the adversary if, for instance, its accuracy in predicting S is over a threshold, say 90%, and not updating the generator if the accuracy is below a threshold, say 60%. We ﬁnd that this sometimes improves results, but more investigation is needed.  4.5 ADVERSARIAL LEARNED FAIR REPRESENTATIONS (ALFR)  We apply the general setup described in Section 4 to the case of learning fair classiﬁers as described in Section 3. In this case the sensitive variable S would correspond to some category like gender or race, and the target variable Y would be the attribute we wish to predict using these fair representa- tions. In this speciﬁc case it is worth pointing out that the discrimination ydisc of the classiﬁer η is bounded above by the empirical H-divergence, as shown in Lemma 2 in Appendix A, that is  ydisc ≤ 1 2  ˆdH( ˆR0, ˆR1),  5  (11)  Published as a conference paper at ICLR 2016  Algorithm 1 Strictly alternating gradient steps.  θ, φ ← initialize network parameters U ← True repeat  2  x,ˆx∈X, ˆX  (cid:80) (cid:80) (cid:80)  X, Y, S ← random mini-batch from dataset R ← Enc(X) ˆX ← Dec(R) ˆY ← Pred(R) ˆS ← Adv(R) C ← 1|X| E ← −1|X| D ← 1|X| L ← αC + βD + γE if U then θ ← φ + α∇φL φ ← θ − α∇θL  (cid:107)x − ˆx(cid:107)2 y · log(ˆy) + (1 − y) · log(1 − ˆy) s · log(ˆs) + (1 − s) · log(1 − ˆs)  y,ˆy∈Y, ˆY  s,ˆs∈S, ˆS  else  end if U ← not U until Deadline  (cid:46) Boolean indicating whether to update parameters θ or φ.  (cid:46) Reconstruction loss for the autoencder.  (cid:46) Log-loss for the predictor.  (cid:46) Negative log-loss for the adversary.  (cid:46) Joint loss. (cid:46) Updating adversary’s parameters.  (cid:46) Updating autoencoder’s parameters.  where H is a symmetric hypothesis class on R including η and ˆR0, ˆR1 are the empirical samples given to us. This is because the discrimination between R0 and R1 given by the best hypothesis must be at least as good as some particular hypothesis η, assuming η ∈ H. So minimizing the divergence minimizes the discrimination.  4.6 ANONYMIZING IMAGES  We apply the the idea of censoring representations to the application of removing text from images. In this problem the input X is an image and the sensitive variable S describes whether or not the image contains private information (text). Here there is no prediction task and so there is no variable Y . In contrast to ALFR, we are not interested in learning a hidden representation, but the recon- structed image ˆX, so in this case we have R = ˆX. In order to evaluate the model, we need to have a small amount of validation/test data where we have pairs of images with and without the text. This is used to choose hyperparameters, but the model itself never gets to train on example input/output pairs.  5 EXPERIMENTAL RESULTS  We used the Adam algorithm with the default parameters for all our optimizations. The experiments were implemented using theano (Bergstra et al. (2010), Bastien et al. (2012)) and the python library lasagne.  5.1 FAIRNESS  5.1.1 DATASETS  We used two datasets from the UCI repository Lichman (2013) to demonstrate the efﬁcacy of ALFR. The Adult dataset consists of census data and the task is to predict whether a person makes over 50K dollars per year. The sensitive attribute we chose was Gender. The data has 45, 222 instances and 102 attributes. We used 35 thousand instances for the training set and approximately 5 thousand instances each for the validation and test sets.  6  Published as a conference paper at ICLR 2016  The Diabetes dataset consists of hospital data from the US and the task is to predict whether a patient will be readmitted to hospital. The sensitive attribute we chose was Race, we changed this to a binary variable by creating a new attribute isCaucasian. The data has around 100 thousand instances and 235 attributes. We used 80 thousand instances for the training set and approximately 10 thousand instances each for the validation and test sets.  5.1.2 PROTOCOLS AND ARCHITECTURE  To compare ALFR with LFR we split each dataset into training, validation and test sets randomly, we then run 100 experiments per model with different hyperparameters. Then for each value of t considered we selected the model maximizing yt,disc on the validation data. This process was repeated 5 times on different data splits. Since each model sees the same data split, the observations are paired and so we get 5 observations of the difference in performance for each value of t. In evaluating the results, we want to evaluate the approaches for different possible tradeoffs between accuracy and discrimination, and so we compare with a range of values of t ∈ [0, 3] and explore hyperparameters using a random search (the 100 settings over hyperparameters are drawn from a product of simple distributions over each hyperparameter). We use random search, as opposed to a sequential, performance driven search, so that we are able to compare the models across a range of values of t. When using these methods in practice one should select the hyperparameters, using Bayesian optimization or a similar approach, to maximize the speciﬁc tradeoff one cares about. We now give details of the priors over hyperparameters used for the random search. The autoencoder in ALFR had U ({1, . . . , 3}) encoding/decoding layers with U ({1, . . . , 100}) hidden units, with all hidden layers having the same number of hidden units. Each encoding/decoding unit used the ReLU (Nair & Hinton (2010)) activation. The critic also had U ({1, . . . , 3}) hidden layers with ReLU activations. The predictor network was simply a logistic regressor on top of the central hidden layer of the autoencoder. In both models the reconstruction error weighting parameter α was ﬁxed at 0.05. For both models we used had β ∼ U [0, 50] and γ ∼ U [0, 10].  In the LFR model the model had U ({5, . . . , 50}) clusters.  5.1.3 RESULTS  We found the LFR model was sensitive to hyperparameters/initialization and could often stick during training, but given sufﬁcient experiments we were able to obtain good results. In Figure 1 and Figure 2 we see the results of applying both LFR and ALFR on the Adult and Diabetes data respectively. In both cases we see that the ALFR model is able to obtain signiﬁcantly better results across most of the range of possible tradeoffs between accuracy and discrimination. The step changes in these ﬁgures correspond to places where the change in tradeoff results in a different form of model. This also clariﬁes that the results are often not very sensitive to this tradeoff parameter. We were also interested in the effect of the hyperparameters α, β, γ on the discrimination of the ALFR model. In particular we consider the ratio β+γ , measuring the relative importance of the dependence term D(X, R) over the prediction error term E(Y, R) in the cost. In Figure 3 we see that the larger β is relative to γ, the lower the discrimination is (up to a point), which matches expectations.  β  5.2  IMAGE ANONYMIZATION  5.2.1 DATASETS  We used the ‘10k US Adult Faces’ dataset Bainbridge et al. (2013) consisting of 10,168 natural face photographs. We preprocessed the data by rescaling each to 100 × 80 pixels and converting to grayscale. In addition, for half the training images we added private information, in the form of text overlayed at a random location in the image. We used 200 photographs as a validation set, and 49 for the test set, on both the validation and test sets we have the image both with and without the text for evaluation. In this problem the sensitive variable S indicates whether or not the image contains text.  7  Published as a conference paper at ICLR 2016  Figure 1: Results on Adult dataset. For each value of t the model maximizing yt,delta on the vali- dation set is selected, the plots show the performance of the selected models on the test data. Top row is yt,delta. Second row down shows the mean paired difference between the yt,delta for the ALFR model and LFR model, where positive values favour ALFR. We also give a 95% CI around the mean. Third row down is yacc. Bottom row is ydisc. We see from the top row that the ALFR model has better yt,delta for every setting of t considered. Moreover the difference is signiﬁcant for most values of t (that is, the CI does not include zero).  5.2.2 PROTOCOLS AND ARCHITECTURE The global image autoencoder model we used was an expert patch based model, using 5× 5 patches. The expert model consists of two parts, a patch classiﬁer and a neural patch autoencoder. The image reconstruction is formed by simply copying the patch if the patch classiﬁer predicts there to be no text-label in the patch. Otherwize it uses the patch autoencoder to construct the patch. The patch classiﬁer is pretrained with weak-supervision using image level labels for each patch. The decision boundary for the patch adversary is a hyperparameter, optimized to use the autoencoder if the probability of a text-label is > 0.7 in this setting. Altogether the whole image was reconstructed from the input image using the expert patch based model for each patch of the image. Along with the image autoencoder is the full image adversary, which takes the whole reconstructed image and classiﬁes whether the image has a text-label present or not. The patch classiﬁer had a single hidden layer with 200 ReLU units. The autoencoder also had a single hidden layer with 2000 ReLU units. The adversary had two convolutional layers each with 10 ﬁlters of size 3 × 3 interspersed with two max-pooling layers with pooling size 4, this was followed by a dense layer with 1000 hidden units with ReLU activations. Other hyperparameters were the α and β weights, which were chosen to be α = 1 and β = 10. All hyperparameters were chosen by evaluation on the validation data. The validation data consists of input/output pairs (xin, xout) where xin contains text, and xout is the corresponding image with- out text. The performance was measured using the mean-square error betweeen the autoencoder’s output ˆx given xin and the target xout.  5.2.3 RESULTS  Sample images produced by the trained model on the test data are shown in Figure 4. The model learns to imagine what the image would have looked like had there been no text, despite having  8  Published as a conference paper at ICLR 2016  Figure 2: Results on Diabetes dataset. For each value of t the model maximizing yt,detla on the validation set is selected, the plots show the performance of the selected models on the test data. Top row is yt,delta. Second row down shows the mean paired difference between the yt,delta for the ALFR model and LFR model, where positive values favour ALFR. We also give a 95% CI around the mean. Third row down is yacc. Bottom row is ydisc.We see from the top row that the ALFR model has better yt,delta for every setting of t considered. We also see that the difference is signiﬁcant for ≈ t ≤ 2 (that is, the CI does not include zero).  never been given example input/output pairs. The produced images are quite plausible, but artifacts become apparent when zooming in. We believe that the quality of the images could be substantially improved through the use of a convolutional autoencoder on a larger dataset, since this could take into account a wider context than a patch when removing the text. In this application the ﬂexibility of the adversarial framework as compared with a clustering ap- proach becomes apparent, it would be extremely difﬁcult to get an LFR-style approach to work for images, since it must reconstruct images as a convex combination of template images.  6 CONCLUSIONS AND FUTURE WORK  We have shown how the adversarial approach can be adapted to the task of removing sensitive information from representations. Our model ALFR improves upon a related approach whilst at the same time being more ﬂexible. We demonstrate this ﬂexibility by showing how the same setup can be used to remove text from an image, with encouraging results. As has been noted before it is difﬁcult to train adversarial models owing to the unstable dynamic between actor and adversary. There is work to be done in the future in developing theory, or at least heuristics, for improving the stability of the training process. Following up from the application on images we would like to investigate the more challenging problem of removing more pervasive information from an image, for instance removing gender from a face. Another interesting problem to investigate would be obscuring text in images where we only have negative examples of images with text. With no further assumptions our method would not be applicable, but if we assume that we have some information about the text then we can gain some traction. For example if the text is the name of the person in the image, and we know the name of the  9  Published as a conference paper at ICLR 2016  β+γ versus the test discrimination of the ALFR model on the Adult Figure 3: A scatter plot of dataset. We see an approximately linear relation up to a certain point, after which further relative increase in β has little effect on the discrimination.  β  Figure 4: Image anonymization results on the test set. In each pair of faces the left image is the input to the autoencoder, and the right image is the censored output.  person in the image, then the adversary could be trained to predict a bag-of-characters of the name, whereas the autoencoder would be trained to make this task difﬁcult for the adversary. The result should be a blurring, rather than removal of the text. One issue one would face in this approach would be a lack of any ground-truth examples for validation, since there are many ways to obscure text.  10  Published as a conference paper at ICLR 2016  ACKNOWLEDGMENTS  This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the Uni- versity of Edinburgh.  REFERENCES Bainbridge, Wilma A, Isola, Phillip, and Oliva, Aude. The intrinsic memorability of face pho-  tographs. Journal of Experimental Psychology: General, 142(4):1323, 2013.  Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: New features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Ben-David, Shai, Blitzer, John, Crammer, Koby, Pereira, Fernando, et al. Analysis of representations  for domain adaptation. Advances in neural information processing systems, 19:137, 2007.  Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira, Fernando, and Vaughan, JenniferWortman. A theory of learning from different domains. Machine Learning, 79(1-2):151– 175, 2010. ISSN 0885-6125. doi: 10.1007/s10994-009-5152-4. URL http://dx.doi.org/ 10.1007/s10994-009-5152-4.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: A CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Calders, Toon and Verwer, Sicco.  Three Naive Bayes approaches for discrimination-free ISSN doi: 10.1007/s10618-010-0190-x. URL http://dx.doi.org/10.1007/  Data Mining and Knowledge Discovery, 21(2):277–292, 2010.  classiﬁcation. 1384-5810. s10618-010-0190-x.  Denton, Emily, Chintala, Soumith, Szlam, Arthur, and Fergus, Rob. Deep generative image models  using a Laplacian Pyramid of adversarial networks. arXiv preprint arXiv:1506.05751, 2015.  Erickson, Zackory, Compiano, Jared, and Shin, Richard. Neural networks for improving wearable  device security.  Ganin, Yaroslav, Ustinova, Evgeniya, Ajakan, Hana, Germain, Pascal, Larochelle, Hugo, Laviolette, Franc¸ois, Marchand, Mario, and Lempitsky, Victor S. Domain-adversarial training of neural networks. CoRR, abs/1505.07818, 2015. URL http://arxiv.org/abs/1505.07818.  Gauthier, J. Conditional generative adversarial nets for convolutional face generation, 2015.  Goodfellow, Ian J. On distinguishability criteria for estimating generative models. arXiv preprint  arXiv:1412.6515, 2014.  Goodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron C., and Bengio, Yoshua. Generative adversarial nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Process- ing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672–2680, 2014. URL http://papers.nips.cc/paper/5423-generative-adversarial-nets.  Kamiran, Faisal and Calders, Toon. Classiﬁcation with no discrimination by preferential sampling.  In In 2nd International Conference on Computer, Control and Communication, pp. 1–6, 2009.  Kamishima, T., Akaho, S., and Sakuma, J. Fairness-aware learning through regularization approach. In Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference on, pp. 643–650, Dec 2011. doi: 10.1109/ICDMW.2011.83.  11  Published as a conference paper at ICLR 2016  Kifer, Daniel, Ben-David, Shai, and Gehrke, Johannes. Detecting change in data streams.  In Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30, VLDB ’04, pp. 180–191. VLDB Endowment, 2004. ISBN 0-12-088469-0. URL http: //dl.acm.org/citation.cfm?id=1316689.1316707.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980, 2014.  Korayem, Mohammed, Templeman, Robert, Chen, Dennis, Crandall, David J., and Kapadia, Apu. ScreenAvoider: protecting computer screens from ubiquitous cameras. CoRR, abs/1412.0008, 2014. URL http://arxiv.org/abs/1412.0008.  Lichman, M. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/  ml.  Louizos, C., Swersky, K., Li, Y., Welling, M., and Zemel, R. The variational fair auto encoder.  ArXiv e-prints, November 2015.  Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21- 24, 2010, Haifa, Israel, pp. 807–814, 2010. URL http://www.icml2010.org/papers/ 432.pdf.  Rudy, Jan and Taylor, Graham. Generative class-conditional autoencoders.  arXiv:1412.7009, 2014.  arXiv preprint  Schmidhuber, J. Learning factorial codes by predictability minimization. Technical Report CU-CS-  565-91, Dept. of Comp. Sci., University of Colorado at Boulder, December 1991.  Schmidhuber, J, Eldracher, M, and Foltin, B. Semilinear predictability minimization produces well- known feature detectors. Neural Computation, 8(4):773–786, May 1996. ISSN 0899-7667. doi: 10.1162/neco.1996.8.4.773.  Schraudolph, Nicol N., Eldracher, Martin, and Schmidhuber, J¨urgen. Processing images by semi- linear predictability minimization. Network: Computation in Neural Systems, 10(2):133–169, 1999.  Zafar, Muhammad Bilal, Valera, Isabel, Gomez-Rodriguez, Manuel, and Gummadi, Krishna P. Fairness constraints: A mechanism for fair classiﬁcation. CoRR, abs/1507.05259, 2015. URL http://arxiv.org/abs/1507.05259.  Zemel, Rich, Wu, Yu, Swersky, Kevin, Pitassi, Toni, and Dwork, Cynthia. Learning fair represen- tations. In Dasgupta, Sanjoy and Mcallester, David (eds.), Proceedings of the 30th International Conference on Machine Learning (ICML-13), volume 28, pp. 325–333. JMLR Workshop and Conference Proceedings, May 2013. URL http://jmlr.org/proceedings/papers/ v28/zemel13.pdf.  12  Published as a conference paper at ICLR 2016  A THE H-DIVERGENCE In this appendix we describe the notion of a H-divergence, as developed in Kifer et al. (2004) and Ben-David et al. (2007). The H-divergence is a way of measuring the difference between two distributions using classiﬁers. Deﬁnition 1. A hypothesis for a random variable X is a mapping η : X → {0, 1}. Deﬁnition 2. A hypothesis class H for a random variable X is a collection of hypotheses on X. Deﬁnition 3. A symmetrical hypothesis class H is a hypothesis class such that for each η ∈ H, the inverse hypothesis η deﬁned η(x) = 1 − η(x) is also in H.  Now we can deﬁne the divergence. Deﬁnition 4. Given two random variables X0, X1 on a common space X and a hypothesis class H on X , the H-divergence between X0 and X1 is dH(X0, X1) = sup η∈H  2|E η(X0) − E η(X1)| .  In case H is symmetric, Ben-David et al. (2010) show that dH(X0, X1) can be approximated em- pirically. Deﬁnition 5. Let H be a symmetrical hypothesis class on random variables X0, X1 on a common space X . Now given i.i.d samples A = {a1, . . . , an} from X0 and i.i.d samples B = {b1, . . . , dm} from X1, we deﬁne the empirical H-divergence between X0, X1 to be  ˆdH(X0, X1) = sup η∈H  2  η(x) − 1 n  η(x)  ,  (cid:34)  (cid:88)  x∈A  1 m  (cid:35)  (cid:88)  x∈B  The nature of this empirical approximation is shown in the following probabilistic bound from Ben- David et al. (2010). Lemma 1. Let H be a symmetrical hypothesis class with VC dimension d on random variables X0, X1 on a common space X . Now given i.i.d samples A = {a1, . . . , am} from X0 and i.i.d samples B = {b1, . . . , dm} from X1, we have that, for any δ ∈ (0, 1), with probability at least 1 − δ,  (cid:115)  dH(X0, X1) ≤ ˆdH(A, B) +  d log(2m) + log( 2 δ )  m  .  So we can see that minimizing the capability of an adversary to tell the difference between two distri- butions relates to minimizing an H-divergence. The empirical divergence can be straightforwardly related to the discrimination of a classiﬁer. Lemma 2. Let X, S, Y and R be as described in Section 4. Let η : R → Y be our classiﬁer and ydisc the discrimination as described in Section 3. Then  ydisc ≤ 1 2  ˆdH( ˆR0, ˆR1),  where H is a symmetric hypothesis class on R including η and ˆR0, ˆR1 are the representations of the empirical samples.  Proof. Recall that  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  ydisc =  (cid:80) | ˆR0| −  η(r)  r∈ ˆR0  13  (cid:80)  η(r)  r∈ ˆR1  | ˆR1|  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ,  Published as a conference paper at ICLR 2016  where m = | ˆR0| = | ˆR1|. We simply observe that  (cid:80) | ˆR0| −  η(r)  r∈ ˆR0  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  (cid:80)  η(r)  r∈ ˆR1  | ˆR1|  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ sup  η∗∈H  = sup η∗∈H  r∈ ˆR0  (cid:80) (cid:80)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   r∈ ˆR1  (cid:80) (cid:80)  −  −  η∗(r) | ˆR0|  η∗(r) | ˆR1|  r∈ ˆR0  η∗(r) | ˆR0|  r∈ ˆR1  η∗(r) | ˆR1|  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   where for the inequality we use the fact that η ∈ H and for the ﬁrst equality we use the fact that H is a symmetric hypothesis class.  =  ˆDH( ˆR0, ˆR1)  1 2  14  ",
1511.06085,2016,Variable Rate Image Compression with Recurrent Neural Networks,"['Variable Rate Image Compression with Recurrent Neural Networks\nGeorge Toderici', ""Sean O'Malley"", 'Damien Vincent', 'Sung Jin Hwang', 'Michele Covell', 'Shumeet Baluja', 'Rahul Sukthankar', 'David Minnen']",https://arxiv.org/pdf/1511.06085,"6 1 0 2    r a  M 1         ]  V C . s c [      5 v 5 8 0 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  VARIABLE RATE IMAGE COMPRESSION WITH RECURRENT NEURAL NETWORKS  George Toderici, Sean M. O’Malley, Sung Jin Hwang, Damien Vincent {gtoderici, smo, sjhwang, damienv}@google.com  David Minnen, Shumeet Baluja, Michele Covell & Rahul Sukthankar {dminnen, shumeet, covell, sukthankar}@google.com  Google Mountain View, CA, USA  ABSTRACT  A large fraction of Internet trafﬁc is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will signiﬁcantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efﬁcient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32×32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10% or more.  1  INTRODUCTION  The task of image compression has been thoroughly examined over the years by researchers and teams such as the Joint Pictures Experts Group, who designed the ubiquitous JPEG and JPEG 2000 (ISO/IEC 15444-1) image formats. More recently, the WebP algorithm was proposed in order to further improve image compression rates (Google, 2015), especially for the high-resolution images that have become more common in recent years. All these efforts approach the compression problem from an empirical standpoint: human experts design various heuristics to reduce the amount of information needing to be retained, then determine ways to transform the resulting data in a way that’s amenable to lossless compression. As this work is almost exclusively focused on the compression of large images, low-resolution thumbnail images are usually ignored (and even harmed, e.g., by requiring more data in ﬁle headers). Standard image compression algorithms tend to make assumptions about image scale. For example, we usually assume that a patch from a high-resolution natural image will contain a lot of redundant information. In fact, the higher-resolution an image is, the more likely it is that its component patches will contain mostly low-frequency information. This fact is exploited by most image codecs and, as such, these codecs tend to be very efﬁcient at compressing high-resolution images. However, such assumptions are broken when creating thumbnails from high-resolution natural images, as a  1  Published as a conference paper at ICLR 2016  patch taken from a thumbnail is much more likely to contain difﬁcult-to-compress, high-frequency information. Large-scale compression of thumbnails (e.g., 32×32 images) is an important application, both in terms of reducing disk storage and making better use of limited Internet bandwidth. Enormous numbers of thumbnails are currently transmitted across the web for page previews, photo galleries, search engine results, and numerous other applications. As such, any improvements to thumbnail compression will signiﬁcantly improve the experience of users accessing content over low-bandwidth connections. In recent years, neural networks have become commonplace to perform tasks that had for decades been accomplished by ad hoc algorithms and heuristics. For instance, in image recognition and object detection, the current state-of-the-art algorithms are all based on neural networks. It is only natural to ask if we can also employ this powerful class of methods to further improve the task of image compression, especially for image sizes for which we do not have carefully designed, hand-tuned compressors. If we consider an image codec broadly as an analysis/synthesis problem with a bottleneck in the middle, then we can ﬁnd a signiﬁcant body of research aimed toward teaching neural networks to discover compressive representations. Most of this work (e.g., Denton et al., 2015; Gregor et al., 2015), has been on synthesis of small images: often 32×32 in part due to CIFAR10 (Krizhevsky, 2009). Much of this work has focused on a class of neural networks known as autoencoders (Krizhevsky & Hinton, 2011). However, standard autoencoders operate under a number of hard constraints that have so far made them infeasible as a drop-in replacement for standard image codecs. Some of these constraints are that variable-rate encoding is typically not possible (one network is trained per compression rate); the visual quality of the output is hard to ensure; and they’re typically trained for a particular scale, being able to capture redundancy only at that scale. We explore several different ways in which neural network-driven image compression can improve compression rates while allowing similar ﬂexibility to modern codecs. To achieve this ﬂexibility, the network architectures we discuss must meet all of the following requirements: (1) the compression rate should be capable of being restricted to a prior bit budget; (2) the compressor should be able to encode simpler patches more cheaply (analogously to modern codecs which may allocate more bits to areas of the image which contain important visual features); and (3) the model should be able to learn from large amounts of existing imagery in order to optimize this compression process toward real-world data.  2 RELATED WORK  The basic principles of using feed-forward neural networks for image compression have been known for some time (Jiang, 1999). In this context, networks can assist or even entirely take over many of the processes used as part of a traditional image compression pipeline: to learn more efﬁcient frequency transforms, more effective quantization techniques, improved predictive coding, etc. More recently, autoencoder architectures (Hinton & Salakhutdinov, 2006) have become viable as a means of implementing end-to-end compression. A typical compressing autoencoder has three parts: (1) an encoder which consumes an input (e.g., a ﬁxed-dimension image or patch) and transforms it into (2) a bottleneck representing the compressed data, which can then be transformed by (3) a decoder into something resembling the original input. These three elements are trained end-to-end, but during deployment the encoder and decoder are normally used independently. The bottleneck is often simply a ﬂat neural net layer, which allows the compression rate and visual ﬁdelity of the encoded images to be controlled by adjusting the number of nodes in this layer before training. For some types of autoencoder, encoding the bottleneck as a simple bit vector can be beneﬁcial (Krizhevsky & Hinton, 2011). In neural net-based classiﬁcation tasks, images are repeatedly downsampled through convolution and pooling operations, and the entire output of the network might be contained in just a single node. In the decoder half of an autoencoder, however, the network must proceed in the opposite direction and convert a short bit vector into a much larger image or image patch. When this upsampling process is spatially-aware, resembling a “backward convolution,” it is commonly referred to as deconvolution (Long et al., 2014).  2  Published as a conference paper at ICLR 2016  Long short-term memory (LSTM) networks are a type of recurrent neural network (Hochreiter & Schmidhuber, 1997) that have proven very successful for tasks such as speech recognition (Graves et al., 2013) and machine translation (Sutskever et al., 2014). Many extensions to the standard LSTM model are possible including explicitly incorporating spatial information, which leads to various types of convolutional LSTMs (Shi et al., 2015) that may be better suited for image compression. We experiment with such models and also try simpler recurrent architectures that use the residual error of one autoencoder as the input to another.  3 VARIABLE RATE COMPRESSION ARCHITECTURES  We start by describing a general neural network-based compression framework and then discuss the details of multiple instantiations of this architecture. Each subsection describes a different architecture that builds on the previous model and improves the compression results. For each architecture, we will discuss a function E that takes an image patch as input and produces an encoded representation. This representation is then processed by a binarization function B, which is the same across architectures, and is discussed in Section 3.2. Finally, for each architecture we also consider a decoder function D, which takes the binary representation produced by B and generates a reconstructed output patch. Taken together, these three components form an autoencoder, x(cid:48) = D(B(E(x))), which is the basic building block for all of the compression networks. For all architectures, an offset and scale are applied to the 8-bit RGB input images to give a range of values between -0.9 and 0.9. This range is compatible with the values that can be emitted by tanh.  3.1  IMAGE COMPRESSION FRAMEWORK  The neural network architectures that we use share the same conceptual stages: an encoder network, followed by a quantizer, and a decoder network. In addition, our framework is tuned for image compression and supports variable compression rates without the need for retraining or for storing multiple encodings of the same image. To make it possible to transmit incremental information, the design should take into account the fact that image decoding will be progressive. With this design goal in mind, we can consider architectures that are built on top of residuals with the goal of minimizing the residual error in the reconstruction as additional information becomes available to the decoder. Formally, we chain multiple copies of a residual autoencoder, Ft, deﬁned as:  Ft(rt−1) = Dt(B(Et(rt−1))).  (1)  This chaining is explicit, in the case of our feed-forward-only networks (Section 3.3 and Section 3.5) and is implicit, through the recurrent structure, in the case of our LSTM networks (described in Section 3.4 and Section 3.6). In all cases, we set r0 to be equal to the original input patch, and then rt for t > 0 represents the residual error after t stages. For non-LSTM architectures (described in Sections 3.3 and 3.5), Ft has no memory, and so we only expect it to predict the residual itself. In this case, the full reconstruction is recovered by summing over all of the residuals, and each stage is penalized due to the difference between the prediction and the previous residual:  rt = Ft(rt−1) − rt−1.  rt = Ft(rt−1) − r0.  On the other hand, LSTM-based architectures (described in Sections 3.4 and 3.6) do hold state, and so we expect them to predict the original image patch in each stage. Accordingly, we compute the residual relative to the original patch:  In both cases, the full, multi-stage network is trained by minimizing (cid:107)rt(cid:107)2 is the total number of residual autoencoders in the model.  2 for t = 1 . . . N, where N  3.2 BINARY REPRESENTATION  In our networks, we employ a binarization technique ﬁrst proposed by Williams (1992), and similar to Krizhevsky & Hinton (2011) and Courbariaux et al. (2015). This binarization has three beneﬁts:  3  (2)  (3)  Published as a conference paper at ICLR 2016  (4)  (5)  (6)  (1) bit vectors are trivially serializable/deserializable for image transmission over the wire, (2) control of the network compression rate is achieved simply by putting constraints on the bit allowance, and (3) a binary bottleneck helps force the network to learn efﬁcient representations compared to standard ﬂoating-point layers, which may have many redundant bit patterns that have no effect on the output. The binarization process consists of two parts. The ﬁrst part consists of generating the required number of outputs (equal to the desired number of output bits) in the continuous interval [−1, 1]. The second part involves taking this real-valued representation as input and producing a discrete output in the set {−1, 1} for each value. For the ﬁrst step in the binarization process, we use a fully-connected layer with tanh activations. For the second part, following Raiko et al. (2015), one possible binarization b(x) of x ∈ [−1, 1] is deﬁned as:  b(x) = x + (cid:15) ∈ {−1, 1},  (cid:26)1 − x  (cid:15) ∼  with probability 1+x 2 , −x − 1 with probability 1−x 2 ,  where (cid:15) corresponds to quantization noise. We will use the regularization provided by the randomized quantization to allow us to cleanly backpropagate gradients through this binarization layer. Therefore, the full binary encoder function is:  B (x) = b(cid:0)tanh(W binx + bbin)(cid:1) .  where W bin and bbin are the standard linear weights and bias that transform the activations from the previous layer in the network. In all of our models, we use the above formulation for the forward pass. For the backward pass of back-propagation, we take the derivative of the expectation (Raiko et al., 2015). Since E[b(x)] = x for all x ∈ [−1, 1], we pass the gradients through b unchanged. In order to have a ﬁxed representation for a particular input, once the networks are trained, only the most likely outcome of b(x) is considered and b can be replaced by binf deﬁned as:  (cid:26)−1  +1  binf (x) =  if x < 0, otherwise.  (7)  The compression rate is determined by the number of bits generated in each stage, which corresponds to the number of rows in the W bin matrix, and by the number of stages, controlled by the number of repetitions of the residual autoencoder structure.  3.3 FEED-FORWARD FULLY-CONNECTED RESIDUAL ENCODER  In the simplest instantiation of our variable rate compression architecture, we set E and D to be composed of stacked fully-connected layers. In order to make the search for architectures more feasible we decided to set the number of outputs in each fully-connected layer to be constant (512) and only used the tanh nonlinearity. Given that E and D can be functions of the encoding stage number, and since the statistics of the residuals change when going from stage t to t + 1 we considered two distinct approaches: in the ﬁrst we share weights across all stages, while in the second, we learn the distinct weights independently in each stage. The details of this architecture are given in Figure 1.  3.4 LSTM-BASED COMPRESSION  In this architecture, we explore the use of LSTM models for both the encoder and the decoder. In particular, both E and D consist of stacked LSTM layers. Following the LSTM formulation and notation proposed by Zaremba et al. (2014), we use superscripts t ∈ Rn denote the hidden to indicate the layer number, and subscripts to indicate time steps. Let hl n : Rm → Rn to be an afﬁne transform state of l-th LSTM layer at time step t. We deﬁne T l n(x) = W lx + bl. Finally, let (cid:12) denote element-wise multiplication, and let h0 t be the input to the T l ﬁrst LSTM layer at time step t.  4  Published as a conference paper at ICLR 2016  Figure 1: The fully-connected residual autoencoder. We depict a two-iteration architecture, with the goal of the ﬁrst iteration being to encode the original input patch and the goal of the second iteration being to encode the residual from the ﬁrst level’s reconstruction. In our 64-bit results, reported in Table 1, we have 16 iterations giving 4 bits each. The blocks marked with 512 are fully-connected neural network layers with 512 units and tanh nonlinearities. The loss applied to the residuals in training is a simple L2 measure.  Figure 2: The fully-connected LSTM residual encoder. The 512 LSTM blocks represent LSTM layers with 512 units. This ﬁgure shows an unrolling of the LSTM, needed for training, to two time steps. The actual architecture would have only the ﬁrst row of blocks, with the functionality of the second row (and subsequent recursions) being realized by feeding the residual from the previous pass back into the ﬁrst LSTM block. For the results reported in Table 1, this repeated feeding back was done 16 times, to generate 64 bit representations. The vertical connections between the LSTM stages in the unrolling shows the effect of the persistent memory instead each LSTM. The loss is applied to the residuals in training is a simple L2 measure. Note that in contrast to Figure 1, in which the network after the ﬁrst step is used to predict the previous step’s residual error, in this LSTM architecture, each step predicts the actual output.  Figure 3: The convolutional / deconvolutional residual encoder. The convolutional layers are depicted as sharp rectangles, while the deconvolutional layers are depicted as rounded rectangles. The loss is applied to the residuals.  5  Published as a conference paper at ICLR 2016  Using this notation, the LSTM architecture can be written succinctly as proposed by Graves (2013):   i  f o g  (cid:19)   T l sigm  = t = o (cid:12) tanh(cid:0)cl  sigm sigm tanh t = f (cid:12) cl cl hl  4n  t  t hl t−1  (cid:18)hl−1 (cid:1) ,  t−1 + i (cid:12) g,  ,  (8)  (9) (10)  where sigm(x) = (1 + exp(−x))−1 denotes the sigmoid function. In these equations, sigm and tanh are applied element-wise. This alternate formulation of LSTM is useful because it reduces the numbers of separate operations needed to evaluate one step, which allows for an efﬁcient implementation on GPU. For the encoder, we use one fully-connected layer followed by two stacked LSTM layers. The decoder has the opposite structure: two stacked LSTM layers followed by a fully-connected layer with a tanh nonlinearity that predicts RGB values (we omit this layer in the diagrams to reduce clutter). The exact architecture used in the experiments is given in Figure 2 (minus the RGB conversion).  3.5 FEED-FORWARD CONVOLUTIONAL/DECONVOLUTIONAL RESIDUAL ENCODER  Section 3.3 proposed a fully-connected residual autoencoder. We extend this architecture by replacing the fully-connected layers with convolution operators in the encoder E and with deconvolutional operators in the decoder D. The ﬁnal layer of the decoder consists of a 1×1 convolution with three ﬁlters that converts the decoded representation into RGB values. We depict this architecture in Figure 3 (minus the RGB conversion). The deconvolutional operator is deﬁned as the transpose of the convolutional operator. Let ⊗k denote the convolutional operator with stride k, and let Sk denote the stride operator with stride factor k, i.e., Sk(x)(i, j) = x(k × i, k × j) for 2D multi-channel image x and pixel coordinate (i, j). Then W ⊗k x = Sk(W ⊗1 x). Note that the transpose of Sk is the “inﬂation” operator Tk:  (cid:26)x(i/k, j/k)  Tk(x)(i, j) =  0  if i, j are multiples of k, otherwise.  Thus we can deﬁne the deconvolutional operator (cid:11)k with stride k as follows:  W (cid:11)k x = W ⊗1 (Tk(x)).  (11)  (12)  3.6 CONVOLUTIONAL/DECONVOLUTIONAL LSTM COMPRESSION  t  , hl  t−1  T l 4n  1 ⊗k hl−1  (cid:0)hl−1  (cid:1) = W l  The ﬁnal architecture combines the convolutional and deconvolutional operators with LSTM. We 4n in equation (8) with convolutions deﬁne convolutional LSTM by replacing the transformation T l plus bias. Then the transformation function for convolutional LSTM with stride k is 2 ⊗1 hl  (13) The subscript belonging to T now refers to the depth (number of features) in the output feature maps. Note that the second convolution term represents the recurrent relation of convolutional LSTM so both its input and output must have the same size. Therefore, when a convolutional LSTM has a stride greater than one, the stride is only applied to the ﬁrst convolution term, while the second term is always computed with a stride of one. Finally, to build the encoder for this architecture, we replace the second and third convolutional layers from Figure 3 with convolutional LSTM layers. For the decoder, we cannot replace all convolutional operations with deconvolution due to the fact that the input to deconvolution often has a different spatial dimension than the output. For the purposes of deﬁning a deconvolutional LSTM, T4n becomes  t + W l  t−1 + bl.  , hl  T l 4n  (14) Here we use the subscripts c and d to differentiate between the weights associated to the convolution and deconvolution operations. To construct the deconvolutional LSTM decoder, we replace the second and third deconvolutional layers of the deconvolutional decoder from Figure 3 with deconvolutional LSTM.  t + W l  t−1 + bl.  t−1  t  d (cid:11)k hl−1  c ⊗1 hl  (cid:1) = W l  (cid:0)hl−1  6  Published as a conference paper at ICLR 2016  3.7 DYNAMIC BIT ASSIGNMENT  For the non-convolutional approaches presented here, it is natural to assign a varying number of bits per patch by allowing a varying number of iterations of the encoder. This could be determined by a target quality metric (e.g., PSNR). While not as natural, in the case of the convolutional approaches, a similar method may also be employed. The input image needs to be split into patches, and each patch processed independently, thereby allowing a different number of bits per region. However, this approach has disadvantages that will be discussed at the end of this paper.  4 EXPERIMENTS & ANALYSIS  4.1 TRAINING  In order to train the various neural network conﬁgurations, we used the Adam algorithm proposed by Kingma & Ba (2014). We experimented with learning rates of {0.1, 0.3, 0.5, 0.8, 1}. The L2 loss was normalized by the number of pixels in the patch and also by the number of total time steps (i.e., number of iterations unrolled) needed to fully encode the patch. We employed no perceptual weighting to improve the compression for evaluation under the SSIM measure. During training we used the unmodiﬁed L2 error measure. We experimented with the number of steps needed to encode each patch, varying this from 8 to 16. For the fully connected networks, we chose to use 8 bits per step for an 8×8 patch, allowing us to ﬁne tune the compression rate in increments of 8 bits. When scaled up to a 32×32 patch size, this allowed us to control the compression in increments of 128 bits. For the convolutional/deconvolutional networks, the encoders reduce the 32×32 input patch down to 8×8 through convolution operations with strides. We experimented with a binary output of 2 bits per pixel at this resolution, yielding a tunable compression rate with increments of 16 bytes per 32×32 block.  4.2 EVALUATION PROTOCOL AND METRICS  Evaluating image compression algorithms is a non-trivial task. The metric commonly used in this context is the peak signal-to-noise ratio (PSNR), however, PSNR is biased toward algorithms which have been tuned to minimize L2 loss. This would not be a fair comparison against methods like JPEG which have been tuned to minimize a form of perceptual loss. In our evaluation protocol we instead employ the Structural Similarity Index (SSIM), a popular perceptual similarity measure proposed by Wang et al. (2004). Since we’re evaluating compression performance on small 32×32 images, we do not smooth the images (a typical preprocess for SSIM). In addition, since we’re interested in quantifying how well local details are preserved, we split the images into 8×8 patches and compute the SSIM on each patch and on each color channel independently. The ﬁnal score is the average SSIM over all patches and channels. When analyzing the results, a higher score implies a better reconstruction, with a score of 1.0 representing a perfect reconstruction. The lowest possible score is 0.0. Note that while there are other metrics (e.g., Ponomarenko et al., 2007) which emulate the human visual system better than SSIM, we chose to use SSIM here due to its ubiquity and ease of comparison with previous work.  32×32 BENCHMARK  4.3 Our 32×32 benchmark dataset contains 216 million random color images collected from the public internet. To be included in the dataset, each image must originally have more than 32 pixels on both axes. Qualiﬁed images were then downsampled to 32×32, losing their original aspect ratios. This downsampling eliminates pre-existing compression artifacts for most images. The ﬁnal 32×32 images were then stored losslessly (as PNG) before being used for training and testing. For training the LSTM models, 90% of the images were used; the remaining 10% were set aside for evaluation. For evaluating the image codecs, we use a subset of this data containing 100k random images.  7  Published as a conference paper at ICLR 2016  Table 1: Comparison between the proposed methods for a given compression target size (in bytes) on the 32x32 image benchmark.  Patch Size  SSIM / 64B Target (Header-less Size)  SSIM / 128B Target (Header-less Size)  Header-less JPEG  Header-less JPEG 2000  Header-less WebP  Fully Connected Residual Encoder (Shared Weights) Fully Connected Residual Encoder (Distinct Weights) LSTM Compressor  Conv/Deconv Residual Encoder (Shared Weights) Conv/Deconv Residual Encoder (Distinct Weights) Convolutional/Deconvolutional Autoencoder Conv/Deconv LSTM Compressor  8×8  8×8  8×8 8×8 32×32  32×32 32×32 32×32  0.70  0.80  (72.5 bytes avg.)  (133 bytes avg.)  0.66  0.77  (73 bytes avg.)  (156 bytes avg.)  0.62  0.73  (80.7 bytes avg.)  (128.2 bytes avg.)  0.46  0.65  0.69  0.45  0.65  0.76 0.77  0.48  0.75  0.81  0.46  0.75  0.86 0.87  Table 1 summarizes the results on the 32×32 benchmark, comparing our two LSTM approaches to two JPEG codecs and to WebP. To avoid unfairly penalizing the codecs due to the unavoidable cost of their ﬁle headers, we exclude the header size from all metrics. Note also that since these standard codecs can not be tuned to an exact byte budget (e.g., 64 bytes excluding the ﬁle header), we search for the encoder quality setting that leads to a ﬁle whose size is as close as possible, but never less than, the target size. On average, this leads to each JPEG and WebP image consuming slightly more space than we allow for the LSTM models.  4.4 ANALYSIS These 32×32 images contain considerable detail that is perceptually relevant. As can be seen in Figure 4, compressing these images without destroying salient visual information or hallucinating false details is challenging. At these very low bitrates and spatial resolution, JPEG block artifacts become extremely prominent, and WebP either introduces blocking or overly blurs the image depending on the strength of the internal ﬁlter. Color smearing artifacts due to the codecs’ default (4:2:0) chroma subsampling are also clearly visible. Compared to JPEG, the non-convolutional LSTM model slightly reduces inter-block boundaries on some images but can also lead to increased color bleeding (e.g., on mandrill as shown in Figure 4). Furthermore, the visual quality never exceeds JPEG on average as measured by SSIM and shown in Figure 5. This motivates the (de)convolutional LSTM model, which eliminates block artifacts while avoiding excessive smoothing. It strikes the best balance between preserving real detail and avoiding color smearing, false gradients, and hallucinated detail not present in the original image. Note that the (de)convolutional LSTM model exhibits perceptual quality levels that are equal to or better than both JPEG and WebP at 4% – 12% lower average bitrate. We see this improvement despite the fact that, unlike JPEG and WebP, the LSTMs do not perform chroma subsampling as a preprocess. However, at the JPEG quality levels used in Figure 4, disabling subsampling (i.e., using 4:4:4 encoding) leads to a costly increase in JPEG’s bitrate: 1.32-1.77 bpp instead of 1.05-1.406 bpp, or 26% greater. This means that if we desired to preserve chroma ﬁdelity, we would need to drastically  8  Published as a conference paper at ICLR 2016  Original (32×32)  JPEG compressed images  WebP compressed images  Compressed images with LSTM architecture  Average bits per pixel (bpp)  Compressed images with conv/deconv LSTM architecture  From left to right  0.641 JPEG 0.789 WebP LSTM 0.625 (De)Convolutional LSTM 0.625  0.875 0.914 0.875 0.875  1.117 1.148 1.125 1.125  1.375 1.398 1.375 1.375  Figure 4: 32×32 image compression comparison between JPEG and convolutional/deconvolutional LSTM architecture.  reduce JPEG encoding quality in order to produce 4:4:4 JPEGs at a comparable bitrate to the LSTM models. In terms of coding efﬁciency, we took an autoencoder architecture (one iteration of the model presented in Section 3.5) with a given bit budget of either 64 or 128 bytes, and compared its SSIM against the (de)convolutional LSTM encoder at these targets. In both cases, the LSTM model produces SSIM values that are equivalent to the autoencoder, even though the resulting model is more ﬂexible.  5 CONCLUSION & FUTURE WORK  We describe various methods for variable-length encoding of image patches using neural networks, and demonstrate that for the given benchmark, the fully-connected LSTM model can perform on par with JPEG, while the convolutional/deconvolutional LSTM model is able to signiﬁcantly outperform JPEG on the SSIM perceptual metric.  9  Published as a conference paper at ICLR 2016  Figure 5: Rate-distortion graph showing the SSIM for different codecs at different target bit rates. The results are averaged over 100k images for JPEG and WebP and over a 10% hold-out set of over 21 million images for the two LSTM models. The (de)convolutional LSTM model provides the highest SSIM across all low bit rates, though we expect WebP to provide better SSIM at higher bit rates.  While our current approach gives favorable results versus modern codecs on small images, codecs that include an entropy coder element tend to improve (in a bits-per-pixel sense) with greater resolution, meaning that by choosing an arbitrarily large test image it is always possible to defeat an approach like that described in this work. Therefore, an obvious need is to extend the current work to function on arbitrarily large images, taking advantage of spatial redundancy in images in a manner similar to entropy coding. Although we presented a solution for dynamic bit assignment in the convolutional case, it is not a fully satisfactory solution as it has the potential to introduce encoding artifacts at patch boundaries. Another topic for future work is determining a dynamic bit assignment algorithm that is compatible with the convolutional methods we present, while not creating such artifacts. The algorithms that we present may also be extended to work on video, which we believe to be the next grand challenge for neural network-based compression.  REFERENCES Courbariaux, M., Bengio, Y., and David, J.-P. BinaryConnect: Training deep neural networks with  binary weights during propagations. In NIPS, 2015.  Denton, E., Chintala, S., Szlam, A., and Fergus, R. Deep generative image models using a laplacian  pyramid of adversarial networks. arXiv preprint arXiv:1506.05751, 2015.  Google. WebP Compression Study. https://developers.google.com/speed/webp/  docs/webp_study, 2015. Accessed: 2015-11-10.  Graves, A. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,  2013.  Graves, A., Mohamed, A., and Hinton, G. Speech recognition with deep recurrent neural networks.  In International Conference on Acoustics, Speech and Signal Processing, 2013.  Gregor, K., Danihelka, I., Graves, A., Rezende, D. J., and Wierstra, D. Draw: A recurrent neural  network for image generation. arXiv preprint arXiv:1502.04623, 2015.  10  Published as a conference paper at ICLR 2016  Hinton, G. E. and Salakhutdinov, R. R. Reducing the dimensionality of data with neural networks.  Science, 313(5786):504–507, 2006.  Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 9(8), 1997.  ISO/IEC 15444-1. Information technology–JPEG 2000 image coding system. Standard, International  Organization for Standardization, Geneva, CH, December 2000.  Jiang, J. Image compression with neural networks–a survey. Signal Processing: Image Communica-  tion, 14:737–760, 1999.  Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.  URL http://arxiv.org/abs/1412.6980.  Krizhevsky, A. and Hinton, G. E. Using very deep autoencoders for content-based image retrieval. In  European Symposium on Artiﬁcial Neural Networks, 2011.  Krizhevsky, Alex. Learning multiple layers of features from tiny images. Technical report, 2009.  Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation.  CoRR, abs/1411.4038, 2014. URL http://arxiv.org/abs/1411.4038.  Ponomarenko, N., Silvestri, F., Egiazarian, K., Carli, M., Astola, J., and Lukin, V. On between- In Proc. 3rd Int’l. Workshop on Video  coefﬁcient contrast masking of DCT basis functions. Processing and Quality Metrics, volume 4, 2007.  Raiko, T., Berglund, M., Alain, G., and Dinh, L. Techniques for learning binary stochastic feedforward  neural networks. ICLR, 2015.  Shi, X., Chen, Z., Wang, H., Yeung, D.-Y., Wong, W.-K., and Woo, W.-C. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. CoRR, abs/1506.04214, 2015. URL http://arxiv.org/abs/1506.04214.pdf.  Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. CoRR,  abs/1409.3215, 2014. URL http://arxiv.org/abs/1409.3215.  Wang, Z., Bovik, A., Conrad, A., Sheikh, H. R., and Simoncelli, E. P. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004.  Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement  learning. Machine learning, 8(3-4):229–256, 1992.  Zaremba, W., Sutskever, I., and Vinyals, O. Recurrent neural network regularization. arXiv preprint  arXiv:1409.2329, 2014.  11  Published as a conference paper at ICLR 2016  Figure 6: The effect of the ﬁrst four bits on compressing a cat image. The image on the top left has been created by using a single bit for each 8×8 block. The subsequent images add one additional bit to be processed by the LSTM decoder (the ordering is top-left going to bottom-right). The ﬁnal image (bottom right) has been created by running four steps of the algorithm, thus allowing a total of four bits to be used to encode each 8×8 block.  Figure 7: Four 8×8 blocks are encoded one bit at a time using the fully connected LSTM model. The blocks were encoded with 1, 4, 8, 12 bits (from left to right).  6 APPENDIX: BITWISE ENCODING & DECODING  In order to better understand the network architecture proposed in Section 3.4, we initially limited it in terms of its capacity (bottleneck size) and target (complexity of reconstruction). Namely, we restricted the output per step to one bit, and trained the network to compress grayscale images. We took this simpler network and encoded a popular image of a cat one bit at a time. Figure 6 shows the effect of the ﬁrst four steps of this encoding. Figure 7 depicts the behavior of additional bits on four 8×8 blocks from the cat image using the same network. In this zoomed-in version it is apparent that the network ﬁrst learns to differentiate between “dark” and “light” patches using the ﬁrst bit. Given an additional bit, the network is able to introduce new solid shades of gray. One more bit starts introducing simple gradients, which are further reﬁned with a fourth bit, and so on.  12  ",
1511.06432,2016,Delving Deeper into Convolutional Networks for Learning Video Representations,"['Delving Deeper into Convolutional Networks for Learning Video Representations\nNicolas Ballas', 'Li Yao', 'Pal Chris', 'Aaron Courville']",https://arxiv.org/pdf/1511.06432,"6 1 0 2    r a  M 1         ]  V C . s c [      4 v 2 3 4 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  DELVING DEEPER INTO CONVOLUTIONAL NETWORKS FOR LEARNING VIDEO REPRESENTATIONS  Nicolas Ballas1, Li Yao1, Chris Pal2, Aaron Courville1 1MILA, Universit´e de Montr´eal. 2 ´Ecole Polytechnique de Mont´real.  ABSTRACT  We propose an approach to learn spatio-temporal features in videos from interme- diate visual representations we call “percepts” using Gated-Recurrent-Unit Re- current Networks (GRUs). Our method relies on percepts that are extracted from all levels of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model ﬁner motion patterns. Using low-level percepts, however, can lead to high-dimensionality video rep- resentations. To mitigate this effect and control the number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler caption-decoder model and without extra 3D CNN features.  1  INTRODUCTION  Video analysis and understanding represents a major challenge for computer vision and machine learning research. While previous work has traditionally relied on hand-crafted and task-speciﬁc representations (Wang et al., 2011; Sadanand & Corso, 2012), there is a growing interest in designing general video representations that could help solve tasks in video understanding such as human action recognition, video retrieval or video captionning (Tran et al., 2014). Two-dimensional Convolutional Neural Networks (CNN) have exhibited state-of-art performance in still image tasks such as classiﬁcation or detection (Simonyan & Zisserman, 2014b). However, such models discard temporal information that has been shown to provide important cues in videos (Wang et al., 2011). On the other hand, recurrent neural networks (RNN) have demonstrated the ability to understand temporal sequences in various learning tasks such as speech recognition (Graves & Jaitly, 2014) or machine translation (Bahdanau et al., 2014). Consequently, Recurrent Convolution Networks (RCN) (Srivastava et al., 2015; Donahue et al., 2014; Ng et al., 2015) that leverage both recurrence and convolution have recently been introduced for learning video representation. Such approaches typically extract “visual percepts” by applying a 2D CNN on the video frames and then feed the CNN activations to an RNN in order to characterize the video temporal variation. Previous works on RCNs has tended to focus on high-level visual percepts extracted from the 2D CNN top-layers. CNNs, however, hierarchically build-up spatial invariance through pooling lay- ers (LeCun et al., 1998; Simonyan & Zisserman, 2014b) as Figure 2 highlights. While CNNs tends to discard local information in their top layers, frame-to-frame temporal variation is known to be smooth. The motion of video patches tend to be restricted to a local neighborhood (Brox & Malik, 2011). For this reason, we argue that current RCN architectures are not well suited for capturing ﬁne motion information. Instead, they are more likely focus on global appearance changes such as shot transitions. To address this issue, we introduce a novel RCN architecture that applies an RNN not solely on the 2D CNN top-layer but also on the intermediate convolutional layers. Convolutional  1  Published as a conference paper at ICLR 2016  Figure 1: Visualization of convolutional maps on successive frames in video. As we go up in the CNN hierarchy, we observe that the convolutional maps are more stable over time, and thus discard variation over short temporal windows.  layer activations, or convolutional maps, preserve a ﬁner spatial resolution of the input video from which local spatio-temporal patterns are extracted. Applying an RNN directly on intermediate convolutional maps, however, inevitably results in a dras- tic number of parameters characterizing the input-to-hidden transformation due to the convolutional maps size. On the other hand, convolutional maps preserve the frame spatial topology. We propose to leverage this topology by introducing sparsity and locality in the RNN units to reduce the memory requirement. We extend the GRU-RNN model (Cho et al., 2014) and replace the fully-connected RNN linear product operation with a convolution. Our GRU-extension therefore encodes the locality and temporal smoothness prior of videos directly in the model structure. We evaluate our solution on UCF101 human action recognition from Soomro et al. (2012) as well as the YouTube2text video captioning dataset from Chen & Dolan (2011). Our experiments show that leveraging “percepts” at multiple resolutions to model temporal variation improves performance over our baseline model with respective gains of 3.4% for action recognition and 10% for video captioning.  2 GRU: GATED RECURRENT UNIT NETWORKS  In this section, we review Gated-Recurrent-Unit (GRU) networks which are a particular type of RNN. An RNN model is applied to a sequence of inputs, which can have variable lengths. It deﬁnes a recurrent hidden state whose activation at each time is dependent on that of the previous time. Speciﬁcally, given a sequence X = (x1, x2, ..., xT ), the RNN hidden state at time t is deﬁned as ht = φ(ht−1, xt), where φ is a nonlinear activation function. RNNs are known to be difﬁcult to train due to the exploding or vanishing gradient effect (Bengio et al., 1994). However, variants of RNNs such as Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Cho et al., 2014) have empirically demonstrated their ability to model long-term temporal dependency in various task such as machine translation or image/video caption generation. In this paper, we will mainly focus on GRU networks as they have shown similar performance to LSTMs but with a lower memory requirement (Chung et al., 2014). GRU networks allow each recurrent unit to adaptively capture dependencies of different time scales. The activation ht of the GRU is deﬁned by the following equations:  zt = σ(Wzxt + Uzht−1), rt = σ(Wrxt + Urht−1), ˜ht = tanh(Wxt + U(rt (cid:12) ht−1), ht = (1 − zt)ht−1 + zt˜ht,  2  (1) (2) (3) (4)  … … … … … … Published as a conference paper at ICLR 2016  Figure 2: High-level visualization of our model. Our approach leverages convolutional maps from different layers of a pretrained-convnet. Each map is given as input to a convolutional GRU-RNN (hence GRU-RCN) at different time-step. Bottom-up connections may be optionally added between RCN layers to form Stack-GRU-RCN.  where (cid:12) is an element-wise multiplication. zt is an update gate that decides the degree to which the unit updates its activation, or content. rt is a reset gate. σ is the sigmoid function. When a unit t is close to 0, the reset gate forgets the previously computed state, and makes the unit act as if ri it is reading the ﬁrst symbol of an input sequence. ˜ht is a candidate activation which is computed similarly to that of the traditional recurrent unit in an RNN.  3 DELVING DEEPER INTO CONVOLUTIONAL NEURAL NETWORKS  This section delves into the main contributions of this work. We aim at leveraging visual percepts from different convolutional levels in order to capture temporal patterns that occur at different spatial resolution. t )(t=1..T ), a set of 2D convolutional maps extracted from L layers Let’s consider (x1 at different time steps in a video. We propose two alternative RCN architectures, GRU-RCN, and Stacked-GRU-RCN (illustrated in Figure 2) that combines information extracted from those convo- lutional maps.  t , ..., xL−1  , xL  t  3.1 GRU-RCN:  t, hl  T , ..., hL  t = φl(xl  In the ﬁrst RCN architecture, we propose to apply L RNNs independently on each convolutional t−1). The hidden representation of map. We deﬁne L RNNs as φ1, ..., φL, such that hl the ﬁnal time step h1 T are then fed to a classiﬁcation layer in the case of action recognition, or to a text-decoder RNN for caption generation. To implement the RNN recurrent function φl, we propose to leverage Gated Recurrent Units (Cho et al., 2014). GRUs were originally introduced for machine translation. They model input to hidden- state and hidden to hidden transitions using fully connected units. However, convolutional map inputs are 3D tensors (spatial dimension and input channels). Applying a GRU directly can lead to a drastic number of parameters. Let N1, N2 and Ox be the input convolutional map spatial size and number of channels. Applying a GRU directly would require input-to-hidden parameters Wl, Wl r to be of size N1 × N2 × Ox × Oh where Oh is the dimensionality of the GRU hidden z and Wl representation.  3  … … … … … … … … … Published as a conference paper at ICLR 2016  Fully-connected GRUs do not take advantage of the underlying structure of convolutional maps. Indeed, convolutional maps are extracted from images that are composed of patterns with strong local correlation which are repeated over different spatial locations. In addition, videos have smooth temporal variation over time, i.e. motion associated with a given patch in successive frames will be restricted in a local spatial neighborhood. We embed such a prior in our model structure and replace the fully-connected units in GRU with convolution operations. We therefore obtain recurrent units that have sparse connectivity and share their parameters across different input spatial locations:  z ∗ xl r ∗ xl  z ∗ hl r ∗ hl  ˜hl t,  t)hl  t−1),  z, Ul  z, Wl  t = (hl  t−1 + zl t  t (cid:12) hl  t + Ul t + Ul  t(i, j)) where hl  t−1), t−1), t + U ∗ (rl  zl t = σ(Wl rl t = σ(Wl t = tanh(Wl ∗ xl ˜hl t = (1 − zl hl  (5) (6) (7) (8) where ∗ denotes a convolution operation. In this formulation, Model parameters W, Wl r and r are 2D-convolutional kernels. Our model results in hidden recurrent representation that Ul, Ul t(i, j)) is a feature vector deﬁned at the preserves the spatial topology, hl location (i, j). To ensure that the spatial size of the hidden representation remains ﬁxed over time, we use zero-padding in the recurrent convolutions. r have a size of k1×k2×Ox×Oh where k1×k2 is the Using convolution, parameters Wl, Wl convolutional kernel spatial size (usually 3 × 3), chosen to be signiﬁcantly lower than convolutional map size N1 × N2. The candidate hidden representation ˜ht(i, j), the activation gate zk(i, j) and the reset gate rk(i, j) are deﬁned based on a local neigborhood of size (k1 × k2) at the location(i, j) in both the input data xt and the previous hidden-state ht−1. In addition, the size of receptive ﬁeld associated with hl(i, j)t increases in the previous presentation hl t−2... as we go back further in time. Our model is therefore capable of characterizing spatio-temporal patterns with high spatial variation in time. A GRU-RCN layer applies 6 2D-convolutions at each time-step (2 per GRU gate and 2 for com- puting the candidate activation). If we assume for simplicity that the input-to-hidden and hidden-to- hidden convolutions have the same kernel size and perserve the input dimension, GRU-RCN requires O(3T N1N2k1k2(OxOh + OhOh)) multiplications. GRU-RCN sparse connectivity therefore saves computation compared to a fully-connected RNN that would require O(3T N1N2N1N2(OxOh + OhOh)) computations. Memorywise, GRU-RCN needs to store the parameters for all 6 convolu- tions kernels leading to O(3k1k2(OxOh + OhOh)) parameters.  z and Wl  t−1, hl  3.2 STACKED GRU-RCN:  In the second RCN architecture, we investigate the importance of bottom-up connection across RNNs. While GRU-RCN applies each layer-wise GRU-RNN in an independent fashion, Stacked GRU-RCN preconditions each GRU-RNN on the output of the previous GRU-RNN at the current t). The previous RNN hidden representation is given as an extra- time step: hl input to the GRU convolutional units:  t−1, hl−1  t = φl(hl  , xl  t  z ∗ xl r ∗ xl  (9) (10) (11) (12) Adding this extra-connection brings more ﬂexibility and gives the opportunity for the model to leverage representations with different resolutions.  zl l = σ(Wl rl t = σ(Wl t = tanh(Wl ∗ xl ˜hl t = (1 − zl hl  t + Ul t + Ul t + Ul ∗ (rt (cid:12) hl  t + Wl t + Wl  t−1 + zl t  t−1),  t)hl  ˜hl t,  z ∗ hl rhl t−1), t−1),  zl ∗ hl−1 rl ∗ hl−1  4 RELATED WORK  Deep learning approaches have recently been used to learn video representations and have produced state-of-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014). Karpathy et al. (2014); Tran et al. (2014) proposed to use 3D CNN learn a video rep- resentations, leveraging large training datasets such as the Sport 1 Million. However, unlike image  4  Published as a conference paper at ICLR 2016  classiﬁcation (Simonyan & Zisserman, 2014b), CNNs did not yield large improvement over these traditional methods (Lan et al., 2014) highlighting the difﬁculty of learning video representations even with large training dataset. Simonyan & Zisserman (2014a) introduced a two-stream frame- work where they train CNNs independently on RGB and optical ﬂow inputs. While the ﬂow stream focuses only on motion information, the RGB stream can leverage 2D CNN pre-trained on image datasets. Based on the Two Stream representation, Wang et al. (2015a) extracted deep feature and conducted trajectory constrained pooling to aggregate convolutional feature as video representations. RNN models have also been used to encode temporal information for learning video representations in conjonction with 2D CNNs. Ng et al. (2015); Donahue et al. (2014) applied an RNN on top of the the two-stream framework, while Srivastava et al. (2015) proposed, in addition, to investigate the beneﬁt of learning a video representation in an unsupervised manner. Previous works on this topic has tended to focus only on high-level CNN “visual percepts”. In contrast, our approach proposes to leverage visual “percepts” extracted from different layers in the 2D-CNN. Recently, Shi et al. (2015) also proposed to leverage convolutional units inside an RNN network. However, they focus on different task (now-casting) and a different RNN model based on an LSTM. In addition, they applied their model directly on pixels. Here, we use recurrent convolutional units on pre-trained CNN convolutional maps, to extract temporal pattern from visual “percepts” with different spatial sizes.  5 EXPERIMENTATION  This section presents an empirical evaluation of the proposed GRU-RCN and Stacked GRU-RCN architectures. We conduct experimentations on two different tasks: human action recognition and video caption generation.  5.1 ACTION RECOGNITION  We evaluate our approach on the UCF101 dataset Soomro et al. (2012). This dataset has 101 action classes spanning over 13320 YouTube videos clips. Videos composing the dataset are subject to large camera motion, viewpoint change and cluttered backgrounds. We report results on the dataset UCF101 ﬁrst split, as this is most commonly used split in the literature. To perform proper hyper- parameter seach, we use the videos from the UCF-Thumos validation split Jiang et al. (2014) as the validation set.  5.1.1 MODEL ARCHITECTURE  In this experiment, we consider the RGB and ﬂow representations of videos as inputs. We extract visual “percept” using VGG-16 CNNs that consider either RGB or ﬂow inputs. VGG-16 CNNs are pretrained on ImageNet (Simonyan & Zisserman, 2014b) and ﬁne-tuned on the UCF-101 dataset, following the protocol in Wang et al. (2015b). We then extract the convolution maps from pool2, pool3, pool4, pool5 layers and the fully-connected map from layer fc-7 (which can be view as a feature map with a 1 × 1 spatial dimension). Those features maps are given as inputs to our RCN models. We design and evaluate three RCN architectures for action recognition. In the ﬁrst RCN architecture, GRU-RCN, we apply 5 convolutional GRU-RNNs independently on each convolutional map. Each convolution in the GRU-RCN has zero-padded 3 × 3 convolutions that preserves the spatial dimen- sion of the inputs . The number of channels of each respective GRU-RNN hidden-representations are 64, 128, 256, 256, 512. After the RCN operation we obtain 5 hidden-representations for each time step. We apply average pooling on the hidden-representations of the last time-step to reduce their spatial dimension to 1 × 1, and feed the representations to 5 classiﬁers, composed by a linear layer with a softmax nonlineary. Each classiﬁer therefore focuses on only 1 hidden-representation extracted from the convolutional map of a speciﬁc layer. The classiﬁer outputs are then averaged to get the ﬁnal decision. A dropout ratio of 0.7 is applied on the input of each classiﬁers. In the second RCN architecture, Stacked GRU-RCN, we investigate the usefulness of bottom-up connections. Our stacked GRU-RCN uses the same base architecture as the GRU-RCN, consist- ing of 5 convolutional GRU-RNNs having 64, 128, 256, 256 channels respectively. However, each  5  Published as a conference paper at ICLR 2016  convolutional GRU-RNN is now preconditioned on the hidden-representation that the GRU-RNN applied on the previous convolution-map outputs. We apply max-pooling on the hidden represen- tations between the GRU-RNN layers for the compatibility of the spatial dimensions. As for the previous architecture, each GRU-RNN hidden-representation at the last time step is pooled and then given as input to a classiﬁer. Finally, in our bi-directional GRU-RCN, we investigate the importance of reverse temporal informa- tion. Given convolutional maps extracted from one layer, we run the GRU-RCN twice, considering the inputs in both sequential and reverse temporal order. We then concatenate the last hidden- representations of the foward GRU-RCN and backward GRU-RCN, and give the resulting vector to a classiﬁer.  5.1.2 MODEL TRAINING AND EVALUATION  We follow the training procedure introduced by the two-stream framework Simonyan & Zisser- man (2014a). At each iteration, a batch of 64 videos are sampled randomly from the the train- ing set. To perform scale-augmentation, we randomly sample the cropping width and height from 256, 224, 192, 168. The temporal cropping size is set to 10. We then resize the cropped volume to 224 × 224 × 10. We estimate each model parameters by maximizing the model log-likelihood:  N(cid:88)  n=1  L(θ) =  1 N  log p(yn | c(xn), θ),  where there are N training video-action pairs (xn, yn), c is a function that takes a crop at random. We use Adam Kingma & Ba (2014) with the gradient computed by the backpropagation algorithm. We perform early stopping and choose the parameters that maximize the log-probability of the vali- dation set. We also follow the evaluation protocol of the two-stream framework Simonyan & Zisserman (2014a). At the test time, we sample 25 equally spaced video sub-volumes with a temporal size of 10 frames. From each of these selected sub-volumes, we obtain 10 inputs for our model, i.e. 4 corners, 1 center, and their horizontal ﬂipping. The ﬁnal pre- diction score is obtained by averaging across the sampled sub-volumes and their cropped regions.  5.1.3 RESULTS  Method VGG-16  VGG-16 RNN  GRU-RCN  Stacked-GRU RCN  Bi-directional GRU-RCN  Two-Stream Simonyan & Zisserman (2014b) Two-Stream + LSTM Donahue et al. (2014)  Two-Stream + LSTM + Unsupervised Srivastava et al. (2015)  Improved Two-Stream Wang et al. (2015b)  C3D one network Tran et al. (2014), 1 million videos as training C3D ensemble Tran et al. (2014), 1 million videos as training  Deep networks Karpathy et al. (2014), 1 million videos as training  - -  RGB Flow 85.4 78.0 84.9 78.1 85.7 79.9 78.3 80.7 72.8 71.1 77.7 79.8 82.3 85.2 65.2  81.2 76.9 83.7 85.7  - - -  Table 1: Classiﬁcation accuracy of different variants of the model on the UCF101 split 1. We report the performance of previous works that learn representation using only RGB information only.  We compare our approach with two different baselines, VGG-16 and VGG-16 RNN. VGG-16 is the 2D spatial stream that is described in Wang et al. (2015b). We take the VGG-16 model, pre- trained on Image-Net and ﬁne-tune it on the UCF-101 dataset. VGG-16 RNN baseline applied an RNN, using fully-connected gated-recurrent units, on top-of VGG-16. It takes as input the VGG-16 fully-connected representation fc-7. Following GRU-RCN top-layer, the VGG-16 RNN has hidden- representation dimensionality of 512.  6  Published as a conference paper at ICLR 2016  The ﬁrst column of Table 1 focuses on RGB inputs. We ﬁrst report results of different GRU-RCN variants and compare them with the two baselines: VGG-16 and VGG-16 RNN. Our GRU-RCN variants all outperform the baselines, showing the beneﬁt of delving deeper into a CNN in order to learn a video representation. We notice that VGG-16 RNN only slightly improve over the VGG- 16 baseline, 78.1 against 78.0. This result conﬁrms that CNN top-layer tends to discard temporal variation over short temporal windows. Stacked-GRU RCN performs signiﬁcantly lower than GRU- RCN and Bi-directional GRU-RCN. We argue that bottom-up connection, increasing the depth of the model, combined with the lack of training data (UCF-101 is train set composed by only 9500 videos) make the Stacked-GRU RCN learning difﬁcult. The bi-directional GRU-RCN performs the best among the GRU-RCN variant with an accuracy of 80.7, showing the advantage of modeling temporal information in both sequential and reverse order. Bi-directional GRU-RCN obtains a gain 3.4% in term of performances, relatively to the baselines that focus only the VGG-16 top layer. Table 1 also reports results from other state-of-art approaches using RGB inputs. C3D Tran et al. (2014) obtains the best performance on UCF-101 with 85.2. However, it should be noted that C3D is trained over 1 million videos. Other approaches use only the 9500 videos of UCF101 training set for learning temporal pattern. Our Bi-directional GRU-RCN compare favorably with other Recurrent Convolution Network (second blocks), conﬁrming the beneﬁt of using different CNN layers to model temporal variation. Table 1 also evaluates the GRU-RCN model applied ﬂow inputs. VGG-16 RNN baseline actually decreases the performance compared to the VGG-16 baseline. On the other hand, GRU-RCN outper- forms the VGG-16 baseline achieving 85.7 against 85.4. While the improvement is less important than the RGB stream, it should be noted that the ﬂow stream of VGG-16 is applied on 10 consecutive ﬂow inputs to extract visual “percepts”, and therefore already captures some motion information. Finally, we investigate the combination of the RGB and ﬂow streams. Following Wang et al. (2015b), we use a weighted linear combination of their prediction scores, where the weight is set to 2 as for the ﬂow stream net and 1 for the temporal stream. Fusion the VGG-16 model baseline achieve an accuracy of 89.1. Combining the RGB Bi-directional GRU-RCN with the ﬂow GRU-RCN achieves a performance gain of 1.9% over baseline, reaching 90.8. Our model is on part with Wang et al. (2015b) that obtain state-of-art results using both RGB and ﬂow streams which obtains 90.9.  5.2 VIDEO CAPTIONING  We also evaluate our representation on the video captioning task using YouTube2Text video cor- pus Chen & Dolan (2011). The dataset has 1,970 video clips with multiple natural language de- scriptions for each video clip. The dataset is open-domain and covers a wide range of topics such as sports, animals, music and movie clips. Following Yao et al. (2015b), we split the dataset into a training set of 1,200 video clips, a validation set of 100 clips and a test set consisting of the remaining clips.  5.2.1 MODEL SPECIFICATIONS  To perform video captioning, we use the so-called encoder-decoder framework Cho et al. (2014). In this framework the encoder maps input videos into abstract representations that precondition a caption-generating decoder. As for encoder, we compare both VGG-16 CNN and Bi-directional GRU-RCN. Both models have been ﬁne-tuned on the UCF-101 dataset and therefore focus on detecting actions. To extract an abstract representation from a video, we sample K equally-space segments. When using the VGG- 16 encoder, we provide the f c7 layer activations of the each segment’s ﬁrst frame as the input to the text-decoder. For the GRU-RCN, we apply our model on the segment’s 10 ﬁrst frames. We concatenate the GRU-RCN hidden-representation from the last time step. The concatenated vector is given as the input to the text decoder. As it has been shown that characterizing entities in addition of action is important for the caption-generation task Yao et al. (2015a), we also use as encoder a CNN Szegedy et al. (2014), pretrained on ImageNet, that focuses on detecting static visual object categories. As for the decoder, we use an LSTM text-generator with soft-attention on the video temporal frames Yao et al. (2015b).  7  Published as a conference paper at ICLR 2016  Model VGG-16 Encoder Bi-directional GRU-RCN Encoder GoogleNet Encoder GoogleNet + Bi-directional GRU-RCN Encoder GoogleNet + Bi-directional GRU-RCN Encoder GoogleNet + Bi-directional GRU-RCN Encoder GoogleNet + Bi-directional GRU-RCN Encoder GoogleNet + HRNE (Pan et al., 2015) VGG + p-RNN (Yu et al., 2015) VGG + C3D + p-RNN (Yu et al., 2015) Soft-attention Yao et al. (2015b) Venugopalan et al. Venugopalan et al. (2015) + Extra Data (Flickr30k, COCO) Thomason et al. Thomason et al. (2014)  model selection  BLEU BLEU BLEU BLEU NLL  METEOR  CIDEr  - - - - - - -  YouTube2Text  BLEU METEOR 0.2640 0.3700 0.2850 0.4100 0.4128 0.2900 0.4963 0.3075 0.3114 0.4790 0.3170 0.4842 0.3160 0.4326 0.321 0.436 0.311 0.443 0.326 0.499 0.2960 0.4192 0.3119 0.2687 0.2907 0.3329 0.1368 0.2390  CIDEr 0.4330 0.5010 0.4804 0.5937 0.6782 0.6538 0.6801  0.5167  - - -  - - -  Table 2: Performance of different variants of the model on YouTube2Text for video captioning. Representations obtained with the proposed RCN architecture combined with decoders from Yao et al. (2015b) offer a signiﬁcant performance boost, reaching the performance of the other state-of- the-art models.  5.2.2 TRAINING  For all video captioning models, we estimated the parameters θ of the decoder by maximizing the log-likelihood:  N(cid:88)  tn(cid:88)  n=1  i=1  L(θ) =  1 N  log p(yn i  | yn  <i, xn, θ),  where there are N training video-description pairs (xn, yn), and each description yn is tn words long. We used Adadelta Zeiler (2012) We optimized the hyperparameters (e.g. number of LSTM units and the word embedding dimensionality, number of segment K) using random search (Bergstra & Bengio, 2012) to maximize the log-probability of the validation set.  5.2.3 RESULTS  Table 2 reports the performance of our proposed method using three automatic evaluation metrics. These are BLEU in Papineni et al. (2002), METEOR in Denkowski & Lavie (2014) and CIDEr in Vedantam et al. (2014). We use the evaluation script prepared and introduced in Chen et al. (2015). All models are early-stopped based on the negative-log-likelihood (NLL) of the validation set. We then select the model that performs best on the validation set according to the metric at consideration. The ﬁrst two lines of Table 2 compare the performances of the VGG-16 and Bi-directional GRU- RCN encoder. Results clearly show the superiority of the Bi-Directional GRU-RCN Encoder as it outperforms the VGG-16 Encoder on all three metrics. In particular, GRU-RCN Encoder obtains a performance gain of 10% compared to the VGG-16 Encoder according to the BLEU metric. Com- bining our GRU-RCN Encoder that focuses on action with a GoogleNet Encoder that captures visual entities further improve the performances. Our GoogleNet + Bi-directional GRU-RCN approach signiﬁcantly outperforms Soft-attention Yao et al. (2015b) that relies on a GoogLeNet and cuboids-based 3D-CNN Encoder, in conjunction to a similar soft-attention decoder. This result indicates that our approach is able to offer more effective representations. According to the BLEU metric, we also outperform other approaches using more complex decoder schemes such as spatial and temporal attention decoder (Yu et al., 2015) or a hierarchical RNN decoder (Pan et al., 2015) Our approach is on par with Yu et al. (2015), without the need of using a C3D-encoder that requires training on large-scale video dataset.  8  Published as a conference paper at ICLR 2016  6 CONCLUSION  In this work, we address the challenging problem of learning discriminative and abstract representa- tions from videos. We identify and underscore the importance of modeling temporal variation from “visual percepts” at different spatial resolutions. While high-level percepts contain highly discrimi- native information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model ﬁner motion patterns. We introduce a novel recurrent convolutional network architecture that leverages convolutional maps, from all lev- els of a deep convolutional network trained on the ImageNet dataset, to take advantage of “percepts” from different spatial resolutions. We have empirically validated our approach on the Human Action Recognition and Video Caption- ing tasks using the UCF-101 and YouTube2Text datasets. Experiments demonstrate that leveraging “percepts” at multiple resolutions to model temporal variation improve over our baseline model, with respective gain of 3.4% and 10% for the action recognition and video captions tasks using RGB inputs. In particular, we achieve results comparable to state-of-art on YouTube2Text using a simpler text-decoder model and without extra 3D CNN features.  ACKNOWLEDGMENTS  The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Qu´ebec, Compute Canada, the Canada Research Chairs and CIFAR. We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) , for developing such a powerful tool for scientiﬁc computing.  REFERENCES Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and  translate. arXiv preprint arXiv:1409.0473, 2014.  Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Bengio, Y., Simard, P., and Frasconi, P. Learning long-term dependencies with gradient descent is  difﬁcult. Neural Networks, IEEE Transactions on, 1994.  Bergstra, James and Bengio, Yoshua. Random search for hyper-parameter optimization. JMLR,  2012.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), 2010.  Brox, T. and Malik, J. Large displacement optical ﬂow: descriptor matching in variational motion  estimation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2011.  Chen, David L and Dolan, William B. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu- man Language Technologies-Volume 1, pp. 190–200. Association for Computational Linguistics, 2011.  Chen, Xinlei, Fang, Hao, Lin, Tsung-Yi, Vedantam, Ramakrishna, Gupta, Saurabh, Dollar, Piotr, and Zitnick, C Lawrence. Microsoft coco captions: Data collection and evaluation server. arXiv 1504.00325, 2015.  Cho, K., Van Merri¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.  9  Published as a conference paper at ICLR 2016  Chung, Junyoung, Gulcehre, Caglar, Cho, KyungHyun, and Bengio, Yoshua. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.  Denkowski, Michael and Lavie, Alon. Meteor universal: Language speciﬁc translation evaluation  for any target language. In EACL Workshop, 2014.  Donahue, J., Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., and Dar- rell, T. Long-term recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389, 2014.  Graves, A. and Jaitly, N. Towards end-to-end speech recognition with recurrent neural networks. In  ICML, 2014.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 1997.  Jiang, YG, Liu, J, Roshan Zamir, A, Toderici, G, Laptev, I, Shah, M, and Sukthankar, R. Thumos  challenge: Action recognition with a large number of classes. Technical Report, 2014.  Karpathy, Andrej, Toderici, George, Shetty, Sachin, Leung, Tommy, Sukthankar, Rahul, and Fei-Fei,  Li. Large-scale video classiﬁcation with convolutional neural networks. In CVPR. IEEE, 2014.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980, 2014.  Lan, Zhenzhong, Lin, Ming, Li, Xuanchong, Hauptmann, Alexander G, and Raj, Bhiksha. Be- arXiv preprint  yond gaussian pyramid: Multi-skip feature stacking for action recognition. arXiv:1411.6660, 2014.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document  recognition. Proceedings of the IEEE, 1998.  Ng, Joe Yue-Hei, Hausknecht, Matthew, Vijayanarasimhan, Sudheendra, Vinyals, Oriol, Monga, Rajat, and Toderici, George. Beyond short snippets: Deep networks for video classiﬁcation. arXiv preprint arXiv:1503.08909, 2015.  Pan, Pingbo, Xu, Zhongwen, Yang, Yi, Wu, Fei, and Zhuang, Yueting. Hierarchical recurrent neural encoder for video representation with application to captioning. arXiv preprint arXiv:1511.03476, 2015.  Papineni, Kishore, Roukos, Salim, Ward, Todd, and Zhu, Wei-Jing. Bleu: a method for automatic  evaluation of machine translation. In ACL, 2002.  Sadanand, S. and Corso, J. Action bank: A high-level representation of activity in video. In CVPR.  IEEE, 2012.  Shi, Xingjian, Chen, Zhourong, Wang, Hao, Yeung, Dit-Yan, Wong, Wai-Kin, and Woo, Wang- chun. Convolutional lstm network: A machine learning approach for precipitation nowcasting. arXiv preprint arXiv:1506.04214, 2015.  Simonyan, Karen and Zisserman, Andrew. Two-stream convolutional networks for action recogni-  tion in videos. In Advances in Neural Information Processing Systems, pp. 568–576, 2014a.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. arXiv preprint arXiv:1409.1556, 2014b.  Soomro, Khurram, Zamir, Amir Roshan, and Shah, Mubarak. Ucf101: A dataset of 101 human  actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.  Srivastava, N., Mansimov, E., and Salakhutdinov, R. Unsupervised learning of video representations  using lstms. In ICML, 2015.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.  10  Published as a conference paper at ICLR 2016  Thomason, Jesse, Venugopalan, Subhashini, Guadarrama, Sergio, Saenko, Kate, and Mooney, Ray- mond. Integrating language and vision to generate natural language descriptions of videos in the wild. In COLING, 2014.  Tran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri, M. C3d: generic features for video  analysis. arXiv preprint arXiv:1412.0767, 2014.  Vedantam, Ramakrishna, Zitnick, C Lawrence, and Parikh, Devi. CIDEr: Consensus-based image  description evaluation. arXiv:1411.5726, 2014.  Venugopalan, Subhashini, Xu, Huijuan, Donahue, Jeff, Rohrbach, Marcus, Mooney, Raymond, and Saenko, Kate. Translating videos to natural language using deep recurrent neural networks. NAACL, 2015.  Wang, H., Kl¨aser, A., Schmid, C., and Liu, C. Action recognition by dense trajectories. In CVPR.  IEEE, 2011.  Wang, Limin, Qiao, Yu, and Tang, Xiaoou. Action recognition with trajectory-pooled deep-  convolutional descriptors. arXiv preprint arXiv:1505.04868, 2015a.  Wang, Limin, Xiong, Yuanjun, Wang, Zhe, and Qiao, Yu. Towards good practices for very deep  two-stream convnets. arXiv preprint arXiv:1507.02159, 2015b.  Yao, Li, Ballas, Nicolas, Cho, Kyunghyun, Smith, John R., and Bengio, Yoshua. Trainable perfor-  mance upper bounds for image and video captioning. arXiv preprint arXiv:1511.0459, 2015a.  Yao, Li, Torabi, Atousa, Cho, Kyunghyun, Ballas, Nicolas, Pal, Christopher, Larochelle, Hugo, and Courville, Aaron. Describing videos by exploiting temporal structure. In Computer Vision (ICCV), 2015 IEEE International Conference on. IEEE, 2015b.  Yu, Haonan, Wang, Jiang, Huang, Zhiheng, Yang, Yi, and Xu, Wei. Video paragraph captioning  using hierarchical recurrent neural networks. arXiv 1510.07712, 2015.  Zeiler, Matthew D. ADADELTA: an adaptive learning rate method. Technical report, 2012.  11  ",
1511.04561,2016,8-Bit Approximations for Parallelism in Deep Learning,['8-Bit Approximations for Parallelism in Deep Learning\nTim Dettmers'],https://arxiv.org/pdf/1511.04561,"6 1 0 2     b e F 9 1         ] E N . s c [      4 v 1 6 5 4 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  8-BIT APPROXIMATIONS FOR PARALLELISM IN DEEP LEARNING  Tim Dettmers The Faculty of Informatics Universi`a della Svizzera italiana Via Giuseppe Bufﬁ 13, CH-6904 Lugano, Switzerland tim.dettmers@gmail.com  ABSTRACT  The creation of practical deep learning data-products often requires paralleliza- tion across processors and computers to make deep learning feasible on large data sets, but bottlenecks in communication bandwidth make it difﬁcult to attain good speedups through parallelism. Here we develop and test 8-bit approximation al- gorithms which make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not decrease predictive performance on MNIST, CIFAR10, and ImageNet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism. We build a predictive model for speedups based on our experimental data, verify its validity on known speedup data, and show that we can obtain a speedup of 50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism. Thus 8-bit approximation is an efﬁcient method to parallelize convolutional networks on very large systems of GPUs.  1  INTRODUCTION  Deep learning is a ﬁeld inherently driven by advances in computational processing (Schmidhuber, 2015). Graphics processing units (GPUs) can accelerate deep learning by a factor of up to 10- 20 compared to a CPU, and these speedups were integral in achieving breakthroughs in speech recognition and computer vision (Ciresan et al., 2012; Dahl et al., 2012; Krizhevsky et al., 2012). After these breakthroughs, GPUs found widespread use and many teams sought to accelerate the traininfple GPUs or computers (Chilimbi et al., 2014; Coates et al., 2013; Dean et al., 2012; Wu et al., 2015). To make deep learning applicable and scalable for large data sets, it is important to develop successful parallel deep learning algorithms. The main difﬁculty in the parallelization of deep learning is the sequential nature of backpropaga- tion, where the parameter updates must be fully completed before the next iteration of stochastic gradient descent can ensue (Rumelhart et al., 1988). This creates an environment where transfer of parameters between GPUs and computers require high bandwidth and low latency for network com- munication. Network communication generally constitutes the major bottleneck in deep learning parallelism. There are two major ways to increase the performance of a parallel deep learning algorithm: (1) Overlap communication and computation in such a way that most of the communication is done while waiting for a computation to ﬁnish; (2) Decrease the number or size of parameters needed to transfer; In this paper we work on (2) and make the following contributions:  • We discuss all important hardware and software bottlenecks in model and data parallel deep  learning algorithms on GPUs and GPU clusters  1  Published as a conference paper at ICLR 2016  • We develop 8-bit gradient approximation data types and by applying them to MNIST, CI- FAR10 and ImageNet show that the error rates remain unchanged despite this approxima- tion • We build a predictive model based on our experimental data and show that it predicts the speedup obtained by Krizhevsky (2014) with a relative error of about 1%. We use this model to show that 8-bit approximation leads to minor speedups in convolutional networks with 4 GPUs, but to a speedup of 50x and more for a system with 96 GPUs compared to up to 23x for 32-bit. • We compare our algorithm with similar work and show that 8-bit approximation is able to circumnavigate problems with large batch sizes for GPU clusters and thus improves convergence rates in convolutional networks • We show that 8-bit approximation sets the state-of-the-art for model parallelism in general  2 BACKGROUND  To understand the properties of a successful parallel deep learning algorithm, it is necessary to understand how the communication between GPUs works and what the bottlenecks for both model and data parallelized deep learning architectures are. First we look at the speciﬁc properties of data and model parallelism, and then we look at general bottlenecks in GPU-to-GPU communication.  2.1 DATA PARALLELISM  In data parallelism, the model is kept constant for all GPUs while each GPU is fed with a different mini-batch. After each pass the gradients are exchanged, i.e. synchronized with each GPU:  forward+backward pass  • How it is done: For fully connected layers, data is split by the sample dimension, e.g. for 4 GPUs, a 1024x784 mini-batch is split into four 256x784 batches; for convolutional layers, the data is split by the sample dimension (better cross-validation error) or by the feature map dimension (decreases memory usage dramatically; slightly worse cross-validation er- ror; makes architecture complicated) • Infrequent synchronization: Parameters are synchronized (averaged) once after each full • Efﬁciency: Data parallelism is efﬁcient when the model has few parameters, e.g.  long short-term memory recurrent neural networks (Hochreiter & Schmidhuber, 1997); or the computational costs per parameter are high, e.g. convolutional layers in convolutional nets • Scaling limitations: Current GPU implementations are optimized for larger matrices, hence data parallelism does not scale indeﬁnitely due to slow matrix operations (especially matrix multiplication) for small mini-batch sizes (< 128 per GPU); convolution implementations that rely on matrix multiplication may suffer from this too; the larger the batch size the slower the convergence to a local minimum which is problematic for large systems • Requires asymptotic accuracy: Good solutions can be found as long as the sequence of  updates converges to the minimum asymptotically (Seide et al., 2014)  2.2 MODEL PARALLELISM  In model parallelism, the data is kept constant for all GPUs while each GPU holds only a part of the full model:  output dimension); pass the same mini-batch through the distributed layer  • How it is done: Distribute the parameters of a layer on multiple GPUs (split by input or • Frequent synchronization: Parameters are synchronized once for every layer; the outputs • Efﬁciency: Model parallelism is efﬁcient when the layer has many parameters, e.g. in fully connected layers (because the parameter matrix is reduced by a factor equal to the number of GPUs)  of the layer are either stacked or added together depending on the matrix operation  2  Published as a conference paper at ICLR 2016  size, the larger the matrix that needs to be synchronized across GPUs  • Scaling limitations: Poor performance for larger mini-batch sizes; the larger the mini-batch • Requires numerical accuracy: Outputs must be precise as small deviation may lead to large errors in later layers; this is similar to the exploding gradient problem (Hochreiter et al., 2001)  2.3 GENERAL BOTTLENECKS  2.3.1 PCIE SWITCHES  PCI Express (PCIe) is built like an ordinary network, where pairs of two PCIe slots share a common switch which can serve one outgoing and incoming connection simultaneously. Thus only a single device in a device-pair can communicate with another device-pair at any given time. This holds for both GPUs and InﬁniBand cards. Thus PCIe switches need to be taken into account to obtain optimal performance in a multi-GPU system.  2.3.2 BANDWIDTH  GPUs within a computer communicate by using the PCIe interface which offers practically about 14 GB/s bandwidth when it contains 2 GPUs and about 7 GB/s bandwidth when a computer contains more than 2 GPUs. GPUs between computers usually communicate by using InﬁniBand network cards which have a practical bandwidth of 3-7GB/s (quad-data-rate (QDR) and fourteen-data-rate cards (FDR), respec- tively). Communication is the main bottleneck in deep learning which can be illustrated with a simple exam- ple: AlexNet is a convolutional network with about 60 million parameters (Krizhevsky et al., 2012); a full forward-backward pass is completed in under 100ms for current generation GPUs1. When im- plementing naive data parallelism with 4 GPUs, we need to synchronize 60 million parameters with the 3 other GPUs. Due to PCIe switches over which GPU pairs have to send their messages, this takes as long as sending 4 messages between 2 unpaired GPUs. Now 60 million 32-bit parameters take up 0.223GB of memory, which at 7GB/s take 32ms for one transfer or 128ms for a full syn- chronization which means that naive data parallelism on four GPUs would be slower than running the network on a single GPU. Thus using more GPUs or computers is not always beneﬁcial. This also demonstrates the need for high communication bandwidth in large-scale deep learning.  2.3.3 LATENCY  PCIe latency between messages is usually in the microsecond regime and does not increase sig- niﬁcantly with message size. Thus PCIe latency is negligible. However for larger systems with multiple computers that make use of InﬁniBand the latency can be a considerable bottleneck, espe- cially for collective communication (one-to-all, all-to-one, all-to-all) on clusters (Sur et al., 2005; Singh, 2012). Latency for current InﬁniBand systems (FDR) increases exponentially, and above 512 kilobytes of data it becomes unmanageable for GPU clusters with more than a dozen of nodes (latency > 0.5ms per message). Thus it is imperative to keep messages relatively small or otherwise performance is crippled considerably. Latency is the biggest bottleneck in deep learning systems for large-scale GPU clusters.  2.4 OPTIMAL PARALLELISM FOR CONVOLUTIONAL NETS  The currently best known method to parallelize convolutional nets on any number of K GPUs or computers is to use data parallelism in the convolutional layers and model parallelism in the fully connected layers (Krizhevsky, 2014). However, we only perform a single model parallel step, but K such steps with a Kth of the full batch size. So after the convolutional layers – that is before the fully connected layers – 1/Kth of each batch (from hereon sub-batch) is distributed across all GPUs and then a model-parallel forward and backwards pass up to the convolutional layers is performed. During each partial forward-backward pass, the next sub-batches are distributed across all GPUs,  1https://github.com/soumith/convnet-benchmarks  3  Published as a conference paper at ICLR 2016  thus hiding the communication time for all incoming sub-batches under the forward-pass computa- tion time. The same procedure can be applied to synchronize the fully connected activities during model parallelism, which thus hides almost all necessary communication. During this procedure, the gradients for the ﬁrst partial forward-backward passes may be used to update the fully connected layers directly rather than waiting for the other K mini-batches for an average gradient. These multiple updates improve the time to convergence and can be used to hide further communication. This whole process is repeated K times until all K sub-batches have completed a full pass up to the convolutional layers. After this model parallelism step in the fully connected layers, normal data parallelism ensues.  3  8-BIT APPROXIMATION  We seek to develop an approximation for gradients which is small yet has sufﬁcient accuracy to be usable for both data and model parallelism. The standard size of gradients in deep learning is currently 32-bit, which is the smallest practical dimension for ﬂoating point numbers on GPUs, as CUDA only supports 32 and 64-bit ﬂoating point arithmetic. We chose to use 8-bit for our gradient approximation data type because (1) it is easy to handle as we can store it in 8-bit unsigned chars, and (2) we reasoned that less than 8 bits would have insufﬁcient accuracy for model parallelism, since existing literature suggested that less than 8 bits can induce considerable reduction in accuracy (Courbariaux et al., 2014).  3.1 DESIGNING 8-BIT DATA TYPES  From these 8 bits, one bit is reserved for the sign of the number, while the rest can be used for exponent and mantissa. The problem with the mantissa is that our range of precision is very limited. With a 3-bit exponent the mantissa will hold values between 0 and 15 and as such decimal values over 15 will have a poor approximation and thus a large error. For example the numbers ending in 2 to 2.499 will be approximated by numbers ending in 2, yielding an average relative error of 22.5% in this range. In order to decrease this error, we can use the bits of the mantissa to represent a binary tree with interval (0.1, 1) which is bisected according to the route taken through the tree; the children thus represent the start and end points for intervals in a bisection method. With this method we can cover a broader range of numbers with the mantissa and can thus reduce the average relative error.  We can decrease this error further with another method: We can use additional bits from the exponent and introduce a dynamic exponent instead. This dynamic exponent may use between 0 to 7 bits, where the number n of leading 0 bits represents the exponent 10−n; the ﬁrst bit which is set to 1 is a ﬂag which indicates that the next bits are part of the binary bisection tree. With this format we lose the ability to represent large exponents (a maximum of 10−6 instead of 10−7) and we lose one bit for the binary bisection tree, but we gain the ability to approximate numbers with large absolute value with smaller error (e.g. 0.2345678 approximated as 0.236719 instead of 0.23125 or 0.2, respectively), while retaining the ability to approximate numbers with small absolute value that have few signiﬁcant digits (0.0000234 approximated as 0.000019). However, one downside is that our approximation of values below 10−3 loses some accuracy because the zeros (e.g. 2 zeros and 1 ﬂag bit) contain less information than the equivalent bits. However, gradients and activations with absolute size greater 10−3 are arguably more important for learning than gradients and activations below 10−3 because they simply have a larger effect. Thus this data type should yield better training and predictive performance for deep learning algorithms. To increase the accuracy for data and model parallelism respectively, we can introduce ﬁtting offsets for the exponent. Because model parallelism often induces larger activation function values which have high variance (especially for piecewise-linear functions), an exponent offset of 102 to 104 is desirable (exact value depends mainly on the used nonlinear activation function). For the data type with dynamic exponent, we can instead normalize a matrix by dividing by its ab- solute maximum value and thus transform all numbers to be in the range [1, 0] which is then suitable  4  Published as a conference paper at ICLR 2016  for the bisection method; upon decompression to a 32-bit ﬂoat we just multiply each approximated value by the absolute maximum value to renormalize it. Using this method with a 7-bit bisection tree with the range [0,1], we receive a data type which is equivalent to linear quantization (Vanhoucke et al., 2011).  Figure 1: Anatomy of the four different 8-bit data types. Note that the dynamic data type shown here is a speciﬁc example and the number of bits for the exponent and for the tree varies between individual ﬂoating point numbers.  3.2  IMPLEMENTATION AND COMPUTATIONAL PERFORMANCE  The fastest implementation for 8-bit compression and decompression we could think of is to use a binary search on a sorted table of all positive 128 values in shared GPU memory and keep track of the sign of the respective number. Shared memory is about 100 times faster than global GPU memory and thus a binary search in shared memory is very fast. In our implementation we used one thread per number in a binary search. Additional parallelization is easily possible by dividing the table into n intervals, where n is the number of threads per number. However, the necessary thread synchronization is expensive and performance gains are probably negligible compared to the additional resource costs (threads). For decompression, the 32-bit values are read into shared memory and we lookup the 32-bit value for the respective 8-bit value. Here we use one thread per number/lookup. On average, these algorithms perform compression and decompression in 1 and 0.5 nanoseconds per number, respectively, as measured on a NVIDIA GTX Titan. Implementations of our 8-bit approximation algorithms are available online 2.  3.3 THEORETICAL SPEEDUP  Table 1: Predicted speedups for AlexNet for 4 and 96 GPUs.  GPUs  Sub-batch size  4 96 96 96 96 96 96  128 128 256 512 1024 2056 12288  Speedup  32-bit 3.53x 13.4x 20x 23x 14.8x 11.3x 1.3x  8-bit 3.67x 13.8x 24.4x 39.3x 48.6x 50.6x 9.7x  We measured the average total transfer time (compression, transfer, and decompression) for our techniques and compared them to 32-bit transfers between GPUs. We measured this time on a board  2https://github.com/TimDettmers/clusterNet/; contact me if you need help with integrating the functions  into your library  5  Published as a conference paper at ICLR 2016  equipped with 4 GPUs which yields 8 PCIe 3.0 lanes for each GPU and thus a theoretical band- width of about 8GB/s; however, bandwidth for small messages is usually considerably lower. The algorithms were run on two NVIDIA GTX Titans. Each matrix was transfered 100 times and the average total transfer time was measured. We used the message passing interface (MPI) implemen- tation provided by OpenMPI 1.8.5, which uses low level CUDA routines to enable GPU-to-GPU communication without the help of the CPU. MPI is commonly used to parallelize algorithms on GPU clusters. We then used these measurements to build a predictive model for speedup for different hardware conﬁgurations. We validated our model by predicting the speedup for Krizhevsky (2014) which we could predict with a relative error of about 1%. We also created a theoretical model for GPU clusters based on publicly available benchmark data on InﬁniBand systems. The predictive model was then used to generate the theoretical speedups in Table 1. For more information on the predictive model see the Appendix where the model is derived. From Table 1 we can see that the sub-batch scheme is essential to obtaining good speedups since we get almost no speedups if we use the full batch size of 12288. We also see that 8-bit approximation increases the speedup considerably as the sub-batch size gets larger. While speedups are larger for larger sub-batches, we will have slower convergence as shown by Krizhevsky (2014), which is simi- lar to how a large batch size for any model increases the time to convergence. So this scheme yields good scaling, yet overcomes problems with large batch sizes, as is typical for full data parallelism approaches such as 1-bit quantization which is discussed further in section 4.2. Furthermore, since no other approximation method is known to us which is usable with model parallelism, we conclude that 8-bit approximation is the currently best method to speed up commu- nication for model parallelism.  3.4 APPROXIMATION ERROR  We tested the approximation error of our data types on multiple distributions and on the gradients (data parallelism) and activations (model parallelism) on MNIST (see Table 2). We calculated the mean absolute and relative error from a sample of size 25 million numbers drawn from normal distributions N (mean, variance), and the uniform distribution U (0, 1). The approximation of N (0, 102) was done by using an exponent offset of 102 while other numbers used an exponent offset of 101. For the dynamic tree and linear quantization the sample was normalized by division by the maximum absolute value and then denormalized after compression.  the 8-bit dynamic tree provides the overall best performance  As one can see from Table 2, for approximation numbers for random distributions and for parallelism on MNIST. For our tests on MNIST we used rectiﬁed linear units, a 784x1024x1024x10 architecture with dropout (0.2,0.3,0.3), a learning rate of 0.003 and RMSProp (Tieleman & Hinton, 2012). Usu- ally we have 8-bit approximation for all incoming GPUs and 32-bit gradients for the local GPU. Since we only had one GPU available for the following experiments, we simulated training on a large GPU cluster by only using the pure 8-bit approximation gradient component by training on a single GPU – so no 32-bit gradients or activations where used. On MNIST, we found that the best test error of all four approximation techniques static tree, dynamic tree, linear quantization, and mantissa did not differ signiﬁcantly from the test error of 32-bit training for both data parallelism F (4, 4) = 0.71, p = 0.59, and model parallelism F (4, 4) = 0.54, p = 0.71 (F-test assumptions were satisﬁed); also the 99% conﬁdence intervals did overlap for all techniques. Experiments with logistic units revealed the same results. This indicates that on MNIST, 8-bit ap- proximation does not degrade classiﬁcation performance compared to 32-bit, and all 8-bit approxi- mation techniques are similar in performance on MNIST for both model and data parallelism. We also tested our data types on CIFAR10, where we used a convolutional network3 with two convolutional layers (64x5x5, 64x3x3) which were followed by max-pooling (3x3) and contrast normalization after each layer. These layers were followed by two locally connected convolutional layers (no weight sharing) and a ﬁnal fully connected softmax layer.  3http://code.google.com/p/cuda-convnet/wiki/Methodology  6  Published as a conference paper at ICLR 2016  Table 2: Approximation errors on different data sets, and for different layers on MNIST.  Distribution  Data type  Mean absolute error Mean relative error in %  U (0, 1)  - - -  N (0, 1)  - - -  N (0, 102)  - - -  N (0, 0.22)  - - -  MNIST model parallel  - - -  MNIST data parallel  - - -  8-bit dynamic tree 8-bit linear quant  8-bit mantissa 8-bit static tree  8-bit dynamic tree 8-bit linear quant  8-bit mantissa 8-bit static tree  8-bit dynamic tree 8-bit linear quant  8-bit mantissa 8-bit static tree  8-bit dynamic tree 8-bit linear quant  8-bit mantissa 8-bit static tree  8-bit dynamic tree 8-bit linear quant  8-bit mantissa 8-bit static tree  8-bit dynamic tree 8-bit linear quant  8-bit mantissa 8-bit static tree  0.00004 0.0024 0.0017 0.0007 0.0005 0.0004 0.0104 0.0093 0.049 0.041 1.066 0.926  0.000018 0.000015 0.00055 0.00022  0.025 / 2 / 3321 0.02 / 2.6 / 3387 1 / 70 / 265000  0.45 / 50 / 187000 0.0005 / 0.004 / 0.95 0.001 / 0.01 / 1.25  0.02 / 0.1 / 55 0.01 / 0.04 / 24  1.39 2.16 7.22 4.82 2.46 6.47 6.89 6.3 2.49 6.44 7.19 6.28 2.45 6.15 8.26 6.58 0.5/2/2 1/2/3 1.5/2/8 1/2/6 3/4/5 5/6/6  4.5/4.5/6  4/4/5  We used both data parallelism (convolutional layers) and model parallelism (local layers) for this convolutional net and we found that test errors for all 8-bit data types and 32-bit training only differed by at most 2% relative to each other, indicating that 8-bit approximation did not decrease performance. We also applied the 8-bit dynamic tree data type to AlexNet on the ImageNet dataset. We used our approximation scheme for both model (fully connected layers) and data parallelism (convolution). Figure 2 shows that the 8-bit dynamic tree data type does not increase the misclassiﬁcation error on the train or test set for convolutional nets. The ﬁnal performance on the test set was comparable to the 32-bit model: 18.65% and 18.55% Top5-test-error for the 8-bit and 32-bit model, respectively.  4 COMPARISON TO OTHER METHODS  4.1 OTHER SUB-32-BIT DATA TYPES  Dynamic ﬁxed point data types are data types which use all their bits for the mantissa and have a dynamic exponent which is kept for collection of numbers (matrix, vector) and is adjusted during run-time. Courbariaux et al. (2014) used dynamic ﬁxed point data types with 10-bit width for com- putation and 12-bit width for parameter updates to train a maxout convolutional network end-to-end. Their results on PI MNIST, MNIST, and CIFAR10 are about 20% worse relative to the state of the art obtained by Goodfellow et al. (2013). In our work we show that we can use 8-bit gradients for the parameter updates without degrading performance. However, dynamic ﬁxed point data types can also be used for end-to-end training and as such a combination of both methods might yield optimal performance.  7  Published as a conference paper at ICLR 2016  Figure 2: Classiﬁcation train and test error for the 8-bit dynamic data type used in AlexNet on the ImageNet dataset.  Vanhoucke et al. (2011) used linear quantization, which is equivalent to our normalized data type with full binary tree and no exponent, in ﬁxed point computation. They show that this data type can be used to achieve signiﬁcant speedups on CPUs for a speech recognition task. Although our 8-bit data type with dynamic binary tree achieves better approximation, it cannot be used in ﬁxed point computation and thus remains useful solely as an intermediate approximate representation. Gupta et al. (2015) used a 16-bit ﬁxed point data type for end-to-end training of convolutional neural networks and showed that stochastic rounding improves the results signiﬁcantly. We did not test stochastic rounding for our 8-bit data types, but it might also improve performance and thus allow the use of less than 8 bits for the approximation. This is left to explore for future research.  4.2  1-BIT QUANTIZATION  Another useful technique for data parallelism is 1-bit quantization which was introduced by Seide et al. (2014): In 1-bit quantization each 32-bit number of the gradient is quantized to a single bit by a quantization function. This quantization function maintains a cumulative quantization error which is used by the quantization function to smoothen out the error over time. The immediate error in quantization is too high to produce stable and accurate forward passes for model parallelism, but in data parallelism 1-bit quantization will converge to a local minimum seamlessly over time. Compared to 8-bit approximation, 1-bit quantization performs well on medium sized systems that run fully connected architectures such as large fully connected networks for speech recognition (Strom, 2015; Seide et al., 2014). For convolutional layers, 1-bit quantization has no advantage over 32 or 8 bits, as communication can be hidden under backward convolution operations even with 32-bit gradients and very large GPU clusters (see Appendix, section 5.2.2 for a worked example). Although no published example is known to us, 1-bit quantization should work ﬂawlessly in the fully connected layers of convolutional networks. However, one problem with 1-bit quantization for large systems is the batch size which increases rapidly with the number of GPUs and slows down convergence (Seide et al., 2014; Strom, 2015; Krizhevsky, 2014). To mitigate this problem Seide et al. (2014) use adaptive batch size selection which determines the best batch size during runtime to improve convergence. Our 8-bit approximation scheme for convolutional networks does not suffer from this, as we use sub-batches in our model parallel pass, thus keeping the batch size small even for large GPU clusters (see Appendix, section 5.2).  8  Published as a conference paper at ICLR 2016  Further improvement of 1-bit quantization comes from Strom (2015) who only transfers ﬂoating point values of the gradient, which have an absolute value greater than a certain value τ. Values that lie outside this interval are accumulated as residual gradients (similarly to the cumulative quantiza- tion error in 1-bit quantization) and applied over time. For sparse gradients, this procedure increases the compression factor considerably and thus decreases the time needed for communication. The problem of slow convergence with very large batch sizes should remain for very large systems, as observed by Krizhevsky (2014). However Strom (2015) observed improvement in convergence rates with larger batch sizes but does not offer an explanation for this effect.  CONCLUSION  Here we have shown that approximation of 32-bit ﬂoating point numbers with 8 bits can speed up communication in parallel training of deep learning architectures considerably for large GPU clusters while retaining predictive performance. We have shown that the dynamic tree data type is able to approximate random numbers better than other known data types, but that during training all approximation techniques seem to perform equally well. We also showed that model parallelism in convolutional networks in conjunction with sub-batches works very well even for large GPU clusters and avoids the problem of large batch sizes that constitute problems for techniques like 1- bit quantization. Since no other approximation technique has been demonstrated for the compression of model parallel activations we set the state-of-the-art for model parallelism. We expect that further important advances in parallel computing for deep learning will come from new hardware (3D GPU memory, EDR InﬁniBand adoption, hardware based InﬁniBand multicast) and new algorithms which maintain the performance of backpropagation while providing qualities which make them easier to parallelize.  REFERENCES Chilimbi, Trishul, Suzue, Yutaka, Apacible, Johnson, and Kalyanaraman, Karthik. Project adam: Building an efﬁcient and scalable deep learning training system. In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14), pp. 571–582, 2014.  Ciresan, Dan, Meier, Ueli, and Schmidhuber, J¨urgen. Multi-column deep neural networks for image classiﬁcation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 3642–3649. IEEE, 2012.  Coates, Adam, Huval, Brody, Wang, Tao, Wu, David, Catanzaro, Bryan, and Andrew, Ng. Deep learning with cots hpc systems. In Proceedings of the 30th international conference on machine learning, pp. 1337–1345, 2013.  Courbariaux, Matthieu, Bengio, Yoshua, and David, Jean-Pierre. Low precision arithmetic for deep  learning. arXiv preprint arXiv:1412.7024, 2014.  Dahl, George E, Yu, Dong, Deng, Li, and Acero, Alex. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42, 2012.  Dean, Jeffrey, Corrado, Greg, Monga, Rajat, Chen, Kai, Devin, Matthieu, Mao, Mark, Senior, An- In  drew, Tucker, Paul, Yang, Ke, Le, Quoc V, et al. Large scale distributed deep networks. Advances in Neural Information Processing Systems, pp. 1223–1231, 2012.  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.  Maxout networks. arXiv preprint arXiv:1302.4389, 2013.  Gupta, Suyog, Agrawal, Ankur, Gopalakrishnan, Kailash, and Narayanan, Pritish. Deep learning  with limited numerical precision. arXiv preprint arXiv:1502.02551, 2015.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Hochreiter, Sepp, Bengio, Yoshua, Frasconi, Paolo, and Schmidhuber, J¨urgen. Gradient ﬂow in  recurrent nets: the difﬁculty of learning long-term dependencies, 2001.  9  Published as a conference paper at ICLR 2016  Krizhevsky, Alex. One weird trick for parallelizing convolutional neural networks. arXiv preprint  arXiv:1404.5997, 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning representations by  back-propagating errors. Cognitive modeling, 5:3, 1988.  Schmidhuber, J¨urgen. Deep learning in neural networks: An overview. Neural Networks, 61:85–  117, 2015.  Seide, Frank, Fu, Hao, Droppo, Jasha, Li, Gang, and Yu, Dong. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.  Singh, Ashish Kumar. Optimizing All-to-All and Allgather Communications on GPGPU Clusters.  PhD thesis, The Ohio State University, 2012.  Strom, Nikko. Scalable distributed dnn training using commodity gpu cloud computing. In Sixteenth  Annual Conference of the International Speech Communication Association, 2015.  Sur, Sayantan, Bondhugula, Uday Kumar Reddy, Mamidala, Amith, Jin, H-W, and Panda, Dha- baleswar K. High performance rdma based all-to-all broadcast for inﬁniband clusters. In High Performance Computing–HiPC 2005, pp. 148–157. Springer, 2005.  Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012.  Vanhoucke, Vincent, Senior, Andrew, and Mao, Mark Z. Improving the speed of neural networks on cpus. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, volume 1, 2011.  Wu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and Sun, Gang. Deep image: Scaling up image  recognition. arXiv preprint arXiv:1501.02876, 2015.  10  Published as a conference paper at ICLR 2016  APPENDIX  5 THEORETICAL MODELS FOR PARALLELISM  5.1 A THEORETICAL SINGLE NODE MODEL FOR 4 GPUS  5.1.1 BENCHMARKING AN ARCHITECTURE FOR SINGLE NODE 4 GPU PARALLELISM  We used NervanaGPU4 benchmarks to generate baseline and parallel time data for the AlexNet architecture as implemented in Krizhevsky (2014). NervanaGPU requires a Maxwell GPU or newer, but other deep learning libraries5 exist with which benchmarking can also be easily handled. We benchmarked the timing of convolutional kernels, pooling operations, and matrix multiplications in fully connected layers for both model parallelism (output size divided by 4) and no parallelism. See Table 3 and Table 4 for the generated benchmark data. Note that the timings of the pooling operations are added to the convolutional timings; the pooling operations are in the order of 5-10% of the timings of the respective preceding convolution. The transfer size is the size of the sub- gradient (one-fourth of the gradient, because we have 4 GPUs) or the size of the activation in model parallelism (batch size × output units / 4). The sync time is the time needed to transfer the buffer to all other GPUs (time equivalent to 4 messages needed due to PCIe switches). We benchmarked PCIe express transfer rates for different transfer buffer sizes and found that for most buffers we achieve a bandwidth of about 5GB/s (for smaller buffers this is less, for larger a bit more; the bandwidth saturates at 5.25GB/s for 4 GPUs on a Sandy Bridge Intel CPU). The total time is the time estimated by NervanaGPU benchmarks for the entire convolutional network and this number serves as the baseline from which we calculate speedup values. See my github repository6 for further data.  Table 3: Communication benchmark data for AlexNet.  Layer/activation  Transfer size (MB)  conv11, stride 4, 3→64  conv5, 64→192 conv3, 192→384 conv3, 384→256 conv3, 256→256 conv 3 activation fc 9216→3072 fc 3072→3072 fc 3072→1000  0.045 0.29 0.63 0.85 0.56 1.125 1.5 1.5 0.5  Sync time (ms) 32-bit 8-bit 0.05 0.45  – – – – – 0.4 0.5 0.5 0.25  1 1.3 0.9 0.9 1.1 1.1 0.4  5.1.2 ANALYSIS OF SINGLE NODE 4 GPU PARALLELISM  With these numbers we can easily calculate the theoretical speedup for AlexNet and with the same reasoning demonstrated in this section we are able to calculate the theoretical speedup for any section. The ﬁrst thing we can see is that we can completely hide the data parallel communication in the convolutional layers under computation, that is, while we are calculating the gradient for the next convolutional layer we can synchronize the gradients of the previous layer before the computation ﬁnishes (sync time < weight update time for any convolutional layer). Only the synchronization of the last layer cannot be overlapped with computation, which means that the synchronization time for this layer is the only cost we pay for the data parallelism scheme in convolutional layers. As we can see, neither 8-bit nor 1-bit compression will yield any speedups in this case and thus we  4https://github.com/NervanaSystems/nervanagpu 5https://github.com/soumith/convnet-benchmarks 6https://github.com/TimDettmers/public-data  11  Published as a conference paper at ICLR 2016  Table 4: Computation benchmark data for AlexNet.  Layer/activation  conv11, stride 4, 3→64  conv5, 64→192 conv3, 192→384 conv3, 384→256 conv3, 256→256 conv 3 activation fc 9216→3072 fc 3072→3072 fc 3072→1000 Total fc layers Total all layers  Baseline time (ms)  Fprop / Bprop / Update  Parallel time (ms)  Fprop / Bprop / Update  4 / 1 / 3.5 10 / 11 / 11  5 / 6 / 5  6.5 / 5.5 / 6.5  4 / 4 / 5  –  1.3 / 1.3 / 1.5 0.45 / 0.45 / 0.5 0.4 / 0.4 / 0.2  6.5 104.1  – – – – – –  0.9 / 0.9 / 0.43 0.3 / 0.3 / 0.13 5 0.15 / 0.15 / 0.08  3.34  proceed our analysis to the model parallel part.  In the model parallel part for AlexNet we follow the scheme introduced in section 2.4, that is we split the convolutional activities in K parts, where K is the number of GPUs, and transfer these sub-batches while we do the forward and backward passes for the previous sub-batches. More speciﬁcally, while we compute the matrix multiplication of the ﬁrst sub-batch, we can transfer the next sub-batch. Similarly we transfer the fully connected activities (the ﬁrst true model par- allel step) while we calculate the fully connected activities on another sub-batch. We can do this procedure in every layer and thus hide the communication time of the previous layer under the com- putation time of the current layer. This works for both the forward and backward pass. We can hide the communication of the error activities to the convolutional layers by synchronizing them while we are updating the weights. We can do this except for the ﬁrst batch of convolutional activities and the last batches of error activities (nothing to overlap with) which incurs a total penalty of 5× 0.9ms for 32-bit. Thus for 32-bit we have a total synchronization penalty of: (0.9−0.9)+(1.1−0.3)+(1.1−0.15)+(0.4−0.3)+(1.1−0.9)+(1.1−0.64)+5×0.9 = 6.81ms For 8-bit transfers we only have a penalty for some transfers because they mostly overlap fully with matrix multiplication.  (0.5 − 0.3) + (0.5 − 0.15) + 5 × 0.4 = 2.55ms  At the same time we receive a speedup because we are operating on smaller matrices due to model parallelism (see Table 4 for details) which saves us 3.16ms for 32-bit matrix multiplications. From this we can see that model parallelism yields only tiny speedups compared to the model parallel stage. However, we still see some speedups, especially when we use 8-bit approximation.  5.1.3 PREDICTIONS OF THE SINGLE NODE 4 GPU MODEL  Putting everything together we now can calculate the theoretical speedup. We use full data paral- lelism in the convolutional layers and model parallelism of K = 4 sub-batches through the fully connected layers. In data parallel layers we need the same time as we need for 1 GPU but without the fully connected part which has K = 4 model parallel passes. On top of this we add all penalties for the model parallel stage and we thus receive the expression:  speedup =  (total time − fc time) + conv penalty + (#sub-batches × parallel fc time) + fc penalty  Number of GPUs × total time  And this yields in our example:  speedup 32 =  (104.1 − 6.5) + 0.05 + (4 × 3.34) + 6.81  416.4  (cid:39) 3.53  12  Published as a conference paper at ICLR 2016  speedup 8 =  (104.1 − 6.5) + 0.05 + (4 × 3.34) + 2.55  416.4  (cid:39) 3.67  Here the estimated total time of 104.1ms for a full pass is based on convolutional kernels which are about 75% faster than those used by Krizhevsky (2014). If we use the total time estimate for the kernels used by Krizhevsky (2014)7 the model predicts a speedup of:  speedup 32 =  speedup 8 =  (177 − 6.5) + 0.05 + (4 × 3.34) + 6.81  (177 − 6.5) + 0.05 + (4 × 3.34) + 2.55  177 × 4  177 × 4  (cid:39) 3.71  (cid:39) 3.8  Which is very close, within 1% to the actual speedup of 3.66 achieved by Krizhevsky (2014).  Our model does not take into account the additional time needed to stack or add the buffers from model parallelism which is typically in the order of 10% of the time for the respective matrix mul- tiplication. Also we used a different GPU (GTX Titan X). Yet our theoretical model nevertheless predicts the underlying speedup accurately, which should attest to the robustness of our model which we will now extend to a GPU cluster case.  5.2 A THEORETICAL GPU CLUSTER MODEL FOR 32 NODES, 96 GPUS  5.2.1 BENCHMARKING COMPONENTS IN A 32 NODE, 96 GPU CLUSTER  The sub-components of a GPU cluster are similar to a single machine, apart from the fact that we now also have a network with switches between computers. The two bottlenecks in network communication are network bandwidth and network latency. InﬁniBand network interfaces usually have very good network bandwidth and network latency and our analysis will be based on a FDR InﬁniBand system used in conjunction with MPI software for communication between nodes in the cluster. Benchmarks for InﬁniBand systems using MPI are readily available online. Table 5 shows the latency for messages in our model-parallel scheme. To ﬁnd the InﬁniBand latencies we looked at charts that show the latency for a given message size. The message size in this case is the sub- gradient (parameters/32) or the convolutional activities (depends on sub-batch size). From this data we also found that most messages will have a bandwidth of 6GB/s for a FDR interconnect. We use this bandwidth estimate in all our following calculations.  Table 5: Benchmark data for the model parallelism step in AlexNet for a GPU cluster with 32 nodes and 96 GPUs.  Layer/activation  Sub-batch size  Forward passes  conv3, 384→256 conv 3 activation conv 3 activation conv 3 activation conv 3 activation conv 3 activation conv 3 activation fc 9216→3072 fc 9216→3072  – 128 256 512 1024 2048 12288 128 256  1 96 48 24 12 6 1 1 1  Size (kB)  32-bit 108 144 288 576 1152 2312 13872  24 48  8-bit 13.5 18 36 72 144 289 1734  3 6  InﬁniBand latency (ms/msg) 32-bit 0.03 0.035 0.07 0.1 0.3 0.7 5(?) 0.008 0.015  8-bit 0.008 0.007 0.01 0.02 0.04 0.07 0.06 0.006 0.006  5.2.2 ANALYSIS AND SPEEDUP PREDICTION OF 32 NODE 96 GPU PARALLELISM  From Table 5 we can calculate the time needed to transfer the largest convolutional layer during data parallelism. For 2 PCIe transfers of a sub-gradient and subsequent 31 InﬁniBand transfers  7see ”convnet2” here https://github.com/soumith/convnet-benchmarks  13  Published as a conference paper at ICLR 2016  we have two messages of 36 kilobytes at 5GB/s and 31 messages of 108 kilobytes at 6GB/s with 0.03ms latency per message, respectively. We do this messaging scheme twice: Once to distribute the raw gradients, twice so we distribute the accumulated gradient to all nodes. The total time for this gradient synchronization scheme is about 1.9ms. This shows, as in the 4 GPU case, that there is no bottleneck in the data parallelism part of convolutional layers and thus 8-bit or 1-bit quantization will not improve performance in these layers.  To look at model parallelism, we ﬁrst need to benchmark the matrix multiplications for a model parallel pass. We generated new matrix multiplication benchmarks for this case but found that model parallelism in fully connected layers will induce a slowdown because the compute-time for the matrix multiplication (0.1 / 0.08 / 0.08 ) are too small while the latencies alone will destroy performance: 32 × 0.008 = 0.256ms and 32 × 0.015 = 0.48ms, for a sub-batch size of 128 or 256, respectively. Different model parallel schemes with speedups are possible, but these are too complex to analyze. Instead we discard model parallelism in these layers and we concentrate on the analysis of the transfer of the convolutional activities to the fully connected layer. However, we still use our sub-batch scheme from section 2.4 in this case.  Table 6: Benchmark data for synchronization of the convolutional activities in AlexNet for a GPU cluster with 32 nodes and 96 GPUs.  Sub-batch size  Forward time  128 256 512 1024 2048 12288  624 312 156 78 39 6.5  Sync time (ms)  Single pass 8-bit 32-bit 0.35 2.1 0.55 4.2 1.13 7.15 17.4 2.25 3.26 25.5 250(?) 31  Full pass /w overlap 32-bit 24.8 89.9 181 499 750  8-bit 0.35 0.55 1.13 29.5 60.8 921  7750(?)  Total time (ms)  32-bit 650 402 337 577 789  7750(?)  8-bit 624 312 157 108 100 928  Table 6 shows the timings for the forward passes for different sub-batch sizes. Along with synchro- nization time this is the main bottleneck in the computation. Table 6 also shows the timings for a single pass of convolutional activities for both 32 and 8 bit and the timing for the full pass (all convolutional activities) where subsequent transfers are hidden under the matrix multiplication of the subsequent layer (1.3ms). The total time is the time of both the forward passes and the synchro- nization time for the full pass. With these data we receive the predictions for speedups shown in Table 7. Note that AlexNet based on NervanaGPU kernels and convnet2 kernels has a baseline of 96 × 104.1 = 10s and 96 × 177 = 17s milliseconds, respectively.  Table 7: Predicted speedups for AlexNet for a GPU cluster with 32 nodes and 96 GPUs using different convolutional kernels.  Sub-batch size  128 256 512 1024 2056 12288  Speedup  NervanaGPU kernels  convnet2 kernels  Baseline 10s 8-bit 13.8x 24.4x 39.3x 48.6x 50.6x 9.7x  32-bit 13.4 20x 23x 14.8x 11.3x 1.3x  Baseline 17s 8-bit 21.4x 35.2x 51.9x 61x 62.8x 15.5x  32-bit 20.7x 29.7x 33.5 22.7x 17.7x 2.15x  14  ",
1511.06856,2016,Data-dependent initializations of Convolutional Neural Networks,"['Data-dependent initializations of Convolutional Neural Networks [code]\nPhilipp Kraehenbuehl', 'Carl Doersch', 'Jeff Donahue', 'Trevor Darrell']",https://arxiv.org/pdf/1511.06856,"6 1 0 2     p e S 2 2         ]  V C . s c [      3 v 6 5 8 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  DATA-DEPENDENT INITIALIZATIONS OF CONVOLUTIONAL NEURAL NETWORKS  Philipp Kr¨ahenb¨uhl1, Carl Doersch1,2, Jeff Donahue1, Trevor Darrell1 1Department of Electrical Engineering and Computer Science, UC Berkeley 2Machine Learning Department, Carnegie Mellon {philkr,jdonahue,trevor}@eecs.berkeley.edu; cdoersch@cs.cmu.edu  ABSTRACT  Convolutional Neural Networks spread through computer vision like a wildﬁre, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of Im- ageNet pre-trained models, and ﬁne-tunes or adapts these for speciﬁc tasks. This is in large part due to the difﬁculty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or explod- ing gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classiﬁcation and object detection, while reducing the pre-training time by three orders of magnitude. When combined with pre-training methods, our initialization signiﬁcantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.  1  INTRODUCTION  In recent years, Convolutional Neural Networks (CNNs) have improved performance across a wide variety of computer vision tasks (Szegedy et al., 2015; Simonyan & Zisserman, 2015; Girshick, 2015). Much of this improvement stems from the ability of CNNs to use large datasets better than previous methods. In fact, good performance seems to require large datasets: the best-performing methods usually begin by “pre-training” CNNs to solve the million-image ImageNet classiﬁcation challenge (Russakovsky et al., 2015). This “pre-trained” representation is then “ﬁne-tuned” on a smaller dataset where the target labels may be more expensive to obtain. These ﬁne-tuning datasets generally do not fully constrain the CNN learning: different initializations can be trained until they achieve equally high training-set performance, but they will often perform very differently at test time. For example, initialization via ImageNet pre-training is known to produce a better-performing network at test time across many problems. However, little else is known about which other factors affect a CNN’s generalization performance when trained on small datasets. There is a pressing need to understand these factors, ﬁrst because we can potentially exploit them to improve performance on tasks where few labels are available. Second they may already be confounding our attempts to evaluate pre-training methods. A pre-trained network which extracts useful semantic information but cannot be ﬁne-tuned for spurious reasons can be easily overlooked. Hence, this work aims to explore how to better ﬁne-tune CNNs. We show that simple statistical properties of the network, which can be easily measured using training data, can have a signiﬁcant impact on test time performance. Surprisingly, we show that controlling for these statistical properties leads to a fast and general way to improve performance when training on relatively little data. Empirical evaluations have found that when transferring deep features across tasks, freezing weights of some layers during ﬁne-tuning generally harms performance (Yosinski et al., 2014). These results suggest that, given a small dataset, it is better to adjust all of the layers a little rather than to adjust just a few layers a large amount, and so perhaps the ideal setting will adjust all of the layers the  Code available: https://github.com/philkr/magic_init  1  Published as a conference paper at ICLR 2016  same amount. While these studies did indeed set the learning rate to be the same for all layers, somewhat counterintuitively this does not actually enforce that all layers learn at the same rate. To see this, say we have a network where there are two convolution layers separated by a ReLU. Multiplying the weights and bias term of the ﬁrst layer by a scalar α > 0, and then dividing the weights (but not bias) of the next (higher) layer by the same constant α will result in a network which computes exactly the same function. However, note that the gradients of the two layers are not the same: they will be divided by α for the ﬁrst layer, and multiplied by α for the second. Worse, an update of a given magnitude will have a smaller effect on the lower layer than the higher layer, simply because the lower layer’s norm is now larger. Using this kind of reparameterization, it is easy to make the gradients for certain layers vanish during ﬁne-tuning, or even to make them explode, resulting in a network that is impossible to ﬁne-tune despite representing exactly the same function. Conversely, this sort of re-parameterization gives us a tool we can use to calibrate layer-by-layer learning to improve ﬁne-tuning performance, provided we have an appropriate principle for making such adjustments. Where can we look to ﬁnd such a principle? A number of works have already suggested that statisti- cal properties of network activations can impact network performance. Many focus on initializations which control the variance of network activations. Krizhevsky et al. (2012) carefully designed their architecture to ensure gradients neither vanish nor explode. However, this is no longer possible for deeper architectures such as VGG (Simonyan & Zisserman, 2015) or GoogLeNet (Szegedy et al., 2015). Glorot & Bengio (2010); Saxe et al. (2013); Sussillo & Abbot (2015); He et al. (2015); Bradley (2010) show that properly scaled random initialization can deal with the vanishing gradi- ent problem, if the architectures are limited to linear transformations, followed by a very speciﬁc non-linearities. Saxe et al. (2013) focus on linear networks, Glorot & Bengio (2010) derive an initial- ization for networks with tanh non-linearities, while He et al. (2015) focus on the more commonly used ReLUs. However, none of the above papers consider more general network including pooling, dropout, LRN layers (Krizhevsky et al., 2012), or DAG-structured networks (Szegedy et al., 2015). We argue that initializing the network with real training data improves these approximations and achieves a better performance. Early approaches to data-driven initializations showed that whiten- ing the activations at all layers can mitigate the vanishing gradient problem (LeCun et al., 1998), but it does not ensure all layers train at an equal rate. More recently, batch normalization (Ioffe & Szegedy, 2015) enforces that the output of each convolution and fully-connected layer are zero mean with unit variance for every batch. In practice, however, this means that the network’s behavior on a single example depends on the other members of the batch, and removing this dependency at test-time relies on approximating batch statistics. The fact that these methods show improved con- vergence speed at training time suggests we are justiﬁed in investigating the statistics of activations. However, the main goal of our work differs in two important respects. First, these previous works pay relatively little attention to the behavior on smaller training sets, instead focusing on training speed. Second, while all above initializations require a random initialization, our approach aims to handle structured initialization, and even improve pre-trained networks.  2 PRELIMINARIES  We are interested in parameterizing (and re-parameterizing) CNNs, where the output is a highly non-convex function of both the inputs and the parameters. Hence, we begin with some notation which will let us describe how a CNN’s behavior will change as we alter the parameters. We focus on feed-forward networks of the form  zk = fk(zk−1; θk),  where zk is a vector of hidden activations of the network, and fk is a transformation with parameters θk. fk may be a linear transformation fk(z(cid:48) k; θk) = Wkzk−1 + bk, or it may be a non-linearity fk+1(zk; θk) = σk+1(z(cid:48) k) such as a rectiﬁed linear unit (ReLU) σ(x) = max(x, 0). Other common non-linearities include local response normalization or pooling (Krizhevsky et al., 2012; Szegedy et al., 2015; Simonyan & Zisserman, 2015). However, as is common in neural networks, we assume these nonlinearities are not parametrized and kept ﬁxed during training. Hence, θk contains only (Wk, bk) for each afﬁne layer k. To deal with spatially-structured inputs like images, most hidden activations zk ∈ RCk×Ak×Bk are arranged in a two dimensional grid of size Ak × Bk (for image width Ak and height Bk) with Ck  2  Published as a conference paper at ICLR 2016  channels per grid cell. We let z0 denote the input image. The ﬁnal output, however, is generally not spatial, and so later layers are reduced to the form zN = RCN×1×1, where CN is the number of output units. The last of these outputs is converted into a loss with respect to some label; for classiﬁcation, the approach is to convert the ﬁnal output into a probability distribution over labels via a Softmax function. Learning aims to minimize the expected loss over the training dataset. Despite the non-convexity of this learning problem, backpropagation and Stochastic Gradient Descent often ﬁnds good local minima if initialized properly (LeCun et al., 1998). Given an arbitrary neural network, we next aim for a good parameterization. A good parameteriza- tion should be able to learn all weights of a network equally well. We measure how well a certain weight in the network learns by how much the gradient of a loss function would change it. A large change means it learns more quickly, while a small change implies it learns more slowly. We initial- ize our network such that all weights in all layers learn equally fast.  3 DATA-DEPENDENT INITIALIZATION  Given an N-layer neural network with loss function (cid:96)(zN ), we ﬁrst deﬁne C 2 norm of the gradient with respect to weights Wk(i, j) in layer k:  i,j,k to be the expected  (cid:34)(cid:18)  (cid:19)2(cid:35)  (cid:20)(cid:18)  k,i,j = Ez0∼D C 2  ∂  ∂Wk(i, j)  (cid:96)(zN )  = Ez0∼D  zk−1(j)  (cid:96)(zN )  ,  (1)  (cid:19)2(cid:21)  (cid:124)  ∂zk(i)  ∂  (cid:123)(cid:122)  yk(i)  (cid:125)  where D is a set of input images and yk is the backpropagated error. Similar reasoning can be applied to the biases bk, but where the activations are replaced by the constant 1. To not rely on any labels during initialization, we use a random linear loss function (cid:96)(zN ) = η(cid:62)zN , where η ∼ N (0, I) is sampled from a unit Gaussian distribution. In other words, we initialize the top gradient to a random Gaussian noise vector η during backpropagation. We sample a different random loss η for each image. In order for all parameters to learn at the same “rate,” we require the change in eq. 1 to be propor- tional to the magnitude of the weights (cid:107)Wk(cid:107)2  2 of the current layer; i.e.,  ˜C 2  k,i,j =  k,i,j  C 2 (cid:107)Wk(cid:107)2  2  (2)  is constant for all weights. However this is hard to enforce, because for non-linear networks the backpropagated error yk is a function of the activations zk−1. A change in weights that affects the activations zk−1 will indirectly change yk. This effect is often non-linear and hard to control or predict. We thus simplify Equation (2): rather than enforce that the individual weights all learn at the same rate, we enforce that the columns of weight matrix Wk do so, i.e.:  (cid:2)zk−1(j)2(cid:107)yk(cid:107)2  (cid:3) ,  2  (3)  ˜C 2  k,j =  1 N  ˜C 2  k,i,j =  1  N(cid:107)Wk(cid:107)2  2  Ez0∼D  (cid:88)  i  should be approximately constant, where N is the number of rows of the weight matrix. As we will show in Section 4.1, all weights tend to train at roughly the same rate even though the objective does not enforce this. Looking at Equation (3), the relative change of a column of the weight matrix is a function of 1) the magnitude of a single activation of the bottom layer, and 2) the norm of the backpropagated gradient. The value of a single input to a layer will generally have a relatively small impact on the norm of the gradient to the entire layer. Hence, we assume zk−1(j) and (cid:107)yk(cid:107) are independent, leading to the following simpliﬁcation of the objective:  (cid:2)zk−1(j)2(cid:3) Ez0∼D  (cid:2)(cid:107)yk(cid:107)2  2  (cid:3)  N(cid:107)Wk(cid:107)2  2  k,j ≈ Ez0∼D ˜C 2  .  (4)  This approximation conveniently decouples the change rate per column, which depends on zk−1(j)2, from the global change rate per layer, which depends on the gradient magnitude (cid:107)yk(cid:107)2 2, allowing us to correct them in two separate steps.  3  Published as a conference paper at ICLR 2016  Algorithm 1 Within-layer initialization.  for each afﬁne layer k do  Initialize weights from a zero-mean Gaussian Wk ∼ N (0, I) and biases bk = 0 Draw samples z0 ∈ ˜D ⊂ D and pass them through the ﬁrst k layers of the network Compute the per-channel sample mean ˆµk(i) and variance ˆσk(i)2 of zk(i) Rescale the weights by Wk(i, :) ← Wk(i, :)/ˆσk(i) Set the bias bk(i) ← β − ˆµk(i)/ˆσk(i) to center activations around β  end for  In Section 3.1, we show how to satisfy Ez0∼D tion 3.2, we then adjust this layer-wise constant ck to ensure that all gradients are properly calibrated between layers, in a way that can be applied to pre-initialized networks. Finally, in Section 3.3 we present multiple data-driven weight initializations.  (cid:2)zk−1(i)2(cid:3) = ck for a layer-wise constant ck. In Sec-  3.1 WITHIN-LAYER WEIGHT NORMALIZATION  (cid:2)(zk(i, a, b) − β)2(cid:3) = 1 simply via properly-scaled random projections, where a  We aim to ensure that each channel that a layer k + 1 receives a similarly distributed input. It is straightforward to initialize weights in afﬁne layers such that the units have outputs following similar distributions. E.g., we could enforce that layer k activations zk(i, a, b) have Ez0∼D,a,b [zk(i, a, b)] = β and Ez0∼D,a,b and b index over the 2D spatial extent of the feature map. However, we next have to contend with the nonlinearity σ(.). Thankfully, most nonlinearities (such as sigmoid or ReLU) operate independently on different channels. Hence, the different channels will undergo the same transformation, and the output channels will follow the same distribution if the input channels do (though the outputs will generally not be the same distribution as the inputs). In fact, most common CNN layers that apply a homogeneous operation to uniformly-sized windows of the input with regular stride, such as local response normalization, and pooling, empirically preserve this identical distribution requirement as well, making it broadly applicable. We normalize the network activations using empirical estimates of activation statistics obtained In particular, for each afﬁne layer k ∈ {1, 2, . . . , N} in a from actual data samples z0 ∼ D. topological ordering of the network graph, we compute the empirical mean and standard deviations for all outgoing activations and normalize the weights Wk such that all activations have unit variance and mean β. This procedure is summarized in Algorithm 1. The variance of our estimate of the sample statistics falls with the size of the sample | ˜D|. In practice, for CNN initialization, we ﬁnd that on the order of just dozens of samples is typically sufﬁcient. Note that this simple empirical initialization strategy guarantees afﬁne layer activations with a par- ticular center and scale while making no assumptions (beyond non-zero variance) about the inputs to the layer, making it robust to any exotic choice of non-linearity or other intermediate operation. This is in contrast with existing approaches designed for particular non-linearities and with archi- tectural constraints. Extending these methods to handle operations for which they weren’t designed while maintaining the desired scaling properties may be possible, but it would at least require careful thought, while our simple empirical initialization strategy generalizes to any operations and DAG architecture with no additional implementation effort. On the other hand, note that for architectures which are not purely feed-forward, the assumption of identically distributed afﬁne layer inputs may not hold. GoogLeNet (Szegedy et al., 2015), for example, concatenates layers which are computed via different operations on the same input, and hence may not be identically distributed, before feeding the result into a convolution. Our method cannot guarantee identically distributed inputs for arbitrary DAG-structured networks, so it should be applied to non-feed-forward networks with care.  3.2 BETWEEN-LAYER SCALE ADJUSTMENT  Because the initialization given in Section 3.1 results in activations zk(i) with unit variance, the expected change rate C 2 k,i of a column i of the weight matrix Wk is constant across all columns i,  4  Published as a conference paper at ICLR 2016  Algorithm 2 Between-layer normalization.  Compute the ratio ˜Ck = Ej  Draw samples z0 ∈ ˜D ⊂ D repeat  (cid:104) ˜Ck,j (cid:105) Compute the average ratio ˜C = ((cid:81) (cid:16) ˜Ck/ ˜C  k Ck)1/N  (cid:17)α/2  Compute a scale correction rk = Correct the weights and biases of layer k: bk ← rkbk, Wk ← rkWk Undo the scaling rk in the layer above until Convergence (roughly 10 iterations)  with a damping factor α < 1  under the approximation given in Equation (4). However, this does not provide any guarantee of the scaling of the change rates between layers. We use an iterative procedure to obtain roughly constant parameter change rates C 2 k,i across all layers k (as well as all columns i within a layer), given previously-initialized weights. At each iteration we estimate the average change ratio ( ˜Ck,i,j) per layer. We also estimate a global change ratio, as the geometric mean of all layer-wise change ratios. The geometric mean ensures that the output remains unchanged in completely homogeneous networks. We then scale the parameters for each layer to be closer to this global change ratio. We simultaneously undo this scaling in the layer above, such that the function that the entire network computes is unchanged. This scaling can be undone by inserting an auxiliary scaling layer after each afﬁne layer. However for homogeneous non-linearities, such as ReLU, Pooling or LRN, this scaling can be undone at in the next afﬁne layer without the need of a special scaling layer. The between-layer scale adjustment procedure is summarized in Algorithm 2. Adjusting the scale of all layers simultaneously can lead to an oscillatory behavior. To prevent this we add a small damping factor α (usually α = 0.25). With a relatively small number of steps (we use 10), this procedure results in roughly constant initial change rates of the parameters in all layers of the network, regardless of its depth.  3.3 WEIGHT INITIALIZATIONS  Until now, we used a random Gaussian initialization of the weights, but our procedure does not require this. Hence, we explored two data-driven initializations: a PCA-based initialization and a k-means based initialization. For the PCA-based initialization, we set the weights such that the layer outputs are white and decorrelated. For each layer k we record the features activations zk−1 of each channel c across all spatial locations for all images in D. Then then use the ﬁrst M principal components of those activations as our weight matrix Wk. For the k-means based initialization, we follow Coates & Ng (2012) and apply spherical k-means on whitened feature activations. We use the cluster centers of k-means as initial weights for our layers, such that each output unit corresponds to one centroid of k-means. k-means usually does a better job than PCA, as it captures the modes of the input data, instead of merely decorrelating it. We use both k-means and PCA on just the convo- lutional layers of the architecture, as we don’t have enough data to estimate the required number of weights for fully connected layers. In summary, we initialize weights or all ﬁlters (§ 3.3), then normalize those weights such that all activations are equally distributed (§ 3.1), and ﬁnally rescale each layer such that the gradient ratio is constant across layers (§ 3.2). This initialization encures that all weights learn at approximately the same rate, leading to a better convergence and more accurate models, as we will show next.  4 EVALUATION  We implement our initialization and all experiments in the open-source deep learning framework Caffe (Jia et al., 2014). To assess how easily a network can be ﬁne-tuned with limited data, we use the classiﬁcation and detection challenges in PASCAL VOC 2007 (Everingham et al., 2014), which contains 5011 images for training and 4952 for testing.  5  Published as a conference paper at ICLR 2016  Gaussian Gaussian (ours) K-Means  Gaussian (caffe) ImageNet K-Means (ours)  103  102  101  e t a r  e g n a h c  e g a r e v a  1 v n o c  2 v n o c  3 v n o c  4 v n o c  5 v n o c  6 c f  7 c f  8 c f  30  20  10  n o i t a i r a v  f o  t n e i c ﬁ f e o c  0  1 v n o c  2 v n o c  3 v n o c  4 v n o c  5 v n o c  6 c f  7 c f  8 c f  (a) average change rate  (b) coefﬁcient of variation  Figure 1: Visualization of the relative change rate ˜Ck,i,j in CaffeNet for various initializations esti- mated on 100 images. (a) shows the average change rate per layer, a ﬂat curve is better, as all layers learn at the same rate. (b) shows the coefﬁcient of variation for the change rate within each layer, lower is better as weights within a layer train more uniformly.  Architectures Most of our experiments are performed on the 8 layer CaffeNet architecture a small modiﬁcation of AlexNet (Krizhevsky et al., 2012). We use the default architecture for all com- parisons, except for Doersch et al. (2015) which removed groups in the convolutional layers. We also show results on the much deeper GoogLeNet (Szegedy et al., 2015) and VGG (Simonyan & Zisserman, 2015) architectures.  Image classiﬁcation The VOC image classiﬁcation task is to predict the presence or absence of each of 20 object classes in an image. For this task we ﬁne-tune all networks using a sigmoid cross- entropy loss on random crops of each image. We optimize each network via Stochastic Gradient Descent (SGD) for 80,000 iterations with an initial learning rate of 0.001 (dropped by 0.5 every 10,000 iterations), batch size of 10, and momentum of 0.9. The total training takes one hour on a Titan X GPU for CaffeNet. We tried different settings for various methods, but found these setting to work best for all initializations. At test time we average 10 random crops of the image to determine the presence or absence of an object. The CNN estimates the likelihood that each object is present, which we use as a score to compute a precision-recall curve per class. We evaluate all algorithms using mean average precision (mAP) (Everingham et al., 2014).  Object detection In addition to predicting the presence of absence of an object in a scene, object detection requires the precise localization of each object using a bounding box. We again eval- uate mean average precision (Everingham et al., 2014). We ﬁne-tune all our models using Fast R-CNN (Girshick, 2015). For a fair comparison we varied the parameters of the ﬁne-tuning for each of the different initializations. We tried three different learning rates (0.01, 0.002 and 0.001) dropped by 0.1 every 50,000 iterations, with a total of 150,000 training iterations. We used multi- scale training and ﬁne-tuned all layers. We evaluate all models on single scale. All other settings were kept at their default values. Training and evaluation took roughly 8 hours in a Titan X GPU for CaffeNet. All models are trained from scratch unless otherwise stated. For both experiments we use 160 images of the VOC2007 training set for our initialization. 160 images are sufﬁcient to robustly estimate activation statistics, as each unit usually sees tens of thou- sands of activations throughout all spacial locations in an images. At the same time, this relatively small set of images keeps the computational cost low.  4.1 SCALING AND LEARNING ALGORITHMS  We begin our evaluation by measuring and comparing the relative change rate ˜Ck,i,j of all weights in the network (see Equation (2)) for different initializations. We estimate ˜Ck,i,j using 100 images of the VOC 2007 validation set. We compare our models to an ImageNet pretrained model, ini- tialized with random Gaussian weights (with standard deviation σ = 0.01), an unscaled k-means initialization, as well as the Gaussian initialization in Caffe (Jia et al., 2014), for which biases and standard deviations were handpicked per layer. Figure 1a visualizes the average change rate per layer. Our initialization, as well as the ImageNet pretrained model, have similar change rates for all layers (i.e., all layers learn at the same rate), while random initializations and k-means have a  6  Published as a conference paper at ICLR 2016  drastically different change rates. Figure 1b measures the coefﬁcient of variation of the change rate for each layer, deﬁned as the standard deviation of the change rate, divided by their mean value. Our coefﬁcient of variation is low throughout all layers, despite scaling the rate of change of columns of the weight matrix, instead of individual elements. Note that the low values are mirrored in the hand-tuned Caffe initialization. Next we explore how those different initializations perform on the VOC 2007 classiﬁcation task, as shown in Table 1. We train both a random Gaussian and k-means initialization using different initial scalings. Without scaling the random Gaussian initialization fares quite well, however the k-means initialization does poorly, due to the worse initial change rate as shown in Figure 1. Correcting for the within-layer scaling alone does not improve the performance much, as it worsens the between- layer scaling for both initializations. However in combination with the between-layer adjustment both initializations perform very well. Both the between-layer and within-layer scaling could potentially be addressed by a stronger second order optimization method, such as ADAM (Kingma & Ba, 2015) or batch normalization (Ioffe & Szegedy, 2015). In general, ADAM is able to slightly improve on SGD for an unscaled initializa- tion, especially when combined with batch normalization. Neither batch-norm nor ADAM alone or combined does perform as well as simple SGD with our k-means initialization. More interestingly, our initialization complements those stronger optimization methods and we see an improvement by combining them with our initialization.  4.2 WEIGHT INITIALIZATION  Next we compare our Gaussian, PCA and k-means based weights, with initializations proposed by Glorot & Bengio (2010) (commonly known as “xavier”), He et al. (2015), and a carefully chosen Gaussian initialization of Jia et al. (2014). We followed the suggestions of He et al. and used their initialization only for the convolutional layers, while choosing a random Gaussian initialization for the fully connected layers. We compare all methods on both classiﬁcation and detection performance in Table 2. The ﬁrst thing to notice is that both Glorot & Bengio and He et al. perform worse than a carefully chosen random Gaussian initialization. One possibility for the drop in performance comes from the additional layers, such as Pooling or LRN used in CaffeNet. Neither Glorot & Bengio nor He et al. consider those layers but rather focus on linear layers followed by tanh or ReLU non- linearities. Our initialization on the other hand has no trouble with those additional layers and substantially improves on the random Gaussian initialization.  4.3 COMPARISON TO UNSUPERVISED PRE-TRAINING  We now compare our simple, properly scaled initializations to the state-of-the-art unsupervised pre- training methods on VOC 2007 classiﬁcation and detection. Table 3 shows a summary of the results, including the amount of pre-training time, as well as the type of supervision used. Agrawal et al. (2015) uses egomotion, as measured by a moving car in a city to pre-train a model. While this information is not always readily available, it can be read from sensors and is thus “free.” We believe egomotion information does not often correlate with the kind of semantic information that is required for classiﬁcation or detection, and hence the egomotion pretrained model performs worse than our random baseline. Wang & Gupta (2015) supervise their pre-training using relative motion  SGD  ADAM  SGD + BN  k-mns. Gaus.  k-mns. Gaus.  ADAM + BN Gaus. k-mns. Scaling 50.8% 41.2% 51.6% 49.4% 50.9% 52.0% 55.7% 53.8% no scaling 47.6% 41.2% 53.2% 53.1% Within-layer (Ours) 52.7% 55.7% 54.5% 57.2% Between-layer (Ours) 53.3% 56.6% 56.6% 60.0% 53.1% 56.9% 56.9% 59.8% Both (Ours) Table 1: Classiﬁcation performance of various initializations, training algorithms and with and with- out batch normalization (BN) on PASCAL VOC2007 for both random Gaussian (Gaus.) and k- means (k-mns.) initialized weights.  k-mns. Gaus.  - -  - -  - -  - -  7  Published as a conference paper at ICLR 2016  Method Xavier Glorot & Bengio (2010) MSRA He et al. (2015) Random Gaussian (hand tuned) Ours (Random Gaussian) Ours (PCA) Ours (k-means)  Classiﬁcation Detection  51.1% 43.3% 53.4% 53.3% 52.8% 56.6%  40.4% 37.2% 41.3% 43.4% 43.1% 45.6%  Table 2: Comparison of different initialization methods on PASCAL VOC2007 classiﬁcation and detection.  of objects in pre-selected youtube videos, as obtained by a tracker. Their model is generally quite well scaled and trains well for both classiﬁcation and detection. Doersch et al. (2015) predict the relative arrangement of image patches to pre-train a model. Their model is trained the longest with 4 weeks of training. It does well on detection, but lags behind other methods in classiﬁcation. Interestingly our k-means initialization is able to keep up with most unsupervised pre-training meth- ods, despite containing very little semantic information. To analyze what information is actually captured, we sampled 100 random ImageNet images and found nearest neighbors for them from a pool of 50,000 other random ImageNet images, using the high-level feature spaces from differ- ent methods. Figure 2 shows the results. Overall, different unsupervised methods seem to focus on different attributes for matching. For example, ours appears to have some texture and material information, whereas the method of Doersch et al. (2015) seems to preserve more speciﬁc shape information. As a ﬁnal experiment we reinitialize all unsupervised pre-training methods to be properly scaled and compare with our initializations which use no auxiliary training beyond the proposed initializations. In particular, we take their pretrained network weights and apply the between-layer adjustment de- scribed in Section 3.2. (We do not perform local scaling as we ﬁnd that the activations in these mod- els are already scaled reasonably well locally.) The bottom three rows of Table 3 give our results for our rescaled versions of these models on the VOC classiﬁcation and detection tasks. We ﬁnd that for two of the three models (Agrawal et al., 2015; Doersch et al., 2015) this rescaling improves results signiﬁcantly; our rescaling of Wang & Gupta (2015) on the other hand does not improve its perfor- mance, indicating it was likely relatively well-scaled globally to begin with. The best-performing method with auxiliary self-supervision using our rescaled features is that of Doersch et al. (2015) – in this case our rescaling improves its results on the classiﬁcation task by a relative margin of 18%. This suggests that our method nicely complements existing unsupervised and self-supervised methods and could facilitate easier future exploration of this rich space of methods.  4.4 DIFFERENT ARCHITECTURES  Finally we compare our initialization across different architectures, again using PASCAL 2007 clas- siﬁcation and detection. We train both the deep architecture of Szegedy et al. (2015) and Simonyan & Zisserman (2015) using our k-means and Gaussian initializations. Unlike prior work we are able  motion  unsupervised  Supervision egomotion  Pretraining time Classiﬁcation Detection  Method Agrawal et al. (2015) 41.8% 47.4% Wang & Gupta (2015)2 Doersch et al. (2015) 46.6% 56.8% Krizhevsky et al. (2012) Ours (k-means) 45.6% Ours + Agrawal et al. (2015) 43.9% Ours + Wang & Gupta (2015) 47.2% 51.1% Ours + Doersch et al. (2015) Table 3: Comparison of classiﬁcation and detection results on the PASCAL VOC2007 test set. 2an earlier version of this paper reported 58.4% and 44.0% for the color model of Wand & Gupta, this  52.9% 62.8% 55.3% 78.2% 56.6% 54.2% 63.1% 65.3%  54 seconds 10 hours 1 week 4 weeks  10 hours 1 week 4 weeks 3 days  initialization egomotion  1000 class labels  unsupervised  motion  version uses the grayscale model which performs better.  8  Published as a conference paper at ICLR 2016  to train those models without any intermediate losses or stage-wise supervised pre-training. We simply add a sigmoid cross-entropy loss to the top of both networks. Unfortunately neither network outperformed CaffeNet in the classiﬁcation tasks. GoogLeNet achieves a 50.0% and 55.0% mAP for the two initializations respectively, while 16-layer VGG performs as 53.8% and 56.5%. This might have to do with the limited amount of supervised training data available to the model at during train- ing. The training time was 4 and 12 times slower than CaffeNet, which made them prohibitively slow for detection.  4.5  IMAGENET TRAINING  Finally, we test our data-dependent initializations on two well-known CNN architectures which have been successfully applied to the ImageNet LSVRC 1000-way classiﬁcation task: CaffeNet (Jia et al., 2014) and GoogLeNet (Szegedy et al., 2015). We initialize the 1000-way classiﬁcation layers to 0 in these experiments (except in our reproductions of the reference models), as we ﬁnd this improves the initial learning velocity.  CaffeNet We train instances of CaffeNet using our initializations, with the architecture and all other hyperparameters set to those used to train the reference model: learning rate 0.01 (dropped by a factor of 0.1 every 105 iterations), momentum 0.9, and batch size 256. We also train a variant of the architecture with no local response normalization (LRN) layers. Our CaffeNet training results are presented in Figure 3. Over the ﬁrst 100,000 iterations (Figure 3, middle row), and particularly over the ﬁrst 10,000 (Figure 3, top row), our initializations reduce the network’s classiﬁcation error on both the training and validation sets at a much faster rate than the reference initialization. With the full 320,000 training iterations, all initializations achieve similar accuracy on the training and validation sets; however, in these experiments the carefully chosen reference initialization pulled non-trivially ahead of our initializations’ error after the second learning rate drop to a rate of 10−4. We do not yet know why this occurs, or whether the difference is signiﬁcant. Over the ﬁrst 100,000 iterations, among models initialized using our method, the k-means initializa- tion reduces the loss slightly faster than the random initialization. Interestingly, the model variant without LRN layers seems to learn just as quickly as the directly comparable network with LRNs, suggesting such normalizations may not be necessary given a well-chosen initialization.  GoogLeNet We apply our best-performing initialization from the CaffeNet experiments—k- means—to a deeper network, GoogLeNet (Szegedy et al., 2015). We use the SGD hyperparam- eters from the Caffe (Jia et al., 2014) GoogleNet implementation (speciﬁcally, the “quick” version which is trained for 2.4 million iterations), and also retrain our own instance of the model with the initialization used in the reference model (based on Glorot & Bengio (2010)). Due to the depth of the architecture (22 layers, compared to CaffeNet’s 8) and the difﬁculty of prop- agating gradient signal to the early layers of the network, GoogLeNet includes additional “auxiliary classiﬁers” branching off from intermediate layers of the network to amplify the gradient signal to learn these early layers. To verify that networks initialized using our proposed method should have no problem backpropagating appropriately scaled gradients through all layers of arbitrarily deep networks, we also train a variant of GoogLeNet which omits the two intermediate loss towers, otherwise keeping the rest of the architecture ﬁxed. Our GoogLeNet training results are presented in Figure 4. We plot only the loss of the ﬁnal clas- siﬁer for comparability with the single-classiﬁer model. The models initialized with our method learn much faster than the model using the reference initialization stategy. Furthermore, the model trained using only a single classiﬁer learns at roughly the same rate as the original three loss tower architecture, and each iteration of training in the single classiﬁer model is slightly faster due to the removal of layers to compute the additional losses. This result suggests that our initialization could signiﬁcantly ease exploration of new, deeper CNN architectures, bypassing the need for architectural tweaks like the intermediate losses used to train GoogLeNet.  9  Published as a conference paper at ICLR 2016  5 DISCUSSION  Our method is a conceptually simple data-dependent initialization strategy for CNNs which en- forces empirically identically distributed activations locally (within a layer), and roughly uniform global scaling of weight gradients across all layers of arbitrarily deep networks. Our experiments (Section 4) demonstrate that this rescaling of weights results in substantially improved CNN repre- sentations for tasks with limited labeled data (as in the PASCAL VOC classiﬁcation and detection training sets), improves representations learned by existing self-supervised and unsupervised meth- ods, and substantially accelerates the early stages of CNN training on large-scale datasets (e.g., ImageNet). We hope that our initializations will facilitate further advancement in unsupervised and self-supervised learning as well as more efﬁcient exploration of deeper and larger CNN architec- tures.  ACKNOWLEDGEMENTS  The thank Alyosha Efros for his input and encouragement, without his “Gelato bet” most of this work would not have been explored. We thank NVIDIA for their generous GPU donations.  REFERENCES Agrawal, Pulkit, Carreira, Joao, and Malik, Jitendra. Learning to see by moving. ICCV, 2015. 7, 8  Bradley, David M. Learning in modular systems. Technical report, DTIC Document, 2010. 2  Coates, Adam and Ng, Andrew Y. Learning feature representations with k-means. In Neural Net-  works: Tricks of the Trade, pp. 561–580. Springer, 2012. 5  Doersch, Carl, Gupta, Abhinav, and Efros, Alexei A. Unsupervised visual representation learning  by context prediction. ICCV, 2015. 6, 8, 11  Everingham, Mark, Eslami, SM Ali, Van Gool, Luc, Williams, Christopher KI, Winn, John, and Zisserman, Andrew. The Pascal Visual Object Classes challenge: A retrospective. IJCV, 111(1): 98–136, 2014. 5, 6  Girshick, Ross. Fast R-CNN. ICCV, 2015. 1, 6  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural  networks. In AISTATS, pp. 249–256, 2010. 2, 7, 8, 9  He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers: Surpass-  ing human-level performance on ImageNet classiﬁcation. In ICCV, 2015. 2, 7, 8, 12  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by  reducing internal covariate shift. In ICML, 2015. 2, 7  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross B., Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. In ACM Multimedia, MM, 2014. 5, 6, 7, 9, 12  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. ICLR, 2015. 7  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. ImageNet classiﬁcation with deep con-  volutional neural networks. In NIPS, 2012. 2, 6, 8  LeCun, Y., Bottou, L., Orr, G., and Muller, K. Efﬁcient backprop. In Neural Networks: Tricks of  the trade. Springer, 1998. 2, 3  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet large scale visual recognition challenge. IJCV, 2015. 1  10  Published as a conference paper at ICLR 2016  Figure 2: Comparison of nearest neighbors for the given input image (top row) in the feature spaces of CaffeNet-based CNNs initialized using our method, the fully supervised CaffeNet, an untrained CaffeNet using Gaussian initialization, and three unsupervised or self-supervised methods from prior work. (For Doersch et al. (2015) we display neighbors in fc6 feature space; the rest use the fc7 features.) While our initialization is clearly missing the semantics of CaffeNet, it does preserve some non-speciﬁc texture and shape information, which is often enough for meaningful matches.  Saxe, Andrew M, McClelland, James L, and Ganguli, Surya. Exact solutions to the nonlinear dy-  namics of learning in deep linear neural networks. arXiv preprint, 2013. 2  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. ICLR, 2015. 1, 2, 6, 8  Sussillo, David and Abbot, Larry. Random walk initialization for training very deep feedforward  networks. ICLR, 2015. 2  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. CVPR, 2015. 1, 2, 4, 6, 8, 9  Wang, Xiaolong and Gupta, Abhinav. Unsupervised learning of visual representations using videos.  ICCV, 2015. 7, 8  Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod. How transferable are features in  deep neural networks? In NIPS, 2014. 1  11  Agrawal et al. AlexNet Doersch et al. Ours (Kmeans) Random Init Wang et al. Input Published as a conference paper at ICLR 2016  7  6  5  4  3 0K 7  6  5  4  3  2 0K 7  6  5  4  3  2  1 0K  2K  4K  6K  8K  10K  20K  40K  60K  80K  100K  7  6  5  4  0K 7  6  5  4  3  2 0K 7  6  5  4  3  2  2K  4K  6K  8K  10K  20K  40K  60K  80K  100K  Reference MSRA Random (ours) k-means (ours) k-means, no LRN (ours)  50K 100K 150K 200K 250K 300K 350K  0K  50K 100K 150K 200K 250K 300K 350K  (a) Training loss  (b) Validation loss  Figure 3: Training and validation loss curves for the CaffeNet architecture trained for the ILSVRC- 2012 classiﬁcation task. The training error is unsmoothed in the topmost plot (10K); smoothed over one epoch in the others. The validation error is computed over the full validation set every 2000 iterations and is unsmoothed. Our initializations (k-means, Random) handily outperform both the carefully chosen reference initialization (Jia et al., 2014) and the MSRA initialization (He et al., 2015) over the ﬁrst 100,000 iterations, but the other initializations catch up after the second learning rate drop at iteration 200,000.  6  4  2  Reference k-means (ours) k-means, single loss (ours)  6  4  2  0M  0.5M  1M  1.5M (a) Training loss  2M  0M  0.5M  1M  1.5M (b) Validation loss  2M  Figure 4: Training and validation loss curves for the GoogLeNet architecture trained for the ILSVRC-2012 classiﬁcation task. The training error plot is again smoothed over roughly the length of an epoch; the validation error (computed every 4000 iterations) is unsmoothed. Note that our k- means initializations outperform the reference initialization, and the single loss model (lacking the auxiliary classiﬁers) learns at roughly the same rate as the model with auxiliary classiﬁers. The ﬁnal top-5 validation error are 11.57% for the reference model, 10.85% for our single loss, and 10.69% for our auxiliary loss model.  12  ",
1511.06391,2016,Order Matters: Sequence to sequence for sets,"['Order Matters: Sequence to sequence for sets\nOriol Vinyals', 'Samy Bengio', 'Manjunath Kudlur']",https://arxiv.org/pdf/1511.06391,"6 1 0 2     b e F 3 2         ] L M  . t a t s [      4 v 1 9 3 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS  Oriol Vinyals, Samy Bengio, Manjunath Kudlur Google Brain {vinyals, bengio, keveman}@google.com  ABSTRACT  Sequences have become ﬁrst class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require map- ping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to ef- ﬁciently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we ﬁrst show using various examples that the order in which we organize input and/or output data matters signiﬁcantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modiﬁcations to the seq2seq framework on bench- mark language modeling and parsing tasks, as well as two artiﬁcial tasks – sorting numbers and estimating the joint probability of unknown graphical models.  1  INTRODUCTION  Deep architectures have shown in the last few years that they often yield state-of-the-art perfor- mance on several tasks, ranging from image classiﬁcation (Ioffe & Szegedy, 2015) to speech recog- nition (Hinton et al., 2012). More recently, recurrent neural networks (RNNs) and variants such as the Long Short Term Memory network (LSTMs) proposed by Hochreiter & Schmidhuber (1997) have shown similar impressive performance on several inherently sequential tasks. Such examples range from machine translation (Sutskever et al., 2014; Bahdanau et al., 2015a), to image caption- ing (Vinyals et al., 2015c; Mao et al., 2015; Donahue et al., 2015), speech recognition (Chan et al., 2015; Bahdanau et al., 2015b), constituency parsing (Vinyals et al., 2015b) and learning to com- pute (Zaremba & Sutskever, 2014; Vinyals et al., 2015a). These approaches all follow a simple architecture, dubbed sequence-to-sequence (seq2seq), where the input is read completely using an encoder, which is either an LSTM when the input is a sequence, or a convolutional network for images. The ﬁnal state of the encoder is then fed to a decoder LSTM whose purpose is to produce the target sequence, one token at a time. When the data is naturally organized as a sequence, the sequence-to-sequence framework is well suited. For example, the chain rule is used to decompose the joint probability of sequences of words, and can be implemented by an LSTM without making any conditional independence assumption. But how should we represent data, either inputs or outputs, for problems where an obvious order cannot be determined? For instance, how should we encode a set of numbers when the task is to sort them? Alternatively, how should we output a set of detected objects in an image when there is no speciﬁc known order among them? Does the a priori choice of ordering of the data to be presented to the model matter? The purpose of this paper is two-fold. First, we show that even when no natural order is known among input or output objects, there might still be one that yields better performance, hence, order matters. Second, we propose two approaches to consider sets either as inputs and/or outputs in our models and evaluate how they perform on various artiﬁcial and real datasets.  1  Published as a conference paper at ICLR 2016  2 RELATED WORK  Since sequence-to-sequence models were proposed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Kalchbrenner & Blunsom, 2013), the research community has proposed several applications in which these models can perform mappings from and/or to sequences. For example, image captioning maps from an image to a sentence (Vinyals et al., 2015c; Mao et al., 2015; Donahue et al., 2015), parsing maps from a sentence to a (linearized) parse tree (Vinyals et al., 2015b), and models for computation map from problem statements (e.g. a python program or a set of points on the plane) to their solutions (the answer to the program (Zaremba & Sutskever, 2014), or the traveling salesman problem tour for the set of points (Vinyals et al., 2015a)). It is out of the scope of this paper to review all successful applications of seq2seq, but the list above already includes some non-trivial examples of mapping to/from objects that are not necessarily sequences. More recently, many related models and key contributions have been proposed, that utilize the con- cept of external memories, including RNNSearch (Bahdanau et al., 2015a), Memory Networks (We- ston et al., 2015) and Neural Turing Machines (Graves et al., 2014). The key element that these models utilize is a reading (or attention) mechanism to read these external memories in a fully dif- ferentiable way (though there has also been work with discrete reading mechanism, most notably RL-NTM (Zaremba & Sutskever, 2015)). Unlike traditional structured prediction algorithms (Bakir et al., 2007), our approach relies on the chain rule to serialize output random variables through the strong capabilities of LSTM networks to model long-term correlation. Similarly, we do not want to assume a known structured input, as is done for instance with recursive neural networks (Socher et al., 2010) which encode sentences recursively as (given) trees.  3 NEURAL NETWORKS FOR SEQUENCES AND SETS  Let us consider a generic supervised task with a given training set of n pairs (X i, Y i)n i=1 where (X i, Y i) is the ith pair of an input and its corresponding target. The sequence-to-sequence paradigm corresponds to tasks where both X i and Y i are represented by sequences, of possibly different lengths: X i = {xi }. In this case, it is reasonable to model each example using the conditional probability P (Y |X) and to use the chain rule to decompose it as follows (we drop the example index i in the rest of this section for readability):  } and Y i = {yi  2, . . . , xi si  2, . . . , yi ti  1, xi  1, yi  T(cid:89)  P (Y |X) =  P (yt|y1, y2, . . . , yt−1, X)  t=1  and implement it as an encoder recurrent neural network (RNN) to read sequentially each xs ∈ X as follows: (1) where hs is the state of the encoder at time s, followed by a decoder RNN to produce each yt ∈ Y one at a time, given the current state gt and the previous yt−1 symbol:  hs = fenc(hs−1, xs)  g1 = hs gt = fdec(gt−1, yt−1)  P (yt|y1, y2, . . . , yt−1, X) = softmax(afﬁne(gt)) .  (2)  The use of the chain rule makes this approach assumption free, so when the input X corresponds to a sequence (like a sentence), it is reasonable to read it sequentially into an RNN, as in eq. (1). However, how should we encode X if it does not correspond naturally to a sequence? For instance, what if it corresponds to an unordered set of elements? Similarly, when the target Y corresponds to a sequence, it is reasonable to produce it sequentially with an RNN, as in eq. (2), but how should we produce Y if it does not correspond naturally to a sequence? Note that sequences can be encoded as sets. Indeed, if we associate to each element of a sequence the index it occupies in it, forming a tuple, we effectively convert this sequence to a set. For example, the  2  Published as a conference paper at ICLR 2016  sequence “I like cats” becomes the set {(I,1), (like,2), (cats,3)} (note that we can permute elements in the set but still recover the original sequence). Although this may be unnecessary in some cases, we argue that, even for sequences, inputting and/or outputting them in a different order could be beneﬁcial. For example, in sorting we may want to employ a divide-and-conquer strategy which ﬁnds the median element ﬁrst (i.e., we may output the solution in neither increasing nor decreasing sequential order). In the following two sections we discuss how to extend seq2seq to handle input sets (Section 4) and output sets (Section 5). We also show the importance of ordering in a variety of tasks in which seq2seq has successfully been applied, and include experimental results to support our claims and extensions to the existing models.  4  INPUT SETS  We ﬁrst study extensions to encoding (reading) sets. As we discussed in the previous section, se- quences can be read with a recurrent neural network which can compress its contents into a vector. An important invariance property that must be satisﬁed when the input is a set (i.e., the order does not matter) is that swapping two elements xi and xj in the set X should not alter its encoding. A simple approach which satisﬁes this, and which in fact has been commonly used for encoding sentences, is the bag-of-words approach. In this case, the representation is simply a reduction (e.g., addition) of counts, word embeddings, or similar embedding functions, and is naturally permutation invariant. For language and other domains which are naturally sequential, this is replaced with more complex encoders such as recurrent neural networks that take order into account and model higher order statistics of the data. An unsatisfying property of using a reduction operation (such as addition) is that it makes the rep- resentation quite inefﬁcient: the model operates over a ﬁxed dimensional embedding regardless of the length of the set. It is unlikely that such representation will succeed, as the amount of memory required to encode a length T set (or sequence, for that matter) should increase as a function of T . Thus, we argue that even deep convolutional architectures will suffer from this limitation – though some modiﬁcations exist (Maas et al., 2012). In our work, we largely rely on attention mechanisms to integrate information from a variable length structure, which we describe in Section 4.2.  4.1  INPUT ORDER MATTERS  In this section, we highlight prior work where we observed that the order of inputs impacted the per- formance of seq2seq models taking sequences as input. In principle, order should not matter when using a complex encoder such as a recurrent neural network, as these are universal approximators that can encode complex features from the input sequence (e.g., n-grams of any order). We believe that the reason order seems to matter is due to the underlying non-convex optimization and more suitable prior. The ﬁrst example which we experimented with was altering the order of sequences in the context of machine translation. In machine translation, the mapping function encodes a sentence in a source language (e.g., English), and decodes it to its translation in a target language (e.g., French). By reversing the order of the input English sentence, Sutskever et al. (2014) got a 5.0 BLEU score improvement which allowed them to close the gap between their model – a fully end-to-end model for machine translation – and state-of-the-art models which were highly engineered. Similarly, for constituency parsing, in which the mapping is from an English sentence to a ﬂattened version of its constituency parse tree, a 0.5% absolute increase in F1 score was observed when reversing the English sentence (Vinyals et al., 2015b). Furthermore, if we preprocess the data for, e.g., convex hull computation that was presented in Vinyals et al. (2015a) by sorting the points by angle, the task becomes simpler (from O(n log n) to O(n)), and as a result the models obtained are much faster to train and better (increasing accuracy by up to 10% absolute in the most challenging cases).  3  Published as a conference paper at ICLR 2016  All these empirical ﬁndings point to the same story: often for optimization purposes, the order in which input data is shown to the model has an impact on the learning performance. Note that we can deﬁne an ordering which is independent of the input sequence or set X (e.g., always reversing the words in a translation task), but also an ordering which is input dependent (e.g., sorting the input points in the convex hull case). This distinction also applies in the discussion about output sequences and sets in Section 5.1. Recent approaches which pushed the seq2seq paradigm further by adding memory and computation to these models allowed us to deﬁne a model which makes no assumptions about input ordering, whilst preserving the right properties which we just discussed: a memory that increases with the size of the set, and which is order invariant. In the next sections, we explain such a modiﬁcation, which could also be seen as a special case of a Memory Network (Weston et al., 2015) or Neural Turing Machine (Graves et al., 2014) – with a computation ﬂow as depicted in Figure 1.  4.2 ATTENTION MECHANISMS  Neural models with memories coupled to differentiable addressing mechanism have been success- fully applied to handwriting generation and recognition (Graves, 2012), machine translation (Bah- danau et al., 2015a), and more general computation machines (Graves et al., 2014; Weston et al., 2015). Since we are interested in associative memories we employed a “content” based attention. This has the property that the vector retrieved from our memory would not change if we randomly shufﬂed the memory. This is crucial for proper treatment of the input set X as such. In particular, our process block based on an attention mechanism uses the following:  qt = LST M (q∗ ei,t = f (mi, qt)  t−1)  ai,t =  exp(ei,t) j exp(ej,t)  (cid:80) (cid:88)  ai,tmi  rt = q∗ t = [qt rt]  i  (3) (4)  (5)  (6)  (7)  Figure 1: The Read-Process-and-Write model. where i indexes through each memory vector mi (typically equal to the cardinality of X), qt is a query vector which allows us to read rt from the memories, f is a function that computes a single scalar from mi and qt (e.g., a dot product), and LST M is an LSTM which computes a recurrent state but which takes no inputs. q∗ t is the state which this LSTM evolves, and is formed by concatenating the query qt with the resulting attention readout rt. t is the index which indicates how many “processing steps” are being carried to compute the state to be fed to the decoder. Note that permuting mi and mi(cid:48) has no effect on the read vector rt.  4.3 READ, PROCESS, WRITE  Our model, which naturally handles input sets, has three components (the exact equations and im- plementation will be released in an appendix prior to publication):  • A reading block, which simply embeds each element xi ∈ X using a small neural network onto a memory vector mi (the same neural network is used for all i).  • A process block, which is an LSTM without inputs or outputs performing T steps of com- putation over the memories mi. This LSTM keeps updating its state by reading mi repeat- edly using the attention mechanism described in the previous section. At the end of this block, its hidden state q∗ T is an embedding which is permutation invariant to the inputs. See eqs. (3)-(7) for more details.  4  ReadProcessWritePublished as a conference paper at ICLR 2016  • A write block, which is an LSTM pointer network (Vinyals et al., 2015a) that takes in q∗  T (as the context it needs from which to produce the output from the input set), and points at ele- ments of mi (implicitly, xi), one step at a time. The original work in Vinyals et al. (2015a) used a pointer mechanism which, instead of issuing a readout of memory by a weighted sum with a soft pointer (see eq. 6), it uses the pointer as part of the loss. We extended this by adding an extra attention step before the pointer (we called this glimpse). This is related to the process block described above, but with the difference that the attention reads happen interleaved between each pointer output. As described later in the results, we found these two mechanisms to complement each other.  The architecture is depicted in Figure 1 and can be seen as a special case of a Neural Turing Machine or Memory Network. It satisﬁes the key property of being invariant to the order of the elements in X, thus effectively processing the inputs as a set. Also note that the write component could simply be an LSTM if the outputs were from a ﬁxed dictionary. For this model, though, we study combinatorial problems where the outputs are pointers to the inputs, so we use a pointer network.  4.4 SORTING EXPERIMENT  In order to verify if our model handles sets more efﬁciently than the vanilla seq2seq approach, we ran the following experiment on artiﬁcial data for the task of sorting numbers: given N unordered random ﬂoating point numbers between 0 and 1, we return them in a sorted order. Note that this problem is an instance of set2seq. We used the architecture deﬁned in Figure 1, where the Read module is a small multilayer perceptron for each number, the Process module is an attention mech- anism over the read numbers, implemented as T steps over an LSTM with no input nor output, but attending the input embeddings, followed by an LSTM to produce indices in the input numbers with a pointer network (Vinyals et al., 2015a), in the proper sorted order. We also compared this archi- tecture with a vanilla seq2seq architecture made of an input LSTM connected to an output LSTM which produces indices in the input numbers with a pointer network (Ptr-Net). Note that the only difference between these two models is the encoding of the set using either an LSTM (as in previ- ous work), or with the architecture proposed in the previous section. We ran multiple experiments, varying the number N of numbers to sort, as well as the number T of processing steps of the Read, Process, Write model. The out-of-sample accuracies (whether we succeeded in sorting all numbers or not) of these experi- ments are summarized in Table 1. We can see that the baseline pointer network LSTM input model is better than the Read-Process-and-Write model when no processing steps (P = 0 step) are used, but as soon as at least one processing step is allowed, the performance of the Read-Process-and-Write model gets better, increasing with the number of processing steps. We can also see that, as the size of the task (expressed in the number of elements to sort N) grows, the performance gets worse, as expected. Also note that with 0 processing steps and 0 glimpses, the writing module is effectively unconditioned on X and has to “blindly” point at the elements of X. Thus, it is unsurprising to see it performing worse than any other model considered in Table 1. Lastly, equipping the writing module with glimpses (i.e., adding an attention mechanism prior to “pointing”) improves both the baseline model (Ptr-Net), and our proposed modiﬁcation quite signiﬁcantly (in the most challenging cases, it more than doubles accuracy).  Table 1: The sorting experiment: out-of-sample sorting accuracy for various problem sizes and processing steps, with or without glimpses. All the reported accuracies are shown after reaching 10000 training iterations, at which point all models had converged but none overﬁtted. Higher is better.  Length N glimpses N = 5 N = 10 N = 15  Ptr-Net 0 1  P = 0 step 0  1  P = 1 step 0  1  P = 5 steps P = 10 steps 0  1  0  1  81% 90% 65% 84% 84% 92% 88% 94% 90% 94% 30% 14% 44% 17% 57% 19% 50% 8% 0% 2% 10%  28% 7% 4% 1%  0%  5%  2%  4%  0%  5  Published as a conference paper at ICLR 2016  5 OUTPUT SETS  So far, we have considered the problem of encoding input sets; let us now turn our attention to out- put representations. The chain rule which describes joint probabilities over sets of random variables Y is, perhaps, the simplest decomposition of the joint probability which does not incur arbitrary restrictions (such as conditional independence). Thus, as long as a powerful model that is trainable exists (which can cope with long range correlations), any order should work without any prior order information of the underlying problem that generated Y . Despite this, and even when a very pow- erful model (in terms of modeling power, and resilience to vanishing long term gradients) like the LSTM is employed, output ordering still plays a key role in successfully training models. In the next subsection, we describe how the order in which we apply the chain rule affects the performance on various tasks.  5.1 OUTPUT ORDER MATTERS  Let Y be a set (or a sequence). In this section, we will study the effect that ordering has on the per- formance of seq2seq models on several tasks. Namely, we will consider arbitrary (and non-arbitrary) orders over the variables in Y , and model the conditional probability distribution P (Y |X) following that order for all training examples. As we will see, order matters (even when considering that the formulation through the chain rule works regardless of the ordering of Y , at least in principle).  5.1.1 LANGUAGE MODELING  For this experiment, we use the PennTree Bank, which is a standard language modeling bench- mark. This dataset is quite small for language modeling standards, so most models are data starved. We trained medium sized LSTMs with large amounts of regularization (see medium model from Zaremba et al. (2014)) to estimate probabilities over sequences of words. We consider three version of the dataset with three orderings: natural, reverse, and a ﬁxed, 3-word reversal: Natural: “This is a sentence .” Reverse: “. sentence a is This” 3-word: “a is This <pad> . sentence” Note that the 3-word reversal destroys the underlying structure of the sentence, and makes modeling the joint probability much more difﬁcult since many higher order n-grams are scrambled. For each ordering we trained a different model. The results for both natural and reverse matched each other at 86 perplexity on the development set (using the same setup as Zaremba et al. (2014)). Surpris- ingly, the 3-word reversal degraded only 10 perplexity points, still achieving an impressive result in this corpus at 96 perplexity. We note, however, that training perplexities were also 10 points higher, which indicates that the model had trouble handling the awkward ordering. Thus, even when consid- ering that the chain rule still properly models the joint probability, some degradation was observed when a confounding ordering was chosen.  5.1.2 PARSING  The task of constituency parsing consists in producing a parse tree given a sentence. The model proposed by Vinyals et al. (2015b) is a sentence encoder LSTM followed by a decoder LSTM trained to generate a depth ﬁrst traversal encoding of the parse tree, using an attention mechanism. This approach matched state-of-the-art results on this task. Even though it seemed more sensible, depth ﬁrst traversal is only one of the many ways one can uniquely encode a tree onto a sequence. We thus tried to train a small model using depth ﬁrst traversal (which matches the baseline of Vinyals et al. (2015b)) and another one using breadth ﬁrst traversal (note that these orderings are input dependent). See Figure 2 for an example on how the tree linearizes under both traversal schemes. The model trained to produce depth ﬁrst traversal linearized trees obtained 89.5% F1 score (as reported by Vinyals et al. (2015b)), whereas the one producing breadth ﬁrst traversal trees had a much lower F1 score at 81.5%,1 showing again the importance of picking the right output ordering.  1In fact, in many cases the decoder failed to produce a valid tree, so the real F1 score is likely lower.  6  Published as a conference paper at ICLR 2016  Figure 2: Depth ﬁrst and breadth ﬁrst linearizations of a parse tree which shows our different setups for output ordering in the parsing task.  5.1.3 COMBINATORIAL PROBLEMS  Unlike in the previous two examples, a problem that more commonly comes up as we try to represent non-sequential data (like tours, triangulations, etc., discussed by (Vinyals et al., 2015a)), is the fact that there may exist a large equivalence class of solutions. Take, as an example, outputting the indices for the sorted inputs of a set of random numbers, X. Indeed, this is a deterministic function. We can choose to output these indices in some order (e.g., increasing, decreasing, or using any arbitrary ﬁxed permutation), or treat them as a set (a tuple of argsort indices with corresponding ranking). As a result, there are n! possible outputs for a given X, all of which are perfectly valid. If our training set is generated with any of these permutations picked uniformly at random, our mapping (when perfectly trained) will have to place equal probability on n! output conﬁgurations for the same input X. Thus, this formulation is much less statistically efﬁcient. In previous work (Vinyals et al., 2015a), it was found that restricting as much as possible the equiv- alence class for the outputs was always better. For instance, to output a tour (i.e. a sequence of cities one has to visit for the traveling salesman problem), we started from the lower indexed city (i.e., the ﬁrst city that we input), and followed a counter-clockwise ordering. Similarly, to output a set of triangles (which triangulate the set of input points), we sorted them in lexicographical order and moved left to right. In all cases, improvements of 5% absolute accuracy or more were observed. Failing to restrict the output equivalence class generally implies much slower convergence (and, thus, requires much more training data). For instance, for sorting, if considering the outputs as sets which we output in any of the possible n! orderings, convergence for n as small as 5 never reached the same performance.  5.1.4 GRAPHICAL MODELS  Let us consider the joint probability of a set of T random variables P (y1, y2, . . . , yT ). Having no prior on how these random variables interact with each other, one way to model their joint probability is to use the chain rule as follows:  P (y1, y2, . . . , yT ) =  P (yt|y1, y2, . . . , yt−1)  (8)  t=1  and model this using an RNN, similar to RNN language models. While for sentences the natural order of words gives a good clue of how to order the random variables in the model, for other kind of data it might be harder to decide on it. Furthermore, in theory, the order should not matter, because of Bayes rule which lets us reorder all the conditional probabilities as needed. In practice however, it might be that one order is easier to model than another, as we have shown in this paper. The purpose of this experiment is to demonstrate this using a controlled toy experiment. We gen- erated star-like graphical models over random variables where one variable (the head) follows an unconditional distribution, while the others follow a conditional distribution based on the value of  7  T(cid:89)  NPVBZis.VPSDTThisNPDTaNNsentenceDepth First Traversal: S NP DT !DT !NP VP VBZ !VBZ NP DT !DT NN !NN !NP !VP . !. !SBreadth First Traversal: S LEV NP VP . LEV DT PAR VBZ NP LEV PAR PAR DT NN DONEPublished as a conference paper at ICLR 2016  the head variable. We expect that it should be easier to model the joint distribution by choosing any ordering which starts with the head variable. We created several artiﬁcial datasets by varying the number of random variables to model (between 10 and 50, each of which was a multinomial over 10 symbols), the training set size (between 200 and 20000 training samples), and the randomness of the marginal distributions, or how deterministic, or peaky, they were. For each problem, we trained two LSTMs for 10,000 mini-batch iterations to model the joint probability, one where the head random variable was shown ﬁrst, and one where it was shown last. The results were as follows:  • when the training set size is large enough (20000), the LSTM is able to learn the joint  probability in whichever order;  • when the marginal distributions are very peaky (and thus almost deterministic), the LSTM  is also able to learn the joint probability independently of the order;  • in all other cases (small training set size, small or large number of random variables, and some amount of randomness in the marginal distributions), it was always easier to learn an LSTM with the optimal order of random variables than any other order.  5.2 FINDING OPTIMAL ORDERINGS WHILE TRAINING  Recall the model we proposed for dealing with input sets: given an embedding for each of the inputs, we have a generic module that is able to process its inputs in any order. This yields an embedding satisfying the key property of being invariant to reorderings, whilst being generic in the kinds of computations to do over the input set. Unfortunately, placing a joint probability over a set of random variables y1, . . . yn when the structure of the joint probability function is unknown is a hard problem. Fortunately, and thanks to recurrent neural networks, we can apply the chain rule which decomposes this joint probability sequentially (see eq. 8) without independence assumptions. In this work, we focus on using the chain rule, discarding more naive decompositions that have strong and unrealistic assumptions (e.g., conditional independence). An obvious drawback of the chain rule which violates the argument of treating y1, . . . yn as a set is that we condition these random variables in a particular order. Even though, in principle, the order should not matter, in the previous section we have shown that this is indeed not the case, and that certain orderings are better than others in a variety of tasks – most likely due to the parameterization of the joint probability (using an LSTM), and the non-convex nature of the optimization problem. Our proposed solution to deal with the aforementioned drawback is extremely simple: as we train, we let the model decide which is the best ordering in which it will apply the chain rule. More formally, assume there exists an ordering which maximally simpliﬁes the task, π(X) (where X is the input sequence or set, which can be empty). We would like to train the model as p(Yπ(X)|X). The number of possible orderings is large – n! where n is the length of the output, and the best order is unknown a priori. Since n! can be very large, we could attempt to do (inexact) search as we train the model. Instead of maximizing the log probability of p(Y |X) for each training example pair, we also maximize over orderings as follows:  θ(cid:63) = arg max  θ  log p(Yπ(Xi)|Xi; θ)  max π(Xi)  (9)  where maxπ(Xi) is computed either naively, or with an inexact search. Note that Equation (9) may not strictly improve the regular maximum likelihood framework due to non-convexity, but we found this to not be an issue in practice. Besides not being scalable, we found that, if done naively and picking the max over ordering as we train, the model would pick a random ordering (as a function of the initial parameters), and would get stuck on it permanently (since it would reinforce it through learning). We added two ways to explore the space of all orderings as follows:  8  (cid:88)  i  Published as a conference paper at ICLR 2016  replacing the maxπ(Xi) in eq. (9) by a(cid:80)  π(Xi).  • We pretrain the model with a uniform prior over π(X) for 1000 steps, which amounts to  • We then pick an ordering by sampling π(X) according to a distribution proportional to p(Yπ(X)|X). This costs O(1) model evaluations (vs. naive search which would be O(n!)). Crucially, sampling p(Yπ(X)|X) can be done very efﬁciently as we can use ancestral sampling (left- to-right) which requires to evaluate p(.) only once instead of n!.  5.2.1  5-GRAM MODELING  In our initial attempt to solve (9), we considered a simpliﬁed version of the language modeling task described in Section 5.1.1. The simpliﬁed task consists of modeling the joint probability of 5- grams without any further context (i.e., there is no input X). This choice allowed us to have a small enough n as initially we were trying to exactly ﬁnd the best ordering out of the n! possible ones. Thus, we disregarded possible effects of inexact search, and focused on the essential of the training dynamics where the model being optimized picks the best ordering π which maximizes p(Yπ) under its current parameters, and reinforces that ordering by applying updates on the gradient of log p(Yπ) w.r.t. the parameters. Eventually, as noted in Section 5.2, we found sampling to be superior in terms of convergence, whilst simplifying the complexity from O(n!) down to O(1), and is the preferred solution which we used in the rest of this section. To test this framework, we converted 5-grams (i.e., sequences of words) to a set in the following way: 5-gram (sequence): y1=This, y2=is, y3=a, y4=ﬁve, y5=gram 5-gram (set): y1=(This,1), y2=(is,2), y3=(a,3), y4=(ﬁve,4), y5=(gram,5) Note that adding the original position alongside the words makes Y a set. Thus, we can shufﬂe Y in arbitrarily without losing the original structure of the sequence. The ﬁrst experiment, which reinforces our result in Section 5.1.1, tests the hypothesis once again that order matters. Training a model which follows the natural order (i.e., produces (This,1), followed by (is,2) conditioned on (This,1), etc.), achieves a validation perplexity of 225.2 If, instead of picking (1, 2, 3, 4, 5), we use (5, 1, 3, 4, 2), perplexity drops to 280. We then test optimization of eq. (9) in two setups: Easy: The training set contains examples from (1, 2, 3, 4, 5) and (5, 1, 3, 4, 2), uniformly sampled. Hard: The training set contains examples from the 5! possible orderings, uniformly sampled. Our results are shown in Table 2. Note that, in the easy case, we restrict the search space over orderings to only 2, where one order is clearly better than the other. We note that, after the pretraining phase, we decide which of the two orderings is better to represent the data under the model being trained. Very quickly, the model settles on the natural (1, 2, 3, 4, 5) ordering, yielding a perplexity of 225. In the most difﬁcult case, where any order is possible, the model settles to orders such as (1, 2, 3, 4, 5), (5, 4, 3, 2, 1), and small variations of them. In all cases, the ﬁnal perplexity is 225. Thus, the framework we propose is able to ﬁnd good orderings without any prior knowledge. We plan to not only recover optimal orderings, but ﬁnd ones that were unknown to us when applying the seq2seq framework naively.  Table 2: Experiments in which the model ﬁnds the optimal ordering of a set for the 5-gram language modeling task. Perplexities are reported on the validation set (lower is better).  Task (1, 2, 3, 4, 5) (5, 1, 3, 4, 2) Easy Hard  Orders considered 1 1 2 5!  Perplexity 225 280 225 225  2This is much worse than the results reported in Section 5.1.1 since modeling 5-grams without context is  much harder than standard language modeling.  9  Published as a conference paper at ICLR 2016  6 CONCLUSION  LSTMs have shown to be powerful models to represent variable length sequential data thanks to their ability to handle reasonably long term dependencies and the use of the chain rule to efﬁciently decompose joint distributions. On the other hand, some problems are expressed in terms of an unordered set of elements, either as input or as outputs; in some other cases, the data is represented by some structure that needs to be linearized to be fed to the LSTM, and there might be more than one way to do so. The ﬁrst goal of this paper was to shed some light on these problems: indeed, we show that order matters to obtain the best performance. We then considered the case of unordered input data, for which we proposed the Read-Process-and-Write architecture, and the case of unordered output data, for which we proposed an efﬁcient training algorithm that includes a search over possible orders during training and inference. We illustrated our proposed approaches for input and output sets through various experiments such as sorting, graphical models, language modeling, and parsing.  ACKNOWLEDGMENTS  We would like to thank Ilya Sutskever, Navdeep Jaitly, Rafal Jozefowicz, Quoc Le, Lukasz Kaiser, Geoffrey Hinton, Jeff Dean, Shane Gu and the Google Brain Team for useful discussions on this topic. We also thank the anonymous reviewers which helped improving our paper.  REFERENCES Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. In  Proc. ICLR, 2015a.  Bahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P., and Bengio, Y. End-to-end attention-based large vocab-  ulary speech recognition. arXiv preprint arXiv:1508.04395, 2015b.  Bakir, G., Hofmann, T., Scholkopf, B., Smola, A. J., Taskar, B., and Vishwanathan, S.V.N. (eds.). Predicting  Structured Data. MIT Press, 2007.  Chan, W., Jaitly, N., Le, Q. V., and Vinyals, O. Listen, attend and spell. arXiv, abs/1508.01211, 2015. URL  http://arxiv.org/abs/1508.01211.  Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning In Proc. EMNLP,  phrase representations using RNN encoder-decoder for statistical machine translation. 2014.  Donahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., and Darrell, T.  Long-term recurrent convolutional networks for visual recognition and description. In Proc. CVPR, 2015.  Graves, A. Supervised Sequence Labelling with Recurrent Neural Networks. Springer, 2012.  Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. In arXiv preprint arXiv:1410.5401, 2014.  Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. N., and Kingsbury, B. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29:82–97, 2012.  Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 9(8), 1997.  Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covari-  ate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML, 2015.  Kalchbrenner, N. and Blunsom, P. Recurrent continuous translation models. In Proc. EMNLP, 2013.  Maas, A. L., Miller, S. D., O’Neil, T. M., and Ng, A. Y. Word-level acoustic modeling with convolutional  vector regression. In ICML 2012 Workshop on Representation Learning, 2012.  Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., and Yuille, A. L. Deep captioning with multimodal recurrent  neural networks (m-RNN). In International Conference on Learning Representations, 2015.  Socher, R., Manning, C. D., and Ng, A. Y. Learning continuous phrase representations and syntactic parsing  with recursive neural networks. In Advances in Neural Information Processing Systems, 2010.  10  Published as a conference paper at ICLR 2016  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural networks. In Proc.  NIPS, 2014.  Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In Advances in Neural Information Processing  Systems, NIPS, 2015a.  Vinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., and Hinton, G. Grammar as a foreign language. In  Advances in Neural Information Processing Systems, 2015b.  Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. Show and tell: A neural image caption generator. In Proc.  CVPR, 2015c.  Weston, J., Chopra, S., and Bordes, A. Memory networks. In International Conference on Learning Represen-  tations, ICLR, 2015.  Zaremba, W. and Sutskever, I. Learning to execute. arXiv, abs/1410.4615, 2014.  Zaremba, W. and Sutskever, I. Reinforcement learning neural turing machines. arXiv, abs/1505.00521, 2015.  Zaremba, W., Sutskever, I., and Vinyals, O. Recurrent neural network regularization. arXiv, abs/1409.2329,  2014.  11  ",
1506.02438,2016,High-Dimensional Continuous Control Using Generalized Advantage Estimation,"['High-Dimensional Continuous Control Using Generalized Advantage Estimation\nJohn Schulman', 'Philipp Moritz', 'Sergey Levine', 'Michael Jordan', 'Pieter Abbeel']",https://arxiv.org/pdf/1506.02438,"8 1 0 2    t c O 0 2         ]  G L . s c [      6 v 8 3 4 2 0  .  6 0 5 1 : v i X r a  Published as a conference paper at ICLR 2016  HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION  John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel Department of Electrical Engineering and Computer Science University of California, Berkeley {joschu,pcmoritz,levine,jordan,pabbeel}@eecs.berkeley.edu  ABSTRACT  Policy gradient methods are an appealing approach in reinforcement learning be- cause they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difﬁ- culty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the ﬁrst challenge by using value functions to substan- tially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomo- tion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy repre- sentations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experi- ence required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.  1  INTRODUCTION  The typical problem formulation in reinforcement learning is to maximize the expected total reward of a policy. A key source of difﬁculty is the long time delay between actions and their positive or negative effect on rewards; this issue is called the credit assignment problem in the reinforcement learning literature (Minsky, 1961; Sutton & Barto, 1998), and the distal reward problem in the behavioral literature (Hull, 1943). Value functions offer an elegant solution to the credit assignment problem—they allow us to estimate the goodness of an action before the delayed reward arrives. Reinforcement learning algorithms make use of value functions in a variety of different ways; this paper considers algorithms that optimize a parameterized policy and use value functions to help estimate how the policy should be improved. When using a parameterized stochastic policy, it is possible to obtain an unbiased estimate of the gradient of the expected total returns (Williams, 1992; Sutton et al., 1999; Baxter & Bartlett, 2000); these noisy gradient estimates can be used in a stochastic gradient ascent algorithm. Unfortunately, the variance of the gradient estimator scales unfavorably with the time horizon, since the effect of an action is confounded with the effects of past and future actions. Another class of policy gradient algorithms, called actor-critic methods, use a value function rather than the empirical returns, ob- taining an estimator with lower variance at the cost of introducing bias (Konda & Tsitsiklis, 2003; Hafner & Riedmiller, 2011). But while high variance necessitates using more samples, bias is more pernicious—even with an unlimited number of samples, bias can cause the algorithm to fail to con- verge, or to converge to a poor solution that is not even a local optimum. We propose a family of policy gradient estimators that signiﬁcantly reduce variance while main- taining a tolerable level of bias. We call this estimation scheme, parameterized by γ ∈ [0, 1] and  1  Published as a conference paper at ICLR 2016  λ ∈ [0, 1], the generalized advantage estimator (GAE). Related methods have been proposed in the context of online actor-critic methods (Kimura & Kobayashi, 1998; Wawrzy´nski, 2009). We provide a more general analysis, which is applicable in both the online and batch settings, and discuss an in- terpretation of our method as an instance of reward shaping (Ng et al., 1999), where the approximate value function is used to shape the reward. We present experimental results on a number of highly challenging 3D locomotion tasks, where we show that our approach can learn complex gaits using high-dimensional, general purpose neural network function approximators for both the policy and the value function, each with over 104 parameters. The policies perform torque-level control of simulated 3D robots with up to 33 state dimensions and 10 actuators. The contributions of this paper are summarized as follows:  1. We provide justiﬁcation and intuition for an effective variance reduction scheme for policy gra- dients, which we call generalized advantage estimation (GAE). While the formula has been pro- posed in prior work (Kimura & Kobayashi, 1998; Wawrzy´nski, 2009), our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments.  2. We propose the use of a trust region optimization method for the value function, which we ﬁnd is a robust and efﬁcient way to train neural network value functions with thousands of parameters. 3. By combining (1) and (2) above, we obtain an algorithm that empirically is effective at learning neural network policies for challenging control tasks. The results extend the state of the art in using reinforcement learning for high-dimensional continuous control. Videos are available at https://sites.google.com/site/gaepapersupp.  2 PRELIMINARIES  We consider an undiscounted formulation of the policy optimization problem. The initial state s0 is sampled from distribution ρ0. A trajectory (s0, a0, s1, a1, . . . ) is generated by sampling ac- tions according to the policy at ∼ π(at | st) and sampling the states according to the dynamics st+1 ∼ P (st+1 | st, at), until a terminal (absorbing) state is reached. A reward rt = r(st, at, st+1) t=0 rt, which is assumed to be ﬁnite for all policies. Note that we are not using a discount as part of the problem spec- iﬁcation; it will appear below as an algorithm parameter that adjusts a bias-variance tradeoff. But t=0 γtrt) can be handled as an instance of the undiscounted problem in which we absorb the discount factor into the reward function, making it time-dependent. Policy gradient methods maximize the expected total reward by repeatedly estimating the gradient t=0 rt]. There are several different related expressions for the policy gradient, which  is received at each timestep. The goal is to maximize the expected total reward(cid:80)∞ the discounted problem (maximizing(cid:80)∞ g := ∇θE [(cid:80)∞  have the form  (cid:35)  g = E  Ψt∇θ log πθ(at | st)  ,  (1)  (cid:34) ∞(cid:88)  t=0  1. (cid:80)∞ 2. (cid:80)∞ 3. (cid:80)∞  where Ψt may be one of the following:  t=0 rt: total reward of the trajectory.  previous formula.  t(cid:48)=t rt(cid:48): reward following action at. t(cid:48)=t rt(cid:48) − b(st): baselined version of (cid:35)  V π(st) := Est+1:∞, at:∞  rt+l  (cid:34) ∞(cid:88)  l=0  Aπ(st, at) := Qπ(st, at) − V π(st),  The latter formulas use the deﬁnitions  4. Qπ(st, at): state-action value function.  5. Aπ(st, at): advantage function. 6. rt + V π(st+1) − V π(st): TD residual.  (cid:35)  rt+l  (cid:34) ∞(cid:88)  l=0  (2)  (3)  Qπ(st, at) := Est+1:∞, at+1:∞  (Advantage function).  2  Published as a conference paper at ICLR 2016  Here, the subscript of E enumerates the variables being integrated over, where states and actions are sampled sequentially from the dynamics model P (st+1 | st, at) and policy π(at | st), respectively. The colon notation a : b refers to the inclusive range (a, a + 1, . . . , b). These formulas are well known and straightforward to obtain; they follow directly from Proposition 1, which will be stated shortly. The choice Ψt = Aπ(st, at) yields almost the lowest possible variance, though in practice, the advantage function is not known and must be estimated. This statement can be intuitively justiﬁed by the following interpretation of the policy gradient: that a step in the policy gradient direction should increase the probability of better-than-average actions and decrease the probability of worse-than- average actions. The advantage function, by it’s deﬁnition Aπ(s, a) = Qπ(s, a) − V π(s), measures whether or not the action is better or worse than the policy’s default behavior. Hence, we should choose Ψt to be the advantage function Aπ(st, at), so that the gradient term Ψt∇θ log πθ(at | st) points in the direction of increased πθ(at | st) if and only if Aπ(st, at) > 0. See Greensmith et al. (2004) for a more rigorous analysis of the variance of policy gradient estimators and the effect of using a baseline. We will introduce a parameter γ that allows us to reduce variance by downweighting rewards cor- responding to delayed effects, at the cost of introducing bias. This parameter corresponds to the discount factor used in discounted formulations of MDPs, but we treat it as a variance reduction parameter in an undiscounted problem; this technique was analyzed theoretically by Marbach & Tsitsiklis (2003); Kakade (2001b); Thomas (2014). The discounted value functions are given by:  V π,γ(st) := Est+1:∞, at:∞  γlrt+l  Qπ,γ(st, at) := Est+1:∞, at+1:∞  Aπ,γ(st, at) := Qπ,γ(st, at) − V π,γ(st).  The discounted approximation to the policy gradient is deﬁned as follows: Aπ,γ(st, at)∇θ log πθ(at | st)  .  gγ := Es0:∞ a0:∞  (cid:35)  (cid:34) ∞(cid:88)  l=0  (cid:34) ∞(cid:88)  t=0  (cid:35)  (cid:34) ∞(cid:88)  l=0  γlrt+l  (4)  (5)  (6)  (cid:35)  The following section discusses how to obtain biased (but not too biased) estimators for Aπ,γ, giving us noisy estimates of the discounted policy gradient in Equation (6). Before proceeding, we will introduce the notion of a γ-just estimator of the advantage function, which is an estimator that does not introduce bias when we use it in place of Aπ,γ (which is not known and must be estimated) in Equation (6) to estimate gγ.1 Consider an advantage estimator ˆAt(s0:∞, a0:∞), which may in general be a function of the entire trajectory. Deﬁnition 1. The estimator ˆAt is γ-just if  (cid:104) ˆAt(s0:∞, a0:∞)∇θ log πθ(at | st)  (cid:105)  = Es0:∞ a0:∞  [Aπ,γ(st, at)∇θ log πθ(at | st)] .  (7)  Es0:∞ a0:∞  It follows immediately that if ˆAt is γ-just for all t, then  (cid:35)  ˆAt(s0:∞, a0:∞)∇θ log πθ(at | st)  = gγ  (8)  (cid:34) ∞(cid:88)  t=0  Es0:∞ a0:∞  One sufﬁcient condition for ˆAt to be γ-just is that ˆAt decomposes as the difference between two functions Qt and bt, where Qt can depend on any trajectory variables but gives an unbiased estimator of the γ-discounted Q-function, and bt is an arbitrary function of the states and actions sampled before at. Proposition 1. Suppose that ˆAt can be written in the form ˆAt(s0:∞, a0:∞) = Qt(st:∞, at:∞) − bt(s0:t, a0:t−1) such that for all (st, at), Est+1:∞,at+1:∞ | st,at [Qt(st:∞, at:∞)] = Qπ,γ(st, at). Then ˆA is γ-just.  1Note, that we have already introduced bias by using Aπ,γ in place of Aπ; here we are concerned with obtaining an unbiased estimate of gγ, which is a biased estimate of the policy gradient of the undiscounted MDP.  3  Published as a conference paper at ICLR 2016  The proof is provided in Appendix B. It is easy to verify that the following expressions are γ-just advantage estimators for ˆAt:  • (cid:80)∞  l=0 γlrt+l • Qπ,γ(st, at)  • Aπ,γ(st, at) • rt + γV π,γ(st+1) − V π,γ(st).  3 ADVANTAGE FUNCTION ESTIMATION  This section will be concerned with producing an accurate estimate ˆAt of the discounted advan- tage function Aπ,γ(st, at), which will then be used to construct a policy gradient estimator of the following form:  N(cid:88)  ∞(cid:88)  n=1  t=0  ˆg =  1 N  t ∇θ log πθ(an ˆAn  t | sn t )  (9)  where n indexes over a batch of episodes. t = rt + γV (st+1) − V (st), i.e., the TD residual Let V be an approximate value function. Deﬁne δV t can be considered as an estimate of the of V with discount γ (Sutton & Barto, 1998). Note that δV advantage of the action at. In fact, if we have the correct value function V = V π,γ, then it is a γ-just advantage estimator, and in fact, an unbiased estimator of Aπ,γ:  (cid:104)  (cid:105)  Est+1  δV π,γ t  = Est+1 [rt + γV π,γ(st+1) − V π,γ(st)] = Est+1 [Qπ,γ(st, at) − V π,γ(st)] = Aπ,γ(st, at).  (10)  However, this estimator is only γ-just for V = V π,γ, otherwise it will yield biased policy gradient estimates. Next, let us consider taking the sum of k of these δ terms, which we will denote by ˆA(k)  t  t  ˆA(1) ˆA(2) ˆA(3)  t  t  := δV t t + γδV := δV t + γδV  := δV  t+1 t+1 + γ2δV  = −V (st) + rt + γV (st+1) = −V (st) + rt + γrt+1 + γ2V (st+2)  t+2 = −V (st) + rt + γrt+1 + γ2rt+2 + γ3V (st+3)  (11) (12) (13)  k−1(cid:88)  l=0  ˆA(k)  t  :=  γlδV  t+l = −V (st) + rt + γrt+1 + ··· + γk−1rt+k−1 + γkV (st+k)  (14)  These equations result from a telescoping sum, and we see that ˆA(k) involves a k-step estimate of the returns, minus a baseline term −V (st). Analogously to the case of δV , we can consider ˆA(k) to be an estimator of the advantage function, which is only γ-just when V = V π,γ. However, note that the bias generally becomes smaller as k → ∞, since the term γkV (st+k) becomes more heavily discounted, and the term −V (st) does not affect the bias. Taking k → ∞, we get  t = ˆA(1)  t  t  t  ∞(cid:88)  ∞(cid:88)  ˆA(∞) t =  γlδV  t+l = −V (st) +  γlrt+l,  (15)  which is simply the empirical returns minus the value function baseline.  l=0  l=0  4  Published as a conference paper at ICLR 2016  ˆAGAE(γ,λ)  t  The generalized advantage estimator GAE(γ, λ) is deﬁned as the exponentially-weighted average of these k-step estimators:  t + γδV  t + λ2 ˆA(3)  t + λ ˆA(2) t + . . . t + λ(δV t+1) + λ2(δV t (1 + λ + λ2 + . . . ) + γδV t+2(λ2 + λ3 + λ4 + . . . ) + . . . )  (cid:18) 1  (cid:19)  (cid:18) λ  1 − λ  + γδV  t+1  1 − λ  + γ2δV  t+2  t+1 + γ2δV  t + γδV t+1(λ + λ2 + λ3 + . . . )  t+2) + . . .(cid:1) (cid:18) λ2 (cid:19)  1 − λ  + . . .  (cid:17)  (cid:19)  := (1 − λ)  (cid:16) ˆA(1) = (1 − λ)(cid:0)δV (cid:18)  = (1 − λ)(δV + γ2δV  = (1 − λ)  δV t  ∞(cid:88)  =  (γλ)lδV  t+l  (cid:19)  (16)  l=0  From Equation (16), we see that the advantage estimator has a remarkably simple formula involving a discounted sum of Bellman residual terms. Section 4 discusses an interpretation of this formula as the returns in an MDP with a modiﬁed reward function. The construction we used above is closely analogous to the one used to deﬁne TD(λ) (Sutton & Barto, 1998), however TD(λ) is an estimator of the value function, whereas here we are estimating the advantage function. There are two notable special cases of this formula, obtained by setting λ = 0 and λ = 1.  GAE(γ, 0) :  ˆAt := δt  = rt + γV (st+1) − V (st)  GAE(γ, 1) :  ˆAt :=  γlδt+l =  γlrt+l − V (st)  ∞(cid:88)  ∞(cid:88)  l=0  l=0  (17)  (18)  GAE(γ, 1) is γ-just regardless of the accuracy of V , but it has high variance due to the sum of terms. GAE(γ, 0) is γ-just for V = V π,γ and otherwise induces bias, but it typically has much lower variance. The generalized advantage estimator for 0 < λ < 1 makes a compromise between bias and variance, controlled by parameter λ. We’ve described an advantage estimator with two separate parameters γ and λ, both of which con- tribute to the bias-variance tradeoff when using an approximate value function. However, they serve different purposes and work best with different ranges of values. γ most importantly determines the scale of the value function V π,γ, which does not depend on λ. Taking γ < 1 introduces bias into the policy gradient estimate, regardless of the value function’s accuracy. On the other hand, λ < 1 introduces bias only when the value function is inaccurate. Empirically, we ﬁnd that the best value of λ is much lower than the best value of γ, likely because λ introduces far less bias than γ for a reasonably accurate value function. Using the generalized advantage estimator, we can construct a biased estimator of gγ, the discounted policy gradient from Equation (6):  (cid:35)  (cid:34) ∞(cid:88)  ∞(cid:88)  (cid:35)  gγ ≈ E  ∇θ log πθ(at | st) ˆAGAE(γ,λ)  t  = E  ∇θ log πθ(at | st)  (γλ)lδV  t+l  ,  (19)  t=0  t=0  l=0  where equality holds when λ = 1.  4  INTERPRETATION AS REWARD SHAPING  In this section, we discuss how one can interpret λ as an extra discount factor applied after per- forming a reward shaping transformation on the MDP. We also introduce the notion of a response function to help understand the bias introduced by γ and λ. Reward shaping (Ng et al., 1999) refers to the following transformation of the reward function of an MDP: let Φ : S → R be an arbitrary scalar-valued function on state space, and deﬁne the transformed reward function ˜r by  ˜r(s, a, s(cid:48)) = r(s, a, s(cid:48)) + γΦ(s(cid:48)) − Φ(s),  (20)  5  (cid:34) ∞(cid:88)  Published as a conference paper at ICLR 2016  ∞(cid:88)  ∞(cid:88)  ∞(cid:88)  ∞(cid:88)  which in turn deﬁnes a transformed MDP. This transformation leaves the discounted advantage function Aπ,γ unchanged for any policy π. To see this, consider the discounted sum of rewards of a trajectory starting with state st:  γl˜r(st+l, at, st+l+1) =  l=0  l=0  γlr(st+l, at+l, st+l+1) − Φ(st).  (21)  Letting ˜Qπ,γ, ˜V π,γ, ˜Aπ,γ be the value and advantage functions of the transformed MDP, one obtains from the deﬁnitions of these quantities that  ˜Qπ,γ(s, a) = Qπ,γ(s, a) − Φ(s) ˜V π,γ(s, a) = V π,γ(s) − Φ(s) ˜Aπ,γ(s, a) = (Qπ,γ(s, a) − Φ(s)) − (V π,γ(s) − Φ(s)) = Aπ,γ(s, a).  (22) (23) (24) Note that if Φ happens to be the state-value function V π,γ from the original MDP, then the trans- formed MDP has the interesting property that ˜V π,γ(s) is zero at every state. Note that (Ng et al., 1999) showed that the reward shaping transformation leaves the policy gradient and optimal policy unchanged when our objective is to maximize the discounted sum of rewards t=0 γtr(st, at, st+1). In contrast, this paper is concerned with maximizing the undiscounted sum  of rewards, where the discount γ is used as a variance-reduction parameter. Having reviewed the idea of reward shaping, let us consider how we could use it to get a policy gradient estimate. The most natural approach is to construct policy gradient estimators that use discounted sums of shaped rewards ˜r. However, Equation (21) shows that we obtain the discounted sum of the original MDP’s rewards r minus a baseline term. Next, let’s consider using a “steeper” discount γλ, where 0 ≤ λ ≤ 1. It’s easy to see that the shaped reward ˜r equals the Bellman residual term δV , introduced in Section 3, where we set Φ = V . Letting Φ = V , we see that  (cid:80)∞  (γλ)l˜r(st+l, at, st+l+1) =  (γλ)lδV  t+l = ˆAGAE(γ,λ)  t  .  (25)  l=0  l=0  Hence, by considering the γλ-discounted sum of shaped rewards, we exactly obtain the generalized advantage estimators from Section 3. As shown previously, λ = 1 gives an unbiased estimate of gγ, whereas λ < 1 gives a biased estimate. To further analyze the effect of this shaping transformation and parameters γ and λ, it will be useful to introduce the notion of a response function χ, which we deﬁne as follows:  Note that Aπ,γ(s, a) = (cid:80)∞  χ(l; st, at) = E [rt+l | st, at] − E [rt+l | st] . (26) l=0 γlχ(l; s, a), hence the response function decomposes the advantage function across timesteps. The response function lets us quantify the temporal credit assignment problem: long range dependencies between actions and rewards correspond to nonzero values of the response function for l (cid:29) 0. Next, let us revisit the discount factor γ and the approximation we are making by using Aπ,γ rather than Aπ,1. The discounted policy gradient estimator from Equation (6) has a sum of terms of the form  l=0  γlχ(l; st, at).  ∇θ log πθ(at | st)Aπ,γ(st, at) = ∇θ log πθ(at | st)  (27) Using a discount γ < 1 corresponds to dropping the terms with l (cid:29) 1/(1 − γ). Thus, the error introduced by this approximation will be small if χ rapidly decays as l increases, i.e., if the effect of an action on rewards is “forgotten” after ≈ 1/(1 − γ) timesteps. If the reward function ˜r were obtained using Φ = V π,γ, we would have E [˜rt+l | st, at] = E [˜rt+l | st] = 0 for l > 0, i.e., the response function would only be nonzero at l = 0. Therefore, this shaping transformation would turn temporally extended response into an immediate response. Given that V π,γ completely reduces the temporal spread of the response function, we can hope that a good approximation V ≈ V π,γ partially reduces it. This observation suggests an interpretation of Equation (16): reshape the rewards using V to shrink the temporal extent of the response function, and then introduce a “steeper” discount γλ to cut off the noise arising from long delays, i.e., ignore terms ∇θ log πθ(at | st)δV  t+l where l (cid:29) 1/(1 − γλ).  ∞(cid:88)  6  Published as a conference paper at ICLR 2016  5 VALUE FUNCTION ESTIMATION  where ˆVt = (cid:80)∞  A variety of different methods can be used to estimate the value function (see, e.g., Bertsekas (2012)). When using a nonlinear function approximator to represent the value function, the sim- plest approach is to solve a nonlinear regression problem:  N(cid:88)  n=1  minimize  φ  (cid:107)Vφ(sn) − ˆVn(cid:107)2,  (28)  l=0 γlrt+l is the discounted sum of rewards, and n indexes over all timesteps in a batch of trajectories. This is sometimes called the Monte Carlo or TD(1) approach for estimating the value function (Sutton & Barto, 1998).2 For the experiments in this work, we used a trust region method to optimize the value function (cid:80)N in each iteration of a batch optimization procedure. The trust region helps us to avoid overﬁtting to the most recent batch of data. To formulate the trust region problem, we ﬁrst compute σ2 = n=1(cid:107)Vφold (sn) − ˆVn(cid:107)2, where φold is the parameter vector before optimization. Then we solve 1 N the following constrained optimization problem:  N(cid:88) N(cid:88)  n=1  1 N  (cid:107)Vφ(sn) − ˆVn(cid:107)2  (cid:107)Vφ(sn) − Vφold (sn)(cid:107)2  n=1  2σ2  minimize  φ  subject to  ≤ (cid:15).  (29)  This constraint is equivalent to constraining the average KL divergence between the previous value function and the new value function to be smaller than (cid:15), where the value function is taken to pa- rameterize a conditional Gaussian distribution with mean Vφ(s) and variance σ2. We compute an approximate solution to the trust region problem using the conjugate gradient algo- rithm (Wright & Nocedal, 1999). Speciﬁcally, we are solving the quadratic program  minimize  φ  subject to  gT (φ − φold)  N(cid:88)  n=1  1 N  (cid:80)  (φ − φold)T H(φ − φold) ≤ (cid:15).  (30)  n , where jn = ∇φVφ(sn). Note that where g is the gradient of the objective, and H = 1 N H is the “Gauss-Newton” approximation of the Hessian of the objective, and it is (up to a σ2 factor) the Fisher information matrix when interpreting the value function as a conditional probability dis- tribution. Using matrix-vector products v → Hv to implement the conjugate gradient algorithm, we compute a step direction s ≈ −H−1g. Then we rescale s → αs such that 1 2 (αs)T H(αs) = (cid:15) and take φ = φold + αs. This procedure is analogous to the procedure we use for updating the policy, which is described further in Section 6 and based on Schulman et al. (2015).  n jnjT  6 EXPERIMENTS  We designed a set of experiments to investigate the following questions: 1. What is the empirical effect of varying λ ∈ [0, 1] and γ ∈ [0, 1] when optimizing episodic total  reward using generalized advantage estimation?  2. Can generalized advantage estimation, along with trust region algorithms for policy and value function optimization, be used to optimize large neural network policies for challenging control problems? 2Another natural choice is to compute target values with an estimator based on the TD(λ) backup (Bertsekas, t = Vφold (sn)+ l=0(γλ)lδt+l. While we experimented with this choice, we did not notice a difference in performance from  2012; Sutton & Barto, 1998), mirroring the expression we use for policy gradient estimation: ˆV λ  (cid:80)∞  the λ = 1 estimator in Equation (28).  7  Published as a conference paper at ICLR 2016  6.1 POLICY OPTIMIZATION ALGORITHM  While generalized advantage estimation can be used along with a variety of different policy gra- dient methods, for these experiments, we performed the policy updates using trust region policy optimization (TRPO) (Schulman et al., 2015). TRPO updates the policy by approximately solving the following constrained optimization problem each iteration:  minimize  θ  subject to D  where Lθold (θ) =  θold  N(cid:88)  Lθold (θ) KL (πθold , πθ) ≤ (cid:15) πθ(an | sn) πθold(an | sn) N(cid:88)  1 N  n=1  ˆAn  1 N  n=1  D  θold KL (πθold , πθ) =  DKL(πθold (· | sn) (cid:107) πθ(· | sn))  (31)  As described in (Schulman et al., 2015), we approximately solve this problem by linearizing the objective and quadraticizing the constraint, which yields a step in the direction θ − θold ∝ −F −1g, where F is the average Fisher information matrix, and g is a policy gradient estimate. This policy update yields the same step direction as the natural policy gradient (Kakade, 2001a) and natural actor-critic (Peters & Schaal, 2008), however it uses a different stepsize determination scheme and numerical procedure for computing the step. Since prior work (Schulman et al., 2015) compared TRPO to a variety of different policy optimiza- tion algorithms, we will not repeat these comparisons; rather, we will focus on varying the γ, λ parameters of policy gradient estimator while keeping the underlying algorithm ﬁxed. For completeness, the whole algorithm for iteratively updating policy and value function is given below:  Initialize policy parameter θ0 and value function parameter φ0. for i = 0, 1, 2, . . . do  Simulate current policy πθi until N timesteps are obtained. Compute δV  t at all timesteps t ∈ {1, 2, . . . , N}, using V = Vφi.  Compute ˆAt =(cid:80)∞  t+l at all timesteps. Compute θi+1 with TRPO update, Equation (31). Compute φi+1 with Equation (30).  l=0(γλ)lδV  end for  Note that the policy update θi → θi+1 is performed using the value function Vφi for advantage estimation, not Vφi+1. Additional bias would have been introduced if we updated the value function ﬁrst. To see this, consider the extreme case where we overﬁt the value function, and the Bellman residual rt + γV (st+1) − V (st) becomes zero at all timesteps—the policy gradient estimate would be zero.  6.2 EXPERIMENTAL SETUP  We evaluated our approach on the classic cart-pole balancing problem, as well as several challenging 3D locomotion tasks: (1) bipedal locomotion; (2) quadrupedal locomotion; (3) dynamically standing up, for the biped, which starts off laying on its back. The models are shown in Figure 1.  6.2.1 ARCHITECTURE  We used the same neural network architecture for all of the 3D robot tasks, which was a feedforward network with three hidden layers, with 100, 50 and 25 tanh units respectively. The same architecture was used for the policy and value function. The ﬁnal output layer had linear activation. The value function estimator used the same architecture, but with only one scalar output. For the simpler cart- pole task, we used a linear policy, and a neural network with one 20-unit hidden layer as the value function.  8  Published as a conference paper at ICLR 2016  Figure 1: Top ﬁgures: robot models used for 3D locomotion. Bottom ﬁgures: a sequence of frames from the learned gaits. Videos are available at https://sites.google.com/site/ gaepapersupp.  6.2.2 TASK DETAILS  For the cart-pole balancing task, we collected 20 trajectories per batch, with a maximum length of 1000 timesteps, using the physical parameters from Barto et al. (1983). The simulated robot tasks were simulated using the MuJoCo physics engine (Todorov et al., 2012). The humanoid model has 33 state dimensions and 10 actuated degrees of freedom, while the quadruped model has 29 state dimensions and 8 actuated degrees of freedom. The initial state for these tasks consisted of a uniform distribution centered on a reference conﬁguration. We used 50000 timesteps per batch for bipedal locomotion, and 200000 timesteps per batch for quadrupedal locomotion and bipedal standing. Each episode was terminated after 2000 timesteps if the robot had not reached a terminal state beforehand. The timestep was 0.01 seconds. The reward functions are provided in the table below.  Task  3D biped locomotion Quadruped locomotion  Biped getting up  Reward  vfwd − 10−5(cid:107)u(cid:107)2 − 10−5(cid:107)fimpact(cid:107)2 + 0.2 vfwd − 10−6(cid:107)u(cid:107)2 − 10−3(cid:107)fimpact(cid:107)2 + 0.05  −(hhead − 1.5)2 − 10−5(cid:107)u(cid:107)2  Here, vfwd := forward velocity, u := vector of joint torques, fimpact := impact forces, hhead := height of the head. In the locomotion tasks, the episode is terminated if the center of mass of the actor falls below a predeﬁned height: .8 m for the biped, and .2 m for the quadruped. The constant offset in the reward function encourages longer episodes; otherwise the quadratic reward terms might lead lead to a policy that ends the episodes as quickly as possible.  6.3 EXPERIMENTAL RESULTS  All results are presented in terms of the cost, which is deﬁned as negative reward and is mini- mized. Videos of the learned policies are available at https://sites.google.com/site/ gaepapersupp. In plots, “No VF” means that we used a time-dependent baseline that did not depend on the state, rather than an estimate of the state value function. The time-dependent baseline was computed by averaging the return at each timestep over the trajectories in the batch.  6.3.1 CART-POLE  The results are averaged across 21 experiments with different random seeds. Results are shown in Figure 2, and indicate that the best results are obtained at intermediate values of the parameters: γ ∈ [0.96, 0.99] and λ ∈ [0.92, 0.99].  9  Published as a conference paper at ICLR 2016  Figure 2: Left: learning curves for cart-pole task, using generalized advantage estimation with varying values of λ at γ = 0.99. The fastest policy improvement is obtain by intermediate values of λ in the range [0.92, 0.98]. Right: performance after 20 iterations of policy optimization, as γ and λ are varied. White means higher reward. The best results are obtained at intermediate values of both.  Figure 3: Left: Learning curves for 3D bipedal locomotion, averaged across nine runs of the algo- rithm. Right: learning curves for 3D quadrupedal locomotion, averaged across ﬁve runs.  6.3.2  3D BIPEDAL LOCOMOTION  Each trial took about 2 hours to run on a 16-core machine, where the simulation rollouts were paral- lelized, as were the function, gradient, and matrix-vector-product evaluations used when optimizing the policy and value function. Here, the results are averaged across 9 trials with different random seeds. The best performance is again obtained using intermediate values of γ ∈ [0.99, 0.995], λ ∈ [0.96, 0.99]. The result after 1000 iterations is a fast, smooth, and stable gait that is effectively completely stable. We can compute how much “real time” was used for this learning process: 0.01 seconds/timestep×50000 timesteps/batch×1000 batches/3600·24 seconds/day = 5.8 days. Hence, it is plausible that this algorithm could be run on a real robot, or multiple real robots learning in par- allel, if there were a way to reset the state of the robot and ensure that it doesn’t damage itself.  6.3.3 OTHER 3D ROBOT TASKS  The other two motor behaviors considered are quadrupedal locomotion and getting up off the ground for the 3D biped. Again, we performed 5 trials per experimental condition, with different random seeds (and initializations). The experiments took about 4 hours per trial on a 32-core machine. We performed a more limited comparison on these domains (due to the substantial computational resources required to run these experiments), ﬁxing γ = 0.995 but varying λ = {0, 0.96}, as well as an experimental condition with no value function. For quadrupedal locomotion, the best results are obtained using a value function with λ = 0.96 Section 6.3.2. For 3D standing, the value function always helped, but the results are roughly the same for λ = 0.96 and λ = 1.  10  01020304050number of policy iterations1086420costCart-pole learning curves (at γ=0.99)No VFλ=1.0λ=0.99λ=0.98λ=0.96λ=0.92λ=0.84λ=0.68λ=0.36λ=00100200300400500number of policy iterations2.52.01.51.00.50.0cost3D Bipedγ=0.96,λ=0.96γ=0.98,λ=0.96γ=0.99,λ=0.96γ=0.995,λ=0.92γ=0.995,λ=0.96γ=0.995,λ=0.98γ=0.995,λ=0.99γ=0.995,λ=1.0γ=1,λ=0.96γ=1, No value fn02004006008001000number of policy iterations1210864202cost3D Quadrupedγ=0.995, No value fnγ=0.995,λ=1γ=0.995,λ=0.96Published as a conference paper at ICLR 2016  Figure 4: (a) Learning curve from quadrupedal walking, (b) learning curve for 3D standing up, (c) clips from 3D standing up.  7 DISCUSSION  Policy gradient methods provide a way to reduce reinforcement learning to stochastic gradient de- scent, by providing unbiased gradient estimates. However, so far their success at solving difﬁcult control problems has been limited, largely due to their high sample complexity. We have argued that the key to variance reduction is to obtain good estimates of the advantage function. We have provided an intuitive but informal analysis of the problem of advantage function estimation, and justiﬁed the generalized advantage estimator, which has two parameters γ, λ which adjust the bias-variance tradeoff. We described how to combine this idea with trust region policy optimization and a trust region algorithm that optimizes a value function, both represented by neural networks. Combining these techniques, we are able to learn to solve difﬁcult control tasks that have previously been out of reach for generic reinforcement learning methods. Our main experimental validation of generalized advantage estimation is in the domain of simulated robotic locomotion. As shown in our experiments, choosing an appropriate intermediate value of λ in the range [0.9, 0.99] usually results in the best performance. A possible topic for future work is how to adjust the estimator parameters γ, λ in an adaptive or automatic way. One question that merits future investigation is the relationship between value function estimation error and policy gradient estimation error. If this relationship were known, we could choose an error metric for value function ﬁtting that is well-matched to the quantity of interest, which is typically the accuracy of the policy gradient estimation. Some candidates for such an error metric might include the Bellman error or projected Bellman error, as described in Bhatnagar et al. (2009). Another enticing possibility is to use a shared function approximation architecture for the policy and the value function, while optimizing the policy using generalized advantage estimation. While for- mulating this problem in a way that is suitable for numerical optimization and provides convergence guarantees remains an open question, such an approach could allow the value function and policy representations to share useful features of the input, resulting in even faster learning. In concurrent work, researchers have been developing policy gradient methods that involve differen- tiation with respect to the continuous-valued action (Lillicrap et al., 2015; Heess et al., 2015). While we found empirically that the one-step return (λ = 0) leads to excessive bias and poor performance, these papers show that such methods can work when tuned appropriately. However, note that those papers consider control problems with substantially lower-dimensional state and action spaces than the ones considered here. A comparison between both classes of approach would be useful for future work.  ACKNOWLEDGEMENTS  We thank Emo Todorov for providing the simulator as well as insightful discussions, and we thank Greg Wayne, Yuval Tassa, Dave Silver, Carlos Florensa Campo, and Greg Brockman for insightful discussions. This research was funded in part by the Ofﬁce of Naval Research through a Young  11  0100200300400500number of policy iterations0.00.51.01.52.02.5cost3D Standing Upγ=0.99, No value fnγ=0.99,λ=1γ=0.99,λ=0.96Published as a conference paper at ICLR 2016  Investigator Award and under grant number N00014-11-1-0688, DARPA through a Young Faculty Award, by the Army Research Ofﬁce through the MAST program.  A FREQUENTLY ASKED QUESTIONS  A.1 WHAT’S THE RELATIONSHIP WITH COMPATIBLE FEATURES?  Compatible features are often mentioned in relation to policy gradient algorithms that make use of a value function, and the idea was proposed in the paper On Actor-Critic Methods by Konda & Tsitsiklis (2003). These authors pointed out that due to the limited representation power of the policy, the policy gradient only depends on a certain subspace of the space of advantage functions. This subspace is spanned by the compatible features ∇θi log πθ(at|st), where i ∈ {1, 2, . . . , dim θ}. This theory of compatible features provides no guidance on how to exploit the temporal structure of the problem to obtain better estimates of the advantage function, making it mostly orthogonal to the ideas in this paper. The idea of compatible features motivates an elegant method for computing the natural policy gradi- ent (Kakade, 2001a; Peters & Schaal, 2008). Given an empirical estimate of the advantage function ˆAt at each timestep, we can project it onto the subspace of compatible features by solving the fol- lowing least squares problem:  minimize  r  (cid:107)r · ∇θ log πθ(at | st) − ˆAt(cid:107)2.  (32)  (cid:88)  t  If ˆA is γ-just, the least squares solution is the natural policy gradient (Kakade, 2001a). Note that any estimator of the advantage function can be substituted into this formula, including the ones we derive in this paper. For our experiments, we also compute natural policy gradient steps, but we use the more computationally efﬁcient numerical procedure from Schulman et al. (2015), as discussed in Section 6.  A.2 WHY DON’T YOU JUST USE A Q-FUNCTION?  Previous actor critic methods, e.g. in Konda & Tsitsiklis (2003), use a Q-function to obtain poten- tially low-variance policy gradient estimates. Recent papers, including Heess et al. (2015); Lillicrap et al. (2015), have shown that a neural network Q-function approximator can used effectively in a policy gradient method. However, there are several advantages to using a state-value function in the manner of this paper. First, the state-value function has a lower-dimensional input and is thus easier to learn than a state-action value function. Second, the method of this paper allows us to smoothly interpolate between the high-bias estimator (λ = 0) and the low-bias estimator (λ = 1). On the other hand, using a parameterized Q-function only allows us to use a high-bias estimator. We have found that the bias is prohibitively large when using a one-step estimate of the returns, i.e., the λ = 0 esti- t = rt + γV (st+1) − V (st). We expect that similar difﬁculty would be encountered mator, ˆAt = δV when using an advantage estimator involving a parameterized Q-function, ˆAt = Q(s, a) − V (s). There is an interesting space of possible algorithms that would use a parameterized Q-function and attempt to reduce bias, however, an exploration of these possibilities is beyond the scope of this work.  B PROOFS  Proof of Proposition 1: First we can split the expectation into terms involving Q and b,  Es0:∞,a0:∞ [∇θ log πθ(at | st)(Qt(s0:∞, a0:∞) − bt(s0:t, a0:t−1))]  = Es0:∞,a0:∞ [∇θ log πθ(at | st)(Qt(s0:∞, a0:∞))] − Es0:∞,a0:∞ [∇θ log πθ(at | st)(bt(s0:t, a0:t−1))]  (33)  12  Published as a conference paper at ICLR 2016  We’ll consider the terms with Q and b in turn.  Es0:∞,a0:∞ [∇θ log πθ(at | st)Qt(s0:∞, a0:∞)]  (cid:2)Est+1:∞,at+1:∞ [∇θ log πθ(at | st)Qt(s0:∞, a0:∞)](cid:3) (cid:2)∇θ log πθ(at | st)Est+1:∞,at+1:∞ [Qt(s0:∞, a0:∞)](cid:3)  = Es0:t,a0:t = Es0:t,a0:t = Es0:t,a0:t−1 [∇θ log πθ(at | st)Aπ(st, at)]  Next,  Es0:∞,a0:∞ [∇θ log πθ(at | st)bt(s0:t, a0:t−1)]  (cid:2)Est+1:∞,at:∞ [∇θ log πθ(at | st)bt(s0:t, a0:t−1)](cid:3) (cid:2)Est+1:∞,at:∞ [∇θ log πθ(at | st)] bt(s0:t, a0:t−1)(cid:3)  = Es0:t,a0:t−1 = Es0:t,a0:t−1 = Es0:t,a0:t−1 [0 · bt(s0:t, a0:t−1)] = 0.  REFERENCES Barto, Andrew G, Sutton, Richard S, and Anderson, Charles W. Neuronlike adaptive elements that can solve difﬁcult learning control problems. Systems, Man and Cybernetics, IEEE Transactions on, (5):834–846, 1983.  Baxter, Jonathan and Bartlett, Peter L. Reinforcement learning in POMDPs via direct gradient ascent. In ICML,  pp. 41–48, 2000.  Bertsekas, Dimitri P. Dynamic programming and optimal control, volume 2. Athena Scientiﬁc, 2012.  Bhatnagar, Shalabh, Precup, Doina, Silver, David, Sutton, Richard S, Maei, Hamid R, and Szepesv´ari, Csaba. In Advances in  Convergent temporal-difference learning with arbitrary smooth function approximation. Neural Information Processing Systems, pp. 1204–1212, 2009.  Greensmith, Evan, Bartlett, Peter L, and Baxter, Jonathan. Variance reduction techniques for gradient estimates  in reinforcement learning. The Journal of Machine Learning Research, 5:1471–1530, 2004.  Hafner, Roland and Riedmiller, Martin. Reinforcement learning in feedback control. Machine learning, 84  (1-2):137–169, 2011.  Heess, Nicolas, Wayne, Greg, Silver, David, Lillicrap, Timothy, Tassa, Yuval, and Erez, Tom. Learning contin-  uous control policies by stochastic value gradients. arXiv preprint arXiv:1510.09142, 2015.  Hull, Clark. Principles of behavior. 1943.  Kakade, Sham. A natural policy gradient. In NIPS, volume 14, pp. 1531–1538, 2001a.  Kakade, Sham. Optimizing average reward using discounted rewards. In Computational Learning Theory, pp.  605–615. Springer, 2001b.  Kimura, Hajime and Kobayashi, Shigenobu. An analysis of actor/critic algorithms using eligibility traces:  Reinforcement learning with imperfect value function. In ICML, pp. 278–286, 1998.  Konda, Vijay R and Tsitsiklis, John N. On actor-critic algorithms. SIAM journal on Control and Optimization,  42(4):1143–1166, 2003.  Lillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander, Heess, Nicolas, Erez, Tom, Tassa, Yuval, Sil- ver, David, and Wierstra, Daan. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.  Marbach, Peter and Tsitsiklis, John N. Approximate gradient methods in policy-space optimization of markov  reward processes. Discrete Event Dynamic Systems, 13(1-2):111–148, 2003.  Minsky, Marvin. Steps toward artiﬁcial intelligence. Proceedings of the IRE, 49(1):8–30, 1961.  Ng, Andrew Y, Harada, Daishi, and Russell, Stuart. Policy invariance under reward transformations: Theory  and application to reward shaping. In ICML, volume 99, pp. 278–287, 1999.  Peters, Jan and Schaal, Stefan. Natural actor-critic. Neurocomputing, 71(7):1180–1190, 2008.  13  Published as a conference paper at ICLR 2016  Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan, Michael I, and Abbeel, Pieter. Trust region policy  optimization. arXiv preprint arXiv:1502.05477, 2015.  Sutton, Richard S and Barto, Andrew G. Introduction to reinforcement learning. MIT Press, 1998.  Sutton, Richard S, McAllester, David A, Singh, Satinder P, and Mansour, Yishay. Policy gradient methods for  reinforcement learning with function approximation. In NIPS, volume 99, pp. 1057–1063. Citeseer, 1999.  Thomas, Philip. Bias in natural actor-critic algorithms. In Proceedings of The 31st International Conference  on Machine Learning, pp. 441–448, 2014.  Todorov, Emanuel, Erez, Tom, and Tassa, Yuval. Mujoco: A physics engine for model-based control.  In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026–5033. IEEE, 2012.  Wawrzy´nski, Paweł. Real-time reinforcement learning by sequential actor–critics and experience replay. Neural  Networks, 22(10):1484–1497, 2009.  Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement learning.  Machine learning, 8(3-4):229–256, 1992.  Wright, Stephen J and Nocedal, Jorge. Numerical optimization. Springer New York, 1999.  14  ",
1511.05440,2016,Deep Multi Scale Video Prediction Beyond Mean Square Error,"['Deep Multi Scale Video Prediction Beyond Mean Square Error\nMichael Mathieu', 'camille couprie', 'Yann Lecun']",https://arxiv.org/pdf/1511.05440,"6 1 0 2     b e F 6 2         ]  G L . s c [      6 v 0 4 4 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  DEEP MULTI-SCALE VIDEO PREDICTION BEYOND MEAN SQUARE ERROR  Michael Mathieu1, 2, Camille Couprie2 & Yann LeCun1, 2 1New York University 2Facebook Artiﬁcial Intelligence Research mathieu@cs.nyu.edu, {coupriec,yann}@fb.com  ABSTRACT  Learning to predict future images from a video sequence involves the construc- tion of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical ﬂow has been a very studied problem in com- puter vision for a long time, future frame prediction is rarely approached. Still, many vision applications could beneﬁt from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectory. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversar- ial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset.  1  INTRODUCTION  Unsupervised feature learning of video representations is a promising direction of research because the resources are quasi-unlimited and the progress remaining to achieve in this area are quite impor- tant. In this paper, we address the problem of frame prediction. A signiﬁcant difference with the more classical problem of image reconstruction (Vincent et al., 2008; Le, 2013) is that the ability of a model to predict future frames requires to build accurate, non trivial internal representations, even in the absence of other constraints (such as sparsity). Therefore, we postulate that the better the predictions of such system are, the better the feature representation should be. Indeed, the work of Srivastava et al. (2015) demonstrates that learning representations by predicting the next sequence of image features improves classiﬁcation results on two action recognition datasets. In this work, however, we focus on predicting directly in pixel space and try to address the inherent problems related to this approach. Top performing algorithms for action recognition exploit the temporal information in a supervised way, such as the 3D convolutional network of Tran et al. (2015), or the spatio-temporal convolutional model of Simonyan & Zisserman (2014), which can require months of training, and heavily labeled datasets. This could be reduced using unsupervised learning. The authors in (Wang & Gupta, 2015) compete with supervised learning performance on ImageNet, by using a siamese architecture (Bromley et al., 1993) to mine positive and negative examples from patch triplets of videos in an unsupervised fashion. Unsupervised learning from video is also exploited in the work of Vondrick et al. (2015), where a convolutional model is trained to predict sets of future possible actions, or in (Jayaraman & Grauman, 2015) which focuses on learning a feature space equivariant to ego-motion. Goroshin et al. (2015) trained a convolutional network to learn to linearize motion in the code space and tested it on the NORB dataset. Beside unsupervised learning, a video predictive system may ﬁnd applications in robotics (Kosaka & Kak, 1992), video compression (Ascenso et al., 2005) and inpainting (Flynn et al., 2015) to name a few.  1  Published as a conference paper at ICLR 2016  Recently, predicting future video sequences appeared in different settings: Ranzato et al. (2014) deﬁned a recurrent network architecture inspired from language modeling, predicting the frames in a discrete space of patch clusters. Srivastava et al. (2015) adapted a LSTM model (Hochreiter & Schmidhuber, 1997) to future frame prediction. Oh et al. (2015) deﬁned an action conditional auto- encoder model to predict next frames of Atari-like games. In the two works dealing with natural images, a blur effect is observed in the predictions, due to different causes. In (Ranzato et al., 2014), the transformation back and forth between pixel and clustered spaces involves the averaging of 64 predictions of overlapping tilings of the image, in order to avoid a blockiness effect in the result. Short term results from Srivastava et al. (2015) are less blurry, however the (cid:96)2 loss function inherently produces blurry results. Indeed, using the (cid:96)2 loss comes from the assumption that the data is drawn from a Gaussian distribution, and works poorly with multimodal distributions. In this work, we address the problem of lack of sharpness in the predictions. We assess different loss functions, show that generative adversarial training (Goodfellow et al., 2014; Denton et al., 2015) may be successfully employed for next frame prediction, and ﬁnally introduce a new loss based on the image gradients, designed to preserve the sharpness of the frames. Combining these two losses produces the most visually satisfying results. Our paper is organised as follows: the model section describes the different model architectures: simple, multi-scale, adversarial, and presents our gradient difference loss function. The experimen- tal section compares the proposed architectures and losses on video sequences from the Sports1m dataset of Karpathy et al. (2014) and UCF101 (Soomro et al., 2012). We further compare our results with (Srivastava et al., 2015) and (Ranzato et al., 2014). We measure the quality of image generation by computing similarity and sharpness measures.  2 MODELS Let Y = {Y 1, ..., Y n} be a sequence of frames to predict from input frames X = {X 1, ..., X m} in a video sequence. Our approach is based on a convolutional network (LeCun et al., 1998), alternating convolutions and Rectiﬁed Linear Units (ReLU) (Nair & Hinton, 2010).  Figure 1: A basic next frame prediction convnet  Input X  First  feature map  Second  feature map  Third  feature map  Fourth  feature map  Fifth  Output feature map G(X)  conv. ReLU conv. ReLU conv. ReLU conv. ReLU conv. Tanh  Such a network G, displayed in Figure 1, can be trained to predict one or several concatenated frames Y from the concatenated frames X by minimizing a distance, for instance (cid:96)p with p = 1 or p = 2, between the predicted frame and the true frame:  Lp(X, Y ) = (cid:96)p(G(X), Y ) = (cid:107)G(X) − Y (cid:107)p p,  (1)  However, such a network has at least two major ﬂaws: 1. Convolutions only account for short-range dependencies, limited by the size of their kernels. However, using pooling would only be part of the solution since the output has to be of the same resolution as the input. There are a number of ways to avoid the loss of resolution brought about by pooling/subsampling while preserving long-range dependencies. The simplest and oldest one is to have no pooling/subsampling but many convolution layers (Jain et al., 2007). Another method is to use connections that “skip” the pooling/unpooling pairs, to preserve the high frequency information (Long et al., 2015; Dosovitskiy et al., 2015; Ronneberger et al., 2015). Finally, we can combine multiple scales linearly as in the reconstruction process of a Laplacian pyramid (Denton et al., 2015). This is the approach we use in this paper.  2  Published as a conference paper at ICLR 2016  2. Using an (cid:96)2 loss, and to a lesser extent (cid:96)1, produces blurry predictions, increasingly worse when predicting further in the future. If the probability distribution for an output pixel has two equally likely modes v1 and v2, the value vavg = (v1 + v2)/2 minimizes the (cid:96)2 loss over the data, even if vavg has very low probability. In the case of an (cid:96)1 norm, this effect diminishes, but do not disappear, as the output value would be the median of the set of equally likely values.  2.1 MULTI-SCALE NETWORK  We tackle Problem 1 by making the model multi-scale. A multi-scale version of the model is de- ﬁned as follows: Let s1, . . . , sNscales be the sizes of the inputs of our network. Typically, in our experiments, we set s1 = 4 × 4, s2 = 8 × 8, s3 = 16 × 16 and s4 = 32 × 32. Let uk be the upscaling operator toward size sk. Let X i k denote the downscaled versions of X i and Y i of size k be a network that learns to predict Yk − uk(Yk−1) from Xk and a coarse guess of Yk. sk, and G(cid:48) We recursively deﬁne the network Gk, that makes a prediction ˆYk of size sk, by  k, Y i  (cid:16)  (cid:17)  ˆYk = Gk(X) = uk( ˆYk−1) + G(cid:48)  k  Xk, uk( ˆYk−1)  .  (2)  Therefore, the network makes a series of predictions, starting from the lowest resolution, and uses the prediction of size sk as a starting point to make the prediction of size sk+1. At the lowest scale s1, the network takes only X1 as an input. This architecture is illustrated on Figure 2, and the speciﬁc details are given in Section 3. The set of trainable parameters is denoted WG and the minimization is performed via Stochastic Gradient Descent (SGD).  Figure 2: Multi-scale architecture  Despite the multi-scale architecture, the search of Y from X without making any assumption on the space of possible conﬁgurations still leads to blurry predictions, because of Problem 2. In order to further reduce this effect, the next two sections introduce an adversarial strategy and the image gradient difference loss.  2.2 ADVERSARIAL TRAINING  Generative adversarial networks were introduced by Goodfellow et al. (2014), where images patches are generated from random noise using two networks trained simultaneously. In that work, the au- thors propose to use a discriminative network D to estimate the probability that a sample comes from the dataset instead of being produced by a generative model G. The two models are simulta- neously trained so that G learns to generate frames that are hard to classify by D, while D learns to discriminate the frames generated by G. Ideally, when G is trained, it should not be possible for D to perform better than chance. We adapted this approach for the purpose of frame prediction, which constitutes to our knowledge the ﬁrst application of adversarial training to video prediction. The generative model G is typically the one described in the previous section. The discriminative model D takes a sequence of frames, and is trained to predict the probability that the last frames of the sequence are generated by G. Note only the last frames are either real of generated by G, the rest of the sequence is always from the dataset. This allows the discriminative model to make use of temporal information, so that G learns to produce sequences that are temporally coherent with its input. Since G is conditioned on the input frames X, there is variability in the input of the generator even in the absence of noise, so noise is  3  Published as a conference paper at ICLR 2016  not a necessity anymore. We trained the network with and without adding noise and did not observe any difference. The results we present are obtained without random noise. Our main intuition on why to use an adversarial loss is that it can, theoretically, address the Problem 2 mentioned in Section 2. Imagine a sequence of frames X = (X 1, . . . , X m) for which, in the dataset, the next frames can either be Y = (Y 1, . . . , Y n) or Y (cid:48) = (Y (cid:48)1, . . . , Y (cid:48)n), with equal probability. As explained before, training the network with an (cid:96)2 loss will result in predicting the average frames Yavg = (Y + Y (cid:48))/2. However, the sequence (X, Yavg), composed of the frames of X followed by the frames of Yavg, is not a likely sequence, and D can discriminate them easily. The only sequences the model D will not be able to classify as fake are (X, Y ) and (X, Y (cid:48)). The discriminative model D is a multi-scale convolutional network with a single scalar output. The training of the pair (G, D) consists of two alternated steps, described below. For the sake of clarity, we assume that we use pure SGD (minibatches of size 1), but there is no difﬁculty to generalize the algorithm to minibatches of size M by summing the losses over the samples.  Training D: Let (X, Y ) be a sample from the dataset. Note that X (respectively Y ) is a sequence of m (respectively n) frames. We train D to classify the input (X, Y ) into class 1 and the input (X, G(X)) into class 0. More precisely, for each scale k, we perform one SGD iteration of Dk while keeping the weights of G ﬁxed. It is trained with in the target 1 for the datapoint (Xk, Yk), and the target 0 for (Xk, Gk(Xk)). Therefore, the loss function we use to train D is  Nscales(cid:88)  LD adv(X, Y ) =  k=1  Lbce(Y, ˆY ) = −(cid:88)  (3)  (4)  Lbce(Dk(Xk, Yk), 1) + Lbce(Dk(Xk, Gk(X)), 0)  where Lbce is the binary cross-entropy loss, deﬁned as  ˆYi log (Yi) + (1 − ˆYi) log (1 − Yi)  where Yi takes its values in {0, 1} and ˆYi in [0, 1].  i  Nscales(cid:88)  Training G: Let (X, Y ) be a different data sample. While keeping the weights of D ﬁxed, we perform one SGD step on G to minimize the adversarial loss:  LG adv(X, Y ) =  Lbce(Dk(Xk, Gk(Xk)), 1)  (5)  k=1  Minimizing this loss means that the generative model G is making the discriminative model D as “confused” as possible, in the sense that D will not discriminate the prediction correctly. However, in practice, minimizing this loss alone can lead to instability. G can always generate samples that “confuse” D, without being close to Y . In turn, D will learn to discriminate these samples, leading G to generate other “confusing” samples, and so on. To address this problem, we train the generator with a combined loss composed of the of the adversarial loss and the Lp loss . The generator G is therefore trained to minimize λadvLG adv + λ(cid:96)pLp. There is therefore a tradeoff to adjust, by the mean of the λadv and λ(cid:96)p parameters, between sharp predictions due to the adversarial principle, and similarity with the ground truth brought by the second term. This process is summarized in Algorithm 1, with minibatches of size M.  2.3  IMAGE GRADIENT DIFFERENCE LOSS (GDL)  Another strategy to sharpen the image prediction is to directly penalize the differences of image gradient predictions in the generative loss function. We deﬁne a new loss function, the Gradient Difference Loss (GDL), that can be combined with a (cid:96)p and/or adversarial loss function. The GDL function between the ground truth image Y , and the prediction G(X) = ˆY is given by  Lgdl(X, Y ) = Lgdl( ˆY , Y ) =  (cid:88)  (cid:12)(cid:12)|Yi,j − Yi−1,j| − | ˆYi,j − ˆYi−1,j|(cid:12)(cid:12)α  +(cid:12)(cid:12)|Yi,j−1 − Yi,j| − | ˆYi,j−1 − ˆYi,j|(cid:12)(cid:12)α  ,  (6)  i,j  4  Published as a conference paper at ICLR 2016  Algorithm 1: Training adversarial networks for next frame generation Set the learning rates ρD and ρG, and weights λadv, λ(cid:96)p. while not converged do  Update the discriminator D: Get M data samples (X, Y ) = (X (1), Y (1)), . . . , (X (M ), Y (M )) WD = WD − ρD Update the generator G: Get M new data samples (X, Y ) = (X (1), Y (1)), . . . , (X (M ), Y (M )) WG = WG − ρG  (cid:80)M (cid:80)M  ∂L(cid:96)p (X (i),Y (i))  adv(X (i),Y (i))  adv(X (i),Y (i))  (cid:16)  (cid:17)  λadv  ∂LD  ∂LG  ∂WD  + λ(cid:96)p  i=1  i=1  ∂WG  ∂WG  Table 1: Network architecture (Input: 4 frames – output: 1 frame)  Generative network scales Number of feature maps  Conv. kernel size  Adversarial network scales Number of feature maps  Conv. kernel size (no padding)  Fully connected  G1  3, 3, 3, 3  D1 64 3  512, 256  128, 256, 128  128, 256, 128  128, 256, 512, 256, 128  128, 256, 512, 256, 128  5, 3, 3, 3, 5  7, 5, 5, 5, 5, 7  G2  5, 3, 3, 5  G3  G4  D4  D2  64, 128, 128  3, 3, 3  1024, 512  D3  128, 256, 256  5, 5, 5  1024, 512  128, 256, 512, 128  7, 7, 5, 5 1024, 512  where α is an integer greater or equal to 1, and |.| denotes the absolute value function. To the best of our knowledge, the closest related work to this idea is the work of Mahendran & Vedaldi (2015), using a total variation regularization to generate images from learned features. Our GDL is funda- mentally different: In (Mahendran & Vedaldi, 2015), the total variation takes only the reconstructed frame in input, whereas our loss penalises gradient differences between the prediction and the true output. Second, we chose the simplest possible image gradient by considering the neighbor pixel intensities differences, rather than adopting a more sophisticated norm on a larger neighborhood, for the sake of keeping the training time low.  2.4 COMBINING LOSSES  In our experiments, we combine the losses previously deﬁned with different weights. The ﬁnal loss is:  L(X, Y ) = λadvLG  adv(X, Y ) + λ(cid:96)pLp(X, Y ) + λgdlLgdl(X, Y )  (7)  3 EXPERIMENTS  We now provide a quantitative evaluation of the quality of our video predictions on UCF101 (Soomro et al., 2012) and Sports1m (Karpathy et al., 2014) video clips. We train and compare two conﬁg- urations: (1) We use 4 input frames to predict one future frame. In order to generate further in the future, we apply the model recursively by using the newly generated frame as an input. (2) We use 8 input frames to produce 8 frames simultaneously. This second conﬁguration represents a signiﬁcantly harder problem and is presented in Appendix.  3.1 DATASETS  We use the Sports1m for the training, because most of UCF101 frames only have a very small portion of the image actually moving, while the rest is just a ﬁxed background. We train our network by randomly selecting temporal sequences of patches of 32 × 32 pixels after making sure they show enough movement (quantiﬁed by the (cid:96)2 difference between the frames). The data patches are ﬁrst normalized so that their values are comprised between -1 and 1.  5  Published as a conference paper at ICLR 2016  3.2 NETWORK ARCHITECTURE  We present results for several models. Unless otherwise stated, we employed mutliscale architec- tures. Our baseline models are using (cid:96)1 and (cid:96)2 losses. The GDL-(cid:96)1 (respectively GDL-(cid:96)2) model is using a combination of the GDL with α = 1 (respectively α = 2) and p = 1 (respectively p = 2) loss; the relative weights λgdl and λ(cid:96)p are both 1. The adversarial (Adv) model uses the adversarial loss, with p = 2 weighted by λadv = 0.05 and λ(cid:96)p = 1. Finally, the Adv+GDL model is a com- bination or the adversarial loss and the GDL, with the same parameters as for Adv with α = 1 and λgdl = 1.  Generative model training: The generative model G architecture is presented in Table 1. It contains padded convolutions interlaced with ReLU non linearities. A Hyperbolic tangent (Tanh) is added at the end of the model to ensure that the output values are between -1 and 1. The learning rate ρG starts at 0.04 and is reduced over time to 0.005. The minibatch size is set to 4, or 8 in the case of the adversarial training, to take advantage of GPU hardware capabilities. We train the network on small patches, and since it is fully convolutional, we can seamlessly apply it on larger images at test time.  Adversarial training: The discriminative model D, also presented in Table 1, uses standard non padded convolutions followed by fully connected layers and ReLU non linearities. For the largest scale s4, a 2 × 2 pooling is added after the convolutions. The network is trained by setting the learning rate ρD to 0.02.  3.3 QUANTITATIVE EVALUATIONS  To evaluate the quality of the image predictions resulting from the different tested systems, we compute the Peak Signal to Noise Ratio (PSNR) between the true frame Y and the prediction ˆY :  PSNR(Y, ˆY ) = 10 log10  1 N  (cid:80)N max2 ˆY i=0(Yi − ˆYi)2  ,  (8)  where max ˆY is the maximum possible value of the image intensities. We also provide the Structural Similarity Index Measure (SSIM) of Wang et al. (2004). It ranges between -1 and 1, a larger score meaning a greater similarity between the two images. To measure the loss of sharpness between the true frame and the prediction, we deﬁne the following sharpness measure based on the difference of gradients between two images Y and ˆY :  Sharp. diff.(Y, ˆY ) = 10 log10  where ∇iY = |Yi,j − Yi−1,j| and ∇jY = |Yi,j − Yi,j−1|.  (cid:16)(cid:80)  i  (cid:80)  j |(∇iY + ∇jY ) − (∇i ˆY + ∇j ˆY )|(cid:17) .  max2 ˆY  1 N  (9)  Figure 3: Our evaluation of the accuracy of future frames prediction only takes the moving areas of the images into account. Left: example of our frame predictions in a entire image with ground truth; Right: images masked with thresholded optical ﬂow.  Target image 1  Target image 2  Adv+GDL Adv+GDL Masked Masked Masked Masked Pred. 2  Target 1 Target 2  Pred. 1  Pred. 2  Pred. 1  As for the other measures, a larger score is better. These quantitative measures on 378 test videos from UCF1011 are given in Table 2. As it is trivial to predict pixel values in static areas, especially on  1We extracted from the test set list video ﬁles every 10 videos, starting at 1, 11, 21 etc.  6  Published as a conference paper at ICLR 2016  Table 2: Comparison of the accuracy of the predictions on 10% of the UCF101 test images. The different models have been trained given 4 frames to predict the next one. Similarity and sharpness measures evaluated only in the areas of movement. Our best model has been ﬁne-tuned on UCF101 after the training on Sports1m.  single sc. (cid:96)2  (cid:96)2 (cid:96)1  GDL (cid:96)1 GDL (cid:96)∗ Adv∗ 1 Adv+GDL∗  Last input Optical ﬂow  Adv+GDL ﬁne-tuned ∗  1st frame prediction scores Sharpness  Similarity  2nd frame prediction scores Sharpness  Similarity  PSNR SSIM 26.5 0.84 0.86 27.6 0.88 28.7 0.90 29.4 0.90 29.9 30.6 0.89 0.91 31.5 32.0 0.92 0.89 28.6 0.93 31.6  24.7 24.7 24.8 25.0 25.0 25.2 25.4 25.4 24.6 25.3  PSNR SSIM 22.4 0.82 0.81 22.5 0.83 23.8 0.84 24.9 0.87 26.4 26.1 0.85 0.87 28.0 28.9 0.89 0.87 26.3 0.90 28.2  24.2 24.2 24.3 24.4 24.5 24.2 25.1 25.0 24.2 24.7  ∗ models ﬁne-tuned on patches of size 64 × 64.  the UCF101 dataset where most of the images are still, we performed our evaluation in the moving areas as displayed in Figure 3. To this end, we use the EpicFlow method of Revaud et al. (2015), and compute the different quality measures only in the areas where the optical ﬂow is higher than a ﬁxed threshold 2. Similarity and sharpness measures computed on the whole images are given in Appendix. The numbers clearly indicate that all strategies perform better than the (cid:96)2 predictions in terms of PSNR, SSIM and sharpness. The multi-scale model brings some improvement, but used with an (cid:96)2 norm, it does not outperform simple frame copy in the moving areas. The (cid:96)1 model improves the results, since it replaces the mean by the median value of individual pixel predictions. The GDL and adversarial predictions are leading to further gains, and ﬁnally the combination of the multi-scale, (cid:96)1 norm, GDL and adversarial training achieves the best PSNR, SSIM and Sharpness difference measure. It is interesting to note that while we showed that the (cid:96)2 norm was a poor metric for training predic- tive models, the PSNR at test time is the worst for models trained optimising the (cid:96)2 norm, although the PSNR is based on the (cid:96)2 metric. We also include the baseline presented in Ranzato et al. (2014) – courtesy of Piotr Dollar – that extrapolates the pixels of the next frame by propagating the optical ﬂow from the previous ones. Figure 4 shows results on test sequences from the Sport1m dataset, as movements are more visible in this dataset.  3.4 COMPARISON TO RANZATO ET AL. (2014)  In this section, we compare our results to (Ranzato et al., 2014). To obtain grayscale images, we make RGB predictions and extract the Y channel of our Adv+GDL model. Ranzato et al. (2014) images are generated by averaging 64 results obtained using different tiling to avoid a blockiness effect, however creating instead a blurriness effect. We compare the PSNR and SSIM values on the ﬁrst predicted images of Figure 5.  2We use default parameters for the Epic Flow computation, and transformed the .ﬂo ﬁle to png using the Matlab code http://vision.middlebury.edu/flow/code/flow-code-matlab.zip. If at least one color channel is lower than 0.2 (image color range between 0 and 1), we replace the corresponding pixel intensity of the output and ground truth to 0, and compute similarity measures in the resulting masked images.  7  Published as a conference paper at ICLR 2016  Figure 4: Results on 3 video clips from Sport1m. Training: 4 inputs, 1 output. Second output computed recursively.  Input frames  Ground truth  (cid:96)2 result  (cid:96)1 result  GDL (cid:96)1 result  Adversarial result  Adversarial+GDL result  Input frames  Ground truth  (cid:96)2 result  (cid:96)1 result  GDL (cid:96)1 result  Adversarial result  Adversarial+GDL result  Input frames  Ground truth  (cid:96)2 result  (cid:96)1 result  GDL (cid:96)1 result  Adversarial result  Adversarial+GDL result  We note that the results of Ranzato et al. appear slightly lighter than our results because of a nor- malization that does not take place in the original images, therefore the errors given here are not reﬂecting the full capacity of their approach. We tried to apply the blind deconvolution method of Krishnan et al. (2011) to improve Ranzato et al. and our different results. As expected, the obtained sharpness scores are higher, but the image similarity measures are deteriorated because often the contours of the predictions do not match exactly the targets. More importantly, Ranzato et al. results appear to be more static in moving areas. Visually, the optical ﬂow result appears similar to the target, but a closer look at thin details reveals that lines, heads of people are bent or squeezed.  4 CONCLUSION  We provided a benchmark of several strategies for next frame prediction, by evaluating the quality of the prediction in terms of Peak Signal to Noise Ratio, Structural Similarity Index Measure and image sharpness. We display our results on small UCF video clips at http://cs.nyu.edu/ ˜mathieu/iclr2016.html. The presented architectures and losses may be used as building blocks for more sophisticated prediction models, involving memory and recurrence. Unlike most optical ﬂow algorithms, the model is fully differentiable, so it can be ﬁne-tuned for another task if necessary. Future work will deal with the evaluation of the classiﬁcation performances of the learned  8  Published as a conference paper at ICLR 2016  Figure 5: Comparison of results on the Basketball Dunk and Ice Dancing clips from UCF101 ap- pearing in (Ranzato et al., 2014). We display 2 frame predictions for each method along with 2 zooms of each image. The PSNR and SSIM values are computed in the moving areas of the images (More than the 2/3 of the pixels in these examples). The values in parenthesis correspond to the second frame predictions measures.  Target  Prediction using a constant optical ﬂow PSNR = 25.4 (18.9), SSIM = 0.88 (0.56)  Ranzato et al. result  PSNR = 16.3 (15.1), SSIM = 0.70 (0.55)  Adv GDL (cid:96)1 result  PSNR = 26.7 (19.0), SSIM = 0.89 (0.59)  Target  Prediction using a constant optical ﬂow PSNR = 24.7 (20.6), SSIM = 0.84 (0.72)  Ranzato et al. result  PSNR = 20.1 (17.8), SSIM = 0.72 (0.65)  Adv GDL (cid:96)1 result  PSNR = 24.6 (20.5), SSIM = 0.81 (0.69)  representations in a weakly supervised context, for instance on the UCF101 dataset. Another ex- tension of this work could be the combination of the current system with optical ﬂow predictions. Alternatively, we could replace optical ﬂow predictions in applications that does not explicitly re-  9  Published as a conference paper at ICLR 2016  quire optical ﬂow but rather next frame predictions. A simple example is causal (where the next frame is unknown) segmentation of video streams.  ACKNOWLEDGMENTS  We thank Florent Perronnin for fruitful discussions, and Nitish Srivastava, Marc’Aurelio Ranzato and Piotr Doll´ar for providing us their results on some video sequences.  REFERENCES Ascenso, Joao, Brites, Catarina, and Pereira, Fernando. Improving frame interpolation with spatial motion smoothing for pixel domain distributed video coding. In 5th EURASIP Conference on Speech and Image Processing, Multimedia Communications and Services, pp. 1–6, 2005.  Bromley, Jane, Bentz, James W, Bottou, L´eon, Guyon, Isabelle, LeCun, Yann, Moore, Cliff, S¨ackinger, Eduard, and Shah, Roopak. Signature veriﬁcation using a siamese time delay neu- ral network. Int. Journal of Pattern Recognition and Artiﬁcial Intelligence, 7(04):669–688, 1993.  Denton, Emily, Chintala, Soumith, Szlam, Arthur, and Fergus, Rob. Deep generative image models  using a laplacian pyramid of adversarial networks. In NIPS, 2015.  Dosovitskiy, Alexey, Springenberg, Jost Tobias, and Brox, Thomas. Learning to generate chairs  with convolutional neural networks. In CVPR, 2015.  Flynn, John, Neulander, Ivan, Philbin, James, and Snavely, Noah. Deepstereo: Learning to predict  new views from the world’s imagery. CoRR, abs/1506.06825, 2015.  Goodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron C., and Bengio, Yoshua. Generative adversarial networks. NIPS, 2014.  Goroshin, Ross, Mathieu, Micha¨el, and LeCun, Yann. Learning to linearize under uncertainty. NIPS,  2015.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural Comput., 9(8):1735–  1780, November 1997.  Jain, Viren, Murray, Joseph F, Roth, Fabian, Turaga, Srinivas, Zhigulin, Valentin, Briggman, Kevin L, N, Helmstaedter Moritz, Denk, Winfried, and Seung, Sebastian H. Supervised learning of image restoration with convolutional networks. 2007.  Jayaraman, Dinesh and Grauman, Kristen. Learning image representations equivariant to ego-  motion. In ICCV, 2015.  Karpathy, Andrej, Toderici, George, Shetty, Sanketh, Leung, Thomas, Sukthankar, Rahul, and Fei-  Fei, Li. Large-scale video classiﬁcation with convolutional neural networks. In CVPR, 2014.  Kosaka, Akio and Kak, Avinash C. Fast vision-guided mobile robot navigation using model-based reasoning and prediction of uncertainties. CVGIP: Image understanding, 56(3):271–329, 1992.  Krishnan, Dilip, Tay, Terence, and Fergus, Rob. Blind deconvolution using a normalized sparsity  measure. In CVPR, pp. 233–240, 2011.  Le, Quoc V. Building high-level features using large scale unsupervised learning. In ICASSP, pp.  8595–8598, 2013.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  Long, Jonathan, Shelhamer, Evan, and Darrell, Trevor. Fully convolutional networks for semantic  segmentation. In CVPR, 2015.  Mahendran, Aravindh and Vedaldi, Andrea. Understanding deep image representations by inverting  them. In CVPR, 2015.  10  Published as a conference paper at ICLR 2016  Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines.  In ICML, pp. 807–814, 2010.  Oh, Junhyuk, Guo, Xiaoxiao, Lee, Honglak, Lewis, Richard L., and Singh, Satinder P. Action-  conditional video prediction using deep networks in atari games. NIPS, 2015.  Ranzato, Marc’Aurelio, Szlam, Arthur, Bruna, Joan, Mathieu, Micha¨el, Collobert, Ronan, and Chopra, Sumit. Video (language) modeling: a baseline for generative models of natural videos. CoRR, abs/1412.6604, 2014.  Revaud, Jerome, Weinzaepfel, Philippe, Harchaoui, Zaid, and Schmid, Cordelia. EpicFlow: Edge- Preserving Interpolation of Correspondences for Optical Flow. In Computer Vision and Pattern Recognition, 2015.  Ronneberger, Olaf, Fischer, Philipp, and Brox, Thomas. U-net: Convolutional networks for biomed-  ical image segmentation. In MICCAI, volume 9351, pp. 234–241. 2015.  Simonyan, Karen and Zisserman, Andrew. Two-stream convolutional networks for action recogni-  tion in videos. In NIPS, 2014.  Soomro, Khurram, Zamir, Amir Roshan, and Shah, Mubarak. UCF101: A dataset of 101 human  actions classes from videos in the wild. CoRR, abs/1212.0402, 2012.  Srivastava, Nitish, Mansimov, Elman, and Salakhutdinov, Ruslan. Unsupervised learning of video  representations using LSTMs. In ICML, 2015.  Tran, Du, Bourdev, Lubomir D., Fergus, Rob, Torresani, Lorenzo, and Paluri, Manohar. C3D:  generic features for video analysis. In ICCV, 2015.  Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Extracting and composing robust features with denoising autoencoders. In ICML, pp. 1096–1103. ACM, 2008.  Vondrick, Carl, Pirsiavash, Hamed, and Torralba, Antonio. Anticipating the future by watching  unlabeled video. CoRR, abs/1504.08023, 2015.  Wang, Xiaolong and Gupta, Abhinav. Unsupervised learning of visual representations using videos.  In ICCV, 2015.  Wang, Zhou, Bovik, Alan C., Sheikh, Hamid R., and Simoncelli, Eero P. Image quality assessment:  From error visibility to structural similarity. IEEE Trans. on Im. Proc., 13(4):600–612, 2004.  11  Published as a conference paper at ICLR 2016  5 APPENDIX  5.1 PREDICTING THE EIGHT NEXT FRAMES  In this section, we trained our different multi-scale models – architecture described in Table 3– with 8 input frames to predict 8 frames simultaneously. Image similarity measures are given between the ground truth and the predictions in Table 4.  Table 3: Network architecture  Models 8 frames in input – 8 frames in output  Generative network scales Number of feature maps  Conv. kernel size  Adversarial network scales Number of feature maps  Conv. kernel size (no padding)  Fully connected  G1  16, 32, 64 3, 3, 3, 3  G2  16, 32, 64 5, 3, 3, 3  G3  32, 64, 128 5, 5, 5, 5  D1 16 3  128, 64  D2  16, 32, 32  3, 3, 3 256, 128  D3  32, 64, 64  5, 5, 5 256, 128  G4  32, 64, 128, 128  7, 5, 5, 5, 5  D4  32, 64, 128, 128  7, 7, 5, 5 256, 128  For the ﬁrst and eighth predicted frames, the numbers clearly indicate that all strategies perform better than the (cid:96)2 predictions in terms of PSNR and sharpness. The (cid:96)1 model, by replacing the mean intensity by the median value in individual pixel predictions, allows us to improve results. The adversarial predictions are leading to further gains, and ﬁnaly the GDL allows the predictions to achieve the best PNSR and sharpness. We note that the size of the network employed in the simultaneous prediction conﬁguration is smaller than in the unique frame prediction setting.  Table 4: Comparison of the accuracy of the predictions on 10% of the UCF101 test images. The different models have been trained given 8 frames to predict the 8 next ones.  1st frame prediction scores Sharpness  Similarity  8th frame prediction scores Sharpness  Similarity  (cid:96)2 Adv (cid:96)1  GDL (cid:96)1 Last input  PSNR SSIM 0.59 18.3 0.61 21.1 21.3 0.66 0.69 21.4 30.6 0.90  17.5 17.6 17.7 17.9 22.3  PSNR SSIM 0.51 15.4 0.52 17.1 17.0 0.55 0.58 17.7 21.0 0.74  17.4 17.4 17.5 17.5 18.5  Figure 6 shows a generation result of eight frames simultaneously, using a large version of the GDL (cid:96)1 model in which all the number of feature maps were multiplied by four.  Figure 6: Results on a UCF101 video using a large GDL-(cid:96)1 model. Training: 8 inputs, 8 outputs. First line: target, second line: our predictions.  Compared to the recursive frame prediction as employed in the rest of the paper, predicting several input simultaneouly leads to better long term results but worst shorter term ones. The gap between the two performances could be reduced by the design of time multi-scale strategies.  12  Published as a conference paper at ICLR 2016  5.2 COMPARISON TO THE LSTM APPROACH OF SRIVASTAVA ET AL. (2015)  Figure 7 shows a comparison with predictions based on LSTMs using sequences of patches from (Srivastava et al., 2015). The model ranking established on UCF101 in terms of sharpness and PSNR remains unchanged on the two sequences. When we employ the setting 8 inputs-8 output described in Table 3, we note that the LSTM ﬁrst frame prediction is sharper than our models predictions, however when looking at a longer term future, our gradient difference loss leads to sharper results. Comparing visually the GDL (cid:96)1 and GDL (cid:96)2, we notice that the predictions suffer from a chessboard effect in the (cid:96)2 case. On the other hand, when employing the recursive strategy (4 inputs, 1 output), the adversarial training lead to much sharper predictions. It does not look like anything close to the ground truth on the long term, but it remains realistic.  Figure 7: Comparison of different methods to predict 32 × 32 patches from UCF101. The LSTM lines are from (Srivastava et al., 2015). Baseline results with pure (cid:96)1 and (cid:96)2 losses are shown in Appendix.  Input  Ground truth  LSTM 2048  LSTM 4096  GDL (cid:96)1  (cid:96)1  GDL (cid:96)2  (cid:96)2  Adversarial  Adv. recursive  Adv. rec. + GDL  GDL (cid:96)1 recursive  (cid:96)1 rec.  (cid:96)2 rec.  5.3 ADDITIONAL RESULTS ON THE UCF101 DATASET  We trained the model described in Table 1 with our different losses to predict 1 frame from the 4 previous ones. We provide in Table 5 similarity (PSNR and SSIM) and sharpness measures between the different tested models predictions and frame to predict. The evaluation is performed on the full images but is not really meaningful because predicting the future location of static pixels is most accurately done by copying the last input frame.  13  Published as a conference paper at ICLR 2016  Table 5: Comparison of the accuracy of the predictions on 10% of the UCF101 test images. The different models have been trained given 4 frames to predict the next one. Similarity and sharpness measures on full images.  1st frame prediction scores Sharpness  Similarity  2nd frame prediction scores Sharpness  Similarity  single sc. (cid:96)2  (cid:96)2 (cid:96)1  GDL (cid:96)1  Adv  Adv+GDL  Last input  Adv+GDL ﬁne-tuned  PSNR SSIM 0.59 19.0 0.64 20.1 0.74 22.3 23.9 0.80 0.77 24.4 0.83 27.2 0.90 29.6 30.0 0.90  PSNR SSIM 0.48 14.2 0.50 14.1 0.56 16.0 18.6 0.64 0.59 18.9 0.72 22.6 26.0 0.83 0.84 25.8  17.5 17.4 17.6 17.7 17.3 18.5 19.4 20.3  17.8 17.8 18.5 18.7 18.7 19.6 20.3 22.1  14  ",
1507.01526,2016,Grid Long Short-Term Memory,"['Grid Long Short-Term Memory\nNal Kalchbrenner', 'Alex Graves', 'Ivo Danihelka']",https://arxiv.org/pdf/1507.01526,"6 1 0 2     n a J    7      ] E N . s c [      3 v 6 2 5 1 0  .  7 0 5 1 : v i X r a  Under review as a conference paper at ICLR 2016  GRID LONG SHORT-TERM MEMORY  Nal Kalchbrenner & Ivo Danihelka & Alex Graves Google DeepMind London, United Kingdom {nalk,danihelka,gravesa}@google.com  ABSTRACT  This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a uniﬁed way of using LSTM for both deep and sequential computation. We ap- ply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to signiﬁcantly outperform the standard LSTM. We then give results for two empirical tasks. We ﬁnd that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to deﬁne a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task.  1  INTRODUCTION  Long Short-Term Memory (LSTM) networks are recurrent neural networks equipped with a special gating mechanism that controls access to memory cells (Hochreiter & Schmidhuber, 1997). Since the gates can prevent the rest of the network from modifying the contents of the memory cells for multiple time steps, LSTM networks preserve signals and propagate errors for much longer than ordinary recurrent neural networks. By independently reading, writing and erasing content from the memory cells, the gates can also learn to attend to speciﬁc parts of the input signals and ignore other parts. These properties allow LSTM networks to process data with complex and separated interdependencies and to excel in a range of sequence learning domains such as speech recognition (Graves et al., 2013), ofﬂine hand-writing recognition (Graves & Schmidhuber, 2008), machine translation (Sutskever et al., 2014) and image-to-caption generation (Vinyals et al., 2014; Kiros et al., 2014). Even for non-sequential data, the recent success of deep networks has shown that long chains of sequential computation are key to ﬁnding and exploiting complex patterns. Deep networks suffer from exactly the same problems as recurrent networks applied to long sequences: namely that infor- mation from past computations rapidly attenuates as it progresses through the chain – the vanishing gradient problem (Hochreiter, 1991) – and that each layer cannot dynamically select or ignore its inputs. It therefore seems attractive to generalise the advantages of LSTM to deep computation. We extend LSTM cells to deep networks within a uniﬁed architecture. We introduce Grid LSTM, a network that is arranged in a grid of one or more dimensions. The network has LSTM cells along any or all of the dimensions of the grid. The depth dimension is treated like the other dimensions and also uses LSTM cells to communicate directly from one layer to the next. Since the number N of dimensions in the grid can easily be 2 or more, we propose a novel, robust way for modulating the N-way communication across the LSTM cells. N-dimensional Grid LSTM (N-LSTM for short) can naturally be applied as feed-forward networks as well as recurrent ones. One-dimensional Grid LSTM corresponds to a feed-forward network that uses LSTM cells in place of transfer functions such as tanh and ReLU (Nair & Hinton, 2010). These networks are related to Highway Networks (Srivastava et al., 2015) where a gated transfer function  1  Under review as a conference paper at ICLR 2016  Figure 1: Blocks form the standard LSTM and those that form Grid LSTM networks of N = 1, 2 and 3 dimensions. The dashed lines indicate identity transformations. The standard LSTM block does not have a memory vector in the vertical dimension; by contrast, the 2d Grid LSTM block has the memory vector m1 applied along the vertical dimension.  is used to successfully train feed-forward networks with up to 900 layers of depth. Grid LSTM with two dimensions is analogous to the Stacked LSTM, but it adds cells along the depth dimension too. Grid LSTM with three or more dimensions is analogous to Multidimensional LSTM (Graves et al., 2013; Sutskever et al., 2014; Graves et al., 2007; Graves, 2012), but differs from it not just by having the cells along the depth dimension, but also by using the proposed mechanism for modulating the N-way interaction that is not prone to the instability present in Multidimesional LSTM. We study some of the learning properties of Grid LSTM in various algorithmic tasks. We compare the performance of two-dimensional Grid LSTM to Stacked LSTM on computing the addition of two 15-digit integers without curriculum learning and on memorizing sequences of numbers (Zaremba & Sutskever, 2014). We ﬁnd that in these settings having cells along the depth dimension is more effective than not having them; similarly, tying the weights across the layers is also more effective than untying the weights, despite the reduced number of parameters. We also apply Grid LSTM to two empirical tasks. The architecture achieves 1.47 bits-per-character in the 100M characters Wikipedia dataset (Hutter, 2012) outperforming other neural networks. Sec- ondly, we use Grid LSTM to deﬁne a novel neural translation model that re-encodes the source sen- tence based on the target words generated up to that point. The network outperforms the reference phrase-based CDEC system (Dyer et al., 2010) on the IWSLT BTEC Chinese-to-Ensligh transla- tion task. The appendix contains additional results for Grid LSTM on learning parity functions and classifying MNIST images. The outline of the paper is as follows. In Sect. 2 we describe standard LSTM networks that comprise the background. In Sect. 3 we deﬁne the Grid LSTM architecture. In Sect. 4 we consider the six experiments and we conclude in Sect. 5.  2 BACKGROUND  We begin by describing the standard LSTM recurrent neural network and the derived Stacked and Multidimensional LSTM networks; some aspects of the networks motivate the Grid LSTM.  2.1 LONG SHORT-TERM MEMORY  The LSTM network processes a sequence of input and target pairs (x1, y1), ..., (xm, ym). For each pair (xi, yi) the LSTM network takes the new input xi and produces an estimate for the target yi given all the previous inputs x1, ..., xi. The past inputs x1, ..., xi−1 determine the state of the network that comprises a hidden vector h ∈ Rd and a memory vector m ∈ Rd. The computation at  2  2d Grid LSTM blockStandard LSTM blockmm0h0hh0I⇤xih1h2h02h01m1m01m02m21d Grid LSTM Block3d Grid LSTM BlockUnder review as a conference paper at ICLR 2016  Figure 2: Stacked LSTM and 2d Grid LSTM applied to character prediction composed from the respective blocks (Fig. 1). Note how in the Grid LSTM the signal ﬂows through LSTM cells (shaded rectangles) along both the time and the depth dimensions.  each step is deﬁned as follows (Graves et al., 2013):  (1)  (2)  gu = σ(WuH) gf = σ(Wf H) go = σ(WoH) gc = tanh(WcH) m(cid:48) = gf (cid:12) m + gu (cid:12) gc h(cid:48) = tanh(go (cid:12) m(cid:48)) (cid:21)  (cid:20)Ixi  H =  h  where σ is the logistic sigmoid function, Wu, Wf , Wo, Wc in Rd×2d are the recurrent weight matrices of the network and H ∈ R2d is the concatenation of the new input xi, transformed by a projection matrix I, and the previous hidden vector h:  The computation outputs new hidden and memory vectors h(cid:48) and m(cid:48) that comprise the next state of the network. The estimate for the target is then computed in terms of the hidden vector h(cid:48). We use the functional LSTM(·,·,·) as shorthand for Eq. 1 as follows:  (h(cid:48), m(cid:48)) = LSTM(H, m, W)  (3)  where W concatenates the four weight matrices Wu, Wf , Wo, Wc. One aspect of LSTM networks is the role of the gates gu, gf , go and gc. The forget gate gf can delete parts of the previous memory vector mi−1 whereas the gate gc can write new content to the new memory mi modulated by the input gate gu. The output gate controls what is then read from the new memory mi onto the hidden vector hi. The mechanism has two important learning properties. Each memory vector is obtained by a linear transformation of the previous memory vector and the gates; this ensures that the forward signals from one step to the other are not repeatedly squashed by a non-linearity such as tanh and that the backward error signals do not decay sharply at each step, an issue known as the vanishing gradient problem (Hochreiter et al., 2001). The mechanism also acts as a memory and implicit attention system, whereby the signal from some input xi can be written to the memory vector and attended to in parts across multiple steps by being retrieved one part at a time.  3  CHAHARCHAHAR2d Grid LSTMStacked LSTMUnder review as a conference paper at ICLR 2016  Figure 3: Instances of one-dimensional and three-dimensional Grid LSTM. The network to the left is used for the parity results in the appendix. The translation and MNIST models below are speciﬁc instances of the 3d Grid LSTM to the right.  2.2 STACKED LSTM  A model that is closely related to the standard LSTM network is Stacked LSTM (Graves et al., 2013; Sutskever et al., 2014). Stacked LSTM adds capacity by stacking LSTM layers on top of each other. The output hidden vector hi in Eq. 1 from the LSTM below is taken as the input to the LSTM above in place of I ∗ xi. The Stacked LSTM is depicted in Fig. 2. Note that although the LSTM cells are present along the sequential computation of each LSTM network, they are not present in the vertical computation from one layer to the next.  2.3 MULTIDIMENSIONAL LSTM  Another related model is Multidimensional LSTM (Graves et al., 2007). Here the inputs are not arranged in a sequence, but in a N-dimensional grid, such as the two-dimensional grid of pixels in an image. At each input x in the array the network receives N hidden vectors h1, ..., hN and N memory vectors m1, ..., mN and computes a hidden vector h and a memory vector m that are passed as the next state for each of the N dimensions. The network concatenates the transformed input I ∗ x and the N hidden vectors h1, ..., hN into a vector H and as in Eq. 1 computes gu, go and gc, as well as N forget gates gf i . These gates are then used to compute the memory vector as follows:  N(cid:88)  i  m =  gf i (cid:12) mi + gu (cid:12) gc  (4)  As the number of paths in a grid grows combinatorially with the size of each dimension and the total number of dimensions N, the values in m can grow at the same rate due to the unconstrained summation in Eq. 4. This can cause instability for large grids, and adding cells along the depth dimension increases N and exacerbates the problem. This motivates the simple alternate way of computing the output memory vectors in the Grid LSTM.  3 ARCHITECTURE  Grid LSTM deploys cells along any or all of the dimensions including the depth of the network. In the context of predicting a sequence, the Grid LSTM has cells along two dimensions, the temporal one of the sequence itself and the vertical one along the depth. To modulate the interaction of the cells in the two dimensions, the Grid LSTM proposes a simple mechanism where the values in the cells cannot grow combinatorially as in Eq. 4. In this section we describe the multidimensional blocks and the way in which they are combined to form a Grid LSTM.  4  100011001001d Grid LSTM3d Grid LSTMUnder review as a conference paper at ICLR 2016  3.1 GRID LSTM BLOCKS  As in multidimensional LSTM, a N-dimensional block in a Grid LSTM receives as input N hidden vectors h1, ..., hN and N memory vectors m1, ..., mN . Unlike the multidimensional case, the block outputs N hidden vectors h(cid:48)1, ..., h(cid:48)N and N memory vectors m(cid:48)1, ..., m(cid:48)N that are all distinct. The computation is simple and proceeds as follows. The model ﬁrst concatenates the input hidden vectors from the N dimensions:     h1  ... hN  H =  Then the block computes N transforms LSTM(·,·,·), one for each dimension, obtaining the desired output hidden and memory vectors:  (h(cid:48)1, m(cid:48)1) = LSTM(H, m1, W1)  ...  (h(cid:48)N , m(cid:48)N ) = LSTM(H, mN , WN )  (6)  i , Wf  i in Rd×N d and applies the standard Each transform has distinct weight matrices Wu LSTM mechanism across the respective dimension. Note how the vector H that contains all the input hidden vectors is shared across the transforms, whereas the input memory vectors affect the N-way interaction but are not directly combined. N-dimensional blocks can naturally be arranged in a N-dimensional grid forming a Grid LSTM. As for a block, the grid has N sides with incoming hidden and memory vectors and N sides with outgoing hidden and memory vectors. Note that a block does not receive a separate data representation. A data point is projected into the network via a pair of input hidden and memory vectors along one of the sides of the grid.  i , Wo  i , Wc  3.2 PRIORITY DIMENSIONS  In a N-dimensional block the transforms for all dimensions are computed in parallel. But it can be useful for a dimension to know the outputs of the transforms from the other dimensions, especially if the outgoing vectors from that dimension will be used to estimate the target. For instance, to prioritize the ﬁrst dimension of the network, the block ﬁrst computes the N − 1 transforms for the other dimensions obtaining the output hidden vectors h(cid:48)2, ..., h(cid:48)N . Then the block concatenates these output hidden vectors and the input hidden vector h1 for the ﬁrst dimension into a new vector H(cid:48) as follows:  (5)  (7)     h1  h(cid:48)2...  h(cid:48)N  H(cid:48) =  The vector is then used in the ﬁnal transform to obtain the prioritized output hidden and memory vectors h(cid:48)1 and m(cid:48)1.  3.3 NON-LSTM DIMENSIONS  In Grid LSTM networks that have only a few blocks along a given dimension in the grid, it can be useful to just have regular connections along that dimension without the use of cells. This can be naturally accomplished inside the block by using for that dimension in Eq. 6 a simple transformation with a nonlinear activation function instead of the transform LSTM(·,·,·). Given a weight matrix V ∈ Rd×N d, for the ﬁrst dimension this looks as follows: (8) h(cid:48)1 = α(V ∗ H) where α is a standard nonlinear transfer function or simply the identity. This allows us to see how, modulo the differences in the mechanism inside the blocks, Grid LSTM networks generalize the models in Sect. 2. A 2d Grid LSTM applied to temporal sequences with cells in the temporal dimension but not in the vertical depth dimension, corresponds to the Stacked LSTM. Likewise, the 3d Grid LSTM without cells along the depth corresponds to Multidimensional LSTM, stacked with one or more layers.  5  Under review as a conference paper at ICLR 2016  Layers Samples Accuracy  Stacked LSTM Untied 2-LSTM Tied 2-LSTM  1 5 18  5M 5M  51% 67% 0.55M > 99%  Figure 4: Results on 15-digit addition. The left table gives results for the best performing networks of each type. The right graph depicts the learning curve of the 18-layer tied 2-LSTM that solves the problem with less than 550K examples. The spike in the curve is likely due to the repetitions in the steps of the addition algorithm.  3.4  INPUTS FROM MULTIPLE SIDES  If we picture a N-dimensional block as in Fig. 1, we see that N of the sides of the block have input vectors associated with them and the other N sides have output vectors. As the blocks are arranged in a grid, this separation extends to the grid as a whole; each side of the grid has either input or output vectors associated with it. In certain tasks that have inputs of different types, a model can exploit this separation by projecting each type of input on a different side of the grid. The mechanism inside the blocks ensures that the hidden and memory vectors from the different sides will interact closely without being conﬂated. This is the case in the neural translation model introduced in Sect. 4 where source words and target words are projected on two different sides of a Grid LSTM.  3.5 WEIGHT SHARING  Sharing of weight matrices can be speciﬁed along any dimension in a Grid LSTM and it can be useful to induce invariance in the computation along that dimension. As in the translation and image models, if multiple sides of a grid need to share weights, capacity can be added to the model by introducing into the grid a new dimension without sharing of weights. If the weights are shared along all dimensions including the depth, we refer to the model as a Tied N-LSTM.  4 EXPERIMENTS  4.1 ADDITION  We ﬁrst experiment with 2-LSTM networks on learning to sum two 15-digit integers. The problem formulation is similar to that in (Zaremba & Sutskever, 2014), where each number is given to the network one digit at a time and the result is also predicted one digit at a time. The input numbers are separated by delimiter symbols and an end-of-result symbol is predicted by the network; these symbols as well as input and target padding are indicated by −. An example is as follows:  − 1  3 − 8  2  9 − − − − −  9  (cid:119)(cid:127)  − − − − − − − − 1  0  2  2 −  Contrary to the work in (Zaremba & Sutskever, 2014) that uses from 4 to 9 digits for the input integers, we ﬁx the number of digits to 15, we do not use curriculum learning strategies and we do not put digits from the partially predicted output back into the network, forcing the network to remember its partial predictions and making the task more challenging. The predicted output numbers have either 15 or 16 digits. We compare the performance of 2-LSTM networks with that of standard Stacked LSTM (Fig. 2). We train the two types of networks with either tied or untied weights, with 400 hidden units each and with between 1 and 50 layers. We train the network with stochastic gradient descent using  6  Samples (millions)AccuracyTied 2-LSTM00.10.20.30.40.50.60.10.410.7Under review as a conference paper at ICLR 2016  Figure 5: Each dot in the three plots corresponds to a neural network of the respective type that has reached the accuracy of, respectively, > 99%, > 80% and > 50% at the memorization task. The networks all have 100 hidden units and the number of layers are indicated on the horizontal axis. The vertical axis indicates the number of samples needed to achieve the threshold accuracy. We see that deeper networks tend to learn faster than shallower ones, and that 2-LSTM networks are more effective than Stacked LSTM networks in both the tied and untied settings.  mini-batches of size 15 and the Adam optimizer with a learning rate of 0.001 (Kingma & Ba, 2014). We train the networks for up to 5 million samples or until they reach 100% accuracy on a random sample of 100 unseen addition problems. Note that since during training all samples are randomly generated, samples are seen only once and it is not possible for the network to overﬁt on training data. The training and test accuracies agree closely. Figure 4 relates the results of the experiments on the addition problem. The best performing tied 2-LSTM is 18 layers deep and learns to perfectly solve the task in less than 550K training samples. We ﬁnd that tied 2-LSTM networks generally perform better than untied 2-LSTM networks, which is likely due to the repetitive nature of the steps involved in the addition algorithm. The best untied 2-LSTM network has 5 layers, learns more slowly and achieves a per-digit accuracy of 67% after 5 million examples. 2-LSTM networks in turn perform better than either tied or untied Stacked LSTM networks, where more stacked layers do not improve over the single-layer models. We see that the cells present a clear advantage for the deep 2-LSTM networks by helping to mitigate the vanishing of gradients along the depth dimension.  4.2 MEMORIZATION  For our third algorithmic task, we analyze the performance of 2-LSTM networks on the task of memorizing a random sequence of symbols. The sequences are 20 symbols long and we use a vocabulary of 64 symbols encoded as one-hot vectors and given to the network one symbol per step. The setup is similar to the one for addition above. The network is tasked with reading the input sequence and outputting the same sequence unchanged:  − α β  γ − − − −  (cid:119)(cid:127)  − − − − α β  γ −  Since the sequences are randomly generated, there is no correlation between successive symbols and the network must memorize the whole sequence without compression. We train 2-LSTM and Stacked LSTM with either tied or untied weights on the memorization task. All networks have 100 hidden units and have between 1 and 50 layers. We use mini-batches of size 15 and optimize the network using Adam and a learning rate of 0.001. As above, we train each network for up to 5 million samples or until they reach 100% accuracy on 100 unseen samples. Accuracy is measured per individual symbol, not per sequence. We do not use curriculum learning or other training strategies. Figure 5 reports the performance of the networks. The small number of hidden units contributes to making the training of the networks difﬁcult. But we see that tied 2-LSTM networks are most  7  LayersSamples (millions)LayersLayersUntied 2-LSTMTied 2-LSTMTied Stacked LSTMUntied Stacked LSTMAccuracy > 99%Accuracy > 80%Accuracy > 50%0.10.4 102 31 0.91.3 1.71.1 1.5 01020304050010203004168120.7 Under review as a conference paper at ICLR 2016  Stacked LSTM (Graves, 2013) MRNN (Sutskever et al., 2011) GFRNN (Chung et al., 2015) Tied 2-LSTM  BPC Parameters Alphabet Size 1.67 1.60 1.58 1.47  27M 4.9M 20M 16.8M  205 86 205 205  Test data last 4MB last 10MB last 5MB last 5MB  Figure 6: Bits-per-character results for various models measured on the Wikipedia dataset together with the respective number of parameters and the size of the alphabet that was used. Note the slight differences in test data and alphabet size.  successful and learn to solve the task with the smallest number of samples. The 43-layer tied 2- LSTM network learns a solution with less than 150K samples. Although there is fairly high variance amid the solving networks, deeper networks tend to learn faster. In addition, there is large difference in the performance of tied 2-LSTM networks and tied Stacked LSTM networks. The latter perform with much lower accuracy and Stacked LSTM networks with more than 16 layers do not reach an accuracy of more than 50%. Here we see that the optimization property of the cells in the depth dimension delivers a large gain. Similarly to the case of the addition problem, both the untied 2- LSTM networks and the untied Stacked LSTM networks take signiﬁcantly longer to learn than the respective counterparts with tied weights, but the advantage of the cells in the depth direction clearly emerges for untied 2-LSTM networks too.  4.3 CHARACTER-LEVEL LANGUAGE MODELLING  We next test the 2-LSTM network on the Hutter challenge Wikipedia dataset (Hutter, 2012). The aim is to successively predict the next character in the corpus. The dataset has 100 million characters. We follow the splitting procedure of (Chung et al., 2015), where the last 5 million characters are used for testing. The alphabet has 205 characters in total. We use a tied 2-LSTM with 1000 hidden units and 6 layers of depth. As in Fig. 2 and in the previous tasks, the characters are projected both to form the initial input hidden and cell vectors and the top softmax layer is connected to the topmost output hidden and cell vectors. The model has a total of 2000 × 4000 + 205 × 4 × 1000 = 8.82 × 106 parameters. As usual the objective is to minimize the negative log-likelihood of the character sequence under the model. Training is performed by sampling sequences of 10000 characters and processing them in order. We back propagate the errors every 50 characters. The initial cell and hidden vectors in the temporal direction are initialized to zero only at the beginning of each sequence; they maintain their forward propagated values after each update in order to simulate full back propagation. We use mini-batches of 100, thereby processing 100 sequences of 10000 characters each in parallel. The network is trained with Adam with a learning rate of 0.001 and training proceeds for approximately 20 epochs. Figure 6 reports the bits-per-character performance together with the number of parameters of var- ious recently proposed models on the dataset. The tied 2-LSTM signiﬁcantly outperforms other models despite having fewer parameters. More layers of depth and adding capacity by untying some of the weights are likely to further enhance the 2-LSTM.  4.4 TRANSLATION  We next use the ﬂexibility of Grid LSTM to deﬁne a novel neural translation model. In the neural approach to machine translation one trains a neural network end-to-end to map the source sentence to the target sentence (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The mapping is usually performed within the encoder-decoder framework. A neural network, that can be convolutional or recurrent, ﬁrst encodes the source sentence and then the computed representation of the source conditions a recurrent neural network to generate the target sentence. This approach has yielded strong empirical results, but it can suffer from a bottleneck. The encoding of the source sentence must contain information about all the words and their order; the decoder network in turn cannot easily revisit the unencoded source sentence to make decisions based on partially produced translations. This issue can be alleviated by a soft attention mechanism in the decoder neural network that uses gates to focus on speciﬁc parts of the source sentence (Bahdanau et al., 2014).  8  Under review as a conference paper at ICLR 2016  Figure 7: Illustration of the 3-LSTM neural translation model.  We use Grid LSTM to view translation in a novel fashion as a two-dimensional mapping. We call this the Reencoder network. One dimension processes the source sentence whereas the other dimension produces the target sentence. The resulting network repeatedly re-encodes the source sentence conditioned on the part of the target sentence generated so far, thus functioning as an implicit attention mechanism. The size of the representation of the source sentence varies with length and the source sentence is repeatedly scanned based on each generated target word. As represented in Fig. 9, for each target word, beginning with the start-of-target-sentence symbol, the network scans the source sentence one way in the ﬁrst layer and the other way in the second layer; the scan depends on all the target words that have been generated so far and at each block the two layers communicate directly. Note that, like the attention-based model (Bahdanau et al., 2014), the two-dimensional translation model has complexity O(nm), where n and m are respectively the length of the source and target; by contrast the recurrent encoder-decoder model only has complexity O(m + n). This gives additional computational capacity to the former models. Besides addressing the bottleneck, the two-dimensional setup aims at explicitly capturing the invari- ance present in translation. Translation patterns between two languages are invariant above all to position and scale of the pattern. For instance, reordering patterns - such as the one that maps the English “do not (cid:104)verb(cid:105)” to the French “ne (cid:104)verb(cid:105) pas”, or the one that sends a part of an English verb to the end of a German sentence - should be detected and applied independently of where they occur in the source sentence or of the number of words involved in that instance of the pattern. To capture this, the Grid LSTM translation model shares the weights across the source and target di- mensions. In addition, a hierarchy of stacked two-dimensional grids in opposite directions is used to both increase capacity and help with learning longer scale translation patterns. The resulting model is a three-dimensional Grid LSTM where hierarchy grows along the third dimension. The model is depicted in Fig. 7. We evaluate the Grid LSTM translation model on the IWSLT BTEC Chinese-to-English corpus that consists of 44016 pairs of source and target sentences for training, 1006 for development and 503 for testing. The corpus has about 0.5M words in each language, a source vocabulary of 7055 Chinese words and a target vocabulary of 5646 English words (after replacing words that occur only once with the UNK symbol). Target sentences are on average around 12 words long. The development and test corpora come with 15 reference translations. The 3-LSTM uses two two-dimensional grids of 3-LSTM blocks for the hierarchy. Since the network has just two layers in the third dimension, we use regular identity connections without nonlinear transfer function along the third dimension,  9  </s>mattheonsatcatThe<s><t>LechatétaitassissurletapisLechatétaitassissurletapis</t>3d Grid LSTMUnder review as a conference paper at ICLR 2016  DGLSTM-Attention (Yao et al., 2015) CDEC (Dyer et al., 2010) 3-LSTM (7 Models)  Valid-1  -  30.1 30.3  Test-1 Valid-15 34.5 41 42.4  50.1 51.8  -  Test-15  -  58.9 60.2  Reference Generated Reference Generated  thank you . please pay for this bill at the cashier .  thank you , ma ’am . please give this bill to the cashier and pay there .  how about having lunch with me some day ? i found a good restaurant near my hotel .  how about lunch with me ? i found a good restaurant near my hotel .  Figure 8: The ﬁrst table contains BLEU-4 scores of the 3-LSTM neural translation model, the CDEC system and the Depth-Gated LSTM (DGLSTM) with attention mechanism; the scores are calculated against either the main reference translation or against the 15 available reference translations in the BTEC corpus. CDEC is a state-of-the-art hierarchical phrase based system with many component models. The second table contains examples of generated translations.  as deﬁned in Sect. 3.3; the source and target dimensions have tied weights and LSTM cells. The processing is bidirectional, in that the ﬁrst grid processes the source sentence from beginning to end and the second one from end to beginning. This allows for the shortest distance that the signal travels between input and output target words to be constant and independent of the length of the source. Note that the second grid receives an input coming from the grid below at each 3-LSTM block. We train seven models with vectors of size 450 and apply dropout with probability 0.5 to the hidden vectors within the blocks. For the optimization we use Adam with a learning rate of 0.001. At decoding the output probabilities are averaged across the models. The beam search has size 20 and we discard all candidates that are shorter than half of the length of the source sentence. The results are shown in Fig. 8. Our best model reaches a perplexity of 4.54 on the test data. We use as baseline the state-of-the-art hierarchical phrase-based system CDEC (Dyer et al., 2010). We see that the Grid LSTM signiﬁcantly outperforms the baseline system on both the validation and test data sets.  5 CONCLUSION  We have introduced Grid LSTM, a network that uses LSTM cells along all of the dimensions and modulates in a novel fashion the multi-way interaction. We have seen the advantages of the cells compared to regular connections in solving tasks such as parity, addition and memorization. We have described powerful and ﬂexible ways of applying the model to character prediction, machine translation and image classiﬁcation, showing strong performance across the board.  ACKNOWLEDGEMENTS  We thank Koray Kavukcuoglu, Razvan Pascanu, Ilya Sutskever and Oriol Vinyals for helpful com- ments and discussions.  REFERENCES Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to  align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/1409.0473.  Cho, Kyunghyun, van Merrienboer, Bart, G¨ulc¸ehre, C¸ aglar, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. URL http://arxiv.org/abs/1406.1078.  Chung, Junyoung, G¨ulc¸ehre, C¸ aglar, Cho, KyungHyun, and Bengio, Yoshua. Gated feedback recurrent neural  networks. CoRR, abs/1502.02367, 2015. URL http://arxiv.org/abs/1502.02367.  Ciresan, Dan Claudiu, Meier, Ueli, and Schmidhuber, J¨urgen. Multi-column deep neural networks for image  classiﬁcation. In arXiv:1202.2745v1 [cs.CV], 2012.  10  Under review as a conference paper at ICLR 2016  Duch, Wlodzislaw. K-separability. In Kollias, Stefanos, Stafylopatis, Andreas, Duch, Wlodzislaw, and Oja, Erkki (eds.), Artiﬁcial Neural Networks, ICANN 2006, volume 4131 of Lecture Notes in Computer Science, pp. 188–197. Springer Berlin Heidelberg, 2006. ISBN 978-3-540-38625-4. doi: 10.1007/11840817 20. URL http://dx.doi.org/10.1007/11840817_20.  Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization. Technical Report UCB/EECS-2010-24, EECS Department, University of Cali- fornia, Berkeley, Mar 2010. URL http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/ EECS-2010-24.html.  Dyer, Chris, Lopez, Adam, Ganitkevitch, Juri, Weese, Johnathan, Ture, Ferhan, Blunsom, Phil, Setiawan, Hendra, Eidelman, Vladimir, and Resnik, Philip. cdec: A decoder, alignment, and learning framework for ﬁnite-state and context-free translation models. In Proceedings of the Association for Computational Linguistics (ACL), 2010.  Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron C., and Bengio, Yoshua. Maxout In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, At- networks. lanta, GA, USA, 16-21 June 2013, pp. 1319–1327, 2013. URL http://jmlr.org/proceedings/ papers/v28/goodfellow13.html.  Graham, Benjamin. Spatially-sparse convolutional neural networks. CoRR, abs/1409.6070, 2014a. URL  http://arxiv.org/abs/1409.6070.  Graham, Benjamin. Fractional max-pooling. CoRR, abs/1412.6071, 2014b. URL http://arxiv.org/  abs/1412.6071.  Graves, A. Supervised sequence labelling with recurrent neural networks, volume 385. Springer, 2012.  Graves, A. and Schmidhuber, J. Ofﬂine handwriting recognition with multidimensional recurrent neural net-  works. In Advances in Neural Information Processing Systems, volume 21, 2008.  Graves, A., Fern´andez, S., and Schmidhuber, J. Multi-dimensional recurrent neural networks. In Proceedings  of the 2007 International Conference on Artiﬁcial Neural Networks, Porto, Portugal, September 2007.  Graves, A., Mohamed, A., and Hinton, G. Speech recognition with deep recurrent neural networks. In Proc  ICASSP 2013, Vancouver, Canada, May 2013.  Graves, Alex. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL  http://arxiv.org/abs/1308.0850.  Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f¨ur Informatik,  Lehrstuhl Prof. Brauer, Technische Universit¨at M¨unchen, 1991.  Hochreiter, S. and Schmidhuber, J. Long Short-Term Memory. Neural Computation, 9(8):1735–1780, 1997.  Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies. In Kremer and Kolen (eds.), A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001.  Hohil, Myron E., Liu, Derong, and Smith, Stanley H. Solving the n-bit parity problem using neural networks. Neural Networks, 12(9):1321–1323, 1999. doi: 10.1016/S0893-6080(99)00069-6. URL http://dx. doi.org/10.1016/S0893-6080(99)00069-6.  Hutter, Marcus. The human knowledge compression context, 2012. URL http://prize.hutter1.net.  Kalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. Seattle, October 2013. Asso-  ciation for Computational Linguistics.  Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,  2014. URL http://arxiv.org/abs/1412.6980.  Kiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S. Unifying visual-semantic embeddings with multi- modal neural language models. CoRR, abs/1411.2539, 2014. URL http://arxiv.org/abs/1411. 2539.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, 1998.  11  Under review as a conference paper at ICLR 2016  Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang, Zhengyou, and Tu, Zhuowen. Deeply-supervised nets. In Proceedings of the Eighteenth International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2015, San Diego, California, USA, May 9-12, 2015, 2015. URL http://jmlr.org/proceedings/ papers/v38/lee15a.html.  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. CoRR, abs/1312.4400, 2013. URL http:  //arxiv.org/abs/1312.4400.  Mairal, Julien, Koniusz, Piotr, Harchaoui, Za¨ıd, and Schmid, Cordelia. Convolutional kernel networks. Neural  Information Processing Systems, 2014. URL http://arxiv.org/abs/1406.3332.  Marvin Minsky, Seymour Papert. Perceptrons: An Introduction to Computational Geometry. MIT Press,  Cambridge MA, 1972.  Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines. In Pro- ceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 807–814, 2010. URL http://www.icml2010.org/papers/432.pdf.  Simard, Patrice Y., Steinkraus, David, and Platt, John C. Best practices for convolutional neural networks applied to visual document analysis. In 7th International Conference on Document Analysis and Recognition (ICDAR 2003), 2-Volume Set, 3-6 August 2003, Edinburgh, Scotland, UK, pp. 958–962, 2003. doi: 10.1109/ ICDAR.2003.1227801. URL http://dx.doi.org/10.1109/ICDAR.2003.1227801.  Srivastava, Rupesh Kumar, Greff, Klaus, and Schmidhuber, J¨urgen. Highway networks. arXiv preprint  arXiv:1505.00387, 2015.  Sutskever, I., Martens, J., and Hinton, G. Generating text with recurrent neural networks. In ICML, 2011.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural networks. In  Advances in Neural Information Processing Systems, pp. 3104–3112, 2014.  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption  generator. arXiv preprint arXiv:1411.4555, 2014.  Visin, Francesco, Kastner, Kyle, Cho, Kyunghyun, Matteucci, Matteo, Courville, Aaron C., and Ben- gio, Yoshua. Renet: A recurrent neural network based alternative to convolutional networks. CoRR, abs/1505.00393, 2015. URL http://arxiv.org/abs/1505.00393.  Wan, Li, Zeiler, Matthew D., Zhang, Sixin, LeCun, Yann, and Fergus, Rob. Regularization of neural networks using dropconnect. In ICML (3), volume 28 of JMLR Proceedings, pp. 1058–1066. JMLR.org, 2013. URL http://dblp.uni-trier.de/db/conf/icml/icml2013.html#WanZZLF13.  Yao, Kaisheng, Cohn, Trevor, Vylomova, Katerina, Duh, Kevin, and Dyer, Chris. Depth-gated LSTM. CoRR,  abs/1508.03790, 2015. URL http://arxiv.org/abs/1508.03790.  Zaremba, Wojciech and Sutskever, Ilya. Learning to execute. CoRR, abs/1410.4615, 2014. URL http:  //arxiv.org/abs/1410.4615.  APPENDIX  We here report on two additional results, one algorithmic and the other one empirical, where we see that without special initialization or training tricks, a 1-LSTM network can learn to compute parity for up to 250 input bits, and a 3-LSTM network applied to images obtains strong results on MNIST.  5.1 PARITY  We apply one-dimensional Grid LSTM to learning parity. Given a string b1, ..., bk of k bits 0 or 1, the parity or generalized XOR of the string is deﬁned to be 1 if the sum of the bits is odd, and 0 if the sum of the bits is even. Although manually crafted neural networks for the problem have been devised (Hohil et al., 1999), training a generic neural network from a ﬁnite number of examples and a generic random initialization of the weights to successfully learn to compute the parity of k-bit strings for signiﬁcant values of k is a longstanding problem (Marvin Minsky, 1972; Duch, 2006). It is core to the problem that the k-bit string is given to the neural network as a whole through a single projection; considering one bit at a time and remembering the previous partial result in a recurrent  12  Under review as a conference paper at ICLR 2016  Figure 9: Results on training tied 1-LSTM networks to compute the k-bit parity of k input bits. The left diagram contains solutions found with 1-LSTM networks with 500 hidden units, whereas the right diagram shows solutions found with 1-LSTM networks with 1500 units. The horizontal axis corresponds to the number k of input bits. The vertical axis corresponds to the number of layers in the networks. Each point in the diagram corresponds to 100% classiﬁcation accuracy of the respective network on a sample of 100 unseen k-bit strings. The networks see up to 10 million bit strings during training but often ﬁnd solutions with many fewer strings. Missing points in the diagram indicate failure to ﬁnd a solution within the training set size or time constraints.  Tied Tanh FFN Tied ReLU FFN Tied 1-LSTM Tied 1-LSTM  Layers Hidden 1500 1500 1500 500  5 4 72 148  Input Bits k  30 30 220 250  Figure 10: The left table reports the best performing networks on k-bit parity. The right ﬁgure is a heat map of activation values of selected counter neurons in a 1-LSTM network that has 25 layers and is trained on the parity of 50-bit strings. The speciﬁc values are obtained by a feed-forward pass through the network using as input the bit string 010140; different bit strings gave similar results.  or multi-step architecture reduces the problem of learning k-bit parity to the simple one of learning just 2-bit parity. Learning parity is difﬁcult because a change in a single bit in the input changes the target value and the decision boundaries in the resulting space are highly non-linear. We train 1-LSTM networks with tied weights and we compare them with fully-connected feed- forward networks with ReLU or tanh activation functions and with either tied or untied weights. We search the space of hyper-parameters as follows. The 1-LSTM networks are trained with either 500 or 1500 hidden units and having from 1 to 150 hidden layers. The 1-LSTM networks are trained on input strings that have from k = 20 to k = 250 bits in increments of 10. The feed-forward ReLU and tanh networks are trained with 500, 1500 or 3000 units and also having from 1 to 150 hidden layers. The latter networks are trained on input bit strings that have between k = 20 and k = 60 bits in increments of 5. Each network is trained with a maximum of 10 million samples or four days of computation on a Tesla K40m GPU. For the optimization we use mini-batches of size 20 and the AdaGrad rule with a learning rate of 0.06 (Duchi et al., 2010). A network is considered to have found the solution if the network correctly computes the parity of 100 randomly sampled unseen k-bit strings. Due to the nature of the problem, during training the predicted accuracy is never better than random guessing and when the network ﬁnds a solution the accuracy suddenly spikes to 100%. Figure 9 depicts the results of the experiments with 1-LSTM networks and Figure 10 relates the best performing networks of each type. For the feed-forward ReLU and tanh networks with either tied or untied weights, we ﬁnd that these networks fail to ﬁnd solutions for k = 35 bits and beyond. Some networks in the search space ﬁnd solutions for k = 30 input bits. By contrast, as represented in Fig. 9, tied 1-LSTM networks ﬁnd solutions for up to k = 250 bits. There appears to be a correlation between the length k of the input bit strings and the minimum depth of the 1-LSTM networks. The minimum depth of the networks increases with k suggesting that longer bit strings need more operations to be applied to them; however, the rate of growth is sub-  13  Input BitsInput BitsLayers050100200150250050100200150250020406080100120140160020406080100120140160Projection1-20020510152025LayerUnder review as a conference paper at ICLR 2016  Figure 11: A 3-LSTM network applied to non-overlapping patches of an image. Each patch is projected to form the input hidden and cell vectors of the depth dimension of the 3-LSTM blocks. The arrows across the spatial dimensions indicate the ﬂow of the computation for that layer. No subsampling or pooling occurs in the networks as the topmost layer simply concatenates all the output hidden and memory vectors of the depth dimension, passes them through a layer of ReLUs and the ﬁnal softmax layer.  linear suggesting that more than a single bit of the input is considered at every step. We visualized the activations of the memory vectors obtained via a feed-forward pass through one of the 1-LSTM networks using selected input bit strings (Fig. 10). This revealed the prominent presence of counting neurons that keep a counter for the number of layers processed so far. These two aspects seem to suggest that the networks are using the cells to process the bit string sequentially by attending to parts of it at each step in the computation, a seemingly crucial feature that is not available in ReLU or tanh transfer functions.  5.2 MNIST DIGIT RECOGNITION  In our last experiment we apply a 3-LSTM network to images. We consider non-overlapping patches of pixels in an image as forming a two-dimensional grid of inputs. The 3-LSTM performs compu- tations with LSTM cells along three different dimensions. Two of the dimensions correspond to the two spatial dimensions of the grid, whereas the remaining dimension is the depth of the network. Like in a convolutional neural network (LeCun et al., 1998), the same three-way transform of the 3-LSTM is applied at all parts of the grid, ensuring that the same features can be extracted across all parts of the input image. Due to the unbounded context size of the 3-LSTM, the computations of features at one end of the image can be inﬂuenced by the features computed at the other end of the image within the same layer. Due to the cells along the depth direction, features from the present patch can be passed onto the next layer either unprocessed or as processed by the layer itself as a function of neighboring patches. We construct the network as depicted in Fig. 11. We divide the 28×28 MNIST image into p×p pixel patches, where p is a small number such as 2 or 4. The patches are then linearized and projected into two vectors of the size of the hidden layer of the 3-LSTM; the projected vectors are the input hidden and memory vectors at the ﬁrst layer in the depth direction of the 3-LSTM. At each layer the computation of the 3-LSTM starts from one corner of the image, follows the two spatial dimensions and ends in the opposite corner of the image. The network has a few layers of depth, each layer starting the computation at one of the corners of the image. In the current form there is no pooling between successive layers of the 3-LSTM. The topmost layer concatenates all the output hidden and memory vectors at all parts of the grid. These are then passed through a layer of ReLUs and a ﬁnal softmax layer. The setup has some similarity with the original application of Multidimensional LSTM to images (Graves, 2012) and with the recently described ReNet architecture (Visin et al., 2015). The differ- ence with the former is that we apply multiple layers of depth to the image, use three-dimensional blocks and concatenate the top output vectors before classiﬁcation. The difference with the ReNet  14  Under review as a conference paper at ICLR 2016  Test Error (%)  Wan et al. (Wan et al., 2013) Graham (Graham, 2014a) Untied 3-LSTM Ciresan et al. (Ciresan et al., 2012) Untied 3-LSTM with ReLU (*) Mairar et al. (Mairal et al., 2014) Lee et al. (Lee et al., 2015) Simard et al. (Simard et al., 2003) Graham (Graham, 2014b) Goodfellow et al. (Goodfellow et al., 2013) Visin et al. (Visin et al., 2015) Lin et al. (Lin et al., 2013)  0.28 0.31 0.32 0.35 0.36 0.39 0.39 0.4 0.44 0.45 0.45 0.47  Figure 12: Test error on the MNIST dataset. All approaches are convolutional networks except for Visin et al. that uses a stack of single-direction recurrent neural networks. (*) This Grid LSTM has non-LSTM connections along the depth only and uses the ReLU instead.  architecture is that the 3-LSTM processes the image according to the two inherent spatial dimen- sions; instead of stacking hidden layers as in the ReNet, the block also modulates directly what information is passed along the depth dimension. The training details are as follows. The MNIST dataset consists of 50000 training images, 10000 validation images and 10000 test images. The pixel values are normalized by dividing them by 255. Data augmentation is performed by shifting training images from 0 to 4 pixels in the horizontal and vertical directions and padding with zero values. The shift in the two directions is chosen uniformly at random. Validation samples are used for retraining the best model settings found during the grid search. We train the 3-LSTM both with and without cells in the depth dimension. The 3-LSTM with the cells uses patches of 2 × 2 pixels, has four LSTM layers with 100 hidden units and one ReLU layer with 4096 units. The 3-LSTM without the cells in the depth dimension has input patches of size 3 × 3 obtained by cropping the image to a size of 27 × 27, it also has four LSTM layers of 100 units and has a ReLU layer of 2048 units. For the latter model we use ReLU as transfer function for the depth direction as in Eq. 6. We use mini-batches of size 128 and train the models using Adam and a learning rate of 0.001. Figure 12 reports test set errors of our models and that of competing approaches. We can see that even in the absence of pooling the 3-LSTM with the cells performs near the state-of-the-art. The 3-LSTM without the cells also performs quite well; the cells in the depth direction likely help with the feature extraction at the higher layers. The other approaches, with the exception of ReNet, are convolutional neural networks.  15  ",
1511.05622,2016,Predicting distributions with Linearizing Belief Networks,"['Predicting distributions with Linearizing Belief Networks\nYann Dauphin', 'David Grangier']",https://arxiv.org/pdf/1511.05622,"6 1 0 2     y a M 2         ]  G L . s c [      4 v 2 2 6 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  PREDICTING DISTRIBUTIONS WITH LINEARIZING BELIEF NETWORKS  Yann N. Dauphin, David Grangier Facebook AI Research 1 Hacker Way Menlo Park, CA 94025, USA {ynd,grangier}@fb.com  ABSTRACT  Conditional belief networks introduce stochastic binary variables in neural net- works. Contrary to a classical neural network, a belief network can predict more than the expected value of the output Y given the input X. It can predict a distribu- tion of outputs Y which is useful when an input can admit multiple outputs whose average is not necessarily a valid answer. Such networks are particularly rele- vant to inverse problems such as image prediction for denoising, or text to speech. However, traditional sigmoid belief networks are hard to train and are not suited to continuous problems. This work introduces a new family of networks called linearizing belief nets or LBNs. A LBN decomposes into a deep linear network where each linear unit can be turned on or off by non-deterministic binary latent units. It is a universal approximator of real-valued conditional distributions and can be trained using gradient descent. Moreover, the linear pathways efﬁciently propagate continuous information and they act as multiplicative skip-connections that help optimization by removing gradient diffusion. This yields a model which trains efﬁciently and improves the state-of-the-art on image denoising and facial expression generation with the Toronto faces dataset.  1  INTRODUCTION  Deep neural networks are universal approximators that can learn any deterministic mapping f : X → Y given enough capacity. However, traditional neural networks are not universal approxima- tors of conditional distributions p(Y |X). In the context of continuous data, neural networks with the mean squared error can be derived from maximum likelihood on a unimodal Gaussian distri- bution p(Y |X) = N (µ = f (X), σ = 1) where the network f predicts the expected value. Thus conventional networks could not learn output distributions with multiple modes. This kind of dis- tribution occurs for instance when trying to predict an image Y based on a description X. This distribution would be the set of images that ﬁts the description - not a single image. In general, a similar situation occurs for most ill-posed or inverse problems - whenever the model does not have enough information to rule out all uncertainty over outcomes. In these situations, the unimodal prior forces the network to average the outcomes as illustrated in Figure 1. This is problematic because in many cases this generates an invalid prediction and in the case of images this is exempliﬁed by blurry average predictions. We observe that this occurs in several important applications of neural networks to the continuous domain – i.e. predicting the next frame of video (Srivastava et al., 2015) or learning unsupervised models with regularized autoencoders (Bengio et al., 2013b). Stochastic feed-forward neural networks (Neal, 1992) (SFNN) solve this problem with the intro- duction of stochastic latent variables to the network. The model can be seen as a mixture of neural networks where each conﬁguration of stochastic variables deﬁnes a different neural network. This is efﬁciently achieved by sharing most of the parameters between conﬁgurations. While conventional neural networks ﬁt a single conditional Gaussian to the data, the stochastic latent variables lead to ﬁtting a mixture of conditional Gaussians. This a powerful extension since mixture of Gaussians are universal approximators of distributions (Sorenson & Alspach, 1971). The network can model multi-modal distributions by learning a different network for each mode. Neal (1992) proposes training Sigmoid Belief Networks (SBN) which have only binary stochastic units. The resulting  1  Published as a conference paper at ICLR 2016  Figure 1: The optimal setting of a uni-modal Gaussian (purple) for a distribution with just two modes (blue) results in an incorrect density model and a high standard deviation. This is a simple illustration of the averaging of outcomes we observe in practice for more interesting problems.  model makes piecewise-constant MAP predictions and thus is unsuitable for continuous problems – it cannot vary smoothly with the input, see Section 5. Tang & Salakhutdinov (2013) addresses this limitation with the addition of deterministic sigmoid units to each layer of the network. This yields a mixture of non-linear neural networks gated by a stochastic non-linear neural network. Tang & Salakhutdinov (2013) showed improved results with this model but the training of the latent stochas- tic units with a high variance variational bound was a challenge. Raiko et al. (2014) suggested to avoid training the latent units, relying only on layers of deterministic units to shape the random distri- bution. This modiﬁcation did not however eliminate a fundamental limitation of stochastic networks in which stochastic and deterministic units interact additively. In these networks, the gradients of the weights tied to deterministic units have much lower variance than those tied to stochastic units, which means it is harder assign credit to stochastic units and training prefers conﬁguration using the deterministic ones as much as possible. In this paper, we propose a new class of stochastic networks called linearizing belief nets (LBN) that can learn any continuous distribution p(Y |X). This model combines deterministic variables with non-deterministic binary variables in a multiplicative fashion. This approach allows using linear deterministic units without losing modeling power. These linear units can be thought of as multiplicative skip connections that allows the gradient to ﬂow without diffusion through deep net- works (Hochreiter et al., 2001). Furthermore, multiplicative interactions allow making tie-breaking choices which would be difﬁcult to emulate with addition. Our experiments on facial expressions conﬁrm that the model can successfully learn multimodal distributions. We demonstrate with image denoising that the model can attain state-of-the-art results modeling natural images - converging faster than even deterministic ReLU networks in some cases.  2 LINEARIZING BELIEF NETS  This section introduces a new family of belief networks dubbed linearizing belief nets (LBN) aimed to address some of the limitations of conditional SBNs. These models factor into a deep linear network where each linear unit is gated by a non-deterministic binary unit. It can be seen as a non-linear mixture of an exponential amount of linear models and it can learn piece-wise linear non- deterministic functions as illustrated in Figure 2. The binary gating units can select the appropriate specialist sub-network for an input by turning off linear units. For instance, the mixture could contain different specialist sub-networks to generate different types of dogs. Let us consider the problem of predicting the target y ∈ RM from the input x ∈ RN . In general, a conditional mixture model can be written as  (cid:88)  (cid:88)  p(y|x) =  p(y, g|x) =  p(g|x)p(y|x, g)  (1)  g  g  where p(g|x) is the probability of selecting the expert identiﬁed by the gating conﬁguration g and p(y|x, g) is the prediction of that expert. Contrary to classical mixtures like Gaussian mixture mod- els (GMM), stochastic networks share parameters across experts which enable training exponentially more experts.  2  Published as a conference paper at ICLR 2016  Figure 2: Flow chart of a LBN with a single gating layer. The information ﬂows linearly through the active units and the gating units introduce non-linearity by deactivating units. The charts at the bottom illustrate how each block can transform its input in a 2D case.  The output of the network p(y|x, g) = N (µ = f (x, g)) for a given input and gating conﬁguration is  h = g ⊗ Wx f (x, g) = Vh + b  (2) (3) where ⊗ denotes component-wise vector product, g ∈ {0, 1}H are the binary gating units and V ∈ RM×H , W ∈ RH×N , b ∈ RM are learned parameters. Each expert is a par- ticular combination of a multi-layer linear network. The non-linearity of the model comes from the gating units g turning linear factors on or off – much like a rectifying non-linearity ReLU(Wx) = max(0, Wx) = (Wx > 0) ⊗ Wx. We show that linearizing blocks generalize the ReLU in Appendix A.3. The binary units can be seen as controlling the ﬂow of information in the network. At the same time, the linear factors transmit continuous values, which is crucial to address continuous prediction tasks. Hence the model disentangles the problem of deciding when to ﬁre and how much to ﬁre. This model is one of the simplest ways to learn smooth functions with binary latent variables. The set of experts are the 2H different conﬁgurations of the binary units g. Now a key question is how to learn an appropriate distribution over these units efﬁciently. The gating units are sampled according to a Bernoulli distribution p(g|x) = B(p = g(Wx)), where each unit gi is sampled independently with rate gi(Wx) set by the non-linear gating function g. Section 3 discusses the speciﬁc parameterization of g. The gating network decides which units to activate/deactivate and allows modeling complex patterns. Therefore, the gating function takes the linear activation vector Wx as input and can implement rich interactions between factors such as winner-takes-all. Adding such pooling interactions to the networks is known to improve the generalization performance (Boureau et al., 2010; Goodfellow et al., 2013). We propose these non- linear gating units as a general strategy to learn pooling functions. In addition, the gating units allows the model to easily represent sparse activations. This is an interesting property since the reconstruction of high frequency signals by summing a few high frequency dictionary elements is a common, effective strategy (Mallat, 2008). In our experiments, we oberve that our model indeed learns sparse, orthogonal features, see Section 6. The conditionals corresponding to f and g deﬁnes the model. Different objectives can be employed to train their parameters: data likelihood, variational bounds (Tang & Salakhutdinov, 2013), or a distiguishibility criteria (Goodfellow et al., 2014). This work focuses on maximum likelihood and we estimate the computationally expensive expectation from Equation 1 using Monte Carlo with k  3  Published as a conference paper at ICLR 2016  samples  log p(y|x) (cid:39) log  k(cid:88)  i=1  1 k  1(cid:112)(2π)M  (cid:18)  exp  (cid:19)  (cid:107)f (x, g{i}) − y(cid:107)2  − 1 2  (4)  where g{1}, . . . , g{k} are k samples from p(g|x). The model p(y|x) is a mixture of Gaussians and can therefore approximate any conditional distribu- tion (Sorenson & Alspach, 1971). In contrast, traditional neural networks estimate the parameters of a single Gaussian. Differentiating the loss reveals the presence of a softmax between the Gaussians  log p(y|x) ∝ k(cid:88)  i=1  − ∂ ∂θ  (cid:18)  softmax  (cid:107)f (x, g(i)) − y(cid:107)2  − 1 2  (cid:19) ∂  ∂θ  (cid:13)(cid:13)(cid:13)f (x, g(i)) − y  (cid:13)(cid:13)(cid:13)2  .  (5)  The gradient concentrates on the sample f (x, g{i}) closest to target y and updates the model to move f (x, g{i}) toward target y. Thus the model can learn different Gaussians to account for the diverse set of alternative outcomes y for the input x. One can further note that relying on a single Monte Carlo sample (k = 1) reverts to mean square error minimization. The combination of the linear skip- connections with binary latent variable helps learning as it prevents gradient diffusion (Hochreiter et al., 2001). For instance, in the gradient  ∂ log p(y|x)  ∂Wi,j  = gj xi  ∂ log p(y|x)  ∂h  +  ∂g  ∂Wi,j  ∂ log p(y|x)  ∂h  there is a path for the gradient to ﬂow without down weighting through the network. gj ∈ {0, 1} either selects the full gradient or cancels it much like in a ReLU networks.  3 LEARNING NON-DETERMINISTIC GATING UNITS The rates of the gating units g ∼ B(p = g(Wx)) are parameterized by the gating function g. We implement g with a neural network which keeps training simple without compromising the power of the model. A simple choice would be to use g(Wx) = σ(Wx + b) with the sigmoid function σ : x → 1+exp(−x) to decide how likely we are to turn on a unit. This function prefers activations Wix which are larger than their learned threshold bi. Our empirical evaluation invariably found better results with deeper functions. Deeper functions allow the gating units to model richer inter- actions between the factors. Therefore we propose a sigmoid multi-layer network  1  g(Wx) = σ(W(2)σ(W(1)Wx + b(1)) + b(2))  with the parameters {W(1), b(1), W(2), b(2)}. This equation showcases a single hidden layer but models with additional layers can also be considered. Deﬁning g as a neural network allows joint training with f through gradient descent. The difﬁculty for through sampling operations g ∼ B(p = g(Wx)) which makes h binary. Bengio et al. (2013a) proposed a solution based on re- inforcement learning, while Tang & Salakhutdinov (2013) explored a variational learning approach. These solutions unfortunately results in high-variance gradient estimates. We use a lower variance estimator introduced recently by Raiko et al. (2014). This approach decomposes the stochastic units into  in computing gradients  training resides  gi = gi(Wx) + (cid:15)i with (cid:15)i =  with probability gi(Wx) with probability 1 − gi(Wx)  (cid:26)1 − gi(Wx)  −gi(Wx)  which expresses the Bernoulli unit as the sum of the deterministic term gi(Wx) and the stochastic term (cid:15). The strategy propagates the gradient only through the deterministic term which is the output of the gating function and ignores the gradient coming from (cid:15)i. Noting that the term (cid:15) has zero mean, that is E[g] = g(Wx), Raiko et al. (2014) ﬁnds this method incurs only a small bias and works well in practice. This strategy is simple to implement as it amounts to sampling the probabilities gi for the forward pass, and back-propagating through the gating function as if there was no sampling.  4  Published as a conference paper at ICLR 2016  4 DEEP LINEARIZING BELIEF NETS  latent variable distribution as p(g|x) = (cid:81)L  Multiple layers of non-deterministic variables g(1), . . . , g(L) can be beneﬁcial. This factorizes the l p(g(l)|x, g(l−1)) and increases modeling efﬁciently. The resulting deep LBN is a hierarchical mixture that has layers of shared factors. This extension yields the following density for two layers  p(y|x) =  p(g(2)|x, g(1))p(g(1)|x)p(y|x, g(1), g(2)).  (cid:88)  g(1),g(2)  In that case, the linear expert adds a new linear layer along with the new gating units  p(y|x) ∝ e− 1  2(cid:107)f (x,g(i))−y(cid:107)2 with f (x, g(1), g(2)) = U(g(2) ⊗ V(g(1) ⊗ Wx)).  The distribution of the ﬁrst layer gating units remain unchanged while the gating units of the second layer follow p(g(2)|x, g(1)) = B(p = g(2)(V(g(1) ⊗ Wx))). One can note that the second layer gating function takes the activation of the second layer linear units as input.  5 RELATED WORK  Neal (1992) proposed one of the earliest uses of neural networks for modeling multi-modal distri- butions. This model is not suitable for continuous distributions because it is piece-wise constant p(y|x, g) = p(y|g) - it does not vary smoothly with the input. Bishop (1994) described a model more suitable for continuous problems called mixture density networks (MDN). The approach for- goes stochastic latent variables and instead has the network directly predict the means of K Gaus- sians. While it can model continuous distributions, the model is intractable in many cases because the number of parameters grows linearly with the number of modes. Tang & Salakhutdinov (2013) improved upon the SBN with the addition of deterministic hidden variables to model continuous distributions. The stochastic and deterministic units are concate- nated at each layer to form the representation. In effect, the contributions of the deterministic and stochastic units are combined additively at the next layer. Thus the stochastic units cannot easily switch on or off the deterministic factors. Moreover, training the network can be cumbersome. It requires training a deep deterministic neural network at the same as the stochastic units through a high variance variational bound. By contrast, the deterministic part of LBNs is linear and easy to train because it is linear. Moreover, we optimize likelihood directly through a technique with lesser variance, see Section 3. The difﬁculty of training SFNNs is discussed by Raiko et al. (2014). This work ﬁnds that better performance can be achieved by training only the deterministic units and setting the probability of activation of the Bernoulli units to p = 0.5. The random units are like additional inputs to the network but it differs from simply injecting noise because the mixture loss of Equation 1 is used. We refer to this method as randomized SFNN (RSFNN) from hereon. The challenge here is that this method is less efﬁcient than adapting the stochastic units. More gener- ally, these different approaches relies on the additive combination of the deterministic units with the stochastic units. The estimates of the gradients of the weights of these units have different variance – with much higher variance for the stochastic part. Training can get trapped in conﬁguration that does not fully exploit the stochastic units. Goroshin et al. (2015) explores an alternative strategy and forgo full probabilistic modeling to focus on MAP inference. It introduces non-deterministic hidden variables and performing MAP inference on them. MAP inference at training time can be intensive computationally. The linearizing net has connections to the spike and slab RBM (Courville et al., 2011) which has both Gaussian and Bernoulli units that interact multiplicatively. However, the spike and slab RBM is more difﬁcult to train because it requires MCMC. The architectures are also different because spike and slab places the binary units before the linear units in the ﬂow graph which would make gradient descent challenging.  6 EXPERIMENTS  This section evaluates the modeling power of LBNs and other stochastic networks on multi-modal distributions. In particular, we will experimentally conﬁrm the claim that LBNs learn faster and  5  Published as a conference paper at ICLR 2016  Figure 3: Output distributions for 3 input test subjects predicted by the RSFNN and LBN. The LBN generates a distribution of faces that have more varied expressions and that maintain the identity more closely. It also showcases the ability of the network to output a diverse distribution of images instead of just the mean prediction.  generalize better than other stochastic networks described in the literature. To do so, we consider the tasks of modeling facial expressions and image denoising on benchmark datasets. We train networks with the Adam (Kingma & Ba, 2014) gradient-based optimizer and the parameter initialization of (Glorot & Bengio, 2010). We found it was optimal to initialize the biases of all units in the gating networks to −2 to promote sparsity. The hyper-parameters of the network are cross-validated using a grid search where the learning rate is always taken from {10−3, 10−4, 10−5}, while the other hyper-paremeters are found in a task speciﬁc manner. All experiments are run the same hardware (Nvidia Tesla K40m GPUs) and all compared techniques are given the same training time.  6.1 MODELING FACIAL EXPRESSIONS  This section compares different approaches to stochastic feed-forward neural networks on the task of predicting the distribution of facial expressions of a person given a picture with a neutral expression as in Tang & Salakhutdinov (2013) and Raiko et al. (2014). The input x is the average face of the person and we have a distribution of pictures ysad, . . . , yangry of that person with 7 different emotions. The goal is to be able to produce the full set of facial expressions for a face we have not seen before. The pictures are taken from the Toronto Face Dataset (TFD) (Susskind et al., 2010) which contains 4,000 images of 900 subjects which were asked to display 7 different emotions. Following the setting of Tang & Salakhutdinov (2013), we randomly selected 95 subjects with 1,318 images for training, 5 subjects with 68 images for validation and 24 individuals totaling 343 images were used as a test set. This reproduces the setting from Tang & Salakhutdinov (2013) as closely as possible. The networks were trained for 200 iterations on the training set with up to k = 200 Monte Carlo samples to estimate the expectation over outcomes. We consider various methods including RSFNNs Raiko et al. (2014), mixtures of factor analysers (MFA), conditional Gaussian RBMs (C-GRBM) and SFNNs. The stochastic networks are trained with 4 layers with either 128 or 256 deterministic hidden units. ReLU activations are used for the deterministic units as they were found to be good for continuous problems. The 2 intermediary layers are augmented with either 32 or 64 random Bernoulli units. The number of hidden units in the LBNs was chosen from {128, 256} with the number of hidden layers ﬁxed to 1. The gating network has 2 hidden layers with {64, 128} hidden units. The hidden units of the gating network are also sampled under a Bernoulli distribution. This allows the gating pattern h to be itself to be multi-modal and results in better results. Table 1 reports our results for RSFNN and LBN as well as the results from Tang & Salakhutdinov (2013) for the other techniques. Test likelihood is evaluated through Monte-Carlo sampling, like for training. The signiﬁcant difference between LBNs and RSFNNs compared to the other models can be explained by their use of training methods which have much more variance. The C-GRBM requires Gibbs sampling during training for instance. The results shows superior generalization  6  Published as a conference paper at ICLR 2016  MFA MDN C-GRBM SFNN  1406  1321  1146  1534  RSFNN (K = 200)  2250  LBN  (K = 1)  2007  (K = 50)  (K = 200)  LBN  2564  LBN  2691  Table 1: Average test log-likelihood predicting facial expressions on TFD. The LBN improves with the number of Monte Carlo samples and reaches state-of-the-art results at K > 50.  performance for the LBN with a difference of 400 nats with RSFNNs. We also ﬁnd that LBN networks converge faster than the RSFNNs in terms of training epochs. By looking at the predicted facial expressions in Figure 3, we conﬁrm that the model is able to output a rich distribution of images. We also ﬁnd that the gap in performance can be explained by the fact that the RSFNNs have difﬁculty maintaining the identity of the face. In the RSFNN, the identity can only be encoded by the deterministic units so this shows that the RSFNN has difﬁculty conditioning on that information. These issues are resolved by the LBN in two ways: the stochastic units can also learn to condition on identity and the multiplicative interactions ease making tie-breaking choices. Interestingly, the log- likelihood on the training set for the LBN is 3081 nats while the RSFNN reaches 2938 nats. Thus the RSFNN allows memorizing the training set in a similar manner, but does not lead to conditional models that generalize as well.  6.2  IMAGE DENOISING  Denoising is a common and challenging task to evaluate unsupervised models of natural images. Typically, the goal is to learn to remove homogeneous additive Gaussian noise from the input. The noise destroys information and so the model must infer the original image from the corrupted signal. In order to do this, the model must discover local statistics of the distribution of images to map a corrupted image to the nearest valid image. Bengio et al. (2013c) showed that under mild conditions the denoising models learn the transition operator of a Markov chain whose stationary distribution is the data distribution. Denoising is an interesting application for LBNs because it is an inverse problem. There is a distribution of clean images that may correspond to a corrupted image, and this distribution may be multimodal for highly corrupted images. This occurs when the noise destroys in- formation beyond perfect recovery. Previous approaches such as the state-of-the-art BM3D (Dabov et al., 2009) method ignore this difﬁculty and simply predict the conditional average. This strategy does not predict plausible images when high noise has truly destroyed information since the con- ditional average exhibits statistics far from genuine images in that case. Typically, such methods results in blurry predictions for highly corrupted images. On the other hand, this problem can be addressed by a model that predicts a distribution from which a set of plausible images explaining the noisy one can be sampled. Of course, this is a difﬁcult problem and we explore here a step in that direction. We show that the LBN improves upon the conditional SBN for image patches and we compare LBNs to state-of-the art methods on full images in the last section. We report results using the Peak Signal to Noise Ration (PSNR) which is given by P SN R(x, y) = −10 log((cid:107)x − y(cid:107)2/N ) where x, y ∈ [0, 1]N . The PSNR is an approximation to the human percep- tion of denoising quality and it relates to the distance between corrupted and clean image. It does not operate on distributions but we use it here since it a well accepted state-of-the-art measure. It would have been better to have a distributional measure but ﬁnding a better denoising quality measure is an open problem. Nonetheless, we can compute the PSNR with a representative point from the distribution either by drawing a random sample, computing the mean or the MAP - which would be the gold standard. For one hidden layer models, we compute the mean exactly by setting g = g(x). We ﬁnd that using this strategy works well in practice. For deeper models, we simply draw a sample from the model since the other quantities are difﬁcult to estimate exactly. In addition to reporting PSNR, we also qualitatively evaluate the diversity of the distribution visually.  6.2.1  IMAGE PATCHES  In this section, we will show that LBNs signiﬁcantly improves upon conditional SBNs for denoising image patches. We consider small image patches here for computational reasons, as training on larger patches is computationally intensive. We extract 19 × 19 image patches from the Imagenet dataset. The dataset has close to 1.3M images and we extract a random subsample of 13M grayscale patches - with 10,000 patches held out as a test set. We will consider the problem of denoising with  7  Published as a conference paper at ICLR 2016  Figure 4: Filters learned by denoising 19 × 19 image patches on Imagenet for 2 layer models. The C-SBN learns poor global features while the ReLU and LBN naturally learn Gabor-like ﬁlters.  a Gaussian with standard deviation 25 (over 255). We preprocess the patches by reducing to the [0, 1] interval, substracting the mean of µ = 0.5 and dividing by the standard deviation σ = 0.2. We will compare conditional SBNs, LBNs and deterministic ReLU networks as a good general baseline. The LBNs have 1 non-deterministic hidden layer and the gating function has 3 layers of sigmoids. The ReLU networks were trained with either 2 or 6 layers, with 6 layers always improving results. We trained the C-SBNs with 6 layers also. All models were trained with 1024 hidden units. The 6 layer ReLU network and C-SBN have close to 4.9M parameters, while the LBN has 1M less with 3.8M parameters. The gradient of the binary units in the C-SBNs is found using the same estimator used in the LBNs so they are directly comparable. The Monte Carlo estimation of the expectation during training is computed with either 1 or 10 samples. All networks are trained with 10 iterations over the 13M patches.  Figure 5: Average test PSNR versus training time for patch-wise image denoising. LBNs achieve superior results with a smaller budget of parameters and time compared to conditional SBNs and even a deep ReLU network.  We have plotted the learning curves with respect to training time in Figure 5. The LBN signif- icantly outperforms the conditional SBN in terms of training time and test PSNR. The improved convergence speed can be explained in part by the linear units which allow the gradient to ﬂow better through the network. What’s more, the fact that LBNs allow higher PSNRs showcases the superior approximation efﬁciency of a mixture of linear models over a mixture of constants. Sur- prisingly, the LBN compares favorably in terms of converge speed even compared to deterministic ReLU networks. The power of the representation learned by LBNs is evidenced by the fact that it requires 6 layers of ReLUs to come close to the same level of performance. Unlike with the ReLU representations, the gating units can resolve ambiguity and competition in the representation. On the qualitative side, we can see ReLU and LBN networks learn localized Gabor ﬁlters while the C-SBN learns point detectors. These results show that the LBN model successfully addresses some of the ﬂaws of conditional SBNs (see Appendix A.1 for more).  8  Published as a conference paper at ICLR 2016  6.2.2 FULL IMAGES  In this section, we evaluate the problem of training LBNs with multiple layers of non-deterministic variables on denoising full images. The state-of-the art method for denoising natural images is BM3D (Dabov et al., 2009). Neural methods (Burger et al., 2012) were state-of-the-art but they were surpassed by later versions of BM3D. BM3D is a non-parametric denoising algorithm that uses self-similarity between patches to average out the noise. While we can expect this method to work well for small noise distributions, it produces blurry images for high noise distributions. In order to scale to large images we use a convolutional architecture for the LBNs. In practice, this amounts to replacing the dot products by convolutions - both for the activations of the linear units and the gaters. We found that using 128 convolutional kernels of size 9 × 9 for the linear and gating units produced good results. The network has 4 convolutional hidden layers in total and 3 layer gating functions for LBNs. We do not use any spatial pooling because the loss of information would be detrimental to denoising. The output pixels have a scalar bias so as to not constrain the size of the images that can be generated. We extract 1M 64 × 64 image patches from Imagenet as our training set for highly corrupted images and 6M 29 × 29 patches otherwise. We use these large image patches instead of the full image to save computation time during training but the same model can be applied to large images at test time. The networks can take up to a week of computation time to train and so in the interest of the PSNR evaluation we set the number of Monte Carlo samples to K = 1. We evaluate the quality of the distribution with a model trained with K = 10. We compare our algorithm to Gaussian Scale Mixture (GSM) (Portilla et al., 2003), Field of Experts (FoE) (Roth & Black, 2005), K-SVD (Elad & Aharon, 2006), BM3D (Dabov et al., 2009) and Learned Simultaneous Sparse Coding (LSSC) (Mairal et al., 2010) on the standard 11 test images they used.  Image Barbara Boat C.man Couple F.print Hill House Lena Man Montage Peppers  GSM FoE 22.61 19.77 20.80 23.75  21.22  25.11 25.64  - -  -  - -  - - - -  - -  21.66 21.87  22.60  19.60  - -  18.30  KSVD BM3D LSSC LBN 21.89 23.22 24.62 22.81 24.08 24.27 21.52 25.15 27.21 26.75 24.76 25.45 24.13  23.62 23.97 23.07 23.51 21.61 24.58 25.87 25.95 24.22 23.89 23.39  23.59 23.84 23.08 23.28 21.26 24.44 25.83 25.82 24.00  23.71 25.64  21.75  23.00  - -  -  -  Table 2: Denoising PSNR on standard test images with σ = 100. The LBN reaches state-of-the-art.  Table 2 shows the results denoising the standard test images in the setting of a Gaussian noise with σ = 100. We ﬁnd that LBNs overall produce a substantial improvement over the state-of- the-art methods (Appendix A.2 shows state-of-the-art results for other settings). The two images where BM3D sets the state-of-the-art have highly repetitive structure which favors its approach. Figure 6 shows that the LBN produces arguably better qualitative results even on those images. In particular the images are much sharper and more closely resemble natural images. In the context of lower noise, the LBN also achieve state-of-the-art results with an average test PSNR of 30.70 dB at σ = 25 (with BM3D at 30.4 dB) but it learns a deterministic mapping. This makes sense because there is little uncertainty over the clean images. At σ = 100, we ﬁnd through visual inspection that the model can generate a distribution of alternatives (Videos available at http: //ynd.github.io/lbn_denoising_demo/). In these videos we see better variety in the distribution with K = 10 compared to K = 1. However, as we can also see in Figure 6 the model does not capture enough about the distribution to produce very plausible denoised images - there is still much averaging. This suggests that estimating the expectation over outcomes with Monte Carlo samples is not a good enough estimator for highly multimodal distributions. An interesting direction would be to improve our estimate using importance sampling. We believe this is a good direction to further improve denoising results. Alternatively, one could forgo maximum likelihood  9  Published as a conference paper at ICLR 2016  Figure 6: Visualization of denoising for the Barbara image with σ = 100. Top-left is the original, top-right the noisy image, bottom-left is BM3D and bottom right is the output of a LBN. The LBN produces qualitatively more pleasing denoised images. There is a video of the distribution at http: //ynd.github.io/lbn_denoising_demo/.  training of the model and use an adversarial objective instead (Goodfellow et al., 2014). Nonetheless - even if the training method has to be improved - our results conﬁrm the LBN model improves upon traditional conditional belief net models.  7 CONCLUSION  This work introduces linearizing belief nets (LBN), a new class of conditional belief network. As a belief network, a LBN relies on stochastic binary units but is well suited to model continuous distributions. Contrary to prior work, LBN stochastic units act as gaters to a deep linear network. This multiplicative interaction between stochastic and deterministic units allows better cooperation between the two parts of the network compared to prior additive strategies. Moreover, LBN linear units propagate continuous information efﬁciently and combined with stochastic binary gating acts as skip-connections that prevent gradient diffusion and help learning. Our experiments conﬁrm these advantages. Our facial expression generation experiments result in better generalization and faster convergence for LBN compared to alternative belief networks. Our image denoising experiments also report better signal-to-noise ratio than previous work. Overall, this work proposes a generic model that can be relevant to various inverse problems. In the future, we want to investigate alterna- tives to our current Monte Carlo maximum likelihood training. In particular, we consider adversarial training or importance sampling to model distribution with more modes efﬁciently.  10  Published as a conference paper at ICLR 2016  ACKNOWLEDGEMENTS  The authors would like to thank Marc’Aurelio Ranzato for insightful comments and discussions.  REFERENCES Bengio, Yoshua, L´eonard, Nicholas, and Courville, Aaron. Estimating or propagating gradients through  stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013a.  Bengio, Yoshua, Thibodeau-Laufer, Eric, Alain, Guillaume, and Yosinski, Jason. Deep generative stochastic  networks trainable by backprop. arXiv preprint arXiv:1306.1091, 2013b.  Bengio, Yoshua, Yao, Li, Alain, Guillaume, and Vincent, Pascal. Generalized denoising auto-encoders as  generative models. In Advances in Neural Information Processing Systems, pp. 899–907, 2013c.  Bishop, Christopher M. Mixture density networks. 1994.  Boureau, Y-Lan, Ponce, Jean, and LeCun, Yann. A theoretical analysis of feature pooling in visual recognition. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 111–118, 2010.  Burger, Harold C, Schuler, Christian J, and Harmeling, Stefan. Image denoising: Can plain neural networks compete with bm3d? In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 2392–2399. IEEE, 2012.  Courville, Aaron C, Bergstra, James, and Bengio, Yoshua. A spike and slab restricted boltzmann machine. In  International Conference on Artiﬁcial Intelligence and Statistics, pp. 233–241, 2011.  Dabov, Kostadin, Foi, Alessandro, Katkovnik, Vladimir, and Egiazarian, Karen. Bm3d image denoising with shape-adaptive principal component analysis. In SPARS’09-Signal Processing with Adaptive Sparse Struc- tured Representations, 2009.  Elad, Michael and Aharon, Michal. Image denoising via sparse and redundant representations over learned  dictionaries. Image Processing, IEEE Transactions on, 15(12):3736–3745, 2006.  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural networks.  In International conference on artiﬁcial intelligence and statistics, pp. 249–256, 2010.  Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672–2680, 2014.  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout net-  works. arXiv preprint arXiv:1302.4389, 2013.  Goroshin, Ross, Mathieu, Micha¨el, and LeCun, Yann. Learning to linearize under uncertainty. CoRR,  abs/1506.03011, 2015. URL http://arxiv.org/abs/1506.03011.  Hochreiter, Sepp, Bengio, Yoshua, Frasconi, Paolo, and Schmidhuber, Jrgen. Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, 2001.  Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,  2014. URL http://arxiv.org/abs/1412.6980.  Mairal, Julien, Bach, Francis, Ponce, Jean, and Sapiro, Guillermo. Online learning for matrix factorization and  sparse coding. The Journal of Machine Learning Research, 11:19–60, 2010.  Mallat, Stephane. A Wavelet Tour of Signal Processing, Third Edition: The Sparse Way. Academic Press, 2008.  Neal, Radford M. Connectionist learning of belief networks. Artiﬁcial intelligence, 56(1):71–113, 1992.  Portilla, Javier, Strela, Vasily, Wainwright, Martin J, and Simoncelli, Eero P.  Image denoising using scale mixtures of gaussians in the wavelet domain. Image Processing, IEEE Transactions on, 12(11):1338–1351, 2003.  Raiko, Tapani, Berglund, Mathias, Alain, Guillaume, and Dinh, Laurent. Techniques for learning binary  stochastic feedforward neural networks. arXiv preprint arXiv:1406.2989, 2014.  11  Published as a conference paper at ICLR 2016  Roth, Stefan and Black, Michael J. Fields of experts: A framework for learning image priors. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 2, pp. 860–867. IEEE, 2005.  Sorenson, H. W. and Alspach, D. L. Recursive bayesian estimation using gaussian sums. Automatica, 7(4):  465–479, 1971.  Srivastava, Nitish, Mansimov, Elman, and Salakhutdinov, Ruslan. Unsupervised learning of video representa-  tions using lstms. arXiv preprint arXiv:1502.04681, 2015.  Susskind, Joshua, Anderson, Adam, and Hinton, Geoffrey. The toronto face database. 2010.  Tang, Yichuan and Salakhutdinov, Ruslan R. Learning stochastic feedforward neural networks. In Advances in  Neural Information Processing Systems, pp. 530–538, 2013.  12  Published as a conference paper at ICLR 2016  A ADDITIONAL RESULTS  A.1 LBNS ON NATURAL IMAGE PATCHES (FROM SECTION 6.2.1)  Figure 7: Histogram of the hidden activations of the LBN before (blue) and after (red) gating. The gating units resolve ambiguity and remove redundant activations leading to a sparser representation.  Figure 8: Full 1024 feature detectors of the the LBN. Virtually all the ﬁlters converge to meaningful feature detectors, which is not the case for several models.  13  - -  -  - -  - -  -  - -  - - - -  - -  - - - -  - -  Published as a conference paper at ICLR 2016  A.2 LBNS ON FULL NATURAL IMAGES (FROM SECTION 6.2.2)  Image Barbara Boat C.man Couple F.print Hill House Lena Man Montage Peppers  GSM FoE 29.13 27.04 28.72 29.37  27.45  31.40 31.69  31.11 30.82  29.21  29.20  - -  18.30  KSVD BM3D LSSC LBN 29.60 30.24 30.25 29.28 29.98 30.19 27.77 30.06 33.31 32.51 30.05 32.90 30.81  30.72 29.91 29.45 29.72 27.7 29.85 32.86 32.08 29.62 32.37 30.16  30.47 29.87 29.51 29.61 27.62 29.80 33.15 31.87 29.63  32.15 31.32  29.73  30.21  - -  -  -  Table 3: Denoising PSNR on standard test images with σ = 25. We compare our algorithm to Gaussian Scale Mixture (GSM) (Portilla et al., 2003), Field of Experts (FoE) (Roth & Black, 2005), K-SVD (Elad & Aharon, 2006), BM3D (Dabov et al., 2009) and Learned Simultaneous Sparse Coding (LSSC) (Mairal et al., 2010).  Image Barbara Boat C.man Couple F.print Hill House Lena Man Montage Peppers  GSM FoE 23.65 23.15 24.53 24.79  22.40  26.41 26.84  26.74 26.49  24.00  24.52  - -  18.30  KSVD BM3D LSSC LBN 25.47 26.37 27.25 25.95 27.11 27.02 24.30 27.48 30.58 29.45 27.13 29.03 27.21  27.23 26.78 26.12 26.46 24.53 27.19 29.69 29.05 26.81 27.9 26.68  27.06 26.74 26.42 26.30 24.25 27.05 30.04 28.87 26.69  27.95 27.79  26.13  26.62  - -  -  -  Table 4: Denoising PSNR on standard test images with σ = 50. We compare our algorithm to Gaussian Scale Mixture (GSM) (Portilla et al., 2003), Field of Experts (FoE) (Roth & Black, 2005), K-SVD (Elad & Aharon, 2006), BM3D (Dabov et al., 2009) and Learned Simultaneous Sparse Coding (LSSC) (Mairal et al., 2010).  A.3 LINKS BETWEEN LBNS AND RELU NETWORKS  ReLU networks can be seen as a particular deterministic subset of the LBN family of networks. The function of a ReLU network is given by f (x) = Vmax(0, Wx) = V((Wx > 0) ◦ Wx). This is the form of a LBN with gating units sampled from a Dirac delta distribution h ∼ δ(g(Wx)) where the gating function g(Wx) = Wx > 0. Altogether we have f (x) = V(h◦ Wx) which is the form of a linearizing network. We can relax the determinism of the gating units by using a Bernoulli distribution with g(Wx) = σ(Wx + b). This function is very close to that of the ReLU because the sigmoid is a relaxation of the threshold function. Interestingly, by performing MAP inference at test time we recover the ReLU since σ(Wx + b) > 0.5 = Wx + b > 0. We have found experimentally that this simple model produces similar results to the ReLU. This link gives some intuition as to why the LBN model is more powerful than simple ReLUs. The LBNs can have more powerful gating functions while the gating function of ReLus is ﬁxed and much less powerful (it is a simple threshold).  14  ",
1511.07289,2016,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),"['Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\nDjork-Arné  Clevert', 'Thomas Unterthiner', 'Sepp Hochreiter']",https://arxiv.org/pdf/1511.07289,"6 1 0 2     b e F 2 2         ]  G L . s c [      5 v 9 8 2 7 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)  Djork-Arn´e Clevert, Thomas Unterthiner & Sepp Hochreiter Institute of Bioinformatics Johannes Kepler University, Linz, Austria {okko,unterthiner,hochreit}@bioinf.jku.at  ABSTRACT  We introduce the “exponential linear unit” (ELU) which speeds up learning in deep neural networks and leads to higher classiﬁcation accuracies. Like recti- ﬁed linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe- LUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gra- dient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of pres- ence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to signiﬁcantly bet- ter generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks signiﬁcantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classiﬁcation error for a single crop, single model network.  1  INTRODUCTION  Currently the most popular activation function for neural networks is the rectiﬁed linear unit (ReLU), which was ﬁrst proposed for restricted Boltzmann machines (Nair & Hinton, 2010) and then suc- cessfully used for neural networks (Glorot et al., 2011). The ReLU activation function is the identity for positive arguments and zero otherwise. Besides producing sparse codes, the main advantage of ReLUs is that they alleviate the vanishing gradient problem (Hochreiter, 1998; Hochreiter et al., 2001) since the derivative of 1 for positive values is not contractive (Glorot et al., 2011). However ReLUs are non-negative and, therefore, have a mean activation larger than zero. Units that have a non-zero mean activation act as bias for the next layer. If such units do not cancel each other out, learning causes a bias shift for units in next layer. The more the units are correlated, the higher their bias shift. We will see that Fisher optimal learning, i.e., the natural gradient (Amari, 1998), would correct for the bias shift by adjusting the weight updates. Thus, less bias shift brings the standard gradient closer to the natural gradient and speeds up learning. We aim at activation functions that push activation means closer to zero to decrease the bias shift effect. Centering the activations at zero has been proposed in order to keep the off-diagonal entries of the Fisher information matrix small (Raiko et al., 2012). For neural network it is known that centering  1  Published as a conference paper at ICLR 2016  the activations speeds up learning (LeCun et al., 1991; 1998; Schraudolph, 1998). “Batch normaliza- tion” also centers activations with the goal to counter the internal covariate shift (Ioffe & Szegedy, 2015). Also the Projected Natural Gradient Descent algorithm (PRONG) centers the activations by implicitly whitening them (Desjardins et al., 2015). An alternative to centering is to push the mean activation toward zero by an appropriate activation function. Therefore tanh has been preferred over logistic functions (LeCun et al., 1991; 1998). Re- cently “Leaky ReLUs” (LReLUs) that replace the negative part of the ReLU with a linear function have been shown to be superior to ReLUs (Maas et al., 2013). Parametric Rectiﬁed Linear Units (PReLUs) generalize LReLUs by learning the slope of the negative part which yielded improved learning behavior on large image benchmark data sets (He et al., 2015). Another variant are Ran- domized Leaky Rectiﬁed Linear Units (RReLUs) which randomly sample the slope of the negative part which raised the performance on image benchmark datasets and convolutional networks (Xu et al., 2015). In contrast to ReLUs, activation functions like LReLUs, PReLUs, and RReLUs do not ensure a noise-robust deactivation state. We propose an activation function that has negative values to allow for mean activations close to zero, but which saturates to a negative value with smaller arguments. The saturation decreases the variation of the units if deactivated, so the precise deactivation argument is less relevant. Such an activation function can code the degree of presence of particular phenomena in the input, but does not quantitatively model the degree of their absence. Therefore, such an acti- vation function is more robust to noise. Consequently, dependencies between coding units are much easier to model and much easier to interpret since only activated code units carry much information. Furthermore, distinct concepts are much less likely to interfere with such activation functions since the deactivation state is non-informative, i.e. variance decreasing.  2 BIAS SHIFT CORRECTION SPEEDS UP LEARNING  To derive and analyze the bias shift effect mentioned in the introduction, we utilize the natural gra- dient. The natural gradient corrects the gradient direction with the inverse Fisher information matrix and, thereby, enables Fisher optimal learning, which ensures the steepest descent in the Riemannian parameter manifold and Fisher efﬁciency for online learning (Amari, 1998). The recently introduced Hessian-Free Optimization technique (Martens, 2010) and the Krylov Subspace Descent methods (Vinyals & Povey, 2012) use an extended Gauss-Newton approximation of the Hessian, therefore they can be interpreted as versions of natural gradient descent (Pascanu & Bengio, 2014). Since for neural networks the Fisher information matrix is typically too expensive to compute, dif- ferent approximations of the natural gradient have been proposed. Topmoumoute Online natural Gradient Algorithm (TONGA) (LeRoux et al., 2008) uses a low-rank approximation of natural gra- dient descent. FActorized Natural Gradient (FANG) (Grosse & Salakhudinov, 2015) estimates the natural gradient via an approximation of the Fisher information matrix by a Gaussian graphical model. The Fisher information matrix can be approximated by a block-diagonal matrix, where unit or quasi-diagonal natural gradients are used (Olivier, 2013). Unit natural gradients or “Unitwise Fisher’s scoring” (Kurita, 1993) are based on natural gradients for perceptrons (Amari, 1998; Yang & Amari, 1998). We will base our analysis on the unit natural gradient. We assume a parameterized probabilistic model p(x; w) with parameter vector w and data x. The training data are X = (x1, . . . , xN ) ∈ R(d+1)×N with xn = (zT n , yn)T ∈ Rd+1, where zn is the input for example n and yn is its label. L(p(.; w), x) is the loss of example x = (zT , y)T using model p(.; w). The average loss on the training data X is the empirical risk Remp(p(.; w), X). Gradient descent updates the weight vector w by wnew = wold− η∇wRemp where η is the learning rate. The natural gradient is the inverse Fisher information matrix ˜F −1 multiplied by the gradient of the empirical risk: ∇nat w Remp = ˜F −1∇wRemp. For a multi-layer perceptron a is the unit activation vector and a0 = 1 is the bias unit activation. We consider the ingoing weights to unit i, therefore we drop the index i: wj = wij for the weight from unit j to unit i, a = ai for the activation, and w0 for j wjaj of unit i to its activation a = f (net). For computing the Fisher information matrix, the derivative of the log-output probability ∂ ∂net ln p(z; w), ∂wj  the bias weight of unit i. The activation function f maps the net input net =(cid:80)  ln p(z; w) is required. Therefore we deﬁne the δ at unit i as δ = ∂  2  Published as a conference paper at ICLR 2016  which can be computed via backpropagation, but using the log-output probability instead of the conventional loss function. The derivative is ∂ ∂wj  ln p(z; w) = δaj.  We restrict the Fisher information matrix to weights leading to unit i which is the unit Fisher in- formation matrix F . F captures only the interactions of weights to unit i. Consequently, the unit natural gradient only corrects the interactions of weights to unit i, i.e. considers the Riemannian parameter manifold only in a subspace. The unit Fisher information matrix is  [F (w)]kj = Ep(z;w)  = Ep(z;w)(δ2 ak aj) .  (1)  Weighting the activations by δ2 is equivalent to adjusting the probability of drawing inputs z. In- puts z with large δ2 are drawn with higher probability. Since 0 ≤ δ2 = δ2(z), we can deﬁne a distribution q(z):  q(z) = δ2(z) p(z)  δ2(z) p(z) dz  = δ2(z) p(z) E−1  p(z)(δ2) .  (2)  (cid:19)−1  (cid:18) ∂ ln p(z; w)  ∂ ln p(z; w)  ∂wk  ∂wj  (cid:19)  Using q(z), the entries of F can be expressed as second moments:  (cid:18)(cid:90)  (cid:90)  [F (w)]kj = Ep(z)(δ2 ak aj) =  δ2 ak aj p(z) dz = Ep(z)(δ2) Eq(z)(ak aj) .  (3)  b = Ep(z)(δ2a) = Ep(z)(δ2) Eq(z)(a) = Covp(z)(δ2, a) + Ep(z)(a) Ep(z)(δ2) .  If the bias unit is a0 = 1 with weight w0 then the weight vector can be divided into a bias part w0 and the rest w: (wT , w0)T . For the row b = [F (w)]0 that corresponds to the bias weight, we have: (4) The next Theorem 1 gives the correction of the standard gradient by the unit natural gradient where the bias weight is treated separately (see also Yang & Amari (1998)). Theorem 1. The unit natural gradient corrects the weight update (∆wT , ∆w0)T to a unit i by following afﬁne transformation of the gradient ∇(wT ,w0)T Remp = (gT , g0)T :  (cid:18) ∆w  (cid:19)  ∆w0  =  (cid:18)A−1 (g − ∆w0 b) s (cid:0)g0 − bT A−1g(cid:1)(cid:19)  ,  (5)  where A = [F (w)]¬0,¬0 = Ep(z)(δ2)Eq(z)(aaT ) is the unit Fisher information matrix without row 0 and column 0 corresponding to the bias weight. The vector b = [F (w)]0 is the zeroth column of F corresponding to the bias weight, and the positive scalar s is  s = E−1  p(z)(δ2)  1 + ET  q(z)(a) Var−1  q(z)(a) Eq(z)(a)  ,  (6)  where a is the vector of activations of units with weights to unit i and q(z) = δ2(z)p(z)E−1  p(z)(δ2).  (cid:16)  (cid:17)  ∆w0  where  b = [F (w)]0  ,  Proof. Multiplying separated (cid:19) ∇(wT ,w0)T Remp((wT , w0)T , X) = (gT , g0)T gives the weight update (∆wT , ∆w0)T :  (cid:18)A−1 g + u s−1uT g + go u  (cid:19)−1 (cid:18) g  (cid:18) A b  (cid:18) ∆w  Fisher matrix F −1 with  inverse  (cid:19)  (cid:19)  the  the  =  bT  c  g0  =  uT g + s g0  , u = − s A−1 b ,  (cid:18) ∆w  (cid:19)  c = [F (w)]00  (cid:18)A−1 g + s−1u ∆w0  (cid:19)  (cid:18) ∆w  (cid:19)  The previous formula is derived in Lemma 1 in the appendix. Using ∆w0 in the update gives  =  ∆w0  (9) The right hand side is obtained by inserting u = −sA−1b in the left hand side update. Since c = F00 = Ep(z)(δ2), b = Ep(z)(δ2)Eq(z)(a), and A = Ep(z)(δ2)Eq(z)(aaT ), we obtain  uT g + s g0  ∆w0  =  ,  .  s = (cid:0)c − bT A−1b(cid:1)−1 (cid:18)A−1 (g − ∆w0 b) s (cid:0)g0 − bT A−1g(cid:1)(cid:19) (cid:17)−1  .  gradient  (7)  . (8)  = E−1  p(z)(δ2)  1 − ET  q(z)(a) E−1  q(z)(aaT ) Eq(z)(a)  .  (10)  s = (cid:0)c − bT A−1b(cid:1)−1  (cid:16)  Applying Lemma 2 in the appendix gives the formula for s.  3  Published as a conference paper at ICLR 2016  The bias shift (mean shift) of unit i is the change of unit i’s mean value due to the weight update. Bias shifts of unit i lead to oscillations and impede learning. See Section 4.4 in LeCun et al. (1998) for demonstrating this effect at the inputs and in LeCun et al. (1991) for explaining this effect using the input covariance matrix. Such bias shifts are mitigated or even prevented by the unit natural gradient. The bias shift correction of the unit natural gradient is the effect on the bias shift due to b which captures the interaction between the bias unit and the incoming units. Without bias shift correction, i.e., b = 0 and s = c−1, the weight updates are ∆w = A−1g and ∆w0 = c−1g0. As only the activations depend on the input, the bias shift can be computed by multiplying the weight update by the mean of the activation vector a. Thus we obtain the bias shift (Ep(z)(a)T , 1)(∆wT , ∆w0)T = p(z)(a)A−1g + c−1g0. The bias shift strongly depends on the correlation of the incoming units ET which is captured by A−1. Next, Theorem 2 states that the bias shift correction by the unit natural gradient can be considered to correct the incoming mean Ep(z)(a) proportional to Eq(z)(a) toward zero. Theorem 2. The bias shift correction by the unit natural gradient is equivalent to an additive cor- rection of the incoming mean by −k Eq(z)(a) and a multiplicative correction of the bias unit by k, where  k = 1 + (cid:0)Eq(z)(a) − Ep(z)(a)(cid:1)T  Var−1  q(z)(a) Eq(z)(a) .  (11)  (12)  (cid:17)  g0 .  (13)  p(z)(a) A−1b (cid:17)−1 (cid:17) (cid:125)  Proof. Using ∆w0 = −sbT A−1g + sg0, the bias shift is:  (cid:19)T (cid:18) ∆w (cid:18)Ep(z)(a) (cid:19) ∆w0 1 (cid:32) p(z)(a) A−1 g + p(z)(a) − (cid:16) (cid:124)  = ET  ET  =  (cid:16)  (cid:18)Ep(z)(a)  (cid:19)T (cid:18)A−1 g − A−1b ∆w0 = 1 1 − ET p(z)(a) A−1b (cid:17) (cid:16) (cid:123)(cid:122)  (cid:17) (cid:33) (cid:125)  p(z)(a) A−1b  A−1 g + s  ∆w0  ∆w0  s bT  1 − ET  (cid:19)  1 − ET  The mean correction term, indicated by an underbrace in previous formula, is  (cid:16)  s  =  (cid:16)  1 − ET 1 − ET 1 − ET  (cid:16) (cid:16) (cid:124)  (cid:17) p(z)(a) A−1b p(z)(a) E−1 q(z)(a) E−1 (cid:17)  (cid:16) (cid:17) (cid:17)−1(cid:16) (cid:123)(cid:122)  k  b = E−1  p(z)(δ2)  q(z)(aaT ) Eq(z)(a)  q(z)(aaT ) Eq(z)(a)  1 − ET  q(z)(a) E−1 Ep(z)(δ2) Eq(z)(a) 1 − ET p(z)(a) E−1  q(z)(aaT ) Eq(z)(a)  q(z)(aaT ) Eq(z)(a)  Eq(z)(a).  The expression Eq. (11) for k follows from Lemma 2 in the appendix. The bias unit correction term is s  g0 = kc−1g0.  1 − ET  p(z)(a)A−1b  In Theorem 2 we can reformulate k = 1 + E−1 q(z)(a)Eq(z)(a). Therefore k increases with the length of Eq(z)(a) for given variances and covariances. Consequently the bias shift correction through the unit natural gradient is governed by the length of Eq(z)(a). The bias shift correction is zero for Eq(z)(a) = 0 since k = 1 does not correct the bias unit multiplicatively. Using Eq. (4), Eq(z)(a) is split into an offset and an information containing term:  p(z)(δ2, a)Var−1  p(z)(δ2)CovT  Eq(z)(a) = Ep(z)(a) + E−1  p(z)(δ2) Covp(z)(δ2, a) .  (14)  In general, smaller positive Ep(z)(a) lead to smaller positive Eq(z)(a), therefore to smaller correc- tions. The reason is that in general the largest absolute components of Covp(z)(δ2, a) are positive, since activated inputs will activate the unit i which in turn will have large impact on the output. To summarize, the unit natural gradient corrects the bias shift of unit i via the interactions of in- coming units with the bias unit to ensure efﬁcient learning. This correction is equivalent to shifting the mean activations of the incoming units toward zero and scaling up the bias unit. To reduce the  4  Published as a conference paper at ICLR 2016  undesired bias shift effect without the natural gradient, either the (i) activation of incoming units can be centered at zero or (ii) activation functions with negative values can be used. We introduce a new activation function with negative values while keeping the identity for positive arguments where it is not contradicting.  3 EXPONENTIAL LINEAR UNITS (ELUS)  The exponential linear unit (ELU) with 0 < α is  f (x) =  α (exp(x) − 1)  if x > 0 if x ≤ 0  f(cid:48)(x) =  ,  (cid:26)x  (cid:26)1  if x > 0 f (x) + α if x ≤ 0  .  (15)  The ELU hyperparameter α controls the value to which an ELU saturates for negative net inputs (see Fig. 1). ELUs diminish the vanishing gradient effect as rectiﬁed linear units (ReLUs) and leaky ReLUs (LReLUs) do. The vanishing gradient problem is alleviated because the positive part of these functions is the identity, therefore their derivative is one and not contractive. In contrast, tanh and sigmoid activation functions are contractive almost everywhere. In contrast to ReLUs, ELUs have negative values which pushes the mean of the activations closer to zero. Mean activations that are closer to zero enable faster learning as they bring the gradient closer to the natural gradient (see Theorem 2 and text thereafter). ELUs saturate to a negative value when the argument gets smaller. Saturation means a small derivative which decreases the variation and the information that is propagated to the next layer. Therefore the representation is both noise-robust and low-complex (Hochreiter & Schmidhuber, 1999). ELUs code the degree of presence of input concepts, while they nei- ther quantify the degree of their absence nor distin- guish the causes of their absence. This property of non-informative deactivation states is also present at ReLUs and allowed to detect biclusters corresponding to biological modules in gene expression datasets (Clevert et al., 2015) and to identify toxicophores in toxicity prediction (Unterthiner et al., 2015; Mayr et al., 2015). The enabling features for these interpretations is that activation can be clearly distinguished from deactivation and that only active units carry relevant information and can crosstalk.  Figure 1: The rectiﬁed linear unit (ReLU), the leaky ReLU (LReLU, α = 0.1), the shifted ReLUs (SReLUs), and the exponen- tial linear unit (ELU, α = 1.0).  4 EXPERIMENTS USING ELUS  In this section, we assess the performance of exponential linear units (ELUs) if used for unsupervised and supervised learning of deep autoencoders and deep convolutional networks. ELUs with α = 1.0 are compared to (i) Rectiﬁed Linear Units (ReLUs) with activation f (x) = max(0, x), (ii) Leaky ReLUs (LReLUs) with activation f (x) = max(αx, x) (0 < α < 1), and (iii) Shifted ReLUs (SReLUs) with activation f (x) = max(−1, x). Comparisons are done with and without batch normalization. The following benchmark datasets are used: (i) MNIST (gray images in 10 classes, 60k train and 10k test), (ii) CIFAR-10 (color images in 10 classes, 50k train and 10k test), (iii) CIFAR-100 (color images in 100 classes, 50k train and 10k test), and (iv) ImageNet (color images in 1,000 classes, 1.3M train and 100k tests).  4.1 MNIST  4.1.1 LEARNING BEHAVIOR  We ﬁrst want to verify that ELUs keep the mean activations closer to zero than other units. Fully connected deep neural networks with ELUs (α = 1.0), ReLUs, and LReLUs (α = 0.1) were trained on the MNIST digit classiﬁcation dataset while each hidden unit’s activation was tracked. Each  5  −1012−10.0−7.5−5.0−2.50.0xf(x)ELULReLUReLUSReLUPublished as a conference paper at ICLR 2016  (a) Average unit activation  (b) Cross entropy loss  Figure 2: ELU networks evaluated at MNIST. Lines are the average over ﬁve runs with different random initializations, error bars show standard deviation. Panel (a): median of the average unit activation for different activation functions. Panel (b): Training set (straight line) and validation set (dotted line) cross entropy loss. All lines stay ﬂat after epoch 25.  network had eight hidden layers of 128 units each, and was trained for 300 epochs by stochastic gra- dient descent with learning rate 0.01 and mini-batches of size 64. The weights have been initialized according to (He et al., 2015). After each epoch we calculated the units’ average activations on a ﬁxed subset of the training data. Fig. 2 shows the median over all units along learning. ELUs stay have smaller median throughout the training process. The training error of ELU networks decreases much more rapidly than for the other networks. Section C in the appendix compares the variance of median activation in ReLU and ELU networks. The median varies much more in ReLU networks. This indicates that ReLU networks continuously try to correct the bias shift introduced by previous weight updates while this effect is much less prominent in ELU networks.  4.1.2 AUTOENCODER LEARNING  (a) Training set  (b) Test set  Figure 3: Autoencoder training on MNIST: Reconstruction error for the test and training data set over epochs, using different activation functions and learning rates. The results are medians over several runs with different random initializations.  6  050100150200250300epoch0.10.20.30.40.50.60.70.8Avg. Unit Activationelureluleaky0510152025epoch0.00.10.20.30.40.50.60.70.8Cross Entropy Losselureluleaky0100200300400500epoch100101102103Reconstruction errorlr 1e­02lr 1e­03lr 1e­04lr 1e­05lr 1e­02lr 1e­03lr 1e­04lr 1e­05lr 1e­02lr 1e­03lr 1e­04lr 1e­050100200300400500epoch100101102Reconstruction errorlr 1e­02lr 1e­03lr 1e­04lr 1e­05lr 1e­02lr 1e­03lr 1e­04lr 1e­05lr 1e­02lr 1e­03lr 1e­04lr 1e­05Published as a conference paper at ICLR 2016  To evaluate ELU networks at unsupervised settings, we followed Martens (2010) and Desjardins et al. (2015) and trained a deep autoencoder on the MNIST dataset. The encoder part consisted of four fully connected hidden layers with sizes 1000, 500, 250 and 30, respectively. The decoder part was symmetrical to the encoder. For learning we applied stochastic gradient descent with mini- batches of 64 samples for 500 epochs using the ﬁxed learning rates (10−2, 10−3, 10−4, 10−5). Fig. 3 shows, that ELUs outperform the competing activation functions in terms of training / test set recon- struction error for all learning rates. As already noted by Desjardins et al. (2015), higher learning rates seem to perform better.  4.2 COMPARISON OF ACTIVATION FUNCTIONS  In this subsection we show that ELUs indeed possess a superior learning behavior compared to other activation functions as postulated in Section 3. Furthermore we show that ELU networks perform better than ReLU networks with batch normalization. We use as benchmark dataset CIFAR-100 and use a relatively simple convolutional neural network (CNN) architecture to keep the computational complexity reasonable for comparisons.  (a) Training loss  (b) Training loss (start)  (c) Training loss (end)  (d) Test error  (e) Test error (start)  (f) Test error (end)  Figure 4: Comparison of ReLUs, LReLUs, and SReLUs on CIFAR-100. Panels (a-c) show the training loss, panels (d-f) the test classiﬁcation error. The ribbon band show the mean and standard deviation for 10 runs along the curve. ELU networks achieved lowest test error and training loss.  The CNN for these CIFAR-100 experiments consists of 11 convolutional layers arranged in stacks of ([1 × 192 × 5], [1 × 192 × 1, 1 × 240 × 3], [1 × 240 × 1, 1 × 260 × 2], [1 × 260 × 1, 1 × 280 × 2], [1 × 280 × 1, 1 × 300 × 2], [1 × 300 × 1], [1 × 100 × 1]) layers × units × receptive ﬁelds. 2×2 max-pooling with a stride of 2 was applied after each stack. For network regularization we used the following drop-out rate for the last layer of each stack (0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.0). The L2-weight decay regularization term was set to 0.0005. The following learning rate schedule was applied (0 − 35k[0.01], 35k − 85k[0.005], 85k − 135k[0.0005], 135k − 165k[0.00005]) (iterations [learning rate]). For fair comparisons, we used this learning rate schedule for all networks. During previous experiments, this schedule was optimized for ReLU networks, however as ELUs converge faster they would beneﬁt from an adjusted schedule. The momentum term learning rate was ﬁxed to 0.9. The dataset was preprocessed as described in Goodfellow et al. (2013) with global contrast  7  1234050100150Updates (1e3)Train Lossreluleakysreluelu12340255075Updates (1e3)Train Lossreluleakysreluelu0.30.40.50.690110130150Updates (1e3)Train Lossreluleakysreluelu406080100050100150Updates (1e3)Test Error [%]reluleakysreluelu4060801000255075Updates (1e3)Test Error [%]reluleakysreluelu2930313290110130150170Updates (1e3)Test Error [%]reluleakysrelueluPublished as a conference paper at ICLR 2016  (a) ELU - ReLU  (b) ELU - SReLU  (c) ELU - LReLU  (d) ELU - ReLU (end)  (e) ELU - SReLU (end)  (f) ELU - LReLU (end)  Figure 5: Pairwise comparisons of ELUs with ReLUs, SReLUs, and LReLUs with and without batch normalization (BN) on CIFAR-100. Panels are described as in Fig. 4. ELU networks outperform ReLU networks with batch normalization.  normalization and ZCA whitening. Additionally, the images were padded with four zero pixels at all borders. The model was trained on 32 × 32 random crops with random horizontal ﬂipping. Besides that, we no further augmented the dataset during training. Each network was run 10 times with different weight initialization. Across networks with different activation functions the same run number had the same initial weights. Mean test error results of networks with different activation functions are compared in Fig. 4, which also shows the standard deviation. ELUs yield on average a test error of 28.75(±0.24)%, while SRe- LUs, ReLUs and LReLUs yield 29.35(±0.29)%, 31.56(±0.37)% and 30.59(±0.29)%, respectively. ELUs achieve both lower training loss and lower test error than ReLUs, LReLUs, and SReLUs. Both the ELU training and test performance is signiﬁcantly better than for other activation func- tions (Wilcoxon signed-rank test with p-value<0.001). Batch normalization improved ReLU and LReLU networks, but did not improve ELU and SReLU networks (see Fig. 5). ELU networks signiﬁcantly outperform ReLU networks with batch normalization (Wilcoxon signed-rank test with p-value<0.001).  4.3 CLASSIFICATION PERFORMANCE ON CIFAR-100 AND CIFAR-10  The following experiments should highlight the generalization capabilities of ELU networks. The CNN architecture is more sophisticated than in the previous subsection and consists of 18 convolu- tional layers arranged in stacks of ([1× 384× 3], [1× 384× 1, 1× 384× 2, 2× 640× 2], [1× 640× 1, 3 × 768 × 2], [1 × 768 × 1, 2 × 896 × 2], [1 × 896 × 3, 2 × 1024 × 2], [1 × 1024 × 1, 1 × 1152 × 2], [1 × 1152 × 1], [1 × 100 × 1]). Initial drop-out rate, Max-pooling after each stack, L2-weight decay, momentum term, data preprocessing, padding, and cropping were as in previous section. The initial learning rate was set to 0.01 and decreased by a factor of 10 after 35k iterations. The mini- batch size was 100. For the ﬁnal 50k iterations ﬁne-tuning we increased the drop-out rate for all  8  406080100050100150Updates (1e3)Test Error [%]relu_bnreluelu_bnelu406080100050100150Updates (1e3)Test Error [%]srelu_bnsreluelu_bnelu406080100050100150Updates (1e3)Test Error [%]leaky_bnleakyelu_bnelu2930313290110130150170Updates (1e3)Test Error [%]relu_bnreluelu_bnelu2930313290110130150170Updates (1e3)Test Error [%]srelu_bnsreluelu_bnelu29303190110130150170Updates (1e3)Test Error [%]leaky_bnleakyelu_bneluPublished as a conference paper at ICLR 2016  layers in a stack to (0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.0), thereafter increased the drop-out rate by a factor of 1.5 for 40k additional iterations.  Table 1: Comparison of ELU networks and other CNNs on CIFAR-10 and CIFAR-100. Reported is the test error in percent misclassiﬁcation for ELU networks and recent convolutional architectures like AlexNet, DSN, NiN, Maxout, All-CNN, Highway Network, and Fractional Max-Pooling. Best results are in bold. ELU networks are second best for CIFAR-10 and best for CIFAR-100.  Network AlexNet DSN NiN Maxout All-CNN Highway Network Fract. Max-Pooling ELU-Network  CIFAR-10 (test error %) 18.04 7.97 8.81 9.38 7.25 7.60 4.50 6.55  CIFAR-100 (test error %) 45.80 34.57 35.68 38.57 33.71 32.24 27.62 24.28  augmented √ √ √ √ √ √  ELU networks are compared to following recent successful CNN architectures: AlexNet (Krizhevsky et al., 2012), DSN (Lee et al., 2015), NiN (Lin et al., 2013), Maxout (Goodfellow et al., 2013), All-CNN (Springenberg et al., 2014), Highway Network (Srivastava et al., 2015) and Fractional Max-Pooling (Graham, 2014). The test error in percent misclassiﬁcation are given in Tab. 1. ELU-networks are the second best on CIFAR-10 with a test error of 6.55% but still they are among the top 10 best results reported for CIFAR-10. ELU networks performed best on CIFAR-100 with a test error of 24.28%. This is the best published result on CIFAR-100, without even resorting to multi-view evaluation or model averaging.  4.4  IMAGENET CHALLENGE DATASET  Finally, we evaluated ELU-networks on the 1000-class ImageNet dataset. It contains about 1.3M training color images as well as additional 50k images and 100k images for validation and testing, respectively. For this task, we designed a 15 layer CNN, which was arranged in stacks of (1 × 96 × 6, 3 × 512 × 3, 5 × 768 × 3, 3 × 1024 × 3, 2 × 4096 × F C, 1 × 1000 × F C) layers × units × receptive ﬁelds or fully-connected (FC). 2×2 max-pooling with a stride of 2 was applied after each stack and spatial pyramid pooling (SPP) with 3 levels before the ﬁrst FC layer (He et al., 2015). For network regularization we set the L2-weight decay term to 0.0005 and used 50% drop-out in the two penultimate FC layers. Images were re-sized to 256×256 pixels and per-pixel mean subtracted. Trained was on 224 × 224 random crops with random horizontal ﬂipping. Besides that, we did not augment the dataset during training. Fig. 6 shows the learning behavior of ELU vs. ReLU networks. Panel (b) shows that ELUs start reducing the error earlier. The ELU-network already reaches the 20% top-5 error after 160k itera- tions, while the ReLU network needs 200k iterations to reach the same error rate. The single-model performance was evaluated on the single center crop with no further augmentation and yielded a top-5 validation error below 10%. Currently ELU nets are 5% slower on ImageNet than ReLU nets. The difference is small because activation functions generally have only minor inﬂuence on the overall training time (Jia, 2014). In terms of wall clock time, ELUs require 12.15h vs. ReLUs with 11.48h for 10k iterations. We ex- pect that ELU implementations can be improved, e.g. by faster exponential functions (Schraudolph, 1999).  5 CONCLUSION  We have introduced the exponential linear units (ELUs) for faster and more precise learning in deep neural networks. ELUs have negative values, which allows the network to push the mean activations  9  Published as a conference paper at ICLR 2016  (a) Training loss  (b) Top-5 test error  (c) Top-1 test error  Figure 6: ELU networks applied to ImageNet. The x-axis gives the number of iterations and the y-axis the (a) training loss, (b) top-5 error, and (c) the top-1 error of 5,000 random validation samples, evaluated on the center crop. Both activation functions ELU (blue) and ReLU (purple) lead for convergence, but ELUs start reducing the error earlier and reach the 20% top-5 error after 160k iterations, while ReLUs need 200k iterations to reach the same error rate.  closer to zero. Therefore ELUs decrease the gap between the normal gradient and the unit natural gradient and, thereby speed up learning. We believe that this property is also the reason for the success of activation functions like LReLUs and PReLUs and of batch normalization. In contrast to LReLUs and PReLUs, ELUs have a clear saturation plateau in its negative regime, allowing them to learn a more robust and stable representation. Experimental results show that ELUs signiﬁcantly outperform other activation functions on different vision datasets. Further ELU networks perform signiﬁcantly better than ReLU networks trained with batch normalization. ELU networks achieved one of the top 10 best reported results on CIFAR-10 and set a new state of the art in CIFAR-100 without the need for multi-view test evaluation or model averaging. Furthermore, ELU networks produced competitive results on the ImageNet in much fewer epochs than a corresponding ReLU network. Given their outstanding performance, we expect ELU networks to become a real time saver in convolutional networks, which are notably time-intensive to train from scratch otherwise.  Acknowledgment. We thank the NVIDIA Corporation for supporting this research with several Titan X GPUs and Roland Vollgraf and Martin Heusel for helpful discussions and comments on this work.  REFERENCES Amari, S.-I. Natural gradient works efﬁciently in learning. Neural Computation, 10(2):251–276, 1998. Clevert, D.-A., Unterthiner, T., Mayr, A., and Hochreiter, S. Rectiﬁed factor networks. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 28. Curran Associates, Inc., 2015.  Desjardins, G., Simonyan, K., Pascanu, R., and Kavukcuoglu, K. Natural neural networks. CoRR,  abs/1507.00210, 2015. URL http://arxiv.org/abs/1507.00210.  Glorot, X., Bordes, A., and Bengio, Y. Deep sparse rectiﬁer neural networks. In Gordon, G., Dunson, D., and Dudk, M. (eds.), JMLR W&CP: Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011), volume 15, pp. 315–323, 2011.  Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. Maxout networks. ArXiv e-prints,  2013.  Graham, Benjamin. Fractional max-pooling. CoRR, abs/1412.6071, 2014. URL http://arxiv.org/  abs/1412.6071.  Grosse, R. and Salakhudinov, R. Scaling up natural gradient by sparsely factorizing the inverse Fisher Journal of Machine Learning Research, 37:2304–2313, 2015. URL http://jmlr.org/ matrix. proceedings/papers/v37/grosse15.pdf. Proceedings of the 32nd International Conference on Machine Learning (ICML15).  He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level performance on  imagenet classiﬁcation. In IEEE International Conference on Computer Vision (ICCV), 2015.  10  2460100200Updates (1e3)Train Lossreluelu2550751000100200Updates (1e3)Top 5 Test Error [%]reluelu4060801000100200Updates (1e3)Top 1 Test Error [%]relueluPublished as a conference paper at ICLR 2016  Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions.  International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(2):107–116, 1998.  Hochreiter, S. and Schmidhuber, J. Feature extraction through LOCOCODE. Neural Computation, 11(3):  679–714, 1999.  Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies. In Kremer and Kolen (eds.), A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001.  Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. Journal of Machine Learning Research, 37:448–456, 2015. URL http://jmlr.org/ proceedings/papers/v37/ioffe15.pdf. Proceedings of the 32nd International Conference on Machine Learning (ICML15).  Jia, Yangqing. Learning Semantic Image Representations at a Large Scale. PhD thesis, EECS Department, University of California, Berkeley, May 2014. URL http://www.eecs.berkeley.edu/Pubs/ TechRpts/2014/EECS-2014-93.html.  Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classiﬁcation with deep convolutional neural net- In Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q. (eds.), Advances in Neural  works. Information Processing Systems 25, pp. 1097–1105. Curran Associates, Inc., 2012.  Kurita, T. Iterative weighted least squares algorithms for neural networks classiﬁers. In Proceedings of the Third Workshop on Algorithmic Learning Theory (ALT92), volume 743 of Lecture Notes in Computer Science, pp. 77–86. Springer, 1993.  LeCun, Y., Kanter, I., and Solla, S. A. Eigenvalues of covariance matrices: Application to neural-network  learning. Physical Review Letters, 66(18):2396–2399, 1991.  LeCun, Y., Bottou, L., Orr, G. B., and M¨uller, K.-R. Efﬁcient backprop.  In Orr, G. B. and M¨uller, K.-R. (eds.), Neural Networks: Tricks of the Trade, volume 1524 of Lecture Notes in Computer Science, pp. 9–50. Springer, 1998.  Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick W., Zhang, Zhengyou, and Tu, Zhuowen. Deeply-supervised  nets. In AISTATS, 2015.  LeRoux, N., Manzagol, P.-A., and Bengio, Y. Topmoumoute online natural gradient algorithm. In Platt, J. C., Koller, D., Singer, Y., and Roweis, S. T. (eds.), Advances in Neural Information Processing Systems 20 (NIPS), pp. 849–856, 2008.  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. CoRR, abs/1312.4400, 2013. URL http:  //arxiv.org/abs/1312.4400.  Maas, A. L., Hannun, A. Y., and Ng, A. Y. Rectiﬁer nonlinearities improve neural network acoustic models. In  Proceedings of the 30th International Conference on Machine Learning (ICML13), 2013.  Martens, J. Deep learning via Hessian-free optimization. In F¨urnkranz, J. and Joachims, T. (eds.), Proceedings  of the 27th International Conference on Machine Learning (ICML10), pp. 735–742, 2010.  Mayr, A., Klambauer, G., Unterthiner, T., and Hochreiter, S. DeepTox: Toxicity prediction using deep learning. Front. Environ. Sci., 3(80), 2015. doi: 10.3389/fenvs.2015.00080. URL http://journal. frontiersin.org/article/10.3389/fenvs.2015.00080.  Nair, V. and Hinton, G. E. Rectiﬁed linear units improve restricted Boltzmann machines. In F¨urnkranz, J. and Joachims, T. (eds.), Proceedings of the 27th International Conference on Machine Learning (ICML10), pp. 807–814, 2010.  Olivier, Y. Riemannian metrics for neural networks i: feedforward networks. CoRR, abs/1303.0818, 2013.  URL http://arxiv.org/abs/1303.0818.  Pascanu, R. and Bengio, Y.  In International Con- ference on Learning Representations 2014, 2014. URL http://arxiv.org/abs/1301.3584. arXiv:1301.3584.  Revisiting natural gradient for deep networks.  Raiko, T., Valpola, H., and LeCun, Y. Deep learning made easier by linear transformations in perceptrons. In Lawrence, N. D. and Girolami, M. A. (eds.), Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS12), volume 22, pp. 924–932, 2012.  Schraudolph, N. N. Centering neural network gradient factor. In Orr, G. B. and M¨uller, K.-R. (eds.), Neural Networks: Tricks of the Trade, volume 1524 of Lecture Notes in Computer Science, pp. 207–226. Springer, 1998.  Schraudolph, Nicol N. A Fast, Compact Approximation of the Exponential Function. Neural Computation, 11:  853–862, 1999.  Springenberg, Jost Tobias, Dosovitskiy, Alexey, Brox, Thomas, and Riedmiller, Martin A. Striving for simplic- ity: The all convolutional net. CoRR, abs/1412.6806, 2014. URL http://arxiv.org/abs/1412. 6806.  Srivastava, Rupesh Kumar, Greff, Klaus, and Schmidhuber, J¨urgen. Training very deep networks. CoRR,  abs/1507.06228, 2015. URL http://arxiv.org/abs/1507.06228.  11  Published as a conference paper at ICLR 2016  Unterthiner, T., Mayr, A., Klambauer, G., and Hochreiter, S. Toxicity prediction using deep learning. CoRR,  abs/1503.01445, 2015. URL http://arxiv.org/abs/1503.01445.  Vinyals, O. and Povey, D. Krylov subspace descent for deep learning.  //arxiv.org/pdf/1111.4259v1. arXiv:1111.4259.  In AISTATS, 2012. URL http:  Xu, B., Wang, N., Chen, T., and Li, M. Empirical evaluation of rectiﬁed activations in convolutional network.  CoRR, abs/1505.00853, 2015. URL http://arxiv.org/abs/1505.00853.  Yang, H. H. and Amari, S.-I. Complexity issues in natural gradient descent method for training multilayer  perceptrons. Neural Computation, 10(8), 1998.  A INVERSE OF BLOCK MATRICES  Lemma 1. The positive deﬁnite matrix M is in block format with matrix A, vector b, and scalar c. The inverse of M is  (cid:18) A b  (cid:19)−1  bT  c  (cid:19) (cid:18) K u  uT  s  ,  =  −1 =  M  where  K = A u = − s A  s =  (cid:16) (cid:19)−1 Proof. For block matrices the inverse is(cid:18) A B (cid:16)  where the matrices on the right hand side are: −1 + A −1 B C − BT A −1B  U T = − (cid:16) (cid:16)  K = A U = − A  C − BT A  BT C  (cid:16)  S =  −1 + u s −1 b c − bT A  −1uT  −1b  (cid:17)−1 (cid:18) K U  U T S  .  (cid:19)  =  −1 B C − BT A  C − BT A −1B  −1B  (cid:17)−1  −1  BT A  (cid:17)−1 (cid:17)−1  −1B  .  Further if follows that  K = A  −1 + U S  −1U T .  ,  (cid:17)−1  −1  BT A  We now use this formula for B = b being a vector and C = c a scalar. We obtain  where the right hand side matrices, vectors, and the scalar s are: c − bT A  =  (cid:16)  ,  (cid:17)−1  −1  bT A  (cid:18) A b (cid:19)−1  bT  c  (cid:18) K u (cid:19)  uT  s  (cid:16)  K = A u = − A  uT = − (cid:16) (cid:16)  −1 + A −1 b c − bT A −1b  c − bT A  s =  −1 b c − bT A  (cid:17)−1 (cid:17)−1  −1b  .  −1b  (cid:17)−1  −1b  −1  bT A  Again it follows that  K = A  −1 + u s  −1uT .  12  (16)  (17) (18)  (19)  (20)  (21)  (22)  (23)  (24)  (25)  (26)  (27)  (28)  (29)  (30)  (31)  Published as a conference paper at ICLR 2016  A reformulation using u gives  −1 + u s K = A u = − s A −1 b uT = − s bT A −1 c − bT A  (cid:16)  s =  −1uT  (cid:17)−1  .  −1b  B QUADRATIC FORM OF MEAN AND INVERSE SECOND MOMENT  Lemma 2. For a random variable a holds  ET (a) E  −1(a aT ) E(a) ≤ 1  and  (cid:16) Furthermore holds(cid:16)  Therefore we have  (cid:17)−1 (cid:17)−1 (cid:16)  1 − ET (a) E  −1(a aT ) E(a)  = 1 + ET (a) Var  −1(a) E(a) .  (cid:17)  1 − ET (a) E −1(a aT ) E(a) = 1 + (E(a) − Ep(a))T Var  1 − ET −1(a) E(a) .  p (a) E  −1(a aT ) E(a)  = A  −1 − A−1 b cT A−1 1 + cT A−1b  (cid:16) A + b cT(cid:17)−1 cT(cid:16) A + b bT(cid:17)−1 −1b − cT A−1 b bT A−1b cT A−1b (cid:0)1 + bT A−1b(cid:1) − (cid:0)cT A−1 b(cid:1)(cid:0)bT A−1 b(cid:1) 1 + bT A−1b  b = cT A  .  =  1 + bT A−1b  Proof. The Sherman-Morrison Theorem states  cT A−1b  1 + bT A−1b  .  =  Using the identity  E(a aT ) = Var(a) + E(a) ET (a)  for the second moment and Eq. (40), we get  ET (a) E  −1(a aT ) E(a) = ET (a)  (cid:16)  Var(a) + E(a) ET (a)  (cid:17)−1  E(a)  ET (a) Var−1(a) E(a)  1 + ET (a) Var−1(a) E(a)  =  ≤ 1 .  (32) (33) (34)  (35)  (36)  (37)  (38)  (39)  (40)  (41)  (42)  The last inequality follows from the fact that Var(a) is positive deﬁnite. From last equation, we obtain further  = 1 + ET (a)Var  −1(a) E(a) .  (43)  Var(a) + E(a) ET (a)  (cid:17)−1  E(a)  (44)  (cid:16)  1 − ET (a)E  −1(a aT ) E(a)  For the mixed quadratic form we get from Eq. (40) −1(a aT ) E(a) = ET p (a) Var−1(a) E(a) ET  p (a) E  ET  =  1 + ET (a) Var−1(a) E(a)  p (a)  (cid:17)−1 (cid:16)  .  13  Published as a conference paper at ICLR 2016  From this equation follows 1 − ET  p (a) E  −1(a aT ) E(a) = 1 − 1 + ET (a) Var−1(a) E(a) − ET  =  1 + ET (a) Var−1(a) E(a)  p (a) Var−1(a) E(a) ET  1 + ET (a) Var−1(a) E(a) p (a) Var−1(a) E(a)  (45) 1 + (E(a) − Ep(a))T Var−1(a) E(a)  =  1 + ET (a) Var−1(a) E(a)  .  Therefore we get (cid:16)  (cid:17)−1 (cid:16)  1 − ET (a) E −1(a aT ) E(a) = 1 + (E(a) − Ep(a))T Var  1 − ET −1(a) E(a) .  p (a) E  −1(a aT ) E(a)  (cid:17)  (46)  C VARIANCE OF MEAN ACTIVATIONS IN ELU AND RELU NETWORKS  To compare the variance of median activation in ReLU and ELU networks, we trained a neural network with 5 hidden layers of 256 hidden units for 200 epochs using a learning rate of 0.01, once using ReLU and once using ELU activation functions on the MNIST dataset. After each epoch, we calculated the median activation of each hidden unit on the whole training set. We then calculated the variance of these changes, which is depicted in Figure 7 . The median varies much more in ReLU networks. This indicates that ReLU networks continuously try to correct the bias shift introduced by previous weight updates while this effect is much less prominent in ELU networks.  Figure 7: Distribution of variances of the median hidden unit activation after each epoch of MNIST training. Each row represents the units in a different layer of the network.  14  0.000.010.020.030.040.05050100150200250300Layer 10.000.020.040.060.080.10050100150200250300Layer 20.000.020.040.060.080.10050100150200250Layer 30.000.050.100.150.200.250.30050100150200250300Layer 40.00.10.20.30.40.5050100150200250300Layer 5reluelu",
1511.06342,2016,Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning,"['Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning\nEmilio Parisotto', 'Jimmy Ba', 'Ruslan Salakhutdinov']",https://arxiv.org/pdf/1511.06342,"6 1 0 2     b e F 2 2         ]  G L . s c [      4 v 2 4 3 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  ACTOR-MIMIC DEEP MULTITASK AND TRANSFER REINFORCEMENT LEARNING  Emilio Parisotto, Jimmy Ba, Ruslan Salakhutdinov Department of Computer Science University of Toronto Toronto, Ontario, Canada {eparisotto,jimmy,rsalakhu}@cs.toronto.edu  ABSTRACT  The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. To- wards this goal, we deﬁne a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultane- ously, and then generalize its knowledge to new domains. This method, termed “Actor-Mimic”, exploits the use of deep reinforcement learning and model com- pression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of general- izing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.  1  INTRODUCTION  Deep Reinforcement Learning (DRL), the combination of reinforcement learning methods and deep neural network function approximators, has recently shown considerable success in high- dimensional challenging tasks, such as robotic manipulation (Levine et al., 2015; Lillicrap et al., 2015) and arcade games (Mnih et al., 2015). These methods exploit the ability of deep networks to learn salient descriptions of raw state input, allowing the agent designer to essentially bypass the lengthy process of feature engineering. In addition, these automatically learnt descriptions often sig- niﬁcantly outperform hand-crafted feature representations that require extensive domain knowledge. One such DRL approach, the Deep Q-Network (DQN) (Mnih et al., 2015), has achieved state-of- the-art results on the Arcade Learning Environment (ALE) (Bellemare et al., 2013), a benchmark of Atari 2600 arcade games. The DQN uses a deep convolutional neural network over pixel inputs to parameterize a state-action value function. The DQN is trained using Q-learning combined with sev- eral tricks that stabilize the training of the network, such as a replay memory to store past transitions and target networks to deﬁne a more consistent temporal difference error. Although the DQN maintains the same network architecture and hyperparameters for all games, the approach is limited in the fact that each network only learns how to play a single game at a time, despite the existence of similarities between games. For example, the tennis-like game of pong and the squash-like game of breakout are both similar in that each game consists of trying to hit a moving ball with a rectangular paddle. A network trained to play multiple games would be able to generalize its knowledge between the games, achieving a single compact state representation as the inter-task similarities are exploited by the network. Having been trained on enough source tasks, the multitask network can also exhibit transfer to new target tasks, which can speed up learning. Training DRL agents can be extremely computationally intensive and therefore reducing training time is a signiﬁcant practical beneﬁt.  1  Published as a conference paper at ICLR 2016  The contribution of this paper is to develop and evaluate methods that enable multitask and trans- fer learning for DRL agents, using the ALE as a test environment. To ﬁrst accomplish multitask learning, we design a method called “Actor-Mimic” that leverages techniques from model compres- sion to train a single multitask network using guidance from a set of game-speciﬁc expert networks. The particular form of guidance can vary, and several different approaches are explored and tested empirically. To then achieve transfer learning, we treat a multitask network as being a DQN which was pre-trained on a set of source tasks. We show experimentally that this multitask pre-training can result in a DQN that learns a target task signiﬁcantly faster than a DQN starting from a random initialization, effectively demonstrating that the source task representations generalize to the target task.  2 BACKGROUND: DEEP REINFORCEMENT LEARNING A Markov Decision Process (MDP) is deﬁned as a tuple (S, A, T , R, γ) where S is a set of states, A is a set of actions, T (s(cid:48)|s, a) is the transition probability of ending up in state s(cid:48) when executing action a in state s, R is the reward function mapping states in S to rewards in R, and γ is a discount factor. An agent’s behaviour in an MDP is represented as a policy π(a|s) which deﬁnes the probability of executing action a in state s. For a given policy, we can further deﬁne the Q-value t=0 γtrt|s0 = s, a0 = a] where H is the step when the game ends. The Q-function represents the expected future discounted reward when starting in a state s, executing a, and then following policy π until a terminating state is reached. There always exists at least one optimal state-action value function, Q∗(s, a), such that ∀s ∈ S, a ∈ A, Q∗(s, a) = maxπ Qπ(s, a) (Sutton & Barto, 1998). The optimal Q-function can be rewritten as a Bellman equation:  function Qπ(s, a) = E[(cid:80)H  (cid:20)  (cid:21)  Q∗(s, a) =  E  s(cid:48)∼T (·|s,a)  r + γ · max  a(cid:48)∈A Q∗(s(cid:48), a(cid:48))  .  (1)  An optimal policy can be constructed from the optimal Q-function by choosing, for a given state, the action with highest Q-value. Q-learning, a reinforcement learning algorithm, uses iterative backups of the Q-function to converge towards the optimal Q-function. Using a tabular representation of the Q-function, this is equivalent to setting Q(n+1)(s, a) = Es(cid:48)∼T (·|s,a)[r + γ · maxa(cid:48)∈A Q(n)(s(cid:48), a(cid:48))] for the (n+1)th update step (Sutton & Barto, 1998). Because the state space in the ALE is too large to tractably store a tabular representation of the Q-function, the Deep Q-Network (DQN) approach uses a deep function approximator to represent the state-action value function (Mnih et al., 2015). To train a DQN on the (n+1)th step, we set the network’s loss to  (cid:34)(cid:18)  (cid:19)2(cid:35) a(cid:48)∈A Q(s(cid:48), a(cid:48); θ(n)) − Q(s, a; θ(n+1))  r + γ · max  ,  (2)  L(n+1)(θ(n+1)) =  E  s,a,r,s(cid:48)∼M(·)  where M(·) is a uniform probability distribution over a replay memory, which is a set of the m previous (s, a, r, s(cid:48)) transition tuples seen during play, where m is the size of the memory. The replay memory is used to reduce correlations between adjacent states and is shown to have large effect on the stability of training the network in some games.  3 ACTOR-MIMIC  3.1 POLICY REGRESSION OBJECTIVE  Given a set of source games S1, ..., SN , our ﬁrst goal is to obtain a single multitask policy network that can play any source game at as near an expert level as possible. To train this multitask policy network, we use guidance from a set of expert DQN networks E1, ..., EN , where Ei is an expert specialized in source task Si. One possible deﬁnition of “guidance” would be to deﬁne a squared loss that would match Q-values between the student network and the experts. As the range of the expert value functions could vary widely between games, we found it difﬁcult to directly distill knowledge from the expert value functions. The alternative we develop here is to instead match policies by ﬁrst transforming Q-values using a softmax. Using the softmax gives us outputs which  2  Published as a conference paper at ICLR 2016  are bounded in the unit interval and so the effects of the different scales of each expert’s Q-function are diminished, achieving higher stability during learning. Intuitively, we can view using the softmax from the perspective of forcing the student to focus more on mimicking the action chosen by the guiding expert at each state, where the exact values of the state are less important. We call this method “Actor-Mimic” as it is an actor, i.e. policy, that mimics the decisions of a set of experts. In particular, our technique ﬁrst transforms each expert DQN into a policy network by a Boltzmann distribution deﬁned over the Q-value outputs, πEi(a|s) =  eτ−1QEi (s,a)  (3)  ,  eτ−1QEi (s,a(cid:48))  where τ is a temperature parameter and AEi is the action space used by the expert Ei, AEi ⊆ A. Given a state s from source task Si, we then deﬁne the policy objective over the multitask network as the cross-entropy between the expert network’s policy and the current multitask policy:  Li policy(θ) =  πEi(a|s) log πAMN(a|s; θ),  (4)  (cid:80)  a(cid:48)∈AEi  where πAMN(a|s; θ) is the multitask Actor-Mimic Network (AMN) policy, parameterized by θ. In contrast to the Q-learning objective which recursively relies on itself as a target value, we now have a stable supervised training signal (the expert network output) to guide the multitask network. To acquire training data, we can sample either the expert network or the AMN action outputs to generate the trajectories used in the loss. Empirically we have observed that sampling from the AMN while it is learning gives the best results. We later prove that in either case of sampling from the expert or AMN as it is learning, the AMN will converge to the expert policy using the policy regression loss, at least in the case when the AMN is a linear function approximator. We use an (cid:15)-greedy policy no matter which network we sample actions from, which with probability (cid:15) picks a random action uniformly and with probability 1 − (cid:15) chooses an action from the network.  (cid:88)  a∈AEi  3.2 FEATURE REGRESSION OBJECTIVE  We can obtain further guidance from the expert networks in the following way. Let hAMN(s) and hEi(s) be the hidden activations in the feature (pre-output) layer of the AMN and i’th expert net- work computed from the input state s, respectively. Note that the dimension of hAMN(s) does not necessarily need to be equal to hEi(s), and this is the case in some of our experiments. We deﬁne a feature regression network fi(hAMN(s)) that, for a given state s, attempts to predict the features hEi(s) from hAMN(s). The architecture of the mapping fi can be deﬁned arbitrarily, and fi can be trained using the following feature regression loss:  Li F eatureRegression(θ, θfi) = (cid:107)fi(hAMN(s; θ); θfi) − hEi(s)(cid:107)2 2 ,  (5) where θ and θfi are the parameters of the AMN and ith feature regression network, respectively. When training this objective, the error is fully back-propagated from the feature regression network output through the layers of the AMN. In this way, the feature regression objective provides pressure on the AMN to compute features that can predict an expert’s features. A justiﬁcation for this objec- tive is that if we have a perfect regression from multitask to expert features, all the information in the expert features is contained in the multitask features. The use of the separate feature prediction network fi for each task enables the multitask network to have a different feature dimension than the experts as well as prevent issues with identiﬁability. Empirically we have found that the feature regression objective’s primary beneﬁt is that it can increase the performance of transfer learning in some target tasks.  3.3 ACTOR-MIMIC OBJECTIVE  Li ActorM imic(θ, θfi) = Li  Combining both regression objectives, the Actor-Mimic objective is thus deﬁned as F eatureRegression(θ, θfi),  (6) where β is a scaling parameter which controls the relative weighting of the two objectives. Intu- itively, we can think of the policy regression objective as a teacher (expert network) telling a student (AMN) how they should act (mimic expert’s actions), while the feature regression objective is anal- ogous to a teacher telling a student why it should act that way (mimic expert’s thinking process).  policy(θ) + β ∗ Li  3  Published as a conference paper at ICLR 2016  3.4 TRANSFERING KNOWLEDGE: ACTOR-MIMIC AS PRETRAINING  Now that we have a method of training a network that is an expert at all source tasks, we can proceed to the task of transferring source task knowledge to a novel but related target task. To enable transfer to a new task, we ﬁrst remove the ﬁnal softmax layer of the AMN. We then use the weights of AMN as an instantiation for a DQN that will be trained on the new target task. The pretrained DQN is then trained using the same training procedure as the one used with a standard DQN. Multitask pretraining can be seen as initializing the DQN with a set of features that are effective at deﬁning policies in related tasks. If the source and target tasks share similarities, it is probable that some of these pretrained features will also be effective at the target task (perhaps after slight ﬁne-tuning).  (cid:20)  (cid:18)  4 CONVERGENCE PROPERTIES OF ACTOR-MIMIC We further study the convergence properties of the proposed Actor-Mimic under a framework similar to (Perkins & Precup, 2002). The analysis mainly focuses on L2-regularized policy regression with- out feature regression. Without losing generality, the following analysis focuses on learning from a single game expert softmax policy πE. The analysis can be readily extended to consider multiple experts on multiple games by absorbing different games into the same state space. Let Dπ(s) be the stationary distribution of the Markov decision process under policy π over states s ∈ S. The policy regression objective function can be rewritten using expectation under the stationary distribution of the Markov decision process:  E  + λ(cid:107)θ(cid:107)2 2,  H  πE(a|s), πAMN(a|s; θ)  θ  min  s∼DπAMN,(cid:15)-greedy (·)  (7) where H(·) is the cross-entropy measure and λ is the coefﬁcient of weight decay that is necessary in the following analysis of the policy regression. Under Actor-Mimic, the learning agent interacts with the environment by following an (cid:15)-greedy strategy of some Q function. The mapping from a Q function to an (cid:15)-greedy policy π(cid:15)-greedy is denoted by an operator Γ, where π(cid:15)-greedy = Γ(Q). To avoid confusion onwards, we use notation p(a|s; θ) for the softmax policies in the policy regression objective. Assume each state in a Markov decision process is represented by a compact K-dimensional feature representation φ(s) ∈ RK. Consider a linear function approximator for Q values with parameter matrix θ ∈ RK×|A|, ˆQ(s, a; θ) = φ(s)T θa, where θa is the ath column of θ. The corresponding softmax policy of the linear approximator is deﬁned by p(a|s; θ) ∝ exp{ ˆQ(s, a; θ)}.  (cid:19)(cid:21)  4.1 STOCHASTIC STATIONARY POLICY For any stationary policy π∗, the stationary point of the objective function Eq. (7) can be found by setting its gradient w.r.t. θ to zero. Let Pθ be a |S| × |A| matrix where its ith row jth column element is the softmax policy prediction p(aj|si; θ) from the linear approximator. Similarly, let ΠE be a |S| × |A| matrix for the softmax policy prediction from the expert model. Additionally, let Dπ be a diagonal matrix whose entries are Dπ(s). A simple gradient following algorithm on the objective function Eq. (7) has the following expected update rule using a learning rate αt > 0 at the tth iteration:  Lemma 1. Under a ﬁxed policy π∗ and a learning rate schedule that satisﬁes (cid:80)∞ (cid:80)∞  (8) t=1 αt = ∞, t < ∞, the parameters θ, updated by the stochastic gradient descent learning algorithm  described above, asymptotically almost surely converge to a unique solution θ∗. When the policy π∗ is ﬁxed, the objective function Eq. (7) is convex and is the same as a multinomial logistic regression problem with a bounded Lipschitz constant due to its compact input features. Hence there is a unique stationary point θ∗ such that ∆θ∗ = 0. The proof of Lemma 1 follows the stochastic approximation argument (Robbins & Monro, 1951).  (cid:20) ΦT Dπ(Pθt−1 − ΠE) + λθt−1  ∆θt = −αt  t=1 α2  (cid:21)  .  4.2 STOCHASTIC ADAPTIVE POLICY Consider the following learning scheme to adapt the agent’s policy. The learning agent interacts with the environment and samples states by following a ﬁxed (cid:15)-greedy policy π(cid:48). Given the samples  4  Published as a conference paper at ICLR 2016  Figure 1: The Actor-Mimic and expert DQN training curves for 100 training epochs for each of the 8 games. A training epoch is 250,000 frames and for each training epoch we evaluate the networks with a testing epoch that lasts 125,000 frames. We report AMN and expert DQN test reward for each testing epoch and the mean and max of DQN performance. The max is calculated over all testing epochs that the DQN experienced until convergence while the mean is calculated over the last ten epochs before the DQN training was stopped. In the testing epoch we use (cid:15) = 0.05 in the (cid:15)-greedy policy. The y-axis is the average unscaled episode reward during a testing epoch. The AMN results are averaged over 2 separately trained networks. and the expert prediction, the linear function approximator parameters are updated using Eq. (8) to a unique stationary point θ(cid:48). The new parameters θ(cid:48) are then used to establish a new (cid:15)-greedy policy π(cid:48)(cid:48) = Γ( ˆQθ(cid:48)) through the Γ operator over the linear function ˆQθ(cid:48). The agent under the new policy π(cid:48)(cid:48) subsequently samples a new set of states and actions from the Markov decision process to update its parameters. The learning agent therefore generates a sequence of policies {π1, π2, π3, ...}. The proof for the following theorem is given in Appendix A. Theorem 1. Assume the Markov decision process is irreducible and aperiodic for any policy π induced by the Γ operator and Γ is Lipschitz continuous with a constant c(cid:15), then the sequence of policies and model parameters generated by the iterative algorithm above converges almost surely to a unique solution π∗ and θ∗.  4.3 PERFORMANCE GUARANTEE The convergence theorem implies the Actor-Mimic learning algorithm also belongs to the family of no-regret algorithms in the online learning framework, see Ross et al. (2011) for more details. Their theoretical analysis can be directly applied to Actor-Mimic and results in a performance guarantee bound on how well the Actor-Mimic model performs with respect to the guiding expert. Let Z π(cid:48) t (s, π) be the t-step reward of executing π in the initial state s and then following policy π(cid:48). The cost-to-go for a policy π after T -steps is deﬁned as JT (π) = −T Es∼D(·) [R(s, a)], where R(s, a) is the reward after executing action a in state s. Proposition 1. For the iterative algorithm described in Section (4.2), if the loss function in Eq. (7) T−t+1(s, a) ≥ u for all actions a ∈ A converges to (cid:15) with the solution πAMN and Z π∗ and t ∈ {1,··· , T}, then the cost-to-go of Actor-Mimic JT (πAMN) grows linearly after executing T actions: JT (πAMN) ≤ JT (πE) + uT (cid:15)/ log 2. The above linear growth rate of the cost-to-go is achieved through sampling from AMN action output πAMN, while the cost grows quadratically if the algorithm only samples from the expert action output. Our empirical observations conﬁrm this theoretical prediction.  T−t+1(s, π∗)− Z π∗  5 EXPERIMENTS In the following experiments, we validate the Actor-Mimic method by demonstrating its effective- ness at both multitask and transfer learning in the Arcade Learning Environment (ALE). For our experiments, we use subsets of a collection of 20 Atari games. 19 games of this set were among the 29 games that the DQN method performed at a super-human level. We additionally chose 1 game, the game of Seaquest, on which the DQN had performed poorly when compared to a human expert. Details on the training procedure are described in Appendix B. 5.1 MULTITASK To ﬁrst evaluate the actor-mimic objective on multitask learning, we demonstrate the effectiveness of training an AMN over multiple games simultaneously. In this particular case, since our focus is  5  036#105ATLANTISAMNDQNDQN-MaxDQN-Mean-5025100BOXING0200400BREAKOUT07.515#104CRAZY CLIMBER05010005001000ENDURO050100-40040PONG050100040008000SEAQUEST050100012502500SPACE INVADERSPublished as a conference paper at ICLR 2016  Network  DQN  AMN  100% × AMN  DQN  Mean Max Mean Max Mean Max  Atlantis Boxing Breakout Crazy Climber 57279 273.15 541000 377.96 165065 347.01 584196 370.32 288.2% 93.61% 127.0% 108.0% 93.00% 97.98%  96189 117593 57070 74342 59.33% 63.22%  81.47 88.02 76.264 81.860  Pong 19.581 20.140 15.275 18.780  Enduro Seaquest 457.60 4278.9 808.00 6200.5 499.3 1177.3 686.77 1466.0 109.1% 78.01% 27.51% 85.00% 93.25% 23.64%  Space Invaders  1669.2 2109.7 1142.4 1349.0 68.44% 63.94%  Table 1: Actor-Mimic results on a set of eight Atari games. We compare the AMN performance to that of the expert DQNs trained separately on each game. The expert DQNs were trained until convergence and the AMN was trained for 100 training epochs, which is equivalent to 25 million input frames per source game. For the AMN, we report maximum test reward ever achieved in epochs 1-100 and mean test reward in epochs 91-100. For the DQN, we report maximum test reward ever achieved until convergence and mean test reward in the last 10 epochs of DQN training. Additionally, at the last row of the table we report the percentage ratio of the AMN reward to the expert DQN reward for every game for both mean and max rewards. These percentage ratios are plotted in Figure 6. The AMN results are averaged over 2 separately trained networks.  on multitask learning and not transfer learning, we disregard the feature regression objective and set β to 0. Figure 1 and Table 1 show the results of an AMN trained on 8 games simultaneously with the policy regression objective, compared to an expert DQN trained separately for each game. The AMN and every individual expert DQN in this case had the exact same network architecture. We can see that the AMN quickly reaches close-to-expert performance on 7 games out of 8, only taking around 20 epochs or 5 million training frames to settle to a stable behaviour. This is in comparison to the expert networks, which were trained for up to 50 million frames. One result that was observed during training is that the AMN often becomes more consistent in its behaviour than the expert DQN, with a noticeably lower reward variance in every game except Atlantis and Pong. Another surprising result is that the AMN achieves a signiﬁcantly higher mean reward in the game of Atlantis and relatively higher mean reward in the games of Breakout and Enduro. This is despite the fact that the AMN is not being optimized to improve reward over the expert but just replicate the expert’s behaviour. We also observed this increase in source task perfor- mance again when we later on increased the AMN model complexity for the transfer experiments (see Atlantis experiments in Appendix D). The AMN had the worst performance on the game of Seaquest, which was a game on which the expert DQN itself did not do very well. It is possible that a low quality expert policy has difﬁculty teaching the AMN to even replicate its own (poor) behaviour. We compare the performance of our AMN against a baseline of two different multitask DQN architectures in Appendix C.  5.2 TRANSFER We have found that although a small AMN can learn how to behave at a close-to-expert level on multiple source tasks, a larger AMN can more easily transfer knowledge to target tasks after be- ing trained on the source tasks. For the transfer experiments, we therefore signiﬁcantly increased the AMN model complexity relative to that of an expert. Using a larger network architecture also allowed us to scale up to playing 13 source games at once (see Appendix D for source task perfor- mance using the larger AMNs). We additionally found that using an AMN trained for too long on the source tasks hurt transfer, as it is likely overﬁtting. Therefore for the transfer experiments, we train the AMN on only 4 million frames for each of the source games. To evaluate the Actor-Mimic objective on transfer learning, the previously described large AMNs will be used as a weight initialization for DQNs which are each trained on a different target task. We additionally independently evaluate the beneﬁt of the feature regression objective during transfer by having one AMN trained with only the policy regression objective (AMN-policy) and another trained using both feature and policy regression (AMN-feature). The results are then compared to the baseline of a DQN that was initialized with random weights. The performance on a set of 7 target games is detailed in Table 2 (learning curves are plotted in Figure 7). We can see that the AMN pretraining provides a deﬁnite increase in learning speed for the 3 games of Breakout, Star Gunner and Video Pinball. The results in Breakout and Video Pinball demonstrate that the policy regression objective alone provides signiﬁcant positive transfer in some target tasks. The reason for this large positive transfer might be due to the source game Pong having very similar mechanics to both Video Pinball and Breakout, where one must use a paddle to prevent a ball from falling off screen. The machinery used to detect the ball in Pong would likely be useful in detecting the ball for these two target tasks, given some ﬁne-tuning. Additionally, the feature regression objective causes a signiﬁcant speed-up in the game of Star Gunner compared to both the random initialization and the network trained solely with policy regression. Therefore even though the feature regression objective can slightly hurt transfer in some source games, it can provide large  6  Published as a conference paper at ICLR 2016  Breakout Random  AMN-policy AMN-feature  Gopher Random  AMN-policy AMN-feature  Krull  Random  AMN-policy AMN-feature Road Runner  Random  AMN-policy AMN-feature  Robotank Random  AMN-policy AMN-feature Star Gunner  Random  AMN-policy AMN-feature Video Pinball  Random  1 mil 1.182 18.35 16.23 1 mil 294.0 715.0 636.2 1 mil 4302 5827 5033 1 mil 327.5 1561 1349 1 mil 4.830 3.502 3.550 1 mil 221.2 274.3 1405 1 mil 2323 2583 1593  2 mil 5.278 102.1 119.0 2 mil 578.9 612.7 1110 2 mil 6193 7279 7256 2 mil 988.1 5119 6659 2 mil 6.965 4.522 6.162 2 mil 468.5 302.0 4570 2 mil 8549 25821 3958  3 mil 29.13 216.0 153.7 3 mil 1360 1362 918.8 3 mil 6576 6838 7008 3 mil 16263 19483 18074 3 mil 9.825 11.03 13.94 3 mil 927.6 978.4 18111 3 mil 6780 95949 21341  4 mil 102.3 271.1 191.8 4 mil 1540 924.8 1073 4 mil 7030 6971 7582 4 mil 27183 22132 16858 4 mil 13.22 9.215 17.58 4 mil 1084 1667 23406 4 mil 5842 143729 12421  5 mil 202.8 308.6 172.6 5 mil 1820 1029 1028 5 mil 6754 7277 7665 5 mil 26639 23391 18099 5 mil 21.07 16.89 17.57 5 mil 1508 4000 36070 5 mil 10383 57114 15409  6 mil 212.8 286.3 233.9 6 mil 1133 1186 810.1 6 mil 5294 7129 8016 6 mil 29488 23813 22985 6 mil 22.54 17.31 20.72 6 mil 1626 14655 46811 6 mil 11093 106873 18992  7 mil 252.9 284.6 248.5 7 mil 633.0 1081 1008 7 mil 5949 7854 8133 7 mil 33197 34673 27023 7 mil 31.94 18.66 20.13 7 mil 3286 31588 50667 7 mil 8468 111074 15920  8 mil 211.8 318.8 178.8 8 mil 1306 936.7 868.8 8 mil 5557 8012 6536 8 mil 27683 33476 24149 8 mil 29.80 20.58 21.13 8 mil 16017 45667 49579 8 mil 5476 73523 48690  9 mil 243.5 281.6 235.6 8 mil 1758 1251 1054 9 mil 5366 7244 7832 9 mil 25235 31967 28225 9 mil 37.12 23.58 26.14 9 mil 36273 38738 50440 9 mil 9964 34908 24366  10 mil 258.7 311.3 225.5 10 mil 1539 1142 982.4 10 mil 6005 7835 6923 10 mil 31647 31416 23342 10 mil 34.04 23.02 23.29 10 mil 45322 53642 56839 10 mil 11893 123337 26379  AMN-policy AMN-feature Table 2: Actor-Mimic transfer results for a set of 7 games. The 3 networks are trained as DQNs on the target task, with the only difference being the weight initialization. “Random” means random initial weights, “AMN- policy” means a weight initialization with an AMN trained using policy regression and “AMN-feature” means a weight initialization with an AMN trained using both policy and feature regression (see text for more details). We report the average test reward every 4 training epochs (equivalent to 1 million training frames), where the average is over 4 testing epochs that are evaluated immediately after each training epoch. For each game, we bold out the network results that have the highest average testing reward for that particular column. beneﬁts in others. The positive transfer in Breakout, Star Gunner and Video Pinball saves at least up to 5 million frames of training time in each game. Processing 5 million frames with the large model is equivalent to around 4 days of compute time on a NVIDIA GTX Titan. On the other hand, for the games of Krull and Road Runner (although the multitask pretraining does help learning at the start) the effect is not very pronounced. When running Krull we observed that the policy learnt by any DQN regardless of the initialization was a sort of unexpected local maximum. In Krull, the objective is to move between a set of varied minigames and complete each one. One of the minigames, where the player must traverse a spiderweb, gives extremely high reward by simply jumping quickly in a mostly random fashion. What the DQN does is it kills itself on purpose in the initial minigame, runs to the high reward spiderweb minigame, and then simply jumps in the corner of the spiderweb until it is terminated by the spider. Because it is relatively easy to get stuck in this local maximum, and very hard to get out of it (jumping in the minigame gives unproportionally high reward compared to the other minigames), transfer does not really help learning. For the games of Gopher and Robotank, we can see that the multitask pretraining does not have any signiﬁcant positive effect. In particular, multitask pretraining for Robotank even seems to slow down learning, providing an example of negative transfer. The task in Robotank is to control a tank turret in a 3D environment to destroy other tanks, so it’s possible that this game is so signiﬁcantly different from any source task (being the only ﬁrst-person 3D game) that the multitask pretraining does not provide any useful prior knowledge. 6 RELATED WORK The idea of using expert networks to guide a single mimic network has been studied in the context of supervised learning, where it is known as model compression. The goal of model compression is to reduce the computational complexity of a large model (or ensemble of large models) to a single smaller mimic network while maintaining as high an accuracy as possible. To obtain high accuracy, the mimic network is trained using rich output targets provided by the experts. These output targets are either the ﬁnal layer logits (Ba & Caruana, 2014) or the high-temperature softmax outputs of the experts (Hinton et al., 2015). Our approach is most similar to the technique of (Hinton et al., 2015)  7  Published as a conference paper at ICLR 2016  which matches the high-temperature outputs of the mimic network with that of the expert network. In addition, we also tried an objective that provides expert guidance at the feature level instead of only at the output level. A similar idea was also explored in the model compression case (Romero et al., 2015), where a deep and thin mimic network used a larger expert network’s intermediate features as guiding hints during training. In contrast to these model compression techniques, our method is not concerned with decreasing test time computation but instead using experts to provide otherwise unavailable supervision to a mimic network on several distinct tasks. Actor-Mimic can also be considered as part of the larger Imitation Learning class of methods, which use expert guidance to teach an agent how to act. One such method, called DAGGER (Ross et al., 2011), is similar to our approach in that it trains a policy to directly mimic an expert’s behaviour while sampling actions from the mimic agent. Actor-Mimic can be considered as an extension of this work to the multitask case. In addition, using a deep neural network to parameterize the policy provides us with several advantages over the more general Imitation Learning framework. First, we can exploit the automatic feature construction ability of deep networks to transfer knowledge to new tasks, as long as the raw data between tasks is in the same form, i.e. pixel data with the same dimen- sions. Second, we can deﬁne objectives which take into account intermediate representations of the state and not just the policy outputs, for example the feature regression objective which provides a richer training signal to the mimic network than just samples of the expert’s action output. Recent work has explored combining expert-guided Imitation Learning and deep neural networks in the single-task case. Guo et al. (2014) use DAGGER with expert guidance provided by Monte-Carlo Tree Search (MCTS) policies to train a deep neural network that improves on the original DQN’s performance. Some disadvantages of using MCTS experts as guidance are that they require both access to the (hidden) RAM state of the emulator as well as an environment model. Another re- lated method is that of guided policy search (Levine & Koltun, 2013), which combines a regularized importance-sampled policy gradient with guiding trajectory samples generated using differential dy- namic programming. The goal in that work was to learn continuous control policies which improved upon the basic policy gradient method, which is prone to poor local minima. A wide variety of methods have also been studied in the context of RL transfer learning (see Tay- lor & Stone (2009) for a more comprehensive review). One related approach is to use a dual state representation with a set of task-speciﬁc and task-independent features known as “problem-space” and “agent-space” descriptors, respectively. For each source task, a task-speciﬁc value function is learnt on the problem-space descriptors and then these learnt value functions are transferred to a single value function over the agent-space descriptors. Because the agent-space value function is deﬁned over features which maintain constant semantics across all tasks, this value function can be directly transferred to new tasks. Banerjee & Stone (2007) constructed agent-space features by ﬁrst generating a ﬁxed-depth game tree of the current state, classifying each future state in the tree as either {win, lose, draw, nonterminal} and then coalescing all states which have the same class or subtree. To transfer the source tasks value functions to agent-space, they use a simple weighted av- erage of the source task value functions, where the weight is proportional to the number of times that a speciﬁc agent-space descriptor has been seen during play in that source task. In a related method, Konidaris & Barto (2006) transfer the value function to agent-space by using regression to predict every source tasks problem-space value function from the agent-space descriptors. A drawback of these methods is that the agent- and problem-space descriptors are either hand-engineered or gener- ated from a perfect environment model, thus requiring a signiﬁcant amount of domain knowledge. 7 DISCUSSION In this paper we deﬁned Actor-Mimic, a novel method for training a single deep policy network over a set of related source tasks. We have shown that a network trained using Actor-Mimic is capable of reaching expert performance on many games simultaneously, while having the same model complexity as a single expert. In addition, using Actor-Mimic as a multitask pretraining phase can signiﬁcantly improve learning speed in a set of target tasks. This demonstrates that the features learnt over the source tasks can generalize to new target tasks, given a sufﬁcient level of similarity between source and target tasks. A direction of future work is to develop methods that can enable a targeted knowledge transfer from source tasks by identifying related source tasks for the given target task. Using targeted knowledge transfer can potentially help in cases of negative transfer observed in our experiments. Acknowledgments: This work was supported by Samsung and NSERC.  8  Published as a conference paper at ICLR 2016  REFERENCES Ba, Jimmy and Caruana, Rich. Do deep nets really need to be deep?  Information Processing Systems, pp. 2654–2662, 2014.  In Advances in Neural  Banerjee, Bikramjit and Stone, Peter. General game learning using knowledge transfer. In Interna-  tional Joint Conferences on Artiﬁcial Intelligence, pp. 672–677, 2007.  Bellemare, Marc G., Naddaf, Yavar, Veness, Joel, and Bowling, Michael. The arcade learning envi- ronment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013.  Bertsekas, Dimitri P. Dynamic programming and optimal control, volume 1. Athena Scientiﬁc  Belmont, MA, 1995.  Guo, Xiaoxiao, Singh, Satinder, Lee, Honglak, Lewis, Richard L, and Wang, Xiaoshi. Deep learning for real-time atari game play using ofﬂine monte-carlo tree search planning. In Advances in Neural Information Processing Systems 27, pp. 3338–3346, 2014.  Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distilling the knowledge in a neural network.  arXiv preprint arXiv:1503.02531, 2015.  Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. In International  Conference on Learning Representations, 2015.  Konidaris, George and Barto, Andrew G. Autonomous shaping: Knowledge transfer in reinforce- In Proceedings of the 23rd international conference on Machine learning, pp.  ment learning. 489–496, 2006.  Levine, Sergey and Koltun, Vladlen. Guided policy search. In Proceedings of the 30th international  conference on Machine Learning, 2013.  Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel, Pieter. End-to-end training of deep  visuomotor policies. CoRR, abs/1504.00702, 2015.  Lillicrap, Timothy P., Hunt, Jonathan J., Pritzel, Alexander, Heess, Nicholas, Erez, Tom, Tassa, Yuval, Silver, David, and Wierstra, Daan. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015.  Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A., Veness, Joel, Bellemare, Marc G., Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K., Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wier- stra, Daan, Legg, Shane, and Hassabis, Demis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.  Perkins, Theodore J and Precup, Doina. A convergent form of approximate policy iteration.  Advances in neural information processing systems, pp. 1595–1602, 2002.  In  Robbins, Herbert and Monro, Sutton. A stochastic approximation method. The annals of mathemat-  ical statistics, pp. 400–407, 1951.  Romero, Adriana, Ballas, Nicolas, Kahou, Samira Ebrahimi, Chassang, Antoine, Gatta, Carlo, and In International Conference on Learning  Bengio, Yoshua. Fitnets: Hints for thin deep nets. Representations, 2015.  Ross, Stephane, Gordon, Geoffrey, and Bagnell, Andrew. A reduction of imitation learning and structured prediction to no-regret online learning. Journal of Machine Learning Research, 15: 627–635, 2011.  Seneta, E. Sensitivity analysis, ergodicity coefﬁcients, and rank-one updates for ﬁnite markov  chains. Numerical solution of Markov chains, 8:121–129, 1991.  Sutton, Richard S. and Barto, Andrew G. Reinforcement learning: An introduction. MIT Press  Cambridge, 1998.  Taylor, Matthew E and Stone, Peter. Transfer learning for reinforcement learning domains: A survey.  The Journal of Machine Learning Research, 10:1633–1685, 2009.  9  Published as a conference paper at ICLR 2016  APPENDIX A PROOF OF THEOREM 1  Lemma 2. For any two policies π1,π2, the stationary distributions over the states under the policies are bounded: (cid:107)Dπ1 − Dπ2(cid:107) ≤ cD(cid:107)π1 − π2(cid:107), for some cD > 0.  Proof. Let T 1 and T 2 be the two transition matrices under the stationary distributions Dπ1, Dπ2. For any ij elements T 1  ij in the transition matrices, we have,  ij, T 2  (cid:107)T 1  ij − T 2  ij(cid:107) =  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)  a  p(si|a, sj)(cid:0)π1(a|sj) − π2(a|sj)(cid:1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  ≤|A|(cid:107)π1(a|sj) − π2(a|sj)(cid:107) ≤|A|(cid:107)π1 − π2(cid:107)∞.  (9)  (10) (11)  The above bound for any ijth elements implies the Euclidean distance of the transition matrices is also upper bounded (cid:107)T 1 − T 2(cid:107) ≤ |S||A|(cid:107)π1 − π2(cid:107). Seneta (1991) has shown that (cid:107)Dπ1 − Dπ2(cid:107) ≤ 1−λ1(cid:107)T 1 − T 2(cid:107)∞, where λ1 is the largest eigenvalue of T 1. Hence, there is a constant cD > 0 such that (cid:107)Dπ1 − Dπ2(cid:107) ≤ cD(cid:107)π1 − π2(cid:107).  1  Lemma 3. For any two softmax policy Pθ1, Pθ2 matrices from the linear function approximator, (cid:107)Pθ1 − Pθ2(cid:107) ≤ cJ(cid:107)Φθ1 − Φθ2(cid:107), for some cJ ≥ 0.  Proof. Note that the ith row jth column element p(aj|si) in a softmax policy matrix P is computed by the softmax transformation on the Q function:  pij = p(aj|si) = sof tmax  Q(si, aj)  =  .  (12)  (cid:18)  (cid:19)  (cid:80)  k  eQ(si,aj )  eQ(si,ak)  Because the softmax function is a monotonically increasing element-wise function on matrices, the Euclidean distance of the softmax transformation is upper bounded by the largest Jacobian in the domain of the softmax function. Namely, for c(cid:48)  (cid:107),  (cid:107)sof tmax(x1) − sof tmax(x2)(cid:107) ≤ c(cid:48)  (13) By bounding the elements in P matrix, it gives (cid:107)Pθ1 − Pθ2(cid:107) ≤ cJ(cid:107) ˆQθ1 − ˆQθ2(cid:107) = cJ(cid:107)Φθ1 − Φθ2(cid:107).  J = maxz∈Dom sof tmax (cid:107) ∂sof tmax(z) J(cid:107)x1 − x2(cid:107),∀x1, x2 ∈ Dom sof tmax.  ∂z  Theorem 1. Assume the Markov decision process is irreducible and aperiodic for any policy π induced by the Γ operator and Γ is Lipschitz continuous with a constant c(cid:15), the sequence of policies and model parameters generated by the iterative algorithm above converges almost surely to a unique solution π∗ and θ∗.  Proof. We follow a similar contraction argument made in Perkins & Precup (2002) , and show the it- erative algorithm is a contraction process. Namely, for any two policies π1 and π2, the learning algo- rithm above produces new policies Γ( ˆQθ1), Γ( ˆQθ2) after one iteration, where (cid:107)Γ( ˆQθ1)−Γ( ˆQθ2)(cid:107) ≤ β(cid:107)π1 − π2(cid:107). Here (cid:107) · (cid:107) is the Euclidean norm and β ∈ [0, 1). By Lipschtiz continuity,  (cid:107)Γ( ˆQθ1 ) − Γ( ˆQθ2)(cid:107) ≤c(cid:15)(cid:107) ˆQθ1 − ˆQθ2(cid:107) = c(cid:15)(cid:107)Φθ1 − Φθ2(cid:107)  ≤c(cid:15)(cid:107)Φ(cid:107)(cid:107)θ1 − θ2(cid:107).  (14) (15)  10  Published as a conference paper at ICLR 2016  Let θ1 and θ2 be the stationary points of Eq. (7) under π1 and π2. That is, ∆θ1 = ∆θ2 = 0 respectively. Rearranging Eq. (8) gives, (cid:107)θ1 − θ2(cid:107) =  (cid:107)ΦT Dπ1(Pθ1 − Πe) − ΦT Dπ2 (Pθ2 − Πe)(cid:107) (cid:107)ΦT (Dπ2 − Dπ1 )Πe + ΦT Dπ1 Pθ1 − ΦT Dπ1Pθ2 + ΦT Dπ1Pθ2 − ΦT Dπ2 Pθ2(cid:107)  (16)  =  1 = λ ≤ 1 λ  (cid:107)ΦT (Dπ2 − Dπ1 )Πe + ΦT Dπ1 (Pθ1 − Pθ2) + ΦT (Dπ1 − Dπ2)Pθ2(cid:107) (18) (cid:107)ΦT(cid:107)(cid:107)Dπ1 − Dπ2(cid:107)(cid:107)Πe(cid:107) + (cid:107)ΦT(cid:107)(cid:107)Dπ1(cid:107)(cid:107)Pθ1 − Pθ2(cid:107) + (cid:107)ΦT(cid:107)(cid:107)Dπ1 − Dπ2(cid:107)(cid:107)Pθ2(cid:107) (19) (20)  ≤c(cid:107)π1 − π2(cid:107).  (17)  (cid:21)  1 λ 1 λ  (cid:20)  The last inequality is given by Lemma 2 and 3 and the compactness of Φ. For a Lipschtiz constant c(cid:15) ≥ c, there exists a β such that (cid:107)Γ( ˆQθ1 )−Γ( ˆQθ2)(cid:107) ≤ β(cid:107)π1−π2(cid:107). Hence, the sequence of policies generated by the algorithm converges almost surely to a unique ﬁxed point π∗ from Lemma 1 and the Contraction Mapping Theorem Bertsekas (1995). Furthermore, the model parameters converge w.p. 1 to a stationary point θ∗ under the ﬁxed point policy π∗.  APPENDIX B AMN TRAINING DETAILS  All of our Actor-Mimic Networks (AMNs) were trained using the Adam (Kingma & Ba, 2015) optimization algorithm. The AMNs have a single 18-unit output, with each output corresponding to one of the 18 possible Atari player actions. Having the full 18-action output simpliﬁes the multitask case when each game has a different subset of valid actions. While playing a certain game, we mask out AMN action outputs that are not valid for that game and take the softmax over only the subset of valid actions. We use a replay memory for each game to reduce correlations between successive frames and stabilize network training. Because the memory requirements of having the standard replay memory size of 1,000,000 frames for each game are prohibitive when we are training over many source games, for AMNs we use a per-game 100,000 frame replay memory. AMN training was stable even with only a per-game equivalent of a tenth of the replay memory size of the DQN experts. For the transfer experiments with the feature regression objective, we set the scaling parameter β to 0.01 and the feature prediction network fi was set to a linear projection from the AMN features to the ith expert features. For the policy regression objective, we use a softmax temperature of 1 in all cases. Additionally, during training for all AMNs we use an (cid:15)-greedy policy with (cid:15) set to a constant 0.1. Annealing (cid:15) from 1 did not provide any noticeable beneﬁt. During training, we choose actions based on the AMN and not the expert DQN. We do not use weight decay during AMN training as we empirically found that it did not provide any large beneﬁts. For the experiments using the DQN algorithm, we optimize the networks with RMSProp. Since the DQNs are trained on a single game their output layers only contain the player actions that are valid in the particular game that they are trained on. The experts guiding the AMNs used the same architecture, hyperparameters and training procedure as that of Mnih et al. (2015). We use the full 1,000,000 frame replay memory when training any DQN.  APPENDIX C MULTITASK DQN BASELINE RESULTS  As a baseline, we trained DQN networks over 8 games simultaneously to test their performance against the Actor-Mimic method. We tried two different architectures, the ﬁrst is using the basic DQN procedure on all 8 games. This network has a single 18 action output shared by all games, but when we train or test in a particular game, we mask out and ignore the action values from actions that are invalid for that particular game. This architecture is denoted the Multitask DQN (MDQN). The second architecture is a DQN but where each game has a separate fully-connected feature layer and action output. In this architecture only the convolutions are shared between games, and thus the features and action values are completely separate. This was to try to mitigate the destabilizing  11  Published as a conference paper at ICLR 2016  Figure 2: The Actor-Mimic, expert DQN, and Multitask DQN (MDQN) training curves for 40 training epochs for each of the 8 games. A training epoch is 250,000 frames and for each training epoch we evaluate the networks with a testing epoch that lasts 125,000 frames. We report AMN, expert DQN and MDQN test reward for each testing epoch. In the testing epoch we use (cid:15) = 0.05 in the (cid:15)-greedy policy. The y-axis is the average unscaled episode reward during a testing epoch.  Figure 3: The Actor-Mimic, expert DQN, and Multitask Convolutions DQN (MCDQN) training curves for 40 training epochs for each of the 8 games. A training epoch is 250,000 frames and for each training epoch we evaluate the networks with a testing epoch that lasts 125,000 frames. We report AMN, expert DQN and MCDQN test reward for each testing epoch. In the testing epoch we use (cid:15) = 0.05 in the (cid:15)-greedy policy. The y-axis is the average unscaled episode reward during a testing epoch.  effect that the different value scales of each game had during learning. This architecture is denoted the Multitask Convolutions DQN (MCDQN). The results for the MDQN and MCDQN are shown in Figures 2 and 3, respectively. From the ﬁgures, we can observe that the AMN is far more stable during training as well as being consistently higher in performance than either the MDQN or MCDQN methods. In addition, it can be seen that the MDQN and MCDQN will often focus on performing reasonably well on a small subset of the source games, such as on Boxing and Enduro, while making little to no progress in others, such as Breakout or Pong. Between the MDQN and MCDQN, we can see that the MCDQN hardly improves results even though it has signiﬁcantly larger computational cost that scales linearly with the number of source games. For the speciﬁc details of the architectures we tested, for the MDQN the architecture was: 8x8x4x32- 4 1 → 4x4x32x64-2 → 3x3x64x64-1 → 512 fully-connected units → 18 actions. This is exactly the same network architecture as used for the 8 game AMN in Section 5.1. For the MCDQN, the bottom convolutional layers were the same as the MDQN, except there are 8 parallel subnetworks on top of the convolutional layers. These game-speciﬁc subnetworks had the architecture: 512 fully- connected units → 18 actions. All layers except the action outputs were followed with a rectiﬁer non-linearity.  1 Here we represent convolutional layers as WxWxCxN-S, where W is the width of the (square) convolution  kernel, C is the number of input images, N is the number of ﬁlter maps and S is the convolution stride.  12  024#105ATLANTISAMNDQNMDQN-5025100BOXING0200400BREAKOUT07.515#104CRAZY CLIMBER0204005001000ENDURO02040-30-520PONG02040020004000SEAQUEST0204007501500SPACE INVADERS024#105ATLANTISAMNDQNMCDQN-5025100BOXING0200400BREAKOUT07.515#104CRAZY CLIMBER0204005001000ENDURO02040-30-520PONG02040020004000SEAQUEST0204007501500SPACE INVADERSPublished as a conference paper at ICLR 2016  APPENDIX D ACTOR-MIMIC NETWORK MULTITASK RESULTS FOR  TRANSFER PRETRAINING  The network used for transfer consisted of the following architecture: 8x8x4x256-4 1 → 4x4x256x512-2 → 3x3x512x512-1 → 3x3x512x512-1 → 2048 fully-connected units → 1024 fully- connected units → 18 actions. All layers except the ﬁnal one were followed with a rectiﬁer non- linearity.  13  Published as a conference paper at ICLR 2016  Figure 4: The Actor-Mimic training curves for the network trained solely with the policy regression objective (AMN-policy). The AMN-policy is trained for 16 epochs, or 4 million frames per game. We compare against the (smaller network) expert DQNs, which are trained until convergence. We also report the maximum test reward the expert DQN achieved over all training epochs, as well as the mean testing reward achieved over the last 10 epochs.  14  051015010002000300040005000assaultAMN-policyDQNDQN-MaxDQN-Mean0510150123456#105atlantis0510150200040006000800010000beam rider051015-50050100boxing051015024681012#104crazy climber05101500.511.522.53#104demon attack05101502004006008001000enduro051015-100-80-60-40-20020fishing derby0510150200040006000800010000kangaroo0510150200040006000800010000name this game051015-30-20-100102030pong05101501000200030004000500060007000seaquest05101505001000150020002500space invadersPublished as a conference paper at ICLR 2016  Figure 5: The Actor-Mimic training curves for the network trained with both the feature and policy regression objective (AMN-feature). The AMN-feature is trained for 16 epochs, or 4 million frames per game. We compare against the (smaller network) expert DQNs, which are trained until convergence. We also report the maximum test reward the expert DQN achieved over all training epochs, as well as the mean testing reward achieved over the last 10 epochs.  15  051015010002000300040005000assaultAMN-featureDQNDQN-MaxDQN-Mean0510150123456#105atlantis0510150200040006000800010000beam rider051015-50050100boxing051015024681012#104crazy climber05101500.511.522.53#104demon attack05101502004006008001000enduro051015-100-80-60-40-20020fishing derby0510150200040006000800010000kangaroo0510150200040006000800010000name this game051015-30-20-100102030pong05101501000200030004000500060007000seaquest05101505001000150020002500space invadersPublished as a conference paper at ICLR 2016  APPENDIX E TABLE 1 BARPLOT  Figure 6: Plots showing relative mean reward improvement (left) and relative max reward improvement (right) of the multitask AMN over the expert DQNs. See Table 1 for details on how these values were calculated.  APPENDIX F TABLE 2 LEARNING CURVES  Figure 7: Learning curve plots of the results in Table2.  16  atlantisboxingbreakoutcrazyclimberenduropongseaquestspaceinvaders050100150200250300RelativeMeanScore(100%#AMNDQN)atlantisboxingbreakoutcrazyclimberenduropongseaquestspaceinvaders020406080100120RelativeMaxScore(100%#AMNDQN)0200400BREAKOUT010002000GOPHER400065009000KRULL024#104ROAD RUNNERRandomAMN-PolicyAMN-Feature051002040ROBOTANK0510036#104STAR GUNNER051007.515#104VIDEO PINBALL",
1511.06018,2016,Segmental Recurrent Neural Networks,"['Segmental Recurrent Neural Networks\nLingpeng Kong', 'Chris Dyer', 'Noah Smith']",https://arxiv.org/pdf/1511.06018,"6 1 0 2    r a  M 1         ] L C . s c [      2 v 8 1 0 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  SEGMENTAL RECURRENT NEURAL NETWORKS  Lingpeng Kong, Chris Dyer School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {lingpenk, cdyer}@cs.cmu.edu Noah A. Smith Computer Science & Engineering University of Washington Seattle, WA 98195, USA nasmith@cs.washington.edu  ABSTRACT  We introduce segmental recurrent neural networks (SRNNs) which deﬁne, given an input sequence, a joint probability distribution over segmentations of the input and labelings of the segments. Representations of the input segments (i.e., contiguous subsequences of the input) are computed by encoding their con- stituent tokens using bidirectional recurrent neural nets, and these “segment em- beddings” are used to deﬁne compatibility scores with output labels. These lo- cal compatibility scores are integrated using a global semi-Markov conditional random ﬁeld. Both fully supervised training—in which segment boundaries and labels are observed—as well as partially supervised training—in which segment boundaries are latent—are straightforward. Experiments on handwriting recog- nition and joint Chinese word segmentation/POS tagging show that, compared to models that do not explicitly represent segments such as BIO tagging schemes and connectionist temporal classiﬁcation (CTC), SRNNs obtain substantially higher accuracies.  1  INTRODUCTION  For sequential data like speech, handwriting, and DNA, segmentation and segment-labeling are abstractions that capture many common data analysis challenges. We consider the joint task of breaking an input sequence into contiguous, arbitrary-length segments while labeling each segment. Our new approach to this problem is the segmental recursive neural network (SRNN). SRNNs com- bine two powerful machine learning tools: representation learning and structured prediction. First, bidirectional recurrent neural networks (RNNs) embed every feasible segment of the input in a con- tinuous space, and these embeddings are then used to calculate the compatibility of each candidate segment with a label. Unlike past RNN-based approaches (e.g., connectionist temporal classiﬁcation or CTC; Graves et al., 2006) each candidate segment is represented explicitly, allowing application in settings where an alignment between segments and labels is desired as part of the output (e.g., protein secondary structure prediction or information extraction from text). At the same time, SRNNs are a variant of semi-Markov conditional random ﬁelds (Sarawagi & Co- hen, 2004), in that they deﬁne a conditional probability distribution over the output space (segmen- tation and labeling) given the input sequence (§2). This allows explicit modeling of statistical de- pendencies, such as those between adjacent labels, and also of segment lengths (unlike widely used symbolic approaches based on “BIO” tagging; Ramshaw & Marcus, 1995). Because the probability score decomposes into chain-structured clique potentials, polynomial-time dynamic programming algorithms exist for prediction and parameter estimation (§3).  1  Published as a conference paper at ICLR 2016  Parameters can be learned with either a fully supervised objective—where both segment boundaries and segment labels are provided at training time—and partially supervised training objectives— where segment boundaries are latent (§4). We compare SRNNs to strong models that do not explicitly represent segments on handwriting recognition and joint word segmentation and part-of-speech tagging for Chinese text, showing sig- niﬁcant accuracy improvements in both, demonstrating the value of models that explicitly model segmentation even when segmentation is not necessary for downstream tasks (§5). 2 MODEL  Given a sequence of input observations x = (cid:104)x1, x2, . . . , x|x|(cid:105) with length |x|, a segmental recur- rent neural network (SRNN) deﬁnes a joint distribution p(y, z | x) over a sequence of labeled segments each of which is characterized by a duration (zi ∈ Z+) and label (yi ∈ Y ). The segment durations constrained such that(cid:80)|z| i=1 zi = |x|. The length of the output sequence |y| = |z| is a random variable, and |y| ≤ |x| with probability 1. We write the starting time of segment i as si = 1 +(cid:80)j<i zj. To motivate our model form, we state several desiderata. First, we are interested in the following prediction problem,  y∗ = arg max  y  p(y | x) = arg max  p(y, z | x) ≈ arg max  y  max  z  p(y, z | x).  (1)  y (cid:88)z  Note the use of joint maximization over y and z as a computationally tractable substitute for marginalizing out z; this is commonly done in natural language processing. Second, for problems where the explicit durations observations are unavailable at training time and are inferred as a latent variable, we must be able to use a marginal likelihood training criterion,  L = − log p(y | x) = − log(cid:88)z  p(y, z | x).  (2)  In Eqs. 1 and 2, the conditional probability of the labeled segment sequence is (assuming kth order dependencies on y):  p(y, z | x) =  1  Z(x)  |y|  (cid:89)i=1  exp f (yi−k:i, zi, x)  (3)  where Z(x) is an appropriate normalization function. To ensure the expressiveness of f and the computational efﬁciency of the maximization and marginalization problems in Eqs. 1 and 2, we use the following deﬁnition of f,  f (yi−k:i, zi, xsi:si+zi−1) = w  (cid:62)  φ(V[gy(yi−k); . . . ; gy(yi); gz(zi);  −−→RNN(csi:si+zi−1);←−−RNN(csi:si+zi−1)] + a) + b  (4)  where −−→RNN(csi:si+zi−1) is a recurrent neural network that computes the forward segment em- bedding by “encoding” the zi-length subsequence of x starting at index si,1 and ←−−RNN computes the reverse segment embedding (i.e., traversing the sequence in reverse order), and gy and gz are functions which map the label candidate y and segmentation duration z into a vector representation. The notation [a; b; c] denotes vector concatenation. Finally, the concatenated segment duration, label candidates and segment embedding are passed through a afﬁne transformation layer param- eterized by V and a and a nonlinear activation function φ (e.g., tanh), and a dot product with a vector w and addition by scalar b computes the log potential for the clique. Our proposed model is equivalent to a semi-Markov conditional random ﬁeld with local features computed using neural networks. Figure 1 shows the model graphically.  1Rather than directly reading the xi’s, each token is represented as the concatenation, ci, of a forward and backward over the sequence of raw inputs. This permits tokens to be sensitive to the contexts they occur in, and this is standardly used with neural net sequence labeling models (Graves et al., 2006).  2  Published as a conference paper at ICLR 2016  We chose bidirectional LSTMs (Graves & Schmidhuber, 2005) as the implementation of the RNNs in Eq. 4. LSTMs (Hochreiter & Schmidhuber, 1997) are a popular variant of RNNs which have been seen successful in many representation learning problems (Graves & Jaitly, 2014; Karpathy & Fei-Fei, 2015). Bidirectional LSTMs enable effective computation for embedings in both directions and are known to be good at preserving long distance dependencies, and hence are well-suited for our task.  Figure 1: Graphical model showing a six-frame input and three output segments with durations z = (cid:104)3, 2, 1(cid:105) (this particular setting of z is shown only to simplify the layout of this ﬁgure; the model assigns probabilities to all valid settings of z). Circles represent random variables. Shaded nodes are observed in training; open nodes are latent random variables; diamonds are deterministic functions of their parents; dashed lines indicate optional statistical dependencies that can be included at the cost of increased inference complexity. The graphical notation we use here draws on conventions used to illustrate neural networks and graphical models.  3  INFERENCE WITH DYNAMIC PROGRAMMING  We are interested in three inference problems: (i) ﬁnding the most probable segmentation/labeling for a model given a sequence x; (ii) evaluating the partition function Z(x); and (iii) computing the posterior marginal Z(x, y), which sums over all segmentations compatible with a reference se- quence y. These can all be solved using dynamic programming. For simplicity, we will assume zeroth order Markov dependencies between the yis. Extensions to the kth order Markov dependen- cies should be straightforward. Since each of these algorithms relies on the forward and reverse segment embeddings, we ﬁrst discuss how these can be computed before going on to the inference algorithms.  3.1 COMPUTING SEGMENT EMBEDDINGS Let the −→h i,j designate the −−−→RNN encoding of the input span (i, j), traversing from left to right, and let ←−h i,j designate the reverse direction encoding using ←−−−RNN. There are thus O(|x|2) vectors that must be computed, each of length O(|x|). Naively this can be computed in time O(|x|3), but the  3  x1x2x3x4x5x6((Encoder BiRNNSegmentation/Labeling Modelx1x2x1x2x1x2x1x2x1x1x2x1c1c2c3c4c5c6z1z2z3y1y2y3x1x2x3x4x5x6!h1,3!h4,5!h6,6 h6,6 h4,5 h1,3Published as a conference paper at ICLR 2016  following dynamic program reduces this to O(|x|2):  −→h i,i = −−−→RNN(−→h 0, ci) −→h i,j = −−−→RNN(−→h i,j−1, cj) ←−h i,i = ←−−−RNN(←−h 0, ci) ←−h i,j = ←−−−RNN(←−h i+1,j, ci)  The algorithm is executed by initializing in the values on the diagonal (representing segments of length 1) and then inductively ﬁlling out the rest of the matrix. In practice, we often can put a upper bound for the length of a eligible segment thus reducing the complexity of runtime to O(|x|). This savings can be substantial for very long sequences (e.g., those encountered in speech recognition).  3.2 COMPUTING THE MOST PROBABLE SEGMENTATION/LABELING AND Z(x) |x|) different labelings For the input sequence x, there are 2|x|−1 possible segmentations and O(|Y | of these segments, making exhaustive computation entirely infeasible. Fortunately, the partition function Z(x) may be computed in polynomial time with the following dynamic program:  α0 = 1  αj =(cid:88)i<j  αi×  (cid:88)y∈Y(cid:16)exp w  (cid:62)  φ(V[gy(y); gz(zi);−−→RNN(csi:si+zi−1);←−−RNN(csi:si+zi−1)] + a) + b(cid:17) .  After computing these values, Z(x) = α|x|. By changing the summations to a max operators (and storing the corresponding arg max values), the maximal a posteriori segmentation/labeling can be computed. Both the partition function evaluation and the search for the MAP outputs run in time O(|x|2 · |Y |) with this dynamic program. Adding nth order Markov dependencies between the yis adds requires additional information in each state and increases the time and space requirements by a factor of O(|Y |n). However, this may be tractable for small |Y | and n. Avoiding overﬂow. Since this dynamic program sums over exponentially many segmentations and labelings, the values in the αi chart can become very large. Thus, to avoid issues with overﬂow, computations of the αi’s must be carried out in log space.2  3.3 COMPUTING Z(x, y)  To compute the posterior marginal Z(x, y), it is necessary to sum over all segmentations that are compatible with a label sequence y given an input sequence x. To do so requires only a minor modiﬁcation of the previous dynamic program to track how much of the reference label sequence y has been consumed. We introduce the variable m as the index into y for this purpose. The modiﬁed recurrences are:  γ0(0) = 1  γj(m) =(cid:88)i<j  γi(m − 1)× (cid:62)  (cid:16)exp w  The value Z(x, y) is γ|x|(|y|).  φ(V[gy(yi); gz(zi);−−→RNN(csi:si+zi−1);←−−RNN(csi:si+zi−1)] + a) + b(cid:17) .  2An alternative strategy for avoiding overﬂow in similar dynamic programs is to rescale the forward sum- mations at each time step (Rabiner, 1989; Graves et al., 2006). Unfortunately, in a semi-Markov architecture each term in αi sums over different segmentations (e.g., the summation for α2 will have contain some terms that include α1 and some terms that include only α0), which means there are no common factors, making this strategy inapplicable.  4  Published as a conference paper at ICLR 2016  4 PARAMETER LEARNING  We consider two different learning objectives.  4.1 SUPERVISED LEARNING  In the supervised case, both the segment durations (z) and their labels (y) are observed.  L = (cid:88)(x,y,z)∈D − log p(y, z | x) = (cid:88)(x,y,z)∈D  log Z(x) − log Z(x, y, z)  In this expression, the unnormalized conditional probability of the reference segmentation/labeling, given the input x is written as Z(x, y, z).  4.2 PARTIALLY SUPERVISED LEARNING  In the partially supervised case, only the labels are observed and the segments (the z) are unobserved and marginalized.  L = (cid:88)(x,y)∈D − log p(y | x) = (cid:88)(x,y)∈D (cid:88)z∈Z(x,y) = (cid:88)(x,y)∈D  log Z(x) − log Z(x, y)  − log p(y, z | x)  For both the fully and partially supervised scenarios, the necessary derivatives can be computed using automatic differentiation or (equivalently) with backward variants of the above dynamic pro- grams (Sarawagi & Cohen, 2004).  5 EXPERIMENTS  We present two sets of experiments to compare segmental recurrent neural networks against mod- els that do not include explicit representations of segmentation. For the handwriting recognition task, we consider connectionist temporal classiﬁcation (CTC) (Graves et al., 2006); for Chinese word segmentation, we consider BIO tagging. In these experiments, we do not include Markovian dependencies between adjacent labels for our models or the baselines.  5.1 ONLINE HANDWRITING RECOGNITION  Dataset We use the handwriting dataset from Kassel (1995). This dataset is an online collection of hand-written words from 150 writers. It is recorded as the coordinates (x, y) at time t plus special pen-down/pen-up notations. We break the coordinates into strokes using the pen-down and pen-up notations. One character typically consists one or more contiguous strokes.3 The dataset is split into train, development and test set following Kassel (1995). Table 1 presents the statistics for the dataset. A well-know variant of this dataset was introduced by Taskar et al. (2004). Taskar et al. (2004) selected a “clean” subset of about 6,100 words and rasterized and normalized the images of each letter. Then, the uppercased letters (since they are usually the ﬁrst character in a word) are removed and only the lowercase letters are used. The main difference between our dataset and theirs is that their dataset is “ofﬂine” — Taskar et al. (2004) mapped each character into a bitmap and treated the segmentation of characters as a preprocessing step. We use the richer representation of the sequence of strokes as input.  3There are infrequent cases where one stroke can go across multiple characters or the strokes which form  the character can be not contiguous. We leave those cases for future work.  5  Published as a conference paper at ICLR 2016  #words 4,368 1,269 637 6,274  #characters  37,247 10,905 5,516 53,668  Train Dev Test Total  Table 1: Statistics of the Online Handwriting Recognition Dataset  Implementation We trained two versions of our model on this dataset, namely, the fully supervised model (§4.1), which takes advantage of the gold segmentations on training data, and the partially supervised model (§4.2) in which the gold segmentations are only used in the evaluation. A CTC model reimplemented on the top of our Encoder BiRNNs layer (Figure 1) is used as a baseline so that we can see the effect of explicitly representing the segmentation.4 For the decoding of the CTC model, we simply use the best path decoding, where we assume that the most probable path will correspond to the most probable labeling, although it is known that preﬁx search decoding can slightly improve the results (Graves et al., 2006). As a preprocessing step, we ﬁrst represented each point in the dataset using a 4 dimensional vector, p = (px, py, ∆px, ∆py), where px and py are the normalized coordinates of the point and ∆px and ∆py are the corresponding changes in the coordinates with respect to the previous point. ∆px and ∆py are meant to capture basic direction information. Then we map the points inside one stroke into a ﬁxed-length vector using a bi-direction LSTM. Speciﬁcally, we concatenated the last position’s hidden states in both directions and use it as the input vector x for the stroke. In all the experiments, we use Adam (Kingma & Ba, 2014) with λ = 1 × 10−6 to optimize the parameters in the models. We train these models until convergence and picked the best model over the iterations based on development set performance then report performance on the test set. We used 5 as the hidden state dimension in the bidirectional RNNs, which map the points into ﬁxed- length stroke embeddings (hence the input vector size 5×2 = 10). We set the hidden dimensions of c in our model and CTC model to 24 and segment embedding h in our model as 18. These dimensions were chosen based on intuitively reasonable values, and it was conﬁrmed on development data that they performed well. We tried to experiment with larger hidden dimensions and we found the performance did not vary much. Future work might more carefully optimize these parameters. As for speed, the partially supervised SRNNs run at ∼40 instances per second and the fully super- vised SRNNs run at ∼53 instances during training using a single CPU. Results The results of the online handwriting recognition task are presented in Table 2. We see that both of our models outperform the baseline CTC model, which does not carry an explicit represen- tation for the segments being labeled, by a signiﬁcant margin. An interesting ﬁnding is, although the partially supervised model performs slightly worse in the development set, it actually outperforms the fully supervised model in the test set. Because the test set is written by different people from the train and development set, they exhibit different styles in their handwriting; our results suggest that the partially supervised model may generalize better across different writing styles.  5.2  JOINT CHINESE WORD SEGMENTATION AND POS TAGGING  In this section, we will look into two related tasks. The ﬁrst task is joint Chinese word segmentation and POS tagging, where the z variables will group the Chinese characters into words and the y variables assign POS tags as labels to these words. We also tested our model on pure Chinese word segmentation task, where the assignments of z is the only thing we care about (simulated using a single label for all segments).  4The CTC interpretation rules specify that repeated symbols, e.g. aa will be interpreted as a single token of a. However since the segments in the handwriting recognition problem are extremely short, we use different rules and interpret this as aa. That is, only the blank symbol may be used to represent extended durations. Our experiments indicate this has little effect, and Graves (p.c.) reports that this change does not harm performance in general.  6  Published as a conference paper at ICLR 2016  Dev  Test  Fseg  Rseg  Error Pseg 98.7% 98.4% 98.6% 4.2% 99.2% 99.1% 99.2% 2.7% 98.9% 98.6% 98.8% 4.3% 98.8% 98.6% 98.6% 5.4% 13.8%  15.2%  Error  Rseg  Fseg  Pseg  -  -  -  -  -  -  SRNNs (Partial) SRNNs (Full)  CTC  Table 2: Hand-writing Recognition Task  Dev Rseg  Test Rseg  BiRNNs SRNNs  BiRNNs SRNNs  Pseg Fseg 93.2% 92.9% 93.0% 94.7% 95.2% 95.0% 93.8% 93.8% 93.8% 95.3% 95.8% 95.5%  Fseg  Pseg  Rtag  Ptag Ftag 87.1% 86.9% 87.0% 88.1% 88.5% 88.3% 89.0% 89.1% 89.0% 89.8% 90.3% 90.0%  Rtag  Ftag  Ptag  Table 3: Joint Chinese Word Segmentation and POS Tagging  Dataset We used standard benchmark datasets for these two tasks. For the joint Chinese word segmentation and POS tagging task, we use the Penn Chinese Treebank 5 (Xue et al., 2005), fol- lowing the standard train/dev/test splits. For the pure Chinese word segmentation task, we used the SIGHAN 2005 dataset5. This dataset contains four portions, covering both simpliﬁed and traditional Chinese. Since there is no pre-assigned dev set in this dataset (only train and test set are provided), we manually split the original train set into two, one of which (roughly the same size as the test set) is used as the dev set. For both tasks, we use Wang2Vec (Ling et al., 2015) to generate the pre-trained character embeddings from the Chinese Gigaword (Graff & Chen, 2005). Implementation Only supervised version SRNNs (§4.1) is tested in these tasks. The baseline model is a bi-directional LSTM tagger (basically the same structure as our Encoder BiRNNs in Figure 1). It takes the c at each time step and pushes it through an element-wise non-linear transformation (tanh) followed by an afﬁne transformation to map it to the same dimension as the number of labels. The total loss is therefore the sum of negative log probabilities over the sequence. Greedy decoding is applied in the baseline model, making it a zeroth order model like our SRNNs. In order to perform segmentation and POS tagging jointly, we composed the POS tags with “B” or “I” to represent the segmentation point. For the segmentation-only task, in the SRNNs we simply used same dummy tag for all y and only care about the z assignments. In the BiRNN case, we used “B” and “I” tags. For both tasks, the dimension for the input character embedding is 64. For our model, the dimension for c and the segment embedding h is set to 24. For the baseline bi-directional LSTM tagger, we set the hidden dimension (the c equivalent) size to 128. Here we deliberately chose a larger size than in our model hoping to make the number of parameters in the bi-directional LSTM tagger roughly the same as our model. We trained these models until convergence and picked the best model over iterations based on its performance on the development set. As for speed, the SRNNs run at ∼3.7 sentence per second during training on the CTB dataset using a single CPU. Results Table 3 presents the results for the joint Chinese word segmentation task. We can see that in both segmentation and POS tagging, the SRNNs achieve higher F -scores than the BiRNNs. Table 4 presents the results for the pure Chinese word segmentation task. The SRNNs perform better than the BiRNNs with the exception of the PKU portion of the dataset. The reason for this is probably because the training set in this portion is the smallest among the four. Thus leads to high variance in the test results.  5http://www.sighan.org/bakeoff2005/  7  Published as a conference paper at ICLR 2016  BiRNNs  SRNNs Rseg  Fseg  Rseg  Pseg Fseg 92.7% 93.1% 92.9% 93.3% 93.7% 93.5% CU 92.8% 93.5% 93.1% 93.2% 94.2% 93.7% AS MSR 89.9% 90.1% 90.0% 90.9% 90.4% 90.7% PKU 91.5% 91.2% 91.3% 90.6% 90.6% 90.6%  Pseg  Table 4: Chinese Word Segmentation Results on SIGHAN 2005 dataset. There are four portions of the dataset from City University of Hong Kong (CU), Academia Sinica (AS), Microsoft Research (MSR) and Peking University (PKU). The former two are in traditional Chinese and the latter two are in simpliﬁed Chinese.  6 RELATED WORK  Segmental labeling problems have been widely studied. A widely used approach to a segmental labeling problems with neural networks is the connectionist temporal classiﬁcation (CTC) objective and decoding rule of Graves et al. (2006). CTC reduces the “segmental” sequence label problem to a classical sequence labeling problem in which every position in an input sequence x is explicitly labeled by interpreting repetitions of input labels—or input labels followed by a special “blank” out- put symbol—as being a single label with a longer duration. During training, the marginal likelihood of the set of labelings compatible (according to the CTC interpretation rules) with the reference label y is maximized. CTC has demonstrated impressive success in various fully discriminative end-to- end speech recognition models (Graves & Jaitly, 2014; Maas et al., 2015; Hannun et al., 2014, inter alia). Although CTC has been used successfully and its reuse of conventional sequence labeling architec- tures is appealing, it has several potentially serious limitations. First, it is not possible to model inter- label dependencies explicitly—these must instead be captured indirectly by the underlying RNNs. Second, CTC has no explicit segmentation model. Although this is most serious in applications where segmentation is a necessary/desired output (e.g., information extraction, protein secondary structure prediction), we argue that explicit segmentation is potentially valuable even when the seg- mentation is not required. To illustrate the value of explicit segments, consider the problem of phone recognition. For this task, segmental duration is strongly correlated with label identity (e.g., while an [o] phone token might last 300ms, it is unlikely that a [t] would) and thus modeling it explicitly may be useful. Finally, making an explicit labeling decision for every position (and introducing a special blank symbol) in an input sequence is conceptually unappealing. Several alternatives to CTC have been approached, such as using various attention mechanisms in place of marginalization (Chan et al., 2015; Bahdanau et al., 2015). These have been applied to end- to-end discriminative speech recognition problem. A more direct alternative to our method—indeed it was proposed to solve several of the same problems we identiﬁed—is due to Graves (2012). However, a crucial difference is that our model explicitly constructs representations of segments which are used to label the segment while that model relies on a marginalized frame-level labeling with a null symbol. The work of Abdel-Hamid (2013) also seeks to construct embeddings of multi-frame segments. Their approach is quite different than the one taken here. First, they compute representations of variable-sized segments by uniformly sampling a ﬁxed number of frames and using these to con- struct a representation of the segment with a simple feedforward network. Second, they do not consider them problem of latent segmentation. Finally, using neural networks to provide local features in conditional random ﬁeld models has also been proposed for sequential models (Peng et al., 2009) and tree-structured models (Durrett & Klein, 2015). To our knowledge, this is the ﬁrst application to semi-Markov structures.  8  Published as a conference paper at ICLR 2016  7 CONCLUSION  We have proposed a new model for segment labeling problems that learns representations of seg- ments of an input sequence and then labels these. We outperform existing alternatives both when segmental information should be recovered and when it is only latent. We have not trained the seg- mental representations to be of any use beyond making good labeling (or segmentation) decisions, but an intriguing avenue for future work would be to construct representations that are useful for other tasks.  ACKNOWLEDGMENTS  The authors thank the anonymous reviewers, Yanchuan Sim, and Hao Tang for their helpful feed- back. This work was sponsored in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Ofﬁce (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114.  REFERENCES Abdel-Hamid, Huda. Structural-Functional Analysis of Plant Cyclic Nucleotide Gated Ion Chan-  nels. PhD thesis, University of Toronto, 2013.  Bahdanau, Dzmitry, Chorowski, Jan, Serdyuk, Dmitriy, Brakel, Phil´emon, and Bengio, Yoshua.  End-to-end attention-based large vocabulary speech recognition. CoRR, abs/1508.04395, 2015.  Chan, William, Jaitly, Navdeep, Le, Quoc V., and Vinyals, Oriol. Listen, attend, and spell. CoRR,  abs/1508.01211, 2015.  Durrett, Greg and Klein, Dan. Neural CRF parsing. In Proc. ACL, 2015.  Graff, David and Chen, Ke. Chinese gigaword. LDC Catalog No.: LDC2003T09, 1, 2005.  Graves, Alex. Sequence transduction with recurrent neural networks. In Proc. ICML, 2012.  Graves, Alex and Jaitly, Navdeep. Towards end-to-end speech recognition with recurrent neural  networks. In Proc. ICML, 2014.  Graves, Alex and Schmidhuber, J¨urgen. Framewise phoneme classiﬁcation with bidirectional LSTM  and other neural network architectures. Neural Networks, 18(5):602–610, 2005.  Graves, Alex, Fern´andez, Santiago, Gomez, Faustino, and Schmidhuber, J¨urgen. Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks. In Proc. ICML, 2006.  Hannun, Awni Y., Case, Carl, Casper, Jared, Catanzaro, Bryan C., Diamos, Greg, Elsen, Erich, Prenger, Ryan, Satheesh, Sanjeev, Sengupta, Shubho, Coates, Adam, and Ng, Andrew Y. Deep speech: Scaling up end-to-end speech recognition. CoRR, abs/1412.5567, 2014.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Karpathy, Andrej and Fei-Fei, Li. Deep visual-semantic alignments for generating image descrip-  tions. In Proc. CVPR, 2015.  Kassel, Robert H. A comparison of approaches to on-line handwritten character recognition. PhD  thesis, Massachusetts Institute of Technology, 1995.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980, 2014.  Ling, Wang, Dyer, Chris, Black, Alan W, and Trancoso, Isabel. Two/too simple adaptations of  word2vec for syntax problems. In Proc. NAACL, 2015.  Maas, Andrew L., Xie, Ziang, Jurafsky, Dan, and Ng, Andrew Y. Lexicon-free conversational speech  recognition with neural networks. In Proc. NAACL, 2015.  9  Published as a conference paper at ICLR 2016  Peng, Jian, Bo, Liefeng, and Xu, Jinbo. Conditional neural ﬁelds. In Proc. NIPS, 2009.  Rabiner, Lawrence R. A tutorion on hidden Markov models and selected applications in speech  recognition. Proc. IEEE, 77(2), 1989.  Ramshaw, Lance A. and Marcus, Mitchell P. Text chunking using transformation-based learning. In  Proceedings of the Workshop on Very Large Corpora, 1995.  Sarawagi, Sunita and Cohen, William W. Semi-Markov conditional random ﬁelds for information  extraction. In Proc. NIPS, 2004.  Taskar, Ben, Guestrin, Carlos, and Koller, Daphne. Max-margin Markov networks. NIPS, 16:25,  2004.  Xue, Naiwen, Xia, Fei, Chiou, Fu-Dong, and Palmer, Martha. The Penn Chinese TreeBank: Phrase  structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238, 2005.  10  ",
1511.04707,2016,Deep Linear Discriminant Analysis,"['Deep Linear Discriminant Analysis [code]\nMatthias Dorfer', 'Rainer Kelz', 'Gerhard Widmer']",https://arxiv.org/pdf/1511.04707,"6 1 0 2     b e F 7 1         ]  G L . s c [      5 v 7 0 7 4 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  DEEP LINEAR DISCRIMINANT ANALYSIS  Matthias Dorfer, Rainer Kelz & Gerhard Widmer ∗ Department of Computational Perception Johannes Kepler University Linz Linz, 4040, AUT {matthias.dorfer, rainer.kelz, gerhard.widmer}@jku.at  ABSTRACT  We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns lin- early separable latent representations in an end-to-end fashion. Classic LDA ex- tracts features which preserve class separability and is used for dimensionality reduction for many classiﬁcation problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear ex- tension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which: (a) have low variance within the same class and (b) high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three dif- ferent benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and CIFAR-10 and outperforms a network trained with categorical cross entropy (having the same architecture) on a supervised set- ting of STL-10.  1  INTRODUCTION  Linear Discriminant Analysis (LDA) is a method from multivariate statistics which seeks to ﬁnd a linear projection of high-dimensional observations into a lower-dimensional space (Fisher, 1936). When its preconditions are fulﬁlled, LDA allows to deﬁne optimal linear decision boundaries in the resulting latent space. The aim of this paper is to exploit the beneﬁcial properties of classic LDA (low intra class variability, hight inter-class variability, optimal decision boundaries) by reformulating its objective to learn linearly separable representations based on a deep neural network (DNN). Recently, methods related to LDA achieved great success in combination with deep neural networks. Andrew et al. published a deep version of Canonical Correlation Analysis (DCCA) (Andrew et al., 2013). In their evaluations, DCCA is used to produce correlated representations of multi-modal input data of simultaneously recorded acoustic and articulatory speech data. Clevert et al. pro- pose Rectiﬁed Factor Networks (RFNs) which are a neural network interpretation of classic factor analysis (Clevert et al., 2015). RFNs are used for unsupervised pre-training and help to improve classiﬁcation performance on four different benchmark datasets. A similar method called PCANet – as well as an LDA based variation – was proposed by Chan et al. (2015). PCANet can be seen as a simple unsupervised convolutional deep learning approach. The method proceeds with cascaded Principal Component Analysis (PCA), binary hashing and block histogram computations. However, one crucial bottleneck of their approach is its limitation to very shallow architectures (two stages) (Chan et al., 2015). Stuhlsatz et. al. already picked up the idea of combining LDA with a neural networks and proposed a generalized version of LDA (Stuhlsatz et al., 2012). Their approach starts with pre-training a stack of restricted Boltzmann machines. In a second step, the pre-trained model is ﬁne-tuned with respect to a linear discriminant criterion. LDA has the disadvantage that it overemphasises large distances at the cost of confusing neighbouring classes. In (Stuhlsatz et al., 2012) this problem is tackled by a heuristic weighting scheme for computing the within-class scatter matrix required for LDA optimization.  ∗http://www.cp.jku.at/  1  Published as a conference paper at ICLR 2016  1.1 MAIN IDEA OF THIS PAPER  The approaches mentioned so far all have in common that they are based on well established methods from multivariate statistics. Inspired by their work, we propose an end-to-end DNN version of LDA - namely Deep Linear Discriminant Analysis (DeepLDA). Deep learning has become the state of the art in automatic feature learning and replaced existing ap- proaches based on hand engineered features in many ﬁelds such as object recognition (Krizhevsky et al., 2012). DeepLDA is motivated by the fact that when the preconditions of LDA are met, it is capable of ﬁnding linear combinations of the input features which allow for optimal linear decision boundaries. In general, LDA takes features as input. The intuition of our method is to use LDA as an objective on top of a powerful feature learning algorithm. Instead of maximizing the likelihood of target labels for individual samples, we propose an LDA eigenvalue-based objective function that pushes the network to produce discriminative feature distributions. The parameters are optimized by back-propagating the error of an LDA-based objective through the entire network. We tackle the feature learning problem by focusing on directions in the latent space with smallest discrimina- tive power. This replaces the weighting scheme of (Stuhlsatz et al., 2012) and allows to operate on the original formulation of LDA. We expect that DeepLDA will produce linearly separable hidden representations with similar discriminative power in all directions of the latent space. Such repre- sentations should also be related with a high classiﬁcation potential of the respective networks. The experimental classiﬁcation results reported below will conﬁrm this positive effect on classiﬁcation accuracy, and two additional experiments (Section 5) will give us some ﬁrst qualitative conﬁrmation that the learned representations show the expected properties. The reminder of the paper is structured as follows. In Section 2 we provide a general formulation of a DNN. Based on this formulation we introduce DeepLDA, a non-linear extension to classic LDA in Section 3. In Section 4 we experimentally evaluate our approach on three benchmark datasets. Section 5 provides a deeper insight into the structure of DeepLDA’s internal represenations. In Section 6 we conclude the paper.  2 DEEP NEURAL NETWORKS  As the proposed model is built on top of a DNN we brieﬂy describe the training paradigm of a network used for classiﬁcation problems such as object recognition. A neural network with P hidden layers is represented as a non-linear function f (Θ) with model parameters Θ = {Θ1, ..., ΘP}. In the supervised setting we are additionally given a set of N train samples x1, ...xN along with corresponding classiﬁcation targets t1, ...tN ∈ {1, ..., C}. We further assume that the network output pi = (pi,1, ..., pi,C) = f (xi, Θ) is normalized by the softmax- function to obtain class (pseudo-)probabilities. The network is then optimized using Stochastic Gradient Descent (SGD) with the goal of ﬁnding an optimal model parametrization Θ with respect to a certain loss function li(Θ) = l(f (xi, Θ), ti).  N(cid:88)  i=1  Θ = arg min  Θ  1 N  li(Θ) = − C(cid:88)  li(Θ)  (1)  For multi-class classiﬁcation problems, Categorical-Cross-Entropy (CCE) is a commonly used op- timization target and formulated for observation xi and target label ti as follows  yi,jlog(pi,j)  (2)  j=1  where yi,j is 1 if observation xi belongs to class ti (j = ti) and 0 otherwise. In particular, the CCE tries to maximize the likelihood of the target class ti for each of the individual training examples xi under the model with parameters Θ. Figure 1a shows a sketch of this general network architecture. We would like to emphasize that objectives such as CCE do not impose any direct constraints – such as linear separability – on the latent space representation.  2  Published as a conference paper at ICLR 2016  (a) The output of the network gets normalized by a soft max layer to form valid probabilities. The CCE objective maximizes the likelihood of the target class under the model.  (b) On the topmost hidden layer we compute an LDA which produces corresponding eigenval- ues. The optimization target is to maximize those eigenvalues.  Figure 1: Schematic sketch of a DNN and DeepLDA. For both architectures the input data is ﬁrst propagated through the layers of the DNN. However, the ﬁnal layer and the optimization target are different.  3 DEEP LINEAR DISCRIMINANT ANALYSIS (DEEPLDA)  In this section we ﬁrst provide a general introduction to LDA. Based on this introduction we pro- pose DeepLDA, which optimizes an LDA-based optimization target in an end-to-end DNN fashion. Finally we describe how DeepLDA is used to predict class probabilities of unseen test samples.  3.1 LINEAR DISCRIMINANT ANALYSIS Let x1, ..., xN = X ∈ RN×d denote a set of N samples belonging to C different classes c ∈ {1, ..., C}. The input representation X can either be hand engineered features, or hidden space representations H produced by a DNN (Andrew et al., 2013). LDA seeks to ﬁnd a linear projection A ∈ Rl×d into a lower l-dimensional subspace L where l = C − 1. The resulting linear combina- tions of features xiAT are maximally separated in this space (Fisher, 1936). The LDA objective to ﬁnd projection matrix A is formulated as:  |ASbAT| |ASwAT|  arg max  A  (3)  where Sb is the between scatter matrix and deﬁned via the total scatter matrix St and within scatter matrix Sw as Sb = St − Sw. Sw is deﬁned as the mean of the C individual class covariance matrices Sc (Equation (4) and (5)). ¯Xc = Xc − mc are the mean-centered observations of class c with per-class mean vector mc ( ¯X is deﬁned analogously for the entire population X). The total scatter matrix St is the covariance matrix over the entire population of observations X.  Sc =  1  Nc − 1  ¯XT c  ¯Xc  (cid:88)  c  Sc  Sw =  1 C  St =  1  N − 1  ¯XT ¯X  (4)  (5)  (6)  The linear combinations that maximize the objective in Equation (3) maximize the ratio of between- and within-class scatter also reffered to as separation. This means in particular that a set of projected observations of the same class show low variance, whereas the projections of observations of differ- ent classes have high variance in the resulting space L. To ﬁnd the optimum solution for Equation (3) one has to solve the general eigenvalue problem Sbe = vSwe. The projection matrix A is the set of eigenvectors e associated with this problem. In the following sections we will cast LDA as an objective function for DNN.  3  Published as a conference paper at ICLR 2016  3.2 DEEPLDA MODEL CONFIGURATION  Figure 1b shows a schematic sketch of DeepLDA. Instead of sample-wise optimization of the CCE loss on the predicted class probabilities (see Section 2) we put an LDA-layer on top of the DNN. This means in particular that we do not penalize the misclassiﬁcation of individual samples. Instead we try to produce features that show a low intra-class and high inter-class variability. We address this maximization problem by a modiﬁed version of the general LDA eigenvalue problem proposed in the following section. In contrast to CCE, DeepLDA optimization operates on the properties of the distribution parameters of the hidden representation produced by the neural net. As eigenvalue optimization is tied to its corresponding eigenvectors (a linear projection matrix), DeepLDA can be also seen as a special case of a dense layer.  3.3 MODIFIED DEEPLDA OPTIMIZATION TARGET  Based on Section 3.1 we reformulate the LDA objective to be suitable for a combination with deep learning. As already discussed by Stuhlsatz et al. (2012) and Lu et al. (2005) the estimation of Sw overemphasises high eigenvalues whereas small eigenvalues are estimated as too low. To weaken this effect, Friedman (1989) proposed to regularize the within scatter matrix by adding a multiple of the identity matrix Sw + λI. Adding the identity matrix has the second advantage of stabilizing small eigenvalues. The resulting eigenvalue problem is then formulated as  Sbei = vi(Sw + λI)ei  (7) where e = e1, ..., eC−1 are the resulting eigenvectors and v = v1, ...vC−1 the corresponding eigen- values. Once the problem is solved, each eigenvalue vi quantiﬁes the amount of discriminative variance (separation) in direction of the corresponding eigenvector ei. If one would like to com- bine this objective with a DNN the optimization target would be the maximization of the individual eigenvalues. In particular, we expect that maximizing the individual eigenvalues – which reﬂect the separation in the respective eigenvector directions – leads to a maximization of the discriminative power of the neural net. In our initial experiments we started to formulate the objective as:  C−1(cid:88)  i=1  arg max  Θ  1  C − 1  vi  (8)  One problem we discovered with the objective in Equation (8) is that the net favours trivial solutions e. g. maximize only the largest eigenvalue as this produces the highest reward. In terms of classiﬁca- tion this means that it maximizes the distance of classes that are already separated at the expense of – potentially non-separated – neighbouring classes. This was already discussed by (Stuhlsatz et al., 2012) and tackled by a weighted computation of the between scatter matrix Sb. We propose a different solution to this problem and address it by focusing our optimization on the smallest of all C − 1 available eigenvalues. In particular we consider only the k eigenvalues that do not exceed a certain threshold for variance maximization:  vi with {v1, ..., vk} = {vj|vj < min{v1, ..., vC−1} + (cid:15)}  (9)  k(cid:88)  i=1  arg max  Θ  1 k  The intuition behind this formulation is to learn a net parametrization that pushes as much discrimi- native variance as possible into all of the C − 1 available feature dimensions. We would like to underline that this formulation allows to train DeepLDA networks with back- propagation in end-to-end fashion (see Appendix for a derivative of the loss functions’s gradient). Our models are optimized with the Nesterov momentum version of mini-batch SGD. Related meth- ods already showed that mini-batch learning on distribution parameters (in this case covariance ma- trices) is feasible if the batch-size is sufﬁciently large to be representative for the entire population (Wang et al., 2015a;b).  3.4 CLASSIFICATION BY DEEPLDA  This section describes how the most likely class label is assigned to an unseen test sample xt once the network is trained and parametrized. In a ﬁrst step we compute the topmost hidden representation  4  Published as a conference paper at ICLR 2016  H on the entire training set X. On this hidden representation we compute the LDA as described in Section 3.1 and 3.3 producing the corresponding eigenvectors e = {ei}C−1 i=1 which form the LDA projection matrix A. We would like to emphasize that since the parameters of the network are ﬁxed at this stage we make use of the entire training set to provide a stable estimate of the LDA projection. Based on A and the per-class mean hidden representations ¯Hc = (¯hT 1 , ..., ¯hT C) the distances of sample ht to the linear decision hyperplanes (Friedman et al., 2001) are deﬁned as  t TT − 1 2  d = hT  (10) where T are the decision hyperplane normal vectors. The subtracted term is the bias of the deci- sion functions placing the decision boundaries in between the means of the respective class hidden representations (no class priors included). The vector of class probabilities for test sample xt is then computed by applying the logistic function p(cid:48) c = 1/(1 + e−d) and further normalized by pc = p(cid:48) i to sum to one. Finally we assign class i with highest probability as arg maxi pi to the unseen test sample xt.  c/(cid:80) p(cid:48)  diag(cid:0) ¯HcTT(cid:1) with T = ¯HcAAT  4 EXPERIMENTS  In this section we present an experimental evaluation of DeepLDA on three benchmark data sets – namely MNIST, CIFAR-10 and STL-10 (see Figure 2 for some sample images). We compare the results of DeepLDA with the CCE based optimization target as well as the present state of the art of the respective datasets. In addition, we provide details on the network architectures, hyper parameters and respective training/optimization approaches used in our experiments.  (a)  (b)  (c)  (d)  (e)  (f)  Figure 2: Example images of evaluation data sets (a)(b) MNIST, (c)(d) CIFAR-10, (e)(f) STL-10. The relative size differences between images from the three data sets are kept in this visualization.  4.1 EXPERIMENTAL SETUP  The general structure of the networks is similar for all of the three datasets and identical for CIFAR- 10 and STL-10. The architecture follows the VGG model with sequences of 3 × 3 convolutions (Simonyan & Zisserman, 2014). Instead of a dense classiﬁcation layer we use global average pool- ing on the feature maps of the last convolution layer (Lin et al., 2013). We picked this architecture as it leads to well-posed problems for covariance estimation: many samples vs. low feature space dimension. We further apply batch normalization (Ioffe & Szegedy, 2015) after each convolutional layer which (1) helped to increase convergence speed and (2) improved the performance of all our models. Batch normalization has a positive effect on both CCE as well as DeepLDA-based op- timization. In Table 1 we outline the structure of our models in detail. All networks are trained using SGD with Nesterov momentum. The initial learning rate is set to 0.1 and the momentum is ﬁxed at 0.9 for all our models. The learning rate is then halved every 25 epochs for CIFAR-10 and STL-10 and every 10 epochs for MNIST. For further regularization we add weight decay with a weighting of 0.0001 on all trainable parameters of the models. The between-class covariance matrix regularization weight λ (see Section 3.3) is set to 0.001 and the (cid:15)-offset for DeepLDA to 1. One hyper-parameter that varies between the datasets is the batch size used for training DeepLDA. Although a large batch size is desired to get stable covariance estimates it is limited by the amount of memory available on the GPU. The mini-batches for DeepLDA were for MNIST: 1000, for CIFAR- 10: 1000 and for STL-10: 200. For CCE training, a batch size of 128 is used for all datasets. The models are trained on an NVIDIA Tesla K40 with 12GB of GPU memory.  5  Published as a conference paper at ICLR 2016  Table 1: Model Speciﬁcations. BN: Batch Normalization, ReLu: Rectiﬁed Linear Activation Func- tion, CCE: Categorical Cross Entropy. The mini-batch sizes of DeepLDA are: MNIST(1000), CIFAR-10(1000), STL-10(200). For CCE training a constant batch size of 128 is used.  CIFAR-10 and STL-10  MNIST  Input 3 × 32 × 32 (96 × 96)  Input 1 × 28 × 28  3 × 3 Conv(pad-1)-64-BN-ReLu 3 × 3 Conv(pad-1)-64-BN-ReLu  2 × 2 Max-Pooling + Drop-Out(0.25)  3 × 3 Conv(pad-1)-96-BN-ReLu 3 × 3 Conv(pad-1)-96-BN-ReLu  2 × 2 Max-Pooling + Drop-Out(0.25)  3 × 3 Conv(pad-1)-128-BN-ReLu 3 × 3 Conv(pad-1)-128-BN-ReLu 2 × 2 Max-Pooling + Drop-Out(0.25) 3 × 3 Conv(pad-1)-256-BN-ReLu 3 × 3 Conv(pad-1)-256-BN-ReLu 3 × 3 Conv(pad-1)-256-BN-ReLu 3 × 3 Conv(pad-1)-256-BN-ReLu 2 × 2 Max-Pooling + Drop-Out(0.25) 3 × 3 Conv(pad-0)-1024-BN-ReLu 1 × 1 Conv(pad-0)-1024-BN-ReLu 1 × 1 Conv(pad-0)-10-BN-ReLu  Drop-Out(0.5)  Drop-Out(0.5)  2 × 2 (10 × 10) Global-Average-Pooling  Drop-Out(0.5)  3 × 3 Conv(pad-0)-256-BN-ReLu 1 × 1 Conv(pad-0)-256-BN-ReLu 1 × 1 Conv(pad-0)-10-BN-ReLu 5 × 5 Global-Average-Pooling  Drop-Out(0.5)  Soft-Max with CCE or LDA-Layer  4.2 EXPERIMENTAL RESULTS  We describe the benchmark datasets as well as the pre-processing and data augmentation used for training. We present our results and relate them to the present state of the art for the respective dataset. As DeepLDA is supposed to produce a linearly separable feature space, we also report the results of a linear Support Vector Machine trained on the latent space of DeepLDA (tagged with LinSVM). The results of our network architecture trained with CCE are marked as OurNetCCE. To provide a complete picture of our experimental evaluation we also show classiﬁcation results of an LDA on the topmost hidden representation of the networks trained with CCE (tagged with OurNetCCE(LDA)).  4.2.1 MNIST The MNIST dataset consists of 28 × 28 gray scale images of handwritten digits ranging from 0 to 9. The dataset is structured into 50000 train samples, 10000 validation samples and 10000 test samples. For training we did not apply any pre-processing nor data augmentation. We present results for two different scenarios. In scenario MNIST-50k we train on the 50000 train samples and use the validation set to pick the parametrization which produces the best results on the validation set. In scenario MNIST-60k we train the model for the same number of epochs as in MNIST-50k but also use the validation set for training. Finally we report the accuracy of the model on the test set after the last training epoch. This approach was also applied in (Lin et al., 2013) which produce state of the art results on the dataset. Table 2 summarizes all results on the MNIST dataset. DeepLDA produces competitive results – having a test set error of 0.29% – although no data augmentation is used. In the approach described in (Graham, 2014) the train set is extended with translations of up to two pixels. We also observe that a linear SVM trained on the learned representation produces comparable results on the test set. It is also interesting that early stopping with best-model-selection (MNIST-50k) performs better than training on MNIST-60k even though 10000 more training examples are available.  6  Published as a conference paper at ICLR 2016  Table 2: Comparison of test errors on MNIST  Method  NIN + Dropout (Lin et al. (2013)) Maxout (Goodfellow et al. (2013)) DeepCNet(5,60) (Graham (2014)) OurNetCCE(LDA)-50k OurNetCCE-50k OurNetCCE-60k DeepLDA-60k OurNetCCE(LDA)-60k DeepLDA-50k DeepLDA-50k(LinSVM)  Test Error 0.47% 0.45% 0.31% (train set translation) 0.39% 0.37% 0.34% 0.32% 0.30% 0.29% 0.29%  Table 3: Comparison of test errors on CIFAR-10  Method  NIN + Dropout (Lin et al. (2013)) Maxout (Graham (2014)) NIN + Dropout (Lin et al. (2013)) DeepCNINet(5,300) (Graham (2014)) DeepLDA(LinSVM) DeepLDA OurNetCCE(LDA) OurNetCCE  Test Error 10.41% 9.38% 8.81% (data augmentation) 6.28% (data augmentation) 7.58% 7.29% 7.19% 7.10%  4.2.2 CIFAR-10 The CIFAR-10 dataset consists of tiny 32 × 32 natural RGB images containing samples of 10 dif- ferent classes. The dataset is structured into 50000 train samples and 10000 test samples. We pre-processed the dataset using global contrast normalization and ZCA whitening as proposed by Goodfellow et al. (2013). During training we only apply random left-right ﬂips on the images – no additional data augmentation is used. In training, we follow the same procedure as described for the MNIST dataset above to make use of the entire 50000 train images. Table 3 summarizes our results and relates them to the present state of the art. Both OurNetCCE and DeepLDA produce state of the art results on the dataset when no data augmentation is used. Although DeepLDA performs slightly worse than CCE it is capable of producing competitive results on CIFAR-10.  4.2.3 STL-10  Like CIFAR-10, the STL-10 data set contains natural RGB images of 10 different object categories. However, with 96 × 96 pixels the size of the images is larger, and the training set is considerably smaller, with only 5000 images. The test set consists of 8000 images. In addition, STL-10 contains 100000 unlabelled images but we do not make use of this additional data at this point as our approach is fully supervised. For that reason we ﬁrst perform an experiment (Method-4k) where we do not follow the evaluation strategy described in (Coates et al., 2011), where models are trained on 1000 labeled and 100000 unlabeled images. Instead, we directly compare CCE and DeepLDA in a fully supervised setting. As with MNIST-50k we train our models on 4000 of the train images and use the rest (1000 images) as a validation set to pick the best performing parametrization. The results on the Method-4k-Setting of STL-10 are presented in the top part of Table 4. Our model trained with CCE achieves an accuracy of 78.39%. The same architecture trained with DeepLDA improves the test set accuracy by more than 3 percentage points and achieves 81.46%. In our second experiment (Method-1k) we follow the evaluation strategy described in (Coates et al., 2011) but without using the unlabelled data. We train our models on the 10 pre-deﬁned folds (each fold contains 1000 train images) and report the average accuracy on the test set. The model optimized with CCE  7  Published as a conference paper at ICLR 2016  Table 4: Comparison of test set accuracy on a purely supervised setting of STL-10. (Method-4k: 4000 train images, Method-1k: 1000 train images.)  Method-4k  OurNetCCE(LDA)-4k OurNetCCE-4k DeepLDA-4k DeepLDA(LinSVM)-4k  Test Accuracy-4k 78.50% 78.84% 81.16% 81.40%  Method-1k  SWWAE (Zhao et al. (2015)) SWWAE (Zhao et al. (2015)) DeepLDA(LinSVM)-1k OurNetCCE-1k OurNetCCE(LDA)-1k DeepLDA-1k  Test Accuracy-1k 57.45% 74.33% (semi-supervised) 55.92% 57.44% 59.48% 66.97%  (OurNetCCE-1k) achieves 57.44% accuracy on the test set which is in line with the supervised results reported in (Zhao et al., 2015). Our model trained with DeepLDA achieves 66.97% average test set accuracy. This is a performance gain of 9.53% in contrast to CCE and it shows that the advantage of DeepLDA compared to CCE be- comes even more apparent when the amount of labeled data is low. When comparing DeepLDA-1k with LDA applied on the features computed by a network trained with CCE (OurNetCCE(LDA)- 1k, 59.48%), we ﬁnd that the end-to-end trained LDA-features outperform the standard CCE ap- proach. A direct comparison with state of the art results as reported in (Zhao et al., 2015; Swersky et al., 2013; Dosovitskiy et al., 2014) is not possible because these models are trained under semi- supervised conditions using both unlabelled and labelled data. However, the results suggest that a combination of DeepLDA with methods such as proposed by Zhao et al. (2015) is a very promising future direction.  5  INVESTIGATONS ON DEEPLDA AND DISCUSSIONS  In this section we provide deeper insights into the representations learned by DeepLDA. We exper- imentally investigate the eigenvalue structure of representations learned by DeepLDA as well as its relation to the classiﬁcation potential of the respective networks.  5.1 DOES IMAGE SIZE AFFECT DEEPLDA?  DeepLDA shows its best performance on the STL-10 dataset (Method-4k) where it outperforms CCE by 3 percentage points. The major difference between STL-10 and CIFAR-10 – apart from the number of train images – is the size of the contained images (see Figure 2 to get an impression of the size relations). To get a deeper insight into the inﬂuence of this parameter we run the following additional experiment: (1) we create a downscaled version of the STL-10 dataset with the same image dimensions as CIFAR-10 (32 × 32). (2) We repeat the experiment (Method-4k) described in Section 4.2.3 on the downscaled 32 × 32 dataset. The results are presented in Figure 3, as curves showing the evolution of train and validation accuracy during training. As expected, downscaling reduces the performance of both CCE and DeepLDA. We further observe that DeepLDA performs best when trained on larger images and has a disadvantage on the small images. However, a closer look at the results on CIFAR-10 (CCE: 7.10% error, DeepLDA: 7.29% error, see Table 3) suggests that this effect is compensated when the training set size is sufﬁciently large. As a reminder: CIFAR- 10 contains 50000 train images in contrast to STL-10 with only 4000 samples.  5.2 EIGENVALUE STRUCTURE OF DEEPLDA REPRESENTATIONS  DeepLDA optimization does not focus on maximizing the target class likelihood of individual sam- ples. As proposed in Section 3 we encourage the net to learn feature representations with discrimi-  8  Published as a conference paper at ICLR 2016  (a) STL-10 (96 × 96)  (b) STL-10 (32 × 32)  Figure 3: Comparison of the learning curves of DeepLDA on the original STL-10 dataset (Method- 4k) with image size 96 × 96 and its downscaled 32 × 32 version.  native distribution parameters (within and between class scatter). We achieve this by exploiting the eigenvalue structure of the general LDA eigenvalue problem and use it as a deep learning objective. Figure 4a shows the evolution of train and test set accuracy of STL-10 along with the mean value of all eigenvalues in the respective training epoch. We observe the expected natural correlation between the magnitude of explained ”discriminative” variance (separation) and the classiﬁcation potential of the resulting representation. In Figure 4b we show how the individual eigenvalues increase during training. Note that in Epoch 0 almost all eigenvalues (1-7) start at a value of 0. This emphasizes the importance of the design of our objective function (compare Equation (9)) which allows to draw discriminability into the lower dimensions of the eigen-space. In Figure 4c we additionally compare the eigenvalue structure of the latent representation produced by DeepLDA with CCE based training. Again results show that DeepLDA helps to distribute the discriminative variance more equally over the available dimensions. To give the reader an additional intuition on the learned representations we visualize the latent space of STL-10 in our supplemental materials on the ﬁnal page of this paper.  (a) Eigenvalues vs. Accuracy  (b) Individual Eigenvalues  (c) CCE vs. DeepLDA  Figure 4: The ﬁgure investigates the eigenvalue structure of the general LDA eigenvalue problem during training a DeepLDA network on STL-10 (Method-4k). (a) shows the evolution of classiﬁ- cation accuracy along with the magnitude of explained discriminative variance (separation) in the latent representation of the network. (b) shows the evolution of individual eigenvalues during train- ing. In (c) we compare the eigenvalue structure of a net trained with CCE and DeepLDA (for better comparability we normalized the maximum eigenvalue to one).  6 CONCLUSION  We have presented DeepLDA, a deep neural network interpretation of linear discriminant analysis. DeepLDA learns linearly separable latent representations in an end-to-end fashion by maximizing the eigenvalues of the general LDA eigenvalue problem. Our modiﬁed version of the LDA optimiza- tion target pushes the network to distribute discriminative variance in all dimensions of the latent  9  0100200300400500Epoch30405060708090100Accuracystl10_dlda_trstl10_dlda_vastl10_cce_trstl10_cce_va0100200300400500Epoch30405060708090100Accuracystl10_down32_dlda_trstl10_down32_dlda_vastl10_down32_cce_trstl10_down32_cce_va05010015020025030405060708090100Accuracytraintest050100150200250Epoch01020304050mean(eigvals)0246810Eigenvalue−100102030405060Explained varianceEpoch 0Epoch 100Epoch 200Epoch 2500246810Eigenvalue0.00.20.40.60.81.0Explained Discriminative VarianceCCEDeepLDAPublished as a conference paper at ICLR 2016  feature space. Experimental results show that representations learned with DeepLDA are discrimina- tive and have a positive effect on classiﬁcation accuracy. Our DeepLDA models achieve competitive results on MNIST and CIFAR-10 and outperform CCE in a fully supervised setting of STL-10 by more than 9% test set accuracy. The results and further investigations suggest that DeepLDA per- forms best, when applied to reasonably-sized images (in the present case 96 × 96 pixel). Finally, we see DeepLDA as a speciﬁc instance of a general fruitful strategy: exploit well-understood ma- chine learning or classiﬁcation models such as LDA with certain desirable properties, and use deep networks to learn representations that provide optimal conditions for these models.  ACKNOWLEDGMENTS  We would like to thank Sepp Hochreiter for helpful discussions, and the three anonymous reviewers for extremely helpful (and partly challenging) remarks. We would also like to thank all developers of Theano (Bergstra et al., 2010) and Lasagne (Dieleman et al., 2015) for providing such great deep learning frameworks. The research reported in this paper has been supported by the Austrian Ministry for Transport, Innovation and Technology, the Federal Ministry of Science, Research and Economy, and the Province of Upper Austria in the frame of the COMET center SCCH. The Tesla K40 used for this research was donated by the NVIDIA Corporation.  REFERENCES Andrew, Galen, Arora, Raman, Bilmes, Jeff, and Livescu, Karen. Deep canonical correlation anal- ysis. In Proceedings of the 30th International Conference on Machine Learning, pp. 1247–1255, 2013.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Chan, Tsung-Han, Jia, Kui, Gao, Shenghua, Lu, Jiwen, Zeng, Zinan, and Ma, Yi. Pcanet: A simple deep learning baseline for image classiﬁcation? IEEE Transactions on Image Processing, 24(12): 5017–5032, 2015. doi: 10.1109/TIP.2015.2475625. URL http://dx.doi.org/10.1109/ TIP.2015.2475625.  Clevert, Djork-Arn´e, Unterthiner, Thomas, Mayr, Andreas, Ramsauer, Hubert, and Hochreiter, Sepp.  Rectiﬁed factor networks. In Advances in neural information processing systems, 2015.  Coates, Adam, Ng, Andrew Y, and Lee, Honglak. An analysis of single-layer networks in unsu- pervised feature learning. In International conference on artiﬁcial intelligence and statistics, pp. 215–223, 2011.  de Leeuw, Jan. Derivatives of generalized eigen systems with applications. 2007.  Dieleman, Sander, Schlueter, Jan, Raffel, Colin, Olson, Eben, Snderby, Sren Kaae, Nouri, Daniel, Maturana, Daniel, Thoma, Martin, Battenberg, Eric, Kelly, Jack, Fauw, Jeffrey De, Heilman, Michael, diogo149, McFee, Brian, Weideman, Hendrik, takacsg84, peterderivaz, Jon, instagibbs, Rasul, Dr. Kashif, CongLiu, Britefury, and Degrave, Jonas. Lasagne: First release., August 2015. URL http://dx.doi.org/10.5281/zenodo.27878.  Dosovitskiy, Alexey, Springenberg, Jost Tobias, Riedmiller, Martin, and Brox, Thomas. Discrimi- native unsupervised feature learning with convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 766–774, 2014.  Fisher, Ronald A. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7  (2):179–188, 1936.  Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. The elements of statistical learning,  volume 1. Springer series in statistics Springer, Berlin, 2001.  Friedman, Jerome H. Regularized discriminant analysis. Journal of the American statistical associ-  ation, 84(405):165–175, 1989.  10  Published as a conference paper at ICLR 2016  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.  Maxout networks. arXiv preprint arXiv:1302.4389, 2013.  Graham, Benjamin. Spatially-sparse convolutional neural networks. CoRR, abs/1409.6070, 2014.  URL http://arxiv.org/abs/1409.6070.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/ abs/1502.03167.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. CoRR, abs/1312.4400, 2013.  URL http://arxiv.org/abs/1312.4400.  Lu, Juwei, Plataniotis, Konstantinos N, and Venetsanopoulos, Anastasios N. Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition. Pattern Recognition Letters, 26(2):181–191, 2005.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. arXiv preprint arXiv:1409.1556, 2014.  Stuhlsatz, Andre, Lippel, Jens, and Zielke, Thomas. Feature extraction with deep neural networks IEEE Transactions on Neural Networks and Learning  by a generalized discriminant analysis. Systems, 23(4):596–608, 2012.  Swersky, Kevin, Snoek, Jasper, and Adams, Ryan P. Multi-task bayesian optimization. In Advances  in Neural Information Processing Systems, pp. 2004–2012, 2013.  Wang, Weiran, Arora, Raman, Livescu, Karen, and Bilmes, Jeff. On deep multi-view representation  learning. In ICML, 2015a.  Wang, Weiran, Arora, Raman, Livescu, Karen, and Bilmes, Jeff A. Unsupervised learning of acous-  tic features via deep canonical correlation analysis. In Proceedings of ICASSP, 2015b.  Zhao, Junbo, Mathieu, Michael, Goroshin, Ross, and Lecun, Yann. Stacked what-where auto-  encoders. arXiv preprint arXiv:1506.02351, 2015.  11  Published as a conference paper at ICLR 2016  APPENDIX A: GRADIENT OF DEEPLDA-LOSS  To train with back-propagation we provide the partial derivatives of optimization target l(H) pro- posed in Equation (9) with respect to the topmost hidden representation H (contains samples as rows and features as columns). As a reminder, the DeepLDA objective focuses on maximizing the k smallest eigenvalues vi of the generalized LDA eigenvalue problem. In particular, we consider only the k eigenvalues that do not exceed a certain threshold for optimization:  vi with {v1, ..., vk} = {vj|vj < min{v1, ..., vC−1} + (cid:15)}  (11)  k(cid:88)  i=1  l(H) =  1 k  For convenience, we change the subscripts of the scatter matrices to superscripts in this section (e.g. St → St). St ij addresses the element in row i and column j in matrix St. Starting from the formulation of the generalized LDA eigenvalue problem:  Sbei = viSwei  (12)  the derivative of eigenvalue vi with respect to hidden representation H is deﬁned in (de Leeuw, 2007) as:  (cid:18) ∂Sb  ∂H  ∂vi ∂H  = eT i  − vi  ∂Sw ∂H  ei  (cid:19)  Recalling the deﬁnitions of the LDA scatter matrices from Section 3.1:  (cid:88)  c  Sc  Sc =  1  Nc − 1  ¯XT c  ¯Xc  Sw =  1 C  St =  1  N − 1  ¯XT ¯X  Sb = St − Sw  (13)  (14)  (15)  we can write the partial derivative of the total scatter matrix St (Andrew et al., 2013; Stuhlsatz et al., 2012) on hidden representation H as:    2  1  N−1 N−1 N−1 0  1  (cid:0)Hij − 1 (cid:0)Hib − 1 (cid:0)Hia − 1  N  N  N  (cid:80) (cid:80) (cid:80)  n Hnj n Hnb n Hna  (cid:1) (cid:1) (cid:1)  ∂St ab ∂Hij  =  if a = j, b = j if a = j, b (cid:54)= j if a (cid:54)= j, b = j if a (cid:54)= j, b (cid:54)= j  (16)  The derivatives for the individual class covariance matrices Sc are deﬁned analogously to Equation (16) for the C classes and we can write the partial derivatives of Sw and Sb with respect to the latent representation H as:  ∂Sw ab ∂Hij  =  1 C  ∂Sc ab ∂Hij  and  ∂Sb ab ∂Hij  =  ∂St ab ∂Hij  − ∂Sw ab ∂Hij  (17)  (cid:88)  c  The partial derivative of the loss function introduced in Section 3.3 with respect to hidden state H is then deﬁned as:  k(cid:88)  i=1  ∂ ∂H  1 k  vi =  1 k  k(cid:88)  i=1  k(cid:88)  i=1  eT i  (cid:18) ∂Sb  ∂H  (cid:19)  − vi  ∂Sw ∂H  ei  (18)  ∂vi ∂H  =  1 k  12  Published as a conference paper at ICLR 2016  APPENDIX B: DEEPLDA LATENT REPRESENTATION  Figure 5 shows the latent space representations on the STL-10 dataset (Method-4k) as n-to-n scatter plots of the latent features on the ﬁrst 1000 test set samples. We plot the test set samples after projection into the C − 1 dimensional DeepLDA feature space. The plot suggest that DeepLDA makes use of all available feature dimensions. An interesting observation is that many of the internal representations are orthogonal to each other (which is an implication of LDA). This of course favours linear decision boundaries.  Figure 5: STL-10 latent representation produced by DeepLDA (n-to-n scatter plots of the latent features of the ﬁrst 1000 test set samples. e.g.: top left plot: latent feature 1 vs. latent feature 2).  13  ",
1511.04773,2016,Large-Scale Approximate Kernel Canonical Correlation Analysis,"['Large-Scale Approximate Kernel Canonical Correlation Analysis\nWeiran Wang', 'Karen Livescu']",https://arxiv.org/pdf/1511.04773,"Published as a conference paper at ICLR 2016  LARGE-SCALE APPROXIMATE KERNEL CANONICAL CORRELATION ANALYSIS  Weiran Wang & Karen Livescu Toyota Technological Institute at Chicago 6045 S. Kenwood Ave., Chicago, IL 60637 Email: {weiranwang,klivescu}@ttic.edu  ABSTRACT  Kernel canonical correlation analysis (KCCA) is a nonlinear multi-view repre- sentation learning technique with broad applicability in statistics and machine learning. Although there is a closed-form solution for the KCCA objective, it involves solving an N × N eigenvalue system where N is the training set size, making its computational requirements in both memory and time prohibitive for large-scale problems. Various approximation techniques have been developed for KCCA. A commonly used approach is to ﬁrst transform the original inputs to an M -dimensional random feature space so that inner products in the feature space approximate kernel evaluations, and then apply linear CCA to the transformed inputs. In many applications, however, the dimensionality M of the random feature space may need to be very large in order to obtain a sufﬁciently good approximation; it then becomes challenging to perform the linear CCA step on the resulting very high-dimensional data matrices. We show how to use a sto- chastic optimization algorithm, recently proposed for linear CCA and its neural- network extension, to further alleviate the computation requirements of approx- imate KCCA. This approach allows us to run approximate KCCA on a speech dataset with 1.4 million training samples and a random feature space of dimen- sionality M = 100000 on a typical workstation.  1  INTRODUCTION  Canonical correlation analysis (CCA, Hotelling, 1936) and its extensions are ubiquitous techniques in scientiﬁc research areas for revealing the common sources of variability in multiple views of the same phenomenon, including meteorology (Anderson, 2003), chemometrics (Montanarella et al., 1995), genomics (Witten et al., 2009), computer vision (Kim et al., 2007; Socher & Li, 2010), speech recognition (Rudzicz, 2010; Arora & Livescu, 2013; Wang et al., 2015a), and natu- ral language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011; Hodosh et al., 2013; Faruqui & Dyer, 2014; Lu et al., 2015a). CCA seeks linear projections of two random vectors (views), such that the resulting low-dimensional vectors are maximally correlated. Given a dataset of N pairs of observations (x1, y1), . . . , (xN , yN ) of the random variables, where xi ∈ Rdx and yi ∈ Rdy for i = 1, . . . , N , the objective of CCA for L-dimensional projections can be written as1 (see, e.g., Borga, 2001)  6 1 0 2     b e F 9 2         ]  G L . s c [      4 v 3 7 7 4 0  .  1 1 5 1 : v i X r a  max  U∈Rdx×L,V∈Rdy ×L s.t. U⊤ΣxxU = V⊤ΣyyV = I,  tr(cid:0)U⊤ΣxyV(cid:1)  u⊤ i Σxyvj = 0, for i 6= j, N PN  where (U, V) are the projection matrices for each view, Σxy = 1 i , Σxx = 1 i + ryI are the cross- and auto-covariance matrices, and (rx, ry) ≥ 0 are regularization parameters (Vinod, 1976; Bie & Moor, 2003). There exists a  i + rxI, Σyy = 1  i=1 yiy⊤  i=1 xix⊤  i=1 xiy⊤  N PN  N PN  1In this paper, we assume that the inputs are centered at the origin for notational simplicity; if they are not,  we can center them as a pre-processing operation.  1  (1)  Published as a conference paper at ICLR 2016  2  − 1 xx ΣxyΣ  closed-form solution to (1) as follows. Let the rank-L singular value decomposition (SVD) of the − 1 yy ∈ Rdx×dy be ˜UΛ ˜V⊤, where Λ contains the top whitened covariance matrix T = Σ L singular values σ1 ≥ · · · ≥ σL on its diagonal, and ( ˜U, ˜V) are the corresponding singular vec- − 1 yy ˜V), and the optimal tors. Then the optimal projection matrices (U, V) in (1) are (Σ l=1 σl.2 The theoretical properties of CCA (Kakade & Foster, 2007; Chaudhuri et al., 2009; Foster et al., 2009) and its connection to other methods (Borga, 2001; Bach & Jordan, 2005; Chechik et al., 2005) have also been studied.  objective value, referred to as the canonical correlation, is PL  − 1 xx ˜U, Σ  2  2  2  One limitation of CCA is its restriction to linear mappings, which are often insufﬁcient to reveal the highly nonlinear relationships in many real-world applications. To overcome this issue, ker- nel CCA (KCCA) was proposed indepedently by several researchers (Lai & Fyfe, 2000; Akaho, 2001; Melzer et al., 2001) and has become a common technique in statistics and machine learn- ing (Bach & Jordan, 2002; Hardoon et al., 2004). KCCA extends CCA by mapping the original inputs in both views into reproducing kernel Hilbert spaces (RKHS) and solving linear CCA in the RKHS. By the representer theorem of RKHS (Sch¨olkopf & Smola, 2001), one can conveniently work with the kernel functions instead of the high-dimensional (possibly inﬁnite-dimensional) RKHS, and the projection mapping is a linear combination of kernel functions evaluated at the training samples. KCCA has been successfully used for cross-modality retrieval (Hardoon et al., 2004; Li & Shawe-Taylor, 2005; Socher & Li, 2010; Hodosh et al., 2013), acoustic feature learn- ing (Arora & Livescu, 2013), computational biology (Yamanishi et al., 2004; Hardoon et al., 2007; Blaschkoa et al., 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al., 2007; Lopez-Paz et al., 2013).  KCCA also has a closed-form solution, via an N × N eigenvalue system (see Sec. 2). However, this solution does not scale up to datasets of more than a few thousand training samples, due to the time complexity of solving the eigenvalue system (O(N 3) for a naive solution) and the memory cost of storing the kernel matrices. As a result, various approximation techniques have been devel- oped, most of which are based on low-rank approximations of the kernel matrices. With rank-M approximations of the kernel matrices, the cost of solving approximate KCCA reduces to O(M 2N ) (see, e.g., Bach & Jordan, 2002; Lopez-Paz et al., 2014). Thus if M ≪ N , the approximation leads to signiﬁcant computational savings. Typically, ranks of a few hundred to a few thousand are used for the low-rank kernel approximations (Yang et al., 2012; Le et al., 2013; Lopez-Paz et al., 2014). In more challenging real-world applications, however, it is observed that the rank M needed for an approximate kernel method to work well can be quite large, on the order of tens or hundreds of thousands (see Huang et al., 2014; Lu et al., 2015b for classiﬁcation tasks, and Wang et al., 2015a for KCCA). In such scenarios, it then becomes challenging to solve even approximate KCCA.  In this paper, we focus on the computational challenges of scaling up approximate kernel CCA using low-rank kernel approximations when both the training set size N and the approxima- tion rank M are large. The particular variant of approximate KCCA we use, called randomized CCA (Lopez-Paz et al., 2014), transforms the original inputs to an M -dimensional feature space us- ing random features (Rahimi & Recht, 2008; 2009) so that inner products in the new feature space approximate the kernel function. This approach thus turns the original KCCA problem into a very high-dimensional linear CCA problem of the form (1). We then make use of a stochastic opti- mization algorithm, recently proposed for linear CCA and its deep neural network extension deep CCA (Ma et al., 2015; Wang et al., 2015c), to reduce the memory requirement for solving the result- ing linear CCA problem. This algorithm updates parameters iteratively based on small minibatches of training samples. This approach allows us to run approximate KCCA on an 8−million sample dataset of MNIST digits, and on a speech dataset with 1.4 million training samples and rank (dimen- sionality of random feature space) M = 100000 on a normal workstation. Using this approach we achieve encouraging results for multi-view learning of acoustic transformations for speech recogni- tion.3 In the following sections we review approximate KCCA and random features (Sec. 2), present the stochastic optimization algorithm (Sec. 3), discuss related work (Sec. 4), and demonstrate our algo- rithm on two tasks (Sec. 5).  2Alternatively, one could also solve some equivalent M × M eigenvalue system instead of the SVD of T,  at a similar cost.  3Our MATLAB implementation is available at http://ttic.uchicago.edu/˜wwang5/knoi.html  2  Published as a conference paper at ICLR 2016  2 APPROXIMATE KCCA  2.1 KCCA SOLUTION  i=1 φx(xi)α⊤  i=1 φy(yi)β⊤  i=1 of view 1 and {yi}N  i and V = PN  i=1 βiky(y, yi) ∈ RL for view 1 and view 2 respectively.  In KCCA, we transform the inputs {xi}N i=1 of view 2 using feature mappings φx and φy associated with some positive semi-deﬁnite kernels kx and ky respectively, and then solve the linear CCA problem (1) for the feature-mapped inputs (Lai & Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach & Jordan, 2002; Hardoon et al., 2004). The key property of such kernels is that kx(x, x′) =< φx(x), φx(x′) > (similarly for view 2) Sch¨olkopf & Smola, 2001. Even though the feature-mapped inputs live in possibly inﬁnite-dimensional RKHS, replacing the original inputs (xi, yi) with (φx(xi), φy(yi)) in (1), and using the KKT theorem (Nocedal & Wright, 2006), one i where αi, βi ∈ RL, i = 1, . . . , N , as a result of the representer theorem (Sch¨olkopf & Smola, 2001). i=1 αikx(x, xi) ∈ RL and  can show that the solution has the form U = PN The ﬁnal KCCA projections can therefore be written as f (x) = PN g(y) = PN  Denote by Kx the N × N kernel matrix for view 1, i.e., (Kx)ij = kx(xi, xj), and similarly denote by Ky the kernel matrix for view 2. Then (1) can be written as a problem in the coefﬁcient matrices A = [α1, . . . , αN ]⊤ ∈ RN ×L and B = [β1, . . . , βN ]⊤ ∈ RN ×L. One can show that the optimal coefﬁcients A correspond to the top L eigenvectors of the N × N matrix (Kx + N rxI)−1Ky(Ky + N ryI)−1Kx, and a similar result holds for B (see, e.g., Hardoon et al., 2004). This involves solving an eiaylorgenvalue problem of size N × N , which is expensive both in memory (storing the kernel matrices) and in time (solving the N × N eigenvalue systems naively costs O(N 3)). Various kernel approximation techniques have been proposed to scale up KCCA, including Cholesky decomposition (Bach & Jordan, 2002), partial Gram-Schmidt (Hardoon et al., 2004), and incremen- tal SVD (Arora & Livescu, 2012). Another widely used approximation technique for kernel matrices is the Nystr¨om method (Williams & Seeger, 2001). In the Nystr¨om method, we select M (random or otherwise) training samples ˜x1, . . . , ˜xM and construct the M × M kernel matrix ˜Kx based on these samples, i.e. ( ˜Kx)ij = kx(˜xi, ˜xj). We compute the eigenvalue decomposition ˜Kx = ˜R ˜Λ ˜R⊤, and then the N × N kernel matrix for the entire training set can be approximated as Kx ≈ C ˜K−1 x C⊤ where C contains the columns of Kx corresponding to the selected subset, i.e., Cij = kx(xi, ˜xj). This means Kx ≈ (C ˜R ˜Λ 2 )⊤ as the new feature representation for view 1 (similarly for view 2), where inner products between samples approximate kernel similarities. We can extract such features for both views, and apply linear CCA to them to approximate the KCCA solution (Yang et al., 2012; Lopez-Paz et al., 2014). Notice that using the Nystr¨om method has a time complexity (for view 1) of O(M 2dx +M 3 +N M dx +M 2N ), where the four terms account for the costs of forming ˜Kx ∈ RM×M , computing the eigenvalue de- composition of ˜Kx, forming C, and computing C ˜R ˜Λ 2 , respectively, and a space complexity of O(M 2) for saving the eigenvalue systems of ˜Kx and ˜Ky, which are expensive for large M . Al- though there have been various sampling/approximation strategies for the Nystr¨om method (Li et al., 2010; Zhang & Kwok, 2009; 2010; Kumar et al., 2012; Gittens & Mahoney, 2013), their construc- tions are more involved.  2 )⊤, so we can use the M × N matrix (C ˜R ˜Λ  2 )(C ˜R ˜Λ  − 1  − 1  − 1  − 1  2.2 APPROXIMATION VIA RANDOM FEATURES  We now describe another approximate KCCA formulation that is particularly well-suited to large- scale problems.  It is known from harmonic analysis that a shift-invariant kernel of the form k(x, x′) = κ(x − x′) is a positive deﬁnite kernel if and only if κ(∆) is the Fourier transform of a non-negative measure (this is known as Bochner’s theorem; see Rudin, 1994; Rahimi & Recht, 2008). Thus we can write the kernel function as an expectation over sinusoidal functions over the underlying probability measures and approximate it with sample averages. Taking as a concrete example the Gaussian radial basis function (RBF) kernel k(x, x′) = e−kx−x′k2 where s is the kernel width, it can be approximated  /2s2  3  Published as a conference paper at ICLR 2016  as (Lopez-Paz et al., 2014)  k(x, x′) = Z e−jw⊤(x−x′)p(w) dw ≈  1 M  M  Xi=1  2 cos(w⊤  i x + bi) cos(w⊤  i x′ + bi),  2  where p(w) is the multivariate Gaussian distribution N (0, 1 s2 I), obtained from the inverse Fourier transform of κ(∆) = e− k∆k 2s2 , and bi is drawn from a uniform distribution over [0, 2π]. Approxima- tions for other shift-invariant kernels (Laplacian, Cauchy) can be found in Rahimi & Recht (2008). This approach has been extended to other types of kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015). A careful quasi-Monte Carlo scheme for sampling from p(w) (Yang et al., 2014), and structured feature transformation for accelerating the computation of w⊤ i x (Le et al., 2013), have also been studied.  Leveraging this result, Rahimi & Recht (2009) propose to ﬁrst extract M -dimensional random Fourier features for input x as (with slight abuse of notation)  φ(x) = r 2  M (cid:2)cos(w⊤  1 x + b1), . . . , cos(w⊤  M x + bM )(cid:3) ∈ RM ,  so that φ(x)⊤φ(x′) ≈ k(x, x′), and then apply linear methods on these features. The computational advantage of this approach is that it turns nonlinear learning problems into convex linear learning problems, for which empirical risk minimization is much more efﬁcient (e.g, Lu et al., 2015b used the recently proposed stochastic gradient method by Roux et al. 2012 for the task of multinomial logistic regression with random Fourier features). Rahimi & Recht (2009) showed that it allows us to effectively learn nonlinear models and still obtain good learning guarantees.  Lopez-Paz et al. (2014) have recently applied the random feature idea to KCCA, by extracting M - dimensional random Fourier features {(φx(xi), φy(yi))}N i=1 for both views and solving exactly a linear CCA on the transformed pairs. They also provide an approximation guarantee for this ap- proach (see Theorem 4 of Lopez-Paz et al., 2014). Comparing random features with the Nystr¨om method described previously, when both techniques use rank-M approximations, the cost of com- puting the solution to (1) is the same and of order O(M 2N ). But using random features, we generate the M -dimensional features in a data-independent fashion with a minimal cost O(N M dx) (for view 1), which is negligible compared to that of the Nystr¨om method. Furthermore, random features do not require saving any kernel matrix and the random features can be generated on the ﬂy by sav- ing the random seeds. Although the Nystr¨om approximation can be more accurate at the same rank (Yang et al., 2012), the computational efﬁciency and smaller memory cost of random features make them more appealing for large-scale problems in practice.  3 STOCHASTIC OPTIMIZATION OF APPROXIMATE KCCA  When the dimensionality M of the random Fourier features is very large, solving the resulting lin- − 1 ear CCA problem is still very costly as one needs to save the M × M matrix ˜T = ˜Σ ˜Σxy ˜Σ 2 yy and compute its SVD, where the covariance matrices are now computed on {(φx(xi), φy(yi))}N i=1 instead of {(xi, yi)}N i=1. It is thus desirable to develop memory-efﬁcient stochastic optimization algorithms for CCA, where each update of the projection mappings depends only on a small mini- batch of b examples, thus reducing the memory cost to O(bM ). Notice, however, in contrast to the classiﬁcation or regression objectives that are more commonly used with random Fourier fea- tures (Rahimi & Recht, 2009; Huang et al., 2014; Lu et al., 2015b), the CCA objective (1) can not be written as an unconstrained sum or expectation of losses incurred at each training sample (in fact all training samples are coupled together through the constraints). As a result, stochastic gra- dient descent, which requires unbiased gradient estimates computed from small minibatches, is not directly applicable here.  − 1 2 xx  Fortunately, Ma et al. (2015); Wang et al. (2015c) have developed stochastic optimization algo- rithms, referred to as AppGrad (Augmented Approximate Gradient) and NOI (Nonlinear Or- thogonal Iterations) respectively, for linear CCA and its deep neural network extension deep CCA (Andrew et al., 2013). Their algorithms are essentially equivalent other than the introduc- tion in (Wang et al., 2015c) of a time constant for smoothing the covariance estimates over time.  4  Published as a conference paper at ICLR 2016  Algorithm 1 KNOI: Stochastic optimization for approximate KCCA. Input: Initialization U ∈ RM×L, V ∈ RM×L, time constant ρ, minibatch size b, learning rate η,  momentum µ. ∆U ← 0, ∆V ← 0 Randomly choose a minibatch (Xb0 , Yb0 ) Sxx ← 1 Syy ← 1 for t = 1, 2, . . . , T do  |b0| Pi∈b0 (cid:0)U⊤φx(xi)(cid:1)(cid:0)U⊤φx(xi)(cid:1)⊤, |b0| Pi∈b0 (cid:0)V⊤φy(yi)(cid:1)(cid:0)V⊤φy(yi)(cid:1)⊤  Randomly choose a minibatch (Xbt , Ybt ) of size b Sxx ← ρSxx + (1 − ρ) 1 Syy ← ρSyy + (1 − ρ) 1 Compute the gradient ∂U of the objective  b Pi∈bt (cid:0)U⊤φx(xi)(cid:1)(cid:0)U⊤φx(xi)(cid:1)⊤ b Pi∈bt (cid:0)V⊤φy(yi)(cid:1)(cid:0)V⊤φy(yi)(cid:1)⊤  min  U  1  b Xi∈bt(cid:13)(cid:13)(cid:13)  U⊤φx(xi) − S  2  − 1  yy V⊤φy(yi)(cid:13)(cid:13)(cid:13)  as ∂U ← 1 Compute the gradient ∂V of the objective  φx(xi)(cid:16)U⊤φx(xi) − S  b Pi∈bt  2  − 1  yy V⊤φy(yi)(cid:17)⊤  2  2  min  V  1  b Xi∈bt(cid:13)(cid:13)(cid:13)  V⊤φy(yi) − S  2  − 1  xx U⊤φx(xi)(cid:13)(cid:13)(cid:13)  as ∂V ← 1 ∆U ← µ∆U − η∂U, ∆V ← µ∆V − η∂V U ← U + ∆U, V ← V + ∆V  φy(yi)(cid:16)V⊤φy(yi) − S  b Pi∈bt  2  − 1  xx U⊤φx(xi)(cid:17)⊤  end for  Output: The updated (U, V).  The idea originates from the alternating least squares (ALS) formulation of CCA (Golub & Zha, 1995; Lu & Foster, 2014), which computes the SVD of T using orthogonal iterations (a general- ization of power iterations to multiple eigenvalues/eigenvectors, Golub & van Loan, 1996) on TT⊤ and T⊤T. Due to the special form of TT⊤ and T⊤T, two least squares problems arise in this iter- ative approach (see, e.g., Wang et al., 2015c, Section III. A for more details). With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm. Ma et al. (2015); Wang et al. (2015c) take a step further and replace the exact solutions to the least squares problems with efﬁcient stochastic gradient descent updates. Although unbiased gradient estimates of these subproblems do not lead to unbiased gradient esti- mates of the original CCA objective, local convergence results (that the optimum of CCA is a ﬁxed point of AppGrad, and the AppGrad iterate converges linearly to the optimal solution when started in its neighborhood) have been established for AppGrad (Ma et al., 2015). It has also been observed that the stochastic algorithms converge fast to approximate solutions that are on par with the exact solution or solutions by batch-based optimizers.  We give our stochastic optimization algorithm for approximate KCCA, named KNOI (Kernel Non- linear Orthogonal Iterations), in Algorithm 1. Our algorithm is adapted from the NOI algorithm of Wang et al. (2015c), which allows the use of smaller minibatches (through the time constant ρ) than does the AppGrad algorithm of Ma et al. (2015). In each iteration, KNOI adaptively estimates the covariance of the projections of each view (∈ RL) using a convex combination (with ρ ∈ [0, 1)) of the previous estimate and the estimate based on the current minibatch,4 uses them to whiten the targets of the cross-view least squares regression problems, derives gradients from these problems,5 and ﬁnally updates the projection matrices (U, V) with momentum. Notice that ρ controls how fast we forget the previous estimate; larger ρ may be necessary for the algorithm to work well if the mini-  4In practice we also adaptively estimate the mean of the projections and center each minibatch. 5We also use small weight decay regularization (∼ 10−5) for (U, V) in the least squares problems.  5  Published as a conference paper at ICLR 2016  batch size b is small (e.g., due to memory constraints), in which case the covariance estimates based on the current minibatch are noisier (see discussions in Wang et al., 2015c). Empirically, we ﬁnd that using momentum µ ∈ [0, 1) helps the algorithm to make rapid progress in the objective with a few passes over the training set, as observed by the deep learning community (Sutskever et al., 2013). Although we have speciﬁcally use random Fourier features in Algorithm 1, in principle other low-rank kernel approximations can be used as well.  2  2  − 1 xx and S  In each iteration of KNOI, the main cost comes from evaluating the random Fourier features and the projections for a minibatch, and computing the gradients. Since we usually look for low-dimensional projections (L is small), it costs little memory and time to compute the covariance estimates Sxx − 1 and Syy (of size L × L) and their eigenvalue decompositions (for S yy ). Overall, KNOI has a memory complexity of O(M b) (excluding the O(M L) cost for saving U and V in memory) and a time complexity of O(bM (dx + dy + 4L)) per iteration. The (U, V) we obtain from Algorithm 1 do not satisfy the constraints U⊤ ˜ΣxxU = V⊤ ˜ΣyyV; one can enforce the constraints via another linear CCA in RL on {(U⊤φx(xi), V⊤φy(yi))}N i=1, which does not change the canonical correlation between the projections. To evaluate the projection of a view 1 test sample x, we generate the random Fourier features φx(x) using the same random seed for the training set, and compute U⊤φx(x) for it. Finally, we comment on the choice of hyperparameters in KNOI. Empirically, we ﬁnd that larger b tends to give more rapid progress in the training objective, in which case ρ can be set to small values or to 0 as there is sufﬁcient covariance information in a large minibatch (also shown by Ma et al., 2015; Wang et al., 2015c). Therefore, we recommend using larger b and ρ = 0 if one can afford the memory cost. For large-scale problems with millions of training examples, we set b to be a small portion of the training set (a few thousands) and enjoy the fast convergence of stochastic training algorithms (Bottou & Bousquet, 2008). In our experiments we initialize (U, V) with values sampled from a Gaussian distribution with standard deviation 0.1, and tune the learning rate η and momentum µ on small grids.  4 RELATED WORK  There have been continuous efforts to scale up classical methods such as principal component analy- sis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012).  Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case). While this approach effectively reduces the training set size N , it provides no remedy for the large M scenario we face in approximate KCCA.  In terms of online/stochastic CCA, Yger et al. (2012) propose an adaptive CCA algorithm with efﬁcient online updates based on matrix manifolds deﬁned by the constraints (and they use a similar form of adaptive estimates for the covariance matrices). However, the goal of their algorithm is anomaly detection for streaming data with a varying distribution, rather than to perform CCA for a given dataset. Regarding the stochastic CCA algorithms of Ma et al. (2015); Wang et al. (2015c) we use here, an intuitively similar approach is proposed in the context of alternating conditional expectation (Makur et al., 2015).  Another related approach is that of Xie et al. (2015), who propose the Doubly Stochastic Gradient Descent (DSGD) algorithm for approximate kernel machines (including KCCA) based on random Fourier features. KNOI and DSGD are different in several respects. First, the stochastic update rule of DSGD for (U, V) is derived from the Lagrangian of an eigenvalue formulation of CCA and is different from ours, e.g., DSGD does not have any whitening steps while KNOI does. Second, DSGD gradually increases the number of random Fourier features (or cycles through blocks of random Fourier features) and updates the corresponding portions of (U, V) as it sees more training samples. While this potentially further reduces the memory cost of the algorithm, it is not essential as we could also process the random Fourier features in minibatches (blocks) within KNOI.  6  Published as a conference paper at ICLR 2016  5 EXPERIMENTS  In this section, we demonstrate the KNOI algorithm on two large-scale problems and compare it to several alternatives:  • CCA, solved exactly by SVD. • FKCCA, low-rank approximation of KCCA using random Fourier features, with the CCA  step solved exactly by SVD.  • NKCCA, low-rank approximation of KCCA using the Nystr¨om method, with the CCA step  solved exactly by SVD.  We implement KNOI in MATLAB with GPU support. Since our algorithm mainly involves simple matrix operations, running it on a GPU provides signiﬁcant speedup.  5.1 MNIST 8M  In the ﬁrst set of experiments, we demonstrate the scability and efﬁciency of KNOI on the MNIST8M dataset (Loosli et al., 2007). The dataset consists of 8.1 million 28 × 28 grayscale im- ages of the digits 0-9. We divide each image into the left and right halves and use them as the two views in KCCA, so the input dimensionality is 392 for both views. The dataset is randomly split into training/test sets of size 8M/0.1M. The task is to learn L = 50 dimensional projections using KCCA, and the evaluation criterion is the total canonical correlation achieved on the test set (upper- bounded by 50). The same task is used by Xie et al. (2015), although we use a different training/test split. As in Xie et al. (2015), we ﬁx the kernel widths using the “median” trick6 for all algorithms. We vary the rank M for FKCCA and NKCCA from 256 to 6000. For comparison, we use the same hyperparamters as those of Xie et al. (2015)7: data minibatch size b = 1024, feature minibatch size 2048, total number of random Fourier features M = 20480,8 and a decaying step size schedule. For KNOI, we tune hyperparameters on a rough grid based on total canonical correlation obtained on a random subset of the training set with 0.1M samples, and set the minibatch size b = 2500, time constant ρ = 0, learning rate η = 0.01, and momentum µ = 0.995. We run the iterative algorithms DSGD and KNOI for one pass over the data, so that they see the same number of samples as FKCCA/NKCCA. We run each algorithm 5 times using different random seeds and report the mean results.  The total canonical correlations achieved by each algorithm on the test set, together with the run times measured on a workstation with 6 3.6GHz CPUs and 64G main memory, are reported in Ta- ble 1. As expected, all algorithms improve monotonically as M is increased. FKCCA and NKCCA achieve competitive results with a reasonably large M , with NKCCA consistently outperforming FKCCA at the cost of longer run times. KNOI outperforms the other iterative algorithm DSGD, and overall achieves the highest canonical correlation with a larger M . We show the learning curve of KNOI with M = 40960 (on the test set) in Figure 1. We can see that KNOI achieves steep improve- ment in the objective in the beginning, and already outperforms the exact solutions of FKCCA and NKCCA with M = 4096 after seeing only 1/4 to 1/2 of the training set. We also run KNOI on an NVIDIA Tesla K40 GPU with 12G memory, and report the run times in parentheses in Table 1; the GPU provides a speedup of more than 12 times. For this large dataset, the KNOI algorithm itself requires less memory (less than 12G) than loading the training data in main memory (∼25G).  5.2 X-RAY MICROBEAM SPEECH DATA  In the second set of experiments, we apply approximate KCCA to the task of learning acous- tic features for automatic speech recognition. We use the Wisconsin X-ray microbeam (XRMB)  6Following Xie et al. (2015), kernel widths are estimated from the median of pairwise distances between  4000 randomly selected training samples.  7We thank the authors for providing their MATLAB implementation of DSGD. 8Xie et al. (2015) used a version of random Fourier features with both cos and sin functions, so the number  of learnable parameters (in U and V) of DSGD is twice that of KNOI for the same M.  7  Published as a conference paper at ICLR 2016  Table 1: Total canonical correlation on MNIST 8M test set and the corresponding run times (with GPU run times in parentheses).  Method linear CCA  FKCCA  NKCCA  DSGD  KNOI  . r r o C  .  n o n a C  l a t o T  45  40  35  30  25  20  15  10  M Canon. Corr. Time (minutes)  1024 2048 4096 5000 6000 1024 2048 4096 5000 6000 20480 20480 40960 100000  26.8 33.5 37.6 40.7 41.4 42.1 39.6 42.2 44.1 44.5 44.8 43.4 44.5 45.0 45.3  0.5 9.8 24.4 68.9 79.4 107.5 17.1 44.9 138.8 196.3 272.2 306.9 97.2 (7.6) 194.0 (15.4) 502.4 (39.7)  FKCCA M = 4096  NKCCA M = 4096  KNOI M = 40960  20 60 # training samples (×105)  40  80  Figure 1: Learning curve of KNOI on the MNIST 8M test set. We show total canonical correlation in the 50-dimensional projections vs. the number of training samples (×105) processed by KNOI. The FKCCA and NKCCA values, always obtained using the entire training set, are shown as horizontal lines.  corpus (Westbury, 1994) of simultaneously recorded speech and articulatory measurements from 47 American English speakers. It has previously been shown that multi-view feature learn- ing via CCA/KCCA greatly improves phonetic recognition performance given audio input alone (Arora & Livescu, 2013; Wang et al., 2015a;b).  We follow the setup of Wang et al. (2015a;b) and use the learned features (KCCA projections) for speaker-independent phonetic recognition.9 The two input views are acoustic features (39D features consisting of mel frequency cepstral coefﬁcients (MFCCs) and their ﬁrst and second derivatives) and articulatory features (horizontal/vertical displacement of 8 pellets attached to several parts of the vo- cal tract) concatenated over a 7-frame window around each frame, giving 273D acoustic inputs and 112D articulatory inputs for each view. The XRMB speakers are split into disjoint sets of 35/8/2/2  9Unlike Wang et al. (2015a;b), who used the HTK toolkit (Young et al., 1999), we use the Kaldi speech recognition toolkit (Povey et al., 2011) for feature extraction and recognition with hidden Markov models. Our results therefore don’t match those in Wang et al. (2015a;b) for the same types of features, but the relative merits of different types of features are consistent.  8  Published as a conference paper at ICLR 2016  Table 2: Mean phone error rates (PER) over 6 folds obtained by each algorithm on the XRMB test speakers.  Method Baseline (MFCCs) CCA FKCCA (M = 5000) FKCCA (M = 30000) NKCCA (M = 5000) KNOI (M = 100000) DCCA  Mean PER (%)  37.6 29.4 28.1 26.9 28.0 26.4 25.4  speakers for feature learning/recognizer training/tuning/testing. The 35 speakers for feature learning are ﬁxed; the remaining 12 are used in a 6-fold experiment (recognizer training on 8 speakers, tun- ing on 2 speakers, and testing on the remaining 2 speakers). Each speaker has roughly 50K frames, giving 1.43M training frames for KCCA training. We remove the per-speaker mean and variance of the articulatory measurements for each training speaker. All of the learned feature types are used in a “tandem” speech recognition approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions.  For each fold, we select the hyperparameters based on recognition accuracy on the tuning set. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}, and the kernel widths for each view are tuned by grid search. We initially set M = 5000 for FKCCA/NKCCA, and also test FKCCA at M = 30000 (the largest M at which we could afford to obtain an exact SVD solution on a workstation with 64G main memory) with kernel widths tuned at M = 5000; we could not obtain results for NKCCA with M = 30000 in 48 hours. For KNOI, we set M = 100000 and tune the optimization parameters on a rough grid. The tuned KNOI uses minibatch size b = 2500, time constant ρ = 0, ﬁxed learning rate η = 0.01, and momentum µ = 0.995. For this combination of b and M , we are able to run the algorithm on a Tesla K40 GPU (with 12G memory), and each epoch (one pass over the 1.43M training samples) takes only 7.3 minutes. We run KNOI for 5 epochs and use the resulting acoustic view projection for recognition. We have also tried to run KNOI for 10 epochs and the recognition performance does not change, even though the total canonical correlation keeps improving on both training and tuning sets.  For comparison, we report the performance of a baseline recognizer that uses only the original MFCC features, and the performance of deep CCA (DCCA) as described in Wang et al. (2015b), which uses 3 hidden layers of 1500 ReLU units followed by a linear output layer in the acoustic view, and only a linear output layer in the articulatory view. With this architecture, each epoch of DCCA takes about 8 minutes on a Tesla K40 GPU, on par with KNOI. Note that this DCCA architecture was tuned carefully for low PER rather than high canonical correlation. This architecture produces a total correlation of about 25 (out of a maximum of L = 70) on tuning data, while KNOI achieves 46.7. DCCA using deeper nonlinear networks for the second view can achieve even better total canonical correlation, but its PER performance then becomes signiﬁcantly worse.  Phone error rates (PERs) obtained by different algorithms are given in Table 2, where smaller PER indicates better recognition performance. It is clear that all CCA-based features signiﬁcantly im- prove over the baseline. Also, a large M is necessary for KCCA to be competitive with deep neural network methods, which is consistent with the ﬁndings of Huang et al. (2014); Lu et al. (2015b) when using random Fourier features for speech data (where the task is frame classiﬁcation). Over- all, KNOI outperforms the other approximate KCCA algorithms, although DCCA is still the best performer.  6 CONCLUSION  We have proposed kernel nonlinear orthogonal iterations (KNOI), a memory-efﬁcient approximate KCCA algorithm based on random Fourier features and stochastic training of linear CCA. It scales  9  Published as a conference paper at ICLR 2016  better to large data and outperforms previous approximate KCCA algorithms in both the objective values (total canonical correlation) and running times (with GPU support).  It is straightforward to incorporate in our algorithm the faster random features of Le et al. (2013) which can be generated (for view 1) in time O(N M log dx) instead of O(N M dx), or the Taylor features of Cotter et al. (2011) which is preferable for sparse inputs, and random features for dot product or polynomial kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015), which have proven to be useful for different domains. It is also worth exploring parallelization and multiple kernel learning strategies of Lu et al. (2015b) with random Fourier features to further bridge the gap between kernel methods and deep neural network methods.  Finally, as noted before, our algorithm does not use unbiased estimates of the gradient of the CCA objective. However, unbiased gradient estimates are not necessary for convergence of stochastic algorithms in general; a prominent example is the popular Oja’s rule for stochastic PCA (see discus- sions in Balsubramani et al., 2013; Shamir, 2015). Deriving global convergence properties for our algorithm is a challenging topic and the subject of ongoing work.  ACKNOWLEDGEMENT  This research was supported by NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily reﬂect the views of the funding agency. The Tesla K40 GPUs used for this research were donated by NVIDIA Corporation. We thank Bo Xie for providing his implementation of the doubly stochastic gradient algorithm for approximate KCCA, and Nati Srebro for helpful discussions.  REFERENCES Akaho, Shotaro. A kernel method for canonical correlation analysis.  Meeting of the Psychometric Society (IMPS2001), 2001.  In Proceedings of the International  Anderson, T. W. An Introduction to Multivariate Statistical Analysis. third edition, 2003.  Andrew, Galen, Arora, Raman, Bilmes, Jeff, and Livescu, Karen. Deep canonical correlation analysis. In Proc.  of the 30th Int. Conf. Machine Learning (ICML 2013), pp. 1247–1255, 2013.  Arora, Raman and Livescu, Karen. Kernel CCA for multi-view learning of acoustic features using articulatory  measurements. In Symposium on Machine Learning in Speech and Language Processing (MLSLP), 2012.  Arora, Raman and Livescu, Karen. Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains. In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP’13), 2013.  Arora, Raman, Cotter, Andy, Livescu, Karen, and Srebro, Nati. Stochastic optimization for PCA and PLS. In  50th Annual Allerton Conference on Communication, Control, and Computing, pp. 861–868, 2012.  Arora, Raman, Cotter, Andy, and Srebro, Nati. Stochastic optimization of PCA with capped MSG. In Advances  in Neural Information Processing Systems (NIPS), volume 26, pp. 1815–1823, 2013.  Avron, Haim, Boutsidis, Christos, Toledo, Sivan, and Zouzias, Anastasios. Efﬁcient dimensionality reduction In Proc. of the 30th Int. Conf. Machine Learning (ICML 2013), pp.  for canonical correlation analysis. 347–355, 2013.  Bach, Francis R. and Jordan, Michael I. Kernel independent component analysis. Journal of Machine Learning  Research, 3:1–48, 2002.  Bach, Francis R. and Jordan, Michael I. A probabilistic interpretation of canonical correlation analysis. Tech-  nical Report 688, Dept. of Statistics, University of California, Berkeley, 2005.  Balsubramani, Akshay, Dasgupta, Sanjoy, and Freund, Yoav. The fast convergence of incremental PCA. In  Advances in Neural Information Processing Systems (NIPS), volume 26, pp. 3174–3182, 2013.  Bie, Tijl De and Moor, Bart De.  On the regularization of canonical correlation analysis.  www.esat.kuleuven.ac.be/sista-cosic-docarch/, 2003. Int. Sympos. ICA and BSS.  Blaschkoa, Matthew B., Sheltonb, Jacquelyn A., Bartelsc, Andreas, Lamperte, Christoph H., and Gretton, Arthur. Semi-supervised kernel canonical correlation analysis with application to human fMRI. Pattern Recognition Letters, 32(11):1572–1583, 2011.  10  Published as a conference paper at ICLR 2016  Borga, Magnus. Canonical correlation: A tutorial. 2001.  Bottou, Leon and Bousquet, Olivier. The tradeoffs of large scale learning. In Advances in Neural Information  Processing Systems (NIPS), volume 20, pp. 161–168, 2008.  Chaudhuri, Kamalika, Kakade, Sham M., Livescu, Karen, and Sridharan, Karthik. Multi-view clustering via canonical correlation analysis. In Proc. of the 26th Int. Conf. Machine Learning (ICML’09), pp. 129–136, 2009.  Chechik, Gal, Globerson, Amir, Tishby, Naftali, and Weiss, Yair. Information bottleneck for Gaussian variables.  Journal of Machine Learning Research, 6:165–188, 2005.  Cotter, Andrew, Keshet, Joseph, and Srebro, Nathan. Explicit approximations of the gaussian kernel.  arXiv:1109.4603 [cs.AI], September 21 2011.  Dhillon, Paramveer, Foster, Dean, and Ungar, Lyle. Multi-view learning of word embeddings via CCA. In  Advances in Neural Information Processing Systems (NIPS), volume 24, pp. 199–207, 2011.  Faruqui, Manaal and Dyer, Chris. Improving vector space word representations using multilingual correlation.  In Proceedings of EACL, 2014.  Foster, Dean P., Johnson, Rie, Kakade, Sham M., and Zhang, Tong. Multi-view dimensionality reduction via  canonical correlation analysis. 2009.  Fukumizu, Kenji, Bach, Francis R., and Gretton, Arthur. Statistical consistency of kernel canonical correlation  analysis. Journal of Machine Learning Research, 8:361–383, 2007.  Gittens, Alex and Mahoney, Michael. Revisiting the nystrom method for improved large-scale machine learn-  ing. In Proc. of the 30th Int. Conf. Machine Learning (ICML 2013), pp. 567–575, 2013.  Golub, Gene H. and van Loan, Charles F. Matrix Computations. third edition, 1996.  Golub, Gene H. and Zha, Hongyuan. Linear Algebra for Signal Processing, volume 69 of The IMA Volumes in Mathematics and its Applications, chapter The Canonical Correlations of Matrix Pairs and their Numerical Computation, pp. 27–49. 1995.  Haghighi, Aria, Liang, Percy, Berg-Kirkpatrick, Taylor, and Klein, Dan. Learning bilingual lexicons from monolingual corpora. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2008), pp. 771–779, 2008.  Halko, Nathan, Martinsson, Per-Gunnar, Shkolnisky, Yoel, and Tygert, Mark. An algorithm for the principal  component analysis of large data sets. SIAM J. Sci. Comput., 33(5):2580–2594, 2011.  Hamid, Raffay, Xiao, Ying, Gittens, Alex, and Decoste, Dennis. Compact random feature maps. In Proc. of  the 31st Int. Conf. Machine Learning (ICML 2014), pp. 19–27, 2014.  Hardoon, David R., Szedmak, Sandor, and Shawe-Taylor, John. Canonical correlation analysis: An overview  with application to learning methods. Neural Computation, 16(12):2639–2664, 2004.  Hardoon, David R., Mour¨ao-Miranda, Janaina, Brammer, Michael, and Shawe-Taylor, John. Unsupervised  analysis of fMRI data using kernel canonical correlation. NeuroImage, 37(4):1250–1259, 2007.  Hermansky, Hynek, Ellis, Daniel P. W., and Sharma, Sangita. Tandem connectionist feature extraction for conventional HMM systems. In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP’00), pp. 1635–1638, 2000.  Hodosh, Micah, Young, Peter, and Hockenmaier, Julia. Framing image description as a ranking task: Data,  models and evaluation metrics. Journal of Artiﬁcial Intelligence Research, 47:853–899, 2013.  Hotelling, Harold. Relations between two sets of variates. Biometrika, 28(3/4):321–377, 1936.  Huang, Po-Sen, Avron, Haim, Sainath, Tara, Sindhwani, Vikas, and Ramabhadran, Bhuvana. Kernel methods match deep neural networks on TIMIT: Scalable learning in high-dimensional random Fourier spaces. In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP’14), pp. 205–209, 2014.  Kakade, Sham M. and Foster, Dean P. Multi-view regression via canonical correlation analysis. In Proc. of the 20th Annual Conference on Learning Theory (COLT’07), number 4539 in Lecture Notes in Computer Science, pp. 82–96, 2007.  11  Published as a conference paper at ICLR 2016  Kar, Purushottam and Karnick, Harish. Random feature maps for dot product kernels. In Proc. of the 15th Int.  Workshop on Artiﬁcial Intelligence and Statistics (AISTATS 2012), pp. 583–591, 2012.  Kim, Tae-Kyun, Kittler, Josef, and Cipolla, Roberto. Discriminative learning and recognition of image set classes using canonical correlations. IEEE Trans. Pattern Analysis and Machine Intelligence, 29(6):1005– 1018, 2007.  Krasulina, T. P. A method of stochastic approximation for the determination of the least eigenvalue of a  symmetric matrix. USSR Computational Mathematics and Mathematical Physics, 9(6):189–195, 1969.  Kumar, Sanjiv, Mohri, Mehryar, and Talwalkar, Ameet. Sampling methods for the nystr¨om method. Journal of  Machine Learning Research, 13:981–1006, 2012.  Lai, P. L. and Fyfe, C. Kernel and nonlinear canonical correlation analysis. Int. J. Neural Syst., 10(5):365–377,  2000.  Le, Quoc, Sarlos, Tamas, and Smola, Alexander. Fastfood - computing Hilbert space expansions in loglinear  time. In Proc. of the 30th Int. Conf. Machine Learning (ICML 2013), pp. 244–252, 2013.  Li, M., Kwok, J. T., and Lu, B. Making large-scale Nystr¨om approximation possible. In Proc. of the 27th Int.  Conf. Machine Learning (ICML 2010), pp. 631–638, 2010.  Li, Yaoyong and Shawe-Taylor, John. Using KCCA for japanese-english cross-language information retrieval  and classiﬁcation. Journal of Intelligent Information Systems, 27(2):117–133, 2005.  Loosli, Ga¨elle, Canu, St´ephane, and Bottou, L´eon. Training invariant support vector machines using selective  sampling. In Large Scale Kernel Machines, pp. 301–320. 2007.  Lopez-Paz, David, Hennig, Philipp, and Sch¨olkopf, Bernhard. The randomized dependence coefﬁcient.  Advances in Neural Information Processing Systems (NIPS), volume 26, pp. 1–9, 2013.  In  Lopez-Paz, David, Sra, Suvrit, Smola, Alex, Ghahramani, Zoubin, and Schoelkopf, Bernhard. Randomized nonlinear component analysis. In Proc. of the 31st Int. Conf. Machine Learning (ICML 2014), pp. 1359– 1367, 2014.  Lu, Ang, Wang, Weiran, Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Deep multilingual correlation for improved word embeddings. In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT 2015), 2015a.  Lu, Yichao and Foster, Dean P. Large scale canonical correlation analysis with iterative least squares.  Advances in Neural Information Processing Systems (NIPS), volume 27, pp. 91–99, 2014.  In  Lu, Zhiyun, May, Avner, Liu, Kuan, Garakani, Alireza Bagheri, Guo, Dong, Bellet, Aur´elien, Fan, Linxi, Collins, Michael, Kingsbury, Brian, Picheny, Michael, and Sha, Fei. How to scale up kernel methods to be as good as deep neural nets. arXiv:1411.4000 [cs.LG], 2015b.  Ma, Zhuang, Lu, Yichao, and Foster, Dean. Finding linear structure in large datasets with scalable canonical  correlation analysis. In Proc. of the 32st Int. Conf. Machine Learning (ICML 2015), pp. 169–178, 2015.  Makur, Anuran, Kozynski, Fab´ıan, Huang, Shao-Lun, and Zheng, Lizhong. An efﬁcient algorithm for infor- mation decomposition and extraction. In 53nd Annual Allerton Conference on Communication, Control and Computing, 2015.  Melzer, Thomas, Reiter, Michael, and Bischof, Horst. Nonlinear feature extraction using generalized canonical correlation analysis. In Proc. of the 11th Int. Conf. Artiﬁcial Neural Networks (ICANN’01), pp. 353–360, 2001.  Mitliagkas, Ioannis, Caramanis, Constantine, and Jain, Prateek. Memory limited, streaming PCA. In Advances  in Neural Information Processing Systems (NIPS), volume 26, pp. 2886–2894, 2013.  Montanarella, Luca, Bassani, Maria Rosa, and Br´eas, Olivier. Chemometric classiﬁcation of some European wines using pyrolysis mass spectrometry. Rapid Communications in Mass Spectrometry, 9(15):1589–1593, 1995.  Nocedal, Jorge and Wright, Stephen J. Numerical Optimization. Springer Series in Operations Research and  Financial Engineering. second edition, 2006.  Oja, Erkki and Karhunen, Juha. On stochastic approximation of the eigenvectors and eigenvalues of the expec-  tation of a random matrix. J. Math. Anal. Appl., 106(1):69–84, 1985.  12  Published as a conference paper at ICLR 2016  Pennington, Jeffrey, Yu, Felix, and Kumar, Sanjiv. Spherical random features for polynomial kernels. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems (NIPS), volume 28, pp. 1837–1845. MIT Press, Cambridge, MA, 2015.  Povey, Daniel, Ghoshal, Arnab, Boulianne, Gilles, Burget, Lukas, Glembek, Ondrej, Goel, Nagendra, Han- nemann, Mirko, Motlicek, Petr, Qian, Yanmin, Schwarz, Petr, Silovsky, Jan, Stemmer, Georg, and Vesely, Karel. The Kaldi speech recognition toolkit. In Proc. of the 2011 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2011), 2011.  Rahimi, Ali and Recht, Ben. Random features for large-scale kernel machines. In Advances in Neural Infor-  mation Processing Systems (NIPS), volume 20, pp. 1177–1184, 2008.  Rahimi, Ali and Recht, Benjamin. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in Neural Information Processing Systems (NIPS), volume 21, pp. 1313–1320, 2009.  Roux, Nicolas Le, Schmidt, Mark, and Bach, Francis. A stochastic gradient method with an exponential In Advances in Neural Information Processing Systems (NIPS),  convergence rate for ﬁnite training sets. volume 25, pp. 2672–2680, 2012.  Rudin, Walter. Fourier Analysis on Groups. 1994.  Rudzicz, Frank. Correcting errors in speech recognition with articulatory dynamics. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 60–68. Association for Computational Linguistics, 2010.  Sch¨olkopf, Bernhard and Smola, Alexander J. Learning with Kernels. Support Vector Machines, Regulariza-  tion, Optimization, and Beyond. Adaptive Computation and Machine Learning Series. 2001.  Shamir, Ohad. A stochastic PCA and SVD algorithm with an exponential convergence rate. In Proc. of the 32st  Int. Conf. Machine Learning (ICML 2015), pp. 144–152, 2015.  Socher, Richard and Li, Fei-Fei. Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora. In Proc. of the 2010 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR’10), pp. 966–973, 2010.  Sutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initialization and momentum in deep learning. In Proc. of the 30th Int. Conf. Machine Learning (ICML 2013), pp. 1139–1147, 2013.  Vinod, H. D. Canonical ridge and econometrics of joint production. Journal of Econometrics, 4(2):147–166,  1976.  Vinokourov, Alexei, Cristianini, Nello, and Shawe-Taylor, John. Inferring a semantic representation of text In Advances in Neural Information Processing Systems (NIPS),  via cross-language correlation analysis. volume 15, pp. 1497–1504, 2003.  Wang, Weiran, Arora, Raman, Livescu, Karen, and Bilmes, Jeff. Unsupervised learning of acoustic features via deep canonical correlation analysis. In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP’15), 2015a.  Wang, Weiran, Arora, Raman, Livescu, Karen, and Bilmes, Jeff. On deep multi-view representation learning.  In Proc. of the 32st Int. Conf. Machine Learning (ICML 2015), pp. 1083–1092, 2015b.  Wang, Weiran, Arora, Raman, Srebro, Nati, and Livescu, Karen. Stochastic optimization for deep cca via nonlinear orthogonal iterations. In 53nd Annual Allerton Conference on Communication, Control and Com- puting, 2015c.  Warmuth, Manfred K. and Kuzmin, Dima. Randomized online PCA algorithms with regret bounds that are  logarithmic in the dimension. Journal of Machine Learning Research, 9:2287–2320, 2008.  Westbury, John R. X-Ray Microbeam Speech Production Database User’s Handbook Version 1.0, 1994.  Williams, Christopher K. I. and Seeger, Matthias. Using the Nystr¨om method to speed up kernel machines. In  Advances in Neural Information Processing Systems (NIPS), volume 13, pp. 682–688, 2001.  Witten, Daniela M., Tibshirani, Robert, and Hastie, Trevor. A penalized matrix decomposition, with applica- tions to sparse principal components and canonical correlation analysis. Biostatistics, 10(3):515–534, 2009.  13  Published as a conference paper at ICLR 2016  Xie, Bo, Liang, Yingyu, and Song, Le. Scale up nonlinear component analysis with doubly stochastic gradients. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems (NIPS), volume 28, pp. 2332–2340. MIT Press, Cambridge, MA, 2015.  Yamanishi, Yoshihiro, Vert, Jean-Philippe, and Kanehisa, Minoru. Kernel Methods in Computational Biology, chapter Heterogeneous Data Comparison and Gene Selection with Kernel Canonical Correlation Analysis, pp. 209–229. MIT Press, 2004.  Yang, Jiyan, Sindhwani, Vikas, Avron, Haim, and Mahoney, Michael. Quasi-Monte Carlo feature maps for  shift-invariant kernels. In Proc. of the 31st Int. Conf. Machine Learning (ICML 2014), pp. 485–493, 2014.  Yang, Tianbao, Li, Yu-Feng, Mahdavi, Mehrdad, Jin, Rong, and Zhou, Zhi-Hua. Nystr¨om method vs random Fourier features: A theoretical and empirical comparison. In Advances in Neural Information Processing Systems (NIPS), volume 25, pp. 476–484, 2012.  Yger, Florian, Berar, Maxime, Gasso, Gilles, and Rakotomamonjy, Alain. Adaptive canonical correlation analysis based on matrix manifolds. In Proc. of the 29th Int. Conf. Machine Learning (ICML 2012), pp. 1071–1078, 2012.  Young, Steve J., Kernshaw, Dan, Odell, Julian, Ollason, Dave, Valtchev, Valtcho, and Woodland, Phil. The  HTK book version 2.2. Technical report, Entropic, Ltd., 1999.  Zhang, Kai and Kwok, James T. Density-weighted Nystr¨om method for computing large kernel eigen-systems.  Neural Computation, 21(1):121–146, 2009.  Zhang, Kai and Kwok, James T. Clustered Nystr¨om method for large scale manifold learning and dimension  reduction. IEEE Trans. Neural Networks, 21(10):1576–1587, 2010.  14  ",
1511.06434,2016,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,"['Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\nAlec Radford', 'Luke Metz', 'Soumith Chintala']",https://arxiv.org/pdf/1511.06434,"Under review as a conference paper at ICLR 2016  UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS  Alec Radford & Luke Metz indico Research Boston, MA {alec,luke}@indico.io  Soumith Chintala Facebook AI Research New York, NY soumith@fb.com  ABSTRACT  In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsuper- vised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolu- tional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image repre- sentations.  1  INTRODUCTION  Learning reusable feature representations from large unlabeled datasets has been an area of active research. In the context of computer vision, one can leverage the practically unlimited amount of unlabeled images and videos to learn good intermediate representations, which can then be used on a variety of supervised learning tasks such as image classiﬁcation. We propose that one way to build good image representations is by training Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks. GANs provide an attractive alternative to maximum likelihood techniques. One can additionally argue that their learning process and the lack of a heuristic cost function (such as pixel-wise independent mean-square error) are attractive to representation learning. GANs have been known to be unstable to train, often resulting in generators that produce nonsensical outputs. There has been very limited published research in trying to understand and visualize what GANs learn, and the intermediate representations of multi-layer GANs. In this paper, we make the following contributions  6 1 0 2     n a J    7      ]  G L . s c [      2 v 4 3 4 6 0  .  1 1 5 1 : v i X r a  • We propose and evaluate a set of constraints on the architectural topology of Convolutional GANs that make them stable to train in most settings. We name this class of architectures Deep Convolutional GANs (DCGAN) • We use the trained discriminators for image classiﬁcation tasks, showing competitive per- • We visualize the ﬁlters learnt by GANs and empirically show that speciﬁc ﬁlters have  formance with other unsupervised algorithms.  learned to draw speciﬁc objects.  1  Under review as a conference paper at ICLR 2016  • We show that the generators have interesting vector arithmetic properties allowing for easy  manipulation of many semantic qualities of generated samples.  2 RELATED WORK  2.1 REPRESENTATION LEARNING FROM UNLABELED DATA  Unsupervised representation learning is a fairly well studied problem in general computer vision research, as well as in the context of images. A classic approach to unsupervised representation learning is to do clustering on the data (for example using K-means), and leverage the clusters for improved classiﬁcation scores. In the context of images, one can do hierarchical clustering of image patches (Coates & Ng, 2012) to learn powerful image representations. Another popular method is to train auto-encoders (convolutionally, stacked (Vincent et al., 2010), separating the what and where components of the code (Zhao et al., 2015), ladder structures (Rasmus et al., 2015)) that encode an image into a compact code, and decode the code to reconstruct the image as accurately as possible. These methods have also been shown to learn good feature representations from image pixels. Deep belief networks (Lee et al., 2009) have also been shown to work well in learning hierarchical representations.  2.2 GENERATING NATURAL IMAGES  Generative image models are well studied and fall into two categories: parametric and non- parametric. The non-parametric models often do matching from a database of existing images, often matching patches of images, and have been used in texture synthesis (Efros et al., 1999), super-resolution (Freeman et al., 2002) and in-painting (Hays & Efros, 2007). Parametric models for generating images has been explored extensively (for example on MNIST digits or for texture synthesis (Portilla & Simoncelli, 2000)). However, generating natural images of the real world have had not much success until recently. A variational sampling approach to generating images (Kingma & Welling, 2013) has had some success, but the samples often suffer from being blurry. Another approach generates images using an iterative forward diffusion process (Sohl-Dickstein et al., 2015). Generative Adversarial Networks (Goodfellow et al., 2014) generated images suffering from being noisy and incomprehensible. A laplacian pyramid extension to this approach (Denton et al., 2015) showed higher quality images, but they still suffered from the objects looking wobbly because of noise introduced in chaining multiple models. A recurrent network approach (Gregor et al., 2015) and a deconvolution network approach (Dosovitskiy et al., 2014) have also recently had some success with generating natural images. However, they have not leveraged the generators for supervised tasks.  2.3 VISUALIZING THE INTERNALS OF CNNS  One constant criticism of using neural networks has been that they are black-box methods, with little understanding of what the networks do in the form of a simple human-consumable algorithm. In the context of CNNs, Zeiler et. al. (Zeiler & Fergus, 2014) showed that by using deconvolutions and ﬁltering the maximal activations, one can ﬁnd the approximate purpose of each convolution ﬁlter in the network. Similarly, using a gradient descent on the inputs lets us inspect the ideal image that activates certain subsets of ﬁlters (Mordvintsev et al.).  3 APPROACH AND MODEL ARCHITECTURE  Historical attempts to scale up GANs using CNNs to model images have been unsuccessful. This motivated the authors of LAPGAN (Denton et al., 2015) to develop an alternative approach to it- eratively upscale low resolution generated images which can be modeled more reliably. We also encountered difﬁculties attempting to scale GANs using CNN architectures commonly used in the supervised literature. However, after extensive model exploration we identiﬁed a family of archi-  2  Under review as a conference paper at ICLR 2016  tectures that resulted in stable training across a range of datasets and allowed for training higher resolution and deeper generative models. Core to our approach is adopting and modifying three recently demonstrated changes to CNN archi- tectures. The ﬁrst is the all convolutional net (Springenberg et al., 2014) which replaces deterministic spatial pooling functions (such as maxpooling) with strided convolutions, allowing the network to learn its own spatial downsampling. We use this approach in our generator, allowing it to learn its own spatial upsampling, and discriminator. Second is the trend towards eliminating fully connected layers on top of convolutional features. The strongest example of this is global average pooling which has been utilized in state of the art image classiﬁcation models (Mordvintsev et al.). We found global average pooling increased model stability but hurt convergence speed. A middle ground of directly connecting the highest convolutional features to the input and output respectively of the generator and discriminator worked well. The ﬁrst layer of the GAN, which takes a uniform noise distribution Z as input, could be called fully connected as it is just a matrix multiplication, but the result is reshaped into a 4-dimensional tensor and used as the start of the convolution stack. For the discriminator, the last convolution layer is ﬂattened and then fed into a single sigmoid output. See Fig. 1 for a visualization of an example model architecture. Third is Batch Normalization (Ioffe & Szegedy, 2015) which stabilizes learning by normalizing the input to each unit to have zero mean and unit variance. This helps deal with training problems that arise due to poor initialization and helps gradient ﬂow in deeper models. This proved critical to get deep generators to begin learning, preventing the generator from collapsing all samples to a single point which is a common failure mode observed in GANs. Directly applying batchnorm to all layers however, resulted in sample oscillation and model instability. This was avoided by not applying batchnorm to the generator output layer and the discriminator input layer. The ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output layer which uses the Tanh function. We observed that using a bounded activation allowed the model to learn more quickly to saturate and cover the color space of the training distribution. Within the discriminator we found the leaky rectiﬁed activation (Maas et al., 2013) (Xu et al., 2015) to work well, especially for higher resolution modeling. This is in contrast to the original GAN paper, which used the maxout activation (Goodfellow et al., 2013).  Architecture guidelines for stable Deep Convolutional GANs  convolutions (generator).  • Replace any pooling layers with strided convolutions (discriminator) and fractional-strided • Use batchnorm in both the generator and the discriminator. • Remove fully connected hidden layers for deeper architectures. • Use ReLU activation in generator for all layers except for the output, which uses Tanh. • Use LeakyReLU activation in the discriminator for all layers.  4 DETAILS OF ADVERSARIAL TRAINING  We trained DCGANs on three datasets, Large-scale Scene Understanding (LSUN) (Yu et al., 2015), Imagenet-1k and a newly assembled Faces dataset. Details on the usage of each of these datasets are given below. No pre-processing was applied to training images besides scaling to the range of the tanh activation function [-1, 1]. All models were trained with mini-batch stochastic gradient descent (SGD) with a mini-batch size of 128. All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02. In the LeakyReLU, the slope of the leak was set to 0.2 in all models. While previous GAN work has used momentum to accelerate training, we used the Adam optimizer (Kingma & Ba, 2014) with tuned hyperparameters. We found the suggested learning rate of 0.001, to be too high, using 0.0002 instead. Additionally, we found leaving the momentum term β1 at the  3  Under review as a conference paper at ICLR 2016  Figure 1: DCGAN generator used for LSUN scene modeling. A 100 dimensional uniform distribu- tion Z is projected to a small spatial extent convolutional representation with many feature maps. A series of four fractionally-strided convolutions (in some recent papers, these are wrongly called deconvolutions) then convert this high level representation into a 64 × 64 pixel image. Notably, no fully connected or pooling layers are used.  suggested value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped stabilize training.  4.1 LSUN  As visual quality of samples from generative image models has improved, concerns of over-ﬁtting and memorization of training samples have risen. To demonstrate how our model scales with more data and higher resolution generation, we train a model on the LSUN bedrooms dataset containing a little over 3 million training examples. Recent analysis has shown that there is a direct link be- tween how fast models learn and their generalization performance (Hardt et al., 2015). We show samples from one epoch of training (Fig.2), mimicking online learning, in addition to samples after convergence (Fig.3), as an opportunity to demonstrate that our model is not producing high quality samples via simply overﬁtting/memorizing training examples. No data augmentation was applied to the images.  4.1.1 DEDUPLICATION  To further decrease the likelihood of the generator memorizing input examples (Fig.2) we perform a simple image de-duplication process. We ﬁt a 3072-128-3072 de-noising dropout regularized RELU autoencoder on 32x32 downsampled center-crops of training examples. The resulting code layer activations are then binarized via thresholding the ReLU activation which has been shown to be an effective information preserving technique (Srivastava et al., 2014) and provides a convenient form of semantic-hashing, allowing for linear time de-duplication . Visual inspection of hash collisions showed high precision with an estimated false positive rate of less than 1 in 100. Additionally, the technique detected and removed approximately 275,000 near duplicates, suggesting a high recall.  4.2 FACES  We scraped images containing human faces from random web image queries of peoples names. The people names were acquired from dbpedia, with a criterion that they were born in the modern era. This dataset has 3M images from 10K people. We run an OpenCV face detector on these images, keeping the detections that are sufﬁciently high resolution, which gives us approximately 350,000 face boxes. We use these face boxes for training. No data augmentation was applied to the images.  4  Under review as a conference paper at ICLR 2016  Figure 2: Generated bedrooms after one training pass through the dataset. Theoretically, the model could learn to memorize training examples, but this is experimentally unlikely as we train with a small learning rate and minibatch SGD. We are aware of no prior empirical evidence demonstrating memorization with SGD and a small learning rate.  Figure 3: Generated bedrooms after ﬁve epochs of training. There appears to be evidence of visual under-ﬁtting via repeated noise textures across multiple samples such as the base boards of some of the beds.  4.3  IMAGENET-1K  We use Imagenet-1k (Deng et al., 2009) as a source of natural images for unsupervised training. We train on 32 × 32 min-resized center crops. No data augmentation was applied to the images.  5  Under review as a conference paper at ICLR 2016  5 EMPIRICAL VALIDATION OF DCGANS CAPABILITIES  5.1 CLASSIFYING CIFAR-10 USING GANS AS A FEATURE EXTRACTOR  One common technique for evaluating the quality of unsupervised representation learning algo- rithms is to apply them as a feature extractor on supervised datasets and evaluate the performance of linear models ﬁtted on top of these features. On the CIFAR-10 dataset, a very strong baseline performance has been demonstrated from a well tuned single layer feature extraction pipeline utilizing K-means as a feature learning algorithm. When using a very large amount of feature maps (4800) this technique achieves 80.6% accuracy. An unsupervised multi-layered extension of the base algorithm reaches 82.0% accuracy (Coates & Ng, 2011). To evaluate the quality of the representations learned by DCGANs for supervised tasks, we train on Imagenet-1k and then use the discriminator’s convolutional features from all layers, maxpooling each layers representation to produce a 4 × 4 spatial grid. These features are then ﬂattened and concatenated to form a 28672 dimensional vector and a regularized linear L2-SVM classiﬁer is trained on top of them. This achieves 82.8% accuracy, out performing all K-means based approaches. Notably, the discriminator has many less feature maps (512 in the highest layer) compared to K-means based techniques, but does result in a larger total feature vector size due to the many layers of 4 × 4 spatial locations. The performance of DCGANs is still less than that of Exemplar CNNs (Dosovitskiy et al., 2015), a technique which trains normal discriminative CNNs in an unsupervised fashion to differentiate between speciﬁcally chosen, aggressively augmented, exemplar samples from the source dataset. Further improvements could be made by ﬁnetuning the discriminator’s representations, but we leave this for future work. Additionally, since our DCGAN was never trained on CIFAR-10 this experiment also demonstrates the domain robustness of the learned features.  Table 1: CIFAR-10 classiﬁcation results using our pre-trained model. Our DCGAN is not pre- trained on CIFAR-10, but on Imagenet-1k, and the features are used to classify CIFAR-10 images. Accuracy Accuracy (400 per class) max # of features units  Model  1 Layer K-means  3 Layer K-means Learned RF  View Invariant K-means  Exemplar CNN  DCGAN (ours) + L2-SVM  80.6% 82.0% 81.9% 84.3% 82.8%  63.7% (±0.7%) 70.7% (±0.7%) 72.6% (±0.7%) 77.4% (±0.2%) 73.8% (±0.4%)  4800 3200 6400 1024 512  5.2 CLASSIFYING SVHN DIGITS USING GANS AS A FEATURE EXTRACTOR  On the StreetView House Numbers dataset (SVHN)(Netzer et al., 2011), we use the features of the discriminator of a DCGAN for supervised purposes when labeled data is scarce. Following similar dataset preparation rules as in the CIFAR-10 experiments, we split off a validation set of 10,000 examples from the non-extra set and use it for all hyperparameter and model selection. 1000 uniformly class distributed training examples are randomly selected and used to train a regularized linear L2-SVM classiﬁer on top of the same feature extraction pipeline used for CIFAR-10. This achieves state of the art (for classiﬁcation using 1000 labels) at 22.48% test error, improving upon another modifcation of CNNs designed to leverage unlabled data (Zhao et al., 2015). Additionally, we validate that the CNN architecture used in DCGAN is not the key contributing factor of the model’s performance by training a purely supervised CNN with the same architecture on the same data and optimizing this model via random search over 64 hyperparameter trials (Bergstra & Bengio, 2012). It achieves a signﬁcantly higher 28.87% validation error.  6  INVESTIGATING AND VISUALIZING THE INTERNALS OF THE NETWORKS  We investigate the trained generators and discriminators in a variety of ways. We do not do any kind of nearest neighbor search on the training set. Nearest neighbors in pixel or feature space are  6  Under review as a conference paper at ICLR 2016  Table 2: SVHN classiﬁcation with 1000 labels  Model KNN TSVM  M1+KNN M1+TSVM  M1+M2  SWWAE without dropout  SWWAE with dropout  DCGAN (ours) + L2-SVM  Supervised CNN with the same architecture  error rate 77.93% 66.55% 65.63% 54.33% 36.02% 27.83% 23.56% 22.48%  28.87% (validation)  trivially fooled (Theis et al., 2015) by small image transforms. We also do not use log-likelihood metrics to quantitatively assess the model, as it is a poor (Theis et al., 2015) metric.  6.1 WALKING IN THE LATENT SPACE  The ﬁrst experiment we did was to understand the landscape of the latent space. Walking on the manifold that is learnt can usually tell us about signs of memorization (if there are sharp transitions) and about the way in which the space is hierarchically collapsed. If walking in this latent space results in semantic changes to the image generations (such as objects being added and removed), we can reason that the model has learned relevant and interesting representations. The results are shown in Fig.4.  6.2 VISUALIZING THE DISCRIMINATOR FEATURES  Previous work has demonstrated that supervised training of CNNs on large image datasets results in very powerful learned features (Zeiler & Fergus, 2014). Additionally, supervised CNNs trained on scene classiﬁcation learn object detectors (Oquab et al., 2014). We demonstrate that an unsupervised DCGAN trained on a large image dataset can also learn a hierarchy of features that are interesting. Using guided backpropagation as proposed by (Springenberg et al., 2014), we show in Fig.5 that the features learnt by the discriminator activate on typical parts of a bedroom, like beds and windows. For comparison, in the same ﬁgure, we give a baseline for randomly initialized features that are not activated on anything that is semantically relevant or interesting.  6.3 MANIPULATING THE GENERATOR REPRESENTATION  6.3.1 FORGETTING TO DRAW CERTAIN OBJECTS  In addition to the representations learnt by a discriminator, there is the question of what representa- tions the generator learns. The quality of samples suggest that the generator learns speciﬁc object representations for major scene components such as beds, windows, lamps, doors, and miscellaneous furniture. In order to explore the form that these representations take, we conducted an experiment to attempt to remove windows from the generator completely. On 150 samples, 52 window bounding boxes were drawn manually. On the second highest con- volution layer features, logistic regression was ﬁt to predict whether a feature activation was on a window (or not), by using the criterion that activations inside the drawn bounding boxes are posi- tives and random samples from the same images are negatives. Using this simple model, all feature maps with weights greater than zero ( 200 in total) were dropped from all spatial locations. Then, random new samples were generated with and without the feature map removal. The generated images with and without the window dropout are shown in Fig.6, and interestingly, the network mostly forgets to draw windows in the bedrooms, replacing them with other objects.  7  Under review as a conference paper at ICLR 2016  Figure 4: Top rows: Interpolation between a series of 9 random points in Z show that the space learned has smooth transitions, with every image in the space plausibly looking like a bedroom. In the 6th row, you see a room without a window slowly transforming into a room with a giant window. In the 10th row, you see what appears to be a TV slowly being transformed into a window.  6.3.2 VECTOR ARITHMETIC ON FACE SAMPLES  In the context of evaluating learned representations of words (Mikolov et al., 2013) demonstrated that simple arithmetic operations revealed rich linear structure in representation space. One canoni- cal example demonstrated that the vector(”King”) - vector(”Man”) + vector(”Woman”) resulted in a vector whose nearest neighbor was the vector for Queen. We investigated whether similar structure emerges in the Z representation of our generators. We performed similar arithmetic on the Z vectors of sets of exemplar samples for visual concepts. Experiments working on only single samples per concept were unstable, but averaging the Z vector for three examplars showed consistent and stable generations that semantically obeyed the arithmetic. In addition to the object manipulation shown in (Fig. 7), we demonstrate that face pose is also modeled linearly in Z space (Fig. 8). These demonstrations suggest interesting applications can be developed using Z representations learned by our models. It has been previously demonstrated that conditional generative models can learn to convincingly model object attributes like scale, rotation, and position (Dosovitskiy et al., 2014). This is to our knowledge the ﬁrst demonstration of this occurring in purely unsupervised  8  Under review as a conference paper at ICLR 2016  Figure 5: On the right, guided backpropagation visualizations of maximal axis-aligned responses for the ﬁrst 6 learned convolutional features from the last convolution layer in the discriminator. Notice a signiﬁcant minority of features respond to beds - the central object in the LSUN bedrooms dataset. On the left is a random ﬁlter baseline. Comparing to the previous responses there is little to no discrimination and random structure.  Figure 6: Top row: un-modiﬁed samples from model. Bottom row: the same samples generated with dropping out ”window” ﬁlters. Some windows are removed, others are transformed into objects with similar visual appearance such as doors and mirrors. Although visual quality decreased, overall scene composition stayed similar, suggesting the generator has done a good job disentangling scene representation from object representation. Extended experiments could be done to remove other objects from the image and modify the objects the generator draws.  models. Further exploring and developing the above mentioned vector arithmetic could dramat- ically reduce the amount of data needed for conditional generative modeling of complex image distributions.  7 CONCLUSION AND FUTURE WORK  We propose a more stable set of architectures for training generative adversarial networks and we give evidence that adversarial networks learn good representations of images for supervised learning and generative modeling. There are still some forms of model instability remaining - we noticed as models are trained longer they sometimes collapse a subset of ﬁlters to a single oscillating mode.  9  Under review as a conference paper at ICLR 2016  Figure 7: Vector arithmetic for visual concepts. For each column, the Z vectors of samples are averaged. Arithmetic was then performed on the mean vectors creating a new vector Y . The center sample on the right hand side is produce by feeding Y as input to the generator. To demonstrate the interpolation capabilities of the generator, uniform noise sampled with scale +-0.25 was added to Y to produce the 8 other samples. Applying arithmetic in the input space (bottom two examples) results in noisy overlap due to misalignment.  Further work is needed to tackle this from of instability. We think that extending this framework  10  Under review as a conference paper at ICLR 2016  Figure 8: A ”turn” vector was created from four averaged samples of faces looking left vs looking right. By adding interpolations along this axis to random samples we were able to reliably transform their pose.  to other domains such as video (for frame prediction) and audio (pre-trained features for speech synthesis) should be very interesting. Further investigations into the properties of the learnt latent space would be interesting as well.  ACKNOWLEDGMENTS  We are fortunate and thankful for all the advice and guidance we have received during this work, especially that of Ian Goodfellow, Tobias Springenberg, Arthur Szlam and Durk Kingma. Addition- ally we’d like to thank all of the folks at indico for providing support, resources, and conversations, especially the two other members of the indico research team, Dan Kuster and Nathan Lintz. Finally, we’d like to thank Nvidia for donating a Titan-X GPU used in this work.  REFERENCES Bergstra, James and Bengio, Yoshua. Random search for hyper-parameter optimization. JMLR,  2012.  Coates, Adam and Ng, Andrew. Selecting receptive ﬁelds in deep networks. NIPS, 2011.  Coates, Adam and Ng, Andrew Y. Learning feature representations with k-means. In Neural Net-  works: Tricks of the Trade, pp. 561–580. Springer, 2012.  Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248–255. IEEE, 2009.  Denton, Emily, Chintala, Soumith, Szlam, Arthur, and Fergus, Rob. Deep generative image models  using a laplacian pyramid of adversarial networks. arXiv preprint arXiv:1506.05751, 2015.  Dosovitskiy, Alexey, Springenberg, Jost Tobias, and Brox, Thomas. Learning to generate chairs  with convolutional neural networks. arXiv preprint arXiv:1411.5928, 2014.  11  Under review as a conference paper at ICLR 2016  Dosovitskiy, Alexey, Fischer, Philipp, Springenberg, Jost Tobias, Riedmiller, Martin, and Brox, Thomas. Discriminative unsupervised feature learning with exemplar convolutional neural net- works. In Pattern Analysis and Machine Intelligence, IEEE Transactions on, volume 99. IEEE, 2015.  Efros, Alexei, Leung, Thomas K, et al. Texture synthesis by non-parametric sampling. In Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on, volume 2, pp. 1033–1038. IEEE, 1999.  Freeman, William T, Jones, Thouis R, and Pasztor, Egon C. Example-based super-resolution. Com-  puter Graphics and Applications, IEEE, 22(2):56–65, 2002.  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.  Maxout networks. arXiv preprint arXiv:1302.4389, 2013.  Goodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair,  Sherjil, Courville, Aaron C., and Bengio, Yoshua. Generative adversarial nets. NIPS, 2014.  Gregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra, Daan. Draw: A recurrent neural network  for image generation. arXiv preprint arXiv:1502.04623, 2015.  Hardt, Moritz, Recht, Benjamin, and Singer, Yoram. Train faster, generalize better: Stability of  stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.  Hauberg, Sren, Freifeld, Oren, Larsen, Anders Boesen Lindbo, Fisher III, John W., and Hansen, Lars Kair. Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data augmentation. arXiv preprint arXiv:1510.02795, 2015.  Hays, James and Efros, Alexei A. Scene completion using millions of photographs. ACM Transac-  tions on Graphics (TOG), 26(3):4, 2007.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by  reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.  Kingma, Diederik P and Ba, Jimmy Lei. Adam: A method for stochastic optimization. arXiv  preprint arXiv:1412.6980, 2014.  Kingma, Diederik P and Welling, Max. Auto-encoding variational bayes.  arXiv:1312.6114, 2013.  arXiv preprint  Lee, Honglak, Grosse, Roger, Ranganath, Rajesh, and Ng, Andrew Y. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 609–616. ACM, 2009.  Loosli, Ga¨elle, Canu, St´ephane, and Bottou, L´eon. Training invariant support vector machines using In Bottou, L´eon, Chapelle, Olivier, DeCoste, Dennis, and Weston, Jason selective sampling. (eds.), Large Scale Kernel Machines, pp. 301–320. MIT Press, Cambridge, MA., 2007. URL http://leon.bottou.org/papers/loosli-canu-bottou-2006.  Maas, Andrew L, Hannun, Awni Y, and Ng, Andrew Y. Rectiﬁer nonlinearities improve neural  network acoustic models. In Proc. ICML, volume 30, 2013.  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed repre- sentations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111–3119, 2013.  Mordvintsev, Alexander, Olah, Christopher, and Tyka, Mike.  Inceptionism : Going deeper into neural networks. http://googleresearch.blogspot.com/2015/06/ inceptionism-going-deeper-into-neural.html. Accessed: 2015-06-17.  Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807– 814, 2010.  12  Under review as a conference paper at ICLR 2016  Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Read- ing digits in natural images with unsupervised feature learning. In NIPS workshop on deep learn- ing and unsupervised feature learning, volume 2011, pp. 5. Granada, Spain, 2011.  Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and transferring mid-level image represen-  tations using convolutional neural networks. In CVPR, 2014.  Portilla, Javier and Simoncelli, Eero P. A parametric texture model based on joint statistics of  complex wavelet coefﬁcients. International Journal of Computer Vision, 40(1):49–70, 2000.  Rasmus, Antti, Valpola, Harri, Honkala, Mikko, Berglund, Mathias, and Raiko, Tapani. Semi-  supervised learning with ladder network. arXiv preprint arXiv:1507.02672, 2015.  Sohl-Dickstein, Jascha, Weiss, Eric A, Maheswaranathan, Niru, and Ganguli, Surya. Deep unsuper-  vised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.  Springenberg, Jost Tobias, Dosovitskiy, Alexey, Brox, Thomas, and Riedmiller, Martin. Striving for  simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.  Srivastava, Rupesh Kumar, Masci, Jonathan, Gomez, Faustino, and Schmidhuber, J¨urgen. Under-  standing locally competitive networks. arXiv preprint arXiv:1410.1165, 2014.  Theis, L., van den Oord, A., and Bethge, M. A note on the evaluation of generative models.  arXiv:1511.01844, Nov 2015. URL http://arxiv.org/abs/1511.01844.  Vincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11:3371–3408, 2010.  Xu, Bing, Wang, Naiyan, Chen, Tianqi, and Li, Mu. Empirical evaluation of rectiﬁed activations in  convolutional network. arXiv preprint arXiv:1505.00853, 2015.  Yu, Fisher, Zhang, Yinda, Song, Shuran, Seff, Ari, and Xiao, Jianxiong. Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.  Zeiler, Matthew D and Fergus, Rob. Visualizing and understanding convolutional networks.  Computer Vision–ECCV 2014, pp. 818–833. Springer, 2014.  In  Zhao, Junbo, Mathieu, Michael, Goroshin, Ross, and Lecun, Yann. Stacked what-where auto-  encoders. arXiv preprint arXiv:1506.02351, 2015.  13  Under review as a conference paper at ICLR 2016  8 SUPPLEMENTARY MATERIAL  8.1 EVALUATING DCGANS CAPABILITY TO CAPTURE DATA DISTRIBUTIONS  We propose to apply standard classiﬁcation metrics to a conditional version of our model, evaluating the conditional distributions learned. We trained a DCGAN on MNIST (splitting off a 10K validation set) as well as a permutation invariant GAN baseline and evaluated the models using a nearest neighbor classiﬁer comparing real data to a set of generated conditional samples. We found that removing the scale and bias parameters from batchnorm produced better results for both models. We speculate that the noise introduced by batchnorm helps the generative models to better explore and generate from the underlying data distribution. The results are shown in Table 3 which compares our models with other techniques. The DCGAN model achieves the same test error as a nearest neighbor classiﬁer ﬁtted on the training dataset - suggesting the DCGAN model has done a superb job at modeling the conditional distributions of this dataset. At one million samples per class, the DCGAN model outperforms InﬁMNIST (Loosli et al., 2007), a hand developed data augmentation pipeline which uses translations and elastic deformations of training examples. The DCGAN is competitive with a probabilistic generative data augmentation technique utilizing learned per class transformations (Hauberg et al., 2015) while being more general as it directly models the data instead of transformations of the data.  Table 3: Nearest neighbor classiﬁcation results.  Test Error @50K samples Test Error @10M samples  Model  AlignMNIST InﬁMNIST Real Data  GAN  DCGAN (ours)  - -  3.1% 6.28% 2.98%  1.4% 2.6%  -  5.65% 1.48%  Figure 9: Side-by-side illustration of (from left-to-right) the MNIST dataset, generations from a baseline GAN, and generations from our DCGAN .  14  Under review as a conference paper at ICLR 2016  Figure 10: More face generations from our Face DCGAN.  15  Under review as a conference paper at ICLR 2016  Figure 11: Generations of a DCGAN that was trained on the Imagenet-1k dataset.  16  ",
1511.06448,2016,Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks,"['Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks [code]\nPouya Bashivan', 'Irina Rish', 'Mohammed Yeasin', 'Noel Codella']",https://arxiv.org/pdf/1511.06448,"6 1 0 2     b e F 9 2         ]  G L . s c [      3 v 8 4 4 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  LEARNING REPRESENTATIONS FROM EEG WITH DEEP RECURRENT-CONVOLUTIONAL NEURAL NETWORKS  Pouya Bashivan Electrical and Computer Engineering Department University of Memphis Memphis, TN , USA {pbshivan}@memphis.edu  Mohammed Yeasin Electrical and Computer Engineering Department University of Memphis Memphis, TN , USA {myeasin}@memphis.edu  Irina Rish IBM T.J. Watson Research Center Yorktown Heights, NY, USA {rish}@us.ibm.com  Noel Codella IBM T.J. Watson Research Center Yorktown Heights, NY, USA {nccodell}@us.ibm.com  ABSTRACT  One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is ﬁnding representations that are invariant to inter- and intra-subject differences, as well as to inherent noise associated with EEG data collection. Herein, we propose a novel approach for learning such representations from multi- channel EEG time-series, and demonstrate its advantages in the context of mental load classiﬁcation task. First, we transform EEG activities into a sequence of topology-preserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent- convolutional network inspired by state-of-the-art video classiﬁcation techniques to learn robust representations from the sequence of images. The proposed ap- proach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to ﬁnding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classiﬁcation task demonstrated signiﬁcant improvements in classiﬁcation accuracy over current state-of-the-art approaches in this ﬁeld.  1  INTRODUCTION  Deep neural networks have recently achieved great success in recognition tasks within a wide range of applications including images, videos, speech, and text (Krizhevsky et al., 2012; Graves et al., 2013; Karpathy & Toderici, 2014; Zhang & LeCun, 2015; Hermann et al., 2015). Convolutional neural networks (ConvNets) lie at the core of best current architectures working with images and video data, primarily due to their ability to extract representations that are robust to partial translation and deformation of input patterns (LeCun et al., 1998). On the other hand, recurrent neural networks have delivered state-of-the-art performance in many applications involving dynamics in temporal sequences, such as, for example, handwriting and speech recognition (Graves et al., 2013; 2008). In addition, combination of these two network types have recently been used for video classiﬁcation (Ng et al., 2015). Despite numerous successful applications of deep neural networks to large-scale image, video and text data, they remain relatively unexplored in neuroimaging domain. Perhaps one of the main reasons here is that the number of samples in most neuroimaging datasets is limited, thus making such data less adequate for training large-scale networks with millions of parameters. As it is often demonstrated, the advantages of deep neural networks over traditional machine-learning techniques become more apparent when the dataset size becomes very large. Nevertheless, deep belief network and ConvNets have been used to learn representations from functional Magnetic Resonance Imaging (fMRI) and Electroencephalogram (EEG) in some previous work with moderate dataset sizes (Plis  1  Published as a conference paper at ICLR 2016  et al., 2014; Mirowski et al., 2009). Plis et al. (2014) showed that adding several Restricted Boltzman Machine layers to a deep belief network and using supervised pretraining results in networks that can learn increasingly complex representations of the data and achieve considerable accuracy increase as compared to other classiﬁers. In other works, convolutional and recurrent neural networks have been used to extract representations from EEG time series (Mirowski et al., 2009; Cecotti & Gr¨aser, 2011; Guler et al., 2005). These studies demonstrated potential beneﬁts of adopting (down-scaled) deep neural networks in neuroimaging, even in the absence of extremely large, million-sample datasets, such as those available for images, video, and text modalities. However, none of these studies attempted to jointly preserve the structure of EEG data within space, time, and frequency. Herein, we explore the capabilities of deep neural nets for modeling cognitive events from EEG data. EEG is a widely used noninvasive neuroimaging modality which operates by measuring changes in electrical voltage on the scalp induced by cortical activity. Using the classical blind-source sepa- ration analogy, EEG data can be thought of as a multi-channel ”speech” signal obtained from sev- eral ”microphones” (associated with EEG electrodes) that record signals from multiple ”speakers” (that correspond to activity in cortical regions). State-of-the-art mental state recognition using EEG consists of manual feature selection from continuous time series and applying supervised learning algorithms to learn the discriminative manifold between the states (Lotte & Congedo, 2007; Sub- asi & Ismail Gursoy, 2010). A key challenge in correctly recognizing mental states from observed brain activity is constructing a model that is robust to translation and deformation of signal in space, frequency, and time, due to inter- and intra-subject differences, as well as signal acquisition proto- cols. Much of the variations originate from slight individual differences in cortical mapping and/or functioning, giving rise to observed differences in spatial, spectral, and temporal patterns. More- over, EEG caps which are used to place the electrodes on top of predetermined cortical regions can be another source of spatial variations in observed responses due to imperfect ﬁtting of the cap on heads of different sizes and shapes. An example illustrating potentially high inter- and intra-subject variability in EEG data is given in Appendix. We propose a novel approach to learning representations from EEG data that relies on deep learning and appears to be more robust to inter- and intra-subject differences, as well as to measurement- related noise. Our approach is fundamentally different from the previous attempts to learn high- level representations from EEG using deep neural networks. Speciﬁcally, rather than representing low-level EEG features as a vector, we transform the data into a multi-dimensional tensor which retains the structure of the data throughout the learning process. In other words, we obtain a se- quence of topology-preserving multi-spectral images, as opposed to standard EEG analysis tech- niques that ignore such spatial information. Once such EEG ”movie” is obtained, we train deep recurrent-convolutional neural network architectures, inspired by state-of-the-art video classiﬁca- tion (Ng et al., 2015), to learn robust representations from the sequence of images, or frames. More speciﬁcally, we use ConvNets to extract spatial and spectral invariant representations from each frame data, and adopt LSTM network to extract temporal patterns in the frame sequence. Over- all, the proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG data, and to extract features that are more robust to variations and distortions within each dimension. Empirical evaluation on the cognitive load classiﬁcation task demonstrated signiﬁcant improvements over current state-of-the-art approaches in this ﬁeld, reducing the classiﬁcation error from 15.3% (state-of-art on this application) to 8.9%.  2 OUR APPROACH  2.1 MAKING IMAGES FROM EEG TIME-SERIES  Electroencephalogram includes multiple time series corresponding to measurements across differ- ent spatial locations over the cortex. Similar to speech signals, the most salient features reside in frequency domain, usually studied using spectrogram of the signal. However, as already noted, EEG signal has an additional spatial dimension. Fast Fourier Transform (FFT) is performed on the time series for each trial to estimate the power spectrum of the signal. Oscillatory cortical activity related to memory operations primarily exists in three frequency bands of theta (4-7Hz), alpha (8-13Hz), and beta (13-30Hz) (Bashivan et al., 2014; Jensen & Tesche, 2002). Sum of squared absolute values within each of the three frequency bands was computed and used as separate measurement for each electrode.  2  Published as a conference paper at ICLR 2016  Aggregating spectral measurements for all electrodes to form a feature vector is the standard ap- proach in EEG data analysis. However, this approach clearly ignores the inherent structure of the data in space, frequency, and time. Instead, we propose to transform the measurements into a 2-D image to preserve the spatial structure and use multiple color channels to represent the spectral di- mension. Finally, we use the sequence of images derived from consecutive time windows to account for temporal evolutions in brain activity. The EEG electrodes are distributed over the scalp in a three-dimensional space. In order to trans- form the spatially distributed activity maps as 2-D images, we need to ﬁrst project the location of electrodes from a 3-dimensional space onto a 2-D surface. However, such transformation should also preserve the relative distance between neighboring electrodes. For this purpose, we used the Azimuthal Equidistant Projection (AEP) also known as Polar Projection, borrowed from mapping applications (Snyder, 1987). The azimuthal projections are formed onto a plane which is usually tangent to the globe at either pole, the Equator, or any intermediate point. In azimuthal equidistant projection, distances from the center of projection to any other point are preserved. Similarly, in our case the shape of the cap worn on a human’s head can be approximated by a sphere and the same method could be used to compute the projection of electrode locations on a 2D surface that is tangent to the top point of the head. A drawback of this method is that the distances between the points on the map are only preserved with respect to a single point (the center point) and therefore the relative distances between all pairs of electrodes will not be exactly preserved. Applying AEP to 3-D electrode locations, we obtain 2-D projected locations of electrodes (Figure 1). Width and height of the image represent the spatial distribution of activities over the cortex. We apply Clough- Tocher scheme (Alfeld, 1984) for interpolating the scattered power measurements over the scalp and for estimating the values in-between the electrodes over a 32 × 32 mesh. This procedure is repeated for each frequency band of interest, resulting in three topographical activity maps corresponding to each frequency band. The three spatial maps are then merged together to form an image with three (color) channels. This three-channel image is given as an input to a deep convolutional network, as discussed in the following section. Figure 2 illustrates an overview of our multi-step approach to mental state classiﬁcation from EEG data, where the novelty resides in transforming raw EEG into sequence of images, or frames (EEG ”movie”), combined with recurrent-convolutional network architecture applied on top of such transformed EEG data. Note that our approach is general enough to be used in any EEG-based classiﬁcation task, and a speciﬁc problem of mental load classiﬁca- tion presented later only serves as an example demonstrating potential advantages of the proposed approach.  Figure 1: Topology-preserving and non-topology-preserving projections of electrode locations. A) 2-D projection of electrode locations using non-topology-preserving simple orthographic projection. B) Location of electrodes in the original 3-D space. C) 2-D projection of electrode locations using topology-preserving azimuthal equidistant projection.  2.2 ARCHITECTURE  We adopted a recurrent-convolutional neural network to deal with the inherent structure of EEG data. ConvNets were used to deal with variations in space and frequency domains due to their ability to learn good two-dimensional representation of the data. Wherever needed, the extracted representa- tions were fed into another layer to account for temporal variations in the data. We evaluated various types of layers used for extracting temporal patterns, including convolutional and recurrent layers. Essentially, we evaluated the following two primary approaches to the cognitive state classiﬁcation  3  Published as a conference paper at ICLR 2016  Figure 2: Overview of our approach: (1) EEG time series from multiple locations are acquired; (2) spectral power within three prominent frequency bands is extracted for each location and used to form topographical maps for each time frame (image); (3) sequence of topographical maps are combined to form a sequence of 3-channel images which are fed into a recurrent-convolutional network for representation learning and classiﬁcation.  problem. 1) Single-frame approach: a single image was constructed from spectral measurements over the complete trial duration. The constructed image was then used as input to the ConvNet. 2) Multi-frame approach: We divided each trial into 0.5 second windows and constructed an image over each time window, delivering 7 frames per trial (see section 4). The sequence of images was then used as input data to the recurrent-convolutional network. We used Lasagne1 to implement different architectures discussed in this paper. The code necessary for generating EEG images and building and training the networks discussed in this paper is available online2.  2.2.1 CONVNET ARCHITECTURE  We adopted an architecture mimicking the VGG network used in Imagenet classiﬁcation challenge (Simonyan & Zisserman, 2015). This network enjoys a highly scalable architecture which uses stacked convolutional layers with small receptive ﬁelds. All convolutional layers use small receptive ﬁelds of size 3 × 3 and stride of 1 pixel with ReLU activation function. The convolution layer inputs are padded with 1 pixel to preserve the spatial resolution after convolution. Multiple convolution layers are stacked together which are followed by maxpool layer. Max-pooling is performed over a 2 × 2 window with stride of 2 pixels. Number of kernels within each convolution layer increases by a factor of two for layers located in deeper stacks. Stacking of multiple convolution layers leads to effective receptive ﬁeld of higher dimensions while requiring much less parameters (Simonyan & Zisserman, 2015).  2.2.2 SINGLE-FRAME APPROACH  For this approach the single EEG image was generated by applying FFT on the whole trial du- ration (3.5 seconds). Purpose of this approach was to ﬁnd the optimized ConvNet conﬁguration. We ﬁrst studied a simpliﬁed version of the problem by computing the average activity over the complete duration of trial. For this, we computed all power features over the whole duration of trial. Following this procedure, EEG recording for each trial was reduced to a single multi-channel image. We evaluated ConvNet conﬁgurations of various depths, as described in Table 1. The con- volutional layer parameters here are denoted as conv<receptive ﬁeld size>-<number of kernels>. Essentially, conﬁguration A involves only two convolutional layers (Conv3-32) stacked together, followed by maxpool layer; conﬁguration B adds on top of architecture A two more convolutional  1https://github.com/Lasagne/Lasagne 2https://github.com/pbashivan/EEGLearn  4  Published as a conference paper at ICLR 2016  layers (Conv3-64), followed by another maxpool; then conﬁguration C adds one more convolutional layer (Conv3-128) followed by maxpool; conﬁguration D differs from C by using 4 rather than 2 Conv3-32 convolutional layers at the beginning. Finally, a fully-connected layer with 512 nodes (FC-512) is added on top of all these architectures, followed by softmax as the last layer.  Table 1: Evaluated ConvNet conﬁgurations for single-frame approach. The convolutional layer parameters are denoted as conv<receptive ﬁeld size>-<number of kernels>.  ConvNet Conﬁgurations  A  B  input (32 × 32 3-channel image)  C  D  Conv3-32 Conv3-32 Conv3-32 Conv3-32  Conv3-32 Conv3-32  Conv3-32 Conv3-32 Conv3-32 Conv3-32  Conv3-64 Conv3-64  maxpool  Conv3-64 Conv3-64  Conv3-64 Conv3-64 maxpool  Conv3-128 Conv3-128  maxpool  FC-512  softmax  2.2.3 MULTI-FRAME APPROACH  We adopted the best performing ConvNet architecture from single frame approach for each frame. In order to reduce the number of parameters in the network, all ConvNets share parameters across frames. Outputs of all ConvNets are reshaped as sequential frames and used to investigate temporal sequence in maps. We evaluated three approaches to extracting temporal information from sequence of activity maps, inspired by a set of deep learning techniques for video classiﬁcation presented in (Ng et al., 2015); see Figure 3: 1) Max-pooling over time; 2) Temporal convolution; 3) LSTM. Finally, the outputs from the last layer are fed to a fully connected layer with 512 hidden units followed by a four-way softmax layer. We kept the number of neurons in the fully connected layer relatively low to control the total number of parameters in the network. 50% dropout was used on the last two fully connected layers. Max-pooling: This model performs max-pooling over ConvNet outputs across time frames. While representations found from this model preserve spatial location, they are nonetheless order invariant. Temporal convolution: This model applies a 1-D convolution to ConvNet outputs across time frames. We evaluated two models consisting of 16 and 32 kernels of size 3 with stride of 1 frame. Kernels capture distinct temporal patterns across multiple frames. Long Short-Term Memory (LSTM): Recurrent neural networks take input in the shape of a se- quence x = (x1, ..., xT ) and compute hidden vector sequence h = (h1, ..., hT ) and output vector y = (y1, ..., yT ) by iterating the following equations from t = 1 to T :  ht = H(Wxhxt + Whhht−1 + bh) yt = Whyht + by,  (1) (2) where the W terms denote weight matrices, b terms denote bias vectors, and H is the hidden layer function. Given the dynamic nature of neural responses and, consequently, of EEG data, recurrent neural networks (RNN) appear to be a reasonable choice for modeling temporal evolution of brain activ- ity. Long Short-Term Memory (LSTM) model (Hochreiter & Schmidhuber, 1997) is a RNN with  5  Published as a conference paper at ICLR 2016  Figure 3: Different multi-frame architectures; the notation here is as follow. C: 7-layer ConvNet; max: maxpool layer across time frame features; FC: fully-connected layer; SM: softmax layer; Conv: 1-D convolution layer across time frames; L: LSTM layer.  improved memory. It uses memory cells with an internal memory and gated inputs/outputs which have shown to be more efﬁcient in capturing long-term dependencies. The hidden layer function for LSTM is computed by the following set of equations:  it = σ(Wxixt + Whiht−1 + Wcict−1 + bi) ft = σ(Wxf xt + Whf ht−1 + Wcf ct−1 + bf ) ct = ftct−1 + it tanh(Wxcxt + Whcht−1 + bc) ot = σ(Wxoxt + Whoht−1 + Wcoct + bo) ht = ot tanh(ct),  (3) (4) (5) (6) (7) where σ is the logistic sigmoid function, and the components of the LSTM model, referred to as input gate, forget gate, output gate and cell activation vectors are denoted, respectively, as i, f, o, and c (see (Hochreiter & Schmidhuber, 1997) for details). We experimented with up to two LSTM layers and various number of memory cells in each layer and obtained the best results with one layer consisting of 128 cells. Only the prediction made by LSTM after seeing the complete sequence of frames was propagated up to the fully connected layer. We adopted LSTM to capture temporal evolution in sequences of ConvNet activations. Since brain activity is a temporally dynamic process, variations between frames may contain additional infor- mation about the underlying mental state.  2.3 TRAINING  Training is carried out by optimizing the cross-entropy loss function. Weight sharing in ConvNets results in vastly different gradients in different layers and for this reason a smaller learning rate is usually used when applying SGD. We trained the recurrent-convolutional network with Adam algorithm (Kingma & Ba, 2015) with a learning factor of 10−3, and decay rate of ﬁrst and sec- ond moments as 0.9 and 0.999 respectively. Batch size was set to 20. Adam has been shown to achieve competitively fast convergence rates when used for training ConvNets as well as multi-layer neural networks. In addition, VGG architecture requires fewer epochs to converge due to implicit regularization imposed by greater depth and smaller convolution ﬁlter sizes. The large number of parameters existing in our network made it susceptible to overﬁtting. We adopted several measures to address the issue. Dropout (Hinton et al., 2012) with a probability of 0.5 was used in all fully connected layers. Additionally, we used early stopping by monitoring model’s performance over  6  Published as a conference paper at ICLR 2016  a randomly selected validation set. Dropout regularization has proved to be an effective method for reducing the overﬁtting in deep neural networks with millions of parameters (Krizhevsky et al., 2012) and in neuroimaging applications (Plis et al., 2014). Moreover, another commonly used approach for addressing the unbalanced ratio between number of samples and number of model parameters is to artiﬁcially expand the dataset using data augmen- tation. We tried training the network with augmented data generated by randomly adding noise to the images. We did not use image ﬂipping or zooming when augmenting the data due to distinct interpretation of direction and location in EEG images (corresponding to various cortical regions). We experimented with various noise levels added to each image. However, augmenting the dataset did not improve the classiﬁcation performance and for higher noise values increased the error rates. Figure 4 shows the validation loss with number of epochs over the training set. We found that the network parameters converge after about 600 iterations (5 epochs).  Figure 4: Validation loss with training epochs for all cross-validation folds. Thick blue line is the average over all folds.  3 BASELINE METHODS  We compared our approach against various classiﬁers commonly used in the ﬁeld, including Support-Vector Machines (SVM), Random Forest, sparse Logistic Regression, and Deep Belief Networks (DBN). Here we brieﬂy describe some of the details and parameter settings used in those methods. SVM: SVM hyperparameters consisting of regularization penalty parameter (C) and inverse of RBF kernel’s standard deviation (γ = 1/σ) were selected by grid-search through cross-validation on training set (C = {0.01, 0.1, 1, 10, 100}, γ = {0.1, 0.2, ..., 1, 2, ..., 10}). Random Forest: Random forest is an ensemble method consisting of a group of independent ran- dom decision trees. Each tree is grown using a randomly selected subset of features. For each input, outputs of all trees are computed, and the class with majority of votes is selected. The number of estimators for the random forest was varied within the set of {5, 10, 20, 50, 100, 500, 1000}. Logistic Regression: l1-regularization was used to introduce sparsity in the logistic regression model. Optimal regularization parameter C was selected via cross-validation on training set, in which the logarithmic range of [10−2, 103] was searched. Deep Belief Network: We used a three-layer Deep Belief Network (DBN). The ﬁrst layer was a Gaussian-Binary Restricted Boltzman Machine (RBM) and the other two layers were Binary RBMs. The output of the ﬁnal level was fed into a two-way softmax layer for predicting the class label. Parameters of each layer of DBN were greedily pre-trained to improve learning by shifting the initial random parameter values toward a good local minimum (Bengio et al., 2007). We used  7  Published as a conference paper at ICLR 2016  the following empirically selected numbers of neurons in the three layers that demonstrated good performance: 512, 512, and 128. The last layer was connected to a softmax layer with 4 units. The network was ﬁne-tuned using batch stochastic gradient descent with l1-regularization to reduce the overﬁtting during training.  4 EXPERIMENTS ON AN EEG DATASET  Every individual has a different cognitive processing capacity which causally determines his/her ability in performing mental tasks. While human brain consists of numerous networks responsible for specialized tasks, many of them rely on more basic functional networks like working memory. Working memory is responsible for transient retention of information which is crucial for any ma- nipulation of information in the brain. Its capacity sets bounds on individual’s ability in a range of cognitive functions. Increasing cognitive demand (load) beyond individual’s capacity leads to over- load state causing confusion and diminished learning ability (Sweller et al., 1998). For this reason, ability to recognize individual’s cognitive load becomes important for many applications including brain-computer interfaces, human-computer interaction, and tutoring services. Here we used an EEG dataset acquired during a working memory experiment. EEG was recorded as ﬁfteen participants (eight female) performed a standard working memory experiment. Details of procedures for data recording and cleaning are reported in our previous publication (Bashivan et al., 2014). In brief, continuous EEG was recorded from 64 electrodes placed over the scalp at standard 10-10 locations with a sampling frequency of 500 Hz. Electrodes are placed at distances of 10% along the medial-lateral contours. Data for two of the subjects was excluded from the dataset because of excessive noise and artifacts in their recorded data. During the experiment, an array of English characters was shown for 0.5 second (SET) and participants were instructed to memorize the characters. A TEST character was shown three seconds later and participants indicated whether the test character was among the ﬁrst array (’SET’) or not by press of a button. Each participant repeated the experiment for 240 times. The number of characters in the SET for each trial was randomly chosen to be 2, 4, 6, or 8. The number of characters in the SET determines the amount of cognitive load induced on the participant as with increasing number of characters more mental resources are required to retain the information. Throughout the paper, we identify each of the conditions containing 2, 4, 6, 8 characters with loads 1-4 respectively. Recorded brain activity during the period which individuals retained the information in their memory (3.5 seconds) was used to recognize the amount of mental workload. Figure 5 demonstrates the time course of the working memory experiment. The classiﬁcation task is to recognize the load level corresponding to set size (number of characters presented to the subject) from EEG recordings. Four distinct classes corresponding to load 1-4 are deﬁned and the 2670 samples collected from 13 subjects are assigned to these four categories.  Figure 5: Working memory experiment diagram; participants brieﬂy observe an array containing multiple English characters SET (500ms) and maintain the information for three seconds. A TEST character is then presented and participants respond by press of a button if TEST charter matches one of the characters in the SET.  8  Published as a conference paper at ICLR 2016  Continuous EEG was sliced ofﬂine to equal lengths of 3.5 seconds corresponding to each trial. A total of 3120 trials were recorded. Only data corresponding to correctly responded trials were included in the data set which reduced the data set size to 2670 trials. For evaluating the performance of each classiﬁer we followed the leave-subject-out cross validation approach. In each of the 13 folds, all trials belonging to one of the subjects were used as the test set. A number of samples equal to the test set were then randomly extracted from rest of data for validation set and the remaining samples were used as training set.  5 RESULTS  We examined the EEG dataset from two approaches. In the ﬁrst approach (single-frame) we ex- tracted the power features by applying FFT on the complete duration of each trial leading to single 3-channel image corresponding to each trial. The second approach included dividing each trial to multiple time windows and extracting power features for each window separately leading to conser- vation of temporal information rather than averaging them out into single slice of activity map.  5.1 SINGLE-FRAME CLASSIFICATION  We ﬁrst present our results on classiﬁcation using a single frame derived by extracting features over the complete trial duration and applying ConvNets. The purpose of this part was to empirically seek the best performing ConvNet architecture working on images generated from complete EEG time series. We evaluated various conﬁgurations with different number of convolution and maxpool lay- ers. We followed the VGG architecture for selection of number of ﬁlters in each layer and grouping convolution layers with small receptive ﬁelds. Table 1 presented earlier summarized the architectures we considered. Table 2 shows the number of parameters used by each type of architecture, and the corresponding error achieved on the test set. We found ConvNet based architectures to be superior to our baseline methods. We can see that increasing the number of layers to seven slightly improved the achievable error rates on the test set. The best result was obtained with architecture D containing 7 convolution layers which was also marginally better than the baseline methods. While the difference in the error rates between the four conﬁgurations was not statistically signiﬁcant, we chose architecture D because of its equal or better error rates on the subset of subjects that were considered hard to classify (up to 12% decrease in error rates). Most of the network parameters lie in the last two layers (fully connected and softmax) containing approximately 1 million parameters. In VGG style network, the number of ﬁlters in each layer is selected in a way that size of the output remains the same after each stack (ﬁlter size × number of kernels). To quantify the importance of projection type on our results, we additionally generated the images using a simple orthographic projection (onto the z=0 plane) and retrained our network. The differ- ences between topology-preserving and non-topology-preserving projections were mostly evident on the peripheral parts of the projected image (Figure 1). In our experiments we observed slight im- provement of classiﬁcation error in using topology preserving projection over non-equidistant ﬂat- tening projection (∼0.6%). However, this observation could be dependent on the particular dataset and requires further exploration to conclude. Moreover, using the equidistant projection approach helps with the interpretability of images and feature maps when visualizing the data. Overall, our claim is that mapping EEG data into a 2D image (specially with equidistant projections) leads to considerably better classiﬁcation of cognitive load levels as compared to standard, non-spatial ap- proaches that treat EEG simply as a collection of time series.  5.2 MULTI-FRAME CLASSIFICATION  For the multi-frame classiﬁcation, we used ConvNet with architecture D from previous step and applied it on each frame. We explored the four different approaches to aggregate temporal features from multiple frames (Figure 3). Using temporal convolution and LSTM signiﬁcantly improved the classiﬁcation accuracy (see Table 3). For the model with temporal convolution, we found the network consisting of 32 kernels to outperform the one with 16 kernels (11.32% Vs. 12.86% error). A closer look at the accuracies derived for each individual, reveals that while both methods are achieving close to perfect classiﬁcation accuracies for eight of participants, most of the differences  9  Published as a conference paper at ICLR 2016  Table 2: The number of parameters in convolutional layers for evaluated single-frame architectures, and the test errors achieved by each architecture.  Model  Number of parameters Test Error (%)  RBF SVM  L1- Logistic Regression  Random Forest  DBN  ConvNet Arch. A ConvNet Arch. B ConvNet Arch. C ConvNet Arch. D  - - -  428k 10k 65.5k 139.4k 158k  14.68 14.55 14.44 13.59 13.05 13.17 13.91 12.39  originated from differences in accuracy for the remaining ﬁve individuals (Table 4). This observation motivated us to use a combination of temporal convolution and LSTM structures together in single structure which led to our best results on the dataset. Comparing the performance of baseline models in multi-frame and single-frame cases, test errors were slightly lower for the single-frame setup in all classiﬁers except random forest. This difference is mostly because of the negative effect of increased number of features in multi-frame case which happens despite the existence of regularization term in all baseline methods. On the other hand, incorporating temporal dynamics (multiple frames over time) in our model increasingly improves the classiﬁcation performance which demonstrates the effectiveness of our model in learning from time-dependent changes. Moreover, while our approach does not directly operate on raw EEG time- series, we drastically reduced the amount of required data by manually extracting power features from EEG. In addition, discovering complex temporal relationships such as those related to spectral properties in time-series using neural networks, is still an open question which has not been fully addressed. ConvNets attain translation invariance through maxpooling which is a downsampling procedure in nature. While this helps with creating invariant (with respect to space and frequency) feature maps in the deeper layers of ConvNet, it might also hurt the performance if the feature map size is reduced to a degree in which the regional activities cannot be distinguished from each other. In a sense, there is a trade-off between the degree of abstraction realized through layers of convolution and maxpooling and the level of detail kept in the feature maps. In addition, ConvNets learn stack of ﬁlters which produce nonlinear feature maps maximizing the classiﬁcation accuracy. When trained on a pool of data containing multiple individuals, the network extracts features that are maximally informative considering the variability in the training set. We note that performance of ConvNet+Maxpool is lower than ConvNet in single-frame setup. Tem- poral maxpool selects the highest activation across the frames whereas features extracted in the single-frame approach are similar to average values over multiple frames. Choosing the maximum value over multiple time frames is not necessarily the best practice when dealing with brain activity time series as it will potentially ignore the periods of inactivation in some cortical regions. This ef-  Table 3: Classiﬁcation results for multi-frame approach  Architecture  Test Error (%) Validation Error (%) Number of parameters  RBF SVM  L1-Logistic Regression  Random Forest  DBN  ConvNet+Maxpool ConvNet+1D-Conv ConvNet+LSTM  ConvNet+LSTM/1D-Conv  - - -  8.37 8.48 9.28 6.10 8.39  15.34 15.32 12.59 14.96 14.80 11.32 10.54 8.89  10  - - -  1.02 mil 1.21 mil 441 k 1.34 mil 1.62 mil  Published as a conference paper at ICLR 2016  Table 4: Classiﬁcation results for each fold  Test Subject  S1  1D-Conv LSTM Mix  88.3 56.7 88.9  S2  72.5 73.5 76.5  S3  93.9 92.2 93.3  S4  97.5 99 99  S5  98.3 99.4 100  S6  98 99.5 98  S7  98.2 98.9 100  S8  100 100 98.5  S9  98.5 100 99  S10  94.5 97.7 96.8  S11  88.5 99 96.5  S12  79.5 88 91  S13  45.9 59.1 46.8  fect is still partially observable when computing the average of activities over all the frames. It also partially explains lower classiﬁcation errors when temporal dynamic models (1D-conv and LSTM) are added to the network.  5.3 VISUALIZING THE LEARNED REPRESENTATIONS  The recurrent-convolutional network in section 5.2 signiﬁcantly reduced the classiﬁcation error rates in comparison to all baseline methods through automatic learning of representations from the se- quence of EEG images. Understanding how this model achieves such performance is of equal im- portance. Viewing the learned kernels as images is a classic approach for understanding the learned representations by the network. However, in our network, due to the small reception ﬁeld dimension of kernels (3×3), displaying the kernels would not give much intuition about the learned repre- sentations. Instead, we adopted deconvolutional network (deconvnet) (Zeiler et al., 2011; Zeiler & Fergus, 2014; Zeiler et al., 2010) to visualize the model’s learned ﬁlters by back-propagating the feature map onto the input space. Deconvnet iteratively approximates the convnet features in pre- vious layer and collectively projects a particular feature map to the input space. This reveals the structures in the input space that excite that particular feature map. To approximately invert the convolution operation, transpose of ﬁlters is used instead. The transposed ﬁlter is applied to the rectiﬁed map at each stage. Maxpool layers are inverted through a bicubic interpolation operation. We computed the back projections of feature maps derived from the last convolution layer of each stack (corresponding to convolution layers 4, 6, and 7 in architecture ’D’) for all training images. Generally, the lower-level feature maps had a more wide-spread input activation area, while for deeper-layer feature maps the activation areas became sparser. There was also strong frequency selectivity in many of the learned ﬁlters. We found some of these features to have noticeable links to well-known electrophysiological markers of cognitive load. Frontal theta and beta activity as well as parietal alpha are most prominent markers of cognitive/memory load in neuroscience literature (Bashivan et al., 2015; Jensen et al., 2002; Onton et al., 2005; Tallon-Baudry et al., 1999). Figure 6 illustrates the back-projected maps for a number of ﬁlters with clear neuroscientiﬁc interpretations selected from different depths of the network. For each ﬁlter we showed the input image, ﬁlter output and the back projected activation for 9 images with highest activations (average activation over all feature map pixels) across the training set. Among the ﬁrst-layer features we found one feature map capturing wide-spread theta (1st stack output-kernel7) and another frontal beta activity (1st stack output-kernel23). In the second- and third-layer features we observed detectors of frontal theta/beta (2nd stack output-kernel7 and 3rd stack output-kernel60, 112) as well as parietal alpha (2nd stack output-kernel29) with increasing focal speciﬁcity of feature maps in deeper layers. The similarity between feature maps derived from different images is noticeable despite considerable dissimilarity in the original input images.  6 CONCLUSIONS  This work is motivated by the high-level goal of ﬁnding robust representations from EEG data, that would be invariant to inter- and intra-subject differences and to inherent noise associated with EEG data collection. We propose a novel methodology for learning representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classiﬁcation task. Our approach is fundamentally different from the previous attempts to learn high-level representa- tions from EEG using deep neural networks. Speciﬁcally, rather than representing low-level EEG features as a vector, we transform the data into a sequence of topology-preserving multi-spectral images (EEG ”movie”), as opposed to standard EEG analysis techniques that ignore such spatial information. We then train deep recurrent-convolutional networks inspired by state-of-the-art video  11  Published as a conference paper at ICLR 2016  Figure 6: Visualization of feature maps and their input activation patterns at various depth levels of convolutional network. The left column (Input EEG images) shows the top 9 images with highest feature activations across the training set. The middle column (Feature Maps) shows the feature map derived in the output of the particular kernel. Right column (Back Projections) shows the back- projected maps derived by applying deconvnet on the feature map displaying structures in the input image that excite that particular feature map.  classiﬁcation to learn robust representations from the sequence of images. The proposed approach demonstrates signiﬁcant improvements in classiﬁcation accuracy over the state-of-the-art results. Since our approach transforms the EEG data into sequence of EEG images, it can be applied on EEG data acquired with different hardware (e.g. with different number of electrodes). The prepro- cessing step used in our approach transforms the EEG time-series acquired from various sources into comparable EEG frames. In this way, various EEG datasets could be merged together. The only information needed to complete this transform would be the spatial coordinates of electrodes for each setup. As a future direction, it would be possible to use unsupervised pretraining methods with larger (or merged) unlabeled EEG datasets prior to training the network with task-speciﬁc data.  12  Published as a conference paper at ICLR 2016  REFERENCES Alfeld, Peter. A trivariate cloughtocher scheme for tetrahedral data. Computer Aided Geometric  Design, 1(2):169–181, 1984.  Bashivan, Pouya, Bidelman, Gavin M., and Yeasin, Mohammed. Spectrotemporal dynamics of the EEG during working memory encoding and maintenance predicts individual behavioral capacity. European Journal of Neuroscience, 40(12):3774–3784, 2014.  Bashivan, Pouya, Yeasin, Mohammed, and M., Bidelman Gavin. Single trial prediction of normal and excessive cognitive load through eeg feature fusion. Proceedings of IEEE Signal Processing in Medicine and Biology (SPMB) conference, pp. 1–5, December 2015.  Bengio, Yoshua, Lamblin, Pascal, Popovici, Dan, and Larochelle, Hugo. Greedy layer-wise training ISSN  of deep networks. Advances in neural information processing systems, 19:153, 2007. 1049-5258.  Cecotti, Hubert and Gr¨aser, Axel. Convolutional neural networks for P300 detection with application to brain-computer interfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):433–445, 2011. ISSN 01628828. doi: 10.1109/TPAMI.2010.125.  Graves, Alex, Fern´andez, S, and Liwicki, Marcus. Unconstrained online handwriting recognition with recurrent neural networks. Advances in Neural Information Processing Systems, 20:1–8, 2008.  Graves, Alex, Mohamed, Abdel-Rahman, and Hinton, Geoffrey E. Speech recognition with deep recurrent neural networks. Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE In- ternational Conference on, pp. 6645–6649, 2013. ISSN 1520-6149. doi: 10.1109/ICASSP.2013. 6638947.  Guler, N, Ubeyli, E, and Guler, I. Recurrent neural networks employing Lyapunov exponents for EEG signals classiﬁcation. Expert Systems with Applications, 29(3):506–514, 2005. ISSN 09574174. doi: 10.1016/j.eswa.2005.04.011.  Hermann, Karm Moritz, Koˇcisk´y, Tom´aˇs, Grefenstette, Edward, Espeholt, Lasse, Kay, Will, Su- leyman, Mustafa, and Blunsom, Phil. Teaching Machines to Read and Comprehend. arXiv, pp. 1–13, 2015.  Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Rus- lan R. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long Short-Term Memory. Neural Computation, 9(8):  1735–1780, 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735.  Jensen, Ole and Tesche, Claudia D. Frontal theta activity in humans increases with memory load in a working memory task. Neuroscience, 15(8):1395–1399, 2002. ISSN 0953816X. doi: 10.1046/ j.1460-9568.2002.01975.x.  Jensen, Ole, Gelfand, Jack, Kounios, John, and Lisman, John E. Oscillations in the alpha band (9-12 Hz) increase with memory load during retention in a short-term memory task. Cerebral cortex (New York, N.Y. : 1991), 12(8):877–82, aug 2002. ISSN 1047-3211.  Karpathy, Andrej and Toderici, G. Large-scale video classiﬁcation with convolutional neural doi:  networks. Computer Vision and Pattern Recognition (CVPR), pp. 1725–1732, 2014. 10.1109/CVPR.2014.223.  Kingma, Diederik P. and Ba, Jimmy Lei. Adam: a Method for Stochastic Optimization. Interna-  tional Conference on Learning Representations, pp. 1–13, 2015.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con- volutional neural networks. Advances in neural information processing systems, pp. 1097–1105, 2012. ISSN 10495258.  13  Published as a conference paper at ICLR 2016  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2323, 1998. ISSN 00189219. doi: 10.1109/5.726791.  Lotte, F and Congedo, M. A Review of Classiﬁcation Algorithms for EEG-based Brain-Computer  Interfaces. Journal of neural engineering, 4:1–24, 2007.  Mirowski, Piotr, Madhavan, Deepak, LeCun, Yann, and Kuzniecky, Ruben. Classiﬁcation of patterns of EEG synchronization for seizure prediction. Clinical Neurophysiology, 120(11):1927–1940, 2009. ISSN 13882457. doi: 10.1016/j.clinph.2009.09.002.  Ng, Jyh, Hausknecht, M, Vijayanarasimhan, S, Vinyals, O, Monga, R, and Toderici, G. Beyond Short Snippets: Deep Networks for Video Classiﬁcation. In CVPR, 2015. ISBN 9781467369640.  Onton, Julie, Delorme, Arnaud, and Makeig, Scott. Frontal midline EEG dynamics during working memory. NeuroImage, 27(2):341–56, aug 2005. ISSN 1053-8119. doi: 10.1016/j.neuroimage. 2005.04.014.  Plis, Sergey M., Hjelm, Devon R., Salakhutdinov, Ruslan, Allen, Elena a., Bockholt, Henry J., Long, Jeffrey D., Johnson, Hans J., Paulsen, Jane S., Turner, Jessica a., and Calhoun, Vince D. Deep learning for neuroimaging: a validation study. Frontiers in Neuroscience, 8(August):1–11, 2014. ISSN 1662-453X. doi: 10.3389/fnins.2014.00229.  Simonyan, K and Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recog-  nition. In ICLR, pp. 1–14, 2015.  Snyder, John Parr. Map projections–A working manual, volume 1395. US Government Printing  Ofﬁce, 1987.  Subasi, Abdulhamit and Ismail Gursoy, M. EEG signal classiﬁcation using PCA, ICA, LDA and support vector machines. Expert Systems with Applications, 37(12):8659–8666, dec 2010. ISSN 09574174. doi: 10.1016/j.eswa.2010.06.065.  Sweller, John, Merrienboer, Jeroen J G Van, and Paas, Fred G W C. Cognitive architecture and  instructional design. Educational Psychology Review, 10(3):251–296, 1998.  Tallon-Baudry, Catherine, Kreiter, Andreas, and Bertrand, Olivier. Sustained and transient oscilla- tory responses in the gamma and beta bands in a visual short-term memory task in humans. Visual neuroscience, 16(03):449–459, 1999.  Zeiler, Matthew D, Krishnan, Dilip, Taylor, Graham W, and Fergus, Rob. Deconvolutional networks. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 2528–2535. IEEE, 2010.  Zeiler, Matthew D., Taylor, Graham W., and Fergus, Rob. Adaptive deconvolutional networks for mid and high level feature learning. Proceedings of the IEEE International Conference on Com- puter Vision, pp. 2018–2025, 2011. ISSN 1550-5499. doi: 10.1109/ICCV.2011.6126474.  Zeiler, Md and Fergus, Rob. Visualizing and understanding convolutional networks. Com- ISSN 978-3-319-10589-5. doi: 10.1007/  puter Vision (ECCV) 2014, 8689:818–833, 2014. 978-3-319-10590-1{\ }53.  Zhang, Xiang and LeCun, Yann.  arXiv:1502.01710, 2015.  Text Understanding from Scratch.  arXiv preprint  14  Published as a conference paper at ICLR 2016  APPENDIX  An example demonstrating potentially high inter- and intra-subject variability of observed responses from different individuals performing same task, as well as different runs for the same subject per- forming the task several times, is shown in Figures 7a and 7b, respectively (for more details on this experiment, see section 4). More speciﬁcally, Figure 7a demonstrates the average frames obtained from two subjects within the same condition. Evidently, there are large inter-subject variations in the patterns emerging from average frames. Similarly, high variations could exist in responses recorded during multiple runs of the same task from the same subject, as shown in Figure 7b.  Figure 7: A: Average frames obtained over multiple runs, under the same exact condition (same cognitive load level of the working memory task) from two different subjects (S2 and S3). B: multiple runs for the same condition (task and load level) for the same subject. For more detail, see section 4.  15  ",
1508.01983,2016,Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance,"['Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance\nAmr Bakry', 'Mohamed Elhoseiny', 'Tarek El-Gaaly', 'Ahmed Elgammal']",https://arxiv.org/pdf/1508.01983,"6 1 0 2     n u J    0 2      ]  V C . s c [      4 v 3 8 9 1 0  .  8 0 5 1 : v i X r a  Published as a conference paper at ICLR 2016  DIGGING DEEP INTO THE LAYERS OF CNNS: IN SEARCH OF HOW CNNS ACHIEVE VIEW INVARIANCE  amrbakry@cs.rutgers.edu  Amr Bakry∗ Tarek El-Gaaly∗  tgaaly@cs.rutgers.edu  Mohamed Elhoseiny∗  m.elhoseiny@cs.rutgers.edu  Ahmed Elgammal  elgammal@cs.rutgers.edu  * indicates Co-ﬁrst authors  Computer Science Department, Rutgers University  ABSTRACT  This paper is focused on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNN). There are several questions that this paper aims to answer: Does the learned CNN rep- resentation achieve viewpoint invariance? How does it achieve viewpoint invari- ance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can the struc- ture of the view manifold at each layer of a deep convolutional neural network be quantiﬁed experimentally? How does ﬁne-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network? In order to answer these questions we propose a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. We apply this methodology and report interesting results in this paper that answer the aforementioned questions.  1  INTRODUCTION  Impressive results have been achieved recently with the application of Convolutional Neural Net- works (CNNs) in the tasks of object categorizations (Krizhevsky et al., 2012) and detection (Ser- manet et al., 2013; Girshick et al., 2013). Several studies recently investigated different properties of the learned representations at different layers of the network, e.g. (Yosinski et al., 2014; Zeiler & Fergus, 2013; Chatﬁeld et al., 2014). One fundamental question is how CNN models achieve dif- ferent invariances. It is well understood that consecutive convolution and pooling layers can achieve translation invariant. Training CNN networks with a large dataset of images, with arbitrary view- points and arbitrary illumination, while optimizing the categorization loss helps to achieve viewpoint invariant and illumination invariant. In this paper we focus on studying the viewpoint invariant properties of CNNs. In many applications, it is desired to estimate the pose of the object, for example for robot manipulation and scene under- standing. Estimating pose and object categorization are tasks that contradict each other; estimating pose requires a representation capable of capturing the viewpoint variance, while viewpoint invari- ance is desired for categorization. Ultimately, the vision system should achieve a representation that can factor out the viewpoint for categorization and preserve viewpoint for pose estimation. The biological vision system is able to recognize and categorize objects under wide variability in visual stimuli, and at the same time is able to recognize object pose. It is clear that images of the same object under different variability, in particular different views, lie on a low-dimensional manifold in the high-dimensional visual space deﬁned by the retinal array (∼100 million photoreceptors and ∼1 million retinal ganglion cells). DiCarlo & Cox (2007) hypothesized that the ability of our brain to recognize objects, invariant to different viewing conditions, such as viewpoint, and at the same time estimate the pose, is fundamentally based on untangling the visual manifold encoded in neural population in the early vision areas (retinal ganglion cells, LGN, V1). They suggested that this is achieved through a series of successive transformation (re-representation) along the ventral stream (V1,V2, V4, to IT) that leads to an untangled population at IT. Despite this, it is unknown how the ventral stream achieves this untangling. They argued that since IT population supports tasks other  1  Published as a conference paper at ICLR 2016  Figure 1: Illustration of DiCarlo and Cox model (DiCarlo & Cox, 2007): Left: tangled manifolds of different objects in early vision areas. Right: untangled (ﬂattened) manifold representation in IT  than recognition, such as pose estimation, the manifold representation is some how ’ﬂattened’ and ’untangled’ in the IT layer. DiCarlo and Cox’s hypothesis is illustrated in Figure 1. They stress that the feedforward cascade of neural re-representation is a way to untangle the visual manifold. Inspired by recent impres- sive results of CNNs and by DiCarlo and Cox’s hy- pothesis (DiCarlo & Cox, 2007) on manifold untan- gling, this paper focuses on studying the view-manifold structure in the feature spaces implied by the dif- ferent layers of CNNs. There are several ques- tions that this paper aims to answer: 1. Does the learned CNN representa- tions achieve viewpoint in- variance? If so, how does it achieve viewpoint invari- ance? Is it by collapsing the view manifolds, or separating them while preserving them? At which layer is the view invariance achieved? 2. How to experimentally quantify the structure of the view- point manifold at each layer of a deep convolutional neural network? 3. How does ﬁne-tuning of a pre-trained CNN, optimized for categorization, on a multi-view dataset, affect the representation at each layer of the network? In order to answer these questions, we present a methodology that helps to get an insight about the structure of the viewpoint manifold of different objects as well as the combined object-view manifold in the layers of CNN. We conducted a series of experiments to quantify the ability of different layers of a CNN to either preserve the view-manifold structure of data or achieve a view- invariant representation. The contributions of the paper are as follows: (1) We propose a methodology to quantify and get insight into the manifold structures in the learned representation at different layers of CNNs. (2) We use this methodology to analyze the viewpoint manifold of pre-trained CNNs. (3) We study the effect of transfer learning a pre-trained network with two different objectives (optimizing category loss vs. optimizing pose loss) on the representation. (4) We draw important conclusions about the structure of the object-viewpoint manifold and how it coincides with DiCarlo and Cox’s hypothesis. The paper begins by reviewing closely related works. Section 3 deﬁnes the problem, experimental setup, and the basic CNN network that our experiments are based upon. Section 4 introduces our methodology of analysis. Sections 5 and 6 describe the ﬁndings on the pre-trained network and the ﬁne-tuned networks respectively. The conclusion section summarizes our ﬁndings.  2 RELATED WORK  LeCun et al. has widely used CNNs for various vision tasks (Sermanet et al., 2013; Kavukcuoglu et al., 2010; Jarrett et al., 2009; Ranzato et al., 2007; LeCun et al., 2004). The success of CNNs can be partially attributed to these efforts, in addition to training techniques that have been adopted. Krizhevsky et al. (2012) used a CNN in the ImageNet Challenge 2012 and achieved state-of-the-art accuracy. Since then, there have been many variations in CNN architectures and learning techniques within different application contexts. In this section we mainly emphasize related works that focused on bringing an understanding of the representation learned at the different layers of CNNs and related architectures. Yosinski et al. (2014) studied how CNN layers transition from general to speciﬁc. An important ﬁnding in this study is that learning can be transferred, and by using ﬁne-tuning, performance is boosted on novel data. Other transfer learning examples include (Razavian et al., 2014; Donahue et al., 2013; Agrawal et al., 2014). Zeiler & Fergus (2013) investigated the properties of CNN  2  Published as a conference paper at ICLR 2016  layers for the purpose of capturing object information. This study is built on the premise that there is no coherent understanding of why CNNs work well or how we can improve them. Interesting visualizations were used to explore the functions of layers and the intrinsics of categorization. The study stated that CNN output layers are invariant to translation and scale but not to rotations. The study in (Chatﬁeld et al., 2014) evaluated different deep architectures and compared between them. The effect of the output-layer dimensionality was explored.  3 PROBLEM DEFINITION AND EXPERIMENTAL SETUP  It is expected that multiple views of an object lie on intrinsically low-dimensional manifolds (view manifold1) in the input space. View manifolds of different instances and different objects are spread out in this input space, and therefore form jointly what we call the object-view manifold. The input space here denotes the RN×M space induced by an input image of size N × M, which is analogous to the retinal array in the biological system. For the case of a viewing circle(s), the view manifold of each object instance is expected to be a 1-dimensional closed curve in the input space. The recovery of the category and pose of a test image reduces to ﬁnding which of the manifolds this image belongs to, and what is the intrinsic coordinate of that image within that manifold. This view of the problem is shared among manifold-based approaches such as (Murase & Nayar., 1995; Zhang et al., 2013; Bakry & Elgammal, 2014) The ability of a vision system to recover the viewpoint is directly related to how the learned rep- resentation preserves the view manifold structure. If the transformation applied to the input space yields a representation that results in collapsing the view manifold, the system will no longer be able to discriminate between different views. Since each layer of a deep NN re-represents the input in a new feature space, the question would be how the re-representations deform a manifold that already exists in the input space. A deep NN would satisfy the hypothesis of ’ﬂattening’ and ’untangling’ by DiCarlo & Cox (2007), if the representation in a given layer separates the view manifolds of different instances, without collapsing them, in a way to be able to put a separating hyperplanes be- tween different categories. Typically CNN layers exhibit general-to-speciﬁc feature encoding, from Gabor-like features and color blobs at low layers to category-speciﬁc features at higher layers (Zeiler & Fergus, 2013). We can hypothesize that for the purpose of pose estimation, lower layers should hold more useful representations that might preserve the view manifold and be better for pose esti- mation. But which of these layers would be more useful, and where does the view-manifold collapse to view-invariance. There are different hypotheses we can make about how the view mani- folds of different objects are arranged in the feature space of a given layer. These hypotheses are shown in Fig- ure 2. We arrange these hypothe- ses based on linear separability of the different objects’ view manifolds and the preservation of the view mani- folds. Case 0 is the non-degenerate case where the visual manifolds pre- serve the pose information but are tangled and there is no linear sepa- ration between them (this might re- semble the input space, similar to left case in Figure 1). Case 1 is the ulti- mate case where the view manifolds of different objects are preserved by the transformation and are separable (similar to the right case in Figure 1). Case 2 is where the transformation in the network leads to separation of the object’s view manifold at the expense of collapsing these manifolds to achieve view invariance. Collapsing of the manifolds can be to differ- ent degrees, to the point where each object’s view manifold can be mapped to a single point. Case 3  Figure 2: Sketches of four hypotheses about possible structures of the view manifolds of two objects in a given feature space.  1we use the terms view manifold and viewpoint manifold interchangeably  3  Published as a conference paper at ICLR 2016  Figure 3: KNN Tradeoffs: accuracy tradeoff between category and pose estimation using KNN. This cartoon illustrates the global measurements, see Section 4.2 for full details.  is where the transformation results in more tangled manifolds (pose collapsing and non-separable). It is worth to notice that both cases 1 and 2 are view invariant representations. However, it is obvi- ous that case 1 would be preferred since it also facilitates pose recovery. It is not obvious whether optimizing a network with a categorization loss result in case 1 or case 2. Getting an insight about which of these hypotheses are true in a given layer of a CNN is the goal of this paper. In Section 4 we propose a methodology to get us to that insight.  3.1 EXPERIMENTAL SETTINGS  To get an insight into the representations of the different layers and answer the questions posed in Section 1 we experiment on two datasets: I) RGB-D dataset (Lai et al., 2011), II) Pascal3D+ dataset (Xiang et al., 2014). We selected the RGB-D dataset since it is the largest available multiview dataset with the most dense viewpoint sampling. The dataset contains 300 instances of tabletop objects (51 categories). Objects are set on a turntable and captured by an Xbox Kinect sensor (Kinect 2010) at 3 heights (30◦, 45◦ and 60◦ elevation angles). The dense view sampling along each height is essential for our study to guarantee good sampling of the view manifold. We ignore the depth channel and only used the RGB channels. Pascal3D+ is very challenging because it consists of images “in the wild”, in other words, images of object categories exhibiting high variability, captured in uncontrolled settings and under many different poses. Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset (Everingham et al., 2010). These objects are annotated with 3D pose information (i.e, azimuth, elevation and distance to camera). Pascal3D+ also adds 3D annotated images of these 12 categories from the ImageNet dataset (Deng et al., 2009). The bottle category is omitted in state- of-the-art results. This leaves 11 categories to experiment with. There are about 11,500 and 7,000 training images in ImageNet and Pascal3D+ subsets, respectively. For testing, there are about 11,200 and 6,900 testing images for ImageNet and Pascal3D+, respectively. On average there are about 3,000 object instances per category in Pascal3D+, making it a challenging dataset for estimating object pose. The two datasets provide different aspect of the analysis. While the RGB-D provides dense sampling of each instance’s view manifold, Pascal3D+ dataset contains only very sparse sampling. Each instance is typically imaged from a single viewpoint, with multiple instances of the same category sampling the view manifold at arbitrary points. Therefore, in our analysis we use the RGB-D dataset to analyze each instance viewpoint manifold and the combined object-viewpoint manifolds, while the Pascal3D provides analysis of the viewpoint manifold at the category level. Evaluation Split: For our study, we need to make sure that the objects we are dealing with have non-degenerate view manifolds. We observed that many of the objects in the RGB-D dataset are ill-posed, in the sense that the poses of the object are not distinct. This happens when the objects  4  Published as a conference paper at ICLR 2016  have no discriminating texture or shape to be able to identify the different poses (e.g. a texture-less ball, apple or orange on a turntable). This will cause view manifold degeneracy. Therefore we select 34 out of the 51 categories as objects that possess pose variations across the viewpoints, and thus are not ill-posed with respect to pose estimation. We split the data into training, validation and testing. Since in this datasets, most categories have few instances, we left out two random object instances per category, one for validation and one for testing. In the case where a category has less than 5 instances, we form the validation set for that category by randomly sampling from the training set. Besides the instance split, we also left out all the middle height for testing. Therefore, the testing set is composed of unseen instances and unseen heights and this allows us to more accurately evaluate the capability of the CNN architectures in discriminating categories and estimating pose of tabletop objects.  3.2 BASE NETWORK: MODEL0  The base network we use is the Convolutional Neural Network described in Krizhevsky et al. (2012) and winner of LSVRC-2012 ImageNet challenge (Russakovsky et al., 2014). The CNN was com- posed of 8 layers (including 1000 neuron output layer corresponding to 1000 classes). We call these layers in order: Conv1, Pool1, Conv2, Pool2, Conv3, Conv4, Conv5, Pool5, FC6, FC7, FC8 where Pool indicates Max-Pooling layers, Conv indicates layers performing convolution on the previous layer and FC indicates fully connected layer. The last fully connected layer (FC8) is fed to a 1000- way softmax, which produces a distribution over the category labels of the dataset.  4 METHODOLOGY  The goal of our methodology is two-folds: (1) study the transformation that happens to the viewpoint manifold of a speciﬁc object instance at different layers, (2) study the structure of the combined object-view manifold at each layer to get an insight about how tangled or untangled the different objects’ viewpoint manifolds are. Both these approaches will get us an insight to which of the hypotheses explained in Section 3 is correct at each layer, at least relatively by comparing layers. This section introduces our methodology, which consists of two sets of measurements to address the aforementioned two points. First, we introduce instance-speciﬁc measurements that quantify the viewpoint manifold in the different layers to help understand whether the layers preserve the manifold structure. We performed extensive analysis on synthetic manifold data to validate the measures, see Appendix C. Second, we introduce empirical measurements that are designed to draw conclusions about the global object-viewpoint manifold (involving all instances).  4.1  INSTANCE-SPECIFIC VIEW MANIFOLD MEASUREMENTS  Let us denote the input data (images taken from a viewing circle and their pose labels) for a speciﬁc object instance as {(xi ∈ RD, θi ∈ [0, 2π]), i = 1··· N}, where D denotes the dimensionality of the input image to the network, and N is the number of the images, which are equally spaced around the viewing circle. These images form the view manifold of that object in the input space denoted by M = {xi}N 1 . Applying each image to the network will result in a series of nonlinear transformations. Let us denote the transformation from the input to layer l by the function fl(x) : RD → Rdl where dl is the dimensionality of the feature space of layer l. With an abuse of notation we also denote the transformation that happens to the manifold M at layer l by Ml = fl(M) = 1 . After centering the data by subtracting the mean, let Al = [ ´fl(xi)··· ´fl(xN )] be the {fl(xi)}N centered feature matrix at layer l of dimension dl×N, which corresponds to the centered transformed images of the given object. We call Al the sample matrix in layer l. Since the dimensionality dl of the feature space of each layer varies, we need to factor out the effect of the dimensionality. Since N (cid:28) dl the transformed images on all the layers lie on subspaces of dimension N in each of the feature spaces. Therefore, we can change the bases to describe the samples using N dimensional subspace, i.e, we deﬁne the N × N matrices ˆAl = UTA where U ∈ Rdl×N are the orthonormal bases spanning the column space of Al (which we can get by SVD of Al = USVT). This projection rotates the samples at each layer without changing the manifold geometric or neighborhood properties. Then the following measures will be applied to  5  Published as a conference paper at ICLR 2016  the N transformed images, representing the view manifold of each object instance individually. To obtain an overall measures for each layer we will average these measures over all the object instances.  1) Measure of spread - Nuclear Norm: There are several possible measures of the spread of the data √ in the sample matrix of each view manifold. We use the nuclear norm (also known as the trace norm (Horn)) deﬁned as ||A||∗ = T r( i=1 σi, i.e, it measures the sum of the singular values of A.  ATA) =(cid:80)N  2) Subspace dimensionality measure - Effective-p: counts the effective dimensionality of the sub- space where the view manifold lives. Smaller number means that the view manifold lives in lower dimensional subspace. We deﬁne Effective-p as the minimum number of singular val- ues (in decreasing order) that sum up to more that or equal to p% of the nuclear norm, i.e,  Effective − p = sup{n :(cid:80)n  i=1 σi/(cid:80)N  i=1 σi ≤ p/100}.  3) Alignment Measure - KTA: Ideally the view manifold resulting of the view sitting of the stud- ied datasets is a single-dimensional closed curve in the feature space, which can be thought as a deformed circle (Zhang et al., 2013). This manifold can be degenerate in the ultimate case to a single point in case of a texture-less object. The goal of this measurement is to quantify how the transformed manifold locally preserves the original manifold structure. To this end we compare the kernel matrix of the transformed manifold at layer l, denote by Kl n, with the kernel matrix of the an embedding of the ideal view manifold on unit circle, denote by K◦ n, where n indicates the local neighborhood size used in constructing the kernel matrix. We construct the neighborhood based on pose labels. Given these two kernel matrices we can deﬁne several convergence measures. We use Kernel Target Alignment (KTA) which has been used in the literature for kernel learning (H et al., 1996). It ﬁnds a scale invariant dependency between two normalized kernel matrices2. Therefore, we deﬁne the alignment of the transformed view manifold Ml at layer l with the ideal manifold as KT An(Ml) = < Kl  n >F /(||Kl  n||F||K◦  n||F ).  n, K◦  4) KPLS-regression measures: Kernel Partial Least Squares (KPLS) (Rosipal & Trejo, 2002) is a supervised regression method. KPLS iteratively extracts the set of principal components of the input n from kernel that are most correlated with the output . We use KPLS to learn mapping Kl the transformed view manifold kernel (input kernel) to the unit circle kernel (output kernel). We enforce this mapping to use maximum of d (cid:28) N principal components (we used d = 5). Then we deﬁne KPLS-Regression Error, which uses the Normalized Cross Correlation to quantify the mapping correctness.  n → K◦  5) TPS-linearity measure: In this measure we learn a regularized Thin Plate Spline (TPS) non- linear mapping (Duchon, 1977) between the unit circle manifold and each Ml. The reason for using TPS in particular is that the mapping has two parts: afﬁne (linear polynomial) and nonlinear part. Analysis of the two parts will tell us if the mapping is mostly linear or nonlinear. We use the reciprocal-condition number (rcond) of the sub coefﬁcient matrices corresponding to the afﬁne and the nonlinear part as a measure of the linearity of the transformation. 3  4.2 GLOBAL OBJECT-VIEWPOINT MANIFOLD MEASURES  To achieve an insight about the global arrangement of the different objects’ view-manifolds in a given feature (layer) space, we use the following three empirical measurements:  6) Local Neighborhood Analysis: To evaluate the local manifold structure we also evaluate the performance of nearest neighbor classiﬁers for both category and pose estimation, with varying size of the neighborhood. This directly tell us whether the neighbors of a given point are from the same category and/or of similar poses. KNN for categorization cannot tell us about the linear separability of classes. However evaluating the pose estimation in neighborhood of a datapoint gives  2We also experimented with HSIC (Gretton et al., 2005b), however HSIC is not scale invariant and not  designed to compare data in different feature spaces. Therefore, HSIC did not give any discriminative signal  3More details and deﬁnitions about KPLS and TPS based measurements in Appendix D.  6  Published as a conference paper at ICLR 2016  us an insight about how the view manifolds are preserved, and even whether the view manifolds of different instances are aligned. To achieve this insight we use two different measurements: KNN- Accuracy: the accuracy of KNN classiﬁers for category and pose estimation. KNN-Gap: the drop in performance of each KNN classiﬁer as the neighborhood size increases. In our experiments we increase K from 1 to 9. Positive gap indicates a drop (expected) and negative gap indicates improvement in performance. The interaction between these two measures and how they tell us about the manifold structure is illustrated in Fig 3. The contrast between the accuracy of the KNN classiﬁers for pose and category directly implies which of the hypotheses in Figure 2 is likely. The analysis of KNN-Gap (assuming good 1-NN accuracy) gives further valuable information. As the KNN-gap reaches zero in both category and pose KNN classiﬁers, this implies that neighborhoods are from the same category and has the same pose, which indicates that the representation aligns the view manifolds of different instances of the same category. If the view manifolds of such instances are preserved and separated in the space, and the neighbors of a given point are from the same instance, this would imply small gap in the category KNN classiﬁer and bigger gap in pose KNN classiﬁer. Low gap in pose KNN vs high gap in category CNN implies the representation aligns view manifolds of instances of different categories. A high gap in both obviously implies the representation is tangling the manifolds such that a small neighborhood contains points from different categories and different poses. Notice that this implications are only valid when the 1-NN accuracy is high.  7) L-SVM: For a test image x transformed to the l-th layer’s feature space, fl(x), we compute the performance of a linear SVM classiﬁer trained for categorization. Better performance of such a classiﬁer directly implies more linear separability between different view manifolds of different categories.  8) Kernel Pose Regression: To evaluate whether the pose information is preserved in a local neigh- borhood of a point in a given feature space we evaluate the performance of kernel ridge regression for the task of pose estimation. Better performance implies better pose-preserving transformation, while poor performance indicates pose-collapsing transformation. The combination of L-SVM and kernel regression should be an indication to which of the hypotheses in Figure 2 is likely to be true.  5 ANALYSIS OF THE PRE-TRAINED NETWORK 5.1  INSTANCE VIEW MANIFOLD ANALYSIS  Figure 4 shows the application of the instance-speciﬁc view manifold measurements on the images of the RGBD dataset when applied to a pre-trained network (Model0 - no ﬁne-tuning). This gives us an insight on the transformation that happens to the view manifold of each object instance at each layer of the network. Figure 4a shows that the nuclear norm of the transformed view manifolds in Model0 is almost monotonically decreasing as we go higher in the network, which indicates that the view manifolds is more spread in the lower layers. In fact at the output layer of Model0 the nuclear norm becomes too small, which indicates that the view manifold is collapsing to reach view invariant representation at this layer. Figure 4b (p = 90%) shows that subspace dimension varies within a small range in the lower layers and it reduces dramatically in fully connected layers, which indicates that the network tries to achieve view invariance. The minimum is achieved at FC8 (even without ﬁne tuning). Figure 4c shows the KTA applied to Model0, where we can notice that the alignment is almost similar across the lower layers, with Pool5 having the maximum alignment, and then starts to drop at the very high layers. which indicates that after Pool5, the FC layers try to achieve view invariant. Fig 4d shows that KPLS regression error on Model0 dramatically reduces from FC8 down to Pool5, where Pool5 has the least error. In general the lower layers have less error. This indicates that the lower layers preserve higher correlation with the ideal manifold structure. Fig 4e shows that the mapping is highly linear, which is expected because of the high dimensionality of the feature spaces. From Fig 4e we can clearly notice that the lower layers has more better-conditioned linear mapping (plots for the nonlinear part is in Appendix D.) From these measurements we can conclude: (1) The lower layers preserve the view manifolds. The manifolds start to collapse in the FC layers to achieve view invariance. Preserving the view manifold at the lower layers is intuitive because of the nature of the convolutional layers. (2) The manifold at Pool5 achieves the best alignment with the pose labels. This is a less intuitive result; why does  7  Published as a conference paper at ICLR 2016  (a) Nuclear Norm  (b) Effective 90% SV’s  (c) KTA  (d) KPLS-Reg err  (e) TPS-RCond(poly)  Figure 4: RGB-D: Local Measurement analysis for the view-manifold. Every ﬁgure shows single measure- ment for three models (Model0, Model1Cat and Model1Pose) at different layers.  the representation after successive convolutions and pooling improves the view manifold alignment? even without seeing any dense view manifold in training, and even without any pose labels being involved in the loss. The hypotheses we have to justify that Pool5 has better alignment than the lower layers is that Pool5 has better translation invariant properties, which results in improvement of the view manifold alignment.  5.2 GLOBAL OBJECT-VIEW MANIFOLD ANALYSIS  To study view-manifold in the network layers, Figure 5 shows the KNN accuracy for pose and category within training split, no test is used in this experiment. The category gap is reducing as we go up in the network up to FC7 (almost 0 gap at FC6 and FC7). In contrast the gap is large at all layers for pose estimation. This indicates separation of the instances’ view manifolds where the individual manifolds are not collapsed (This is why as we increase the neighborhood, the category performance stays the same while pose estimation decreases smoothly - See Figure 3-right for illustration). The results above consistently imply that the higher layers of CNN (expect FC8 which is task speciﬁc), even without any ﬁne-tuning on the dataset, and even without any pose label optimization achieve representations that separate and highly preserve the view manifold structure. The aforementioned con- clusion is also conﬁrmed by the test performance of Linear SVM and Kernel Regression in Figure 6, us- ing RGBD dataset. In this experiment, the models are learned in train-split and the plots generated using test-split. Figure 6 clearly shows the conﬂict in the representation of the pre- trained network (categorization increases and pose estimation decreases). Linear separability of category is almost monotonically increasing up to FC6. Linear separability in FC7 and FC8 is worse, which is expected as they are task speciﬁc (no ﬁne-tuning). Surprisingly Pool1 features per- form very bad, despite being the most general features (typically they show Gabor like features and color blobs). In contrast, for pose estimation, the performance increases as we go lower in the net- work up to Conv4 and then slightly decreases. This conﬁrms our hypothesis that lower layers offer better feature encoding for pose estimation. It seems that Pool5 provides feature encoding that offer the best compromise in performance, which indicates that it is the best in compromising between the linear separation of categories and the preservation of the view-manifold structure.  Figure 5: RGB-D: KNN for categorization and pose estimation over the lay- ers of pre-trained model (Model0). For K = {1, 3, 5, 7, 9}  .  8  Published as a conference paper at ICLR 2016  Surprisingly, the pose estimation results do not drop dramatically in FC6 and FC7. We can still estimate the pose (with accuracy around 63%) from the representation at these layers, even without any training on pose labels. This highly suggests that the network preserves the view manifold structure to some degree. For examples taking the accuracy as probability at layer FC6, we can vaguely conclude that 90% of the manifolds are linearly separable and 65% are pose preserved (we are somewhere between hypotheses 1 and 2 at this layer). Table 1 shows the quanti- tative results of our mod- els on Pascal3D+ dataset. It also shows comparison against two previous meth- ods (Zhang et al., 2015) and (Xiang et al., 2014), using the two metrics < 45◦ and < 22.5◦ 4. It is important to note that the comparison with (Xi- ang et al., 2014) is unfair because they solve for de- tection and pose simultaneously while we solve for categorization and pose estimation. Model1 here outperforms both baselines (despite the unfair comparison with the latter approach). Quantita- tive results on RGBD dataset is presented in Appendix B.  Figure 6: RGB-D: test performance of linear SVM category classiﬁcation over the layers of different models (Left), and pose regression (Right).  Approach Model0 (SVM/Kernel Regression)  Model1 (SVM/Kernel Regression) Model0 NN Model1 NN Model1 (ﬁnal prediction) (Zhang et al., 2015) (Xiang et al., 2014)  Categorization % Pose (AAAI metric %)  Pose (other metrics %)  FC6/FC7/FC8  73.64/76.38/71.13 74.65/79.25/84.12 60.05/69.89/61.26 73.50/77.30/83.07  84.00  - -  FC6/FC7/FC8  49.72/48.24/45.41 54.41/54.07/60.31 61.11/61.38/60.32 65.87/66.07/70.54  71.60  - -  47.34(<22.5), 61.30 (<45) 44.20 (< 22.5), 59.00 (<45)  15.6 (<22.5), 18.7 (<45)  Table 1: Pascal3D Performance computed for Model0 and Model1 using different classiﬁcation techniques. Comparsion indicates that Model1 outperforms the baselines.  6 EFFECT OF TRANSFER LEARNING In order to study the effect of ﬁne-tuning the network (transfer learning to a new dataset) on the representation we trained the following model (denoted as Model1). This architecture consists of two parallel CNNs: one with category output nodes (Model1-Cat), and one with binned pose out- put nodes (Model1-Pose). We used 34 and 11 category nodes for RGBD and Pascal3D datasets respectively; while we used 16 pose nodes for both datasets). The parameters of both CNNs were initialized by Model0 parameters up to FC7. The parameters connecting FC7 to the output nodes are randomly initialized on both networks and they are ﬁne-tuned by minimizing the categorization loss for Model1-Cat and the pose loss for Model1-pose. The purpose of these architectures is to study the effect of ﬁne-tuning when the category and pose are independently optimized. We applied all the measures described in Sec 4 to understand how the view manifolds will be affected after such tuning. The questions are: To what degree optimizing on category should damage the ability of the network to encode view manifolds. On the other hand, how optimizing on pose should enhance that ability. Model1-Cat indicates the effect of optimizing on category, while Model1-Pose indicates the effect of optimizing on pose. Fig 4 shows the ﬁve view manifold measures for the different layers of Model1(Cat/Pose), in com- parison with Model0. In terms of data spread, from Fig 4a shows that the spread at FC8 has doubled after ﬁne tuning on pose (Model1-Pose). Fig 4b shows the ﬁne tuning on category (Model1-Cat) caused the view manifold subspace dimensionality to signiﬁcantly reduce to 1, where it became to- tally view invariant. Optimizing on pose slightly enlarged the subspace dimensionality (i.e, become  4Pose accuracy metrics are deﬁned in (Zhang et al., 2015; Xiang et al., 2014) and stated in Appendix A  9  Published as a conference paper at ICLR 2016  better) at FC8 and FC7. Fig 4c clearly shows the signiﬁcant improvement achieved by ﬁne tuning on pose, where the alignment of FC8 jumped to close to 0.9 from about 0.78, while ﬁne tuning on category reduces the alignment of FC8 to close to 0.65. Similar behavior is also apparent in the KPLS ratio for FC8 and FC7 (sup-mat). One very surprising result is that optimizing on pose makes the pose KTA alignment worse at the lower layers, while optimizing on category makes the pose alignment better compared to model0. In fact, although optimizing on pose signiﬁcantly helps aligning FC8 with pose labels, Pool5 still achieves the best KTA alignment and the least regression reconstruction error. The regression re- construction error in Fig 4d clearly shows signiﬁcant improvement in the representation of FC8 and FC7 to preserve the view manifold. One surprising ﬁnding from these plots is that the representa- tion of FC6 becomes worse after ﬁne tuning for both pose and category. Fig 4e indicates that the deformation of the view manifold is reduced as a result of ﬁne tuning on pose (larger rcond number), while it increases as a result of ﬁne tuning on category. On the global object-view manifold structure, we notice from Figures 6 some intuitive behavior at FC8. Basically optimizing on pose reduces the linear separability and increases the view manifold preservation (moves the representation towards hypothesis 0). In contrast, optimizing the category signiﬁcantly improves the linear separability at FC8, however, interestingly, it only slightly reduces the pose estimation performance to be slightly less than 50%. Combining this conclusion with the observation from Fig 4b, that the view manifold subspace dimensionality reduces to 1, this implies that optimizing on category collapses the view manifolds to a line, but they are not totally degen- erate. What is less obvious is the effect of ﬁne tuning on the lower layers than FC8. Surprisingly, optimizing on pose did not affect the linear separability of FC7. Another very interesting observation is that optimizing on category actually improves the pose estimation slightly at the FC7, FC6, and Pool5; and did not reduce it at lower layers. This implies that ﬁne tuning by optimizing on category only improved the internal view manifold preservation at the network, even without any pose labels.  7 CONCLUSIONS  In this paper we present an in-depth analysis and discussion of the view-invariant properties of CNNs. We proposed a methodology to analyze individual instance’s view manifolds, as well as the global object-view manifold. We applied the methodology on a pre-trained CNN, as well as two ﬁne-tuned CNNs, one optimized for category and one for pose. We performed the analysis based on two multi view datasets (RGBD and Pascal3D+). Applications on both datasets give consistent conclusions. Based on the proposed methodology and the datasets, we analyzed the layers of the pre-trained and ﬁne-tuned CNNs. There are several ﬁndings from our analysis that are detailed throughout the paper, some of them are intuitive and some are surprising. We ﬁnd that a pre-trained network captures representations that highly preserve the manifold structure at most of the network layers, including the fully connected layers, except the ﬁnal layer. Although the model is pre-trained on ImageNet, not a densely sampled multi-view dataset, still, the layers have the capacity to encode view manifold structure. It is clear from the analysis that, except of the last layer, the representation tries to achieve view invariance by separating individual instances’ view manifolds while preserving them, instead of collapsing the view manifolds to degenerate representations. This is violated at the last layer which enforces view invariance. Overall, our analysis using linear SVM, kernel regression, KNN, combined with the manifold anal- ysis, makes us believe that CNN is a model that simulate the manifold ﬂattening hypothesis of Di- Carlo & Cox (2007) even without training on multi-view dataset and without involving pose labels in the objective’s loss. Another interesting ﬁnding is that Pool 5 offers a feature space where the manifold structure is still preserved to the best degree. Pool 5 shows better representation for the view-manifold than early layers like Pool1. We hypothesize that this is because Pool5 has better translation and rotation invariant properties, which enhance the representation of the view manifold encoding. We also showed the effect of ﬁne-tuning the network on multi-view datasets, which can achieve very good pose estimation performance. In this paper we only studied the effect of independent pose and category loss optimization. Optimizing on category achieves view invariance at the very last fully  10  Published as a conference paper at ICLR 2016  connected layers; interestingly it enhances the viewpoint preservation at earlier layers. We also ﬁnd that ﬁne-tuning mainly affects the higher layers and rarely affects the lower layers. In this work our goal is not to propose any new architecture or algorithm to compete with the state of the art in pose estimation. However, the proposed methodology can be used to guide deep network design for solving several tasks. To show that and based on the analysis and the conclusions of this paper, we introduced and studied in (Elhoseiny et al., 2015) several variants of CNN architectures for joint learning of pose and category, which outperform the state of the art . We keep these results as a guide to the reviewers, without distracting the reader from our main goal. Acknowledgment: This work is funded by NSF-USA award # 1409683.  REFERENCES Agrawal, Pulkit, Girshick, Ross, and Malik, Jitendra. Analyzing the performance of multilayer neural networks for object recognition. In Computer Vision–ECCV 2014, pp. 329–344. Springer, 2014. 2  Bakry, Amr and Elgammal, Ahmed. Untangling object-view manifold for multiview recognition  and pose estimation. ECCV, 2014. 3  Chatﬁeld, Ken, Simonyan, Karen, Vedaldi, Andrea, and Zisserman, Andrew. Return of the devil in the details: Delving deep into convolutional nets. CoRR, abs/1405.3531, 2014. URL http: //arxiv.org/abs/1405.3531. 1, 3  Deng, Jia, Dong, Wei, Socher, R., Li, Li-Jia, Li, Kai, and Fei-Fei, Li.  hierarchical image database. In CVPR, 2009. 4  Imagenet: A large-scale  DiCarlo, James J and Cox, David D. Untangling invariant object recognition. Trends in cognitive  sciences, 11(8):333–341, 2007. 1, 2, 3, 10  Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013. 2  Duchon, Jean. Splines minimizing rotation-invariant semi-norms in sobolev spaces. In Constructive  theory of functions of several variables, pp. 85–100. Springer, 1977. 6, 17  Elhoseiny, Mohamed, El-Gaaly, Tarek, Bakry, Amr, and Elgammal, Ahmed. Convolutional models  for joint object categorization and pose estimation. arXiv preprint arXiv:1511.05175, 2015. 11  Everingham, Mark, Gool, Luc Van, Williams, C. K. I., Winn, J., and Zisserman, Andrew. The pascal visual object classes (VOC) challenge. International Journal of Computer Vision (IJCV), 2010. 4  Girshick, Ross B., Donahue, Jeff, Darrell, Trevor, and Malik, Jitendra. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524, 2013. URL http://arxiv.org/abs/1311.2524. 1  Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and Sch¨olkopf, Bernhard. Measuring statistical dependence with hilbert-schmidt norms. In Proceedings of the 16th International Conference on Algorithmic Learning Theory, ALT’05, pp. 63–77, Berlin, Heidelberg, 2005a. Springer-Verlag. ISBN 3-540-29242-X, 978-3-540-29242-5. doi: 10.1007/11564089 7. URL http://dx.doi. org/10.1007/11564089_7. 15  Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and Sch¨olkopf, Bernhard. Measuring statistical In Algorithmic learning theory, pp. 63–77. Springer,  dependence with hilbert-schmidt norms. 2005b. 6  H, Drucker, CJC, Burges, L, Kaufman, A, Smola, and V, Vapnik. Support vector regression ma-  chines. Advances in Neural Information Processing Systems, 1996. 6  Horn, Roger A. Matrix Analysis. 6  11  Published as a conference paper at ICLR 2016  Jarrett, Kevin, Kavukcuoglu, Koray, and LeCun, Yann. What is the best multi-stage architecture for  object recognition?, 2009. 2  Kavukcuoglu, Koray, Sermanet, Pierre, Boureau, Y-Lan, Gregor, Karol, Mathieu, Michal, and Le- Cun, Yann. Learning convolutional feature hierarchies for visual recognition. In Lafferty, John D., Williams, Christopher K. I., Shawe-Taylor, John, Zemel, Richard S., and Culotta, Aron (eds.), NIPS, pp. 1090–1098. Curran Associates, Inc., 2010. URL http://dblp.uni-trier.de/ rec/bib/conf/nips/KavukcuogluSBGML10. 2  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Imagenet classiﬁcation with deep convolu- In Advances in Neural Information Processing Systems 25, pp. 1106–  tional neural networks. 1114, 2012. 1, 2, 5  Lai, K., Bo, L., Ren, X., and Fox, D. A large-scale hierarchical multi-view rgb-d object dataset. In Robotics and Automation (ICRA), 2011 IEEE International Conference on, pp. 1817–1824. IEEE, 2011. 4  LeCun, Yann, Huang, Fu Jie, and Bottou, L´eon. Learning methods for generic object recognition with invariance to pose and lighting. In Proceedings of the 2004 IEEE Computer Society Con- ference on Computer Vision and Pattern Recognition, CVPR’04, pp. 97–104, Washington, DC, USA, 2004. IEEE Computer Society. URL http://dl.acm.org/citation.cfm?id= 1896300.1896315. 2  Murase, H. and Nayar., S. Visual learning and recognition of 3d objects from appearance. Interna-  tional Journal of Computer Vision, 14:5–24, 1995. 3  Ranzato, Marc’Aurelio, Huang, Fu-Jie, Boureau, Y-Lan, and LeCun, Yann. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In Proc. Computer Vision and Pattern Recognition Conference (CVPR’07). IEEE Press, 2007. 2  Razavian, Ali Sharif, Azizpour, Hossein, Sullivan, Josephine, and Carlsson, Stefan. Cnn features off-the-shelf: an astounding baseline for recognition. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pp. 512–519. IEEE, 2014. 2  Rosipal, Roman and Trejo, Leonard J. Kernel partial least squares regression in reproducing kernel ISSN 1532-4435. URL http:  hilbert space. J. Mach. Learn. Res., 2:97–123, March 2002. //dl.acm.org/citation.cfm?id=944790.944806. 6, 16  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael S., Berg, Alexander C., and Fei- Fei, Li. Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. URL http://arxiv.org/abs/1409.0575. 5  Sermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Micha¨el, Fergus, Rob, and LeCun, Yann. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229, 2013. URL http://arxiv.org/abs/1312.6229. 1, 2  Xiang, Yu, Mottaghi, Roozbeh, and Savarese, Silvio. Beyond pascal: A benchmark for 3d object detection in the wild. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2014. 4, 9  Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are features in deep neural  networks? ArXiv e-prints, November 2014. 1, 2  Zeiler, Matthew D. and Fergus, Rob. Visualizing and understanding convolutional networks. CoRR,  abs/1311.2901, 2013. URL http://arxiv.org/abs/1311.2901. 1, 2, 3  Zhang, Haopeng, El-Gaaly, Tarek, and Elgammal, Ahmed. Joint object and pose recognition using  homeomorphic manifold analysis. AAAI, 2013. 3, 6  Zhang, Haopeng, El-Gaaly, Tarek, Elgammal, Ahmed, and Jiang, Zhiguo. view-object manifolds for joint object recognition and pose estimation. arXiv:1503.06813, 2015. 9  Factorization of arXiv preprint  12  Published as a conference paper at ICLR 2016  Approach HOG (SVM/Kernel Regression) Model0 (SVM/Kernel Regression) on conv4 Model0 (SVM/Kernel Regression) on FC6 Model1  Categorization %  Pose %  80.26 58.64 86.71 89.63  27.95 (AAAI) 67.39 (AAAI) 64.39 (AAAI)  81.21 (AAAI), 69.58 (< 22.5), 81.09 (< 45)  Table 2: RGBD Dataset Results for HOG, Model0 and Model1.  APPENDIX  A POSE AND CATEGORIZATION PERFORMANCE ON RGBD AND PASCAL3D  DATASETS  The two metrics < 22.5 and < 45 are the percentages of test samples that satisfy AE < 22.5◦ and AE < 45◦, respectively where the Absolute Error (AE) is AE = |EstimatedAngle − GroundT ruth|). The AAAI pose metric is deﬁned as  ∆(θi, θj) = min(|θi − θj|, 2π − |θi − θj|)/π  (1)  B QUANTITATIVE RESULTS FOR RGBD DATASET  Table 2 shows qualitative ressult of Kernel-SVM Regression on different layers features in Model0 and Model1. Comparing their results with images HOG features. Comparison to state of Art using this model is not possible since we work on the wellposed objects split which is not explored by any other work.  C SYNTHETIC DATA ANALYSIS  In this work, we have explored many different measurements and ﬁlter them out to use only those that expose the correct properties of the view-manifolds. Besides the intuitive reasoning that we provided for choosing the measurements, in this section, we show empirical results to quantify efﬁciency of the chosen measurements. To this end, we synthesized a set of well designed view-manifolds. Analyzing these manifolds is intended to identify the robust and informative set of measurements to be used in further analysis. To be qualiﬁed for comparing different manifolds, the synthesized manifolds is designed to encode interesting properties of any view-manifold such as:  • Dimensionality (of the Euclidean space where the manifold lives) • Sparsity of the manifold • Smoothness of the manifold • Deformation of the manifold w.r.t the view-circle • Variance of data-points  Recall, The view-circle is a view-manifold, where all the viewpoints form a perfect circle and the object is assumed to be located at the center of this circle. In the rest of this section, we list detailed description of the synthetic manifolds. Then, we use them to analyze the selected measurements.  C.1 DATASET DESCRIPTION  As in Figure 7, manifolds in this dataset can be categories as:  • Circle Orthogonally projected to high-dimensional subspaces (Manifold sets 1 and 2) • Unit circle projected to a nonlinear surface (manifold 3) • Unit circle projected to 3D-Sphere with radius r (Sr • Nonlinear smooth curve projected on Sr  2) (sets 4 an 5)  2 (set 6)  13  Published as a conference paper at ICLR 2016  (a) Sinosoidal Surface  (b) Circle projected on S2  (c) Circle projected on S2  Figure 7: Manifold Visualization  • Discontinuous smooth curve projected on Sr • Random manifolds (sets 8 and 9 ) • Collapsed manifolds in a single point or very small region (set 10).  2 (set 7)  The manifolds are described using the dimensionality (d), sparsity (s = n points representing the view-manifold, smoothness, deformation w.r.t the view-circle. Let the view-manifold be parameterized by the single dimensional variable. Let S is the two dimen- sional representation of the unit circle. S = {(cos(t), sin(t))|t = {0, 2π }}. For each view-manifold (M), we generated n points in a d-Dim space.  d ) and n, the number of  n , ..., 2(n−1)π  n , 4π  n  • Perfect view-circle in high-dimensional space  – Manifold 1: n = 100, d ∈ {10, 300, 600, 900, 1200, 1500, 1800}, therefore, the spar- – Manifold 2: d = 500, n ∈ {50, 150, 250, 350, 450, 550, 650, 750}, therefore, the  sity varies from very dense (s = 10) to very sparse (s = 1/20)  manifold varies from very sparse (s = 1/10) to dense (s = 1.5)  • View-circle projected nonlinearly to Sinusoidal Surface. The manifold has n = 100 points and live in d = 3-Dim space, so it is very dense s = 33.33 To project the view-circle on this surface we follow these steps:  – Let f n be the projection function on the surface,  f n(x, y) = sin(3x)cos(2y)2  – The projected manifold Z is deﬁned by  Z = {(x, y, f n(x, y))|(x, y) ∈ S} – Manifold 3 represents this type of manifolds in our dataset.  • Dense view-circle projected nonlinearly to Sr  2, with r ∈ {1, 50, 100, 150}, d = 3, n =  100 (s = 33.33) To project the view-circle on Sr  2, we use the following projection function  f (θ, φ) = (sin(φ)cos(θ), sin(φ)sin(θ), cos(φ))  Where  θ ∈ {0,  2π n  ,  4π n  , ...,  2(n − 1)π  }  n – Manifold 4: Slightly deformed manifold, Figure 7b π 2  sin(θ) +  φ =  π 4  ;∀θ  – Manifold 5: Slightly deformed manifold with added Gaussian noise with µ = 0 mean  σ = 0.01. θ and φ as in Manifold 4.  – Manifold 6: Highly deformed manifold, Figure 7c π 2  sin(5θ) +  φ =  π 4  ;∀θ  14  Published as a conference paper at ICLR 2016  – Manifold 7: Highly deformed and broken/discontinuous manifold.  φ =  π 4  tan(0.75θ) +  ;∀θ  π 2  • Random manifold with independent dimensions  – Manifold 8: Uniform random points with d ∈ {10, 100, 500, 1000, 4000}, n = 100,  therefore, the sparsity varies from very dense (s = 10) to very sparse (s = 1/100)  – Manifold 9: Normal random points has been generated with d = 100, n ∈ {20, 40, ..., 200}, therefore, sparsity varies from very sparse (s = 1/10) to dense (s = 2)  • Collapsed Manifold with random noise  – Manifold 10: Portion of the points (m = n/4), in this manifold, have been generated by Gaussian Random with µ = 0, σ = 0.01, therefore, the rest are a copied version of this portion d ∈ {10, 100, 500, 1000, 4000}, n = 100  C.2 ANALYSIS  Recall, the objective of using the synthetic data is to verify the efﬁciency of selected measurements. Figure 8 shows the results of applying the measurements to the synthetic-data. Figure 8a shows the Nuclear Norm (deﬁned in the main paper) for all manifolds. This ﬁgure shows the variability between the manifolds in the variance. For the set of manifolds 4-7, projecting the view-circle onto sphere with different sizes affects the variance of the points. Encoding different Nuclear Norm is subjected to discover the measurements that are sensitive to the data variance. From Figure 8b, deﬁned in the main paper, we can see the effective dimensions for each manifold. Manifolds 1 and 2 have two effective dimensions. Manifolds 3-7 has three effective dimensions. Since the points in Manifolds 8-10 are generated randomly so they have maximum rank. The kernel alignment measures: KTA (Figure 8c) and HISIC (Figure 8d) measure the correlation between the view-manifold and the view-circle. These two ﬁgures show signiﬁcant better alignment of the view-manifold of sets 1-6 than the alignment of the random manifolds. Since Hilbert-Schmidt Independence Criterion (HSIC) Gretton et al. (2005a) does not add any information more than KTA. We select the KTA measurement because it exposes absolute alignment conﬁdence for the manifolds 1 and 2. KPLS-regression Error is shown in Figure 8e. Dispite the vast variability of variance and dimen- sionality, this measure is consistent and gives small value for all smooth manifold. This measure can also detect the collapsing manifolds, since it gives very large error value. As we mentioned in Section D, that using both measurements KPLS-Regression Error and KPLS- Norm Ratio gives more robust conclusion about the manifold. Fig 8f shows a clear trend, since it gives signiﬁcant high values for random manifolds. This is because, the subspace of the random norm(G0) ≡ 1, this means that the ﬁrt d components extracted points covers the entire space. When norm(Gd) from G0 are far from being principal. If they are pricipal components, they would change the energy of the matrix G signiﬁcantly. On the other side, the Effecive dimensionality of the smooth manifolds 1-6 is D ≤ 3, which make the limit d > D. That is why the ratio norm(Gd) norm(G0) (cid:28) because we have extracted all the pricipal components of those manifolds. That is why KPLS-Regression Error for these manifolds is very small. As mentioned in the main paper, TPS-lineairty measure (T P S − RCond(CF − P oly)) scores on the stability of the polynomial mapping from the points on the view-circle and the points on the view-manifold. Fig 8g shows perfect scoring for Manifolds 1 and 2. Combining this ﬁgure with Fig 8h gives a complete impression about the mapping stability (Polynomial and Non-Polynomial). However, the range of the values of TPS-nonlinearity measure (T P S − RCond(CF − nonP oly)) is in BigO(10−8), which decrease its robustness.  15  Published as a conference paper at ICLR 2016  (a) Nuclear Norm  (b) Effective 90% SV’s  (c) KTA  (d) HSIC  (e) KPLS-Regression error  (f) KPLS- norm(Gd) norm(G0)  (g) TPS-RCond(CF − poly)  (h) TPS-RCond(CF − nonP oly)  Figure 8: Measurement analysis for the synthetic manifolds. Every ﬁgure shows single measure- ment. X-axis is labeled by the manifold category number. D MORE ON KPLS AND TPS RELATED MEASUREMENTS  We deﬁne and show the results of more measurements such as KPLS-Norm Ratio and TPS- nonPolynomial. We use here the same notations and deﬁnitions stated in Section 4 in the main paper.  1) KPLS-Norm Ratio: Kernel Partial Least Squares (KPLS) Rosipal & Trejo (2002) is a supervised regression method. KPLS iteratively extracts a set of principal components of the input kernel that are most correlated with the output. While KPCA extracts the principal components (PCs) of the kernel of the input data to maximize the variance of the output space, KPLS extracts the PCs of the kernel of the input data that maximize the correlation with the output data. We use KPLS to map the afﬁnity matrix of the transformed view-manifold (view-kernel) to the circle afﬁnity matrix (circle-kernel). Following the convention of the main paper, let the view-kernel is denoted by Kl, and the circle-kernel is denoted by K◦ (The subscript n is removed to simplify the notation). We limit the number of extracted PCs to d , where d (cid:28) N and N is the dimensionality of the input kernel (in this work, we use d = 5). More speciﬁcally, KPLS maps the rows of Kl to the rows of K◦. So that  (2) Where the set of extracted PCs are the columns of the matrix TN×d, UN×d is auxiliary matrix, and the Gram-matrix G0 is deﬁned by  ˆK◦ = G0U(T(cid:62)G0U)−1T(cid:62)Kl  (3)  KlKl(cid:62) bb(cid:62)  G0 =  16  Published as a conference paper at ICLR 2016  Where b ∈ RN , so that b(i) is the Frobenius norm of the i-th row of Kl. Based on the mapping in Eq 2, we extract two measurements: First: KPLS-Regression Error (δ) which measures geometric deformation of the generated output image of view-kernel in the circle-kernel space ( ˆK◦ with respect to the the circle-kernel (K◦) and the ). One choice for measuring this is  δ( ˆK◦, K◦) = 1 − KT A( ˆK◦, K◦)  Where KTA stands for Kernel Target Alignment (stated in Equation 2 in the main paper). The Regression error measures the reconstruction error of the circle-kernel from the view-kernel. Second, KPLS-NormK Ratio ((cid:107)Gd(cid:107)F ) measure the residual energy after extracting the ﬁrst d-PC’s. (cid:107)G0(cid:107)F Where Gd is the residual of G0 after d-iterations. The intuition behind this measure is that the larger the ratio (cid:107)Gd(cid:107)F , this means that the view-manifold has more than d-PC’s correlated with the (cid:107)G0(cid:107)F circle-kernel. While KPLS-regression Error is self-explanatory (this measure presented in the main paper), using the two KPLS measurements together gives more precise view on the correlation between the view- manifold and the circle-manifold. From Fig 9b, KPLS-Norm Ratio supports the observation that we noted in the main paper, from Fig 9a, that the lower layers in Model0 are more correlated to the circle-manifold than the higher layers. Except for Pool5, which encodes maximum correlation between the view-manifold and the circle-manifold.  2) TPS-nonlinearity measure: In this measure we learn a regularized Thin Plate Spline (TPS) non- linear mapping Duchon (1977) between the unit circle manifold and each manifold Mk. The map- ping function (γ) can be written as  γk(x) = Ck · ψ(x),  where Cd×(N +e+1) is the mapping matrix, e = 2, and the vector ψ(x) = [φ(|x − z1|)··· φ(|x − zM|), 1, xT ]T represents a nonlinear kernel map from the conceptual representation to a kernel in- duced space. The thin plate spline is deﬁned as: φ(r) = r3 and {zi}M i=1 are the set of center points. The solution for Ck can be obtained by directly solving the linear system:  (cid:18)Kl + λI  (cid:19)  (cid:18) Ak  (cid:19)  Px  Ck T  ,  k  =  Nk  PT t  0(e+1)×d  0(e+1)×(e+1)  1 ,··· , yk  ], Pt is M × (e + 1) matrix with i-th row [1, zT  (4) A, Px and Pt are deﬁned for the k − th set of object images as: A is a Nk × M matrix with ij = φ(|xk i − zj|), i = 1,··· , Nk, j = 1,··· , M, Px is a Nk × (e + 1) matrix with i-th row Kl i ]. Ak is a Nk × d matrix containing the set [1, xkT of images for manifold Mk, i.e. Ak = [yk i ]. Solution for Ck is guaranteed under certain conditions on the basic functions used. The reason for using TPS in particular is that the mapping has two parts, an afﬁne part (linear poly- nomial) and a nonlinear part. Inquiring into the two parts gives an impression about the mapping, if it is mostly linear or nonlinear. We used the reciprocal-condition number (RCond) of the submatrices of the coefﬁcient matrix that correspond to the afﬁne and the nonlinear part. While Fig 9c shows that the lower layers has more (better) conditioned linear mapping. Fig 9d shows that the lower layer has complete stable mapping. This is expected since the lower layers have high dimensionality. At the same time, Fig 9d shows that the Convolution layers (Conv 3,4 and 5) have unstable nonlinear mappings. An additional observation is that ﬁne-tuning against the pose labels increases the mapping stability (polynomial and non-polynomial). It is clear in Fig 9d that the T P S − RCond(CF − nonP oly) has very small order of values (10−11), therefore, we do not rely on it in our analysis.  17  Published as a conference paper at ICLR 2016  (a) KPLS-Regression Error (δ)  (b) KPLS-Norm Ratio ( norm(Gd) norm(G0) )  (c) TPS-RCond(CF − poly)  (d) TPS-RCond(CF − nonP oly)  Figure 9: Measurement analysis for the view-manifold in RGBD dataset based on features extracted from different layers of several CNN models. Every ﬁgure shows single measurement. Multiple lines is for different CNN model. X-axis is labeled by the layers.  18  ",
1511.05042,2016,An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family,"['An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family\nAlexandre De Brébisson', 'Pascal Vincent']",https://arxiv.org/pdf/1511.05042,"6 1 0 2     b e F 8 2         ] E N . s c [      3 v 2 4 0 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  AN EXPLORATION OF SOFTMAX ALTERNATIVES BELONGING TO THE SPHERICAL LOSS FAMILY  Alexandre de Br´ebisson and Pascal Vincent∗ MILA, D´epartement d’Informatique et de Recherche Op´erationnelle, University of Montr´eal alexandre.de.brebisson@umontreal.ca vincentp@iro.umontreal.ca  ABSTRACT  In a multi-class classiﬁcation problem, it is standard to model the output of a neu- ral network as a categorical distribution conditioned on the inputs. The output must therefore be positive and sum to one, which is traditionally enforced by a softmax. This probabilistic mapping allows to use the maximum likelihood prin- ciple, which leads to the well-known log-softmax loss. However the choice of the softmax function seems somehow arbitrary as there are many other possible normalizing functions. It is thus unclear why the log-softmax loss would per- form better than other loss alternatives. In particular Vincent et al. (2015) recently introduced a class of loss functions, called the spherical family, for which there exists an efﬁcient algorithm to compute the updates of the output weights irre- spective of the output size. In this paper, we explore several loss functions from this family as possible alternatives to the traditional log-softmax. In particular, we focus our investigation on spherical bounds of the log-softmax loss and on two spherical log-likelihood losses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) and the log-Taylor Softmax that we introduce. Although these alternatives do not yield as good results as the log-softmax loss on two language modeling tasks, they surprisingly outperform it in our experiments on MNIST and CIFAR10, suggesting that they might be relevant in a broad range of applications.  INTRODUCTION  Classiﬁcation problems with high dimensional outputs are particularly common in many language applications in which a target word has to be predicted out of a very large vocabulary. The standard application of backpropagation does not take advantage of the sparsity of the categorical targets and, as a result, the computations to update the weights of the output layer can be prohibitively expensive. Popular workarounds are based on approximations and can be divided into two main approaches. The ﬁrst are sampling methods approximations, which compute only a tiny fraction of the output’s dimensions (see for example Gutmann & Hyvarinen (2010); Mnih & Kavukcuoglu (2013); Mikolov et al. (2013); Shrivastava & Li (2014)). The second is the hierarchical softmax, which modiﬁes the original architecture by replacing the large output softmax by a heuristically deﬁned hierarchical tree (Morin & Bengio (2005); Mikolov et al. (2013)).  Vincent et al. (2015) recently proposed an algorithm to compute the exact updates of the output weights in a very efﬁcient fashion, provided that the loss belongs to a particular class of functions, which they call the spherical family because it includes an alternative to softmax, named spherical softmax by Ollivier (2013). In the rest of the paper, we call these losses the spherical losses. If we denote d the dimension of the last hidden layer and D the dimension of the high dimensional output layer, they showed that for a spherical loss, it is possible to compute the exact updates of the output weights in O(d2) instead of the naive O(d × D) implementation. However it remains unclear how the spherical losses compare to the more traditional log-softmax loss in the context of classiﬁcation. This is precisely what we aim to investigate in this paper.  ∗and CIFAR  1  Published as a conference paper at ICLR 2016  We ﬁrst describe precisely the spherical family and extract spherical bounds of the log-softmax loss from it. We then identify two particular normalizing activation functions, namely the spherical softmax and the Taylor softmax, that lead to log-likelihoods that belong to the spherical family and that may be suitable to train neural network classiﬁers. Finally we evaluate these different losses empirically by training models on several tasks: MNIST, CIFAR10/100 and language models on the Penntree bank and the One Billion Word dataset.  1 CHARACTERIZATION OF THE SPHERICAL FAMILY  Let o = W h be the linear outputs of a neural network, where o has dimension D and h represents the d dimensional output of the last hidden layer. Let y be a sparse target and A(y) the indices of the non-zero elements of y. The spherical family described in Vincent et al. (2015) is composed of  the functions that can be expressed using only the oc associated to non-zero yc, q = kok2 = Pi o2 the squared norm of the whole output vector and s = sum(o) = Pi oi:  sphericalf amily = L(s = sum(o), q = kok2, {(oc, yc)|c ∈ A(y)}).  i  Vincent et al. (2015) showed that for such loss functions, it is possible to compute the exact gradient updates of W in O(d2) without even computing the output o, instead of the O(d × D) naive im- plementation. As we focus on classiﬁcation problems, we will assume for the rest of the paper that A(y) contains the single target class index c corresponding to the single non-zero element yc of y. The resulting family can be rewritten as follows:  The square error following the linear output o belongs to this family:  L(s, q, oc, yc).  LM SE(o, y) = ko − yk2  = q − 2ocyc + y2 c .  It is the loss of choice in regression problems. It is also sometimes used in classiﬁcation prob- lems1 even though the log-softmax loss is nowadays considerably more popular. Contrary to the log-softmax loss, using the square error for classiﬁcation does not correspond to the conditional likelihood of a categorical distribution. Nevertheless, like the log-softmax loss and other likelihood losses, the mean square error has the desirable property that its minimum is the conditional expec- tation.  2 SPHERICAL UPPER BOUNDS OF THE LOG-SOFTMAX LOSS FUNCTION  In this Section, we consider functions from the spherical family that are upper bounds of the log- softmax loss:  L(o, c) = − log  eoc k=1 eok  PD  D  = −oc + log  eok .  Xk=1  Bouchard (2007) proposed the following upper bound for the log sum of exponentials (for any α ∈ R and ξk ∈ R):  D  D  ok − α − ξk  1  2  log  eok ≤ α +  Xk=1 2ξ (  + λ(ξk)((ok − α)2 − ξ2  Xk=1 2 ). To be able to use the algorithm developed by Vincent et al. (2015), where λ(ξ) = 1 the ξk have to be the equal for all k. By replacing ξk by ξ and by optimizing α so that the bound is as tight as possible, we can derive the following2 bound for L(o, c), which holds for any ξ ∈ R: D(cid:19) λ(ξ) − oc(cid:19) . L ≤ (cid:18)−  ξ − Dλ(ξ)ξ2 + D log(1 + eξ) +  k) + log(1 + eξk ),  s +(cid:18)q −  1+e−ξ − 1  (D − 2)2  D 2  1 D  16D  λ(ξ)  s2  −  1  1In classiﬁcation, it is actually often used with a logistic sigmoid applied to o beforehand, but this results in  a loss that does not belong to the spherical family  2This derivation is a little tedious but trivial, we leave it out due to space constraints.  2  Published as a conference paper at ICLR 2016  This bound clearly belongs to the spherical family. We tried two approaches to determine an optimal ξ: either considering it as a ﬁxed hyperparameter or optimizing it for every example to yield the tightest bound. By minimizing this bound, we hope to minimize indirectly the negative log-softmax.  3 SPHERICAL LOSSES MODELING CATEGORICAL LIKELIHOODS  In classiﬁcation problems, it is standard to model the output as a categorical posterior distribution P (categories|input). Hence, the computed output must consist of positive values that sum to one, which is generally enforced by a softmax function applied to the linear output o = W h. However, this property holds for a more general class of normalizing functions:  fnorm : RD → RD  o 7→ [fnorm(o)k]1≤k≤D,  where  ∀k, fnorm(o)k =  and where each gk has only positive values gk : RD → R+. We can restrict this family to be component-wise:  gk(o) i=1 gi(o)  ,  PD  with g being a real function common to all the components.  ∀k : o 7→ gk(o) = g(ok),  Now that the output represents a categorical distribution, the corresponding network can be trained by maximizing the likelihood on a training dataset, i.e. minimizing the negative log-likelihood (equivalently the cross-entropy)  Llog loss(o, c) = − log (fnorm(o)c) ,  where c is the index of the target class for the example o.  The exponential is commonly used for g, which gives the softmax function:  o 7→ fsof t(o)k =  exp(ok) i=1 exp(oi)  .  PD  However, despite being widely used, it remains unclear how the softmax compares to other nor- malizing functions. In particular, normalizing activation functions of the following form lead to log-likelihoods that belong to the spherical family:  o 7→ fsph(o)k =  a1 + a2ok + a3o2 k  Pk(a1 + a2oi + a3o2  i )  ,  where a1, a2 and a3 are scalars such that x 7→ a1 + a2x + a3x2 is a positive polynomial (which is equivalent to a3 and 4a1a3 − a2 2 being positive). The corresponding spherical log-likelihood loss can indeed be rewritten into the canonical form of the spherical family:  Llog sph(o, c) = − log fsph(o)c  = − log  a1 + a2oc + a3o2 c a1D + a2s + a3q  ,  In the next sections, we consider two particular instances of this family: the log-spherical softmax and the log-taylor softmax.  3  Published as a conference paper at ICLR 2016  3.1 SPHERICAL SOFTMAX  The ﬁrst spherical alternative to the softmax function that we consider is the spherical softmax, a minor modiﬁcation of the non-linearity investigated by Ollivier (2013) to which a small constant ǫ is added for numerical stability reasons:  o 7→ fsph sof t(o)k =  o2 k + ǫ i=1(o2  i + ǫ)  .  PD  The corresponding log-loss is the log-spherical softmax Llog sph sof t(o, c) = − log fsph sof t(o)c, whose gradient are  ∂L ∂oc  ∂L  ∂ok6=c  =  =  2oc i=1(o2 2ok i=1(o2  PD PD  i + ǫ)  i + ǫ)  −  2oc o2 c + ǫ  ,  ,  where c is the index corresponding to the target class. From the expression of the gradients, we i or oc are very small. In  i=1 o2 practice, we found ǫ to be very important and it should be carefully tuned.  can see that ǫ is necessary to avoid numerical issues when either PD  An interesting property of the spherical softmax (with ǫ = 0) is that it is invariant to a global rescaling of the pre-activations o. This contrasts with the translation invariance of the softmax but it is unclear if this is a desirable property.  We can also notice that, contrary to the softmax function, the spherical softmax is even, i.e. ignores the sign of the pre-activation o. softmax function taken on the absolute value of the pre-activations.  it In the experiments Section, we will compare it to the  3.2 TAYLOR SOFTMAX  Our second spherical alternative to the softmax comes from the second-order Taylor expansion of the exponential around zero exp(x) ≈ 1 + x + 1 2 x2, which leads to the following function, which we call the Taylor softmax:  o 7→ ftay sof t(o)k =  1 + ok + 1 2 o2 k i=1(1 + oi + 1 2 o2 i )  .  PD  Its corresponding log-loss is the log-Taylor softmax Llog tay sof t(o, c) = − log ftay sof t(o)c, whose gradient are  ∂L ∂oc  ∂L  ∂ok6=c  =  =  1 + oc  1 + ok  PD i=1(1 + oi + 1 PD i=1(1 + oi + 1  2 o2 i )  2 o2 i )  −  1 + oc 1 + oc + 1  2 o2 c  ,  .  The numerator 1 + xc + 0.5x2 c of the Taylor softmax is assured to be strictly positive and greater than 0.5, its minimum value. The gradients are well-behaved as well, with no risk of numerical instability. Therefore, contrary to the spherical softmax, we do not need to use the extra hyperparameter ǫ. Furthermore, unlike the spherical softmax, the Taylor softmax has a small asymmetry around zero.  4 EXPERIMENTS  In this Section we compare the log-softmax and different spherical alternatives on several tasks: MNIST, CIFAR10/100 and a language modeling task on the Penntree bank and the One Billion Word datasets. Our goal was not to reach the state of the art on each task but to compare the inﬂuence of each loss. Therefore we restricted ourselves to reasonably sized standard architectures with little  4  Published as a conference paper at ICLR 2016  regularization, no ensembling and no data augmentation apart from CIFAR. In all the experiments, we used hidden layers with rectiﬁers, whose weights were initialized with a standard deviation of q 2 f an in as suggested in He et al. (2015). For the output layers, we set the initial weights to zero. In our language experiments, we set the bias values such that the initial network outputs matched the prior frequencies of the classes.  We ran experiments to train neural language models with softmax outputs by minimizing the spher- ical upper bounds given in Section 1 but results were disappointing. Optimizing the bound actually degraded the initial perplexity (at initialization, the network outputs the frequencies of the words), which means that the minimum of the bound was worse than the simple initialization. In the sub- sections below, we will thus provide detailed results only for the more promising spherical losses outlined in Section 2.  4.1 MNIST  We ﬁrst compared the effectiveness of our different loss functions for training MNIST digit classi- ﬁers (LeCun et al. (1998)). We used the same architecture for all the different losses: a convolutional neural network composed of two conv-pooling layers (30 and 60 feature maps, ﬁlter sizes 5, pooling windows of size 5) followed by a fully connected layer of 500 neurons and the output layer. We used rectiﬁers for all hidden neurons and initialized the weights with the He scheme (He et al. (2015)). The networks were trained with minibatches of size 200, a Nesterov momentum (Sutskever et al. (2013)) of 0.9 and a decaying learning rate 3. The initial learning rate is the only hyperparameter that we tuned individually for each loss. We used early stopping on the validation dataset as our stopping criterion.  Table 1: Test set performances of a convolutional network trained on MNIST with different loss functions. For each loss, results were averaged over 100 runs (each with different splits of the train- ing/valid/test sets and different initial parameter values), the standard deviation being in parenthesis. The loss column reports the training loss evaluated on the test set. negll refers to the negative log- likelihood. The log softmax abs row corresponds to the log-softmax loss except that the softmax is applied on the absolute value of the pre-activations. The log-Taylor softmax outperforms the log-softmax, especially with respect to the negative log-likelihood.  loss function MSE Log softmax Log softmax abs Log spherical softmax Log Taylor softmax  loss  error rate  number of epochs  mse: 0.0035 (0.00036) negll: 0.0433 (0.0080) negll: 0.0437 (0.0097) negll: 0.0311 (0.0031) negll: 0.0292 (0.0034)  0.889% (0.100) 0.812% (0.104) 0.813% (0.095) 0.828% (0.094) 0.785% (0.097)  60 (17) 26 (7) 25 (8) 27 (9) 22 (7)  Table 2: Test set performances of a convolutional network trained on MNIST with different loss functions trained and evaluated on the ofﬁcial training and testing sets of MNIST (contrary to the results of table 1, for which the data splits were random). For each loss, results were averaged over 100 runs with different initial random parameters, the standard deviation being in parenthesis. The loss column reports the training loss evaluated on the test set. negll refers to the negative log-likelihood. The results are signiﬁcantly better than those reported in table 1, suggesting that the ofﬁcial MNIST set is particularly advantageous. The log-Taylor softmax still outperforms the log-softmax.  loss function Log softmax Log Taylor softmax  loss  error rate  number of epochs  negll: 0.0335 (0.0052) negll: 0.0247 (0.0020)  0.716% (0.084) 0.688% (0.061)  26 (7) 22 (8)  3we used the heuristic of dividing the learning rate by two every time the performance did not improve for  5 consecutive epochs.  5  Published as a conference paper at ICLR 2016  In order to obtain results that more reliably reﬂect the effect of each individual loss, we repeated each training 100 times with different random splits of the training/validation/testing datasets and different initial random weight values. The results reported in table 1 are the averaged scores ob- tained on the test set over all runs. The standard deviations are reported in parenthesis. Note that these results were computed and averaged on random splits of the training/valid/test datasets in order to be more reliable. We also trained the two best models on the original dataset split of MNIST and results are reported in table 2: they are signiﬁcantly better than those on random splits suggesting that the ofﬁcial testing set is simpler to classify than a randomly extracted one.  4.2 CIFAR10  CIFAR10 (Krizhevsky & Hinton (2009)) is a dataset composed of 60k images of size 32 × 32 × 3 and 10 output categories. For our experiments, we used a large convnet architecture of 14 layers with ﬁlters of size 3 and pooling windows of size 3 (inspired from the architecture of Simonyan & Zisserman (2015)). We used a weight decay, batch normalization (Ioffe & Szegedy (2015)) and random horizontal ﬂips. For the log softmax and the log Taylor softmax, we averaged the testing scores over 10 runs with different splits of the training/validation/testing datasets and dif- ferent initial weight values. We tuned the initial learning rate for each loss function. Table 3 reports the performances on the test set with the different losses.  Table 3: Test set performances of a convolutional network trained on CIFAR10 with different loss functions. For the log-softmax and the log-Taylor softmax, results were averaged over 10 experi- ments in order to be more reliable, the standard deviation being in parenthesis. For the MSE and the log-spherical softmax, we only had time to run a single experiment. The log-Taylor softmax outperforms the log-softmax.  Models MSE Log softmax Log spherical softmax Log Taylor softmax  loss  mse: 0.0251  error rate 9.00%  negll: 0.411 (0.032)  8.52% (0.20)  negll: 0.410  negll: 0.403 (0.034)  8.37%  8.07% (0.12)  4.3 CIFAR100  We used the same network and the same procedure as those of CIFAR10. We did not manage to train the network successfully with the MSE criterion (it yielded 99% error rate). Table 4 reports the performances on the test set with the different losses.  Table 4: Performances on the test set of a convolutional network trained on CIFAR100 with different loss functions. For each loss, results were averaged over 5 experiments in order to be more reliable (the only difference being the initial random parameter values), the standard deviation being in parenthesis. The log-softmax outperforms the spherical losses.  Models Log softmax Log spherical softmax Log Taylor softmax  loss  error rate  negll: 1.69 (0.091) negll: 1.90 (0.053) negll: 1.88 (0.047)  32.4% (0.85) 33.1% (0.97) 33.1% (0.85)  4.4 LANGUAGE MODELING  4.4.1 PENNTREE BANK  We trained word-level language models on the Penntree Bank (Marcus et al. (1993)), which is a cor- pus split into a training set of 929k words, a validation set of 73k words, and a test set of 82k words.  6  Published as a conference paper at ICLR 2016  The vocabulary has 10k words. We trained our neural language model (Bengio et al. (2001)) with vanilla stochastic gradient descent on mini-batches of size 250 using an input context of 6 words. For all the models, the embedding size is 250 and the hidden activation functions are rectiﬁers. For each loss function, we hyper-optimized the learning rate, the number of layers and the number of neurons per layer. For each model, we computed its perplexity on the test set, which is the exponen- tial of the mean negative log-likelihood. We also computed the simlex-999 score (Hill et al. (2014)), which measures the quality of word embeddings based on the similarity between words as evaluated by humans. Table 6 reports the results obtained by the best models for the different losses.  Table 5: Comparison of different losses used to train a neural language model on the Penntree bank dataset. For each loss, the hyperparameters controlling the model architecture have been tuned individually to yield the best perplexity on the validation set. The top 10 error rate measures the proportion of time the target word is among the top 10 predicted words.  Models and losses [1] Log softmax [2] Log spherical softmax [3] Log Taylor softmax [4] Log softmax abs  Perplexity  top 10 error rate  Simlex 999  number of epochs  126.7 149.2 147.2 128.2  0.501 0.508 0.503 0.503  0.109 0.052 0.066 0.0777  6 7 6 7  [1] two hidden layers of 809 neurons each. [2] three hidden layers of 1264 neurons each. ǫ is set to 0.0198. [3] three hidden layers of 1427 neurons each. [4] Same architecture as [1] except that the softmax is applied to the absolute value of the pre-activations.  4.5 ONE BILLION WORD  We also trained word-level neural language models on the One Billion Word dataset (Chelba et al. (2014)), which is composed of 0.8 billion words belonging to a vocabulary of 0.8 million words. For our experiments, we chose to restrict the vocabulary to 10k words. As this training dataset is almost 1000 times bigger than the Penntree bank dataset, we can not even do one full epoch and we are thus constantly in a regime of online learning in which each new training example has not been seen before. As a result, it is almost impossible to overﬁt the training dataset with reasonable size models. Bigger models tend to always perform better, so we chose to restrict ourselves to a few architecture sizes and we compared the different losses on those, rather than doing an exhaustive architecture search for each case, as we did in the experiments on the PennTree bank dataset.  5 DISCUSSION  On MNIST and CIFAR10, the spherical losses work surprisingly well and, for the ﬁxed architectures we used, they even outperform the log-softmax. This suggests that the log-softmax is not necessarily the best loss function for classiﬁcation and that alternatives such as categorical log-losses from the spherical family might be preferred in a broad range of applications.  On the other hand, in our experiments with higher output dimensions, i.e. on CIFAR100, the Pen- ntree bank and the one Billion Word dataset, we found that the log softmax yields better results than the log-spherical softmax and the log-Taylor softmax. The reasons for this apparent qualitative shift as the number of output categories increases remain unclear but we venture two hypothetical leads. The ﬁrst is that the exponential non-linearity in the softmax boosts the large pre-activations rela- tively to the smaller ones a lot more than a squaring operation. It thus yields a stronger competition between pre-activations and a more discriminative behavior. It is possible that the resulting ability to more precisely single out a few top winning classes becomes increasingly crucial as the number of categories grows. Our second lead relies on the fact that the exponential of a linear combination of features is the product of the exponentials of each weighted feature. Each of these exponential factors may represent a probability or an (unnormalized) density and the resulting product of these  7  Published as a conference paper at ICLR 2016  Table 6: Comparison of the different losses used to train a neural language model on the One Billion Word dataset. The log-softmax outperforms the spherical losses, even though adding layers reduces the gap. For the log-softmax, adding hidden layers degrades the simlex score, while it improves it for the spherical losses.  Models and losses [0] Log softmax [0] Log Spherical softmax [0] Log Taylor softmax [1] Log softmax [1] Log spherical softmax [1] Log Taylor softmax [2] Log softmax [2] Log spherical softmax [2] Log Taylor softmax  Perplexity  top 10 error rate  Simlex 999  27.3 72.9 75.8 19.4 29.6 28.9 19.2 29.4 28.4  0.283 0.417 0.421 0.245 0.313 0.313 0.244 0.306 0.309  0.365 0.164 0.168 0.336 0.254 0.262 0.318 0.262 0.265  [0] no hidden layer [1] one hidden layer of 1000 relus [2] two hidden layers of 1000 relus  exponential factors can be seen as computing a conjunction (an ”AND”) of features. This is not possible with the simple squared linear combinations of the spherical losses.  To increase the ﬂexibility of the networks with spherical losses, we tried to increase the non-linearity of the network prior to the loss by adding layers and by replacing the rectiﬁers by stronger nonlin- earities. In particular, we tried to use a softmax as the activation function of our last hidden layer. We also tried to use directly the exponential as the activation function of the hidden layers. In this case, to avoid excessive values of the exponential, we used a truncated version of it and we also used batch normalization (Ioffe & Szegedy (2015)) to obtain reasonable ranges of pre-activations to prevent the exponential from exploding. Although we were able to train these models, they did not perform better than simple rectiﬁer layers.  Among the approaches we explored, upper-bounding the negative log softmax with a spherical loss was an unsuccessful attempt. The spherical losses we tried were more promising and in particular the log-Taylor Softmax seems the most appropriate for classiﬁcation. Contrary to the spherical softmax, it does not require the extra hyperparameter ǫ, which can make the spherical softmax quite unstable and difﬁcult to train. It also has a small asymmetry, which may be a desirable property.  CONCLUSION  Our experiments showed that for several low dimensional problems, the log-softmax is surprisingly outperformed by certain losses of the spherical family, in particular the log-Taylor softmax. On the other hand, in higher dimensional problems, the log-softmax yields better results. The reasons of this qualitative shift remain unclear and further research should be carried out to understand it.  ACKNOWLEDGMENTS  We would like to thank Harm de Vries for helpful discussions about the optimization of the log spherical softmax and for providing us with a good baseline model for CIFAR10.  REFERENCES  Bengio, Yoshua, Ducharme, R´ejean, and Vincent, Pascal. A neural probabilistic language model. In  NIPS 13. MIT Press, 2001.  8  Published as a conference paper at ICLR 2016  Bouchard, Guillaume. Efﬁcient bounds for the softmax function and applications to approximate In NIPS 2007 Workshop for Approximate Bayesian Inference in  inference in hybrid models. Continuous/Hybrid Systems. Citeseer, 2007.  Chelba, Ciprian, Mikolov, Tomas, Schuster, Mike, Ge, Qi, Brants, Thorsten, Koehn, Phillipp, and Robinson, Tony. One billion word benchmark for measuring progress in statistical language modeling. INTERSPEECH 2014, 2014.  Gutmann, M. and Hyvarinen, A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of The Thirteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS’10), 2010.  He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers: Sur- passing human-level performance on imagenet classiﬁcation. IEEE International Conference on Computer Vision (ICCV), 2015.  Hill, Felix, Reichart, Roi, and Korhonen, Anna. Simlex-999: Evaluating semantic models with  (genuine) similarity estimation. CoRR, abs/1408.3456, 2014.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Blei, David and Bach, Francis (eds.), Proceedings of the 32th International Conference on Machine Learning (ICML-15). JMLR Workshop and Conference Proceedings, 2015.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images.  Technical report, University of Toronto, 2009.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  Marcus, Mitchell P, Marcinkiewicz, Mary Ann, and Santorini, Beatrice. Building a large annotated  corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.  Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., and Dean, J. Distributed representations of  words and phrases and their compositionality. In NIPS’2013, pp. 3111–3119. 2013.  Mnih, Andriy and Kavukcuoglu, Koray. Learning word embeddings efﬁciently with noise- contrastive estimation. In Burges, C.J.C., Bottou, L., Welling, M., Ghahramani, Z., and Wein- berger, K.Q. (eds.), Advances in Neural Information Processing Systems 26, pp. 2265–2273. Cur- ran Associates, Inc., 2013.  Morin, Frederic and Bengio, Yoshua.  guage model. ings tics, pp. 246–252. Society for Artiﬁcial http://www.iro.umontreal.ca/˜lisa/pointeurs/hierarchical-nnlm-aistats05.pdf.  Hierarchical probabilistic neural network lan- In Cowell, Robert G. and Ghahramani, Zoubin (eds.), Proceed- Statis- URL  Intelligence and Statistics,  International Workshop  on Artiﬁcial  Intelligence  the Tenth  2005.  and  of  Ollivier, Yann. Riemannian metrics for neural networks. CoRR, abs/1303.0818, 2013. URL  http://arxiv.org/abs/1303.0818.  Shrivastava, Anshumali and Li, Ping.  maximum inner product search (MIPS). C., Lawrence, N.D., (eds.), Advances tion Processing Systems 27, pp. 2321–2329. Curran Associates, http://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner  time In Ghahramani, Z., Welling, M., Cortes, Informa- URL  Asymmetric LSH (ALSH)  and Weinberger, K.Q.  in Neural Inc., 2014.  for sublinear  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. In ICLR, 2015.  Sutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initial-  ization and momentum in deep learning. In ICML, 2013.  Vincent, Pascal, de Br´ebisson, Alexandre, and Bouthillier, Xavier. Efﬁcient exact gradient update  for training deep networks with very large sparse targets. NIPS, 2015.  9  ",
1511.06747,2016,Data-Dependent Path Normalization in Neural Networks,"['Data-Dependent Path Normalization in Neural Networks\nBehnam Neyshabur', 'Ryota Tomioka', 'Ruslan Salakhutdinov', 'Nathan Srebro']",https://arxiv.org/pdf/1511.06747,"6 1 0 2     n a J    9 1      ]  G L . s c [      4 v 7 4 7 6 0  .  1 1 5 1 : v i X r a  Under review as a conference paper at ICLR 2016  DATA-DEPENDENT PATH NORMALIZATION IN NEURAL NETWORKS  Behnam Neyshabur Toyota Technological Institute at Chicago Chicago, IL 60637, USA bneyshabur@ttic.edu  Ryota Tomioka Microsoft Research Cambridge, UK ryoto@microsoft.com  Ruslan Salakhutdinov Department of Computer Science University of Toronto, Canada rsalakhu@cs.toronto.edu  Nathan Srebro Toyota Technological Institute at Chicago Chicago, IL 60637, USA nati@ttic.edu  ABSTRACT  We propose a uniﬁed framework for neural net normalization, regularization and optimization, which includes Path-SGD and Batch-Normalization and interpolates between them across two different dimensions. Through this framework we inves- tigate the issue of invariance of the optimization, data dependence and the connec- tion with natural gradients.  1  INTRODUCTION  The choice of optimization method for non-convex, over-parametrized models such as feed-forward neural networks is crucial to the success of learning—not only does it affect the runtime until con- vergence, but it also effects which minimum (or potentially local minimum) we will converge to, and thus the generalization ability of the resulting model. Optimization methods are inherently tied to a choice of geometry over parameter space, which in turns induces a geometry over model space, which plays an important role in regularization and generalization (Neyshabur et al., 2015c). In this paper, we focus on two efﬁcient alternative optimization approaches proposed recently for feed-forward neural networks that are based on intuitions about parametrization, normalization and the geometry of parameter space: Path-SGD (Neyshabur et al., 2015a) was derived as steepest de- scent algorithm with respect to particular regularizer (the (cid:96)2-path regularizer, i.e. the sum over all paths in the network of the squared product over all weights in the path (Neyshabur et al., 2015b)) and is invariant to weight reparametrization. Batch-normalization (Ioffe & Szegedy, 2015) was de- rived by adding normalization layers in the network as a way of controlling the variance of the input each unit receives in a data-dependent fashion. In this paper, we propose a uniﬁed framework which includes both approaches, and allows us to obtain additional methods which interpolate between them. Using our uniﬁed framework, we can also tease apart and combine two different aspects of these two approaches: data-dependence and invariance to weight reparametrization. Our uniﬁed framework is based on ﬁrst choosing a per-node complexity measure we refer to as γv (deﬁned in Section 3). The choice of complexity measure is parametrized by a choice of “nor- malization matrix” R, and different choices for this matrix incorporate different amounts of data dependencies: for path-SGD, R is a non-data-dependent diagonal matrix, while for batch normal- ization it is a data-dependent covariance matrix, and we can interpolate between the two extremes. Once γv is deﬁned, and for any choice of R, we identify two different optimization approaches: one relying on a normalized re-parameterization at each layer, as in batch normalization (Section 4), and the other an approximate steepest descent as in path-SGD, which we refer to as DDP-SGD (Data Dependent Path SGD) and can be implemented efﬁciently via forward and backward propagation on the network (Section 5). We can now mix and match between the choice of R (i.e. the extent of data dependency) and the choice of optimization approach.  1  Under review as a conference paper at ICLR 2016  One particular advantage of the approximate steepest descent approach (DDP-SGD) over the nor- malization approach is that it is invariant to weight rebalancing (discussed in Section 6). This is true regardless of the amount of data-dependence used. That is, it operates more directly on the model (the function deﬁned by the weights) rather than the parametrization (the values of the weights them- selves). This brings us to a more general discussion of parametrization invariance in feedforward networks (Section 7). Our uniﬁed framework and study of in invariances also allows us to relate the different optimization approaches to Natural Gradients (Amari, 1998). In particular, we show that DDP-SGD with full data-dependence can be seen as an efﬁcient approximation of the natural gradient using only the diagonal of the Fisher information matrix (Section 5).  RELATED WORKS  There has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training (Le Cun et al., 1998; Larochelle et al., 2009; Glorot & Bengio, 2010; Sutskever et al., 2013). Natural gradient algorithm (Amari, 1998) is known to have a very strong invariance property; it is not only invariant to reparametrization, but also to the choice of network architecture. However it is known to be computationally demanding and thus many approximations have been proposed (Grosse & Salakhudinov, 2015; Martens & Grosse, 2015; Desjardins et al., 2015). However, such approximations make the algorithms less invariant than the original natural gradient algorithm. Pascanu & Bengio (2014) also discuss the connections between Natural Gradients and some of the other proposed methods for training neural networks, namely Hessian-Free Optimization (Martens, 2010), Krylov Subspace Descent (Vinyals & Povey, 2011) and TONGA (Roux et al., 2008). Ollivier (2015) also recently studied the issue of invariance and proposed computationally efﬁcient approximations and alternatives to natural gradient. They study invariances as different mappings from parameter space to the same function space while we look at the invariances as transformations (inside a ﬁxed parameter space) to which the function is invariant in the model space (see Section 7). Unit-wise algorithms suggested in Olivier’s work are based on block-diagonal approximations of Natural Gradient in which blocks correspond to non-input units. The computational cost of the these unit-wise algorithms is quadratic in the number of incoming weights. To alleviate this cost, Ollivier (2015) also proposed quasi-diagonal approximations which avoid the quadratic dependence but they are only invariant to afﬁne transformations of activation functions. The quasi-diagonal approximations are more similar to DDP-SGD in terms of computational complexity and invariances (see Section 6). In particular, ignoring the non-diagonal terms related to the biases in quasi-diagonal natural gradient suggested in Ollivier (2015), it is then equivalent to diagonal Natural Gradient which is itself equivalent to special case of DDP-SGD when Rv is the second moment (see Table 1 and the discussion on relation to the Natural Gradient in Section 5).  2 FEEDFORWARD NEURAL NETS  We brieﬂy review our formalization and notation of feedforward neural nets. We view feedforward neural networks as a parametric class of functions mapping input vectors to output vectors, where parameters correspond to weights on connections between different units. We focus speciﬁcally on networks of ReLUs (Rectiﬁed Linear Units). Rather than explicitly discussing units arranged in layers, it will be easier for us (and more general) to refer to the connection graph as a directed acyclic graph G(V, E) over the set of node V , corresponding to units v ∈ V in the network. V includes the inputs nodes Vin (which do not have any incoming edges), the output nodes Vout (which do not have any outgoing edges) and additional internal nodes (possibly arranged in multiple layers). Each directed edge (u → v) ∈ E (i.e. each connection between units) is associated with a weight wu→v. Given weight settings w for each edge, the network implements a function fw : R|Vin| → R|Vout| as follows, for any input x ∈ R|Vin|:  • For each internal node v we deﬁne recursively zv =(cid:80)  • For the input nodes v ∈ Vin, their output hv is the corresponding coordinate of the input x. (u→v)∈E wu→v · hu and hv = [zv]+ where [z]+ = max(z, 0) is the ReLU activation function and the summation is over all edges incoming into v.  2  Under review as a conference paper at ICLR 2016  Meaning  Symbol x / y w  Symbol Vin / Vout wu→v wv→ the vector of incoming weights to v w→v N in(v) N out(v)  input vector / label the parameter vector  the set of nodes feeding into v  hv  the output value of node v  zv  Meaning  the set of input / output nodes the weight of the edge (u → v)  the vector of outgoing weights from v  the set of nodes that v feeds into the activation value of node v  Figure 1: An example of layered feedforward networks and notation used in the paper  • For output nodes v ∈ Vout we also have zv =(cid:80)  (u→v)∈E wu→v·hu, and the corresponding coordinate of the output fw(x) is given by zv. No non-linearity is applied at the output nodes, and the interpretation of how the real-valued output corresponds to the desired label is left to the loss function (see below).  • In order to also allow for a “bias” at each unit, we can include an additional special node vbias that is connected to all internal and output nodes, where hvbias = 1 always (vbias can thus be viewed as an additional input node whose value is always 1).  We denote N in(v) = {u|(u → v) ∈ E} and N out(v) = {u|(v → u) ∈ E}, the sets of nodes feeding into v, and w→v ∈ R|N in(v)| for the vector of weights of unit v, so that zv =(cid:10)w→v, hN in(v) into v and that v feeds into. We can then write hN in(v) ∈ R|N in(v)| for the vector of outputs feeding  (cid:11).  recover the layered recursive formula hv = [(cid:10)w→v, hVi−1  We do not restrict to layered networks, nor do we ever need to explicitly discuss layers, and can instead focus on a single node at a time (we view this as the main advantage of the graph notation). But to help those more comfortable with layered networks understand the notation, let us consider a layered fully-connected network: The nodes are partitioned into layers V = V0 ∪ V1 ∪ . . . Vd, with Vin = V0, Vout = Vd. For all nodes v ∈ Vi on layer i, N in(v) is the same and equal to N in(v) = Vi−1, and so hN in(v) = hVi−1 consists of all outputs from the previous layer and we W ∈ R|Vi|×|Vi−1| is a matrix with entries wu→v, for each u ∈ Vi−1, v ∈ Vi. This description ignores the bias term, which could be modeled as a direct connection from vbias into every node on every layer, or by introducing a bias unit (with output ﬁxed to 1) at each layer. Please see Figure 1 for an example of a layered feedforward network and a summary of notation used in the paper. We consider supervised training tasks, where each input x is associated with a desired label y and how well the network captures this label is quantiﬁed by a loss function (cid:96)(fw(x), y). For example, in a classiﬁcation problem y is one of |Vout| classes and a cross-entropy (soft-max) loss might be used. We also refer to a source distribution D(x,y) over input-label pairs, where the goal is to minimize the expected loss:  (cid:11)]+ and hVi = [WihVi−1 ]+, where  LD(w) = E(x,y)∼D [(cid:96)(fw(x), y)]  (1)  All expectations, unless speciﬁed otherwise, refer to expectation w.r.t. this source distribution.  INVARIANCES AND NODE-WISE RESCALING  Once the architecture (graph G) is ﬁxed, every choice of weight w deﬁnes a function fw. But this parameterization is not unique–the same function f could be parameterized by two different  3  𝑣ℎ#𝑢𝑁in(𝑣)𝐰→#𝐰#→𝑁Out(𝑣).	  .	  ..	  .	  ..	  .	  ..	  .	  ..	  .	  ..	  .	  ..	  .	  ..	  .	  .𝑉in𝑉234Softmax+cross-­entropy,Squared  error,or  …Input	  vector	  𝐱𝑓𝐰(𝑥)ℓ𝓁(𝑓𝐰𝐱,𝐲)=𝒛𝒗E𝑧#=𝒘→#,𝐡	  	  	  	  	  	  	  	  	  𝑁in(𝑣)Under review as a conference paper at ICLR 2016  (cid:1) (cid:17)  (cid:105)  Rv  D = diag(cid:0)γ2 (cid:16) M = E(cid:104)  N in(v) hN in(v)) hN in(v))h(cid:62) αM + (1 − α)D αC + (1 − α)D  C = Cov  N in(v))  Node-wise Rescaling Invariant  Measure Path-Norm Variance  Second Moment  Normalized reparametrization Diagonal steepest descent Unit-wise Path-Normalization  Path-SGD  Batch-Normalization  Diag. Natural Gradient  DDP-Norm  DDP-Normalization  Yes  No  DDP-SGD  Yes  Table 1: Some of the choices for Rv in the proposed uniﬁed framework.  weight settings (i.e. we could have fw = fw(cid:48) even though w (cid:54)= w(cid:48)). Ideally, we’d like to work as directly as possibly on the functions rather then the parameterization. It is therefor important to understand different “invariances”, i.e. different transformations that can be applied to the weights without changing the function. We note that the notion of invariance we deﬁne here is tied to a ﬁxed network architecture G: we do not consider transformation that changes the network architecture, such as insertion of a linear layer (as in, e.g. Ollivier, 2015). We say that network G is invariant to an invariant transformation T (w), iff for any weight setting w, fw = fT (w). We say that an update rule A (e.g. a rule for obtaining the next iterate from the current iterate in an optimization procedure) is invariant to transformation T iff for any weight setting w, fA(w) = fA(T (w)). That is, whether we start an iterative optimization procedure using updates A at w or the at the equivalent w(cid:48) = T (w), we would always be working on the same function (only with a different parameterization). An important invariance in feedforward ReLU network is node-wise rescaling (or rebalancing). For any positive scalar ρ and for any internal node v (v /∈ Vin and v /∈ Vout), we can scale all the incoming weights into v by ρ and all the outgoing weights by 1/ρ without changing the computation in the networks. That is, the following transformation w(cid:48) = T (w) satisﬁes fw = fw(cid:48):  w(cid:48) v→u = ρwv→u w(cid:48) u→v = ρ−1wu→v w(cid:48) u→u(cid:48) = wu→u(cid:48)  (∀u ∈ N out(u)), (∀u ∈ N in(v))  (otherwise)  (2)  We can combined multiple such rescalings to push the scaling up or down the network without changing the computation. One of our goals is obtaining optimization algorithms that are invariant to such transformations.  3 A UNIFIED FRAMEWORK  (cid:113)  We deﬁne a complexity measure on each node as follows:  w(cid:62)→vRvw→v  γv(w) =  (3) where Rv is a positive semideﬁnite matrix that could depend on the computations feeding into v, and captures both the complexity of the nodes feeding into v and possibly their interactions. We consider several possibilities for Rv, summarized also in Table 1.  (cid:1) to a diagonal matrix consisting of the complexities  A ﬁrst possibility is to set Rv = diag(cid:0)γ2  of the incoming units. This choice does not depend on the source distribution (i.e. the data), and also ignores the effect of activations (since the activation pattern depends on the input distribution) and of dependencies between different paths in the network. Intuitively, with this choice of Rv, the measure γv(w) captures the “potential” (data independent) variability or instability at the node. Another possibility is to set Rv to either the covariance (centralized second moment) or to the (un- centralized) second moment matrix of the outputs feeding into v. In this case, γ2 v (w) would evaluate to the variance or (uncentralized) second moment of zv. We could also linearly combined the data independent measure, which measures inherent instability, with one of these the data-dependent measure to obtain:  N in(v)  v (w) = αS(zv) + (1 − α) γ2  γ2 u(w)w2  u→v  (v /∈ Vin),  (4)  (cid:88)  u∈N in(v)  4  Under review as a conference paper at ICLR 2016  (cid:88)  where S(zv) is either the variance or uncentralized second moment, and α is a parameter. The complexity measure above is deﬁned for each node of the network separately, and propagates through the network. To get an overall measure of complexity we sum over the output units and deﬁne the following complexity measure for the function fw as represented by the network:  (5)  γ2 net(w) =  γ2 v (w).  For Rv = diag(cid:0)γ2  N in(v)  (cid:1), this complexity measure agrees with the (cid:96)2-Path-regularizer as introduced  v∈Vout  net(w), as the Data-Dependent-Path (DDP) regularizer.  by Neyshabur et al. (2015b). This is the sum over all paths in the network of the squared product of weights along the path. The path-regularizer is also equivalent to looking at the minimum over all “node rescalings” of w (i.e. all possibly rebalancing of weights yielding the same function fw) of the maxv (cid:107)w→v(cid:107). But, unlike this max-norm measure, the path-regularizer does not depend on the rebalancing and is invariant to node rescalings (Neyshabur et al., 2015b). For data-dependent choices of Rv, we also get a similar invariance property. We refer to the resulting complexity measure, γ2 After choosing Rv, we will think of γv as specifying the basic “geometry” and bias (for both opti- mization and learning) over weights. In terms of learning, we will (implicitly) prefer weights with smaller γv measure, and correspondingly in terms of optimization we will bias toward smaller γv “balls” (i.e. search over the part of the space where γv is smaller). We will consider two basic ways of doing this: In Section 4 we will consider methods that explicitly try to keep γv small for all internal nodes in the network, that is explicitly search over simpler weights. Any scaling is pushed to the output units, and this scaling hopefully does not grow too much due. In Section 5 we will consider (approximate) steepest descent methods with respect to the overall γnet, i.e. updates that aim at improving the training objective while being small in terms of their effect on γnet.  4 DDP-NORMALIZATION: A BATCH-NORMALIZATION APPROACH  In this Section, we discuss an optimization approach based on ensuring γv for all internal nodes v are ﬁxed and equal to one—that is, the complexity of all internal nodes is “normalized”, and any  scaling happens only at the output nodes. We show that with a choice of Rv = Cov(cid:0)hN in(v))  is essentially equivalent to Batch Normalization (Ioffe & Szegedy, 2015). Batch-Normalization Ioffe & Szegedy (2015) was suggested as an alternate architecture, with special “normalization” layers, that ensure the variance of node outputs are normalized throughout training. Considering a feed-forward network as a graph, for each node v, the Batch-Normalization archi- tecture has as parameters an (un-normalized) incoming weight vector ˜w and two additional scalars cv, bv ∈ R specifying scaling and shift respectively. The function computed by the network is then given by a forward propagation similar to standard feed-forward ReLU networks as described in Section 2, except that for each node an un-normalized activation is ﬁrst computed:  (cid:1), this  Then, this activation is normalized to obtain the normalized activation, which is also scaled and shifted, and the output of the unit is the output of the activation function for this activation value:  (cid:11)  ˜zv =(cid:10) ˜w→v, hN in(v) (cid:112)Var(˜zv)  ˜zv − E[ ˜zv]  zv = cv  + bv  (6)  (7)  hv = [zv]+  The variance and expectation are actually calculated on a “mini-batch” of training examples, giving the method its name. Batch-normalization then proceeds by training the architecture speciﬁed in (6) and (7) through mini-batch stochastic gradient descent, with each gradient mini-batch also used for estimating the variance and expectation in (7) for all points in the mini-batch. Instead of viewing batch-normalization as modifying the architecture, or forward propagation, we can view it as a re-parameterization, or change of variables, of the weights in standard feed-forward networks as speciﬁed in Section 2. In particular, instead of specifying the weights directly through  5  Under review as a conference paper at ICLR 2016  w, we specify them through ˜w, b and c, with the mapping:  v = ˜w(cid:62) ˜γ2  wu→v =  →vRv ˜w→v  (cid:40)c ˜wu→v  ˜γv  b − c  Rv = Cov(hN in(v)) u (cid:54)= vbias u = vbias  ˜γv  E[(cid:104) ˜w→v,hNin(v)(cid:105)]  (8)  (9)  The model class of functions used by Batch-Normalization is thus exactly the same model class corresponding to standard feed-forward network, just the parameterization is different. However, the change of variables from w to ˜w, b, c changes the geometry implied by the parameter space, and consequently the trajectory (in model space) of gradient updates—effectively transforming the gradient direction by the Jacobian between the two parameterizations. Batch-Normalization can thus be viewed as an alternate optimization on the same model class as standard feed-forward networks, but with a different geometry. The reparametrization ensures that γv(w) = cv for all nodes—that is, the complexity is explicit in the parameterization and thus gets implicitly regularized through the implicit regularization inherent in stochastic gradient updates. The re-parameterization (9) is redundant and includes more parameters than the original parame- terization w—in addition to one parameter per edge, it includes also two additional parameters per node, namely the shift bv and scaling cv. The scaling parameters at internal nodes can be avoided and removed by noting that in ReLU networks, due to the node-rescaling property, all scaling can be done at the output nodes. That is, ﬁxing cv = 1 for all internal v does not actually change the model class (all functions realizable by the model can be realized this way). Similarly, we can also avoid the additional shift parameter bv and rely only on bias units and bias weights ˜wvbias→v that get renormalized together with weights. The bias term ˜wvbias→v does not affect normalization (since it is deterministic and so has no effect on the variance), it just gets rescaled with the other weights. We thus propose using a simpler reparametrization (change of variables), with the same number of parameters, using only ˜w and deﬁning for each internal unit:  wu→v =  ˜wu→v  ˜γv  (10)  with ˜γv as in (8), and with the output nodes un-normalized: w→Vout = ˜w→Vout. This ensures that for all internal nodes γv(w) = 1. Going beyond Batch-Normalization, we can also use the same approach with other choices of Rv, including all those in Table 1: We work with a reparametrization ˜w, deﬁned through (8) and (10) but with different choices of Rv, and take gradient (or stochastic gradient) steps with respect to ˜w. Expectations in the deﬁnition of Rv can be estimated on the stochastic gradient descent mini- batch as in Batch-Normalization, or on independent samples of labeled or unlabeled examples. We refer to such methods as “DDP-Normalized” optimization. Gradients in DDP-Normalization can be calculated implemented very efﬁciently similar to Batch-Normalization (see Appendix A.1). When using this type of DDP-Normalization, we ensure that for any internal node γv(w) = 1 (the value of ˜γv can be very different from 1, but what is ﬁxed is the value of γv as deﬁned in (3) in terms of the weights w, which in turn can be derived from ˜w through (9)), and so the overall complexity γnet(w) depends only on the scaling at the output layer. Another interesting property of DDP-Normalization updates is that for any internal node v, the updates direction of ˜w→v is exactly orthogonal to the weights: Theorem 1. For any weight ˜w in DDP-Normalization and any non-input node v /∈ Vin  (cid:28)  ˜w→v,  ∂L  ∂ ˜w→v  (cid:29)  = 0  (11)  that  the gradient  The fact to the parameters means weight updates in DDP- Normalization are done in a way that it prevents the norm of weights to change considerably after each updates (the proof is given in Appendix C).  is orthogonal  6  Under review as a conference paper at ICLR 2016  5 DDP-SGD  We now turn to a more direct approach of using our complexity measure for optimization. To do so, let us ﬁrst recall the strong connection between geometry, regularization and optimization through the speciﬁc example of gradient descent. Gradient descent can be thought of as steepest descent with respect to the Euclidean norm—that is, it takes a step in a direction that maximizes improvement in the objective while also being small in terms of the Euclidean norm of the step. The step can also be viewed as a regularized optimiza- tion of the linear approximation given by the gradient, where the regularizer is squared Euclidean norm. Gradient Descent is then inherently linked to the Euclidean norm—runtime of optimization is controlled by the Euclidean norm of the optimum and stochastic gradient descent yields implicit Euclidean norm regularization. A change in norm or regularizer, which we think of as a change of geometry, would then yield different optimization procedure linked to that norm. What we would like is to use the DDP-regularizer γnet(w) to deﬁne our geometry, and for that we need a distance (or divergence) measure corresponding to it by which we can measure the “size” of each step, and require steps to be small under this measure. We cannot quite do this, but instead we use a diagonal quadratic approximation of γnet(w) about our current iterate, and then take a steepest descent step w.r.t. the quadratic norm deﬁned by this approximation. Speciﬁcally, given a choice of Rv and so complexity measure γnet(w), for the current iterate w(t) we deﬁne the following quadratic approximation:  ˆγ2 net(w(t) + ∆w) = γ2  net(w(t)) +  and the corresponding quadratic norm:  (cid:107)w(cid:48) − w(cid:107)2  ˆγ2  net  = (cid:107)w(cid:48) − w(cid:107)2  diag( 1  We can now deﬁne the DDP-update as:  w(t+1) = min w  η  +  (cid:68)∇γ2  net(w(t)), ∆w  (cid:69) (cid:88) (cid:68)∇L(w), w − w(t)(cid:69)  net(w(t))) =  2∇2γ2  (u→v)∈G  (cid:16)∇2γ2  (cid:17)  ∆w(cid:62) diag  1 2  net(w(t))  ∆w (12)  1 2  ∂2γ2 ∂w2  net u→v  (w(cid:48)  u→v − wu→v)2.  (13)  (cid:107)w(cid:48) − w(cid:107)2  ˆγ2  net  1 2  .  (14)  +  Another way of viewing the above approximation is as taking a diagonal quadratic approximation of the Bergman divergence of the regularizer. Solving (14) yields the update:  u→v = wu→v − w(t+1)  η  ∂L  κu→v(w)  ∂wu→v  (w(t))  where: κu→v(w) =  1 2  ∂2γ2 ∂w2  net u→v  .  (15)  ∂L  Instead of using the full gradient, we can also use a limited number of training examples to obtain stochastic estimates of  (w(t))—we refer to the resulting updates as DDP-SGD.  ∂wu→v For the choice Rv = diag(γ2 net is the Path-norm and we recover Path-SGD Neyshabur et al. (2015a). As was shown there, the Path-SGD updates can be calculated efﬁciently using a forward and backward propagation on the network, similar to classical back-prop. In Ap- pendix A.2 we show how this type of computation can be done more generally also for other choices of Rv in Table 1.  N in(v)), we have that γ2  RELATION TO THE NATURAL GRADIENT  The DDP updates are similar in some ways to Natural Gradient updates, and it is interesting to understand this connection. Like the DDP, the Natural Gradients direction is a steepest descent direction, but it is based on a divergence measure calculated directly on the function fw, and not the parameterization w, and as such is invariant to reparametrizations. The natural gradient is deﬁned as a steepest descent direction with respect to the KL-divergence between probability distributions, and so to refer to it we must refer to some probabilistic model. In our case, this will be a conditional probability model for labels y conditioned on the inputs x, taking expectation with respect to the true marginal data distribution over x.  7  Under review as a conference paper at ICLR 2016  What we will show that for the choice Rv = E[hN in(v)h(cid:62) N in(v)], the DDP update can also be viewed as an approximate Natural Gradient update. More speciﬁcally, it is a diagonal approximation of the Natural Gradient for a conditional probability model q(y|x; w) (of the labels y given an input x) parametrized by w and speciﬁed by adding spherical Gaussian noise to the outputs of the network: y|x ∼ N (fw(x), I|Vout|). Given the conditional probability distribution q(y|x; w), we can calculate the expected Fisher infor- mation matrix. This is a matrix indexed by parameters of the model, in our case edges e = (u → v) on the graph and their corresponding weights we, with entries deﬁned as follows: ∂ log q(y|x; w)  (cid:20) ∂ log q(y|x; w)  (16) where x ∼ p(x) refers to the marginal source distribution (the data distribution). That is, we use the true marginal distributing over x, and the model conditional distribution y|x, ignoring the true labels. The Natural Gradient updates can then be written as(see appendix B for more information):  F (w)[e, e(cid:48)] = Ex∼p(x)Ey∼q(y|x;w)  ∂we(cid:48)  (cid:21)  ∂we  ,  w(t+1) = w(t) − ηF (w(t))−1∇wL(w(t)).  (17)  If we approximate the Fisher information matrix with its diagonal elements, the update step normal- izes each dimension of the gradient with the corresponding element on the diagonal of the Fisher information matrix:  w(t+1)  e  = w(t)  e −  η  F (w)[e, e]  ∂L ∂we  (w(t)).  (18)  Using diagonal approximation of Fisher information matrix to normalize the gradient values has been suggested before as a computationally tractable alternative to the full Natural Gradient (LeCun et al., 1998; Schaul et al., 2013). Ollivier (2015) also suggested a “quasi-diagonal” approxima- tions that includes, in addition to the diagonal, also some non-diagonal terms corresponding to the relationship between the bias term and every other incoming weight into a unit. For our Gaussian probability model, where log q(y|x) = 1 be calculated as:  2 (cid:107)y − fw(x)(cid:107)2 + const, the diagonal can  (cid:34) (cid:88)  (cid:18) ∂fw(x)[v(cid:48)]  (cid:19)2(cid:35)  ,  (19)  F (w)[e, e] = Ex∼p(x)  v(cid:48)∈Vout  ∂we  using (37). We next prove that this update is equivalent to DDP-SGD for a speciﬁc choice of Rv, namely the second moment. Theorem 2. The Diagonal Natural Gradient indicated in equations (18) and (19) is equivalent to  DDP-SGD for Rv = E(cid:104)  Proof. We calculate the scaling factor κu→v(w) for DDP-SGD as follows:  hN in(v)h(cid:62)  .  =  1 2  N in(v)  (cid:105) (cid:88) (cid:18) (cid:20) (cid:19)2(cid:35) (cid:34) (cid:18) ∂zv(cid:48) (cid:19)2(cid:35) (cid:18) ∂fw(x)[v(cid:48)]  v(cid:48)∈Vout E  ∂zv  h2 u  zv(cid:48)  ∂  ∂2E[z2 v(cid:48)] ∂w2 u→v  =  (cid:21)(cid:19)  ∂zv(cid:48)  ∂wu→v  (cid:34)  = E  h2 u  ∂wu→v  1 2  v(cid:48)∈Vout  net u→v  ∂2γ2 ∂w2  (cid:88) (cid:88) (cid:34) (cid:88)  E  v(cid:48)∈Vout  v(cid:48)∈Vout  ∂we  κu→v(w) =  =  =  (cid:19)  ∂E[z2 v(cid:48)] ∂wu→v  (cid:20)  E  zv(cid:48)hu  (cid:21)(cid:19)  ∂zv(cid:48) ∂zv  (cid:18) 1 (cid:18)  2  ∂wu→v  v(cid:48)∈Vout  ∂  (cid:88) (cid:88) (cid:18) ∂zv(cid:48) (cid:88)  v(cid:48)∈Vout  =  v(cid:48)∈Vout  ∂zv  ∂wu→v  ∂  (cid:19)2(cid:35)  = E  = F (w)[u → v, u → v]  Therefore, the scaling factors in DDP-SGD with Rv = E(cid:104)  (cid:105)  are exactly the diagonal  hN in(v)h(cid:62)  N in(v)  elements of the Fisher Information matrix used in the Natural Gradient updates.  8  Under review as a conference paper at ICLR 2016  6 NODE-WISE INVARIANCE  In this section, we show that DDP-SGD is invariant to node-wise rescalings (see Section 2), while DDP-Normalization does not have favorable invariance properties.  6.1 DDP-SGD ON FEEDFORWARD NETWORKS  In Section 2, we observed that feedforward ReLU networks are invariant to node-wise rescaling. To see if DDP-SGD is also invariant to such rescaling, consider a rescaled w(cid:48) = T (w), where T is a rescaling by ρ at node v as in (2). Let w+ denote the weights after a step of DDP-SGD. To establish invariance to node-rescaling we need to show that w(cid:48)+ = T (w+). For the outgoing weights from v we have:  v→j = ρwv→j − ρ2η w(cid:48)+  κv→j(w)  (cid:18)  = ρ  wv→j −  η  κv→j(w)  ∂wv→j  ∂L  ρ∂wv→j ∂L  (w)  (cid:19)  (w)  = ρw+  v→j  Similar calculations can be done for incoming weights to the node v. The only difference is that ρ will be substituted by 1/ρ. Moreover, note that due to non-negative homogeneity of ReLU activation function, the updates for the rest of the weights remain exactly the same. Therefore, DDP-SGD is node-wise rescaling invariant.  6.2 SGD ON DDP-NORMALIZED NETWORKS  Since DDP-Normalized networks are reparametrization of feedforward networks, their invariances are different. Since the operations in DDP-Normalized networks are based on ˜w, we should study the invariances for ˜w. The invariances in this case are given by rescaling of incoming weights into a node, i.e. for an internal node v and scaling ρ > 0: T ( ˜w)k→v = ρ ˜wk→v  (∀k ∈ N in(v))  while all other weights are unchanged. The DDP-Normalized networks are invariant to the above transformation because the output of each node is normalized. The SGD update rule is however not invariant to this transformation:  (cid:18)  (cid:19)  ˜wk→v − η  ∂L  ∂ ˜wk→v  ( ˜w)  = ρ ˜w+  k→v  T ( ˜w)+  k→v = ρ ˜wk→v − η  ∂L  ρ∂ ˜wk→v  ( ˜w) (cid:54)= ρ  7 UNDERSTANDING INVARIANCES  The goal of this section is to discuss whether being invariant to node-wise rescaling transformations is sufﬁcient or not. Ideally we would like our algorithm to be at least invariant to all the transformations to which the model G is invariant. Note that this is different than the invariances studied in Ollivier (2015), in that they study algorithms that are invariant to reparametrizations of the same model but we look at transformations within the the parameter space that preserve the function in the model. This will eliminate the need for non-trivial initialization. Thus our goal is to characterize the whole variety of transformations to which the model is invariant and check if the algorithm is invariant to all of them. We ﬁrst need to note that invariance can be composed. If a network G is invariant to transformations T1 and T2, it is also invariant to their composition T1 ◦ T2. This is also true for an algorithm. If an algorithm is invariant to transformations T1 and T2, it is also invariant to their composition. This is because fT2◦T1◦A(w) = fT2◦A(T1◦w) = fA(T2◦T1(w)). Then it is natural to talk about the basis of invariances. The intuition is that although there are inﬁnitely many transformations to which the model (or an algorithm) is invariant, they could be generated as compositions of ﬁnite number of transformations.  9  Under review as a conference paper at ICLR 2016  In fact, in the inﬁnitesimal limit the directions of inﬁnitesimal changes in the parameters to which the function fw is insensitive form a subspace. This is because for a ﬁxed input x, we have  fw+∆(x) = fw(x) +  ∂fw(x)  ∂we  · ∆e + O((cid:107)∆(cid:107)2),  (20)  e∈E  where E is the set of edges, due to a Taylor expansion around w. Thus the function fw is insensitive (up to O((cid:107)∆(cid:107)2)) to any change in the direction ∆ that lies in the (right) null space of the Jacobian matrix ∂fw(x)/∂w for all input x simultaneously. More formally, the subspace can be deﬁned as  (cid:88)  (cid:92)  N (w) =  x∈R|Vin| Null  .  (21)  (cid:18) ∂fw(x)  (cid:19)  ∂w  Again, any change to w in the direction ∆ that lies in N (w) leaves the function fw unchanged (up to O((cid:107)∆(cid:107)2)) at every input x. Therefore, if we can calculate the dimension of N (w) and if we have dimN (w) = |Vinternal|, where we denote the number of internal nodes by |Vinternal|, then we can conclude that all inﬁnitesimal transformations to which the model is invariant can be spanned by inﬁnitesimal node-wise rescaling transformations. Note that the null space N (w) and its dimension is a function of w. Therefore, there are some points in the parameter space that have more invariances than other points. For example, suppose that v is an internal node with ReLU activation that receives connections only from other ReLU units (or any unit whose output is nonnegative). If all the incoming weights to v are negative including the bias, the output of node v will be zero regardless of the input, and the function fw will be insensitive to any transformation to the outgoing weights of v. Nevertheless we conjecture that as the network size grows, the chance of being in such a degenerate conﬁguration during training will diminish exponentially. When we study the dimension of N (w), it is convenient to analyze the dimension of the span of the row vectors of the Jacobian matrix ∂fw(x)/∂w instead. We deﬁne the degrees of freedom of model G at w as  (cid:18)(cid:91)  (cid:18) ∂fw(x)  ∂w  (cid:19)(cid:19)  dG(w) = dim  x∈R|Vin| Span  [v, :] : v ∈ Vout  ,  (22)  where ∂fw(x)[v, :]/∂w denotes the vth row vector of the Jacobian matrix and x runs over all pos- sible input x. Intuitively, dG(w) is the dimension of the set of directions that changes fw(x) for at least one input x. Due to the rank nullity theorem dG(w) and the dimension of N (w) are related as follows:  dG(w) + dim (N (w)) = |E|,  where |E| is the number of parameters. Therefore, again if dG(w) = |E| − |Vinternal|, then we can conclude that inﬁnitesimally speaking, all transformations to which the model is invariant can be spanned by node-wise rescaling transformations. Considering only invariances that hold uniformly over all input x could give an under-estimate of the class of invariances, i.e., there might be some invariances that hold for many input x but not all. An alternative approach for characterizing invariances is to deﬁne a measure of distance between functions that the neural network model represents based on the input distribution, and inﬁnitesimally study the subspace of directions to which the distance is insensitive. We can deﬁne distance between two functions f and g as  D(f, g) = Ex∼D [m(f (x), g(x))] ,  where m : R|Vout|×|Vout| → R is a (possibly asymmetric) distance measure between two vectors z, z(cid:48) ∈ R|Vout|, which we require that m(z, z) = 0 and ∂m/∂z(cid:48) z=z(cid:48) = 0. For example, m(z, z(cid:48)) = (cid:107)z − z(cid:48)(cid:107)2. The second-order Taylor expansion of the distance D can be written as  D(fw(cid:107)fw+∆) =  1 2  ∆(cid:62) · F (w) · ∆ + o((cid:107)∆(cid:107)2),  10  Under review as a conference paper at ICLR 2016  where  (cid:34)(cid:18) ∂fw(x)  (cid:19)(cid:62)  ∂w  · ∂2m(z, z(cid:48))  ∂z(cid:48)2  (cid:12)(cid:12)(cid:12)(cid:12)z=z(cid:48)=fw(x)  ·  (cid:18) ∂fw(x)  (cid:19)(cid:35)  ∂w  F (w) = Ex∼D  and ∂2m(z, z(cid:48))/∂z(cid:48)2|z=z(cid:48)=fw(x) is the Hessian of the distance measure m at z = z(cid:48) = fw(x). Using the above expression, we can deﬁne the input distribution dependent version of N (w) and dG(w) as  ND(w) = NullF (w),  dG,D(w) = rankF (w).  Again due to the rank-nullity theorem we have dG,D(w) + dim(ND(w)) = |E|. As a special case, we obtain the Kullback-Leibler divergence DKL, which is commonly considered as the way to study invariances, by choosing m as the conditional Kullback-Leibler divergence of output y given the network output as  m(z, z(cid:48)) = Ey∼q(y|z)  where q(y|z) is a link function, which can be, e.g., the soft-max q(y|z) = ezy /(cid:80)|Vout|  y(cid:48)=1 ezy(cid:48) . How- ever, note that the invariances in terms of DKL depends not only on the input distribution but also on the choice of the link function q(y|z).  log  ,  (cid:20)  (cid:21)  q(y|z) q(y|z(cid:48))  7.1 PATH-BASED CHARACTERIZATION OF THE NETWORK  (cid:88)  A major challenge in studying the degrees of freedom (22) is the fact that the Jacobian ∂fw(x)/∂w depends on both parameter w and input x. In this section, we ﬁrst tease apart the two dependencies by rewriting fw(x) as the sum over all directed paths from every input node to each output node as follows:  fw(x)[v] =  gp(x) · πp(w) · x[head(p)],  p∈Π(v)  (23)  πp(w) =(cid:81)  where Π(v) is the set of all directed path from any input node to v, head(p) is the ﬁrst node of path p, gp(x) takes 1 if all the rectiﬁed linear units along path p is active and zero otherwise, and e∈E(p) w(e) is the product of the weights along path p; E(p) denotes the set of edges  that appear along path p. Let Π = ∪v∈Vout Π(v) be the set of all directed paths. We deﬁne the path-Jacobian matrix J(w) ∈ R|Π|×|E| as J(w) = (∂πp(w)/∂we)p∈Π,e∈E. In addition, we deﬁne φ(x) as a |Π| dimensional vector with gp(x) · x[head(p)] in the corresponding entry. The Jacobian of the network fw(x) can now be expressed as  ∂fw(x)[v]  ∂w  = Jv(w)(cid:62)φv(x),  (24)  where where Jv(w) and φv(x) are the submatrix (or subvector) of J(w) and φ(x) that corresponds to output node v, respectively1. Expression (24) clearly separates the dependence to the parameters w and input x. Now we have the following statement (the proof is given in Appendix C). Theorem 3. The degrees-of-freedom dG(w) of neural network model G is at most the rank of the the dimension of the space spanned by φ(x) equals the total number of paths |Π|.  path Jacobian matrix J(w). The equality holds if dim(cid:0)Span(φ(x) : x ∈ R|Vin|)(cid:1) = |Π|; i.e. when  An analogous statement holds for the input distribution dependent degrees of freedom dG,D(w), namely, dG,D(w) ≤ rankJ(w) and the equality holds if the rank of the |Π| × |Π| path covariance 1Note that although path activation gp(x) is a function of w, it is insensitive to an inﬁnitesimal change in the parameter, unless the input to one of the rectiﬁed linear activation functions along path p is at exactly zero, which happens with probability zero. Thus we treat gp(x) as constant here.  11  Under review as a conference paper at ICLR 2016  matrix (Ex∼D(cid:2)∂2m(z, z(cid:48))/∂z(cid:48)  v(cid:48)φp(x)φp(cid:48)(x)(cid:3))p,p(cid:48)∈Π is full, where v and v(cid:48) are the end nodes  v∂z(cid:48)  of paths p and p(cid:48), respectively. It remains to be understood when the dimension of the span of the path vectors φ(x) become full. The answer depends on w. Unfortunately, there is no typical behavior as we know from the example of an internal ReLU unit connected to ReLU units by negative weights. In fact, we can choose any number of internal units in the network to be in this degenerate state creating different degrees of degeneracy. Another way to introduce degeneracy is to insert a linear layer in the network. This will superﬁcially increase the number of paths but will not increase the dimension of the span of φ(x). For example, consider a linear classiﬁer zout = (cid:104)w, x(cid:105) with |Vin| inputs. If the whole input space is spanned by x, the dimension of the span of φ(x) is |Vin|, which agrees with the number of paths. Now let’s insert a linear layer with units V1 in between the input and the output layers. The number of paths has increased from |Vin| to |Vin| · |V1|. However the dimension of the span of φ(x) = 1|V1|⊗x is still |Vin|, because the linear units are always active. Nevertheless we conjecture  that there is a conﬁguration w such that dim(cid:0)Span(φ(x) : x ∈ R|Vin|)(cid:1) = |Π| and the set of such  w grows as the network becomes larger.  7.2 COMBINATORIAL CHARACTERIZATION OF THE RANK OF PATH JACOBIAN  Finally, we show that the rank of the path-Jacobian matrix J(w) is determined purely combinatori- ally by the graph G except a subset of the parameter space with zero Lebesgue measure. The proof is given in Appendix C. Theorem 4. The rank of the path Jacobian matrix J(w) is generically (excluding set of parameters with zero Lebesgue measure) equal to the number of parameters |E| minus the number of internal nodes of the network.  Note that the dimension of the space spanned by node-wise rescaling (2) equals the number of internal nodes. Therefore, node-wise rescaling is the only type of invariance for a ReLU network  with ﬁxed architecture G, if dim(cid:0)Span(φ(x) : x ∈ R|Vin|)(cid:1) = |Π| at parameter w.  As an example, let us consider a simple 3 layer network with 2 nodes in each layer except for the output layer, which has only 1 node (see Figure 2). The network has 10 parameters (4, 4, and 2 in each layer respectively) and 8 paths. The Jacobian (∂fw(x)/∂w) can be written as (∂fw(x)/∂w) = J(w)(cid:62) · φ(x), where  w9w1 w9w2  w6w9  w6w9  w9w3 w9w4  w5w9  w5w9  w7w10  w7w10  w8w10  w8w10  w5w1 w5w2 w6w3 w6w4  w10w1 w10w2  w10w3 w10w4  w7w1 w7w2 w8w3 w8w4   (25)    J(w) =  and  φ(x)(cid:62) = [g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2]] . The rank of J(w) in (25) is (generically) equal to 10− 4 = 6, which is smaller than both the number of parameters and the number of paths.  8 CONCLUSION AND FUTURE WORK  We proposed a uniﬁed framework as a complexity measure or regularizer for neural networks and discussed normalization and optimization with respect to this regularizer. We further showed how this measure interpolates between data-dependent and data-independent regularizers and discussed how Path-SGD and Batch-Normalization are special cases of optimization with respect to this mea- sure. We also looked at the issue of invariances and brought new insights to this area.  12  Under review as a conference paper at ICLR 2016  Figure 2: A 3 layer network with 10 parameters and 8 paths.  REFERENCES Amari, Shun-Ichi. Natural gradient works efﬁciently in learning. Neural computation, 10(2):251–  276, 1998.  Desjardins, Guillaume, Simonyan, Karen, Pascanu, Razvan, and Kavukcuoglu, Koray. Natural neu-  ral networks. arXiv preprint arXiv:1507.00210, 2015.  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural  networks. In AISTATS, 2010.  Grosse, Roger and Salakhudinov, Ruslan. Scaling up natural gradient by sparsely factorizing the  inverse Fisher matrix. In ICML, 2015.  Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by  reducing internal covariate shift. In ICML, 2015.  Larochelle, Hugo, Bengio, Yoshua, Louradour, J´erˆome, and Lamblin, Pascal. Exploring strategies  for training deep neural networks. The Journal of Machine Learning Research, 10:1–40, 2009.  Le Cun, Yann, Bottou, L´eon, Orr, Genevieve B., and M¨uller, Klaus-Robert. Efﬁcient backprop. In Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524. Springer Verlag, 1998. URL http://leon.bottou.org/papers/lecun-98x.  LeCun, Yann, Bottou, Leon, Orr, Genevieve B, and Muller, Klaus-Robert. Neural networks-tricks  of the trade. Springer Lecture Notes in Computer Sciences, 1524(5-50):7, 1998.  Martens, James. Deep learning via hessian-free optimization. In ICML, 2010.  Martens, James and Grosse, Roger. Optimizing neural networks with Kronecker-factored approxi-  mate curvature. In ICML, 2015.  Neyshabur, Behnam, Salakhutdinov, Ruslan, and Srebro, Nathan. Path-SGD: Path-normalized opti-  mization in deep neural networks. In NIPS, 2015a.  Neyshabur, Behnam, Tomioka, Ryota, and Srebro, Nathan. Norm-based capacity control in neural  networks. In COLT, 2015b.  Neyshabur, Behnam, Tomioka, Ryota, and Srebro, Nathan.  On the role of implicit regularization in deep learning. Representations (ICLR) workshop track, 2015c.  In search of the real inductive bias: International Conference on Learning  Ollivier, Yann. Riemannian metrics for neural networks i: feedforward networks. Information and  Inference, 4(2):108–153, 2015.  Pascanu, Razvan and Bengio, Yoshua. Revisiting natural gradient for deep networks. In ICLR, 2014.  Roux, Nicolas L, Manzagol, Pierre-Antoine, and Bengio, Yoshua. Topmoumoute online natural  gradient algorithm. In NIPS, 2008.  Schaul, Tom, Zhang, Sixin, and Lecun, Yann. No more pesky learning rates. In ICML, 2013.  13  𝑤""𝑤#𝑤$𝑤%𝑤&𝑤’𝑤(𝑤)𝑤*𝑤""+Under review as a conference paper at ICLR 2016  Sutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initial-  ization and momentum in deep learning. In ICML, 2013.  Vinyals, Oriol and Povey, Daniel. Krylov subspace descent for deep learning. In ICML, 2011.  A IMPLEMENTATION  A.1 DDP-NORMALIZATION  Given any batch of n data points to estimate mean, variance and the gradient, the stochastic gradients for the weight ˜w (weights in the DDP-Normalized network) can then be calculated through the chain rule:  h(i)  n(cid:88)  j=1  − 1 n  N in(v) − 1  n   ∂L  ∂z(i) v    ∂˜γ2 v ∂ ˜w→v  1 − α  v ˆz(j) ˆz(i) v ˜γ2 v  v 2˜γ2 v  N in(v) − ˆz(i) h(j) (cid:32) n(cid:88)  ∂L ∂z(j)  v  j=1  ∂L  ∂ ˜w→v  =  1 n˜γv  ∂L ∂z(i) v  ∂L ∂z(i) u  =  1 ˜γv  ˜wu→v  v∈N out(u)  (cid:33)  u ≥0 z(i)  where ˆz(i)  v = ˜z(i)  v − 1  n  j=1 ˜z(j)  v  and we have:  ∂˜γ2 v ∂ ˜w→v  = 2(1 − α) ˜w→v +  2α n  n(cid:88)  i=1  ˆz(i) v  h(i)  N in(v) − 1  n    n(cid:88)  j=1  h(j)  N in(v)  Similar to Batch-Normalization, all the above calculations can be efﬁciently carried out as vector operations with negligible extra memory and computations.  A.2 DDP-SGD  In order to compute the second derivatives κe(w) = ∂2γ2 The backpropagation can be done through γ2 derivatives. Instead we propagate the loss through γ2  , we ﬁrst calculate the ﬁrst derivative. u but this makes it difﬁcult to ﬁnd the second u1 z(i) u and the second order terms of the form z(i) u2 :  u and z(i)  ∂w2 e  net  i=1  n(cid:88)  (cid:88) (cid:80)n  (26)  (27)  (28)  ∂γ2 net ∂γ2 u  +  u1=u2  (cid:20) ∂γ2  net  (cid:21)  ∂γ2 u1  ∂γ2 net u1 z(i) ∂(z(i) u2 )  = α  = (1 − α)   (cid:88)  (v1,v2)∈(N out(u1))2  Now we can calculate the partials for wu→v as follows:  (cid:88)  ∂γ2 net ∂γ2 v  w2  u→v  v∈N out(u)  ∂γ2 net v1 z(i) ∂(z(i) v2 )  wu1→v1 wu2→v2  n(cid:88)  (cid:88)  i=1  v(cid:48)∈N out(u)  (29)    z(i) u1 >0,z(i)  u2 >0  (30)  ∂γ2 net ∂wu→v  = 2(1 − α)  ∂γ2 net ∂γ2 v  γ2 uwu→v + 2  ∂γ2 net v z(i) ∂(z(i) v(cid:48) )  u z(i) h(i) v(cid:48)  (31)  Since the partials ∂γ2 net ∂γ2 u calculated directly:  and  net  ∂γ2 ∂(z(i) u1 z(i) u2 )  do not depend on wu→v, the second order derivative can be  n(cid:88)  i=1  ∂  2(cid:17)(cid:16)  (cid:16)  ∂γ2 net z(i) v  (cid:17)2  h(i) u  (32)  κu→v(w) =  1 2  ∂2γ2 ∂w2  net u→v  = (1 − α)  ∂γ2 net ∂γ2 v  γ2 u +  14  Under review as a conference paper at ICLR 2016  B NATURAL GRADIENT  The natural gradient algorithm (Amari, 1998) achieves invariance by applying the inverse of the Fisher information matrix F (w(t)) at the current parameter w(t) to the negative gradient direction as follows:  w(t+1) = w(t) + η∆(natural),  where  ∆(natural) = argmin ∆∈R|E|  (cid:29)  (w(t)), ∆  (cid:28)  − ∂L ∂w ∂L ∂w  = −F −1(w(t))  (w(t)).  s.t. ∆(cid:62)F (w(t))∆ ≤ δ2  ,  (33)  (34)  Here F (w) is the Fisher information matrix at point w and is deﬁned with respect to the probabilistic view of the feedforward neural network model, which we describe in more detail below. Suppose that we are solving a classiﬁcation problem and the ﬁnal layer of the network is fed into a softmax layer that determines the probability of candidate classes given the input x. Then the neural network with the softmax layer can be viewed as a conditional probability distribution  (cid:80)  q(y|x) =  exp(fw(x)[vy]) v∈Vout  exp(fw(x)[v])  ,  (35)  where vy is the output node corresponding to class y. If we are solving a regression problem a Gaussian distribution is probably more appropriate for q(y|x). Given the conditional probability distribution q(y|x), the Fisher information matrix can be deﬁned as follows:  F (w)[e, e(cid:48)] = Ex∼p(x)Ey∼q(y|x)  ,  (36)  (cid:20) ∂ log q(y|x)  ∂ log q(y|x)  ∂we  ∂we(cid:48)  (cid:21)  where p(x) is the marginal distribution of the data. Since we have  ∂ log q(y|x)  ∂ log q(y|x)  ∂wu→v  =  ∂zv  · hu =  (cid:88)  ∂ log q(y|x)  v(cid:48)∈Vout  ∂zv(cid:48)  · ∂zv(cid:48) ∂zv  · hu  (37)  using the chain rule, each entry of the Fisher information matrix can be computed efﬁciently by forward and backward propagations on a minibatch.  C PROOFS  Proof of Theorem 1. First note that we can calculate the following inner product using equation (28):  Next, by equation (26) we get:  (cid:28)  ˜w→v,  ∂L  ∂ ˜w→v  (cid:28)  ˜w→v,  ∂˜γ2 v ∂ ˜w→v  (cid:29)  n(cid:88)  i=1  (cid:28)  = 2(1 − α)(cid:107) ˜w→v(cid:107)2 = 2(1 − α)(cid:107) ˜w→v(cid:107)2  2 +  2α n  2 + 2αVar(˜zv) = 2˜γ2  v  (ˆz(i))2  (cid:29)(cid:35)  v − ˆz(i) ˆz(i)  v 2˜γ2 v  v − ˆz(i) ˆz(i)  v 2˜γ2 v  ∂˜γ2 v ∂ ˜w→v  ˜w→v,  (cid:35)  2˜γ2 v  = 0  (cid:29)  (cid:34) (cid:34)  n(cid:88) n(cid:88)  i=1  i=1  ∂L ∂z(i) v  ∂L ∂z(i) v  =  =  1 n˜γv  1 n˜γv  15  Under review as a conference paper at ICLR 2016  Proof of Theorem 3. First we see that (24) is true because  (cid:17)  =  (cid:16) (cid:88) (cid:18) ∂fw(x)[v]  p∈Π(v)  ∂w  ∂fw(x)[v]  ∂w  Span  Therefore,(cid:91)  x∈R|Vin|  ∂πp(w)  ∂we  · gp(x) · x[head(p)]  = Jv(w)(cid:62) · φv(x).  e∈E  (cid:19)  =  (cid:91)  x∈R|Vin|  : v ∈ Vout  Span(cid:0)Jv(w)(cid:62) · φv(x) : v ∈ Vout  (cid:1)  (cid:16)  φ(x) : x ∈ R|Vin|(cid:17)  .  (38)  = J(w)(cid:62) · Span  Consequently, any vector of the form ( ∂fw(x)[v] vectors of the path Jacobian J(x).  The second part says dG(w) = rankJ(w) if dim(cid:0)Span(φ(x) : x ∈ R|Vin|)(cid:1) = |Π|, which is the  )e∈E for a ﬁxed input x lies in the span of the row  ∂we  number of rows of J(w). We can see that this is true from expression (38).  Proof of Theorem 4. First, J(w) can be written as an Hadamard product between path incidence matrix M and a rank-one matrix as follows:  J(w) = M ◦(cid:0)w−1 · π(cid:62)(w)(cid:1) ,  where M is the path incidence matrix whose i, j entry is one if the ith edge is part of the jth path, w−1 is an entry-wise inverse of the parameter vector w, π(w) = (πp(w)) is a vector containing the product along each path in each entry, and (cid:62) denotes transpose. Since we can rewrite  J(w) = diag(w−1) · M · diag(π(w)),  we see that (generically) the rank of J(w) is equal to the rank of zero-one matrix M. Note that the rank of M is equal to the number of linearly independent columns of M, in other words, the number of linearly independent paths. In general, most paths are not independent. For example, in Figure 2, we can see that the column corresponding to the path w2w7w10 can be produced by combining 3 columns corresponding to paths w1w5w9, w1w7w10, and w2w5w9. In order to count the number of independent paths, we use mathematical induction. For simplicity, consider a layered graph with d layers. All the edges from the (d − 1)th layer nodes to the output layer nodes are linearly independent, because they correspond to different parameters. So far we have ndnd−1 independent paths. Next, take one node u0 (e.g., the leftmost node) from the (d− 2)th layer. All the paths starting from this node through the layers above are linearly independent. However, other nodes in this layer only contributes linearly to the number of independent paths. This is the case because we can take an edge (u, v), where u is one of the remaining nd−2 − 1 vertices in the (d − 2)th layer and v is one of the nd−1 nodes in the (d − 1)th layer, and we can take any path (say p0) from there to the top layer. Then this is the only independent path that uses the edge (u, v), because any other combination of edge (u, v) and path p from v to the top layer can be produced as follows (see Figure 3):  (u, v) → p = (u, v) → p0 − (u0, v) → p0 + (u0, v) → p.  Therefore after considering all nodes in the d − 2th layer, we have  ndnd−1 + nd−1(nd−2 − 1) independent paths. Doing this calculation inductively, we have  ndnd−1 + nd−1(nd−2 − 1) + ··· + n1(n0 − 1)  independent paths, where n0 is the number of input units. This number is clearly equal to the number of parameters (ndnd−1 + ··· + n1n0) minus the number of internal nodes (nd−1 + ··· + n1).  16  Under review as a conference paper at ICLR 2016  Figure 3: Schematic illustration of the linear dependence of the four paths (u0, v) → p0, (u0, v) → p, (u, v) → p0, and (u, v) → p. Because of this dependence, any additional edge (u, v) only contributes one additional independent path.  17  𝑝""𝑝𝑢""𝑢𝑣𝑑thlayer𝑑−1thlayer𝑑−2thlayer.  .  ..  .  .",
1511.06426,2016,Reasoning in Vector Space: An Exploratory Study of Question Answering,"['Reasoning in Vector Space: An Exploratory Study of Question Answering\nMoontae Lee', 'Xiaodong He', 'Wen-tau Yih', 'Jianfeng Gao', 'Li Deng', 'Paul Smolensky']",https://arxiv.org/pdf/1511.06426,"6 1 0 2     b e F 6 2         ] L C . s c [      4 v 6 2 4 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  REASONING IN VECTOR SPACE: AN EXPLORATORY STUDY OF QUESTION ANSWERING  Moontae Lee∗ Department of Computer Science Cornell University University Ithaca, NY 14850, USA moontae@cs.cornell.edu  Xiaodong He, Wen-tau Yih, Jianfeng Gao & Li Deng Microsoft Research Redmond, WA 98052, USA {xiaohe,scottyih,jfgao,deng}@microsoft.com  Paul Smolensky∗ Department of Cognitive Science Johns Hopkins University Baltimore, MD 21218, USA smolensky@jhu.edu  ABSTRACT  Question answering tasks have shown remarkable progress with distributed vec- tor representation. In this paper, we investigate the recently proposed Facebook bAbI tasks which consist of twenty different categories of questions that require complex reasoning. Because the previous work on bAbI are all end-to-end mod- els, errors could come from either an imperfect understanding of semantics or in certain steps of the reasoning. For clearer analysis, we propose two vector space models inspired by Tensor Product Representation (TPR) to perform knowledge encoding and logical reasoning based on common-sense inference. They together achieve near-perfect accuracy on all categories including positional reasoning and path ﬁnding that have proved difﬁcult for most of the previous approaches. We hypothesize that the difﬁculties in these categories are due to the multi-relations in contrast to uni-relational characteristic of other categories. Our exploration sheds light on designing more sophisticated dataset and moving one step toward integrating transparent and interpretable formalism of TPR into existing learning paradigms.  1  INTRODUCTION  Ideal machine learning systems should be capable not only of learning rules automatically from training data, but also of transparently incorporating existing principles. While an end-to-end frame- work is suitable for learning without human intervention, existing human knowledge is often valu- able in leveraging data toward better generalization to novel input. Question answering (QA) is one of the ultimate tasks in Natural Language Processing (NLP) on which synergy between the two capabilities could enable better understanding and reasoning.  Recently the Facebook bAbI tasks were introduced to evaluate complex reading comprehension via QA (Weston et al. (2015)); these have received considerable attention. Understanding natural ques- tions, for example in WebQuestions tasks (Berant et al. (2013)), requires signiﬁcant comprehension of the semantics, yet reasoning out the answers is then relatively simple (e.g., Bordes et al. (2014);  ∗This research was conducted while the ﬁrst author held a summer internship in Microsoft Research, Red-  mond, and the last author was a Visiting Researcher there.  1  Published as a conference paper at ICLR 2016  Yih et al. (2015)). In contrast, the synthetic questions in bAbI require rather complex reasoning over multiple computational steps while demanding only minimal semantic understanding. As the pre- vious work on bAbI consists only of end-to-end models (Weston et al. (2014); Kumar et al. (2015); Sukhbaatar et al. (2015); Peng et al. (2015)), it is unclear whether incorrect answers arise from an imperfect semantic understanding, inadequate knowledge encoding, or insufﬁcient model capac- ity (Dupoux (2015)). This is partly because the current paradigms based on neural networks have no interpretable intermediate representations which modelers can use to assess the knowledge present in the vectorial encoding of the system’s understanding of the input sentences. Our approach, in contrast, can illuminate what knowledge is caputred in each representation via the formalism of TPR.  Tensor Product Representation (TPR), proposed by Smolensky (1990); Smolensky & Legendre (2006), is a mathematical method to represent complex structures from basic vectorial building blocks, so called ﬁllers and roles. For example, one can encode a binary tree by binding ﬁller vec- tors corresponding to the left- and right-child entities to role vectors corresponding to the ‘left child’ and ‘right child’ positions, respectively. Arbitrary trees can be represented by recursively applying the same method. As an outer product (i.e., tensor product) realizes the binding operation, both ﬁller and role components are decodable from the resulting representation via the inner product; this is called unbinding. TPR is known to be capable of various applications such as tree operations, grammar processing and lambda-calculus evaluation (Smolensky (2012)).  In this paper, we endeavor to disentangle the problem cleanly into semantic parsing, knowledge encoding, and logical reasoning. Proposing two vector-space models inspired by TPR, we ﬁrst pro- vide an in-depth analysis of the bAbI dataset by clustering, based solely on their logical properties, the twenty question categories deﬁned by bAbI. Such analysis enables us to conjecture why most existing models, in spite of their complexity, have failed to achieve good accuracy on positional reasoning and path ﬁnding tasks, whereas Peng et al. (2015) achieved successful results. If the bAbI tasks turn out to be considerably simpler than intended for its ultimate purpose of providing a major step towards “AI-complete question answering”, then more elaborated tasks will be required to test the power of proposed QA models such as memory networks.  As a further contribution, we also develop the foundation of a theory that maps inference for log- ical reasoning to computation over TPRs, generalizing our models under the rigorous TPR for- malism. Due to the page limit, this theoretical foundation is relegated to the supplementary ma- terials (Smolensky et al. (2016)). The experimental results show that accurate inference based on common-sense knowledge is transparently attainable in this formalism. We hope our exploration can contribute to the further improvement of end-to-end models toward the transparency and inter- pretability. To the best of our knowledge, our in-depth analysis of bAbI and of logical reasoning over distributed vectorial representations are each the ﬁrst of their kind.  2 RELATED WORK  Since the seminal work of Bengio et al. (2003), researchers have paid increasing attention to various distributed representations in continuous vector spaces. In the computer science literature, Skip- gram/CBoW (Mikolov et al. (2013)) and GloVe (Pennington et al. (2014)) are popular models that are trained based on the distributional similarities in word co-occurrence patterns; they have been frequently utilized as initial embeddings for a variety of other NLP tasks. In the cognitive science literature, on the other hand, BEAGLE (Jones & Mewhort (2007)) and DVRS (Ustun et al. (2014)) are trained differently, with random initializations and circular convolution. They assign two vec- tors for each word: an environmental vector to describe physical properties and a lexical vector to indicate meaning.  Whereas such representations are known to provide a useful way to incorporate prior linguistic knowledge, their usefulness is not clear for reasoning-oriented tasks. In other contexts, Grefenstette (2013) shows how to simulate predicate logic with matrices and tensors. Similarly, Rocktaschel et al. (2014) try to ﬁnd low-dimensional embeddings which can model ﬁrst-order logic in a vectorial manner. These models are only concentrated on general logic problems without considering NLP tasks. Note that vectorial encodings are necessary in many machine learning models such as neural networks. Reasoning based on linguistic cues in vector space uniquely characterizes our paper among these relevant work.  2  Published as a conference paper at ICLR 2016  Category 1: Single Supporting Fact 01: Mary moved to the bathroom. 02: John went to the hallway. 03: Where is Mary? bathroom 1 04: Daniel went back to the hallway. 05: Sandra moved to the garden. 06: Where is Daniel? hallway 4  Category 2: Two Supporting Facts 01: Mary went to the kitchen. 02: Sandra journeyed to the ofﬁce. 03: Mary got the football there. 04: Mary travelled to the garden. 05: Where is the football? garden 3 4 06: John travelled to the ofﬁce. 07: Sandra moved to the garden. 08: Where is the football? garden 3 4 09: Mary dropped the football. 10: Mary journeyed to the kitchen. 11: Where is the football? garden 9 4  Category 3: Three Supporting Facts 01: Sandra went back to the hallway. 02: Daniel took the apple. 03: John travelled to the kitchen. 04: Daniel travelled to the bedroom. 05: Daniel got the football there. 06: Daniel went to the hallway. 07: Where was the apple before the hallway? bedroom 2 6 4 08: Mary went back to the bedroom. 09: Daniel discarded the football. 10: Daniel got the football. 11: Mary went to the garden. 12: Daniel travelled to the ofﬁce. 13: Daniel went back to the bedroom. 14: Where was the football before the bedroom? ofﬁce 10 13 12 15: Daniel went back to the hallway. 16: Mary went back to the bathroom. 17: Daniel dropped the apple. 18: Sandra journeyed to the kitchen. 19: Where was the apple before the ofﬁce? hallway 17 12 6  Figure 1: Sample statements(black), questions(blue), answers(red), and clues(green) for Categories 1, 2, and 3.  The tasks in bAbI have been studied mainly within the context of the Memory Network (MemNN) model, which consists of an array of representations called “memory” and four learnable modules: the I-module encodes the input into feature representation, the G-module updates relevant memory slots, the O-module performs inferences to compute output features given the input representation and the current memory, and ﬁnally the R-module decodes the output feature-based representation to the ﬁnal response. Since the proposal of the basic MemNN (Weston et al. (2014)) model, the Adaptive/Nonlinear MemNN (Weston et al. (2015)), DMN (Kumar et al. (2015)), and MemN2N (Sukhbaatar et al. (2015)) models have been developed by varying certain parts of these modules. Nonetheless, none of these models except Peng et al. (2015) successfully accomplish either posi- tional reasoning or path ﬁnding tasks. Our speculation about the performance by Peng et al. (2015) will be given in a later section based on our bAbI analysis.  3 MODELS AND ANALYSIS  The bAbI dataset consists of twenty different types of questions where each question category is claimed to be atomic and independent from the others (Weston et al. (2015)). In this section, we investigate clusters of categories with sample QA problems, analyzing what kinds of logical proper- ties are shared across various types. We also elucidate, based on our vector space models, why it is difﬁcult to achieve good accuracy on certain categories: positional reasoning and path ﬁnding.  3.1 CONTAINEE-CONTAINER RELATIONSHIP  Supporting Facts (1, 2, 3) The ﬁrst three question categories of bAbI ask for the current or previ- ous locations of actors and objects based on the statements given prior to the question. Category 1–3 questions respectively require precisely one, two, or three supporting facts to reason out the proper answers. Figure 1 illustrates sample statements and questions extracted from real examples in the training set. Reasoning in Category 1 implicitly requires a simple common-sense reasoning rule that “An actor cannot exist in two different locations at the same time.” In order to answer the questions in Category 2, we implicitly need another rule that “An object that belongs to an actor follows its owner’s location.” Further, if an item is dropped at one particular location, it will permanently stay in that location until someone grabs it and moves around with it later.  While two independent relations, pick/drop and move, seem to be involved in parallel in the Category 2 tasks, these questions can be all uniformly answered under the transitivity of a containee belongs to a container. If an actor moves to a location, he/she (a containee) now belongs to that location  3  Published as a conference paper at ICLR 2016  (a container). Similarly, if an actor acquires an object, the item (a containee) newly belongs to that actor (a container). Transitivity then logically implies that the object belongs to the location occupied by the owner.  Relational Translations/Answers Mary belongs to the kitchen (from nowhere). The football belongs to Mary.  # Statements/Questions 1 Mary went to the kitchen. 3 Mary got the football there. 4 Mary travelled to the garden. Mary belongs to the garden (from the kitchen). mgT m(g ◦ k)T 5 Where is the football? 9 Mary dropped the football. 10 Mary journeyed to the kitchen. Mary belongs to the kitchen (from the garden). mkT m(k ◦ g)T 11 Where is the football?  3, 4 garden The football belongs to where Mary belongs to. f gT  Encodings/Clues mkT m(k ◦ n)T f mT f mT  garden  f gT  9, 4  Table 1: Sample containee-belongs to-container translations and corresponding encodings about Mary from Category 2. Symbols in encodings are all d-dimensional vectors for actors (mary), ob- jects (f ootball), and locations(nowhere, kitchen, garden). Translations and encodings for Category 3 are also speciﬁed with the parentheses and circle operation, respectively.  Knowing that every actor and object is unique without any ambiguity, one can encode such containee-conatainer relationships by the following model using distributed representations. As- sume all entities: actors, objects, and locations are represented by d-dimensional unit vectors in Rd.1 Then each statement is encoded by a second-order tensor (or matrix) in which the con- tainee vector is bound to the container vector via the fundamental binding operation of TPR, the tensor (or outer) product2 — in tensor notation, (containee) ⊗ (container), or in matrix notation, (containee)(container)T — and then stored in a slot in a memory. When an item is dropped, we perform an inference to store the appropriate knowledge in memory. For the example in Table 1, the container of the football at Statement 9 — the garden — is determined after ﬁguring out the most recent owner of the football, Mary; transitivity is implemented through simple matrix multiplica- tion of the encodings of Statement 3 (locating the football) and Statement 4 (locating the football’s current owner, Mary):  (f mT ) · (mgT ) = f (mT · m)gT = f gT  (∵ mT m = ||m||2  2 = 1)  Finally, Category 3 asks the trajectory of items considering the previous locations of actors. Thus the overall task is to understand the relocation sequence of each actor and from this to reconstruct the trajectory of item locations. Whereas MemNNs introduced an additional vector for each statement for encoding a time stamp, we deﬁne another binding operation ◦ : Rd × Rd −→ Rd. This binding operation maps a pair of (next, prev) location vectors into a d-dimensional vector via a d × 2d temporal encoding matrix U like the following:  n ◦ p = U (cid:20)n  p(cid:21) ∈ Rd.  In Table 1, the second expression in the Encodings column speciﬁes temporal encodings that identify location transitions: Statement 4, translated as Mary belongs to the garden (from the kitchen), is encoded as m(g ◦ k)T . We can now reason to the proper answers for the questions in Figure 1 by the following inference steps, using basic encodings (for C1 & C2) and temporal encodings (for C3):  C1. Where is Mary?  (a) Left-multiply by mT all statements prior to time 3. (Yields m · mT bT , mT · jhT .) (b) Pick the most recent container where 2-norms of the multiplications in (a) are approximately  1.0. (Yields bT ; mT j is small.)  (c) Answer by ﬁnding the location corresponding to the result representation. ⇒ bathroom  C2. Where is the football?  1Topologically speaking, the unit hypersphere can be constructed by adding one more point (“at inﬁnity”)  to Euclidean space. Thus sampling from the hypersphere does not limit the generality of representations.  2In TPR terms, the containee corresponds to a ﬁller, and the container corresponds to a role.  4  Published as a conference paper at ICLR 2016  (a) Left-multiply by f T all statements prior to the current time. (Yields f T · mkT , f T · soT ,  f T · f mT , f T · mgT .)  (b) Pick the most recent container where 2-norms of the multiplications in (a) are approximately  1.0. (Yields mT .)  (c) If the container is an actor (e.g., Mary in statement 3),  • Find the most recent container of the actor by left-multiplying by mT (Yields gT .) • Answer by the most recent container. ⇒ garden for the questions at time 5 and 8.  (d) If the container is a location (e.g., garden in statement 9), simply answer by the container.  C3. Where was the apple before the hallway?  (a) Left-multiply by aT all existing temporal encodings prior to time 7. (Yields aT · s(h ◦ n)T ,  aT · adT , ... .)  (b) Pick the earliest container (the start of the trajectory). ⇒ Daniel in statement 2. (c) Find the containers of Daniel by left-multiplying by dT the temporal encodings between time 2 and 7. (Yields dT · adT , dT · j(k ◦ n)T , dT · d(b ◦ n)T , dT · f dT , dT · d(h ◦ b), ... .) (d) By multiplying by the pseudo-inverse U †, unbind 2d-dimensional vectors between time 4  and 7. (Yields U †(b ◦ n) ≈ [b; n], then [h; b].)  (e) Reconstruct the item trajectory in sequence. ⇒ nowhere → bedroom → hallway (f) Answer with (the most recent) location which is prior to the hallway. ⇒ bedroom  Three Argument Relations (5) In this category, there is a new type of statement which speciﬁes ownership transfer: an actor gives an object to another actor. Since now some relations involve three arguments, (source-actor, object, target-actor), we need to encode an ownership trajectory instead of a location trajectory.  Encodings/Clues  # Statements/Questions 1 Jeff took the milk there. 2 Jeff gave the milk to Bill. 3 Who did Jeff give the milk to? 4 Daniel travelled to the ofﬁce. 5 Daniel journeyed to the hallway. Daniel belongs to the hallway. 6 Who received the milk? 7 Bill went to the kitchen. 8 Fred grabbed the apple there. 9 What did Jeff give to Bill?  Relational Translations/Answers The milk belongs to Jeff (from None). m(j ∗ n)T The milk belongs to Bill (from Jeff). m(b ∗ j)T 2 Bill Daniel belongs to the ofﬁce. doT dhT 2 bkT  Bill Bill belongs to the kitchen. The apple belongs to Fred (from none). a(f ∗ n)T milk  2  Table 2: Sample containee-belongs to-container translations and corresponding encodings for an example from Category 5. Symbols in encodings are all d-dimensional vectors for actors (nobody, jeff, daniel, bill, f red), objects (milk, apple), and locations (ofﬁce, kitchen).3  Analogously to the ◦ operation used for Category 3, we realize the ∗ operation by deﬁning a map ∗ : Rd × Rd −→ Rd. This new binding operation maps a pair of (next, prev) owner vectors into a d-dimensional vector via a d × 2d matrix V in the exactly same fashion: n ∗ p = V [n; p] ∈ Rd. Due to the similarity in encoding, the inference is also analogous to the inference for Category 3.  C5. Three questions of Table 2?  (a) Find the owners of the milk by left-multiplying by mT the encodings prior to time 3. (b) Unbind the owner transitions by multiplying them by the pseudo-inverse V †. (c) Reconstruct the ownership trajectory for the milk. ⇒ Nobody → Jeff → Bill (d) Answer accordingly each question based on the trajectory.  Though no more complex examples or distinct categories exist in the dataset, it is clear that our en- coding scheme is capable of inferring the full trajectory of item location considering both relocation of actors and transfers of ownership. In such cases, both ◦ and ∗ will be used at the same time in  3To avoid notational confusion, we modify the name of an actor (from Mary to Daniel) and a location (from  the bathroom to the ofﬁce) from the real example in Category 5.  5  Published as a conference paper at ICLR 2016  Category 6: Yes/No Questions 01: Daniel went back to the hallway. 02: John got the apple there. 03: Is Daniel in the hallway? yes 1 04: John dropped the apple. 05: Mary got the apple there. 06: Is Daniel in the hallway? yes 1 07: Daniel moved to the bedroom. 08: Sandra travelled to the hallway. 09: Is Daniel in the hallway? no 7  Category 8: List/Sets 01: Mary took the milk there. 02: Mary went to the ofﬁce. 03: What is Mary carrying? milk 1 04: Mary took the apple there. 05: Sandra journeyed to the bedroom. 06: What is Mary carrying? milk,apple 1 4 07: Mary put down the milk. 08: Mary discarded the apple. 09: What is Mary carrying? nothing 1 7 4 8  Category 7: Counting 01: Mary took the apple there. 02: John travelled to the ofﬁce. 03: How many objects is Mary carrying? one 1 04: Mary travelled to the bathroom. 05: Sandra went back to the bedroom. 06: How many objects is Mary carrying? one 1 07: Mary got the football there. 08: Mary went to the ofﬁce. 09: How many objects is Mary carrying? two 1 7 10: Mary passed the apple to John. 11: Mary left the football. 12: How many objects is Mary carrying? none 1 7 10 11  Category 9: Simple Negation 01: Sandra travelled to the garden. 02: Sandra is no longer in the garden. 03: Is Sandra in the garden? no 2 04: Sandra is in the garden. 05: Sandra journeyed to the hallway. 06: Is Sandra in the hallway? yes 5  Figure 2: Sample statements(black), questions(blue), answers(red), and clues(green) for Category 6, 7, 8, and 9. Answer types are different from the previous categories.  encoding. (e.g., encoding for time 5 will be then d(h ◦ o)T . Note also that there may be multiple transfers between the same pair of actors in a history prior to the given question. While any of them could be appropriate evidence to justify different answers, the ground-truth answers in the training set turned out to be all based on the most recent clues.  Answer Variations (6, 7, 8, 9) As shown in Figure 2, the responses to questions of Categories 6-9 require different measures of the inferred element. For example, the statements in Category 6 are structurally equivalent to the statements in Category 2, while the questions concern only a current location, similar to Category 1. However, each question is formulated in a binary yes/no format, conﬁrming “Is Daniel in the hallway?” instead of asking “Where is Daniel?”. Category 7 is isomorphic to Category 5 in the sense that actors can pick up, drop, and pass objects to other actors. However, each question inquires the number of objects currently belonging to the given actor. On the other hand, a response in Category 8 must give the actual names of objects instead of counting their number. The statements in this category are based not on Category 5, but on Category 2 which is simpler due to the lack of ownership transfer. Lastly, statements in Category 9 can contain a negative quantiﬁer such as ‘no’ or ‘no longer’. Responses conﬁrm or not the location of actors via yes/no dichotomy as for Category 6. However, the overall story is based on the simplest Category 1.  Since answer measures are the only differences of these categories from Category 1, 2, 3, and 5, no additional encodings or inferences are necessary. However, there are several caveats in formulating actual answers: 1) For yes/no questions, we should know the answers must be either yes or no in advance based on the training examples. 2) When counting the number of belongings, the answer must use English number words rather than Arabic numerals. 3) When enumerating the names of belongings, names must be sequenced by their order of acquisition. 4) A negative quantiﬁer is realized by binding the initial default location nowhere back to the given actor. Note that there is no double negation.  Statement Variations (10, 11, 12, 13) Statements in Categories 10-13 contain more challenging linguistic elements such as conjunctions (and/or) or pronouns (he/she/they). While statements in Category 10 is structurally similar to Category 1’s, an actor can be located in either one or another location. Due to such uncertainty, some questions must be answered indeﬁnitely by ‘maybe’. On the other hand, each statement in Category 12 can contain multiple actors conjoined by ‘and’ to indicate that these actors all carry out the action. Aside from such conjunctions, statements and questions are isomorphic to Category 1’s. Statements in Categories 11/13 can consist of a singular/plural pronoun to indicate single/multiple actors mentioned earlier. Since coreference resolution is itself a  6  Published as a conference paper at ICLR 2016  Category 10: Indeﬁnite Knowledge 01: Julie travelled to the kitchen. 02: Bill is either in the school or the ofﬁce. 03: Is Bill in the ofﬁce? maybe 2 04: Bill went back to the bedroom. 05: Bill travelled to the kitchen. 06: Is Bill in the kitchen? yes 5  Category 12: Conjunction 01: Daniel and Sandra went back to the kitchen. 02: Daniel and John went back to the hallway. 03: Where is Daniel? hallway 2 04: Daniel and John moved to the bathroom. 05: Sandra and Mary travelled to the ofﬁce. 06: Where is Daniel? bathroom 4  Category 11: Basic Coreference 01: Mary went back to the bathroom. 02: After that she went to the bedroom. 03: Where is Mary? bedroom 1 2 04: Daniel moved to the ofﬁce. 05: Afterwards he moved to the hallway. 06: Where is Daniel? hallway 4 5  Category 13: Compound Coreference 01: Mary and Daniel went to the bathroom. 02: Then they journeyed to the hallway. 03: Where is Daniel? hallway 1 2 04: Sandra and John moved to the kitchen. 05: Then they moved to the hallway. 06: Where is John? hallway 4 5  Figure 3: Sample statements(black), questions(blue), answers(red), and clues(green) for Category 10, 11, 12, and 13. Statement types are different from the previous categories.  difﬁcult problem, all pronouns are limited to refer only to actors mentioned in the immediately prior statement.  To encode conjunctions, we can still leverage the same method: conjoin two objects by another bilinear binding operation ⋆ : Rd × Rd −→ Rd, and unbind similarly via the pseudo-inverse of the corresponding matrix. In our implementation, every statement is encoded using such a binding operation. For instance, the ﬁrst two statements of the given Category 10 example are encoded into j(k ⋆ k)T and b(s ⋆ o)T , with ⋆ encoding or. If two locations unbound from the target actor are identical, we output a yes/no deﬁnite answer, whereas two different locations imply the indeﬁnite answer ‘maybe’ if one of the unbound locations matches the queried location. For the conjunction and in Category 12, exactly the same formalism is applicable for conjoining actors instead. Whereas a singular pronoun appearing at time t in Category 11 is simply replaced by the actor mentioned at time t − 1, we also use ⋆-binding to provide the multiple coreference needed for Category 13. For instance, the ﬁrst statement in the given Category 13 example is encoded as (m ⋆ d)bT and the same encoding is substituted for ‘they’ to represent the actors in the following statement.  Deduction/Induction (15, 16, 18, 20) While the statements and questions in these categories seem different at ﬁrst glance, their goals are all to reason using a transitivity-like rule. Categories 15 creates a food chain among various animals, and Category 18 yields a partial/total order of sizes among various objects. Whereas inference in these two categories is deductive, Categories 16 and 20 require inductive inference. In all four categories, every statement is easily represented by a containee-container relation obeying transitivity. For instance, the Category 15 example of Figure 4 is encoded by {mcT , wmT , csT , swT }. Then the answer for the ﬁrst question: “What is Jessica afraid of?” will be answered by left-multiplying these by the transpose of j = m and ﬁnding the one whose norm is approximately 1.0, which is mcT . Thus the result j T · (mcT ) = mT (mcT ) = (mT m)cT = cT produces the desired answer cat. Similarly, in Category 18, if question encoding (e.g., “Does the chocolate ﬁt in the box?” = cbT ) is achievable by some inner products of statement encodings, the answer must be ‘yes’, otherwise, ‘no’.  On the other hand, in Category 16, transitivity is applied reversely as a container-containee fashion. For instance, “Lily is a ℓion” is encoded by ℓlT , whereas “Lily is green” is encoded by lgT . In encoding “x is-a Y”, we put the more general concept at the left side of the outer-product binding Y xT ; to encode “x has-property Z” we use xZ T . This allows us to induce a property for the general category Y based on the single observation of one of its members, via simple matrix multiplication, just as transitive inference was implemented above: (ℓlT ) · (lgT ) = ℓgT , meaning “ℓion is green.” Similarly in Category 20, there exists precisely one statement which describes a property of an actor (e.g., “Sumit is bored.” = bsT ). Then a statement describes the actor’s relocation (e.g., “Sumit journeyed to the garden.” = sgT ), yielding an inductive conclusion by matrix multiplication: “Being boring makes people go to the garden.” = (bsT ) · (sgT ) = bgT . The inductive reasoning also generalizes to other actions (e.g., the reason for later activity, “Sumit grabbed the football.” = sf T , is also being bored, because (bsT ) · (sf T ) = bf T ).  7  Published as a conference paper at ICLR 2016  Category 15. Basic Deduction 01: Mice are afraid of cats. 02: Emily is a mouse. 03: Wolves are afraid of mice. 04: Cats are afraid of sheep. 05: Winona is a cat. 06: Sheep are afraid of wolves. 07: Jessica is a mouse. 08: Gertrude is a sheep. 09: What is Jessica afraid of? cat 7 1 10: What is Emily afraid of? cat 2 1 11: What is Jessica afraid of? cat 7 1 12: What is Winona afraid of? sheep 5 4  Category 16: Basic Induction 01: Bernhard is a lion. 02: Julius is a lion. 03: Lily is a lion. 04: Bernhard is green. 05: Lily is green. 06: Brian is a lion. 07: Greg is a swan. 08: Greg is gray. 09: Julius is yellow. 10: What color is Brian? green 6 3 5  Category 18: Reasoning about Size 01: The suitcase is bigger than the container. 02: The container ﬁts inside the box. 03: The chest is bigger than the chocolate. 04: The suitcase ﬁts inside the box. 05: The chest ﬁts inside the box. 06: Does the chocolate ﬁt in the box? yes 5 3 07: Does the chocolate ﬁt in the box? yes 5 3 08: Does the box ﬁt in the container? no 1 4 09: Is the box bigger than the chocolate? yes 5 3 10: Does the box ﬁt in the chocolate? no 3 5  Category 20: Reasoning about Motivations 01: Sumit is bored. 02: Where will Sumit go? garden 1 03: Yann is hungry. 04: Where will Yann go? kitchen 3 05: Yann went back to the kitchen. 06: Why did Yann go to the kitchen? hungry 3 07: Sumit journeyed to the garden. 08: Why did Sumit go to the garden? bored 1 09: Yann picked up the apple there. 10: Why did Yann get the apple? hungry 3 11: Sumit grabbed the football there. 12: Why did Sumit get the football? bored 1  Figure 4: Sample statements(black), questions(blue), answers(red), and clues(green) for Category 15, 16, 18, and 20. Categories 15 and 18 create chains from smaller/weaker to stronger/larger, whereas Categories 16 and 20 from general ones to speciﬁc ones.  cats  box  mice  sheep  suitcase  chest  wolves  container  chocolate  Figure 5: The circular food chain (Category 16) and the partial order (Category 18) corresponding to the examples in Figure 4. The arrows imply afraid-of and ﬁts-inside relations, respectively.  Prior Knowledge (4, 14) Though statements in Category 4 looks quite dissimilar from those in the other categories, they can be eventually modeled by a uni-relational reasoning chain based on the containee-container relation, provided we know that ‘north’ and ‘south’ are opposite to each other. Thus the ﬁrst two statements in the ﬁrst Category 4 example in Figure 6 yield {koT , gkT }, from which we infer (gkT ) · (koT ) = goT “The ofﬁce is north of the garden.” While the questions are all simple knowledge conﬁrmation, note that a relational word (e.g., ‘east’) might never appear in the prior statements, as illustrated in the second example of Category 4 in Figure 6. However the most important point is that two non-collinear relations (e.g., ‘north’, ‘east’) never appear together in the same example.  On the other hand, statements in Category 14 are no longer chronologically ordered. In order to infer a correct locational trajectory without repeating statements multiple times, we predeﬁne four vectors for each time stamp: yesterday, this morning, this afternoon, and this evening, and bind location with the corresponding stamp instead of the previous location. For example, the encoding for the statement at time 2 now becomes j(b ◦ m)T instead of j(b ◦ p)T . Knowing the correct order of these four time stamps, which could be learned from the training examples, we can easily reorder by unbinding time stamps.  8  Published as a conference paper at ICLR 2016  Category 4. Two Argument Relation 01: The ofﬁce is north of the kitchen. 02: The garden is south of the kitchen. 03: What is north of the kitchen? ofﬁce 1 ———————————————————— 01: The kitchen is west of the garden. 02: The hallway is west of the kitchen. 03: What is the garden east of? kitchen 1  Category 14: Time Manipulation 01: Yesterday Julie went back to the park. 02: Julie went to the bedroom this morning. 03: Bill journeyed to the cinema yesterday. 04: This morning Bill went back to the park. 05: Where was Bill before the park? cinema 4 3 06: This evening Julie went to the school. 07: This afternoon Julie went back to the park. 08: Where was Julie before the bedroom? park 2 1  Category 17: Positional Reasoning 01: The triangle is above the pink rectangle. 02: The blue square is to the left of the triangle. 03: Is the pink rectangle to the right of the blue square? yes 1 2 ———————————————————– 01: The red sphere is below the yellow square. 02: The red sphere is above the blue square. 03: Is the blue square below the yellow square? yes 2 1  Category 19: Path Finding 01: The bedroom is south of the hallway. 02: The bathroom is east of the ofﬁce. 03: The kitchen is west of the garden. 04: The garden is south of the ofﬁce. 05: The ofﬁce is south of the bedroom. 06: How do you go from the garden to the bed- room? n,n 4 5  Figure 6: Sample statements(black), questions(blue), answers(red), and clues(green) for Category 4, 14, 17, and 19. Categories 4 and 17 contains two different examples separated by a horizontal line.  3.2 MULTIPLE RELATIONSHIPS  Path Finding (19) Our goal in this category is to ﬁnd the path from one location to another location in a Manhattan-grid-like sense. Note that if A is north of B, and B is north of C, then the right path from A to C in grid must be ‘north, north’ rather than simply ’north’. We assume given four d × d non-singular matrices N, E, W, S encoding four different directions satisfying N = S−1 and E = W −1. Then  # Statements/Questions 1 The bedroom is south of the hallway. 2 The βathroom is east of the ofﬁce. 3 The kitchen is west of the garden. 4 The garden is south of the ofﬁce. 5 The ofﬁce is south of the bedroom. 6 How do you go from the garden to the bedroom? n,n  Encodings Seq Translations/Answers/Clues (1) Decides b given the initial h. b = Sh (3) Defer until we know either o or β. β = Eo (5) Defer until we know either g or k. k = W g (4) Defer until we know either o or g. g = So (2) Decides o given b. o = Sb (6) b = Xg  4, 5  Table 3: Sample multi-relational translations and corresponding encodings from Category 19. Sym- bols in encodings are either d-dimensional object vectors (hallway, bedroom, ofﬁce, βathroom, garden, kitchen) or d × d directional matrices (South, East, W est, N orth). The last column shows the sequence of actual running order.  After initializing the ﬁrst object in the right-hand side (e.g., ‘hallway’) by a random vector, we decide the rest of the object vectors in sequence by multiplying the directional matrix (or its inverse in case that the right-hand side is unknown and the left-hand side is known). In case that both sides are unknown, we defer such a statement by putting it into a queue. In fact, the solution path X can be determined either by selecting, of all combinations of two directions {NN, NE, NW, NS, ... SN, SE, SW, SS}, the one which best satisﬁes b = Xg (in the example of Table 3) or by solving this equation based on iterative substitutions. Note also that we need to know that (n, e, w, s) in the answers correspond to (north, east, west, south), respectively, which could be learned from training data.  Positional Reasoning (17) While this category could be seen similar to Path Finding, each ques- tion only asks a relative position between two objects. For instance, if “r is below s”, and “b is below r”, then the position of b with respect to s must be simply ‘below’ rather than ‘below, below’. Even if an object is mentioned to be left of another object, it could be also located in left-above or left- below of another object. Due to these subtleties, we here adopt redundant representations with four d × d singular matrices (A, B, L, R) corresponding to four directions: (above, below, left, right).  9  Published as a conference paper at ICLR 2016  For this directional subsumption, in contrast to the non-singularity of the directional matrices for Category 19, we now strictly enforce idempotency to these matrices (i.e., An = ... = A2 = A 6= I). Then we deﬁne the following four 4d × 4d block matrices and encode each statement with these matrices in the same manner as for Category 19.  A =    A 0 0 I 0 0 0 0  0 0 I 0  0 0 0 I     B =    0  0 I 0 B 0 0 I 0 0  0 0  0 0 0 I     L =    I 0 0 0  0 0  0 0 0 I 0 L 0 0 I  0     R =    I 0 0 0  0 I 0 0  0 0 0 0 0 I 0 R     In this encoding, each of the four d-dimensional subspaces of R4d plays a role of indicating relative positions with respect to (above, below, left, right), independently. Carrying out the encoding of “r is below s”, r = Bs, ensures that the components of r and s differ only in the dimensions from (d + 1) to 2d (from the B block of B); that is, rk = sk for k = 1, 3, 4 (where si indicates the i-th d-dimensional sub-block of s). This is actually inconsistent with the encoding of “s is above r”, which demands that s and r differ only in their ﬁrst sub-block. Thus in order to determine whether or not s is indeed above r, it is necessary to check whether r2 = Bs2 as well as whether s1 = Ar1. If either condition is satisﬁed, we can conﬁrm ‘s is above to r’ . Similarly, horizontal relations must be checked on both the third and fourth d-dimensional sub-blocks.  4 EXPERIMENTAL RESULTS  We implement our models and algorithms under the analysis given in the previous section. Due to the small vocabulary (mostly less than or equal to four elements among actors, objects, locations, and actions) and non-ambiguous grammars, a simple dependency parser4 and basic named entity recognition enable us to achieve 100% accurate semantic parsing. Then we translate every statement into a representation based on the appropriate containee-container or multiway relation, and then store it in an array of memory slots. The logical reasoning after semantic parsing and knowledge representation no longer refers to the original text symbols.  Type  Accuracy  Model Type  Accuracy  Model  C5  C4  C3  C2  C10 C1 C6 99% 100% 100% 100% 100% 99.3% 100% DMN SSVM DMN MNN MNN MNN MNN MNN C20 C19 C11 C14 C15 C16 100% 100% 100% 100% 100% 100% 36% 100% MNN MNN MNN DMN MNN MNN Multitask MNN MNN MNN  C9 96.5% 100% DMN C18 95%  96.9% DMN C17 72%  C12  C13  C7  C8  Table 4: Best accuracies for each category and the model that achieved the best accuracy. MNN indicates Strongly-Supervised MemNN trained with the clue numbers, and DMN indicates Dynamic MemNN, and SSVM indicates Structured SVM with the coreference resolution and SRL features. Multitask indicates multitask training.  In contrast to all previous models reported in Table 4, in Table 5 we also report test accuracy on the training data to measure how well our models incorporate common sense. Note that testing on the training data is available because our training procedure only parses the appropriate semantic components such as actors, objects, locations, actions, and the forms of answers without using given answers and clues for tuning the model parameters.  Note that the imperfect accuracy in Category 16 is due to the ambiguity of evidence. As given in Figure 4, one can answer the color of Brian as ‘yellow’ because the latest evidence tells Julius who is a lion is yellow. Similarly, in Category 5, the 8th story consists of incorrect/inconsistent answers at time 14 and 17 (for training), as they ignore the most recent ownership transfers and choose some old history as ground-truth answers. (The 63rd and 186th stories in the test data also consist of incorrect answers, at time 27 and 22, respectively) Other than these two categories, we achieve perfect accuracies performing common-sense operations only on representations in memory.  4We use Stanford Dependency Parser. http://nlp.stanford.edu/software/stanford-dependencies.shtml  10  Published as a conference paper at ICLR 2016  Type  Training  Test Type  Training  Test  C5  C4  C3  C2  C1 C10 100% 100% 100% 100% 99.8% 100% 100% 100% 100% 100% 100% 100% 100% 100% 99.8% 100% 100% 100% 100% 100% C11 C20 100% 100% 100% 100% 100% 99.4% 100% 100% 100% 100% 100% 100% 100% 100% 100% 99.5% 100% 100% 100% 100%  C19  C18  C12  C13  C14  C15  C16  C17  C6  C7  C8  C9  Table 5: Accuracies on training and test data on our models. We achieve near-perfect accuracy in almost every category including positional reasoning and path ﬁnding.  As the experimental results show, there is a clear distinction between two sets of tasks. Tasks in most categories can be modeled by a containee-container-like relationship respecting a transitivity- like inference rule, whose goals are to create a linear/circular chain. On the other hand, positional reasoning and path ﬁnding require multiple relationships where each corresponding pair (e.g., north vs. south) has its own transitivity structure, operating independently of other pairs (e.g. east vs. west). We hypothesize that this difference poses a major difﬁculty for most of Memory Network models to perform an accurate inference for positional reasoning and path ﬁnding.  Recently, Neural Reasoner (NR) by Peng et al. (2015) improves the accuracy for these two difﬁcult categories by a large margin, achieving 97.9% and 87.0% when using 10k training set. 5 Different from other memory network models, NR has multiple reasoning layers. Starting from the initial statements and questions, NR constructs new statements and questions at the next layer, and repeats this process recursively over multiple layers. As both positional reasoning and path ﬁnding require generating inferences from, and new versions of, relevant statements for each relationship (e.g., “x is north of ” can become “y is south of x”), the abilities to generate new facts and to derive ﬁnal answers by integrating them from multiple relationships could be a key reason why NR is successful, like our TPR-based reasoner. While NR in experiment is simpliﬁed so that all new facts maintain the same initial representations, the question representation changes for each layer considering all existing facts and the previously evolved question. Due to the simplicity of the task, we conjecture that evolving representations of the question could be sufﬁcient to comprise the key ingredient for each multi-relationship. However, it seems that training such multiple layers requires a large amount of training data, yielding drastically different performance of NR on two different dataset sizes.  5 CONCLUSION  The major contributions of this paper are two-fold. First, we throughly analyze the recently ac- claimed bAbI question-answering tasks by grouping the twenty categories based on their relational properties. Our analysis reveals that most categories except positional reasoning and path ﬁnding are governed by uni-relational characteristics. As these turn out to support inference in a similar manner under transitivity, it could be dangerous to evaluate the capacity of network models based only on their performance on bAbI. In contrast, two more difﬁcult categories require the capability of performing multi-relational reasoning, a capability which is apparently missing in most previous models. One could later develop a more sophisticated dataset that needs substantially harder reason- ing by introducing multiple relationships. Second, we propose two vector space models which can perform logistic reasoning for QA with distributed representations. While TPR has been used for various problems such as tree/grammar encoding and lambda-calculus evaluation, logical reasoning is a new area of application that requires iterative processing of TPRs. In subsequent work, we will generalize the vector-space approach for multi-relational problems. We hope these studies shed light on the viability of developing further reasoning models which can perform inference with existing knowledge in an interpretable and transparent manner.  55 All accuracy values of various models reported in the experimental section of the present paper are based  on a 1k training set. Neural Reasoner achieves 66.4% and 17.3% when using the 1k dataset.  11  Published as a conference paper at ICLR 2016  REFERENCES Bengio, Yoshua, Ducharme, Rejean, Vincent, Pascal, and Janvin, Christian. A neural probabilistic language  model. JMLR, 3:1137–1155, 2003.  Berant, Jonathan, Chou, Andrew, Frostig, Roy, and Liang, Percy. Semantic parsing on Freebase from question- answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/D13-1160.  Bordes, Antoine, Chopra, Sumit, and Weston, Jason.  Question answering with subgraph embed- dings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 615–620, Doha, Qatar, October 2014. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/D14-1067.  Dupoux, Emmanuel. Deconstructing ai-complete question-answering: going beyond toy tasks. 2015. URL  http://bootphon.blogspot.com/2015/04/deconstructing-ai-complete-question.html.  Grefenstette, Edward. Towards a formal distributional semantics: Simulating logical calculi with tensors.  Association for Computational Linguistics, 2013.  Jones, Michael N. and Mewhort, Douglas J. K. Representing word meaning and order information in a com-  posite holographic lexicon. Psychological Review, 114:1–37, 2007.  Kumar, Ankit, Irsoy, Ozan, Su, Jonathan, Bradbury, James, English, Robert, Pierce, Brian, Ondruska, Peter, Gulrajani, Ishaan, and Socher, Richard. Ask me anything: Dynamic memory networks for natural language processing. CoRR, abs/1506.07285, 2015. URL http://arxiv.org/abs/1506.07285.  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed representations of  words and phrases and their compositionality. pp. 3111–3119. 2013.  Peng, Baolin, Lu, Zhengdong, Li, Hang, and Wong, Kam-Fai. Towards neural network-based reasoning. CoRR,  abs/1508.05508, 2015. URL http://arxiv.org/abs/1508.05508.  Pennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: Global vectors for word represen-  tation. pp. 1532–1543, 2014. URL http://www.aclweb.org/anthology/D14-1162.  Rocktaschel, Tim, Singh, Sameer, Bosnjak, Matko, and Riedel, Sebastian. Low-dimensional embeddings of  logic. 2014.  Smolensky, Paul. Tensor product variable binding and the representation of symbolic structures in connectionist  systems. Artiﬁcial Intelligence, 46(1-2), 1990.  Smolensky, Paul. Symbolic functions from neural computation. Philosophical Transactions of the Royal  Society, 370:3543–3569, 2012.  Smolensky, Paul and Legendre, Geraldine. The Harmonic Mind: From Neural Computation to Optimality-  Theoretic GrammarVolume I: Cognitive Architecture. The MIT Press, 2006.  Smolensky, Paul, Lee, Moontae, He, Xiaodong, Yih, Wen-tau, Gao, Jianfeng, and Deng, Li. Basic Technical Report, Microsoft Research, 2016. URL  reasoning with tensor product representations. http://arxiv.org/abs/1601.02745.  Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and Fergus, Rob. End-to-end memory networks. CoRR,  abs/1503.08895, 2015. URL http://arxiv.org/abs/1503.08895.  Ustun, Volkan, Rosenbloom, Paul S., Sagae, Kenji, and Demski, Abram. Distributed vector representations of  words in the sigma cognitive architecture. 2014.  Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Memory networks. CoRR, abs/1410.3916, 2014. URL  http://arxiv.org/abs/1410.3916.  Weston, Jason, Bordes, Antoine, Chopra, Sumit, Mikolov, Tomas, Rush, Alexander M., and van Merrienboer, Bart. Towards ai-complete question answering: A set of prerequisite toy tasks. volume abs/1502.05698. 2015. URL http://arxiv.org/abs/1502.05698.  Yih, Wen-Tau, Chang, MingWei, He, Xiaodong, and Gao, Jianfeng. Semantic parsing via staged query graph  generation: Question answering with knowledge base. 2015.  12  ",
1511.08228,2016,Neural GPUs Learn Algorithms,"['Neural GPUs Learn Algorithms [code] [video]\nLukasz Kaiser', 'Ilya Sutskever']",https://arxiv.org/pdf/1511.08228,"6 1 0 2    r a     M 5 1      ]  G L . s c [      3 v 8 2 2 8 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  NEURAL GPUS LEARN ALGORITHMS  Łukasz Kaiser & Ilya Sutskever Google Brain {lukaszkaiser,ilyasu}@google.com  ABSTRACT  Learning an algorithm from examples is a fundamental problem that has been widely studied. It has been addressed using neural networks too, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efﬁcient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an al- gorithmic task and successfully generalize to long instances. We veriﬁed it on a number of tasks including long addition and long multiplication of numbers rep- resented in binary. We train the Neural GPU on numbers with up-to 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent net- works: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.  1  INTRODUCTION  Deep neural networks have recently proven successful at various tasks, such as computer vision (Krizhevsky et al., 2012), speech recognition (Dahl et al., 2012), and in other domains. Recurrent neural networks based on long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997) have been successfully applied to a number of natural language processing tasks. Sequence-to- sequence recurrent neural networks with such cells can learn very complex tasks in an end-to-end manner, such as translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing (Vinyals & Kaiser et al., 2015), speech recognition (Chan et al., 2016) or image caption generation (Vinyals et al., 2014). Since so many tasks can be solved with essentially one model, a natural question arises: is this model the best we can hope for in supervised learning?  Despite its recent success, the sequence-to-sequence model has limitations. In its basic form, the entire input is encoded into a single ﬁxed-size vector, so the model cannot generalize to inputs much longer than this ﬁxed capacity. One way to resolve this problem is by using an attention mechanism (Bahdanau et al., 2014). This allows the network to inspect arbitrary parts of the input in every de- coding step, so the basic limitation is removed. But other problems remain, and Joulin & Mikolov (2015) show a number of basic algorithmic tasks on which sequence-to-sequence LSTM networks fail to generalize. They propose a stack-augmented recurrent network, and it works on some prob- lems, but is limited in other ways.  In the best case one would desire a neural network model able to learn arbitrarily complex algorithms given enough resources. Neural Turing Machines (Graves et al., 2014) have this theoretical property. However, they are not computationally efﬁcient because they use soft attention and because they tend to be of considerable depth. Their depth makes the training objective difﬁcult to optimize and im- possible to parallelize because they are learning a sequential program. Their use of soft attention requires accessing the entire memory in order to simulate 1 step of computation, which introduces substantial overhead. These two factors make learning complex algorithms using Neural Turing Ma-  1  Published as a conference paper at ICLR 2016  chines difﬁcult. These issues are not limited to Neural Turing Machines, they apply to other architec- tures too, such as stack-RNNs (Joulin & Mikolov, 2015) or (De)Queue-RNNs (Grefenstette et al., 2015). One can try to alleviate these problems using hard attention and reinforcement learning, but such non-differentiable models do not learn well at present (Zaremba & Sutskever, 2015b).  In this work we present a neural network model, the Neural GPU, that addresses the above issues. It is a Turing-complete model capable of learning arbitrary algorithms in principle, like a Neural Turing Machine. But, in contrast to Neural Turing Machines, it is designed to be as parallel and as shallow as possible. It is more similar to a GPU than to a Turing machine since it uses a smaller num- ber of parallel computational steps. We show that the Neural GPU works in multiple experiments:  • A Neural GPU can learn long binary multiplication from examples. It is the ﬁrst neural network able to learn an algorithm whose run-time is superlinear in the size of its input. Trained on up-to 20-bit numbers, we see no single error on any inputs we tested, and we tested on numbers up-to 2000 bits long.  • The same architecture can also learn long binary addition and a number of other algorith-  mic tasks, such as counting, copying sequences, reversing them, or duplicating them.  1.1 RELATED WORK  The learning of algorithms with neural networks has seen a lot of interest after the success of sequence-to-sequence neural networks on language processing tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014). An attempt has even been made to learn to evaluate sim- ple python programs with a pure sequence-to-sequence model (Zaremba & Sutskever, 2015a), but more success was seen with more complex models. Neural Turing Machines (Graves et al., 2014) were shown to learn a number of basic sequence transformations and memory access patterns, and their reinforcement learning variant (Zaremba & Sutskever, 2015b) has reasonable performance on a number of tasks as well. Stack, Queue and DeQueue networks (Grefenstette et al., 2015) were also shown to learn basic sequence transformations such as bigram ﬂipping or sequence reversal.  The Grid LSTM (Kalchbrenner et al., 2016) is another powerful architecture that can learn to mul- tiply 15-digit decimal numbers. As we will see in the next section, the Grid-LSTM is quite similar to the Neural GPU – the main difference is that the Neural GPU is less recurrent and is explicitly constructed from the highly parallel convolution operator.  In image processing, convolutional LSTMs, an architecture similar to the Neural GPU, have recently been used for weather prediction (Shi et al., 2015) and image compression (Toderici et al., 2016). We ﬁnd it encouraging as it hints that the Neural GPU might perform well in other contexts.  Most comparable to this work are the prior experiments with the stack-augmented RNNs (Joulin & Mikolov, 2015). These networks manage to learn and generalize to unseen lengths on a number of algorithmic tasks. But, as we show in Section 3.1, stack-augmented RNNs trained to add numbers up-to 20-bit long generalize only to ∼ 100-bit numbers, never to 200-bit ones, and never without error. Still, their generalization is the best we were able to obtain without using the Neural GPU and far surpasses a baseline LSTM sequence-to-sequence model with attention.  The quest for learning algorithms has been pursued much more widely with tools other than neu- ral networks. It is known under names such as program synthesis, program induction, automatic programming, or inductive synthesis, and has a long history with many works that we do not cover here; see, e.g., Gulwani (2010) and Kitzelmann (2010) for a more general perspective.  Since one of our results is the synthesis of an algorithm for long binary addition, let us recall how this problem has been addressed without neural networks. Importantly, there are two cases of this problem with different complexity. The easier case is when the two numbers that are to be added are aligned at input, i.e., if the ﬁrst (lower-endian) bit of the ﬁrst number is presented at the same time as the ﬁrst bit of the second number, then come the second bits, and so on, as depicted below for x = 9 = 8 + 1 and y = 5 = 4 + 1 written in binary with least-signiﬁcant bit left.  Input  (x and y aligned)  Desired Output (x + y)  1 1 0  0 0 1  0 1 1  1 0 1  2  Published as a conference paper at ICLR 2016  In this representation the triples of bits from (x, y, x + y), e.g., (1, 1, 0) (0, 0, 1) (0, 1, 1) (1, 0, 1) as in the ﬁgure above, form a regular language. To learn binary addition in this representation it therefore sufﬁces to ﬁnd a regular expression or an automaton that accepts this language, which can be done with a variant of Anguin’s algorithm (Angluin, 1987). But only few interesting functions have regular representations, as for example long multiplication does not (Blumensath & Gr¨adel, 2000). It is therefore desirable to learn long binary addition without alignment, for example when x and y are provided one after another. This is the representation we use in the present paper.  Input (x, y)  Desired Output (x + y)  1 0  0 1  0 1  1 1  +  1  0  1  0  2 THE NEURAL GPU  Before we introduce the Neural GPU, let us recall the architecture of a Gated Recurrent Unit (GRU) (Cho et al., 2014). A GRU is similar to an LSTM, but its input and state are the same size, which makes it easier for us to generalize it later; a highway network could have also been used (Srivastava et al., 2015), but it lacks the reset gate. GRUs have shown performance similar to LSTMs on a number of tasks (Chung et al., 2014; Greff et al., 2015). A GRU takes an input vector x and a current state vector s, and outputs:  GRU(x, s) = u ⊙ s + (1 − u) ⊙ tanh(W x + U (r ⊙ s) + B), where  u = σ(W ′x + U ′s + B′)  and r = σ(W ′′x + U ′′s + B′′).  In the equations above, W, W ′, W ′′, U, U ′, U ′′ are matrices and B, B′, B′′ are bias vectors; these are the parameters that will be learned. We write W x for a matrix-vector multiplication and r ⊙ s for elementwise vector multiplication. The vectors u and r are called gates since their elements are in [0, 1] — u is the update gate and r is the reset gate. In recurrent neural networks a unit like GRU is applied at every step and the result is both passed as new state and used to compute the output. In a Neural GPU we do not process a new input in every step. Instead, all inputs are written into the starting state s0. This state has 2-dimensional structure: it consists of w × h vectors of m numbers, i.e., it is a 3-dimensional tensor of shape [w, h, m]. This mental image evolves in time in a way deﬁned by a convolutional gated recurrent unit:  CGRU(s) = u ⊙ s + (1 − u) ⊙ tanh(U ∗ (r ⊙ s) + B), where  u = σ(U ′ ∗ s + B′) and r = σ(U ′′ ∗ s + B′′).  U ∗ s above denotes the convolution of a kernel bank U with the mental image s. A kernel bank is a 4-dimensional tensor of shape [kw, kh, m, m], i.e., it contains kw · kh · m2 parameters, where kw and kh are kernel width and height. It is applied to a mental image s of shape [w, h, m] which results in another mental image U ∗ s of the same shape deﬁned by:  U ∗ s[x, y, i] =  ⌊kw/2⌋ X  ⌊kh/2⌋ X  u=⌊−kw/2⌋  v=⌊−kh/2⌋  m X c=1  s[x + u, y + v, c] · U [u, v, c, i].  In the equation above the index x + u might sometimes be negative or larger than the size of s, and in such cases we assume the value is 0. This corresponds to the standard convolution operator used in convolutional neural networks with zero padding on both sides and stride 1. Using the standard operator has the advantage that it is heavily optimized (see Section 4 for Neural GPU performance). New work on faster convolutions, e.g., Lavin & Gray (2015), can be directly used in a Neural GPU.  Knowing how a CGRU gate works, the deﬁnition of a l-layer Neural GPU is simple, as depicted in Figure 1. The given sequence i = (i1, . . . , in) of n discrete symbols from {0, . . . , I} is ﬁrst em- bedded into the mental image s0 by concatenating the vectors obtained from an embedding lookup of the input symbols into its ﬁrst column. More precisely, we create the starting mental image s0 of shape [w, n, m] by using an embedding matrix E of shape [I, m] and setting s0[0, k, :] = E[ik] (in python notation) for all k = 1 . . . n (here i1, . . . , in is the input). All other elements of s0 are set to 0. Then, we apply l different CGRU gates in turn for n steps to produce the ﬁnal mental image sﬁn:  st+1 = CGRUl(CGRUl−1 . . . CGRU1(st) . . .) and sﬁn = sn.  3  Published as a conference paper at ICLR 2016  i1  ...  in  CGRU1  CGRU2  . . .  CGRU1  CGRU2  o1  ...  on  s0  s1  sn−1  sn  Figure 1: Neural GPU with 2 layers and width w = 3 unfolded in time.  The result of a Neural GPU is produced by multiplying each item in the ﬁrst column of sﬁn by an output matrix O to obtain the logits lk = Osﬁn[0, k, :] and then selecting the maximal one: ok = argmax(lk). During training we use the standard loss function, i.e., we compute a softmax over the logits lk and use the negative log probability of the target as the loss. Since all components of a Neural GPU are clearly differentiable, we can train using any stochastic gradient descent optimizer. For the results presented in this paper we used the Adam optimizer (Kingma & Ba, 2014) with ε = 10−4 and gradients norm clipped to 1. The number of layers was set to l = 2, the width of mental images was constant at w = 4, the number of maps in each mental image point was m = 24, and the convolution kernels width and height was always kw = kh = 3.  Computational power of Neural GPUs. While the above deﬁnition is simple, it might not be immediately obvious what kind of functions a Neural GPU can compute. Why can we expect it to be able to perform long multiplication? To answer such questions it is useful to draw an analogy between a Neural GPU and a discrete 2-dimensional cellular automaton. Except for being discrete and the lack of a gating mechanism, such automata are quite similar to Neural GPUs. Of course, these are large exceptions. Dense representations have often more capacity than purely discrete states and the gating mechanism is crucial to avoid vanishing gradients during training. But the computational power of cellular automata is much better understood. In particular, it is well known that a cellular automaton can exploit its parallelism to multiply two n-bit numbers in O(n) steps using Atrubin’s algorithm. We recommend the online book (Vivien, 2003) to get an understanding of this algorithm and the computational power of cellular automata.  3 EXPERIMENTS  In this section, we present experiments showing that a Neural GPU can successfully learn a number of algorithmic tasks and generalize well beyond the lengths that it was trained on. We start with the two tasks we focused on, long binary addition and long binary multiplication. Then, to demonstrate the generality of the model, we show that Neural GPUs perform well on several other tasks as well.  3.1 ADDITION AND MULTIPLICATION  The two core tasks on which we study the performance of Neural GPUs are long binary addition and long binary multiplication. We chose them because they are fundamental tasks and because there is no known linear-time algorithm for long multiplication. As described in Section 2, we input a sequence of discrete symbols into the network and we read out a sequence of symbols again. For binary addition, we use a set of 4 symbols: {0, 1, +, PAD} and for multiplication we use {0, 1, ·, PAD}. The PAD symbol is only used for padding so we depict it as empty space below.  Long binary addition (badd) is the task of adding two numbers represented lower-endian in binary notation. We always add numbers of the same length, but we allow them to have 0s at start, so numbers of differing lengths can be padded to equal size. Given two d-bit numbers the full sequence length is n = 2d + 1, as seen in the example below, representing (1 + 4) + (2 + 4 + 8) = 5 + 14 = 19 = (16 + 2 + 1).  4  Published as a conference paper at ICLR 2016  Task@Bits badd@20 badd@25 badd@100 badd@200 badd@2000 bmul@20 bmul@25 bmul@200 bmul@2000  Neural GPU  stackRNN LSTM+A  100% 100% 100% 100% 100% 100% 100% 100% 100%  100% 100% 88% 0% 0% N/A N/A N/A N/A  100% 73% 0% 0% 0% 0% 0% 0% 0%  Table 1: Neural GPU, stackRNN, and LSTM+A results on addition and multiplication. The table shows the fraction of test cases for which every single bit of the model’s output is correct.  Input Output  1 1  0 1  1 0  0 0  + 1  0  1  1  1  Long binary multiplication (bmul) is the task of multiplying two binary numbers, represented lower-endian. Again, we always multiply numbers of the same length, but we allow them to have 0s at start, so numbers of differing lengths can be padded to equal size. Given two d-bit numbers, the full sequence length is again n = 2d+1, as seen in the example below, representing (2+4)·(2+8) = 6 · 10 = 60 = 32 + 16 + 8 + 4.  Input Output  0 0  1 0  1 1  0 1  · 1  0  1  0  1  Models. We compare three different models on the above tasks. In addition to the Neural GPU we include a baseline LSTM recurrent neural network with an attention mechanism. We call this model LSTM+A as it is exactly the same as described in (Vinyals & Kaiser et al., 2015). It is a 3-layer model with 64 units in each LSTM cell in each layer, which results in about 200k param- eters (the Neural GPU uses m = 24 and has about 30k paramters). Both the Neural GPU and the LSTM+A baseline were trained using all the techniques described below, including curriculum training and gradient noise. Finally, on binary addition, we also include the stack-RNN model from (Joulin & Mikolov, 2015). This model was not trained using our training regime, but in exactly the way as provided in its source code, only with nmax = 41. To match our training procedure, we ran it 729 times (cf. Section 3.3) with different random seeds and we report the best obtained result.  Results. We measure also the rate of fully correct output sequences and report the results in Ta- ble 1. For both tasks, we show ﬁrst the error at the maximum length seen during training, i.e., for 20-bit numbers. Note that LSTM+A is not able to learn long binary multiplication at this length, it does not even ﬁt the training data. Then we report numbers for sizes not seen during training.  As you can see, a Neural GPU can learn a multiplication algorithm that generalizes perfectly, at least as far as we were able to test (technical limits of our implementation prevented us from testing much above 2000 bits). Even for the simpler task of binary addition, stack-RNNs work only up-to length 100. This is still much better than the LSTM+A baseline which only generalizes to length 25.  3.2 OTHER ALGORITHMIC TASKS  In addition to the two main tasks above, we tested Neural GPUs on the following simpler algorithmic tasks. The same architecture as used above was able to solve all of the tasks described below, i.e., after being trained on sequences of length up-to 41 we were not able to ﬁnd any error on sequences on any length we tested (up-to 4001).  Copying sequences very easy for a Neural GPU, in fact all models converge quickly and generalize perfectly.  is the simple task of producing on output the same sequence as on input. It is  Reversing sequences  is the task of reversing a sequence of bits, n is the length of the sequence.  5  Published as a conference paper at ICLR 2016  Duplicating sequences is the task of duplicating the input bit sequence on output twice, as in the example below. We use the padding symbol on input to make it match the output length. We trained on sequences of inputs up-to 20 bits, so outputs were up-to 40-bits long, and tested on inputs up-to 2000 bits long.  Input Output  0 0  0 0  1 1  1 1  0  0  1  1  Counting by sorting bits is the task of sorting the input bit sequence on output. Since there are only 2 symbols to sort, this is a counting tasks – the network must count how many 0s are in the input and produce the output accordingly, as in the example below.  Input Output  1 0  0 0  1 0  1 0  0 1  0 1  1 1  0 1  3.3 TRAINING TECHNIQUES  Here we describe the training methods that we used to improve our results. Note that we applied these methods to the LSTM+A baseline as well, to keep the above comparison fair. We focus on the most important elements of our training regime, all less relevant details can be found in the code which is released as open-source.1  Grid search. Each result we report is obtained by running a grid search over 36 = 729 instances. We consider 3 settings of the learning rate, initial parameters scale, and 4 other hyperparameters discussed below: the relaxation pull factor, curriculum progress threshold, gradient noise scale, and dropout. An important effect of running this grid search is also that we train 729 models with differ- ent random seeds every time. Usually only a few of these models generalize to 2000-bit numbers, but a signiﬁcant fraction works well on 200-bit numbers, as discussed below.  Curriculum learning. We use a curriculum learning approach inspired by Zaremba & Sutskever (2015a). This means that we train, e.g., on 7-digit numbers only after crossing a curriculum progress threshold (e.g., over 90% fully correct outputs) on 6-digit numbers. However, with 20% probability we pick a minibatch of d-digit numbers with d chosen uniformly at random between 1 and 20.  Gradients noise. To improve training speed and stability we add noise to gradients in each training step. Inspired by the schedule from Welling & Teh (2011), we add to gradients a noise drawn from the normal distribution with mean 0 and variance inversely proportional to the square root of step- number (i.e., with standard deviation proportional to the 4-th root of step-number). We multiply this noise by the gradient noise scale and, to avoid noise in converged models, we also multiply it by the fraction of non-fully-correct outputs (which is 0 for a perfect model).  Gate cutoff. In Section 2 we deﬁned the gates in a CGRU using the sigmoid function, e.g., we wrote u = σ(U ′ ∗ s + B′). Usually the standard sigmoid function is used, σ(x) = 1 1+e−x . We found that adding a hard threshold on the top and bottom helps slightly in our setting, so we use 1.2σ(x) − 0.1 cut to the interval [0, 1], i.e., σ′(x) = max(0, min(1, 1.2σ(x) − 0.1)).  3.3.1 DROPOUT ON RECURRENT CONNECTIONS  Dropout is a widely applied technique for regularizing neural networks. But when applying it to recurrent networks, it has been counter-productive to apply it on recurrent connections – it only worked when applied to the non-recurrent ones, as reported by Pham et al. (2014).  Since a Neural GPU does not have non-recurrent connections it might seem that dropout will not be useful for this architecture. Surprisingly, we found the contrary – it is useful and improves generalization. The key to using dropout effectively in this setting is to set a small dropout rate.  When we run a grid search for dropout rates we vary them between 6%, 9%, and 13.5%, meaning that over 85% of the values are always preserved. It turns out that even this small dropout has large  1The code is at https://github.com/tensorflow/models/tree/master/neural_gpu.  6  Published as a conference paper at ICLR 2016  effect since we apply it to the whole mental image si in each step i. Presumably the network now learns to include some redundancy in its internal representation and generalization beneﬁts from it.  Without dropout we usually see only a few models from a 729 grid search generalize reasonably, while with dropout it is a much larger fraction and they generalize to higher lengths. In particular, dropout was necessary to train models for multiplication that generalize to 2000 bits.  3.3.2 PARAMETER SHARING RELAXATION.  To improve optimization of our deep network we use a relaxation technique for shared parameters which works as follows. Instead of training with parameters shared across time-steps we use r identical sets of non-shared parameters (we often use r = 6, larger numbers work better but use more memory). At time-step t of the Neural GPU we use the i-th set if t mod r = i. The procedure described above relaxes the network, as it can now perform different operations in different time-steps. Training becomes easier, but we now have r parameters instead of the single shared set we want. To unify them we add a term to the cost function representing the distance of each parameter from the average of this parameter in all the r sets. This term in the ﬁnal cost function is multiplied by a scalar which we call the relaxation pull. If the relaxation pull is 0, the network behaves as if the r parameter sets were separate, but when it is large, the cost forces the network to unify the parameters across different set.  During training, we gradually increase the relaxation pull. We start with a small value and every time the curriculum makes progress, e.g., when the model performs well on 6-digit numbers, we multiply the relaxation pull by a relaxation pull factor. When the curriculum reaches the maximal length we average the parameters from all sets and continue to train with a single shared parameter set.  This method is crucial for learning multiplication. Without it, a Neural GPU with m = 24 has trouble to even ﬁt the training set, and the few models that manage to do it do not generalize. With relaxation almost all models in our 729 runs manage to ﬁt the training data.  4 DISCUSSION  We prepared a video of the Neural GPU trained to solve the tasks mentioned above.2. It shows the state in each step with values of −1 drawn in white, 1 in black, and other in gray. This gives an intuition how the Neural GPU solves the discussed problems, e.g., it is quite clear that for the duplication task the Neural GPU learned to move a part of the embedding downwards in each step. What did not work well? For one, using decimal inputs degrades performance. All tasks above can easily be formulated with decimal inputs instead of binary ones. One could hope that a Neural GPU will work well in this case too, maybe with a larger m. We experimented with this formulation and our results were worse than when the representation was binary: we did not manage to learn long decimal multiplication. Increasing m to 128 allows to learn all other tasks in the decimal setting. Another problem is that often only a few models in a 729 grid search generalize to very long unseen instances. Among those 729 models, there usually are many models that generalize to 40 or even 200 bits, but only a few working without error for 2000-bit numbers. Using dropout and gradient noise improves the reliability of training and generalization, but maybe another technique could help even more. How could we make more models achieve good generalization? One idea that looks natural is to try to reduce the number of parameters by decreasing m. Surprisingly, this does not seem to have any inﬂuence. In addition to the m = 24 results presented above we ran experiments with m = 32, 64, 128 and the results were similar. In fact using m = 128 we got the most models to generalize. Additionally, we observed that ensembling a few models, just by averaging their outputs, helps to generalize: ensembles of 5 models almost always generalize perfectly on binary tasks. Why use width? The Neural GPU is deﬁned using two-dimensional convolutions and in our exper- iments one of the dimensions is always set to 4. Doing so is not necessary since a one-dimensional Neural GPU that uses four times larger m can represent every function representable by the original one. In fact we trained a model for long binary multiplication that generalized to 2000-bit numbers using a Neural GPU with width 1 and m = 64. However, the width of the Neural GPU increases the  2The video is available at https://www.youtube.com/watch?v=LzC8NkTZAF4  7  Published as a conference paper at ICLR 2016  amount of information carried in its hidden state without increasing the number of its parameters. Thus it can be thought of as a factorization and might be useful for other tasks. Speed and data efﬁciency. Neural GPUs use the standard, heavily optimized convolution operation and are fast. We experimented with a 2-layer Neural GPU for n = 32 and m = 64. After unfolding in time it has 128 layers of CGRUs, each operating on 32 mental images, each 4 × 64 × 64 . The joint forward-backward step time for this network was about 0.6s on an NVIDIA GTX 970 GPU. We were also surprised by how data-efﬁcient a Neural GPU can be. The experiments presented above were all performed using 10k random training data examples for each training length. Since we train on up-to 20-bit numbers this adds to about 200k training examples. We tried to train using only 100 examples per length, so about 2000 total training instances. We were surprised to see that it actually worked well for binary addition: there were models that generalized well to 200-bit numbers and to all lengths below despite such small training set. But we never managed to train a good model for binary multiplication with that little training data.  5 CONCLUSIONS AND FUTURE WORK  The results presented in Table 1 show clearly that there is a qualitative difference between what can be achieved with a Neural GPU and what was possible with previous architectures. In particular, for the ﬁrst time, we show a neural network that learns a non-trivial superlinear-time algorithm in a way that generalized to much higher lengths without errors.  This opens the way to use neural networks in domains that were previously only addressed by discrete methods, such as program synthesis. With the surprising data efﬁciency of Neural GPUs it could even be possible to replicate previous program synthesis results, e.g., Kaiser (2012), but in a more scalable way. It is also interesting that a Neural GPU can learn symbolic algorithms without using any discrete state at all, and adding dropout and noise only improves its performance.  Another promising future work is to apply Neural GPUs to language processing tasks. Good results have already been obtained on translation with a convolutional architecture over words (Kalchbrenner & Blunsom, 2013) and adding gating and recursion, like in a Neural GPU, should allow to train much deeper models without overﬁtting. Finally, the parameter sharing relaxation technique can be applied to any deep recurrent network and has the potential to improve RNN train- ing in general.  REFERENCES Angluin, Dana. Learning regaular sets from queries and counterexamples. Information and Computation, 75:  87–106, 1987.  Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to  align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/1409.0473.  Blumensath, Achim and Gr¨adel, Erich. Automatic Structures. In Proceedings of LICS 2000, pp. 51–62, 2000.  URL http://www.logic.rwth-aachen.de/pub/graedel/BlGr-lics00.ps.  Chan, William, Jaitly, Navdeep, Le, Quoc V., and Vinyals, Oriol. Listen, attend and spell. In International  Conference on Acoustics, Speech and Signal Processing, ICASSP’16, 2016.  Cho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. URL http://arxiv.org/abs/1406.1078.  Chung, Junyoung, G¨ulc¸ehre, C¸ aglar, Cho, Kyunghyun, and Bengio, Yoshua.  of gated recurrent neural networks on sequence modeling. http://arxiv.org/abs/1412.3555.  CoRR, abs/1412.3555, 2014.  Empirical evaluation URL  Dahl, George E., Yu, Dong, Deng, Li, and Acero, Alex. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech & Language Processing, 20 (1):30–42, 2012.  Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural turing machines. CoRR, abs/1410.5401, 2014. URL  http://arxiv.org/abs/1410.5401.  8  Published as a conference paper at ICLR 2016  Grefenstette,  Edward, Hermann, Karl Moritz,  Suleyman, Mustafa,  and Blunsom,  Learning to transduce with unbounded memory. http://arxiv.org/abs/1506.02516.  CoRR,  abs/1506.02516,  2015.  Phil. URL  Greff, Klaus, Srivastava, Rupesh Kumar, Koutn´ık, Jan, Steunebrink, Bas R., and Schmidhuber, J¨urgen. LSTM: A search space odyssey. CoRR, abs/1503.04069, 2015. URL http://arxiv.org/abs/1503.04069.  Gulwani, Sumit. Dimensions in program synthesis. In Proceedings of PPDP 2010, PPDP ’10, pp. 13–24, 2010.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):1735–1780,  1997.  Joulin, Armand and Mikolov, Tomas.  Inferring algorithmic patterns with stack-augmented recurrent nets.  CoRR, abs/1503.01007, 2015. URL http://arxiv.org/abs/1503.01007.  Kaiser, Łukasz. Learning games from videos guided by descriptive complexity. In Proceedings of the AAAI-12,  pp. 963–970. AAAI Press, 2012. URL http://goo.gl/mRbfV5.  Kalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. In Proceedings EMNLP 2013,  pp. 1700–1709, 2013. URL http://nal.co/papers/KalchbrennerBlunsom_EMNLP13.  Kalchbrenner, Nal, Danihelka, Ivo, and Graves, Alex. Grid long short-term memory. In International Confer-  ence on Learning Representations, 2016. URL http://arxiv.org/abs/1507.01526.  Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,  2014. URL http://arxiv.org/abs/1412.6980.  Kitzelmann, Emanuel. Inductive programming: A survey of program synthesis techniques. In Approaches and  Applications of Inductive Programming, AAIP 2009, volume 5812 of LNCS, pp. 50–73, 2010.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey. Imagenet classiﬁcation with deep convolutional neural  network. In Advances in Neural Information Processing Systems, 2012.  Lavin, Andrew and Gray, Scott. Fast algorithms for convolutional neural networks. CoRR, abs/1509.09308,  2015. URL http://arxiv.org/abs/1509.09308.  Pham, Vu, Bluche, Th´eodore, Kermorvant, Christopher, and Louradour, J´erˆome. Dropout improves recur- rent neural networks for handwriting recognition. In International Conference on Frontiers in Handwriting Recognition (ICFHR), pp. 285–290. IEEE, 2014. URL http://arxiv.org/pdf/1312.4569.pdf.  Shi, Xingjian, Chen, Zhourong, Wang, Hao, Yeung, Dit-Yan, kin Wong, Wai, and chun Woo, Wang. Convo- lutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in Neural Information Processing Systems, 2015. URL http://arxiv.org/abs/1506.04214.  Srivastava, Rupesh Kumar, Greff, Klaus, and Schmidhuber, J¨urgen.  abs/1505.00387, 2015. URL http://arxiv.org/abs/1505.00387.  Highway networks.  CoRR,  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV.  In Advances in Neural Information Processing Systems, pp. 3104–3112, 2014.  Sequence to sequence learning with neural net- URL  works. http://arxiv.org/abs/1409.3215.  Toderici, George, O’Malley, Sean M., Hwang, Sung Jin, Vincent, Damien, Minnen, David, Baluja, Shumeet, Covell, Michele, and Sukthankar, Rahul. Variable rate image compression with recur- rent neural networks. URL http://arxiv.org/abs/1511.06085.  In International Conference on Learning Representations, 2016.  Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural  Information Processing Systems, 2015. URL http://arxiv.org/abs/1412.7449.  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption  generator. CoRR, abs/1411.4555, 2014. URL http://arxiv.org/abs/1411.4555.  Vivien,  Helene.  URL http://www.liafa.univ-paris-diderot.fr/˜yunes/ca/archives/bookvivien.pdf. Welling, Max and Teh, Yee Whye. Bayesian learning via stochastic gradient Langevin dynamics. In Proceed-  Introduction  automata.  cellular  2003.  An  to  ings of ICML 2011, pp. 681–688, 2011.  Zaremba, Wojciech and Sutskever, Ilya.  Learning to execute. CoRR, abs/1410.4615, 2015a. URL  http://arxiv.org/abs/1410.4615.  Zaremba, Wojciech and Sutskever,  learning neural abs/1505.00521, 2015b. URL http://arxiv.org/abs/1505.00521.  Reinforcement  Ilya.  turing machines.  CoRR,  9  ",
1511.05946,2016,ACDC: A Structured Efficient Linear Layer ,"['ACDC: A Structured Efficient Linear Layer \nMarcin Moczulski', 'Misha Denil', 'Jeremy Appleyard', 'Nando de Freitas']",https://arxiv.org/pdf/1511.05946,"6 1 0 2    r a     M 9 1      ]  G L . s c [      5 v 6 4 9 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  ACDC: A STRUCTURED EFFICIENT LINEAR LAYER  Marcin Moczulski1 Misha Denil1  Jeremy Appleyard2 Nando de Freitas1,3  1University of Oxford 2NVIDIA 3CIFAR marcin.moczulski@stcatz.ox.ac.uk misha.denil@gmail.com jappleyard@nvidia.com nando.de.freitas@cs.ox.ac.uk  ABSTRACT  The linear layer is one of the most pervasive modules in deep learning representa- tions. However, it requires O(N 2) parameters and O(N 2) operations. These costs can be prohibitive in mobile applications or prevent scaling in many domains. Here, we introduce a deep, differentiable, fully-connected neural network module composed of diagonal matrices of parameters, A and D, and the discrete cosine transform C. The core module, structured as ACDC−1, has O(N ) parameters and incurs O(N log N ) operations. We present theoretical results showing how deep cascades of ACDC layers approximate linear layers. ACDC is, however, a stand-alone module and can be used in combination with any other types of mod- ule. In our experiments, we show that it can indeed be successfully interleaved with ReLU modules in convolutional neural networks for image recognition. Our experiments also study critical factors in the training of these structured modules, including initialization and depth. Finally, this paper also points out avenues for implementing the complex version of ACDC using photonic devices.  1  INTRODUCTION  The linear layer is the central building block of nearly all modern neural network models. A notable exception to this is the convolutional layer, which has been extremely successful in computer vision; however, even convolutional networks typically feed into one or more linear layers after processing by convolutions. Other specialized network modules including LSTMs (Hochreiter & Schmidhuber, 1997), GRUs (Cho et al., 2014), the attentional mechanisms used for image captioning (Xu et al., 2015) and machine translation (Bahdanau et al., 2015), reading in Memory Networks (Sukhbaatar et al., 2015), and both reading and writing in Neural Turing Machines (Graves et al., 2015), are all built from compositions of linear layers and nonlinear modules, such as sigmoid, softmax and ReLU layers. The linear layer is essentially a matrix-vector operation, where the input x is scaled with a matrix of parameters W as follows:  (1) When the number of inputs and outputs is N, the number of parameters stored in W is O(N 2). It also takes O(N 2) operations to compute the output y. In spite of the ubiquity and convenience of linear layers, their O(N 2) size is extremely wasteful. Indeed, several studies focusing on feedforward perceptrons and convolutional networks have shown that the parametrisation of linear layers is extremely wasteful, with up to 95% of the parameters being redundant (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013). Given the importance of this research topic, we have witnessed a recent explosion of works introduc- ing structured efﬁcient linear layers (SELLs). We adopt the following notation to describe SELLs within a common framework:  y = xW  Torch implementation of ACDC is available at https://github.com/mdenil/acdc-torch  y = xΦ = xΦ(D, P, S, B)  1  (2)  Published as a conference paper at ICLR 2016  We reserve the capital bold symbol D for diagonal matrices, P for permutations, S for sparse matri- ces, and B ∈ {F, H, C} for bases such as Fourier, Hadamard and Cosine transforms respectively. In this setup, the parameters are typically in the diagonal or sparse entries of the matrices D and S. Sparse matrices aside, the computational cost of most SELLs is O(N log N ), while the number of parameters is reduced from O(N 2) to a mere O(N ). These costs are a consequence of the facts that we only need to store the diagonal matrices, and that the Fourier, Hadamard or Discrete Cosine transforms can be efﬁciently computed in O(N log N ) steps. Often the diagonal and sparse matrices have ﬁxed random entries. When this is the case, we will use tildes to indicate this fact (e.g., ˜D). Our ﬁrst SELL example is the Fast Random Projections method of Ailon & Chazelle (2009):  Φ = ˜DH˜S  (3) Here, the sparse matrix ˜S has Gaussian entries, the diagonal ˜D has {+1,−1} entries drawn inde- pendently with probability 1/2, and H is the Hadamard matrix. The embeddings generated by this SELL preserve metric information with high probability, as formalized by the theory of random projections. Fastfood (Le et al., 2013), our second SELL example, extends fast random projections as follows:  Φ = ˜D1HP ˜D2H ˜D3.  (4) In (Yang et al., 2015), the authors introduce an adaptive variant of Fastfood, with the random diago- nal matrices replaced by diagonal matrices of parameters, and show that it outperforms the random counterpart when applied to the problem of replacing one of the fully connected layers of a convo- lutional neural network for ImageNet (Jia et al., 2014). Interestingly, while the random variant is competitive in simple applications (MNIST), the adaptive variant has a considerable advantage in more demanding applications (ImageNet). The adaptive SELLs, including Adaptive Fastfood and the alternatives discussed subsequently, are end to end differentiable. They require only O(N ) parameters and O(N log N ) operations in both the forward and backward passes of backpropagation. These beneﬁts can be achieved both at train and test time. Cheng et al. (2015) introduced a SELL consisting of the product of a circulant matrix (R) and a random diagonal matrix ( ˜D1). Since circulant matrices can be diagonalized with the discrete Fourier transform (Golub & Van Loan, 1996), this SELL falls within our general notation:  Φ = ˜D1R = ˜D1FD2F−1.  (5) Sindhwani et al. (2015) introduced a Toeplitz-like structured transform, within the framework of displacement operators. Since Toeplitz matrices can be “embedded” in circulant matrices, they can also be diagonalized with the discrete Fourier transform (Golub & Van Loan, 1996). In this work, we introduce a SELL that could be thought of as an adaptive variant of the method of Cheng et al. (2015). In addition, instead of using a (single) shallow SELL as in previous works (Yang et al., 2015; Cheng et al., 2015; Sindhwani et al., 2015), we consider deep SELLs:  Φ =  AkFDkF−1.  (6)  k=1  Here, A is also a diagonal matrix of parameters, but we use a different symbol to emphasize that A scales the signal in the original domain while D scales it in the Fourier domain. While adaptive SELLs perform better than their random counterparts in practice, there is a lack of theory for adaptive SELLs. Moreover, the empirical studies of recent adaptive SELLs have many de- ﬁciencies. For instance, it is often not clear how performance varies depending on implementation, and many critical details such as initialization and the treatment of biases are typically obviated. In addition, the gains are often demonstrated in models of different size, making objective comparison very difﬁcult. In addition to demonstrating good performance replacing the fully connected layers of CaffeNet, we present a theoretical approximation guarantee for our deep SELL in Section 3. We also discuss  2  K(cid:89)  Published as a conference paper at ICLR 2016  Figure 1: Example of a 4f system. This system implements the multiplication of an optical signal by a circulant matrix FDF−1. The lenses apply Fourier transforms to the signal and the diffraction element applies the diagonal multiplication.  the crucial issue of implementing deep SELLs efﬁciently in modern GPU architectures in Section 5 and release this software with this paper. This engineering contribution is important as many of the recently proposed methods for accelerating linear layers often fail to take into account the attributes and limitations of GPUs, and hence fail to be adopted.  1.1 LIGHTNING FAST DEEP SELL  Our deep SELL (equation (6)) offers several possibilities for analog physical implementation. Given the great demand for fast low energy neural networks, the possibility of harnessing physical phenom- ena to perform efﬁcient computation in deep networks is worthy of consideration. In the Fourier optics ﬁeld, it is well known that the two-dimensional Fourier transform can be im- plemented with a paraxial optical system consiting of a lens of focal length f in free space. In this setup, known as a 2f system, a waveform in the frontal focal plane of the lens, viewed as a two-dimensional complex array, is transformed to another one in the focal plane behind the lens that corresponds to the Fourier transform of the array. A 4f system is obtained by placing a diffractive element in between two 2f systems at a distance f from each (shown in Figure 1). Every circulant matrix R = FDF−1 can be realized optically using a 4f system, with the transfor- mation by the diffractive optical device corresponding to the multiplication by the complex diagonal matrix D (Reif & Tyagi, 1997; M¨uller-Quade et al., 1998; Huhtanen, 2008; Schmid et al., 2000). Moreover, paraxial diffractive optical systems with consecutive products of circulant and diagonal matrices can factor a complex matrix into products of diagonal and circulant matrices (M¨uller-Quade et al., 1998; Huhtanen & Per¨am¨aki, 2015). Hence, in principle the mapping of equation (6) can be implemented with optical elements. In a separate research community, Hermans & Vaerenbergh (2015) recently discussed using waves in a trainable medium for learning linear layers by backpropagation, and suggested a potential im- plementation using an integrated photonics chip. The nanophotonic chip consists of a cascade of unitary trasformations of the optical signals interleaved with tuneable waveguides (phase shifters). Hermans & Vaerenbergh (2015) present an abstraction of this chip. In particular, if we let o and o(cid:48) represent the optical ﬁelds at the input and output waveguides, the chip implements the following transformation:  DkUko  (7)  k=1  where Uk is a unitary transformation of the signal and Dk is a diagonal matrix Dk = diag(exp(jϕk)) with tuneable phase shifts ϕk. By restricting the diagonal matrices in equation (6) to be of this complex form, the circulant R = FDF−1 is unitary and we obtain an equivalence be- tween equations (6) and (7). This points to a potential nanophotonic implementation of our complex deep SELL. More recently, Saade et al. (2015) disclosed an invention that peforms optical analog random pro- jections.  3  K(cid:89)  o(cid:48)  =  ffffInputLensDiﬀractionLensOutputPublished as a conference paper at ICLR 2016  2 FURTHER RELATED WORKS  The literature on this topic is vast, and consequently this section only aims to capture some of the signiﬁcant trends. We refer readers to the related work sections of the papers cited in the previous and present section for further details. As mentioned earlier, many studies have shown that the parametrisation of linear layers is extremely wasteful (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013). In spite of this redundancy, there has been little success in improving the linear layer, since natural extensions, such as low rank factorizations, lead to poor performance when trained end to end. For instance, Sainath et al. (2013) demonstrate signiﬁcant improvements in reducing the number of parameters of the output softmax layers, but only modest improvements for the hidden linear layers. Several methods based on low-rank decomposition and sparseness have been proposed to eliminate parameter redundancy at test time, but they provide only a partial solution as the full network must be instantiated during training (Collins & Kohli, 2014; Xue et al., 2013; Blundell et al., 2015; Liu et al., 2015; Han et al., 2015b). That is, these approaches require training the original full model. Hashing techniques have been proposed to reduce the number of parameters (Chen et al., 2015; Bakhtiary et al., 2015). Hashes have irregular memory access patterns and, consequently, good performance on large GPU-based platforms is an open problem. Distillation (Hinton et al., 2015; Romero et al., 2015) also offers a way of compressing neural networks, as a post-processing step. Novikov et al. (2015) use a multi-linear transform (Tensor-Train decomposition) to attain signiﬁcant reductions in the number of parameters in some of the linear layers of convolutional networks.  3 DEEP SELL We deﬁne a single component of deep SELL as AFDF(x) = xAFDF−1, where F is the Fourier transform and A, D are complex diagonal matrices. It is straightforward to see that the AFDF transform is not sufﬁcient to express an arbitrary linear operator W ∈ Cn×n. An AFDF transform has 2n degrees of freedom, whereas an arbitrary linear operator has n2 degrees of freedom. To this end, we turn our attention to studying compositions of AFDF transforms. By composing AFDF transforms we can boost the number of degrees of freedom, and we might expect that any linear operator could be constructed as a composition of sufﬁciently many AFDF transforms. In the following we show that this is indeed possible, and that a bounded number of AFDF transforms is sufﬁcient. Deﬁnition 1. The order-K AFDF transformation is the composition of K consecutive AFDF oper- ations with (optionally) different A and D matrices. We write an order-K complex AFDF transfor- mation as follows  y = AFDFK(x) = x  AkFDkF−1  .  (8)  (cid:34) K(cid:89)  k=1  (cid:35)  We also assume, without loss of generality, that A1 = I so that AFDF1(x) = xFD1F−1. For the analysis it will be convenient to rewrite the AFDF transformation in a different way, which we refer to as the optical presentation. Deﬁnition 2. If y = AFDFK(x) then we deﬁne the optical presentation of an order-K AFDF transform as  (cid:34)K−1(cid:89)  (cid:35)  ˆy = ˆx  DkRk+1  DK  where ˆx and ˆy are the Fourier transforms of x and y, and Rk+1 = F−1Ak+1F. Remark 3. The matrix R = F−1AF is circulant. This follows from the duality between convolution in the spatial domain and pointwise multiplication in the Fourier domain.  k=1  The optical presentation shows how the spectrum of x is related to the spectrum of y. Importantly, it shows that we can express an order-K AFDF transform as a linear operator in Fourier space that  4  Published as a conference paper at ICLR 2016  is composed of a product of circulant and diagonal matrices. Transformations of this type are well studied in the Fourier optics literature, as they can be realized with cascades of lenses. Of particular relevance to us is the main result of Huhtanen & Per¨am¨aki (2015) which states that almost all (in the Lebesgue sense) matrices M ∈ CN×N can be factored as  (cid:34)N−1(cid:89)  (cid:35)  M =  D2i−1R2i  D2N−1  i=1  where D2j−1 is diagonal and R2j is circulant. This factorization corresponds exactly to the optical presentation of an order-N AFDF transform, therefore we conclude the following: Theorem 4. An order-N AFDF transform is sufﬁcient to approximate any linear operator in CN×N to arbitrary precision.  Proof. Every AFDF transform has an optical presentation, and by the main result of Huhtanen & Per¨am¨aki (2015) operators of this type are dense in CN×N .  4 ACDC: A PRACTICAL DEEP SELL  Thus far we have focused on a complex SELL, where theoretical guarantees can be obtained. In practice we ﬁnd it useful to consider instead a real SELL. The real version of AFDFK, denoted ACDCK has the same form as Equation (8), with complex diagonals replaced with real diagonals, and Fourier transforms replaced with Cosine Transforms. This change departs from the theory of Section 3; however, our experiments show that this does not appear to be a problem in practice. The reasons for considering ACDC over AFDF are purely practical.  1. Most existing deep learning frameworks support only real numbers, and thus working with real valued transformations simpliﬁes the interface between our SELL and the rest of the network.  2. Working with complex numbers effectively doubles the memory footprint of of the trans-  form itself, and more importantly, of the activations that interact with it.  The importance of the second point should not be underestimated, since the computational complex- ity of our SELL is quite low, a typical GPU implementation will be bottlenecked by the overhead of moving data through the GPU memory hierarchy. Reducing the amount of data to be moved allows for a signiﬁcantly faster implementation. We discuss these concerns in more detail in Section 5. In this work, we use the DCT (type II) matrix with entries  (cid:20)  (cid:114)  2 N  (cid:18) π(2n + 1)k  (cid:19)(cid:21)  2N  cnk =  (cid:15)k cos √ for n, k = 0, 1, . . . , N, and where (cid:15)k = 1/ 2 for k = 0 or k = N and (cid:15)k = 1 otherwise. DCT matrices are real and orthogonal: C−1 = CT . Moreover, the DCTs are separable transforms. That is, the DCT of a multi-dimensional signal can be decomposed in terms of successive DCTs of the appropriate one-dimensional components of the signal. The DCT can be computed efﬁciently using the Fast Fourier Transform (FFT) algorithm (or the specialized fast cosine transform). Denoting h1 = xiA, h2 = h1C, h3 = h2D, yi = h3C−1, and A = diag(a), D = diag(d) we have the following derivatives in the backward pass:  (9)  5  Published as a conference paper at ICLR 2016  ∂L ∂d  =  ∂yi ∂d  ∂L ∂yi  =  ∂h2D  ∂h3C−1  ∂d  ∂h3  ∂L ∂yi  = diag(h2)C  ∂L ∂yi  ∂L ∂a  =  ∂yi ∂a  ∂L ∂yi  =  ∂xiA  ∂a  ∂h1C ∂h1  ∂h2D ∂h2  ∂L ∂h3  ∂L ∂xi  =  ∂yi ∂xi  ∂L ∂yi  =  ∂xiA ∂xi  ∂L ∂h1  5 EFFICIENT IMPLEMENTATION OF ACDC  = h2 (cid:12) C  ∂L ∂D  = diag( = xi (cid:12) C−1d (cid:12) C  ∂L ∂A  = diag( = a (cid:12) C−1d (cid:12) C  ∂L ∂yi ∂L ∂d ∂L ∂yi ∂L ∂a ∂L ∂yi  )  )  (10)  (11)  (12)  (13)  (14)  The processor used to benchmark the ACDC layer was an NVIDIA Titan X. The peak ﬂoating point throughput of the Titan X is 6605 GFLOPs, and the peak memory bandwidth is 336.5GB/s1. This gives an arithmetic intensity (FLOPs per byte) of approximately 20. In the ideal case, where there is enough parallelism for the GPU to hide all latencies, an algorithm with a higher arithmetic intensity than this would be expected to be ﬂoating point throughput bound, while an algorithm with lower arithmetic intensity would be expected to be memory throughput bound. The forward pass of a single example through a size-N ACDC layer when calculated using 32-bit ﬂoating point arithmetic requires at least 24N bytes to be moved to and from main memory. Eight bytes per element for each of A and D, four bytes per element for the input, and four bytes per element for the output. It also requires approximately 4N + 5N log2(N ) ﬂoating point operations2. When batching, the memory transfers for A and D are expected to be cached as they are reused for each example in the batch, so for the purposes of calculating arithmetic intensity in the batched case it is reasonable to discount them. The arithmetic intensity of a minibatch passing through an ACDC layer is therefore approximately:  AI = (4 + 5 log2(N ))/8  For the values of N we are interested in (128 −→ 16, 384) this arithmetic intensity varies between 4.9 and 9.3, indicating that the peak performance of a large ACDC layer with a large batch size is ex- pected to be limited by the peak memory throughput of the GPU (336.5GB/s), and that optimization of an ACDC implementation should concentrate on removing any extraneous memory operations. Two versions of ACDC have been implemented. One performs the ACDC in a single call, with the minimum of 8N bytes moved per layer (assuming perfect caching of A and D). The other performs ACDC with multiple calls, with signiﬁcantly more than 8N bytes moved per layer.  5.1 SINGLE CALL IMPLEMENTATION  To minimize trafﬁc to and from main memory intermediate loads or stores during the layer must be eliminated. To accomplish this kernel fusion is used to fuse all of the operations of ACDC into a single call, with intermediate values being stored in temporary low-level memory instead of main memory. This presents two challenges to the implementation. Firstly, the size of the ACDC layer is limited by the availability of temporary memory on the GPU. This limits the size of the ACDC layer that can be calculated. It also has performance implications: the temporary memory used to store intermediate values in the computation is shared with the regis- ters required for basic calculation, such as loop indices. The more of this space that is used by data, the fewer threads can ﬁt on the GPU at once, limiting parallelism.  1http://www.geforce.co.uk/hardware/desktop-gpus/geforce-gtx-titan-x/ specifications 2http://www.fftw.org/speed/method.html  6  Published as a conference paper at ICLR 2016  Figure 2: Performance comparison of theoretical and actual performance of our ACDC implemen- tations to an ordinary dense linear layer using a batch size of 128. Peak curves show maximum theoretical performance achievable by the hardware.  Secondly, the DCT and IDCT layers must be written by hand so that they can be efﬁciently fused with the linear layers. Implementations of DCT and IDCT are non-trivial, and a generic imple- mentation able to handle any input size would be a large project in itself. For this reason, the implementation is constrained to power-of-two and multiples of large power-of-two layer sizes.  5.2 MULTIPLE CALL IMPLEMENTATION  While expected to be less efﬁcient a multiple call implementation is both much simpler program- matically, and much more generically usable. Using the method of Makhoul (1980) it is possible to perform size-N DCTs and IDCTs using size-N FFTs. As such, the NVIDIA library cuFFT can be used to greatly simplify the code required, as well as achieve reasonable performance across a wide range of ACDC sizes. The procedure is as follows:  1. Multiply input by A and set up C1 2. Perform C1 using a C2C cuFFT call 3. Finalize C1, multiply by D and setup C2 4. Perform C2 using a C2C cuFFT call 5. Finalize C2  The total memory moved for this implementation is signiﬁcantly higher as each call requires a load and a store for each element. The performance trade-off with the single call method is therefore one of parallelism against memory trafﬁc.  5.3 PERFORMANCE COMPARISON  Figure 2 compares the speed of the single and multiple call implementations of ACDC against dense matrix-matrix multiplication for a variety of layer sizes. It is clear that in both the forward and backward pass ACDC layers have a signiﬁcantly lower runtime than fully connected layers using dense matrices. Even if the matrix-matrix operations were running at peak, ACDC still would outperform them by up to 10 times. As expected, the single call version of ACDC outperforms the multiple call version, although for smaller layer sizes the gap is larger. When the layer size increases the multiple call version suffers signiﬁcantly more from small per-call overheads. Both single and multiple call versions of ACDC perform signiﬁcantly worse on non power-of-two layer sizes. This is because they rely on FFT operations, which are known to be more efﬁcient when the input sizes are of lengths zn, where z is a small integer3.  3http://docs.nvidia.com/cuda/cufft/#accuracy-and-performance  7  2565121024204840968192layer size101102103104time (ns)forward2565121024204840968192layer size101102103104time (ns)backwardpeak_gemmgemmpeak_acdcsingle_callmultiple_callPublished as a conference paper at ICLR 2016  While the backward pass of ACDC is expected to take approximately the same time as the forward pass, it takes noticeably longer. To compute the parameter gradients one needs the input into the D operation and the gradient of the output from the A operation. As the aim of the layer is to reduce memory footprint it was decided instead to recompute these during the backward pass, increasing runtime while saving memory.  6 EXPERIMENTS  6.1 LINEAR LAYERS  In this section we show that we are able to approximate linear operators using ACDC as predicted by the theory of Section 3. These experiments serve two purposes  1. They show that recovery of a dense linear operator by SGD is feasible in practice. The theory of Section 3 guarantees only that it is possible to approximate any operator, but does not provide guidance on how to ﬁnd this approximation. Additionally, Huhtanen & Per¨am¨aki (2015) suggest that this is a difﬁcult problem.  2. They validate empirically that our decision to focus on ACDC over the complex AFDF does not introduce obvious difﬁculties into the approximation. The theory provides guar- antees only for the complex case, and the experiments in this section suggest that restricting ourselves to real matrices is not a problem.  We investigate using ACDC on a synthetic linear regression problem  Y = XWtrue + (cid:15)(cid:15)(cid:15),  (15) where X of size 10, 000 × 32 and Wtrue of size 32 × 32 are both constructed by sampling their entries uniformly at random in the unit interval. Gaussian noise (cid:15)(cid:15)(cid:15) ∼ N (0, 10−4) is added to the generated targets. The results of approximating the operator Wtrue using ACDCK for different values of K are shown in Figure 3. The theory of Section 3 predicts that, in the complex case, for a 32× 32 matrix it should be sufﬁcient to have 32 layers of ACDC to express an arbitrary Wtrue. We found that initialization of the matrices A and D to identity I, with Gaussian noise N (0, 10−2) added the diagonals in order to break symmetries, is essential for models having many ACDC layers. (We found the initialization to be robust to the speciﬁcation of the noise added to the diagonals.) The need for thoughtful initialization is very clear in Figure 3. With the right initialization (leftmost plot), the approximation results of Section 3 are conﬁrmed, with improved accuracy as we increase the number of ACDC layers. However, if we use standard strategies for initializing linear layers (rightmost plot), we observe very poor optimization results as the number of ACDC layers increases. This experiment suggests that fewer layers sufﬁce to arrive at a reasonable approximation of the original Wtrue than what the theory guarantees. With neural networks in mind this is a very relevant observation. It is well known that the linear layers of neural networks are compressible, indicating that we do not need to express an arbitrary linear operator in order to achieve good performance. Instead, we need only express a sufﬁciently interesting subset of matrices, and the result with 16 ACDC layers points to this being the case. In Section 6.2 we show that by interspersing nonlinearities between ACDC layers in a convolutional network it is possible to use dramatically fewer ACDC layers than the theory suggests are needed while still achieving good performance.  6.2 CONVOLUTIONAL NETWORKS  In this section we investigate replacing the fully connected layers of a deep convolutional network with a cascade of ACDC layers. In particular we use the CaffeNet architecture4 for ImageNet (Deng et al., 2009). We target the two fully connected layers located between features extracted from  4https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet  8  Published as a conference paper at ICLR 2016  Figure 3: Training loss for different number of ACDC layers compared to loss for the dense matrix. Left: Initialization: N (1, σ2) with σ = 10−1. Right: Initialization: N (0, σ2) with σ = 10−3. Note the difference in scale on the y-axis.  Test Time Post-Processing Collins & Kohli (2014) Han et al. (2015b) Han et al. (2015a) (P+Q)  Top-1 Err Increase 1.81% 0.00% 0.00%  # of Param Reduction x4.0 x9 x27  15.2M 6.7M ∼2.3M  Train and Test Time Reduction Cheng et al. (2015) (Circulant CNN 2) ∗Novikov et al. (2015) (TT4 FC FC) ∗Novikov et al. (2015) (TT4 TT4 FC) Yang et al. (2015) (Finetuned SVD 1) Yang et al. (2015) (Finetuned SVD 2) Yang et al. (2015) (Adaptive Fastfood 16) ACDC CaffeNet Reference Model  0.40% 0.30% 1.30% 0.14% 1.22% 0.30% 0.67% 0.00%  > 16.3M - - 46.6M 23.4M 16.4M 9.7M 58.7M  < x3.8 x3.9 x7.4 x1.3 x2.0 x3.6 x6.0 x1.0  Table 1: Comparison of SELL with alternative factorization methods achieving marginal perfor- mance drop on the ImageNet dataset. Entries in italics incur an increase in top-1 error of >1.0%. Entries marked with a star use VGG16, which makes them not directly comparable to our own. Pre- vious works have shown that it is typically possible to achieve ∼ 30% greater compression factors on VGG16 than on AlexNet-style architectures (Han et al., 2015a;b).  the last convolutional layer and the ﬁnal logistic regression layer, which we replace with 12 stacked ACDC transforms interleaved with ReLU non-linearities and permutations. The permutations assure that adjacent SELLs are incoherent. The model was trained using the SGD algorithm with learning rate 0.1 multiplied by 0.1 every 100,000 iterations, momentum 0.65 and weight decay 0.0005. The output from the last convolu- tional layer was scaled by 0.1, and the learning rates for each matrix A and D were multiplied by 24 and 12. All diagonal matrices were initialized from N (1, 0.061) distribution. No weight decay was applied to A or D. Additive biases were added to the matrices D, but not to A, as this sufﬁced to provide the ACDC layer with a bias terms just before the ReLU non-linearities. Biases were initialized to 0. To prevent the model from overﬁtting dropout regularization was placed before each of the last 5 SELL layers with dropout probability equal to 0.1. The resulting model arrives at 43.26% error which is only 0.67% worse when compared to the reference model, so SELL conﬁdently stays within 1% of the performance of the original network. We report this result, as well as a comparison to several other works in Table 1. The two fully connected layers of CaffeNet, consisting of more than 41 million parameters, are replaced with SELL modules which contain a combined 165, 888 parameters. These results agree with the hypothesis that neural networks are over-parameterized formulated by Denil et al. (2013) and supported by Yang et al. (2015). At the same time such a tremendous reduction without sig- niﬁcant loss of accuracy suggests that SELL is a powerful concept and a way to use parameters efﬁciently.  9  0200040006000800010000Iterations0.01.53.0Train Loss1 ACDC2 ACDC4 ACDC8 ACDC16 ACDC32 ACDC64 ACDCDense0200040006000800010000Iterations024681012Train Loss1 ACDC2 ACDC4 ACDC8 ACDC16 ACDC32 ACDC64 ACDCPublished as a conference paper at ICLR 2016  Figure 4: Visual comparison of the tradeoff between parameter and accuracy reduction for train time applicable SELLs. Red entries (marked with a star in the labels) use VGG16, which makes them not directly comparable to the others, as discussed in the caption of Table 1.  This approach is an improvement over Deep Fried Convnets (Yang et al., 2015) and other Fast- Food (Le et al., 2013) based transforms in the sense that the layers remain narrow and become deep (potentially interleaved with non-linearites) as opposed to wide and shallow, while maintaining com- parable or better performance. The result of narrower layers is that the ﬁnal softmax classiﬁcation layer requires substantially fewer parameters, meaning that the resulting compression ratio is higher. Our experiment shows that ACDC transforms are an attractive building block for feedforward con- volutional architectures, that can be used as a structured alternative to fully connected layers, while ﬁtting very well into the deep learning philosophy of introducing transformations executed in steps as the signal is propagated down the network rather than projecting to higher-dimensional spaces. It should be noted that the method of pruning proposed in (Han et al., 2015b) and the follow-up method of pruning, quantizing and Huffman coding proposed in (Han et al., 2015a) achieve com- pression rates between x9 and x27 on AlexNet5 by applying a pipeline of reducing operations on a trained models. Usually it is necessary to perform at least a few iterations of such reductions to arrive at the stated compression rates. For the AlexNet model one such iteration takes 173 hours according to (Han et al., 2015b). On top of that as this method requires training the original full model the time cost of that operation should be taken into consideration as well. Compressing pipelines target models that are ready for deployment and function in the environ- ment where amount of time spent on training is absolutely dominated by the time spent evaluating predictions. In contrast, SELL methods are appropriate for incorporation into the design of a model.  7 CONCLUSION  We introduced a new Structured Efﬁcient Linear Layer, which adds to a growing literature on using memory efﬁcient structured transforms as efﬁcient replacements for the dense matrices in the fully connected layers of neural networks. The structure of our SELL is motivated by matrix approxima- tion results from Fourier optics, but has been specialized for efﬁcient implementation on NVIDIA GPUs. We have shown that proper initialization of our SELL allows us to build very deep cascades of SELLs that can be optimized using SGD. Proper initialization is simple, but is essential for training cascades of SELLs with more than a few layers. Working with deep and narrow cascades of SELLs  5Han et al. (2015a) report x35 compression by using Huffman coding and counting bytes. We report the number of parameters here for consistency.  10  0.00.20.40.60.81.01.21.4Top-1 Error Increase12345678ReductionReference ModelCheng (Circulant CNN 2)*Novikov (TT4 FC FC)*Novikov (TT4 TT4 FC)Yang (Fine SVD 1)Yang (Fine SVD 2)Yang (Adaptive Fastfood 16)ACDCPublished as a conference paper at ICLR 2016  makes our networks more parameter efﬁcient than previous works using shallow and wide cascades because the cost of layers interfacing between the SELL and the rest of the network is reduced (e.g. the size of the input to the dense logistic regression layer of the network is much smaller). In future work we plan to investigate replacing the diagonal layers of ACDC with other efﬁcient structured matrices such as band or block diagonals. These alternatives introduce additional param- eters in each layer, but may give us the opportunity to explore the continuum between depth and expressive power per layer more precisely. Another interesting avenue of investigation is to include SELL layers in other neural network mod- els such as RNNs or LSTMs. Recurrent nets are a particularly attractive targets as they are typi- cally composed entirely of linear layers. This means that the potential parameter savings are quite substantial, and since the computational bottleneck is in these models comes from matrix-matrix multiplications there is a potential speed advantage as well.  REFERENCES Ailon, Nir and Chazelle, Bernard. The Fast Johnson Lindenstrauss Transform and approximate nearest neigh-  bors. SIAM Journal on Computing, 39(1):302–322, 2009.  Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to  align and translate. In International Conference on Learning Representations, 2015.  Bakhtiary, Amir H., Lapedriza, `Agata, and Masip, David. Speeding up neural networks for large scale classiﬁ-  cation using WTA hashing. arXiv preprint arXiv:1504.07488, 2015.  Blundell, Charles, Cornebise, Julien, Kavukcuoglu, Koray, and Wierstra, Daan. Weight uncertainty in neural  networks. In ICML, 2015.  Chen, Wenlin, Wilson, James T., Tyree, Stephen, Weinberger, Kilian Q., and Chen, Yixin. Compressing neural  networks with the hashing trick. In ICML, 2015.  Cheng, Yu, Yu, Felix X, Feris, R, Kumar, Sanjiv, Choudhary, Alok, and Chang, Shih-Fu. An exploration of  parameter redundancy in deep networks with circulant projections. In ICCV, 2015.  Cho, Kyunghyun, Van Merri¨enboer, Bart, Gulcehre, Caglar, Bahdanau, Dzmitry, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using rnn encoder-decoder for statistical ma- chine translation. In Empiricial Methods in Natural Language Processing, 2014.  Collins, Maxwell D. and Kohli, Pushmeet. Memory bounded deep convolutional networks. Technical report,  University of Wisconsin-Madison, 2014.  Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: A Large-Scale Hierarchical Image  Database. In CVPR, 2009.  Denil, Misha, Shakibi, Babak, Dinh, Laurent, Ranzato, Marc’Aurelio, and de Freitas, Nando. Predicting  parameters in deep learning. In NIPS, pp. 2148–2156, 2013.  Golub, Gene H. and Van Loan, Charles F. Matrix Computations. Johns Hopkins University Press, 1996.  Gong, Yunchao, Liu, Liu, Yang, Ming, and Bourdev, Lubomir. Compressing deep convolutional networks  using vector quantization. arXiv preprint arXiv:1412.6115, 2014.  Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural Turing machines. Technical report, Google DeepMind,  2015.  Han, Song, Mao, Huizi, and Dally, William J. Deep compression: Compressing deep neural network with  pruning, trained quantization and Huffman coding. arXiv preprint arXiv:1510.00149, 2015a.  Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efﬁcient  neural networks. In NIPS, 2015b.  Hermans, Michiel and Vaerenbergh, Thomas Van. Towards trainable media: Using waves for neural network-  style training. arXiv preprint arXiv:1510.03776, 2015.  Hinton, Geoffrey E., Vinyals, Oriol, and Dean, Jeffrey. Distilling the knowledge in a neural network. arXiv  preprint arXiv:1503.02531, 2015.  11  Published as a conference paper at ICLR 2016  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):1735–1780,  1997.  Huhtanen, Marko. Approximating ideal diffractive optical systems. Journal of Mathematical Analysis and  Applications, 345:53–62, 2008.  Huhtanen, Marko and Per¨am¨aki, Allan. Factoring matrices into the product of circulant and diagonal matrices.  Journal of Fourier Analysis and Applications, 2015.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.  Le, Quoc, Sarl´os, Tam´as, and Smola, Alex. Fastfood – approximating kernel expansions in loglinear time. In  ICML, 2013.  Liu, Baoyuan, Wang, Min, Foroosh, Hassan, Tappen, Marshall, and Pensky, Marianna. Sparse convolutional  neural networks. In CVPR, 2015.  Makhoul, John. A fast cosine transform in one and two dimensions. IEEE Transactions on Acoustics, Speech  and Signal Processing, 28(1):27–34, 1980.  M¨uller-Quade, J¨orn, Aagedal, Harald, Beth, Th, and Schmid, Michael. Algorithmic design of diffractive optical  systems for information processing. Physica D: Nonlinear Phenomena, 120(1):196–205, 1998.  Novikov, Alexander, Podoprikhin, Dmitry, Osokin, Anton, and Vetrov, Dmitry. Tensorizing neural networks.  In NIPS, 2015.  Reif, John and Tyagi, Akhilesh. Efﬁcient parallel algorithms for optical computing with the DFT primitive.  Applied Optics, 36(29):7327–7340, 1997.  Romero, Adriana, Ballas, Nicolas, Kahou, Samira Ebrahimi, Chassang, Antoine, Gatta, Carlo, and Bengio,  Yoshua. FitNets: Hints for thin deep nets. In ICLR, 2015.  Saade, Alaa, Caltagirone, Francesco, Carron, Igor, Daudet, Laurent, Dremeau, Angelique, Gigan, Sylvain, and Krzakala, Florent. Random projections through multiple optical scattering: Approximating kernels at the speed of light. arXiv preprint arXiv:1510.06664, 2015.  Sainath, Tara N., Kingsbury, Brian, Sindhwani, Vikas, Arisoy, Ebru, and Ramabhadran, Bhuvana. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In ICASSP, pp. 6655–6659, 2013.  Schmid, Michael, Steinwandt, Rainer, Mller-Quade, Jrn, Rtteler, Martin, and Beth, Thomas. Decomposing a  matrix into circulant and diagonal factors. Linear Algebra and its Applications, 306(1–3):131–143, 2000.  Sindhwani, Vikas, Sainath, Tara N, and Kumar, Sanjiv. Structured transforms for small-footprint deep learning.  In NIPS, 2015.  Sukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and Fergus, Rob. End-to-end memory networks.  NIPS, 2015.  In  Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun, Courville, Aaron, Salakhutdinov, Ruslan, Zemel, Richard, and Bengio, Yoshua. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.  Xue, Jian, Li, Jinyu, and Gong, Yifan. Restructuring of deep neural network acoustic models with singular  value decomposition. In INTERSPEECH, pp. 2365–2369, 2013.  Yang, Zichao, Moczulski, Marcin, Denil, Misha, de Freitas, Nando, Smola, Alex, Song, Le, and Wang, Ziyu.  Deep fried convnets. In ICCV, 2015.  12  ",
1511.05122,2016,Adversarial Manipulation of Deep Representations,"['Adversarial Manipulation of Deep Representations [code]\nSara Sabour', 'Yanshuai Cao', 'Fartash Faghri', 'David Fleet']",https://arxiv.org/pdf/1511.05122,"6 1 0 2    r a  M 4         ]  V C . s c [      9 v 2 2 1 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  ADVERSARIAL MANIPULATION OF DEEP REPRESENTATIONS  Sara Sabour ∗1, Yanshuai Cao∗ 1,2, Fartash Faghri1,2 & David J. Fleet1 1 Department of Computer Science, University of Toronto, Canada 2 Architech Labs, Toronto, Canada {saaraa,caoy,faghri,fleet}@cs.toronto.edu  ABSTRACT  We show that the image representations in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, impercep- tible perturbations to the original image. Previous methods for generating adver- sarial images focused on image perturbations designed to produce erroneous class labels. Here we instead concentrate on the internal layers of DNN representations, to produce a new class of adversarial images that differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representa- tion appears remarkably similar to a different image, from a different class and bearing little if any apparent similarity to the input. Further, they appear generic and consistent with the space of natural images. This phenomenon demonstrates the possibility to trick a DNN to confound almost any image with any other chosen image, and raises questions about DNN representations, as well as the properties of natural images themselves.  1  INTRODUCTION  Recent papers have shown that deep neural networks (DNNs) for image classiﬁcation can be fooled, often using relatively simple methods to generate so-called adversarial images (Fawzi et al., 2015; Goodfellow et al., 2014; Gu & Rigazio, 2014; Nguyen et al., 2015; Szegedy et al., 2014; Tabacof & Valle, 2015). The existence of adversarial images is important, not just because they reveal weaknesses in learned representations and classiﬁers, but because 1) they provide opportunities to explore fundamental questions about the nature of DNNs, e.g., whether they are inherent in the network structure per se or in the learned models, and 2) such adversarial images might be harnessed to improve learning algorithms that yield better generalization and robustness (Goodfellow et al., 2014; Gu & Rigazio, 2014). Research on adversarial images to date has focused mainly on disrupting classiﬁcation, i.e., on algorithms that produce images classiﬁed with labels that are patently inconsistent with human per- ception. Given the large, potentially unbounded regions of feature space associated with a given class label, it may not be surprising that it is easy to disrupt classiﬁcation. In this paper, in constrast to such label adversaries, we consider a new, somewhat more incidious class of adversarial images, called feature adversaries, which are confused with other images not just in the class label, but in their internal representations as well. Given a source image, a target (guide) image, and a trained DNN, we ﬁnd small perturbations to the source image that produce an internal representation that is remarkably similar to that of the guide image, and hence far from that of the source. With this new class of adversarial phenomena we demonstrate that it is possible to fool a DNN to confound almost any image with any other chosen image. We further show that the deep representations of such adversarial images are not outliers per se. Rather, they appear generic, indistinguishable from representations of natural images at multiple layers of a DNN. This phenomena raises questions about DNN representations, as well as the properties of natural images themselves.  ∗The ﬁrst two authors contributed equally.  1  Published as a conference paper at ICLR 2016  2 RELATED WORK  Several methods for generating adversarial images have appeared in recent years. Nguyen et al. (2015) describe an evolutionary algorithm to generate images comprising 2D patterns that are clas- siﬁed by DNNs as common objects with high conﬁdence (often 99%). While interesting, such adversarial images are quite different from the natural images used as training data. Because natu- ral images only occupy a small volume of the space of all possible images, it is not surprising that discriminative DNNs trained on natural images have trouble coping with such out-of-sample data. Szegedy et al. (2014) focused on adversarial images that appear natural. They used gradient-based optimization on the classiﬁcation loss, with respect to the image perturbation, (cid:15). The magnitude of the perturbation is penalized ensure that the perturbation is not perceptually salient. Given an image I, a DNN classiﬁer f, and an erroneous label (cid:96), they ﬁnd the perturbation (cid:15) that minimizes loss(f (I + (cid:15)), (cid:96)) + c(cid:107)(cid:15)(cid:107)2. Here, c is chosen by line-search to ﬁnd the smallest (cid:15) that achieves f (I + (cid:15)) = (cid:96). The authors argue that the resulting adversarial images occupy low probability “pockets” in the manifold, acting like “blind spots” to the DNN. The adversarial construction in our paper extends the approach of Szegedy et al. (2014). In Sec. 3, we use gradient-based optimization to ﬁnd small image perturbations. But instead of inducing misclassiﬁcation, we induce dramatic changes in the internal DNN representation. Later work by Goodfellow et al. (2014) showed that adversarial images are more common, and can be found by taking steps in the direction of the gradient of loss(f (I + (cid:15)), (cid:96)). Goodfellow et al. (2014) also show that adversarial examples exist for other models, including linear classiﬁers. They argue that the problem arises when models are “too linear”. Fawzi et al. (2015) later propose a more general framework to explain adversarial images, formalizing the intuition that the problem occurs when DNNs and other models are not sufﬁciently “ﬂexible” for the given classiﬁcation task. In Sec. 4, we show that our new category of adversarial images exhibits qualitatively different prop- erties from those above. In particular, the DNN representations of our adversarial images are very similar to those of natural images. They do not appear unnatural in any obvious way, except for the fact that they remain inconsistent with human perception.  3 ADVERSARIAL IMAGE GENERATION  Let Is and Ig denote the source and guide images. Let φk be the mapping from an image to its internal DNN representation at layer k. Our goal is to ﬁnd a new image, Iα, such that the Euclidian distance between φk(Iα) and φk(Ig) is as small as possible, while Iα remains close to the source Is. More precisely, Iα is deﬁned to be the solution to a constrained optimization problem:  Iα = arg min  (cid:107) φk(I) − φk(Ig)(cid:107)2 subject to (cid:107)I − Is(cid:107)∞ < δ  2  I  (2) The constraint on the distance between Iα and Is is formulated in terms of the L∞ norm to limit the maximum deviation of any single pixel color to δ. The goal is to constrain the degree to which the perturbation is perceptible. While the L∞ norm is not the best available measure of human visual discriminability (e.g., compared to SSIM (Wang et al., 2004)), it is superior to the L2 norm often used by others. Rather than optimizing δ for each image, we ﬁnd that a ﬁxed value of δ = 10 (out of 255) pro- duces compelling adversarial images with negligible perceptual distortion. Further, it works well with different intermediate layers, different networks and most images. We only set δ larger when optimizing lower layers, close to the input (e.g., see Fig. 5). As δ increases distortion becomes per- ceptible, but there is little or no perceptible trace of the guide image in the distortion. For numerical optimization, we use l-BFGS-b, with the inequality (2) expressed as a box constraint around Is. Figure 1 shows nine adversarial images generated in this way, all using the well-known BVLC Caffe Reference model (Caffenet) (Jia et al., 2014). Each row in Fig. 1 shows a source, a guide, and three adversarial images along with their differences from the corresponding source. The adversarial examples were optimized with different perturbation bounds (δ), and using different layers, namely FC7 (fully connected level 7), P5 (pooling layer 5), and C3 (convolution layer 3). Inspecting the adversarial images, one can see that larger values of δ allow more noticeable perturbations. That  (1)  2  Published as a conference paper at ICLR 2016  Source  Guide  IFC7 α , δ = 5  ∆s  I P 5 α , δ = 10  ∆s  I C3 α , δ = 15  ∆s  Figure 1: Each row shows examples of adversarial images, optimized using different layers of Caf- fenet (FC7, P5, and C3), and different values of δ = (5, 10, 15). Beside each adversarial image is the difference between its corresponding source image.  said, we have found no natural images in which the guide image is perceptible in the adversarial image. Nor is there a signiﬁcant amount of salient structure readily visible in the difference images. While the class label was not an explicit factor in the optimization, we ﬁnd that class labels assigned to adversarial images by the DNN are almost always that of the guide. For example, we took 100 random source-guide pairs of images from Imagenet ILSVRC data (Deng et al., 2009), and applied optimization using layer FC7 of Caffenet, with δ = 10. We found that class labels assigned to adversarial images were never equal to those of source images. Instead, in 95% of cases they matched the guide class. This remains true for source images from training, validation, and test ILSVRC data. We found a similar pattern of behavior with other networks and datasets, including AlexNet (Krizhevsky et al., 2012), GoogleNet (Szegedy et al., 2015), and VGG CNN-S (Chatﬁeld et al., 2014), all trained on the Imagenet ILSVRC dataset. We also used AlexNet trained on the Places205 dataset, and on a hybrid dataset comprising 205 scene classes and 977 classes from ImageNet (Zhou et al., 2014). In all cases, using 100 random source-guide pairs the class labels assigned to the ad- versarial images do not match the source. Rather, in 97% to 100% of all cases the predicted class label is that of the guide. Like other approaches to generating adversarial images (e.g., Szegedy et al. (2014)), we ﬁnd that those generated on one network are usually misclassiﬁed by other networks Using the same 100 source-guide pairs with each of the models above, we ﬁnd that, on average, 54% of adversarial images obtained from one network are misclassiﬁed by other networks. That said, they are usually not consistently classiﬁed with the same label as the guide on different netowrks. We next turn to consider internal representations – do they resemble those of the source, the guide, or some combination of the two? One way to probe the internal representations, following Mahendran & Vedaldi (2014), is to invert the mapping, thereby reconstructing images from internal representa- tions at speciﬁc layers. The top panel in Fig. 2 shows reconstructed images for a source-guide pair. The Input row displays a source (left), a guide (right) and adervarisal images optimized to match representations at layers FC7, P5 and C3 of Caffenet (middle). Subsequent rows show reconstruc- tions from the internal representations of these ﬁve images, again from layers C3, P5 and FC7. Note how lower layers bear more similarity to the source, while higher layers resemble the guide. When optimized using C3, the reconstructions from C3 shows a mixture of source and guide. In almost all cases we ﬁnd that internal representations begin to mimic the guide at the layer targeted by the optimization. These reconstructions suggest that human perception and the DNN representations of these adversarial images are clearly at odds with one another. The bottom panel of Fig. 2 depicts FC7 and P5 activation patterns for the source and guide images in Fig. 2, along with those for their corresponding adversarial images. We note that the adversarial activations are sparse and much more closely resemble the guide encoding than the source encoding. The supplementary material includes several more examples of adversarial images, their activation patterns, and reconstructions from intermediate layers.  3  Published as a conference paper at ICLR 2016  Source  FC7  P5  C3  Guide  Input  Inv(C3)  Inv(P5)  Inv(FC7)  Source  FC7 Advers.  Guide  Source  P5 Advers.  Guide  Figure 2: (Top Panel) The top row shows a source (left), a guide (right), and three adversarial images (middle), optimized using layers FC7, P5, and C3 of Caffenet. The next three rows show images obtained by inverting the DNN mapping, from layers C3, P5, and FC7 respectively (Mahendran & Vedaldi, 2014). (Lower Panel) Activation patterns are shown at layer FC7 for the source, guide and FC7 adversarial above, and at layer P5 for the source, guide and P5 adversarial image above.  4 EXPERIMENTAL EVALUATION  We investigate further properties of adversarial images by asking two questions. To what extent do internal representations of adversarial images resemble those of the respective guides, and are the representations unnatural in any obvious way? To answer these questions we focus mainly on Caffenet, with random pairs of source-guide images drawn from the ImageNet ILSVRC datasets.  4.1 SIMILARITY TO THE GUIDE REPRESENTATION We ﬁrst report quantitative measures of proximity between the source, guide, and adversarial im- age encodings at intermediate layers. Surprisingly, despite the constraint that forces adversarial and source images to remain perceptually indistinguishable, the intermediate representations of the ad- versarial images are much closer to guides than source images. More interestingly, the adversarial representations are often nearest neighbors of their respective guides. We ﬁnd this is true for a remarkably wide range of natural images. For optimizations at layer FC7, we test on a dataset comprising over 20,000 source-guide pairs, sampled from training, test and validation sets of ILSVRC, plus some images from Wikipedia to increase diversity. For layers with higher dimensionality (e.g., P5), for computational expedience, we use a smaller set of 2,000 pairs. Additional details about how images are sampled can be found in the supplementary material. To simplify the exposition in what follows, we use s, g and α to denote  4  Published as a conference paper at ICLR 2016  (a) d(α,g)/d(s,g)  (b) d(α,g)(cid:14) d1(g)  (c) d(α,s)(cid:14) d(s)  Figure 3: Histogram of the Euclidean distances between FC7 adversarial encodings (α) and corre- sponding source (s) and guide (g), for optimizations targetting FC7. Here, d(x, y) is the distance between x and y, d(s) denotes the average pairwise distances between points from images of the same class as the source, and d1(g) is the average distance to the nearest neighbor encoding among images with the same class as the guide. Histograms aggregate over all source-guide pairs.  DNN representations of source, guide and adversarial images, whenever there is no confusion about the layer of the representations.  Euclidean Distance: As a means of quantifying the qualitative results in Fig. 2, for a large en- semble of source-guide pairs, all optimized at layer FC7, Fig. 3(a) shows a histogram of the ratio of Euclidean distance between adversarial α and guide g in FC7, to the distance between source s and guide g in FC7. Ratios less than 0.5 indicate that the adversarial FC7 encoding is closer to g than s. While one might think that the L∞ norm constraint on the perturbation will limit the extent to which adversarial encodings can deviate from the source, we ﬁnd that the optimization fails to reduce the FC7 distance ratio to less than 0.8 in only 0.1% of pairs when δ = 5. Figure 5 below shows that if we relax the L∞ bound on the deviation from the source image, then α is even closer to g, and that adversarial encodings become closer to g as one goes from low to higher layers of a DNN. Figure 3(b) compares the FC7 distances between α and g to the average FC7 distance between representations of all ILSVRC training images from the same class as the guide and their FC7 nearest neighbors (NN). Not only is α often the 1-NN of g, but the distance between α and g is much smaller than the distance between other points and their NN in the same class. Fig. 3(c) shows that the FC7 distance between α and s is relatively large compared to typical pairwise distances between FC7 encodings of images of the source class. Only 8% of adversarial images (at δ = 10) are closer to their source than the average pairwise FC7 distance within the source class.  Intersection and Average Distance to Nearest Neighbors: Looking at one’s nearest neighbors provides another measure of similarity. It is useful when densities of points changes signiﬁcantly through feature space, in which case Euclidean distance may be less meaningful. To this end we quantify similarity through rank statistics on near neighbors. We take the average distance to a point’s K NNs as a scalar score for the point. We then rank that point along with all other points of the same label class within the training set. As such, the rank is a non-parametric transformation of average distance, but independant of the unit of distance. We denote the rank of a point x as rK(x); we use K = 3 below. Since α is close to g by construction, we exclude g when ﬁnding NNs for adversarial points α. Table 1 shows 3NN intersection as well as the difference in rank between adversarial and guide encodings, ∆r3(α, g) = r3(α) − r3(g). When α is close enough to g, we expect the intersection to be high, and rank differences to be small in magnitude. As shown in Table 1, in most cases they share exactly the same 3NN; and in at least 50% of cases their rank is more similar than 90% of data points in that class. These results are for sources and guides taken from the ILSVRC training set. The same statistics are observed for data from test or validation sets.  4.2 SIMILARITY TO NATURAL REPRESENTATIONS Having established that internal representations of adversarial images (α) are close to those of guides (g), we then ask, to what extent are they typical of natural images? That is, in the vicinity of g, is α an inlier, with the same characteristics as other points in the neighborhood? We answer this question by examining two neighborhood properties: 1) a probabilistic parametric measure giving the log  5  Published as a conference paper at ICLR 2016  Model  CaffeNet (Jia et al., 2014)  AlexNet (Krizhevsky et al., 2012) GoogleNet (Szegedy et al., 2015)  VGG CNN S (Chatﬁeld et al., 2014) Places205 AlexNet (Zhou et al., 2014) Places205 Hybrid (Zhou et al., 2014)  pool5/7 × 7 s1  Layer FC7 FC7  FC7 FC7 FC7  ∩3NN = 3 ∩3NN ≥ 2 ∆r3 median, [min, max] (%)  71 72 87 84 91 85  95 97 100 100 100 100  −5.98, [−64.69, 0.00] −5.64, [−38.39, 0.00] −1.94, [−12.87, 0.10] −3.34, [−26.34, 0.00] −1.24, [−18.20, 8.04] −1.25, [−8.96, 8.29]  Table 1: Results for comparison of nearest neighbors of the adversarial and guide. We randomly select 100 pairs of guide and source images such that the guide is classiﬁed correctly and the source is classiﬁed to a different class. The optimization is done for a maximum of 500 iterations, with δ = 10. The statistics are in percentiles.  likelihood of a point relative to the local manifold at g; 2) a geometric non-parametric measure inspired by high dimensional outlier detection methods. For the analysis that follows, let NK(x) denote the set of K NNs of point x. Also, let Nref be a set of reference points comprising 15 random points from N20(g), and let Nc be the remaining “close” NNs of the guide, Nc = N20(g) \ Nref . Finally, let Nf = N50(g) \ N40(g) be the set of “far” NNs of the guide. The reference set Nref is used for measurement construction, while α, Nc and Nf are scored relative to g by the two measures mentioned above. Because we use up to 50 NNs, for which Euclidean distance might not be meaningful similarity measure for points in a high-dimensional space like P5, we use cosine distance for deﬁning NNs. (The source images used below are the same 20 used in Sec. 4.1. For expedience, the guide set is a smaller version of that used in Sec. 4.1, comprising three images from each of only 30 random classes.) Manifold Tangent Space: We build a probabilistic subspace model with probabilistic PCA (PPCA) around g and compare the likelihood of α to other points. More precisely, PPCA is applied to Nref , whose principal space is a secant plane that has approximately the same normal direction as the tangent plane, but generally does not pass through g because of the curvature of the manifold. We correct this small offset by shifting the plane to pass through g; with PPCA this is achieved by moving the mean of the high-dimensional Gaussian to g. We then evaluate the log likelihood of points under the model, relative to the log likelihood of g, denoted ∆L(·, g) = L(·) − L(g). We repeat this measurement for a large number of guide and source pairs, and compare the distribution of ∆L for α with points in Nc and Nf . For guide images sampled from ILSVRC training and validation sets, results for FC7 and P5 are shown in the ﬁrst two columns of Fig. 4. Since the Gaussian is centred at g, ∆L is bounded above by zero. The plots show that α is well explained locally by the manifold tangent plane. Comparing α obtained when g is sampled from training or validation sets (Fig. 4(a) vs 4(b), 4(d) vs 4(e)), we observe patterns very similar to those in plots of the log likelihood under the local subspace models. This suggests that the phenomenon of adversarial perturbation in Eqn. (1) is an intrinsic property of the representation itself, rather than the generalization of the model. Angular Consistency Measure: If the NNs of g are sparse in the high-dimensional feature space, or the manifold has high curvature, a linear Gaussian model will be a poor ﬁt. So we consider a way to test whether α is an inlier in the vicinity of g that does not rely on a manifold assumption. We take a set of reference points near a g, Nref , and measure directions from g to each point. We then compare the directions from g with those from α and other nearby points, e.g., in Nc or Nf , to see whether α is similar to other points around g in terms of angular consistency. Compared to points within the local manifold, a point far from the manifold will tend to exhibit a narrower range of directions to others points in the manifold. Speciﬁcally, given reference set Nref , with cardinality k, and with z being α or a point from Nc or Nf , our angular consistency measure is deﬁned as  Ω(z, g) =  1 k  (cid:104)xi − z, xi − g(cid:105) (cid:107)xi − z(cid:107)(cid:107)xi − g(cid:107)  (cid:88)  xi∈Nref  (3)  Fig. 4(c) and 4(f) show histograms of Ω(α, g) compared to Ω(nc, g) where nc ∈ Nc and Ω(nf , g) where nf ∈ Nf . Note that maximum angular consistency is 1, in which case the point behaves like g. Other than differences in scaling and upper bound, the angular consistency plots 4(c) and 4(f) are strikingly similar to those for the likelihood comparisons in the ﬁrst two columns of Fig. 4, supporting the conclusion that α is an inlier with respect to representations of natural images.  6  Published as a conference paper at ICLR 2016  (a) ∆L, FC7, g ∈ training (b) ∆L, FC7, g ∈ validation  (c) Ω, FC7, g ∈ training  (d) ∆L, P5, g ∈ training  (e) ∆L, P5, g ∈ validation  (f) Ω, P5, g ∈ training  Figure 4: Manifold inlier analysis: the ﬁrst two columns (4(a),4(b),4(d),4(e)) for results of mani- fold tangent space analysis, showing distribution of difference in log likelihood of a point and g, ∆L(·, g) = L(·) − L(g); the last column (4(c)),(4(f)) for angular consistency analysis, showing distribution of angular consistency Ω(·, g), between a point and g. See Eqn. 3 for deﬁnitions.  (a): Rank of adversaries vs rank of n1(α): Average distance of 3-NNs is used to rank all points in predicted class (excl. guide). Adversaries with same horizontal coordinate share the same guide.  (b): Manifold analysis for label-opt adversaries, at layer FC7, with tan- gent plane through n1(α).  Figure 4: Label-opt and feature-opt PPCA and rank measure comparison plots.  4.3 COMPARISONS AND ANALYSIS We now compare our feature adversaries to images created to optimize mis-classiﬁcation (Szegedy et al., 2014), in part to illustrate qualitative differences. We also investigate if the linearity hypoth- esis for mis-classiﬁcation adversaries of Goodfellow et al. (2014) is consistent with and explains with our class of adversarial examples. We hereby refer to our results as feature adversaries via optimization (feature-opt). The adversarial images designed to trigger mis-classiﬁcation via opti- mization (Szegedy et al., 2014), described brieﬂy in Sec. 2, are referred to as label adversaries via optimization (label-opt). Comparison to label-opt: To demonstrate that label-opt differs qualitatively from feature-opt, we report three empirical results. First, we rank α, g, and other points assigned the same class label as g, according to their average distance to three nearest neighbours, as in Sec. 4.1. Fig. 4(a) shows rank of α versus rank of its nearest neighbor-n1(α) for the two types of adversaries. Unlike feature- opt, for label-opt, the rank of α does not correlate well with the rank of n1(α). In other words, for feature-opt α is close to n1(α), while for label-opt it is not. Second, we use the manifold PPCA approach in Sec. 4.2. Comparing to peaked histogram of stan- dardized likelihood of feature-opt shown in Fig. 4, Fig. 4(b) shows that label-opt examples are not represented well by the Gaussian around the ﬁrst NN of α. Third, we analyze the sparsity patterns on different DNN layers for different adversarial construction methods. It is well known that DNNs with ReLU activation units produce sparse activations (Glorot et al. (2011)). Therefore, if the degree of sparsity increases after the adversarial perturbation, the  7  Published as a conference paper at ICLR 2016  ∆S  I/U with s  feature-opt label-opt feature-opt label-opt 39 ± 9 70 ± 5 85 ± 3 94 ± 1  12 ± 4 33 ± 2 60 ± 1 78 ± 0  13 ± 5 0 ± 0 0 ± 0 0 ± 0  7 ± 7 0 ± 1 2 ± 1 0 ± 0  FC7 C5 C3 C1  Table 2: Sparsity analysis: Sparsity is quantiﬁed as a percentage of the size of each layer.  Figure 5: Distance ratio d(α,g)/d(s,g) vs δ. C2, C3, P5, F7 are for feature-opt adversaries; (cid:96)-f7 denotes FC7 distances for feature-linear.  adversarial example is using additional paths to manipulate the resulting represenation. We also in- vestigate how many activated units are shared between the source and the adversary, by computing the intersection over union I/U of active units. If the I/U is high on all layers, then two repre- senations share most active paths. On the other hand, if I/U is low, while the degree of sparsity remains the same, then the adversary must have closed some activation paths and opened new ones. In Table 2, ∆S is the difference between the proportion of non-zero activations on selected layers between the source image represenation for the two types of adversaries. One can see that for all except FC7 of label-opt, the difference is signiﬁcant. The column “I/U with s” also shows that feature-opt uses very different activation paths from s when compared to label-opt. Testing The Linearity Hypothesis for feature-opt: Goodfellow et al. (2014) suggests that the existence of label adversaries is a consequence of networks being too linear. If this linearity hypoth- esis applies to our class of adversaries, it should be possible to linearize the DNN around the source image, and then obtain similar adversaries via optimization. Formally, let Js = J(φ(Is)) be the Ja- cobian matrix of the internal layer encoding with respect to source image input. Then, the linearity s (I − Is)− φ(Ig)(cid:107)2 hypothesis implies φ(I) ≈ φ(Is) + J(cid:62) 2 subject to the same inﬁnity norm constraint in Eqn. 2. We refer to these adversaries as feature-linear. As shown in Fig. 5, such adversaries do not get particularly close to the guide. They get no closer than 80%, while for feature-opt the distance is reduced to 50% or less for layers down to C2. Note that unlike feature-opt, the objective of feature-linear does not guarantee a reduction in distance when the constraint on δ is relaxed. These results suggest that the linearity hypothesis may not explain the existence of feature-opt adversaries. Networks with Random Weights: We further explored whether the existence of feature-opt ad- versaries is due to the learning algorithm and the training set, or to the structure of deep networks per se. For this purpose, we randomly initialized layers of Caffenet with orthonormal weights. We then optimized for adversarial images as above, and looked at distance ratios (as in Fig. 3). Interestingly, the distance ratios for FC7 and Norm2 are similar to Fig. 5 with at most 2% deviation. On C2, the results are at most 10% greater than those on C2 for the trained Caffenet. We note that both Norm2 and C2 are overcomplete representations of the input. The table of distance ratios can be found in the Supplementary Material. These results with random networks suggest that the existence of feature-opt adversaries may be a property of the network architecture.  s (I − Is). Hence, we optimize (cid:107)φ(Is) + J(cid:62)  5 DISCUSSION  We introduce a new method for generating adversarial images that appear perceptually similar to a given source image, but whose deep representations mimic the characteristics of natural guide im- ages. Indeed, the adversarial images have representations at intermediate layers appear quite natural and very much like the guide images used in their construction. We demonstrate empirically that these imposters capture the generic nature of their guides at different levels of deep representations. This includes their proximity to the guide, and their locations in high density regions of the feature space. We show further that such properties are not shared by other categories of adversarial images. We also ﬁnd that the linearity hypothesis (Goodfellow et al., 2014) does not provide an obvious explanation for these new adversarial phenomena. It appears that the existence of these adversarial images is not predicated on a network trained with natural images per se. For example, results on random networks indicate that the structure of the network itself may be one signiﬁcant factor.  8  Published as a conference paper at ICLR 2016  Nevertheless, further experiments and analysis are required to determine the true underlying reasons for this discrepancy between human and DNN representations of images. Another future direction concerns the exploration of failure cases we observed in optimizing feature adversaries. As mentioned in supplementary material, such cases involve images of hand-written digits, and networks that are ﬁne-tuned with images from a narrow domain (e.g., the Flicker Style dataset). Such failures suggest that our adversarial phenomena may be due to factors such as network depth, receptive ﬁeld size, or the class of natural images used. Since our aim here was to analyze the representation of well-known networks, we leave the exploration of these factors to future work. Another interesting question concerns whether existing discriminative models might be trained to detect feature adversaries. Since training such models requires a diverse and relatively large dataset of adversarial images we also leave this to future work.  ACKNOWLEDGMENTS Financial support for this research was provided, in part, by MITACS, NSERC Canada, and the Canadian Institute for Advanced Research (CIFAR). We would like to thank Foteini Agraﬁoti for her support. We would also like to thank Ian Goodfellow, Xavier Boix, as well as the anoynomous reviewers for helpful feedback.  REFERENCES Chatﬁeld, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details:  Delving deep into convolutional nets. In BMVC, 2014. 3, 6  Deng, J, Dong, W, Socher, R, Li, LJ, Li, K, and Fei-Fei, L. Imagenet: A large-scale hierarchical  image database. In IEEE CVPR, pp. 248–255, 2009. 3  Fawzi, A, Fawzi, O, and Frossard, P. Fundamental limits on adversarial robustness. In ICML, 2015.  1, 2  Glorot, X, Bordes, A, and Bengio, Y. Deep sparse rectiﬁer neural networks. In AISTATS, volume 15,  pp. 315–323, 2011. 7  Goodfellow, IJ, Shlens, J, and Szegedy, C. Explaining and harnessing adversarial examples. In ICLR  (arXiv:1412.6572), 2014. 1, 2, 7, 8, 11  Gu, S and Rigazio, L. Towards deep neural network architectures robust to adversarial examples. In  Deep Learning and Representation Learning Workshop (arXiv:1412.5068), 2014. 1  Jia, Y, Shelhamer, E, Donahue, J, Karayev, S, Long, J, Girshick, R, Guadarrama, S, and Darrell, T. Caffe: Convolutional architecture for fast feature embedding. In ACM Int. Conf. Multimedia, pp. 675–678, 2014. 2, 6  Krizhevsky, A, Sutskever, I, and Hinton, GE. Imagenet classiﬁcation with deep convolutional neural  networks. In NIPS, pp. 1097–1105, 2012. 3, 6  Mahendran, A and Vedaldi, A. Understanding deep image representations by inverting them. In  IEEE CVPR (arXiv:1412.0035), 2014. 3, 4  Nguyen, A, Yosinski, J, and Clune, J. Deep neural networks are easily fooled: High conﬁdence  predictions for unrecognizable images. In IEEE CVPR (arXiv:1412.1897), 2015. 1, 2  Szegedy, C, Zaremba, W, Sutskever, I, Bruna, J, Erhan, D, Goodfellow, I, and Fergus, R. Intriguing  properties of neural networks. In ICLR (arXiv:1312.6199), 2014. 1, 2, 3, 7  Szegedy, C, Liu, W, Jia, Y, Sermanet, P, Reed, S, Anguelov, D, Erhan, D, Vanhoucke, V, and Rabi-  novich, A. Going deeper with convolutions. In CVPR, 2015. 3, 6  Tabacof, P and Valle, E. arXiv:1510.05328, 2015. 1  Exploring the space of adversarial  images.  arXiv preprint  Wang, Z, Bovik, AC, Sheikh, HR, and Simoncelli, EP.  Image quality assessment: From error  visibility to structural similarity. IEEE Trans. PAMI, 3(4):600–612, 2004. 2  Zhou, B, Lapedriza, A, Xiao, J, Torralba, A, and Oliva, A. Learning deep features for scene recog-  nition using places database. In NIPS, pp. 487–495, 2014. 3, 6  9  Published as a conference paper at ICLR 2016  SUPPLEMENTARY MATERIAL  S1  ILLUSTRATION OF THE IDEA  Fig. S1 illustrates the achieved goal in this paper. The image of the fancy car on the left is a train- ing example from the ILSVRC dataset. On the right of it, there is an adversarial image that was generated by guiding the source image by an image of Max (the dog). While the two fancy car images are very close in image space, the activation pattern of the adversarial car is almost identical to that of Max. This shows that the mapping from the image space to the representation space is such that for each natural image, there exists a point in a small neighborhood in the image space that is mapped by the network to a point in the representation space that is in a small neighborhood of the representation of a very different natural image.  Figure S1: Summary of the main idea behind the paper.  S2 DATASETS FOR EMPIRICAL ANALYSIS  Unless stated otherwise, we have used the following two sets of source and guide images. The ﬁrst set is used for experiments on layer FC7 and the second set is used for computational expedience on other layers (e.g. P5). The source images are guided by all guide images to show that the convergence does not depend on the class of images. To simplify the reporting of classiﬁcation behavior, we only used guides from training set whose labels are correctly predicted by Caffenet. In both sets we used 20 source images, with ﬁve drawn at random from each of the ILSVRC train, test and validation sets, and ﬁve more selected manually from Wikipedia and the ILSVRC validation set to provide greater diversity. The guide set for the ﬁrst set consisted of three images from each of 1000 classes, drawn at random from ILSVRC training images, and another 30 images from each of the validation and test sets. For the second set, we drew guide images from just 100 classes.  10  Published as a conference paper at ICLR 2016  S3 EXAMPLES OF ADVERSARIES  Fig. S2 shows a random sample of source and guide pairs along with their FC7 or Pool5 adversarial images. In none of the images the guide is perceptable in the adversary, regardless of the choice of source, guide or layer. The only parameter that affects the visibility of the noise is δ.  S4 DIMENSIONALITY OF REPRESENTATIONS  The main focus of this study is on the well-known Caffenet model. The layer names of this model and their representation dimensionalities are provided in Tab. S1.  Layer Name Dimensions Total  Input  3 × 227 × 227  Conv2  256 × 27 × 27  Norm2  256 × 13 × 13  Conv3  384 × 13 × 13  Pool5  256 × 6 × 6  154587  186624  43264  64896  9216  FC7 4096 4096  Table S1: Caffenet layer dimensions.  S5 RESULTS FOR NETWORKS WITH RANDOM WEIGHTS  As described in Sec. 4.3, we attempt at analyzing the architecture of Caffenet independent of the training by initializing the model with random weights and generating feature adversaries. Results in Tab. S2 show that we can generate feature adversaries on random networks as well. We use the ratio of distances of the adversary to the guide over the source to the guide for this analysis. In each cell, the mean and standard deviation of this ratio is shown for each of the three random, orthonormal random and trained Caffenet networks. The weights of the random network are drawn from the same distribution that Caffenet is initialized with. Orthorgonal random weights are obtained using singular value decomposition of the regular random weights. Results in Tab. S2 indicate that convergence on Norm2 and Conv2 is almost similar while the di- mensionality of Norm2 is quite smaller than Conv2. On the other hand, Fig. 5 shows that although Norm2 has smaller dimensionality than Conv3, the optimization converges to a closer point on Conv3 rather than Conv2 and hence Norm2. This means that the relation between dimensionality and the achieved distance of the adversary is not straightforward.  Layer  conv2  norm2  fc7  δ = 5  T:0.79 ± 0.04 OR:0.89 ± 0.03 R:0.90 ± 0.02 T:0.80 ± 0.04 OR:0.82 ± 0.05 R:0.85 ± 0.03 T:0.32 ± 0.10 OR:0.34 ± 0.12 R:0.52 ± 0.09  δ = 10  T:0.66 ± 0.06 OR:0.78 ± 0.05 R:0.81 ± 0.04 T:0.66 ± 0.05 OR:0.69 ± 0.08 R:0.73 ± 0.06 T:0.12 ± 0.06 OR:0.12 ± 0.09 R:0.26 ± 0.11  δ = 15  T:0.57 ± 0.06 OR:0.71 ± 0.07 R:0.74 ± 0.06 T:0.57 ± 0.06 OR:0.59 ± 0.10 R:0.63 ± 0.08 T:0.07 ± 0.04 OR:0.07 ± 0.06 R:0.13 ± 0.10  δ = 20  T:0.50 ± 0.07 OR:0.64 ± 0.09 R:0.67 ± 0.08 T:0.50 ± 0.06 OR:0.51 ± 0.11 R:0.55 ± 0.09 T:0.06 ± 0.03 OR:0.05 ± 0.04 R:0.07 ± 0.08  δ = 25  T:0.45 ± 0.07 OR:0.58 ± 0.10 R:0.61 ± 0.09 T:0.45 ± 0.06 OR:0.44 ± 0.11 R:0.48 ± 0.10 T:0.05 ± 0.02 OR:0.05 ± 0.02 R:0.04 ± 0.06  Table S2: Ratio of d(α,g)/d(s,g) as δ changes from 5 to 25 on randomly weighted(R), orthogonal randomly weighted(OR) and trained(T) Caffenet optimized on layers Conv2, Norm2 and FC7.  S6 ADVERSARIES BY FAST GRADIENT  As we discussed in Sec. 4.3, Goodfellow et al. (2014) also proposed a method to construct label adversaries efﬁciently by taking a small step consistent with the gradient. While this fast gradient method shines light on the label adversary misclassiﬁcations, and is useful for adversarial training, it is not relevant to whether the linearity hypothesis explains the feature adversaries. Therefore we omitted the comparison in Sec. 4.3 to fast gradient method, and continue the discussion here. The fast gradient method constructs adversaries (Goodfellow et al. (2014)) by taking the pertur- bation deﬁned by δsign(∇I loss(f (I), (cid:96))), where f is the classiﬁer, and (cid:96) is an erroneous label  11  Published as a conference paper at ICLR 2016  Source  IFC7 α , δ = 5  Guide  Source  I P 5 α , δ = 10  Guide  Figure S2: Each row shows examples of adversarial images, optimized using different layers of Caffenet (FC7, P5), and different values of δ = (5, 10).  12  Published as a conference paper at ICLR 2016  for input image I. We refer to the resulting adversarial examples label-fgrad. We can also ap- ply the fast gradient method to an internal representation, i.e. taking the perturbation deﬁned by δsign(∇I(cid:107)φ(I) − φ(Ig)(cid:107)2). We call this type feature adversaries via fast gradient (feat-fgrad). The same experimental setup as in Sec. 4.3 is used here. In Fig. S3, we show the nearest neighbor rank analysis and manifold analysis as done in Sec. 4.2 and Sec. 4.3. Moreover, Figs. S3(a)-S3(b) in compare to Figs. 4(a)-4(b) from feature-opt results and Fig. 4(b) from label-opt results indicates that this adversaries are not represented as well as feature-opt by a Gaussian around the NN of the adversary too. Also, Figs. S3(c)-S3(d) in compare to Fig. 4(a) show the obvious difference in adversarial distribution for the same set of source and guide.  (a) label-fgrad, Gaussian at n1(α)  (b) feat-fgrad, Gaussian at g  (c) label-fgrad  (d) feat-fgrad  Figure S3: Local property analysis of label-fgrad and feat-fgrad on FC7: S3(a)-S3(b) manifold analysis; S3(c)-S3(d) neighborhood rank analysis.  S7 FAILURE CASES  There are cases in which our optimization was not successful in generating good adversaries. We observed that for low resolution images or hand-drawn characters, the method does not always work well. It was successful on LeNet with some images from MNIST or CIFAR10, but for other cases we found it necessary to relax the magnitude bound on the perturbations to the point that traces of guide images were perceptible. With Caffenet, pre-trained on ImageNet and then ﬁne-tuned on the Flickr Style dataset, we could readily generate adversarial images using FC8 in the optimization (i.e., the unnormalized class scores), however, with FC7 the optimization often terminated without producing adversaries close to guide images. One possible cause may be that the ﬁne-tuning distorts the original natural image representation to beneﬁt style classiﬁcation. As a consequence, the FC7 layer no longer gives a good generic image represenation, and Euclidean distance on FC7 is no longer useful for the loss function.  S8 MORE EXAMPLES WITH ACTIVATION PATTERNS  Finally, we dedicate the remaining pages to several pairs of source and guide along with their ad- versaries, activation patterns and inverted images as a complementary to Fig. 2. Figs. S4, S5, S6, S7 and S8 all have similar setup as it is discussed in Sec. 3.  13  Published as a conference paper at ICLR 2016  Source  FC7  P5  C3  Guide  Input  Inv(C3)  Inv(P 5)  Inv(F C7)  Source  FC7 Advers.  Guide  Source  P5 Advers.  Guide  Figure S4: Inverted images and activation plot for a pair of source and guide image shown in the ﬁrst row (Input). This ﬁgure has same setting as Fig. 2.  14  Published as a conference paper at ICLR 2016  Source  FC7  P5  C3  Guide  Input  Inv(C3)  Inv(P5)  Inv(FC7)  Source  FC7 Advers.  Guide  Source  P5 Advers.  Guide  Figure S5: Inverted images and activation plot for a pair of source and guide image shown in the ﬁrst row (Input). This ﬁgure has same setting as Fig. 2.  15  Published as a conference paper at ICLR 2016  Source  FC7  P5  C3  Guide  Input  Inv(C3)  Inv(P 5)  Inv(F C7)  Source  FC7 Advers.  Guide  Source  P5 Advers.  Guide  Figure S6: Inverted images and activation plot for a pair of source and guide image shown in the ﬁrst row (Input). This ﬁgure has same setting as Fig. 2.  16  Published as a conference paper at ICLR 2016  Source  FC7  P5  C3  Guide  Input  Inv(C3)  Inv(P 5)  Inv(F C7)  Source  FC7 Advers.  Guide  Source  P5 Advers.  Guide  Figure S7: Inverted images and activation plot for a pair of source and guide image shown in the ﬁrst row (Input). This ﬁgure has same setting as Fig. 2.  17  Published as a conference paper at ICLR 2016  Source  FC7  P5  C3  Guide  Input  Inv(C3)  Inv(P 5)  Inv(F C7)  Source  FC7 Advers.  Guide  Source  P5 Advers.  Guide  Figure S8: Inverted images and activation plot for a pair of source and guide image shown in the ﬁrst row (Input). This ﬁgure has same setting as Fig. 2.  18  ",
1511.06394,2016,Geodesics of learned representations,"['Geodesics of learned representations\nOlivier Hénaff', 'Eero Simoncelli']",https://arxiv.org/pdf/1511.06394,"6 1 0 2     b e F 2 2         ]  V C . s c [      4 v 4 9 3 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  GEODESICS OF LEARNED REPRESENTATIONS  Olivier J. H´enaff & Eero P. Simoncelli Howard Hughes Medical Institute, Center for Neural Science and Courant Institute of Mathematical Sciences New York University New York, NY 10003, USA {henaff, eero}@cns.nyu.edu  ABSTRACT  We develop a new method for visualizing and reﬁning the invariances of learned representations. Speciﬁcally, we test for a general form of invariance, lineariza- tion, in which the action of a transformation is conﬁned to a low-dimensional sub- space. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of mini- mal length in the space of the representation (a “representational geodesic”). If the transformation relating the two reference images is linearized by the representa- tion, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image clas- siﬁcation network and ﬁnd that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated trans- formations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modiﬁed representation is able to linearize a variety of geometric image transformations.  1  INTRODUCTION  A fundamental requirement of pattern recognition is the ability to ignore irrelevant variations in the input (Duda et al., 2001). Most visual recognition problems are thwarted by variations in position, size, pose, lighting, and other viewing conditions that can bring objects from different classes closer, while increasing within-class variability (DiCarlo & Cox, 2007), and the construction of represen- tations that are invariant to these variations remains an active area of research. Recent examples of learned visual representations have proven highly effective for recognition (Krizhevsky et al., 2012), but a precise understanding of exactly what they represent remains elusive. And although these rep- resentations are hypothesized to be invariant to various identity-preserving deformations, apart from a few exceptions, these claims are rarely tested directly (Kavukcuoglu et al., 2009). Image synthesis provides a powerful methodology for examining the invariances of arbitrary rep- resentations. It has been used to explore and reﬁne texture models, incrementally augmenting the representation with new statistical constraints until images synthesized with matching parameters are indistinguishable to human observers (Portilla & Simoncelli, 2000). When applied to deep recognition networks, synthesis has revealed failures in the form of “adversarial examples”: images that appear entirely different to a human observer, and yet are identiﬁed by the network as belonging to the same category (Szegedy et al., 2013). In these cases, samples from the equivalence class of images that map to the same representation vector provide a means of verifying or falsifying the hypothesis that the invariances of the representation are also invariances for human observers. But the synthesis test, in which human observers try to discriminate synthesized images, is one- sided: failures (i.e. visually distinct images) can reveal inappropriate invariances of a representa- tion, but successes can mask a lack of desired invariances. Consider the standard case of translation- invariance. The Fourier amplitude spectrum (i.e., the set of magnitudes of Fourier transform coefﬁ- cients) provides a well-known example of a translation-invariant representation, but it is invariant to far more than translations, and this is immediately revealed by a synthesis test (ﬁgure 1, top right). On the other hand, simply representing an image with its raw pixel values (the identity representa-  1  Published as a conference paper at ICLR 2016  Figure 1: Geodesics can reveal either insufﬁcient or excessive invariance, whereas synthesis reveals only the latter. Top: images synthesized so that their representation is matched to that of the ground truth image (left). Middle image has matching pixel intensities (i.e., it is identical to the ground truth image) and right image has matching Fourier magnitudes. Bottom: synthetic geodesic sequences connecting two translated copies of the same image, via different representations. Shown are the middle frame of each sequence, and below it, the temporal evolution of each row of pixels indicated by a horizontal red line. The ground truth transformation (left) is a translation (as can be seen from the diagonal lines in the temporal slice), but both geodesics deviate from the true transformation. The pixel representation fully constrains the image, and has no invariances, and thus the synthesized geodesic images are simply linearly interpolated between the initial and ﬁnal images. The Fourier magnitudes, while translation-invariant, are also invariant to arbitrary phase perturbations, and the synthesized geodesic image contains Fourier components whose phases are shifted inconsistently.  tion) will trivially produce visually perfect synthetic examples (ﬁgure 1, top center) despite the fact that it has no invariance properties at all. We seek a more general method of evaluation that penalizes a model for discarding too much infor- mation (as with synthesis) but also for discarding too little information. Each of these failures can be seen as an inadequacy of the image metric induced by the representation. Speciﬁcally, an image representation deforms the input space, bringing some images closer to each other while spreading others out, and thus inducing a new metric in image space. We can expose properties of this image metric by generating a geodesic sequence of images. Speciﬁcally, given an initial and ﬁnal image, we synthesize a sequence of images that follow a minimal-length path in the response space of the representation. In the absence of any other constraints, this path will be a straight line connecting the representations of the two images; more generally, it will be the straightest path connecting the two points. In the case where the two images differ by a simple transformation (e.g. a translation, ﬁgure 1, left column) that is not linearized by the representation (i.e. mapped to the straightest path connecting the two representations), the geodesic will differ from the original transformation con- necting the images (ﬁgure 1, middle column). Similarly, if the representation is invariant to many  2  ground truthpixel intensitiesFourier magnitudesPublished as a conference paper at ICLR 2016  transformations, the geodesic may correspond to a path that uses a mixture of transformations, and thus differ from the ground truth path (ﬁgure 1, right column). As a result, by visualizing whether a representation has linearized the action of various deformations, representational geodesics can reveal both excessive and insufﬁcient invariance in an image model. We develop an algorithm for synthesizing geodesic sequences for a representation, and use it to examine whether learned representations linearize various real-world transformations such as trans- lation, rotation, and dilation. We ﬁnd that a current state-of-the-art object recognition network fails to linearize these basic transformations. However, these failures point to a deﬁciency in the repre- sentation, leading to a simple way of improving it. We show that the improved representation is able to linearize a range of parametric transformations as well as generic distortions found in natural image sequences.  2 SYNTHESIZING GEODESIC SEQUENCES  Suppose we have an image representation, y = f (x), where x is the vector of image pixel intensities and f (·) a continuous function that maps it to an abstract vector-valued representation y (e.g. the responses of an intermediate stage of a hierarchical neural network). Given initial and ﬁnal images, we wish to synthesize a sequence of images that lies along the path of minimal length in the repre- sentation space (a representational geodesic). If the mapping is many-to-one (as is usually the case), this sequence of images is not unique. We resolve this ambiguity by selecting the representational geodesic that is also of minimal length in the space of images (i.e., a conditional geodesic in image space).  2.1 OBJECTIVE FUNCTION  In order to generate such a sequence, we optimize an objective function that expresses a discrete approximation of the problem, directly in terms of images sampled along the path. Given a desired sequence length N and initial and ﬁnal images, {x0, xN}, we wish to synthesize a sequence of images, γ = {xn; n = 0 . . . N}, lying along a geodesic in representation space. The representational path length is  L[f (γ)] =  (cid:107)f (xn) − f (xn−1)(cid:107)2  which is bounded by the representational energy  E[f (γ)] =  (cid:107)f (xn) − f (xn−1)(cid:107)2  2  thanks to the Cauchy-Schwartz inequality  n=1  L[f (γ)]2 ≤ N E[f (γ)]  with equality if and only if the representations are equispaced, which is encouraged by minimizing the representational energy. As a result, a path that meets this condition (e.g. the red curve in ﬁgure 2) while minimizing the representational energy E[f (γ)] is a representational geodesic. When the mapping to representation space is many-to-one, there are many possible solutions to this problem. To uniquely constrain the solution, we deﬁne an analogous energy term that ensures that this path is also of minimal length in the image domain  N(cid:88)  E[γ] =  (cid:107)xn − xn−1(cid:107)2  2  Since we are looking for the shortest path in image space that is also a geodesic in representation space, we minimize E[γ] conditioned on the path also minimizing E[f (γ)]. Furthermore, during the optimization we constrain image pixel intensities to the [0, 1] range.  n=1  3  N(cid:88)  n=1  N(cid:88)  Published as a conference paper at ICLR 2016  2.2 OPTIMIZATION  We optimize this objective in three steps. First, we initialize the path with the minimum of E[γ], which is simply a sequence of images that are linearly interpolated between the initial and ﬁnal images. Next we minimize the representational geodesic objective E[f (γ)]. Finally, we minimize the image-domain geodesic objective, conditioned on staying in the set of representational geodesics. Minimizing the representational geodesic objective in the second step requires optimizing an image for its representation via a non-linear function, and thus shares much of the non-convexity found in training deep neural networks. In particular, the curvature of the energy surface can vary widely over the course of the optimization. For this reason, we used the Adam optimization method (Kingma & Ba, 2014), which scales gradients by a running estimate of their variance, providing robustness to these changes in the energy landscape. We run Adam, using the default parameters, for 104 iterations to ensure that we reach the minimum of the representational geodesic cost. To optimize the image-domain geodesic objective while constraining the solution to remain in the set of representational geodesics, we start by computing a descent direction for the image-domain geodesic objective. We then project out the component of this direction that lies along the gradient of the representational geodesic objective. We take a step in that direction, then project back onto the set of representational geodesics by re-minimizing the representational geodesic cost (again using Adam), and repeat until convergence. We summarize our method with the following algorithm.  Conditional geodesic computation Require: f: continuous mapping Require: x0, xN : initial and ﬁnal images Require: N: number of steps along geodesic path (N = 10 in all our experiments) Require: λ: gradient descent step size Ensure: γ = {xn; n = 0 . . . N} minimizes E[γ] conditioned on minimizing E[f (γ)]  N xN n ∈(cid:74)0, 1, . . . N(cid:75)  xn ← N−n N x0 + n minimize E[f (γ)] while γ has not converged do  initialize with pixel-based interpolation project onto set of representational geodesics  dr  project out representational gradient  re-project onto set of representational geodesics  dr ← ∇γE[f (γ)] dp ← ∇γE[γ]  (cid:98)dp ← dp − <dr,dp> γ ← γ − λ(cid:98)dp  (cid:107)dr(cid:107)2  2  minimize E[f (γ)]  end while return γ  Despite the non-convexity of the problem, we have good reason to believe that solving this opti- mization problem should be feasible for trained neural networks. Since the output of the ﬁrst layer is equal to the convolution of the input image with a ﬁlter bank, our problem is similar in complexity to optimizing the weights of the ﬁrst layer of a network, for the same objective. Recent theoretical work shows that optimizing all layers of a network jointly makes the problem signiﬁcantly more difﬁcult than optimizing a single layer in isolation (Saxe et al., 2013). Hence optimizing E[f (γ)] should be easier than training the full network for recognition. In practice we were able to solve the optimization problem for a variety of deep networks. It should be noted that if the mapping f (·) is not surjective, not all vectors in the representation space are attainable from an input image. Speciﬁcally, if the mapping is non-linear (as for most represen- tations of interest) the set of attainable vectors is non-convex, and vectors lying along the straight line connecting two representations are not necessarily attainable. As such, we can only expect to ﬁnd a geodesic path whose representation is as close as possible to this straight line by minimizing the representational geodesic cost E[f (γ)]. Figure 2 shows an example of this, for the case of image  4  Published as a conference paper at ICLR 2016  Figure 2: Deviation from the straight line connecting the representations of a pair of images, for different paths in representation space. Due to the non-linearity of the rep- resentation (the third stage of L2 pooling of a deep neu- ral network, see section 3) the geodesic deviates slightly from the straight line. The ground truth transformation (here, a translation) deviates similarly, indicating that the representation has linearized the transformation to a large extent. For reference, a pixel-based interpolation deviates signiﬁcantly more from a straight line. Axes are in the same units, normalized by the distance sepa- rating the end point representations. Knots along each curve indicate samples used to compute the path.  translation. By construction, the geodesic is closer to a straight line in representation space than either the ground truth transformation or a pixel interpolation. The ground truth transformation lies close to the geodesic, indicating that this representation has almost (but not completely) linearized this transformation. The differences between these two paths can be made explicit by visualizing the geodesic sequence, as detailed in the following section.  3 VISUALIZING GEODESIC SEQUENCES  We used our geodesic framework to examine the invariance properties of the 16-layer VGG network (Simonyan & Zisserman, 2014), which we chose for its conceptual simplicity and strong perfor- mance on object recognition benchmarks. As a “representation” for our tests, we used the output of the third stage of pooling. Each stage of this continuous non-linear mapping is constructed as a composition of three elementary operations: linear ﬁltering, half-wave rectiﬁcation, and max pool- ing (which summarizes a local region with its maximum). We followed the preprocessing steps de- scribed in the original work: images are rescaled to the [0, 255] range, color channels are permuted from RGB to BGR, and the mean BGR pixel value, [104, 117, 124], is subtracted. We veriﬁed that our implementation could replicate the published object recognition results.  3.1 GEODESICS AS A DIAGNOSTIC TOOL  We ﬁrst examined whether this representation linearizes basic geometric transformations: transla- tion, rotation and dilation. To do so, we compute the geodesic sequence between two images that differ by one of these transformations, and compare it to the ground truth sequence obtained by incremental application of the same transformation. The extent of the overall transformation deter- mines the difﬁculty of this task: all representations (even trivial ones) will produce geodesics that are close to the ground truth for very small transformations, whereas all are likely to fail for very large transformations. For our discriminative test we chose intermediate values: an 8 pixel translation, a 4◦ rotation, and a 10% dilation. We found that the VGG network, despite its impressive classiﬁcation performance, failed to lin- earize these simple geometric deformations and produced geodesics with salient aliasing artifacts (ﬁgure 3, middle column). Given that no subsampling is used in the convolutional layers, we at- tributed this failure to the max pooling layers, which subsample the representation by a factor of 2 in each direction, despite their small spatial extent (a 2×2 pooling region). To avoid aliasing arti- facts when subsampling by a factor of 2, the Nyquist theorem requires blurring with a ﬁlter whose cutoff frequency is below π 2 . Following this indication, we replaced the max pooling layers with L2 pooling:  where the squaring and square-root operations are point-wise, and the blurring kernel g(·) is chosen as a 6×6 pixel Hanning window that approximately enforces the Nyquist criterion. This type of pooling is often used to describe the behavior of neurons in primary visual cortex (Vintch et al.,  L2(x) =  g ∗ x2  (cid:112)  5  distance from representation lineprojection on representation linegeodesicground truthpixel fadePublished as a conference paper at ICLR 2016  Figure 3: Comparison of geodesic sequences for VGG network representation with max pooling (middle column) and VGG network with L2 pooling (right column) with ground truth sequence (left column). Three different types of geometric transformation are tested: horizontal translation (top), rotation around the center (middle), dilation about the center (bottom). As in ﬁgure 1, square images are the middle frame from the corresponding sequence, and underneath is the temporal evolution of three image slices, taken along the red lines shown in the left column. The original VGG network is unable to linearize these transformations (as indicated by the ‘double exposure’ in the middle frame, and the discontinuous temporal slices), whereas the same VGG network with L2 pooling (right column) induces a geodesic that is close to ground truth.  6  ground truthVGG network, max poolingVGG network, L2 poolingPublished as a conference paper at ICLR 2016  2015), and also bears resemblance to the complex modulus used in the “scattering transform” (Mal- lat, 2011) which has been shown to be robust to smooth deformations. We found that this modiﬁed VGG network not only produced geodesic sequences that were free of most aliasing artifacts, but also linearized these geometric transformations convincingly, as can be seen in the temporal slices of the geodesic (ﬁgure 3, right column). This conﬁrms that, as with the Fourier magnitude and the scattering transform, smooth, quadratic pooling operators are able to linearize local deformations. Unlike the Fourier magnitude however, the locality and hierarchical nature of these representations tailors their invariances to a much more limited set of transformations. Furthermore, this demonstrates the power of geodesics as a visualization tool for understanding learned representations. Not only does this diagnostic report a deﬁciency of a representation (ﬁgure 3, middle column), it also points to the mechanism of this failure, suggesting a simple way to improve the model. This suggests that the VGG network’s performance on object recognition tasks could be improved by substituting max pooling with L2 pooling, and retraining the network to decode this new repre- sentation. Indeed, the added invariance of this representation could enable the network to generalize to new viewing conditions more robustly.  3.2 DISAMBIGUATING SPATIAL SCALE AND NONLINEAR COMPLEXITY WITH GEODESICS  Thus far we have found that a deep representation is able to linearize a range of real-world transfor- mations (ﬁgure 3, right column) whereas a shallow one (e.g., the pixel intensities) is not (ﬁgure 1, middle column). It is unclear, however, whether the improved invariance of the deep representation is due to the spatial extent over which it computes its responses, or its nonlinear complexity. Indeed, as we progress up the hierarchy of a neural network, the effective input region for each unit (the “receptive ﬁeld”) increases in size, simply due to cascaded convolution and subsampled pooling. At the same time, the complexity of the representation increases as a longer sequence of non-linear operations are composed. In order to separate these two effects, we varied the complexity of the representation while keeping the size of the receptive ﬁeld constant. For an artiﬁcial neuron, the receptive ﬁeld quantiﬁes the strength of the connection between a location in the image and that neuron’s activity, and can be measured by computing the magnitude of the gradient of the neuron’s activity with respect to the image. Hence, the receptive ﬁeld of a non-linear neuron changes as a function of the input image. In order to measure the extent of a neuron’s receptive ﬁeld across all images, we averaged the  Figure 4: Even when matched for “receptive ﬁeld” size, shallow representations cannot linearize translations as well as deep ones. From left to right: geodesics generated from 1st, 2nd and 3rd pooling layers, with receptive ﬁeld sizes approximately matched by altering the spatial extent of the L2 pooling (to 36×36, 18×18 and 6×6 pixels, respectively). As the complexity of the representation increases, so does the quality of the corresponding geodesic.  7  2nd pooling layer3rd pooling layer1st pooling layerPublished as a conference paper at ICLR 2016  magnitude of the gradient of its activity over a large set of white noise images. We generalized this method to measuring the receptive ﬁeld of an entire population by computing the average magnitude of the gradient of an entire ‘cortical column’, or set of hidden units at a given location. Using this method, we measured the receptive ﬁeld size of the representation used in our previ- ous experiments (third pooling layer of the VGG network with L2 pooling). We then computed geodesics from shallower representations (ﬁrst and second pooling layers of the VGG network) for which we increased the pooling extent (from 6×6 to 36×36 and 18×18 respectively) in order to match the receptive ﬁeld size of the deep representation. These experiments show that shallower layers, despite being matched for receptive ﬁeld size, are unable to linearize translations as well as deeper ones (ﬁgure 4). Interestingly, we ﬁnd a gradual increase in the quality of the geodesics as the complexity of the representation increases. Hence the curvature of representational geodesics, more than their dimensionality, is essential for capturing these non-linear deformations of the image.  3.3 LINEARIZING NATURAL IMAGE SEQUENCES  Having tested for the modiﬁed VGG network’s ability to linearize simple parametric transforma- tions, we asked whether it can linearize compositions of these transformations that arise in natural image sequences. To explore this, we extracted 5 frames from the movie Melancholia and gener- ated a geodesic from the ﬁrst to the last of these. We ﬁnd that this geodesic smoothly transitions between the two images, and captures much of the true temporal evolution of the video (ﬁgure 5, left and right panels). Relative to the original sequence, the only errors it produces are due to well known problems in motion estimation. A large component of the transformation in the video is an out-of-plane rotation due to the camera panning, creating a composition of translations and dilations throughout the image. In a region of the image with periodic structure (e.g. the woven cane texture of the chair), the motion between the two end frames is ambiguous, because the translation between them exceeds one half of this period. This problem, known as temporal aliasing, can be seen in the temporal slices, which reveal that the back of the chair is smoothly shifted in the opposite direction of the rest of the image (ﬁgure 5, right panel). In motion estimation, this problem is usually solved using a coarse-to-ﬁne approach, in which the motion of the low frequencies is estimated ﬁrst, and used to condition (or initialize) motion estimates derived from higher frequencies. This method can be naturally embedded in our framework by generalizing the nested conditionalization of geodesic  Figure 5: Comparison of geodesic sequences for a pixel-based representation (middle column) and the VGG network with L2 pooling (right column) for a natural movie (left column). Geodesic sequences are generated between the ﬁrst and last frame of the original movie. The pixel intensity representation fails to linearize the sequence, while the VGG network with L2 pooling induces a geodesic that is close to the original movie. The main deﬁciency in this geodesic is due to temporal aliasing, where periodic structure in the image is shifted backwards relatively to the rest of the image (see ﬁrst and second temporal slices).  8  VGG network, L2 poolingground truthpixel intensitiesPublished as a conference paper at ICLR 2016  objective functions (section 2.1). That is, each layer of the network can impose its own geodesic constraints, conditioned on those imposed by deeper layers. This hierarchical construction provides a means of solving the problem of temporal aliasing, and more generally should allow the network to linearize a broader class of transformations.  4 DISCUSSION  The synthesis of geodesic sequences provide a means of visualizing and assessing metric properties of a representation. We have developed a methodology for generating such sequences, and shown that they can be used as a powerful diagnostic tool for evaluating the invariance properties of learned representations. Speciﬁcally, evaluating geodesics enables one to test the “untangling hypothesis” by which hierarchical representations (in particular, biological sensory systems) linearize the action of identity-preserving transformations (DiCarlo & Cox, 2007). Such an “untangled” representation can be linearly decoded, projecting out unwanted variations arising from image-domain renderings to achieve invariant object recognition, or projecting out the orthogonal space, so as to estimate latent rendering variables such as position, lighting, and pose. We used this methodology to test a state-of-the-art recognition network and found that it was unable to linearize basic image transformations such as translation, rotation and dilation. Importantly, these results suggested a simple improvement in the architecture of the network, which in turn enabled it to linearize parametric distortions as well as those found in a natural image sequence. Hence, our geodesic visualization method provides both a means of testing the untangling hypothesis for artiﬁcial networks, as well as a design tool for guiding improvements in learned representations. Alternatively, one could directly test the invariance of a system to a given transformation by exam- ining the variability of responses to objects deformed by the corresponding operation. But such a test relies on establishing a meaningful measure of variability in the representation space, which is undermined by the fact that essentially equivalent representations (e.g., that differ by an invertible afﬁne transformation) can have dramatically different distance or variability measures. As a result, it can be difﬁcult to compare invariance properties of different models, or even across different stages of the same network, with this direct method. The use of geodesic sequences (which are unaffected by invertible afﬁne transformations) avoids this problem by expressing the invariance properties of the representation back in the input (image) domain, where they can be directly compared. Moreover, our method can be applied to arbitrary image pairs, including but not limited to para- metrically transformed images and frames from natural videos. For example, generating geodesics between two arbitrary images from the same object category can reveal whether object identity is an invariant of a representation. An afﬁrmative answer implies that, back in the representation space, all of the images along the geodesic could be correctly identiﬁed using a linear decoder (as is commonly done when reading out the penultimate layer of a deep neural network). Finally, our method suggests a natural extension to hierarchical representations. Our geodesic se- quences were computed by minimizing path length in the pixel domain, conditioned on minimizing path length in a network representation. This process could be applied recursively in a hierarchical representation, minimizing path length at each stage conditioned on minimal path length at higher stages. The resulting non-linear coarse-to-ﬁne computation has the potential to solve well-known problems of temporal aliasing, and to enable hierarchical representations to linearize a much broader class of naturalistic transformations. The resulting image sequences, in turn, could be used to probe and characterize perceptual and physiological aspects of the representation of these transformations in biological visual systems.  ACKNOWLEDGMENTS  This work was supported by the Howard Hughes Medical Institute.  REFERENCES DiCarlo, James J and Cox, David D. Untangling invariant object recognition. Trends in Cognitive  Sciences, 11(8):333–341, August 2007.  9  Published as a conference paper at ICLR 2016  Duda, R., Hart, P., and Stork, D. Pattern classiﬁcation. Wiley, New York, 2001.  Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, Fergus, Rob, and LeCun, Yann. Learning invariant  features through topographic ﬁlter maps. pp. 1605–1612, 2009.  Kingma, Diederik and Ba, Jimmy. Adam: A Method for Stochastic Optimization. arXiv.org, cs.LG,  December 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Imagenet classiﬁcation with deep convolu-  tional neural networks. pp. 1106–1114, 2012.  Mallat, St´ephane. Group Invariant Scattering. arXiv.org, math.FA, January 2011.  Portilla, Javier and Simoncelli, Eero P. A Parametric Texture Model Based on Joint Statistics of  Complex Wavelet Coefﬁcients. International Journal of Computer Vision, 40(1):49–70, 2000.  Saxe, Andrew M, McClelland, James L, and Ganguli, Surya. Exact solutions to the nonlinear dy-  namics of learning in deep linear neural networks. arXiv.org, cs.NE, December 2013.  Simonyan, Karen and Zisserman, Andrew. Very Deep Convolutional Networks for Large-Scale  Image Recognition. arXiv.org, cs.CV, September 2014.  Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian, and Fergus, Rob. Intriguing properties of neural networks. arXiv.org, cs.CV, December 2013.  Vintch, B, Movshon, J A, and Simoncelli, Eero P. A convolutional subunit model for neuronal  responses in macaque V1. The Journal of Neuroscience, 35, 2015.  10  ",
1511.06732,2016,Sequence Level Training with Recurrent Neural Networks,"[""Sequence Level Training with Recurrent Neural Networks\nMarc'Aurelio Ranzato"", 'Sumit Chopra', 'Michael Auli', 'Wojciech Zaremba']",https://arxiv.org/pdf/1511.06732,"6 1 0 2     y a M 6         ]  G L . s c [      7 v 2 3 7 6 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS  Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba Facebook AI Research {ranzato, spchopra, michealauli, wojciech}@fb.com  ABSTRACT  Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.  1  INTRODUCTION  Natural language is the most natural form of communication for humans. It is therefore essential that interactive AI systems are capable of generating text (Reiter & Dale, 2000). A wide variety of applications rely on text generation, including machine translation, video/text summarization, question answering, among others. From a machine learning perspective, text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context. For instance, given an image, generate an appropriate caption or given a sentence in English language, translate it into French. Popular choices for text generation models are language models based on n-grams (Kneser & Ney, 1995), feed-forward neural networks (Morin & Bengio, 2005), and recurrent neural networks (RNNs; Mikolov et al., 2010). These models when used as is to generate text suffer from two major drawbacks. First, they are trained to predict the next word given the previous ground truth words as input. However, at test time, the resulting models are used to generate an entire sequence by predicting one word at a time, and by feeding the generated word back as input at the next time step. This process is very brittle because the model was trained on a different distribution of in- puts, namely, words drawn from the data distribution, as opposed to words drawn from the model distribution. As a result the errors made along the way will quickly accumulate. We refer to this discrepancy as exposure bias which occurs when a model is only exposed to the training data dis- tribution, instead of its own predictions. Second, the loss function used to train these models is at the word level. A popular choice is the cross-entropy loss used to maximize the probability of the next correct word. However, the performance of these models is typically evaluated using discrete metrics. One such metric is called BLEU (Papineni et al., 2002) for instance, which measures the n-gram overlap between the model generation and the reference text. Training these models to di- rectly optimize metrics like BLEU is hard because a) these are not differentiable (Rosti et al., 2011), and b) combinatorial optimization is required to determine which sub-string maximizes them given some context. Prior attempts (McAllester et al., 2010; He & Deng, 2012) at optimizing test metrics were restricted to linear models, or required a large number of samples to work well (Auli & Gao, 2014). This paper proposes a novel training algorithm which results in improved text generation compared to standard models. The algorithm addresses the two issues discussed above as follows. First, while training the generative model we avoid the exposure bias by using model predictions at training time. Second, we directly optimize for our ﬁnal evaluation metric. Our proposed methodology bor-  1  Published as a conference paper at ICLR 2016  rows ideas from the reinforcement learning literature (Sutton & Barto, 1988). In particular, we build on the REINFORCE algorithm proposed by Williams (1992), to achieve the above two objectives. While sampling from the model during training is quite a natural step for the REINFORCE algo- rithm, optimizing directly for any test metric can also be achieved by it. REINFORCE side steps the issues associated with the discrete nature of the optimization by not requiring rewards (or losses) to be differentiable. While REINFORCE appears to be well suited to tackle the text generation problem, it suffers from a signiﬁcant issue. The problem setting of text generation has a very large action space which makes it extremely difﬁcult to learn with an initial random policy. Speciﬁcally, the search space for text generation is of size O(W T ), where W is the number of words in the vo- cabulary (typically around 104 or more) and T is the length of the sentence (typically around 10 to 30). Towards that end, we introduce Mixed Incremental Cross-Entropy Reinforce (MIXER), which is our ﬁrst major contribution of this work. MIXER is an easy-to-implement recipe to make REINFORCE work well for text generation applications. It is based on two key ideas: incremental learning and the use of a hybrid loss function which combines both REINFORCE and cross-entropy (see Sec. 3.2.2 for details). Both ingredients are essential to training with large action spaces. In MIXER, the model starts from the optimal policy given by cross-entropy training (as opposed to a random one), from which it then slowly deviates, in order to make use of its own predictions, as is done at test time. Our second contribution is a thorough empirical evaluation on three different tasks, namely, Text Summarization, Machine Translation and Image Captioning. We compare against several strong baselines, including, RNNs trained with cross-entropy and Data as Demonstrator (DAD) (Bengio et al., 2015; Venkatraman et al., 2015). We also compare MIXER with another simple yet novel model that we propose in this paper. We call it the End-to-End BackProp model (see Sec. 3.1.3 for details). Our results show that MIXER with a simple greedy search achieves much better accuracy compared to the baselines on all the three tasks. In addition we show that MIXER with greedy search is even more accurate than the cross entropy model augmented with beam search at inference time as a post-processing step. This is particularly remarkable because MIXER with greedy search is at least 10 times faster than the cross entropy model with a beam of size 10. Lastly, we note that MIXER and beam search are complementary to each other and can be combined to further improve performance, although the extent of the improvement is task dependent. 1  2 RELATED WORK  Sequence models are typically trained to predict the next word using the cross-entropy loss. At test time, it is common to use beam search to explore multiple alternative paths (Sutskever et al., 2014; Bahdanau et al., 2015; Rush et al., 2015). While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it makes the generation at least k times slower, where k is the number of active paths in the beam (see Sec. 3.1.1 for more details). The idea of improving generation by letting the model use its own predictions at training time (the key proposal of this work) was ﬁrst advocated by Daume III et al. (2009). In their seminal work, the authors ﬁrst noticed that structured prediction problems can be cast as a particular instance of reinforcement learning. They then proposed SEARN, an algorithm to learn such structured predic- tion tasks. The basic idea is to let the model use its own predictions at training time to produce a sequence of actions (e.g., the choice of the next word). Then, a search algorithm is run to determine the optimal action at each time step, and a classiﬁer (a.k.a. policy) is trained to predict that action. A similar idea was later proposed by Ross et al. (2011) in an imitation learning framework. Unfortu- nately, for text generation it is generally intractable to compute an oracle of the optimal target word given the words predicted so far. The oracle issue was later addressed by an algorithm called Data As Demonstrator (DAD) (Venkatraman et al., 2015) and applied for text generation by Bengio et al. (2015), whereby the target action at step k is the k-th action taken by the optimal policy (ground truth sequence) regardless of which input is fed to the system, whether it is ground truth, or the model’s prediction. While DAD usually improves generation, it seems unsatisfactory to force the model to predict a certain word regardless of the preceding words (see sec. 3.1.2 for more details).  1Code available at: https://github.com/facebookresearch/MIXER  2  Published as a conference paper at ICLR 2016  PROPERTY avoids exposure bias end-to-end sequence level  XENT DAD E2E MIXER  No No No  Yes No No  Yes Yes No  Yes Yes Yes  Table 1: Text generation models can be described across three dimensions: whether they suffer from exposure bias, whether they are trained in an end-to-end manner using back-propagation, and whether they are trained to predict one word ahead or the whole sequence.  Finally, REINFORCE has already been used for other applications, such as in computer vision (Mnih et al., 2014; Xu et al., 2015; Ba et al.), and for speech recognition Graves & Jaitly (2014). While they simply pre-trained with cross-entropy loss, we found that the use of a mixed loss and a more gentle incremental learning scheduling to be important for all the tasks we considered.  3 MODELS  The learning algorithms we describe in the following sections are agnostic to the choice of the underlying model, as long as it is parametric. In this work, we focus on Recurrent Neural Networks (RNNs) as they are a popular choice for text generation. In particular, we use standard Elman RNNs (Elman, 1990) and LSTMs (Hochreiter & Schmidhuber, 1997). For the sake of simplicity but without loss of generality, we discuss next Elman RNNs. This is a parametric model that at each time step t, takes as input a word wt ∈ W as its input, together with an internal representation ht. W is the the vocabulary of input words. This internal representation ht is a real-valued vector which encodes the history of words the model has seen so far. Optionally, the RNN can also take as input an additional context vector ct, which encodes the context to be used while generating the output. In our experiments ct is computed using an attentive decoder inspired by Bahdanau et al. (2015) and Rush et al. (2015), the details of which are given in Section 6.2 of the supplementary material. The RNN learns a recursive function to compute ht and outputs the distribution over the next word:  ht+1 = φθ(wt, ht, ct), wt+1 ∼ pθ(w|wt, ht+1) = pθ(w|wt, φθ(wt, ht, ct)).  ht+1 = σ(Mi1(wt) + Mhht + Mcct), ot+1 = Moht+1, wt+1 ∼ softmax(ot+1),  tional parameters used to compute ct. Softmax(x) is a vector whose components are exj /(cid:80)  (1) (2) The parametric expression for pθ and φθ depends on the type of RNN. For Elman RNNs we have: (3) (4) (5) where the parameters of the model θ are the set of matrices {Mo, Mi, Mh, Mc} and also the addi- k exk, and 1(i) is an indicator vector with only the i-th component set to 1 and the rest to 0. We assume the ﬁrst word of the sequence is a special token indicating the beginning of a sequence, denoted by w1 = ∅. All entries of the ﬁrst hidden state h1 are set to a constant value. Next, we are going to introduce both baselines and the model we propose. As we describe these models, it is useful to keep in mind the key characteristics of a text generation system, as outlined in Table 1. There are three dimensions which are important when training a model for text gen- eration: the exposure bias which can adversely affect generation at test time, the ability to fully back-propagate gradients (including with respect to the chosen inputs at each time step), and a loss operating at the sequence level. We will start discussing models that do not possess any of these de- sirable features, and then move towards models that better satisfy our requirements. The last model we propose, dubbed MIXER, has all the desiderata.  3.1 WORD-LEVEL TRAINING  We now review a collection of methodologies used for training text generation models which opti- mize the prediction of only one word ahead of time. We start with the simplest and the most popular method which optimizes the cross-entropy loss at every time step. We then discuss a recently pro- posed modiﬁcation to it which explicitly uses the model predictions during training. We ﬁnish by  3  Published as a conference paper at ICLR 2016  Figure 1: RNN training using XENT (top), and how it is used at test time for generation (bottom). The RNN is unfolded for three time steps in this example. The red oval is a module computing a loss, while the rectangles represent the computation done by the RNN at one step. At the ﬁrst step, all inputs are given. In the remaining steps, the input words are clamped to ground truth at training time, while they are clamped to model predictions (denoted by wg t ) at test time. Predictions are produced by either taking the argmax or by sampling from the distribution over words.  proposing a simple yet novel baseline which uses its model prediction during training and also has the ability to back propagate the gradients through the entire sequence. While these extensions tend to make generation more robust, they still lack explicit supervision at the sequence level.  3.1.1 CROSS ENTROPY TRAINING (XENT)  Cross-entropy loss (XENT) maximizes the probability of the observed sequence according to the model. If the target sequence is [w1, w2, . . . , wT ], then XENT training involves minimizing:  T(cid:89)  T(cid:88)  t=1  p(wt|w1, . . . , wt−1) = −  log p(wt|w1, . . . , wt−1). (6) L = − log p(w1, . . . , wT ) = − log When using an RNN, each term p(wt|w1, . . . , wt−1) is modeled as a parametric function as given in Equation (5). This loss function trains the model to be good at greedily predicting the next word at each time step without considering the whole sequence. Training proceeds by truncated back- propagation through time (Rumelhart et al., 1986) with gradient clipping (Mikolov et al., 2010). Once trained, one can use the model to generate an entire sequence as follows. Let wg word generated by the model at the t-th time step. Then the next word is generated by:  t denote the  t=1  w  wg  t , ht+1).  pθ(w|wg  t+1 = argmax  (7) Notice that, the model is trained to maximize pθ(w|wt, ht+1), where wt is the word in the ground truth sequence. However, during generation the model is used as pθ(w|wg t , ht+1). In other words, during training the model is only exposed to the ground truth words. However, at test time the model has only access to its own predictions, which may not be correct. As a result, during generation the model can potentially deviate quite far from the actual sequence to be generated. Figure 1 illustrates this discrepancy. The generation described by Eq. (7) is a greedy left-to-right process which does not necessarily produce the most likely sequence according to the model, because:  max wt+1  pθ(wt+1|wg  t , ht+1) ≤ max  w1,...,wT  pθ(wt+1|wg  t , ht+1)  The most likely sequence [w1, w2, . . . , wT ] might contain a word wt which is sub-optimal at an intermediate time-step t. This phenomena is commonly known as a search error. One popular way  4  T(cid:89)  t=1  T(cid:89)  t=1  h2=✓(;,h1)h3=✓(w2,h2)p✓(w|;,h1)XENTh1;w2XENTw3p✓(w|w2,h2)h2=✓(;,h1)p✓(w|;,h1)h1;wg2argmaxp✓(w|wg2,h2)wg3h3=✓(wg2,h2)argmaxPublished as a conference paper at ICLR 2016  Figure 2: Illustration of DAD (Bengio et al., 2015; Venkatraman et al., 2015). Training proceeds similar to XENT, except that at each time step we choose with a certain probability whether to take the previous model prediction or the ground truth word. Notice how a) gradients are not back- propagated through the eventual model predictions wg t , and b) the XENT loss always uses as target the next word in the reference sequence, even when the input is wg t .  to reduce the effect of search error is to pursue not only one but k next word candidates at each point. While still approximate, this strategy can recover higher scoring sequences that are often also better in terms of our ﬁnal evaluation metric. This process is commonly know as Beam Search. The downside of using beam search is that it signiﬁcantly slows down the generation process. The time complexity grows linearly in the number of beams k, because we need to perform k forward passes for our network, which is the most time intensive operation. The details of the Beam Search algorithm are described in Section 6.3.  3.1.2 DATA AS DEMONSTRATOR (DAD)  Conventional training with XENT suffers from exposure bias since training uses ground truth words as opposed to model predictions. DAD, proposed in (Venkatraman et al., 2015) and also used in (Bengio et al., 2015) for sequence generation, addresses this issue by mixing the ground truth training data with model predictions. At each time step and with a certain probability, DAD takes as input either the prediction from the model at the previous time step or the ground truth data. Bengio et al. (2015) proposed different annealing schedules for the probability of choosing the ground truth word. The annealing schedules are such that at the beginning, the algorithm always chooses the ground truth words. However, as the training progresses the model predictions are selected more often. This has the effect of making the model somewhat more aware of how it will be used at test time. Figure 2 illustrates the algorithm. A major limitation of DAD is that at every time step the target labels are always selected from the ground truth data, regardless of how the input was chosen. As a result, the targets may not be aligned with the generated sequence, forcing the model to predict a potentially incorrect sequence. For instance, if the ground truth sequence is “I took a long walk” and the model has so far predicted “I took a walk”, DAD will force the model to predict the word “walk” a second time. Finally, gradients are not back-propagated through the samples drawn by the model and the XENT loss is still at the word level. It is not well understood how these problems affect generation.  3.1.3 END-TO-END BACKPROP (E2E)  The novel E2E algorithm is perhaps the most natural and na¨ıve approach approximating sequence level training, which can also be interpreted as a computationally efﬁcient approximation to beam search. The key idea is that at time step t + 1 we propagate as input the top k words predicted at the previous time step instead of the ground truth word. Speciﬁcally, we take the output distribution over words from the previous time step t, and pass it through a k-max layer. This layer zeros all but the k largest values and re-normalizes them to sum to one. We thus have: {it+1,j, vt+1,j}j=1,...,k = k-max pθ(wt+1|wt, ht),  (8) where it+1,j are indexes of the words with k largest probabilities and vt+1,j are their corresponding scores. At the time step t + 1, we take the k largest scoring previous words as input whose con- tributions is weighted by their scores v’s. Smoothing the input this way makes the whole process differentiable and trainable using standard back-propagation. Compared to beam search, this can be interpreted as fusing the k possible next hypotheses together into a single path, as illustrated in Figure 3. In practice we also employ a schedule, whereby we use only the ground truth words at the beginning and gradually let the model use its own top-k predictions as training proceeds.  5  h2=✓(;,h1)p✓(w|;,h1)XENTh1;w2w3samplerwg2w0XENTsamplerwg3w00p✓(w|w0,h2)h3=✓(w0,h2)Published as a conference paper at ICLR 2016  Figure 3: Illustration of the End-to-End BackProp method. The ﬁrst steps of the unrolled sequence (here just the ﬁrst step) are exactly the same as in a regular RNN trained with cross-entropy. How- ever, in the remaining steps the input to each module is a sparse vector whose non-zero entries are the k largest probabilities of the distribution predicted at the previous time step. Errors are back- propagated through these inputs as well.  While this algorithm is a simple way to expose the model to its own predictions, the loss function optimized is still XENT at each time step. There is no explicit supervision at the sequence level while training the model.  3.2 SEQUENCE LEVEL TRAINING  We now introduce a novel algorithm for sequence level training, which we call Mixed Incremental Cross-Entropy Reinforce (MIXER). The proposed method avoids the exposure bias problem, and also directly optimizes for the ﬁnal evaluation metric. Since MIXER is an extension of the REIN- FORCE algorithm, we ﬁrst describe REINFORCE from the perspective of sequence generation.  3.2.1 REINFORCE  In order to apply the REINFORCE algorithm (Williams, 1992; Zaremba & Sutskever, 2015) to the problem of sequence generation we cast our problem in the reinforcement learning (RL) frame- work (Sutton & Barto, 1988). Our generative model (the RNN) can be viewed as an agent, which interacts with the external environment (the words and the context vector it sees as input at every time step). The parameters of this agent deﬁnes a policy, whose execution results in the agent pick- ing an action. In the sequence generation setting, an action refers to predicting the next word in the sequence at each time step. After taking an action the agent updates its internal state (the hid- den units of RNN). Once the agent has reached the end of a sequence, it observes a reward. We can choose any reward function. Here, we use BLEU (Papineni et al., 2002) and ROUGE-2 (Lin & Hovy, 2003) since these are the metrics we use at test time. BLEU is essentially a geometric mean over n-gram precision scores as well as a brevity penalty (Liang et al., 2006); in this work, we consider up to 4-grams. ROUGE-2 is instead recall over bi-grams. Like in imitation learning, we have a training set of optimal sequences of actions. During training we choose actions according to the current policy and only observe a reward at the end of the sequence (or after maximum sequence length), by comparing the sequence of actions from the current policy against the optimal action sequence. The goal of training is to ﬁnd the parameters of the agent that maximize the expected reward. We deﬁne our loss as the negative expected reward:  pθ(wg  1, . . . , wg  T )r(wg  1, . . . , wg  T ) = −E  [wg  1 ,...wg  T ]∼pθ r(wg  1, . . . , wg  T ),  (9)  (cid:88)  Lθ = −  wg  1 ,...,wg  T  where wg n is the word chosen by our model at the n-th time step, and r is the reward associated with the generated sequence. In practice, we approximate this expectation with a single sample from the distribution of actions implemented by the RNN (right hand side of the equation above and Figure 9 of Supplementary Material). We refer the reader to prior work (Zaremba & Sutskever, 2015; Williams, 1992) for the full derivation of the gradients. Here, we directly report the partial derivatives and their interpretation. The derivatives w.r.t. parameters are:  ∂Lθ ∂θ  =  ∂Lθ ∂ot  ∂ot ∂θ  (10)  (cid:88)  t  6  h2=✓(;,h1)p✓(w|;,h1)XENTh1;w2w3XENTtop-kw01,...,kp✓(w|w01,...,k,h2)w001,...,kh3=✓(w01,...,k,h2)top-kPublished as a conference paper at ICLR 2016  where ot is the input to the softmax. The gradient of the loss Lθ with respect to ot is given by:  ∂Lθ ∂ot  = (r(wg  1, . . . , wg  T ) − ¯rt+1)(cid:0)pθ(wt+1|wg  t+1)(cid:1) ,  t , ht+1, ct) − 1(wg  (11)  where ¯rt+1 is the average reward at time t + 1. The interpretation of this weight update rule is straightforward. While Equation 10 is standard back- propagation (a.k.a. chain rule), Equation 11 is almost exactly the same as the gradient of a multi- class logistic regression classiﬁer. In logistic regression, the gradient is the difference between the prediction and the actual 1-of-N representation of the target word:  ∂LXENT  θ ∂ot  = pθ(wt+1|wt, ht+1, ct) − 1(wt+1)  Therefore, Equation 11 says that the chosen word wg t+1 acts like a surrogate target for our output distribution, pθ(wt+1|wg t , ht+1, ct) at time t. REINFORCE ﬁrst establishes a baseline ¯rt+1, and then either encourages a word choice wg t+1 if r > ¯rt+1, or discourages it if r < ¯rt+1. The actual derivation suggests that the choice of this average reward ¯rt is useful to decrease the variance of the gradient estimator since in Equation 9 we use a single sample from the distribution of actions. In our implementation, the baseline ¯rt is estimated by a linear regressor which takes as input the hidden states ht of the RNN. The regressor is an unbiased estimator of future rewards since it only uses past information. The parameters of the regressor are trained by minimizing the mean squared loss: ||¯rt − r||2. In order to prevent feedback loops, we do not backpropagate this error through the recurrent network (Zaremba & Sutskever, 2015). REINFORCE is an elegant algorithm to train at the sequence level using any user-deﬁned reward. In this work, we use BLEU and ROUGE-2 as reward, however one could just as easily use any other metric. When presented as is, one major drawback associated with the algorithm is that it assumes a random policy to start with. This assumption can make the learning for large action spaces very challenging. Unfortunately, text generation is such a setting where the cardinality of the action set is in the order of 104 (the number of words in the vocabulary). This leads to a very high branching factor where it is extremely hard for a random policy to improve in any reasonable amount of time. In the next section we describe the MIXER algorithm which addresses these issues, better targeting text generation applications.  3.2.2 MIXED INCREMENTAL CROSS-ENTROPY REINFORCE (MIXER)  The MIXER algorithm borrows ideas both from DAGGER (Ross et al., 2011) and DAD (Venkatra- man et al., 2015; Bengio et al., 2015) and modiﬁes the REINFORCE appropriately. The ﬁrst key idea is to change the initial policy of REINFORCE to make sure the model can effectively deal with the large action space of text generation. Instead of starting from a poor random policy and training the model to converge towards the optimal policy, we do the exact opposite. We start from the optimal policy and then slowly deviate from it to let the model explore and make use of its own predictions. We ﬁrst train the RNN with the cross-entropy loss for N XENT epochs using the ground truth sequences. This ensures that we start off with a much better policy than random because now the model can focus on a good part of the search space. This can be better understood by comparing the perplexity of a language model that is randomly initialized versus one that is trained. Perplexity is a measure of uncertainty of the prediction and, roughly speaking, it corresponds to the average number of words the model is ‘hesitating’ about when making a prediction. A good language model trained on one of our data sets has perplexity of 50, whereas a random model is likely to have perplexity close to the size of the vocabulary, which is about 10, 000. The second idea is to introduce model predictions during training with an annealing schedule in order to gradually teach the model to produce stable sequences. Let T be the length of the sequence. After the initial N XENT epochs, we continue training the model for N XE+R epochs, such that, for every sequence we use the XENT loss for the ﬁrst (T − ∆) steps, and the REINFORCE algorithm for the remaining ∆ steps. In our experiments ∆ is typically set to two or three. Next we anneal the number of steps for which we use the XENT loss for every sequence to (T − 2∆) and repeat the training for another N XE+R epochs. We repeat this process until only REINFORCE is used to train the whole sequence. See Algorithm 1 for the pseudo-code.  7  Published as a conference paper at ICLR 2016  Figure 4: Illustration of MIXER. In the ﬁrst s unrolling steps (here s = 1), the network resembles a standard RNN trained by XENT. In the remaining steps, the input to each module is a sample from the distribution over words produced at the previous time step. Once the end of sentence is reached (or the maximum sequence length), a reward is computed, e.g., BLEU. REINFORCE is then used to back-propagate the gradients through the sequence of samplers. We employ an annealing schedule on s, starting with s equal to the maximum sequence length T and ﬁnishing with s = 1.  Data: a set of sequences with their corresponding context. Result: RNN optimized for generation. Initialize RNN at random and set NXENT, NXE+R and ∆; for s = T , 1, −∆ do if s == T then  train RNN for NXENT epochs using XENT only;  else  end  end  train RNN for NXE+R epochs. Use XENT loss in the ﬁrst s steps, and REINFORCE (sampling from the model) in the remaining T − s steps;  Algorithm 1: MIXER pseudo-code.  We call this algorithm Mixed Incremental Cross-Entropy Reinforce (MIXER) because we combine both XENT and REINFORCE, and we use incremental learning (a.k.a. curriculum learning). The overall algorithm is illustrated in Figure 4. By the end of training, the model can make effective use of its own predictions in-line with its use at test time.  4 EXPERIMENTS  In all our experiments, we train conditional RNNs by unfolding them up to a certain maximum length. We chose this length to cover about 95% of the target sentences in the data sets we consider. The remaining sentences are cropped to the chosen maximum length. For training, we use stochastic gradient descent with mini-batches of size 32 and we reset the hidden states at the beginning of each sequence. Before updating the parameters we re-scale the gradients if their norm is above 10 (Mikolov et al., 2010). We search over the values of hyper-parameter, such as the initial learning rate, the various scheduling parameters, number of epochs, etc., using a held-out validation set. We then take the model that performed best on the validation set and compute BLEU or ROUGE score on the test set. In the following sections we report results on the test set only. Greedy generation is performed by taking the most likely word at each time step. 2  4.1 TEXT SUMMARIZATION  We consider the problem of abstractive summarization where, given a piece of “source” text, we aim at generating its summary (the “target” text) such that its meaning is intact. The data set we use to train and evaluate our models consists of a subset of the Gigaword corpus (Graff et al., 2003) as described in Rush et al. (2015). This is a collection of news articles taken from different sources over the past two decades. Our version is organized as a set of example pairs, where each pair is composed of the ﬁrst sentence of a news article (the source sentence) and its corresponding headline (the target sentence). We pre-process the data in the same way as in (Rush et al., 2015), which consists of  2Code available at: https://github.com/facebookresearch/MIXER  8  h2=✓(;,h1)p✓(w|;,h1)XENTh1;w2samplersamplerwg2p✓(w|wg2,h2)wg3BLEU[wg1,...,wgT][w1,...,wT]h3=✓(wg2,h2)Published as a conference paper at ICLR 2016  lower-casing and replacing the infrequent words with a special token denoted by “<unk>”. After pre-processing there are 12321 unique words in the source dictionary and 6828 words in the target dictionary. The number of sample pairs in the training, validation and test set are 179414, 22568, and 22259 respectively. The average sequence length of the target headline is about 10 words. We considered sequences up to 15 words to comply with our initial constraint of covering at least 95% of the data. Our generative model is a conditional Elman RNN (Equation 3) with 128 hidden units, where the conditioning vector ct is provided by a convolutional attentive encoder, similar to the one described in Section 3.2 of Rush et al. (2015) and inspired by Bahdanau et al. (2015). The details of our attentive encoder are mentioned in Section 6.2 of the Supplementary Material. We also tried LSTMs as our generative model for this task, however it did not improve performance. We conjecture this is due to the fact that the target sentences in this data set are rather short.  4.2 MACHINE TRANSLATION  For the translation task, our generative model is an LSTM with 256 hidden units and it uses the same attentive encoder architecture as the one used for summarization. We use data from the German- English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014). The corpus consists of sentence-aligned subtitles of TED and TEDx talks. We pre-process the training data using the tokenizer of the Moses toolkit (Koehn et al., 2007) and remove sentences longer than 50 words as well as casing. The training data comprises of about 153000 sentences where the average English sentence is 17.5 words long and the average German sentence is 18.5 words long. In order to retain at least 95% of this data, we unrolled our RNN for 25 steps. Our validation set comprises of 6969 sentence pairs which was taken from the training data. The test set is a concatenation of dev2010, dev2012, tst2010, tst2011 and tst2012 which results in 6750 sentence pairs. The English dictionary has 22822 words while the German has 32009 words.  4.3  IMAGE CAPTIONING  For the image captioning task, we use the MSCOCO dataset (Lin et al., 2014). We use the entire training set provided by the authors, which consists of around 80k images. We then took the orig- inal validation set (consisting of around 40k images) and randomly sampled (without replacement) 5000 images for validation and another 5000 for test. There are 5 different captions for each im- age. At training time we sample one of these captions, while at test time we report the maximum BLEU score across the ﬁve captions. The context is represented by 1024 features extracted by a Convolutional Neural Network (CNN) trained on the Imagenet dataset (Deng et al., 2009); we do not back-propagate through these features. We use a similar experimental set up as described in Bengio et al. (2015). The RNN is a single layer LSTM with 512 hidden units and the image features are provided to the generative model as the ﬁrst word in the sequence. We pre-process the captions by lower-casing all words and replacing all the words which appear less than 3 times with a special token “<unk>”. As a result the total number of unique words in our dataset is 10012. Keeping in mind the 95% rule, we unroll the RNN for 15 steps.  4.4 RESULTS  In order to validate MIXER, we compute BLEU score on the machine translation and image cap- tioning task, and ROUGE on the summarization task. The input provided to the system is only the context and the beginning of sentence token. We apply the same protocol to the baseline methods as well. The scores on the test set are reported in Figure 5. We observe that MIXER produces the best generations and improves generation over XENT by 1 to 3 points across all the tasks. Unfortunately the E2E approach did not prove to be very effective. Training at the sequence level and directly optimizing for testing score yields better generations than turning a sequence of discrete decisions into a differentiable process amenable to standard back-propagation of the error. DAD is usually better than the XENT, but not as good as MIXER. Overall, these experiments demonstrate the importance of optimizing for the metric used at test time. In summarization for instance, XENT and MIXER trained with ROUGE achieve a poor performance in terms of BLEU (8.16 and 5.80 versus 9.32 of MIXER trained with BLEU); likewise, MIXER  9  Published as a conference paper at ICLR 2016  TASK  summarization translation image captioning  XENT DAD 12.18 13.01 17.74 20.12 28.16 27.8  E2E MIXER 12.78 17.77 26.42  16.22 20.73 29.16  Figure 5: Left: BLEU-4 (translation and image captioning) and ROUGE-2 (summarization) scores using greedy generation. Right: Relative gains produced by DAD, E2E and MIXER on the three tasks. The relative gain is computed as the ratio between the score of a model over the score of the reference XENT model on the same task. The horizontal line indicates the performance of XENT.  Figure 6: Test score (ROUGE for summarization and BLEU for machine translation and image captioning) as a function of the number of hypotheses k in the beam search. Beam search always improves performance, although the amount depends on the task. The dark line shows the perfor- mance of MIXER using greedy generation, while the gray line shows MIXER using beam search with k = 10.  trained with BLEU does not achieve as good ROUGE score as a MIXER optimizing ROUGE at training time as well (15.1 versus 16.22, see also Figure 8 in Supplementary Material). Next, we experimented with beam search. The results in Figure 6 suggest that all methods, including MIXER, improve the quality of their generation by using beam search. However, the extent of the improvement is very much task dependent. We observe that the greedy performance of MIXER (i.e., without beam search) cannot be matched by baselines using beam search in two out of the three tasks. Moreover, MIXER is several times faster since it relies only on greedy search. It is worth mentioning that the REINFORCE baseline did not work for these applications. Explo- ration from a random policy has little chance of success. We do not report it since we were never able to make it converge within a reasonable amount of time. Using the hybrid XENT-REINFORCE loss without incremental learning is also insufﬁcient to make training take off from random chance. In order to gain some insight on what kind of schedule works, we report in Table 2 of Supplementary Material the best values we found after grid search over the hyper-parameters of MIXER. Finally, we report some anecdotal examples of MIXER generation in Figure 7 of Supplementary Material.  5 CONCLUSIONS  Our work is motivated by two major deﬁciencies in training the current generative models for text generation: exposure bias and a loss which does not operate at the sequence level. While Rein- forcement learning can potentially address these issues, it struggles in settings when there are very large action spaces, such as in text generation. Towards that end, we propose the MIXER algorithm, which deals with these issues and enables successful training of reinforcement learning models for text generation. We achieve this by replacing the initial random policy with the optimal policy of a cross-entropy trained model and by gradually exposing the model more and more to its own predictions in an incremental learning framework.  10  Published as a conference paper at ICLR 2016  Our results show that MIXER outperforms three strong baselines for greedy generation and it is very competitive with beam search. The approach we propose is agnostic to the underlying model or the form of the reward function. In future work we would like to design better estimation techniques for the average reward ¯rt, because poor estimates can lead to slow convergence of both REINFORCE and MIXER. Finally, our training algorithm relies on a single sample while it would be interesting to investigate the effect of more comprehensive search methods at training time.  ACKNOWLEDGMENTS  The authors would like to thank David Grangier, Tomas Mikolov, Leon Bottou, Ronan Collobert and Laurens van der Maaten for their insightful comments. We also would like to thank Alexander M. Rush for his help in preparing the data set for the summarization task and Sam Gross for providing the image features.  REFERENCES Auli, M. and Gao, J. Decoder integration and expected bleu training for recurrent neural network  language models. In Proc. of ACL, June 2014.  Ba, J.L., Mnih, V., and Kavukcuoglu, K. Multiple object recognition with visual attention.  Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and  translate. In ICLR, 2015.  Bengio, S., Vinyals, O., Jaitly, N., and Shazeer, N. Scheduled sampling for sequence prediction with  recurrent neural networks. In NIPS, 2015.  Cettolo, M., Niehues, J., St¨uker, S., Bentivogli, L., , and Federico, M. Report on the 11th iwslt  evaluation campaign. In Proc. of IWSLT, 2014.  Daume III, H., Langford, J., and Marcu, D. Search-based structured prediction as classiﬁcation.  Machine Learning Journal, 2009.  Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., and Fei-Fei, L. Imagenet: a large-scale hierarchical  image database. In IEEE Conference on Computer Vision and Pattern Recognition, 2009.  Elman, Jeffrey L. Finding structure in time. Cognitive Science, 14(2):179–211, 1990.  Graff, D., Kong, J., Chen, K., and Maeda, K. English gigaword. Technical report, 2003.  Graves, A. and Jaitly, N. Towards end-to-end speech recognition with recurrent neural networks. In  ICML, 2014.  He, X. and Deng, L. Maximum expected bleu training of phrase and lexicon translation models. In  ACL, 2012.  Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 9(8):1735–1780,  1997.  Kneser, Reinhard and Ney, Hermann. Improved backing-off for M-gram language modeling. In Proc. of the International Conference on Acoustics, Speech, and Signal Processing, pp. 181–184, May 1995.  Koehn, Philipp, Hoang, Hieu, Birch, Alexandra, Callison-Burch, Chris, Federico, Marcello, Bertoldi, Nicola, Cowan, Brooke, Shen, Wade, Moran, Christine, Zens, Richard, Dyer, Chris, Bojar, Ondrej, Constantin, Alexandra, and Herbst, Evan. Moses: Open source toolkit for statisti- cal machine translation. In Proc. of ACL Demo and Poster Sessions, Jun 2007.  Liang, Percy, Bouchard-Cˆot´e, Alexandre, Taskar, Ben, and Klein, Dan. An end-to-end discrimina-  tive approach to machine translation. In acl-coling2006, pp. 761–768, Jul 2006.  Lin, C.Y. and Hovy, E.H. Automatic evaluation of summaries using n-gram co-occurrence statistics.  In HLT-NAACL, 2003.  11  Published as a conference paper at ICLR 2016  Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C.L.  Microsoft coco: Common objects in context. Technical report, 2014.  McAllester, D., Hazan, T., and Keshet, J. Direct loss minimization for structured prediction. In  NIPS, 2010.  Mikolov, T., Karaﬁt, M., Burget, L., Cernock, J., and Khudanpur, S. Recurrent neural network based  language model. In INTERSPEECH, 2010.  Mnih, V., Heess N., Graves, A., and Kavukcuoglu, K. Recurrent models of visual attention.  NIPS, 2014.  In  Morin, F. and Bengio, Y. Hierarchical probabilistic neural network language model. In AISTATS,  2005.  Papineni, K., Roukos, S., Ward, T., and Zhu, W.J. Bleu: a method for automatic evaluation of  machine translation. In ACL, 2002.  Reiter, E. and Dale, R. Building natural language generation systems. Cambridge university press,  2000.  Ross, S., Gordon, G.J., and Bagnell, J.A. A reduction of imitation learning and structured prediction  to no-regret online learning. In AISTATS, 2011.  Rosti, Antti-Veikko I, Zhang, Bing, Matsoukas, Spyros, and Schwartz, Richard. Expected bleu In Proc. of  training for graphs: Bbn system description for wmt11 system combination task. WMT, pp. 159–165. Association for Computational Linguistics, July 2011.  Rumelhart, D.E., Hinton, G.E., and Williams, R.J. Learning internal representations by back-  propagating errors. Nature, 323:533–536, 1986.  Rush, A.M., Chopra, S., and Weston, J. A neural attention model for abstractive sentence summa-  rization. In EMNLP, 2015.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc. Sequence to sequence learning with neural networks.  In Proc. of NIPS, 2014.  Sutton, R.S. and Barto, A.G. Reinforcement learning: An introduction. MIT Press, 1988.  Venkatraman, A., Hebert, M., and Bagnell, J.A. Improving multi-step prediction of learned time  series models. In AAAI, 2015.  Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement  learning. Machine Learning, 8:229–256, 1992.  Xu, X., Ba, J., Kiros, R., Courville, A., Salakhutdinov, R., Zemel, R., and Bengio, Y. Show, attend  and tell: Neural image caption generation with visual attention. In ICML, 2015.  Zaremba, W. and Sutskever, I. Reinforcement learning neural turing machines. Technical report,  2015.  12  Published as a conference paper at ICLR 2016  6 SUPPLEMENTARY MATERIAL  6.1 EXPERIMENTS  6.1.1 QUALITATIVE COMPARISON  CONTEXT:  a chinese government official on sunday dismissed reports that the government was delaying the issuing of third generation -lrb- #g -rrb- mobile phone licenses in order to give a developing <unk> system an advantage  GROUND TRUTH: foreign phone operators to get equal access to china ’s #g market XENT: DAD: E2E: MIXER:  china dismisses report of #g mobile phone phone china denies <unk> <unk> mobile phone licenses china ’s mobile phone licenses delayed china official dismisses reports of #g mobile licenses  CONTEXT:  greece risks bankruptcy if it does not take radical extra measures to fix its finances , prime minister george papandreou warned on tuesday , saying the country was in a ‘‘ wartime situation  GROUND TRUTH: greece risks bankruptcy without radical action XENT: DAD: E2E: MIXER:  greece warns <unk> measures to <unk> finances greece says no measures to <unk> <unk> greece threatens to <unk> measures to <unk> finances greece does not take radical measures to <unk> deficit  CONTEXT:  the indonesian police were close to identify the body parts resulted from the deadly explosion in front of the australian embassy by the dna test , police chief general <unk> <unk> said on wednesday  GROUND TRUTH: indonesian police close to <unk> australian embassy bomber XENT: DAD: E2E: MIXER:  indonesian police close to <unk> indonesian police close to <unk> indonesian police close to monitor deadly australia indonesian police close to <unk> parts of australian embassy  CONTEXT:  hundreds of catholic and protestant youths attacked security forces with <unk> bombs in a flashpoint area of north belfast late thursday as violence erupted for the second night in a row , police said  GROUND TRUTH: second night of violence erupts in north belfast XENT: DAD: E2E: MIXER:  urgent hundreds of catholic and <unk> <unk> in <unk> hundreds of belfast <unk> <unk> in n. belfast hundreds of catholic protestant , <unk> clash with <unk> hundreds of catholic <unk> attacked in north belfast  CONTEXT:  uganda ’s lord ’s resistance army -lrb- lra -rrb- rebel leader joseph <unk> is planning to join his commanders in the ceasefire area ahead of talks with the government , ugandan army has said  GROUND TRUTH: rebel leader to move to ceasefire area XENT: DAD: E2E: MIXER:  uganda ’s <unk> rebel leader to join ceasefire ugandan rebel leader to join ceasefire talks ugandan rebels <unk> rebel leader ugandan rebels to join ceasefire in <unk>  CONTEXT:  a russian veterinary official reported a fourth outbreak of dead domestic poultry in a suburban moscow district sunday as experts tightened <unk> following confirmation of the presence of the deadly h#n# bird flu strain  GROUND TRUTH: tests confirm h#n# bird flu strain in # <unk> moscow <unk> XENT: DAD: E2E: MIXER:  russian official reports fourth flu in <unk> bird flu outbreak in central china russian official official says outbreak outbreak outbreak in <unk> russian official reports fourth bird flu  CONTEXT:  a jewish human rights group announced monday that it will offer <unk> a dlrs ##,### reward for information that helps them track down those suspected of participating in nazi atrocities during world war ii  GROUND TRUTH: jewish human rights group offers reward for information on nazi suspects in lithuania XENT: DAD: E2E: MIXER:  jewish rights group announces <unk> to reward for war during world war rights group announces <unk> dlrs dlrs dlrs reward jewish rights group offers reward for <unk> jewish human rights group to offer reward for <unk>  CONTEXT:  a senior u.s. envoy reassured australia ’s opposition labor party on saturday that no decision had been made to take military action against iraq and so no military assistance had been sought from australia  GROUND TRUTH: u.s. envoy meets opposition labor party to discuss iraq XENT: DAD: E2E: MIXER:  australian opposition party makes progress on military action against iraq australian opposition party says no military action against iraq us envoy says no decision to take australia ’s labor u.s. envoy says no decision to military action against iraq  CONTEXT:  republican u.s. presidential candidate rudy giuliani met privately wednesday with iraqi president jalal talabani and indicated that he would keep a u.s. presence in iraq for as long as necessary , campaign aides said  GROUND TRUTH: giuliani meets with iraqi president , discusses war XENT: DAD: E2E: MIXER:  <unk> meets with president of iraqi president republican presidential candidate meets iraqi president u.s. president meets with iraqi president u.s. presidential candidate giuliani meets with iraqi president  Figure 7: Examples of greedy generations after conditioning on sentences from the test summariza- tion dataset. The ”<unk>” token is produced by our tokenizer and it replaces rare words.  13  Published as a conference paper at ICLR 2016  6.1.2 HYPERPARAMETERS  TASK  summarization machine translation image captioning  N XENT N XE+R ∆ 20 2 3 25 20 2  5 5 5  Table 2: Best scheduling parameters found by hyper-parameter search of MIXER.  6.1.3 RELATIVE GAINS  Figure 8: Relative gains on summarization with respect to the XENT baseline. Left: relative BLEU score. Right: relative ROUGE-2. The models are: DAD, E2E, MIXER trained for the objective used at test time (method proposed in this paper), and MIXER trained with a different metric. When evaluating for BLEU, the last column on the left reports the evaluation of MIXER trained using ROUGE-2. When evaluating for ROUGE-2, the last column on the right reports the evaluation of MIXER trained using BLEU.  w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w...  w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w... w...  w000  w001  w010  w011  w100  w101  w110  w111  w000 w000  w001 w001  w010 w010  w011 w011  w100 w100  w101 w101  w110 w110  w111 w111  w00  w01  w10  w11  w00  w01  w10  w11  w0  w1  w  w0  w1  w  Training with exposure bias  Training in expectation (Reinforce)  Figure 9: Search space for the toy case of a binary vocabulary and sequences of length 4. The trees represent all the 24 possible sequences. The solid black line is the ground truth sequence. (Left) Greedy training such as XENT optimizes only the probability of the next word. The model may consider choices indicated by the green arrows, but it starts off from words taken from the ground truth sequence. The model experiences exposure bias, since it sees only words branching off the ground truth path; (Right) REINFORCE and MIXER optimize over all possible sequences, using the predictions made by the model itself. In practice, the model samples only a single path indicated by the blue solid line. The model does not suffer from exposure bias; the model is trained as it is tested.  14  TimePublished as a conference paper at ICLR 2016  6.2 THE ATTENTIVE ENCODER  Here we explain in detail how we generate the conditioning vector ct for our RNN using the source sentence and the current hidden state ht. Let us denote by s the source sentence which is composed of a sequence of M words s = [w1, . . . , wM ]. With a slight overload of notation let wi also denote the d dimensional learnable embedding of the i-th word (wi ∈ Rd). In addition the position i of the word wi is also associated with a learnable embedding li of size d (li ∈ Rd). Then the full embedding for the i-th word in the input sentence is given by ai = wi + li. In order for the embeddings to capture local context, we associate an aggregate embedding zi to each word in the source sentence. In particular for a word in the i-th position, its aggregate embedding zi is computed by taking a window of q consecutive words centered at position i and averaging the embeddings of all the words in this window. More precisely, the aggregate embedding zi is given by:  ai+h.  (12)  In our experiments the width q was set to 5. In order to account for the words at the two boundaries of the input sentence we ﬁrst pad the sequence on both sides with dummy words before computing the aggregate vectors zis. Given these aggregate vectors of words, we compute the context vector ct (the ﬁnal output of the encoder) as:  q/2(cid:88)  h=−q/2  zi =  1 q  M(cid:88)  ct =  αj,twj,  where the weights αj,t are computed as  αj,t =  6.3 BEAM SEARCH ALGORITHM  j=1  (cid:80)M  exp(zj · ht) i=1 exp(zi · ht)  .  (13)  (14)  Equation 7 always chooses the highest scoring next word candidate at each time step. At test time we can reduce the effect of search error by pursuing not only one but k next word candidates at each point, which is commonly known as beam search. While still approximate, this strategy can recover higher scoring sequences that are often also better in terms of our ﬁnal evaluation metric. The algorithm maintains the k highest scoring partial sequences, where k is a hyper-parameter. Setting k = 1 reduces the algorithm to a greedy left-to-right search (Eq. (7)).  1, wg  2, . . . , wg n]  Input: model pθ, beam size k Result: sequence of words [wg empty heaps {Ht}t=1,...T ; an empty hidden state vector h1; H1.push(1, [[∅], h1]); for t ← 1 to T − 1 do  for i ← 1 to min(k, #Ht) do  (p, [[w1, w2, . . . , wt], h]) ← Ht.pop(); h(cid:48) = φθ(w, h) ; for w(cid:48) ← k-most likely words w(cid:48) from pθ(w(cid:48)|wt, h) do  p(cid:48) = p ∗ pθ(w(cid:48)|w, h); Ht+1.push(p(cid:48), [[w1, w2, . . . , wt, w(cid:48)], h(cid:48)]);  end  end  end (p, [[w1, w2, . . . , wT ], h]) ← HT .pop(); Output: [w1, w2, . . . , wT ]  Algorithm 2: Pseudo-code of beam search with beam size k.  15  Published as a conference paper at ICLR 2016  6.4 NOTES  The current version of the paper updates the ﬁrst version uploaded on arXiv as follows:  strate that MIXER can work with any metric.  • on the summarization task, we report results using both ROUGE-2 and BLEU to demon- • on machine translation and image captioning we use LSTM instead of Elman RNN to • BLEU is evaluated using up to 4-grams, and it is computed at the corpus level (except in the image captioning case) as this seems the most common practice in the summarization and machine translation literature.  demonstrate the MIXER can work with any underlying parametric model.  • we have added several references as suggested by our reviewers • we have shortened the paper by moving some content to the Supplementary Material.  16  ",
1511.05666,2016,Super-resolution with deep convolutional sufficient statistics,"['Super-resolution with deep convolutional sufficient statistics\nJoan Bruna', 'Pablo Sprechmann', 'Yann Lecun']",https://arxiv.org/pdf/1511.05666,"6 1 0 2    r a  M 1         ]  V C . s c [      4 v 6 6 6 5 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  SUPER-RESOLUTION WITH DEEP CONVOLUTIONAL SUFFICIENT STATISTICS  Joan Bruna Department of Statistics University of California, Berkeley joan.bruna@berkeley.edu  Pablo Sprechmann Courant Institute New York University pablo@cims.nyu.edu  Yann LeCun Facebook, Inc., & Courant Institute New York University yann@cims.nyu.edu  ABSTRACT  Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low- resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the- mean problem, result of their inability to capture the multi-modality of this condi- tional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufﬁcient statistics are given by deep Convolutional Neural Networks (CNN). The features computed by the network are stable to local defor- mation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufﬁcient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly infor- mative. The ﬁlters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to ﬁne-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed framework in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension.  1  INTRODUCTION  Single-image super resolution aims to construct a high-resolution image from a single low-resolution input. Traditionally, these type of inverse problems in image and signal processing are approached by constructing models with appropriate priors to regularize the signal estimation. Classic Tychonov regularization considers Sobolev spaces as a means to characterize the regularity of the desired solutions. While numerically very efﬁcient, such priors are not appropriate in most applications, since typical signals are not globally smooth. More recently, data-driven approaches have lead to very successful results in the context of inverse problems, as training data can be used as a means to adjust the prior to the empirical distribution. Most methods in the literature fall into two main categories: non-parametric and parametric. The former aim to obtain the co-occurrence prior between the high-resolution and low-resolution local structures from an external training database. The high-resolution instance is produced by copying small patches from the most similar examples found in the training set (Freeman et al. (2002)). Parametric models with sparse regularization have enjoyed great success in this problem (Elad & Aharon (2006); Yang et al. (2008)), thanks to their capacity to capture local regularity with efﬁcient convex optimization methods (Mairal et al. (2009)). Recent works have shown that better results can be obtained when the models are tuned on recovery performance rather than on data ﬁtting (Yang et al. (2012); Mairal et al. (2012)). However, the non-explicit form of the estimator leads to a more difﬁcult bi-level optimization problem (Mairal et al. (2012)). This difﬁculty can be mitigated by learning approximate inference mechanisms that take advantage of the speciﬁcs of the data on which they are applied (Gregor & LeCun (2010); Sprechmann et al. (2015)).  1  Published as a conference paper at ICLR 2016  With the mindset of maximizing recovery performance, several works have proposed to replace the inference step by a generic neural network architecture having enough capacity to perform non- linear regression (Dong et al. (2014); Cui et al. (2014)). This approach closely relates with the auto-encoders paradigm extensively studied in the representation learning literature. Auto-encoders learn mid level-features using an information-preservation criterion, maximizing recoverability of the training signals from the extracted features (Goodfellow et al. (2009)). In the context of super resolution, the systems are trained as to minimize a measure of ﬁtness between the ground truth signal and its reconstruction from the distorted observation. There is an intrinsic limitation in this approach: the mapping being approximated is highly unstable (or even multi- valued). The biggest challenge to overcome is the design of an objective function that encourages the system to discover meaningful regularities and produce sensible estimations. The most popular objective is the squared Euclidean distance in the signal domain, despite the fact it is not correlated to good perceptual quality, as it is not stable to small deformations and uncertainty leads to linear blurring, a well known effect commonly referred as regression to the mean. In order to model the multi-modality intrinsic in many inverse problems, it is thus necessary to replace point estimates by inferential models. A popular approach is to use variational inference over a certain mixture graphical model. If x ∈ RM are the low-resolution observations and y ∈ RN (commonly with N (cid:29) M) are the target, high-resolution samples, we might attempt to model the conditional distribution p(y | x) via a collection of hidden variables, in order to account for the multi- modality. Variational autoencoders (Kingma & Welling (2013)) and other recent generative models showed that one can encode relatively complex geometrical properties by mapping separable latent random variables with a deep neural network (Bengio et al. (2013); Goodfellow et al. (2014); Denton et al. (2015); Gregor et al. (2015); Sohl-Dickstein et al. (2015); Jimenez-Rezende & Mohamed (2015)) . However, a question remains open: can these models scale and account for highly non-Gaussian, stationary processes that form textured regions in images and sounds, without incurring in an ex- plosion on the number of hidden variables? In this paper, we take a different approach. Instead  of considering a mixture model of the form p(y|x) =(cid:82) p(y|x, h)p(h|x)dh, we propose to learn a  non-linear representation of the target signal Ψ(y) that expresses this multi-modal distribution in terms of a Gibbs density:  p(y|x) ∝ exp(−(cid:107)Φ(x) − Ψ(y)(cid:107)2),  (1)  where Φ : RN → RP and Ψ : RM → RP are non-linear mappings that take, respectively, ob- servations and targets to a common high-dimensional space of dimension, P . In other words, we propose to learn a collection of non-linear sufﬁcient statistics Ψ(y) that minimize the uncertainty of y given x while being highly informative. In order for (1) to be an efﬁcient conditional model, the features Ψ(y) must therefore reduce the uninformative variability while preserving discriminative information. We argue that a good parametric model for such sufﬁcient statistics are given by Convolutional Neu- ral Networks (CNNs) (LeCun et al. (1998)), since they incorporate two important aspects in their architecture. On the one hand, they provide stability to small geometric deformations thanks to the rectiﬁer and pooling units. On the other hand, when the input is a locally stationary process or texture, they provide features with smaller variance. These properties can be proved for a certain class of CNNs where ﬁlters are given by multi-scale wavelets (Bruna & Mallat (2013)), but remain valid over a certain region of the CNN parameter space. Learning non-linear convolutional features that capture natural image statistics has also been considered in the context of unsupervised learning (H´enaff et al. (2014)), as well as Gibbs models whose sufﬁcient statistics are given by deep convo- lutional features Dai et al. (2015). Our approach also relates to the method proposed by Gatys et al. (2015) for texture synthesis. The authors show that the texture representations obtained across the layers of a CNN increasingly capture the statistical properties of natural images, producing impres- sive texture synthesis results. The recent work by Lamb et al. (2016) proposed to use representations derived from CNNs trained on discriminative tasks to regularize the objective function of the vari- ational autoencoder. Unlike the work presented in this paper, these methods use representations obtained from CNNs trained in a purely discriminative fashion, without further adaptation. In summary, our main contributions are:  2  Published as a conference paper at ICLR 2016  • We develop a framework for solving inverse problems in a suitable non-linear representa-  • We propose an algorithm to ﬁne-tune the collection of sufﬁcient statistics on a conditionally  • We demonstrate the validity of the approach on a challenging ill-posed problem: image  tion space.  generative model.  super-resolution.  In Section 2, we present the set-up of the super- The rest of the paper is organized as follows. resolution problem. Then, we describe the proposed approach Section 3. In Section 4 we analyze the proposed approach experimentally using examples of image super-resolution. We conclude the paper and discuss future research directions in Section 5.  2 PROBLEM SET-UP We consider the task of estimating a high-dimensional vector y ∈ RN given observations x = U (y) ∈ RM . In all problems of interest, the operator U is not invertible. In the case of image or audio super-resolution, U performs a downsampling. In absence of training data, inverse problems are approached as regularized signal recovery problems, leading to programs of the form  ˆy = arg min  y  1 2  (cid:107)U (y) − x(cid:107)2 + λR(y) ,  where R captures the structure one wishes to enforce amongst the inﬁnity of possible solutions matching the observations. Possible regularizations include total variation norms of the form R(y) = (cid:107)∇y(cid:107)1 or sparsity in predeﬁned signal decompositions, such as wavelets. Training data can be incorporated into the problem as a means to adjust the prior to the empirical dis- tribution. In this section we describe the setting in which the estimation is treated as pure regression problem, leading to a point estimation problem of the form  (cid:107)Φ(xi, Θ) − yi(cid:107)2 ,  (2)  (cid:88)  i  min  Θ  where (xi, yi) are training examples and Φ(x, Θ) is a CNN parametrized by Θ. Model (2) can be interpreted probabilistically by associating the mean squared loss with the negative log-likelihood of a Gaussian model:  p(y | x) = N (Φ(x, Θ), Id) .  (3) In that case, one is only interested in the point estimate ˆy = Φ(x, Θ), which corresponds to the Maximum Likelihood Estimate (MLE) of (3). Although appealing for its simplicity, the previous formulation has a limitation, that comes from the instability induced by the inverting the operator U. If y1 is a signal with sharp structures, and y2 is such that y2(u) = y1(u − τ (u)) is a small deformation of y1, then one has (cid:107)U y1 − U y2(cid:107) ≈ 0 if the warping ﬁeld τ is sufﬁciently small, although (cid:107)y1 − y2(cid:107) might be of the order of (cid:107)y1(cid:107).1 In other words, the high-frequency information of y is intrinsically unstable to geometric deformations under the standard Euclidean metric. It results that a point estimate optimizing the Euclidean distance as in (2) will necessarily discard the high frequency responsible for the large difference (cid:107)y1 − y2(cid:107). Moreover, if y is a stationary process, such as those modeling auditory or image textures, the point estimate will converge to the conditional expectation E(y | U y = x), which in general does not contain the same amount of high-frequency information than typical realizations of y. In both previ- ous examples, the point estimate exhibits the so-called “regression-to-the-mean” phenomena, which is accentuated as the scaling ratio of the operator U increases. Our goal in the next section is to develop an alternative framework that combines the simplicity of point estimates while capturing high-frequency information.  1More precisely, one can verify that the Euclidean distance is proportional to ξ(cid:107)τ(cid:107) where ξ is the central  frequency of y1.  3  Published as a conference paper at ICLR 2016  3  INFERENCE MODEL  In this section we develop our energy model based on a collection of stable sufﬁcient statistics. We begin by motivating the model, then we present examples of predeﬁned statistics, and next we propose an algorithm to ﬁne-tune those statistics to the data.  3.1 FROM POINT ESTIMATES TO STABLE INFERENCE  Similarly as in the work by Denton et al. (2015), we model the conditional distribution of high resolution samples given the low-resolution input through the residuals of a linear prediction. In other words, given a pair (x, y), we deﬁne ¯x = ¯U x to be the best linear predictor (in terms of Mean Squared Error (MSE)) of y given x, and consider the task of modeling the residual r = y − ¯U (x), which carries all the high-frequency information. In this work, we consider a conditional model deﬁned by a Gibbs energy of the form  (cid:107)Φ(x) − Ψ(r)(cid:107)2 ,  (4) where Φ and Ψ are a pair of generic CNNs. As with every energy-based model, (4) admits a probabilistic interpretation, by considering the corresponding Gibbs distribution  p(r| x) = exp(cid:0)−(cid:107)Φ(x) − Ψ(r)(cid:107)2 − log Z(cid:1) ,  (5) where Z is the partition function. The model (4), when viewed as a (conditionally) generative model of r, is thus characterized by a collection of sufﬁcient statistics Ψ(r). In particular, the invariance properties of Ψ translate directly into iso-probability sets: if r, r(cid:48) are such that Ψ(r) = Ψ(r(cid:48)), then p(r | x) = p(r(cid:48) | x). The model (4) thus breaks the estimation task into two sub-problems: (i) approximate a collection of features Ψ(r) using any parametric regression conditional on x, Φ(x), and (ii) sample a candidate r(cid:48) such that Ψ(r(cid:48)) matches the resulting features. These two problems are connected by the trade-off caused by the amount of information contained in the sufﬁcient statistics Ψ. If Ψ is nearly invertible, then we put all the pressure on the approximation step, but the level sets of Ψ are nearly singular, thus making the subsequent inference transparent. On the other hand, if the features Ψ(r) are too compressive (for instance, Ψ(r) = (cid:107)r(cid:107)), the approximation step is alleviated, but the inference of r becomes powerless since the level sets of Ψ will have too large dimensionality. It is thus necessary to ﬁnd the right balance between stability and discriminability in the design of the sufﬁcient statistics. Motivated by the success on supervised learning in image and audio tasks, we consider signal representations, Ψ, given by complex CNNs. The deep convolutional architecture provides generic stability with respect to small geometric deformations, thanks to the rectiﬁcation and average pooling layers. Moreover, the average pooling reduces the variance of locally stationary processes. At the same time, CNN features have the ability to achieve such local invariance while being highly informative (Bruna et al. (2014); Bruna & Mallat (2013)). The main question is thus how to determine Ψ. This problem has been studied extensively in the energy-based learning literature, see (LeCun et al. (2006)) and references therein. Gibbs models with sufﬁcient statistics given by deep representations have been previously considered in the context of unsupervised learning in Ngiam et al. (2011); Dai et al. (2015) and concurrently with our work in Xie et al. (2016). Before addressing the learning of Ψ, we discuss the setup where Ψ is a predetermined CNN with speciﬁc properties. Finally it is worth mentioning that the model can also be seen as the nonlinear equivalent of Canon- ical Correlation Analysis (CCA). Whereas CCA is concerned with ﬁnding linear transformations of both x and r that optimize the cross-correlation structure, thus maximizing predictability, our model considers non-linear transformations in both predictors and responses that will be maximally correlated.  3.2 PREDEFINED CNN STATISTICS  3.2.1 SCATTERING  Scattering networks (Bruna & Mallat (2013); Mallat (2012)) are obtained by cascading complex wavelet decompositions with complex modulus. The wavelet decompositions consist of oriented  4  Published as a conference paper at ICLR 2016  Figure 1: Model overview. (A) For a given pair (x, y), we ﬁrst preprocess the data to obtain the high- frequency residuals r. The sufﬁcient statistics Ψ are computed for r, and these are approximated by a network Φ. (B) The sampling procedure: Given x, we obtain a mode of the distribution p(r | x) by solving minr (cid:107)Ψ(r) − Φ(x)(cid:107)2.  complex band-pass ﬁlters that span uniformly the frequency plane. The resulting scattering coefﬁ- cients are thus local averages of cascaded complex wavelet modulus coefﬁcients ||x ∗ ψ1| ∗ ψ2| ∗ φ for different combination of scales and orientations. Thanks to their particular multi-scale structure, scattering coefﬁcients guarantee a number of stability properties in face of geometric deformations and stationary processes (Mallat (2012)). Moreover, scattering representations can be inverted un- der some conditions, which suggest that a regression in the scattering metric is more tolerant to perceptually small high-frequency variations than the original Euclidean metric. Whereas scattering networks used in recognition applications typically contain only steerable ﬁlters (Bruna & Mallat (2013)), in our setup it is worth pointing out that one can recover traditional total variation norms by properly adjusting the ﬁrst wavelet decomposition. Indeed, if we consider an extra orientation at the ﬁnest scale of the form  ψh(u) = ∇x(u) + i∇y(u) ,  where ∇x and ∇y are respectively the discrete horizontal and vertical derivative operators, then the resulting ﬁrst order scattering coefﬁcient |x ∗ ψh| ∗ φ = |∇x| ∗ φ is a local total variation norm. The model thus contains predictions of local total variation based on the low-resolution samples. Shrinking the resulting Total Variation (TV) prediction is an effective technique to regularize the estimation, similarly to wavelet shrinkage estimators, that generalizes the ﬂat TV prior. For a scattering decomposition with J scales, L orientations, and two layers of non-linearities, one has O(J 2L2) scattering coefﬁcients per patch of size 2J. For a given downsampling factor α > 1 (typically α = 2, 3, 4), the number of effective scattering coefﬁcients visible in the low-resolution x is O((J −log(α))2L2), which gives a rough indication of how to adjust the scale and dimensionality of the scattering representation as a function of the downsampling factor.  3.2.2 PRE-TRAINED DISCRIMINATIVE NETWORKS  Recent ﬁndings in computer vision problems have shown that the representations learned for su- pervised classiﬁcation problems can be readily transferred to others tasks (Oquab et al. (2014)). Thus, another possibility is to transfer convolutional features learnt on a large supervised task. As such an example, we use one of the networks proposed by Simonyan & Zisserman (2014). These network follow the standard CNN architecture with the particularity of using very small (3 × 3) convolution ﬁlters, which enables the use of networks with signiﬁcantly larger depth compared to prior approaches. Speciﬁcally, we use the VGG-19 network, containing 16 convolutional layers, 5 max-pooling layers, three fully connected layers and ReLU non-linearities. We deﬁne the network  5  Published as a conference paper at ICLR 2016  Ψ as a truncated VGG-19 network. Through testing on a small validation set, we found that best performance is achieved by keeping only the layer up to the forth pooling layer and replacing the max-pooling layers with average pooling ones, as the gradient ﬂow is smother. Note that keep- ing higher level representations entails higher computational cost. At testing time, this truncated network can be applied on images of arbitrary sizes. In Section 3.1, we model the conditional dis- tribution of high resolution samples given the low-resolution input through the residuals of a linear prediction. The main practical advantage of this choice is to reduce model complexity. When us- ing pre-trained networks, however, this is not an option. The VGG networks were trained with full images, thus it is not clear whether the representation obtained for residual images would be at all meaningful. Hence, when using the VGG networks, Ψ takes the high-resolution image y as an input.  3.3 TRAINING PREDICTIVE MODEL  We train our predictive model Φ(x) by directly minimizing (4) with respect to the parameters of Φ, which amounts to solving for a point estimate in the transformed feature space. Notice, however, that this is not equivalent to directly optimizing (5) with respect to the parameters of Φ. Indeed, contrary to the linear case, the partition function Z depends on x. By optimizing the approximation error in (4) we obtain a fast, suboptimal solution, which assumes that the approximation errors in the feature space are isotropic (i.e. the direction of the error does not inﬂuence the resulting likelihood). This assumption could be mitigated by replacing the Euclidean distance in (4) with a Mahalanobis distance. At test-time, given a low-resolution query x0, samples r0 are produced by solving an optimization problem as explained in the next section.  3.4 SAMPLING An essential component of the model is a method to sample from a distribution of the form p(r | x) ∝ exp(−(cid:107)Φ(x) − Ψ(r)(cid:107)2) when Ψ is a non-linear, not necessarily invertible transformation. In particular, we will be interested in sampling a mode of this distribution (which in general will not be unique), which amounts to solving  min  r(cid:48) (cid:107)Φ(x) − Ψ(r(cid:48))(cid:107)2 .  (6)  Similarly as other works (Portilla & Simoncelli (2000); H´enaff et al. (2014)), we solve (6) using gradient descent, by initializing r(cid:48) = ¯U (x), where ¯U is a Point Estimate (which can be linear or non- linear). From a computational perspective, this sampling procedure is signiﬁcantly more expensive than evaluating the feed-forward baseline CNN. The gradient descent strategy requires running as many forward and backard passes of Ψ as iterations. When using the pre-trained networks, (6) is solved directly over the high resolution image and the gradient descent optimization is initialized with the the best linear predictor of y given x.  3.5 FINE-TUNING SUFFICIENT STATISTICS  As discussed previously, the design of good sufﬁcient statistics requires striking the right balance between stability with respect to perturbations that are “invisible” by the operator U, and discrim- inability, so that the iso-probability sets contain only relevant samples. We study in this section a model to adjust automatically this trade-off by approximating the gradient of the log-likelihood. Given a conditional model of the form,  p(r|x) = exp(−||Φ(x) − Ψ(r)||2 − log Z)  (7)  we shall consider optimizing the negative log-likelihood in an alternate fashion with respect to Φ and Ψ. By taking logs and using the log-partition deﬁnition, we verify that  ∇Ψ − log p(r|x) = −∇Ψ(r)T (Φ(x) − Ψ(r)) + Er(cid:48)∼p(r(cid:48)|x)∇Ψ(r(cid:48))T (Φ(x) − Ψ(r(cid:48))) ,  ∇Φ − log p(r|x) = ∇Φ(x)T (Φ(x) − Ψ(r)) − ∇Φ(x)T(cid:0)Φ(x) − Er(cid:48)∼p(r(cid:48)|x)(Ψ(r(cid:48)))(cid:1)  = ∇Φ(x)T(cid:0)Er(cid:48)∼p(r(cid:48)|x)(Ψ(r(cid:48))) − Ψ(r)(cid:1) .  6  Published as a conference paper at ICLR 2016  (a) Original Size: 200 × 200 pixels  (b) Scattering PSNR: 37.54dB  (c) VGG-19 4th level PSNR: 34.13dB  (d) Baseline PSNR: 34.77dB  (e) Original Size: 200 × 200 pixels  (f) Scattering PSNR: 24.76dB  (g) VGG-19 4th level PSNR: 26.47dB  (h) Baseline PSNR: 26.91dB  Figure 2: Synthesized images. We compare the original images (ﬁrst column) with sampled syn- thesized images from Scattering (second column) and VGG (third column) networks as well as the result of our baseline (fourth column) up-scaling ×3, as a reference.  An estimator of the gradients is obtained by sampling p(r(cid:48)|x) and replacing the expectation by sample averaging:  (cid:92)∇Ψ − log p(r|x) = −∇Ψ(r)T (Φ(x) − Ψ(r)) +  ∇Ψ(r(cid:48))T (Φ(x) − Ψ(r(cid:48))) ,  1 L  (cid:88)  ,  r(cid:48)∼p(r(cid:48)|x)  Ψ(r(cid:48)) − Ψ(r)   1  L  (cid:88)  r(cid:48)∼p(r(cid:48)|x)  (cid:92)∇Φ − log p(r|x) = ∇Φ(x)T  which is unbiased even for L = 1. Sampling from the exact conditional p(r(cid:48)| x) requires an MCMC algorithm, such as Metropolis-Hastings, which would be computationally very expensive. In order to accelerate the ﬁne-tuning process, we consider the following biased sampling procedure. Instead of sampling from the true Gibbs distribution, p(r(cid:48)|x), we obtain typical samples having large likeli- hood by solving (6) every time we require a new sample. This algorithm can be viewed as a co-area representation of p(r(cid:48) |x): we ﬁrst sample a high likelihood value, represented by a small approxi- mation error (cid:107)Φ(x) − Ψ(r(cid:48))(cid:107)2, and then we obtain a sample from the corresponding iso-probability set by randomly perturbing the initialization. Using gradient descent as a means to approximately sample from Gibbs distributions was proposed in Zhu et al. (1998), and later exploited in Portilla & Simoncelli (2000) amongst other works. Al- though under some ergodicity assumptions one can show that the Gibbs distribution converges to the uniform measure on the a set of the form {r ; Ψ(r) = ψ0}, there are no guarantees that a gradient descent will produce samples from that uniform distribution. The resulting model thus bears some resemblances with the Generative Adversarial Network frame- work introduced by Goodfellow et al. (2014), later extended by Denton et al. (2015). The main difference is that in our case the discriminator and the generator networks are associated with a fea- ture representation Ψ and its inverse, respectively. In other words, the “fake” samples are generated by inverting the discriminator in its current state.  7  Published as a conference paper at ICLR 2016  4 EXPERIMENTAL EVALUATION  4.1 BASELINE AND DATASET  As a baseline we use a CNN inspired in the one used by Dong et al. (2014). Speciﬁcally, a 4-layer CNN with {64, 64, 64, 32} feature maps and a linear output layer. Filter sizes are 7 × 7, 3 × 3, 3×3, and 5×5 respectively, with ReLU non-linearities at each hidden layer. As a “sanity-check” we evaluated our trained architectures in the same benchmarks provided by Dong et al. (2014) obtaining comparable (slightly better) results in terms of PSNR. All models were trained using 64×64 images patches randomly chosen from a subset of the training set of ImageNet (Deng et al. (2009)). Speciﬁcally we used 12.5M patches, extracting at most two patches per image. For testing we used images extracted from the test set of ImageNet and other ex- ternal images. In this work we evaluate results using up-scaling factors of 3 and 4. To synthesize the low-resolution samples, the high-resolution images were sub-sampled using a proper anti-aliasing ﬁlters.  4.2 PERCEPTUAL RELEVANCE OF THE REPRESENTATION  As discussed in Section 3.1, there is a compromise between stability and discriminability in the design of the sufﬁcient statistics. In this work, we argue that CNN are good candidates for this task. The only way to validate is to visually inspect what information the iso-probability sets are actually capturing about an image. We do this by sampling different candidates, r(cid:48), such that Ψ(r(cid:48)) matches the ground truth features, Ψ(r), for a high-resolution residual image r. Figure 2 shows the obtained results for two examples with very different image content. The image given in Figure 2a is a textured natural image while Figure 2e has ﬁne geometric patterns. As candidate representations we use Scattering CNN and the VGG network as discussed in sections 3.2.1 and 3.2.2. We initialize r as Gaussian noise. It can be observed that the synthesized images retain excellent perceptual quality. As a reference, we included the results obtained when up-scaling these images (with factor of 3) using baseline CNN. It is interesting to point out that the quality does not always correlate with MSE. In the example of Figure 2a, we see that the sampled image achieves smaller MSE producing a higher PSNR. For Figure 2e the PSNR of the sampled image is lower than both of the baselines, while captures much better the ﬁne geometric structure. By inspecting carefully, one can see that the synthesized patterns in ﬁgures 2f and 2g do not exactly match those in Figure 2e but their perceptual quality is much higher than the one of Figure 2h. The MSE is known to penalize heavily local image deformation while being more tolerant to blur- ring. To further stress this point we performed a simple experiment comparing the relative distance of the residuales and their corresponding representations when undergoing degradations given by translation and blurring. Results are depicted in Figure 3. We observe that both metrics present opposite behaviors. The MSE is less sensitive to blur and more sensitive to local translations.  4.3 SUPER RESOLUTION  The scattering network Ψ is a 3-layer complex convolutional network that uses complex modulus as point-wise non-linearities. Its ﬁlters are near-analytic Morlet wavelets that span 8 orientations and 3 scales. We also include a feature map that captures the local total variation, as explained in Section 3. The resulting number of feature maps is 219. We used a 5-layer CNN mimicking the scattering network for representing the function Φ. We used an architecture with { 32, 64, 64, 64, 219} feature maps with ﬁlter sizes {9× 9, 9× 9, 9× 9, 3× 3, 1× 1} respectively. Unlike the scattering network, Φ has real-valued coefﬁcients using ReLU non-linearity instead of the modulus operator. Between the 2rd and 3rd layers, and between the 3rd and 4th layers, we included down-sampling layers. No explicit pooling layers were included in Φ, even though the scattering CNN, Ψ, includes it its ﬁlter maps several low-pass and downsampling stages. For the VGG-19 network, we use the same (given) architecture for both networks. We ﬁrst trained model given a ﬁxed scattering and VGG networks. In this case, as discussed earlier, the problem can be approximated to minimizing the MSE as given in (4). Then ran a ﬁne-tuning round for the scattering, as explained in Section 3.5, by initializing the system using the pre-trained network as initial networks. In the previous section we discussed the known problems of using the  8  Published as a conference paper at ICLR 2016  Figure 3: Relative error in pixel and in scattering domains observed when applying image deforma- tions to high-resolution images: (left) rigid shift; (right) Gaussian blur. Relative error plotted agains the “severity” of the degradation measured in pixel shifts or standard deviation of the blur. Results are the average over 10 images of size 200 × 200.  (a) Original  (b) Baseline  (c) VGG-19  (d) Scattering  (e) Fine Tunned  Figure 4: Super-resolution results for up-scaling ×3. We compare the original images (column (a) ), the baseline CNN (column (b) ) with sampled synthesized images from our model using VGG (column (c) ), Scattering (column (d) ), and ﬁne-funned Scattering (column (e) ) as CNNs.  9  Shift in pixels123456Relative error00.050.10.150.20.250.3ScatteringPixelBlurr sigma00.511.5200.050.10.150.20.250.3Relative errorScatteringPixelPublished as a conference paper at ICLR 2016  MSE as a way of measuring the perceptual relevance images. Thus we limit our analysis to visual evaluation. The point estimate leads in all cases to better PSNR, which is to be expected as, unlike the proposed approach, the model is optimized for maximizing this quantity. Figure 4 depicts ex- emplar results for super-resolution problem for an up-scaling factor of ×3 ( results for up-scaling ×4 are provided in Appendix A), which represents a good benchmark for inferential models, since the point estimates start to exhibit serious regression to the mean. We compare examples synthe- sized from predicted Scattering and VGG networks, a ﬁne-funned scattering network, as well as our baseline CNN. It is visually apparent that the predictions by the proposed model are able to produce results with more stable high-frequency content, avoiding the “regression to the mean”, observable in the baseline results. The ﬁne-tuned version of our model is able to reduce artifacts present in the original version, this is further analyzed in Appendix A. This effect becomes even more clear when comparing the residual images (with respect to the bicubic up-sampling), as shown in Ap- pendix A. The proposed method, has some natural limitations. When tested with very ﬁne textures, the recovered high frequencies appear artiﬁcial, see Appendix A for some examples. Finally, from a computational perspective, the baseline CNN is more efﬁcient than the proposed approach. The proposed approach requires solving an inference problem as explained in Section 3.4. To give a practical idea, upscaling a factor of 3 an image of size 200 × 200 takes 0.1 seconds for the baseline CNN, 5.26 for the Scattering and 5.09 seconds for the VGG. Reported times are average over ﬁfty runs and were run on a Nvidia GTX Titan Black GPU, using 100 iterations of gradient decent.  5 DISCUSSION  Generating realistic high-frequency content is a hard problem due to the curse of dimensionality. In this paper, we have argued that sharp geometric structures and textured regions are not well approximated by low-dimensionality models – if that was the case, point estimates trained on low- resolution patches would scale better. In order to overcome this problem, we have proposed a conditionally generative model that uses sufﬁcient statistics from CNNs to characterize with stable features textures and high-frequency content. By properly initializing the networks with ﬁlters with good geometrical properties, such as in scattering networks, we obtain a good baseline that generates realistic high frequency content. The underlying uncertainty in the generation of output samples is in our case encoded in the multiple combinations of complex phases in the intermediate layers of the network that are compatible with the output features. Recent work on signal recovery from these non-linear representations (Bruna et al. (2014)) suggests that the effective dimensionality of this set of admissible phases ranges from low (when the redundancy of the network is high) to nearly the ambient dimension (when the redundancy is low), thus suggesting that these models could scale well. We proposed an algorithm to ﬁne-tune those sufﬁcient statistics to the data, by optimizing a surrogate for the conditional likelihood. Although the model appears to move in the right direction, as shown in the numerical experiments, there is still a lot to be done and understood. The ﬁne-tuning step is costly and we observed the learning parameters (such as the learning rates of each network) need to be adjusted carefully, due mostly to the bias and variance of the gradient estimates. Another valid critique of the model is that the test-time inference is more expensive than a point estimate, or a generative adversarial network alternative. The upside is that the inference enforces solutions with spatial coherence, and it is not obvious that a feedforward procedure is able to align high frequency content without any feedback loop. Also, the proposed model provides an explicit representation. This paper focused on super-resolution, but the underlying challenge is how to ﬁnd an appropriate metric that is compatible with the high-frequency content of natural images. This poses a fundamen- tal trade-off between sharpness and stability: the phase of high-frequency coefﬁcients (encoding the precise location of sharp structures) is fundamentally unstable to small deformations and thus un- predictable; therefore, restoring high-frequency information comes at the cost of misalignments.  REFERENCES  Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. Deep generative stochastic networks  trainable by backprop. In ICML, 2013.  10  Published as a conference paper at ICLR 2016  Bruna, J. and Mallat, S. Invariant scattering convolution networks. Pattern Analysis and Machine  Intelligence, IEEE Transactions on, 35(8):1872–1886, 2013.  Bruna, J., Szlam, A., and LeCun, Y. Signal recovery from pooling representations. ICML, 2014.  Cui, Z., Chang, H., Shan, S., Zhong, B., and Chen, X. Deep network cascade for image super-  resolution. In Computer Vision–ECCV, pp. 49–64. Springer, 2014.  Dai, Jifeng, Lu, Yang, and Wu, Ying Nian. Generative modeling of convolutional neural networks.  In ICLR, 2015.  Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical  image database. In CVPR, pp. 248–255. IEEE, 2009.  Denton, E., Chintala, S., Szlam, A., and Fergus, R. Deep generative image models using a laplacian  pyramid of adversarial networks. arXiv preprint arXiv:1506.05751, 2015.  Dong, C., Loy, Chen C., He, K., and Tang, X. Learning a deep convolutional network for image  super-resolution. In Computer Vision–ECCV, pp. 184–199. Springer, 2014.  Elad, M. and Aharon, M. Image denoising via sparse and redundant representations over learned  dictionaries. Image Processing, IEEE Transactions on, 15(12):3736–3745, 2006.  Freeman, W T, Jones, T R, and Pasztor, E C. Example-based super-resolution. Computer Graphics  and Applications, IEEE, 22(2):56–65, 2002.  Gatys, Leon A, Ecker, Alexander S, and Bethge, Matthias. Texture synthesis and the controlled gen- eration of natural stimuli using convolutional neural networks. arXiv preprint arXiv:1505.07376, 2015.  Goodfellow, I., Le, Q., Saxe, A., Lee, H., and Ng, A. Y. Measuring invariances in deep networks. In  NIPS, pp. 646–654. 2009.  Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and  Bengio, Y. Generative adversarial nets. In NIPS, pp. 2672–2680, 2014.  Gregor, K. and LeCun, Y. Learning fast approximations of sparse coding. In ICML, pp. 399–406,  2010.  Gregor, K., Danihelka, I., Graves, A., and Wierstra, D. DRAW: A recurrent neural network for  image generation. arXiv preprint arXiv:1502.04623, 2015.  H´enaff, O. J., Ball´e, J., Rabinowitz, N. C., and Simoncelli, E. P. The local low-dimensionality of  natural images. arXiv preprint arXiv:1412.6626, 2014.  Jimenez-Rezende, D and Mohamed, S. Variational inference with normalizing ﬂows. arXiv preprint  arXiv:1505.05770, 2015.  Kingma, D. P and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,  2013.  Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980, 2014.  Lamb, Alex, Dumoulin, Vincent, and Courville, Aaron. Discriminative regularization for generative  models. arXiv preprint arXiv:1602.03220, 2016.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document  recognition. In Proceedings of the IEEE, pp. 2278–2324, 1998.  LeCun, Yann, Chopra, Sumit, Hadsell, Raia, Ranzato, M, and Huang, F. A tutorial on energy-based  learning. Predicting structured data, 1:0, 2006.  Mairal, J., Bach, F., Ponce, J., and Sapiro, G. Online dictionary learning for sparse coding. In ICML,  pp. 689–696, 2009.  11  Published as a conference paper at ICLR 2016  Mairal, J., Bach, F., and Ponce, J. Task-driven dictionary learning. Pattern Analysis and Machine  Intelligence, IEEE Transactions on, 34(4):791–804, 2012.  Mallat, S. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10):  1331–1398, 2012.  Ngiam, Jiquan, Chen, Zhenghao, Koh, Pang W, and Ng, Andrew Y. Learning deep energy models. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 1105– 1112, 2011.  Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and transferring mid-level image represen-  tations using convolutional neural networks. In CVPR, pp. 1717–1724. IEEE, 2014.  Portilla, Javier and Simoncelli, Eero P. A parametric texture model based on joint statistics of  complex wavelet coefﬁcients. International Journal of Computer Vision, 40(1):49–70, 2000.  Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recogni-  tion. arXiv preprint arXiv:1409.1556, 2014.  Sohl-Dickstein, J, Weiss, E A, Maheswaranathan, N, and Ganguli, S. Deep unsupervised learning  using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.  Sprechmann, P., Bronstein, A.M., and Sapiro, G. Learning efﬁcient sparse and low rank models.  Pattern Analysis and Machine Intelligence, IEEE Transactions on, 37(9):1821–1833, 2015.  Xie, J., Lu, Y., Zhu, S., and Wu, Y. A theory of generative convnet.  arXiv:1602:03264, 2016.  arXiv preprint  Yang, J., Wright, J., Huang, T., and Ma, Y. Image super-resolution as sparse representation of raw  image patches. In CVPR, pp. 1–8. IEEE, 2008.  Yang, J., Wang, Z., Lin, Z., Cohen, S., and Huang, T. S. Coupled dictionary training for image  super-resolution. Image Processing, IEEE Transactions on, 21(8):3467–3478, 2012.  Zhu, S. C., Wu, Y, and Mumford, D. Filters, random ﬁelds and maximum entropy (frame): Towards  a uniﬁed theory for texture modeling. IJCV, 27(2):107–126, 1998.  APPENDIX A FURTHER EXPERIMENTAL EVALUATION  In this appendix we provide further experimental evaluation and analysis that could not be included in the main body of the paper due to space limitations. Figures 5-9 show exemplar results for up- scaling ×3, while ﬁgures 10 and 11 provide results for ×4 magniﬁcation. Different networks were trained for different up-scaling factors. We compare the same models as in Section 4. Figure 5 is a detailed version the bottom image in Figure 4. This image has been widely used as a benchmark for the super-resolution problem. It is interesting because it has sharp edges, ﬁne details and textures. The proposed approach is able to synthesize more high-frequency content than the baseline approach. This can be well observed in the texture in the hat as well as the eye. This can be also seen in the details of the torch in Figure 7, which are completely lost in image recovered by the baseline CNN. A natural risk for the proposed approach is to produce results with high frequency content that looks un-natural. Both Scattering and VGG networks can suffer from this problem. We observe that this effect is a bit stronger in the latter. This can be appreciated in ﬁgures 7 and 8. The ﬁne-tuning stage is able to signiﬁcantly reduce this effect. To better show this feature, Figure 6 shows the high frequency content added by each method. The residual in Figure 6e appears smoother while maintaining the sharpness and ﬁne texture. We included two examples as a way of showing images in which adding structure can lead to percep- tually noticeable artifacts. The ﬁrst one is the very challenging example ﬁrst discussed in Figure 2, here shown in Figure 9. This image has very ﬁne textures, whose information is almost completely lost in the low resolution image. The proposed method for both, VGG and Scattering networks, is able to predict the presence of high-frequency content, however the results exhibit noticeable ar- tifacts. Interestingly the ﬁne tuning seems to hurt despite sharpening the image. This example is  12  Published as a conference paper at ICLR 2016  particularly challenging as the texture is highly structured and human observers expect to see a very deﬁned pattern. The second example shows an image where the high-resolution version contains blurred areas (due to defocus of the background). While producing a good reconstruction of the bird, the proposed approach tends to over sharpen these regions, as there is no way to know a-priori where (and where not) to sharpen. Arguably, producing a blurred image can be a “better” alternative.  13  Published as a conference paper at ICLR 2016  (a) Original  (b) Bicubic  (c) Baseline  (d) Scattering  (e) Scattering ﬁne-tunned  (f) VGG-19  Figure 5: Synthesis results sale ×3. Images 200× 200 pixels. Residual images provided in Figure 7  (a) Original size  (b) Ground truth  (c) Baseline  (d) Scattering  (e) Scattering ﬁne-tunned  (f) VGG-19  Figure 6: Residual image with respect to the Bicubic interpolation, corresponding to Figure 5. Top left original sized image.  14  Published as a conference paper at ICLR 2016  (a) Original  (b) Bicubic  (c) Baseline  (d) Scattering  (e) Scattering ﬁne-tunned  (f) VGG-19  Figure 7: Synthesis results sale ×3.  (a) Original  (b) Bicubic  (c) Baseline  (d) Scattering  (e) Scattering ﬁne-tunned  (f) VGG-19  Figure 8: Synthesis results sale ×3.  15  Published as a conference paper at ICLR 2016  (a) Original  (b) Bicubic  (c) Baseline  (d) Scattering  (f) VGG-19 Figure 9: Synthesis results sale ×3. Challenging example with ﬁne texture.  (e) Scattering ﬁne-tunned  (a) Original  (b) Bicubic  (c) Baseline  (d) Scattering  (e) Scattering ﬁne-tunned  (f) VGG-19  Figure 10: Synthesis results sale ×4.  16  Published as a conference paper at ICLR 2016  (a) Original  (b) Bicubic  (c) Baseline  (d) Scattering  (e) Scattering ﬁne-tunned  (f) VGG-19  Figure 11: Synthesis results sale ×4.  APPENDIX B TRAINING DETAILS  In this Appendix we provide further details of our training implementation.  sponding to the parameters of Ψ by a factor η = 10−4.  • Fine-tuning: We alternate between optimizing Φ and Ψ. We adjust the learning rate corre- • We use Adam (Kingma & Ba (2014)) to perform the inference to generate negative samples. • We adjust the shrinkage of the total variation features at test time by adding a term λ(cid:107)r(cid:107)TV, • We renormalize the scattering features by scaling each output feature by ck, where c > 1 and k is the number of nonlinearities corresponding to each scattering path (k = 0, 1, 2 in our experiments).  with λ = 10−8.  17  ",
