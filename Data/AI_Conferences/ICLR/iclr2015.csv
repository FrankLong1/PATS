unique_id,year,title,authors,pdf_url,paper_text,abstract
1412.6623,2015,Word Representations via Gaussian Embedding,"['Word Representations via Gaussian Embedding', 'Luke Vilnis and Andrew McCallum']",https://arxiv.org/pdf/1412.6623,"5 1 0 2     y a M 1         ] L C . s c [      4 v 3 2 6 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  WORD REPRESENTATIONS VIA GAUSSIAN EMBEDDING  Luke Vilnis, Andrew McCallum School of Computer Science University of Massachusetts Amherst Amherst, MA 01003 luke@cs.umass.edu, mccallum@cs.umass.edu  ABSTRACT  Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representa- tion and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distribu- tions. We compare performance on various word embedding benchmarks, inves- tigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.  1  INTRODUCTION  In recent years there has been a surge of interest in learning compact distributed representations or embeddings for many machine learning tasks, including collaborative ﬁltering (Koren et al., 2009), image retrieval (Weston et al., 2011), relation extraction (Riedel et al., 2013), word semantics and language modeling (Bengio et al., 2006; Mnih & Hinton, 2008; Mikolov et al., 2013), and many others. In these approaches input objects (such as images, relations or words) are mapped to dense vectors having lower-dimensionality than the cardinality of the inputs, with the goal that the ge- ometry of his low-dimensional latent embedded space be smooth with respect to some measure of similarity in the target domain. That is, objects associated with similar targets should be mapped to nearby points in the embedded space. While this approach has proven powerful, representing an object as a single point in space carries some important limitations. An embedded vector representing a point estimate does not naturally express uncertainty about the target concepts with which the input may be associated. Point vec- tors are typically compared by dot products, cosine-distance or Euclean distance, none of which provide for asymmetric comparisons between objects (as is necessary to represent inclusion or en- tailment). Relationships between points are normally measured by distances required to obey the triangle inequality. This paper advocates moving beyond vector point representations to potential functions (Aizerman et al., 1964), or continuous densities in latent space. In particular we explore Gaussian function embeddings (currently with diagonal covariance), in which both means and variances are learned from data. Gaussians innately represent uncertainty, and provide a distance function per object. KL- divergence between Gaussian distributions is straightforward to calculate, naturally asymmetric, and has a geometric interpretation as an inclusion between families of ellipses. There is a long line of previous work in mapping data cases to probability distributions, perhaps the most famous being radial basis functions (RBFs), used both in the kernel and neural network literature. We draw inspiration from this work to propose novel word embedding algorithms that embed words directly as Gaussian distributional potential functions in an inﬁnite dimensional func- tion space. This allows us to map word types not only to vectors but to soft regions in space, modeling uncertainty, inclusion, and entailment, as well as providing a rich geometry of the latent space.  1  Published as a conference paper at ICLR 2015  Figure 1: Learned diagonal vari- ances, as used in evaluation (Section 6), for each word, with the ﬁrst let- ter of each word indicating the po- sition of its mean. We project onto generalized eigenvectors between the mixture means and variance of query word Bach. Nearby words to Bach are other composers e.g. Mozart, which lead to similar pictures.  After discussing related work and presenting our algorithms below we explore properties of our al- gorithms with multiple qualitative and quantitative evaluation on several real and synthetic datasets. We show that concept containment and speciﬁcity matches common intuition on examples concern- ing people, genres, foods, and others. We compare our embeddings to Skip-Gram on seven standard word similarity tasks, and evaluate the ability of our method to learn unsupervised lexical entail- ment. We also demonstrate that our training method also supports new styles of supervised training that explicitly incorporate asymmetry into the objective.  2 RELATED WORK  This paper builds on a long line of work on both distributed and distributional semantic word vec- tors, including distributional semantics, neural language models, count-based language models, and, more broadly, the ﬁeld of representation learning. Related work in probabilistic matrix factorization (Mnih & Salakhutdinov, 2007) embeds rows and columns as Gaussians, and some forms of this do provide each row and column with its own vari- ance (Salakhutdinov & Mnih, 2008). Given the parallels between embedding models and matrix factorization (Deerwester et al., 1990; Riedel et al., 2013; Levy & Goldberg, 2014), this is relevant to our approach. However, these Bayesian methods apply Bayes’ rule to observed data to infer the latent distributions, whereas our model works directly in the space of probability distributions and discriminatively trains them. This allows us to go beyond the Bayesian approach and use arbitrary (and even asymmetric) training criteria, and is more similar to methods that learn kernels (Lanckriet et al., 2004) or function-valued neural networks such as mixture density networks (Bishop, 1994). Other work in multiplicative tensor factorization for word embeddings (Kiros et al., 2014) and met- ric learning (Xing et al., 2002) learns some combinations of representations, clusters, and a distance metric jointly; however, it does not effectively learn a distance function per item. Fitting Gaussian mixture models on embeddings has been done in order to apply Fisher kernels to entire documents (Clinchant & Perronnin, 2013b;a). Preliminary concurrent work from Kiyoshiyo et al. (2014) de- scribes a signiﬁcantly different model similar to Bayesian matrix factorization, using a probabilistic Gaussian graphical model to deﬁne a distribution over pairs of words, and they lack quantitative experiments or evaluation. In linguistic semantics, work on the distributional inclusion hypothesis (Geffet & Dagan, 2005), uses traditional count-based vectors to deﬁne regions in vector space (Erk, 2009) such that subordinate concepts are included in these regions. In fact, one strength of our proposed work is that we extend these intuitively appealing ideas (as well as the ability to use a variety of asymmetric distances between vectors) to the dense, low-dimensional distributed vectors that are now gaining popularity.  3 BACKGROUND Our goal is to map every word type w in some dictionary D and context word type c in a dictionary C to a Gaussian distribution over a latent embedding space, such that linguistic properties of the words  2  Published as a conference paper at ICLR 2015  are captured by properties of and relationships between the distributions. For precision, we call an element of the dictionary a word type, and a particular observed token in some context a word token. This is analogous to the class vs. instance distinction in object-oriented programming. In unsupervised learning of word vectors, we observe a sequence of word tokens {t(w)i} for each type w, and their contexts (sets of nearby word tokens), {c(w)i}. The goal is to map each word type w and context word type c to a vector, such that types that appear in similar contexts have similar vectors. When it is unambiguous, we also use the variables w and c to denote the vectors associated to that given word type or context word type. An energy function (LeCun et al., 2006) is a function Eθ(x, y) that scores pairs of inputs x and outputs y, parametrized by θ. The goal of energy-based learning is to train the parameters of the energy function to score observed positive input-output pairs higher (or lower, depending on sign conventions) than negative pairs. This is accomplished by means of a loss function L which deﬁnes which pairs are positive and negative according to some supervision, and provides gradients on the parameters given the predictions of the energy function. In prediction-based (energy-based) word embedding models, the parameters θ correspond to our learned word representations, and the x and y input-output pairs correspond to word tokens and their contexts. These contexts can be either positive (observed) or negative (often randomly sampled). In the word2vec Skip-Gram (Mikolov et al., 2013) word embedding model, the energy function takes the form of a dot product between the vectors of an observed word and an observed context w(cid:62)c. The loss function is a binary logistic regression classiﬁer that treats the score of a word and its observed context as the score of a positive example, and the score of a word and a randomly sampled context as the score of a negative example. Backpropagating (Rumelhart et al., 1986) this loss to the word vectors trains them to be predictive of their contexts, achieving the desired effect (words in similar contexts have similar vectors). In recent work, word2vec has been shown to be equivalent to factoring certain types of weighted pointwise mutual information matrices (Levy & Goldberg, 2014). In our work, we use a slightly different loss function than Skip-Gram word2vec embeddings. Our energy functions take on a more limited range of values than do vector dot products, and their dynamic ranges depend in complex ways on the parameters. Therefore, we had difﬁculty using the word2vec loss that treats scores of positive and negative pairs as positive and negative examples to a binary classiﬁer, since this relies on the ability to push up on the energy surface in an absolute, rather than relative, manner. To avoid the problem of absolute energies, we train with a ranking-based loss. We chose a max-margin ranking objective, similar to that used in Rank-SVM (Joachims, 2002) or Wsabie (Weston et al., 2011), which pushes scores of positive pairs above negatives by a margin:  Lm(w, cp, cn) = max(0, m − E(w, cp) + E(w, cn))  In this terminology, the contribution of our work is a pair of energy functions for training Gaussian distributions to represent word types.  4 WARMUP: EMPIRICAL COVARIANCES  Given a pre-trained set of word embeddings trained from contexts, there is a simple way to construct variances using the empirical variance of a word type’s set of context vectors. For a word w with N word vector sets {c(w)i} representing the words found in its contexts, and window size W , the empirical variance is  N(cid:88)  W(cid:88)  i  j  Σw =  1  N W  (c(w)ij − w)(c(w)ij − w)(cid:62)  This is an estimator for the covariance of a distribution assuming that the mean is ﬁxed at w. In practice, it is also necessary to add a small ridge term δ > 0 to the diagonal of the matrix to regularize and avoid numerical problems when inverting. However, in Section 6.2 we note that the distributions learned by this empirical estimator do not possess properties that we would want from Gaussian distributional embeddings, such as unsuper- vised entailment represented as inclusion between ellipsoids. By discriminatively embedding our  3  Published as a conference paper at ICLR 2015  predictive vectors in the space of Gaussian distributions, we can improve this performance. Our models can learn certain forms of entailment during unsupervised training, as discussed in Section 6.2 and exempliﬁed in Figure 1.  5 ENERGY-BASED LEARNING OF GAUSSIANS  As discussed in Section 3, our architecture learns Gaussian distributional embeddings to predict words in context given the current word, and ranks these over negatively sampled words. We present two energy functions to train these embeddings.  5.1 SYMMETRIC SIMILARITY: EXPECTED LIKELIHOOD OR PROBABILITY PRODUCT  KERNEL  While the dot product between two means of independent Gaussians is a perfectly valid measure of similarity (it is the expected dot product), it does not incorporate the covariances and would not enable us to gain any beneﬁt from our probabilistic model. The most logical next choice for a symmetric similarity function would be to take the inner product between the distributions themselves. Recall that for two (well-behaved) functions f, g ∈ Rn → R, a standard choice of inner product is  (cid:90)  i.e. the continuous version of(cid:80)  f (x)g(x)dx  x∈Rn  i figi = (cid:104)f, g(cid:105) for discrete vectors f and g.  (cid:90)  This idea seems very natural, and indeed has appeared before – the idea of mapping data cases w into probability distributions (often over their contexts), and comparing them via integrals has a history under the name of the expected likelihood or probability product kernel (Jebara et al., 2004). For Gaussians, the inner product is deﬁned as  E(Pi, Pj) =  x∈Rn  N (x; µi, Σi)N (x; µj, Σj)dx = N (0; µi − µj, Σi + Σj)  The proof of this identity follows from simple calculus. This is a consequence of the broader fact that the Gaussian is a stable distribution, i.e. the convolution of two Gaussian random variables is another Gaussian. Since we aim to discriminatively train the weights of the energy function, and it is always positive, we work not with this quantity directly, but with its logarithm. This has two motivations: ﬁrstly, we plan to use ranking loss, and ratios of densities and likelihoods are much more commonly worked with than differences – differences in probabilities are less interpretable than an odds ratio. Secondly, it is easier numerically, as otherwise the quantities can get exponentially small and harder to deal with. The logarithm of the energy (in d dimensions) is log N (0; µi−µj, Σi+Σj) = − 1 2  log(2π). ∂A log det A = A−1, and the gradient Recalling that the gradient of the log determinant is ∂A x(cid:62)A−1y = −A−(cid:62)xy(cid:62)A−(cid:62) (Petersen, 2006) we can take the gradient of this energy function ∂ with respect to the means µ and covariances Σ:  (µi−µj)(cid:62)(Σi+Σj)−1(µi−µj)− d 2  log det(Σi+Σj)− 1 2  ∂  ∂ log E(Pi, Pj)  ∂µi  = − ∂ log E(Pi, Pj)  ∂µj  = −∆ij  ∂ log E(Pi, Pj)  ∂Σi  ∂ log E(Pi, Pj)  = where ∆ij = (Σi + Σj)−1(µi − µj)  ∂Σj  =  1 2  (∆ij∆(cid:62)  ij − (Σi + Σj)−1)  For diagonal and spherical covariances, these matrix inverses are trivial to compute, and even in the full-matrix case can be solved very efﬁciently for the small dimensionality common in embedding  4  Published as a conference paper at ICLR 2015  models. If the matrices have a low-rank plus diagonal structure, they can be computed and stored even more efﬁciently using the matrix inversion lemma. This log-energy has an intuitive geometric interpretation as a similarity measure. Gaussians are measured as close to one another based on the distance between their means, as measured through the Mahalanobis distance deﬁned by their joint inverse covariance. Recalling that log det A + const. is equivalent to the log-volume of the ellipse spanned by the principle components of A, we can interpret this other term of the energy as a regularizer that prevents us from decreasing the distance by only increasing joint variance. This combination pushes the means together while encouraging them to have more concentrated, sharply peaked distributions in order to have high energy.  5.2 ASYMMETRIC SIMILARITY: KL DIVERGENCE  Training vectors through KL-divergence to encode their context distributions, or even to incorporate more explicit directional supervision re: entailment from a knowledge base or WordNet, is also a sensible objective choice. We optimize the following energy function (which has a similarly tractable closed form solution for Gaussians):  −E(Pi, Pj) = DKL(Nj||Ni) =  x∈Rn  N (x; µi, Σi) log  N (x; µj, Σj) N (x; µi, Σi) (µi − µj) − d − log  dx  det(Σj) det(Σi)  )  =  1 2  (tr(Σ−1  i Σj) + (µi − µj)(cid:62)Σ−1  i  (cid:90)  Note the leading negative sign (we deﬁne the negative energy), since KL is a distance function and not a similarity. KL divergence is a natural energy function for representing entailment between concepts – a low KL divergence from x to y indicates that we can encode y easily as x, implying that y entails x. This can be more intuitively visualized and interpreted as a soft form of inclusion between the level sets of ellipsoids generated by the two Gaussians – if there is a relatively high expected log-likelihood ratio (negative KL), then most of the mass of y lies inside x. Just as in the previous case, we can compute the gradients for this energy function in closed form:  ∂E(Pi, Pj)  ∂µi  ∂E(Pi, Pj)  ∂Σi  ∂E(Pi, Pj)  ∂Σj  (Σ−1  =  i + ∆(cid:48)  ij∆  ij − Σ−1 (cid:48)(cid:62)  i  )  = − ∂E(Pi, Pj)  = −∆(cid:48)  ij  ∂µj i ΣjΣ−1 j − Σ−1 ) ij = Σ−1  i  i  (Σ−1 = where ∆(cid:48)  1 2 1 2  (µi − µj)  using the fact that ∂ 2006).  ∂A tr(X(cid:62)A−1Y ) = −(A−1Y X(cid:62)A−1)(cid:62) and ∂  ∂A tr(XA) = X(cid:62) (Petersen,  5.3 UNCERTAINTY OF INNER PRODUCTS  Another beneﬁt of embedding objects as probability distributions is that we can look at the distribu- tion of dot products between vectors drawn from two Gaussian representations. This distribution is not itself a one-dimensional Gaussian, but it has a ﬁnite mean and variance with a simple structure in the case where the two Gaussians are assumed independent (Brown & Rutemiller, 1977). For the distribution P (z = x(cid:62)y), we have  y Σyµy + tr(ΣxΣy)  this means we can ﬁnd e.g. a lower or upper bound on the dot products of random samples from these distributions, that should hold some given percent of the time. Parametrizing this energy by some number of standard deviations c, we can also get a range for the dot product as:  x µy ± c µ(cid:62)  µ(cid:62) x Σxµx + µ(cid:62)  y Σyµy + tr(ΣxΣy)  µz = µ(cid:62) Σz = µ(cid:62)  x µy x Σxµx + µ(cid:62) (cid:113)  5  Published as a conference paper at ICLR 2015  We can choose c in a principled using an (incorrect) Gaussian approximation, or more general con- centration bounds such as Chebyshev’s inequality.  5.4 LEARNING  To learn our model, we need to pick an energy function (EL or KL), a loss function (max-margin), and a set of positive and negative training pairs. As the landscape is highly nonconvex, it is also helpful to add some regularization. We regularize the means and covariances differently, since they are different types of geometric objects. The means should not be allowed to grow too large, so we can add a simple hard constraint to the (cid:96)2 norm:  (cid:107)µi(cid:107)2 ≤ C, ∀i  However, the covariance matrices need to be kept positive deﬁnite as well as reasonably sized. This is achieved by adding a hard constraint that the eigenvalues λi lie within the hypercube [m, M ]d for constants m and M.  mI ≺ Σi ≺ M I, ∀i  For diagonal covariances, this simply involves either applying the min or max function to each element of the diagonal to keep it within the hypercube, Σii ← max(m, min(M, Σii)). Controlling the bottom eigenvalues of the covariance is especially important when training with expected likelihood, since the energy function includes a log det term that can give very high scores to small covariances, dominating the rest of the energy. We optimize the parameters using AdaGrad (Duchi et al., 2011) and stochastic gradients in small minibatches containing 20 sentences worth of tokens and contexts.  6 EVALUATION  We evaluate the representation learning algorithms on several qualitative and quantitative tasks, including modeling asymmetric and linguistic relationships, uncertainty, and word similarity. All Gaussian experiments are conducted with 50-dimensional vectors, with diagonal variances except where noted otherwise. Unsupervised embeddings are learned on the concatenated ukWaC and WaCkypedia corpora (Baroni et al., 2009), consisting of about 3 billion tokens. This matches the experimental setup used by Baroni et al. (2012), aside from leaving out the small British National Corpus, which is not publicly available and contains only 100 million tokens. All word types that appear less than 100 times in the training set are dropped, leaving a vocabulary of approximately 280 thousand word types. When training word2vec Skip-Gram embeddings for baselines, we follow the above training setup (50 dimensional embeddings), using our own implementation of word2vec to change as little as possible between the two models, only the loss function. We train both models with one pass over the data, using separate embeddings for the input and output contexts, 1 negative sample per positive example, and the same subsampling procedure as in the word2vec paper (Mikolov et al., 2013). The only other difference between the two training regimes is that we use a smaller (cid:96)2 regularization constraint when using the word2vec loss function, which improves performance vs. the diagonal Gaussian model which does better with “spikier” mean embeddings with larger norms (see the comment in Section 6.4). The original word2vec implementation uses no (cid:96)2 constraint, but we saw better performance when including it in our training setup.  6.1 SPECIFICITY AND UNCERTAINTY OF EMBEDDINGS  In Figure 2, we examine some of the 100 nearest neighbors of several query words as we sort from largest to smallest variance, as measured by determinant of the covariance matrix, using diagonal Gaussian embeddings. Note that more speciﬁc words, such as joviality and electroclash have smaller variance, while polysemous words or those denoting broader concepts have larger variances, such as mix, mind, and graph. This is not merely an artifact of higher frequency words getting more variance – when sorting by those words whose rank by frequency and rank by variance are most dissimilar, we see that genres with names like chillout, avant, and shoegaze overindex their variance compared  6  Published as a conference paper at ICLR 2015  Query Word Nearby Words, Descending Variance rock  food feeling  algebra  mix sound blue folk jazz rap avant hardcore chillout shoegaze powerpop electroclash drink meal meat diet spice juice bacon soya gluten stevia sense mind mood perception compassion sadness coldness sincerity perplexity difﬁdence joviality theory graph equivalence ﬁnite predicate congruence topology quaternion symplectic homomorphism  Figure 2: Elements of the top 100 nearest neighbor sets for chosen query words, sorted by descend- ing variance (as measured by determinant of covariance matrix). Note that less speciﬁc and more ambiguous words have greater variance.  Test  Model Baroni et al. (2012) E E Empirical (D) E Empirical (D) Empirical (S) E E Empirical (S) E Learned (D) E Learned (D) E Learned (S) Learned (S) E  Similarity Best F1 AP balAPinc KL Cos KL Cos KL Cos KL Cos  75.1 70.05 76.24 71.18 76.24 79.01 76.99 79.34 77.36  – .68 .71 .69 .71 .80 .73 .78 .73  Figure 3: Entailment: We compare empirical and learned variances, both diagonal (D) and spherical (S). E is the dataset of Baroni et al. (2012). Measures of similarity are symmetric (cosine between means) and asymmetric (KL) divergence for Gaussians. balAPinc is an asymmetric similarity mea- sure speciﬁc to sparse, distributional count-based representations.  to how frequent they are, since they appear in different contexts. Similarly, common emotion words like sadness and sincerity have less variance than their frequency would predict, since they have fairly ﬁxed meanings. Another emotion word, coldness, is an uncommon word with a large variance due to its polysemy.  6.2 ENTAILMENT  As can be seen qualitatively in Figure 1, our embeddings can learn some forms of unsupervised entailment directly from the source data. We evaluate quantitatively on the Entailment dataset of Baroni et al. (2012). Our setup is essentially the same as theirs but uses slightly less data, as men- tioned in the beginning of this section. We evaluate with Average Precision and best F1 score. We include the best F1 score (by picking the optimal threshold at test) because this is used by Baroni et al. (2012), but we believe AP is better to demonstrate the correlation of various asymmetric and symmetric measures with the entailment data. In Figure 3, we compare variances learned jointly during embedding training by using the expected likelihood objective, with empirical variances gathered from contexts on pre-trained word2vec-style embeddings. We compare both diagonal (D) and spherical (S) variances, using both cosine similarity between means, and KL divergence. Baseline asymmetric measurements, such as the difference between the sizes of the two embeddings, did worse than the cosine. We see that KL divergence between the entailed and entailing word does not give good performance for the empirical variances, but beats the count-based balAPinc measure when used with learned variances. For the baseline empirical model to achieve reasonable performance when using KL divergence, we regularized the covariance matrices, as the unregularized matrices had very small entries. We regularized the empirical covariance by adding a small ridge δ to the diagonal, which was tuned to maximize performance, to give the largest possible advantage to the baseline model. Interestingly, the empirical variances do worse with KL than the symmetric cosine similarity when predicting en- tailment. This appears to be because the empirically learned variances are so small that the choice is  7  Published as a conference paper at ICLR 2015  Figure 4: Synthetic experiments on embedding two simple hierarchies in two dimensions directly using KL divergence. The embedding model captures all of the hierarchical relationships present in the tree. Sibling leaves are pushed into overlapping areas by the objective function.  between either leaving them small, making it very difﬁcult to have one Gaussian located “inside” an- other Gaussian, or regularizing so much that their discriminative power is washed out. Additionally, when examining the empirical variances, we noted that common words like “such,” which receive very large variances in our learned model, have much smaller empirical variances relative to rarer words. A possible explanation is that the contrastive objective forces variances of commonly sam- pled words to spread out to avoid loss, while the empirical variance sees only “positive examples” and has no penalty for being close to many contexts at once. While these results indicate that we can do as well or better at unsupervised entailment than previ- ous distributional semantic measures, we would like to move beyond purely unsupervised learning. Although certain forms of entailment can be learned in an unsupervised manner from distributional data, many entailing relationships are not present in the training text in the form of lexical substitu- tions that reﬂect the is-a relationship. For example, one might see phrases such as “look at that bird,” “look at that eagle,” “look at that dog,” but rarely “look at that mammal.” One appealing aspect of our models versus count-based ones is that they can be directly discriminatively trained to embed hierarchies.  6.3 DIRECTLY LEARNING ASYMMETRIC RELATIONSHIPS  In Figure 4, we see the results of directly embedding simple tree hierarchies as Gaussians. We embed nodes as Gaussians with diagonal variances in two-dimensional space using gradient descent on the KL divergence between parents and children. We create a Gaussian for each node in the tree, and randomly initialize means. Negative contexts come from randomly sampled nodes that are neither ancestors nor descendents, while positive contexts come from ancestors or descendents using the appropriate directional KL divergence. Unlike our experiments with symmetric energy, we must use the same set of embeddings for nodes and contexts, or else the objective function will push the variances to be unboundedly large. Our training process captures the hierarchical relationships, although leaf-level siblings are not differentiated from each other by this objective function. This is because out of all the negative examples that a leaf node can receive, only one will push it away from its sibling node.  6.4 WORD SIMILARITY BENCHMARKS  We evaluate the embeddings on seven different standard word similarity benchmarks (Rubenstein & Goodenough, 1965; Szumlanski et al., 2013; Hill et al., 2014; Miller & Charles, 1991; Bruni et al., 2014; Yang & Powers, 2006; Finkelstein et al., 2001). A comparison to all of the state of the art word-embedding numbers for different dimensionalities as in (Baroni et al., 2014) is out of the scope of this evaluation. However, we note that the overall performance of our 50-dimensional embeddings matches or beats reported numbers on these datasets for the 80-dimensional Skip-Gram vectors at wordvectors.org (Faruqui & Dyer, 2014), as well as our own Skip-Gram implementation. Note that the numbers are not directly comparable since we use a much older version of Wikipedia (circa 2009) in our WaCkypedia dataset, but this should not give us an edge.  8  Published as a conference paper at ICLR 2015  SG (50d) Dataset 29.39 SimLex 59.89 WordSim WordSim-S 69.86 WordSim-R 53.03 70.27 MEN 63.96 MC 70.01 RG YP 39.34 49.14 Rel-122  SG (100d) LG/50/m/S LG/50/d/S LG/50/m/D LG/50/d/D 31.13 59.33 70.19 54.64 70.70 66.76 69.38 35.76 51.26  31.25 62.12 74.64 54.44 71.30 67.01 70.41 36.05 52.28  32.23 65.49 76.15 58.96 71.31 70.41 71.00 41.50 53.74  29.84 62.03 73.92 54.37 69.65 69.17 74.76 42.55 51.09  30.50 61.00 72.79 53.36 70.18 68.50 77.00 39.30 53.54  Figure 5: Similarity: We evaluate our learned Gaussian embeddings (LG) with spherical (S) and diagonal (D) variances, on several word similarity benchmarks, compared against standard Skip- Gram (SG) embeddings on the trained on the same dataset. We evaluate Gaussian embeddings with both cosine between means (m), and cosine between the distributions themselves (d) as deﬁned by the expected likelihood inner product.  While it is good to sanity-check that our embedding algorithms can achieve standard measures of distributional quality, these experiments also let us compare the different types of variances (spher- ical and diagonal). We also compare against Skip-Gram embeddings with 100 latent dimensions, since our diagonal variances have 50 extra parameters. We see that the embeddings with spherical covariances have an overall slight edge over the embed- dings with diagonal covariances in this case, in a reversal from the entailment experiments. This could be due to the diagonal variance matrices making the embeddings more axis-aligned, making it harder to learn all the similarities and reducing model capacity. To test this theory, we plotted the absolute values of components of spherical and diagonal variance mean vectors on a q-q plot and noted a signiﬁcant off-diagonal shift, indicating that diagonal variance embedding mean vectors have “spikier” distributions of components, indicating more axis-alignment. We also see that the distributions with diagonal variances beneﬁt more from including the variance in the comparison (d) than the spherical variances. Generally, the data sets in which the cosine between distributions (d) outperforms cosine between means (m) are similar for both spherical and diagonal covariances. Using the cosine between distributions never helped when using empirical variances, so we do not include those numbers.  7 CONCLUSION AND FUTURE WORK  In this work we introduced a method to embed word types into the space of Gaussian distribu- tions, and learn the embeddings directly in that space. This allows us to represent words not as low-dimensional vectors, but as densities over a latent space, directly representing notions of uncer- tainty and enabling a richer geometry in the embedded space. We demonstrated the effectiveness of these embeddings on a linguistic task requiring asymmetric comparisons, as well as standard word similarity benchmarks, learning of synthetic hierarchies, and several qualitative examinations. In future work, we hope to move beyond spherical or diagonal covariances and into combinations of low rank and diagonal matrices. Efﬁcient updates and scalable learning is still possible due to the Sherman-Woodbury-Morrison formula. Additionally, going beyond diagonal covariances will enable us to keep our semantics from being axis-aligned, which will increase model capacity and expressivity. We also hope to move past stochastic gradient descent and warm starting and be able to learn the Gaussian representations robustly in one pass from scratch by using e.g. proximal or block coordinate descent methods. Improved optimization strategies will also be helpful on the highly nonconvex problem of training supervised hierarchies with KL divergence. Representing words and concepts as different types of distributions (including other elliptic distri- butions such as the Student’s t) is an exciting direction – Gaussians concentrate their density on a thin spherical ellipsoidal shell, which can lead to counterintuitive behavior in high dimensions. Multimodal distributions represent another clear avenue for future work. Combining ideas from  9  Published as a conference paper at ICLR 2015  kernel methods and manifold learning with deep learning and linguistic representation learning is an exciting frontier. In other domains, we want to extend the use of potential function representations to other tasks requiring embeddings, such as relational learning with the universal schema (Riedel et al., 2013). We hope to leverage the asymmetric measures, probabilistic interpretation, and ﬂexible training criteria of our model to tackle tasks involving similarity-in-context, comparison of sentences and paragraphs, and more general common sense reasoning.  8 ACKNOWLEDGEMENTS  This work was supported in part by the Center for Intelligent Information Retrieval, in part by IARPA via DoI/NBC contract #D11PC20152, and in part by NSF grant #CNS-0958392 The U.S. Govern- ment is authorized to reproduce and distribute reprint for Governmental purposes notwithstanding any copyright annotation thereon. Any opinions, ﬁndings and conclusions or recommendations ex- pressed in this material are those of the authors and do not necessarily reﬂect those of the sponsor.  REFERENCES Aizerman, M. A., Braverman, E. A., and Rozonoer, L. Theoretical foundations of the potential function method in pattern recognition learning. In Automation and Remote Control,, number 25 in Automation and Remote Control,, pp. 821–837, 1964.  Baroni, Marco, Bernardini, Silvia, Ferraresi, Adriano, and Zanchetta, Eros. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209–226, 2009.  Baroni, Marco, Bernardi, Raffaella, Do, Ngoc-Quynh, and Shan, Chung-chieh. Entailment above the word level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, pp. 23–32, Stroudsburg, PA, USA, 2012. Association for Computational Linguistics.  Baroni, Marco, Dinu, Georgiana, and Kruszewski, Germ´an. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 238–247. Association for Computational Linguistics, 2014.  Bengio, Yoshua, Schwenk, Holger, Sen´ecal, Jean-S´ebastien, Morin, Fr´ederic, and Gauvain, Jean- Luc. Neural probabilistic language models. In Innovations in Machine Learning, pp. 137–186. Springer, 2006.  Bishop, Christopher M. Mixture density networks, 1994.  Brown, Gerald G and Rutemiller, Herbert C. Means and variances of stochastic vector products with  applications to random linear models. Management Science, 24(2):210–216, 1977.  Bruni, Elia, Tran, Nam-Khanh, and Baroni, Marco. Multimodal distributional semantics. JAIR,  2014.  Clinchant, St´ephane and Perronnin, Florent. Textual similarity with a bag-of-embedded-words model. In Proceedings of the 2013 Conference on the Theory of Information Retrieval, ICTIR ’13, pp. 25:117–25:120, New York, NY, USA, 2013a. ACM.  Clinchant, St´ephane and Perronnin, Florent. Aggregating continuous word embeddings for infor-  mation retrieval. ACL 2013, pp. 100, 2013b.  Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., and Harshman, R.A. Indexing by latent semantic analysis. Journal of the American Society for Information Science 41, pp. 391–407, 1990.  Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning  and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.  10  Published as a conference paper at ICLR 2015  Erk, Katrin. Representing words as regions in vector space. In Proceedings of the Thirteenth Confer- ence on Computational Natural Language Learning, pp. 57–65. Association for Computational Linguistics, 2009.  Faruqui, Manaal and Dyer, Chris. Community evaluation and exchange of word vectors at word- vectors.org. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Baltimore, USA, June 2014. Association for Computational Linguistics.  Finkelstein, Lev, Gabrilovich, Evgeniy, Matias, Yossi, Rivlin, Ehud, Solan, Zach, Wolfman, Gadi, and Ruppin, Eytan. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pp. 406–414. ACM, 2001.  Geffet, Maayan and Dagan, Ido. The distributional inclusion hypotheses and lexical entailment. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pp. 107–114. Association for Computational Linguistics, 2005.  Hill, Felix, Reichart, Roi, and Korhonen, Anna. Simlex-999: Evaluating semantic models with  (genuine) similarity estimation. arXiv preprint arXiv:1408.3456, 2014.  Jebara, Tony, Kondor, Risi, and Howard, Andrew. Probability product kernels. The Journal of  Machine Learning Research, 5:819–844, 2004.  Joachims, Thorsten. Optimizing search engines using clickthrough data.  In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 133–142. ACM, 2002.  Kiros, Ryan, Zemel, Richard, and Salakhutdinov, Ruslan R. A multiplicative model for learning  distributed text-based attribute representations. In NIPS, 2014.  Kiyoshiyo, Shimaoka, Masayasu, Muraoka, Futo, Yamamoto, Watanabe, Yotaro, Okazaki, Naoaki, and Inui, Kentaro. Distribution representation of the meaning of words and phrases by a gaussian distribution. In Language Processing Society 20th Annual Conference (In Japanese), 2014.  Koren, Yehuda, Bell, Robert, and Volinsky, Chris. Matrix factorization techniques for recommender  systems. Computer, 42(8):30–37, August 2009. ISSN 0018-9162.  Lanckriet, Gert RG, Cristianini, Nello, Bartlett, Peter, Ghaoui, Laurent El, and Jordan, Michael I. Learning the kernel matrix with semideﬁnite programming. The Journal of Machine Learning Research, 5:27–72, 2004.  LeCun, Yann, Chopra, Sumit, and Hadsell, Raia. A tutorial on energy-based learning. 2006.  Levy, Omer and Goldberg, Yoav. Neural word embedding as implicit matrix factorization.  Advances in Neural Information Processing Systems, pp. 2177–2185, 2014.  In  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed repre- sentations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp. 3111–3119, 2013.  Miller, George A and Charles, Walter G. Contextual correlates of semantic similarity. Language  and cognitive processes, 6(1):1–28, 1991.  Mnih, Andriy and Hinton, Geoffrey E. A scalable hierarchical distributed language model.  Advances in neural information processing systems, pp. 1081–1088, 2008.  In  Mnih, Andriy and Salakhutdinov, Ruslan. Probabilistic matrix factorization. In Advances in neural  information processing systems, pp. 1257–1264, 2007.  Petersen, Kaare Brandt. The matrix cookbook. 2006.  Riedel, Sebastian, Yao, Limin, McCallum, Andrew, and Marlin, Benjamin M. Relation extraction  with matrix factorization and universal schemas. 2013.  11  Published as a conference paper at ICLR 2015  Rubenstein, Herbert and Goodenough, John B. Contextual correlates of synonymy. Commun. ACM,  8(10):627–633, October 1965. ISSN 0001-0782.  Rumelhart, D.E., Hintont, G.E., and Williams, R.J. Learning representations by back-propagating  errors. Nature, 323(6088):533–536, 1986.  Salakhutdinov, Ruslan and Mnih, Andriy. Bayesian probabilistic matrix factorization using markov chain monte carlo. In Proceedings of the 25th international conference on Machine learning, pp. 880–887. ACM, 2008.  Szumlanski, Sean R, Gomez, Fernando, and Sims, Valerie K. A new set of norms for semantic  relatedness measures. In ACL, 2013.  Weston, Jason, Bengio, Samy, and Usunier, Nicolas. Wsabie: Scaling up to large vocabulary image  annotation. In IJCAI, volume 11, pp. 2764–2770, 2011.  Xing, Eric P, Jordan, Michael I, Russell, Stuart, and Ng, Andrew Y. Distance metric learning with In Advances in neural information processing  application to clustering with side-information. systems, pp. 505–512, 2002.  Yang, Dongqiang and Powers, David M. W. Verb similarity on the taxonomy of wordnet. In In the  3rd International WordNet Conference (GWC-06), Jeju Island, Korea, 2006.  12  ",
1412.6632,2015,Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN),"['Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)', 'Junhua Mao', 'Wei Xu', 'Yi Yang', 'Jiang Wang', 'and Alan Yuille']",https://arxiv.org/pdf/1412.6632,"5 1 0 2     n u J    1 1      ]  V C . s c [      5 v 2 3 6 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  DEEP CAPTIONING WITH MULTIMODAL RECURRENT NEURAL NETWORKS (M-RNN)  Junhua Mao University of California, Los Angeles; Baidu Research mjhustc@ucla.edu  Wei Xu & Yi Yang & Jiang Wang & Zhiheng Huang Baidu Research {wei.xu,yangyi05,wangjiang03,huangzhiheng}@baidu.com  Alan Yuille University of California, Los Angeles yuille@stat.ucla.edu  ABSTRACT  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are gen- erated according to this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves signiﬁcant performance improvement over the state-of-the-art methods which di- rectly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/˜junhua.mao/m-RNN.html. 1  1  INTRODUCTION  Obtaining sentence level descriptions for images is becoming an important task and it has many ap- plications, such as early childhood education, image retrieval, and navigation for the blind. Thanks to the rapid development of computer vision and natural language processing technologies, recent work has made signiﬁcant progress on this task (see a brief review in Section 2). Many previous methods treat it as a retrieval task. They learn a joint embedding to map the features of both sen- tences and images to the same semantic space. These methods generate image captions by retrieving them from a sentence database. Thus, they lack the ability of generating novel sentences or describ- ing images that contain novel combinations of objects and scenes. In this work, we propose a multimodal Recurrent Neural Networks (m-RNN) model 2 to address both the task of generating novel sentences descriptions for images, and the task of image and sentence retrieval. The whole m-RNN model contains a language model part, a vision part and a multimodal part. The language model part learns a dense feature embedding for each word in the  1Most recently, we adopt a simple strategy to boost the performance of image captioning task signiﬁcantly. More details are shown in Section 8. The code and related data (e.g. reﬁned image features and hypotheses sentences generated by the m-RNN model) are available at https://github.com/mjhucla/mRNN-CR. 2A previous version of this work appears in the NIPS 2014 Deep Learning Workshop with the title “Explain Images with Multimodal Recurrent Neural Networks” http://arxiv.org/abs/1410.1090 (Mao et al. (2014)). We observed subsequent arXiv papers which also use recurrent neural networks in this topic and cite our work. We gratefully acknowledge them.  1  Published as a conference paper at ICLR 2015  Figure 1: Examples of the generated and two top-ranked retrieved sentences given the query image from IAPR TC-12 dataset. The sentences can well describe the content of the images. We show a failure case in the fourth image, where the model mistakenly treats the lake as the sky and misses all the people. More examples from the MS COCO dataset can be found on the project page: www.stat.ucla.edu/˜junhua.mao/m-RNN.html.  dictionary and stores the semantic temporal context in recurrent layers. The vision part contains a deep Convolutional Neural Network (CNN) which generates the image representation. The multi- modal part connects the language model and the deep CNN together by a one-layer representation. Our m-RNN model is learned using a log-likelihood cost function (see details in Section 4). The errors can be backpropagated to the three parts of the m-RNN model to update the model parameters simultaneously. In the experiments, we validate our model on four benchmark datasets: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO (Lin et al. (2014)). We show that our method achieves state-of-the-art performance, signiﬁcantly outperforming all the other methods for the three tasks: generating novel sentences, retrieving im- ages given a sentence and retrieving sentences given an image. Our framework is general and can be further improved by incorporating more powerful deep representations for images and sentences.  2 RELATED WORK  Deep model for computer vision and natural language. The methods based on the deep neural network developed rapidly in recent years in both the ﬁeld of computer vision and natural lan- guage. For computer vision, Krizhevsky et al. (2012) propose a deep Convolutional Neural Net- works (CNN) with 8 layers (denoted as AlexNet) and outperform previous methods by a large margin in the image classiﬁcation task of ImageNet challenge (Russakovsky et al. (2014)). This network structure is widely used in computer vision, e.g. Girshick et al. (2014) design a object de- tection framework (RCNN) based on this work. Recently, Simonyan & Zisserman (2014) propose a CNN with over 16 layers (denoted as VggNet) and performs substantially better than the AlexNet. For natural language, the Recurrent Neural Network (RNN) shows the state-of-the-art performance in many tasks, such as speech recognition and word embedding learning (Mikolov et al. (2010; 2011; 2013)). Recently, RNNs have been successfully applied to machine translation to extract semantic information from the source sentence and generate target sentences (e.g. Kalchbrenner & Blunsom (2013), Cho et al. (2014) and Sutskever et al. (2014)). Image-sentence retrieval. Many previous methods treat the task of describing images as a retrieval task and formulate the problem as a ranking or embedding learning problem (Hodosh et al. (2013); Frome et al. (2013); Socher et al. (2014)). They ﬁrst extract the word and sentence features (e.g. Socher et al. (2014) uses dependency tree Recursive Neural Network to extract sentence features) as well as the image features. Then they optimize a ranking cost to learn an embedding model that maps both the sentence feature and the image feature to a common semantic feature space. In this way, they can directly calculate the distance between images and sentences. Recently, Karpathy et al. (2014) show that object level image features based on object detection results can generate better results than image features extracted at the global level.  2  Retr.Gen.1. Tourists are sitting at a long table with beer bottles on it in a rather dark restaurant and are raising their bierglaeser; 2. Tourists are sitting at a long table with a white table-cloth in a somewhat dark restaurant;Tourists are sitting at a long table with a white table cloth and are eating;1. Top view of the lights of a city at night, with a well-illuminated square in front of a church in the foreground;2. People on the stairs in front of an illuminated cathedral with two towers at night;A square with burning street lamps and a street in the foreground;1. A dry landscape with light brown grass and green shrubs and trees in the foreground and large reddish-brown rocks and a blue sky in the background;2. A few bushes at the bottom and a clear sky in the background; A dry landscape with green trees and bushes and light brown grass in the foreground and reddish-brown round rock domes and a blue sky in the background;1. Group picture of nine tourists and one local on a grey rock with a lake in the background;2. Five people are standing and four are squatting on a brown rock in the foreground;A blue sky in the background;Published as a conference paper at ICLR 2015  Figure 2: Illustration of the simple Recurrent Neural Network (RNN) and our multimodal Recurrent Neural Network (m-RNN) architecture. (a). The simple RNN. (b). Our m-RNN model. The inputs of our model are an image and its corresponding sentence descriptions. w1, w2, ..., wL represents the words in a sentence. We add a start sign wstart and an end sign wend to all the training sentences. The model estimates the probability distribution of the next word given previous words and the image. It consists of ﬁve layers (i.e. two word embedding layers, a recurrent layer, a multimodal layer and a softmax layer) and a deep CNN in each time frame. The number above each layer indicates the dimension of the layer. The weights are shared among all the time frames. (Best viewed in color)  Generating novel sentence descriptions for images. There are generally three categories of meth- ods for this task. The ﬁrst category assumes a speciﬁc rule of the language grammar. They parse the sentence and divide it into several parts (Mitchell et al. (2012); Gupta & Mannem (2012)). Each part is associated with an object or an attribute in the image (e.g. Kulkarni et al. (2011) uses a Con- ditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the re- trieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al. (2003); Jia et al. (2011)). They generate sentences with richer and more ﬂexible structure than the ﬁrst group. The probability of generating sentences using the model can serve as the afﬁnity metric for retrieval. Our method falls into this category. More closely related to our tasks and method is the work of Kiros et al. (2014b), which is built on a Log-BiLinear model (Mnih & Hinton (2007)) and use AlexNet to extract visual features. It needs a ﬁxed length of context (i.e. ﬁve words), whereas in our model, the temporal context is stored in a recurrent architecture, which allows arbitrary context length. Shortly after Mao et al. (2014), several papers appear with record breaking results (e.g. Kiros et al. (2014a); Karpathy & Fei-Fei (2014); Vinyals et al. (2014); Donahue et al. (2014); Fang et al. (2014); Chen & Zitnick (2014)). Many of them are built on recurrent neural networks. It demonstrates the effectiveness of storing context information in a recurrent layer. Our work has two major difference from these methods. Firstly, we incorporate a two-layer word embedding system in the m-RNN network structure which learns the word representation more efﬁciently than the single-layer word embedding. Secondly, we do not use the recurrent layer to store the visual information. The image representation is inputted to the m-RNN model along with every word in the sentence description. It utilizes of the capacity of the recurrent layer more efﬁciently, and allows us to achieve state-of- the-art performance using a relatively small dimensional recurrent layer. In the experiments, we show that these two strategies lead to better performance. Our method is still the best-performing approach for almost all the evaluation metrics.  3 MODEL ARCHITECTURE 3.1 SIMPLE RECURRENT NEURAL NETWORK  We brieﬂy introduce the simple Recurrent Neural Network (RNN) or Elman network (Elman (1990)). Its architecture is shown in Figure 2(a). It has three types of layers in each time frame:  3  (b). The m-RNN modelEmbedding IEmbedding IIRecurrentMultimodalSoftMaxwstartImageCNNw1ImageCNNwLImageCNN...Predictw1Predictw2Predictwend(a). The simple RNN modelw(t)r(t-1)r(t)y(t)w(t-1)...y(t-1)Input Word Layer wRecurrentLayer rOutputLayer yunfoldThe m-RNN model for one time frame128256256512Published as a conference paper at ICLR 2015  the input word layer w, the recurrent layer r and the output layer y. The activation of input, re- current and output layers at time t is denoted as w(t), r(t), and y(t) respectively. w(t) denotes the current word vector, which can be a simple 1-of-N coding representation h(t) (i.e. the one-hot representation, which is binary and has the same dimension as the vocabulary size with only one non-zero element) Mikolov et al. (2010). y(t) can be calculated as follows:  x(t) = [w(t) r(t − 1)]; r(t) = f1(U · x(t)); y(t) = g1(V · r(t));  (1) where x(t) is a vector that concatenates w(t) and r(t− 1), f1(.) and g1(.) are element-wise sigmoid and softmax function respectively, and U, V are weights which will be learned. The size of the RNN is adaptive to the length of the input sequence. The recurrent layers connect the sub-networks in different time frames. Accordingly, when we do backpropagation, we need to propagate the error through recurrent connections back in time (Rumelhart et al. (1988)).  3.2 OUR M-RNN MODEL  The structure of our multimodal Recurrent Neural Network (m-RNN) is shown in Figure 2(b). It has ﬁve layers in each time frame: two word embedding layers, the recurrent layer, the multimodal layer, and the softmax layer). The two word embedding layers embed the one-hot input into a dense word representation. It en- codes both the syntactic and semantic meaning of the words. The semantically relevant words can be found by calculating the Euclidean distance between two dense word vectors in embedding layers. Most of the sentence-image multimodal models (Karpathy et al. (2014); Frome et al. (2013); Socher et al. (2014); Kiros et al. (2014b)) use pre-computed word embedding vectors as the initialization of their model. In contrast, we randomly initialize our word embedding layers and learn them from the training data. We show that this random initialization is sufﬁcient for our architecture to generate the state-of-the-art result. We treat the activation of the word embedding layer II (see Figure 2(b)) as the ﬁnal word representation, which is one of the three direct inputs of the multimodal layer. After the two word embedding layers, we have a recurrent layer with 256 dimensions. The calcula- tion of the recurrent layer is slightly different from the calculation for the simple RNN. Instead of concatenating the word representation at time t (denoted as w(t)) and the recurrent layer activation at time t− 1 (denoted as r(t− 1)), we ﬁrst map r(t− 1) into the same vector space as w(t) and add them together:  r(t) = f2(Ur · r(t − 1) + w(t));  (2) where “+” represents element-wise addition. We set f2(.) to be the Rectiﬁed Linear Unit (ReLU), inspired by its the recent success when training very deep structure in computer vision ﬁeld (Nair & Hinton (2010); Krizhevsky et al. (2012)). This differs from the simple RNN where the sigmoid function is adopted (see Section 3.1). ReLU is faster, and harder to saturate or overﬁt the data than non-linear functions like the sigmoid. When the backpropagation through time (BPTT) is conducted for the RNN with sigmoid function, the vanishing or exploding gradient problem appears since even the simplest RNN model can have a large temporal depth 3. Previous work (Mikolov et al. (2010; 2011)) use heuristics, such as the truncated BPTT, to avoid this problem. The truncated BPTT stops the BPTT after k time steps, where k is a hand-deﬁned hyperparameter. Because of the good properties of ReLU, we do not need to stop the BPTT at an early stage, which leads to better and more efﬁcient utilization of the data than the truncated BPTT. After the recurrent layer, we set up a 512 dimensional multimodal layer that connects the language model part and the vision part of the m-RNN model (see Figure 2(b)). This layer has three inputs: the word-embedding layer II, the recurrent layer and the image representation. For the image rep- resentation, here we use the activation of the 7th layer of AlexNet (Krizhevsky et al. (2012)) or 15th layer of VggNet (Simonyan & Zisserman (2014)), though our framework can use any image fea- tures. We map the activation of the three layers to the same multimodal feature space and add them together to obtain the activation of the multimodal layer:  m(t) = g2(Vw · w(t) + Vr · r(t) + VI · I);  (3)  3We tried Sigmoid and Scaled Hyperbolic Tangent function as the non-linear functions for RNN in the  experiments but they lead to the gradient explosion problem easily.  4  Published as a conference paper at ICLR 2015  where “+” denotes element-wise addition, m denotes the multimodal layer feature vector, I denotes the image feature. g2(.) is the element-wise scaled hyperbolic tangent function (LeCun et al. (2012)):  g2(x) = 1.7159 · tanh(  2 3  x)  (4)  This function forces the gradients into the most non-linear value range and leads to a faster training process than the basic hyperbolic tangent function. Both the simple RNN and m-RNN models have a softmax layer that generates the probability dis- tribution of the next word. The dimension of this layer is the vocabulary size M, which is different for different datasets.  4 TRAINING THE M-RNN  To train our m-RNN model we adopt a log-likelihood cost function. It is related to the Perplexity of the sentences in the training set given their corresponding images. Perplexity is a standard measure for evaluating language model. The perplexity for one word sequence (i.e. a sentence) w1:L is calculated as follows:  log2 PPL(w1:L|I) = − 1 L  log2 P (wn|w1:n−1, I)  (5)  where L is the length of the word sequence, PPL(w1:L|I) denotes the perplexity of the sentence w1:L given the image I. P (wn|w1:n−1, I) is the probability of generating the word wn given I and previous words w1:n−1. It corresponds to the activation of the SoftMax layer of our model. The cost function of our model is the average log-likelihood of the words given their context words and corresponding images in the training sentences plus a regularization term. It can be calculated by the perplexity:  L(cid:88)  n=1  Li · log2 PPL(w(i)  1:Li  |I(i)) + λθ · (cid:107)θ(cid:107)2  2  (6)  Ns(cid:88)  i=1  C =  1 N  where Ns and N denotes the number of sentences and the number of words in the training set receptively, Li denotes the length of ith sentences, and θ represents the model parameters. Our training objective is to minimize this cost function, which is equivalent to maximize the proba- bility of generating the sentences in the training set using the model. The cost function is differen- tiable and we use backpropagation to learn the model parameters.  5 SENTENCE GENERATION, IMAGE RETRIEVAL AND SENTENCE RETRIEVAL  We use the trained m-RNN model for three tasks: 1) Sentences generation, 2) Image retrieval (re- trieving most relevant images to the given sentence), 3) Sentence retrieval (retrieving most relevant sentences to the given image). The sentence generation process is straightforward. Starting from the start sign wstart or arbitrary number of reference words (e.g. we can input the ﬁrst K words in the reference sentence to the model and then start to generate new words), our model can calculate the probability distribution of the next word: P (wn|w1:n−1, I). Then we can sample from this probability distribution to pick the next word. In practice, we ﬁnd that selecting the word with the maximum probability performs slightly better than sampling. After that, we input the picked word to the model and continue the process until the model outputs the end sign wend. For the retrieval tasks, we use our model to calculate the probability of generating a sentence w1:L n P (wn|w1:n−1, I). The probability can be treated as an afﬁnity  given an image I: P (w1:L|I) =(cid:81)  measurement between sentences and images. For the image retrieval task, given the query sentence wQ ing to the probability P (wQ perplexity-based image retrieval in Kiros et al. (2014b).  1:L, we rank the dataset images ID accord- 1:L|ID) and retrieved the top ranked images. This is equivalent to the  5  Published as a conference paper at ICLR 2015  The sentence retrieval task is trickier because there might be some sentences that have high proba- bility or perplexity for any image query (e.g. sentences consist of many frequently appeared words). To solve this problem, Kiros et al. (2014b) uses the perplexity of a sentence conditioned on the averaged image feature across the training set as the reference perplexity to normalize the original perplexity. Different from them, we use the normalized probability where the normalization factor is the marginal probability of wD  1:L);  P (wD  P (wD  1:L denotes the sentence in the dataset, IQ denotes the query image, and I  (7) are images where wD sampled from the training set. We approximate P (I ) by a constant and ignore this term. This strategy leads to a much better performance than that in Kiros et al. (2014b) in the experiments. The normalized probability is equivalent to the probability P (IQ|wD 1:L), which is symmetric to the probability P (wQ  1:L|ID) used in the image retrieval task.  (cid:48) P (wD I  )  (cid:48)  (cid:48)  (cid:48)  1:L|I  ) · P (I  (cid:48)  1:L:  1:L|IQ)/P (wD  1:L) =(cid:80)  6 LEARNING OF SENTENCE AND IMAGE FEATURES  The architecture of our model allows the gradients from the loss function to be backpropagated to both the language modeling part (i.e. the word embedding layers and the recurrent layer) and the vision part (e.g. the AlexNet or VggNet). For the language part, as mentioned above, we randomly initialize the language modeling layers and learn their parameters. For the vision part, we use the pre-trained AlexNet (Krizhevsky et al. (2012)) or the VggNet (Simonyan & Zisserman (2014)) on ImageNet dataset (Russakovsky et al. (2014)). Recently, Karpathy et al. (2014) show that using the RCNN object detection results (Girshick et al. (2014)) combined with the AlexNet features performs better than simply treating the image as a whole frame. In the experiments, we show that our method performs much better than Karpathy et al. (2014) when the same image features are used, and is better than or comparable to their results even when they use more sophisticated features based on object detection. We can update the CNN in the vision part of our model according to the gradient backpropagated from the multimodal layer. In this paper, we ﬁx the image features and the deep CNN network in the training stage due to a shortage of data. In future work, we will apply our method on large datasets (e.g. the complete MS COCO dataset, which has not yet been released) and ﬁnetune the parameters of the deep CNN network in the training stage. The m-RNN model is trained using Baidu’s internal deep learning platform PADDLE, which allows us to explore many different model architectures in a short period. The hyperparameters, such as layer dimensions and the choice of the non-linear activation functions, are tuned via cross-validation on Flickr8K dataset and are then ﬁxed across all the experiments. It takes 25 ms on average to generate a sentence (excluding image feature extraction stage) on a single core CPU.  7 EXPERIMENTS  7.1 DATASETS  We test our method on four benchmark datasets with sentence level annotations: IAPR TC-12 (Grub- inger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO (Lin et al. (2014)). IAPR TC-12. This dataset consists of around 20,000 images taken from different locations around the world. It contains images of different sports and actions, people, animals, cities, landscapes, etc. For each image, it provides at least one sentence annotation. On average, there are about 1.7 sentence annotations for one image. We adopt the standard separation of training and testing set as previous works (Guillaumin et al. (2010); Kiros et al. (2014b)) with 17,665 images for training and 1962 images for testing. Flickr8K. This dataset consists of 8,000 images extracted from Flickr. For each image, it provides ﬁve sentence annotations. We adopt the standard separation of training, validation and testing set provided by the dataset. There are 6,000 images for training, 1,000 images for validation and 1,000 images for testing.  6  Published as a conference paper at ICLR 2015  Flickr30K. This dataset is a recent extension of Flickr8K. For each image, it also provides ﬁve sentences annotations. It consists of 158,915 crowd-sourced captions describing 31,783 images. The grammar and style for the annotations of this dataset is similar to Flickr8K. We follow the previous work (Karpathy et al. (2014)) which used 1,000 images for testing. This dataset, as well as the Flick8K dataset, were originally used for the image-sentence retrieval tasks. MS COCO. The current release of this recently proposed dataset contains 82,783 training images and 40,504 validation images. For each image, it provides ﬁve sentences annotations. We randomly sampled 4,000 images for validation and 1,000 images for testing from their currently released validation set. The dataset partition of MS COCO and Flickr30K is available in the project page 4.  7.2 EVALUATION METRICS  Sentence Generation. Following previous works, we use the sentence perplexity (see Equ. 5) and BLEU scores (i.e. B-1, B-2, B-3, and B-4) (Papineni et al. (2002)) as the evaluation metrics. BLEU scores were originally designed for automatic machine translation where they rate the quality of a translated sentences given several reference sentences. Similarly, we can treat the sentence gener- ation task as the “translation” of the content of images to sentences. BLEU remains the standard evaluation metric for sentence generation methods for images, though it has drawbacks. For some images, the reference sentences might not contain all the possible descriptions in the image and BLEU might penalize some correctly generated sentences. Please see more details of the calcula- tion of BLEU scores for this task in the supplementary material section 10.3 5. Sentence Retrieval and Image Retrieval. We adopt the same evaluation metrics as previous works (Socher et al. (2014); Frome et al. (2013); Karpathy et al. (2014)) for both the tasks of sentences retrieval and image retrieval. We use R@K (K = 1, 5, 10) as the measurement. R@K is the recall rate of a correctly retrieved groundtruth given top K candidates. Higher R@K usually means better retrieval performance. Since we care most about the top-ranked retrieved results, the R@K scores with smaller K are more important. The Med r is another metric we use, which is the median rank of the ﬁrst retrieved groundtruth sentence or image. Lower Med r usually means better performance. For IAPR TC-12 datasets, we use additional evaluation metrics to conduct a fair comparison with previous work (Kiros et al. (2014b)). Please see the details in the supplementary material section 10.3.  7.3 RESULTS ON IAPR TC-12  The results of the sentence generation task6 are shown in Table 1. Ours-RNN-Base serves as a baseline method for our m-RNN model. It has the same architecture as m-RNN except that it does not have the image representation input. To conduct a fair comparison, we follow the same experimental settings of Kiros et al. (2014b) to calculate the BLEU scores and perplexity. These two evaluation metrics are not necessarily correlated to each other for the following reasons. As mentioned in Section 4, perplexity is calculated according to the conditional probability of the word in a sentence given all of its previous reference words. Therefore, a strong language model that successfully captures the distributions of words in sentences can have a low perplexity without the image content. But the content of the generated sentences might be uncorrelated to images. From Table 1, we can see that although our baseline method of RNN generates a low perplexity, its BLEU score is low, indicating that it fails to generate sentences that are consistent with the content of images. Table 1 shows that our m-RNN model performs much better than our baseline RNN model and the state-of-the-art methods both in terms of the perplexity and BLEU score.  4www.stat.ucla.edu/˜junhua.mao/m-RNN.html 5The BLEU outputted by our implementation is slightly lower than the recently released MS COCO caption evaluation toolbox (Chen et al. (2015)) because of different tokenization methods of the sentences. We re- evaluate our method using the toolbox in the current version of the paper.  6Kiros et al. (2014b) further improved their results after the publication. We compare our results with their  updated ones here.  7  Published as a conference paper at ICLR 2015  LBL, Mnih & Hinton (2007) MLBLB-AlexNet, Kiros et al. (2014b) MLBLF-AlexNet, Kiros et al. (2014b) Gupta et al. (2012) Gupta & Mannem (2012) Ours-RNN-Base Ours-m-RNN-AlexNet  PPL 9.29 9.86 9.90  - -  7.77 6.92  B-1 0.321 0.393 0.387 0.15 0.33 0.307 0.482  B-2 0.145 0.211 0.209 0.06 0.18 0.177 0.357  B-3 0.064 0.112 0.115 0.01 0.07 0.096 0.269  B-4 - - - - -  0.043 0.208  Table 1: Results of the sentence generation task on the IAPR TC-12 dataset. “B” is short for BLEU.  Sentence Retrival (Image to Text) R@1 R@5 R@10 Med r  Ours-m-RNN 20.9  43.8  54.4  8  Image Retrival (Text to Image) R@1 R@5 R@10 Med r 13.2  31.2  40.8  21  Table 2: R@K and median rank (Med r) for IAPR TC-12 dataset.  Random SDT-RNN-AlexNet Socher-avg-RCNN DeViSE-avg-RCNN DeepFE-AlexNet DeepFE-RCNN Ours-m-RNN-AlexNet  Sentence Retrival (Image to Text) R@1 R@5 R@10 Med r 631 0.1 4.5 32 23 6.0 28 4.8 34 5.9 14 12.6 14.5 11  0.5 18.0 22.7 16.5 19.2 32.9 37.2  1.0 28.6 34.0 27.3 27.3 44.0 48.5  Image Retrival (Text to Image) R@1 R@5 R@10 Med r 500 0.1 6.1 29 25 6.6 29 5.9 32 5.2 15 9.7 11.5 15  0.5 18.5 21.6 20.1 17.6 29.6 31.0  1.0 29.0 31.7 29.6 26.5 42.5 42.4  Table 3: Results of R@K and median rank (Med r) for Flickr8K dataset. “-AlexNet” denotes the image representation based on AlexNet extracted from the whole image frame. “-RCNN” denotes the image representation extracted from possible objects detected by the RCNN algorithm.  For the retrieval tasks, since there are no publicly available results of R@K and Med r in this dataset, we report R@K scores of our method in Table 2 for future comparisons. The result shows that 20.9% top-ranked retrieved sentences and 13.2% top-ranked retrieved images are groundtruth. We also adopt additional evaluation metrics to compare our method with Kiros et al. (2014b), see sup- plementary material Section 10.2.  7.4 RESULTS ON FLICKR8K  This dataset was widely used as a benchmark dataset for image and sentence retrieval. The R@K and Med r of different methods are shown in Table 3. We compare our model with several state-of- the-art methods: SDT-RNN (Socher et al. (2014)), DeViSE (Frome et al. (2013)), DeepFE (Karpathy et al. (2014)) with various image representations. Our model outperforms these methods by a large margin when using the same image representation (e.g. AlexNet). We also list the performance of methods using more sophisticated features in Table 3. “-avg-RCNN” denotes methods with features of the average CNN activation of all objects above a detection conﬁdence threshold. DeepFE-RCNN Karpathy et al. (2014) uses a fragment mapping strategy to better exploit the object detection results. The results show that using these features improves the performance. Even without the help from the object detection methods, however, our method performs better than these methods in almost all the evaluation metrics. We will develop our framework using better image features based on object detection in the future work. The PPL, B-1, B-2, B-3 and B-4 of the generated sentences using our m-RNN-AlexNet model in this dataset are 24.39, 0.565, 0.386, 0.256, and 0.170 respectively.  8  Published as a conference paper at ICLR 2015  Sentence Retrival (Image to Text) R@1 R@5 R@10 Med r  Image Retrival (Text to Image) R@1 R@5 R@10 Med r  Flickr30K  Random DeViSE-avg-RCNN DeepFE-RCNN RVR MNLM-AlexNet MNLM-VggNet NIC LRCN DeepVS Ours-m-RNN-AlexNet Ours-m-RNN-VggNet  Random DeepVS-RCNN Ours-m-RNN-VggNet  0.1 4.8 16.4 12.1 14.8 23.0 17.0 14.0 22.2 18.4 35.4  0.1 29.4 41.0  0.6 16.5 40.2 27.8 39.2 50.7 56.0 34.9 48.2 40.2 63.8  0.6 62.0 73.0  1.1 27.3 54.7 47.8 50.9 62.9  -  47.0 61.4 50.9 73.7  1.1 75.9 83.5  MS COCO  631 28 8 11 10 5 7 11 4.8 10 3  631 2.5 2  0.1 5.9 10.3 12.7 11.8 16.8 17.0  -  15.2 12.6 22.8  0.5 20.1 31.4 33.1 34.0 42.0 57.0  -  37.7 31.2 50.7  1.0 29.6 44.5 44.9 46.3 56.5  - -  50.5 41.5 63.1  0.1 20.9 29.0  0.5 52.8 42.2  1.0 69.2 77.0  500 29 13 12.5 13 8 7 - 9.2 16 5  500 4 3  Table 4: Results of R@K and median rank (Med r) for Flickr30K dataset and MS COCO dataset.  Flickr30K  PPL B-1 B-2 B-3 B-4 PPL B-1 B-2 B-3 B-4 0.19  0.13  -  -  -  -  -  MS COCO  -  - -  0.47 0.21 0.09 21.20 0.50 0.30 0.15  RVR DeepVS-AlexNet DeepVS-VggNet NIC LRCN DMSM Ours-m-RNN-AlexNet 35.11 0.54 0.36 0.23 0.15 Ours-m-RNN-VggNet  0.66 0.59 0.39 0.25 0.16  - - -  - - -  -  -  -  -  -  -  - -  - - - -  0.53 0.28 0.15 19.64 0.57 0.37 0.19  -  -  0.67 0.63 0.44 0.31 0.21 0.21  - -  - -  - -  - - -  -  20.72 0.60 0.41 0.28 0.19 13.60 0.67 0.49 0.35 0.25  Table 5: Results of generated sentences on the Flickr 30K dataset and MS COCO dataset.  RNN Dim. LSTM  Our m-RNN MNLM NIC 512 Yes  256 No  300 Yes  LRCN 1000 (×4)  Yes  RVR DeepVS 100 300-600 No  No  Table 6: Properties of the recurrent layers for the ﬁve very recent methods. LRCN has a stack of four 1000 dimensional LSTM layers. We achieves state-of-the-art performance using a relatively small dimensional recurrent layer. LSTM (Hochreiter & Schmidhuber (1997)) can be treated as a sophisticated version of the RNN.  7.5 RESULTS ON FLICKR30K AND MS COCO  We compare our method with several state-of-the-art methods in these two recently released dataset (Note that the last six methods appear very recently, we use the results reported in their papers): DeViSE (Frome et al. (2013)), DeepFE (Karpathy et al. (2014)), MNLM (Kiros et al. (2014a)), DMSM (Fang et al. (2014)), NIC (Vinyals et al. (2014)), LRCN (Donahue et al. (2014)), RVR (Chen & Zitnick (2014)), and DeepVS (Karpathy & Fei-Fei (2014)). The results of the retrieval tasks and the sentence generation task 7 are shown in Table 4 and Table 5 respectively. We also summarize some of the properties of the recurrent layers adopted in the ﬁve very recent methods in Table 6.  7We only select the word with maximum probability each time in the sentence generation process in Table 5 while many comparing methods (e.g. DMSM, NIC, LRCN) uses a beam search scheme that keeps the best K candidates. The beam search scheme will lead to better performance in practice using the same model.  9  Published as a conference paper at ICLR 2015  m-RNN-greedy-c5 m-RNN-greedy-c40 m-RNN-beam-c5 m-RNN-beam-c40  B1 0.668 0.845 0.680 0.865  B2 0.488 0.730 0.506 0.760  B3 0.342 0.598 0.369 0.641  B4 0.239 0.473 0.272 0.529  CIDEr ROUGE L METEOR 0.729 0.740 0.791 0.789  0.489 0.616 0.499 0.640  0.221 0.291 0.225 0.304  Table 7: Results of the MS COCO test set evaluated by MS COCO evaluation server  Our method with VggNet image representation (Simonyan & Zisserman (2014)) outperforms the state-of-the-art methods, including the very recently released methods, in almost all the evaluation metrics. Note that the dimension of the recurrent layer of our model is relatively small compared to the competing methods. It shows the advantage and efﬁciency of our method that directly inputs the visual information to the multimodal layer instead of storing it in the recurrent layer. The m- RNN model with VggNet performs better than that with AlexNet, which indicates the importance of strong image representations in this task. 71% of the generated sentences for MS COCO datasets are novel (i.e. different from training sentences). We also validate our method on the test set of MS COCO by their evaluation server (Chen et al. (2015)). The results are shown in Table 7. We evaluate our model with greedy inference (select the word with the maximum probability each time) as well as with the beam search inference. “- c5” represents results using 5 reference sentences and “-c40” represents results using 40 reference sentences. To further validate the importance of different components of the m-RNN model, we train sev- eral variants of the original m-RNN model and compare their performance. In particular, we show that the two-layer word embedding system outperforms the single-layer version and the strategy of directly inputting the visual information to the multimodal layer substantially improves the perfor- mance (about 5% for B-1). Due to the limited space, we put the details of these experiments in Section 10.1 in the supplementary material after the main paper.  8 NEAREST NEIGHBOR AS REFERENCE  Recently, Devlin et al. (2015b) proposed a nearest neighbor approach that retrieves the captions of the k nearest images in the training set, ranks these captions according to the consensus of the caption w.r.t. to the rest of the captions, and output the top ranked one. Inspired by this method, we ﬁrst adopt the m-RNN model with the transposed weight sharing strat- egy (Mao et al. (2015), denoted as m-RNN-shared) to generate n hypotheses using a beam search scheme. Speciﬁcally, we keep the n best candidates in the sentence generation process until the model generates the end sign wend. These n best candidates are approximately the n most probable sentences generated by the model, and can be treated as the n hypotheses. In our experiments, we set n = 10 since it gives us a diversiﬁed set of hypotheses without too much outliers on our validation set. 8 After generating the hypotheses of a target image, we retrieve its nearest neighbors in the image feature space on the training set (see details in Section 8.1). Then we calculate the “consensus” scores (Devlin et al. (2015a)) of the hypotheses w.r.t. to the groundtruth captions of the nearest neighbor images, and rerank the hypotheses according to these scores (see details in Section 8.2).  8.1  IMAGE FEATURES FOR THE NEAREST NEIGHBOR IMAGE SEARCH  We try two types of image features for the nearest neighbor image search 9. The ﬁrst one is the original image features extracted by the VggNet (Simonyan & Zisserman (2014)). We ﬁrst resize the image so that its short side is 256 pixels. Then we extract features on ten 224 × 224 windows  8If we directly output the top hypotheses generated by the model, then n = 5 gives us the best performance.  But if we want to rerank the hypotheses, then n = 10 gives us a better result on the validation set.  9We release both types of the features on MS COCO 2014 train, val and test sets. Please refer to the readme  ﬁle at https://github.com/mjhucla/mRNN-CR to see how to download and use them.  10  Published as a conference paper at ICLR 2015  Figure 3: The sample images and their nearest neighbors retrieved by two types of features. Com- pared to the original VggNet features, the features reﬁned by the m-RNN model are better for cap- turing richer and more accurate visual information.  (the four corners, the center and their mirrored versions) on the resized image. Finally, we average pool the ten features to make it a 4,096 dimensional feature. The second type is the feature reﬁned by our m-RNN model. It can be calculated as: Ir = g2(VI·I), where VI is the weight matrix between the image representation and the multimodal layer (see Equation 3), and g2(.) is the scaled hyperbolic tangent function. We show the sample images and their nearest neighbors in Figure 3. We ﬁnd that compared to the original VggNet features, the features reﬁned by the m-RNN model capture richer and more accurate visual information. E.g., the target image in the second row contains an old woman with a bunch of bananas. The original VggNet features do not retrieve images with bananas in them.  8.2 CONSENSUS RERANKING  Suppose we have get the k nearest neighbor images in the training set as the reference. We follow Devlin et al. (2015a) to calculate the consensus score of a hypotheses. The difference is that Devlin et al. (2015a) treat the captions of the k nearest neighbor images as the hypotheses while our hy- potheses are generated by the m-RNN model. More speciﬁcally, for each hypothesis, we calculate the mean similarity between this hypothesis and all the captions of the k nearest neighbor images.  MS COCO val for consensus reranking  B4 CIDEr ROUGE L METEOR  B2  B1 m-RNN-shared 0.686 0.511 0.375 0.280 0.842 m-RNN-shared-NNref-BLEU 0.718 0.550 0.409 0.305 0.909 m-RNN-shared-NNref-CIDEr 0.714 0.543 0.406 0.304 0.938 m-RNN-shared-NNref-BLEU-Orcale 0.792 0.663 0.543 0.443 1.235 m-RNN-shared-NNref-CIDEr-Oracle 0.784 0.648 0.529 0.430 1.272  B3  m-RNN-shared m-RNN-shared-NNref-BLEU m-RNN-shared-NNref-CIDEr  MS COCO 2014 test server  B3  B2  B1 0.685 0.512 0.376 0.279 0.819 0.720 0.553 0.410 0.302 0.886 0.716 0.545 0.404 0.299 0.917  B4 CIDEr ROUGE L METEOR  0.500 0.519 0.519 0.602 0.593  0.228 0.235 0.239 0.287 0.287  0.504 0.524 0.521  0.229 0.238 0.242  Table 8: Results of m-RNN-shared model after applying consensus reranking using nearest neigh- bors as references (m-RNN-shared-NNref), compared with those of the original m-RNN model on our validation set and MS COCO test server.  11  Target ImageNearest Five Neighbors In Terms of m-RNN Refined FeatureNearest Five Neighbors In Terms of Original VGG FeaturePublished as a conference paper at ICLR 2015  The consensus score of this hypothesis is the mean similarity score of the m nearest captions. The similarity between a hypothesis and one of its nearest neighbor reference captions is deﬁned by a sentence-level BLEU score (Papineni et al. (2002)) or a sentence-level CIDEr (Vedantam et al. (2014)). We cross-validate the hyperparamters k and m. For the BLEU-based similarity, the opti- mal k and m are 60 and 175 respectively. For the CIDEr-based similarity, the optimal k and m are 60 and 125 respectively.  8.3 EXPERIMENTS  We show the results of our model on our validation set and the MS COCO testing server in Table 8. For BLEU-based consensus reranking, we get an improvement of 3.5 points on our validation set and 3.3 points on the MS COCO test 2014 set in terms of BLEU4 score. For the CIDEr-based consensus reranking, we get an improvement of 9.4 points on our validation set and 9.8 points on the MS COCO test 2014 set in terms of CIDEr.  8.4 DISCUSSION  We show the rank of the ten hypotheses before and after reranking in Figure 4. Although the hy- potheses are similar to each other, there are some variances among them (E.g., some of them capture more details of the images. Some of them might be partially wrong). The reranking process is able to improve the rank of good captions. We also show the oracle performance of the ten hypotheses, which is the upper bound of the con- sensus reranking. More speciﬁcally, for each image in our validation set, we rerank the hypotheses according to the scores (BLEU or CIDEr) w.r.t to the groundtruth captions. The results of this oracle reranking are shown in Table 8 (see rows with “-oracle”). The oracle performance is surprisingly high, indicating that there is still room for improvement, both for the m-RNN model itself and the reranking strategy.  9 CONCLUSION  We propose a multimodal Recurrent Neural Network (m-RNN) framework that performs at the state-of-the-art in three tasks: sentence generation, sentence retrieval given query image and image  Figure 4: The original rank of the hypotheses and the rank after consensus reranking (CIDEr).  12  Original After Reranking (C(Der) 1. a piece of cake on a plate on a table 2. a piece of cake on a white plate 3. a piece of cake sitting on top of a white plate 4. a piece of cake sitting on top of a plate 5. a piece of cake on a plate with a fork 6. a close up of a piece of cake on a plate 7. a piece of chocolate cake on a plate 8. a piece of cake sitting on a plate 9. a slice of cake on a white plate 10. a slice of cake on a plate with a fork 1. a piece of cake on a plate with a fork 2. a slice of cake on a plate with a fork 3. a close up of a piece of cake on a plate 4. a piece of cake on a plate on a table 5. a piece of cake on a white plate 6. a piece of cake sitting on top of a plate 7. a piece of cake sitting on top of a white plate 8. a piece of chocolate cake on a plate 9. a piece of cake sitting on a plate 10. a slice of cake on a white plate 1. a black and white photo of a black bear 2. a black and white photo of a bear 3. a black bear laying on top of a rock 4. a black bear sitting on top of a wooden bench 5. a black bear sitting on top of a rock 6. a black bear laying on top of a wooden bench 7. a black and white photo of a dog 8. a black bear laying on top of a wooden floor 9. a close up of a black and white dog 10. a close up of a black and white cat 1. a black bear sitting on top of a rock 2. a black bear laying on top of a rock 3. a black bear sitting on top of a wooden bench 4. a black bear laying on top of a wooden bench 5. a black bear laying on top of a wooden floor 6. a black and white photo of a black bear 7. a black and white photo of a bear 8. a close up of a black and white dog 9. a black and white photo of a dog 10. a close up of a black and white cat 1. a group of people standing next to each other 2. a group of people standing around a train 3. a group of people standing in a room 4. a group of people in a room with luggage 5. a group of people that are standing in a room 6. a group of people standing next to a train 7. a group of people standing in front of a train 8. a group of people sitting on a bench 9. a group of people standing in a room with luggage 10. a group of people standing next to each other on a train 1. a group of people standing in a room with luggage 2. a group of people in a room with luggage 3. a group of people standing next to a train 4. a group of people standing in front of a train 5. a group of people standing around a train 6. a group of people standing next to each other on a train 7. a group of people standing in a room 8. a group of people standing next to each other 9. a group of people that are standing in a room 10. a group of people sitting on a bench Published as a conference paper at ICLR 2015  retrieval given query sentence. The model consists of a deep RNN, a deep CNN and these two sub-networks interact with each other in a multimodal layer. Our m-RNN is powerful of connecting images and sentences and is ﬂexible to incorporate more complex image representations and more sophisticated language models.  ACKNOWLEDGMENTS  We thank Andrew Ng, Kai Yu, Chang Huang, Duohao Qin, Haoyuan Gao, Jason Eisner for useful discussions and technical support. We also thank the comments and suggestions of the anonymous reviewers from ICLR 2015 and NIPS 2014 Deep Learning Workshop. We acknowledge the Center for Minds, Brains and Machines (CBMM), partially funded by NSF STC award CCF-1231216, and ARO 62250-CS.  REFERENCES Barnard, Kobus, Duygulu, Pinar, Forsyth, David, De Freitas, Nando, Blei, David M, and Jordan,  Michael I. Matching words and pictures. JMLR, 3:1107–1135, 2003.  Chen, X., Fang, H., Lin, TY, Vedantam, R., Gupta, S., Dollr, P., and Zitnick, C. L. Microsoft coco  captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.  Chen, Xinlei and Zitnick, C Lawrence. Learning a recurrent visual representation for image caption  generation. arXiv preprint arXiv:1411.5654, 2014.  Cho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.  Devlin, Jacob, Cheng, Hao, Fang, Hao, Gupta, Saurabh, Deng, Li, He, Xiaodong, Zweig, Geoffrey, and Mitchell, Margaret. Language models for image captioning: The quirks and what works. arXiv preprint arXiv:1505.01809, 2015a.  Devlin, Jacob, Gupta, Saurabh, Girshick, Ross, Mitchell, Margaret, and Zitnick, C Lawrence. Ex- ploring nearest neighbor approaches for image captioning. arXiv preprint arXiv:1505.04467, 2015b.  Donahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio, Rohrbach, Marcus, Venugopalan, Sub- hashini, Saenko, Kate, and Darrell, Trevor. Long-term recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389, 2014.  Elman, Jeffrey L. Finding structure in time. Cognitive science, 14(2):179–211, 1990.  Fang, Hao, Gupta, Saurabh, Iandola, Forrest, Srivastava, Rupesh, Deng, Li, Doll´ar, Piotr, Gao, Jianfeng, He, Xiaodong, Mitchell, Margaret, Platt, John, et al. From captions to visual concepts and back. arXiv preprint arXiv:1411.4952, 2014.  Farhadi, Ali, Hejrati, Mohsen, Sadeghi, Mohammad Amin, Young, Peter, Rashtchian, Cyrus, Hock- enmaier, Julia, and Forsyth, David. Every picture tells a story: Generating sentences from images. In ECCV, pp. 15–29. 2010.  Frome, Andrea, Corrado, Greg S, Shlens, Jon, Bengio, Samy, Dean, Jeff, Mikolov, Tomas, et al.  Devise: A deep visual-semantic embedding model. In NIPS, pp. 2121–2129, 2013.  Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object  detection and semantic segmentation. In CVPR, 2014.  Grubinger, Michael, Clough, Paul, M¨uller, Henning, and Deselaers, Thomas. The iapr tc-12 bench- In International Workshop  mark: A new evaluation resource for visual information systems. OntoImage, pp. 13–23, 2006.  Guillaumin, Matthieu, Verbeek, Jakob, and Schmid, Cordelia. Multiple instance metric learning  from automatically labeled bags of faces. In ECCV, pp. 634–647, 2010.  13  Published as a conference paper at ICLR 2015  Gupta, Ankush and Mannem, Prashanth. From image annotation to image description. In ICONIP,  2012.  Gupta, Ankush, Verma, Yashaswi, and Jawahar, CV. Choosing linguistics over vision to describe  images. In AAAI, 2012.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Hodosh, Micah, Young, Peter, and Hockenmaier, Julia. Framing image description as a ranking  task: Data, models and evaluation metrics. JAIR, 47:853–899, 2013.  Jia, Yangqing, Salzmann, Mathieu, and Darrell, Trevor. Learning cross-modality similarity for  multinomial data. In ICCV, pp. 2407–2414, 2011.  Kalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. In EMNLP, pp.  1700–1709, 2013.  Karpathy, Andrej and Fei-Fei, Li. Deep visual-semantic alignments for generating image descrip-  tions. arXiv preprint arXiv:1412.2306, 2014.  Karpathy, Andrej, Joulin, Armand, and Fei-Fei, Li. Deep fragment embeddings for bidirectional  image sentence mapping. In arXiv:1406.5679, 2014.  Kiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S. Unifying visual-semantic embeddings  with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014a.  Kiros, Ryan, Zemel, R, and Salakhutdinov, Ruslan. Multimodal neural language models. In ICML,  2014b.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, pp. 1097–1105, 2012.  Kulkarni, Girish, Premraj, Visruth, Dhar, Sagnik, Li, Siming, Choi, Yejin, Berg, Alexander C, and  Berg, Tamara L. Baby talk: Understanding and generating image descriptions. In CVPR, 2011.  Kuznetsova, Polina, Ordonez, Vicente, Berg, Tamara L, and Choi, Yejin. Treetalk: Composition and compression of trees for image descriptions. Transactions of the Association for Computational Linguistics, 2(10):351–362, 2014.  LeCun, Yann A, Bottou, L´eon, Orr, Genevieve B, and M¨uller, Klaus-Robert. Efﬁcient backprop. In  Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012.  Lin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Doll´ar, Piotr, and Zitnick, C Lawrence. Microsoft coco: Common objects in context. arXiv preprint arXiv:1405.0312, 2014.  Mao, Junhua, Xu, Wei, Yang, Yi, Wang, Jiang, and Yuille, Alan L. Explain images with multimodal  recurrent neural networks. NIPS DeepLearning Workshop, 2014.  Mao, Junhua, Xu, Wei, Yang, Yi, Wang, Jiang, Huang, Zhiheng, and Yuille, Alan. Learning like a child: Fast novel visual concept learning from sentence descriptions of images. arXiv preprint arXiv:1504.06692, 2015.  Mikolov, Tomas, Karaﬁ´at, Martin, Burget, Lukas, Cernock`y, Jan, and Khudanpur, Sanjeev. Recur-  rent neural network based language model. In INTERSPEECH, pp. 1045–1048, 2010.  Mikolov, Tomas, Kombrink, Stefan, Burget, Lukas, Cernocky, JH, and Khudanpur, Sanjeev. Exten-  sions of recurrent neural network language model. In ICASSP, pp. 5528–5531, 2011.  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed represen-  tations of words and phrases and their compositionality. In NIPS, pp. 3111–3119, 2013.  14  Published as a conference paper at ICLR 2015  Mitchell, Margaret, Han, Xufeng, Dodge, Jesse, Mensch, Alyssa, Goyal, Amit, Berg, Alex, Ya- maguchi, Kota, Berg, Tamara, Stratos, Karl, and Daum´e III, Hal. Midge: Generating image descriptions from computer vision detections. In EACL, 2012.  Mnih, Andriy and Hinton, Geoffrey. Three new graphical models for statistical language modelling.  In ICML, pp. 641–648. ACM, 2007.  Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines.  In ICML, pp. 807–814, 2010.  Papineni, Kishore, Roukos, Salim, Ward, Todd, and Zhu, Wei-Jing. Bleu: a method for automatic  evaluation of machine translation. In ACL, pp. 311–318, 2002.  Rashtchian, Cyrus, Young, Peter, Hodosh, Micah, and Hockenmaier, Julia. Collecting image anno-  tations using amazon’s mechanical turk. In NAACL-HLT workshop 2010, pp. 139–147, 2010.  Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning representations by  back-propagating errors. Cognitive modeling, 1988.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge, 2014.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. arXiv preprint arXiv:1409.1556, 2014.  Socher, Richard, Le, Q, Manning, C, and Ng, A. Grounded compositional semantics for ﬁnding and  describing images with sentences. In TACL, 2014.  Srivastava, Nitish and Salakhutdinov, Ruslan. Multimodal learning with deep boltzmann machines.  In NIPS, pp. 2222–2230, 2012.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural net-  works. In NIPS, pp. 3104–3112, 2014.  Vedantam, Ramakrishna, Zitnick, C Lawrence, and Parikh, Devi. Cider: Consensus-based image  description evaluation. arXiv preprint arXiv:1411.5726, 2014.  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural  image caption generator. arXiv preprint arXiv:1411.4555, 2014.  Young, Peter, Lai, Alice, Hodosh, Micah, and Hockenmaier, Julia. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In ACL, pp. 479–488, 2014.  10 SUPPLEMENTARY MATERIAL  10.1 EFFECTIVENESS OF THE DIFFERENT COMPONENTS OF THE M-RNN MODEL  m-RNN m-RNN-NoEmbInput m-RNN-OneLayerEmb m-RNN-EmbOneInput m-RNN-visInRnn m-RNN-visInRnn-both m-RNN-visInRnn-both-shared  B-1 0.600 0.592 0.594 0.590 0.466 0.546 0.478  B-2 0.412 0.408 0.406 0.406 0.267 0.333 0.279  B-3 0.278 0.277 0.274 0.274 0.157 0.191 0.171  B-4 0.187 0.188 0.184 0.185 0.101 0.120 0.110  Table 9: Performance comparison of different versions of m-RNN models on the Flickr30K dataset. All the models adopt VggNet as the image representation. See Figure 5 for details of the models.  15  Published as a conference paper at ICLR 2015  Figure 5: Illustration of the seven variants of the m-RNN models.  In this section, we compare different variants of our m-RNN model to show the effectiveness of the two-layer word embedding and the strategy to input the visual information to the multimodal layer. The word embedding system. Intuitively, the two word embedding layers capture high-level se- mantic meanings of words more efﬁciently than the single layer word embedding. As an input to the multimodal layer, it offers useful information for predicting the next word distribution. To validate its efﬁciency, we train three different m-RNN networks: m-RNN-NoEmbInput, m-RNN- OneLayerEmb, m-RNN-EmbOneInput. They are illustrated in Figure 5. “m-RNN-NoEmbInput” denotes the m-RNN model whose connection between the word embedding layer II and the mul- timodal layer is cut off. Thus the multimodal layer has only two inputs: the recurrent layer and the image representation. “m-RNN-OneLayerEmb” denotes the m-RNN model whose two word embedding layers are replaced by a single 256 dimensional word-embedding layer. There are much more parameters of the word-embedding layers in the m-RNN-OneLayerEmb than those in the original m-RNN (256 · M v.s. 128 · M + 128 · 256) if the dictionary size M is large. “m-RNN- EmbOneInput” denotes the m-RNN model whose connection between the word embedding layer II and the multimodal layer is replaced by the connection between the word embedding layer I and the multimodal layer. The performance comparisons are shown in Table 9. Table 9 shows that the original m-RNN model with the two word embedding layers and the con- nection between word embedding layer II and multimodal layer performs the best. It veriﬁes the effectiveness of the two word embedding layers. How to connect the vision and the language part of the model. We train three variants of m-RNN models where the image representation is inputted into the recurrent layer: m-RNN-VisualInRNN, m-RNN-VisualInRNN-both, and m-RNN-VisualInRNN-Both-Shared. For m-RNN-VisualInRNN, we only input the image representation to the word embedding layer II while for the later two mod- els, we input the image representation to both the multimodal layer and word embedding layer II.  16  Embedding IEmbedding IIRecurrentMultimodalSoftMaxwstartImageCNNPredictw1The Original m-RNN model for one time frame128256256512wstartImageCNNPredictw1128256256512m-RNN-NoEmbInputwstartImageCNNPredictw1256256512m-RNN-OneLayerEmbwstartImageCNNPredictw1128256256512m-RNN-EmbOneInputwstartImageCNNPredictw1128256256512wstartImageCNNPredictw1128256256512VI(1)VI(2)wstartImageCNNPredictw1128256256512VIVIm-RNN-VisualInRnnm-RNN-VisualInRnn-Bothm-RNN-VisualInRnn-Both-SharedPublished as a conference paper at ICLR 2015  I  I  , V (2)  The weights of the two connections V (1) are shared for m-RNN-VisualInRNN-Both-Shared. Please see details of these models in Figure 5. Table 9 shows that the original m-RNN model performs much better than these models, indicating that it is effective to directly input the visual information to the multimodal layer. In practice, we ﬁnd that it is harder to train these variants than to train the original m-RNN model and we have to keep the learning rate very small to avoid the exploding gradient problem. Increasing the dimension of the recurrent layer or replacing RNN with LSTM (a sophisticated version of RNN Hochreiter & Schmidhuber (1997)) might solve the problem. We will explore this issue in future work.  10.2 ADDITIONAL RETRIEVAL PERFORMANCE COMPARISONS ON IAPR TC-12  For the retrieval results in this dataset, in addition to the R@K and Med r, we also adopt exactly the same evaluation metrics as Kiros et al. (2014b) and plot the mean number of matches of the retrieved groundtruth sentences or images with respect to the percentage of the retrieved sentences or images for the testing set. For the sentence retrieval task, Kiros et al. (2014b) uses a shortlist of 100 images which are the nearest neighbors of the query image in the feature space. This shortlist strategy makes the task harder because similar images might have similar descriptions and it is often harder to ﬁnd subtle differences among the sentences and pick the most suitable one. The recall accuracy curves with respect to the percentage of retrieved images (sentence retrieval task) or sentences (sentence retrieval task) are shown in Figure 6. The ﬁrst method, bowdecaf, is a strong image based bag-of-words baseline (Kiros et al. (2014b)). The second and the third models (Kiros et al. (2014b)) are all multimodal deep models. Our m-RNN model signiﬁcantly outperforms these three methods in this task.  10.3 THE CALCULATION OF BLEU SCORE  The BLEU score was proposed by Papineni et al. (2002) and was originally used as a evaluation metric for machine translation. To calculate BLEU-N (i.e. B-N in the paper where N=1,2,3,4) score, we ﬁrst compute the modiﬁed n-gram precision (Papineni et al. (2002)), pn. Then we compute the geometric mean of pn up to length N and multiply it by a brevity penalty BP:  (8)  (9) where r is the length of the reference sentence and c is the length of the generated sentence. We use the same strategy as Fang et al. (2014) where pn, r, and c are computed over the whole testing corpus. When there are multiple reference sentences, the length of the reference that is closest (longer or shorter) to the length of the candidate is used to compute r.  n=1 log pn  BP = min(1, e1− r c ) (cid:80)N  B-N = BP · e  1 N  (a) Image to Text Curve  (b) Text to Image Curve  Figure 6: Retrieval recall curve for (a). Sentence retrieval task (b). Image retrieval task on IAPR TC-12 dataset. The behavior on the far left (i.e. top few retrievals) is most important.  17  0.010.020.050.1 0.250.5 1   00.10.20.30.40.50.60.70.80.91  Ours−mRNNbow−decafMLBL−F−decafMLBL−B−decaf0.00050.001 0.002 0.005 0.01  0.02  0.05  0.1   0.25  0.5   1     00.10.20.30.40.50.60.70.80.91  Ours−mRNNbow−decafMLBL−F−decafMLBL−B−decaf",
1412.5903,2015,Deep Structured Output Learning for Unconstrained Text Recognition,"['Deep Structured Output Learning for Unconstrained Text Recognition', 'Max Jaderberg', 'Karen Simonyan', 'Andrea Vedaldi', 'and Andrew Zisserman']",https://arxiv.org/pdf/1412.5903,"5 1 0 2    r p A 0 1         ]  V C . s c [      5 v 3 0 9 5  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  DEEP STRUCTURED OUTPUT LEARNING FOR UNCONSTRAINED TEXT RECOGNITION  Max Jaderberg∗, Karen Simonyan*, Andrea Vedaldi & Andrew Zisserman+ Visual Geometry Group, Department of Engineering Science, University of Oxford {max,karen,vedaldi,az}@robots.ox.ac.uk  ABSTRACT  We develop a representation suitable for the unconstrained recognition of words in natural images, where unconstrained means that there is no ﬁxed lexicon and words have unknown length. To this end we propose a convolutional neural network (CNN) based architecture which incorporates a Conditional Random Field (CRF) graphical model, taking the whole word image as a single input. The unaries of the CRF are provided by a CNN that predicts characters at each position of the output, while higher order terms are provided by another CNN that detects the presence of N-grams. We show that this entire model (CRF, character predictor, N-gram predictor) can be jointly optimised by back-propagating the structured output loss, essentially requiring the system to perform multi-task learning, and training requires only synthetically generated data. The resulting model is a more accurate system on standard real-world text recognition benchmarks than character prediction alone, setting a benchmark for systems that have not been trained on a particular lexicon. In addition, our model achieves state-of-the-art accuracy in lexicon-constrained scenarios, without being speciﬁcally modelled for constrained recognition. To test the generalisation of our model, we also perform experiments with random alpha-numeric strings to evaluate the method when no visual language model is applicable.  1  INTRODUCTION  In this work we tackle the problem of unconstrained text recognition – recognising text in natural images without restricting the words to a ﬁxed lexicon or dictionary. Usually this problem is de- composed into a word detection stage followed by a word recognition stage. The word detection stage generates bounding boxes around words in an image, while the word recognition stage takes the content of these bounding boxes and recognises the text within. This paper focuses on the text recognition stage, developing a model based on deep convolutional neural networks (CNNs) (LeCun et al. (1998)). Previous methods using CNNs for word recognition (discussed in more detail in section Section 2) has either constrained (Jaderberg et al. (2014b)) or heavily weighted (Bissacco et al. (2013)) the recognition results to be from a dictionary of known words. This works very well when training and testing are limited to a ﬁxed vocabulary, but does not generalise to where previously unseen or non-language based text must be recognised – for example for generic alpha-numeric strings such as number plates or phone numbers. The shift of focus towards a model which performs accurately without a ﬁxed dictionary increases the complexity of the text recognition problem. To solve this, we propose a novel CNN architecture (Figure 2) employing a Conditional Random Field (CRF) whose unary terms are outputs of a CNN character predictor, which are position-dependent, and whose higher order terms are outputs of a CNN N-gram predictor, which are position-independent. The recognition result is then obtained by ﬁnding the character sequence that maximises the CRF score, enforcing the consistency of the individual predictions.  ∗Current afﬁliation Google DeepMind. +Current afﬁliation University of Oxford and Google DeepMind.  1  Published as a conference paper at ICLR 2015  The CRF model builds on our previous work where we explored dictionary-based recognition (Jader- berg et al. (2014a)) for two scenarios: the ﬁrst was to train a different CNN character classiﬁer for each position in the word being recognised, using the whole image of the word as input to each classiﬁer (an idea also expored by Goodfellow et al. (2013)); the second was to construct a CNN predictor to detect the N-grams contained in the word, effectively encoding the text as a bag-of-N- grams. The dictionary-free joint model proposed here is trained by deﬁning a structured output learning problem, and back-propagating the corresponding structured output loss. This formulation results in multi-task learning of both the character and N-gram predictors, and additionally learns how to combine their representations in the CRF, resulting in more accurate text recognition. The result is a highly ﬂexible text recognition system that achieves excellent unconstrained text recognition performance as well as state-of-the-art recognition performance when using standard dictionary constraints. While performance is measured on real images as contained in standard text recognition benchmarks, all results are obtained by training the model purely on synthetic data. The model is evaluated on this synthetic data as well in order to study its performance under different scenarios. Section 2 outlines work related to ours. Section 3.1 reviews the character sequence model and Section 3.2 the bag-of-N-grams model. Section 4 shows how these predictors can be combined to form a joint CRF model and formulates the training of the latter as structured-output learning. Section 5 evaluates these models extensively and Section 6 summarises our ﬁndings.  2 RELATED WORK  We concentrate here on text recognition methods, recognising from a cropped image of a single word, rather than the text detection stages of scene text recognition (‘text spotting’) that generate the word detections. Traditional text recognition methods are based on sequential character classiﬁcation, ﬁnding char- acters by sliding window methods (Wang et al. (2011; 2012); Jaderberg et al. (2014c), after which a word prediction is made by integrating character classiﬁer predictions in a left-to-right manner. The character classiﬁers include random ferns (Ozuysal et al. (2007)) in Wang et al. (2011), and CNNs in Wang et al. (2012); Jaderberg et al. (2014c). Both Wang et al. (2011) and Wang et al. (2012) use a small ﬁxed lexicon as a language model to constrain word recognition. More recent works such as Bissacco et al. (2013); Alsharif & Pineau (2014) make use of over- segmentation methods, guided by a supervised classiﬁer, to generate candidate character proposals in a single-word image, which are subsequently classiﬁed as true or false positives. For example, PhotoOCR (Bissacco et al. (2013)) uses binarization and a sliding window classiﬁer to generate can- didate character regions, with words recognised through a beam search driven by classiﬁer scores and static N-gram language model, followed by a re-ranking using a dictionary of 100k words. Jader- berg et al. (2014c) uses the convolutional nature of CNNs to generate response maps for characters and bigrams which are integrated to score lexicon words. In contrast to these approaches based on character classiﬁcation, the work by Almaz´an et al. (2014); Gordo (2014); Goel et al. (2013); Rodriguez-Serrano et al. (2013); Novikova et al. (2012); Mishra et al. (2012) instead uses the notion of holistic word recognition. Mishra et al. (2012); Novikova et al. (2012) still rely on explicit character classiﬁers, but construct a graph to infer the word, pool- ing together the full word evidence. Rodriguez-Serrano et al. (2013) use aggregated Fisher Vec- tors (Perronnin et al. (2010)) and a Structured SVM framework to create a joint word-image and text embedding. Almaz´an et al. (2014) and more recently Gordo (2014) also formluate joint embed- ding spaces, achieving impressive results with minimal training data. Goel et al. (2013) use whole word-image features to recognize words by comparing to simple black-and-white font-renderings of lexicon words. In our own previous work (Jaderberg et al. (2014a;b)) we use large CNNs acting on the full word image region to perform 90k-way classiﬁcation to a dictionary word. It should be noted that all the methods make use of strong static language models, either relying on a constrained dictionary or re-ranking mechanism.  2  Published as a conference paper at ICLR 2015  (a)  (b)  Figure 1: (a) The character sequence model. A word image is recognised by predicting the character at each position in the output, spelling out the text character by character. Each positional classiﬁer is learnt independently but shares a jointly optimised set of features. (b) The N-gram encoding model. The recognised text is represented by its bag-of-N-grams. This can be thought of as 10k independently trained binary classiﬁers using a shared set of jointly learnt features, trained to detect the presence of a particular N-gram.  Goodfellow et al. (2013) had great success using a CNN with multiple position-sensitive character classiﬁer outputs (closely related to the character sequence model in Section 3.1) to perform street number recognition. This model was extended to CAPTCHA sequences (up to 8 characters long) where they demonstrated impressive performance using synthetic training data for a synthetic prob- lem (where the generative model is known), but we show that synthetic training data can be used for a real-world data problem (where the generative model is unknown). There have been previous uses of graphical models with back-propagated loss functions for neural networks, such as the early text recognition work of LeCun et al. (1998) to combine character clas- siﬁer results on image segmentations. Another example is the recent work of Tompson et al. (2014) for human pose estimation, where an MRF-like model over the distribution of spatial locations for each body part is constructed, incorporating a single round of message-passing.  3 CNN TEXT RECOGNITION MODELS  We now review the component CNN models, originally presented in our tech report Jaderberg et al. (2014a), that form the basis of our joint model in Section 4.  3.1 CHARACTER SEQUENCE MODEL REVIEW  In this section we describe our character sequence model. This model encodes the character at each position in the word and so predicts the sequence of characters in an image region (hereafter we simply refer to the image region as an image). Each position in the word is modelled by an independent classiﬁer acting on a shared set of features from a single CNN. By construction, this model makes no assumptions about the underlying language and allows completely unconstrained recognition. A word w of length N is modelled as a sequence of characters such that w = (c1, c2, . . . , cN ) where each ci ∈ C = {1, 2, . . . , 36} represents a character at position i in the word, from the set of 10 digits and 26 letters. Each ci can be predicted with a single classiﬁer, one for each character in the word. However, since words have variable length N which is unknown at test time, we ﬁx the number of characters to Nmax (here set to 23), the maximum length of a word in the training set, and introduce a null character class. Therefore a word is represented by a string w ∈ (C ∪ {φ})Nmax. For a given input image x, we want to return the estimated word w∗ which maximises P (w∗|x). Since we seek an unconstrained recognition system with this model, we assume independence be- tween characters leading to  w∗ = arg max  w  P (w|x) = arg max  c1,c2,...,cNmax  P (ci|Φ(x))  (1)  where P (ci|Φ(x)) is given by the classiﬁer for the i-th position acting on a single set of shared CNN features Φ(x). The word w∗ can be computed by taking the most probable character at each position i = arg maxci∈C∪{φ} P (ci|Φ(x)). c∗  3  Nmax(cid:89)  i=1  Published as a conference paper at ICLR 2015  The CNN (Figure 1 (a)) takes the whole word image x as input. Word images can be of different sizes, in particular due to the variable number of characters in the image. However, our CNN requires a ﬁxed size input for all input images. This problem is overcome by simply resampling the original word image to a canonical height and width, without regard to preserving the aspect ratio, producing a ﬁxed size input x. The base CNN has a number of convolutional layers followed by a series of fully connected layers, giving Φ(x). The full details of the network architecture are given in Section 5.2. Φ(x) is fed to Nmax separate fully connected layers with 37 neurons each, one for each character class including the null character. These fully connected layers are independently softmax normalised and can be interpreted as the probabilities P (ci|Φ(x)) of the width-resized input image x. The CNN is trained with multinomial logistic regression loss, back-propagation, and stochastic gra- dient descent (SGD) with dropout regularisation similar to Hinton et al. (2012).  3.2 BAG-OF-N-GRAMS MODEL REVIEW  This section describes our second word recognition model, which exploits compositionality to rep- resent words. In contrast to the sequential character encoding of Section 3.1, words can be seen as a composition of an unordered set of character N-grams, a bag-of-N-grams. In the follow- ing, if s ∈ CN and w ∈ CM are two strings, the symbol s ⊂ w indicates that s is a sub- string of w. An N-gram of word w is a substring s ⊂ w of length |s| = N. We will de- note with GN (w) = {s : s ⊂ w ∧ |s| ≤ N} the set of all N-grams of word w of length up to N and with GN = ∪w∈W GN (w) the set of all such grams in the language. For example, G3(spires) = {s, p, i, r, e, sp, pi, ir, re, es, spi, pir, ire, res}. This method of encod- ing variable length sequences is similar to the Wickelphone phoneme-encoding methods (Wickelgran (1969)). Even for small values of N, GN (w) encodes each word w ∈ W nearly uniquely. For example, with N = 4, this map has only 7 collisions out of a dictionary of 90k words. The encoding GN (w) can be represented as a |GN|-dimensional binary vector of N-gram occurrences. This vector is very sparse, as on average |GN (w)| ≈ 22 whereas |GN| = 10k. Using a CNN we can predict GN (w) for a word w depicted in the input image x. We can use the same architecture as in Section 3.1, but now have a ﬁnal fully connected layer with GN neurons to represent the encoding vector. The scores from the fully connected layer can be interpreted as probabilities of an N-gram being present in the image by applying the logistic function to each neuron. The CNN is therefore learning to recognise the presence of each N-gram somewhere within the input image, so is an N-gram detector. With the applied logistic function, the training problem becomes that of |GN| separate binary clas- siﬁcation tasks, and so we back-propagate the logistic regression loss with respect to each N-gram class independently. To jointly train a whole range of N-grams, some of which occur very frequently and some barely at all, we have to scale the gradients for each N-gram class by the inverse frequency of their appearance in the training word corpus. We also experimented with hinge loss and simple regression to train but found frequency weighted binary logistic regression was superior. As with the other model, we use dropout and SGD. In this model we exploit the statistics of our underlying language in choosing a subset of |GN| N- grams from the space of all possible N-grams to be modelled. This can be seen as using a language model to compress the representation space of the encoding, but is not restraining the predictive capability for unconstrained recognition. While the encoding GN (w) is almost always unique for words from natural language, non-language words often contain much fewer N-grams from the modelled set GN leading to more ambiguous and non-unique encodings.  4  JOINT MODEL  imising the log-score log P (w|x) = S(w, x) =(cid:80)Nmax  In Section 3.1, maximising the posterior probability of a character sequence (1) is equivalent to max- c(ci, x) = log P (ci|Φ(x)) is the logarithm of the posterior probability of the character at position i in the sequence. The graph  c(ci, x) where Si  i=1 Si  4  Published as a conference paper at ICLR 2015  Figure 2: An illustration of the construction of the path score S(camel, x) for the word camel. The unary and edge terms used for the score are selected by the path through the graph of character positions shown in the upper right corner. The values of these terms, Sc(ci, x) and Se(s, x), where s ⊂ w, are given by the outputs of the character sequence model CNN (CHAR CNN) and the N-gram encoding CNN (NGRAM CNN).  associated with this function is a set of nodes, one for each unary term Si contain any edges. Hence maximising the function reduces to maximising each term individually. The model can now be extended to incorporate the N-gram predictors of Section 3.2, encoding the presence of N-grams in the word image x. The N-gram scoring function Se(s, x) assigns a score to each string s of length |s| ≤ N, where N is the maximum order of N-gram modelled. Note that, c deﬁned before, the function Se is position-independent. However, differently from the functions Si it is applied repeatedly at each position i in the word:  c(ci, x), and does not  Nmax(cid:88)  |w|(cid:88)  min(N,|w|−i+1)(cid:88)  S(w, x) =  Si  c(ci, x) +  Se(cici+1 . . . ci+n−1, x).  (2)  i=1  i=1  n=1  c(ci, x) are obtained from the CNN character predictors of As illustrated in Figure 2, the scores Si Section 3.1 whereas the score Se(s, x) is obtained from the CNN N-gram predictor of Section 3.2; note that the N-gram scoring function is only deﬁned for the subset GN of N-grams modelled in the CNN; if s (cid:54)∈ GN , the score Se(s, x) = 0 is deﬁned to be zero. The graph associated with the function (2) has cliques of order N; hence, when N is even moderately large, we resort to beam search (Russel et al. (1994)) to maximise (2) and ﬁnd the predicted word w∗. Also, the score (2) can be interpreted as a potential function deﬁning a word posterior probability as before; however, evaluating this probability would require computing a normalisation factor, which is non-trivial. Instead, the function is trained discriminatively, as explained in the next section.  Structured Output Loss. The unary and edge score functions Si c(ci, x) and Se(s, x), should in- corporate the outputs of the character sequence model and N-gram encoding model respectively. A simple way to do this is to apply a weighting to the output of the CNNs after removing the softmax normalisation and the logistic loss:  Nmax(cid:88)  |w|(cid:88)  min(N,|w|−i+1)(cid:88)  i=1  i=1  n=1  S(w, x) =  αi ci  f i ci  (x) +  βcici+1...ci+n−1gcici+1...ci+n−1 (x),  (3)  ci  (x) is the output of the character sequence CNN for character ci at position i and gs(x) If desired, the character weights } and edge weights β = {βs} can be constrained to be shared across different characters,  where f i ci is the output of the N-gram encoding CNN for the N-gram s. α = {αi character positions, different N-grams of the same order, or across all N-grams. The sets of weights α and β in Equation 3, or any weight-constrained variant of Equation 3, can be learnt in a structured output learning framework, encouraging the score of the ground-truth word wgt to be greater than or equal to the highest scoring incorrect word prediction plus a margin, i.e. S(wgt, x) ≥ µ + S(w∗, x) where S(w∗, x) = maxw(cid:54)=wgt S(w, x). Enforcing this as a soft- constraint results in the convex loss  max(0, µ + S(w, x) − S(wgt,i, xi))  (4)  L(xi, wgt,i, S) = max w(cid:54)=wgt,i  and averaging over M example pairs (xi, wgt,i) results in the regularised empirical risk objective  L(xi, wgt,i, S).  (5)  M(cid:88)  i=1  E(S) =  (cid:107)α(cid:107)2 +  λα 2  λβ 2  (cid:107)β(cid:107)2 +  1 M  5  Nmax(cid:88)  (cid:26)1  Published as a conference paper at ICLR 2015  Figure 3: The architecture for training the joint model, comprising of the character sequence model (CHAR) and and the N-gram encoding model (NGRAM) with structured output loss. The Path Select Layer generates the score S(wgt, x) by summing the inputs of the groundtruth word. The Beam Search Layer uses beam search to try to select the path with the largest score S(w∗, x) from the inputs. The hinge loss implements a ranking loss, constraining the highest scoring path to be the groundtruth path, and can be back-propagated through the entire network to jointly learn all the parameters.  However, in the general scenario of Equation 3, the weights can be incorporated into the CNN functions f and g, resulting in the score  |w|(cid:88)  min(N,|w|−i+1)(cid:88)  (cid:26)1  0  |w|−|s|+1(cid:88)  S(w, x) =  f i ci  (x) +  gcici+1...ci+n−1(x),  (6)  i=1  i=1  n=1  The functions f and g are deﬁned by CNNs and so we can optimise the parameters of them to reduce the cost in Equation 5. This can be done through standard back-propagation and SGD. Differentiating the loss L with respect to S gives  ∂L(x, wgt, S) ∂S(w∗, x)  =  (7) where z = maxw(cid:54)=wgt,i µ + S(w, x) − S(wgt, x). Differentiating the score function of Equation 6 ci and gs gives with respect to the character sequence model and N-gram encoding model outputs f i  ∂S(wgt, x)  =  0  0  ∂L(x, wgt, S)  if z > 0 otherwise  if z > 0 otherwise  (cid:26)−1  ∂S(w, x)  =  ∂f i c  if ci = c otherwise ,  ∂S(w, x)  =  ∂gs  1{cici+1...ci+|s|−1=s}  (8)  i=1 This allows errors to be back-propagated to the entire network. Intuitively, the errors are back- propagated through the CNN outputs which are responsible for margin violations, since they con- tributed to form an incorrect score. Using this structured output loss allows the parameters of the entire model to be jointly optimised within the structure imposed by Equation 6. Figure 3 shows the training architecture used. Due to the presence of high order scores in Equation 6, it is too expensive to exhaustively search the space of all possible paths to ﬁnd w∗, even with dynamic programming, so instead we use beam search to ﬁnd the approximate highest scoring path. The structured output loss described in this section bares resemblance to the discriminative Viterbi training introduced by LeCun et al. (1998). However, our model includes higher-order terms, terms of a different nature (N-grams), and uses a structured-output formulation. Furthermore, our method incorporates only a very weak language model, limited to assigning a score of 0 to all N-grams outside a target set GN . Note that this does not mean that these N-grams cannot be recognised (this would require assigning to them a score of −∞); instead, it is a smoothing technique that assigns a nominal score to infrequent N-grams.  5 EVALUATION  In this section we evaluate the three models introduced in the previous sections. The datasets used for training and testing are described in Section 5.1, the implementation details given in Section 5.2, and the results of experiments reported in Section 5.3.  6  Published as a conference paper at ICLR 2015  5.1 DATASETS  We evaluate our models on a number of standard datasets – ICDAR 2003, ICDAR 2013, Street View Text, and IIIT5k, whereas for training, as well as testing across a larger vocabulary, we turn to the synthetic Synth90k and SynthRand datasets. ICDAR 2003 (Lucas et al. (2003)) is a scene text recognition dataset, with the test set containing 251 full scene images and 860 groundtruth cropped images of the words contained with the full images. We follow the standard evaluation protocol deﬁned by Wang et al. (2011) and perform recognition on the words containing only alphanumeric characters and at least three characters. The test set of 860 cropped word images is referred to as IC03. The lexicon of all test words is IC03-Full (563 words), and the per-image 50 word lexicons deﬁned by Wang et al. (2011) and used in a number of works (Wang et al. (2011; 2012); Alsharif & Pineau (2014)) are referred to as IC03-50. ICDAR 2013 (Karatzas et al. (2013)) test dataset contains 1015 groundtruth cropped word images from scene text. Much of the data is inherited from the ICDAR 2003 datasets. We refer to the 1015 groundtruth cropped words as IC13. Street View Text (Wang et al. (2011)) is a more challenging scene text dataset than the ICDAR datasets. It contains 250 full scene test images downloaded from Google Street View. The test set of 647 groundtruth cropped word images is referred to as SVT. The lexicon of all test words is SVT- Full (4282 words), and the smaller per-image 50 word lexicons deﬁned by Wang et al. (2011) and used in previous works (Wang et al. (2011; 2012); Alsharif & Pineau (2014); Bissacco et al. (2013)) are referred to as SVT-50. IIIT 5k-word (Mishra et al. (2012)) test dataset contains 3000 cropped word images of scene text downloaded from Google image search. Each image has an associated 50 word lexicon (IIIT5k-50) and 1k word lexicon (IIIT5k-1k). Synth90k1 (Jaderberg et al. (2014a;b)) is a dataset of 9 million cropped word images that have been synthetically generated. The synthetic data is highly realistic and can be used to train on and as a challenging test benchmark. The dataset covers 90k different English words, and there are predeﬁned training and test splits with approximately 8 million training images and 900k test images. In addition, we use the same synthetic text engine from Jaderberg et al. (2014a;b) to generate word images with completely random strings of up to 10 uniformly sampled alphanumeric characters. We refer to this dataset as SynthRand. The training set consists of 8 million training images and the test set of 900k images. In this corpus there are very few word repetitions (in addition to the random rendering variations). There is a wide range of difﬁculty in this dataset, from perfectly readable text to almost impossible to read samples.  5.2  IMPLEMENTATION DETAILS  In the following, the character sequence model is referred to as CHAR, the N-gram encoding model as NGRAM, and the joint model as JOINT. The CHAR and NGRAM models both have the same base CNN architecture. The base CNN has ﬁve convolutional layers and two fully connected layers. The input is a 32 × 100 greyscale image obtained by resizing the word image (ignoring its aspect ratio) and then subtracting its mean and dividing by its standard deviation. Rectiﬁed linear units are used throughout after each weight layer except for the last one. In forward order, the convolutional layers have 64, 128, 256, 512, and 512 square ﬁlters with an edge size of 5, 5, 3, 3, and 3. Convolutions are performed with stride 1 and there is input feature map padding to preserve spatial dimensionality. 2× 2 max-pooling follows the ﬁrst, second and third convolutional layers. The fully connected layers have 4096 units. On top of this base CNN, the CHAR model has 23 independent fully connected layers with 37 units, allowing recognition of words of up to Nmax = 23 characters long. The NGRAM model operates on a selection of 10k frequent N-grams of order N ≤ 4 (identiﬁed as the ones that occur at least 10 times in the Synth90k word corpus, resulting in 36 1-grams, 522 2-grams, 3965 3-grams, and 5477 4-grams). This requires a ﬁnal fully connected layer on top of the base CNN with 10k units. Therefore, the graph of function (6) has cliques of sizes at most 4. Beam search uses a width of  1http://www.robots.ox.ac.uk/˜vgg/data/text/  7  Published as a conference paper at ICLR 2015  Training Data  Test Data  Synth90k-train  Synth1-72k Synth1-45k SynthRand  Synth90k-test IC03 SVT IC13 Synth72k-90k Synth45k-90k SynthRand  CHAR JOINT 91.0 87.3 89.6 85.9 71.7 68.0 81.8 79.5 89.7 82.4 89.1 80.3 80.7 79.5  Table 1: Left: The accuracy (%) of the character sequence model, CHAR, and the joint model, JOINT. Differ- ent combinations of training and test data are evaluated. Synthx-y refers to a subset of the Synth90k that only contains words in the label interval [x, y] (word label indices are in random, non-alphabetical order). Train- ing and testing on completely distinct words demonstrates the power of a general, unconstrained recognition model. Right: Some results of the CHAR model on the SynthRand test dataset. Letters in red have been predicted incorrectly with the groundtruth (GT) shown below. Notice the range in difﬁculty of the SynthRand data. 5 during training and of 10 during testing. If a lexicon is used to constrain the output, instead of performing beam search, the paths associated with the lexicon words are scored with Equation 6, and the word with the maximum score is selected as the ﬁnal result. The three models are all trained with SGD and dropout regularisation. The learning rates are dynam- ically decreased as training progresses. The JOINT model is initialised with the pre-trained CHAR and NGRAM network weights and the convolutional layers’ weights are frozen during training.  5.3 EXPERIMENTS  We evaluate our models on a combination of real-world test data and synthetic data to highlight different operating characteristics.  N-gram Encoding Results. The NGRAM model predicts the N-grams contained in input word image. Due to the highly unbalanced nature of this problem (where only 10-20 N-grams are con- tained in any given image), results are reported as the maximum achieved F-score, computed as the harmonic mean of precision and recall. The latter are computed by sweeping the threshold prob- ability for an N-gram to be classiﬁed as present in the word. The maximum achieved F-score on Synth90k is 87.0% and on IC03 is 87.1%. This demonstrates that, while not perfect, the NGRAM model accurately models the presence of N-grams in word images.  Character Sequence and Joint Model Results. The CHAR and JOINT models are evaluated on standard as well as synthetic benchmarks (Table 1), but both models are trained on Synth90k. While the CHAR model achieves good performance, it is consistently outperformed by the JOINT model; the accuracy improvement is as much as +4% on IC03 and SVT, despite the difﬁculty of the latter. Figure 4 shows some example results using the JOINT model. Next, we evaluate the ability of our model to generalise by recognising words unseen during training. This effectively amounts to zero-shot learning and is a key contribution compared to Jaderberg et al. (2014a;b). In order to do so, the training vocabulary is split into two parts, with one part (50% or 80%) used for training and the other one for evaluation (50% or 20%). In this case the CHAR model is signiﬁcantly penalised, but the JOINT model can recover most of the performance. For instance, on the 50/50 split, the JOINT model accuracy is 89.1%, only -2% compared to the 91.0% obtained when the training and testing vocabularies are equal. The ﬁnal test pushes generalisation by training and testing on completely random strings from Syn- thRand. As this dataset is a lot less regular than a natural language, the performance of the CHAR model suffers, dropping to 80.7% accuracy. Furthermore, as could be expected form the absence of common N-grams in the random language, the JOINT model performs slightly worse at 79.5% accuracy. However this drop is very small because N-grams are not used as hard constraints on the predicted words, but rather to nudge the word scores based on further visual cues.  Comparison to the state-of-the-art. Table 2 compares the accuracy of CHAR and JOINT to previous works. Whereas these works make use of strong language models, our models make min-  8  Published as a conference paper at ICLR 2015  (a)  (b)  Figure 4: Results where the unary terms of the JOINT model cannot solely recognise the word correctly, but the addition of the edge scores result in correct recognition, from SVT (a,b) and IC13 (c). The input image is shown in the top left corner. The unary scores for characters (rows) at each position (columns, up to 12 out of 23 characters) are shown, with the selected path using only the unary score term Si c (orange) and when edge scores Se are incorporated (cyan). The bars show the NGRAM strengths, with lighter colour representing larger values.  (c)  Model  IC03-50*  IC03-Full*  IC03-50k*  IC03 SVT-50* SVT IC13 IIIT5k-50*  IIIT5k-1k*  Baseline ABBYY (Wang et al. (2011)) Wang et al. (2011) Mishra et al. (2012) Novikova et al. (2012) Wang et al. (2012) Goel et al. (2013) Bissacco et al. (2013) Alsharif & Pineau (2014) Almaz´an et al. (2014) Yao et al. (2014) Jaderberg et al. (2014c) Gordo (2014) DICT Jaderberg et al. (2014a;b) CHAR JOINT  56.0 76.0 81.8 82.8 90.0 89.7  93.1  88.5 96.2  98.7 98.5 97.8  -  -  -  55.0 62.0 67.8  84.0  88.6  80.3 91.5  98.6 96.7 97.0  -  - -  -  -  - - - - - - -  - - - -  85.1  93.3 92.3 93.4  - - - - - - - - - - - -  93.1 85.9 89.6  35.0 57.0 73.2 72.9 70.0 77.3 90.4 74.3 89.2 75.9 86.1 90.7 95.4 93.5 93.2  - - - - - -  - - - - -  - - - - - -  - - - - -  78.0  87.6  80.7 68.0 71.7  90.8 79.5 81.8  - -  - - - -  -  24.3  64.1  91.2 80.2  93.3 97.1 95.0 95.5  - - -  - - - -  -  57.5  82.1 69.3  86.6 92.7 89.3 89.6  Table 2: Comparison to previous methods. The baseline method is from a commercially available OCR system. Note that the training data for DICT includes the lexicons of the test sets, so it has the capacity to recognise all test words. *Results are constrained to the lexicons described in Section 5.1.  imal assumptions about the language. In the constrained lexicon cases (the starred columns of Ta- ble 2), both CHAR and JOINT are very close to the state-of-the-art DICT model of Jaderberg et al. (2014a;b). Furthermore, if the same 90k dictionary used in by the DICT model is used to constrain the output of the JOINT model, the performance is identical at 93.1% accuracy on IC03. While in the constrained lexicon experiments the lexicon is limited at test time, these results are still remark- able because, differently from DICT, CHAR and JOINT are not trained on a speciﬁc dictionary. In particular, DICT would not be able to operate on random strings. The recognition results without a lexicon are still behind that of some constrained models, however the JOINT model provides competitive performance and is far more ﬂexible to recognise unseen words than previous works, while still achieving state-of-the-art performance if a lexicon is then applied as a constraint at test time. Figure 4 shows some example results where the CHAR model does not recognise the word correctly but the JOINT model succeeds.  6 CONCLUSION  In this paper we have introduced a new formulation for word recognition, designed to be used iden- tically in language and non-language scenarios. By modelling character positions and the presence of common N-grams, we can deﬁne a joint graphical model. This can be trained effectively by back propagating structured output loss, and results in a more accurate word recognition system than pre- dicting characters alone. We show impressive results for unconstrained text recognition with the ability to generalise recognition to previously unseen words, and match state-of-the-art accuracy when comparing in lexicon constrained scenarios.  Acknowledgments. This work was supported by the EPSRC and ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.  9  Published as a conference paper at ICLR 2015  REFERENCES  Almaz´an, J., Gordo, A., Forn´es, A., and Valveny, E. Word spotting and recognition with embedded attributes.  In TPAMI, 2014.  Alsharif, O. and Pineau, J. End-to-End Text Recognition with Hybrid HMM Maxout Models. In Proc. ICLR,  2014.  Bissacco, A., Cummins, M., Netzer, Y., and Neven, H. PhotoOCR: Reading text in uncontrolled conditions. In  Proc. ICCV, 2013.  Goel, V., Mishra, A., Alahari, K., and Jawahar, C. V. Whole is greater than sum of parts: Recognizing scene  text words. In ICDAR, pp. 398–402, 2013.  Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street  view imagery using deep convolutional neural networks. arXiv:1312.6082, 2013.  Gordo, A. Supervised mid-level features for word image representation. ArXiv e-prints, Oct 2014. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Improving neural networks  by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.  Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman, A. Synthetic data and artiﬁcial neural networks for  natural scene text recognition. NIPS Deep Learning Workshop 2014, 2014a.  Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman, A. Reading text in the wild with convolutional neural  networks. arXiv pre-print, 2014b.  Jaderberg, M., Vedaldi, A, and Zisserman, A. Deep features for text spotting. In European Conference on  Computer Vision, 2014c.  Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., Mestre, S. R., i Bigorda, L. G., Mas, J., Mota, D. F., Almazan, J., and de las Heras, L. P. ICDAR 2013 robust reading competition. In ICDAR, pp. 1484–1493, 2013.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, 1998.  Lucas, S. M., Panaretos, A., Sosa, L., Tang, A., Wong, S., and Young, R. ICDAR 2003 robust reading compe-  titions. In ICDAR, pp. 682–687, 2003.  Mishra, A., Alahari, K., and Jawahar, C. Scene text recognition using higher order language priors. 2012. Novikova, T., Barinova, O., Kohli, P., and Lempitsky, V. Large-lexicon attribute-consistent text recognition in  natural images. In Proc. ECCV, pp. 752–765. Springer, 2012.  Ozuysal, M., Fua, P., and Lepetit, V. Fast keypoint recognition in ten lines of code. In Proc. CVPR, 2007. Perronnin, F., Liu, Y., S´anchez, J., and Poirier, H. Large-scale image retrieval with compressed ﬁsher vectors.  In Proc. CVPR, 2010.  Rodriguez-Serrano, J. A., Perronnin, F., and Meylan, F. Label embedding for text recognition. In Proc. BMVC.,  2013.  Russel, Stuart, Norvig, Peter, et al. Artiﬁcial intelligence: A modern approach, 1995. Cited on, pp. 20, 1994. Tompson, Jonathan J, Jain, Arjun, LeCun, Yann, and Bregler, Christoph. Joint training of a convolutional network and a graphical model for human pose estimation. In Advances in Neural Information Processing Systems, pp. 1799–1807, 2014.  Wang, K., Babenko, B., and Belongie, S. End-to-end scene text recognition. In Proc. ICCV, pp. 1457–1464.  IEEE, 2011.  Wang, T., Wu, D. J, Coates, A., and Ng, A. Y. End-to-end text recognition with convolutional neural networks.  In ICPR, pp. 3304–3308. IEEE, 2012.  Wickelgran, Wayne A. Context-sensitive coding, associative memory, and serial order in (speech) behavior.  Psychological Review, 76(1):1, 1969.  Yao, Cong, Bai, Xiang, Shi, Baoguang, and Liu, Wenyu. Strokelets: A learned multi-scale representation for scene text recognition. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 4042–4049. IEEE, 2014.  10  ",
1409.1556,2015,Very Deep Convolutional Networks for Large-Scale Image Recognition,"['Very Deep Convolutional Networks for Large-Scale Image Recognition', 'Karen Simonyan and Andrew Zisserman']",https://arxiv.org/pdf/1409.1556,"5 1 0 2    r p A 0 1         ]  V C . s c [      6 v 6 5 5 1  .  9 0 4 1 : v i X r a  Published as a conference paper at ICLR 2015  VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION  Karen Simonyan∗ & Andrew Zisserman+ Visual Geometry Group, Department of Engineering Science, University of Oxford {karen,az}@robots.ox.ac.uk  ABSTRACT  In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisa- tion and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili- tate further research on the use of deep visual representations in computer vision.  1  INTRODUCTION  Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale im- age and video recognition (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014) which has become possible due to the large public image reposito- ries, such as ImageNet (Deng et al., 2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al., 2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recog- nition Challenge (ILSVRC) (Russakovsky et al., 2014), which has served as a testbed for a few generations of large-scale image classiﬁcation systems, from high-dimensional shallow feature en- codings (Perronnin et al., 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky et al., 2012) (the winner of ILSVRC-2012).  With ConvNets becoming more of a commodity in the computer vision ﬁeld, a number of at- tempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC- 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) utilised smaller receptive window size and smaller stride of the ﬁrst convolutional layer. Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet et al., 2014; Howard, 2014). In this paper, we address another important aspect of ConvNet architecture design – its depth. To this end, we ﬁx other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution ﬁlters in all layers. As a result, we come up with signiﬁcantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classiﬁcation and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classiﬁed by a linear SVM without ﬁne-tuning). We have released our two best-performing models1 to facilitate further research. The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet conﬁgurations. The details of the image classiﬁcation training and evaluation are then presented in Sect. 3, and the  ∗current afﬁliation: Google DeepMind +current afﬁliation: University of Oxford and Google DeepMind 1http://www.robots.ox.ac.uk/˜vgg/research/very_deep/  1  Published as a conference paper at ICLR 2015  conﬁgurations are compared on the ILSVRC classiﬁcation task in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B. Finally, Appendix C contains the list of major paper revisions.  2 CONVNET CONFIGURATIONS  To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer conﬁgurations are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012). In this section, we ﬁrst describe a generic layout of our ConvNet conﬁgurations (Sect. 2.1) and then detail the speciﬁc conﬁgurations used in the evaluation (Sect. 2.2). Our design choices are then discussed and compared to the prior art in Sect. 2.3.  2.1 ARCHITECTURE  During training, the input to our ConvNets is a ﬁxed-size 224 × 224 RGB image. The only pre- processing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use ﬁlters with a very small receptive ﬁeld: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the conﬁgurations we also utilise 1 × 1 convolution ﬁlters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is ﬁxed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers. Spatial pooling is carried out by ﬁve max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2. A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the ﬁrst two have 4096 channels each, the third performs 1000- way ILSVRC classiﬁcation and thus contains 1000 channels (one for each class). The ﬁnal layer is the soft-max layer. The conﬁguration of the fully connected layers is the same in all networks.  All hidden layers are equipped with the rectiﬁcation (ReLU (Krizhevsky et al., 2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory con- sumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al., 2012).  2.2 CONFIGURATIONS  The ConvNet conﬁgurations, evaluated in this paper, are outlined in Table 1, one per column. In the following we will refer to the nets by their names (A–E). All conﬁgurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the ﬁrst layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512. In Table 2 we report the number of parameters for each conﬁguration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive ﬁelds (144M weights in (Sermanet et al., 2014)).  2.3 DISCUSSION  Our ConvNet conﬁgurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al., 2014). Rather than using relatively large receptive ﬁelds in the ﬁrst conv. lay- ers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al., 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al., 2014)), we use very small 3 × 3 receptive ﬁelds throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 × 3 conv. layers (without spatial pooling in between) has an effective receptive ﬁeld of 5 × 5; three  2  Published as a conference paper at ICLR 2015  Table 1: ConvNet conﬁgurations (shown in columns). The depth of the conﬁgurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as “convhreceptive ﬁeld sizei-hnumber of channelsi”. The ReLU activation function is not shown for brevity.  ConvNet Conﬁguration  A  11 weight  layers  A-LRN 11 weight  layers  B  C  D  E  13 weight  layers  16 weight  layers  16 weight  layers  19 weight  layers  input (224 × 224 RGB image)  conv3-64  conv3-64  LRN  conv3-64 conv3-64  conv3-64 conv3-64  conv3-64 conv3-64  conv3-64 conv3-64  conv3-128  conv3-128  conv3-128 conv3-128  conv3-128 conv3-128  conv3-128 conv3-128  conv3-128 conv3-128  maxpool  conv3-256 conv3-256  conv3-256 conv3-256  conv3-256 conv3-256  conv3-256 conv3-256 conv1-256  conv3-256 conv3-256 conv3-256  maxpool  conv3-512 conv3-512  conv3-512 conv3-512  conv3-512 conv3-512  conv3-512 conv3-512 conv1-512  conv3-512 conv3-512 conv3-512  maxpool  conv3-512 conv3-512  conv3-512 conv3-512  conv3-512 conv3-512  conv3-512 conv3-512 conv1-512  conv3-512 conv3-512 conv3-512  maxpool  conv3-256 conv3-256 conv3-256 conv3-256  conv3-512 conv3-512 conv3-512 conv3-512  conv3-512 conv3-512 conv3-512 conv3-512  maxpool FC-4096 FC-4096 FC-1000 soft-max  Table 2: Number of parameters (in millions).  Network Number of parameters  A,A-LRN  133  B 133  C 134  D 138  E 144  such layers have a 7 × 7 effective receptive ﬁeld. So what have we gained by using, for instance, a stack of three 3 × 3 conv. layers instead of a single 7 × 7 layer? First, we incorporate three non-linear rectiﬁcation layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by 3 (cid:0)32C 2(cid:1) = 27C 2 weights; at the same time, a single 7 × 7 conv. layer would require 72C 2 = 49C 2 parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7 × 7 conv. ﬁlters, forcing them to have a decomposition through the 3 × 3 ﬁlters (with non-linearity injected in between). The incorporation of 1 × 1 conv. layers (conﬁguration C, Table 1) is a way to increase the non- linearity of the decision function without affecting the receptive ﬁelds of the conv. layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectiﬁcation function. It should be noted that 1 × 1 conv. layers have recently been utilised in the “Network in Network” architecture of Lin et al. (2014).  Small-size convolution ﬁlters have been previously used by Ciresan et al. (2011), but their nets are signiﬁcantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classiﬁcation task, was developed independently of our work, but is similar in that it is based on very deep ConvNets  3  Published as a conference paper at ICLR 2015  (22 weight layers) and small convolution ﬁlters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions). Their network topology is, however, more complex than ours, and the spatial reso- lution of the feature maps is reduced more aggressively in the ﬁrst layers to decrease the amount of computation. As will be shown in Sect. 4.5, our model is outperforming that of Szegedy et al. (2014) in terms of the single-network classiﬁcation accuracy.  3 CLASSIFICATION FRAMEWORK  In the previous section we presented the details of our network conﬁgurations. In this section, we describe the details of classiﬁcation ConvNet training and evaluation.  3.1 TRAINING  The ConvNet training procedure generally follows Krizhevsky et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al., 1989)) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the L2 penalty multiplier set to 5 · 10−4) and dropout regularisation for the ﬁrst two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to 10−2, and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al., 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. ﬁlter sizes; (b) pre-initialisation of certain layers.  The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the conﬁguration A (Table 1), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the ﬁrst four convolutional layers and the last three fully- connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and 10−2 variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010).  To obtain the ﬁxed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal ﬂipping and random RGB colour shift (Krizhevsky et al., 2012). Training image rescaling is explained below.  Training image size. Let S be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to S as the training scale). While the crop size is ﬁxed to 224 × 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for S ≫ 224 the crop will correspond to a small part of the image, containing a small object or an object part.  We consider two approaches for setting the training scale S. The ﬁrst is to ﬁx S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi- scale image statistics). In our experiments, we evaluated models trained at two ﬁxed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014)) and S = 384. Given a ConvNet conﬁguration, we ﬁrst trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of 10−3. The second approach to setting S is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [Smin, Smax] (we used Smin = 256 and Smax = 512). Since objects in images can be of different size, it is beneﬁcial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single  4  Published as a conference paper at ICLR 2015  model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by ﬁne-tuning all layers of a single-scale model with the same conﬁguration, pre-trained with ﬁxed S = 384.  3.2 TESTING  At test time, given a trained ConvNet and an input image, it is classiﬁed in the following way. First, it is isotropically rescaled to a pre-deﬁned smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect. 4, using several values of Q for each S leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sermanet et al., 2014). Namely, the fully-connected layers are ﬁrst converted to convolutional layers (the ﬁrst FC layer to a 7 × 7 conv. layer, the last two FC layers to 1 × 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a ﬁxed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal ﬂipping of the images; the soft-max class posteriors of the original and ﬂipped images are averaged to obtain the ﬁnal scores for the image.  Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efﬁcient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szegedy et al. (2014), can lead to improved accuracy, as it results in a ﬁner sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive ﬁeld, so more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 ﬂips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014).  3.3  IMPLEMENTATION DETAILS  Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of signiﬁcant modiﬁcations, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU.  While more sophisticated methods of speeding up ConvNet training have been recently pro- posed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.  4 CLASSIFICATION EXPERIMENTS  Dataset. In this section, we present the image classiﬁcation results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 chal- lenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The clas- siﬁcation performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classiﬁcation error, i.e. the proportion of incorrectly classiﬁed images; the latter is the  5  Published as a conference paper at ICLR 2015  main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories.  For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the ofﬁcial ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al., 2014).  4.1 SINGLE SCALE EVALUATION  We begin with evaluating the performance of individual ConvNet models at a single scale with the layer conﬁgurations described in Sect. 2.2. The test image size was set as follows: Q = S for ﬁxed S, and Q = 0.5(Smin + Smax) for jittered S ∈ [Smin, Smax]. The results of are shown in Table 3. First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B–E).  Second, we observe that the classiﬁcation error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the conﬁguration C (which contains three 1 × 1 conv. layers), performs worse than the conﬁguration D, which uses 3 × 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. ﬁlters with non-trivial receptive ﬁelds (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneﬁcial for larger datasets. We also compared the net B with a shallow net with ﬁve 5 × 5 conv. layers, which was derived from B by replacing each pair of 3 × 3 conv. layers with a single 5 × 5 conv. layer (which has the same receptive ﬁeld as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which conﬁrms that a deep net with small ﬁlters outperforms a shallow net with larger ﬁlters.  Finally, scale jittering at training time (S ∈ [256; 512]) leads to signiﬁcantly better results than training on images with ﬁxed smallest side (S = 256 or S = 384), even though a single scale is used at test time. This conﬁrms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics.  Table 3: ConvNet performance at a single test scale.  ConvNet conﬁg. (Table 1)  smallest image side train (S) test (Q)  top-1 val. error (%)  top-5 val. error (%)  A A-LRN B  C  D  E  256 256 256 256 384  [256;512]  256 384  [256;512]  256 384  [256;512]  256 256 256 256 384 384 256 384 384 256 384 384  29.6 29.7 28.7 28.1 28.1 27.3 27.0 26.8 25.6 27.3 26.9 25.5  10.4 10.5 9.9 9.4 9.3 8.8 8.8 8.7 8.1 9.0 8.7 8.0  4.2 MULTI-SCALE EVALUATION  Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with ﬁxed S were evaluated over three test image sizes, close to the training one: Q = {S − 32, S, S + 32}. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable S ∈ [Smin; Smax] was evaluated over a larger range of sizes Q = {Smin, 0.5(Smin + Smax), Smax}.  6  Published as a conference paper at ICLR 2015  The results, presented in Table 4, indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table 3). As before, the deepest conﬁgurations (D and E) perform the best, and scale jittering is better than training with a ﬁxed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table 4). On the test set, the conﬁguration E achieves 7.3% top-5 error.  Table 4: ConvNet performance at multiple test scales.  ConvNet conﬁg. (Table 1)  smallest image side test (Q)  train (S)  top-1 val. error (%)  top-5 val. error (%)  B  C  D  E  256 256 384  [256; 512]  256 384  [256; 512]  256 384  [256; 512]  224,256,288 224,256,288 352,384,416 256,384,512 224,256,288 352,384,416 256,384,512 224,256,288 352,384,416 256,384,512  28.2 27.7 27.8 26.3 26.6 26.5 24.8 26.9 26.7 24.8  9.6 9.2 9.2 8.2 8.6 8.6 7.5 8.7 8.6 7.5  4.3 MULTI-CROP EVALUATION  In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2 for de- tails). We also assess the complementarity of the two evaluation techniques by averaging their soft- max outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions.  Table 5: ConvNet evaluation techniques comparison. In all experiments the training scale S was sampled from [256; 512], and three test scales Q were considered: {256, 384, 512}.  ConvNet conﬁg. (Table 1)  Evaluation method  top-1 val. error (%)  top-5 val. error (%)  D  E  4.4 CONVNET FUSION  dense  multi-crop  multi-crop & dense  dense  multi-crop  multi-crop & dense  24.8 24.6 24.4 24.8 24.6 24.4  7.5 7.5 7.2 7.5 7.4 7.1  Up until now, we evaluated the performance of individual ConvNet models. In this part of the exper- iments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014).  The results are shown in Table 6. By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by ﬁne-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (conﬁgurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table 5).  4.5 COMPARISON WITH THE STATE OF THE ART  Finally, we compare our results with the state of the art in Table 7. In the classiﬁcation task of ILSVRC-2014 challenge (Russakovsky et al., 2014), our “VGG” team secured the 2nd place with  7  Published as a conference paper at ICLR 2015  Combined ConvNet models  Table 6: Multiple ConvNet fusion results.  Error  top-1 val top-5 val top-5 test  ILSVRC submission  (D/256/224,256,288), (D/384/352,384,416), (D/[256;512]/256,384,512) (C/256/224,256,288), (C/384/352,384,416) (E/256/224,256,288), (E/384/352,384,416)  post-submission  (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), dense eval. (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop & dense eval.  24.7  24.0 23.9 23.7  7.5  7.1 7.2 6.8  7.3  7.0 - 6.8  7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models. As can be seen from Table 7, our very deep ConvNets signiﬁcantly outperform the previous gener- ation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competi- tions. Our result is also competitive with respect to the classiﬁcation task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models – signiﬁcantly less than used in most ILSVRC submissions. In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart from the classical ConvNet architecture of LeCun et al. (1989), but improved it by substantially increasing the depth.  top-1 val. error (%) top-5 val. error (%) top-5 test error (%)  23.7 24.4 24.7  Table 7: Comparison with the state of the art in ILSVRC classiﬁcation. Our method is denoted as “VGG”. Only the results obtained without outside training data are reported. Method VGG (2 nets, multi-crop & dense eval.) VGG (1 net, multi-crop & dense eval.) VGG (ILSVRC submission, 7 nets, dense eval.) GoogLeNet (Szegedy et al., 2014) (1 net) GoogLeNet (Szegedy et al., 2014) (7 nets) MSRA (He et al., 2014) (11 nets) MSRA (He et al., 2014) (1 net) Clarifai (Russakovsky et al., 2014) (multiple nets) Clarifai (Russakovsky et al., 2014) (1 net) Zeiler & Fergus (Zeiler & Fergus, 2013) (6 nets) Zeiler & Fergus (Zeiler & Fergus, 2013) (1 net) OverFeat (Sermanet et al., 2014) (7 nets) OverFeat (Sermanet et al., 2014) (1 net) Krizhevsky et al. (Krizhevsky et al., 2012) (5 nets) Krizhevsky et al. (Krizhevsky et al., 2012) (1 net)  8.1 9.1 11.7 12.5 14.8 16.1 13.6  36.0 37.5 34.0 35.7 38.1 40.7  14.7 16.0 13.2 14.2 16.4 18.2  - 9.1 - -  6.8 7.1 7.5  6.8 7.0 7.3  7.9 6.7  27.9  16.4  - - -  - -  -  -  5 CONCLUSION  In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large- scale image classiﬁcation. It was demonstrated that the representation depth is beneﬁcial for the classiﬁcation accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again conﬁrm the importance of depth in visual representations.  ACKNOWLEDGEMENTS  This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.  8  Published as a conference paper at ICLR 2015  REFERENCES  Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context  database. CoRR, abs/1412.0623, 2014.  Chatﬁeld, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep  into convolutional nets. In Proc. BMVC., 2014.  Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional ﬁlter banks for texture recognition and segmentation.  CoRR, abs/1411.6836, 2014.  Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance  convolutional neural networks for image classiﬁcation. In IJCAI, pp. 1237–1242, 2011.  Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang,  K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In NIPS, pp. 1232–1240, 2012.  Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image  database. In Proc. CVPR, 2009.  Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional  activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013.  Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual  object classes challenge: A retrospective. IJCV, 111(1):98–136, 2015.  Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An In IEEE CVPR Workshop of Generative  incremental bayesian approach tested on 101 object categories. Model Based Vision, 2004.  Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection  and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014.  Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604,  2014.  Glorot, X. and Bengio, Y. Understanding the difﬁculty of training deep feedforward neural networks. In Proc.  AISTATS, volume 9, pp. 249–256, 2010.  Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street  view imagery using deep convolutional neural networks. In Proc. ICLR, 2014.  Grifﬁn, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California  Institute of Technology, 2007.  He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual  recognition. CoRR, abs/1406.4729v2, 2014.  Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014. Howard, A. G. Some improvements on deep convolutional neural network based image classiﬁcation. In Proc.  ICLR, 2014.  Jia, Y.  Caffe:  An open source  convolutional  architecture  for  fast  feature  embedding.  http://caffe.berkeleyvision.org/, 2013.  Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR,  abs/1412.2306, 2014.  Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural  language models. CoRR, abs/1411.2539, 2014.  Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014. Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classiﬁcation with deep convolutional neural net-  works. In NIPS, pp. 1106–1114, 2012.  LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropa-  gation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.  Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. ICLR, 2014. Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR,  abs/1411.4038, 2014.  Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations  using Convolutional Neural Networks. In Proc. CVPR, 2014.  Perronnin, F., S´anchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classiﬁcation. In  Proc. ECCV, 2010.  Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline  for Recognition. CoRR, abs/1403.6382, 2014.  9  Published as a conference paper at ICLR 2015  Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., ImageNet large scale visual recognition challenge. CoRR,  Bernstein, M., Berg, A. C., and Fei-Fei, L. abs/1409.0575, 2014.  Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition,  Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014.  Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR,  abs/1406.2199, 2014. Published in Proc. NIPS, 2014.  Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,  A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.  Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR,  abs/1406.5726, 2014.  Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901,  2013. Published in Proc. ECCV, 2014.  A LOCALISATION  In the main body of the paper we have considered the classiﬁcation task of the ILSVRC challenge, and performed a thorough evaluation of ConvNet architectures of different depth. In this section, we turn to the localisation task of the challenge, which we have won in 2014 with 25.3% error. It can be seen as a special case of object detection, where a single object bounding box should be predicted for each of the top-5 classes, irrespective of the actual number of objects of the class. For this we adopt the approach of Sermanet et al. (2014), the winners of the ILSVRC-2013 localisation challenge, with a few modiﬁcations. Our method is described in Sect. A.1 and evaluated in Sect. A.2.  A.1 LOCALISATION CONVNET  To perform object localisation, we use a very deep ConvNet, where the last fully connected layer predicts the bounding box location instead of the class scores. A bounding box is represented by a 4-D vector storing its center coordinates, width, and height. There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR (Sermanet et al., 2014)) or is class-speciﬁc (per-class regression, PCR). In the former case, the last layer is 4-D, while in the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers and was found to be the best-performing in the classiﬁcation task (Sect. 4).  Training. Training of localisation ConvNets is similar to that of the classiﬁcation ConvNets (Sect. 3.1). The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth. We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 submission). Training was initialised with the corresponding classiﬁcation models (trained on the same scales), and the initial learning rate was set to 10−3. We explored both ﬁne-tuning all layers and ﬁne-tuning only the ﬁrst two fully-connected layers, as done in (Sermanet et al., 2014). The last fully-connected layer was initialised randomly and trained from scratch.  Testing. We consider two testing protocols. The ﬁrst is used for comparing different network modiﬁcations on the validation set, and considers only the bounding box prediction for the ground truth class (to factor out the classiﬁcation errors). The bounding box is obtained by applying the network only to the central crop of the image.  The second, fully-ﬂedged, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classiﬁcation task (Sect. 3.2). The difference is that instead of the class score map, the output of the last fully-connected layer is a set of bounding box predictions. To come up with the ﬁnal prediction, we utilise the greedy merging procedure of Sermanet et al. (2014), which ﬁrst merges spatially close predictions (by averaging their coor- dinates), and then rates them based on the class scores, obtained from the classiﬁcation ConvNet. When several localisation ConvNets are used, we ﬁrst take the union of their sets of bounding box predictions, and then run the merging procedure on the union. We did not use the multiple pooling  10  Published as a conference paper at ICLR 2015  offsets technique of Sermanet et al. (2014), which increases the spatial resolution of the bounding box predictions and can further improve the results.  A.2 LOCALISATION EXPERIMENTS  In this section we ﬁrst determine the best-performing localisation setting (using the ﬁrst test proto- col), and then evaluate it in a fully-ﬂedged scenario (the second protocol). The localisation error is measured according to the ILSVRC criterion (Russakovsky et al., 2014), i.e. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5.  Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the ﬁndings of Sermanet et al. (2014), where PCR was outperformed by SCR. We also note that ﬁne-tuning all layers for the lo- calisation task leads to noticeably better results than ﬁne-tuning only the fully-connected layers (as done in (Sermanet et al., 2014)). In these experiments, the smallest images side was set to S = 384; the results with S = 256 exhibit the same behaviour and are not shown for brevity.  Table 8: Localisation error for different modiﬁcations with the simpliﬁed testing protocol: the bounding box is predicted from a single central image crop, and the ground-truth class is used. All ConvNet layers (except for the last one) have the conﬁguration D (Table 1), while the last layer performs either single-class regression (SCR) or per-class regression (PCR). Fine-tuned layers regression type GT class localisation error  1st and 2nd FC  all  SCR PCR PCR  36.4 34.3 33.1  Fully-ﬂedged evaluation. Having determined the best localisation setting (PCR, ﬁne-tuning of all layers), we now apply it in the fully-ﬂedged scenario, where the top-5 class labels are predicted us- ing our best-performing classiﬁcation system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014). As can be seen from Ta- ble 9, application of the localisation ConvNet to the whole image substantially improves the results compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of the ground truth. Similarly to the classiﬁcation task (Sect. 4), testing at several scales and combining the predictions of multiple networks further improves the performance.  Table 9: Localisation error  smallest image side test (Q)  train (S)  256 384 384  256 384  352,384  fusion: 256/256 and 384/352,384  test.  top-5 localisation error (%) val. 29.5 28.2 27.5 26.9  25.3  26.7  -  -  Comparison with the state of the art. We compare our best localisation result with the state of the art in Table 10. With 25.3% test error, our “VGG” team won the localisation challenge of ILSVRC-2014 (Russakovsky et al., 2014). Notably, our results are considerably better than those of the ILSVRC-2013 winner Overfeat (Sermanet et al., 2014), even though we used less scales and did not employ their resolution enhancement technique. We envisage that better localisation per- formance can be achieved if this technique is incorporated into our method. This indicates the performance advancement brought by our very deep ConvNets – we got better results with a simpler localisation method, but a more powerful representation.  B GENERALISATION OF VERY DEEP FEATURES  In the previous sections we have discussed training and evaluation of very deep ConvNets on the ILSVRC dataset. In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature  11  Published as a conference paper at ICLR 2015  Table 10: Comparison with the state of the art in ILSVRC localisation. Our method is denoted as “VGG”.  top-5 val. error (%) top-5 test error (%)  Method VGG GoogLeNet (Szegedy et al., 2014) OverFeat (Sermanet et al., 2014) Krizhevsky et al. (Krizhevsky et al., 2012)  26.9  -  30.0  -  25.3 26.7 29.9 34.2  extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-ﬁtting. Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al., 2013; Razavian et al., 2014; Chatﬁeld et al., 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin. Following that line of work, we investigate if our models lead to better performance than more shallow models utilised in the state-of-the-art methods. In this evaluation, we consider two models with the best classiﬁcation performance on ILSVRC (Sect. 4) – conﬁgurations “Net-D” and “Net-E” (which we made publicly available).  To utilise the ConvNets, pre-trained on ILSVRC, for image classiﬁcation on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classiﬁcation), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales. The resulting image descriptor is L2-normalised and combined with a linear SVM classiﬁer, trained on the target dataset. For simplicity, pre-trained ConvNet weights are kept ﬁxed (no ﬁne-tuning is performed).  Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure (Sect. 3.2). Namely, an image is ﬁrst rescaled so that its smallest side equals Q, and then the net- work is densely applied over the image plane (which is possible when all weight layers are treated as convolutional). We then perform global average pooling on the resulting feature map, which produces a 4096-D image descriptor. The descriptor is then averaged with the descriptor of a hori- zontally ﬂipped image. As was shown in Sect. 4.2, evaluation over multiple scales is beneﬁcial, so we extract features over several scales Q. The resulting multi-scale features can be either stacked or pooled across scales. Stacking allows a subsequent classiﬁer to learn how to optimally combine image statistics over a range of scales; this, however, comes at the cost of the increased descriptor dimensionality. We return to the discussion of this design choice in the experiments below. We also assess late fusion of features, computed using two networks, which is performed by stacking their respective image descriptors.  Table 11: Comparison with the state of the art in image classiﬁcation on VOC-2007, VOC-2012, Caltech-101, and Caltech-256. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (2000 classes). Method  Caltech-101  Caltech-256  VOC-2007 VOC-2012 (mean AP) (mean AP)  (mean class recall) (mean class recall)  Zeiler & Fergus (Zeiler & Fergus, 2013) Chatﬁeld et al. (Chatﬁeld et al., 2014) He et al. (He et al., 2014) Wei et al. (Wei et al., 2014) VGG Net-D (16 layers) VGG Net-E (19 layers) VGG Net-D & Net-E  -  82.4 82.4  79.0 83.2  -  81.5 (85.2∗) 81.7 (90.3∗)  89.3 89.3 89.7  89.0 89.0 89.3  86.5 ± 0.5 88.4 ± 0.6 93.4 ± 0.5  -  91.8 ± 1.0 92.3 ± 0.5 92.7 ± 0.5  74.2 ± 0.3 77.6 ± 0.1  - -  85.0 ± 0.2 85.1 ± 0.3 86.2 ± 0.3  Image Classiﬁcation on VOC-2007 and VOC-2012. We begin with the evaluation on the image classiﬁcation task of PASCAL VOC-2007 and VOC-2012 benchmarks (Everingham et al., 2015). These datasets contain 10K and 22.5K images respectively, and each image is annotated with one or several labels, corresponding to 20 object categories. The VOC organisers provide a pre-deﬁned split into training, validation, and test data (the test data for VOC-2012 is not publicly available; instead, an ofﬁcial evaluation server is provided). Recognition performance is measured using mean average precision (mAP) across classes.  Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs sim-  12  Published as a conference paper at ICLR 2015  ilarly to the aggregation by stacking. We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-speciﬁc seman- tics which a classiﬁer could exploit. Since averaging has a beneﬁt of not inﬂating the descrip- tor dimensionality, we were able to aggregated image descriptors over a wide range of scales: Q ∈ {256, 384, 512, 640, 768}. It is worth noting though that the improvement over a smaller range of {256, 384, 512} was rather marginal (0.3%). The test set performance is reported and compared with other approaches in Table 11. Our networks “Net-D” and “Net-E” exhibit identical performance on VOC datasets, and their combination slightly improves the results. Our methods set the new state of the art across image representations, pre- trained on the ILSVRC dataset, outperforming the previous best result of Chatﬁeld et al. (2014) by more than 6%. It should be noted that the method of Wei et al. (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, semantically close to those in VOC datasets. It also beneﬁts from the fusion with an object detection-assisted classiﬁcation pipeline.  Image Classiﬁcation on Caltech-101 and Caltech-256. In this section we evaluate very deep fea- tures on Caltech-101 (Fei-Fei et al., 2004) and Caltech-256 (Grifﬁn et al., 2007) image classiﬁcation benchmarks. Caltech-101 contains 9K images labelled into 102 classes (101 object categories and a background class), while Caltech-256 is larger with 31K images and 257 classes. A standard eval- uation protocol on these datasets is to generate several random splits into training and test data and report the average recognition performance across the splits, which is measured by the mean class recall (which compensates for a different number of test images per class). Following Chatﬁeld et al. (2014); Zeiler & Fergus (2013); He et al. (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training images per class (and the rest is used for testing). In each split, 20% of training images were used as a validation set for hyper-parameter selection.  We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multi- ple scales, performs better than averaging or max-pooling. This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are se- mantically different (capturing the whole object vs. object parts), and stacking allows a classiﬁer to exploit such scale-speciﬁc representations. We used three scales Q ∈ {256, 384, 512}. Our models are compared to each other and the state of the art in Table 11. As can be seen, the deeper 19-layer Net-E performs better than the 16-layer Net-D, and their combination further improves the performance. On Caltech-101, our representations are competitive with the approach of He et al. (2014), which, however, performs signiﬁcantly worse than our nets on VOC-2007. On Caltech-256, our features outperform the state of the art (Chatﬁeld et al., 2014) by a large margin (8.6%).  Action Classiﬁcation on VOC-2012. We also evaluated our best-performing image representa- tion (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classiﬁcation task (Everingham et al., 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action. The dataset contains 4.6K training im- ages, labelled into 11 classes. Similarly to the VOC-2012 object classiﬁcation task, the performance is measured using the mAP. We considered two training settings: (i) computing the ConvNet fea- tures on the whole image and ignoring the provided bounding box; (ii) computing the features on the whole image and on the provided bounding box, and stacking them to obtain the ﬁnal representation. The results are compared to other approaches in Table 12.  Our representation achieves the state of art on the VOC action classiﬁcation task even without using the provided bounding boxes, and the results are further improved when using both images and bounding boxes. Unlike other approaches, we did not incorporate any task-speciﬁc heuristics, but relied on the representation power of very deep convolutional features.  Other Recognition Tasks. Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, consistently outperform- ing more shallow representations. For instance, Girshick et al. (2014) achieve the state of the object detection results by replacing the ConvNet of Krizhevsky et al. (2012) with our 16-layer model. Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been ob-  13  Published as a conference paper at ICLR 2015  Table 12: Comparison with the state of the art in single-image action classiﬁcation on VOC- 2012. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (1512 classes).  Method (Oquab et al., 2014) (Gkioxari et al., 2014) (Hoai, 2014) VGG Net-D & Net-E, image-only VGG Net-D & Net-E, image and bounding box  VOC-2012 (mean AP)  70.2∗ 73.6 76.3 79.2 84.0  served in semantic segmentation (Long et al., 2014), image caption generation (Kiros et al., 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al., 2014; Bell et al., 2014).  C PAPER REVISIONS  Here we present the list of major paper revisions, outlining the substantial changes for the conve- nience of the reader. v1 Initial version. Presents the experiments carried out before the ILSVRC submission. v2 Adds post-submission ILSVRC experiments with training set augmentation using scale jittering, which improves the performance. v3 Adds generalisation experiments (Appendix B) on PASCAL VOC and Caltech image classiﬁca- tion datasets. The models used for these experiments are publicly available. v4 The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple crops for classiﬁcation. v6 Camera-ready ICLR-2015 conference paper. Adds a comparison of the net B with a shallow net and the results on PASCAL VOC action classiﬁcation benchmark.  14  ",
1412.7580,2015,Fast Convolutional Nets With fbfft: A GPU Performance Evaluation,"['Fast Convolutional Nets With fbfft: A GPU Performance Evaluation', 'Nicolas Vasilache', 'Jeff Johnson', 'Michael Mathieu', 'Soumith Chintala', 'Serkan Piantino', 'and Yann LeCun']",https://arxiv.org/pdf/1412.7580,"5 1 0 2    r p A 0 1         ]  G L . s c [      3 v 0 8 5 7  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  FAST CONVOLUTIONAL NETS WITH fbfft : A GPU PERFORMANCE EVALUATION  Nicolas Vasilache, Jeff Johnson, Michael Mathieu, Soumith Chintala, Serkan Piantino & Yann LeCun Facebook AI Research 770 Broadway, New York, NY 10003, USA {ntv,jhj,myrhev,soumith,spiantino,yann}@fb.com  ABSTRACT  We examine the performance proﬁle of Convolutional Neural Network (CNN) training on the current generation of NVIDIA Graphics Processing Units (GPUs). We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA’s cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides signiﬁcant speedups over cuFFT (over 1.5×) for whole CNNs. Both of these convolution implementations are avail- able in open source, and are faster than NVIDIA’s cuDNN implementation for many common convolutional layers (up to 23.5× for a synthetic kernel conﬁgura- tion). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hard- ware speciﬁcs in the implementation of fbfft are also provided.  1  INTRODUCTION  Deep convolutional neural networks (CNNs) have emerged as one of the most promising techniques to tackle large scale learning problems, whether in image and face recognition, audio and speech processing or natural language understanding. A convolutional layer within these networks pro- vides useful properties such as translation equivariance of activations. A limiting factor for use of convolutional nets on large data sets was, until recently, their computational expense.  Krizhevsky et al. (2012) demonstrated that training of large CNNs with millions of weights and massive data sets is tractable when graphics processing units (GPUs) are properly put to use. Since then, renewed interest in CNNs insufﬂated a fresh breath in various frameworks and implemen- tations, including Torch (Collobert et al. (2011a)), Theano (Bergstra et al. (2010)), cuda-convnet (Krizhevsky (2014)) and Caffe (Jia et al. (2014)). Many of these frameworks are based around codes for NVIDIA GPUs using CUDA (Garland et al. (2008)).  We discuss our contributions to convolution performance on these GPUs, namely using Fast Fourier Transform (FFT) implementations within the Torch framework. We summarize the theory behind training convolutional layers both in the time and frequency domain in Section 2. We then detail our implementations. The ﬁrst is based on NVIDIA’s cuFFT and cuBLAS libraries (Section 3). We evaluate our relative performance to NVIDIA’s cuDNN library (Chetlur et al. (2014)) on over 8, 000 different conﬁgurations (Section 4). We signiﬁcantly outperform cuDNN and other time domain convolution implementations for a wide range of problem sizes.  Our second implementation is motivated by limitations in using a black box library such as cuFFT in our application domain, which we describe. In reaction, we implemented a from-scratch open- source implementation of batched 1-D FFT and batched 2-D FFT, called Facebook FFT (fbfft), which achieves over 1.5× speedup over cuFFT for the sizes of interest in our application domain. This implementation achieves GPU efﬁciency ratios of over 75% in certain cases. We describe an on- going effort to further improve the performance of our solution based on algorithmic tiling (Section 6) before we conclude. Our implementation is released as part of the fbcuda and fbcunn open- source libraries at http://github.com/facebook.  1  Published as a conference paper at ICLR 2015  2 CONVOLUTION  Discrete convolution and cross-correlation are used in CNNs. We quickly summarize these and their implementation, with a formulation mirroring Mathieu et al. (2013). Forward propagation (fprop) inputs are a set f of input feature planes xi, i ∈ f . These are cross-correlated1 with f ′ × f different ﬁlter kernel weights w(j,i), j ∈ f ′, i ∈ f , producing output feature planes yj, j ∈ f ′. Each input and output feature can be part of a minibatch S, so we have x(s,i) and y(s,j), i ∈ f, j ∈ f ′, s ∈ S:  y(s,j) =Xi∈f  x(s,i) ⋆ w(j,i)  The feature planes f are reduced (summed) pointwise. For back-propagation (bprop), the gradient of the loss with respect to outputs are convolved with the kernels:  ∂L  ∂x(s,i)  ∂L  ∂y(s,j)  = Xj∈f ′  ∗ w(j,i)  Reduction is over f ′ here. Finally, the kernel weights are updated using the gradient of the loss with respect to the weights (accGrad):  ∂L  ∂w(j,i)  ∂L  ∂y(s,j)  =Xs∈S  ⋆ x(s,i)  Reduction is over S here. For purposes of this paper, we use set symbols interchangeably to refer to their size: each input plane is a 2-D matrix of size h × w, and each ﬁlter kernel is a 2-D matrix of 2. The output planes y(s,i) are of size (h − kh + 1) × (w − kw + 1), and implement size kh × kw valid-only convolution, as per MATLAB terminology. Input zero padding and input mirror padding around the margins of the input (ph, pw) can be optionally added.3 A popular convolution implementation is to unroll the data until the computation is in the form of a large matrix multiplication (Chellapilla et al. (2006)). This is the strategy followed by many imple- mentors, since matrix multiplication is a well-tuned linear algebra primitive available on virtually any platform. While it is possible to provide instances of direct calculation that are faster than matrix unrolling (e.g., for large S, Krizhevsky (2014)), it is challenging to provide an implementation that is faster for more than just a small subset of possible convolution problems. Introducing strides in this form of convolution (i.e., performing the convolution at every dh, dw-th offset) is a popular way to reduce the computational cost at the expense of precision. The memory accesses required are very similar but with fewer reuse opportunities. On the other hand, by the convolution theorem, a convolution of two discrete signals can be performed with lower asymptotic complexity by performing the multiplication in the frequency domain. Applied to the forward pass, it becomes:  y(s,j) =Xi∈f  x(s,i) ⋆ w(j,i) =Xi∈f  F −1(cid:0)F (x(s,i)) ◦ F (w(j,i))∗(cid:1)  where ∗ denotes complex conjugation and ◦ is the pointwise product. The discrete Fourier basis used is the largest of the two components convolved and the output.4 Linearity of the DFT allows one to perform the sum above in the Fourier domain if desired. Applying the FFT then yields a O(Sf f ′n2 + (Sf + f f ′ + Sf ′)n2 log n) procedure in lieu of the original O(Sf f ′n2k2), n = h = w, k = kh = kw. Similar transformations apply for the other two passes. We call this a frequency domain convolution, in contrast to time domain convolution via direct computation.  1Torch practice is that the forward pass is cross-correlation, hence the ⋆. 22-D can be extended to n-D, n ≥ 1. 3Input size (h + ph) × (w + pw), output size (h + ph − kh + 1) × (w + pw − kw + 1). 4(h × w)-dimensional or even bigger for performance (Section 3.2).  2  Published as a conference paper at ICLR 2015  Strided convolutions via FFT can be implemented efﬁciently to obtain good performance Brosch & Tam (2015). We do not consider those in this paper.  3 CUFFT CONVOLUTION IMPLEMENTATION  In this section we discuss implementation strategies using the NVIDIA cuFFT libraries and their efﬁciency.  3.1 FFT CONVOLUTION DETAILS  We described the general formulation for the three types of convolutions in section 2. Here, we borrow the Torch naming convention: input for x(s,i); weight for w(j,i); output for y(s,j); gradOutput for ∂L/∂y(s,j); gradInput for ∂L/∂x(s,i); and gradWeight for ∂L/∂w(j,i). All are stored as single- precision ﬂoating point 4-D tensors in row-major layout, and are stored in memory using the so- called BDHW format. This is explicit in the expression InS×f ×h×w, with input image row data as the innermost or most varying dimension.  Table 1 describes the in-order operations for FFT computation of the forward pass, using the F F T 2D and IF F T 2D operators and Cgemm matrix multiplication. Similar implementations fol- low for the other two passes. The G preﬁx denotes gradients. The F sufﬁx denotes C-valued fre- quency domain tensors; the rest are over R. The T sufﬁx denotes transposed tensors.  Table 1: Implementation detail for forward propagation  INPUT  OUTPUT  InS×f ×h×w  W eif ′×f ×kh×kw InFS×f ×(h+ph)×(⌊ w+pw W eiFf ′×f ×(h+ph)×(⌊ w+pw  2  2  ( InF T(h+ph)×(⌊ w+pw  W eiF T ∗  (h+ph)×(⌊ w+pw  2  2  ⌋+1)  ⌋+1)  ⌋+1)×S×f  ⌋+1)×f ′×f  OutF T(h+ph)×(⌊ w+pw OutFS×f ′×(h+ph)×(⌊ w+pw  2  2  ⌋+1)  ⌋+1)×S×f ′  F F T 2D −−−−−→ InFS×f ×(h+ph)×(⌊ w+pw F F T 2D −−−−−→ W eiFf ′×f ×(h+ph)×(⌊ w+pw T rans2D −−−−−−→ InF T(h+ph)×(⌊ w+pw T rans2D −−−−−−→ W eiF T(h+ph)×(⌊ w+pw  2  2  2  ⌋+1)  ⌋+1)  ⌋+1)×S×f  ⌋+1)×f ′×f  2  Cgemm −−−→  OutF T(h+ph)×(⌊ w+pw  2  ⌋+1)×S×f ′  T rans2D −−−−−−→ OutFS×f ′×(h+ph)×(⌊ w+pw ⌋+1) IF F T 2D −−−−−−→ OutS×f ′×(h−kh+1)×(w−kw+1)  2  Exact tensor dimensions are also given above. By taking advantage of the Hermitian symmetry property of the 2-D DFT for R-valued inputs we only store about half the complex entries; the remaining can be obtained by complex conjugation. This results in array sizes such as ⌊ w+pw ⌋ + 1. We also perform interpolation by zero-padding, which serves multiple purposes. First, it is necessary to handle boundary conditions.5 Second, it is required to interpolate all operands over the same Fourier basis.6 Finally, padding has an impact on the FFT algorithm used in practice, as well as on the ﬂoating point operation count of non-FFT operations (Section 3.2).  2  Following the conversion into frequency domain, we perform transpositions to prepare the tensors for Cgemm matrix multiplication library calls. The transposition converts the BDHW layout into HWBD. The transposition is currently out-of-place and implemented using the Cgeam routine; we are also considering our own, in-place transposition routine. Cgemm library calls are performed on transposed tensors in the frequency domain. Casting the operation as a Cgemm call allows us to beneﬁt from the heavily tuned cuBLAS routine. Eventually, we transpose the result back into the BDHW format and perform a 2-D inverse FFT. At this point, the resulting real tensor, always  5In this case, we typically have ph = ⌊ k 6All tensors are zero-padded to (h + ph) × (w + pw) before F F T 2D.  2 ⌋ and pw = ⌊ kw  2 ⌋.  h  3  Published as a conference paper at ICLR 2015  (h + ph) × (w + pw), is clipped to the appropriate ﬁnal size: (h − kh + 1) × (w − kw + 1) for fprop, h × w for bprop, kh × kw for accGrad.  3.2 CUFFT DESIGN SPACE  We now discuss implementation aspects we explored. Multiple factors inﬂuence the computational efﬁciency of FFTs: transform size n, n’s prime factor decomposition, and whether batched or it- erated single transforms are applied. In the deep learning domain, it is commonplace to deal with small sizes, n 6= 2k. If n has undesirable properties, efﬁciency can drop by an order of magnitude.7 cuFFT implements FFTs with the ubiquitous Cooley-Tukey algorithm (Cooley & Tukey (1965)) which takes advantage of trigonometric equalities to recursively decompose and reuse computa- tions. This is further discussed in the Supplement. Decomposition is built on specialized kernels of ﬁxed sizes which correspond to the prime factor decomposition of n. cuFFT implements specialized building blocks for radix sizes 2, 3, 5, 7, and for sizes n where 4|n, it can use more efﬁcient kernels exploiting the conjugate symmetry property. When n does not admit a prime factor decomposition using those radices only, the expensive Bluestein algorithm is used (Bluestein (1970)). Because our results are used in the time domain, we can in fact zero-pad the image and kernel to perform the FFT at any larger size that may be handled more efﬁciently. Exploiting more efﬁcient, larger sizes should be balanced against the extra cost introduced in the subsequent transposition and matrix multiplica- tion steps. Table 4’s last case is one in which the best tradeoff is not easily guessed. cuFFT also has batched mode optimizations when multiple FFTs of the same size are being performed.  3.3 CUBLAS DESIGN SPACE  The cuBLAS library also comes with different implementations for batched and single operation modes. We had the choice between 3 implementation options:  • for larger batches over small matrices, the cublasCgemmBatched library call; • for smaller batches over larger matrices, multiple cublasCgemm calls from the host; • for intermediate batch and matrix sizes, devices of compute capability 3.5 and higher sup- port dynamic parallelism which allows CUDA kernels to launch other kernels. This can be beneﬁcial for many launches over small matrices.  Note that the discussion above applies to multiplications after transposition. So the matrix size is either S × f , S × f ′ or f × f ′ and the number of such matrices is h × w. Vendor libraries are usually optimized for throughput and not latency, so we expect it to be more efﬁcient for larger sizes along critical dimensions (i.e., image size for the batch case and S × f , S × f ′ or f × f ′ for the multiple kernel case). Due to build system limitations we were not able to experiment with the dynamic parallelism strategy; we leave this for future work.  At the system level, we use CUDA streams and buffering of all CUDA resources and intermediate buffers to remove synchronization points across convolutions. We are mindful of memory consump- tion; to address this we keep one single buffered copy of each type of tensor involved. This behavior is tailored for a bulk synchronous execution of layers on a GPU and is not adapted for multiple asynchronous convolutions on the same GPU. The buffers are automatically expanded as required and reused as much as possible.  3.4 AUTOTUNING  We combine the above implementation with a simple autotuning strategy. We devise a strategy selection mechanism that runs once for each problem size and caches the fastest strategy out of a few dozen for later reuse. The autotuning strategy explores different possible Fourier basis sizes that can be decomposed in powers for which cuFFT has an efﬁcient implementation. In other words, for an FFT dimension of size n, we explore the sizes i ∈ [n, 2⌊log2 n⌋] where i = 2a3b5c7d. When the input size is a power of 2, the search space is reduced to a single point. In addition to Fourier basis sizes, we weigh in various cuBLAS calls and asynchronous modes.  7http://docs.nvidia.com/cuda/cufft/index.html#accuracy-and-performance  4  Published as a conference paper at ICLR 2015  4 CUFFT CONVOLUTION PERFORMANCE  4.1 PERFORMANCE VERSUS CUDNN: 8,232 CONFIGURATIONS  We compare our cuFFT convolution results against NVIDIA’s cuDNN 1.0 library (Chetlur et al. (2014)), which contains one of the fastest, general purpose convolution methods for the GPU, using matrix unrolling. It has decent performance for many problem sizes thanks to heavy autotuning of cuBLAS codes for different problems. It is a strong baseline for this reason.  Image CNNs to date have for the most part used square input images and ﬁlters, though rectan- gular ﬁlters are valid for other problems (notably text CNNs, Collobert et al. (2011b)). Thus, we restrict ourselves to a 5-D problem domain {S, f, f ′, n(= h = w), k(= kh = kw)}. Much of this space is not used in practice. Some areas are perhaps over-emphasized (large S, small k) due to current engineering concerns. We evaluate cuDNN vs cuFFT-based convolution for Table 2’s 8, 232 conﬁgurations.8  Table 2: Conﬁguration elements evaluated  DIMENSION  SIZES EVALUATED  Minibatch size (S) Input ﬁlters (f ) Output ﬁlters (f ′) Kernel h/w (k = kh = kw) Output h/w (y = h − kh + 1 = w − kw + 1)  1, 16, 64, 128 1, 4, 16, 64, 96, 128, 256 1, 4, 16, 64, 96, 128, 256 3, 5, 7, 9, 11, 13 1, 2, 4, 8, 16, 32, 64  Figures 1-6 are performance summaries of cuFFT convolution versus cuDNN on a NVIDIA Tesla K40m, averaged across all three passes. The y-axis problem size corresponds to the minibatch size multiplied by number of input and output planes (Sf f ′); each one of these is a pass reduction dimension. Many possible combinations of S, f, f ′ may map to the same problem size. cuDNN per- formance varies to a greater degree than cuFFT across passes. This is due to the asymmetry of convolution sizes in each pass, and the fact that a larger convolution kernel (as seen with gradient accumulation) is essentially free in the Fourier domain. Averaging the three passes together pro- vides a proxy for overall performance. The x-axis corresponds to output height/width. For deeper layers in image CNNs, output size will decrease while f, f ′ will increase, so depth corresponds to moving from the upper right to the lower left of the graph. Black areas in the chart are due to failed cuFFT runs, due to memory pressure or undetermined potential cuFFT 6.5 issues.  FFT convolutions make large kernel sizes inexpensive, which make the performance of all three passes roughly equal (Table 4). On the other hand, zero-padding kh × kw to h × w penalizes smaller kernels compared to cuDNN. For 3 × 3 kernels (Figure 1), cuFFT performance is poor compared to cuDNN. The overhead of multiple kernel launches, streaming memory in and out multiple times, and zero-padding to the input size often outweigh the algorithmic advantage of FFT. However, for the largest problem sizes, 3 × 3 convolution via FFT can still be advantageous, with top speed 1.84× faster than cuDNN. 5 × 5 kernels (Figure 2) show an increasing dominance of the FFT strategy, with top speed 5.33× faster. The tendency is conﬁrmed for larger kernel sizes: at 13 × 13, maximum speedup is 23.54× over cuDNN.  8Parameterized on output rather than input size h, w because the implied h = y + kh − 1, w = y + kw − 1  will be valid for any choice of kh, kw.  5  Published as a conference paper at ICLR 2015  1/16x  16x  speedup  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  e z i s    m e l b o r p  1 2 4 8  6 1  2 3  4 6  1 2 4 8  6 1  2 3  4 6  output size  output size  Figure 1: 3 × 3 kernel (K40m)  Figure 2: 5 × 5 kernel (K40m)  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  e z i s    m e l b o r p  1 2 4 8  6 1  2 3  4 6  1 2 4 8  6 1  2 3  4 6  output size  output size  Figure 3: 7 × 7 kernel (K40m)  Figure 4: 9 × 9 kernel (K40m)  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  e z i s    m e l b o r p  1 2 4 8  6 1  2 3  4 6  1 2 4 8  6 1  2 3  4 6  output size  output size  Figure 5: 11 × 11 kernel (K40m)  Figure 6: 13 × 13 kernel (K40m)  6  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  e z i s    m e l b o r p  e z i s    m e l b o r p  e z i s    m e l b o r p  Published as a conference paper at ICLR 2015  4.2 CNN PERFORMANCE  In table 3, we show performance for real CNNs, AlexNet (Krizhevsky et al. (2012)) and OverFeat fast (Sermanet et al. (2014)), comparing against cuDNN and cuda-convnet2 (ccn2) kernels in Torch. The ﬁrst layer uses cuDNN for the cuFFT runs because it is strided, but all other layers use cuFFT. The timings include all convolutional layers of the network.  Table 3: AlexNet and OverFeat fast performance (K40, ms)  NETWORK KERNEL FPROP BPROP ACCGRAD TOTAL  AlexNet  OverFeat fast  cuFFT cuDNN ccn2  cuFFT cuDNN ccn2  94.34 147.32 99.03  96.69 167.79 104.59  93.20 153.96 103.29  375.65 459.06 433.11  460.48 634.26 398.87  397.85 508.02 450.82  284.23 469.07 306.91  1233.98 1601.35 1282.80  Table 4 shows the performance of the cuDNN and our cuFFT convolution implementation for some representative layer sizes, assuming all the data is present on the GPU. Our speedups range from 1.4× to 14.5× over cuDNN. Unsurprisingly, larger h, w, smaller S, f, f ′, kh, kw all contribute to reduced efﬁciency with the FFT. More surprisingly, we experience noticeable speedups on small 3 × 3 kernels as long as the input tensor remains of small size. The optimal FFT sizes that au- totuning ﬁnds are reported in columns 2 and 3; note L5 padding being found by the autotuner. Column 7 has the trillion equivalent time-domain reductions per second (single-precision ﬂoating point multiply-adds) achieved by our implementation on a NVIDIA Tesla K40m on CUDA 6.5. This number represents the throughput a time-domain kernel needs to achieve in order to match our implementation; it is computed as (Sf f ′khkw(h − kh + 1)(w − kw + 1))/time. This is a metric to compare relative efﬁciency across problem and padding sizes. In the cases L2, L3 and L4, a time domain convolution would need to exceed the K40m peak of 4.29 Tﬂop/sec in order to match our throughput.  5 fbfft IMPLEMENTATION  This section presumes familiarity with GPU architecture. Refer to the Supplement for details.  When designing high-performance libraries, multiple objectives must be balanced against each other: memory latency/bandwidth tradeoffs, maximizing locality without sacriﬁcing too much par- allelism, good instruction mix, register usage and mapping strategy of computation and data to memories and compute elements. A key principle is to design a set of leaf kernels with well-tuned in-register performance and reduce the larger problem to a combination of these kernels by data and loop tiling (Irigoin & Triolet (1988)) and recursive decompositions (Gunnels et al. (2001)). Since vendors have to sustain high performance for a large class of application domains, there exist pa- rameter conﬁgurations for which a carefully tuned approach signiﬁcantly outperforms vendor-tuned libraries (Shin et al. (2010)). For common deep learning use, convolutional layers consist of many batched small 2-D convolutions. These are tiny relative to DSP and HPC standards and put us in a regime where (a) we fall outside of the highly tuned regime, (b) feature dimensions are often smaller than GPU warp sizes and can often ﬁt exclusively in registers rather than in shared memory (SMEM), and (c) we are very sensitive to latencies. We determined that it is possible to obtain better efﬁciency than the existing batched cuFFT mode for CNNs.  5.1 LIMITATIONS OF CUFFT  Because the cuFFT library is a black box, zero-padding9 has to be explicitly embedded in the input and output arrays. The consequence is that one may need to allocate a duplicate, larger memory  9This is different from the FFTW compatibility padding mode for in-place transforms.  7  Published as a conference paper at ICLR 2015  Table 4: Representative layer performance (S = 128, K40m)  LAYER h + ph w + pw  cuDNN  cuFFT  SPEEDUP TRED/s  L1 fprop bprop accGrad L2 fprop bprop accGrad L3 fprop bprop accGrad L4 fprop bprop accGrad L5 fprop bprop accGrad  64 64 64  128 128 128  46.44 ms 46.25 ms 47.03 ms  80.98 ms 66.49 ms 69.63 ms  125.11 ms 153.39 ms 155.07 ms  354.83 ms 579.37 ms 416.34 ms  Params: f = 3, f ′ = 96, h = w = 128, kh = kw = 11 1.54× 128 2.30× 128 128 2.22× Params: f = 64, f ′ = 64, h = w = 64, kh = kw = 9 7.64× 64 12.5× 64 64 8.85× Params: f = 128, f ′ = 128, h = w = 32, kh = kw = 9 32 32 32 Params: f = 128, f ′ = 128, h = w = 16, kh = kw = 7 16 16 16 Params: f = 384, f ′ = 384, h = w = 13, kh = kw = 3 13 13 13  130.89 ms 245.57 ms 154.96 ms  17.77 ms 16.97 ms 17.00 ms  15.13 ms 20.80 ms 18.17 ms  39.82 ms 28.33 ms 47.84 ms  21.35 ms 20.22 ms 21.26 ms  4.88 ms 4.71 ms 4.70 ms  7.36× 14.5× 9.29×  3.10× 4.41× 3.86×  1.86× 1.40× 2.25×  32 32 32  16 16 16  14 14 14  0.9 1.1 1.05  7.49 7.52 7.40  9.90 10.37 10.34  5.54 5.76 5.75  1.34 1.42 1.35  region (only once) and copy data from non-padded tensors to padded tensors. This memory con- sumption and spurious copies affect latency signiﬁcantly. Instead, we devised an implementation for batched 1-D FFT and 2-D FFT of sizes 2-256 and reaches up to 78% efﬁciency at 97.5% occupancy. We also implemented an IFFT kernel based on our FFT kernel.  In our implementation we use clipping to conditionally load a value if reading within bounds or a constant (0) otherwise. This is an approach used in automatic code generation tools such as Halide (Ragan-Kelley et al. (2013)) and relies on aggressive if-conversion properties of the CUDA compiler. It allows for more efﬁcient control ﬂow rather than using explicit loop prologues and epilogues. This mechanism does not require any additional memory allocation and is zero-copy; this is particularly desirable in the latency sensitive mode.  Additionally, since cuFFT and cuBLAS are closed source, it is impossible to take advantage of algo- rithmic simpliﬁcations that may be available. For instance, in the forward pass of our computation as shown in Table 1, the result of the ﬁrst cuFFT call is of the form S×f ×(h+ph)×(⌊(w+pw)/2⌋+1). With fbfft we return it in the form S × f × (⌊(w + pw)/2⌋ + 1) × (h + ph) where the two inner- most data dimensions are transposed. This allows us to remove a full data transposition from each of the FFT kernels. Another domain-speciﬁc optimization we have yet to explore is eliminating bit reversal portions of the FFT and IFFT. This can be done by performing the FFT with decimation in frequency (DIF) and the IFFT with decimation in time (DIT), discussed in the Supplement.  5.2 WARP-LEVEL 1-D FFT AND 2-D FFT FOR SIZE n ≤ 32  For batched FFT of power of two sizes we view a single warp as a small distributed system with lockstep collective communication capabilities and we program it in a bulk-synchronous fashion (Valiant (1990)). We implement DIF and enforce the following invariants for the log2 n steps:  • each warp thread originally loads one real element of the input vector and locally computes  one complex twiddle factor (i.e. a root of unity);  • at each step, all warp threads exchange data with another thread in the warp in parallel and  produce a new value;  8  Published as a conference paper at ICLR 2015  • then, all warp threads exchange twiddle factors with another thread in the warp in parallel,  and produce a new value.  The two bulk-synchronous exchanges can be written each with one warp-wide instruction. After the log2 n steps, the FFT is computed and stored in a distributed and bit reversed manner within 1 register across a warp. For sizes n ≤ 32, bit reversal can be implemented with a single warp shufﬂe. We either load twiddle factors from device memory or compute them with the sincosf function only once, and subsequently swap them within registers. This greatly reduces the reliance on either memory bandwidth or on the special functional unit at the expense of a few additional registers. The decision between explicitly loading twiddle factors from device memory or computing them is a tradeoff between arithmetic intensity and memory bandwidth. For sizes 16 and 32 the arithmetic pipeline is the bottleneck. Loading twiddle factors from memory for these two special sizes results in a performance increase of 15% and 20% respectively. The discussion above applies to 1-D FFT and to each independent FFT within a larger 2-D FFT. A n-D Fourier transform is separable and can be implemented with sets of multiple 1-D FFT with transpositions between each of these sets. In 2-D FFT R-to-C, the ﬁrst set comprises n FFTs and the second set comprises n/2 + 1 FFTs by Hermitian symmetry. Following standard techniques Lyons (1996) we further pack 2 real FFTs into a single complex FFT . The extra 1 term in the quantity n/2+ 1 makes the computation ill-balanced and can bring down performance by lowering occupancy. We chose to dimension our kernels to have size n×(n/2) and introduce additional control ﬂow to handle the border case. This results in 30% additional performance. We implement the transposition in SMEM across warps following Ruetsch & Micikevicius (2009). Data is already resident in registers so our main concerns are limiting SMEM usage to keep occupancy high, and limiting load/stores by using vector instructions to avoid saturating the load-store unit (LSU).  5.3  1-D FFT AND 2-D FFT FOR SIZE 32 < n ≤ 256  With size 32 as our building block, we extend our strategy to larger sizes. We use the same single warp approach to compute a full 1-D FFT. The main difference is that the computation is now dis- tributed across multiple registers across threads in a warp (⌈n/32⌉ Fourier coefﬁcients and twiddle factors in registers per thread). Because we perform a full FFT per warp, a performance cross-over where cuFFT wins happens after register usage limits occupancy too much. We outperform 1-D cuFFT for n ≤ 256, with a hard register limit at n = 512 (128 and 256 similarly for 2-D FFT). This is still well within our application domain. The following modiﬁcations handle multiple registers per thread:  • Hermitian symmetry allows us to perform half the computation. There is a tradeoff be- tween adding control-ﬂow divergence and performing less work. At n ≥ 64, beneﬁts from reduced computations dominate divergence losses;  • we take advantage of trigonometric symmetries and twiddle factor distribution to compute only a fraction of the roots of unity needed for each FFT, distributed with register to register copies;  • twiddle factor re-balancing across a warp and across registers requires a different imple-  mentation. We managed to implement it fully within registers;  • bit reversal occurs across registers and across warps. The high-order bits represent the reg- ister while the low-order bits represent the warp. Without a sophisticated implementation, this results in indirect addressing of registers which is costly. We implement a simple bit reversal in SMEM, which is an occupancy bottleneck at n ≥ 256 for 1-D FFT.  In the 2-D FFT case, the intermediate transpose becomes signiﬁcantly more expensive. We exper- imented with various strategies to keep occupancy high, including partial transpositions within a warp to use minimal amounts of SMEM.  5.4 DISCUSSION  We report the relative performance of our implementation fbfft compared to cuFFT for various batch and input sizes of interest. The number of batches to consider depends on the dimension of  9  Published as a conference paper at ICLR 2015  CNN layers as well as any multi-GPU parallelization strategy that may be involved. At typical sizes of interest, fbfft is between 1.5× and 5× faster. We tried up to 4 million batches and at larger sizes gains stabilize around 1.4× but efﬁciency goes down as more and more memory is used.  p u d e e p S   T F F B F  p u d e e p S   T F F B F  4.5  4.0  3.5  3.0  2.5  2.0  1.5  1.0  0.5  0.0  4.5  4.0  3.5  3.0  2.5  2.0  1.5  1.0  0.5  0.0  FBFFT-1D Speedup at various sizes and batches  8 16 32 64 128 256  4  32  128  1024  4096  16384  65536  Number of batches  p u d e e p S   T F F I B F  4.5  4.0  3.5  3.0  2.5  2.0  1.5  1.0  0.5  0.0  FBIFFT-1D Speedup at various sizes and batches  8 16 32 64 128 256  4  32  128  1024  4096  16384  65536  Number of batches  Figure 7: fbfft-1D FFT and IFFT (K40m, cuFFT 6.5 @ 1x)  FBFFT-2D Speedup at various sizes and batches  8 16 32 64 128  4  32  128  1024  4096  16384  65536  Number of batches  p u d e e p S   T F F I B F  4.5  4.0  3.5  3.0  2.5  2.0  1.5  1.0  0.5  0.0  FBIFFT-2D Speedup at various sizes and batches  8 16 32 64 128  4  32  128  1024  4096  16384  65536  Number of batches  Figure 8: fbfft-2D FFT and IFFT (K40m, cuFFT 6.5 @ 1x)  Figure 7 shows the performance in the 1-D case. These numbers do not exercise our implicit zero- copy padding, so we expect additional gains when we incorporate our FFT in the convolution. Our implementation outperforms cuFFT for all cases of interest, more dramatically so for smaller batch sizes. Small batch sizes also correspond to the latency sensitive regime in Figures 1-6 for which the cuFFT based implementation performs quite worse than cuDNN. We achieve 78% efﬁciency at 97.5% occupancy for size 64 at batch size 16, 384, as reported by nvvp. Figure 8 shows the performance in the 2-D case. Relative performance gains for sizes 64 are more modest than in the 1-D case, even losing to cuFFT at size 128 and small batch sizes. The magnitude of the relative gains at various batch sizes drops faster than in the 1-D case. Looking at the perfor- mance of the 32 × 32 FFT, we obtain 1.6× speedup over cuFFT at 1, 024 batches. The same ratio is not obtained until 16, 384 batches in 1-D FFT.10 When coupled with the tiling strategy in Section 6, we emphasize that the sizes of interest are actually 8-64, and depend on kh, kw but not input h, w. Batch sizes can vary on the whole spectrum.  We interfaced fbfft into our convolution module and ran experiments with 3 × 3 kernels for the 3 different convolution passes over inputs of sizes x = h = w, x ∈ {13, 16, 27, 32, 57, 64}. For problem size, we used p = S = f = f ′, p ∈ {16, 32, 64, 128}. By swapping our FFT implemen- tation we observed an overall mean speedup of 1.51× with standard deviation 0.21 and geometric mean 1.49×. The minimum speedup was 1.21×, despite sometimes performing more computations  10This is not unexpected because these two computations perform the same number of ﬂops when accounting for Hermitian symmetry, plus the fact that the efﬁciency of cuFFT increases while fbfft remains high but almost constant.  10  Published as a conference paper at ICLR 2015  with fbfft which can only interpolate to a power of 2. These experiments exercise the zero-copy padding and lower memory footprints of fbfft compared to cuFFT but do not yet reﬂect additional optimizations such as tiling and bit twiddling elision.  6 CURRENT LIMITATIONS AND FUTURE WORK  In our current implementation, fbfft heavily relies on shufﬂe instructions. In spite of a good efﬁciency, we only utilize 60% of the available memory bandwidth. This is due to the load and store instructions in our kernel competing with the shufﬂe instructions for the Load-Store Unit (LSU). As a consequence, our ﬁrst bottleneck is the number of instructions issued on the LSU. For instance, on Kepler (capability 3.5), the throughput for 32-bit ﬂoating point multiply-add operations is 192 per cycle but the throughput for shufﬂes is only 32. In the future we will investigate and release faster implementations as they become available.  Temporary memory overhead requirements are a common issue when performing convolutions in the Fourier domain. In this ﬁrst implementation, we introduced the following memory buffers to support our implementation:  • for each of input, output and weight tensors we store 1 buffer for the frequency array and 1 buffer for its complex transpose. These buffers store the Fourier representation and are generally limited by the weight tensor which is independent of the mini-batch size. Because of the global memory pressure we introduce, we reuse buffers at each layer and pass on the opportunity to (1) reuse 2 FFT results in each hidden layer, reducing the cost of forward FFTs by 33%; and (2) asynchronously precompute FFTs of the weight tensors and their gradients to better ﬁll the gpu utilization pipeline,  • when using cuFFT we additionally pad the input, weight and output tensors explicitly to  the best performing common fft size  • when using cuFFT additional temporary memory is reserved by each cufftPlan • with fbfft padding is implicit but and no temporary memory buffer is needed until we reach size 64. On the other hand, fbfft only supports square convolutions whose size is a power of 2. As a consequence, too much padding could occur and adversely affect both performance and memory consumption. The tiling strategy we describe next is a good way to circumvent the problem.  Additionally, we recently developed an in-place transposed batched CGEMM which permits the removal of the complex transposed buffer. For this problem, a tool like MaxAS Lavin (2015) could be valuable.  fbfft provides the most gains over cuFFT at sizes 8-64. A tiling strategy for the input can be used to exploit this advantage. When the kernel is signiﬁcantly smaller than the input, we can decompose a large convolution into several smaller ones. For simplicity, we consider 1D convolution on a single input plane, as it can trivially be extended. Let x be an input of size n, c a kernel of size w and y = x ⋆ c. We write x[i,j] for the vector formed by contiguous elements of x: {xi, xi+1, ..., xj−1}. Let d ≤ n. From the deﬁnition of the convolution, we have:  y[i,i+d] = x[i,i+d+w] ⋆ c  So the convolution of the input of size n can be computed with ⌊n/d⌋ convolutions with inputs of size d + w. The cost of the convolution goes down from O(n log(n)) to O(⌊n/d⌋(d + w) log(d + w)) = O((n + w/d) log(d + w)). From this formula, we see that the optimal d is of the order of w, to get the complexity O(n log(w)). This strategy allows us to speed up forward and backward propagation. Tiling can also be used to reduce memory cost for temporary storage by not running all the tiles in parallel (just the tiles which do run in parallel need their scratch space), at the potential expense of parallelism or efﬁciency.  For the gradient accumulation, we cannot reuse this strategy, since it involves a larger convolution between an input x of size n and a kernel z = ∂L ∂y of size n − w + 1. However, we have a similar formula:  11  Published as a conference paper at ICLR 2015  ∂c(cid:19)j (cid:18) ∂L  =  n−1  Xi=0  xj+i · zi =  ⌊n/d⌋−1  Xk=0  d−1  Xi=0  xj+i+kd · zi+kd +  n−1  Xi=d⌊n/d⌋  xj+i · zi  And so  (cid:18) ∂L ∂c(cid:19) =  ⌊n/d⌋−1  Xk=0  x[dk,(d+1)k+w−1] ⋆ z[dk,(d+1)k] + x[d⌊n/d⌋,n] ⋆ z[d⌊n/d⌋,n−w+1]  We have a few other optimizations that are planned as well. Since much of the data we have is al- ready available in registers or in shared memory, we are implementing our own in-place, in-register transpose via recursive decomposition. The pointwise multiplications in the Fourier domain, es- pecially with tiling, are rather small, so our own matrix multiplication routines integrated with the rest of the convolution kernel code might win over cuBLAS, and prevent the need for multiple CUDA kernel launches and their associated overhead. Finally, as mentioned earlier, bit reversal portions can be eliminated with the FFT using DIF and the IFFT using DIT.  7 CONCLUSION  To summarize, we achieve signiﬁcant gains in CNNs using FFTs, with a cuFFT convolution im- plementation achieving 1.4 × −14.5× speedups over cuDNN for common sizes. In reaction to cuFFT and cuBLAS limitations in the context of our speciﬁc application domain, we developed our own FFT implementation, fbfft, which is more suited to deep learning problem sizes (large batches, small feature planes). fbfft itself is ≥ 1.4× faster than cuFFT transforms for these prob- lems of interest. For convolution, it is faster than the cuFFT as well, with a mean of 1.51× for sizes that we wish to exploit.  Given our new efﬁcient primitive for size 8-64 convolution, we are continuing work on bit twiddling, transposition and pointwise multiplication optimizations, and continuing work on tiling to make the computational advantage at that size apply to larger convolution problems. These will all allow for reduced training time and use of ever larger and deeper CNNs.  ACKNOWLEDGMENTS  We would like to thank Julien Demouth from NVIDIA who suggested further improvements are still possible by virtue of the current implementation being LSU throughput-bound rather than memory- bound.  REFERENCES Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Bluestein, Leo I. A linear ﬁltering approach to the computation of discrete Fourier transform. Audio and Electroacoustics, IEEE Transactions on, 18(4):451–455, December 1970. ISSN 0018-9278.  Brosch, Tom and Tam, Roger C. Efﬁcient training of convolutional deep belief networks in the frequency domain for application to high-resolution 2d and 3d images. Neural Computation, 27 (1):211–227, 2015. doi: 10.1162/NECO a 00682. URL http://dx.doi.org/10.1162/ NECO_a_00682.  Burrus, C. Sidney. Fast fourier transforms, 2008. URL http://cnx.org/contents/  16e8e5e8-4f22-4b53-9cd6-a15b14f01ce4@5.6:16/Fast_Fourier_ Transforms_(6x9_V.  Chellapilla, Kumar, Puri, Sidd, and Simard, Patrice. High Performance Convolutional Neural Net- works for Document Processing. In Lorette, Guy (ed.), Tenth International Workshop on Frontiers  12  Published as a conference paper at ICLR 2015  in Handwriting Recognition, La Baule (France), October 2006. Universit´e de Rennes 1, Suvisoft. URL https://hal.inria.fr/inria-00112631. http://www.suvisoft.com.  Chetlur, Sharan, Woolley, Cliff, Vandermersch, Philippe, Cohen, Jonathan, Tran, John, Catanzaro, Bryan, and Shelhamer, Evan. cudnn: Efﬁcient primitives for deep learning. CoRR, abs/1410.0759, 2014. URL http://arxiv.org/abs/1410.0759.  Collobert, R., Kavukcuoglu, K., and Farabet, C. Torch7: A matlab-like environment for machine  learning. In BigLearn, NIPS Workshop, 2011a.  Collobert, Ronan, Weston, Jason, Bottou, L´eon, Karlen, Michael, Kavukcuoglu, Koray, and Kuksa, Pavel. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November 2011b. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id= 1953048.2078186.  Cooley, James W. and Tukey, John W. An algorithm for the machine calculation of complex fourier  series. Mathematics of computation, 19(90):297–301, 1965.  Garland, Michael, Le Grand, Scott, Nickolls, John, Anderson, Joshua, Hardwick, Jim, Morton, Scott, Phillips, Everett, Zhang, Yao, and Volkov, Vasily. Parallel computing experiences with cuda. IEEE Micro, 28(4):13–27, July 2008. ISSN 0272-1732. doi: 10.1109/MM.2008.57. URL http://dx.doi.org/10.1109/MM.2008.57.  Giles, Mike. Course on cuda programming on nvidia gpus, lecture 3, 2014. URL http:  //people.maths.ox.ac.uk/gilesm/cuda/lecs/lec3.pdf.  Gunnels, John A., Henry, Greg M., and van de Geijn, Robert A. A family of high-performance matrix multiplication algorithms. In Proceedings of the International Conference on Computa- tional Sciences-Part I, ICCS ’01, pp. 51–60, London, UK, UK, 2001. Springer-Verlag. ISBN 3-540-42232-3. URL http://dl.acm.org/citation.cfm?id=645455.653765.  Irigoin, F. and Triolet, R. Supernode partitioning.  In Proceedings of the 15th ACM SIGPLAN- SIGACT Symposium on Principles of Programming Languages, POPL ’88, pp. 319–329, New York, NY, USA, 1988. ACM. ISBN 0-89791-252-7. doi: 10.1145/73560.73588. URL http: //doi.acm.org/10.1145/73560.73588.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Krizhevsky, Alex.  convnet2/.  cuda-convnet2, 2014. URL https://code.google.com/p/cuda-  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.  Imagenet classiﬁcation with deep convolutional neural networks. In Pereira, F., Burges, C.J.C., Bottou, L., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 25, pp. 1097–1105. Cur- ran Associates, Inc., 2012. URL http://papers.nips.cc/paper/4824-imagenet- classification-with-deep-convolutional-neural-networks.pdf.  Lavin, Andrew. maxdnn: An efﬁcient convolution kernel for deep learning with maxwell gpus.  CoRR, abs/1501.06633, 2015. URL http://arxiv.org/abs/1501.06633.  Lyons, Richard G. Understanding Digital Signal Processing. Addison-Wesley Longman Publishing  Co., Inc., Boston, MA, USA, 1st edition, 1996. ISBN 0201634678.  Mathieu, Micha¨el, Henaff, Mikael, and LeCun, Yann. Fast training of convolutional networks  through ffts. CoRR, abs/1312.5851, 2013. URL http://arxiv.org/abs/1312.5851.  Ragan-Kelley, Jonathan, Barnes, Connelly, Adams, Andrew, Paris, Sylvain, Durand, Fr´edo, and Amarasinghe, Saman P. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. In ACM SIGPLAN Conference on Program- ming Language Design and Implementation, PLDI ’13, Seattle, WA, USA, June 16-19, 2013, pp. 519–530, 2013. doi: 10.1145/2462156.2462176. URL http://doi.acm.org/10.1145/ 2462156.2462176.  13  Published as a conference paper at ICLR 2015  Ruetsch, Greg and Micikevicius, Paulius. Optimizing matrix transpose in cuda. Technical report,  NVIDIA Corp., January 2009.  Sermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Michael, Fergus, Rob, and LeCun, Yann. In Overfeat: Integrated recognition, localization and detection using convolutional networks. International Conference on Learning Representations (ICLR 2014). CBLS, April 2014. URL http://openreview.net/document/d332e77d-459a-4af8-b3ed-55ba.  Shin, Jaewook, Hall, Mary W., Chame, Jacqueline, Chen, Chun, Fischer, Paul F., and Hovland, Paul D. Speeding up nek5000 with autotuning and specialization. In Proceedings of the 24th ACM International Conference on Supercomputing, ICS ’10, pp. 253–262, New York, NY, USA, 2010. ACM. ISBN 978-1-4503-0018-6.  Valiant, Leslie G. A bridging model for parallel computation. Commun. ACM, 33(8):103–111, August 1990. ISSN 0001-0782. doi: 10.1145/79173.79181. URL http://doi.acm.org/ 10.1145/79173.79181.  Volkov, V. Better performance at lower occupancy. In GPU Technology Conference, 2010. URL  http://www.cs.berkeley.edu/˜volkov/volkov10-GTC.pdf.  14  Published as a conference paper at ICLR 2015  8 SUPPLEMENT  8.1 CUFFT CONVOLUTION PERFORMANCE BREAKDOWN  We show a breakdown of cuFFT convolution performance for the steps indicated in Table 1. The timings do not add up to 100% of the reported performance in the previous table because we do not report additional copies needed for zero-padding here. We also enforce force extra synchronizations to isolate the contribution of each operation. Abstracting from these details, the FFT and IFFT take up a signiﬁcant amount of compute resources, which we address in Section 5.  Table 5: cuFFT convolution performance breakdown (K40m, ms)  LAYER FFT A TRANS. A FFT B TRANS. B CGEMM TRANS. C IFFT C  L1 fprop bprop accGrad L2 fprop bprop accGrad L3 fprop bprop accGrad L4 fprop bprop accGrad L5 fprop bprop accGrad  0.86 0.86 1.14  2.99 2.99 5.94  3.07 3.08 3.07  0.84 0.83 0.84  7.07 7.07 2.40  0.24 0.24 0.32  0.98 0.98 2.04  0.89 0.89 0.89  0.24 0.24 0.24  1.58 1.59 0.51  1.13 34.55 34.60  0.32 10.26 10.26  15.13 12.62 12.37  12.67 0.39 0.26  36.46 1.19 0.91  5.91 5.92 5.93  3.08 3.07 3.06  0.83 0.83 0.82  2.39 2.40 2.38  2.03 2.03 2.02  0.89 0.90 0.89  0.24 0.24 0.24  0.51 0.51 0.52  8.92 8.85 8.38  4.40 4.05 4.03  1.21 1.13 1.10  6.23 5.59 6.18  1.67 1.67 0.83  0.87 0.86 0.87  0.24 0.23 0.24  0.50 0.51 1.54  6.24 6.23 3.15  3.49 3.48 3.48  0.95 0.94 0.95  2.54 2.54 7.51  In the particular case of L1, the FFTs take more than 50% of the runtime. This is due to the wasteful interpolation of the kernel tensor from a 11 × 11 up to 128 × 128, which is the minimal size to compute the FFT of the input array without interpolation loss. In such cases, the tiling strategy we are developing (see section 6) will result in large additional performance gains.  8.2 FFT : DECIMATION IN TIME VS FREQUENCY  A Fourier transform projects R and C-valued functions onto a harmonic orthogonal basis. The discrete Fourier transform of a vector {xk}, k ∈ [0, n − 1] is the vector:  {Xk} =   n−1  Xj=0  xjwkj  n   , k ∈ [0, n − 1]  where wj recursively decomposes the computation between an odd and even part:  n = e−2πij/n is the jth n-root of unity. The traditional radix-2 Cooley-Tukey algorithm  {Xk} =   (n−1)/2  (n−1)/2  xjwk(2j)  n  +  Xj=0  Xj=0  15  x2j+1wk(2j+1)  n    , k ∈ [1, n]  Published as a conference paper at ICLR 2015  Figure 9: DIT output ordered (left); DIF input ordered (right) (Burrus (2008))  This decomposition is called decimation in time (DIT). An alternate decomposition performs deci- mation in frequency (DIF):  {Xk} =   (n−1)/2  Xj=0  xjwkj  n +  n  Xj=(n−1)/2  xjwkj  n   , k ∈ [1, n]  When n is a power of 2, both decimations recursively decompose into a perfectly balanced tree and take advantage of the symmetry properties of the roots of unity. The dataﬂow graph for the radix-2 FFT has a butterﬂy shape and is a good way of visualizing the computations. There is a symmetry between DIT and DIF in both the order of operations applied and in whether the input or the output order is shufﬂed (Figure 9).  8.3 GPU PROGRAMMING  There are a variety of references available that describe CUDA and NVIDIA’s various GPU architec- tures (Garland et al. (2008)) which we won’t discuss in detail, but the implementation of fbfft very much depends upon speciﬁcs of the Kepler GPU architecture.  NVIDIA GPUs execute code at the granularity of a warp which is deﬁned as a set of 32 threads in all existing architectures; each thread is assigned a lane within the warp. These threads execute in a SIMT (single instruction, multiple thread) fashion, meaning that a warp is an atomic unit of execution. It holds a single program counter (PC) and can thus only execute a single instruction at a time across all of its threads. Collections of warps are brought together in blocks or CTAs, which together share a region of fast shared memory resident on chip. Blocks themselves can only exchange data via much slower global memory, resident on the GPU or in the host CPU’s address space.  Individual threads within a warp are free to take divergent paths, but since a single PC is present, each branch in the execution will be serialized. Threads that aren’t participating in the branch in question are disabled. In other words, if all 32 threads were to take divergent code paths, we would obtain only 1/32× of the computational efﬁciency. Divergent code paths are hard to avoid, but the NVIDIA instruction set has means to reduce their cost (Giles (2014)). One is with predicated instructions, which are used for small branches, in which all warp threads execute both parts of the branch, with non-participating threads having no side effects.  Block threads have access to a register ﬁle, with up to 255 registers per thread for Kepler. Registers are allocated statically by the CUDA compiler. An important performance factor when writing CUDA kernels is that data should be kept in registers as much as possible to avoid communications.  16  Published as a conference paper at ICLR 2015  Registers in CUDA are “addressable”: it is possible to declare a static array within registers and operate on its elements. The limitation is that all addressing should be performed using statically determined constants so the compiler can translate these accesses to a register number known at compile time. Indirect addressing is also supported but results in copies to a local region within global memory, which essentially constitutes register spilling. Even with the presence of caches, using local memory usually comes with a performance hit.11 As a consequence, we design our kernels using aggressive inlining, template parameters and unrolling directives to make all register accesses statically addressable.  The Kepler architecture introduced specialized shufﬂe instructions to exchange data between regis- ters within a warp synchronously, which avoids round-trips to shared or global memory. Interest- ingly, these shufﬂe instructions allow the dynamic indexing of an array held in registers, as long as the array is distributed in a cyclic fashion across registers in each thread within a warp.  float arr[3]; ... // This simulates a linear array float realArr[96]: // arr[0] holds elements 0-31 (lane i holds element i) // arr[1] holds elements 32-63 (lane i holds element 32 + i) // arr[2] holds elements 64-95 (lane i holds element 64 + i) // Example: all warp threads read value held at realArr[34] float val = __shfl(arr[1], 2); // ‘1‘ must be statically known  // ‘2‘ can be dynamic  Many warps run in parallel and can be switched by the GPU hardware at each cycle. When enough parallelism is available (measured in occupancy of the GPU as a ﬁrst approximation), long latency operations are hidden thanks to fast context switching. Registers and shared memory come in ﬁnite quantities on each GPU compute multiprocessor. These limited resources are partitioned by the compiler and the hardware amongst computations at the level of a CUDA kernel. Increased usage of registers or of shared memory can reduce GPU occupancy, which limits the ability to hide long latency operations. Reduced occupancy does not necessarily result in performance loss (Volkov (2010)). There are often non-obvious performance tradeoffs in increasing or decreasing threads per block, shared memory per block or registers per thread that are hard to discover. This problem is one of the many reasons why designing a one-size-ﬁts-all implementation that aims to be efﬁcient for any problem is difﬁcult.  11There are bleeding edge cases where a little local memory consumption helps performance; for instance,  when restricting the number of registers per thread to increase occupancy.  17  ",
1406.2751,2015,Reweighted Wake-Sleep,"['Reweighted Wake-Sleep', 'Jorg Bornschein and Yoshua Bengio']",https://arxiv.org/pdf/1406.2751,"5 1 0 2    r p A 6 1         ]  G L . s c [      4 v 1 5 7 2  .  6 0 4 1 : v i X r a  Published as a conference paper at ICLR 2015  REWEIGHTED WAKE-SLEEP  J¨org Bornschein and Yoshua Bengio ∗ Department of Computer Science and Operations Research University of Montreal Montreal, Quebec, Canada  ABSTRACT  Training deep directed graphical models with many hidden variables and perform- ing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed gen- erative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of la- tent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is conﬁrmed ex- perimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure. Based on this interpretation, we propose that a sigmoidal belief network is not sufﬁciently powerful for the layers of the inference network in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.  1  INTRODUCTION  Training directed graphical models – especially models with multiple layers of hidden variables – remains a major challenge. This is unfortunate because, as has been argued previously (Hin- ton et al., 2006; Bengio, 2009), a deeper generative model has the potential to capture high-level abstractions and thus generalize better. The exact log-likelihood gradient is intractable, be it for Helmholtz machines (Hinton et al., 1995; Dayan et al., 1995), sigmoidal belief networks (SBNs), or deep belief networks (DBNs) (Hinton et al., 2006), which are directed models with a restricted Boltzmann machine (RBM) as top-layer. Even obtaining an unbiased estimator of the gradient of the DBN or Helmholtz machine log-likelihood is not something that has been achieved in the past. Here we show that it is possible to get an unbiased estimator of the likelihood (which unfortunately makes it a slightly biased estimator of the log-likelihood), using an importance sampling approach. Past proposals to train Helmholtz machines and DBNs rely on maximizing a variational bound as proxy for the log-likelihood (Hinton et al., 1995; Kingma and Welling, 2014; Rezende et al., 2014). The ﬁrst of these is the wake-sleep algorithm (Hinton et al., 1995), which relies on combining a “recognition” network (which we call an approximate inference network, here, or simply inference network) with a generative network. In the wake-sleep algorithm, they basically provide targets for each other. We review these previous approaches and introduce a novel approach that generalizes the wake-sleep algorithm. Whereas the original justiﬁcation of the wake-sleep algorithm has been questioned (because we are optimizing a KL-divergence in the wrong direction), a contribution of this paper is to shed a different light on the wake-sleep algorithm, viewing it as a special case of the proposed reweighted wake-sleep (RWS) algorithm, i.e., as reweighted wake-sleep with a single sample. This shows that wake-sleep corresponds to optimizing a somewhat biased estimator of the likelihood gradient, while using more samples makes the estimator less biased (and asymptotically unbiased as more samples are considered). We empirically show that effect, with clearly better re- sults obtained with K = 5 samples than with K = 1 (wake-sleep), and 5 or 10 being sufﬁcient to  ∗J¨org Bornschein is a CIFAR Global Scholar; Yoshua Bengio is a CIFAR Senior Fellow  1  Published as a conference paper at ICLR 2015  achieve good results. Unlike in the case of DBMs, which rely on a Markov chain to get samples and estimate the gradient by a mean over those samples, here the samples are i.i.d., avoiding the very serious problem of mixing between modes that can plague MCMC methods (Bengio et al., 2013) when training undirected graphical models. Another contribution of this paper regards the architecture of the deep approximate inference net- work. We view the inference network as estimating the posterior distribution of latent variables given the observed input. With this view it is plausible that the classical architecture of the inference network (a SBN, details below) is inappropriate and we test this hypothesis empirically. We ﬁnd that more powerful parametrizations that can represent non-factorial posterior distributions yield better results.  2 REWEIGHTED WAKE-SLEEP  2.1 THE WAKE-SLEEP ALGORITHM  The wake-sleep algorithm was proposed as a way to train Helmholtz machines, which are deep directed graphical models p(x, h) over visible variables x and latent variables h, where the latent variables are organized in layers hk. In the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995), the top layer hL has a factorized unconditional distribution, so that ancestral sampling can proceed from hL down to h1 and then the generated sample x is generated by the bottom layer, given h1. In the deep belief network (DBN) (Hinton et al., 2006), the top layer is instead generated by a RBM, i.e., by a Markov chain, while simple ancestral sampling is used for the others. Each intermediate layer is speciﬁed by a conditional distribution parametrized as a stochastic sigmoidal layer (see section 3 for details). The wake-sleep algorithm is a training procedure for such generative models, which involves train- ing an auxiliary network, called the inference network, that takes a visible vector x as input and stochastically outputs samples hk for all layers k = 1 to L. The inference network outputs sam- ples from a distribution that should estimate the conditional probability of the latent variables of the generative model (at all layers) given the input. Note that in these kinds of directed models exact inference, i.e., sampling from p(h|x) is intractable. The wake-sleep algorithm proceeds in two phases. In the wake phase, an observation x is sampled from the training distribution D and propagated stochastically up the inference network (one layer at a time), thus sampling latent values h from q(h|x). Together with x, the sampled h forms a target for training p, i.e., one performs a step of gradient ascent update with respect to maximum likelihood over the generative model p(x, h), with the data x and the inferred h. This is useful because whereas h p(x, h) is intractable, computing the gradient of the complete log-likelihood log p(x, h) is easy. In addition, these updates decouple all the layers (because both the input and the target of each layer are considered observed). In the sleep-phase, a “dream” sample is obtained from the generative network by ancestral sampling from p(x, h) and is used as a target for the maximum likelihood training of the inference network, i.e., q is trained to estimate p(h|x). The justiﬁcation for the wake-sleep algorithm that was originally proposed is based on the following variational bound,  computing the gradient of the marginal likelihood p(x) =(cid:80)  log p(x) ≥(cid:88)  q(h|x) log  p(x, h) q(h|x)  h  that is true for any inference network q, but the bound becomes tight as q(h|x) approaches p(h|x). Maximizing this bound with respect to p corresponds to the wake phase update. The update with respect to q should minimize KL(q(h|x)||p(h|x)) (with q as the reference) but instead the sleep phase update minimizes the reversed KL divergence, KL(p(h|x)||q(h|x)) (with p as the reference).  2.2 AN IMPORTANCE SAMPLING VIEW YIELDS REWEIGHTED WAKE-SLEEP If we think of q(h|x) as estimating p(h|x) and train it accordingly (which is basically what the sleep phase of wake-sleep does), then we can reformulate the likelihood as an importance-weighted  2  Published as a conference paper at ICLR 2015  average:  (cid:88)  h  p(x) =  q (h| x)  p(x, h) q (h| x)  =  E  h∼q(h | x)  (cid:21)  (cid:20) p(x, h)  q (h| x)  K(cid:88)  (cid:39) 1 K h(k)∼q(h | x)  k=1  q(cid:0)h(k) | x(cid:1)  p(x, h(k))  (1)  Eqn. (1) is a consistent and unbiased estimator for the marginal likelihood p(x). The optimal q that results in a minimum variance estimator is q∗(h| x) = p (h| x). In fact we can show that this is a zero-variance estimator, i.e., the best possible one that will result in a perfect p(x) estimate even with a single arbitrary sample h ∼ p (h| x):  E  = p(x)  h∼p(h | x)  p(h| x)  (2) Any mismatch between q and p (h| x) will increase the variance of this estimator, but it will not In practice however, we are typically interested in an estimator for the log- introduce any bias. likelihood. Taking the logarithm of (1) and averaging over multiple datapoints will result in a con- servative biased estimate and will, on average, underestimate the true log-likelihood due to the concavity of the logarithm. Increasing the number of samples will decrease both the bias and the variance. Variants of this estimator have been used in e.g. (Rezende et al., 2014; Gregor et al., 2014) to evaluate trained models.  h∼p(h | x)  [1] = p(x)  (cid:20) p (h| x) p(x)  (cid:21)  E  2.3 TRAINING BY REWEIGHTED WAKE-SLEEP  We now consider the models p and q parameterized with parameters θ and φ respectively. Updating pθ for given qφ: We propose to use an importance sampling estimator based on eq. (1) to compute the gradient of the marginal log-likelihood Lp(θ, x) = log pθ(x):  (cid:20) p(x, h)  q (h| x)  (cid:21)  ∂ ∂θ  log p(x, h)  Lp(θ, x ∼ D) =  ∂ ∂θ  E  h∼q(h | x)  1  p(x)  (cid:39) K(cid:88)  k=1  ˜ωk  ∂ ∂θ  and the importance weights ˜ωk =  ; ωk =  log p(x, h(k)) with h(k) ∼ q (h| x)  (3)  ωk(cid:80)K  k(cid:48)=1 ωk(cid:48)  q(cid:0)h(k) | x(cid:1).  p(x, h(k))  See the supplement for a detailed derivation. Note that this is a biased estimator because it implicitly contains a division by the estimated p(x). Furthermore, there is no guarantee that q = p (h| x) results in a minimum variance estimate of this gradient. But both, the bias and the variance, decrease as the number of samples is increased. Also note that the wake-sleep algorithm uses a gradient that is equivalent to using only K = 1 sample. Another noteworthy detail about eq. (3) is that the importance weights ˜ω are automatically normalized such that they sum up to one. Updating qφ for given pθ: In order to minimize the variance of the estimator (1) we would like q (h| x) to track p (h| x). We propose to train q using maximum likelihood learning with the loss Lq(φ, x, h) = log qφ(x|h). There are at least two reasonable options how to obtain training data for Lq: 1) maximize Lq under the empirical training distribution x ∼ D, h ∼ p (h| x), or 2) maximize Lq under the generative model (x, h) ∼ pθ(x, h). We will refer to the former as wake phase q-update and to the latter as sleep phase q-update. In the case of a DBN (where the top layer is generated by an RBM), there is an intermediate solution called contrastive-wake-sleep, which has been proposed in (Hinton et al., 2006). In contrastive wake-sleep we sample x from the training distribution, propagate it stochastically into the top layer and use that h as starting point for a short Markov chain in the RBM, then sample the other layers in the generative network p to generate the rest of (x, h). The objective is to put the inference network’s capacity where it matters most, i.e., near the input conﬁgurations that are seen in the training set. Analogous to eqn. (1) and (3) we use importance sampling to derive gradients for the wake phase q-update:  ˜ωk  ∂ ∂φ  log qφ(h(k)|x)  (4)  Lq(φ, x ∼ D) (cid:39) K(cid:88)  ∂ ∂φ  k=1  3  Published as a conference paper at ICLR 2015  with the same importance weights ˜ωk as in (3) (the details of the derivation can again be found in the supplement). Note that this is equivalent to optimizing q so as to minimize KL(p(·|x) (cid:107) q(·|x)). For the sleep phase q-update we consider the model distribution p(x, h) a fully observed system and can thus derive gradients without further sampling:  Lq(φ, (x, h)) =  ∂ ∂φ  ∂ ∂φ  log qφ(h|x) with x, h ∼ p(x, h)  (5)  This update is equivalent to the sleep phase update in the classical wake-sleep algorithm.  Algorithm 1 Reweighted Wake-Sleep training procedure and likelihood estimator. K is the number of approximate inference samples and controls the trade-off between computation and accuracy of the estimators (both for the gradient and for the likelihood). We typically use a large value (K = 100, 000) for test set likelihood estimator but a small value (K = 5) for estimating gradients. Both the wake phase and sleep phase update rules for q are optionally included (either one or both can be used, and best results were obtained using both). The original wake-sleep algorithm has K=1 and only uses the sleep phase update of q. To estimate the log-likelihood at test time, only the  computations up to (cid:98)L are required.  for number of training iterations do  • Sample example(s) x from the training distribution for k = 1 to K do  • Layerwise sample latent variables h(k) from q(h|x) • Compute q(h(k)|x) and p(x, h(k))  end for • Compute unnormalized weights ωk = p(x,h(k)) • Normalize the weights ˜ωk = ωk(cid:80) q(h(k) | x) • Compute log-likelihood estimator (cid:98)L(x) = log averagek ωk • Compute unbiased likelihood estimator ˆp(x) = averagek ωk • Wake-phase update of p: Use gradient estimator(cid:80) • Optionally, wake phase update of q: Use gradient estimator(cid:80)  k ˜ωk  k(cid:48) ωk(cid:48)  • Optionally, sleep phase update of q: Sample (x(cid:48), h(cid:48)) from p and use gradient ∂ log q(h(cid:48)|x(cid:48))  ∂φ  end for  ∂ log p(x,h(k))  ∂θ k ˜ωk  ∂ log q(h(k)|x)  ∂φ  2.4 RELATION TO WAKE-SLEEP AND VARIATIONAL BAYES  Recently, there has been a resurgence of interest in algorithms related to the Helmholtz machine and to the wake-sleep algorithm for directed graphical models containing either continuous or discrete latent variables: In Neural Variational Inference and Learning (NVIL, Mnih and Gregor, 2014) the authors propose to maximize the variational lower bound on the log-likelihood to get a joint objective for both p It was known that this approach results in a gradient estimate of very high variance for and q. the recognition network q (Dayan and Hinton, 1996). In the NVIL paper, the authors therefore propose variance reduction techniques such as baselines to obtain a practical algorithm that enhances signiﬁcantly over the original wake-sleep algorithm. In respect to the computational complexity we note that while we draw K samples from the inference network for RWS, NVIL on the other hand draws only a single sample from q but maintains, queries and trains an additional auxiliary baseline estimating network. With RWS and a typical value of K = 5 we thus require at least twice as many arithmetic operations, but we do not have to store the baseline network and do not have to ﬁnd suitable hyperparameters for it. Recent examples include the auto-encoding variational Bayes (Kingma and Welling, 2014) and stochastic backpropagation papers (Rezende et al., 2014). In both cases one maximizes a variational lower bound on the log-likelihood that is rewritten as two terms: one that is log-likelihood reconstruction error through a stochastic encoder (approximate inference) - decoder (generative model) pair, and one that regularizes the output of the approximate inference stochastic encoder so that its marginal distribution matches the generative prior on the  latent variables  for continuous  4  Published as a conference paper at ICLR 2015  latent variables (and the latter is also trained, to match the marginal of the encoder output). Besides the fact that these variational auto-encoders are only for continuous latent variables, another differ- ence with the reweighted wake-sleep algorithm proposed here is that in the former, a single sample from the approximate inference distribution is sufﬁcient to get an unbiased estimator of the gradient of a proxy (the variational bound). Instead, with the reweighted wake-sleep, a single sample would correspond to regular wake-sleep, which gives a biased estimator of the likelihood gradient. On the other hand, as the number of samples increases, reweighted wake-sleep provides a less biased (asymptotically unbiased) estimator of the log-likelihood and of its gradient. Similar in spirit, but aimed at a structured output prediction task is the method proposed by Tang and Salakhutdinov (2013). The authors optimize the variational bound of the log-likelihood instead of the direct IS estimate but they also derive update equations for the proposal distribution that resembles many of the properties also found in reweighted wake-sleep.  3 COMPONENT LAYERS  Although the framework can be readily applied to continuous variables, we here restrict our- selves to distributions over binary visible and binary latent variables. We build our models by combining probabilistic components, each one associated with one of the layers of the gener- ative network or of the inference network. The generative model can therefore be written as pθ(x, h) = p0(x| h1) p1(h1| h2) ··· pL(hL), while the inference network has the form qφ(h| x) = q1(h1 | x)··· qL(hL | hL−1). For a distribution P to be a suitable component we must have a method to efﬁciently compute P (x(k)| y(k)) given (x(k) , y(k)), and we must have a method to efﬁciently draw i.i.d. samples x(k) ∼ P (x| y) for a given y. In the following we will describe experiments containing three kinds of layers: Sigmoidal Belief Network (SBN) layer: A SBN layer (Saul et al., 1996) is a directed graphical model with independent variables xi given the parents y:  P SBN(xi = 1| y) = σ(W i,: y + bi)  (6) Although a SBN is a very simple generative model given y, performing inference for y given x is in general intractable. Autoregressive SBN layer (AR-SBN, DARN): If we consider xi an ordered set of observed vari- ables and introduce directed, autoregressive links between all previous x<i and a given xi, we obtain a fully-visible sigmoid belief network (FVSBN, Frey, 1998; Bengio and Bengio, 2000). When we additionally condition a FVSBN on the parent layer’s y we obtain a layer model that was ﬁrst used in Deep AutoRegressive Networks (DARN, Gregor et al., 2014):  P AR-SBN(xi = 1| x<i, y) = σ(W i,: y + Si,<ix<i + bi)  (7) We use x<i = (x1, x2,··· , xi−1) to refer to the vector containing the ﬁrst i-1 observed variables. The matrix S is a lower triangular matrix that contains the autoregressive weights between the vari- ables xi, and with Si,<j we refer to the ﬁrst j-1 elements of the i-th row of this matrix. In contrast to a regular SBN layer, the units xi are thus not independent of each other but can be predicted like in a logistic regression in terms of its predecessors x<i and of the input of the layer, y. Conditional NADE layer: The Neural Autoregressive Distribution Estimator (NADE, Larochelle and Murray, 2011) is a model that uses an internal, accumulating hidden layer to predict variables xi given the vector containing all previously variables x<i. Instead of logistic regression in a FVSBN or an AR-SBN, the dependency between the variables xi is here mediated by an MLP (Bengio and Bengio, 2000):  (8) With W and V denoting the encoding and decoding matrices for the NADE hidden layer. For our purposes we condition this model on the random variables y:  P (xi = 1| x<i) = σ(V i,:σ(W :,<i x<i + a) + bi))  P NADE(xi = 1| x<i, y) = σ(V i,:σ(W :,<i x<i + Ua y + a) + U i,:  (9) Such a conditional NADE has been used previously for modeling musical sequences (Boulanger- Lewandowski et al., 2012). For each layer distribution we can construct an unconditioned distribution by removing the condi- tioning variable y. We use such unconditioned distributions as top layer p(h) for the generative network p.  b y + bi))  5  Published as a conference paper at ICLR 2015  Figure 1: A Final log-likelihood estimate w.r.t. number of samples used during training. B L2-norm of the bias and standard deviation of the low-sample estimated pθ gradient relative to a high-sample (K=5,000) based estimate.  NVIL  wake-sleep  (113.1) (99.8) (96.7)  116.3 (120.7) 106.9 (109.4) 101.3 (104.4)  P-model SBN SBN SBN AR-SBN AR-SBN NADE NADE  size 200 200-200 200-200-200 200 200-200 200 200-200  RWS  103.1 93.4 90.1  RWS  95.0 91.1 88.9 89.2 92.8 86.8 87.6  Q-model: SBN Q-model: NADE  Table 1: MNIST results for various architectures and training methods. In the 3rd column we cite the numbers reported by Mnih and Gregor (2014). Values in brackets are variational NLL bounds, values without brackets report NLL estimates (see section 2.2).  4 EXPERIMENTS  Here we present a series of experiments on the MNIST and the CalTech-Silhouettes datasets. The supplement describes additional experiments on smaller datasets from the UCI repository. With these experiments we 1) quantitatively analyze the inﬂuence of the number of samples K, 2) demonstrate that using a more powerful layer-model for the inference network q can signif- icantly enhance the results even when the generative model is a factorial SBN, and 3) show that we approach state-of-the-art performance when using either relatively deep models or when using powerful layer models such as a conditional NADE. Our implementation is available at https://github.com/jbornschein/reweighted-ws/.  4.1 MNIST We use the MNIST dataset that was binarized according to Murray and Salakhutdinov (2009) and downloaded in binarized form from (Larochelle, 2011). For training we use stochastic gradient decent with momentum (β=0.95) and set mini-batch size to 25. The experiments in this paragraph were run with learning rates of {0.0003, 0.001, and 0.003}. From these three we always report the experiment with the highest validation log-likelihood. In the majority of our experiments a learning rate of 0.001 gave the best results, even across different layer models (SBN, AR-SBN and NADE). If not noted otherwise, we use K = 5 samples during training and K = 100, 000 samples to estimate the ﬁnal log-likelihood on the test set1. To disentangle the inﬂuence of the different q updating methods we setup p and q networks consisting of three hidden SBN layers with 10, 200 and 200 units (SBN/SBN 10-200-200). After convergence, the model trained updating q during the sleep phase only reached a ﬁnal estimated log-likelihood of −93.4, the model trained with a q-update during the wake phase reached −92.8, and the model trained with both wake and sleep phase update reached −91.9. As a control we trained a model that does not update q at all. This model reached  1We refer to the lower bound estimates which can be arbitrarily tightened by increasing the number of test  samples as LL estimates to distiguish them from the variational LL lower bounds (see section 2.2).  6  100101102trainingsamples−130−120−110−100−90−80FinalLLestimate(testset)NADE200SBN10-200-200SBN200ABbias (epoch 50)bias (last epoch)std dev. (epoch50)100101102training samples0.30.40.50.60.70.80.9bias0.60.81.01.21.41.61.8std-dev.std dev. (last epoch)Published as a conference paper at ICLR 2015  Figure 2: A Final log-likelihood estimate w.r.t. number of test samples used. B Samples from the SBN/SBN 10-200-200 generative model. C Samples from the NADE/NADE 250 generative model. (We show the probabilities from which each pixel is sampled)  Results on binarized MNIST  Results on CalTech 101 Silhouettes  Method RWS (SBN/SBN 10-100-200-300-400) RWS (NADE/NADE 250) RWS (AR-SBN/SBN 500)† NADE (500 units, [1]) EoNADE (2hl, 128 orderings, [2]) DARN (500 units, [3]) RBM (500 units, CD3, [4]) RBM (500 units, CD25, [4]) DBN (500-2000, [5])  NLL bound  105.5 86.34 86.22  NLL est. 85.48 85.23 84.18 88.35 85.10 84.13  84.55  Method RWS (SBN/SBN 10-50-100-300) RWS (NADE/NADE 150)  NADE (500 hidden units) RBM (4000 hidden units, [6])  NLL est. 113.3 104.3  110.6 107.8  Table 2: Various RWS trained models in relation to previously published methods: [1] Larochelle and Murray (2011), [2] Murray and Larochelle (2014), [3] Gregor et al. (2014) [4] Salakhutdinov and Murray (2008), [5] Murray and Salakhutdinov (2009), [6] Cho et al. (2013). † Same model as the best performing in [3]: a AR-SBN with deterministic hidden variables between the observed and latent. All RWS NLL estimates on MNIST have conﬁdence intervals of ≈ ±0.40.  −171.4. We conﬁrmed that combining wake and sleep phase q-updates generally gives the best results by repeating this experiment with various other architectures. For the remainder of this paper we therefore train all models with combined wake and sleep phase q-updates. Next we investigate the inﬂuence of the number of samples used during training. The results are visualized in Fig. 1 A. Although the results depend on the layer-distributions and on the depth and width of the architectures, we generally observe that the ﬁnal estimated log-likelihood does not improve signiﬁcantly when using more than 5 samples during training for NADE models, and using more than 25 samples for models with SBN layers. We can quantify the bias and the variance of the gradient estimator (3) using bootstrapping. While training a SBN/SBN 10-200-200 model with K = 100 training samples, we use K = 5, 000 samples to get a high quality estimate of the gradient for a small but ﬁxed set of 25 datapoints (the size of one mini-batch). By repeatedly resampling smaller sets of {1, 2, 5,··· , 5000} samples with replacement and by computing the gradient based on these, we get a measure for the bias and the variance of the small sample estimates relative the high quality estimate. These results are visualized in Fig. 1 B. In Fig. 2 A we ﬁnally investigate the quality of the log-likelihood estimator (eqn. 1) when applied to the MNIST test set. Table 1 summarizes how different architectures compare to each other and how RWS compares to related methods for training directed models. We essentially observe that RWS trained models consistently improve over classical wake-sleep, especially for deep architectures. We furthermore observe that using autoregressive layers (AR-SBN or NADE) for the inference network improves the results even when the generative model is composed of factorial SBN layers. Finally, we see that the best performing models with autoregressive layers in p are always shallow with only a single  7  100101102samples−130−120−110−100−90−80est.LLNADE-NADE200SBN-SBN200-200-10SBN-SBN200ABCSBN/SBN 10-100-200-300-400NADE/NADE 250Published as a conference paper at ICLR 2015  Figure 3: CalTech 101 Silhouettes: A Random selection of training data points. B Random samples from the SBN/SBN 10-50-100-300 generative network. C Random Samples from the NADE-150 generative network. (We show the probabilities from which each pixel is sampled)  hidden layer. In Table 2 (left) we compare some of our best models to the state-of-the-art results published on MNIST. The deep SBN/SBN 10-100-200-300-400 model was trained for 1000 epochs with K = 5 training samples and a learning rate of 0.001. For ﬁne-tuning we run additional 500 epochs with a learning rate decay of 1.005 and 100 training samples. For comparison we also train the best performing model from the DARN paper (Gregor et al., 2014) with RWS, i.e., a single layer AR-SBN with 500 latent variables and a deterministic layer of hidden variables between the observed and the latents. We essentially obtain the same ﬁnal testset log-likelihood. For this shallow network we thus do not observe any improvement from using RWS.  4.2 CALTECH 101 SILHOUETTES We applied reweighted wake-sleep to the 28 × 28 pixel CalTech 101 Silhouettes dataset. This dataset consists of 4,100 examples in the training set, 2,264 examples in the validation set and 2,307 examples in the test set. We trained various architectures on this dataset using the same hyperparameter as for the MNIST experiments. Table 2 (right) summarizes our results. Note that our best SBN/SBN model is a relatively deep network with 4 hidden layers (300-100-50-10) and reaches a estimated LL of -116.9 on the test set. Our best network, a shallow NADE/NADE-150 network reaches -104.3 and improves over the previous state of the art (−107.8, a RBM with 4000 hidden units by Cho et al. (2013)).  5 CONCLUSIONS We introduced a novel training procedure for deep generative models consisting of multiple layers of binary latent variables. It generalizes and improves over the wake-sleep algorithm providing a lower bias and lower variance estimator of the log-likelihood gradient at the price of more samples from the inference network. During training the weighted samples from the inference network decouple the layers such that the learning gradients only propagate within the individual layers. Our experiments demonstrate that a small number of ≈ 5 samples is typically sufﬁcient to jointly train relatively deep architectures of at least 5 hidden layers without layerwise pretraining and without carefully tuning learning rates. The resulting models produce reasonable samples (by visual inspection) and they approach state-of-the-art performance in terms of log-likelihood on several discrete datasets. We found that even in the cases when the generative networks contain SBN layers only, better results can be obtained with inference networks composed of more powerful, autoregressive layers. This however comes at the price of reduced computational efﬁciency on e.g. GPUs as the individual variables hi ∼ q(h|x) have to be sampled in sequence (even though the theoretical complexity is not signiﬁcantly worse compared to SBN layers). We furthermore found that models with autoregressive layers in the generative network p typically produce very good results. But the best ones were always shallow with only a single hidden layer. At this point it is unclear if this is due to optimization problems. Acknowledgments We would like to thank Laurent Dinh, Vincent Dumoulin and Li Yao for helpful discussions and the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) for their powerful software. We furthermore acknowledge CIFAR and Canada Research Chairs for funding and Compute Canada, and Calcul Qu´ebec for providing computational resources.  8  Published as a conference paper at ICLR 2015  REFERENCES Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  Bengio, Y. (2009). Learning deep architectures for AI. Now Publishers. Bengio, Y. and Bengio, S. (2000). Modeling high-dimensional discrete data with multi-layer neural  networks. In NIPS’99, pages 400–406. MIT Press.  Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013). Better mixing via deep representations.  In Proceedings of the 30th International Conference on Machine Learning (ICML’13). ACM.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- In  Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012). Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In ICML’2012.  Cho, K., Raiko, T., and Ilin, A. (2013). Enhanced gradient for training restricted boltzmann ma-  chines. Neural computation, 25(3), 805–831.  Dayan, P. and Hinton, G. E. (1996). Varieties of helmholtz machine. Neural Networks, 9(8), 1385–  1403.  Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. (1995). The Helmholtz machine. Neural  computation, 7(5), 889–904.  Frey, B. J. (1998). Graphical models for machine learning and digital communication. MIT Press. Gregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. (2014). Deep autoregressive  networks. In Proceedings of the 31st International Conference on Machine Learning.  Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995). The wake-sleep algorithm for unsu-  pervised neural networks. Science, 268, 1558–1161.  Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets.  Neural Computation, 18, 1527–1554.  Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In Proceedings of the  International Conference on Learning Representations (ICLR).  Larochelle, H. (2011). Binarized mnist dataset. http://www.cs.toronto.edu/˜larocheh/ public/datasets/binarized_mnist/binarized_mnist_[train|valid|test].amat.  Larochelle, H. and Murray, I. (2011). The Neural Autoregressive Distribution Estimator. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS’2011), volume 15 of JMLR: W&CP.  Mnih, A. and Gregor, K. (2014). Neural variational inference and learning in belief networks. In Proceedings  of the 31st International Conference on Machine Learning (ICML 2014). to appear.  Murray, B. U. I. and Larochelle, H. (2014). A deep and tractable density estimator. In ICML’2014.  Murray, I. and Salakhutdinov, R. (2009). Evaluating probabilities under high-dimensional latent variable mod-  els. In NIPS’08, volume 21, pages 1137–1144.  Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference  in deep generative models. In ICML’2014.  Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief networks. In Proceedings  of the International Conference on Machine Learning, volume 25.  Saul, L. K., Jaakkola, T., and Jordan, M. I. (1996). Mean ﬁeld theory for sigmoid belief networks. Journal of  Artiﬁcial Intelligence Research, 4, 61–76.  Tang, Y. and Salakhutdinov, R. (2013). Learning stochastic feedforward neural networks. In NIPS’2013.  9  Published as a conference paper at ICLR 2015  6 SUPPLEMENT  6.1 GRADIENTS FOR p(x, h)  Lp(θ, x ∼ D) =  ∂ ∂θ  =  =  =  (cid:39)  6.2 GRADIENTS FOR THE WAKE PHASE Q UPDATE  with ωk =  Lq(φ, x ∼ D) =  ∂ ∂φ  =  (cid:39)  ∂ ∂φ  1  h  p(x)  1(cid:80)  k ωk  (cid:88)  h  1  p(x)  ∂ ∂θ  p(x, h)  log p(x, h)  log pθ(x) =  (cid:88) (cid:88)  h  h  ∂ ∂θ  1  p(x)  1  p(x)  1  E  p(x, h)  ∂ ∂θ q (h| x)  ∂ ∂θ  (cid:20) p(x, h)  p(x, h) q (h| x) ∂ ∂θ  log p(x, h)  (cid:21)  log p(x, h)  k=1  ωk  p(x)  h∼q(h | x)  log p(x, h(k))  k ωk p(x, h(k))  q (h| x) ∂ ∂θ  K(cid:88) 1(cid:80) q(cid:0)h(k) | x(cid:1) and h(k) ∼ q (h| x) (cid:88)  p(x, h)log qφ(h|x)  (cid:20) p(x, h)  log qφ(h|x)  h∼q(h | x)  E  K(cid:88)  k=1  ∂ ∂φ  q (h| x) log qφ(h(k)|x) ∂ ∂φ  ωk  (cid:21)  h  ∂ ∂φ  (cid:88) = −(cid:88) (cid:39) − 1(cid:80)  h  k ωk  pθ(h|x)  ∂ ∂φ  K(cid:88)  k=1  pθ(h|x) qφ(h|x) log qφ(h|x)  Note that we arrive at the same gradients when we set out to minimize the KL(p(·|x) (cid:107) q(·|x) for a given datapoint x:  KL(pθ(h|x) (cid:107) qφ(h|x)) =  pθ(h|x) log  ∂ ∂φ  ωk  ∂ ∂φ  log qφ(h(k)|x)  (12)  (10)  (11)  10  Published as a conference paper at ICLR 2015  6.3 ADDITIONAL EXPERIMENTAL RESULTS  6.3.1 LEARNING CURVES FOR MNIST EXPERIMENTS  Figure 4: Learning curves for various MNIST experiments.  6.3.2 BOOTSTRAPPING BASED log(p(x)) BIAS/VARIANCE ANALYSIS  Here we show the bias/variance analysis from Fig. 1 B (main paper) applied to the estimated log(p(x)) w.r.t. the number of test samples.  Figure 5: Bias and standard deviation of the low-sample estimated log(p(x)) (bootstrapping with K=5,000 primary samples from a SBN/SBN 10-200-200 network trained on MNIST).  11  100101102103# samples20151050Biasbias (epoch 50)bias (last epoch)0.00.20.40.60.81.01.21.41.6std-dev.std. dev. (epoch 50)std. dev. (last epoch)Published as a conference paper at ICLR 2015  6.3.3 UCI BINARY DATASETS  We performed a series of experiments on 8 different binary datasets from the UCI database: For each dataset we screened a limited hyperparameter space: The learning rate was set to a value in 0.001, 0.003, 0.01. For SBNs we use K=10 training samples and we tried the following archi- tectures: Two hidden layers with 10-50, 10-75, 10-100, 10-150 or 10-200 hidden units and three hidden layers with 5-20-100, 10-50-100, 10-50-150, 10-50-200 or 10-100-300 hidden units. We trained NADE/NADE models with K=5 training samples and one hidden layer with 30, 50, 75, 100 or 200 units in it.  Model FVSBN NADE∗ EoNADE+ DARN3 RWS - SBN hidden units RWS - NADE hidden units  ADULT 13.17 13.19 13.19 13.19 13.65  5-20-100  13.16 30  CONNECT4  12.39 11.99 12.58 11.91 12.68  11.68 50  10-50-150  DNA 83.64 84.81 82.31 81.04 90.63 10-150 84.26 100  MUSHROOMS  10.27 9.81 9.68 9.55 9.90  9.71 50  10-50-150  NIPS-0-12 276.88 273.08 272.38 274.68 272.54  10-50-150  271.11  75  OCR-LETTERS  39.30 27.22 27.31 28.17 29.99  26.43 100  10-100-300  RCV1 49.84 46.66 46.12 46.10 46.16  10-50-200  46.09  WEB 29.35 28.39 27.87 28.83 28.18  10-50-300  27.92  Table 3: Results on various binary datasets from the UCI repository. The top two rows quote the baseline results from Larochelle & Murray (2011); the third row shows the baseline results taken from Uria, Murray, Larochelle (2014). (NADE∗: 500 hidden units; EoNADE+: 1hl, 16 ord)  12  ",
1412.6626,2015,The local low-dimensionality of natural images,"['The local low-dimensionality of natural images', 'Olivier Henaff', 'Johannes Balle', 'Neil Rabinowitz', 'and Eero Simoncelli']",https://arxiv.org/pdf/1412.6626,"5 1 0 2    r a     M 4 2      ]  V C . s c [      4 v 6 2 6 6  .  2 1 4 1 : v i X r a  Published as conference paper at ICLR 2015  THE LOCAL LOW-DIMENSIONALITY OF NATURAL IMAGES  Olivier J. H´enaff, Johannes Ball´e, Neil C. Rabinowitz & Eero P. Simoncelli Howard Hughes Medical Institute, and Center for Neural Science New York University New York, NY 10003, USA {ojh221, jb4726, nr66, eero.simoncelli}@nyu.edu  ABSTRACT  We develop a new statistical model for photographic images, in which the local responses of a bank of linear ﬁlters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of ﬁlters so as to minimize the nuclear norm of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a ﬂexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters opti- mized according to this objective are oriented and band-pass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local ﬁlter response covariance alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.  1  INTRODUCTION  Many problems in computer vision and image processing can be formulated in terms of statistical inference, based on probabilistic models of natural, photographic images. Whereas natural images themselves are complex, locally structured, and high-dimensional, the vision community has traditionally sought probabilistic models of images that are simple, global, and low-dimensional. For exam- ple, in the classical spectral model, the coefﬁcients of the Fourier transform are assumed independent and Gaussian, with variance falling with frequency; in block-based PCA, a set of orthogonal ﬁlters are used to decompose each block into components that are modeled as independent and Gaussian; and in ICA, ﬁlters are cho- sen so as to optimize for non-Gaussian (heavy-tailed, or “sparse”) projections (Bell & Sejnowski (1997); ﬁgure 1). To add local structure to these models, a simple observation has proved very useful: the local variance in natural images ﬂuctu- ates over space (Ruderman, 1994; Simoncelli, 1997). This has been made explicit in the Gaussian Scale Mixture model, which represents neighborhoods of individual ﬁlter coefﬁcients as a sin- gle global Gaussian combined with a locally-varying multiplica- tive scale factor (Wainwright & Simoncelli, 2000). The Mixture of GSMs model builds upon this by modeling the local density as a sum of such scale mixtures (Guerrero-Col´on et al., 2008). Here, we extend this concept to a richer and more powerful model by making another simple ob- servation about the local structure of natural images: the covariance of ﬁlter coefﬁcients at a given location can vary smoothly with spatial position, and these local covariances tend to be highly elon-  Figure 1: Global responses of oriented band-pass ﬁlters to natural images are heavy tailed.  1  Published as conference paper at ICLR 2015  gated, i.e. they lie close to low-dimensional subspaces. In section 2, we motivate the model, showing that these properties hold for a pair of simple oriented ﬁlters. We ﬁnd that the low-dimensionality of local covariances depends on both the ﬁlter choice, and the content of the images—speciﬁcally, it does not hold for phase-randomized ﬁlters or images. In section 3, we use this distinctive property to optimize a set of ﬁlters for a measure of low-dimensionality over natural images. Finally, in sec- tion 4, we demonstrate that the local low-dimensional covariance description captures most of the visual appearance of images, by synthesizing images with matching local covariance structure.  2 ANALYSING LOCAL IMAGE STATISTICS  Figure 2: Locally, we can approximate the responses of a pair of oriented band-pass ﬁlters to pho- tographic images as jointly Gaussian, with a covariance that changes continuously across space. In regions with oriented content, these responses are low-dimensional, as indicated by a high correla- tion between ﬁlter responses.  To understand the statistics of local image patches, we begin with a simple example, based on analysis with two orthogonal, oriented band-pass ﬁlters. If we aggregate the two ﬁlters’ responses over the whole image, the resulting joint distribution is approximately spherically symmetric but the marginal distributions are heavy-tailed (Zetzsche & Krieger (1999); Lyu & Simoncelli (2009); ﬁgure 1). However, if we aggregate only over local neighborhoods, the distributions are more ir- regular, with covariance structure that varies in scale (contrast/energy), aspect ratio, and orientation (ﬁgure 2). Our interest here is in the second property, which provides a measure of the dimensional- ity of the data. Speciﬁcally, a covariance with large aspect ratio (i.e., ratio of eigenvalues) indicates that the ﬁlter responses lie close to a one-dimensional subspace (line). In the case of two ﬁlters, we can construct a simple, continuous measure of local dimensionality by calculating the correlation coefﬁcient between ﬁlter responses in local neighborhoods. The dis- tribution of correlation coefﬁcient magnitudes across image patches is very skewed (ﬁgure 3, left): in many locations, the responses are correlated, i.e. the local Gaussians are low-dimensional. In contrast, if we repeat the same experiment with spectrally-matched noise images rather than a pho- tograph (ﬁgure 3, center), the correlations are typically lower, i.e. the local Gaussians are more high-dimensional. The spectral properties of natural images alone are thus insufﬁcient to produce local low-dimensional structure. Similarly, if we analyze a photograph with phase-randomized ﬁl- ters (ﬁgure 3, right), we do not ﬁnd the same local low-dimensionality. We take this as evidence that local low-dimensional structure is a property that emerges from the combination of local band-pass ﬁlters and photographic images.  2  |r| = 0.78|r| = 0.10|r| = 0.37|r| = 0.48Published as conference paper at ICLR 2015  Figure 3: Local low-dimensional structure is not a guaranteed property of all ﬁlter responses to all images. For each panel, the pair of ﬁlters in the top left corner (enlarged 15×) are applied to the image in the top right, and the histogram of local correlation coefﬁcient magnitudes across locations is plotted below. Oriented ﬁlters analyzing natural images (left) exhibit locally low-dimensional responses, but when oriented ﬁlters are applied to spectrally matched noise images (center) or phase- randomized ﬁlters are applied to photographic images (right) this behavior vanishes.  3 LOCAL LOW DIMENSIONALITY AS A LEARNING OBJECTIVE  3.1 OBJECTIVE FUNCTION  We now ask whether these oriented band-pass ﬁlters are the best ﬁlters, or indeed the only ﬁlters, for producing representations of natural images that are locally low-dimensional. Just as marginal sparsity has been used as an objective for optimizing ﬁlters for representation of natural images (Ol- shausen & Field, 1996; Bell & Sejnowski, 1997), we aim to derive a ﬁlter bank that minimizes a measure of the local dimensionality of responses to natural images. Here we describe the construc- tion of the objective function and the motivation behind its components; in section 3.2 we cover some technical details relating to the optimization; in section 3.3 we present the results. To begin, suppose we have a convolutional ﬁlter bank f, and an image x. We compute a map of ﬁlter responses yi(t) = (fi∗x)(t), with the index t indicating spatial position, and consider this map in terms of a set of overlapping patches. For each patch P , we can form a matrix YP = [y(t)]t∈P composed of all the response vectors in that patch. Next, we need to deﬁne an appropriate measure of dimensionality for the local covariance on each patch. The correlation coefﬁcient presented in section 2 does not extend beyond the simple two- dimensional case. Instead, we choose to measure the nuclear norm of YP , i.e. the sum of its singular values. This is the convex envelope of the matrix rank function, so it provides a continuous measure of dimensionality; unlike the rank, it is robust to small perturbations of ﬁlter responses away from true subspaces. The local low-dimensionality component of the objective is thus:  Elocal dim =(cid:88)P (cid:107)YP(cid:107)∗  To ensure that the ﬁlter responses provide a reasonable representation of the original image, we reconstruct the image from them via the ﬁlter bank transpose ˜f (t) = f (−t), and penalize for reconstruction error:  Erecons =(cid:88)t(cid:16)x(t) −(cid:88)i  ( ˜fi ∗ yi)(t)(cid:17)2  Finally, in order to ensure that all ﬁlters are used and the ﬁlter responses are sufﬁciently diverse, we include a term that makes the global distribution of ﬁlter responses high-dimensional. One way to achieve this would be to form the matrix Y of all response vectors from the ensemble, and maximize its nuclear norm:  Eglobal dim = −(cid:107)Y (cid:107)∗  3   0 1 correlation 0 1 correlation 0 1 correlationPublished as conference paper at ICLR 2015  Together with the reconstruction penalty, this tends to whiten the ﬁlter responses (i.e. Cov[y] ∝ I). In practice however, this term allows degenerate solutions with ﬁlters that are related to each other by a phase shift. This is best understood in the Fourier domain: with the Parseval identity,  Cov[y] =(cid:20)(cid:90) yi(t)yj(t) dt(cid:21)i,j =(cid:20)(cid:90) ˆyi(ω)ˆyj(ω) dω(cid:21)i,j =(cid:20)(cid:90) |ˆx(ω)|2 ˆfi(ω) ˆfj(ω) dω(cid:21)i,j  where ˆa = F[a] is the Fourier transform of a. Two ﬁlters with identical Fourier magnitudes but different phases can make this expectation zero. To eliminate this degeneracy, we can maximize the dimensionality of the ﬁlter reconstructions zi = ˜fi∗fi∗x, rather than the ﬁlter responses yi = fi∗x. As  Cov[z] =(cid:20)(cid:90) |ˆx(ω)|2| ˆfi(ω)|2| ˆfj(ω)|2 dω(cid:21)i,j  maximizing the nuclear norm of Z = [z(t)]t pushes Cov[z] towards a multiple of the identity and hence penalizes any overlap between ﬁlter Fourier magnitudes. Since this tends to insist that ﬁlters have hard edges in the Fourier domain, we relax this constraint by only penalizing for overlaps between blurred versions of the ﬁlters’ Fourier magnitudes. Using a Gaussian blurring window h, we compute modulated ﬁlter reconstructions ˜zi = (h ˜fi) ∗ (hfi) ∗ x, and whiten Cov[˜z] =(cid:20)(cid:90) |ˆx(ω)|2|h ∗ ˆfi(ω)|2|h ∗ ˆfj(ω)|2 dω(cid:21)i,j  by maximizing the dimensionality of ˜Z = [˜z(t)]t via the term  Summarizing, we optimize our ﬁlter bank for  E = Elocal dim + λ Erecons  + µ Eglobal dim  Eglobal dim = −(cid:13)(cid:13)(cid:13) ˜Z(cid:13)(cid:13)(cid:13)∗ + λ(cid:13)(cid:13)(cid:13)x −(cid:88)i zi(cid:13)(cid:13)(cid:13) 2 − µ(cid:13)(cid:13)(cid:13) ˜Z(cid:13)(cid:13)(cid:13)∗  2  =(cid:88)P (cid:107)YP(cid:107)∗  We now describe the practical details of the learning procedure and our results.  3.2 OPTIMIZATION  3.2.1 MODEL SPECIFICATIONS  We trained the model described above on the van Hateren dataset (van Hateren & van der Schaaf, 1998) using the Torch machine learning library (Collobert et al., 2011). We used 20×20 pixel ﬁlter kernels, varying in number from 4 to 12, and estimated local dimensionality over neighborhoods of size 16× 16 pixels, weighted by a Gaussian window with a standard devia- tion of 3 pixels. We ﬁxed the blurring window h to be Gaussian with a standard deviation of 3 pixels, such that it only becomes negligible at the kernel boundary. The hyperparameters λ and µ are easily tuned: we increased the reconstruction weight λ until a desired reconstruction level was reached (e.g. a relative L2 error of 1%) and increased the diversity weight µ until none of the ﬁlters were zero nor perfectly correlated with another. In the experiments below they were set to 3500 and 100 respectively. We optimized our ﬁlter bank using stochastic gradient descent with a ﬁxed learning rate, chosen as high as possible without causing any instability.  Figure 4: Gradient scaling. Left: Input spectrum ranges from 1 to 100. Right: Value of objective over time, with and without gradient scaling.  4   0 5unscaledscaled log10(t)162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215UnderreviewasconferencepaperatICRL2015from**to**.WeﬁxedtheblurringwindowhtobeGaussianwithastandarddeviationof**,suchthatitonlybecomesnegligeableatthekernelboundary.Thehyperparametersλandµareveryeasilytuned,inthatthereconstructionweightλcanbeincreaseduntiladesiredreconstructionlevelisreached(inourcasearelativeL2errorof1%)andthediversityweightµcanbeincreaseduntilnoneoftheﬁltersarenulnorperfectlycorrelatedwithoneanother.Intheexperimentsbelowtheyweresetto3500and100respectively.Weoptimizedourﬁlterbankusingstochasticgradientdescentwithaﬁxedlearningrate,chosenashighaspossiblewithoutcausinganyinstability.3.2.2ACCELERATEDLEARNINGWITHGRADIENTSCALINGWedeveloppedamethodtoscalegradientsaccordingtotheinputspectrum,andfoundthatitcon-siderablyacceleratedtheoptimizationprocedure.Givenagradient∂E∂yioftheobjectivefunctionwithrespecttoﬁlteractivationsyi=fi∗x,thebackpropagationrule(citeBP?)statesthattheﬁltercomponentfi(t)receivesthegradient∂E∂fi(t)=∂E∂yi∗˜x(t)where˜xisahorizontally-andvertically-ﬂippedcopyoftheimagex.Thisimpliesthatthefrequencycomponentoftheﬁlterˆfi(ω)receivesthegradient**DETAILEDPROOF?**∂E∂ˆfi(ω)=ˆx(ω)∂E∂ˆyi(ω)Giventhehyperbolicshapeofˆx(ω)fornaturalimages(seeﬁgure*****),thelowerfrequencycomponentsoftheﬁltereffectivelyreceivesubstantiallylargergradientsthanthehighfrequencyones.Wecorrectforthisbydividingthegradient∂E∂ˆfi(ω)bythecorrespondingmeanimagefrequencymagnitude.∂E∂ˆfi(ω)←1E|ˆx(w)|∂E∂ˆfi(ω)=ˆx(ω)E|ˆx(w)|∂E∂ˆfi(ω)Sinceourﬁlterkernelsandgradientsaredeﬁnedinthespatialdomain,wecompletethisoperationwithtwoFouriertransforms∂E∂fi(t)←F−1(cid:31)1E|ˆx(w)|F(cid:31)∂E∂fi(cid:30)(ω)(cid:30)(t)Weestimatethemeanfrequencymagnitudeofthedatasetonce,thendivideallgradientsbythismeanratherthanbythelocalfrequencymagnitudeˆx(ω)inordertoavoidanyinstabilityduetosmallvaluesofˆx(ω),andtoreducethecomplexityoftheoperation.Thisenablesustousemuchlargerlearningratesandweestimatethatitaccelerateslearningbyafactorof10to100(refﬁgure).Werepeatedourexperimentswithoutthegradientscalingmethodandfoundnodifferenceintheﬁnalresult.FIGUREshowingobjectiveasafunctionoftimefortwoidenticalnetworks,oneoptimizedwithgradientscaling,theothernot.Nexttoitshow2dplotofmeanimagespectrum,onalogscale.3.3RESULTSOurresults(ﬁgure2)showthattheoptimalﬁlterbankforexhibitingthelowdimensionalstructureofnaturalimagesiscomposedofalow-pass,ahigh-pass,andasetoforientedband-passﬁlters.Asweincreasethenumberofﬁlters,weobtainaﬁnerpartitionofscalesandorientations:8ﬁlterspartitionFourierspaceintoalow-passband,ahigh-passresidual,and2scalesand3orientations.Thisﬁlterbankalsobecomessensitivetoaliasingintheinputimages,asindicatedbythecircularboundariesofcertainﬁlterspectra.4LOCALLOWDIMENSIONALITYASAPRIORFORNATURALIMAGESHavinglearnedalineartransformationthatuncoversthelowdimensionalstructureofnaturalim-ages,weconstructanon-linearrepresentationwhichparametrizeslocalsubspaces.Wedosoby4log102Published as conference paper at ICLR 2015  3.2.2 ACCELERATED LEARNING WITH GRADIENT SCALING  We developed a method to scale gradients according to the input spectrum, and found that it consid- erably accelerated the optimization procedure. In ordinary gradient descent, the descent direction for the ﬁlter fi(t) is the negative gradient. Using the chain rule, it can be expressed in terms of the ﬁlter responses yi = fi ∗ x:  ∆fi(t) = −  ∂E  ∂fi(t)  In the Fourier domain, this is  = −(cid:18) ∂E  ∂yi ∗ ˜x(cid:19) (t)  ∆ ˆfi(ω) = −  ∂E  ∂ ˆfi(ω)  = −ˆx(ω)  ∂E  ∂ ˆyi(ω)  Due to the hyperbolic spectrum of ˆx(ω) (ﬁgure 4, left panel), the low frequency components of the gradient are substantially larger than the high frequency ones. We offset this imbalance by dividing the gradient by the corresponding mean image frequency magnitude. The modiﬁed descent direction is thus  ∆ ˆfi(ω) =  −1  (cid:112)E [|ˆx(w)|2]  ∂E  ∂ ˆfi(ω)  =  −ˆx(ω)  (cid:112)E [|ˆx(w)|2]  ∂E  ∂ ˆyi(ω)  where the expectation is over the ensemble of all images. The gradient scaling algorithm accelerates convergence by a factor of at least 10 (ﬁgure 4, right panel) and does not affect the ﬁnal result.  3.3 RESULTS  Figure 5: Examples of optimized ﬁlter banks of size 4, 7 and 12. Top row: spatial ﬁlters, bottom row: Fourier magnitudes (zero frequency is in center).  Even though our objective function is not convex with respect to the ﬁlter bank, we found empir- ically that different initializations lead to qualitatively similar results. The optimized ﬁlter bank for uncovering the local low-dimensional structure of natural images is composed of a low-pass, a high-pass, and a set of oriented band-pass ﬁlters (ﬁgure 5). As we increase the number of ﬁlters, we obtain a ﬁner partition of scales and orientations: 7 ﬁlters divide the band-pass region into 2 radial sub-bands, with 3 orientations in the mid-low band and 2 orientations in the mid-high band. With 12 ﬁlters, the mid-low band remains mostly unchanged, while the mid-high band is partitioned into 6 orientations.  4 REPRESENTING NATURAL IMAGES AS LOCAL COVARIANCE MAPS  Having optimized a linear transform to reveal the local low-dimensional covariance structure of natural images, we now ask what information these local covariances actually capture about an im- age. More precisely, we construct a nonlinear representation of an image by ﬁltering it through the learned ﬁlter bank, estimating the local covariances and subsampling the resulting covariance  5  Published as conference paper at ICLR 2015  (a) Original image  #pixels: 21600  (b) neighborhood: 8×8 subsampling: 2×2 #measurements: 54000 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 1.5%  (c) neighborhood: 16×16 subsampling: 4×4 #measurements: 13500 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 5.7%  (d) neighborhood: 24×24 subsampling: 4×4 #measurements: 13500 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 11.1%  Figure 6: Synthesized images, matched for local covariance maps of a bank of 4 optimized ﬁlters (ﬁgure 5), are almost indistinguishable from the original. As the neighborhood over which the covariance is estimated increases, the errors increase but are still far less visible than equivalent amounts of additive white noise. Top row: original image x and synthetic images. Middle row: pixelwise magnitude of difference with original ∆x. Each difference image is individually scaled to full dynamic range for display. Bottom row: original image corrupted with additive Gaussian noise, such that the relative error ((cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2) is the same.  map. To explore the power of this representation, we synthesize new images with the same covari- ance representation. This method of image synthesis is useful for probing the equivalence class of images that are identical with respect to an analysis model, thereby exhibiting its selectivities and invariances (Portilla & Simoncelli, 2000). The procedure for these experiments is as follows. We ﬁrst build a representation of the original image by estimating the covariance matrix of ﬁlter responses in each local neighborhood P :  CP (x) =(cid:104)(cid:88)t∈P  w(t)yi(t)yj(t)(cid:105)i,j  where w is a spatial windowing function. The local covariance map φ of the original (target) image is then:  To synthesize a new image with the same local covariance map, we start with a white noise image x, and perform gradient descent on the objective  φ(xtarget) = [CP (xtarget)]P  E(x) = (cid:107)Vec (φ(x) − φ(xtarget))(cid:107)1  where Vec(a) is a vector composed of the elements of the multi-dimensional array a. We choose an L1 penalty in order to obtain a piecewise quadratic error term, and use a harmonically decaying gradient step to ensure convergence.  6  Published as conference paper at ICLR 2015  4.1 PERCEPTUAL RELEVANCE OF LOCAL COVARIANCES  We distinguish two regimes which lead to very different synthesis results: overcomplete and un- dercomplete. When φ(x) is overcomplete, the solution of the synthesis problem at x = xtarget is often unique. However, even if this holds, ﬁnding this solution can be difﬁcult or expensive as the original image must be recovered from a set of quadratic measurements (Bruna et al., 2013). When φ(x) is undercomplete, many images are represented with the same φ(x), and synthesis amounts to sampling from this equivalence class of solutions (Portilla & Simoncelli, 2000). In an overcomplete setting (ﬁgure 6b), the simple synthesis algorithm is able to reconstruct the image almost perfectly from the local covariance map. Surprisingly, as we move into the undercomplete regime by further subsampling the covariance map (ﬁgure 6c), the synthetic images retain excellent perceptual quality. In the undercomplete regime, the diversity amongst solutions reveals information which is lost by this representation. When we subsample the covariance maps by a factor of 4 in each direction (ﬁgure 6c), samples include slight phase shifts in high frequency content. When we estimate the covariance over an even larger neighborhood (ﬁgure 6d), these phase shifts get larger as indicated by the white lines in the difference image (ﬁgure 6, middle row). Nevertheless, despite the large differences in mean squared error, the synthetic images are almost indistinguishable from the orig- inal, especially when compared to images corrupted by additive white noise of equivalent variance (ﬁgure 6, bottom row). As a control, we compared these results against syntheses from a representation of natural images in terms of local variance maps. These correspond to a subset of the parameters in local covari- ance maps, namely the diagonals of the local covariances. To offset the fact that the local variance maps have fewer parameters (the off-diagonal terms of each local covariance being discarded), we increased the number of ﬁlters to match the cardinality of the covariance maps. We expected that the results would be similar, as the covariance between two ﬁlters can be expressed as the variance of their sum, subtracted by their respective variances. However, the reconstructions from the variance maps are substantially worse (ﬁgure 8), both in terms of mean squared error and perceptual quality. This is because successful synthesis from variance maps requires the ﬁlters to satisfy a stringent set of properties (Bruna et al., 2013). Synthesis from covariance maps appears to be more robust, both in the oversampled and undersampled regimes. Having constructed a representation which captures the shape, scale, and orientation of local dis- tributions of ﬁlter responses, and tested its perceptual relevance, we now investigate the role of the shape of these distributions.  4.2 PERCEPTUAL RELEVANCE OF LOCAL LOW-DIMENSIONAL COVARIANCES  In section 2, we found that natural images distinguish themselves from noise by the proximity of their local distributions to low-dimensional subspaces. We can now ask if these subspaces carry the important information of natural images. Speciﬁcally, if we project the representation onto local subspaces, how much of the original image is preserved? We answer this question by synthesizing from a map of local covariances from which we have discarded information corresponding to the smallest eigenvalues. For every covariance matrix, we compute its eigendecomposition, threshold its eigenvalues at a ﬁxed value, and synthesize from the resulting covariance map. Truncating the distribution of eigenvalues results in the removal of high frequency noise as well as low-contrast edges (ﬁgure 8, 2nd column). Since a ﬁxed threshold does not distinguish between scale and shape, we repeated the experiment with a threshold value that was scaled adaptively by the local energy (sum of all eigenvalues but the ﬁrst, which corresponds to the mean luminance). This modiﬁes the shape of local distributions regardless of their scale. Projecting local distributions onto their local subspaces enhances the image by removing noise while preserving any oriented structure (ﬁgure 8, 3rd column). On the other hand, making the image locally high-dimensional by raising eigenvalues to a power less than one degrades it by texturizing it artiﬁcally (ﬁgure 8, 4th column).  7  Published as conference paper at ICLR 2015  (a) Original image  #pixels: 21600  (b) neighborhood: 8×8 subsampling: 2×2 #measurements: 54000 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 8.4%  (c) neighborhood: 16×16 subsampling: 4×4 #measurements: 13500 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 15.4%  (d) neighborhood: 24×24 subsampling: 4×4 #measurements: 13500 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 20.7%  Figure 7: Synthesized images matched for local variance maps fail to capture the relevant structure of the original. Local variances are computed from 10 ﬁlters, matching the cardinality of the full covariance representation used in ﬁgure 6.  Figure 8: Effects of various modiﬁcations of the local covariance map. Top row: histograms of (log) eigenvalues of local covariance matrices. Applying a ﬁxed threshold to the corresponding singular values (2nd column) removes non-oriented content but also low-contrast edges, whereas adaptive thresholding (3rd column) preserves oriented structure regardless of contrast. Dimensionality ex- pansion (4th column) corrupts the image with artiﬁcial texture.  8  Published as conference paper at ICLR 2015  5 CONCLUSION  We have arrived at a representation of natural images in terms of a spatial map of local covariances. We apply a bank of ﬁlters to the image, and view the vector of responses at each location as a sample from a multivariate Gaussian distribution with zero mean and a covariance that varies smoothly across space. We optimize the ﬁlter bank to minimize the dimensionality of these local covariances; this yields ﬁlters that are local and oriented. We used synthesis to demonstrate the power of this representation. Via a descent method, we im- pose the covariance map estimated from an original image onto a second, noise image. The result- ing image is nearly identical to the original image, and appears to degrade gracefully as we blur and undersample the covariance map. Visually, these reconstructions are vastly better than those obtained from variance maps of equivalent cardinality. The low-dimensionality of the covariances appears to be a crucial factor: when we squeeze the local covariances to make them even more low- dimensional, images retain much of their perceptual quality; when we do the opposite, and make the local covariances more high dimensional, images are corrupted with artiﬁcial textures. A related line of work by Karklin and Lewicki has studied the statistical ﬂuctuations of natural im- ages. Their initial model describes the variance of ﬁlter responses as linear combinations of a sparse set of latent coefﬁcients, thereby approximating the joint distributions of local ﬁlter responses as sep- arable (Karklin & Lewicki, 2005). More recently, they examined the invariance and discriminability of local ﬁlter response distributions, and modeled the log-covariance of image patches as a sparse sum of outer products drawn from a learned dictionary (Karklin & Lewicki, 2009). Ultimately, as in traditional sparse models such as sparse coding, ICA, ISA, or K-SVD (Olshausen & Field, 1996; Bell & Sejnowski, 1997; Hyv¨arinen & Hoyer, 2000; Rubinstein et al., 2013), these higher order coefﬁcients are assumed to be sparse along the axes of a ﬁxed, ﬁnite basis. On the contrary, the ﬁlter responses in our model are not required to be sparse along ﬁxed axes, but along the axes speciﬁed adaptively by the eigenvectors of the local covariance matrix. Approximat- ing an arbitrary subspace in a conventional sparse model would require dictionaries of a size that scales exponentially in the dimensionality of the input space (the so-called “curse of dimensional- ity”). In addition to its computational cost, the high coherence of such an overcomplete dictionary would make the inference of sparse coefﬁcients infeasible with convex relaxations (Donoho, 2006). Our model circumvents these problems by continuously parameterizing orientation in feature space. Similarly, continuous parametrizations of translation have been successfully embedded into sparse optimization problems (Ekanadham et al., 2011). These models avoid the brittleness of conventional sparse representations, which exhibit discontinuities when switching coefﬁcients on or off as a signal smoothly varies across space or time. Our model appears promising for a number of image processing applications. The property of local low-dimensionality provides a means of discriminating between natural images and noise, and thus offers a potentially powerful prior for denoising. Our synthesis experiments indicate that undercomplete or thresholded representations of the covariance map are sufﬁcient to reconstruct the original image with a high perceptual quality, suggesting that lossy compression schemes might make use of this representation to produce less visually salient distortions. Finally, we believe that local covariance map representations offer a natural path for extension into a hierarchical model. As an example, scattering networks have developed decompositions based on alternations of linear ﬁlters and local variance maps for applications such as texture synthesis and invariant object recognition (Bruna & Mallat, 2012). Hierarchical models of alternating linear ﬁlters and nonlinear pooling have also been proposed as an approximate description of computation along the visual cortical hierarchy in mammals (Riesenhuber & Poggio, 1999; Jarrett et al., 2009). Our synthesis experiments suggest that a similar infrastructure which recursively stacks linear decompo- sitions and covariance maps, with an objective of reducing local dimensionality, could offer a new canonical description for the biological visual hierarchy, and an unsupervised architecture for use in machine inference, synthesis, and recognition tasks.  ACKNOWLEDGMENTS  We would like to thank Joan Bruna for helpful discussions on the use of the nuclear norm and technical aspects of the phase-recovery problem.  9  Published as conference paper at ICLR 2015  REFERENCES Bell, Anthony J. and Sejnowski, Terrence J. The ‘independent components’ of natural scenes are  edge ﬁlters. 37(23):3327–3338, 1997.  Bruna, Joan and Mallat, St´ephane. Invariant Scattering Convolution Networks. arXiv.org, cs.CV,  March 2012.  Bruna, Joan, Szlam, Arthur, and LeCun, Yann. Signal Recovery from Pooling Representations.  arXiv.org, stat.ML, November 2013.  Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement. Torch7: A matlab-like environment  for machine learning. BigLearn, 2011.  Donoho, David. For most large underdetermined systems of equations, the minimal l1-norm near- solution approximates the sparsest near-solution. Communications on Pure and Applied Mathe- matics, 59(7):907–934, 2006.  Ekanadham, Chaitanya, Tranchina, Daniel, and Simoncelli, Eero P. Recovery of sparse translation- invariant signals with continuous basis pursuit. IEEE transactions on signal processing : a pub- lication of the IEEE Signal Processing Society, 59(10), October 2011.  Guerrero-Col´on, Jose A., Simoncelli, Eero P., and Portilla, Javier. Image denoising using mixtures  of Gaussian scale mixtures. pp. 565–568, 2008.  Hyv¨arinen, Aapo and Hoyer, Patrik.  by Decomposition  tures http://dx.doi.org/10.1162/089976600300015312, 2000.  of Natural  Emergence of Phase- and Shift-Invariant Fea- Images Independent Feature Subspaces.  into  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09). IEEE, 2009.  Karklin, Yan and Lewicki, Michael S. A Hierarchical Bayesian Model for Learning Nonlinear Statistical Regularities in Nonstationary Natural Signals. Neural Computation, 17(2):397–423, February 2005.  Karklin, Yan and Lewicki, Michael S. Emergence of complex cell properties by learning to gener-  alize in natural scenes. Nature, 457(7225):83–86, January 2009.  Lyu, Siwei and Simoncelli, Eero P. Nonlinear extraction of ‘independent components’ of natural images using radial Gaussianization. Neural Computation, 21(6):1485–1519, Jun 2009. doi: 10.1162/neco.2009.04-08-773.  Olshausen, Bruno A. and Field, David J. Emergence of simple-cell receptive ﬁeld properties by  learning a sparse code for natural images. Nature, 381(6583):607–609, 1996.  Portilla, Javier and Simoncelli, Eero P. A Parametric Texture Model Based on Joint Statistics of  Complex Wavelet Coefﬁcients. International Journal of Computer Vision, 40(1):49–70, 2000.  Riesenhuber, Maximilian and Poggio, Tomaso. Hierarchical models of object recognition in cortex.  Nature Neuroscience, 2:1019–1025, 1999.  Rubinstein, Ron, Peleg, Tomer, and Elad, Michael. Analysis K-SVD: a dictionary-learning algo- rithm for the analysis sparse model. IEEE transactions on signal processing : a publication of the IEEE Signal Processing Society, 61(3):661–677, 2013.  Ruderman, Daniel L. The statistics of natural images. Network: computation in neural systems,  1994.  Simoncelli, Eero P. Statistical models for images: Compression, restoration and synthesis. 1:673–  678, 1997.  10  Published as conference paper at ICLR 2015  van Hateren, J. H. and van der Schaaf, A. Independent component ﬁlters of natural images compared with simple cells in primary visual cortex. Proceedings: Biological Sciences, 265(1394):359–366, Mar 1998.  Wainwright, Martin J. and Simoncelli, Eero P. Scale mixtures of Gaussians and the statistics of natural images. In Solla, S. A., Leen, T. K., and M¨uller, K.-R. (eds.), Adv. Neural Information Processing Systems (NIPS*99), volume 12, pp. 855–861, Cambridge, MA, May 2000. MIT Press.  Zetzsche, C. and Krieger, G. The atoms of vision: Cartesian or polar? 16(7), July 1999.  11  ",
1410.3916,2015,Memory Networks,"['Memory Networks', 'Jason Weston', 'Sumit Chopra', 'and Antoine Bordes']",https://arxiv.org/pdf/1410.3916,"5 1 0 2     v o N 9 2         ] I  A . s c [      1 1 v 6 1 9 3  .  0 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  MEMORY NETWORKS  Jason Weston, Sumit Chopra & Antoine Bordes Facebook AI Research 770 Broadway New York, USA {jase,spchopra,abordes}@fb.com  ABSTRACT  We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term mem- ory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to an- swer questions that require understanding the intension of verbs.  1  INTRODUCTION  Most machine learning models lack an easy way to read and write to part of a (potentially very large) long-term memory component, and to combine this seamlessly with inference. Hence, they do not take advantage of one of the great assets of a modern day computer. For example, consider the task of being told a set of facts or a story, and then having to answer questions on that subject. In principle this could be achieved by a language modeler such as a recurrent neural network (RNN) (Mikolov et al., 2010; Hochreiter & Schmidhuber, 1997) as these models are trained to predict the next (set of) word(s) to output after having read a stream of words. However, their memory (en- coded by hidden states and weights) is typically too small, and is not compartmentalized enough to accurately remember facts from the past (knowledge is compressed into dense vectors). RNNs are known to have difﬁculty in performing memorization, for example the simple copying task of outputting the same input sequence they have just read (Zaremba & Sutskever, 2014). The situation is similar for other tasks, e.g., in the vision and audio domains a long term memory is required to watch a movie and answer questions about it.  In this work, we introduce a class of models called memory networks that attempt to rectify this problem. The central idea is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. The model is then trained to learn how to operate effectively with the memory component. We introduce the general framework in Section 2, and present a speciﬁc implementation in the text domain for the task of question answering in Section 3. We discuss related work in Section 4, describe our experiments in 5, and ﬁnally conclude in Section 6.  2 MEMORY NETWORKS  A memory network consists of a memory m (an array of objects1 indexed by mi) and four (poten- tially learned) components I, G, O and R as follows:  I: (input feature map) – converts the incoming input to the internal feature representation.  1For example an array of vectors or an array of strings.  1  Published as a conference paper at ICLR 2015  G: (generalization) – updates old memories given the new input. We call this generalization as there is an opportunity for the network to compress and generalize its memories at this stage for some intended future use.  O: (output feature map) – produces a new output (in the feature representation space), given  the new input and the current memory state.  R: (response) – converts the output into the response format desired. For example, a textual  response or an action.  Given an input x (e.g., an input character, word or sentence depending on the granularity chosen, an image or an audio signal) the ﬂow of the model is as follows:  1. Convert x to an internal feature representation I(x). 2. Update memories mi given the new input: mi = G(mi, I(x), m), ∀i. 3. Compute output features o given the new input and the memory: o = O(I(x), m). 4. Finally, decode output features o to give the ﬁnal response: r = R(o).  This process is applied at both train and test time, if there is a distinction between such phases, that is, memories are also stored at test time, but the model parameters of I, G, O and R are not updated. Memory networks cover a wide class of possible implementations. The components I, G, O and R can potentially use any existing ideas from the machine learning literature, e.g., make use of your favorite models (SVMs, decision trees, etc.).  I component: Component I can make use of standard pre-processing, e.g., parsing, coreference and entity resolution for text inputs. It could also encode the input into an internal feature represen- tation, e.g., convert from text to a sparse or dense feature vector.  G component: The simplest form of G is to store I(x) in a “slot” in the memory:  mH(x) = I(x),  (1) where H(.) is a function selecting the slot. That is, G updates the index H(x) of m, but all other parts of the memory remain untouched. More sophisticated variants of G could go back and update earlier stored memories (potentially, all memories) based on the new evidence from the current input x. If the input is at the character or word level one could group inputs (i.e., by segmenting them into chunks) and store each chunk in a memory slot.  If the memory is huge (e.g., consider all of Freebase or Wikipedia) one needs to organize the memo- ries. This can be achieved with the slot choosing function H just described: for example, it could be designed, or trained, to store memories by entity or topic. Consequently, for efﬁciency at scale, G (and O) need not operate on all memories: they can operate on only a retrieved subset of candidates (only operating on memories that are on the right topic). We explore a simple variant of this in our experiments.  If the memory becomes full, a procedure for “forgetting” could also be implemented by H as it chooses which memory is replaced, e.g., H could score the utility of each memory, and overwrite the least useful. We have not explored this experimentally yet.  O and R components: The O component is typically responsible for reading from memory and performing inference, e.g., calculating what are the relevant memories to perform a good response. The R component then produces the ﬁnal response given O. For example in a question answering setup O ﬁnds relevant memories, and then R produces the actual wording of the answer, e.g., R could be an RNN that is conditioned on the output of O. Our hypothesis is that without conditioning on such memories, such an RNN will perform poorly.  3 A MEMNN IMPLEMENTATION FOR TEXT  One particular instantiation of a memory network is where the components are neural networks. We refer to these as memory neural networks (MemNNs). In this section we describe a relatively simple implementation of a MemNN with textual input and output.  2  Published as a conference paper at ICLR 2015  3.1 BASIC MODEL  In our basic architecture, the I module takes an input text. Let us ﬁrst assume this to be a sentence: either the statement of a fact, or a question to be answered by the system (later we will consider word-based input sequences). The text is stored in the next available memory slot in its original form2, i.e., S(x) returns the next empty memory slot N : mN = x, N = N + 1. The G module is thus only used to store this new memory, so old memories are not updated. More sophisticated models are described in subsequent sections.  The core of inference lies in the O and R modules. The O module produces output features by ﬁnding k supporting memories given x. We use k up to 2, but the procedure is generalizable to larger k. For k = 1 the highest scoring supporting memory is retrieved with:  o1 = O1(x, m) = arg max i=1,...,N  sO(x, mi)  (2)  where sO is a function that scores the match between the pair of sentences x and mi. For the case k = 2 we then ﬁnd a second supporting memory given the ﬁrst found in the previous iteration:  o2 = O2(x, m) = arg max i=1,...,N  sO([x, mo1 ], mi)  (3)  where the candidate supporting memory mi is now scored with respect to both the original in- put and the ﬁrst supporting memory, where square brackets denote a list3. The ﬁnal output o is [x, mo1 , mo2], which is input to the module R. Finally, R needs to produce a textual response r. The simplest response is to return mok, i.e., to output the previously uttered sentence we retrieved. To perform true sentence generation, one can instead employ an RNN. In our experiments we also consider an easy to evaluate compromise approach where we limit textual responses to be a single word (out of all the words seen by the model) by ranking them:  r = argmaxw∈W sR([x, mo1 , mo2], w)  (4)  where W is the set of all words in the dictionary, and sR is a function that scores the match. An example task is given in Figure 1. In order to answer the question x = “Where is the milk now?”, the O module ﬁrst scores all memories, i.e., all previously seen sentences, against x to retrieve the most relevant fact, mo1 = “Joe left the milk” in this case. Then, it would search the memory again to ﬁnd the second relevant fact given [x, mo1], that is mo2 = “Joe travelled to the ofﬁce” (the last place Joe went before dropping the milk). Finally, the R module using eq. (4) would score words given [x, mo1 , mo2] to output r = “ofﬁce”. In our experiments, the scoring functions sO and sR have the same form, that of an embedding model:  s(x, y) = Φx(x)⊤U ⊤U Φy(y).  (5) where U is a n × D matrix where D is the number of features and n is the embedding dimension. The role of Φx and Φy is to map the original text to the D-dimensional feature space. The simplest feature space to choose is a bag of words representation, we choose D = 3|W | for sO, i.e., every word in the dictionary has three different representations: one for Φy(.) and two for Φx(.) depending on whether the words of the input arguments are from the actual input x or from the supporting memories so that they can be modeled differently.4 Similarly, we used D = 3|W | for sR as well. sO and sR use different weight matrices UO and UR.  2Technically, we will be using an embedding model to represent text, so we could store the incoming input using its learned embedding vector in memory instead. The downside of such a choice is that during learning the embedding parameters are changing, and hence the stored vectors would go stale. However, at test time (where the parameters are not changing) storing as embedding vectors could make sense, as this is faster than reading the original words and then embedding them repeatedly.  3As we will use a bag-of-words model where both x and mo1 are represented in the bag (but with two differ- ent dictionaries) this is equivalent to using the sum sO(x, mi) + sO(mo1 , mi), however a more sophisticated modeling of the inputs (e.g., with nonlinearities) may not separate into a sum.  4Experiments with only a single dictionary and linear embeddings performed worse (not shown). In order to model with only a single dictionary, one could consider deeper networks that transform the words dependent on their context. We leave this to future work.  3  Published as a conference paper at ICLR 2015  Figure 1: Example “story” statements, questions and answers generated by a simple simulation. Answering the question about the location of the milk requires comprehension of the actions “picked up” and “left”. The questions also require comprehension of the time elements of the story, e.g., to answer “where was Joe before the ofﬁce?”.  Joe went to the kitchen. Fred went to the kitchen. Joe picked up the milk. Joe travelled to the ofﬁce. Joe left the milk. Joe went to the bathroom. Where is the milk now? A: ofﬁce Where is Joe? A: bathroom Where was Joe before the ofﬁce? A: kitchen  Training We train in a fully supervised setting where we are given desired inputs and responses, and the supporting sentences are labeled as such in the training data (but not in the test data, where we are given only the inputs). That is, during training we know the best choice of both max functions in eq. (2) and (3)5. Training is then performed with a margin ranking loss and stochastic gradient descent (SGD). Speciﬁcally, for a given question x with true response r and supporting sentences mo1 and mo2 (when k = 2), we minimize over model parameters UO and UR:  max(0, γ − sO(x, mo1 ) + sO(x, ¯f )) +  P¯f 6=mo1 max(0, γ − sO([x, mo1], mo2]) + sO([x, mo1 ], ¯f ′])) +  max(0, γ − sR([x, mo1 , mo2], r) + sR([x, mo1 , mo2], ¯r]))  P¯f ′6=mo2 P¯r6=r  (6)  (7)  (8)  where ¯f , ¯f ′ and ¯r are all other choices than the correct labels, and γ is the margin. At every step of SGD we sample ¯f , ¯f ′, ¯r rather than compute the whole sum for each training example, following e.g., Weston et al. (2011).  In the case of employing an RNN for the R component of our MemNN (instead of using a single word response as above) we replace the last term with the standard log likelihood used in a language modeling task, where the RNN is fed the sequence [x, o1, o2, r]. At test time we output its prediction r given [x, o1, o2]. In contrast the absolute simplest model, that of using k = 1 and outputting the located memory mo1 as response r, would only use the ﬁrst term to train. In the following subsections we consider some extensions of our basic model.  3.2 WORD SEQUENCES AS INPUT  If input is at the word rather than sentence level, that is words arrive in a stream (as is often done, e.g., with RNNs) and not already segmented as statements and questions, we need to modify the approach we have so far described. We hence add a “segmentation” function, to be learned, which takes as in- put the last sequence of words that have so far not been segmented and looks for breakpoints. When the segmenter ﬁres (indicates the current sequence is a segment) we write that sequence to memory, and can then proceed as before. The segmenter is modeled similarly to our other components, as an embedding model of the form:  seg(c) = W ⊤  segUSΦseg(c)  (9)  where Wseg is a vector (effectively the parameters of a linear classiﬁer in embedding space), and c is the sequence of input words represented as bag of words using a separate dictionary. If seg(c) > γ, where γ is the margin, then this sequence is recognised as a segment. In this way, our MemNN has a learning component in its write operation. We consider this segmenter a ﬁrst proof of concept: of course, one could design something much more sophisticated. Further details on the training mechanism are given in Appendix B.  5 However, note that methods like RNNs and LSTMs cannot easily use this information.  4  Published as a conference paper at ICLR 2015  3.3 EFFICIENT MEMORY VIA HASHING  If the set of stored memories is very large it is prohibitively expensive to score all of them as in equations (2) and (3). Instead we explore hashing tricks to speed up lookup: hash the input I(x) into one or more buckets and then only score memories mi that are in the same buckets. We investigated two ways of doing hashing: (i) via hashing words; and (ii) via clustering word embeddings. For (i) we construct as many buckets as there are words in the dictionary, then for a given sentence we hash it into all the buckets corresponding to its words. The problem with (i) is that a memory mi will only be considered if it shares at least one word with the input I(x). Method (ii) tries to solve this by clustering instead. After training the embedding matrix UO, we run K-means to cluster word vectors (UO)i, thus giving K buckets. We then hash a given sentence into all the buckets that its individual words fall into. As word vectors tend to be close to their synonyms, they cluster together and we thus also will score those similar memories as well. Exact word matches between input and memory will still be scored by deﬁnition. Choosing K controls the speed-accuracy trade-off.  3.4 MODELING WRITE TIME  We can extend our model to take into account when a memory slot was written to. This is not important when answering questions about ﬁxed facts (“What is the capital of France?”) but is important when answering questions about a story, see e.g., Figure 1. One obvious way to implement this is to add extra features to the representations Φx and Φy that encode the index j of a given memory mj, assuming that j follows write time (i.e., no memory slot rewriting). However, that requires dealing with absolute rather than relative time. We had more success empirically with the following procedure: instead of scoring input, candidate pairs with s as above, learn a function on triples sOt (x, y, y′):  sOt (x, y, y′) = Φx(x)⊤UOt  ⊤UOt(cid:16)Φy(y) − Φy(y′) + Φt(x, y, y′)(cid:17).  (10) Φt(x, y, y′) uses three new features which take on the value 0 or 1: whether x is older than y, x is older than y′, and y older than y′. (That is, we extended the dimensionality of all the Φ embeddings by 3, and set these three dimensions to zero when not used.) Now, if sOt (x, y, y′) > 0 the model prefers y over y′, and if sOt (x, y, y′) < 0 it prefers y′. The argmax of eq. (2) and (3) are replaced by a loop over memories i = 1, . . . , N , keeping the winning memory (y or y′) at each step, and always comparing the current winner to the next memory mi. This procedure is equivalent to the argmax before if the time features are removed. More details are given in Appendix C.  3.5 MODELING PREVIOUSLY UNSEEN WORDS  Even for humans who have read a lot of text, new words are continuously introduced. For example, the ﬁrst time the word “Boromir” appears in Lord of The Rings (Tolkien, 1954). How should a machine learning model deal with this? Ideally it should work having seen only one example. A possible way would be to use a language model: given the neighboring words, predict what the word should be, and assume the new word is similar to that. Our proposed approach takes this idea, but incorporates it into our networks sO and sR, rather than as a separate step. Concretely, for each word we see, we store a bag of words it has co-occurred with, one bag for the left context, and one for the right. Any unknown word can be represented with such features. Hence, we increase our feature representation D from 3|W | to 5|W | to model these contexts (|W | features for each bag). Our model learns to deal with new words during training using a kind of “dropout” technique: d% of the time we pretend we have not seen a word before, and hence do not have a n-dimensional embedding for that word, and represent it with the context instead.  3.6 EXACT MATCHES AND UNSEEN WORDS  Embedding models cannot efﬁciently use exact word matches due to the low dimensionality n. One solution is to score a pair x, y with  (11) instead. That is, add the “bag of words” matching score to the learned embedding score (with a mixing parameter λ). Another, related way, that we propose is to stay in the n-dimensional em- bedding space, but to extend the feature representation D with matching features, e.g., one per  Φx(x)⊤U ⊤U Φy(y) + λΦx(x)⊤Φy(y)  5  Published as a conference paper at ICLR 2015  word. A matching feature indicates if a word occurs in both x and y. That is, we score with Φx(x)⊤U ⊤U Φy(y, x) where Φy is actually built conditionally on x: if some of the words in y match the words in x we set those matching features to 1. Unseen words can be modeled similarly by using matching features on their context words. This then gives a feature space of D = 8|W |.  4 RELATED WORK  Classical QA methods use a set of documents as a kind of memory, and information retrieval meth- ods to ﬁnd answers, see e.g., (Kolomiyets & Moens, 2011) and references therein. More recent methods try instead to create a graph of facts – a knowledge base (KB) – as their memory, and map questions to logical queries (Berant et al., 2013; 2014). Neural network and embedding approaches have also been recently explored (Bordes et al., 2014a; Iyyer et al., 2014; Yih et al., 2014). Com- pared to recent knowledge base approaches, memory networks differ in that they do not apply a two-stage strategy: (i) apply information extraction principles ﬁrst to build the KB; followed by (ii) inference over the KB. Instead, extraction of useful information to answer a question is performed on-the-ﬂy over the memory which can be stored as raw text, as well as other choices such as embed- ding vectors. This is potentially less brittle as the ﬁrst stage of building the KB may have already thrown away the relevant part of the original data.  Classical neural network memory models such as associative memory networks aim to provide content-addressable memory, i.e., given a key vector to output a value vector, see e.g., Haykin (1994) and references therein. Typically this type of memory is distributed across the whole network of weights of the model rather than being compartmentalized into memory locations. Memory-based learning such as nearest neighbor, on the other hand, does seek to store all (typically labeled) exam- ples in compartments in memory, but only uses them for ﬁnding closest labels. Memory networks combine compartmentalized memory with neural network modules that can learn how to (poten- tially successively) read and write to that memory, e.g., to perform reasoning they can iteratively read salient facts from the memory.  However, there are some notable models that have attempted to include memory read and write operations from the 90s. In particular (Das et al., 1992) designed differentiable push and pop actions called a neural network pushdown automaton. The work of Schmidhuber (1992) incorporated the concept of two neural networks where one has very fast changing weights which can potentially be used as memory. Schmidhuber (1993) proposed to allow a network to modify its own weights “self- referentially” which can also be seen as a kind of memory addressing. Finally two other relevant works are the DISCERN model of script processing and memory (Miikkulainen, 1990) and the NARX recurrent networks for modeling long term dependencies (Lin et al., 1996).  Our work was submitted to arxiv just before the Neural Turing Machine work of Graves et al. (2014), which is one of the most relevant related methods. Their method also proposes to perform (sequence) prediction using a “large, addressable memory” which can be read and written to. In their experi- ments, the memory size was limited to 128 locations, whereas we consider much larger storage (up to 14M sentences). The experimental setups are notably quite different also: whereas we focus on language and reasoning tasks, their paper focuses on problems of sorting, copying and recall. On the one hand their problems require considerably more complex models than the memory network de- scribed in Section 3. On the other hand, their problems have known algorithmic solutions, whereas (non-toy) language problems do not.  There are other recent related works. RNNSearch (Bahdanau et al., 2014) is a method of machine translation that uses a learned alignment mechanism over the input sentence representation while predicting an output in order to overcome poor performance on long sentences. The work of (Graves, 2013) performs handwriting recognition by dynamically determining “an alignment between the text and the pen locations” so that “it learns to decide which character to write next”. One can view these as particular variants of memory networks where in that case the memory only extends back a single sentence or character sequence.  6  Published as a conference paper at ICLR 2015  Table 1: Results on the large-scale QA task of (Fader et al., 2013).  Method (Fader et al., 2013) (Bordes et al., 2014b) MemNN (embedding only) MemNN (with BoW features)  F1 0.54 0.73 0.72 0.82  Table 2: Memory hashing results on the large-scale QA task of (Fader et al., 2013).  Method MemNN (no hashing) MemNN (word hash) MemNN (cluster hash)  Embedding F1 Embedding + BoW F1 Candidates (speedup)  0.72 0.63 0.71  0.82 0.68 0.80  14M (0x) 13k (1000x) 177k (80x)  5 EXPERIMENTS  5.1 LARGE-SCALE QA  We perform experiments on the QA dataset introduced in Fader et al. (2013). It consists of 14M statements, stored as (subject, relation, object) triples, which are stored as memories in the MemNN model. The triples are REVERB extractions mined from the ClueWeb09 corpus and cover di- verse topics such as (milne, authored, winnie-the-pooh) and (sheep, be-afraid-of, wolf). Following Fader et al. (2013) and Bordes et al. (2014b), training combines pseudo-labeled QA pairs made of a question and an associated triple, and 35M pairs of paraphrased questions from WikiAnswers like “Who wrote the Winnie the Pooh books?” and “Who is poohs creator?”.  We performed experiments in the framework of re-ranking the top returned candidate answers by several systems measuring F1 score over the test set, following Bordes et al. (2014b). These answers have been annotated as right or wrong by humans, whereas other answers are ignored at test time as we do not know their label. We used a MemNN model of Section 3 with a k = 1 supporting memory, which ends up being similar to the approach of Bordes et al. (2014b).6 We also tried adding the bag of words features of Section 3.6 as well. Time and unseen word modeling were not used. Results are given in Table 1. The results show that MemNNs are a viable approach for large scale QA in terms of performance. However, lookup is linear in the size of the memory, which with 14M facts is slow. We therefore implemented the memory hashing techniques of Section 3.3 using both hashing of words and clustered embeddings. For the latter we tried K = 1000 clusters. The results given in Table 2 show that one can get signiﬁcant speedups (∼80x) while maintaining similar performance using the cluster-based hash. The string hash on the other hand loses performance (whilst being a lot faster) because answers which share no words are now no longer matched.  5.2 SIMULATED WORLD QA  Similar to the approach of Bordes et al. (2010) we also built a simple simulation of 4 characters, 3 objects and 5 rooms – with characters moving around, picking up and dropping objects. The actions are transcribed into text using a simple automated grammar, and labeled questions are generated in a similar way. This gives a QA task on simple “stories” such as in Figure 1. The overall difﬁculty of the task is that multiple statements have to be used to do inference when asking where an object is, e.g. to answer where is the milk in Figure 1 one has to understand the meaning of the actions “picked up” and “left” and the inﬂuence of their relative order. We generated 7k statements and 3k questions from the simulator for training7, and an identical number for testing and compare MemNNs to RNNs and LSTMs (long short term memory RNNs (Hochreiter & Schmidhuber, 1997)) on this task. To  6We use a larger 128 dimension for embeddings, and no ﬁne tuning, hence the result of MemNN slightly  differs from those reported in Bordes et al. (2014b).  7Learning curves with different numbers of training examples are given in Appendix D.  7  Published as a conference paper at ICLR 2015  Table 3: Test accuracy on the simulation QA task.  Method RNN LSTM MemNN k = 1 MemNN k = 1 (+time) MemNN k = 2 (+time)  actor w/o before  Difﬁculty 1 actor 60.9% 64.8% 31.0% 60.2% 100%  Difﬁculty 5  actor+object  27.9% 49.1% 24.0% 42.5% 100%  actor 23.8% 35.2% 21.9% 60.8% 100%  actor+object  17.8% 29.0% 18.5% 44.4% 99.9%  100% 100% 97.8% 99.9% 100%  test with sequences of words as input (Section 3.2) the statements are joined together again with a simple grammar8, to produce sentences that may contain multiple statements, see e.g., Figure 2. We control the complexity of the task by setting a limit on the number of time steps in the past the entity we ask the question about was last mentioned. We try two experiments: using a limit of 1, and of 5, i.e., if the limit is 5 then we pick a random sentence between 1-5 time steps in the past. If this chosen sentence only mentions an actor, e.g., “Bill is in the kitchen” then we generate the question “where is Bill?” or “where was Bill before the kitchen?”. If the sentence mentions an object, e.g., “Bill dropped the football” then we ask the question “where is the football?”. For the answers we consider two options: (i) single word answers; and (ii) a simple grammar for generating true answers in sentence form, e.g., “kitchen” for (i) and “He is in the kitchen I believe” (and other variants) for (ii). More details on the dataset generation are given in Appendix A. Note that in the object case the supporting statements necessary to deduce the answer may not lie in the last 5 sentences, e.g., in this example the answer depends on other sentences to ﬁnd out where Bill actually was when he dropped the football. In fact, in the dataset we generated necessary supporting statements can be up to 65 sentences before (but are usually closer). For that reason, we also conducted two further types of experiments: where we only ask questions about actors (easier) and about actors and objects (harder). We also consider the actor-based questions without the “before” questions for the simplest possible task (i.e. “where is Bill?” but not “where was Bill before the kitchen?” questions).  Figure 2: Sample test set predictions (in red) for the simulation in the setting of word-based input and where answers are sentences and an LSTM is used as the R component of the MemNN.  Joe went to the garden then Fred picked up the milk; Joe moved to the bathroom and Fred dropped the milk, and then Dan moved to the living room. Where is Dan? A: living room I believe Where is Joe? A: the bathroom Fred moved to the bedroom and Joe went to the kitchen then Joe took the milk there and Dan journeyed to the bedroom; Joe discarded the milk. Where is the milk now ? A: the milk is in the kitchen Where is Dan now? A: I think he is in the bedroom Joe took the milk there, after that Mike travelled to the ofﬁce, then Joe went to the living room, next Dan went back to the kitchen and Joe travelled to the ofﬁce. Where is Joe now? A: I think Joe is in the ofﬁce  For the baseline RNN and LSTM systems we perform language modeling with backpropagation through time (Mikolov et al., 2010), but where we backprop only on answer words9. We optimized the hyperparameters: size of the hidden layer, bptt steps, and learning rate for each dataset. For MemNNs we ﬁxed the embedding dimension to 100, learning rate to 0.01 and margin γ to 0.1 and 10 epochs of training in all experiments.  Results The results for the single word answer setting (i) are given in Table 3. For the actor-only tasks, RNN and LSTMs solve the simpler difﬁculty level 1 task without before questions (“w/o  8We also tried the same kind of experiments with sentence-level rather than word-sequence input, without  joining sentences, giving results with similar overall conclusions, see Appendix E.  9We tried using standard language modeling on the questions as well, with slightly worse results.  8  Published as a conference paper at ICLR 2015  Figure 3: An example story with questions correctly answered by a MemNN. The MemNN was trained on the simulation described in Section 5.2 and had never seen many of these words before, e.g., Bilbo, Frodo and Gollum.  Bilbo travelled to the cave. Gollum dropped the ring there. Bilbo took the ring. Bilbo went back to the Shire. Bilbo left the ring there. Frodo got the ring. Frodo journeyed to Mount-Doom. Frodo dropped the ring there. Sauron died. Frodo went back to the Shire. Bilbo travelled to the Grey-havens. The End. Where is the ring? A: Mount-Doom Where is Bilbo now? A: Grey-havens Where is Frodo now? A: Shire  Figure 4: An example dialogue with a MemNN system trained on both the simulation data and the large-scale QA data. The system is able to (attempt to) answer questions about general world knowledge and about speciﬁc story-based statements in the dialogue.  Fred went to the kitchen. Fred picked up the milk. Fred travelled to the ofﬁce. Where is the milk ? A: ofﬁce Where does milk come from ? A: milk come from cow What is a cow a type of ? A: cow be female of cattle Where are cattle found ? A: cattle farm become widespread in brazil What does milk taste like ? A: milk taste like milk What does milk go well with ? A: milk go with coffee Where was Fred before the ofﬁce ? A: kitchen  before”), but perform worse with before questions, and even worse on the difﬁculty 5 tasks. This demonstrates that the poor performance of the RNN is due to its failure to encode long(er)-term memory. This would likely deteriorate even further with higher difﬁculty levels (distances). LSTMs are however better than RNNs, as expected, as they are designed with a more sophisticated memory model, but still have trouble remembering sentences too far in the past. MemNNs do not have this memory limitation and its mistakes are instead due to incorrect usage of its memory, when the wrong statement is picked by sO. Time features are necessary for good performance on before questions or difﬁculty > 1 (i.e., when the answer is not in the last statement), otherwise sO can pick a statement about a person’s whereabouts but they have since moved. Finally, results on the harder actor+object task indicate that MemNN also successfully perform 2-stage inference using k = 2, whereas MemNNs without such inference (with k = 1) and RNNs and LSTMs fail. We also tested MemNNs in the multi-word answer setting (ii) with similar results, whereby MemNNs outperform RNNs and LSTMs, which are detailed in Appendix F. Example test prediction output demonstrating the model in that setting is given in Figure 2.  5.2.1 QA WITH PREVIOUSLY UNSEEN WORDS  We then tested the ability of MemNNs to deal with previously unseen words at test time using the unseen word modeling approach of Sections 3.5 and 3.6. We trained the MemNN on the same sim- ulated dataset as before and test on the story given in Figure 3. This story is generated using similar structures as in the simulation data, except that the nouns are unknowns to the system at training time. Despite never seeing any of the Lord of The Rings speciﬁc words before (e.g., Bilbo, Frodo, Sauron, Gollum, Shire and Mount-Doom), MemNNs are able to correctly answer the questions.  MemNNs can discover simple linguistic patterns based on verbal forms such as (X, dropped, Y), (X, took, Y) or (X, journeyed to, Y) and can successfully generalize the meaning of their instantiations using unknown words to perform 2-stage inference. Without the unseen word modeling described in Section 3.5, they completely fail on this task.  9  Published as a conference paper at ICLR 2015  5.3 COMBINING SIMULATED DATA AND LARGE-SCALE QA  Combining simulated world learning with real-world data might be one way to show the power and generality of the models we design. We implemented a naive setup towards that goal: we took the two models from Sections 5.1 and 5.2, trained on large-scale QA and simulated data respectively, and built an ensemble of the two. We present the input to both systems and then for each question simply output the response of the two choices with the highest score. This allows us to perform simple dialogues with our combined MemNN system. The system is then capable of answering both general knowledge questions and speciﬁc statements relating to the previous dialogue. An example dialogue trace is given in Fig. 4. Some answers appear ﬁne, whereas others are nonsensical. Future work should combine these models more effectively, for example by multitasking directly the tasks with a single model.  6 CONCLUSIONS AND FUTURE WORK  In this paper we introduced a powerful class of models, memory networks, and showed one instanti- ation for QA. Future work should develop MemNNs for text further, evaluating them on harder QA and open-domain machine comprehension tasks (Richardson et al., 2013). For example, large scale QA tasks that require multi-hop inference such as WebQuestions should also be tried Berant et al. (2013). More complex simulation data could also be constructed in order to bridge that gap, e.g., requiring coreference, involving more verbs and nouns, sentences with more structure and requiring more temporal and causal understanding. More sophisticated architectures should also be explored in order to deal with these tasks, e.g., using more sophisticated memory management via G and more sophisticated sentence representations. Weakly supervised settings are also very important, and should be explored, as many datasets only have supervision in the form of question answer pairs, and not supporting facts as well as we used here. Finally, we believe this class of models is much richer than the one speciﬁc variant we detail here, and that we have currently only explored one speciﬁc variant of memory networks. Memory networks should be applied to other text tasks, and other domains, such as vision, as well.  ACKNOWLEDGMENTS  We thank Tomas Mikolov for useful discussions.  REFERENCES Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly  learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.  Berant, Jonathan, Chou, Andrew, Frostig, Roy, and Liang, Percy. Semantic parsing on freebase from  question-answer pairs. In EMNLP, pp. 1533–1544, 2013.  Berant, Jonathan, Srikumar, Vivek, Chen, Pei-Chun, Huang, Brad, Manning, Christopher D, Van- der Linden, Abby, Harding, Brittany, and Clark, Peter. Modeling biological processes for reading comprehension. In Proc. EMNLP, 2014.  Bordes, Antoine, Usunier, Nicolas, Collobert, Ronan, and Weston, Jason. Towards understanding  situated natural language. In AISTATS, 2010.  Bordes, Antoine, Chopra, Sumit, and Weston, Jason. Question answering with subgraph embed-  dings. In Proc. EMNLP, 2014a.  Bordes, Antoine, Weston, Jason, and Usunier, Nicolas. Open question answering with weakly su-  pervised embedding models. ECML-PKDD, 2014b.  Das, Sreerupa, Giles, C Lee, and Sun, Guo-Zheng. Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana University, 1992.  Fader, Anthony, Zettlemoyer, Luke, and Etzioni, Oren. Paraphrase-driven learning for open question  answering. In ACL, pp. 1608–1618, 2013.  10  Published as a conference paper at ICLR 2015  Graves, Alex.  Generating sequences with recurrent neural networks.  arXiv:1308.0850, 2013.  Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural turing machines.  arXiv:1410.5401, 2014.  Haykin, Simon. Neural networks: A comprehensive foundation. 1994.  arXiv preprint  arXiv preprint  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Iyyer, Mohit, Boyd-Graber, Jordan, Claudino, Leonardo, Socher, Richard, and III, Hal Daum´e. A neural network for factoid question answering over paragraphs. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pp. 633–644, 2014.  Kolomiyets, Oleksandr and Moens, Marie-Francine. A survey on question answering technology  from an information retrieval perspective. Information Sciences, 181(24):5412–5434, 2011.  Lin, Tsungnam, Horne, Bil G, Tiˇno, Peter, and Giles, C Lee. Learning long-term dependencies in narx recurrent neural networks. Neural Networks, IEEE Transactions on, 7(6):1329–1338, 1996.  Miikkulainen, Risto. {DISCERN}:{A} distributed artiﬁcial neural network model of script process-  ing and memory. 1990.  Mikolov, Tomas, Karaﬁ´at, Martin, Burget, Lukas, Cernock`y, Jan, and Khudanpur, Sanjeev. Recur-  rent neural network based language model. In Interspeech, pp. 1045–1048, 2010.  Richardson, Matthew, Burges, Christopher JC, and Renshaw, Erin. Mctest: A challenge dataset for  the open-domain machine comprehension of text. In EMNLP, pp. 193–203, 2013.  Schmidhuber, J¨urgen. Learning to control fast-weight memories: An alternative to dynamic recur-  rent networks. Neural Computation, 4(1):131–139, 1992.  Schmidhuber, J¨urgen. A self-referentialweight matrix. In ICANN93, pp. 446–450. Springer, 1993.  Tolkien, John Ronald Reuel. The Fellowship of the Ring. George Allen & Unwin, 1954.  Weston, Jason, Bengio, Samy, and Usunier, Nicolas. Wsabie: Scaling up to large vocabulary im- age annotation. In Proceedings of the Twenty-Second international joint conference on Artiﬁcial Intelligence-Volume Volume Three, pp. 2764–2770. AAAI Press, 2011.  Yih, Wen-Tau, He, Xiaodong, and Meek, Christopher. Semantic parsing for single-relation question answering. In Proceedings of ACL. Association for Computational Linguistics, June 2014. URL http://research.microsoft.com/apps/pubs/default.aspx?id=214353.  Zaremba, Wojciech and Sutskever, Ilya. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.  11  Published as a conference paper at ICLR 2015  A SIMULATION DATA GENERATION  Aim We have built a simple simulation which behaves much like a classic text adventure game. The idea is that generating text within this simulation allows us to ground the language used.  Some comments about our intent:  • Firstly, while this currently only encompasses a very small part of the kind of language and understanding we want a model to learn to move towards full language understanding, we believe it is a prerequisite that models should perform well on this kind of task for them to work on real-world environments.  • Secondly, our aim is to make this simulation more complex and to release improved ver- sions over time. Hopefully it can then scale up to evaluate more and more useful properties.  Currently, tasks within the simulation are restricted to question answering tasks about the location of people and objects. However, we envisage other tasks should be possible, including asking the learner to perform actions within the simulation (“Please pick up the milk”, “Please ﬁnd John and give him the milk”) and asking the learner to describe actions (”What did John just do?”).  Actions The underlying actions in the simulation consist of the following:  go <location>, get <object>, put <object1> in/on <object2>, drop <object>,  look,  inventory,  get <object1> from <object2>, give <object> to <actor>,  examine <object>.  There are a set of constraints on those actions. For example an actor cannot get something that they or someone else already has, they cannot go to a place they are already at, cannot drop something they do not already have, and so on.  Executing Actions and Asking Questions Using the underlying actions and their constraints, there is then a (hand-built) model that deﬁnes how actors act. Currently this is very simple: they try to make a random valid action, at the moment restricted to go or go, get and drop depending on the which of two types of experiments we are running: (i) actor; or (ii) actor + object.  If we write these actions down in text form this gives us a very simple “story” which is executable by the simulation, e.g., joe go kitchen; fred go kitchen; joe get milk; joe go ofﬁce; joe drop milk; joe go bathroom. This example corresponds to the story given in Figure 1. The system can then ask questions about the state of the simulation e.g., where milk?, where joe?, where joe before ofﬁce? It is easy to calculate the true answers for these questions as we have access to the underlying world. What remains is to convert both the statements and the questions to look more like natural language.  Simple Grammar For Generating Language In order to produce more natural looking text with lexical variety we built a simple automated grammar. Each verb is assigned a set of synonyms, e.g., the simulation command get is replaced with either picked up, got, grabbed or took, and drop is replace with either dropped, left, discarded or put down. Similarly, each object and actor can have a set of replacement synonyms as well, although currently there is no ambiguity there in our experiments, we simply add articles or not. We do add lexical variation to questions, e.g., “Where is John ?” or “Where is John now ?”.  Joining Statements Finally, for the word sequence training setting, we join the statements above into compound sentences. To do this we simply take the set of statements and then join them randomly with one of the following: “.”, “and”, “then”, “, then”, “;”, “, later”, “, after that”, “, and then”, or “, next”. Example output can be seen in Figure 2.  Issues There are a great many aspects of language not yet modeled. For example, currently coref- erence is not modeled (e.g., “He picked up the milk”) and similarly there are no compound noun phrases (“John and Fred went to the kitchen”). Some of these seem easy to add to the simulation. The hope is that adding these complexities will help evaluate models in a controlled way, within the simulated environment, which is hard to do with real data. Of course, this is not a substitute for real data which our models should be applied to as well, but does serve as a useful testbed.  12  Published as a conference paper at ICLR 2015  B WORD SEQUENCE TRAINING  For segmenting an input word stream as generated in Appendix A we use a segmenter of the form:  seg(c) = W ⊤  segUSΦseg(c)  where Wseg is a vector (effectively the parameters of a linear classiﬁer in embedding space). As we are already in the fully supervised setting, where for each question in the training set we are given the answer and the supporting facts from the input stream, we can also use that supervision for the segmenter as well. That is, for any known supporting fact, such as “Bill is in the Kitchen” for the question “Where is Bill?” we wish the segmenter to ﬁre for such a statement, but not for unﬁnished statements such as “Bill is in the”. We can thus write our training criterion for segmentation as the minimization of:  X  f ∈F  max(0, γ − seg(f )) + X  ¯f ∈ ¯F  max(0, γ + seg( ¯f ))  (12)  where F are all known supporting segments in the labeled training set, and ¯F are all other segments in the training set.  C WRITE TIME FEATURE TRAINING  The training procedure to take into account modeling write time is slightly different to that described in Section 3.1. Write time features are important so that the MemNN knows when each memory was written, and hence knows the ordering of statements that comprise a story or dialogue. Note that this is different to time information described in the text of a statement, such as the tense of a statement, or statements containing time expressions, e.g., “He went to the ofﬁce yesterday”. For such cases, write time features are not directly necessary, and they could (potentially) be modeled directly from the text.  As was described in Section 3.4 we add three write time features to the model and score triples using:  sOt (x, y, y′) = Φx(x)⊤UOt  ⊤UOt(cid:16)Φy(y) − Φy(y′) + Φt(x, y, y′)(cid:17).  (13)  If sO(x, y, y′) > 0 the model prefers y over y′, and if sO(x, y, y′) < 0 it prefers y′. The argmax of eq. (2) and (3) are replaced by a loop over memories i = 1, . . . , N , keeping the winning memory (y or y′) at each step, and always comparing the current winner to the next memory mi. That is, at inference time, for a k = 2 model the arg max functions of eq. (2) and (3) are replaced with o1 = Ot(x, m) and o2 = Ot([x, mo1 ], m) where Ot is deﬁned in Algorithm 1 below.  Algorithm 1 Ot replacement to arg max when using write time features  function Ot(q, m)  t ← 1 for i = 2, . . . , N do  if sOt (q, mi, mt) > 0 then  t ← i  end if  end for return t end function  Φt(x, y, y′) uses three new features which take on the value 0 or 1: whether x is older than y, x is older than y′, and y older than y′. When ﬁnding the second supporting memory (computing Ot([x, mo1], m)) we encode whether mo1 is older than y, mo1 is older than y′, and y older than y′ to capture the relative age of the ﬁrst supporting memory w.r.t. the second one in the ﬁrst two features. Note that when ﬁnding the ﬁrst supporting memory (i.e., for Ot(x, m)) the ﬁrst two features are useless as x is the last thing in the memory and hence y and y′ are always older.  13  Published as a conference paper at ICLR 2015  To train our model with write time features we need to replace the hinge loss in eqs. (6)-(7) with a loss that matches Algorithm 1. To do this, we instead minimize:  max(0, γ − sOt (x, mo1 , ¯f )) + P¯f 6=mo1 P¯f 6=mo1 max(0, γ − sOt ([x, mo1], mo2 , ¯f ′)) + P¯f ′6=mo2  P¯f ′6=mo2  max(0, γ + sOt (x, ¯f , mo1)) +  max(0, γ + sOt ([x, mo1], ¯f ′, mo2) +  max(0, γ − sR([x, mo1 , mo2], r) + sR([x, mo1 , mo2], ¯r]))  P¯r6=r  The last term is the same as in eq. (8) and is for the ﬁnal ranking of words to return a response, which remains unchanged (as usual, this can also be replaced by an RNN for a more sophisticated model). Terms 1-4 replace eqs. (6)-(7) by considering triples directly. For both mo1 and mo2 we need to have two terms considering them as the second or third argument to SOt as they may appear on either side during inference (via Algorithm 1). As before, at every step of SGD we sample ¯f , ¯f ′, ¯r rather than compute the whole sum for each training example.  D WORD-SEQUENCE LEARNING CURVE EXPERIMENTS  We computed the test accuracy of MemNNs k = 2 (+ time) for varying amounts of training data: 100, 500, 1000 and 3000 training questions. The results are given in Table 4. These results can be compared with RNNs and LSTMs on the full data (3000 examples) by comparing with Figure 3. For example, on the difﬁculty 5 actor and actor + object tasks MemNNs outperform LSTMs even using 30 times less training examples.  Table 4: Test accuracy of MemNNs k = 2 (+time) on the word-sequence simulation QA task for differing numbers of training examples (number of questions).  Num. training questions 100 500 1000 3000  Difﬁculty 1 actor  actor  + object 73.8% 64.9% 99.9% 99.2% 99.9% 100% 100% 100%  Difﬁculty 5  actor  actor  + object 74.4% 49.8% 99.8% 95.1% 100% 98.4% 100% 99.9%  E SENTENCE-LEVEL EXPERIMENTS  We conducted experiments where input was at the sentence-level, that is the data was already pre- segemented into statements and questions as input to the MemNN (as opposed to being input as a stream of words). Results comparing RNNs with MemNNs are given in Table 5. The conclusions are similar to those at the word level from Section 5.2. That is, MemNNs outperform RNNs, and that inference that ﬁnds k = 2 supporting statements and time features are necessary for the actor w/o before + object task.  Table 5: Test accuracy on the sentence-level simulation QA task.  Method RNN MemNN k = 1 MemNN k = 1 (+time) MemNN k = 2 (+time)  Difﬁculty 1  Difﬁculty 5  actor  w/o before  actor w/o before  + object  actor  w/o before  actor w/o before  + object  29% 46% 100% 100%  17% 21% 73% 99.4%  100% 90% 100% 100%  58% 9% 73%  99.95%  14  Published as a conference paper at ICLR 2015  F MULTI-WORD ANSWER SETTING EXPERIMENTS  We conducted experiments for the simulation data in the case where the answers are sentences (see Appendix A and Figure 2). As the single word answer model can no longer be used, we simply compare MemNNs using either RNNs or LSTMs for the response module R. As baselines we can still use RNNs and LSTMs in the standard setting of being fed words only including the statements and the question as a word stream. In contrast, the MemNN RNN and LSTMs are effectively fed the output of the O module (see Section 3.1). In these experiments we only consider the difﬁculty 5 actor+object setting in the case of MemNNs with k = 2 iterations (eq. (3)), which means the module R is fed the features [x, mo1 , mo2] after the modules I, G and O have run. The sentence generation is performed on the test data, and the evaluation we chose is as follows. A correct generation has to contain the correct location answer, and can optionally contain the subject or a correct pronoun referring to it. For example the question “Where is Bill?” allows the correct answers “Kitchen”, “In the kitchen”, “Bill is in the kitchen”, “He is in the kitchen” and “I think Bill is in the kitchen”. However incorrect answers contain an incorrect location or subject reference, for example “Joe is in the kitchen”, “It is in the kitchen” or “Bill is in the bathroom I believe”. We can then measure the percentage of text examples that are correct using this metric.  The numerical results are given in Table 6, and example output is given in Figure 2. The results indicate that MemNNs with LSTMs perform quite strongly, outperforming MemNNs using RNNs. However, both MemNN variant outperform both RNNs and LSTMs by some distance.  Table 6: Test accuracy on the multi-word answer simulation QA task. We compare conventional RNN and LSTMs with MemNNs using an RNN or LSTM module R (i.e., where R is fed features [x, mo1 , mo2] after the modules I, G and O have run).  Model MemNN: IGO features [x, mo1 , mo2] Word features RNN LSTM  68.83% 90.98%  13.97% 14.01%  15  ",
1412.6856,2015,Object detectors emerge in Deep Scene CNNs,"['Object detectors emerge in Deep Scene CNNs', 'Bolei Zhou', 'Aditya Khosla', 'Agata Lapedriza', 'Aude Oliva', 'and Antonio Torralba']",https://arxiv.org/pdf/1412.6856,"5 1 0 2    r p A 5 1         ]  V C . s c [      2 v 6 5 8 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS  Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba Computer Science and Artiﬁcial Intelligence Laboratory, MIT {bolei,khosla,agata,oliva,torralba}@mit.edu  ABSTRACT  With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classiﬁcation. As scenes are composed of objects, the CNN for scene classiﬁca- tion automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.  1  INTRODUCTION  Current deep neural networks achieve remarkable performance at a number of vision tasks surpass- ing techniques based on hand-crafted features. However, while the structure of the representation in hand-crafted features is often clear and interpretable, in the case of deep networks it remains unclear what the nature of the learned representation is and why it works so well. A convolutional neural network (CNN) trained on ImageNet (Deng et al., 2009) signiﬁcantly outperforms the best hand crafted features on the ImageNet challenge (Russakovsky et al., 2014). But more surprisingly, the same network, when used as a generic feature extractor, is also very successful at other tasks like object detection on the PASCAL VOC dataset (Everingham et al., 2010). A number of works have focused on understanding the representation learned by CNNs. The work by Zeiler & Fergus (2014) introduces a procedure to visualize what activates each unit. Recently Yosinski et al. (2014) use transfer learning to measure how generic/speciﬁc the learned features are. In Agrawal et al. (2014) and Szegedy et al. (2013), they suggest that the CNN for ImageNet learns a distributed code for objects. They all use ImageNet, an object-centric dataset, as a training set. When training a CNN to distinguish different object classes, it is unclear what the underlying repre- sentation should be. Objects have often been described using part-based representations where parts can be shared across objects, forming a distributed code. However, what those parts should be is unclear. For instance, one would think that the meaningful parts of a face are the mouth, the two eyes, and the nose. However, those are simply functional parts, with words associated with them; the object parts that are important for visual recognition might be different from these semantic parts, making it difﬁcult to evaluate how efﬁcient a representation is. In fact, the strong internal conﬁgu- ration of objects makes the deﬁnition of what is a useful part poorly constrained: an algorithm can ﬁnd different and arbitrary part conﬁgurations, all giving similar recognition performance. Learning to classify scenes (i.e., classifying an image as being an ofﬁce, a restaurant, a street, etc) using the Places dataset (Zhou et al., 2014) gives the opportunity to study the internal representation learned by a CNN on a task other than object recognition. In the case of scenes, the representation is clearer. Scene categories are deﬁned by the objects they contain and, to some extent, by the spatial conﬁguration of those objects. For instance, the important parts of a bedroom are the bed, a side table, a lamp, a cabinet, as well as the walls, ﬂoor and ceiling. Objects represent therefore a distributed code for scenes (i.e., object classes are shared across different scene categories). Importantly, in scenes, the spatial conﬁguration of objects,  1  Published as a conference paper at ICLR 2015  Table 1: The parameters of the network architecture used for ImageNet-CNN and Places-CNN. fc7 Layer Units 4096 Feature  conv2 256 27×27  conv3 384 13×13  conv5 256 13×13  conv1 55×55  pool1 27×27  pool2 256 13×13  conv4 384 13×13  pool5 256 6×6  fc6 4096  96  96  1  1  although compact, has a much larger degree of freedom. It is this loose spatial dependency that, we believe, makes scene representation different from most object classes (most object classes do not have a loose interaction between parts). In addition to objects, other feature regularities of scene categories allow for other representations to emerge, such as textures (Renninger & Malik, 2004), GIST (Oliva & Torralba, 2006), bag-of-words (Lazebnik et al., 2006), part-based models (Pandey & Lazebnik, 2011), and ObjectBank (Li et al., 2010). While a CNN has enough ﬂexibility to learn any of those representations, if meaningful objects emerge without supervision inside the inner layers of the CNN, there will be little ambiguity as to which type of representation these networks are learning. The main contribution of this paper is to show that object detection emerges inside a CNN trained to recognize scenes, even more than when trained with ImageNet. This is surprising because our results demonstrate that reliable object detectors are found even though, unlike ImageNet, no supervision is provided for objects. Although object discovery with deep neural networks has been shown before in an unsupervised setting (Le, 2013), here we ﬁnd that many more objects can be naturally discovered, in a supervised setting tuned to scene classiﬁcation rather than object classiﬁcation. Importantly, the emergence of object detectors inside the CNN suggests that a single network can support recognition at several levels of abstraction (e.g., edges, texture, objects, and scenes) without needing multiple outputs or a collection of networks. Whereas other works have shown that one can detect objects by applying the network multiple times in different locations (Girshick et al., 2014), or focusing attention (Tang et al., 2014), or by doing segmentation (Grangier et al., 2009; Farabet et al., 2013), here we show that the same network can do both object localization and scene recognition in a single forward-pass. Another set of recent works (Oquab et al., 2014; Bergamo et al., 2014) demonstrate the ability of deep networks trained on object classiﬁcation to do localization without bounding box supervision. However, unlike our work, these require object-level supervision while we only use scenes.  2  IMAGENET-CNN AND PLACES-CNN  Convolutional neural networks have recently obtained astonishing performance on object classiﬁ- cation (Krizhevsky et al., 2012) and scene classiﬁcation (Zhou et al., 2014). The ImageNet-CNN from Jia (2013) is trained on 1.3 million images from 1000 object categories of ImageNet (ILSVRC 2012) and achieves a top-1 accuracy of 57.4%. With the same network architecture, Places-CNN is trained on 2.4 million images from 205 scene categories of Places Database (Zhou et al., 2014), and achieves a top-1 accuracy of 50.0%. The network architecture used for both CNNs, as proposed in (Krizhevsky et al., 2012), is summarized in Table 11. Both networks are trained from scratch using only the speciﬁed dataset. The deep features from Places-CNN tend to perform better on scene-related recognition tasks com- pared to the features from ImageNet-CNN. For example, as compared to the Places-CNN that achieves 50.0% on scene classiﬁcation, the ImageNet-CNN combined with a linear SVM only achieves 40.8% on the same test set2 illustrating the importance of having scene-centric data. To further highlight the difference in representations, we conduct a simple experiment to identify the differences in the type of images preferred at the different layers of each network: we create a set of 200k images with an approximately equal distribution of scene-centric and object-centric images3, and run them through both networks, recording the activations at each layer. For each layer, we obtain the top 100 images that have the largest average activation (sum over all spatial locations for  1We use unit to refer to neurons in the various layers and features to refer to their activations. 2Scene recognition demo of Places-CNN is available at http://places.csail.mit.edu/demo. html. The demo has 77.3% top-5 recognition rate in the wild estimated from 968 anonymous user responses. 3100k object-centric images from the test set of ImageNet LSVRC2012 and 108k scene-centric images from  the SUN dataset (Xiao et al., 2014).  2  Published as a conference paper at ICLR 2015  Figure 1: Top 3 images producing the largest activation of units in each layer of ImageNet-CNN (top) and Places-CNN (bottom).  a given layer). Fig. 1 shows the top 3 images for each layer. We observe that the earlier layers such as pool1 and pool2 prefer similar images for both networks while the later layers tend to be more specialized to the speciﬁc task of scene or object categorization. For layer pool2, 55% and 47% of the top-100 images belong to the ImageNet dataset for ImageNet-CNN and Places-CNN. Starting from layer conv4, we observe a signiﬁcant difference in the number of top-100 belonging to each dataset corresponding to each network. For fc7, we observe that 78% and 24% of the top-100 images belong to the ImageNet dataset for the ImageNet-CNN and Places-CNN respectively, illustrating a clear bias in each network. In the following sections, we further investigate the differences between these networks, and focus on better understanding the nature of the representation learned by Places-CNN when doing scene classiﬁcation in order to clarify some part of the secret to their great performance.  3 UNCOVERING THE CNN REPRESENTATION  The performance of scene recognition using Places-CNN is quite impressive given the difﬁculty of the task. In this section, our goal is to understand the nature of the representation that the network is learning.  3.1 SIMPLIFYING THE INPUT IMAGES  Simplifying images is a well known strategy to test human recognition. For example, one can remove information from the image to test if it is diagnostic or not of a particular object or scene (for a review see Biederman (1995)). A similar procedure was also used by Tanaka (1993) to understand the receptive ﬁelds of complex cells in the inferior temporal cortex (IT). Inspired by these approaches, our idea is the following: given an image that is correctly classiﬁed by the network, we want to simplify this image such that it keeps as little visual information as possible while still having a high classiﬁcation score for the same category. This simpliﬁed image (named minimal image representation) will allow us to highlight the elements that lead to the high classiﬁcation score. In order to do this, we manipulate images in the gradient space as typically done in computer graphics (P´erez et al., 2003). We investigate two different approaches described below. In the ﬁrst approach, given an image, we create a segmentation of edges and regions and remove segments from the image iteratively. At each iteration we remove the segment that produces the smallest decrease of the correct classiﬁcation score and we do this until the image is incorrectly classiﬁed. At the end, we get a representation of the original image that contains, approximately, the minimal amount of information needed by the network to correctly recognize the scene category. In Fig. 2 we show some examples of these minimal image representations. Notice that objects seem to contribute important information for the network to recognize the scene. For instance, in the case of bedrooms these minimal image representations usually contain the region of the bed, or in the art gallery category, the regions of the paintings on the walls. Based on the previous results, we hypothesized that for the Places-CNN, some objects were crucial for recognizing scenes. This inspired our second approach: we generate the minimal image repre- sentations using the fully annotated image set of SUN Database (Xiao et al., 2014) (see section 4.1 for details on this dataset) instead of performing automatic segmentation. We follow the same pro- cedure as the ﬁrst approach using the ground-truth object segments provided in the database. This led to some interesting observations: for bedrooms, the minimal representations retained the bed in 87% of the cases. Other objects kept in bedrooms were wall (28%) and window (21%).  3  ImageNet-CNNPlaces-CNNpool1pool2pool5fc7conv4conv3Published as a conference paper at ICLR 2015  Figure 2: Each pair of images shows the original image (left) and a simpliﬁed image (right) that gets classiﬁed by the Places-CNN as the same scene category as the original image. From top to bottom, the four rows show different scene categories: bedroom, auditorium, art gallery, and dining room.  Figure 3: The pipeline for estimating the RF of each unit. Each sliding-window stimuli contains a small randomized patch (example indicated by red arrow) at different spatial locations. By compar- ing the activation response of the sliding-window stimuli with the activation response of the original image, we obtain a discrepancy map for each image (middle top). By summing up the calibrated discrepancy maps (middle bottom) for the top ranked images, we obtain the actual RF of that unit (right).  For art gallery the minimal image representations contained paintings (81%) and pictures (58%); in amusement parks, carousel (75%), ride (64%), and roller coaster (50%); in bookstore, bookcase (96%), books (68%), and shelves (67%). These results suggest that object detection is an impor- tant part of the representation built by the network to obtain discriminative information for scene classiﬁcation.  3.2 VISUALIZING THE RECEPTIVE FIELDS OF UNITS AND THEIR ACTIVATION PATTERNS  In this section, we investigate the shape and size of the receptive ﬁelds (RFs) of the various units in the CNNs. While theoretical RF sizes can be computed given the network architecture (Long et al., 2014), we are interested in the actual, or empirical size of the RFs. We expect the empirical RFs to be better localized and more representative of the information they capture than the theoretical ones, allowing us to better understand what is learned by each unit of the CNN. Thus, we propose a data-driven approach to estimate the learned RF of each unit in each layer. It is simpler than the deconvolutional network visualization method (Zeiler & Fergus, 2014) and can be easily extended to visualize any learned CNNs4. The procedure for estimating a given unit’s RF, as illustrated in Fig. 3, is as follows. As input, we use an image set of 200k images with a roughly equal distribution of scenes and objects (similar to Sec. 2). Then, we select the top K images with the highest activations for the given unit.  4More visualizations are available at http://places.csail.mit.edu/visualization  4  receptive ﬁeld sliding-window stimuli calibrated discrepancy maps discrepancy maps for top 10 images  Published as a conference paper at ICLR 2015  Figure 4: The RFs of 3 units of pool1, pool2, conv4, and pool5 layers respectively for ImageNet- and Places-CNNs, along with the image patches corresponding to the top activation regions inside the RFs.  Table 2: Comparison of the theoretical and empirical sizes of the RFs for Places-CNN and ImageNet-CNN at different layers. Note that the RFs are assumed to be square shaped, and the sizes reported below are the length of each side of this square, in pixels.  Theoretic size Places-CNN actual size ImageNet-CNN actual size  19  pool1 17.8± 1.6 17.9± 1.6  67  pool2 37.4± 5.9 36.7± 5.4  conv3  99  52.1±10.6 51.1±9.9  conv4 131  60.0± 13.7 60.4± 16.0  pool5 195  72.0± 20.0 70.3± 21.6  For each of the K images, we now want to identify exactly which regions of the image lead to the high unit activations. To do this, we replicate each image many times with small random occluders (image patches of size 11×11) at different locations in the image. Speciﬁcally, we generate occlud- ers in a dense grid with a stride of 3. This results in about 5000 occluded images per original image. We now feed all the occluded images into the same network and record the change in activation as compared to using the original image. If there is a large discrepancy, we know that the given patch is important and vice versa. This allows us to build a discrepancy map for each image. Finally, to consolidate the information from the K images, we center the discrepancy map around the spatial location of the unit that caused the maximum activation for the given image. Then we average the re-centered discrepancy maps to generate the ﬁnal RF. In Fig. 4 we visualize the RFs for units from 4 different layers of the Places-CNN and ImageNet- CNN, along with their highest scoring activation regions inside the RF. We observe that, as the layers go deeper, the RF size gradually increases and the activation regions become more semantically meaningful. Further, as shown in Fig. 5, we use the RFs to segment images using the feature maps of different units. Lastly, in Table 2, we compare the theoretical and empirical size of the RFs at different layers. As expected, the actual size of the RF is much smaller than the theoretical size, especially in the later layers. Overall, this analysis allows us to better understand each unit by focusing precisely on the important regions of each image.  Figure 5: Segmentation based on RFs. Each row shows the 4 most conﬁdent images for some unit.  5  (cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:1)(cid:2)(cid:2)(cid:3)(cid:5)(cid:1)(cid:2)(cid:2)(cid:3)(cid:6)(cid:7)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)(cid:3)(cid:12)(cid:7)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:17)(cid:18)(cid:19)(cid:12)(cid:20)(cid:13)(cid:17)(cid:13)(cid:21)(cid:15)(cid:16)(cid:17)(cid:17)pool1Places-CNNpool2conv4pool5ImageNet-CNNPublished as a conference paper at ICLR 2015  Figure 6: AMT interface for unit concept annotation. There are three tasks in each annotation.  3.3  IDENTIFYING THE SEMANTICS OF INTERNAL UNITS  In Section 3.2, we found the exact RFs of units and observed that activation regions tended to become more semantically meaningful with increasing depth of layers. In this section, our goal is to understand and quantify the precise semantics learned by each unit. In order to do this, we ask workers on Amazon Mechanical Turk (AMT) to identify the common theme or concept that exists between the top scoring segmentations for each unit. We expect the tags provided by naive annotators to reduce biases. Workers provide tags without being constrained to a dictionary of terms that could bias or limit the identiﬁcation of interesting properties. Speciﬁcally, we divide the task into three main steps as shown in Fig. 6. We show workers the top 60 segmented images that most strongly activate one unit and we ask them to (1) identify the concept, or semantic theme given by the set of 60 images e.g., car, blue, vertical lines, etc, (2) mark the set of images that do not fall into this theme, and (3) categorize the concept provided in (1) to one of 6 semantic groups ranging from low-level to high-level: simple elements and colors (e.g., horizontal lines, blue), materials and textures (e.g., wood, square grid), regions ans surfaces (e.g., road, grass), object parts (e.g., head, leg), objects (e.g., car, person), and scenes (e.g., kitchen, corridor). This allows us to obtain both the semantic information for each unit, as well as the level of abstraction provided by the labeled concept. To ensure high quality of annotation, we included 3 images with high negative scores that the work- ers were required to identify as negatives in order to submit the task. Fig. 7 shows some example annotations by workers. For each unit, we measure its precision as the percentage of images that were selected as ﬁtting the labeled concept. In Fig. 8.(a) we plot the average precision for ImageNet- CNN and Places-CNN for each layer. In Fig. 8.(b-c) we plot the distribution of concept categories for ImageNet-CNN and Places-CNN at each layer. For this plot we consider only units that had a precision above 75% as provided by the AMT workers. Around 60% of the units on each layer where above that threshold. For both networks, units at the early layers (pool1, pool2) have more units responsive to simple elements and colors, while those at later layers (conv4, pool5) have more high-level semantics (responsive more to objects and scenes). Furthermore, we observe that conv4 and pool5 units in Places-CNN have higher ratios of high-level semantics as compared to the units in ImageNet-CNN. Fig. 9 provides a different visualization of the same data as in Fig. 8.(b-c). This plot better reveals how different levels of abstraction emerge in different layers of both networks. The vertical axis indicates the percentage of units in each layer assigned to each concept category. ImageNet-CNN has more units tuned to simple elements and colors than Places-CNN while Places-CNN has more objects and scenes. ImageNet-CNN has more units tuned to object parts (with the maximum around conv4). It is interesting to note that Places-CNN discovers more objects than ImageNet-CNN despite having no object-level supervision.  6  Published as a conference paper at ICLR 2015  Figure 7: Examples of unit annotations provided by AMT workers for 6 units from pool5 in Places- CNN. For each unit the ﬁgure shows the label provided by the worker, the type of label, the images selected as corresponding to the concept (green box) and the images marked as incorrect (red box). The precision is the percentage of correct images. The top three units have high performance while the bottom three have low performance (< 75%).  7  Pool5, unit 77; Label:legs; Type: object part; Precision: 96%Pool5, unit 76; Label: ocean; Type: scene; Precision: 93%Pool5, unit 13; Label: Lamps; Type: object; Precision: 84%Pool5, unit 22; Label: dinner table; Type: scene; Precision: 60%Pool5, unit 112; Label: pool table; Type: object; Precision: 70%Pool5, unit 168; Label: shrubs; Type: object; Precision: 54%Published as a conference paper at ICLR 2015  Figure 8: (a) Average precision of all the units in each layer for both networks as reported by AMT workers. (b) and (c) show the number of units providing different levels of semantics for ImageNet- CNN and Places-CNN respectively.  Figure 9: Distribution of semantic types found for all the units in both networks. From left to right, each plot corresponds to the distribution of units in each layer assigned to simple elements or colors, textures or materials, regions or surfaces, object parts, objects, and scenes. The vertical axis is the percentage of units with each layer assigned to each type of concept.  4 EMERGENCE OF OBJECTS AS THE INTERNAL REPRESENTATION  As shown before, a large number of units in pool5 are devoted to detecting objects and scene- regions (Fig. 9). But what categories are found? Is each category mapped to a single unit or are there multiple units for each object class? Can we actually use this information to segment a scene?  4.1 WHAT OBJECT CLASSES EMERGE?  To answer the question of why certain objects emerge from pool5, we tested ImageNet-CNN and Places-CNN on fully annotated images from the SUN database (Xiao et al., 2014). The SUN database contains 8220 fully annotated images from the same 205 place categories used to train Places-CNN. There are no duplicate images between SUN and Places. We use SUN instead of COCO (Lin et al., 2014) as we need dense object annotations to study what the most informative object classes for scene categorization are, and what the natural object frequencies in scene images are. For this study, we manually mapped the tags given by AMT workers to the SUN categories. Fig. 10(a) shows the distribution of objects found in pool5 of Places-CNN. Some objects are detected by several units. For instance, there are 15 units that detect buildings. Fig. 11 shows some units from the Places-CNN grouped by the type of object class they seem to be detecting. Each row shows the top ﬁve images for a particular unit that produce the strongest activations. The segmentation shows the regions of the image for which the unit is above a certain threshold. Each unit seems to be selective to a particular appearance of the object. For instance, there are 6 units that detect lamps, each unit detecting a particular type of lamp providing ﬁner-grained discrimination; there are 9 units selective to people, each one tuned to different scales or people doing different tasks.  8  pool1pool2conv3conv4pool5020406080100Places−CNNpool1pool2conv3conv4pool5020406080100ImageNet−CNN  simple elements & colortexture materialsregion surfaceobject partobjectscenepool1pool2conv3conv4pool550556065707580Average precision per layer  Places−CNNImageNet−CNNNumber of units (p>75%)Number of units (p>75%)a)c)b)01020304050percent units (perf>75%)0246810024681012024681012051015200246810places-CNNimagenet-CNNpool1pool2conv4pool5conv3Simple elements & colorsTexture materialsObject partRegion or surfaceObjectScenepool1pool2conv4pool5conv3pool1pool2conv4pool5conv3pool1pool2conv4pool5conv3pool1pool2conv4pool5conv3pool1pool2conv4pool5conv3Published as a conference paper at ICLR 2015  Figure 10: Object counts of CNN units discovering each object class for (a) Places-CNN and (b) ImageNet-CNN.  Fig. 10(b) shows the distribition of objects found in pool5 of ImageNet-CNN. ImageNet has an abundance of animals among the categories present: in the ImageNet-CNN, out of the 256 units in pool5, there are 15 units devoted to detecting dogs and several more detecting parts of dogs (body, legs, ...). The categories found in pool5 tend to follow the target categories in ImageNet. Why do those objects emerge? One possibility is that the objects that emerge in pool5 correspond to the most frequent ones in the database. Fig. 12(a) shows the sorted distribution of object counts in the SUN database which follows Zipf’s law. Fig. 12(b) shows the counts of units found in pool5 for each object class (same sorting as in Fig. 12(a)). The correlation between object frequency in the database and object frequency discovered by the units in pool5 is 0.54. Another possibility is that the objects that emerge are the objects that allow discriminating among scene categories. To measure the set of discriminant objects we used the ground truth in the SUN database to measure the classiﬁcation performance achieved by each object class for scene classiﬁcation. Then we count how many times each object class appears as the most informative one. This measures the number of scene categories a particular object class is the most useful for. The counts are shown in Fig. 12(c). Note the similarity between Fig. 12(b) and Fig. 12(c). The correlation is 0.84 indicating that the network is automatically identifying the most discriminative object categories to a large extent. Note that there are 115 units in pool5 of Places-CNN not detecting objects. This could be due to incomplete learning or a complementary texture-based or part-based representation of the scenes. Therefore, although objects seem to be a key part of the representation learned by the network, we cannot rule out other representations being used in combination with objects.  4.2 OBJECT LOCALIZATION WITHIN THE INNER LAYERS  Places-CNN is trained to do scene classiﬁcation using the output of the ﬁnal layer of logistic regres- sion and achieves state-of-the-art performance. From our analysis above, many of the units in the inner layers could perform interpretable object localization. Thus we could use this single Places- CNN with the annotation of units to do both scene recognition and object localization in a single forward-pass. Fig. 13 shows an example of the output of different layers of the Places-CNN using the tags provided by AMT workers. Bounding boxes are shown around the areas where each unit is activated within its RF above a certain threshold. In Fig. 14 we provide the segmentation performance of the objects discovered in pool5 using the SUN database. The performance of many units is very high which provides strong evidence that they are indeed detecting those object classes despite being trained for scene classiﬁcation.  5 CONCLUSION  We ﬁnd that object detectors emerge as a result of learning to classify scene categories, showing that a single network can support recognition at several levels of abstraction (e.g., edges, textures, objects, and scenes) without needing multiple outputs or networks. While it is common to train a network to do several tasks and to use the ﬁnal layer as the output, here we show that reliable outputs can be extracted at each layer. As objects are the parts that compose a scene, detectors tuned to the objects that are discriminant between scenes are learned in the inner layers of the network. Note  9  051015051015building    tree    grass    floor    mountain    person    plant    water    window    ceiling lamp    pitch    road    arcade    bridge    cabinet    chair    food    lighthouse    path    sky    tower    wall    water tower    bed    bookcase    car    ceiling    cementery    column    desk    desk lamp    field    grandstand    ground    iceberg    phone booth    railing    river    rocks    sand    screen    sea    seats    showcase    snowy ground    street    swimming pool    tent    text    wardrobe    waterfall    windmilldog    bird    person    wheel    animal body    flower    ground    head    legs    animal face    animal head    building    car    cat    ceiling    face    human face    leg    monkey    plant    plants    pot    road    sea    tower    tree    water    windowCountsCountsa)b)Published as a conference paper at ICLR 2015  Figure 11: Segmentations using pool5 units from Places-CNN. Many classes are encoded by several units covering different object appearances. Each row shows the 5 most conﬁdent images for each unit. The number represents the unit number in pool5.  that only informative objects for speciﬁc scene recognition tasks will emerge. Future work should explore which other tasks would allow for other object classes to be learned without the explicit supervision of object labels.  ACKNOWLEDGMENTS  This work is supported by the National Science Foundation under Grant No. 1016862 to A.O, ONR MURI N000141010933 to A.T, as well as MIT Big Data Initiative at CSAIL, Google and Xerox Awards, a hardware donation from NVIDIA Corporation, to A.O and A.T.  REFERENCES Agrawal, Pulkit, Girshick, Ross, and Malik, Jitendra. Analyzing the performance of multilayer  neural networks for object recognition. ECCV, 2014.  Bergamo, Alessandro, Bazzani, Loris, Anguelov, Dragomir, and Torresani, Lorenzo. Self-taught  object localization with deep networks. arXiv preprint arXiv:1409.3964, 2014.  Biederman, Irving. Visual object recognition, volume 2. MIT press Cambridge, 1995.  Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale  hierarchical image database. In CVPR, 2009.  Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. The pascal visual  object classes challenge. IJCV, 2010.  Farabet, Clement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical  features for scene labeling. TPAMI, 2013.  10  120) arcade116) bed18) billard table155) bookcase8) bridge56) buildingBuildingsOutdoor objectsNaturePeopleIndoor objectsFurniture119) building123) building38) cabinet87) car55) ceiling lamp174) ceiling lampLighting223) ceiling lamp145) cementery85) chair13) desk lamp182) foodScenes9) lighthouse195) grass89) iceberg140) mountain3) person49) person100) person138) person46) painting159) sand61) road106) screen53) staircase127) street96) swimming pool107) wardrobe28) water tower6) windmill218) pitchPublished as a conference paper at ICLR 2015  Figure 12: (a) Object frequency in SUN (only top 50 objects shown), (b) Counts of objects discov- ered by pool5 in Places-CNN. (c) Frequency of most informative objects for scene classiﬁcation.  Figure 13: Interpretation of a picture by different layers of the Places-CNN using the tags provided by AMT workers. The ﬁrst shows the ﬁnal layer output of Places-CNN. The other three show detection results along with the conﬁdence based on the units’ activation and the semantic tags.  Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object  detection and semantic segmentation. 2014.  Grangier, D., Bottou, L., and Collobert, R. Deep convolutional networks for scene parsing. TPAMI,  2009.  Jia, Yangqing. Caffe: An open source convolutional architecture for fast feature embedding, 2013.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, 2012.  Lazebnik, Svetlana, Schmid, Cordelia, and Ponce, Jean. Beyond bags of features: Spatial pyramid  matching for recognizing natural scene categories. In CVPR, 2006.  Le, Quoc V. Building high-level features using large scale unsupervised learning. In ICASSP, 2013.  11  Object counts in SUN050001000015000Object counts of most informative objects for scene recognitionCounts of CNN units discovering each object class.b)c)a)051015200102030    wall    window    chair    building    floor    tree    ceiling lamp    cabinet    ceiling    person    plant    cushion    sky    picture    curtain    painting    door    desk lamp    side table    table    bed    books    pillow    mountain    car    pot    armchair    box    vase    flowers    road    grass    bottle    shoes    sofa    outlet    worktop    sign    book    sconce    plate    mirror    column    rug    basket    ground    desk    coffee table    clock    shelves    wall    window    chair    building    floor    tree    ceiling lamp    cabinet    ceiling    person    plant    cushion    sky    picture    curtain    painting    door    desk lamp    side table    table    bed    books    pillow    mountain    car    pot    armchair    box    vase    flowers    road    grass    bottle    shoes    sofa    outlet    worktop    sign    book    sconce    plate    mirror    column    rug    basket    ground    desk    coffee table    clock    shelves    wall    window    chair    building    floor    tree    ceiling lamp    cabinet    ceiling    person    plant    cushion    sky    picture    curtain    painting    door    desk lamp    side table    table    bed    books    pillow    mountain    car    pot    armchair    box    vase    flowers    road    grass    bottle    shoes    sofa    outlet    worktop    sign    book    sconce    plate    mirror    column    rug    basket    ground    desk    coffee table    clock    shelvesPublished as a conference paper at ICLR 2015  Figure 14: (a) Segmentation of images from the SUN database using pool5 of Places-CNN (J = Jaccard segmentation index, AP = average precision-recall.) (b) Precision-recall curves for some discovered objects. (c) Histogram of AP for all discovered object classes.  Li, Li-Jia, Su, Hao, Fei-Fei, Li, and Xing, Eric P. Object bank: A high-level image representation  for scene classiﬁcation & semantic feature sparsiﬁcation. In NIPS, pp. 1378–1386, 2010.  Lin, Tg-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Dollr, Piotr, and Zitnick, C. Lawrence. Microsoft COCO: Common objects in context. In ECCV, 2014. Long, Jonathan, Zhang, Ning, and Darrell, Trevor. Do convnets learn correspondence? In NIPS,  2014.  Oliva, A. and Torralba, A. Building the gist of a scene: The role of global image features in recog-  nition. Progress in Brain Research, 2006.  Oquab, Maxime, Bottou, L´eon, Laptev, Ivan, Sivic, Josef, et al. Weakly supervised object recogni-  tion with convolutional neural networks. In NIPS. 2014.  Pandey, M. and Lazebnik, S. Scene recognition and weakly supervised object localization with  deformable part-based models. In ICCV, 2011.  P´erez, Patrick, Gangnet, Michel, and Blake, Andrew. Poisson image editing. ACM Trans. Graph.,  2003.  Renninger, Laura Walker and Malik, Jitendra. When is scene identiﬁcation just texture recognition?  Vision research, 44(19):2301–2311, 2004.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge, 2014.  Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian, and Fergus, Rob. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.  Tanaka, Keiji. Neuronal mechanisms of object recognition. Science, 262(5134):685–688, 1993. Tang, Yichuan, Srivastava, Nitish, and Salakhutdinov, Ruslan R. Learning generative models with  visual attention. In NIPS. 2014.  Xiao, J, Ehinger, K A., Hays, J, Torralba, A, and Oliva, A. SUN database: Exploring a large  collection of scene categories. IJCV, 2014.  Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod. How transferable are features in  deep neural networks? In NIPS, 2014.  Zeiler, M. and Fergus, R. Visualizing and understanding convolutional networks. In ECCV, 2014. Zhou, Bolei, Lapedriza, Agata, Xiao, Jianxiong, Torralba, Antonio, and Oliva, Aude. Learning deep  features for scene recognition using places database. In NIPS, 2014.  12  Fireplace (J=5.3%, AP=22.9%)Wardrobe (J=4.2%, AP=12.7%)Billiard table (J=3.2%, AP=42.6%)Bed (J=24.6%, AP=81.1%)Mountain (J=11.3%, AP=47.6%)Sofa (J=10.8%, AP=36.2%)Building (J=14.6%, AP=47.2%)Washing machine (J=3.2%, AP=34.4%)0246810120102030405060708090100102030405060708090100SofaDesk lampSwimming poolBedCarPrecisionRecallCountsAverage precision (AP)0102030405060708090100a)b)c)",
1412.6544,2015,Qualitatively characterizing neural network optimization problems,"['Qualitatively characterizing neural network optimization problems', 'Ian Goodfellow and Oriol Vinyals']",https://arxiv.org/pdf/1412.6544,"5 1 0 2     y a M 1 2         ] E N . s c [      6 v 4 4 5 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  QUALITATIVELY CHARACTERIZING NEURAL NETWORK OPTIMIZATION PROBLEMS  Ian J. Goodfellow∗ & Oriol Vinyals∗ & Andrew M. Saxe∗∗ ∗Google Inc., Mountain View, CA ∗∗Department of Electrical Engineering, Stanford University, Stanford, CA {goodfellow,vinyals}@google.com, asaxe@stanford.edu  ABSTRACT  Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difﬁcult, with fear of local minima and other obstacles motivating a variety of schemes to improve opti- mization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct train- ing with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We ﬁnd that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any signiﬁcant obstacles.  1  INTRODUCTION  Neural networks are generally regarded as difﬁcult to optimize. The objective functions we must optimize in order to train them are non-convex and there are not many theoretical guarantees about the performance of the most popular algorithms on these problems. Nevertheless, neural networks are commonly trained successfully and obtain state of the art results on many tasks. In this paper, we present a variety of simple experiments designed to roughly characterize the objec- tive functions involved in neural network training. These experiments are not intended to measure any speciﬁc quantitative property of the objective function, but rather to answer some simple qual- itative questions. Do neural networks enter and escape a series of local minima? Do they move at varying speed as they approach and then pass a variety of saddle points? Answering these questions deﬁnitively is difﬁcult, but we present evidence strongly suggesting that the answer to all of these questions is no. We show that there exists a linear subspace in which neural network training could proceed by descending a single smooth slope with no barriers. Early symmetry breaking is the most conspicuous example of non-convexity. One important question is what happens after SGD leaves this well-behaved linear subspace. The main text of this article is restricted to experiments that were peer-reviewed prior to ICLR 2014, but the appendix presents additional experiments added after the review process ended. These experi- ments show that in some cases SGD does encounter obstacles, such as a ravine that shapes its path, but we never found evidence that local minima or saddle points slowed the SGD trajectory. This suggests that less exotic problems such as poor conditioning and variance in the gradient estimate are the primary difﬁculties in training neural networks. In all cases, we examine the total cost function (added up across all training examples). SGD of course only ever acts on unbiased stochastic approximations to this loss function. The structure of these stochastic approximations could be different from the global loss functions that we examine here, so it remains possible that neural networks are difﬁcult to train due to exotic structures in individual terms of the total cost function, or due to the noise induced by sampling minibatches of these terms. The results of our linear subspace experiments were qualitatively the same for all seven models we examined, which were drawn from a variety of categories, including fully-connected supervised feed-forward networks (Rumelhart et al., 1986) with a variety of activation functions, supervised  1  Published as a conference paper at ICLR 2015  convolutional networks (LeCun et al., 2010), unsupervised models, recurrent models of sequences, and analytically tractable factored linear models. (The additional experiments in the appendix found some qualitatively unique behavior outside this linear subspace for two of our models, but the re- mainder have the same qualitative behavior as factored linear models) Our models were all chosen because they performed well on competitive benchmark tasks. More research is needed to determine whether one should interpret our results as implying that SGD never encounters exotic obstacles when training neural networks, or as implying that SGD only works well when it does not encounter these structures.  2 LINEAR PATH EXPERIMENTS  Training a neural network consists of ﬁnding the optimal set of parameters θ. These are initialized to some set of small, random, initial parameters θ = θi. We then train using stochastic gradient de- scent (usually with extra features such as momentum) to minimize J(θ) until reaching convergence (usually some early stopping criterion). At the end of training, θ = θf . The trajectory that SGD follows from θi to θf is complicated and high-dimensional. It is difﬁcult to summarize such a trajectory meaningfully in a two-dimensional visualization. Simple learning curves showing the value of the objective function over time do not convey some fairly simple information. For example, when a learning curve bounces up and down repeatedly, we do not know whether the objective function is highly bumpy or whether SGD is rapidly changing direction due to noise in the stochastic, minibatch-based, estimate of the gradient. When the objective function remains constant for long periods of time, we do not know whether the parameters are stuck in a ﬂat region, oscillating around a local minimum, or tracing their way around the perimeter of a large obstacle. In this paper, we introduce a simple technique for qualitatively analyzing objective functions. We simply evaluate J(θ) at a series of points θ = (1− α)θ0 + αθ1 for varying values of α. This sweeps out a line in parameter space. We can see whether the cross-section of the objective function along this line is well-behaved. When we set θ0 = θi and θ1 = θf , we ﬁnd that the objective function has a simple, approximately convex shape along this cross-section. In other words, if we knew the correct direction, a single coarse line search could do a good job of training a neural network. These results are consistent with recent empirical and theoretical work arguing that local minima are not a signiﬁcant problem for training large neural networks (Saxe et al., 2013; Dauphin et al., 2014; Choromanska et al., 2014).  3 FEED-FORWARD FULLY CONNECTED NETWORKS  We begin our investigation with the simplest kind of neural network, the deterministic, feed-forward, fully-connected supervised network. For these experiments we use the MNIST dataset (LeCun et al., 1998). When not using dataset augmentation, the best result in this category is a maxout network (Goodfellow et al., 2013c) regularized with dropout (Srivastava et al., 2014) and adversarial training (Goodfellow et al., 2014), and trained using SGD with momentum. See the appendix of this paper for a full speciﬁcation of the architecture and training algorithm for this and all subsequent experiments. This conﬁguration results in an average of 78.2 mistakes on the MNIST test set, out of 10,000 examples total. Without adversarial training, the model also performs very well, with only 94 mistakes. Running the linear interpolation experiment on this problem, we ﬁnd in Fig. 1 that the 1-D subspace spanning the initial parameters and ﬁnal parameters is very well-behaved, and that SGD spends most of its time exploring the ﬂat region at the bottom of the valley. Maxout units do not saturate (they can saturate with respect to their input, but not with respect to their parameters), so perhaps it should not be too surprising that optimization is simple in this case. To determine whether the hard zero saturation of ReLUs (Jarrett et al., 2009; Glorot et al., 2011) or the two-sided saturation of logistic sigmoids can induce additional difﬁculties, we ran the linear interpolation experiment for both of these activation functions. The results are presented in Fig. 2 and Fig. 3. Again, we ﬁnd that the 1-D subspace spanning the initial and ﬁnal parameters contains no difﬁcult, exotic structures.  2  Published as a conference paper at ICLR 2015  Figure 1: Experiments with maxout on MNIST. Top row) The state of the art model, with adversarial training. Bottom row) The previous best maxout network, without adversarial training. Left column) The linear interpolation experiment. This experiment shows that the objective function is fairly smooth within the 1-D subspace spanning the initial and ﬁnal parameters of the model. Apart from the ﬂattening near α = 0, it appears nearly convex in this subspace. If we chose the initial direction correctly, we could solve the problem with a coarse line search. Right column) The progress of the actual SGD algorithm over time. The vast majority of learning happens in the ﬁrst few epochs. Thereafter, the algorithm struggles to make progress.  Figure 2: The linear interpolation curves for fully connected networks with different activation functions. Left) Sigmoid activation function. Right) ReLU activation function.  3  0.00.51.01.52.0α0.00.51.01.52.02.5J(θ)Linear interpolation of adversarially trained maxout on MNISTJ(θ) trainJ(θ) validation0100200300400500600time (epochs)0.51.01.52.02.5J(θ)SGD training of adversarial maxout on MNISTJ(θ) trainJ(θ) validation0.00.51.01.52.0α0.00.51.01.52.02.5J(θ)Linear interpolation of maxout on MNISTJ(θ) trainJ(θ) validation050100150200time (epochs)0.00.51.01.52.0J(θ)SGD training of maxout on MNISTJ(θ) trainJ(θ) validation0.00.51.01.52.0α0.00.51.01.52.02.53.0J(θ)Linear interpolation of sigmoids on MNISTJ(θ) trainJ(θ) validation0.00.51.01.52.0α0.00.51.01.52.02.5J(θ)Linear interpolation of ReLUs on MNISTJ(θ) trainJ(θ) validationPublished as a conference paper at ICLR 2015  Figure 3: The linear interpolation experiment for maxout, ReLUs, and sigmoids on MNIST, all plotted on the same axis for comparison. For this plot, we put the y axis in log scale, otherwise differences at the bottom of the curve are difﬁcult to see.  (a)  (c)  (b)  (d)  Figure 4: Higher resolution linear interpolation experiments. a) Tiling the interval [0, 1] with 200 values of α. b) A zoomed-in view of the same plot. c) Tiling the interval [0, .01] with 200 values of α, to see whether the initial symmetry breaking causes difﬁcult structures. d) Tiling the interval [.99, 1.] with 200 values of α, to see if the behavior of the objective function is more exotic in regions where the parameters encode fully learned intermediate concepts. We do not show the validation set objective here because it is too widely separated from the training set objective and would require zooming out the plot too far.  One possible objection to these results is that we have explored α with too coarse of a resolution to expose local non-convex structures. We therefore ran a variety of higher-resolution experiments, presented in Fig. 4. For these experiments, we did not use dropout, because the resolution we use here is high enough to expose artifacts induced by the Monte Carlo approximation to the true dropout loss function, which involves a sum over all (exponentially many) dropout masks. Maxout tends to overﬁt on MNIST if used without dropout, so we used ReLUs for these experiments. We found that increased resolution did not expose any small, difﬁcult structures.  4  0.00.51.01.52.0α10-210-1100101J(θ)Comparison of activation functions on MNISTJ(θ) train (maxout)J(θ) train (ReLU)J(θ) train (sigmoid)0.00.20.40.60.81.0α0.00.51.01.52.02.5J(θ)High resolution ReLU interpolation on MNISTJ(θ) trainJ(θ) validation0.50.60.70.80.91.0α0.000.050.100.150.200.250.300.35J(θ)High resolution ReLU interpolation on MNISTJ(θ) trainJ(θ) validation0.0000.0020.0040.0060.0080.010α2.2942.2962.2982.3002.3022.304J(θ)Initial portion of high resolution ReLU curveJ(θ) trainJ(θ) validation0.9900.9920.9940.9960.9981.000α0.001840.001860.001880.001900.00192J(θ)Final portion of high resolution ReLU curveJ(θ) trainPublished as a conference paper at ICLR 2015  Figure 5: Here we use linear interpolation to search for local minima. Left) By interpolating between two different SGD solutions, we show that each solution is a different local minimum within this 1-D subspace. Right) If we interpolate between a random point in space and an SGD solution, we ﬁnd no local minima besides the SGD solution, suggesting that local minima are rare.  Figure 6: The linear interpolation experiment for a convolutional maxout network on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009). Left) At a global scale, the curve looks very well-behaved. Right) Zoomed in near the initial point, we see there is a shallow barrier that SGD must navigate.  There are of course multiple minima in neural network optimization problems, and the shortest path between two minima can contain a barrier of higher cost. We can ﬁnd two different solutions by us- ing different random seeds for the random number generators used to initialize the weights, generate dropout masks, and select examples for SGD minibatches. (It is possible that these solutions are not minima but saddle points that SGD failed to escape) We do not ﬁnd any local minima within this subspace other than solution points, and these different solutions appear to correspond to different choices of how to break the symmetry of the saddle point at the origin, rather than to fundamentally different solutions of varying quality. See Fig. 5.  4 ADVANCED NETWORKS  Having performed experiments to understand the behavior of neural network optimization on su- pervised feedforward networks, we now verify that the same behavior occurs for more advanced networks. In the case of convolutional networks, we ﬁnd that there is a single barrier in the objective function, near where the network is initialized. This may simply correspond to the network being initialized with too large of random weights. This barrier is reasonably wide but not very tall. See Fig. 6 for details. To examine the behavior of SGD on generative models, we experimented with an MP-DBM (Good- fellow et al., 2013a). This model is useful for our purposes because it gets good performance as a generative model and as a classiﬁer, and its objective function is easy to evaluate (no MCMC business). Here we ﬁnd a secondary local minimum with high error, but a visualization of the SGD trajectory reveals that SGD passed far enough around the anomaly to avoid having it affect learning.  5  0.00.51.01.52.0α02468101214J(θ)Linear interpolation of maxout on MNIST between two solutionsJ(θ) trainJ(θ) validation0.00.51.01.52.0α02000400060008000100001200014000J(θ)Linear interpolation of maxout on MNIST from random starting pointJ(θ) trainJ(θ) validation0.00.51.01.52.0α0510152025J(θ)Linear interpolation of convolutional maxout on CIFARJ(θ) train0.000.050.100.150.200.250.30α2.222.242.262.282.302.322.34J(θ)Linear interpolation of convolutional maxout on CIFARJ(θ) trainPublished as a conference paper at ICLR 2015  Figure 7: Experiments with the MP-DBM. Left) The linear interpolation experiment reveals a sec- ondary local minimum with high error. Right) On the two horizonal axes, we plot components of θ that capture the extrema of θ throughout the learning process. On the vertical axis, we plot the objective function. Each point is another epoch of actual SGD learning. This plot allows us to see that SGD did not pass near this anomaly.  Figure 8: The linear interpolation experiment for an LSTM trained on the Penn Treebank dataset.  See Fig. 7. The MP-DBM was initialized with very large, sparse, weights, which may have con- tributed to our visualization technique exploring more non-convex areas, e.g. due to saturation of sigmoidal units. Finally, we performed the linear interpolation experiment for an LSTM regularized with dropout (Hochreiter & Schmidhuber, 1997; Zaremba et al., 2014) on the Penn Treebank dataset (Marcus et al., 1993). See Fig. 8. This experiment did not ﬁnd any difﬁcult structures, showing that the exotic features of non-convex optimization do not appear to cause difﬁculty even for recurrent models of sequences.  5 DEEP LINEAR NETWORKS  Saxe et al. (2013) have advocated developing a mathematical theory of deep networks by studying simpliﬁed mathematical models of these networks. Deep networks are formed by composing an alternating series of learned afﬁne transformations and ﬁxed non-linearities. One simpliﬁed way to model these functions is to compose only a series of learned linear transformations. The composition of a series of linear transformations is itself a linear transformation, so this mathematical model lacks the expressive capacity of a general deep network. However, because the weights of such a model are factored, its learning dynamics resemble those of the deep network. The output of the model is linear in the input to the model, but non-linear as a function of the model parameters. In particular, while ﬁtting linear regression parameters is a convex problem, ﬁtting deep linear regression parameters is a non-convex problem. Deep linear regression suffers from saddle points but does not suffer from local minima of varying quality. All minima are global minima, and are linked to each other in a continuous manifold.  6  0.00.51.01.52.0α0.040.060.080.100.120.140.160.180.200.22J(θ)Linear interpolation of an MP-DBM on MNISTJ(θ) trainJ(θ) validation1952002052102152202250204060800.060.080.100.120.140.160.180.200.00.20.40.60.81.0α345678910J(θ)Linear interpolation of an LSTM on Penn TreebankJ(θ) trainJ(θ) validationPublished as a conference paper at ICLR 2015  Figure 9: Linear interpolation from a small random initialization point to a solution for a linear regression model of depth 2. This shows the same qualitative features as our linear interpolation experiments for neural networks: a ﬂattening of the objective function near the saddle point at the origin, and only one minimum within this 1-D subspace.  Figure 10: Left) Interpolation between two solutions to deep linear regression. Though these two solutions lie on connected manifold of globally minimal values, the straight line between them en- counters a barrier of higher cost. The curve for the low dimensional linear model has all the same qualitative characteristics as the curve for the high dimensional non-linear networks we studied. Right) Interpolation between a random point with large norm and an solution to deep linear regres- sion. As with the neural network, this search does not encounter any minima other than the solution used to initialize the search.  Our linear interpolation experiments can be carried out analytically rather than experimentally in the case of deep linear regression. The results are strikingly similar to our results with deep non-linear networks. Speciﬁcally, we show that the problem of training y = w1w2x to output 1 when x = 1 using mean squared error is sufﬁcient to produce all of the qualitative features of neural network training that our linear interpolation experiments have exposed. See Fig. 9 and Fig. 10.  6 DISCUSSION  The reason for the success of SGD on a wide variety of tasks is now clear: these tasks are relatively easy to optimize. Finding a good direction in a high-dimensional space is a difﬁcult problem, but it is not nearly as difﬁcult as navigating an error surface that has complicated obstacles within multiple different low-dimensional subspaces. This work has only considered neural networks that perform very well. It is possible that these neural networks perform well because extensive hyperparameter search has found problems that SGD is able to optimize easily, but that other hyperparameters correspond to optimization problems that are too hard. In particular, it seems likely that very large neural networks are easier to ﬁt to a particular task.  7  0.00.51.01.52.0α0123456789J(θ)Linear interpolation of a deep linear modelJ(θ) train0.00.20.40.60.81.01.21.4α0.00.20.40.60.81.01.21.41.6J(θ)Linear interpolation between two solutions for a deep linear modelJ(θ) train0.00.20.40.60.81.01.21.4α0.00.20.40.60.81.01.2J(θ)Interpolating from a random point to a solution for a deep linear modelJ(θ) train, trial 0J(θ) train, trial 1J(θ) train, trial 2J(θ) train, trial 3J(θ) train, trial 4Published as a conference paper at ICLR 2015  Future work should aim to characterize the set of problems that are easy for SGD, to understand why SGD is able to avoid the obstacles that are present, and to determine why the training of large models remains slow despite the scarcity of obstacles. More advanced optimization algorithms could reduce the computational cost of deploying neural networks by enabling small networks to reach good performance, and could reduce the cost of training large networks by reducing the amount of time required to reach their solution.  ACKNOWLEDGMENTS  We would like to thank J¨org Bornschein, Eric Drexler, and Yann Dauphin for helpful discussions. We would like to thank Yaroslav Bulatov, Chung-Cheng Chiu, Greg Corrado, and Jeff Dean for their reviews of drafts of this work. We would like to thank the developers of Theano(Bergstra et al., 2010; Bastien et al., 2012) and Pylearn2(Goodfellow et al., 2013b).  REFERENCES Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Choromanska, A., Henaff, M., Mathieu, M., Ben Arous, G., and LeCun, Y. The Loss Surface of  Multilayer Networks. ArXiv e-prints, November 2014.  Dauphin, Yann N, Pascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, Ganguli, Surya, and Ben- gio, Yoshua. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 27, pp. 2933–2941. Curran Asso- ciates, Inc., 2014.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectiﬁer neural networks. In JMLR W&CP: Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011), April 2011.  Goodfellow, Ian, Shlens, Jonathon, and Szegedy, Christian. Explaining and harnessing adversarial  examples. 2014. URL http://arxiv.org/abs/1412.6572.  Goodfellow, Ian J., Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Multi-prediction deep  Boltzmann machines. In Neural Information Processing Systems, December 2013a.  Goodfellow, Ian J., Warde-Farley, David, Lamblin, Pascal, Dumoulin, Vincent, Mirza, Mehdi, Pas- canu, Razvan, Bergstra, James, Bastien, Fr´ed´eric, and Bengio, Yoshua. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013b.  Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout networks. In Dasgupta, Sanjoy and McAllester, David (eds.), International Conference on Machine Learning, pp. 1319–1327, 2013c.  Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 9(8):1735–1780,  1997.  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pp. 2146–2153. IEEE, 2009.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images.  Technical report, University of Toronto, 2009.  8  Published as a conference paper at ICLR 2015  LeCun, Yann, Bottou, Leon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998.  LeCun, Yann, Kavukcuoglu, Koray, and Farabet, Cl´ement. Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pp. 253–256. IEEE, 2010.  Marcus, Mitchell P., Santorini, Beatrice, and Marcinkiewicz, Mary Ann. Building a large annotated corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330, 1993. Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning internal representations by error  propagation. volume 1, chapter 8, pp. 318–362. MIT Press, Cambridge, 1986.  Saxe, Andrew M., McClelland, James L., and Ganguli, Surya. Exact solutions to the nonlinear  dynamics of learning in deep linear neural networks. In ICLR, 2013.  Srivastava, Nitish.  Improving Neural Networks with Dropout. Master’s thesis, University of  Toronto, Toronto, Canada, January 2013.  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15(1):1929–1958, 2014.  Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol. Recurrent neural network regularization.  CoRR, abs/1409.2329, 2014. URL http://arxiv.org/abs/1409.2329.  A EXPERIMENT DETAILS  All of our experiments except for the sigmoid network were using hyperparameters taken directly from the literature. We fully specify each of them here. Adversarially trained maxout network: This model is the one used by Goodfellow et al. (2014). There is no public conﬁguration for it, but the paper describes how to modify the previous best maxout network to obtain it. Maxout network: This model was retrained using the publicly available implementation used by Goodfellow et al. (2013c). The code is available at: https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/scripts/ papers/maxout/mnist_pi.yaml ReLU network with dropout: This model is intended to nearly reproduce the the dropout ReLU network described by Srivastava (2013). It is a standard reference implementation provided by Pylearn2 (Goodfellow et al., 2013b) and the speciﬁc ﬁle is available here: https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/scripts/ papers/dropout/mnist_valid.yaml ReLU network without dropout: We simply removed the dropout from the preceding conﬁguration ﬁle. Sigmoid network: We simply replaced the ReLU non-linearities with sigmoids. This performs ac- ceptably for a sigmoid network; it gets a test set error rate of 1.66%. Convolutional network: We used the best convolutional network available in Pylearn2 for the CIFAR-10 dataset, speciﬁcally the one developed by Goodfellow et al. (2013c). to reduce the computational cost of computing the training set objective, In order we used the the variant without data augmentation. The conﬁguration ﬁle is avail- able here: https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/ scripts/papers/maxout/cifar10_no_aug_valid.yaml MP-DBM: We used the best MP-DBM for classifying MNIST, as described by Goodfellow et al. (2013a). Dropout LSTM: We used the conﬁguration described in the paper introducing this method (Zaremba et al., 2014).  9  Published as a conference paper at ICLR 2015  Figure 11: Plots of the projection along the axis from initialization to solution versus the norm of the residual of this projection for random walks of varying dimension. Each plot is formed by using 1,000 steps. We designate step 900 as being the “solution” and continue to plot 100 more steps, in order to simulate the way neural network training trajectories continue past the point that early stopping on a validation set criterion chooses as being the solution. Each step is made by incrementing the current coordinate by a sample from a Gaussian distribution with zero mean and unit covariance. Because the dimensionality of the space forces most trajectories to have this highly regular shape, this kind of plot is not a meaningful way of investigating how SGD behaves as it moves away from the 1-D subspace we study in this paper.  B STRAYING FROM THE PATH  We have shown that, for seven different models of practical interest, there exists a straight path from initialization to solution along which the objective function decreases smoothly and monotonically, at least up to the resolution that our experiments investigated. Stochastic gradient descent does not actually follow this path. We know that SGD matches this path at the beginning and at the end. One might naturally want to plot the norm of the residual of the parameter value after projecting the parameters at each point in time into the 1-D subspace we have identiﬁed. SGD begins at θi, the solution point chosen by early stopping is θf , and SGD visits θ(t) at time t. If we deﬁne u to be a unit vector pointing in the direction θf −θi, then the primary subspace we have investigated so far is the line θi + α(t)u where α(t) = (θ(t) − θi)(cid:62)u. We can plot the coordinate within this subspace, α(t), on the horizontal axis. We can then deﬁne a second unit vector v pointed in the direction θ(t) − (θi + α(t)u). The projection into this subspace is β(t) = (θ(t) − θi − α(t)u)(cid:62)v. In other words, β(t) is the norm of the residual of the projection of θi onto the line spanning initialization and solution. We can plot β on the vertical axis. In the next section, we will see that the shape of the objective function in terms of α and β coor- dinates has interesting features that vary from one problem to another. However, it is important to understand that this kind of plot does not tell us much about the shape of the SGD trajectory. All that it tells us is how far SGD strays from the primary linear subspace. This is because in high dimensional spaces, the shape of this curve does not convey very much information. See Fig. 11 for a demonstration of how this plot converges to a simple geometric shape as the dimensionality of a random walk increases.  10  10010203040506070Distance along main ray0510152025L2 norm of residualDivergence of random walk from the main ray (2 dimensional)20020406080100120140Distance along main ray01020304050L2 norm of residualDivergence of random walk from the main ray (10 dimensional)02004006008001000Distance along main ray0100200300400500L2 norm of residualDivergence of random walk from the main ray (1000 dimensional)0200040006000800010000Distance along main ray010002000300040005000L2 norm of residualDivergence of random walk from the main ray (100000 dimensional)Published as a conference paper at ICLR 2015  Figure 12: To show the effect of different learning rates (cid:15) and momentum coefﬁcients µ, we plot the projection and residual norm of gradient descent with several different hyperparameters. In this case, to make the plots comparable, we use the true solution to a synthetic, convex problem as the endpoint for all trajectories. (In the non-convex case, different trajectories could go to different solutions, and this would confound our interpretation of the differences between them) Because this problem is 100,000 dimensional, the curves all have a very simple shape, and the primary quantity distinguishing them is the maximum norm of the residual.  Figure 13: This plot examines how far off the linear path SGD strays when training a maxout network on MNIST. The x axis is the projection along the linear path from initialization to solution. The y axis is the norm of the residual. The plot uses the Lp norm with p = 2, also known as the Euclidean norm. It is not the squared norm.  Plots of the residual norm of the projection for SGD trajectories converge to a very similar geometric shape in high dimensional spaces. See Fig. 12 for an example of several different runs of SGD on the same problem. However, we can still glean some information from this kind of plot by looking at the maximum norm of the residual and comparing this to the maximum norm of the parameter vector as a whole. We show this same kind of plot for a maxout network in Fig. 13. Keep in mind that the shape of the trajectory is not interesting, but the ratio of the norm of the residual to the total norm of the parameter vector at each point does give us some idea of how much information the 1-D projection discards. We see from this plot that our linear subspace captures at least 2/3 the norm of the parameter vector at all points in time.  C THREE-DIMENSIONAL VISUALIZATIONS  A natural question is whether there exist obstacles in between the well-behaved linear subspace we have described and the path followed by SGD. One way to investigate this is to introduce an additional direction of exploration. Speciﬁcally, we would like to explore the line passing through each SGD point and its projection on the primary 1-D subspace we have investigated so far. If this line contains obstacles, it could explain why SGD does not exploit the simple behavior within this subspace.  11  0100200300400500600Distance along main ray051015L2 norm of residualDivergence of SGD from the main ray†=0.0001,µ=0.†=0.0001,µ=0.5†=0.0001,µ=0.9†=0.0005,µ=0.†=0.0005,µ=0.5†=0.0005,µ=0.9†=0.001,µ=0.†=0.001,µ=0.5†=0.001,µ=0.9100102030405060Distance along main ray05101520L2 norm of residualDivergence of SGD path from the main rayPublished as a conference paper at ICLR 2015  Figure 14: The error surface (1 − w1w2)2 of a factored linear model with two layers and one unit per layer. This error surface shows a saddle point at the origin and a non-linear manifold of global solutions. The SGD trajectory from initialization to solution encounters negative curvature near its initial point and positive curvature near its ﬁnal point, but does not encounter any exotic obstacles.  For our factored linear model, where the training loss is just (1 − w1w2)2, we can accomplish this by viewing a heatmap of the cost function. See Fig 14. This ﬁgure predicts many properties that we expect to see for our more complicated neural network models: negative curvature at initialization, positive curvature surrounding the solution, a lack of obstacles separating the SGD trajectory from the well-behaved region of the function, and a connected manifold of globally optimal points. In this case, the connected manifold is the hyperbola w2 = 1/w1. For neural networks, the pattern of equivalent solutions will be different. For example, we can take a deep rectiﬁed linear network and obtain an equivalent network by rescaling its parameters. If we modify the parameters of layer i by multiplying bias j and column j of the weight matrix by γ, then we can preserve the function of the input that the network respesents by dividing row j of the weights for layer i + 1 by γ. In the case of the factored linear model, the hyperbolic shape of this manifold means that linearly interpolating between two solution points reveals a region of high cost in the middle, even though the two solutions are connected by a manifold of other solutions. This kind of 3-D plot is not directly achievable for problems with more than two parameters. We must summarize the parameters with a 2-D coordinate somehow. In the remainder of this section, we summarize the parameters via their projection in to the line spanning initialization and solution (the α(t) coordinate deﬁned in the previous section) and their projection into the line orthogonal to this subspace that passes through the SGD point θ(t), which we deﬁned as the coordinate β(t) in the previous section.  12  w1w2  −2−1.5−1−0.500.511.52−2−1.5−1−0.500.511.52GradientManifold of global minimaSaddle pointGradient descent pathLinear pathInterpolation between minima0.511.522.533.544.5Student Version of MATLABPublished as a conference paper at ICLR 2015  Figure 15: As a canonical example of the deep factored linear model of deep network training, we trained a linear model with mean squared error on MNIST. Speciﬁcally, we multiply together ten matrices, the ﬁrst nine of which are square and have the same dimension as the MNIST input, and the last of which has only ten columns. The output is thus a ten dimensional vector. The mean squared error encourages element i of this vector to have value close to 1 and the other elements to have zero when the true class is i. This 3-D plot shows negative curvature near the initialization point, positive curvature near the solution point, and a general lack of obstacles.  If we plot the cost as a function of α and β, we see that the cost function of a deep factored linear model (Fig. 15) has roughly the same structure as the cost function of our LSTM (Fig. 16) and as most of our feedforward networks (we show one in Fig. 17). These models show only structures predicted by the factored linear model of learning dynamics. However, for the adversarially trained maxout network, we ﬁnd that an obstacle that is small in height yet very steep constrains SGD into a narrow canyon, preventing it from accessing the subspace studied in the main text of this paper (Fig. 18 and Fig. 19). Finally, recall that the MP-DBM had a local minimum within the primary 1-D subspace (which could be a local minimum or a saddle point in high dimensional space). With our 3-D plot (Fig. 20), we see that SGD passed far around the plateau surrounding this point. Note that in most of these visualizations we can see signﬁcant negative curvature in the early part of the SGD trajectory, and that SGD does not seem to have any difﬁculty escaping the saddle point near the origin. One possible explanation for this behavior is that one model of SGD with sufﬁciently small step size naturally avoids saddle points. Consider the SGD trajectory as a function of time, θ(t). As an analytical model of SGD with small step size, we can consider the continuous-time dt θ = −∇θJ(θ). If we make a second-order Taylor series expansion gradient descent process with d  13  Projection01234567Residual2.01.51.00.50.00.51.01.52.02.50.00.20.40.60.8Published as a conference paper at ICLR 2015  Figure 16: The 3-D visualization of the LSTM cost reveals a simple structure, qualitatively the same as that of the deep factored linear model.  14  Projection0100200300400500Residual200100010020024681012Published as a conference paper at ICLR 2015  Figure 17: Most of our 3-D visualizations feedforward networks had shapes that were qualitatively the same as the factored linear network. Here we show the adversarially trained ReLU network as a representative sample.  15  Projection020406080100Residual40200204060800.00.51.01.52.0Published as a conference paper at ICLR 2015  Figure 18: The 3-D visualization technique applied to adversarially trained maxout reveals some obstacles.  16  Projection0102030405060Residual3020100102030400.00.51.01.52.0Published as a conference paper at ICLR 2015  Figure 19: The same as Fig. 18, but zoomed in to show detail near the end of learning.  in time  it simpliﬁes to  θ(t) ≈ θ(0) − t  d dt  θ(t) +  1 2  t2 d2  dt2 θ(t)  θ(t) ≈ θ(0) − t∇θ(0)J(θ(0)) +  t2H(0)∇θ(0)J(θ(0))  1 2  where H is the Hessian matrix of J(θ(0)) with respect to θ(0). This view shows that a second-order approximation in time of continuous-time gradient descent incorporates second-order information in space via the Hessian matrix. Speciﬁcally, the second-order term of the Taylor series expansion is equivalent to ascending the gradient of ||∇θJ(θ)||2. In other words, the ﬁrst-order term says to go downhill, while the second-order term says to make the gradient get bigger. The latter term encourages SGD to exploit directions of negative curvature.  D CONTROL VISUALIZATIONS  Visualization has not typically been used as a tool for understanding the structure of neural net- work objective functions. This is mostly because neural network objective functions are very high- dimensional and visualizations are by necessity fairly low dimensional. In this section, we include a few “control” visualizations as a reminder of the need to interpret any low-dimensional visualization carefully. Most of our visualizations showed rich structure in the cost function and a relatively simple shape in the SGD trajectory. It’s important to remember that our 3-D visualizations are not showing a 2-D linear subspace. Instead, they are showing multiply 1-D subspaces rotated to be parallel to each other. Our particular choice of subspaces was intended to capture a lot of variation in the cost function, and as a side effect it discards all variation in a high-dimensional trajectory, reducing most trajectories to semi-circles. If as a control we instead plot a randomly selected 2-D linear subspace  17  Projection404244464850525456Residual3020100102030400.140.160.180.200.220.240.26Published as a conference paper at ICLR 2015  Figure 20: The 3-D visualization of the MP-DBM contains a plateau, but SGD avoided it.  intersecting the solution point, then we see that there is almost no variation in the cost function within this subspace, and the SGD trajectory is quite noisy. See Fig. 21. As an intermediate control, we generated the plot for the MP-DBM, with α(t) on one axis, and the other axis being a random linear projection. This allows us to see a true 2-D linear subspace that has signiﬁcant variation in the cost function due to the choice of the ﬁrst axis, but also allows us to see that the SGD trajectory is not a semi-circle. See Fig. 22  18  Projection020406080100120140160180Residual500500.000.050.100.150.20Published as a conference paper at ICLR 2015  Figure 21: As a control experiment, we plot a random 2-D subspace intersecting the solution point. In this subspace, we see a complicated SGD trajectory and essentially no variation in the cost func- tion value. This visualization is useful as a reminder that the visualizations presented in this paper are designed to expose variation in the cost function and discard variation in the shape of SGD tra- jectory. Not all directions in the cost function have high variability and SGD trajectories do vary greatly.  19  0.040.030.020.010.000.010.020.030.040.050.140.120.100.080.060.040.020.000.020.00.51.01.52.02.5Published as a conference paper at ICLR 2015  Figure 22: A control experiment with α(t) on one axis and the other axis being a random projection.  20  500501001502000.070.060.050.040.030.020.010.000.010.00.10.20.30.40.5",
1409.0473,2015,Neural Machine Translation by Jointly Learning to Align and Translate,"['Neural Machine Translation by Jointly Learning to Align and Translate', 'Dzmitry Bahdanau', 'Kyunghyun Cho', 'and Yoshua Bengio']",https://arxiv.org/pdf/1409.0473,"6 1 0 2     y a M 9 1         ] L C . s c [      7 v 3 7 4 0  .  9 0 4 1 : v i X r a  Published as a conference paper at ICLR 2015  NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE  Dzmitry Bahdanau Jacobs University Bremen, Germany  KyungHyun Cho Universit´e de Montr´eal  Yoshua Bengio∗  ABSTRACT  Neural machine translation is a recently proposed approach to machine transla- tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu- ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec- ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.  1  INTRODUCTION  Neural machine translation is a newly emerging approach to machine translation, recently proposed by Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Unlike the traditional phrase-based translation system (see, e.g., Koehn et al., 2003) which consists of many small sub-components that are tuned separately, neural machine translation attempts to build and train a single, large neural network that reads a sentence and outputs a correct translation. Most of the proposed neural machine translation models belong to a family of encoder– decoders (Sutskever et al., 2014; Cho et al., 2014a), with an encoder and a decoder for each lan- guage, or involve a language-speciﬁc encoder applied to each sentence whose outputs are then com- pared (Hermann and Blunsom, 2014). An encoder neural network reads and encodes a source sen- tence into a ﬁxed-length vector. A decoder then outputs a translation from the encoded vector. The whole encoder–decoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence. A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a ﬁxed-length vector. This may make it difﬁcult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus. Cho et al. (2014b) showed that indeed the performance of a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases. In order to address this issue, we introduce an extension to the encoder–decoder model which learns to align and translate jointly. Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.  ∗CIFAR Senior Fellow  1  Published as a conference paper at ICLR 2015  The most important distinguishing feature of this approach from the basic encoder–decoder is that it does not attempt to encode a whole input sentence into a single ﬁxed-length vector. Instead, it en- codes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a ﬁxed-length vector. We show this allows a model to cope better with long sentences. In this paper, we show that the proposed approach of jointly learning to align and translate achieves signiﬁcantly improved translation performance over the basic encoder–decoder approach. The im- provement is more apparent with longer sentences, but can be observed with sentences of any length. On the task of English-to-French translation, the proposed approach achieves, with a single model, a translation performance comparable, or close, to the conventional phrase-based system. Furthermore, qualitative analysis reveals that the proposed model ﬁnds a linguistically plausible (soft-)alignment between a source sentence and the corresponding target sentence.  2 BACKGROUND: NEURAL MACHINE TRANSLATION  From a probabilistic perspective, translation is equivalent to ﬁnding a target sentence y that max- imizes the conditional probability of y given a source sentence x, i.e., arg maxy p(y | x). In neural machine translation, we ﬁt a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a translation model, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability. Recently, a number of papers have proposed the use of neural networks to directly learn this condi- tional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al., 2014; Cho et al., 2014b; Forcada and ˜Neco, 1997). This neural machine translation approach typ- ically consists of two components, the ﬁrst of which encodes a source sentence x and the second decodes to a target sentence y. For instance, two recurrent neural networks (RNN) were used by (Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a ﬁxed-length vector and to decode the vector into a variable-length target sentence. Despite being a quite new approach, neural machine translation has already shown promising results. Sutskever et al. (2014) reported that the neural machine translation based on RNNs with long short- term memory (LSTM) units achieves close to the state-of-the-art performance of the conventional phrase-based machine translation system on an English-to-French translation task.1 Adding neural components to existing translation systems, for instance, to score the phrase pairs in the phrase table (Cho et al., 2014a) or to re-rank candidate translations (Sutskever et al., 2014), has allowed to surpass the previous state-of-the-art performance level.  2.1 RNN ENCODER–DECODER  Here, we describe brieﬂy the underlying framework, called RNN Encoder–Decoder, proposed by Cho et al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns to align and translate simultaneously. In the Encoder–Decoder framework, an encoder reads the input sentence, a sequence of vectors x = (x1,··· , xTx ), into a vector c.2 The most common approach is to use an RNN such that  ht = f (xt, ht−1)  (1)  and  c = q ({h1,··· , hTx}) ,  where ht ∈ Rn is a hidden state at time t, and c is a vector generated from the sequence of the hidden states. f and q are some nonlinear functions. Sutskever et al. (2014) used an LSTM as f and q ({h1,··· , hT}) = hT , for instance.  1 We mean by the state-of-the-art performance, the performance of the conventional phrase-based system  without using any neural network-based component.  2 Although most of the previous works (see, e.g., Cho et al., 2014a; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013) used to encode a variable-length input sentence into a ﬁxed-length vector, it is not necessary, and even it may be beneﬁcial to have a variable-length vector, as we will show later.  2  Published as a conference paper at ICLR 2015  T(cid:89)  The decoder is often trained to predict the next word yt(cid:48) given the context vector c and all the previously predicted words {y1,··· , yt(cid:48)−1}. In other words, the decoder deﬁnes a probability over the translation y by decomposing the joint probability into the ordered conditionals:  where y =(cid:0)y1,··· , yTy  p(y) =  p(yt | {y1,··· , yt−1} , c),  (cid:1). With an RNN, each conditional probability is modeled as  t=1  (2)  p(yt | {y1,··· , yt−1} , c) = g(yt−1, st, c),  (3) where g is a nonlinear, potentially multi-layered, function that outputs the probability of yt, and st is the hidden state of the RNN. It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used (Kalchbrenner and Blunsom, 2013).  3 LEARNING TO ALIGN AND TRANSLATE  In this section, we propose a novel architecture for neural machine translation. The new architecture consists of a bidirectional RNN as an encoder (Sec. 3.2) and a decoder that emulates searching through a source sentence during decoding a translation (Sec. 3.1).  3.1 DECODER: GENERAL DESCRIPTION  In a new model architecture, we deﬁne each conditional probability in Eq. (2) as:  p(yi|y1, . . . , yi−1, x) = g(yi−1, si, ci),  (4)  where si is an RNN hidden state for time i, computed by  si = f (si−1, yi−1, ci).  It should be noted that unlike the existing encoder–decoder ap- proach (see Eq. (2)), here the probability is conditioned on a distinct context vector ci for each target word yi. The context vector ci depends on a sequence of annotations (h1,··· , hTx ) to which an encoder maps the input sentence. Each annotation hi contains information about the whole input sequence with a strong focus on the parts surrounding the i-th word of the input sequence. We explain in detail how the annotations are com- puted in the next section. The context vector ci is, then, computed as a weighted sum of these annotations hi:  ci =  αijhj.  (5)  Tx(cid:88)  j=1  Figure 1: The graphical illus- tration of the proposed model trying to generate the t-th tar- get word yt given a source sentence (x1, x2, . . . , xT ).  The weight αij of each annotation hj is computed by  (cid:80)Tx  exp (eij) k=1 exp (eik)  αij =  ,  (6)  where  eij = a(si−1, hj)  is an alignment model which scores how well the inputs around position j and the output at position i match. The score is based on the RNN hidden state si−1 (just before emitting yi, Eq. (4)) and the j-th annotation hj of the input sentence. We parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system. Note that unlike in traditional machine translation,  3  x1x2x3xT+αt,1αt,2αt,3αt,Tyt-1yth1h2h3hTh1h2h3hTst-1stPublished as a conference paper at ICLR 2015  the alignment is not considered to be a latent variable. Instead, the alignment model directly com- putes a soft alignment, which allows the gradient of the cost function to be backpropagated through. This gradient can be used to train the alignment model as well as the whole translation model jointly. We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation, where the expectation is over possible alignments. Let αij be a probability that the target word yi is aligned to, or translated from, a source word xj. Then, the i-th context vector ci is the expected annotation over all the annotations with probabilities αij. The probability αij, or its associated energy eij, reﬂects the importance of the annotation hj with respect to the previous hidden state si−1 in deciding the next state si and generating yi. Intuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a ﬁxed- length vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.  3.2 ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES  The usual RNN, described in Eq. (1), reads an input sequence x in order starting from the ﬁrst symbol x1 to the last one xTx. However, in the proposed scheme, we would like the annotation of each word to summarize not only the preceding words, but also the following words. Hence, we propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been successfully used recently in speech recognition (see, e.g., Graves et al., 2013).  A BiRNN consists of forward and backward RNN’s. The forward RNN as it is ordered (from x1 to xTx) and calculates a sequence of forward hidden states ( The backward RNN sequence of backward hidden states (  −→ f reads the input sequence −→ h Tx ). ←− f reads the sequence in the reverse order (from xTx to x1), resulting in a  −→ We obtain an annotation for each word xj by concatenating the forward hidden state h j and the backward one . In this way, the annotation hj contains the summaries of both the preceding words and the following words. Due to the tendency of RNNs to better represent recent inputs, the annotation hj will be focused on the words around xj. This sequence of annotations is used by the decoder and the alignment model later to compute the context vector (Eqs. (5)–(6)). See Fig. 1 for the graphical illustration of the proposed model.  ←− h j, i.e., hj =  −→ h 1,··· ,  ←− h 1,··· ,  ←− h Tx ).  (cid:104)−→  (cid:105)(cid:62)  ←− h (cid:62)  h (cid:62) j ;  j  4 EXPERIMENT SETTINGS  We evaluate the proposed approach on the task of English-to-French translation. We use the bilin- gual, parallel corpora provided by ACL WMT ’14.3 As a comparison, we also report the perfor- mance of an RNN Encoder–Decoder which was proposed recently by Cho et al. (2014a). We use the same training procedures and the same dataset for both models.4  4.1 DATASET  WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively, totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).5 We do not use any monolingual data other than the mentioned parallel corpora, although it may be possible to use a much larger monolingual corpus to pretrain an encoder. We concatenate news-test-  3 http://www.statmt.org/wmt14/translation-task.html 4 Implementations are available at https://github.com/lisa-groundhog/GroundHog. 5 Available online at http://www-lium.univ-lemans.fr/˜schwenk/cslm_joint_paper/.  4  Published as a conference paper at ICLR 2015  Figure 2: The BLEU scores of the generated translations on the test set with respect to the lengths of the sen- tences. The results are on the full test set which in- cludes sentences having un- known words to the models.  2012 and news-test-2013 to make a development (validation) set, and evaluate the models on the test set (news-test-2014) from WMT ’14, which consists of 3003 sentences not present in the training data. After a usual tokenization6, we use a shortlist of 30,000 most frequent words in each language to train our models. Any word not included in the shortlist is mapped to a special token ([UNK]). We do not apply any other special preprocessing, such as lowercasing or stemming, to the data.  4.2 MODELS  We train two types of models. The ﬁrst one is an RNN Encoder–Decoder (RNNencdec, Cho et al., 2014a), and the other is the proposed model, to which we refer as RNNsearch. We train each model twice: ﬁrst with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then with the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50). The encoder and decoder of the RNNencdec have 1000 hidden units each.7 The encoder of the RNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000 hidden units. Its decoder has 1000 hidden units. In both cases, we use a multilayer network with a single maxout (Goodfellow et al., 2013) hidden layer to compute the conditional probability of each target word (Pascanu et al., 2014). We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model. Each SGD update direction is computed using a minibatch of 80 sen- tences. We trained each model for approximately 5 days. Once a model is trained, we use a beam search to ﬁnd a translation that approximately maximizes the conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al., 2013). Sutskever et al. (2014) used this approach to generate translations from their neural machine translation model. For more details on the architectures of the models and training procedure used in the experiments, see Appendices A and B.  5 RESULTS  5.1 QUANTITATIVE RESULTS  In Table 1, we list the translation performances measured in BLEU score. It is clear from the table that in all the cases, the proposed RNNsearch outperforms the conventional RNNencdec. More importantly, the performance of the RNNsearch is as high as that of the conventional phrase-based translation system (Moses), when only the sentences consisting of known words are considered. This is a signiﬁcant achievement, considering that Moses uses a separate monolingual corpus (418M words) in addition to the parallel corpora we used to train the RNNsearch and RNNencdec.  6 We used the tokenization script from the open-source machine translation package, Moses. 7 In this paper, by a ’hidden unit’, we always mean the gated hidden unit (see Appendix A.1.1).  5  0102030405060Sentencelength051015202530BLEUscoreRNNsearch-50RNNsearch-30RNNenc-50RNNenc-30Published as a conference paper at ICLR 2015  (a)  (b)  (c)  (d)  Figure 3: Four sample alignments found by RNNsearch-50. The x-axis and y-axis of each plot correspond to the words in the source sentence (English) and the generated translation (French), respectively. Each pixel shows the weight αij of the annotation of the j-th source word for the i-th target word (see Eq. (6)), in grayscale (0: black, 1: white). (a) an arbitrary sentence. (b–d) three randomly selected samples among the sentences without any unknown words and of length between 10 and 20 words from the test set.  One of the motivations behind the proposed approach was the use of a ﬁxed-length context vector in the basic encoder–decoder approach. We conjectured that this limitation may make the basic encoder–decoder approach to underperform with long sentences. In Fig. 2, we see that the perfor- mance of RNNencdec dramatically drops as the length of the sentences increases. On the other hand, both RNNsearch-30 and RNNsearch-50 are more robust to the length of the sentences. RNNsearch- 50, especially, shows no performance deterioration even with sentences of length 50 or more. This superiority of the proposed model over the basic encoder–decoder is further conﬁrmed by the fact that the RNNsearch-30 even outperforms RNNencdec-50 (see Table 1).  6  TheagreementontheEuropeanEconomicAreawassignedinAugust1992.<end>L'accordsurlazoneéconomiqueeuropéenneaétésignéenaoût1992.<end>Itshouldbenotedthatthemarineenvironmentistheleastknownofenvironments.<end>Ilconvientdenoterquel'environnementmarinestlemoinsconnudel'environnement.<end>DestructionoftheequipmentmeansthatSyriacannolongerproducenewchemicalweapons.<end>Ladestructiondel'équipementsignifiequelaSyrienepeutplusproduiredenouvellesarmeschimiques.<end>""Thiswillchangemyfuturewithmyfamily,""themansaid.<end>""Celavachangermonaveniravecmafamille"",aditl'homme.<end>Published as a conference paper at ICLR 2015  Model  RNNencdec-30 RNNsearch-30 RNNencdec-50 RNNsearch-50 RNNsearch-50(cid:63)  Moses  All 13.93 21.50 17.82 26.75 28.45 33.30  No UNK◦  24.19 31.44 26.71 34.16 36.15 35.63  Table 1: BLEU scores of the trained models com- puted on the test set. The second and third columns show respectively the scores on all the sentences and, on the sentences without any unknown word in them- selves and in the reference translations. Note that RNNsearch-50(cid:63) was trained much longer until the performance on the development set stopped improv- ing. (◦) We disallowed the models to generate [UNK] tokens when only the sentences having no unknown words were evaluated (last column).  5.2 QUALITATIVE ANALYSIS  5.2.1 ALIGNMENT  The proposed approach provides an intuitive way to inspect the (soft-)alignment between the words in a generated translation and those in a source sentence. This is done by visualizing the annotation weights αij from Eq. (6), as in Fig. 3. Each row of a matrix in each plot indicates the weights associated with the annotations. From this we see which positions in the source sentence were considered more important when generating the target word. We can see from the alignments in Fig. 3 that the alignment of words between English and French is largely monotonic. We see strong weights along the diagonal of each matrix. However, we also observe a number of non-trivial, non-monotonic alignments. Adjectives and nouns are typically ordered differently between French and English, and we see an example in Fig. 3 (a). From this ﬁgure, we see that the model correctly translates a phrase [European Economic Area] into [zone ´economique europ´een]. The RNNsearch was able to correctly align [zone] with [Area], jumping over the two words ([European] and [Economic]), and then looked one word back at a time to complete the whole phrase [zone ´economique europ´eenne]. The strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance, from Fig. 3 (d). Consider the source phrase [the man] which was translated into [l’ homme]. Any hard alignment will map [the] to [l’] and [man] to [homme]. This is not helpful for translation, as one must consider the word following [the] to determine whether it should be translated into [le], [la], [les] or [l’]. Our soft-alignment solves this issue naturally by letting the model look at both [the] and [man], and in this example, we see that the model was able to correctly translate [the] into [l’]. We observe similar behaviors in all the presented cases in Fig. 3. An additional beneﬁt of the soft align- ment is that it naturally deals with source and target phrases of different lengths, without requiring a counter-intuitive way of mapping some words to or from nowhere ([NULL]) (see, e.g., Chapters 4 and 5 of Koehn, 2010).  5.2.2 LONG SENTENCES  As clearly visible from Fig. 2 the proposed model (RNNsearch) is much better than the conventional model (RNNencdec) at translating long sentences. This is likely due to the fact that the RNNsearch does not require encoding a long sentence into a ﬁxed-length vector perfectly, but only accurately encoding the parts of the input sentence that surround a particular word. As an example, consider this source sentence from the test set:  An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital.  The RNNencdec-50 translated this sentence into:  Un privil`ege d’admission est le droit d’un m´edecin de reconnaˆıtre un patient `a l’hˆopital ou un centre m´edical d’un diagnostic ou de prendre un diagnostic en fonction de son ´etat de sant´e.  7  Published as a conference paper at ICLR 2015  The RNNencdec-50 correctly translated the source sentence until [a medical center]. However, from there on (underlined), it deviated from the original meaning of the source sentence. For instance, it replaced [based on his status as a health care worker at a hospital] in the source sentence with [en fonction de son ´etat de sant´e] (“based on his state of health”). On the other hand, the RNNsearch-50 generated the following correct translation, preserving the whole meaning of the input sentence without omitting any details:  Un privil`ege d’admission est le droit d’un m´edecin d’admettre un patient `a un hˆopital ou un centre m´edical pour effectuer un diagnostic ou une proc´edure, selon son statut de travailleur des soins de sant´e `a l’hˆopital.  Let us consider another sentence from the test set:  This kind of experience is part of Disney’s efforts to ”extend the lifetime of its series and build new relationships with audiences via digital platforms that are becoming ever more important,” he added.  The translation by the RNNencdec-50 is  Ce type d’exp´erience fait partie des initiatives du Disney pour ”prolonger la dur´ee de vie de ses nouvelles et de d´evelopper des liens avec les lecteurs num´eriques qui deviennent plus complexes.  As with the previous example, the RNNencdec began deviating from the actual meaning of the source sentence after generating approximately 30 words (see the underlined phrase). After that point, the quality of the translation deteriorates, with basic mistakes such as the lack of a closing quotation mark. Again, the RNNsearch-50 was able to translate this long sentence correctly:  Ce genre d’exp´erience fait partie des efforts de Disney pour ”prolonger la dur´ee de vie de ses s´eries et cr´eer de nouvelles relations avec des publics via des plateformes num´eriques de plus en plus importantes”, a-t-il ajout´e.  In conjunction with the quantitative results presented already, these qualitative observations con- ﬁrm our hypotheses that the RNNsearch architecture enables far more reliable translation of long sentences than the standard RNNencdec model. In Appendix C, we provide a few more sample translations of long source sentences generated by the RNNencdec-50, RNNsearch-50 and Google Translate along with the reference translations.  6 RELATED WORK  6.1 LEARNING TO ALIGN  A similar approach of aligning an output symbol with an input symbol was proposed recently by Graves (2013) in the context of handwriting synthesis. Handwriting synthesis is a task where the model is asked to generate handwriting of a given sequence of characters. In his work, he used a mixture of Gaussian kernels to compute the weights of the annotations, where the location, width and mixture coefﬁcient of each kernel was predicted from an alignment model. More speciﬁcally, his alignment was restricted to predict the location such that the location increases monotonically. The main difference from our approach is that, in (Graves, 2013), the modes of the weights of the annotations only move in one direction. In the context of machine translation, this is a severe limi- tation, as (long-distance) reordering is often needed to generate a grammatically correct translation (for instance, English-to-German). Our approach, on the other hand, requires computing the annotation weight of every word in the source sentence for each word in the translation. This drawback is not severe with the task of translation in which most of input and output sentences are only 15–40 words. However, this may limit the applicability of the proposed scheme to other tasks.  8  Published as a conference paper at ICLR 2015  6.2 NEURAL NETWORKS FOR MACHINE TRANSLATION  Since Bengio et al. (2003) introduced a neural probabilistic language model which uses a neural net- work to model the conditional probability of a word given a ﬁxed number of the preceding words, neural networks have widely been used in machine translation. However, the role of neural net- works has been largely limited to simply providing a single feature to an existing statistical machine translation system or to re-rank a list of candidate translations provided by an existing system. For instance, Schwenk (2012) proposed using a feedforward neural network to compute the score of a pair of source and target phrases and to use the score as an additional feature in the phrase-based statistical machine translation system. More recently, Kalchbrenner and Blunsom (2013) and Devlin et al. (2014) reported the successful use of the neural networks as a sub-component of the existing translation system. Traditionally, a neural network trained as a target-side language model has been used to rescore or rerank a list of candidate translations (see, e.g., Schwenk et al., 2006). Although the above approaches were shown to improve the translation performance over the state- of-the-art machine translation systems, we are more interested in a more ambitious objective of designing a completely new translation system based on neural networks. The neural machine trans- lation approach we consider in this paper is therefore a radical departure from these earlier works. Rather than using a neural network as a part of the existing system, our model works on its own and generates a translation from a source sentence directly.  7 CONCLUSION  The conventional approach to neural machine translation, called an encoder–decoder approach, en- codes a whole input sentence into a ﬁxed-length vector from which a translation will be decoded. We conjectured that the use of a ﬁxed-length context vector is problematic for translating long sen- tences, based on a recent empirical study reported by Cho et al. (2014b) and Pouget-Abadie et al. (2014). In this paper, we proposed a novel architecture that addresses this issue. We extended the basic encoder–decoder by letting a model (soft-)search for a set of input words, or their annotations com- puted by an encoder, when generating each target word. This frees the model from having to encode a whole source sentence into a ﬁxed-length vector, and also lets the model focus only on information relevant to the generation of the next target word. This has a major positive impact on the ability of the neural machine translation system to yield good results on longer sentences. Unlike with the traditional machine translation systems, all of the pieces of the translation system, including the alignment mechanism, are jointly trained towards a better log-probability of producing correct translations. We tested the proposed model, called RNNsearch, on the task of English-to-French translation. The experiment revealed that the proposed RNNsearch outperforms the conventional encoder–decoder model (RNNencdec) signiﬁcantly, regardless of the sentence length and that it is much more ro- bust to the length of a source sentence. From the qualitative analysis where we investigated the (soft-)alignment generated by the RNNsearch, we were able to conclude that the model can cor- rectly align each target word with the relevant words, or their annotations, in the source sentence as it generated a correct translation. Perhaps more importantly, the proposed approach achieved a translation performance comparable to the existing phrase-based statistical machine translation. It is a striking result, considering that the proposed architecture, or the whole family of neural machine translation, has only been proposed as recently as this year. We believe the architecture proposed here is a promising step toward better machine translation and a better understanding of natural languages in general. One of challenges left for the future is to better handle unknown, or rare words. This will be required for the model to be more widely used and to match the performance of current state-of-the-art machine translation systems in all contexts.  9  Published as a conference paper at ICLR 2015  ACKNOWLEDGMENTS  The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Qu´ebec, Compute Canada, the Canada Research Chairs and CIFAR. Bah- danau thanks the support from Planet Intelligent Systems GmbH. We also thank Felix Hill, Bart van Merri´enboer, Jean Pouget-Abadie, Coline Devin and Tae-Ho Kim.  REFERENCES Axelrod, A., He, X., and Gao, J. (2011). Domain adaptation via pseudo in-domain data selection. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 355–362. Association for Computational Linguistics.  Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient  descent is difﬁcult. IEEE Transactions on Neural Networks, 5(2), 157–166.  Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic language model.  J. Mach. Learn. Res., 3, 1137–1155.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- In  Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2013). Audio chord recognition with  recurrent neural networks. In ISMIR.  Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. (2014a). Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014). to appear.  Cho, K., van Merri¨enboer, B., Bahdanau, D., and Bengio, Y. (2014b). On the properties of neural machine translation: Encoder–Decoder approaches. In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. to appear.  Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. In Association for Computational Linguistics.  Forcada, M. L. and ˜Neco, R. P. (1997). Recursive hetero-associative memories for translation. In J. Mira, R. Moreno-D´ıaz, and J. Cabestany, editors, Biological and Artiﬁcial Computation: From Neuroscience to Technology, volume 1240 of Lecture Notes in Computer Science, pages 453–462. Springer Berlin Heidelberg.  Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). Maxout net- works. In Proceedings of The 30th International Conference on Machine Learning, pages 1319– 1327.  Graves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the  29th International Conference on Machine Learning (ICML 2012).  Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv:1308.0850  [cs.NE].  Graves, A., Jaitly, N., and Mohamed, A.-R. (2013). Hybrid speech recognition with deep bidirec- tional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Work- shop on, pages 273–278.  10  Published as a conference paper at ICLR 2015  Hermann, K. and Blunsom, P. (2014). Multilingual distributed representations without word align- ment. In Proceedings of the Second International Conference on Learning Representations (ICLR 2014).  Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut  f¨ur Informatik, Lehrstuhl Prof. Brauer, Technische Universit¨at M¨unchen.  Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8),  1735–1780.  Kalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700–1709. Association for Computational Linguistics.  Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press, New York, NY,  USA.  Koehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Stroudsburg, PA, USA. Association for Computational Linguistics.  Pascanu, R., Mikolov, T., and Bengio, Y. (2013a). On the difﬁculty of training recurrent neural  networks. In ICML’2013.  Pascanu, R., Mikolov, T., and Bengio, Y. (2013b). On the difﬁculty of training recurrent neural In Proceedings of the 30th International Conference on Machine Learning (ICML  networks. 2013).  Pascanu, R., Gulcehre, C., Cho, K., and Bengio, Y. (2014). How to construct deep recurrent neural networks. In Proceedings of the Second International Conference on Learning Representations (ICLR 2014).  Pouget-Abadie, J., Bahdanau, D., van Merri¨enboer, B., Cho, K., and Bengio, Y. (2014). Overcoming In  the curse of sentence length for neural machine translation using automatic segmentation. Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. to appear.  Schuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. Signal Processing,  IEEE Transactions on, 45(11), 2673–2681.  Schwenk, H. (2012). Continuous space translation models for phrase-based statistical machine translation. In M. Kay and C. Boitet, editors, Proceedings of the 24th International Conference on Computational Linguistics (COLIN), pages 1071–1080. Indian Institute of Technology Bombay.  Schwenk, H., Dchelotte, D., and Gauvain, J.-L. (2006). Continuous space language models for statistical machine translation. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 723–730. Association for Computational Linguistics.  Sutskever, I., Vinyals, O., and Le, Q. (2014). Sequence to sequence learning with neural networks.  In Advances in Neural Information Processing Systems (NIPS 2014).  Zeiler, M. D. (2012). ADADELTA: An adaptive learning rate method.  [cs.LG].  arXiv:1212.5701  11  Published as a conference paper at ICLR 2015  A MODEL ARCHITECTURE  A.1 ARCHITECTURAL CHOICES  The proposed scheme in Section 3 is a general framework where one can freely deﬁne, for instance, the activation functions f of recurrent neural networks (RNN) and the alignment model a. Here, we describe the choices we made for the experiments in this paper.  A.1.1 RECURRENT NEURAL NETWORK  For the activation function f of an RNN, we use the gated hidden unit recently proposed by Cho et al. (2014a). The gated hidden unit is an alternative to the conventional simple units such as an element-wise tanh. This gated unit is similar to a long short-term memory (LSTM) unit proposed earlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn long-term dependencies. This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1. These paths allow gradients to ﬂow backward easily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013a). It is therefore possible to use LSTM units instead of the gated hidden unit described here, as was done in a similar context by Sutskever et al. (2014). The new state si of the RNN employing n gated hidden units8 is computed by  si = f (si−1, yi−1, ci) = (1 − zi) ◦ si−1 + zi ◦ ˜si,  where ◦ is an element-wise multiplication, and zi is the output of the update gates (see below). The proposed updated state ˜si is computed by  ˜si = tanh (W e(yi−1) + U [ri ◦ si−1] + Cci) ,  where e(yi−1) ∈ Rm is an m-dimensional embedding of a word yi−1, and ri is the output of the reset gates (see below). When yi is represented as a 1-of-K vector, e(yi) is simply a column of an embedding matrix E ∈ Rm×K. Whenever possible, we omit bias terms to make the equations less cluttered. The update gates zi allow each hidden unit to maintain its previous activation, and the reset gates ri control how much and what information from the previous state should be reset. We compute them by  zi = σ (Wze(yi−1) + Uzsi−1 + Czci) , ri = σ (Wre(yi−1) + Ursi−1 + Crci) ,  where σ (·) is a logistic sigmoid function. At each step of the decoder, we compute the output probability (Eq. (4)) as a multi-layered func- tion (Pascanu et al., 2014). We use a single hidden layer of maxout units (Goodfellow et al., 2013) and normalize the output probabilities (one for each word) with a softmax function (see Eq. (6)).  A.1.2 ALIGNMENT MODEL The alignment model should be designed considering that the model needs to be evaluated Tx × Ty times for each sentence pair of lengths Tx and Ty. In order to reduce computation, we use a single- layer multilayer perceptron such that  a(si−1, hj) = v(cid:62)  a tanh (Wasi−1 + Uahj) ,  where Wa ∈ Rn×n, Ua ∈ Rn×2n and va ∈ Rn are the weight matrices. Since Uahj does not depend on i, we can pre-compute it in advance to minimize the computational cost.  8 Here, we show the formula of the decoder. The same formula can be used in the encoder by simply  ignoring the context vector ci and the related terms.  12  Published as a conference paper at ICLR 2015  A.2 DETAILED DESCRIPTION OF THE MODEL  A.2.1 ENCODER  In this section, we describe in detail the architecture of the proposed model (RNNsearch) used in the experiments (see Sec. 4–5). From here on, we omit all bias terms in order to increase readability. The model takes a source sentence of 1-of-K coded word vectors as input  x = (x1, . . . , xTx), xi ∈ RKx and outputs a translated sentence of 1-of-K coded word vectors y = (y1, . . . , yTy ), yi ∈ RKy ,  where Kx and Ky are the vocabulary sizes of source and target languages, respectively. Tx and Ty respectively denote the lengths of source and target sentences. First, the forward states of the bidirectional recurrent neural network (BiRNN) are computed:  0  h i  (1 − −→z i) ◦ −→ (cid:16)−→ (cid:16)−→ (cid:16)−→  h i−1 + −→z i ◦ −→ (cid:104)−→r i ◦ −→ (cid:17) (cid:17)  , if i > 0 , if i = 0  (cid:105)(cid:17)  where  h i−1  W zExi +  −→ h i = tanh −→z i =σ −→r i =σ  W Exi + −→ U z −→ U r −→ W ,  −→ U −→ h i−1 −→ h i−1 W rExi + −→ −→ −→ E ∈ Rm×Kx is the word embedding matrix. U r ∈ Rn×n are W r ∈ Rn×m, W z, weight matrices. m and n are the word embedding dimensionality and the number of hidden units, respectively. σ(·) is as usual a logistic sigmoid function. The backward states ( E between the forward and backward RNNs, unlike the weight matrices. We concatenate the forward and backward states to to obtain the annotations (h1, h2,··· , hTx ), where  ←− h Tx ) are computed similarly. We share the word embedding matrix  ←− h 1,··· ,  −→ U z,  −→ U ,  .  (cid:40)  −→ h i =  (7)  (cid:35)  (cid:34) −→  h i←− h i  hi =  A.2.2 DECODER  The hidden state si of the decoder given the annotations from the encoder is computed by  si =(1 − zi) ◦ si−1 + zi ◦ ˜si,  where  ˜si = tanh (W Eyi−1 + U [ri ◦ si−1] + Cci) zi =σ (WzEyi−1 + Uzsi−1 + Czci) ri =σ (WrEyi−1 + Ursi−1 + Crci)  E is the word embedding matrix for the target language. W, Wz, Wr ∈ Rn×m, U, Uz, Ur ∈ Rn×n, and C, Cz, Cr ∈ Rn×2n are weights. Again, m and n are the word embedding dimensionality and the number of hidden units, respectively. The initial hidden state s0 is computed by s0 = tanh  , where Ws ∈ Rn×n.  ←− h 1  (cid:16)  (cid:17)  Ws  The context vector ci are recomputed at each step by the alignment model:  ci =  αijhj,  Tx(cid:88)  j=1  13  Published as a conference paper at ICLR 2015  Model  RNNenc-30 RNNenc-50 RNNsearch-30 RNNsearch-50 RNNsearch-50(cid:63)  Updates (×105) Epochs Hours 109 108 113 111 252  8.46 6.00 4.71 2.88 6.67  6.4 4.5 3.6 2.2 5.0  GPU  Train NLL Dev. NLL  TITAN BLACK Quadro K-6000 TITAN BLACK Quadro K-6000 Quadro K-6000  28.1 44.0 26.7 40.7 36.7  53.0 43.6 47.2 38.1 35.2  Table 2: Learning statistics and relevant information. Each update corresponds to updating the parameters once using a single minibatch. One epoch is one pass through the training set. NLL is the average conditional log-probabilities of the sentences in either the training set or the development set. Note that the lengths of the sentences differ.  where  (cid:80)Tx  exp (eij) k=1 exp (eik)  αij = eij =v(cid:62)  a tanh (Wasi−1 + Uahj) ,  and hj is the j-th annotation in the source sentence (see Eq. (7)). va ∈ Rn(cid:48) , Wa ∈ Rn(cid:48)×n and Ua ∈ Rn(cid:48)×2n are weight matrices. Note that the model becomes RNN Encoder–Decoder (Cho et al., 2014a), if we ﬁx ci to With the decoder state si−1, the context ci and the last generated word yi−1, we deﬁne the probability of a target word yi as  −→ h Tx.  where  (cid:1) ,  i Woti  p(yi|si, yi−1, ci) ∝ exp(cid:0)y(cid:62) ti =(cid:2)max(cid:8)˜ti,2j−1, ˜ti,2j (cid:9)(cid:3)(cid:62)  j=1,...,l  and ˜ti,k is the k-th element of a vector ˜ti which is computed by  ˜ti =Uosi−1 + VoEyi−1 + Coci.  Wo ∈ RKy×l, Uo ∈ R2l×n, Vo ∈ R2l×m and Co ∈ R2l×2n are weight matrices. This can be under- stood as having a deep output (Pascanu et al., 2014) with a single maxout hidden layer (Goodfellow et al., 2013).  A.2.3 MODEL SIZE  For all the models used in this paper, the size of a hidden layer n is 1000, the word embedding dimensionality m is 620 and the size of the maxout hidden layer in the deep output l is 500. The number of hidden units in the alignment model n(cid:48) is 1000.  B TRAINING PROCEDURE  B.1 PARAMETER INITIALIZATION  −→ U r as random or- We initialized the recurrent weight matrices U, Uz, Ur, thogonal matrices. For Wa and Ua, we initialized them by sampling each element from the Gaussian distribution of mean 0 and variance 0.0012. All the elements of Va and all the bias vectors were ini- tialized to zero. Any other weight matrix was initialized by sampling from the Gaussian distribution of mean 0 and variance 0.012.  −→ U z and  ←− U z,  ←− U r,  −→ U ,  ←− U ,  B.2 TRAINING  We used the stochastic gradient descent (SGD) algorithm. Adadelta (Zeiler, 2012) was used to automatically adapt the learning rate of each parameter ((cid:15) = 10−6 and ρ = 0.95). We explicitly  14  Published as a conference paper at ICLR 2015  normalized the L2-norm of the gradient of the cost function each time to be at most a predeﬁned threshold of 1, when the norm was larger than the threshold (Pascanu et al., 2013b). Each SGD update direction was computed with a minibatch of 80 sentences. At each update our implementation requires time proportional to the length of the longest sentence in a minibatch. Hence, to minimize the waste of computation, before every 20-th update, we retrieved 1600 sentence pairs, sorted them according to the lengths and split them into 20 minibatches. The training data was shufﬂed once before training and was traversed sequentially in this manner. In Tables 2 we present the statistics related to training all the models used in the experiments.  C TRANSLATIONS OF LONG SENTENCES  Source  Reference  RNNenc-50  An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital. Le privil`ege d’admission est le droit d’un m´edecin, en vertu de son statut de membre soignant d’un hˆopital, d’admettre un patient dans un hˆopital ou un centre m´edical aﬁn d’y d´elivrer un diagnostic ou un traitement. Un privil`ege d’admission est le droit d’un m´edecin de reconnaˆıtre un patient `a l’hˆopital ou un centre m´edical d’un diagnostic ou de prendre un diagnostic en fonction de son ´etat de sant´e.  RNNsearch-50 Un privil`ege d’admission est le droit d’un m´edecin d’admettre un patient `a un hˆopital ou un centre m´edical pour effectuer un diagnostic ou une proc´edure, selon son statut de travailleur des soins de sant´e `a l’hˆopital. Un privil`ege admettre est le droit d’un m´edecin d’admettre un patient dans un hˆopital ou un centre m´edical pour effectuer un diagnostic ou une proc´edure, fond´ee sur sa situation en tant que travailleur de soins de sant´e dans un hˆopital.  Google Translate  Source  Reference  RNNenc-50  RNNsearch-50  Google Translate  Source  Reference  RNNenc-50  RNNsearch-50  Google Translate  This kind of experience is part of Disney’s efforts to ”extend the lifetime of its series and build new relationships with audiences via digital platforms that are becoming ever more important,” he added. Ce type d’exp´erience entre dans le cadre des efforts de Disney pour ”´etendre la dur´ee de vie de ses s´eries et construire de nouvelles relations avec son public grˆace `a des plateformes num´eriques qui sont de plus en plus importantes”, a-t-il ajout´e. Ce type d’exp´erience fait partie des initiatives du Disney pour ”prolonger la dur´ee de vie de ses nouvelles et de d´evelopper des liens avec les lecteurs num´eriques qui deviennent plus com- plexes. Ce genre d’exp´erience fait partie des efforts de Disney pour ”prolonger la dur´ee de vie de ses s´eries et cr´eer de nouvelles relations avec des publics via des plateformes num´eriques de plus en plus importantes”, a-t-il ajout´e. Ce genre d’exp´erience fait partie des efforts de Disney `a “´etendre la dur´ee de vie de sa s´erie et construire de nouvelles relations avec le public par le biais des plates-formes num´eriques qui deviennent de plus en plus important”, at-il ajout´e.  In a press conference on Thursday, Mr Blair stated that there was nothing in this video that might constitute a ”reasonable motive” that could lead to criminal charges being brought against the mayor. En conf´erence de presse, jeudi, M. Blair a afﬁrm´e qu’il n’y avait rien dans cette vid´eo qui puisse constituer des ”motifs raisonnables” pouvant mener au d´epˆot d’une accusation criminelle contre le maire. Lors de la conf´erence de presse de jeudi, M. Blair a dit qu’il n’y avait rien dans cette vid´eo qui pourrait constituer une ”motivation raisonnable” pouvant entraˆıner des accusations criminelles port´ees contre le maire. Lors d’une conf´erence de presse jeudi, M. Blair a d´eclar´e qu’il n’y avait rien dans cette vid´eo qui pourrait constituer un ”motif raisonnable” qui pourrait conduire `a des accusations criminelles contre le maire. Lors d’une conf´erence de presse jeudi, M. Blair a d´eclar´e qu’il n’y avait rien dans cette vido qui pourrait constituer un ”motif raisonnable” qui pourrait mener `a des accusations criminelles portes contre le maire.  Table 3: The translations generated by RNNenc-50 and RNNsearch-50 from long source sentences (30 words or more) selected from the test set. For each source sentence, we also show the gold- standard translation. The translations by Google Translate were made on 27 August 2014.  15  ",
1412.6550,2015,FitNets: Hints for Thin Deep Nets,"['FitNets: Hints for Thin Deep Nets', 'Adriana Romero', 'Nicolas Ballas', 'Samira Ebrahimi Kahou', 'Antoine Chassang', 'Carlo Gatta', 'and Yoshua Bengio']",https://arxiv.org/pdf/1412.6550,"5 1 0 2    r a     M 7 2      ]  G L . s c [      4 v 0 5 5 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  FITNETS: HINTS FOR THIN DEEP NETS  Adriana Romero1, Nicolas Ballas2, Samira Ebrahimi Kahou3, Antoine Chassang2, Carlo Gatta4 & Yoshua Bengio2† 1Universitat de Barcelona, Barcelona, Spain. 2Universit´e de Montr´eal, Montr´eal, Qu´ebec, Canada. †CIFAR Senior Fellow. 3 ´Ecole Polytechnique de Montr´eal, Montr´eal, Qu´ebec, Canada. 4Centre de Visi´o per Computador, Bellaterra, Spain.  ABSTRACT  While depth tends to improve network performances, it also makes gradient-based training more difﬁcult since deeper networks tend to be more non-linear. The re- cently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate represen- tations learned by the teacher as hints to improve the training process and ﬁnal performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher’s intermediate hidden layer, additional pa- rameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.  1  INTRODUCTION  Deep networks have recently exhibited state-of-the-art performance in computer vision tasks such as image classiﬁcation and object detection (Simonyan & Zisserman, 2014; Szegedy et al., 2014). However, top-performing systems usually involve very wide and deep networks, with numerous parameters. Once learned, a major drawback of such wide and deep models is that they result in very time consuming systems at inference time, since they need to perform a huge number of multiplications. Moreover, having large amounts of parameters makes the models high memory demanding. For these reasons, wide and deep top-performing networks are not well suited for applications with memory or time limitations.  There have been several attempts in the literature to tackle the problem of model compression to reduce the computational burden at inference time. In Bucila et al. (2006), authors propose to train a neural network to mimic the output of a complex and large ensemble. The method uses the ensem- ble to label unlabeled data and trains the neural network with the data labeled by the ensemble, thus mimicking the function learned by the ensemble and achieving similar accuracy. The idea has been recently adopted in Ba & Caruana (2014) to compress deep and wide networks into shallower but even wider ones, where the compressed model mimics the function learned by the complex model, in this case, by using data labeled by a deep (or an ensemble of deep) networks. More recently, Knowl- edge Distillation (KD) (Hinton & Dean, 2014) was introduced as a model compression framework, which eases the training of deep networks by following a student-teacher paradigm, in which the student is penalized according to a softened version of the teacher’s output. The framework com- presses an ensemble of deep networks (teacher) into a student network of similar depth. To do so, the student is trained to predict the output of the teacher, as well as the true classiﬁcation labels. All previous works related to Convolutional Neural Networks focus on compressing a teacher network or an ensemble of networks into either networks of similar width and depth or into shallower and wider ones; not taking advantage of depth.  1  Published as a conference paper at ICLR 2015  Depth is a fundamental aspect of representation learning, since it encourages the re-use of features, and leads to more abstract and invariant representations at higher layers (Bengio et al., 2013). The importance of depth has been veriﬁed (1) theoretically: deep representations are exponentially more expressive than shallow ones for some families of functions (Montufar et al., 2014); and (2) empiri- cally: the two top-performers of ImageNet use deep convolutional networks with 19 and 22 layers, respectively (Simonyan & Zisserman, 2014) and (Szegedy et al., 2014).  Nevertheless, training deep architectures has proven to be challenging (Larochelle et al., 2007; Erhan et al., 2009), since they are composed of successive non-linearities and, thus result in highly non-convex and non-linear functions. Signiﬁcant effort has been devoted to alleviate this optimiza- tion problem. On the one hand, pre-training strategies, whether unsupervised (Hinton et al., 2006; Bengio et al., 2007) or supervised (Bengio et al., 2007) train the network parameters in a greedy lay- erwise fashion in order to initialize the network parameters in a potentially good basin of attraction. The layers are trained one after the other according to an intermediate target. Similarly, semi- supervised embedding (Weston et al., 2008) provides guidance to an intermediate layer to help learn very deep networks. Along this line of reasoning, (Cho et al., 2012) ease the optimization problem of DBM by borrowing the activations of another model every second layer in a purely unsupervised scenario. More recently, (Chen-Yu et al., 2014; Szegedy et al., 2014; Gulcehre & Bengio, 2013) showed that adding supervision to intermediate layers of deep architectures assists the training of deep networks. Supervision is introduced by stacking a supervised MLP with a softmax layer on top of intermediate hidden layers to ensure their discriminability w.r.t. labels. Alternatively, Curriculum Learning strategies (CL) (Bengio, 2009) tackle the optimization problem by modifying the training distribution, such that the learner network gradually receives examples of increasing and appropriate difﬁculty w.r.t. the already learned concepts. As a result, curriculum learning acts like a continuation method, speeds up the convergence of the training process and ﬁnds potentially better local minima of highly non-convex cost functions.  In this paper, we aim to address the network compression problem by taking advantage of depth. We propose a novel approach to train thin and deep networks, called FitNets, to compress wide and shallower (but still deep) networks. The method is rooted in the recently proposed Knowledge Dis- tillation (KD) (Hinton & Dean, 2014) and extends the idea to allow for thinner and deeper student models. We introduce intermediate-level hints from the teacher hidden layers to guide the training process of the student, i.e., we want the student network (FitNet) to learn an intermediate repre- sentation that is predictive of the intermediate representations of the teacher network. Hints allow the training of thinner and deeper networks. Results conﬁrm that having deeper models allow us to generalize better, whereas making these models thin help us reduce the computational burden sig- niﬁcantly. We validate the proposed method on MNIST, CIFAR-10, CIFAR-100, SVHN and AFLW benchmark datasets and provide evidence that our method matches or outperforms the teacher’s performance, while requiring notably fewer parameters and multiplications.  2 METHOD  In this section, we detail the proposed student-teacher framework to train FitNets from shallower and wider nets. First, we review the recently proposed KD. Second, we highlight the proposed hints algorithm to guide the FitNet throughout the training process. Finally, we describe how the FitNet is trained in a stage-wise fashion.  2.1 REVIEW OF KNOWLEDGE DISTILLATION  In order to obtain a faster inference, we explore the recently proposed compression framework (Hinton & Dean, 2014), which trains a student network, from the softened output of an ensemble of wider networks, teacher network. The idea is to allow the student network to capture not only the information provided by the true labels, but also the ﬁner structure learned by the teacher network. The framework can be summarized as follows. Let T be a teacher network with an output softmax PT = softmax(aT ) where aT is the vector of teacher pre-softmax activations, for some example. In the case where the teacher model is a single network, aT represents the weighted sums of the output layer, whereas if the teacher model is the result of an ensemble either PT or aT are obtained by averaging outputs from different networks (respectively for arithmetic or geometric averaging). Let S be a student network with parameters  2  Published as a conference paper at ICLR 2015  WS and output probability PS = softmax(aS), where aS is the student’s pre-softmax output. The student network will be trained such that its output PS is similar to the teacher’s output PT, as well as to the true labels ytrue. Since PT might be very close to the one hot code representation of the sample’s true label, a relaxation τ > 1 is introduced to soften the signal arising from the output of the teacher network, and thus, provide more information during training1. The same relaxation S), when it is compared to the teacher’s softened is applied to the output of the student network (Pτ output (Pτ  T):  τ (cid:17) . The student network is then trained to optimize the following loss function:  T = softmax(cid:16) aT Pτ  S = softmax(cid:16) aS  τ (cid:17) , Pτ  (1)  LKD(WS) = H(ytrue, PS) + λH(Pτ  (2) where H refers to the cross-entropy and λ is a tunable parameter to balance both cross-entropies. Note that the ﬁrst term in Eq. (2) corresponds to the traditional cross-entropy between the output of a (student) network and labels, whereas the second term enforces the student network to learn from the softened output of the teacher network.  T, Pτ  S),  To the best of our knowledge, KD is designed such that student networks mimic teacher architectures of similar depth. Although we found the KD framework to achieve encouraging results even when student networks have slightly deeper architectures, as we increase the depth of the student network, KD training still suffers from the difﬁculty of optimizing deep nets (see Section 4.1).  2.2 HINT-BASED TRAINING  In order to help the training of deep FitNets (deeper than their teacher), we introduce hints from the teacher network. A hint is deﬁned as the output of a teacher’s hidden layer responsible for guiding the student’s learning process. Analogously, we choose a hidden layer of the FitNet, the guided layer, to learn from the teacher’s hint layer. We want the guided layer to be able to predict the output of the hint layer. Note that having hints is a form of regularization and thus, the pair hint/guided layer has to be chosen such that the student network is not over-regularized. The deeper we set the guided layer, the less ﬂexibility we give to the network and, therefore, FitNets are more likely to suffer from over-regularization. In our case, we choose the hint to be the middle layer of the teacher network. Similarly, we choose the guided layer to be the middle layer of the student network.  Given that the teacher network will usually be wider than the FitNet, the selected hint layer may have more outputs than the guided layer. For that reason, we add a regressor to the guided layer, whose output matches the size of the hint layer. Then, we train the FitNet parameters from the ﬁrst layer up to the guided layer as well as the regressor parameters by minimizing the following loss function:  LHT (WGuided, Wr) =  1 2  ||uh(x; WHint) − r(vg(x; WGuided); Wr)||2,  (3)  where uh and vg are the teacher/student deep nested functions up to their respective hint/guided layers with parameters WHint and WGuided, r is the regressor function on top of the guided layer with parameters Wr. Note that the outputs of uh and r have to be comparable, i.e., uh and r must be the same non-linearity.  Nevertheless, using a fully-connected regressor increases the number of parameters and the memory consumption dramatically in the case where the guided and hint layers are convolutional. Let Nh,1 × Nh,2 and Oh be the teacher hint’s spatial size and number of channels, respectively. Similarity, let Ng,1 × Ng,2 and Og be the FitNet guided layer’s spatial size and number of channels. The number of parameters in the weight matrix of a fully connected regressor is Nh,1×Nh,2×Oh×Ng,1×Ng,2×Og. To mitigate this limitation, we use a convolutional regressor instead. The convolutional regressor is designed such that it considers approximately the same spatial region of the input image as the teacher hint. Therefore, the output of the regressor has the same spatial size as the teacher hint. Given a teacher hint of spatial size Nh,1 × Nh,2, the regressor takes the output of the Fitnet’s guided  1For example, as argued by Hinton & Dean (2014), with softened outputs, more information is provided  about the relative similarity of the input to classes other than the one with the highest probability.  3  Published as a conference paper at ICLR 2015  Teacher Network  FitNet  W  T  W  Hint  L  W  T .  .  .  W  h  T  .  .  .  W  2  T  W W  1 1  T T  W  S  W  M  S  .  .  .  g  W S  W  . Guided .  .  W  2  S  1  W S  W*  Guided   =   (W  Guided, W  r)  =   W* S  (W  )  S  W  r  W  Hint  W  Guided  W T  W  S  W*  Guided  (a) Teacher and Student Networks  (b) Hints Training  (c) Knowledge Distillation  Figure 1: Training a student network using hints.  layer of size Ng,1 × Ng,2 and adapts its kernel shape k1 × k2 such that Ng,i − ki + 1 = Nh,i, where i ∈ {1, 2}. The number of parameters in the weight matrix of a the convolutional regressor is k1 × k2 × Oh × Og, where k1 × k2 is signiﬁcantly lower than Nh,1 × Nh,2 × Ng,1 × Ng,2.  2.3 FITNET STAGE-WISE TRAINING  We train the FitNet in a stage-wise fashion following the student/teacher paradigm. Figure 1 sum- marizes the training pipeline. Starting from a trained teacher network and a randomly initialized FitNet (Fig. 1 (a)), we add a regressor parameterized by Wr on top of the FitNet guided layer and train the FitNet parameters WGuided up to the guided layer to minimize Eq. (3) (see Fig. 1 (b)). Finally, from the pre-trained parameters, we train the parameters of whole FitNet WS to minimize Eq. (2) (see Fig. 1 (c)). Algorithm 1 details the FitNet training process.  Algorithm 1 FitNet Stage-Wise Training. The algorithm receives as input the trained parameters WT of a teacher, the randomly initialized parameters WS of a FitNet, and two indices h and g corresponding to hint/guided layers, respec- tively. Let WHint be the teacher’s parameters up to the hint layer h. Let WGuided be the FitNet’s parameters up to the guided layer g. Let Wr be the regressor’s parameters. The ﬁrst stage consists in pre-training the student network up to the guided layer, based on the prediction error of the teacher’s hint layer (line 4). The second stage is a KD training of the whole network (line 6).  Input: WS, WT, g, h Output: W∗  S  1, . . . , WT  1: WHint ← {WT 2: WGuided ← {WS 3: Intialize Wr to small random values 4: W∗  h} 1, . . . , WS  Guided ← argmin WGuided  g}  LHT (WGuided, Wr)  5: {WS 6: W∗  1, . . . , WS S ← argmin  g} ← {WGuided LKD(WS)  ∗1, . . . , WGuided  ∗g}  WS  2.4 RELATION TO CURRICULUM LEARNING  In this section, we argue that our hint-based training with KD can be seen as a particular form of Curriculum Learning (Bengio, 2009). Curriculum learning has proven to accelerate the training convergence as well as potentially improve the model generalization by properly choosing a se- quence of training distributions seen by the learner: from simple examples to more complex ones. A curriculum learning extension (Gulcehre & Bengio, 2013) has also shown that by using guidance hints on an intermediate layer during the training, one could considerably ease training. However, Bengio (2009) uses hand-deﬁned heuristics to measure the “simplicity” of an example in a sequence and Gulcehre & Bengio (2013)’s guidance hints require some prior knowledge of the end-task. Both of these curriculum learning strategies tend to be problem-speciﬁc.  Our approach alleviates this issue by using a teacher model. Indeed, intermediate representations learned by the teacher are used as hints to guide the FitNet optimization procedure. In addition, the teacher conﬁdence provides a measure of example “simplicity” by means of teacher cross-entropy  4  Published as a conference paper at ICLR 2015  term in Eq. (2). This term ensures that examples with a high teacher conﬁdence have a stronger impact than examples with low teacher conﬁdence: the latter correspond to probabilities closer to the uniform distribution, which exert less of a push on the student parameters. In other words, the teacher penalizes the training examples according to its conﬁdence. Note that parameter λ in Eq. (2) controls the weight given to the teacher cross-entropy, and thus, the importance given to each example. In order to promote the learning of more complex examples (examples with lower teacher conﬁdence), we gradually anneal λ during the training with a linear decay. The curriculum can be seen as composed of two stages: ﬁrst learn intermediate concepts via the hint/guided layer transfer, then train the whole student network jointly, annealing λ, which allows easier examples (on which the teacher is very conﬁdent) to initially have a stronger effect, but progressively decreasing their importance as λ decays. Therefore, the hint-based training introduced in the paper is a generic curriculum learning approach, where prior information about the task-at-hand is deduced purely from the teacher model.  # params  Accuracy  Algorithm Compression  FitNet Teacher  Mimic single Mimic single  ∼2.5M ∼9M ∼54M ∼70M ∼70M  Mimic ensemble State-of-the-art methods Maxout Network in Network Deeply-Supervised Networks Deeply-Supervised Networks (19)  91.61% 90.18% 84.6% 84.9% 85.8%  90.65% 91.2% 91.78% 88.2%  Algorithm Compression  # params  Accuracy  FitNet Teacher  ∼2.5M ∼9M State-of-the-art methods Maxout Network in Network Deeply-Supervised Networks  64.96% 63.54%  61.43% 64.32% 65.43%  Table 1: Accuracy on CIFAR-10  Table 2: Accuracy on CIFAR-100  3 RESULTS ON BENCHMARK DATASETS  In this section, we show the results on several benchmark datasets2. The architectures of all networks as well as the training details are reported in the supplementary material.  3.1 CIFAR-10 AND CIFAR-100  The CIFAR-10 and CIFAR-100 datasets (Krizhevsky & Hinton, 2009) are composed of 32x32 pixel RGB images belonging to 10 and 100 different classes, respectively. They both contain 50K training images and 10K test images. CIFAR-10 has 1000 samples per class, whereas CIFAR-100 has 100 samples per class. Like Goodfellow et al. (2013b), we normalized the datasets for contrast normal- ization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional lay- ers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 maxout convolutional layers, followed by a maxout fully-connected layer and a top softmax layer, with roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random ﬂipping during training. Table 1 summarizes the obtained results. Our student model outperforms the teacher model, while requiring notably fewer parameters, suggesting that depth is crucial to achieve better representations. When compared to network compression methods, our algorithm achieves outstanding results; i.e., the student network achieves an accuracy of 91.61%, which is signiﬁcantly higher than the top-performer 85.8% of Ba & Caruana (2014), while requir- ing roughly 28 times fewer parameters. When compared to state-of-the-art methods, our algorithm matches the best performers.  One could argue the choice of hinting the inner layers with the hidden state of a wide teacher net- work. A straightforward alternative would be to hint them with the desired output. This could be addressed in a few different ways: (1) Stage-wise training, where stage 1 optimizes the 1st half of the network w.r.t. classiﬁcation targets and stage 2 optimizes the whole network w.r.t. classiﬁcation  2Code to reproduce the experiments publicly available: https://github.com/adri-romsor/FitNets  5  Published as a conference paper at ICLR 2015  targets. In this case, stage 1 set the network parameters in a good local minima but such initializa- tion did not seem to help stage 2 sufﬁciently, which failed to learn. To further assist the training of the thin and deep student network, we could add extra hints with the desired output at different hidden layers. Nevertheless, as observed in (Bengio et al., 2007), with supervised pre-training the guided layer may discard some factors from the input, which require more layers and non-linearity before they can be exploited to predict the classes. (2) Stage-wise training with KD, where stage 1 optimizes the 1st half of the net w.r.t. classiﬁcation targets and stage 2 optimizes the whole network w.r.t. Eq. (2). As in the previous case, stage 1 set the network parameters in a good local minima but such initialization did not seem to help stage 2 sufﬁciently, which failed to learn. (3) Jointly optimizing both stages w.r.t. the sum of the supervised hint for the guided layer and classiﬁcation target for the output layer. We performed this experiment, tried different initializations and learning rates with RMSprop (Tieleman & Hinton, 2012) but we could not ﬁnd any combination to make the network learn. Note that we could ease the training by adding hints to each layer and optimizing jointly as in Deeply Supervised Networks (DSN). Therefore, we built the above-mentioned 19-layer architecture and trained it by means of DSN, achieving a test performance of 88.2%, which is sig- niﬁcantly lower than the performance obtained by the FitNets hint-based training (91.61%). Such result suggests that using a very discriminative hint w.r.t. classiﬁcation at intermediate layers might be too aggressive; using a smoother hint (such as the guidance from a teacher network) offers better generalization. (4) Jointly optimizing both stages w.r.t. the sum of supervised hint for the guided layer and Eq. (2) for the output layer. Adding supervised hints to the middle layer of the network did not ease the training of such a thin and deep network, which failed to learn. CIFAR-100: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and used the same FitNet architecture as in CIFAR-10. As in Chen-Yu et al. (2014), we augmented the data with random ﬂipping during training. Table 2 summarizes the obtained results. As in the previous case, our FitNet outperforms the teacher model, reducing the number of parameters by a factor of 3 and, when compared to state-of-the-art methods, the FitNet provides near state-of-the-art performance.  3.2 SVHN  The SVHN dataset (Netzer et al., 2011) is composed by 32 × 32 color images of house numbers collected by GoogleStreet View. There are 73,257 images in the training set, 26,032 images in the test set and 531,131 less difﬁcult examples. We follow the evaluation procedure of Goodfellow et al. (2013b) and use their maxout network as teacher. We trained a 13-layer FitNet composed of 11 maxout convolutional layers, a fully-connected layer and a softmax layer.  Algorithm Compression  # params  Misclass  FitNet Teacher  ∼1.5M ∼4.9M State-of-the-art methods Maxout Network in Network Deeply-Supervised Networks  2.42% 2.38%  2.47% 2.35% 1.92%  Algorithm  # params Misclass  Compression Teacher  Standard backprop  KD FitNet  ∼361K ∼30K ∼30K ∼30K  State-of-the-art methods Maxout Network in Network Deeply-Supervised Networks  0.55% 1.9% 0.65% 0.51%  0.45% 0.47% 0.39%  Table 3: SVHN error  Table 4: MNIST error  Table 3 shows that our FitNet achieves comparable accuracy than the teacher despite using only 32% of teacher capacity. Our FitNet is comparable in terms of performance to other state-of-art methods, such as Maxout and Network in Network.  3.3 MNIST  As a sanity check for the training procedure, we evaluated the proposed method on the MNIST dataset (LeCun et al., 1998). MNIST is a dataset of handwritten digits (from 0 to 9) composed of 28x28 pixel greyscale images, with 60K training images and 10K test images. We trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a  6  Published as a conference paper at ICLR 2015  FitNet twice as deep as the teacher network and with roughly 8% of the parameters. The 4th layer of the student network was trained to mimic the 2nd layer of the teacher network.  Table 4 reports the obtained results. To verify the inﬂuence of using hints, we trained the FitNet architecture using either (1) standard backprop (w.r.t. classiﬁcation labels), (2) KD or (3) Hint- based Training (HT). When training the FitNet with standard backprop from the softmax layer, the deep and thin architecture achieves 1.9% misclassiﬁcation error. Using KD, the very same network achieves 0.65%, which conﬁrms the potential of the teacher network; and when adding hints, the error still decreases to 0.51%. Furthermore, the student network achieves slightly better results than the teacher network, while requiring 12 times less parameters.  3.4 AFLW  AFLW (Koestinger et al., 2011) is a real-world face database, containing 25K annotated images. In order to evaluate the proposed framework in a face recognition setting, we extracted positive samples by re-sizing the annotated regions of the images to ﬁt 16x16 pixels patches. Similarly, we extracted 25K 16x16 pixels patches not containing faces from ImageNet (Russakovsky et al., 2014) dataset, as negative samples. We used 90% of the extracted patches to train the network. In this experiment, we aimed to evaluate the method on a different kind of architecture. Therefore, we trained a teacher network of 3 ReLU convolutional layers and a sigmoid output layer. We de- signed a ﬁrst FitNet (FitNet 1) with 15 times fewer multiplications than the teacher network, and a second FitNet (FitNet 2) with 2.5 times fewer multiplications than the teacher network. Both FitNets have 7 ReLU convolutional layers and a sigmoid output layer.  The teacher network achieved 4.21% misclassiﬁcation error on the validation set. We trained both FitNets by means of KD and HT. On the one hand, we report a misclassiﬁcation error of 4.58% when training FitNet 1 with KD and a misclassiﬁcation error of when 2.55% when training it with HT. On the other hand, we report a missclassifation error of 1.95% when training FitNet 2 with KD and a misclassiﬁcation error of 1.85% when training it with HT. These results show how the method is extensible to different kind of architectures and highlight the beneﬁts of using hints, especially when dealing with thinner architectures.  4 ANALYSIS OF EMPIRICAL RESULTS  We empirically investigate the beneﬁts of our approach by comparing various networks trained us- ing standard backpropagation (cross-entropy w.r.t. labels), KD or Hint-based Training (HT). Exper- iments are performed on CIFAR-10 dataset (Krizhevsky & Hinton, 2009).  We compare networks of increasing depth given a ﬁxed computational budget. Each network is com- posed of successive convolutional layers of kernel size 3 × 3, followed by a maxout non-linearity and a non-overlapping 2 × 2 max-pooling. The last max-pooling takes the maximum over all re- maining spatial dimensions leading to a 1 × 1 vector representation. We only change the depth and the number of channels per convolution between different networks, i.e. the number of channels per convolutional layer decreases as a network depth increases to respect a given computational budget.  4.1 ASSISTING THE TRAINING OF DEEP NETWORKS  In this section, we investigate the impact of HT. We consider two computational budgets of ap- proximately 30M and 107M operations, corresponding to the multiplications needed in an image forward propagation. For each computational budget, we train networks composed of 3, 5, 7 and 9 convolutional layers, followed by a fully-connected layer and a softmax layer. We compare their performances when they are trained with standard backpropagation, KD and HT. Figure 2 reports test on CIFAR-10 using early stopping on the validation set, i.e. we do not retrain our models on the training plus validation sets.  Due to their depth and small capacity, FitNets are hard to train. As shown in Figure 2(a), we could not train 30M multiplications networks with more than 5 layers with standard backprop. When using KD, we succesfully trained networks up to 7 layers. Adding KD’s teacher cross-entropy to the training objective (Eq. (2)) gives more importance to easier examples, i.e. samples for which the teacher network is conﬁdent and, can lead to a smoother version of the training cost (Bengio, 2009). Despite some optimization beneﬁts, it is worth noticing that KD training still suffers from  7  Published as a conference paper at ICLR 2015  y c a r u c c A   t s e T  88.5  88.0  87.5  87.0  86.5  86.0  85.5  85.0  84.5  84.0  83.5  83.0  Back Propagation Knowledge Distillation Hint Training  5  7 9 Number of Layers  11  (a) 30M Multiplications  y c a r u c c A   t s e T  91.0  90.5  90.0  89.5  89.0  88.5  88.0  87.5  87.0  86.5  86.0  85.5  Back Propagation Knowledge Distillation Hint Training  5  7 9 Number of Layers  11  (b) 107M Multiplications  Figure 2: Comparison of Standard Back-Propagation, Knowledge Distillation and Hint-based Train- ing on CIFAR-10.  Network # layers # params # mult Teacher FitNet 1 FitNet 2 FitNet 3 FitNet 4  ∼9M ∼250K ∼862K ∼1.6M ∼2.5M  ∼725M 90.18% ∼30M 89.01% ∼108M 91.06% ∼392M 91.10% ∼382M 91.61%  5 11 11 13 19  1  13.36 4.64 1.37 1.52  1 36  10.44 5.62 3.60  Acc  Speed-up Compression rate  Table 5: Accuracy/Speed Trade-off on CIFAR-10.  the increasing depth and reaches its limits for 7-layer networks. HT tends to ease these optimization issues and is able to train 13-layer networks of 30M multiplications. The only difference between HT and KD is the starting point in the parameter space: either random or obtained by means of the teacher’s hint. On the one hand, the proliferation of local minima and especially saddle points in highly non-linear functions such as very deep networks highlights the difﬁculty of ﬁnding a good starting point in the parameter space at random (Dauphin et al., 2014). On the other hand, results in Figure 2(a) indicate that HT can guide the student to a better initial position in the parameter space, from which we can minimize the cost through stochastic gradient descent. Therefore, HT provides beneﬁts from an optimization point of view. Networks trained with HT also tend to yield better test performances than the other training methods when we ﬁx the capacity and number of layers. For instance, in Figure 2(b), the 7-layers network, trained with hints, obtains a +0.7% performance gain on the test set compared to the model that does not use any hints (the accuracy increases from 89.45% to 90.1%). As pointed by Erhan et al. (2009), pre-training strategies can act as regularizers. These results suggest that HT is a stronger regularizer than KD, since it leads to better generalization performance on the test set. Finally, Figure 2 highlights that deep models have better performances than shallower ones given a ﬁxed computational budget. Indeed, considering networks that are trained with hints, an 11-layer network outperforms a 5-layer network by an absolute improvement of 4.11% for 107M multiplications and of 3.4% for 30M multiplications. Therefore, the experiments validate our hypothesis that given a ﬁxed number of computations, we leverage depth in a model to achieve faster computation and better generalization.  In summary, this experiment shows that (1) using HT, we are able to train deeper models than with standard back-propagation and KD; and (2) given a ﬁxed capacity, deeper models performed better than shallower ones.  4.2 TRADE-OFF BETWEEN MODEL PERFORMANCE AND EFFICIENCY  To evaluate FitNets efﬁciency, we measure their total inference times required for processing CIFAR-10 test examples on a GPU as well as their parameter compression. Table 5 reports both the speed-up and compression rate obtained by various FitNets w.r.t. the teacher model along with their number of layers, capacity and accuracies. In this experiment, we retrain our FitNets on train- ing plus validation sets as in Goodfellow et al. (2013b), for fair comparison with the teacher.  8  Published as a conference paper at ICLR 2015  FitNet 1, our smallest network, with 36× less capacity than the teacher, is one order of magnitude faster than the teacher and only witnesses a minor performance decrease of 1.3%. FitNet 2, slightly increasing the capacity, outperforms the teacher by 0.9%, while still being faster by a strong 4.64 factor. By further increasing network capacity and depth in FitNets 3 and 4, we improve the perfor- mance gain, up to 1.6%, and still remain faster than the teacher. Although a trade-off between speed and accuracy is introduced by the compression rate, FitNets tend to be signiﬁcantly faster, matching or outperforming their teacher, even when having low capacity.  A few works such as matrix factorization (Jaderberg et al., 2014; Denton et al., 2014) focus on speeding-up deep networks’ convolutional layers at the expense of slightly deteriorating their per- formance. Such approaches are complementary to FitNets and could be used to further speed-up the FitNet’s convolutional layers.  Other works related to quantization schemes (Chen et al., 2010; J´egou et al., 2011; Gong et al., 2014) aim at reducing storage requirements. Unlike FitNets, such approaches witness a little de- crease in performance when compressing the network parameters. Exploiting depth allows FitNets to obtain performance improvements w.r.t. their teachers, even when reducing the number of param- eters 10×. However, we believe that quantization approaches are also complementary to FitNets and could be used to further reduce the storage requirements. It would be interesting to compare how much redundancy is present in the ﬁlters of the teacher networks w.r.t. the ﬁlters of the FitNet and, therefore, how much FitNets ﬁlters could be compressed without witnessing signiﬁcant performance drop. This analysis is out of the scope of the paper and is left as future work.  5 CONCLUSION  We proposed a novel framework to compress wide and deep networks into thin and deeper ones, by introducing intermediate-level hints from the teacher hidden layers to guide the training process of the student. We are able to use these hints to train very deep student models with less parameters, which can generalize better and/or run faster than their teachers. We provided empirical evidence that hinting the inner layers of a thin and deep network with the hidden state of a teacher network generalizes better than hinting them with the classiﬁcation targets. Our experiments on benchmark datasets emphasize that deep networks with low capacity are able to extract feature representations that are comparable or even better than networks with as much as 10 times more parameters. The hint-based training suggests that more efforts should be devoted to explore new training strategies to leverage the power of deep networks.  ACKNOWLEDGMENTS We thank the developers of Theano (Bastien et al., 2012) and Pylearn2 (Goodfellow et al., 2013a) and the computational resources provided by Compute Canada and Calcul Qu´ebec. This work has been partially supported by NSERC, CIFAR, and Canada Research Chairs, Project TIN2013-41751, grant 2014-SGR-221 and Spanish MINECO grant TIN2012-38187-C03.  REFERENCES Ba, J. and Caruana, R. Do deep nets really need to be deep? In NIPS, pp. 2654–2662. 2014.  Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Warde-Farley, D., and Bengio, Y. Theano: new features and speed improvements. Deep Learning & Unsupervised Feature Learning Workshop, NIPS, 2012.  Bengio, Y. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2009.  Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. Greedy layer-wise training of deep networks. In  NIPS, pp. 153–160, 2007.  Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. TPAMI,  2013.  Bucila, C., Caruana, R., and Niculescu-Mizil, A. Model compression. In KDD, pp. 535–541, 2006.  Chen, Yongjian, Guan, Tao, and Wang, Cheng. Approximate nearest neighbor search by residual vector quan-  tization. Sensors, 10(12):11259–11273, 2010.  9  Published as a conference paper at ICLR 2015  Chen-Yu, L., Saining, X., Patrick, G., Zhengyou, Z., and Zhuowen, T. Deeply-supervised nets. CoRR,  abs/1409.5185, 2014.  Cho, Kyunghyun, Raiko, Tapani, Ilin, Alexander, and Karhunen, Juha. A two-stage pretraining algorithm for deep Boltzmann machines. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2012.  Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. Identifying and attacking the  saddle point problem in high-dimensional non-convex optimization. In NIPS, 2014.  Denton, Emily L, Zaremba, Wojciech, Bruna, Joan, LeCun, Yann, and Fergus, Rob. Exploiting linear structure  within convolutional networks for efﬁcient evaluation. In NIPS, pp. 1269–1277. 2014.  Erhan, D., Manzagol, P.A., Bengio, Y., Bengio, S., and Vincent, P. The difﬁculty of training deep architectures  and the effect of unsupervised pre-training. In AISTATS, pp. 153–160, 2009.  Gong, Yunchao, Liu, Liu, Yang, Min, and Bourdev, Lubomir. Compressing deep convolutional networks using  vector quantization. CoRR, abs/1412.6115, 2014.  Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien,  F., and Bengio, Y. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013a.  Goodfellow, I.J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. Maxout networks.  2013b.  In ICML,  Gulcehre, C. and Bengio, Y. Knowledge matters: Importance of prior information for optimization. In ICLR,  2013.  Hinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning algorithm for deep belief nets. Neural Computation,  18(7):1527–1554, 2006.  Hinton, G. Vinyals, O. and Dean, J. Distilling knowledge in a neural network. In Deep Learning and Repre-  sentation Learning Workshop, NIPS, 2014.  Jaderberg, M., Vedaldi, A., and Zisserman, A. Speeding up convolutional neural networks with low rank  expansions. In BMVC, 2014.  J´egou, Herv´e, Douze, Matthijs, and Schmid, Cordelia. Product quantization for nearest neighbor search. IEEE  TPAMI, 33(1):117–128, 2011.  Koestinger, M., Wohlhart, P., Roth, P.M., and Bischof, H. Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization. In First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies, 2011.  Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Master’s thesis, Depart-  ment of Computer Science, University of Toronto, 2009.  Larochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio, Y. An empirical evaluation of deep architec-  tures on problems with many factors of variation. In ICML, pp. 473–480, 2007.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, November 1998.  Montufar, G.F., Pascanu, R., Cho, K., and Bengio, Y. On the number of linear regions of deep neural networks.  In NIPS. 2014.  Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Reading digits in natural images with unsupervised feature learning. In Deep Learning & Unsupervised Feature Learning Workshop, NIPS, 2011.  Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A.,  Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge, 2014.  Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. CoRR,  abs/1409.1556, 2014.  Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., D.A., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going  deeper with convolutions. CoRR, abs/1409.4842, 2014.  Tieleman, T. and Hinton, G. Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent  magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  Weston, J., Ratle, F., and Collobert, R. Deep learning via semi-supervised embedding. In ICML, 2008.  10  Published as a conference paper at ICLR 2015  A SUPPLEMENTARY MATERIAL: NETWORK ARCHITECTURES AND  TRAINING PROCEDURES  In the supplementary material, we describe all network architectures and hyper-parameters used throughout the paper.  A.1 CIFAR-10/CIFAR-100  In this section, we describe the teacher and FitNet architectures as well as hyper-parameters used in both CIFAR-10/CIFAR-100 experiments.  A.1.1 TEACHERS  We used the CIFAR-10/CIFAR-100 maxout convolutional networks reported in Goodfellow et al. (2013b) as teachers. Both teachers have the same architecture, composed of 3 convolutional hidden layers of 96-192-192 units, respectively. Each convolutional layer is followed by a maxout non- linearity (with 2 linear pieces) and a max-pooling operator with respective windows sizes of 4x4, 4x4 and 2x2 pixels. All max-pooling units have an overlap of 2x2 pixels. The third convolutional layer is followed by a fully-connected maxout layer of 500 units (with 5 linear pieces) and a top softmax layer. The CIFAR-10/CIFAR-100 teachers are trained using stochastic gradient descent and momentum. Please refer to Goodfellow et al. (2013b) for more details.  A.1.2 FITNETS  Here, we describe the FitNet architectures used in the Section 3 and Section 4. Each FitNet is composed of successive zero-padded convolutional layers of kernel size 3 × 3, followed by a maxout non-linearity with two linear pieces. A non-overlapping 2 × 2 max-pooling follows some of the convolutional layers; each network has a total of 3 max-pooling units. The last max-pooling takes the maximum over all remaining spatial dimensions, leading to a 1 × 1 vector representation. The last convolutional layer is followed by a fully-connected and a softmax layer, as the ones on CIFAR- 10/100 teachers.  Table 6 describes the architectures used for the depth experiment in Figure 2. Table 7 describes the architectures for the efﬁciency-performance trade-off experiment in Table 5. The results reported in Table 1, Table 2 and Table 3 correspond to the FitNet 4 architecture.  All FitNet parameters were initialized randomly in U(-0.005,0.005). We used stochastic gradient descent with RMSProp (Tieleman & Hinton, 2012) to train the FitNets, with an initial learning rate 0.005 and a mini-batch size of 128. Parameter λ in Eq. (2) was initialized to 4 and decayed linearly during 500 epochs reaching λ = 1. The relaxation term τ was set to 3. On CIFAR-10, we divided the training set into 40K training examples and 10K validation examples. We trained stage 1 by minimizing Eq. (3) and stopped the training after 100 epochs of no validation error improvement, performing a maximum of 500 epochs. After that, we trained stage 2 by mini- mizing Eq. (2) using RMSprop, the same stopping criterion and the same hyper-parameters as stage 1. We picked the optimal number of epochs according to the above-mentioned stopping criterion and retrained the FitNet on the whole 50K training examples (training + validation sets).  On CIFAR-100, we trained directly on the whole training set using stochastic gradient descent with RMSprop, the same hyper-parameters as CIFAR-10 FitNets and the number of epochs determined by CIFAR-10 stopping criterion.  A.2 MNIST  In this section, we describe the teacher and FitNet architectures as well as the hyper-parameters used in the MNIST experiments.  We trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b). The teacher architecture has three convolutional maxout hidden layers (with 2 linear pieces each) of 48-48-24 units, respectively, followed by a spatial max-pooling of 4x4-4x4-2x2 pixels, with  11  Published as a conference paper at ICLR 2015  5 Layer  7 Layer  9 Layer  11 Layer  conv 3x3x64 (3x3x128)  pool 2x2  conv 3x3x16 (3x3x32) conv 3x3x32 (3x3x64)  pool 2x2  conv 3x3x16 (3x3x32) conv 3x3x32 (3x3x32)  pool 2x2  conv 3x3x64 (3x3x128)  pool 2x2  conv 3x3x32 (3x3x80) conv 3x3x64 (3x3x80)  pool 2x2  conv 3x3x64 (3x3x128)  pool 8x8  conv 3x3x64 (3x3x128)  pool 8x8  conv 3x3x32 (3x3x64) conv 3x3x32 (3x3x80) conv 3x3x32 (3x3x80)  pool 2x2  conv 3x3x48 (3x3x96) conv 3x3x64 (3x3x128)  pool 8x8  fc  softmax hint: 2←2  fc  softmax hint: 4←2  fc  softmax hint: 5←2  conv 3x3x16 (3x3x16) conv 3x3x16 (3x3x32) conv 3x3x16 (3x3x32)  pool 2x2  conv 3x3x32 (3x3x48) conv 3x3x32 (3x3x64) conv 3x3x32 (3x3x80)  pool 2x2  conv 3x3x48 (3x3x96) conv 3x3x48 (3x3x96) conv 3x3x64 (3x3x128)  pool 8x8  fc  softmax hint: 7←2  Table 6: Fitnet architectures with a computational budget of 30M (or 107M) of multiplications: conv sx × sy × c is a convolution of kernel size sx × sy with c outputs channels; pool sx × sy is a non-overlapping pooling of size sx × sy; fc stands for fully connected. hint: FitNet ← teacher speciﬁes the hint and guided layers used for hint-based training, respectively.  FitNet 1  conv 3x3x16 conv 3x3x16 conv 3x3x16  pool 2x2  FitNet 2  conv 3x3x16 conv 3x3x32 conv 3x3x32  pool 2x2  conv 3x3x32 conv 3x3x32 conv 3x3x32  pool 2x2  conv 3x3x48 conv 3x3x64 conv 3x3x80  pool 2x2  FitNet 3  conv 3x3x32 conv 3x3x48 conv 3x3x64 conv 3x3x64  pool 2x2  conv 3x3x80 conv 3x3x80 conv 3x3x80 conv 3x3x80  pool 2x2  conv 3x3x48 conv 3x3x48 conv 3x3x64  pool 8x8  conv 3x3x96 conv 3x3x96 conv 3x3x128  pool 8x8  conv 3x3x128 conv 3x3x128 conv 3x3x128  pool 8x8  fc  softmax hint: 6←2  fc  softmax hint: 6←2  fc  softmax hint: 8←2  FitNet 4  conv 3x3x32 conv 3x3x32 conv 3x3x32 conv 3x3x48 conv 3x3x48  pool 2x2  conv 3x3x80 conv 3x3x80 conv 3x3x80 conv 3x3x80 conv 3x3x80 conv 3x3x80  pool 2x2  conv 3x3x128 conv 3x3x128 conv 3x3x128 conv 3x3x128 conv 3x3x128 conv 3x3x128  pool 8x8  fc  softmax  hint: 11←2  Table 7: Performance-Efﬁciency FitNet architectures.  12  Published as a conference paper at ICLR 2015  an overlap of 2x2 pixels. The 3rd hidden layer is followed by a fully-connected softmax layer. As is Goodfellow et al. (2013b), we added zero padding to the second convolutional layer.  We designed a FitNet twice as deep as the teacher network and with roughly 8% of the parameters. The student architecture has 6 maxout convolutional hidden layers (with 2 linear pieces each) of 16-16-16-16-12-12 units, respectively. Max-pooling is only applied every second layer in regions of 4x4-4x4-2x2 pixels, with an overlap of 2x2 pixels. The 6th convolutional hidden layer is followed by a fully-connected softmax layer.  The teacher network was trained as described in Goodfellow et al. (2013b). The FitNet was trained in a stage-wise fashion as described in Section 2. We divided the training set into a training set of 50K samples and a validation set of 10K samples.  All network parameters where initialized randomly in U(-0.005,0.005). In the ﬁrst stage, the 4th layer of the FitNet was trained to mimic the 2nd layer of the teacher network, by minimizing Eq. (3) through stochastic gradient descent. We used a mini-batch size of 128 samples and ﬁxed the learning rate to 0.0005. We initialized λ to 4 and decayed it for the ﬁrst 150 epochs until it reached 1. The training was stopped according to the following criterion: after 100 epochs of no validation error improvement and performning a maximum of 500 epochs. We used the same mini-batch size, learning rate and stopping criterion to train the second stage. The relaxation term τ was set to 3.  A.3 SVHN  In this section, we describe the teacher and FitNet architectures as well as the hyper-parameters used in the SVHN experiments.  We used SVHN maxout convolutional network described in as Goodfellow et al. (2013b) teacher. The network is composed of 3 convolutional hidden layers of 64-128-128 units, respectively, fol- lowed by a fully-connected maxout layer of 400 units and a top softmax layer. The teacher training was carried out as in Goodfellow et al. (2013b).  We used the FitNet 4 architecture outlined in Table 7, initializing the network parameters randomly in U(-0.005,0.005) and training with the same hyper-parameters as in CIFAR-10. In this case, we used the same early stopping as in CIFAR-10, but we did not retrain the FitNet on the whole training set (training + validation). The same hyper-parameters where used for both stages.  A.4 AFLW  In this section, we describe the teacher and FitNet architectures as well as the hyper-parameters used in the AFLW experiments.  We trained a teacher network of 3 ReLU convolutional layers of 128-512-512 units, respectively, followed by a sigmoid layer. Non-overlapping max-pooling of size 2 × 2 was performed after the ﬁrst convolutional layer. We used receptive ﬁelds of 3-2-5 for each layer, respectively.  We designed two FitNets of 7 ReLU convolutional layers. Fitnet 1’s layers have 16-32-32-32-32- 32-32-32 units, respectively, followed by a sigmoid layer. Fitnet 2’s layers have 32-64-64-64-64- 64-64-64 units, respectively, followed by a sigmoid layer. In both cases, we used receptive ﬁelds of 3 × 3 and, due to the really small image resolution, we did not perform any max-pooling. All network parameters of both FitNets where initialized randomly in U(-0.05,0.05). Both FitNets were trained in the stage-wise fashion described in Section 2. We used 90% of the data for training. In the ﬁrst stage, the 5th layer of the FitNets were trained to mimic the 3rd layer of the teacher network, by minimizing Eq. (3) through stochastic gradient descent. We used a mini-batch size of 128 samples and initialized the learning rate to 0.001 and decayed it for the ﬁrst 100 epochs until reaching 0.01. We also used momentum. We initialized momentum to 0.1 and saturated it to 0.9 at epoch 100. We picked the best validation value after a 500 epochs. We used the same mini-batch size, learning rate and stopping criterion to train the second stage. The relaxation term τ was set to 3.  13  ",
1406.2989,2015,Techniques for Learning Binary Stochastic Feedforward Neural Networks,"['Techniques for Learning Binary Stochastic Feedforward Neural Networks', 'Tapani Raiko', 'Mathias Berglund', 'Guillaume Alain', 'and Laurent Dinh']",https://arxiv.org/pdf/1406.2989,"5 1 0 2    r p A 9         ] L M  . t a t s [      3 v 9 8 9 2  .  6 0 4 1 : v i X r a  Published as a conference paper at ICLR 2015  TECHNIQUES FOR LEARNING BINARY STOCHASTIC FEEDFORWARD NEURAL NETWORKS  Tapani Raiko & Mathias Berglund Department of Information and Computer Science Aalto University Espoo, Finland {tapani.raiko,mathias.berglund}@aalto.fi  Guillaume Alain & Laurent Dinh Department of Computer Science and Operations Research Universit´e de Montr´eal Montr´eal, Canada guillaume.alain.umontreal@gmail.com, dinhlaur@iro.umontreal.ca  ABSTRACT  Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential beneﬁts when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in struc- tured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. However, train- ing stochastic networks is considerably more difﬁcult. We study training using M samples of hidden activations per input. We show that the case M = 1 leads to a fundamentally different behavior where the network tries to avoid stochasticity. We propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms. Our experiments conﬁrm that training stochastic networks is difﬁcult and show that the proposed two estimators perform favorably among all the ﬁve known estimators.  1  INTRODUCTION  Feedforward neural networks, or multi-layer perceptron (MLP) networks, model mappings from inputs x to outputs y through hidden units h. Typically the network output deﬁnes a simple (uni- modal) distribution such as an isotropic Gaussian or a fully factorial Bernoulli distribution. In case the hidden units are deterministic (using a function x → h as opposed to a distribution P (h|x)), the conditionals P (y|x) belong to the same family of simple distributions. Stochastic feedforward neural networks (SFNN) (Neal, 1990; 1992) have the advantage when the conditionals P (y|x) are more complicated. While each conﬁguration of hidden units h produces a simple output, the mixture over them can approximate any distribution, including multimodal distributions required for one-to-many type of mappings. In the extreme case of using empty vectors as the input x, they can be used for unsupervised learning of the outputs y. Another potential advantage of stochastic networks is in generalization performance. Adding noise or stochasticity to the inputs of a deterministic neural network has been found useful as a regular- ization method (Sietsma & Dow, 1991). Introducing multiplicative binary noise to the hidden units (dropout, Hinton et al., 2012) regularizes even better. Binary units have additional advantages in certain settings. For instance conditional computations require hard decisions (Bengio et al., 2013). In addition, some harwdare solutions are restricted to binary outputs (e.g. the IBM SyNAPSE, Esser et al., 2013). The early work on SFNNs approached the inference of h using Gibbs sampling (Neal, 1990; 1992) or mean ﬁeld (Saul et al., 1996), which both have their downsides. Gibbs sampling can mix poorly and  1  Published as a conference paper at ICLR 2015  the mean-ﬁeld approximation can both be inefﬁcient and optimize a lower bound on the likelihood that may be too loose. More recent work proposes simply drawing samples from P (h|x) during the feedforward phase (Hinton et al., 2012; Bengio et al., 2013; Tang & Salakhutdinov, 2013). This guarantees independent samples and an unbiased estimate of P (y|x). We can use standard back-propagation when using stochastic continuous-valued units (e.g. with additive noise or dropout), but back-propagation is no longer possible with discrete units. There are several ways of estimating the gradient in that case. Bengio et al. (2013) proposes two such estimators: an unbiased estimator with a large variance, and a biased version that approximates back-propagation. Tang & Salakhutdinov (2013) propose an unbiased estimator of a lower bound that works reasonably well in a hybrid network containing both deterministic and stochastic units. Their approach relies on using more than one sample from P (y|x) for each training example, and in this paper we provide theory to show that using more than one sample is an important requirement. They also demonstrate interesting applications such as mapping the face of a person into varying expressions, or mapping a silhouette of an object into a color image of the object. Tang & Salakhutdinov (2013) argue for the choice of a hybrid network structure based on the ﬁnite (and thus limited) number of hidden conﬁgurations in a fully discrete h. However, we offer an alternate hypothesis: It is much easier to learn a deterministic network around a small number of stochastic units, so that it might not even be important to train the stochastic units properly. In an extreme case, the stochastic units are not trained at all, and the deterministic units do all the work. In this work, we take a step back and study more rigorously the training problem with fully stochastic networks. We compare different methods for estimating the gradient and propose two new estima- tors. One is an approximate back-propagation with less bias than the one by Bengio et al. (2013), and the other is a modiﬁcation of the estimator by Tang & Salakhutdinov (2013) with less variance. We propose a benchmark test setting based on the well-known MNIST data and the Toronto Face Database.  2 STOCHASTIC FEEDFORWARD NEURAL NETWORKS  (cid:80)  We study a model that maps inputs x to outputs y through stochastic binary hidden units h. The equations are given for just one hidden layer, but the extension to multiple layers is easy1. The acti- vation probability is computed just like the activation function in deterministic multilayer perceptron (MLP) networks:  (1) where Wi: denotes the ith row vector of matrix W and σ(·) is the sigmoid function. For classiﬁca- tion problems, we use softmax for the output probability  P (hi = 1 | x) = σ(ai) = σ(Wi:x + bi),  P (y = i | h) =  exp(Vi:h + ci) j exp(Vj:h + cj)  .  (2)  For predicting binary vectors y, we use a product of Bernoulli distributions  P (yi = 1 | h) = σ(Vi:h + ci).  (3) The probabilistic training criterion for deterministic MLP networks is log P (y|x). Its gradient with respect to model parameters θ = {W, V, b, c} can be computed using the back-propagation al- gorithm, which is based on the chain rule of derivatives. Stochasticity brings difﬁculties in both estimating the training criterion and in estimating the gradient. The training criterion of the stochas- tic network  P (y, h | x) = log  P (y | h)P (h | x)  (4)  (cid:88)  (cid:88)  h  h  (5) requires summation over an exponential number of conﬁgurations of h. Also, derivatives with respect to discrete variables cannot be directly deﬁned. We will review and propose solutions to both problems below.  1Apply P (h|x) separately for each layer such that x denotes the layer below instead of the original input.  2  C = log P (y | x) = log = log EP (h|x)P (y | h)  Published as a conference paper at ICLR 2015  2.1 PROPOSED ESTIMATOR OF THE TRAINING CRITERION  We propose to estimate the training criterion in Equation (4) by  M(cid:88)  1 M  ˆCM = log h(m) ∼ P (h | x).  m=1  P (y | h(m))  (6)  (7) This can be interpreted as the performance of a ﬁnite mixture model over M samples drawn from P (h|x). One could hope that using just M = 1 sample just like in many other stochastic networks (e.g. Hin- ton et al., 2012) would work well enough. However, here we show in that case the network always prefers to minimize the stochasticity, for instance by increasing the input weights to a stochastic sigmoid unit such that it behaves as a deterministic step-function nonlinearity. Theorem 1. When maximizing the expectation of ˆC1 in Equation (6) using M = 1, a hidden unit hi never prefers a stochastic output over a deterministic one. However, when maximizing the ex- pectation of C in Equation (4), the hidden unit hi may prefer a stochastic output over any of the deterministic ones. Proof. The expected ˆC1 over the data distribution can be upper-bounded as  (cid:105)  (cid:104) ˆC1  EPd(x,y)EP (h|x)  (cid:104)EPd(y|x)EP (h\i|x,hi) log P (y | h) (cid:105)  = EPd(x,y)EP (h|x) log P (y | h) = EPd(x)EP (hi|x) = EPd(x)EP (hi|x)f (hi, x) ≤ EPd(x) max  f (hi, x),  hi  (8)  (9)  (10) (11)  where Pd denotes the data distribution. The value in the last inequality is achievable by selecting the distribution of P (hi|x) to be a Dirac delta around the value hi which maximizes the deterministic function f (hi, x). This can be done for every x under the expectation in an independent way. This is analogous to a idea from game theory: since the performance achieved with P (hi|x) is a linear combination of the performances f (hi, x) of the deterministic choices hi, any mixed strategy P (hi|x) cannot be better than the best deterministic choice. Let us now look at the situation for the expectation C and see how it differs from the case of C1 that we had with one particle. We can see that the original training criterion can be written as the expectation of a KL-divergence.  EPd(x,y) [C] = EPd(x)EPd(y|x) log P (y | x)  = EPd(x) [−KL (Pd(y | x)(cid:107)P (y | x)) + const]  (12) (13) The fact that this expression features a negative KL-divergence means that the maximum is achieved when the conditionals match exactly. That is, it it maximized when we have that P (y|x) = Pd(y|x) for each value of x. We give a simple example in which (x, h, y) each take values in {0, 1}. We deﬁne the following conditions on Pd(y|x) and P (y|h), and we show how any deterministic P (h|x) is doing a bad job at maximizing (12).  (cid:20) 0.5  0.5  (cid:21) (cid:21)  Pd(y | x) =  (cid:20)  0.5 0.5  , P (y | h) =  (cid:20) 0.9  0.1  (cid:21)  0.1 0.9  A deterministic P (h | x) =  a b 1 − a 1 − b  is one in which a, b take values in {0, 1}.  Criterion (12) is maximized by (a, b) = (0.5, 0.5), regardless of the distribution Pd(x). For the purposes of comparing solutions, we can simply take Pd(x) = [0.5 0.5]. In that case, we get that the expected C takes the value 0.5 log(0.5) + 0.5 log(0.5) ≈ −0.30. On the other hand, all the deterministic solutions yield a lower value 0.5 log(0.9) + 0.5 log(0.1) ≈ −0.52.  3  Published as a conference paper at ICLR 2015  2.2 GRADIENT FOR TRAINING P (y|h)  We will be exploring ﬁve different estimators for the gradient of a training criterion wrt. parameters θ. However, all of them will share the following gradient for training P (y|h). Let o = Vh + c be the incoming signal to the activation function φ(·) in the ﬁnal output layer. For training P (y|h), we compute the gradient of the training criterion ˆCM in Equation (6)  ˆCM = log  P (y | h(m)) = log  φ(o(m))  M(cid:88) (cid:80)M  =  m=1  1 M ∂ ˆCM ∂o(m)  1 M  M(cid:88) (cid:80)M  m=1  G(o(m)) :=  =  φ(cid:48)(o(m)) m(cid:48)=1 φ(o(m(cid:48)))  =  (cid:80)M P (y | h(m)) m(cid:48)=1 P (y | h(m(cid:48)))  ∂ log φ(o(m))  ∂o(m)  ∂ log φ(o(m))  ∂o(m)  φ(o(m)) m(cid:48)=1 φ(o(m(cid:48))) w(m) m(cid:48)=1 w(m(cid:48))  (cid:80)M  =  ∂ log φ(o(m))  ∂o(m)  ,  (16)  ized weights ¯w(m) = w(m)/(cid:80)M  where w(m) = P (y|h(m)) are unnormalized weights. In other words, we get the gradient in the mixture by computing the gradient of the individual contribution m and multiplying it with normal- m(cid:48)=1 w(m(cid:48)). The normalized weights ¯w(m) can be interpreted as  responsibilities in a mixture model (see e.g. Bishop et al., 2006, Section 2.3.9). The gradients G(V), G(h), and G(c) are computed from G(o) using the chain rule of derivatives just like in standard back-propagation.  2.3 FIRST ESTIMATORS OF THE GRADIENT FOR TRAINING P (h|x) Bengio et al. (2013) proposed two estimators of the gradient for P (h|x). The ﬁrst one is unbiased but has high variance. It is deﬁned as  G1(ai) := (hi − σ(ai))(L − ¯Li)  E(cid:2)(hi − σ(ai))2L(cid:3)  E [(hi − σ(ai))2]  ,  ¯Li =  (14)  (15)  (17)  (18)  where we plug in L = ˆCM as the training criterion. We estimate the numerator and the denominator of ¯Li with an exponential moving average. The second estimator is biased but has lower variance. It is based on back-propagation where we set ∂hi ∂ai  := 1 resulting in  G2(ai) := G(hi) = (V:i)T G(o).  (19)  2.4 PROPOSED BIASED ESTIMATOR OF THE GRADIENT FOR TRAINING P (h|x)  We propose a new way of propagating the gradient of the training criterion through discrete hidden units hi. Let us consider hi continuous random variables with additive noise (cid:15)i  (cid:26)1 − σ(ai)  hi = σ(ai) + (cid:15)i (cid:15)i ∼  −σ(ai)  with probability σ(ai) with probability 1 − σ(ai)  (20)  (21)  Note that hi has the same distribution as in Equation (1), that is, it only gets values 0 and 1. With this formulation, we propose to back-propagate derivatives through hi by G3(ai) := σ(cid:48)(ai)G(hi) = σ(cid:48)(ai)(V:i)T G(o)  (22)  This gives us a biased estimate of the gradient since we ignore the fact that the structure of the noise (cid:15)i depends on the input signal ai. One should note, however, that the noise is zero-mean with any input ai, which should help keep the bias relatively small.  4  Published as a conference paper at ICLR 2015  2.5 VARIATIONAL TRAINING Tang & Salakhutdinov (2013) use a variational lower bound L(Q) on the training criterion C as =: L(Q).  C = log P (y | x) =  ≥(cid:88)  P (h | y, x) log  P (y, h | x)  (cid:88)  Q(h) log  P (y, h | x) P (h | y, x)  Q(h)  h  h  (23)  The above inequality holds for any distribution Q(h), but we get more usefulness out of it by choos- ing Q(h) so that it serves as a good approximation of P (h | y, x). We start by noting that we can use importance sampling to express P (h | y, x) in terms of a proposal distribution R(h|y, x) from which we can draw samples.  P (h | y, x) ∝ P (y | h)P (h | x) =  R(h | y, x)  (24)  P (y | h)P (h | x)  R(h | y, x)  Let δ(h) be the Dirac delta function centered at h. We construct Q(h) based on this expansion:  M(cid:88)  Q(h) =  ¯w(m)δ(h(m))  m=1  h(m) ∼ R(h | y, x)  w(m) =  ¯w(m) =  P (y | h(m))P (h(m) | x)  (cid:80)M  R(h(m) | y, x) w(m) m(cid:48)=1 w(m(cid:48))  (25)  (26)  (27)  (28)  where w(m) and ¯w(m) are called the unnormalized and normalized important weights. It would be an interesting line of research to train an auxiliary model for the proposal distribution R(h|x, y) following ideas from Kingma & Welling (2013); Rezende et al. (2014); Mnih & Gregor (2014) that call the equivalent of R the recognition model or the inference network. However, we do not pursue that line further in this paper and follow Tang & Salakhutdinov (2013) who chose R(h|x, y) := P (h|x), in which case the importance weights simplify to w(m) = P (y|h(m)). Tang & Salakhutdinov (2013) use a generalized EM algorithm, where they compute the gradient for the lower bound L(Q) given that Q(h) is ﬁxed  (cid:88) M(cid:88)  h  m=1  G4(θ) :=  ∂ ∂θ  =  ∂ ∂θ  Q(h) log  ¯w(m)(cid:104)  P (y, h | x)  Q(h)  log P (y | h(m)) + log P (h(m) | x)  (cid:105)  (29)  (30)  .  Thus, we train P (h(m)|x) using h(m) as target outputs. It turns out that the resulting gradient for P (y|h) is exactly the same as in Section 2.2, despite the rather different way of obtaining it. The importance weights ¯w(m) have the same role as responsi- bilities ¯w(m) in the mixture model, so we can use the same notation for them. Proposed Unbiased Estimator of the Gradient We propose a new gradient estimator by applying a variance reduction technique (Weaver & Tao, 2001; Mnih & Gregor, 2014) to the estimator by Tang & Salakhutdinov (2013). First we note that  EP (h|x)  log P (h | x)  =  P (h | x)  ∂  ∂θ P (h | x) P (h | x)  dh =  ∂ ∂θ  P (h | x)dh =  ∂ ∂θ  1 = 0.  (31) That is, when training P (h|x) with samples h(m) ∼ P (h|x) drawn from the model distribution, the gradient is on average zero. Therefore we can change the estimator of P (h|x) by subtracting any  (cid:20) ∂  ∂θ  (cid:21)  (cid:90)  (cid:90)  5  Published as a conference paper at ICLR 2015  Figure 1: Left: The norm of the gradient for the weights of the ﬁrst hidden layer as a function of cM M in Equation (32) corresponds to cM = 1. The norm is averaged over where the proposed c = 1 a mini-batch, after {1,7,50} epochs of training (curves from top to bottom) with G5 in the MNIST classiﬁcation experiment (see Appendix A). Varying c only changes the variance of the estimator, so the minimum norm corresponds to the minimum variance. Right: ˆCM as a function of the number of particles used during test time for the MNIST structured prediction task for the two proposed models trained with M = 20.  constant c from the weights ¯w(m) without introducing any bias. We choose c = E(cid:2) ¯w(m)(cid:3) = 1  M which is empirically shown to be sufﬁciently close to the optimum (see Figure 1 (left)). Finally, the proposed estimator becomes  (cid:18)  (cid:19)  (cid:21)  ¯w(m) log P (y | h(m)) +  ¯w(m) − 1 M  log P (h(m) | x)  .  (32)  (cid:20)  M(cid:88)  m=1  G5(θ) :=  ∂ ∂θ  3 EXPERIMENTS  We propose two experiments as benchmarks for stochastic feedforward networks based on the MNIST handwritten digit dataset (LeCun et al., 1998) and the Toronto Face Database (Susskind et al., 2010). In both experiments, the output distribution is likely to be complex and multimodal. In the ﬁrst experiment, we predicted the lower half of the MNIST digits using the upper half as in- puts. The MNIST dataset used in the experiments was binarized as a preprocessing step by sampling each pixel independently using the grey-scale value as its expectation. In the second experiment, we followed Tang & Salakhutdinov (2013) and predicted different facial expressions in the Toronto Face Database (Susskind et al., 2010). As data, we used all individuals with at least 10 different fa- cial expression pictures, which we do not binarize. We set the input to the mean of these images per subject, and as output predicted the distribution of the different expressions of the same subject2. We randomly chose 100 subjects for the training data (1372 images) and the remaining 31 subjects for the test data (427 images). As the data in the second problem is continuous, we assumed unit vari- ance Gaussian noise and thus trained the network using the sum of squares error. We used a network structure of 392-200-200-392 and 2304-200-200-2304 in the ﬁrst and second problem, respectively. Before running the experiments, we did a simple viability check of the gradient estimators by train- ing a network to do MNIST classiﬁcation. Based on the results, we kept G3 to G5 that performed signiﬁcantly better than G1 and G2. The results of the viability experiment can be found in Ap- pendix A. For comparison, we also trained four additional networks (labeled A-D) in addition to the stochastic feedforward networks. Network A is a deterministic network (corresponding to G3 with (cid:15)i = 0 in Equation (21)). In network B, we used the weights trained to produce deterministic values for the hidden units, but instead of using these deterministic values at test time we use their stochastic equivalent. We therefore trained the network in the same way as network A, but ran the tests as  2We hence discarded the expression labels  6  −0.500.511.5051015202530cMnorm of gradient020406080100−100−90−80−70−60−50Number of particles MˆCMPublished as a conference paper at ICLR 2015  the network would be a stochastic network. Network C is a hybrid network inspired by Tang & Salakhutdinov (2013), where each hidden layer consists of 40 binary stochastic neurons and 160 deterministic neurons. However, the stochastic neurons have incoming connections from the de- terministic input from the previous layer, and outgoing connections to the deterministic neurons in the same layer. As in the original paper, the network was trained using the gradient estimator G4. Network D is the same as the hybrid network C with one difference: the stochastic neurons have a constant activation probability of 0.5, and do hence not have any incoming weights or biases to learn. In all of the experiments, we used stochastic gradient descent with a mini-batch size of 100 and momentum of 0.9. We used a learning rate schedule where the learning rate increases linearly from zero to maximum during the ﬁrst ﬁve epochs and back to zero during the remaining epochs. The maximum learning rate was chosen among {0.0001, 0.0003, 0.001, . . . , 1} and the best test error for each method is reported.3 The models were trained with M ∈ {1, 20}, and during test time we always used M = 100. As can be seen in Table 1, excluding the comparison methods, the proposed biased estimator G3 performs the best in both tasks. It is notable that the performance of G3 increased signiﬁcantly when using more than M = 1 particles, as could be predicted from Theorem 1. In Figure 1 (right) we plot the objective CM at test time based on a number of particles M = 1, . . . , 100. In theory, a larger number of particles M is always better (if given inﬁnite computational resources), but here Figure 1 (right) shows how the objective CM is estimated very accurately with only M = 20 or M = 40. Of all the networks tested, the best performing network in both tasks was however comparison network D, i.e. the deterministic network with added binary stochastic neurons that have a constant activation probability of 0.5. It is especially interesting to note that this network also outperformed the hybrid network C where the output probabilities of the stochastic neurons are learned. Network D seems to gain from being able to model stochasticity without the need to propagate errors through the binary stochastic variables. The results give some support to the hypothesis that a hybrid network outperforms a stochastic network because it is easier to learn a deterministic network around a small number of stochastic units than learning a full stochastic network, although the stochastic units are not trained properly. The results could possibly be improved by making the networks larger and continuing training longer if given enough computational capacity. This might be the case especially in the experiments with the Toronto Face Dataset, where the deterministic network A outperforms some of the stochastic networks. However, the critical difference between the stochastic networks and the deterministic network can be be observed in Figure 2, where the stochastic networks are able to generate re- constructions that correspond to different digits for an ambiguous input. Clearly, the deterministic network cannot model such a distribution.  4 DISCUSSION In the proposed estimator of the gradient for P (h|x) in Equation (32), there are both positive and negative weights for various particles h(m). Positive weights can be interpreted as pulling proba- bility mass towards the particle, and negative weights as pushing probability mass away from the particle. Although we showed that the variance of the gradient estimate is smaller when using both positive and negative weights (G5 vs. G4), the difference in the ﬁnal performance of the two esti- mators was not substantial One challenge with structured outputs y is to ﬁnd samples h(m) that give a reasonably large proba- bility P (y|h(m)) with a reasonably small sample size M. Training a separate R(h|x, y) (cid:54)= P (h|x) as a proposal distribution looks like a promising direction for addressing that issue. It might still be useful to use a mix of particles from R and P (h|x), and subtract a constant from the weights of the latter ones. This approach would yield both particles that explain y well, and particles that have negative weights.  3In the MNIST experiments we used a separate validation set to select the learning rate. However, as we chose just one hyperparameter with a fairly sparse grid, we only report the best test error in the TFD experiments without a separate validation set.  7  Published as a conference paper at ICLR 2015  Figure 2: Samples drawn from the prediction of the lower half of the MNIST test data digits based on the upper half with models trained using G3 (left), G5 (middle), and for the deterministic network (right). The leftmost column is the original MNIST digit, followed by the masked out image and ten samples. The ﬁgures illustrate how the stochastic networks are able to model different digits in the case of ambiguous inputs.  Table 1: Results obtained on MNIST and TFD structured prediction using various number of samples M during training and various estimators of the gradient Gi. Error margins are ± two standard deviations from 10 runs.  MNIST Neg. test log-likelihood ( ˆC100) M = 20 M = 1 53.8 ± 0.2 59.8 ± 0.1 64.0 ± 1.7 63.2 ± 1.2  na na  68.4 ± 0.1 59.1 ± 0.2 67.9 ± 1.1  na  na na  58.4 ± 0.8 52.0 ± 0.2  TFD test Sum of Squared Errors  M = 1 31.7 ± 0.7  na na  35.3 ± 0.4 48.3 ± 7.5 33.4 ± 0.6  na  M = 20 26.3 ± 3.7 51.4 ± 0.1 51.3 ± 0.1  na na  35.5 ± 1.0 21.4 ± 0.6  G3 (proposed biased) G4 (Tang et al., 2013) G5 (proposed unbiased) deterministic (A) deterministic as stochastic (B) hybrid (C) deterministic, binary noise (D)  5 CONCLUSION  Using stochastic neurons in a feedforward network is more than just a computational trick to train deterministic models. The model itself can be deﬁned in terms of stochastic particles in the hidden layers, and we have shown many valid alternatives to the usual gradient formulation. These proposals for the gradient involve particles in the hidden layers with normalized weights that represent how well the particles explain the output targets. We showed both theoretically and experimentally how involving more than one particle signiﬁcantly enhances the modeling capacity. We demonstrated the validity of these techniques in three sets of experiments: we trained a clas- siﬁer on MNIST that achieved a reasonable performance, a network that could ﬁll in the missing information when we deleted the bottom part of the MNIST digits, and a network that could output individual expressions of face images based on the mean expression. We hope that we have provided some insight into the properties of stochastic feedforward neural networks, and that the theory can be applied to other contexts such as the study of Dropout or other important techniques that give a stochastic ﬂavor to deterministic models.  ACKNOWLEDGEMENTS  The authors would like to acknowledge NSERC, Nokia Labs and the Academy of Finland as sources of funding, in addition to the developers of Theano (Bastien et al., 2012; Bergstra et al., 2010)  8  Published as a conference paper at ICLR 2015  REFERENCES Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Bengio, Yoshua, L´eonard, Nicholas, and Courville, Aaron. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Bishop, Christopher M et al. Pattern recognition and machine learning, volume 1. Springer New  York, 2006.  Esser, Steve K, Andreopoulos, Alexander, Appuswamy, Rathinakumar, Datta, Pallab, Barch, Davis, Amir, Arnon, Arthur, John, Cassidy, Andrew, Flickner, Myron, Merolla, Paul, et al. Cognitive computing systems: Algorithms and applications for networks of neurosynaptic cores. In Neural Networks (IJCNN), The 2013 International Joint Conference on, pp. 1–10. IEEE, 2013.  Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Rus- lan R. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.  Kingma, Diederik P and Welling, Max. Auto-encoding variational Bayes.  arXiv:1312.6114, 2013.  arXiv preprint  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  Mnih, Andriy and Gregor, Karol. Neural variational inference and learning in belief networks. arXiv  preprint arXiv:1402.0030, 2014.  Neal, Radford M. Learning stochastic feedforward networks. Department of Computer Science,  University of Toronto, 1990.  Neal, Radford M. Connectionist learning of belief networks. Artiﬁcial intelligence, 56(1):71–113,  1992.  Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep learning made easier by linear transfor- mations in perceptrons. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 924–932, 2012.  Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic back-propagation and  variational inference in deep latent Gaussian models. arXiv preprint arXiv:1401.4082, 2014.  Saul, Lawrence K, Jaakkola, Tommi, and Jordan, Michael I. Mean ﬁeld theory for sigmoid belief  networks. arXiv preprint cs/9603102, 1996.  Sietsma, Jocelyn and Dow, Robert JF. Creating artiﬁcial neural networks that generalize. Neural  Networks, 4(1):67–79, 1991.  Susskind, Joshua, Anderson, Adam, and Hinton, Geoffrey. The Toronto face database. Technical  report, University of Toronto, UTML TR 2010001, 06 2010.  Tang, Yichuan and Salakhutdinov, Ruslan. Learning stochastic feedforward neural networks. In  Advances in Neural Information Processing Systems, pp. 530–538, 2013.  Weaver, Lex and Tao, Nigel. The optimal reward baseline for gradient-based reinforcement learning. In Proceedings of the Seventeenth conference on Uncertainty in artiﬁcial intelligence, pp. 538– 545. Morgan Kaufmann Publishers Inc., 2001.  9  Published as a conference paper at ICLR 2015  A CLASSIFICATION EXPERIMENT  MNIST classiﬁcation is a well studied problem where performances of a huge variety of approaches are known. Since the output y is just a class label, the advantage of being able to model complex out- put distributions is not applicable. Still, the benchmark is useful for comparing training algorithms against each other, and was used in this paper to test the viability of the gradient estimators. We used a network structure with dimensionalities 784-200-200-10. The input data was ﬁrst scaled to the range of [0, 1], and the mean of each pixel was then subtracted. As a regularization method, Gaussian noise with standard deviation 0.4 was added to each pixel separately in each epoch (Raiko et al., 2012). The models were trained for 50 epochs. Table 2 gives the test set error rate for each method. As can be seen from the table, deterministic networks give the best results. Excluding the comparison networks, the best result is obtained with the proposed biased gradient G3 followed by the proposed unbiased gradient G5. Based on the results, gradient estimators G1 and G2 were left out from the structured prediction experiments.  Table 2: Results obtained on MNIST classiﬁcation using various number of samples M during training and various estimators of the gradient Gi.  Test error (%) G1 (Bengio et al., 2013, unbiased) G2 (Bengio et al., 2013, biased) G3 (proposed biased) G4 (Tang et al., 2013) G5 (proposed unbiased) deterministic (A) deterministic as stochastic (B) hybrid (C) deterministic, binary noise (D)  M = 1 M = 20 11.30 7.85 7.86 7.97 1.82 1.63 3.99 na 2.72 na 1.51 na 1.80 na 2.19 na 1.80 1.92  10  ",
1406.2751,2015,Reweighted Wake-Sleep,"['Reweighted Wake-Sleep', 'Jorg Bornschein and Yoshua Bengio']",https://arxiv.org/pdf/1406.2751,"5 1 0 2    r p A 6 1         ]  G L . s c [      4 v 1 5 7 2  .  6 0 4 1 : v i X r a  Published as a conference paper at ICLR 2015  REWEIGHTED WAKE-SLEEP  J¨org Bornschein and Yoshua Bengio ∗ Department of Computer Science and Operations Research University of Montreal Montreal, Quebec, Canada  ABSTRACT  Training deep directed graphical models with many hidden variables and perform- ing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed gen- erative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of la- tent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is conﬁrmed ex- perimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure. Based on this interpretation, we propose that a sigmoidal belief network is not sufﬁciently powerful for the layers of the inference network in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.  1  INTRODUCTION  Training directed graphical models – especially models with multiple layers of hidden variables – remains a major challenge. This is unfortunate because, as has been argued previously (Hin- ton et al., 2006; Bengio, 2009), a deeper generative model has the potential to capture high-level abstractions and thus generalize better. The exact log-likelihood gradient is intractable, be it for Helmholtz machines (Hinton et al., 1995; Dayan et al., 1995), sigmoidal belief networks (SBNs), or deep belief networks (DBNs) (Hinton et al., 2006), which are directed models with a restricted Boltzmann machine (RBM) as top-layer. Even obtaining an unbiased estimator of the gradient of the DBN or Helmholtz machine log-likelihood is not something that has been achieved in the past. Here we show that it is possible to get an unbiased estimator of the likelihood (which unfortunately makes it a slightly biased estimator of the log-likelihood), using an importance sampling approach. Past proposals to train Helmholtz machines and DBNs rely on maximizing a variational bound as proxy for the log-likelihood (Hinton et al., 1995; Kingma and Welling, 2014; Rezende et al., 2014). The ﬁrst of these is the wake-sleep algorithm (Hinton et al., 1995), which relies on combining a “recognition” network (which we call an approximate inference network, here, or simply inference network) with a generative network. In the wake-sleep algorithm, they basically provide targets for each other. We review these previous approaches and introduce a novel approach that generalizes the wake-sleep algorithm. Whereas the original justiﬁcation of the wake-sleep algorithm has been questioned (because we are optimizing a KL-divergence in the wrong direction), a contribution of this paper is to shed a different light on the wake-sleep algorithm, viewing it as a special case of the proposed reweighted wake-sleep (RWS) algorithm, i.e., as reweighted wake-sleep with a single sample. This shows that wake-sleep corresponds to optimizing a somewhat biased estimator of the likelihood gradient, while using more samples makes the estimator less biased (and asymptotically unbiased as more samples are considered). We empirically show that effect, with clearly better re- sults obtained with K = 5 samples than with K = 1 (wake-sleep), and 5 or 10 being sufﬁcient to  ∗J¨org Bornschein is a CIFAR Global Scholar; Yoshua Bengio is a CIFAR Senior Fellow  1  Published as a conference paper at ICLR 2015  achieve good results. Unlike in the case of DBMs, which rely on a Markov chain to get samples and estimate the gradient by a mean over those samples, here the samples are i.i.d., avoiding the very serious problem of mixing between modes that can plague MCMC methods (Bengio et al., 2013) when training undirected graphical models. Another contribution of this paper regards the architecture of the deep approximate inference net- work. We view the inference network as estimating the posterior distribution of latent variables given the observed input. With this view it is plausible that the classical architecture of the inference network (a SBN, details below) is inappropriate and we test this hypothesis empirically. We ﬁnd that more powerful parametrizations that can represent non-factorial posterior distributions yield better results.  2 REWEIGHTED WAKE-SLEEP  2.1 THE WAKE-SLEEP ALGORITHM  The wake-sleep algorithm was proposed as a way to train Helmholtz machines, which are deep directed graphical models p(x, h) over visible variables x and latent variables h, where the latent variables are organized in layers hk. In the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995), the top layer hL has a factorized unconditional distribution, so that ancestral sampling can proceed from hL down to h1 and then the generated sample x is generated by the bottom layer, given h1. In the deep belief network (DBN) (Hinton et al., 2006), the top layer is instead generated by a RBM, i.e., by a Markov chain, while simple ancestral sampling is used for the others. Each intermediate layer is speciﬁed by a conditional distribution parametrized as a stochastic sigmoidal layer (see section 3 for details). The wake-sleep algorithm is a training procedure for such generative models, which involves train- ing an auxiliary network, called the inference network, that takes a visible vector x as input and stochastically outputs samples hk for all layers k = 1 to L. The inference network outputs sam- ples from a distribution that should estimate the conditional probability of the latent variables of the generative model (at all layers) given the input. Note that in these kinds of directed models exact inference, i.e., sampling from p(h|x) is intractable. The wake-sleep algorithm proceeds in two phases. In the wake phase, an observation x is sampled from the training distribution D and propagated stochastically up the inference network (one layer at a time), thus sampling latent values h from q(h|x). Together with x, the sampled h forms a target for training p, i.e., one performs a step of gradient ascent update with respect to maximum likelihood over the generative model p(x, h), with the data x and the inferred h. This is useful because whereas h p(x, h) is intractable, computing the gradient of the complete log-likelihood log p(x, h) is easy. In addition, these updates decouple all the layers (because both the input and the target of each layer are considered observed). In the sleep-phase, a “dream” sample is obtained from the generative network by ancestral sampling from p(x, h) and is used as a target for the maximum likelihood training of the inference network, i.e., q is trained to estimate p(h|x). The justiﬁcation for the wake-sleep algorithm that was originally proposed is based on the following variational bound,  computing the gradient of the marginal likelihood p(x) =(cid:80)  log p(x) ≥(cid:88)  q(h|x) log  p(x, h) q(h|x)  h  that is true for any inference network q, but the bound becomes tight as q(h|x) approaches p(h|x). Maximizing this bound with respect to p corresponds to the wake phase update. The update with respect to q should minimize KL(q(h|x)||p(h|x)) (with q as the reference) but instead the sleep phase update minimizes the reversed KL divergence, KL(p(h|x)||q(h|x)) (with p as the reference).  2.2 AN IMPORTANCE SAMPLING VIEW YIELDS REWEIGHTED WAKE-SLEEP If we think of q(h|x) as estimating p(h|x) and train it accordingly (which is basically what the sleep phase of wake-sleep does), then we can reformulate the likelihood as an importance-weighted  2  Published as a conference paper at ICLR 2015  average:  (cid:88)  h  p(x) =  q (h| x)  p(x, h) q (h| x)  =  E  h∼q(h | x)  (cid:21)  (cid:20) p(x, h)  q (h| x)  K(cid:88)  (cid:39) 1 K h(k)∼q(h | x)  k=1  q(cid:0)h(k) | x(cid:1)  p(x, h(k))  (1)  Eqn. (1) is a consistent and unbiased estimator for the marginal likelihood p(x). The optimal q that results in a minimum variance estimator is q∗(h| x) = p (h| x). In fact we can show that this is a zero-variance estimator, i.e., the best possible one that will result in a perfect p(x) estimate even with a single arbitrary sample h ∼ p (h| x):  E  = p(x)  h∼p(h | x)  p(h| x)  (2) Any mismatch between q and p (h| x) will increase the variance of this estimator, but it will not In practice however, we are typically interested in an estimator for the log- introduce any bias. likelihood. Taking the logarithm of (1) and averaging over multiple datapoints will result in a con- servative biased estimate and will, on average, underestimate the true log-likelihood due to the concavity of the logarithm. Increasing the number of samples will decrease both the bias and the variance. Variants of this estimator have been used in e.g. (Rezende et al., 2014; Gregor et al., 2014) to evaluate trained models.  h∼p(h | x)  [1] = p(x)  (cid:20) p (h| x) p(x)  (cid:21)  E  2.3 TRAINING BY REWEIGHTED WAKE-SLEEP  We now consider the models p and q parameterized with parameters θ and φ respectively. Updating pθ for given qφ: We propose to use an importance sampling estimator based on eq. (1) to compute the gradient of the marginal log-likelihood Lp(θ, x) = log pθ(x):  (cid:20) p(x, h)  q (h| x)  (cid:21)  ∂ ∂θ  log p(x, h)  Lp(θ, x ∼ D) =  ∂ ∂θ  E  h∼q(h | x)  1  p(x)  (cid:39) K(cid:88)  k=1  ˜ωk  ∂ ∂θ  and the importance weights ˜ωk =  ; ωk =  log p(x, h(k)) with h(k) ∼ q (h| x)  (3)  ωk(cid:80)K  k(cid:48)=1 ωk(cid:48)  q(cid:0)h(k) | x(cid:1).  p(x, h(k))  See the supplement for a detailed derivation. Note that this is a biased estimator because it implicitly contains a division by the estimated p(x). Furthermore, there is no guarantee that q = p (h| x) results in a minimum variance estimate of this gradient. But both, the bias and the variance, decrease as the number of samples is increased. Also note that the wake-sleep algorithm uses a gradient that is equivalent to using only K = 1 sample. Another noteworthy detail about eq. (3) is that the importance weights ˜ω are automatically normalized such that they sum up to one. Updating qφ for given pθ: In order to minimize the variance of the estimator (1) we would like q (h| x) to track p (h| x). We propose to train q using maximum likelihood learning with the loss Lq(φ, x, h) = log qφ(x|h). There are at least two reasonable options how to obtain training data for Lq: 1) maximize Lq under the empirical training distribution x ∼ D, h ∼ p (h| x), or 2) maximize Lq under the generative model (x, h) ∼ pθ(x, h). We will refer to the former as wake phase q-update and to the latter as sleep phase q-update. In the case of a DBN (where the top layer is generated by an RBM), there is an intermediate solution called contrastive-wake-sleep, which has been proposed in (Hinton et al., 2006). In contrastive wake-sleep we sample x from the training distribution, propagate it stochastically into the top layer and use that h as starting point for a short Markov chain in the RBM, then sample the other layers in the generative network p to generate the rest of (x, h). The objective is to put the inference network’s capacity where it matters most, i.e., near the input conﬁgurations that are seen in the training set. Analogous to eqn. (1) and (3) we use importance sampling to derive gradients for the wake phase q-update:  ˜ωk  ∂ ∂φ  log qφ(h(k)|x)  (4)  Lq(φ, x ∼ D) (cid:39) K(cid:88)  ∂ ∂φ  k=1  3  Published as a conference paper at ICLR 2015  with the same importance weights ˜ωk as in (3) (the details of the derivation can again be found in the supplement). Note that this is equivalent to optimizing q so as to minimize KL(p(·|x) (cid:107) q(·|x)). For the sleep phase q-update we consider the model distribution p(x, h) a fully observed system and can thus derive gradients without further sampling:  Lq(φ, (x, h)) =  ∂ ∂φ  ∂ ∂φ  log qφ(h|x) with x, h ∼ p(x, h)  (5)  This update is equivalent to the sleep phase update in the classical wake-sleep algorithm.  Algorithm 1 Reweighted Wake-Sleep training procedure and likelihood estimator. K is the number of approximate inference samples and controls the trade-off between computation and accuracy of the estimators (both for the gradient and for the likelihood). We typically use a large value (K = 100, 000) for test set likelihood estimator but a small value (K = 5) for estimating gradients. Both the wake phase and sleep phase update rules for q are optionally included (either one or both can be used, and best results were obtained using both). The original wake-sleep algorithm has K=1 and only uses the sleep phase update of q. To estimate the log-likelihood at test time, only the  computations up to (cid:98)L are required.  for number of training iterations do  • Sample example(s) x from the training distribution for k = 1 to K do  • Layerwise sample latent variables h(k) from q(h|x) • Compute q(h(k)|x) and p(x, h(k))  end for • Compute unnormalized weights ωk = p(x,h(k)) • Normalize the weights ˜ωk = ωk(cid:80) q(h(k) | x) • Compute log-likelihood estimator (cid:98)L(x) = log averagek ωk • Compute unbiased likelihood estimator ˆp(x) = averagek ωk • Wake-phase update of p: Use gradient estimator(cid:80) • Optionally, wake phase update of q: Use gradient estimator(cid:80)  k ˜ωk  k(cid:48) ωk(cid:48)  • Optionally, sleep phase update of q: Sample (x(cid:48), h(cid:48)) from p and use gradient ∂ log q(h(cid:48)|x(cid:48))  ∂φ  end for  ∂ log p(x,h(k))  ∂θ k ˜ωk  ∂ log q(h(k)|x)  ∂φ  2.4 RELATION TO WAKE-SLEEP AND VARIATIONAL BAYES  Recently, there has been a resurgence of interest in algorithms related to the Helmholtz machine and to the wake-sleep algorithm for directed graphical models containing either continuous or discrete latent variables: In Neural Variational Inference and Learning (NVIL, Mnih and Gregor, 2014) the authors propose to maximize the variational lower bound on the log-likelihood to get a joint objective for both p It was known that this approach results in a gradient estimate of very high variance for and q. the recognition network q (Dayan and Hinton, 1996). In the NVIL paper, the authors therefore propose variance reduction techniques such as baselines to obtain a practical algorithm that enhances signiﬁcantly over the original wake-sleep algorithm. In respect to the computational complexity we note that while we draw K samples from the inference network for RWS, NVIL on the other hand draws only a single sample from q but maintains, queries and trains an additional auxiliary baseline estimating network. With RWS and a typical value of K = 5 we thus require at least twice as many arithmetic operations, but we do not have to store the baseline network and do not have to ﬁnd suitable hyperparameters for it. Recent examples include the auto-encoding variational Bayes (Kingma and Welling, 2014) and stochastic backpropagation papers (Rezende et al., 2014). In both cases one maximizes a variational lower bound on the log-likelihood that is rewritten as two terms: one that is log-likelihood reconstruction error through a stochastic encoder (approximate inference) - decoder (generative model) pair, and one that regularizes the output of the approximate inference stochastic encoder so that its marginal distribution matches the generative prior on the  latent variables  for continuous  4  Published as a conference paper at ICLR 2015  latent variables (and the latter is also trained, to match the marginal of the encoder output). Besides the fact that these variational auto-encoders are only for continuous latent variables, another differ- ence with the reweighted wake-sleep algorithm proposed here is that in the former, a single sample from the approximate inference distribution is sufﬁcient to get an unbiased estimator of the gradient of a proxy (the variational bound). Instead, with the reweighted wake-sleep, a single sample would correspond to regular wake-sleep, which gives a biased estimator of the likelihood gradient. On the other hand, as the number of samples increases, reweighted wake-sleep provides a less biased (asymptotically unbiased) estimator of the log-likelihood and of its gradient. Similar in spirit, but aimed at a structured output prediction task is the method proposed by Tang and Salakhutdinov (2013). The authors optimize the variational bound of the log-likelihood instead of the direct IS estimate but they also derive update equations for the proposal distribution that resembles many of the properties also found in reweighted wake-sleep.  3 COMPONENT LAYERS  Although the framework can be readily applied to continuous variables, we here restrict our- selves to distributions over binary visible and binary latent variables. We build our models by combining probabilistic components, each one associated with one of the layers of the gener- ative network or of the inference network. The generative model can therefore be written as pθ(x, h) = p0(x| h1) p1(h1| h2) ··· pL(hL), while the inference network has the form qφ(h| x) = q1(h1 | x)··· qL(hL | hL−1). For a distribution P to be a suitable component we must have a method to efﬁciently compute P (x(k)| y(k)) given (x(k) , y(k)), and we must have a method to efﬁciently draw i.i.d. samples x(k) ∼ P (x| y) for a given y. In the following we will describe experiments containing three kinds of layers: Sigmoidal Belief Network (SBN) layer: A SBN layer (Saul et al., 1996) is a directed graphical model with independent variables xi given the parents y:  P SBN(xi = 1| y) = σ(W i,: y + bi)  (6) Although a SBN is a very simple generative model given y, performing inference for y given x is in general intractable. Autoregressive SBN layer (AR-SBN, DARN): If we consider xi an ordered set of observed vari- ables and introduce directed, autoregressive links between all previous x<i and a given xi, we obtain a fully-visible sigmoid belief network (FVSBN, Frey, 1998; Bengio and Bengio, 2000). When we additionally condition a FVSBN on the parent layer’s y we obtain a layer model that was ﬁrst used in Deep AutoRegressive Networks (DARN, Gregor et al., 2014):  P AR-SBN(xi = 1| x<i, y) = σ(W i,: y + Si,<ix<i + bi)  (7) We use x<i = (x1, x2,··· , xi−1) to refer to the vector containing the ﬁrst i-1 observed variables. The matrix S is a lower triangular matrix that contains the autoregressive weights between the vari- ables xi, and with Si,<j we refer to the ﬁrst j-1 elements of the i-th row of this matrix. In contrast to a regular SBN layer, the units xi are thus not independent of each other but can be predicted like in a logistic regression in terms of its predecessors x<i and of the input of the layer, y. Conditional NADE layer: The Neural Autoregressive Distribution Estimator (NADE, Larochelle and Murray, 2011) is a model that uses an internal, accumulating hidden layer to predict variables xi given the vector containing all previously variables x<i. Instead of logistic regression in a FVSBN or an AR-SBN, the dependency between the variables xi is here mediated by an MLP (Bengio and Bengio, 2000):  (8) With W and V denoting the encoding and decoding matrices for the NADE hidden layer. For our purposes we condition this model on the random variables y:  P (xi = 1| x<i) = σ(V i,:σ(W :,<i x<i + a) + bi))  P NADE(xi = 1| x<i, y) = σ(V i,:σ(W :,<i x<i + Ua y + a) + U i,:  (9) Such a conditional NADE has been used previously for modeling musical sequences (Boulanger- Lewandowski et al., 2012). For each layer distribution we can construct an unconditioned distribution by removing the condi- tioning variable y. We use such unconditioned distributions as top layer p(h) for the generative network p.  b y + bi))  5  Published as a conference paper at ICLR 2015  Figure 1: A Final log-likelihood estimate w.r.t. number of samples used during training. B L2-norm of the bias and standard deviation of the low-sample estimated pθ gradient relative to a high-sample (K=5,000) based estimate.  NVIL  wake-sleep  (113.1) (99.8) (96.7)  116.3 (120.7) 106.9 (109.4) 101.3 (104.4)  P-model SBN SBN SBN AR-SBN AR-SBN NADE NADE  size 200 200-200 200-200-200 200 200-200 200 200-200  RWS  103.1 93.4 90.1  RWS  95.0 91.1 88.9 89.2 92.8 86.8 87.6  Q-model: SBN Q-model: NADE  Table 1: MNIST results for various architectures and training methods. In the 3rd column we cite the numbers reported by Mnih and Gregor (2014). Values in brackets are variational NLL bounds, values without brackets report NLL estimates (see section 2.2).  4 EXPERIMENTS  Here we present a series of experiments on the MNIST and the CalTech-Silhouettes datasets. The supplement describes additional experiments on smaller datasets from the UCI repository. With these experiments we 1) quantitatively analyze the inﬂuence of the number of samples K, 2) demonstrate that using a more powerful layer-model for the inference network q can signif- icantly enhance the results even when the generative model is a factorial SBN, and 3) show that we approach state-of-the-art performance when using either relatively deep models or when using powerful layer models such as a conditional NADE. Our implementation is available at https://github.com/jbornschein/reweighted-ws/.  4.1 MNIST We use the MNIST dataset that was binarized according to Murray and Salakhutdinov (2009) and downloaded in binarized form from (Larochelle, 2011). For training we use stochastic gradient decent with momentum (β=0.95) and set mini-batch size to 25. The experiments in this paragraph were run with learning rates of {0.0003, 0.001, and 0.003}. From these three we always report the experiment with the highest validation log-likelihood. In the majority of our experiments a learning rate of 0.001 gave the best results, even across different layer models (SBN, AR-SBN and NADE). If not noted otherwise, we use K = 5 samples during training and K = 100, 000 samples to estimate the ﬁnal log-likelihood on the test set1. To disentangle the inﬂuence of the different q updating methods we setup p and q networks consisting of three hidden SBN layers with 10, 200 and 200 units (SBN/SBN 10-200-200). After convergence, the model trained updating q during the sleep phase only reached a ﬁnal estimated log-likelihood of −93.4, the model trained with a q-update during the wake phase reached −92.8, and the model trained with both wake and sleep phase update reached −91.9. As a control we trained a model that does not update q at all. This model reached  1We refer to the lower bound estimates which can be arbitrarily tightened by increasing the number of test  samples as LL estimates to distiguish them from the variational LL lower bounds (see section 2.2).  6  100101102trainingsamples−130−120−110−100−90−80FinalLLestimate(testset)NADE200SBN10-200-200SBN200ABbias (epoch 50)bias (last epoch)std dev. (epoch50)100101102training samples0.30.40.50.60.70.80.9bias0.60.81.01.21.41.61.8std-dev.std dev. (last epoch)Published as a conference paper at ICLR 2015  Figure 2: A Final log-likelihood estimate w.r.t. number of test samples used. B Samples from the SBN/SBN 10-200-200 generative model. C Samples from the NADE/NADE 250 generative model. (We show the probabilities from which each pixel is sampled)  Results on binarized MNIST  Results on CalTech 101 Silhouettes  Method RWS (SBN/SBN 10-100-200-300-400) RWS (NADE/NADE 250) RWS (AR-SBN/SBN 500)† NADE (500 units, [1]) EoNADE (2hl, 128 orderings, [2]) DARN (500 units, [3]) RBM (500 units, CD3, [4]) RBM (500 units, CD25, [4]) DBN (500-2000, [5])  NLL bound  105.5 86.34 86.22  NLL est. 85.48 85.23 84.18 88.35 85.10 84.13  84.55  Method RWS (SBN/SBN 10-50-100-300) RWS (NADE/NADE 150)  NADE (500 hidden units) RBM (4000 hidden units, [6])  NLL est. 113.3 104.3  110.6 107.8  Table 2: Various RWS trained models in relation to previously published methods: [1] Larochelle and Murray (2011), [2] Murray and Larochelle (2014), [3] Gregor et al. (2014) [4] Salakhutdinov and Murray (2008), [5] Murray and Salakhutdinov (2009), [6] Cho et al. (2013). † Same model as the best performing in [3]: a AR-SBN with deterministic hidden variables between the observed and latent. All RWS NLL estimates on MNIST have conﬁdence intervals of ≈ ±0.40.  −171.4. We conﬁrmed that combining wake and sleep phase q-updates generally gives the best results by repeating this experiment with various other architectures. For the remainder of this paper we therefore train all models with combined wake and sleep phase q-updates. Next we investigate the inﬂuence of the number of samples used during training. The results are visualized in Fig. 1 A. Although the results depend on the layer-distributions and on the depth and width of the architectures, we generally observe that the ﬁnal estimated log-likelihood does not improve signiﬁcantly when using more than 5 samples during training for NADE models, and using more than 25 samples for models with SBN layers. We can quantify the bias and the variance of the gradient estimator (3) using bootstrapping. While training a SBN/SBN 10-200-200 model with K = 100 training samples, we use K = 5, 000 samples to get a high quality estimate of the gradient for a small but ﬁxed set of 25 datapoints (the size of one mini-batch). By repeatedly resampling smaller sets of {1, 2, 5,··· , 5000} samples with replacement and by computing the gradient based on these, we get a measure for the bias and the variance of the small sample estimates relative the high quality estimate. These results are visualized in Fig. 1 B. In Fig. 2 A we ﬁnally investigate the quality of the log-likelihood estimator (eqn. 1) when applied to the MNIST test set. Table 1 summarizes how different architectures compare to each other and how RWS compares to related methods for training directed models. We essentially observe that RWS trained models consistently improve over classical wake-sleep, especially for deep architectures. We furthermore observe that using autoregressive layers (AR-SBN or NADE) for the inference network improves the results even when the generative model is composed of factorial SBN layers. Finally, we see that the best performing models with autoregressive layers in p are always shallow with only a single  7  100101102samples−130−120−110−100−90−80est.LLNADE-NADE200SBN-SBN200-200-10SBN-SBN200ABCSBN/SBN 10-100-200-300-400NADE/NADE 250Published as a conference paper at ICLR 2015  Figure 3: CalTech 101 Silhouettes: A Random selection of training data points. B Random samples from the SBN/SBN 10-50-100-300 generative network. C Random Samples from the NADE-150 generative network. (We show the probabilities from which each pixel is sampled)  hidden layer. In Table 2 (left) we compare some of our best models to the state-of-the-art results published on MNIST. The deep SBN/SBN 10-100-200-300-400 model was trained for 1000 epochs with K = 5 training samples and a learning rate of 0.001. For ﬁne-tuning we run additional 500 epochs with a learning rate decay of 1.005 and 100 training samples. For comparison we also train the best performing model from the DARN paper (Gregor et al., 2014) with RWS, i.e., a single layer AR-SBN with 500 latent variables and a deterministic layer of hidden variables between the observed and the latents. We essentially obtain the same ﬁnal testset log-likelihood. For this shallow network we thus do not observe any improvement from using RWS.  4.2 CALTECH 101 SILHOUETTES We applied reweighted wake-sleep to the 28 × 28 pixel CalTech 101 Silhouettes dataset. This dataset consists of 4,100 examples in the training set, 2,264 examples in the validation set and 2,307 examples in the test set. We trained various architectures on this dataset using the same hyperparameter as for the MNIST experiments. Table 2 (right) summarizes our results. Note that our best SBN/SBN model is a relatively deep network with 4 hidden layers (300-100-50-10) and reaches a estimated LL of -116.9 on the test set. Our best network, a shallow NADE/NADE-150 network reaches -104.3 and improves over the previous state of the art (−107.8, a RBM with 4000 hidden units by Cho et al. (2013)).  5 CONCLUSIONS We introduced a novel training procedure for deep generative models consisting of multiple layers of binary latent variables. It generalizes and improves over the wake-sleep algorithm providing a lower bias and lower variance estimator of the log-likelihood gradient at the price of more samples from the inference network. During training the weighted samples from the inference network decouple the layers such that the learning gradients only propagate within the individual layers. Our experiments demonstrate that a small number of ≈ 5 samples is typically sufﬁcient to jointly train relatively deep architectures of at least 5 hidden layers without layerwise pretraining and without carefully tuning learning rates. The resulting models produce reasonable samples (by visual inspection) and they approach state-of-the-art performance in terms of log-likelihood on several discrete datasets. We found that even in the cases when the generative networks contain SBN layers only, better results can be obtained with inference networks composed of more powerful, autoregressive layers. This however comes at the price of reduced computational efﬁciency on e.g. GPUs as the individual variables hi ∼ q(h|x) have to be sampled in sequence (even though the theoretical complexity is not signiﬁcantly worse compared to SBN layers). We furthermore found that models with autoregressive layers in the generative network p typically produce very good results. But the best ones were always shallow with only a single hidden layer. At this point it is unclear if this is due to optimization problems. Acknowledgments We would like to thank Laurent Dinh, Vincent Dumoulin and Li Yao for helpful discussions and the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) for their powerful software. We furthermore acknowledge CIFAR and Canada Research Chairs for funding and Compute Canada, and Calcul Qu´ebec for providing computational resources.  8  Published as a conference paper at ICLR 2015  REFERENCES Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  Bengio, Y. (2009). Learning deep architectures for AI. Now Publishers. Bengio, Y. and Bengio, S. (2000). Modeling high-dimensional discrete data with multi-layer neural  networks. In NIPS’99, pages 400–406. MIT Press.  Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013). Better mixing via deep representations.  In Proceedings of the 30th International Conference on Machine Learning (ICML’13). ACM.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- In  Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012). Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In ICML’2012.  Cho, K., Raiko, T., and Ilin, A. (2013). Enhanced gradient for training restricted boltzmann ma-  chines. Neural computation, 25(3), 805–831.  Dayan, P. and Hinton, G. E. (1996). Varieties of helmholtz machine. Neural Networks, 9(8), 1385–  1403.  Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. (1995). The Helmholtz machine. Neural  computation, 7(5), 889–904.  Frey, B. J. (1998). Graphical models for machine learning and digital communication. MIT Press. Gregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. (2014). Deep autoregressive  networks. In Proceedings of the 31st International Conference on Machine Learning.  Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995). The wake-sleep algorithm for unsu-  pervised neural networks. Science, 268, 1558–1161.  Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets.  Neural Computation, 18, 1527–1554.  Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In Proceedings of the  International Conference on Learning Representations (ICLR).  Larochelle, H. (2011). Binarized mnist dataset. http://www.cs.toronto.edu/˜larocheh/ public/datasets/binarized_mnist/binarized_mnist_[train|valid|test].amat.  Larochelle, H. and Murray, I. (2011). The Neural Autoregressive Distribution Estimator. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS’2011), volume 15 of JMLR: W&CP.  Mnih, A. and Gregor, K. (2014). Neural variational inference and learning in belief networks. In Proceedings  of the 31st International Conference on Machine Learning (ICML 2014). to appear.  Murray, B. U. I. and Larochelle, H. (2014). A deep and tractable density estimator. In ICML’2014.  Murray, I. and Salakhutdinov, R. (2009). Evaluating probabilities under high-dimensional latent variable mod-  els. In NIPS’08, volume 21, pages 1137–1144.  Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference  in deep generative models. In ICML’2014.  Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief networks. In Proceedings  of the International Conference on Machine Learning, volume 25.  Saul, L. K., Jaakkola, T., and Jordan, M. I. (1996). Mean ﬁeld theory for sigmoid belief networks. Journal of  Artiﬁcial Intelligence Research, 4, 61–76.  Tang, Y. and Salakhutdinov, R. (2013). Learning stochastic feedforward neural networks. In NIPS’2013.  9  Published as a conference paper at ICLR 2015  6 SUPPLEMENT  6.1 GRADIENTS FOR p(x, h)  Lp(θ, x ∼ D) =  ∂ ∂θ  =  =  =  (cid:39)  6.2 GRADIENTS FOR THE WAKE PHASE Q UPDATE  with ωk =  Lq(φ, x ∼ D) =  ∂ ∂φ  =  (cid:39)  ∂ ∂φ  1  h  p(x)  1(cid:80)  k ωk  (cid:88)  h  1  p(x)  ∂ ∂θ  p(x, h)  log p(x, h)  log pθ(x) =  (cid:88) (cid:88)  h  h  ∂ ∂θ  1  p(x)  1  p(x)  1  E  p(x, h)  ∂ ∂θ q (h| x)  ∂ ∂θ  (cid:20) p(x, h)  p(x, h) q (h| x) ∂ ∂θ  log p(x, h)  (cid:21)  log p(x, h)  k=1  ωk  p(x)  h∼q(h | x)  log p(x, h(k))  k ωk p(x, h(k))  q (h| x) ∂ ∂θ  K(cid:88) 1(cid:80) q(cid:0)h(k) | x(cid:1) and h(k) ∼ q (h| x) (cid:88)  p(x, h)log qφ(h|x)  (cid:20) p(x, h)  log qφ(h|x)  h∼q(h | x)  E  K(cid:88)  k=1  ∂ ∂φ  q (h| x) log qφ(h(k)|x) ∂ ∂φ  ωk  (cid:21)  h  ∂ ∂φ  (cid:88) = −(cid:88) (cid:39) − 1(cid:80)  h  k ωk  pθ(h|x)  ∂ ∂φ  K(cid:88)  k=1  pθ(h|x) qφ(h|x) log qφ(h|x)  Note that we arrive at the same gradients when we set out to minimize the KL(p(·|x) (cid:107) q(·|x) for a given datapoint x:  KL(pθ(h|x) (cid:107) qφ(h|x)) =  pθ(h|x) log  ∂ ∂φ  ωk  ∂ ∂φ  log qφ(h(k)|x)  (12)  (10)  (11)  10  Published as a conference paper at ICLR 2015  6.3 ADDITIONAL EXPERIMENTAL RESULTS  6.3.1 LEARNING CURVES FOR MNIST EXPERIMENTS  Figure 4: Learning curves for various MNIST experiments.  6.3.2 BOOTSTRAPPING BASED log(p(x)) BIAS/VARIANCE ANALYSIS  Here we show the bias/variance analysis from Fig. 1 B (main paper) applied to the estimated log(p(x)) w.r.t. the number of test samples.  Figure 5: Bias and standard deviation of the low-sample estimated log(p(x)) (bootstrapping with K=5,000 primary samples from a SBN/SBN 10-200-200 network trained on MNIST).  11  100101102103# samples20151050Biasbias (epoch 50)bias (last epoch)0.00.20.40.60.81.01.21.41.6std-dev.std. dev. (epoch 50)std. dev. (last epoch)Published as a conference paper at ICLR 2015  6.3.3 UCI BINARY DATASETS  We performed a series of experiments on 8 different binary datasets from the UCI database: For each dataset we screened a limited hyperparameter space: The learning rate was set to a value in 0.001, 0.003, 0.01. For SBNs we use K=10 training samples and we tried the following archi- tectures: Two hidden layers with 10-50, 10-75, 10-100, 10-150 or 10-200 hidden units and three hidden layers with 5-20-100, 10-50-100, 10-50-150, 10-50-200 or 10-100-300 hidden units. We trained NADE/NADE models with K=5 training samples and one hidden layer with 30, 50, 75, 100 or 200 units in it.  Model FVSBN NADE∗ EoNADE+ DARN3 RWS - SBN hidden units RWS - NADE hidden units  ADULT 13.17 13.19 13.19 13.19 13.65  5-20-100  13.16 30  CONNECT4  12.39 11.99 12.58 11.91 12.68  11.68 50  10-50-150  DNA 83.64 84.81 82.31 81.04 90.63 10-150 84.26 100  MUSHROOMS  10.27 9.81 9.68 9.55 9.90  9.71 50  10-50-150  NIPS-0-12 276.88 273.08 272.38 274.68 272.54  10-50-150  271.11  75  OCR-LETTERS  39.30 27.22 27.31 28.17 29.99  26.43 100  10-100-300  RCV1 49.84 46.66 46.12 46.10 46.16  10-50-200  46.09  WEB 29.35 28.39 27.87 28.83 28.18  10-50-300  27.92  Table 3: Results on various binary datasets from the UCI repository. The top two rows quote the baseline results from Larochelle & Murray (2011); the third row shows the baseline results taken from Uria, Murray, Larochelle (2014). (NADE∗: 500 hidden units; EoNADE+: 1hl, 16 ord)  12  ",
1412.7062,2015,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,"['Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs', 'Liang-Chieh Chen', 'George Papandreou', 'Iasonas Kokkinos', 'Kevin Murphy', 'and Alan Yuille']",https://arxiv.org/pdf/1412.7062,"6 1 0 2     n u J    7      ]  V C . s c [      4 v 2 6 0 7  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  SEMANTIC IMAGE SEGMENTATION WITH DEEP CON- VOLUTIONAL NETS AND FULLY CONNECTED CRFS  Liang-Chieh Chen Univ. of California, Los Angeles lcchen@cs.ucla.edu  George Papandreou ∗ Google Inc. gpapan@google.com  Iasonas Kokkinos CentraleSup´elec and INRIA iasonas.kokkinos@ecp.fr  Kevin Murphy Google Inc. kpmurphy@google.com  Alan L. Yuille Univ. of California, Los Angeles yuille@stat.ucla.edu  ABSTRACT  Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classiﬁcation and ob- ject detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classiﬁcation (also called ”semantic image segmentation”). We show that responses at the ﬁnal layer of DCNNs are not sufﬁciently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the ﬁnal DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantita- tively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efﬁciently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.  1  INTRODUCTION  Deep Convolutional Neural Networks (DCNNs) had been the method of choice for document recog- nition since LeCun et al. (1998), but have only recently become the mainstream of high-level vision research. Over the past two years DCNNs have pushed the performance of computer vision sys- tems to soaring heights on a broad array of high-level problems, including image classiﬁcation (Krizhevsky et al., 2013; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014; ∗Work initiated when G.P. was with the Toyota Technological Institute at Chicago. The ﬁrst two authors  contributed equally to this work.  1  Published as a conference paper at ICLR 2015  Papandreou et al., 2014), object detection (Girshick et al., 2014), ﬁne-grained categorization (Zhang et al., 2014), among others. A common theme in these works is that DCNNs trained in an end-to-end manner deliver strikingly better results than systems relying on carefully engineered representations, such as SIFT or HOG features. This success can be partially attributed to the built-in invariance of DCNNs to local image transformations, which underpins their ability to learn hierarchical abstrac- tions of data (Zeiler & Fergus, 2014). While this invariance is clearly desirable for high-level vision tasks, it can hamper low-level tasks, such as pose estimation (Chen & Yuille, 2014; Tompson et al., 2014) and semantic segmentation - where we want precise localization, rather than abstraction of spatial details. There are two technical hurdles in the application of DCNNs to image labeling tasks: signal down- sampling, and spatial ‘insensitivity’ (invariance). The ﬁrst problem relates to the reduction of signal resolution incurred by the repeated combination of max-pooling and downsampling (‘striding’) per- formed at every layer of standard DCNNs (Krizhevsky et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014). Instead, as in Papandreou et al. (2014), we employ the ‘atrous’ (with holes) algorithm originally developed for efﬁciently computing the undecimated discrete wavelet transform (Mallat, 1999). This allows efﬁcient dense computation of DCNN responses in a scheme substan- tially simpler than earlier solutions to this problem (Giusti et al., 2013; Sermanet et al., 2013). The second problem relates to the fact that obtaining object-centric decisions from a classiﬁer re- quires invariance to spatial transformations, inherently limiting the spatial accuracy of the DCNN model. We boost our model’s ability to capture ﬁne details by employing a fully-connected Condi- tional Random Field (CRF). Conditional Random Fields have been broadly used in semantic seg- mentation to combine class scores computed by multi-way classiﬁers with the low-level information captured by the local interactions of pixels and edges (Rother et al., 2004; Shotton et al., 2009) or superpixels (Lucchi et al., 2011). Even though works of increased sophistication have been proposed to model the hierarchical dependency (He et al., 2004; Ladicky et al., 2009; Lempitsky et al., 2011) and/or high-order dependencies of segments (Delong et al., 2012; Gonfaus et al., 2010; Kohli et al., 2009; Chen et al., 2013; Wang et al., 2015), we use the fully connected pairwise CRF proposed by Kr¨ahenb¨uhl & Koltun (2011) for its efﬁcient computation, and ability to capture ﬁne edge details while also catering for long range dependencies. That model was shown in Kr¨ahenb¨uhl & Koltun (2011) to largely improve the performance of a boosting-based pixel-level classiﬁer, and in our work we demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level classiﬁer. The three main advantages of our “DeepLab” system are (i) speed: by virtue of the ‘atrous’ algo- rithm, our dense DCNN operates at 8 fps, while Mean Field Inference for the fully-connected CRF requires 0.5 second, (ii) accuracy: we obtain state-of-the-art results on the PASCAL semantic seg- mentation challenge, outperforming the second-best approach of Mostajabi et al. (2014) by a margin of 7.2% and (iii) simplicity: our system is composed of a cascade of two fairly well-established mod- ules, DCNNs and CRFs.  2 RELATED WORK  Our system works directly on the pixel representation, similarly to Long et al. (2014). This is in con- trast to the two-stage approaches that are now most common in semantic segmentation with DCNNs: such techniques typically use a cascade of bottom-up image segmentation and DCNN-based region classiﬁcation, which makes the system commit to potential errors of the front-end segmentation sys- tem. For instance, the bounding box proposals and masked regions delivered by (Arbel´aez et al., 2014; Uijlings et al., 2013) are used in Girshick et al. (2014) and (Hariharan et al., 2014b) as inputs to a DCNN to introduce shape information into the classiﬁcation process. Similarly, the authors of Mostajabi et al. (2014) rely on a superpixel representation. A celebrated non-DCNN precursor to these works is the second order pooling method of (Carreira et al., 2012) which also assigns labels to the regions proposals delivered by (Carreira & Sminchisescu, 2012). Understanding the perils of committing to a single segmentation, the authors of Cogswell et al. (2014) build on (Yadollah- pour et al., 2013) to explore a diverse set of CRF-based segmentation proposals, computed also by (Carreira & Sminchisescu, 2012). These segmentation proposals are then re-ranked according to a DCNN trained in particular for this reranking task. Even though this approach explicitly tries to handle the temperamental nature of a front-end segmentation algorithm, there is still no explicit ex-  2  Published as a conference paper at ICLR 2015  ploitation of the DCNN scores in the CRF-based segmentation algorithm: the DCNN is only applied post-hoc, while it would make sense to directly try to use its results during segmentation. Moving towards works that lie closer to our approach, several other researchers have considered the use of convolutionally computed DCNN features for dense image labeling. Among the ﬁrst have been Farabet et al. (2013) who apply DCNNs at multiple image resolutions and then employ a segmentation tree to smooth the prediction results; more recently, Hariharan et al. (2014a) propose to concatenate the computed inter-mediate feature maps within the DCNNs for pixel classiﬁcation, and Dai et al. (2014) propose to pool the inter-mediate feature maps by region proposals. Even though these works still employ segmentation algorithms that are decoupled from the DCNN classiﬁer’s results, we believe it is advantageous that segmentation is only used at a later stage, avoiding the commitment to premature decisions. More recently, the segmentation-free techniques of (Long et al., 2014; Eigen & Fergus, 2014) di- rectly apply DCNNs to the whole image in a sliding window fashion, replacing the last fully con- nected layers of a DCNN by convolutional layers. In order to deal with the spatial localization issues outlined in the beginning of the introduction, Long et al. (2014) upsample and concatenate the scores from inter-mediate feature maps, while Eigen & Fergus (2014) reﬁne the prediction result from coarse to ﬁne by propagating the coarse results to another DCNN. The main difference between our model and other state-of-the-art models is the combination of pixel-level CRFs and DCNN-based ‘unary terms’. Focusing on the closest works in this direction, Cogswell et al. (2014) use CRFs as a proposal mechanism for a DCNN-based reranking system, while Farabet et al. (2013) treat superpixels as nodes for a local pairwise CRF and use graph-cuts for discrete inference; as such their results can be limited by errors in superpixel computations, while ig- noring long-range superpixel dependencies. Our approach instead treats every pixel as a CRF node, exploits long-range dependencies, and uses CRF inference to directly optimize a DCNN-driven cost function. We note that mean ﬁeld had been extensively studied for traditional image segmenta- tion/edge detection tasks, e.g., (Geiger & Girosi, 1991; Geiger & Yuille, 1991; Kokkinos et al., 2008), but recently Kr¨ahenb¨uhl & Koltun (2011) showed that the inference can be very efﬁcient for fully connected CRF and particularly effective in the context of semantic segmentation. After the ﬁrst version of our manuscript was made publicly available, it came to our attention that two other groups have independently and concurrently pursued a very similar direction, combining DCNNs and densely connected CRFs (Bell et al., 2014; Zheng et al., 2015). There are several differences in technical aspects of the respective models. Bell et al. (2014) focus on the problem of material classiﬁcation, while Zheng et al. (2015) unroll the CRF mean-ﬁeld inference steps to convert the whole system into an end-to-end trainable feed-forward network. We have updated our proposed “DeepLab” system with much improved methods and results in our latest work (Chen et al., 2016). We refer the interested reader to the paper for details.  3 CONVOLUTIONAL NEURAL NETWORKS FOR DENSE IMAGE LABELING  Herein we describe how we have re-purposed and ﬁnetuned the publicly available Imagenet- pretrained state-of-art 16-layer classiﬁcation network of (Simonyan & Zisserman, 2014) (VGG-16) into an efﬁcient and effective dense feature extractor for our dense semantic image segmentation system.  3.1 EFFICIENT DENSE SLIDING WINDOW FEATURE EXTRACTION WITH THE HOLE  ALGORITHM  Dense spatial score evaluation is instrumental in the success of our dense CNN feature extractor. As a ﬁrst step to implement this, we convert the fully-connected layers of VGG-16 into convolutional ones and run the network in a convolutional fashion on the image at its original resolution. However this is not enough as it yields very sparsely computed detection scores (with a stride of 32 pixels). To compute scores more densely at our target stride of 8 pixels, we develop a variation of the method previously employed by Giusti et al. (2013); Sermanet et al. (2013). We skip subsampling after the last two max-pooling layers in the network of Simonyan & Zisserman (2014) and modify the convolutional ﬁlters in the layers that follow them by introducing zeros to increase their length (2×in  3  Published as a conference paper at ICLR 2015  Figure 1: Illustration of the hole algorithm in 1-D, when kernel size = 3, input stride = 2, and output stride = 1.  the last three convolutional layers and 4×in the ﬁrst fully connected layer). We can implement this more efﬁciently by keeping the ﬁlters intact and instead sparsely sample the feature maps on which they are applied on using an input stride of 2 or 4 pixels, respectively. This approach, illustrated in Fig. 1 is known as the ‘hole algorithm’ (‘atrous algorithm’) and has been developed before for efﬁcient computation of the undecimated wavelet transform (Mallat, 1999). We have implemented this within the Caffe framework (Jia et al., 2014) by adding to the im2col function (it converts multi- channel feature maps to vectorized patches) the option to sparsely sample the underlying feature map. This approach is generally applicable and allows us to efﬁciently compute dense CNN feature maps at any target subsampling rate without introducing any approximations. We ﬁnetune the model weights of the Imagenet-pretrained VGG-16 network to adapt it to the image classiﬁcation task in a straightforward fashion, following the procedure of Long et al. (2014). We replace the 1000-way Imagenet classiﬁer in the last layer of VGG-16 with a 21-way one. Our loss function is the sum of cross-entropy terms for each spatial position in the CNN output map (subsampled by 8 compared to the original image). All positions and labels are equally weighted in the overall loss function. Our targets are the ground truth labels (subsampled by 8). We optimize the objective function with respect to the weights at all network layers by the standard SGD procedure of Krizhevsky et al. (2013). During testing, we need class score maps at the original image resolution. As illustrated in Figure 2 and further elaborated in Section 4.1, the class score maps (corresponding to log-probabilities) are quite smooth, which allows us to use simple bilinear interpolation to increase their resolution by a factor of 8 at a negligible computational cost. Note that the method of Long et al. (2014) does not use the hole algorithm and produces very coarse scores (subsampled by a factor of 32) at the CNN output. This forced them to use learned upsampling layers, signiﬁcantly increasing the complexity and training time of their system: Fine-tuning our network on PASCAL VOC 2012 takes about 10 hours, while they report a training time of several days (both timings on a modern GPU).  3.2 CONTROLLING THE RECEPTIVE FIELD SIZE AND ACCELERATING DENSE  COMPUTATION WITH CONVOLUTIONAL NETS  Another key ingredient in re-purposing our network for dense score computation is explicitly con- trolling the network’s receptive ﬁeld size. Most recent DCNN-based image recognition methods rely on networks pre-trained on the Imagenet large-scale classiﬁcation task. These networks typi- cally have large receptive ﬁeld size: in the case of the VGG-16 net we consider, its receptive ﬁeld is 224× 224 (with zero-padding) and 404× 404 pixels if the net is applied convolutionally. After converting the network to a fully convolutional one, the ﬁrst fully connected layer has 4,096 ﬁl- ters of large 7× 7 spatial size and becomes the computational bottleneck in our dense score map computation. We have addressed this practical problem by spatially subsampling (by simple decimation) the ﬁrst FC layer to 4×4 (or 3×3) spatial size. This has reduced the receptive ﬁeld of the network down to 128×128 (with zero-padding) or 308×308 (in convolutional mode) and has reduced computation time for the ﬁrst FC layer by 2 − 3 times. Using our Caffe-based implementation and a Titan GPU, the resulting VGG-derived network is very efﬁcient: Given a 306×306 input image, it produces 39×39  4  Input strideOutput stridePublished as a conference paper at ICLR 2015  dense raw feature scores at the top of the network at a rate of about 8 frames/sec during testing. The speed during training is 3 frames/sec. We have also successfully experimented with reducing the number of channels at the fully connected layers from 4,096 down to 1,024, considerably further decreasing computation time and memory footprint without sacriﬁcing performance, as detailed in Section 5. Using smaller networks such as Krizhevsky et al. (2013) could allow video-rate test-time dense feature computation even on light-weight GPUs.  4 DETAILED BOUNDARY RECOVERY: FULLY-CONNECTED CONDITIONAL  RANDOM FIELDS AND MULTI-SCALE PREDICTION  4.1 DEEP CONVOLUTIONAL NETWORKS AND THE LOCALIZATION CHALLENGE  As illustrated in Figure 2, DCNN score maps can reliably predict the presence and rough position of objects in an image but are less well suited for pin-pointing their exact outline. There is a natural trade-off between classiﬁcation accuracy and localization accuracy with convolutional networks: Deeper models with multiple max-pooling layers have proven most successful in classiﬁcation tasks, however their increased invariance and large receptive ﬁelds make the problem of inferring position from the scores at their top output levels more challenging. Recent work has pursued two directions to address this localization challenge. The ﬁrst approach is to harness information from multiple layers in the convolutional network in order to better estimate the object boundaries (Long et al., 2014; Eigen & Fergus, 2014). The second approach is to employ a super-pixel representation, essentially delegating the localization task to a low-level segmentation method. This route is followed by the very successful recent method of Mostajabi et al. (2014). In Section 4.2, we pursue a novel alternative direction based on coupling the recognition capacity of DCNNs and the ﬁne-grained localization accuracy of fully connected CRFs and show that it is remarkably successful in addressing the localization challenge, producing accurate semantic seg- mentation results and recovering object boundaries at a level of detail that is well beyond the reach of existing methods.  4.2 FULLY-CONNECTED CONDITIONAL RANDOM FIELDS FOR ACCURATE LOCALIZATION  Image/G.T.  DCNN output  CRF Iteration 1 CRF Iteration 2 CRF Iteration 10  Figure 2: Score map (input before softmax function) and belief map (output of softmax function) for Aeroplane. We show the score (1st row) and belief (2nd row) maps after each mean ﬁeld iteration. The output of last DCNN layer is used as input to the mean ﬁeld inference. Best viewed in color.  Traditionally, conditional random ﬁelds (CRFs) have been employed to smooth noisy segmentation maps (Rother et al., 2004; Kohli et al., 2009). Typically these models contain energy terms that couple neighboring nodes, favoring same-label assignments to spatially proximal pixels. Qualita- tively, the primary function of these short-range CRFs has been to clean up the spurious predictions of weak classiﬁers built on top of local hand-engineered features. Compared to these weaker classiﬁers, modern DCNN architectures such as the one we use in this work produce score maps and semantic label predictions which are qualitatively different. As illus- trated in Figure 2, the score maps are typically quite smooth and produce homogeneous classiﬁcation results. In this regime, using short-range CRFs can be detrimental, as our goal should be to recover detailed local structure rather than further smooth it. Using contrast-sensitive potentials (Rother  5  Published as a conference paper at ICLR 2015  Figure 3: Model Illustration. The coarse score map from Deep Convolutional Neural Network (with fully convolutional layers) is upsampled by bi-linear interpolation. A fully connected CRF is applied to reﬁne the segmentation result. Best viewed in color.  et al., 2004) in conjunction to local-range CRFs can potentially improve localization but still miss thin-structures and typically requires solving an expensive discrete optimization problem. To overcome these limitations of short-range CRFs, we integrate into our system the fully connected CRF model of Kr¨ahenb¨uhl & Koltun (2011). The model employs the energy function  E(x) =  θi(xi) +  θij(xi, xj)  (1)  (cid:88)  (cid:88)  i  ij  is θij(xi, xj) = µ(xi, xj)(cid:80)K  where x is the label assignment for pixels. We use as unary potential θi(xi) = − log P (xi), where P (xi) is the label assignment probability at pixel i as computed by DCNN. The pairwise potential (cid:54)= xj, and zero otherwise (i.e., Potts Model). There is one pairwise term for each pair of pixels i and j in the image no matter how far from each other they lie, i.e. the model’s factor graph is fully connected. Each km is the Gaussian kernel depends on features (denoted as f) extracted for pixel i and j and is weighted by parameter wm. We adopt bilateral position and color terms, speciﬁcally, the kernels are  m=1 wm · km(f i, f j), where µ(xi, xj) = 1 if xi  (cid:16) − ||pi − pj||2  2σ2 α  w1 exp  (cid:17)  − ||Ii − Ij||2  2σ2 β  + w2 exp  (cid:16) − ||pi − pj||2  (cid:17)  2σ2 γ  (2)  tion b(x) = (cid:81)  where the ﬁrst kernel depends on both pixel positions (denoted as p) and pixel color intensities (denoted as I), and the second kernel only depends on pixel positions. The hyper parameters σα, σβ and σγ control the “scale” of the Gaussian kernels. Crucially, this model is amenable to efﬁcient approximate probabilistic inference (Kr¨ahenb¨uhl & Koltun, 2011). The message passing updates under a fully decomposable mean ﬁeld approxima- i bi(xi) can be expressed as convolutions with a Gaussian kernel in feature space. High-dimensional ﬁltering algorithms (Adams et al., 2010) signiﬁcantly speed-up this computation resulting in an algorithm that is very fast in practice, less that 0.5 sec on average for Pascal VOC images using the publicly available implementation of (Kr¨ahenb¨uhl & Koltun, 2011).  4.3 MULTI-SCALE PREDICTION  Following the promising recent results of (Hariharan et al., 2014a; Long et al., 2014) we have also explored a multi-scale prediction method to increase the boundary localization accuracy. Specif- ically, we attach to the input image and the output of each of the ﬁrst four max pooling layers a two-layer MLP (ﬁrst layer: 128 3x3 convolutional ﬁlters, second layer: 128 1x1 convolutional ﬁl- ters) whose feature map is concatenated to the main network’s last layer feature map. The aggregate feature map fed into the softmax layer is thus enhanced by 5 * 128 = 640 channels. We only adjust the newly added weights, keeping the other network parameters to the values learned by the method of Section 3. As discussed in the experimental section, introducing these extra direct connections from ﬁne-resolution layers improves localization performance, yet the effect is not as dramatic as the one obtained with the fully-connected CRF.  6  Deep Convolutional Neural NetworkInputAeroplaneCoarse Score mapBi-linear InterpolationFully Connected CRFFinal OutputPublished as a conference paper at ICLR 2015  Method DeepLab DeepLab-CRF DeepLab-MSc DeepLab-MSc-CRF DeepLab-7x7 DeepLab-CRF-7x7 DeepLab-LargeFOV DeepLab-CRF-LargeFOV DeepLab-MSc-LargeFOV DeepLab-MSc-CRF-LargeFOV  (a)  mean IOU (%)  59.80 63.74 61.30 65.21 64.38 67.64 62.25 67.64 64.21 68.70  Method MSRA-CFM FCN-8s TTI-Zoomout-16 DeepLab-CRF DeepLab-MSc-CRF DeepLab-CRF-7x7 DeepLab-CRF-LargeFOV DeepLab-MSc-CRF-LargeFOV  mean IOU (%)  61.8 62.2 64.4 66.4 67.1 70.3 70.3 71.6  (b)  Table 1: (a) Performance of our proposed models on the PASCAL VOC 2012 ‘val’ set (with training in the augmented ‘train’ set). The best performance is achieved by exploiting both multi-scale features and large ﬁeld-of-view. (b) Performance of our proposed models (with training in the augmented ‘trainval’ set) compared to other state-of-art methods on the PASCAL VOC 2012 ‘test’ set.  5 EXPERIMENTAL EVALUATION  Dataset We test our DeepLab model on the PASCAL VOC 2012 segmentation benchmark (Ev- eringham et al., 2014), consisting of 20 foreground object classes and one background class. The original dataset contains 1, 464, 1, 449, and 1, 456 images for training, validation, and testing, re- spectively. The dataset is augmented by the extra annotations provided by Hariharan et al. (2011), resulting in 10, 582 training images. The performance is measured in terms of pixel intersection- over-union (IOU) averaged across the 21 classes.  Training We adopt the simplest form of piecewise training, decoupling the DCNN and CRF train- ing stages, assuming the unary terms provided by the DCNN are ﬁxed during CRF training. For DCNN training we employ the VGG-16 network which has been pre-trained on ImageNet. We ﬁne-tuned the VGG-16 network on the VOC 21-way pixel-classiﬁcation task by stochastic gradient descent on the cross-entropy loss function, as described in Section 3.1. We use a mini-batch of 20 images and initial learning rate of 0.001 (0.01 for the ﬁnal classiﬁer layer), multiplying the learning rate by 0.1 at every 2000 iterations. We use momentum of 0.9 and a weight decay of 0.0005. After the DCNN has been ﬁne-tuned, we cross-validate the parameters of the fully connected CRF model in Eq. (2) along the lines of Kr¨ahenb¨uhl & Koltun (2011). We use the default values of w2 = 3 and σγ = 3 and we search for the best values of w1, σα, and σβ by cross-validation on a small subset of the validation set (we use 100 images). We employ coarse-to-ﬁne search scheme. Speciﬁcally, the initial search range of the parameters are w1 ∈ [5, 10], σα ∈ [50 : 10 : 100] and σβ ∈ [3 : 1 : 10] (MATLAB notation), and then we reﬁne the search step sizes around the ﬁrst round’s best values. We ﬁx the number of mean ﬁeld iterations to 10 for all reported experiments.  Evaluation on Validation set We conduct the majority of our evaluations on the PASCAL ‘val’ set, training our model on the augmented PASCAL ‘train’ set. As shown in Tab. 1 (a), incorporating the fully connected CRF to our model (denoted by DeepLab-CRF) yields a substantial performance boost, about 4% improvement over DeepLab. We note that the work of Kr¨ahenb¨uhl & Koltun (2011) improved the 27.6% result of TextonBoost (Shotton et al., 2009) to 29.1%, which makes the improvement we report here (from 59.8% to 63.7%) all the more impressive. Turning to qualitative results, we provide visual comparisons between DeepLab and DeepLab-CRF in Fig. 7. Employing a fully connected CRF signiﬁcantly improves the results, allowing the model to accurately capture intricate object boundaries.  Multi-Scale features We also exploit the features from the intermediate layers, similar to Hariha- ran et al. (2014a); Long et al. (2014). As shown in Tab. 1 (a), adding the multi-scale features to our  7  Published as a conference paper at ICLR 2015  Method DeepLab-CRF-7x7 DeepLab-CRF DeepLab-CRF-4x4 DeepLab-CRF-LargeFOV  kernel size  7×7 4×4 4×4 3×3  input stride  receptive ﬁeld  # parameters  mean IOU (%)  Training speed (img/sec)  4 4 8 12  224 128 224 224  134.3M 65.1M 65.1M 20.5M  67.64 63.74 67.14 67.64  1.44 2.90 2.90 4.84  Table 2: Effect of Field-Of-View. We show the performance (after CRF) and training speed on the PASCAL VOC 2012 ‘val’ set as the function of (1) the kernel size of ﬁrst fully connected layer, (2) the input stride value employed in the atrous algorithm.  DeepLab model (denoted as DeepLab-MSc) improves about 1.5% performance, and further incor- porating the fully connected CRF (denoted as DeepLab-MSc-CRF) yields about 4% improvement. The qualitative comparisons between DeepLab and DeepLab-MSc are shown in Fig. 4. Leveraging the multi-scale features can slightly reﬁne the object boundaries.  Field of View The ‘atrous algorithm’ we employed allows us to arbitrarily control the Field-of- View (FOV) of the models by adjusting the input stride, as illustrated in Fig. 1. In Tab. 2, we experiment with several kernel sizes and input strides at the ﬁrst fully connected layer. The method, DeepLab-CRF-7x7, is the direct modiﬁcation from VGG-16 net, where the kernel size = 7×7 and input stride = 4. This model yields performance of 67.64% on the ‘val’ set, but it is relatively slow (1.44 images per second during training). We have improved model speed to 2.9 images per second by reducing the kernel size to 4× 4. We have experimented with two such network variants with different FOV sizes, DeepLab-CRF and DeepLab-CRF-4x4; the latter has large FOV (i.e., large input stride) and attains better performance. Finally, we employ kernel size 3×3 and input stride = 12, and further change the ﬁlter sizes from 4096 to 1024 for the last two layers. Interestingly, the resulting model, DeepLab-CRF-LargeFOV, matches the performance of the expensive DeepLab- CRF-7x7. At the same time, it is 3.36 times faster to run and has signiﬁcantly fewer parameters (20.5M instead of 134.3M). The performance of several model variants is summarized in Tab. 1, showing the beneﬁt of exploiting multi-scale features and large FOV.  Figure 4: Incorporating multi-scale features improves the boundary segmentation. We show the results obtained by DeepLab and DeepLab-MSc in the ﬁrst and second row, respectively. Best viewed in color.  Mean Pixel IOU along Object Boundaries To quantify the accuracy of the proposed model near object boundaries, we evaluate the segmentation accuracy with an experiment similar to Kohli et al. (2009); Kr¨ahenb¨uhl & Koltun (2011). Speciﬁcally, we use the ‘void’ label annotated in val set, which usually occurs around object boundaries. We compute the mean IOU for those pixels that are located within a narrow band (called trimap) of ‘void’ labels. As shown in Fig. 5, exploiting the multi-scale features from the intermediate layers and reﬁning the segmentation results by a fully connected CRF signiﬁcantly improve the results around object boundaries.  Comparison with State-of-art In Fig. 6, we qualitatively compare our proposed model, DeepLab- CRF, with two state-of-art models: FCN-8s (Long et al., 2014) and TTI-Zoomout-16 (Mostajabi et al., 2014) on the ‘val’ set (the results are extracted from their papers). Our model is able to capture the intricate object boundaries.  8  Published as a conference paper at ICLR 2015  (a)  (b)  (c)  Figure 5: (a) Some trimap examples (top-left: image. top-right: ground-truth. bottom-left: trimap of 2 pixels. bottom-right: trimap of 10 pixels). Quality of segmentation result within a band around the object boundaries for the proposed methods. (b) Pixelwise accuracy. (c) Pixel mean IOU.  (a) FCN-8s vs. DeepLab-CRF  (b) TTI-Zoomout-16 vs. DeepLab-CRF  Figure 6: Comparisons with state-of-the-art models on the val set. First row: images. Second row: ground truths. Third row: other recent models (Left: FCN-8s, Right: TTI-Zoomout-16). Fourth row: our DeepLab-CRF. Best viewed in color.  Reproducibility We have implemented the proposed methods by extending the excellent Caffe framework (Jia et al., 2014). We share our source code, conﬁguration ﬁles, and trained models that allow reproducing the results in this paper at a companion web site https://bitbucket.org/ deeplab/deeplab-public.  Test set results Having set our model choices on the validation set, we evaluate our model variants on the PASCAL VOC 2012 ofﬁcial ‘test’ set. As shown in Tab. 3, our DeepLab-CRF and DeepLab- MSc-CRF models achieve performance of 66.4% and 67.1% mean IOU1, respectively. Our models outperform all the other state-of-the-art models (speciﬁcally, TTI-Zoomout-16 (Mostajabi et al., 2014), FCN-8s (Long et al., 2014), and MSRA-CFM (Dai et al., 2014)). When we increase the FOV of the models, DeepLab-CRF-LargeFOV yields performance of 70.3%, the same as DeepLab-CRF- 7x7, while its training speed is faster. Furthermore, our best model, DeepLab-MSc-CRF-LargeFOV, attains the best performance of 71.6% by employing both multi-scale features and large FOV.  1http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?  challengeid=11&compid=6  9  05101520253035405560657075808590Pixelwise Accuracy (%)Trimap Width (pixels)  DL−MSc−CRFDeepLab−CRFDeepLab−MScDeepLab051015202530354035404550556065mean IOU (%)Trimap Width (pixels)  DL−MSc−CRFDeepLab−CRFDeepLab−MScDeepLabPublished as a conference paper at ICLR 2015  Figure 7: Visualization results on VOC 2012-val. For each row, we show the input image, the segmentation result delivered by the DCNN (DeepLab), and the reﬁned segmentation result of the Fully Connected CRF (DeepLab-CRF). We show our failure modes in the last three rows. Best viewed in color.  10  Published as a conference paper at ICLR 2015  bkg aero bike bird boat bottle bus - -  Method 75.7 26.7 69.5 48.8 65.6 81.0 69.2 73.3 30.0 68.7 51.5 69.1 68.1 MSRA-CFM 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 FCN-8s 89.8 81.9 35.1 78.2 57.4 56.5 80.5 74.0 79.8 22.4 69.6 53.7 74.0 76.0 TTI-Zoomout-16 92.1 78.4 33.1 78.2 55.6 65.3 81.3 75.5 78.6 25.3 69.2 52.7 75.2 69.0 DeepLab-CRF 92.6 80.4 36.8 77.4 55.2 66.4 81.5 77.5 78.9 27.1 68.2 52.7 74.3 69.6 DeepLab-MSc-CRF 92.8 83.9 36.6 77.5 58.4 68.0 84.6 79.7 83.1 29.5 74.6 59.3 78.9 76.0 DeepLab-CRF-7x7 92.6 83.5 36.6 82.5 62.3 66.5 85.4 78.5 83.7 30.4 72.9 60.4 78.5 75.5 DeepLab-CRF-LargeFOV DeepLab-MSc-CRF-LargeFOV 93.1 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1  car  cat  71.7 76.5 76.6 79.1 79.4 82.1 82.1 83.2  67.5 73.9 68.8 77.6 79.0 80.6 79.7 80.8  50.4 45.2 44.3 54.7 56.9 60.3 58.2 59.7  66.5 44.4 58.9 53.5 72.4 37.4 70.9 55.1 70.2 40.2 68.9 55.3 78.3 45.1 73.3 56.2 78.8 45.2 72.7 59.3 81.7 49.2 78.0 60.7 82.0 48.8 73.7 63.3 82.2 50.4 73.1 63.7  tv mean 61.8 62.2 64.4 66.4 67.1 70.3 70.3 71.6  chair cow table dog horse mbike person plant sheep sofa train  Table 3: Labeling IOU (%) on the PASCAL VOC 2012 test set, using the trainval set for training.  6 DISCUSSION  Our work combines ideas from deep convolutional neural networks and fully-connected conditional random ﬁelds, yielding a novel method able to produce semantically accurate predictions and de- tailed segmentation maps, while being computationally efﬁcient. Our experimental results show that the proposed method signiﬁcantly advances the state-of-art in the challenging PASCAL VOC 2012 semantic image segmentation task. There are multiple aspects in our model that we intend to reﬁne, such as fully integrating its two main components (CNN and CRF) and train the whole system in an end-to-end fashion, similar to Kr¨ahenb¨uhl & Koltun (2013); Chen et al. (2014); Zheng et al. (2015). We also plan to experiment with more datasets and apply our method to other sources of data such as depth maps or videos. Re- cently, we have pursued model training with weakly supervised annotations, in the form of bounding boxes or image-level labels (Papandreou et al., 2015). At a higher level, our work lies in the intersection of convolutional neural networks and probabilistic graphical models. We plan to further investigate the interplay of these two powerful classes of methods and explore their synergistic potential for solving challenging computer vision tasks.  ACKNOWLEDGMENTS  This work was partly supported by ARO 62250-CS, NIH Grant 5R01EY022247-03, EU Project RECONFIG FP7-ICT-600825 and EU Project MOBOT FP7-ICT-2011-600796. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research. We would like to thank the anonymous reviewers for their detailed comments and constructive feed- back.  PAPER REVISIONS  Here we present the list of major paper revisions for the convenience of the readers.  v1 Submission to ICLR 2015. Introduces the model DeepLab-CRF, which attains the performance of 66.4% on PASCAL VOC 2012 test set.  v2 Rebuttal for ICLR 2015. Adds the model DeepLab-MSc-CRF, which incorporates multi-scale features from the intermediate layers. DeepLab-MSc-CRF yields the performance of 67.1% on PASCAL VOC 2012 test set.  v3 Camera-ready for ICLR 2015. Experiments with large Field-Of-View. On PASCAL VOC 2012 test set, DeepLab-CRF-LargeFOV achieves the performance of 70.3%. When exploiting both mutli- scale features and large FOV, DeepLab-MSc-CRF-LargeFOV attains the performance of 71.6%.  v4 Reference to our updated “DeepLab” system (Chen et al., 2016) with much improved results.  REFERENCES Adams, A., Baek, J., and Davis, M. A. Fast high-dimensional ﬁltering using the permutohedral  lattice. In Computer Graphics Forum, 2010.  Arbel´aez, P., Pont-Tuset, J., Barron, J. T., Marques, F., and Malik, J. Multiscale combinatorial  grouping. In CVPR, 2014.  11  Published as a conference paper at ICLR 2015  Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials  in context database. arXiv:1412.0623, 2014.  Carreira, J. and Sminchisescu, C. Cpmc: Automatic object segmentation using constrained para-  metric min-cuts. PAMI, 2012.  Carreira, J., Caseiro, R., Batista, J., and Sminchisescu, C. Semantic segmentation with second-order  pooling. In ECCV, 2012.  Chen, L.-C., Papandreou, G., and Yuille, A. Learning a dictionary of shape epitomes with applica-  tions to image labeling. In ICCV, 2013.  Chen, L.-C., Schwing, A., Yuille, A., and Urtasun, R.  arXiv:1407.2538, 2014.  Learning deep structured models.  Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv:1606.00915, 2016.  Chen, X. and Yuille, A. L. Articulated pose estimation by a graphical model with image dependent  pairwise relations. In NIPS, 2014.  Cogswell, M., Lin, X., Purushwalkam, S., and Batra, D. Combining the best of graphical models  and convnets for semantic segmentation. arXiv:1412.4313, 2014.  Dai, J., He, K., and Sun, J. Convolutional feature masking for joint object and stuff segmentation.  arXiv:1412.1283, 2014.  Delong, A., Osokin, A., Isack, H. N., and Boykov, Y. Fast approximate energy minimization with  label costs. IJCV, 2012.  Eigen, D. and Fergus, R. Predicting depth, surface normals and semantic labels with a common  multi-scale convolutional architecture. arXiv:1411.4734, 2014.  Everingham, M., Eslami, S. M. A., Gool, L. V., Williams, C. K. I., Winn, J., and Zisserma, A. The  pascal visual object classes challenge a retrospective. IJCV, 2014.  Farabet, C., Couprie, C., Najman, L., and LeCun, Y. Learning hierarchical features for scene label-  ing. PAMI, 2013.  Geiger, D. and Girosi, F. Parallel and deterministic algorithms from mrfs: Surface reconstruction.  PAMI, 13(5):401–412, 1991.  Geiger, D. and Yuille, A. A common framework for image segmentation.  1991.  IJCV, 6(3):227–243,  Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object  detection and semantic segmentation. In CVPR, 2014.  Giusti, A., Ciresan, D., Masci, J., Gambardella, L., and Schmidhuber, J. Fast image scanning with  deep max-pooling convolutional neural networks. In ICIP, 2013.  Gonfaus, J. M., Boix, X., Van de Weijer, J., Bagdanov, A. D., Serrat, J., and Gonzalez, J. Harmony  potentials for joint classiﬁcation and segmentation. In CVPR, 2010.  Hariharan, B., Arbel´aez, P., Bourdev, L., Maji, S., and Malik, J. Semantic contours from inverse  detectors. In ICCV, 2011.  Hariharan, B., Arbel´aez, P., Girshick, R., and Malik, J. Hypercolumns for object segmentation and  ﬁne-grained localization. arXiv:1411.5752, 2014a.  Hariharan, B., Arbel´aez, P., Girshick, R., and Malik, J. Simultaneous detection and segmentation.  In ECCV, 2014b.  12  Published as a conference paper at ICLR 2015  He, X., Zemel, R. S., and Carreira-Perpindn, M. Multiscale conditional random ﬁelds for image  labeling. In CVPR, 2004.  Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., and Darrell,  T. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014.  Kohli, P., Ladicky, L., and Torr, P. H. Robust higher order potentials for enforcing label consistency.  IJCV, 2009.  Kokkinos, I., Deriche, R., Faugeras, O., and Maragos, P. Computational analysis and learning for a biologically motivated model of boundary detection. Neurocomputing, 71(10):1798–1812, 2008.  Kr¨ahenb¨uhl, P. and Koltun, V. Efﬁcient inference in fully connected crfs with gaussian edge poten-  tials. In NIPS, 2011.  Kr¨ahenb¨uhl, P. and Koltun, V. Parameter learning and convergent inference for dense random ﬁelds.  In ICML, 2013.  Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional  neural networks. In NIPS, 2013.  Ladicky, L., Russell, C., Kohli, P., and Torr, P. H. Associative hierarchical crfs for object class image  segmentation. In ICCV, 2009.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document  recognition. In Proc. IEEE, 1998.  Lempitsky, V., Vedaldi, A., and Zisserman, A. Pylon model for semantic segmentation. In NIPS,  2011.  Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation.  arXiv:1411.4038, 2014.  Lucchi, A., Li, Y., Boix, X., Smith, K., and Fua, P. Are spatial and global constraints really necessary  for segmentation? In ICCV, 2011.  Mallat, S. A Wavelet Tour of Signal Processing. Acad. Press, 2 edition, 1999.  Mostajabi, M., Yadollahpour, P., and Shakhnarovich, G. Feedforward semantic segmentation with  zoom-out features. arXiv:1412.0774, 2014.  Papandreou, G., Kokkinos, I., and Savalle, P.-A. Untangling local and global deformations in deep convolutional networks for image classiﬁcation and sliding window detection. arXiv:1412.0296, 2014.  Papandreou, G., Chen, L.-C., Murphy, K., and Yuille, A. L. Weakly- and semi-supervised learning  of a DCNN for semantic image segmentation. arXiv:1502.02734, 2015.  Rother, C., Kolmogorov, V., and Blake, A. Grabcut: Interactive foreground extraction using iterated  graph cuts. In SIGGRAPH, 2004.  Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. Overfeat: Integrated  recognition, localization and detection using convolutional networks. arXiv:1312.6229, 2013.  Shotton, J., Winn, J., Rother, C., and Criminisi, A. Textonboost for image understanding: Multi- class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV, 2009.  Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recogni-  tion. arXiv:1409.1556, 2014.  Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and  Rabinovich, A. Going deeper with convolutions. arXiv:1409.4842, 2014.  13  Published as a conference paper at ICLR 2015  Tompson, J., Jain, A., LeCun, Y., and Bregler, C. Joint Training of a Convolutional Network and a  Graphical Model for Human Pose Estimation. In NIPS, 2014.  Uijlings, J., van de Sande, K., Gevers, T., and Smeulders, A. Selective search for object recognition.  IJCV, 2013.  Wang, P., Shen, X., Lin, Z., Cohen, S., Price, B., and Yuille, A. Towards uniﬁed depth and semantic  prediction from a single image. In CVPR, 2015.  Yadollahpour, P., Batra, D., and Shakhnarovich, G. Discriminative re-ranking of diverse segmenta-  tions. In CVPR, 2013.  Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. In ECCV, 2014.  Zhang, N., Donahue, J., Girshick, R., and Darrell, T. Part-based r-cnns for ﬁne-grained category  detection. In ECCV, 2014.  Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., and Torr, P.  Conditional random ﬁelds as recurrent neural networks. arXiv:1502.03240, 2015.  14  ",
1412.7755,2015,Multiple Object Recognition with Visual Attention,"['Multiple Object Recognition with Visual Attention', 'Jimmy Ba', 'Volodymyr Mnih', 'and Koray Kavukcuoglu']",https://arxiv.org/pdf/1412.7755,"5 1 0 2    r p A 3 2         ]  G L . s c [      2 v 5 5 7 7  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  MULTIPLE OBJECT RECOGNITION WITH VISUAL ATTENTION  Jimmy Lei Ba∗  University of Toronto  jimmy@psi.utoronto.ca  Volodymyr Mnih Google DeepMind vmnih@google.com  Koray Kavukcuoglu Google DeepMind  korayk@google.com  ABSTRACT  We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.  1  INTRODUCTION  Convolutional neural networks have recently been very successful on a variety of recognition and classiﬁcation tasks (Krizhevsky et al., 2012; Goodfellow et al., 2013; Jaderberg et al., 2014a; Vinyals et al., 2014; Karpathy et al., 2014). One of the main drawbacks of convolutional networks (Con- vNets) is their poor scalability with increasing input image size so efﬁcient implementations of these models on multiple GPUs (Krizhevsky et al., 2012) or even spanning multiple machines (Dean et al., 2012b) have become necessary. Applications of ConvNets to multi-object and sequence recognition from images have avoided work- ing with big images and instead focused on using ConvNets for recognizing characters or short sequence segments from image patches containing reasonably tightly cropped instances (Goodfel- low et al., 2013; Jaderberg et al., 2014a). Applying such a recognizer to large images containing uncropped instances requires integrating it with a separately trained sequence detector or a bottom- up proposal generator. Non-maximum suppression is often performed to obtain the ﬁnal detections. While combining separate components trained using different objective functions has been shown to be worse than end-to-end training of a single system in other domains, integrating object localization and recognition into a single globally-trainable architecture has been difﬁcult. In this work, we take inspiration from the way humans perform visual sequence recognition tasks such as reading by continually moving the fovea to the next relevant object or character, recog- nizing the individual object, and adding the recognized object to our internal representation of the sequence. Our proposed system is a deep recurrent neural network that at each step processes a multi-resolution crop of the input image, called a glimpse. The network uses information from the glimpse to update its internal representation of the input, and outputs the next glimpse location and possibly the next object in the sequence. The process continues until the model decides that there are no more objects to process. We show how the proposed system can be trained end-to-end by approximately maximizing a variational lower bound on the label sequence log-likelihood. This training procedure can be used to train the model to both localize and recognize multiple objects purely from label sequences. We evaluate the model on the task of transcribing multi-digit house numbers from publicly available Google Street View imagery. Our attention-based model outperforms the state-of-the-art ConvNets on tightly cropped inputs while using both fewer parameters and much less computation. We also show that our model outperforms ConvNets by a much larger margin in the more realistic setting of larger and less tightly cropped input sequences.  1Work done while at Google DeepMind.  1  Published as a conference paper at ICLR 2015  Figure 1: The deep recurrent attention model.  2 RELATED WORK  Recognizing multiple objects in images has been one of the most important goals of computer vision. Perhaps the most common approach to image-based classiﬁcation of character sequences involves combining a sliding window detector with a character classiﬁer (Wang et al., 2012; Jaderberg et al., 2014b). The detector and the classiﬁer are typically trained separately, using different loss functions. The seminal work on ConvNets of LeCun et al. (1998) introduced a graph transformer network architecture for recognizing a sequence of digits when reading checks, and also showed how the whole system could be trained end-to-end. That system however, still relied on a number of ad-hoc components for extracting candidate locations. More recently, ConvNets operating on cropped sequences of characters have achieved state-of-the- art performance on house number recognition (Goodfellow et al., 2013) and natural scene text recog- nition (Jaderberg et al., 2014a). Goodfellow et al. (2013) trained a separate ConvNets classiﬁer for each character position in a house number with all weights except for the output layer shared among the classiﬁers. Jaderberg et al. (2014a) showed that synthetically generated images of text can be used to train ConvNets classiﬁers that achieve state-of-the-art text recognition performance on real- world images of cropped text. Our work builds on the long line of the previous attempts on attention-based visual processing (Itti et al., 1998; Larochelle & Hinton, 2010; Alexe et al., 2012), and in particular extends the recurrent attention model (RAM) proposed in Mnih et al. (2014). While RAM was shown to learn successful gaze strategies on cluttered digit classiﬁcation tasks and on a toy visual control problem it was not shown to scale to real-world image tasks or multiple objects. Our approach of learning by maximizing variational lower bound is equivalent to the reinforcement learning procedure used in RAM and is related to the work of Maes et al. (2009) who showed how reinforcement learning can be used to tackle general structured prediction problems.  3 DEEP RECURRENT VISUAL ATTENTION MODEL  For simplicity, we ﬁrst describe how our model can be applied to classifying a single object and later show how it can be extended to multiple objects. Processing an image x with an attention- based model is a sequential process with N steps, where each step consists of a saccade followed by a glimpse. At each step n, the model receives a location ln along with a glimpse observation xn taken at location ln. The model uses the observation to update its internal state and outputs the location ln+1 to process at the next time-step. Usually the number of pixels in the glimpse xn is much smaller than the number of pixels in the original image x, making the computational cost of processing a single glimpse independent of the size of the image. A graphical representation of our model is shown in Figure 2. The model can be broken down into a number of sub-components, each mapping some input into a vector output. We will use the term “network” to describe these non-linear sub-components since they are typically multi-layered neural networks.  2  ˆl1r(1)1r(2)1r(2)0ˆl2ˆl3ˆl4contextglimpseclassiﬁcationy1Icoarseemissionr(2)2r(2)3r(2)nr(1)nr(1)3r(1)2ˆln+1ys(x1,l1)(x2,l2)(x3,l3)(xn,ln)Published as a conference paper at ICLR 2015  Glimpse network: The glimpse network is a non-linear function that receives the current input im- age patch, or glimpse, xn and its location tuple ln , where ln = (xn, yn), as input and outputs a vector gn. The job of the glimpse network is to extract a set of useful features from location ln of the raw visual input. We will use Gimage(xn|Wimage) to denote the output vector from function Gimage(·) that takes an image patch xn and is parameterized by weights Wimage. Gimage(·) typ- ically consists of three convolutional hidden layers without any pooling layers followed by a fully connected layer. Separately, the location tuple is mapped by Gloc(ln|Wloc) using a fully connected hidden layer where, both Gimage(xn|Wimage) and Gloc(ln|Wloc) have the same dimension. We combine the high bandwidth image information with the low bandwidth location tuple by multiply- ing the two vectors element-wise to get the ﬁnal glimpse feature vector gn,  gn = Gimage(xn|Wimage)Gloc(ln|Wloc).  (1) This type of multiplicative interaction between “what” and “where” was initially proposed by Larochelle & Hinton (2010). Recurrent network: The recurrent network aggregates information extracted from the individual glimpses and combines the information in a coherent manner that preserves spatial information. The glimpse feature vector gn from the glimpse network is supplied as input to the recurrent network at each time step. The recurrent network consists of two recurrent layers with non-linear function Rrecur. We deﬁned the two outputs of the recurrent layers as r(1) and r(2). n , r(2)  (2) We use Long-Short-Term Memory units (Hochreiter & Schmidhuber, 1997) for the non-linearity Rrecur because of their ability to learn long-range dependencies and stable learning dynamics. Emission network: The emission network takes the current state of recurrent network as input and makes a prediction on where to extract the next image patch for the glimpse network. It acts as a controller that directs attention based on the current internal states from the recurrent network. It consists of a fully connected hidden layer that maps the feature vector r(2) n from the top recurrent layer to a coordinate tuple ˆln+1.  n−1|Wr1) and r(2)  n = Rrecur(gn, r(1) r(1)  n = Rrecur(r(1)  n−1|Wr2)  ˆln+1 = E(r(2)  n |We)  (3)  Context network: The context network provides the initial state for the recurrent network and its output is used by the emission network to predict the location of the ﬁrst glimpse. The context network C(·) takes a down-sampled low-resolution version of the whole input image Icoarse and outputs a ﬁxed length vector cI. The contextual information provides sensible hints on where the potentially interesting regions are in a given image. The context network employs three convolu- tional layers that map a coarse image Icoarse to a feature vector used as the initial state of the top recurrent layer r2 in the recurrent network. However, the bottom layer r1 is initialized with a vector of zeros for reasons we will explain later. Classiﬁcation network: The classiﬁcation network outputs a prediction for the class label y based on the ﬁnal feature vector r(1) N of the lower recurrent layer. The classiﬁcation network has one fully connected hidden layer and a softmax output layer for the class y.  P (y|I) = O(r1  n|Wo)  (4)  Ideally, the deep recurrent attention model should learn to look at locations that are relevant for classifying objects of interest. The existence of the contextual information, however, provides a “short cut” solution such that it is much easier for the model to learn from contextual information than by combining information from different glimpses. We prevent such undesirable behavior by connecting the context network and classiﬁcation network to different recurrent layers in our deep model. As a result, the contextual information cannot be used directly by the classiﬁcation network and only affects the sequence of glimpse locations produced by the model.  3.1 LEARNING WHERE AND WHAT  Given the class labels y of image I, we can formulate learning as a supervised classiﬁcation problem with the cross entropy objective function. The attention model predicts the class label conditioned on  3  Published as a conference paper at ICLR 2015  intermediate latent location variables l from each glimpse and extracts the corresponding patches. We can thus maximize likelihood of the class label by marginalizing over the glimpse locations  The marginalized objective function can be learned through optimizing its variational free energy lower bound F: log  p(l|I, W ) log p(y, l|I, W ) + H[l]  (5)  log p(y|I, W ) = log(cid:80) (cid:88)  l p(l|I, W )p(y|l, I, W ).  p(l|I, W )p(y|l, I, W ) ≥(cid:88) (cid:88)  l  l  =  l  p(l|I, W ) log p(y|l, I, W )  (6)  The learning rules can be derived by taking derivatives of the above free energy with respect to the model parameter W :  (cid:88)  ∂ log p(y|l, I, W )  (cid:20) ∂ log p(y|l, I, W )  ∂W  +  p(l|I, W )  p(l|I, W )  ∂W  log p(y|l, I, W )  ∂p(l|I, W )  l  + log p(y|l, I, W )  ∂ log p(l|I, W )  ∂W  ∂W  (cid:21)  (7)  (8)  ∂F ∂W  (cid:88) (cid:88)  l  l  =  =  For each glimpse in the glimpse sequence, it is difﬁcult to evaluate exponentially many glimpse locations during training. The summation in equation 8 can then be approximated using Monte Carlo samples.  ∂F ∂W  ≈ 1 M  M(cid:88)  m=1  ˜lm ∼ p(ln|I, W ) = N (ln; ˆln, Σ)  (cid:20) ∂ log p(y|˜lm, I, W )  + log p(y|˜lm, I, W )  ∂ log p(˜lm|I, W )  ∂W  ∂W  (cid:21)  (9)  (10)  The equation 10 gives a practical algorithm to train the deep attention model. Namely, we can sample the glimpse location prediction from the model after each glimpse. The samples are then used in the standard backpropagation to obtain an estimator for the gradient of the model parameters. Notice that log likelihood log p(y|˜lm, I, W ) has an unbounded range that can introduce substantial high variance in the gradient estimator. Especially when the sampled location is off from the object in the image, the log likelihood will induce an undesired large gradient update that is backpropagated through the rest of the model. We can reduce the variance in the estimator 10 by replacing the log p(y|˜lm, I, W ) with a 0/1 discrete indicator function R and using a baseline technique used in Mnih et al. (2014).  (cid:26)1  0  R =  y = arg maxy log p(y|˜lm, I, W ) otherwise  bn = Ebaseline(r(2)  n |Wbaseline)  As shown, the recurrent network state vector r(2) n is used to estimate a state-based baseline b for each glimpse that signiﬁcantly improve the learning efﬁciency. The baseline effectively centers the random variable R and can be learned by regressing towards the expected value of R. Given both the indicator function and the baseline, we have the following gradient update:  M(cid:88)  (cid:20) ∂ log p(y|˜lm, I, W )  m=1  ∂W  ∂F ∂W  ≈ 1 M  (cid:21)  + λ(R − b)  ∂ log p(˜lm|I, W )  ∂W  where, hyper-parameter λ balances the scale of the two gradient components. In fact, by us- ing the 0/1 indicator function, the learning rule from equation 13 is equivalent to the REIN- FORCE (Williams, 1992) learning rule employed in Mnih et al. (2014) for training their attention model. When viewed as a reinforcement learning update, the second term in equation 13 is an unbi- ased estimate of the gradient with respect to W of the expected reward R under the model glimpse  4  (11)  (12)  (13)  Published as a conference paper at ICLR 2015  M(cid:88)  m=1  policy. Here we show that such learning rule can also be motivated by simply approximately opti- mizing the free energy. During inference, the feedforward location prediction can be used as a deterministic prediction on the location coordinates to extract the next input image patch for the model. The model behaves as a normal feedforward network. Alternatively, our marginalized objective function equation 5 sug- gests a procedure to estimate the expected class prediction by using samples of location sequences {˜lm 1 ,··· , ˜lm  N} and averaging their predictions,  El[p(y|I)] ≈ 1 M  p(y|I, ˜lm).  (14)  This allows the attention model to be evaluated multiple times on each image with the classiﬁcation predictions being averaged. In practice, we found that averaging the log probabilities gave the best performance. In this paper, we encode the real valued glimpse location tuple ln using a Cartesian coordinate that is centered at the middle of the input image. The ratio converting unit width in the coordinate system to the number of pixels is a hyper-parameter. This ratio presents an exploration versus exploitation trade off. The proposed model performance is very sensitive to this setting. We found that setting its value to be around 15% of the input image width tends to work well.  3.2 MULTI-OBJECT/SEQUENTIAL CLASSIFICATION AS A VISUAL ATTENTION TASK  Our proposed attention model can be easily extended to solve classiﬁcation tasks involving multiple objects. To train the deep recurrent attention model for the sequential recognition task, the multiple object labels for a given image need to be cast into an ordered sequence {y1, y2,··· , ys}. The deep recurrent attention model then learns to predict one object at a time as it explores the image in a sequential manner. We can utilize a simple ﬁxed number of glimpses for each target in the sequence. In addition, a new class label for the “end-of-sequence” symbol is included to deal with variable numbers of objects in an image. We can stop the recurrent attention model once a terminal symbol is predicted. Concretely, the objective function for the sequential prediction is  log p(y1, y2,··· , yS|I, W ) =  p(ls|I, W )p(ys|ls, I, W )  (15)  The learning rule is derived as in equation 13 from the free energy and the gradient is accumulated across all targets. We assign a ﬁxed number of glimpses, N, for each target. Assuming S targets in an image, the model would be trained with N × (S + 1) glimpses. The beneﬁt of using a recurrent model for multiple object recognition is that it is a compact and simple form yet ﬂexible enough to deal with images containing variable numbers of objects. Learning a model from images of many objects is a challenging setup. We can reduce the difﬁ- culty by modifying our indicator function R to be proportional to the number of targets the model predicted correctly.  Rs =  Rj  (16)  S(cid:88)  (cid:88)  log  s=1  l  (cid:88)  j≤s  In addition, we restrict the gradient of the objective function so that it only contains glimpses up to the ﬁrst mislabeled target and ignores the targets after the ﬁrst mistake. This curriculum-like adap- tion to the learning is crucial to obtain a high performance attention model for sequential prediction.  4 EXPERIMENTS  To show the effectiveness of the deep recurrent attention model (DRAM), we ﬁrst investigate a num- ber of multi-object classiﬁcation tasks involving a variant of MNIST. We then apply the proposed attention model to a real-world object recognition task using the multi-digit SVHN dataset Netzer et al. (2011) and compare with the state-of-the-art deep ConvNets. A description of the models and training protocols we used can be found in the Appendix.  5  Published as a conference paper at ICLR 2015  Table 1: Error rates on the MNIST pairs classiﬁcation task.  Table 2: Error rates on the MNIST two digit addition task.  Model RAM Mnih et al. (2014) DRAM w/o context DRAM  Test Err.  9% 7% 5%  Model ConvNet 64-64-64-512 DRAM  Test Err.  3.2% 2.5%  Figure 2: Left) Two examples of the learned policy on the digit pair classiﬁcation task. The ﬁrst column shows the input image while the next 5 columns show the selected glimpse locations. Right) Two examples of the learned policy on the digit addition task. The ﬁrst column shows the input image while the next 5 columns show the selected glimpse locations.  n, x2  As suggested in Mnih et al. (2014), classiﬁcation performance can be improved by having a glimpse network with two different scales. Namely, given a glimpse location ln, we extract two patches n is a down-sampled coarser image patch. We use the n) where x1 (x1 concatenation of x1 The hyper-parameters in our experiments are the learning rate η and the location variance Σ in equation 9. They are determined by grid search and cross-validation.  n is the original patch and x2 n and x2  n as the glimpse observation. “foveal” feature.  4.1 LEARNING TO FIND DIGITS  We ﬁrst evaluate the effectiveness of the controller in the deep recurrent attention model using the MNIST handwritten digit dataset. We generated a dataset of pairs of randomly picked handwritten digits in a 100x100 image with distraction noise in the background. The task is to identify the 55 different combinations of the two digits as a classiﬁcation problem. The attention models are allowed 4 glimpses before making a classiﬁcation prediction. The goal of this experiment is to evaluate the ability of the controller and recurrent network to combine information from multiple glimpses with minimum effort from the glimpse network. The results are shown in table (4.1). The DRAM model with a context network signiﬁcantly outperforms the other models.  4.2 LEARNING TO DO ADDITION  For a more challenging task, we designed another dataset with two MNIST digits on an empty 100x100 background where the task is to predict the sum of the two digits in the image as a classi- ﬁcation problem with 19 targets. The model has to ﬁnd where each digit is and add them up. When the two digits are sampled uniformly from all classes, the label distribution is heavily imbalanced for the summation where most of the probability mass concentrated around 10. Also, there are many digit combinations that can be mapped to the same target, for example, [5,5] and [3,7]. The class label provides a weaker association between the visual feature and supervision signal in this task than in the digit combination task. We used the same model as in the combination task. The deep recurrent attention model is able to discover a glimpse policy to solve this task achieving a 2.5% error rate. In comparison, the ConvNets take longer to learn and perform worse when given weak supervision. Some inference samples are shown in ﬁgure 2 It is surprising that the learned glimpses policy for predicting the next glimpse is very different in the addition task comparing to the predicting combi- nation task. The model that learned to do addition toggles its glimpses between the two digits.  6  Published as a conference paper at ICLR 2015  Table 3: Whole sequence recognition error rates on multi-digit SVHN.  Table 4: Whole sequence recognitionn error rate on enlarged multi-digit SVHN.  Model 11 layer CNN Goodfellow et al. (2013) 10 layer CNN Single DRAM Single DRAM MC avg. forward-backward DRAM MC avg.  Test Err. 3.96% 4.11% 5.1% 4.4% 3.9%  Model 10 layer CNN resize 10 layer CNN re-trained Single DRAM focus forward-backward DRAM focus Single DRAM ﬁne-tuned forward-backward DRAM ﬁne-tuning  Test Err.  50% 5.60% 5.7% 5.0% 5.1% 4.46%  4.3 LEARNING TO READ HOUSE NUMBERS  The publicly available multi-digit street view house number (SVHN) dataset Netzer et al. (2011) consists of images of digits taken from pictures of house fronts. Following Goodfellow et al. (2013), we formed a validation set of 5000 images by randomly sampling images from the training set and the extra set, and these were used for selecting the learning rate and sampling variance for the stochastic glimpse policy. The models are trained using the remaining 200,000 training images. We follow the preprocessing technique from Goodfellow et al. (2013) to generate tightly cropped 64 x 64 images with multi-digits at the center and similar data augmentation is used to create 54x54 jittered images during training. We also convert the RGB images to grayscale as we observe the color information does not affect the ﬁnal classiﬁcation performance. We trained a model to classify all the digits in an image sequentially with the objective function deﬁned in equation 15. The label sequence ordering is chosen to go from left to right as the natural ordering of the house number. The attention model is given 3 glimpses for each digit before making a prediction. The recurrent model keeps running until it predicts a terminal label or until the longest digit length in the dataset is reached. In the SVHN dataset, up to 5 digits can appear in an image. This means the recurrent model will run up to 18 glimpses per image, that is 5 x 3 plus 3 glimpses for a terminal label. Learning the attention model took around 3 days on a GPU. The model performance is shown in table (4.3). We found that there is still a performance gap between the state-of-the-art deep ConvNet and a single DRAM that “reads” from left to right, even with the Monte Carlo averaging. The DRAM often over predicts additional digits in the place of the terminal class. In addition, the distribution of the leading digit in real-life follows Benford’s law. We therefore train a second recurrent attention model to “read” the house numbers from right to left as a backward model. The forward and backward model can share the same weights for their glimpse networks but they have different weights for their recurrent and their emission net- works. The predictions of both forward and backward models can be combined to estimate the ﬁnal sequence prediction. Following the observation that attention models often overestimate the sequence length, we can ﬂip ﬁrst k number of sequence prediction from the backwards model, where k is the shorter length of the sequence length prediction between the forward and back- ward model. This simple heuristic works very well in practice and we obtain state-of-the-art performance on the Street View house number dataset with the forward-backward recurrent at- tention model. Videos showing sample runs of the forward and backward models on SVHN test data can be found at http://www.psi.toronto.edu/˜jimmy/dram/forward.avi and http://www.psi.toronto.edu/˜jimmy/dram/backward.avi respectively. These vi- sualizations show that the attention model learns to follow the slope of multi-digit house numbers when they go up or down. For comparison, we also implemented a deep ConvNet with a similar architecture to the one used in Goodfellow et al. (2013). The network had 8 convolutional layers with 128 ﬁlters in each followed by 2 fully connected layers of 3096 ReLU units. Dropout is applied to all 10 layers with 50% dropout rate to prevent over-ﬁtting. Moreover, we generate a less cropped 110x110 multi-digit SVHN dataset by enlarging the bounding box of each image such that the relative size of the digits stays the same as in the 54x54 images. Our deep attention model trained on 54x54 can be directly applied to the new 110x110 dataset with no modiﬁcation. The performance can be further improved by “focusing” the model on where the digits  7  Published as a conference paper at ICLR 2015  (Giga) ﬂoating-point op. 54x54 110x110  10 layer CNN DRAM DRAM MC avg.  F-B DRAM MC avg.  2.1 8.5  ≤0.2 ≤0.2  0.35 1.1  0.7 2.2  param. (millions) 54x54 110x110  10 layer CNN DRAM DRAM MC avg.  F-B DRAM MC avg.  51 169  14 14  14 14  28 28  Table 5: Computation cost of DRAM V.S. deep ConvNets  are. We run the model once and crop a 54x54 bounding box around the glimpse location sequence and feed the 54x54 bounding box to the attention model again to generate the ﬁnal prediction. This allows DRAM to “focus” and obtain a similar prediction accuracy on the enlarged images as on the cropped image without ever being trained on large images. We also compared the deep ConvNet trained on the 110x110 images with the ﬁne tuned attention model. The deep attention model signiﬁcantly outperforms the deep ConvNet with very little training time. The DRAM model only takes a few hours to ﬁne-tune on the enlarged SVHN data, compared to one week for the deep 10 layer ConvNet.  5 DISCUSSION  In our experiments, the proposed deep recurrent attention model (DRAM) outperforms the state-of- the-art deep ConvNets on the standard SVHN sequence recognition task. Moreover, as we increase the image area around the house numbers or lower the signal-to-noise ratio, the advantage of the attention model becomes more signiﬁcant. In table 5, we compare the computational cost of our proposed deep recurrent attention model with that of deep ConvNets in terms of the number of ﬂoat-pointing operations for the multi-digit SVHN models along with the number of parameters in each model. The recurrent attention models that only process a selected subset of the input scales better than a ConvNet that looks over an entire image. The estimated cost for the DRAM is calculated using the maximum sequence length in the dataset, however the expected computational cost is much lower in practice since most of the house numbers are around 2 − 3 digits long. In addition, since the attention based model does not process the whole image, it can naturally work on images of different size with the same computational cost independent of the input dimensionality. We also found that the attention-based model is less prone to over-ﬁtting than ConvNets, likely because of the stochasticity in the glimpse policy during training. Though it is still beneﬁcial to regularize the attention model with some dropout noise between the hidden layers during training, we found that it gives a very marginal performance boost of 0.1% on the multi-digit SVHN task. On the other hand, the deep 10 layer ConvNet is only able to achieve 5.5% error rate when dropout is only applied to the last two fully connected hidden layer. Finally, we note that DRAM can easily deal with variable length label sequences. Moreover, a model trained on a dataset with a ﬁxed sequence length can easily be transferred and ﬁne tuned with a similar dataset but longer target sequences. This is especially useful when there is lack of data for the task with longer sequences.  6 CONCLUSION  We described a novel computer vision model that uses an attention mechanism to decide where to focus its computation and showed how it can be trained end-to-end to sequentially classify multiple objects in an image. The model outperformed the state-of-the-art ConvNets on a multi-digit house number recognition task while using both fewer parameters and less computation than the best Con- vNets, thereby showing that attention mechanisms can improve both the accuracy and efﬁciency of ConvNets on a real-world task. Since our proposed deep recurrent attention model is ﬂexible, pow- erful, and efﬁcient, we believe that it may be a promising approach for tackling other challenging computer vision tasks.  8  Published as a conference paper at ICLR 2015  7 ACKNOWLEDGEMENTS  We would like to thank Geoffrey Hinton, Nando de Freitas and Chris Summerﬁeld for many helpful comments and discussions. We would also like to thank the developers of DistBelief (Dean et al., 2012a).  REFERENCES Alexe, Bogdan, Heess, Nicolas, Teh, Yee Whye, and Ferrari, Vittorio. Searching for objects driven by context.  In NIPS, 2012. 2  Dean, Jeffrey, Corrado, Greg, Monga, Rajat, Chen, Kai, Devin, Matthieu, Mao, Mark, Senior, Andrew, Tucker, Paul, Yang, Ke, Le, Quoc V, et al. Large scale distributed deep networks. In Advances in Neural Information Processing Systems, pp. 1223–1231, 2012a. 9  Dean, Jeffrey, Corrado, Greg, Monga, Rajat, Chen, Kai, Devin, Matthieu, Mao, Mark, Senior, Andrew, Tucker, Paul, Yang, Ke, Le, Quoc V, et al. Large scale distributed deep networks. In Advances in Neural Information Processing Systems, pp. 1223–1231, 2012b. 1  Goodfellow, Ian J, Bulatov, Yaroslav, Ibarz, Julian, Arnoud, Sacha, and Shet, Vinay. Multi-digit number recog- nition from street view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082, 2013. 1, 2, 7  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):1735–1780,  1997. 3  Itti, L., Koch, C., and Niebur, E. A model of saliency-based visual attention for rapid scene analysis. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 20(11):1254–1259, 1998. 2  Jaderberg, Max, Simonyan, Karen, Vedaldi, Andrea, and Zisserman, Andrew. Synthetic data and artiﬁcial  neural networks for natural scene text recognition. arXiv preprint arXiv:1406.2227, 2014a. 1, 2  Jaderberg, Max, Vedaldi, Andrea, and Zisserman, Andrew. Deep features for text spotting.  Vision–ECCV 2014, pp. 512–528. Springer, 2014b. 2  In Computer  Karpathy, Andrej, Joulin, Armand, and Li, Fei Fei F. Deep fragment embeddings for bidirectional image  sentence mapping. In Advances in Neural Information Processing Systems, pp. 1889–1897, 2014. 1  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Imagenet classiﬁcation with deep convolutional neural  networks. In Advances in Neural Information Processing Systems 25, pp. 1106–1114, 2012. 1  Larochelle, Hugo and Hinton, Geoffrey E. Learning to combine foveal glimpses with a third-order boltzmann  machine. In NIPS, 2010. 2, 3  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, November 1998. 2  Maes, Francis, Denoyer, Ludovic, and Gallinari, Patrick. Structured prediction with reinforcement learning.  Machine learning, 77(2-3):271–301, 2009. 2  Mnih, Volodymyr, Heess, Nicolas, Graves, Alex, and Kavukcuoglu, Koray. Recurrent models of visual atten-  tion. arXiv preprint arXiv:1406.6247, 2014. 2, 4, 6  Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 4, 2011. 5, 7  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption  generator. arXiv preprint arXiv:1411.4555, 2014. 1  Wang, Tao, Wu, David J, Coates, Adam, and Ng, Andrew Y. End-to-end text recognition with convolutional neural networks. In Pattern Recognition (ICPR), 2012 21st International Conference on, pp. 3304–3308. IEEE, 2012. 2  Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement learning.  Machine learning, 8(3-4):229–256, 1992. 4  9  Published as a conference paper at ICLR 2015  Model small DRAM small DRAM + dropout  Test Err.  5.1% 4.6%  Table 6: Effectiveness of Regularization  8 APPENDIX  8.1 GENERAL TRAINING DETAILS  We used the ReLU activation function in the hidden layers, g(x) = max(0, x), for the rest of the results reported here or otherwise noted. We found that ReLU units signiﬁcantly speed up training. We optimized the model parameters using stochastic gradient descent with the Nesterov momentum technique. A mini-batch size of 128 was used to estimate the gradient direction. The momentum coefﬁcient was set to 0.9 throughout the training. The learning rate η scheduling was applied in training to improve the convergence of the learning process. η starts at 0.01 in the ﬁrst epoch and was exponentially reduced by a factor of 0.97 after each epoch.  8.2 DETAILS OF LEARNING TO FIND DIGITS  The unit width for the Cartesian coordinates was set to 20 and glimpse location sampling standard deviation was set to 0.03. There are 512 LSTM units and 256 hidden units in each fully connected layer of the model. We intentionally used a simple fully connected single hidden layer network of 256 hidden units as Gimage(·) in the glimpse network.  8.3 DETAILS OF LEARNING TO READ HOUSE NUMBERS  Unlike in the MNIST experiment, the number of digits in each image varies and digits have more variations due to natural backgrounds, lighting variation, and highly variable resolution. We use a much larger deep recurrent attention model for this task. It was crucial to have a powerful glimpse network to obtain good performance. As described in section 3, the glimpse network consists of three convolutional layers with 5x5 ﬁlter kernels in the ﬁrst layer and 3x3 in the later two. The number of ﬁlters in those layers was {64, 64, 128}.There are 512 LSTM units in each layer of the recurrent network. Also, the fully connected hidden layers all have 1024 ReLU hidden units in each module listed in section 3. The Cartesian coordinate unit width was set to 12 pixels and glimpse location is sampled from a ﬁxed variance of 0.03.  10  ",
1411.3784,2015,Deep Narrow Boltzmann Machines are Universal Approximators,"['Deep Narrow Boltzmann Machines are Universal Approximators', 'Guido Montufar']",https://arxiv.org/pdf/1411.3784,"5 1 0 2    r p A 0 1         ] L M  . t a t s [      3 v 4 8 7 3  .  1 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  DEEP NARROW BOLTZMANN MACHINES ARE UNIVERSAL APPROXIMATORS  Guido Mont´ufar ∗ Max Planck Institute for Mathematics in the Sciences Inselstrasse 22, 04103 Leipzig, Germany montufar@mis.mpg.de  ABSTRACT  We show that deep narrow Boltzmann machines are universal approximators of probability distributions on the activities of their visible units, provided they have sufﬁciently many hidden layers, each containing the same number of units as the visible layer. We show that, within certain parameter domains, deep Boltzmann machines can be studied as feedforward networks. We provide upper and lower bounds on the sufﬁcient depth and width of universal approximators. These re- sults settle various intuitions regarding undirected networks and, in particular, they show that deep narrow Boltzmann machines are at least as compact uni- versal approximators as narrow sigmoid belief networks and restricted Boltzmann machines, with respect to the currently available bounds for those models.  1  INTRODUCTION  It is an interesting question how the representational power of deep artiﬁcial neural networks, with several layers of hidden units, compares with that of shallow neural networks, with one single layer of hidden units. Furthermore, it is interesting how the representational power of layered networks compares in the cases of undirected and directed connections between the layers. A basic question in this respect is whether the classes of function approximators represented by the different network architectures can possibly reach any desired degree of accuracy, when endowed with sufﬁciently many computational units. This property, referred to as universal approximation property, has been established for a wide range of network architectures, including various kinds of shallow feedfor- ward, shallow undirected, and deep feedforward networks, both in the deterministic and stochastic settings. Nevertheless, for deep narrow undirected network architectures, universal approximation has remained so far an open problem. In this paper we prove that deep narrow Boltzmann machines are universal approximators, provided they have sufﬁciently many layers of hidden units, each hav- ing at least as many units as the visible layer.  A Boltzmann machine (Ackley et al. 1985) is a network of stochastic binary units with undirected pairwise interactions. A deep Boltzmann machine (DBM) (Salakhutdinov & Hinton 2009) is a Boltzmann machine whose units build a stack of layers, where only pairs of units from subsequent layers interact, and only the units in the bottom layer are visible. The units within any given layer are conditionally independent, given the states of the units in the adjacent layers. Figure 1 illustrates this architecture.  Since the ﬁrst appearance of DBMs, a number of papers have addressed various practical and the- oretical aspects of these networks, especially regarding training and estimation (see Montavon & M¨uller 2012; Goodfellow et al. 2013a; Cho et al. 2015). The undirected nature of DBMs leads to interesting and desirable properties, but it also brings with it challenges in training and analyzing them. A number of anticipated properties of DBMs still are missing formal veriﬁcation. We prove that narrow DBMs have the universal approximation property. We focus on DBMs with layers of constant size. We note that, in order to obtain the universal approximation property, the ﬁrst hidden layer must have at least the same size as the visible layer (minus one, when this is even). As a direct corollary of our main theorem, we obtain the universal approximation of conditional probability dis-  ∗http://personal-homepages.mis.mpg.de/montufar/  1  Published as a conference paper at ICLR 2015  x3,1  x3,2  x3,3  · · · x3,n3  b3  W2  x2,1  x2,2  x2,3  · · · x2,n2  b2  W1  x1,1  x1,2  x1,3  · · · x1,n1  b1  s r e y a l n e d d i H  x3,1  x3,2  x3,3  · · · x3,n3  b3  W2  s r e y a l n e d d i H  x2,1  x2,2  x2,3  · · · x2,n2  b2  W1  x1,1  x1,2  x1,3  · · · x1,n1  b1  W0  x0,1  x0,2  x0,3  · · · x0,n0  b0  DBN  Visible layer  W0  Hidden layer  x1,1  x1,2  x1,3  x1,4  x1,5  · · · x1,n1  b1  x0,1  x0,2  x0,3  · · · x0,n0  b0  W0  DBM  Visible layer  x0,1  x0,2  x0,3  · · · x0,n0  b0  RBM  Visible layer  Figure 1: The left panel illustrates the architecture of a DBM with a visible layer of n0 units and three hidden layers of n1, n2, n3 units. Pairs of units form consecutive layers are undirectedly connected. There are no connections between units from the same layer nor between units from non-consecutive layers. The right panel shows the architectures of a DBN and an RBM, which are the directed and the shallow versions of DBMs.  tributions on the activations of subsets of visible units, given the activations of the remaining visible units. Our analysis applies not only to binary units, but also to softmax (ﬁnite-valued) units.  At an intuitive level, undirected networks are expected to be more powerful than their directed coun- terparts, since “they allow information to ﬂow both ways.” Given that narrow deep belief networks (DBNs) (Hinton et al. 2006) have the universal approximation property (Sutskever & Hinton 2008), the natural expectation is that narrow DBMs also have the universal approximation property (DBNs can be regarded as the directed counterparts of DBMs). There are several reasons why this intuition is not straightforward to verify. While feedforward networks can be studied in a sequential way, with the output of any given layer being the input of the next layer, in the undirected case, each internal layer receives inputs from both the previous and the next layers. This renders recurrent sig- nals between all units and complicates a sequential analysis. The key component of our proof lies in showing that, within certain parameter regions, a DBM can be regarded as a feedforward network. More precisely, the upper part of the network can “neutralize” the upward signals arriving from the lower part of the network, in such a way that the visible probability distributions represented by the entire network have the same form as those represented by a DBN. This allows us to analyze the representational power of DBMs in a sequential way and show that, in some well deﬁned sense, DBMs are at least as powerful as DBNs.  Montavon, Braun, and M¨uller (2012) have also proposed a feedforward perspective on DBMs. How- ever, their motivation was different from ours, and they used the term “feedforward” to refer to a Gibbs sampling pass traversing the network in a feedforward manner, rather than to the structure of the joint probability distributions represented by the entire network. They showed, experimentally, that a DBM outputs a feedforward hierarchy of increasingly invariant representations.  In the remainder of the introduction we comment on a few results that appear helpful for contextu- alizing the present paper. From the network architectures mentioned above (deep, shallow, directed, undirected), the most extensively studied ones are the shallow feedforward networks (with one sin- gle layer of hidden units). A shallow feedforward network is understood as a composition of simple computational units, all having the same inputs. It is well known that, by tuning the parameters of the individual units, these networks can approximate any function on the set of inputs arbitrarily  2  Published as a conference paper at ICLR 2015  well,1 provided they have sufﬁciently many hidden units (Hornik et al. 1989; Cybenko 1989). In other words, any function can be written, approximately, as a superposition (e.g., linear combina- tion) of simple functions. This universal approximation property has been established under very general conditions both on the type of units and the type of functions being approximated (see, e.g., Leshno et al. 1993; Chen & Chen 1995). See also (Barron 1993; Burger & Neubauer 2001) for works addressing the accuracy of the approximations. An interesting recent example are shallow feedforward networks with maxout units (Goodfellow et al. 2013b). Besides from standard func- tions, i.e., deterministic output assignments given the inputs, shallow feedforward networks are also capable of approximating stochastic functions arbitrarily well, i.e., probabilistic output assignments given the inputs, when constructed with sufﬁciently many stochastic units.  Deep neural networks have seen exceptional success in applications in recent years. Aiming at a better understanding and development of this success, a number of recent papers have addressed the theory of deep architectures (see Bengio & Delalleau 2011; Baldi 2012; Pascanu et al. 2014; Mont´ufar et al. 2014b). It is not so long ago that Sutskever & Hinton (2008) investigated deep belief networks (DBNs) (Hinton et al. 2006) with narrow layers of stochastic binary units (all having about the same number of units). They showed that these architectures can approximate any binary probability distribution on the states of their visible units arbitrarily well, provided the number of hidden layers is large enough (exponential in the number of visible units). The minimal depth of universal approximators of this kind has been studied subsequently in (Le Roux & Bengio 2010; Mont´ufar & Ay 2011; Mont´ufar 2014). The approximation properties of DBNs with real-valued visible units and binary hidden units have been treated in recent work as well (Krause et al. 2013).  Boltzmann machines (Hinton and Sejnowski 1983; Ackley, Hinton, and Sejnowski 1985; Hinton and Sejnowski 1986) are energy based models describing the statistical behavior of pairwise interacting stochastic binary units. They have roots in statistical physics and have been studied intensively as special types of graphical probability models and exponential families. In particular, information geometry has provided deep geometric insights about learning and approximation of probability distributions by this kind of networks (Amari et al. 1992). It is well known that Boltzmann machines are universal approximators of probability distributions over the states of their visible units, provided they have sufﬁciently many hidden units (see Sussmann 1988; Younes 1996). The situation is more differentiated when a speciﬁc structure is imposed on the network, e.g., a layered structure, where only pairs of units in subsequent layers may be connected. This imposes non-trivial restrictions on the sets of representable distributions. For the shallow layered version of the Boltzmann machine, called restricted Boltzmann machine (RBM) (Smolensky 1986; Freund & Haussler 1991), the universal approximation capability has been shown in (Freund & Haussler 1991; Younes 1996; Le Roux & Bengio 2008), provided the hidden layer is large enough (having exponentially more units than the visible layer). More recently, questions related to the minimal number of hidden units that is sufﬁcient for universal approximation by RBMs have been studied in (Le Roux & Bengio 2008; Mont´ufar & Ay 2011; Mont´ufar et al. 2011; Mont´ufar & Morton 2013; Martens et al. 2013). Nonetheless, universal approximation results for the deep versions of RBMs, the deep Boltzmann machines (DBMs) (Salakhutdinov & Hinton 2009), have been miss- ing so far (except when the hidden layers have exponentially many more units than the visible layer).  This paper is organized as follows. In Section 2 we give necessary deﬁnitions and ﬁx notations. In Section 3 we present our main result: the universal approximation property of narrow DBMs. The proof of this result is elaborated in Sections 4 and 5. In Section 4 we address the compositional structure of DBMs. We express the probability distributions represented by a DBM in terms of the probability distributions represented by two smaller DBMs and a feedforward layer with shared pa- rameters. In Section 5 we elaborate an approach to study DBMs from a feedforward perspective. We ﬁrst present a trick to effectively disentangle the shared parameters between intermediate marginal distributions and lower conditional distributions. This is followed by a feedforward analysis proving the universal approximation property. In Section 6 we offer a discussion of the result.  1Meant are reasonably well behaved functions and reasonable measures of approximation.  3  Published as a conference paper at ICLR 2015  2 DEFINITIONS  In this section we ﬁx notation and technical details. A deep Boltzmann machine with L + 1 layers of n0, n1, . . . , nL units is a model of joint probability distributions of the form  pW,b(x0, x1 . . . , xL) =  1  Z(W, b)  L−1  exp(  x⊤ l  X  l=0  Wlxl+1 +  L  bl),  x⊤ l  X l=0 0 , . . . , x⊤  for all (x⊤  L )⊤ ∈ {0, 1}n0+···+nL .  (1)  0 , . . . , x⊤  L )⊤ ∈ {0, 1}N, N = PL  Here xl = (xl,1, . . . , xl,nl)⊤ ∈ {0, 1}nl denotes the joint state of the units in the l-th layer and l=0 nl, the joint state of all units. The parameters of this (x⊤ model are W = {W0, . . . , WL−1} and b = {b0, . . . , bL}, where Wl ∈ Rnl×nl+1 is a matrix of interaction weights between units from the l-th and (l + 1)-th layers, for l = 0, . . . , L − 1, and bl ∈ Rnl is a vector of biases for the units in the l-th layer, for l = 0, . . . , L. The function Z(W, b) is deﬁned in such a way that the entries of pW,b add to one. The set of all probability distributions of the form (1), for all choices of W and b, is a smooth man- ifold (an exponential family) of dimension PL−1 l=0 nl. This manifold is embedded in the (2N − 1)-dimensional set ∆N of all possible probability distributions over {0, 1}N. Note that every probability distribution of the form (1) is strictly positive, meaning that it assigns strictly positive probability to every state. We denote this model by DBMn0,...,nL, or DBM, for simplicity, when n0, . . . , nL are clear. The marginal probability distributions over the joint states x0 of the units in the bottom layer are obtained by marginalizing out x1, . . . , xL:  l=0 nlnl+1 + PL  pW,b(x0) = X  x1,...,xL  pW,b(x0, x1, . . . , xL),  for all x0 ∈ {0, 1}n0.  (2)  The set of probability distributions of this form, for all W and b, is the DBM probability model with a visible layer of n0 units and L hidden layers of n1, . . . , nL units. Note that every distribution of the form (2) is strictly positive.  In the case that the network has only one hidden layer, L = 1, as illustrated in the lower right panel of Figure 1, the model reduces to a restricted Boltzmann machine (with n0 visible and n1 hidden units). The corresponding set of probability distributions is denoted RBMn0,n1 ≡ DBMn0,n1. If we replace all interactions, except those between the top to layers, by interactions directed towards the bottom layer, we obtain a DBN, illustrated in the upper right panel of Figure 1. We provide more details on RBMs and DBNs in the Supplementary Material.  3 UNIVERSAL APPROXIMATION  A set M of probability distributions on {0, 1}n is called universal approximator when, for any distribution q on {0, 1}n and any ǫ > 0, there is a distribution p in M with D(qkp) ≤ ǫ. Here the Kullback-Leibler divergence between q and p is deﬁned as D(qkp) := Px q(x) log q(x) p(x) . This is never negative and is only zero if q = p. The main result of this paper is the following: Theorem 1. A DBM with a visible layer of n units and L hidden layers of n units each is a univer- sal approximator of probability distributions on the states of the visible layer, provided L is large enough. More precisely, for any n ≤ n′ := 2k + k + 1, for some k ∈ N, a sufﬁcient condition is L ≥  2(n′−log2(n′)−1) . For any n a necessary condition is L ≥ 2n−(n+1)  n(n+1)  2n′  .  A direct implication of this result is the universal approximation property for conditional probability distributions of a subset of visible units, given the states of the remaining visible units. Corollary 2. A DBM with a visible layer of n units and L hidden layers of n units each is a universal approximator of stochastic input-output maps with inputs (x0,1, . . . , x0,k) ∈ {0, 1}k and outputs (x0,k+1, . . . , x0,n) ∈ {0, 1}n−k, for any 1 ≤ k ≤ n, provided L is as in Theorem 1.  4  Published as a conference paper at ICLR 2015  DBM(1)  x3,1  x3,2  x3,3  · · · x3,n3  x2,1  x2,2  x2,3  · · · x2,n2  DBM(2)  r(x1)  x1,1 x1,1  x1,2 x1,2  x1,3 x1,3  · · · x1,n1 · · · x1,n1  s(x1)  (r ∗ s)(x1)  x0,1 x0,1  x0,2 x0,2  x0,3 x0,3  · · · x0,n0 · · · x0,n0  p(x0)  Figure 2: Composition of an upper and a lower DBM to form a larger DBM.  We note that the number of visible units (minus one when this is even) is the smallest possible number of units in the ﬁrst hidden layer of a DBM universal approximator. Proposition 3. A DBM with n0 visible units can be a universal approximator only if the ﬁrst hidden layer has at least n1 ≥ n0 − 1 units, when n0 is even, and at least n1 ≥ n0 units, when n0 is odd.  Furthermore, Theorem 1 can be extended to softmax units with any ﬁnite number of states. Theorem 4. A DBM with a visible layer of n softmax q-valued units and L hidden layers of n softmax q-valued units each is a universal approximator of probability distributions on the states of the visible layer, provided L is large enough. More precisely, for any n ≤ n′ := qk + k + 1, for some q(q−1)(n′−logq(n′)−1) . For any n a necessary condition is k ∈ N, a sufﬁcient condition is L ≥ 1 +  qn′  −1  L ≥  qn −1  n(q−1)(n(q−1)+2) .  The proof of these statements is elaborated in the next two sections. First we discuss the composi- tional structure of DBMs and then we present a feedforward analysis.  4 COMPOSITIONAL STRUCTURE  In this section we take a look at the compositional structure of DBMs. We will regard a DBM as a composition of two smaller DBMs. In order to describe these compositions, we use the renormalized entry-wise (Hadamard) product. The Hadamard product of two distributions r, s ∈ ∆n is deﬁned as  (r ∗ s)(z) := r(z)s(z)/X  z′  r(z′)s(z′),  for all z ∈ {0, 1}n.  1  In this deﬁnition we assume that r and s have at least one non-zero entry in common, such that Pz′ r(z′)s(z′) 6= 0. We write r ∗ M := {r ∗ s : s ∈ M} for the set of Hadamard products of a probability distribution r and the elements of a probability model M. The Hadamard product is a very natural operation for describing compositions of energy based models. Note that, if r(z) = Z(f ) exp(f (z)) and s = 1 We can write the probability distributions represented by a DBM in terms of the probability distri- butions represented by two smaller DBMs. Such decompositions have been discussed previously by Salakhutdinov & Hinton (2012). We identify the bottom layer of DBM(1) with the top layer of DBM(2), as illustrated in Figure 2. By this composition, the distribution s that was originally repre-  Z(g) exp(g(z)), then (r ∗ s)(z) =  Z(f +g) exp(f (z) + g(z)).  1  5  Published as a conference paper at ICLR 2015  sented on the bottom layer of DBM(1) becomes r ∗ s, where r is the distribution that was originally represented on the top layer of DBM(2): Proposition 5. Consider the model DBM = DBMn0,...,nL, for some n0, . . . , nL ∈ N. For any 0 < k < L the marginal distributions of the k-th layer’s units are the distributions of the form  p(xk) = (p(2) ∗ p(1))(xk),  for all xk ∈ {0, 1}nk,  where p(1)(xk) is a bottom layer marginal of DBM(1) = DBMnk,...,nL and p(2)(xk) is a top layer marginal of DBM(2) = DBMn0,n1,...,nk .  For completeness we provide a proof of this statement in the Supplementary Material.  Next we deﬁne the model of conditional probability distributions represented by a feedforward layer. For n1 input units and n0 output units, the feedforward model FFn0,n1 consists of all conditional probability distributions of the form  qW0,b0(x0|x1) =  1  Z(W0x1 + b0)  exp(x⊤ 0  W0x1+x⊤ 0  b0),  for all x0 ∈ {0, 1}n0, x1 ∈ {0, 1}n1.  Here W0 ∈ Rn0×n1 is a matrix of input weights and b0 ∈ Rn0 is a vector of biases. Clearly, these conditionals correspond exactly to the conditionals represented between ﬁrst hidden layer and the visible layer of a DBM, for the same choices of parameters.  The next Proposition 6 gives an expression for the visible distributions represented by a DBM in terms of the distributions represented by two smaller DBMs and the conditionals represented by a feedforward layer with shared parameters. Proposition 6. The bottom layer marginal distributions representable by DBMn0,...,nL are those of the from  p(x0) = X  x1  q(x0|x1)(r ∗ s)(x1),  for all x0 ∈ {0, 1}n0,  where q(x0|x1)r(x1) is a joint probability distribution of the fully observable RBMn0,n1 and s is a bottom layer marginal of DBMn1,...,nL.  Proof of Proposition 6. We have  p(x0) = X  x1  p(x0|x1)p(x1),  for all x0 ∈ {0, 1}n0.  By Proposition 5, p(x1) = (r ∗ s)(x1) for all x1 ∈ {0, 1}n1.  The proposition is illustrated in Figure 2. Note that r(x1) is a top layer marginal of RBMn0,n1 and q(x0|x1) is the top-to-bottom conditional of RBMn0,n1 corresponding to the feedforward layer FFn0,n1. Proposition 6 suggests that it is possible to study the representational power of DBMs in terms of the representational power of smaller DBMs composed with simple feedforward networks. The problem is that the distribution r ∗ s, intended as the input of the feedforward layer, depends on the same parameters W0, b0 as the feedforward layer. Hence the input of the feedforward layer cannot be chosen independently from the transformation that the feedforward layer applies on it. Nonetheless, as we will show in the next section, it is possible to resolve this difﬁculty and analyze the representational power of the DBM in a sequential way.  5 FEEDFORWARD ANALYSIS  Consider a DBM composed of an upper and a lower part, as shown in Figure 2. If the upper DBM(1) is able to “disable” or neutralize the top layer marginal r of DBM(2), then the distribution repre- sented at the bottom layer of the compound DBM can be regarded as the feedforward pass of the distribution s represented at the bottom layer of DBM(1). Namely, by Proposition 6 the visible distribution of the combined network is the result of passing the marginal distribution (r ∗ s)(x1) feedforward through the conditional distribution q(x0|x1).  6  Published as a conference paper at ICLR 2015  5.1 DISABLING THE BACKWARD SIGNAL  In order to make the feedforward approach work, we need to resolve the problem that the marginal r and the conditional q share the same parameters. When we modify these parameters in order to obtain a speciﬁc conditional q representing a desired feedforward transformation, the marginal r changes as well, and with it also the input r ∗ s. We resolve this dilemma in the following way. Instead of regarding the bottom layer marginals of DBM(1) as the input model, we restrict our attention to a subset G of the bottom layer marginals of DBM(1) with the following property:  r ∗ G = G for all top layer marginals r of DBM(2).  (3)  In this case, any desired input s ∈ G, together with any desired conditional q ∈ FFn0,n1, can be obtained by the following procedure:  1. Tune the parameters of DBM(2) to represent any desired (representable) conditional distri- bution q. By tuning the parameters in this way, the top layer marginal of DBM(2) becomes a distribution r that depends on q.  2. Tune the parameters of DBM(1) to represent a bottom layer marginal s′ ∈ G with r∗s′ = s.  Now we just need to ﬁnd a good choice of G, from which we require the following.  • The set G has to satisfy (3). • We have to make sure that G is contained in, or can be approximated arbitrarily well, by  the distributions representable at the bottom layer of DBM(1).  • Furthermore, G should be as large as possible, in order to account for the largest possible  fraction of the representational power of DBM(1).  We choose G as the set of probability distributions on {0, 1}n1 that assign positive probability only to a subset of vectors S ⊆ {0, 1}n1, i.e., as the set  ∆n1 (S) := {p ∈ ∆n1 : p(x1) = 0 for all x1 6∈ S}.  In the next Proposition 7 we show that this set satisﬁes the ﬁrst item of the list, regardless of S. For the second and third items, we have to choose S depending on the size of DBM(1). We will discuss the details of this further below, in Section 5.2. Given a set of probability distributions M ⊆ ∆n, let M ⊆ ∆n denote the set of probability distributions that can be approximated arbitrarily well by elements from M. Proposition 7. Let r ∈ ∆n be a strictly positive probability distribution and let M ⊆ ∆n be a set of probability distributions with M ⊇ ∆n(S). Then r ∗ M ⊇ ∆n(S).  Proof of Proposition 7. Since M can approximate any distribution from ∆n(S) arbitrarily well, ′ s(z′)/r(z′) , it can approximate any distribution of the form s′(z) = (s/r)(z) := (s(z)/r(z)) z ∈ {0, 1}n, arbitrarily well, where s is any distribution from ∆n(S). Any such s′ is contained in ∆n(S), as it is strictly supported on S. Now, the Hadamard product of r and s′ is given by  Pz  1  (r ∗ s′)(z) = r(z)s′(z)  1  Pz′ r(z′)s′(z′) 1  = r(z)(s(z)/r(z))  1  P z′′ s(z′′)/r(z′′)  Pz′ r(z′)s′(z′)  1  1  = s(z)  = s(z)  Pz′′ s(z′′)/r(z′′)  1  Pz′ r(z′)(s(z′)/r(z′)) Pz′′′ s(z′′′)/r(z′′′)  Pz′′ s(z′′)/r(z′′)  for all z ∈ {0, 1}n.  Pz′ s(z′)  = s(z),  1  Pz  ′′′ s(z′′′)/r(z′′′)  Since s was an arbitrary distribution from the set ∆n(S), this proves the claim.  7  Published as a conference paper at ICLR 2015  5.2 PROOF OF THEOREM 1  In the previous subsection we have shown that DBMs can be studied from a feedforward perspective. Let us make this more explicit. Putting Propositions 6 and 7 together, we arrive at: Proposition 8. If DBMn1,...,nL can approximate every distribution from the set ∆n1 (S) arbitrarily well as its bottom layer marginal, then DBMn0,n1,...,nL can approximate every distribution from the set FFn0,n1(∆n1 (S)) arbitrarily well as its bottom layer marginal.  With this proposition, we can study the representational power of DBMs sequentially, increasing from layer to layer. A feedforward layer is able to compute many interesting transformations of its input. For any choice of parameters, the conditional distribution qW0,b0 represented by the feedfor- ward layer FFn0,n1 deﬁnes a map ∆n1 → ∆n0 taking a probability distribution p to a probability p(x1)qW0,b0(x0|x1). As we vary the parameters W0, b0, every input distribu- distribution Px1 tion p is mapped to a collection of output distributions. Hence the feedforward layer can augment the representational power of the input model. After a sufﬁcient number of feedforward layers, the output distribution can be made to approximate any desired probability distribution arbitrarily well.  We focus on the DBM with layers of constant size n. First, we need to show that a DBM with n visible units and l hidden layers of n units each can approximate any distribution from ∆n(Sl) arbitrarily well, for some Sl ⊆ {0, 1}n. Then, we need to show that by transformations with a feed- forward layer, we obtain a larger set ∆(Sl+1) ⊆ FFn,n(∆(Sl)), which in turn can be approximated arbitrarily well by the DBM with l + 1 hidden layers. The idea is to obtain an increasing sequence  S1 ⊂ S2 ⊂ S3 ⊂ · · · ⊂ SL = {0, 1}n,  meaning that the DBM with L hidden layers can approximate any distribution on SL = {0, 1}n arbitrarily well.  We start with l = 1. The representational power of RBMs has been studied in previous papers. We use the following Proposition 9 (taken from Mont´ufar & Ay 2011). We call a pair of states x, x′ ∈ {0, 1}n adjacent if their Hamming distance is one, i.e., dH (x, x′) := |{i ∈ [n] : xi 6= x′ Proposition 9. The model RBMn0,n1 can approximate every distribution from ∆n0 (S) arbitrarily well as its bottom marginal, where S ⊆ {0, 1}n0 is any union of n1 + 1 pairs of adjacent states.  i}| = 1.  Feedforward layers have been studied in previous papers as well and we can take advantage of the tools that have been developed there. The following Proposition 10 (taken from Mont´ufar 2014) describes the augmentation of an input set ∆n(S) to an output set ∆n(S ∪ P ). The ﬂip of a state vector x along j is the vector x¯j that results from inverting the j-th entry of x. Proposition 10. The image of ∆n(S) by FFn,n can approximate every distribution from ∆n(S ∪P ) arbitrarily well, where P ⊆ {0, 1}n is any set of the following form. Take n disjoint pairs of adjacent states p1, . . . , pn and n distinct directions i1, . . . , in. Intersect each pair pj with S and ﬂip the result along the direction ij, to obtain ¯p1 = (S ∩ p1)¯i1 , . . . , ¯pn = (S ∩ pn)¯in. Set P = {¯p1, . . . , ¯pn}.  Mont´ufar & Ay (2011) show that, for any k ∈ N and n = 2k + k + 1, there is a choice of S1 of the form described in Proposition 9 (e.g., the set of all length-n strings whose last 2k entries are zero), and a sequence S2 = S1 ∪ P 1, . . . , SL = SL−1 ∪ P L−1 of the form described in Proposition 10, such that SL = {0, 1}n for L = 2n−1 2k . This implies the existence and sufﬁciency statements from Theorem 1. The necessary condition results from straightforward parameter counting arguments; comparing the dimension dim(∆n) = 2n − 1 of the set being approximated and the number of parameters Ln2 + (L + 1)n of the DBM. This concludes the proof of Theorem 1. Details on the other statements are given in the Supplementary Material.  6 CONCLUSION  This paper proves that undirected layered deep networks are, in a well deﬁned sense, as powerful as their feedforward counterparts. We see this as an important contribution to developing better intuitions about the advantages and disadvantages of using feedforward vs. undirected architectures. The methods developed in this paper seem valuable for studying the effects of training undirected networks sequentially, or using the trained weights of a DBN to initialize a DBM.  8  Published as a conference paper at ICLR 2015  This paper proves the universal approximation property for narrow DBMs thereby settling an in- tuition that had been missing formal veriﬁcation for a surprisingly long time. This complements previously known results addressing RBMs and narrow DBNs, which can be regarded the shal- low and feedforward counterparts of narrow DBMs. We investigated the compositional structure of DBMs and presented a trick to separate the activities on the upper part of the network from those on the lower part of the network. This allowed us to trace parameter regions where DBMs can be regarded as feedforward networks, passing the probability distributions represented at the higher layers downwards from layer to layer by multiplication with independent conditional probability distributions which have the same form as those represented by feedforward layers.  ACKNOWLEDGMENTS  I am grateful to the Santa Fe Institute, where I was hosted while working on this article.  REFERENCES  Ackley, David H., Hinton, Geoffrey E., and Sejnowski, Terrence J. A learning algorithm for Boltz-  mann machines. Cognitive Science, pp. 147–169, 1985.  Amari, Shun-ichi, Kurata, Koji, and Nagaoka, Hiroshi. Information geometry of Boltzmann ma-  chines. Neural Networks, IEEE Transactions on, 3(2):260–271, Mar 1992.  Baldi, Pierre. Autoencoders, unsupervised learning, and deep architectures. In Guyon, Isabelle, Dror, Gideon, Lemaire, Vincent, Taylor, Graham W., and Silver, Daniel L. (eds.), ICML Unsuper- vised and Transfer Learning, volume 27 of JMLR Proceedings, pp. 37–50. JMLR.org, 2012.  Barron, Andrew R. Universal approximation bounds for superpositions of a sigmoidal function.  Information Theory, IEEE Transactions on, 39(3):930–945, May 1993.  Bengio, Yoshua and Delalleau, Olivier. On the expressive power of deep architectures. In Kivinen, Jyrki, Szepesvri, Csaba, Ukkonen, Esko, and Zeugmann, Thomas (eds.), Algorithmic Learning Theory, volume 6925 of Lecture Notes in Computer Science, pp. 18–36. Springer Berlin Heidel- berg, 2011.  Burger, Martin and Neubauer, Andreas. Error bounds for approximation with neural networks.  Journal of Approximation Theory, 112(2):235 – 250, 2001.  Chen, Tianping and Chen, Hong. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. Neural Networks, IEEE Transactions on, 6(4):911–917, Jul 1995.  Cho, Kyunghyun, Raiko, Tapani, Ilin, Alexander, and Karhunen, Juha. How to pretrain deep Boltzmann machines in two stages. In Koprinkova-Hristova, Petia, Mladenov, Valeri, and Kasabov, Nikola K. (eds.), Artiﬁcial Neural Networks, volume 4 of Springer Series in Bio- /Neuroinformatics, pp. 201–219. Springer International Publishing, 2015.  Cybenko, George. Approximation by superpositions of a sigmoidal function. Mathematics of Con-  trol, Signals and Systems, 2(4):303–314, 1989.  Freund, Yoav and Haussler, David. Unsupervised learning of distributions of binary vectors using 2-layer networks. In Moody, J. E., Hanson, S. J., and Lippmann, R. (eds.), Advances in Neural Information Processing Systems 4, NIPS ’91, pp. 912–919. Morgan Kaufmann, 1991.  Goodfellow, Ian J., Courville, Aaron C., and Bengio, Yoshua. Joint training deep Boltzmann ma-  chines for classiﬁcation. CoRR, abs/1301.3568, 2013a.  Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron C., and Bengio, Yoshua. Maxout networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of JMLR Proceedings, pp. 1319– 1327. JMLR.org, 2013b.  9  Published as a conference paper at ICLR 2015  Hinton, Geoffrey E. and Sejnowski, T. J. Parallel distributed processing: Explorations in the mi- crostructure of cognition, vol. 1. chapter Learning and Relearning in Boltzmann Machines, pp. 282–317. MIT Press, Cambridge, MA, USA, 1986.  Hinton, Geoffrey E. and Sejnowski, Terrence J. Optimal Perceptual Inference. In Proceedings of  the IEEE Conference on Computer Vision and Pattern Recognition, 1983.  Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee-Whye. A fast learning algorithm for deep  belief nets. Neural Computation, 18(7):1527–1554, July 2006.  Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert. Multilayer feedforward networks are  universal approximators. Neural Networks, 2(5):359 – 366, 1989.  Krause, Oswin, Fischer, Asja, Glasmachers, Tobias, and Igel, Christian. Approximation properties of DBNs with binary hidden units and real-valued visible units. In ICML (1), volume 28 of JMLR Proceedings, pp. 419–426. JMLR.org, 2013.  Le Roux, Nicolas and Bengio, Yoshua. Representational power of restricted Boltzmann machines  and deep belief networks. Neural Computation, 20(6):1631–1649, 2008.  Le Roux, Nicolas and Bengio, Yoshua. Deep belief networks are compact universal approximators.  Neural Computation, 22:2192–2207, 2010.  Leshno, Moshe, Lin, Vladimir Ya, Pinkus, Allan, and Schocken, Shimon. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural net- works, 6(6):861–867, 1993.  Martens, James, Chattopadhya, Arkadev, Pitassi, Toni, and Zemel, Richard. On the representational efﬁciency of restricted Boltzmann machines. In Burges, C.J.C., Bottou, L., Welling, M., Ghahra- mani, Z., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 26, pp. 2877–2885. Curran Associates, Inc., 2013.  Montavon, Gr´egoire and M¨uller, Klaus-Robert. Deep Boltzmann machines and the centering trick.  In Neural Networks: Tricks of the Trade, pp. 621–637. Springer Berlin Heidelberg, 2012.  Montavon, Gr´egoire, Braun, Mikio L., and M¨uller, Klaus-Robert. Deep Boltzmann machines as feed-forward hierarchies. In Lawrence, Neil D. and Girolami, Mark A. (eds.), Proceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS-12), vol- ume 22, pp. 798–804, 2012.  Mont´ufar, Guido. Mixture decompositions of exponential families using a decomposition of their  sample spaces. Kybernetika, 49(1), 2013.  Mont´ufar, Guido. Universal approximation depth and errors of narrow belief networks with discrete  units. Neural Computation, 26(7):1386–1407, 2014.  Mont´ufar, Guido and Ay, Nihat. Reﬁnements of universal approximation results for deep belief  networks and restricted Boltzmann machines. Neural Computation, 23(5):1306–1319, 2011.  Mont´ufar, Guido and Morton, Jason. Discrete restricted Boltzmann machines.  In International Conference on Learning Representations, ICLR ’13, 2013. Accepted for JMLR special topics issue Learning Representations.  Mont´ufar, Guido and Morton, Jason. When does a mixture of products contain a product of mix-  tures? SIAM Journal on Discrete Mathematics, 29:321–347, 2015.  Mont´ufar, Guido, Rauh, Johannes, and Ay, Nihat. Expressive power and approximation errors of restricted Boltzmann machines. In Shawe-Taylor, J., Zemel, R.S., Bartlett, P.L., Pereira, F., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 24, pp. 415–423. Curran Associates, Inc., 2011.  Mont´ufar, Guido, Ay, Nihat, and Zahedi, Keyan. Expressive power of conditional restricted Boltz-  mann machines. arXiv preprint arXiv:1402.3346, 2014a.  10  Published as a conference paper at ICLR 2015  Mont´ufar, Guido, Pascanu, Razvan, Cho, Kyunghyun, and Bengio, Yoshua. On the number of linear  regions of deep neural networks. In NIPS ’14, 2014b.  Pascanu, Razvan, Mont´ufar, Guido, and Bengio, Yoshua. On the number of response regions of deep feed forward networks with piece-wise linear activations. In International Conference on Learning Representations, ICLR ’14, 2014.  Salakhutdinov, Ruslan and Hinton, Geoffrey E. Deep Boltzmann machines. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics, volume 5, pp. 448–455, 2009.  Salakhutdinov, Ruslan and Hinton, Geoffrey E. An efﬁcient learning procedure for deep Boltzmann  machines. Neural Computation, 24(8):1967–2006, August 2012. ISSN 0899-7667.  Smolensky, Paul. Information processing in dynamical systems: foundations of harmony theory. In  Symposium on Parallel and Distributed Processing, 1986.  Sussmann, H´ector J. Learning algorithms for Boltzmann machines.  trol, 1988., Proceedings of the 27th IEEE Conference on, pp. 786–791 vol.1, 1988. 10.1109/CDC.1988.194417.  In Decision and Con- doi:  Sutskever, Ilya and Hinton, Geoffrey E. Deep narrow sigmoid belief networks are universal approx-  imators. Neural Computation, 20:2629–2636, 2008.  van der Maaten, Laurens. Discriminative restricted Boltzmann machines are universal approxima- tors for discrete data. Technical Report EWI-PRB TR 2011001, Delft University of Technology, 2011.  Younes, Laurent. Synchronous Boltzmann machines can be universal approximators. Applied Math-  ematics Letters, 9(3):109 – 113, 1996.  SUPPLEMENTARY MATERIAL  PROOF OF PROPOSITION 5  The k-th layer marginal of the DBM satisﬁes  p(xk) =  X  x0,...,xk−1,xk+1,...,xL  p(x0, x1, . . . , xL)  =  =  =  =  X  x0,...,xk−1,xk+1,...,xL  X  x0,...,xk−1,xk+1,...,xL  1  Z(W, b)  1  Z(W, b)  exp(cid:16)  exp(cid:16)  L−1  X  l=0  k−1  X  l=0  x⊤ l  Wlxl+1 +  x⊤ l  Wlxl+1 +  L  X  l=0  k−1  X  l=0  x⊤ l  bl(cid:17)  x⊤ l  bl + x⊤ k  k(cid:17) b′  × exp(cid:16)  L−1  X  l=k  x⊤ l  Wlxl+1 +  L  X  l=k  x⊤ l  bl − x⊤ k  k(cid:17) b′  exp(cid:16)  k−1  X  l=0  x⊤ l  Wlxl+1 +  k−1  X  l=0  x⊤ l  bl + x⊤ k  k(cid:17) b′  1  Z(W, b) X  x0,...,xk−1  × X  xk+1,...,xL  exp(cid:16)  L−1  X  l=k  x⊤ l  Wlxl+1 +  L  X  l=k  x⊤ l  bl − x⊤ k  k(cid:17) b′  1  Z(W, b)  Z(W(2), b(2))p(2)(xk) Z(W(1), b(1))p(1)(xk),  for all xk ∈ {0, 1}nk.  This shows that for any k-th layer marginal p(xk) representable by DBM, there is a distri- bution p(2)(xk) representable as the top layer marginal of DBM(2) with parameters W(2) =  11  Published as a conference paper at ICLR 2015  {W0, . . . , Wk−1}, b(2) = {b0, . . . , bk−1, b′ k}, and a distribution p(1)(xk) representable as the bottom layer marginal of DBM(1) with parameters W(1) = {Wk, . . . , WL−1}, b(1) = {bk − k, bk+1, . . . , bL}, such that the equation p(xk) = (p(2) ∗ p(1))(xk) holds, and vice versa. b′  APPROXIMATION OF STOCHASTIC MAPS  A stochastic map with inputs {0, 1}k and outputs {0, 1}m assigns a probability distribution p(·|i) ∈ ∆m to each input vector i ∈ {0, 1}k. DBMs can be used to deﬁne such maps by clamping the states of some of their units to the input values i, and taking the resulting conditional probability distribution over the states of some other units as the output distributions. One way of doing this is by dividing the visible units in two groups, corresponding to inputs and outputs, as x0 = (i, o). Given that p(x0) = p(i, o) stands in one to one relation to the pair (p(i), p(o|i)), Corollary 2 is a direct implication of Theorem 1.  Note that a universal approximator of stochastic maps is also a universal approximator of determin- istic maps. Every deterministic map i 7→ o = f (i) can be regarded as the special type of stochastic map i 7→ δf (i)(o), where δf (i) is the Dirac delta assigning probability one to o = f (i). Corollary 2 complements previous results addressing universal approximation of stochastic maps by conditional RBMs (van der Maaten 2011; Mont´ufar et al. 2014a). As discussed in (Mont´ufar et al. 2014a), in contrast to joint probability distributions, stochastic maps do not need to model the input distributions, and hence universal approximators of stochastic maps need not be universal approximators of joint probability distributions. It would be interesting to investigate corresponding reﬁnements of Corollary 2 in future work.  SOFTMAX UNITS  All arguments presented in the main part of this article hold for arbitrary ﬁnite valued units (not only binary units). An analysis of sequences of feedforward layers of ﬁnite valued units is available from (Mont´ufar 2014). This allows us to formulate Theorem 4 as a direct generalization of Theo- rem 1. The result can be further reﬁned to cases where each layer has units with different numbers of possible states. We omit further details at this point.  MINIMAL WIDTH OF UNIVERSAL APPROXIMATORS  In a layered network, a too narrow layer represents a bottleneck. It is an interesting question how narrow a universal approximator can be. Proposition 3 shows that if the visible layer has n0 units, then the ﬁrst hidden layer of a universal approximator must have at least n1 ≥ n0 − 1 units. In fact, when n0 is odd, this has to be at least n1 ≥ n0.  Proof of Proposition 3. This follows from the fact that the visible distributions of the DBM are mixtures of the conditionals p(x0|x1), for all x1 ∈ {0, 1}n1. Each of these conditional distributions is a product distribution. There are distributions on {0, 1}n0 that can only be approximated by mixtures of product distributions, if these mixtures involve mixture components that approximate all point measures assigning probability one to the binary strings with an odd number of ones (see Mont´ufar 2013). Now, Mont´ufar & Morton (2015; Proposition 3.19) show that when n0 is odd, there is no (n0 − 1)- generated zonoset with a point in each odd (or each even) orthant of Rn0. Without going into more details, this implies that, when n1 = n0 − 1, with odd n0, the set of conditionals {p(x0|x1) : x1 ∈ {0, 1}n1} cannot approximate the set of point measures that assign probability one to the binary strings with an odd (or even) number of ones.  We note that the same width bound holds for DBNs, since the visible distributions represented by DBNs are mixtures of the same product distributions as the visible distributions of DBMs.  12  Published as a conference paper at ICLR 2015  COMPARISON WITH NARROW DBNS  DBNs have the same network topology as DBMs, but with interactions directed towards the bot- tom layer, except for the interactions between the deepest two layers, which are undirected. The corresponding joint probability distributions have the form  L−2  pW,b(x0, x1 . . . , xL) = pWL−1,bL−1,bL (xL−1, xL)  pWl,bl(xl|xl+1),  Y l=0 for all (x0, . . . , xL) ∈ {0, 1}n0+···+nL .  (4)  Here the distributions of the states in the deepest two layers are given by  pWL−1,bL−1,bL (xL−1, xL) =  1  Z(WL−1, bL−1, bL)  exp(x⊤  L−1  WL−1xL+x⊤  L−1  bL−1+x⊤ L  bL),  for all (xL−1, xL) ∈ {0, 1}nL−1+nL .  (5)  The conditional distributions (feedforward layers), are given by  pWl,bl(xl|xl+1) =  1  Z(Wlxl+1, bl)  exp(x⊤ l  WlxL + x⊤ l  bl),  for all xl ∈ {0, 1}nl, for all xl+1 ∈ {0, 1}nl+1.  (6)  Although DBNs have undirected interactions between the top two layers, in the narrow case the universal approximation capability stems essentially from the feedforward part. A DBN with layers of width n is a universal approximator if the number of hidden layers satisﬁes L ≥ 2(n−log2(n)−1) and only if L ≥ 2n−(n+1) (Mont´ufar & Ay 2011). These bounds correspond exactly to the bounds n(n+1) we obtained in Theorem 1 for DBMs. In our proof we showed that the kinds of transformations of probability distributions exploited in (Mont´ufar & Ay 2011) in the context of DBNs can also be rep- resented by DBMs. In particular, our analysis shows that many distributions that are representable by DBNs are also representable by DBMs of the same size.  2n  COMPARISON WITH RBMS  In the case of one single hidden layer, the DBM reduces to an RBM. RBMs are universal approxi- mators, provided the hidden layer contains sufﬁciently many units. The minimal number of hidden units m for which an RBM with n visible units is a universal approximator is at least 2n−n n+1 and at most 2n−1 − 1 (Mont´ufar & Ay 2011). The exact value is not known, but there are examples where the lower bound is not attained. For narrow DBMs we obtained an upper bound on the minimal number of layers sufﬁcient for universal approximation of the form L ≥ 2n/2(n − log2(n) − 1). Hence both RBMs and narrow DBMs require at most a number of interaction weights and biases of order O(n2n−1). We should note that in both cases, it is possible to formulate restrictions on the interaction weights and biases in such a way that the total number of free parameters needed for universal approximation is 2n − 1, i.e., just as large as the dimension of the set ∆n.  EXPLOITING THE BACKWARD ACTIVITY  The product r∗s arising in Proposition 6 can be used to augment the input model that is passed to the feedforward layer. As long as this does not interfere with the choice of a desirable conditional q, this could be exploited to obtain a more compact construction of a universal approximator. Investigating this in detail could help us better understand the differences of DBNs and DBMs. It would be interesting to take a closer look at this in future work.  13  ",
1412.7659,2015,Transformation Properties of Learned Visual Representations,"['Transformation Properties of Learned Visual Representations', 'Taco Cohen and Max Welling']",https://arxiv.org/pdf/1412.7659,"5 1 0 2    r p A 7         ]  G L . s c [      3 v 9 5 6 7  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  TRANSFORMATION PROPERTIES OF LEARNED VISUAL REPRESENTATIONS  Taco S. Cohen & Max Welling Machine Learning Group Department of Computer Science University of Amsterdam {t.s.cohen, m.welling}@uva.nl  ABSTRACT  When a three-dimensional object moves relative to an observer, a change occurs on the observer’s image plane and in the visual representation computed by a learned model. Starting with the idea that a good visual representation is one that transforms linearly under scene motions, we show, using the theory of group representations, that any such representation is equivalent to a combination of the elementary irreducible representations. We derive a striking relationship be- tween irreducibility and the statistical dependency structure of the representation, by showing that under restricted conditions, irreducible representations are decor- related. Under partial observability, as induced by the perspective projection of a scene onto the image plane, the motion group does not have a linear action on the space of images, so that it becomes necessary to perform inference over a la- tent representation that does transform linearly. This idea is demonstrated in a model of rotating NORB objects that employs a latent representation of the non- commutative 3D rotation group SO(3).  1  INTRODUCTION  Much has been written about invariant representations (e.g. Anselmi et al. (2014)), and invariance to groups such as translations, rotations and projective transformations is indeed very important for object recognition. However, for a general purpose visual representation – capable not only of supporting recognition tasks but also motion understanding and geometrical reasoning – invariance is not enough. Instead, the transformation properties of a representation are crucially important. If we could un- derstand how a given representation of visual data transforms under various rigid or non-rigid trans- formations of the latent 3D scene, we would be in a better position to build an integrated system that computes invariant representations as well as motion and relative poses of objects. However, performing a mathematical analysis of the transformation properties of, for example, the hidden layer in a deep neural network under motions of a 3D scene is extremely complicated. A better approach is to directly impose good transformation properties on a representation space, and then learn the mapping between data and representation space such that these transformation properties are realized (Hinton et al., 2011). In this paper we study the transformation properties of distributed representations, using tools from group representation theory (Sugiura, 1990). We relate the transformation properties of a distributed representation to statistical notions such as decorrelation and conditional independence under the assumption of complete observability. Under partial observability (due to occlusion, for example) it becomes necessary to introduce latent variables in order to obtain a representation space with good transformation properties. We propose a number of transformation properties that a good representation should have, and present a simple model that demonstrates the idea by modelling 3D rotations of objects from the NORB dataset (LeCun & Bottou, 2004). Our model uses a single latent vector of coefﬁcients to represent a set of images of the same object seen in different (rotated) poses, and uses one latent element of the 3D rotation group SO(3) for each pose. A generative neural network model maps each transformed latent representation to an  1  Published as a conference paper at ICLR 2015  image. Unlike previous work on learning group representations (Rao & Ruderman, 1999; Miao & Rao, 2007; Sohl-Dickstein et al., 2010; Wang et al., 2011; Bruna et al., 2013; Cohen & Welling, 2014), our model does not assume a linear action of the group in the input space, but instead acts linearly on a latent representation of the 3D scene. Furthermore, our model is the ﬁrst learned Lie group model that can properly deal with non-commutative transformations. The rest of the paper is organized as follows. In the next section, we introduce the concept of a group representation which is at the core of our analysis. Section three contains the main theoretical results on the dependency structure of irreducible representations. It is followed by a discussion of the problems that arise when partial observability is taken into account in section 4. Section 5 presents a model and training algorithm for learning latent group representations, followed by experiments, related work and a conclusion.  2 SYMMETRIES AND REPRESENTATIONS  We start from the basic assumption that our learning agent is situated in space, and this space con- tains a scene. Formally, we represent the scene as a function x : R3 → RK that at each point p in space gives a list of numbers x(p) describing, for example, the color, transparency value, material properties, etc. at p. In this section and the next, we further assume full observability, i.e. that x is known entirely. We think of x as a vector in a Hilbert space S of sufﬁciently well-behaved functions. As we will see, the following analysis does not depend on this particular data representation, but it provides useful intuition and is ultimately realistic. We say that the vector x is a representation of the scene, because the numerical values that one would store in a computer to describe or approximate x depend on both “what is in the scene” and “how it is represented in our preferred frame of reference”. If we transform our reference frame by g, an element of the special Euclidean group SE(3) of rigid body motions, the points in space transform as g−1p. Such a transformation leaves invariant Euclidean distances, angles and areas, and is therefore called a symmetry of Euclidean space. Under this symmetry, the scene transforms as  (1) Notice that T (g)[αx + βy](p) = α[T (g)x](p) + β[T (g)y](p), so T (g) is a linear operator. We say that T is a representation of SE(3) in the Hilbert space S. Generically, a group representation is a map T : G → GL(V ) from a group G to the set of invertible linear transformations GL(V ) on a vector space V , that preserves the group structure in the following sense:  x(cid:48)(p) = x(g−1p) ≡ [T (g)x](p),  T (g)T (h) = T (gh),  (2)  for all g, h ∈ G. One can check that the map T deﬁned in eq. 1 is indeed a group representation. The requirement that (T, V ) forms a representation of SE(3) is a sufﬁcient condition for the vectors in V to describe “a thing in space”, because it requires them to transform as space does (Kanatani, 1990). This is true in particular for the Hilbert space construction given above, but applies more generally to any learned or hand-designed vector space representation. Should eq. 2 fail to hold, key aspects of what it means to transform as Euclidean space are lost: for example, two 180◦ rotations about the same axis might not equal the identity transformation, or two translations might fail to commute. Hence, we want our representation (in the representation learning sense) to be a linear representation of the special Euclidean group (in the group representation theory sense). From a modelling perspec- tive, one would like to understand all the possible ways in which this can be achieved. To this end, observe that if we have a representation T (g) and an invertible matrix F , then T (cid:48)(g) = F T (g)F −1 is also a representation, which is said to be equivalent to T . A key result in group representation theory tells us that every unitary representation is equivalent in this sense to a simple composition of basic building blocks called irreducible representations. A representation (T, V ) is called irreducible if there is no nontrivial subspace W ⊂ V that is mapped onto itself by all operators T (g) for g ∈ G. It can be shown (Sugiura, 1990) that uni- tary representations are fully reducible, which means that the representation T (g) is equivalent to a block-diagonal representation ˆT (g) whose blocks are irreducible. Such a block-diagonal representa-  2  Published as a conference paper at ICLR 2015  tion ˆT = F T (g)F −1 is said to be fully reduced. Each block in ˆT (g) is identiﬁed by an index which we denote by l, so that we can write ˆT l(g) for a block of index l in ˆT (g) and xl for the component of x ∈ V in the corresponding subspace. Irreducible representations are important for representation learning in a number of ways. Firstly, using irreducible representations is computationally more efﬁcient than using reducible ones. Ir- reducibility can also be used to deﬁne precisely what a “disentangled” representation is (Cohen & Welling (2014); see also Bengio et al. (2013)), and as shown in the next section, such a representa- tion will have a simple dependency structure when certain conditions are met. Finally, it is easier to compute a set of generators for the ring of polynomial invariants in an irreducible representation, which can be used to build invariant representations. Kazhdan et al. (2003) use a subset of generators (the power spectrum) to build invariant (but lossy) shape descriptors.  3  IRREDUCIBILITY, INDEPENDENCE AND DECORRELATION  Representation learning is often seen as a form of generative modelling, where the goal is to learn a latent variable model with a simple dependency structure in the latent space. The simplest examples are PCA and ICA, where the goal is to learn a linear model whose latent variables are all independent (with Gaussian or non-Gaussian marginal distributions, respectively). Alternatively, one can put the the transformation properties center stage and learn a representa- tion that transforms irreducibly under symmetry transformations (Cohen & Welling, 2014). In this perspective, the irreducible representations (and not the independent factors) are the elementary parts from which observation vectors are constructed. Given these contrasting conceptualizations of representation learning, it is interesting to investigate how the transformation properties of a repre- sentation are related its statistical properties. In this section we show that under certain conditions, irreducible representations are decorrelated or even conditionally independent.  3.1  IRREDUCIBILITY AND DECORRELATION: AN ELEMENTARY EXAMPLE  In order to gain some intuition, we introduce a simple toy model of a completely observable system with symmetry. The states of the system are sufﬁciently well-behaved functions x : S → R on the circle. Observations are generated by sampling a uniformly distributed rotation angle θ ∈ [0, 2π) and using it to rotate a template τ. That is, we have observations x(ϕ) = [T (θ)τ ](ϕ) = τ (ϕ− θ) for θ ∼ U[0, 2π). In practice, we will observe discretized functions with a ﬁnite number of coefﬁcients xn = x(ϕn). In this case, the linear transformation F that achieves the reduction into irreducible representations is the standard Fourier transform, and indeed it will decorrelate the data (Bruna et al., 2013). To see this, let ˆx = F x, and observe that  ˆx = F T (θ)τ = F T (θ)F −1 ˆτ ≡ ˆT (θ)ˆτ .  (3)  Thus ˆT = F T (θ)F −1 is the representation of the rotation group in the spectral domain. We know from linear algebra that a set of commuting diagonalizable matrices can be simultaneously diagonalized (see Memisevic (2012) and Henriques et al. (2014) for a discussion). Hence, the fully reduced representation ˆT is diagonal (not just block-diagonal), and the irreducible representations are one-dimensional. The diagonal elements are complex exponentials Tll(θ) = exp (ilθ). It follows immediately that the covariance matrix of the Fourier-transformed data is diagonal:  Ep(θ)[ˆxl ˆx∗  l(cid:48)] =  eilθ ˆτl e−il(cid:48)θ ˆτ∗ l(cid:48)  = δll(cid:48) |ˆτl|2,  dθ 2π  (4)  where δll(cid:48) equals 1 if l = l(cid:48) and 0 otherwise.  3.2  IRREDUCIBILITY AND DECORRELATION: GENERAL CASE  The following theorem gives a generalization of this result to the case of compact but not necessarily commutative groups.  3  (cid:90) 2π  0  Published as a conference paper at ICLR 2015  Theorem 1. Let G be a compact group, V a real vector space, and ˆT a fully reduced unitary representation of G in V . Furthermore, let ˆx = ˆT (g)ˆτ for a ﬁxed template ˆτ ∈ V and g distributed uniformly on G. The covariance matrix of the vectors in V is diagonal:  (cid:104)  (cid:105)  Ep(g)  m ˆxl(cid:48) ˆxl m(cid:48)  = δll(cid:48)δmm(cid:48)  (cid:107)ˆτ l(cid:107)2 dim ˆT l  .  Proof. Using orthogonality of the matrix elements of irreducible representations. See appendix.  The theorem is easily generalized to more than one template τ (in which case one should consider the class-conditional covariance), and it is likely that a slightly weaker theorem can be proven for locally compact groups, but we will not do so here. The main concern regarding the applicability of the above result is not the type of groups and spaces it applies to, but the fact that in reality the orbits are not sampled uniformly. For example, in a sample of natural images, a human face is more likely to appear in upright position than upside down. While the assumption of uniform sampling of orbits will not hold exactly in real datasets, it is nevertheless likely that irreducible and decorrelated representations will be similar to the degree that the data density is invariant to the group under consideration. A statistical objective such as decorrelation makes sense when one is working with iid draws from an underlying distribution of images, but this is a rather impoverished model of visual experience. As such, we think of decorrelation and independence as surrogate objectives for a deeper structural objective such as irreducibility.  3.3  IRREDUCIBILITY AND CONDITIONAL INDEPENDENCE  The concept of an irreducible representation can also shed light on time-series models based on transformations (Cohen & Welling, 2014; Michalski et al., 2014). We deﬁne  p(xt | xt−1, g) = N (xt | T (g)xt−1, σ2),  (5) where xt and xt−1 are observation vectors at times t and t − 1, respectively, and T (g) is a uni- tary representation of a compact group G. For G, one can construct an exponential family whose sufﬁcient statistics are given by the matrix elements ˆT l mn(g) of irreducible unitary representations of G. As shown in (Cohen & Welling, 2014) for the case of compact commutative groups, the invariance of the l2-norm in the exponent of the Gaussian to unitary transformations results in a posterior p(g|xt, xt−1) that is in the same exponential family as the prior p(g) (conjugacy). Furthermore, the marginal p(xt | xt−1) will factorize according the irreducible representations: t−1), so in this model an irreducible representation gives us conditional  p(ˆxt | ˆxt−1) =(cid:81)  t | ˆxl  l pl(ˆxl  independence.  4 PARTIAL OBSERVABILITY In reality, we do not observe the complete scene x ∈ S but only a projected image I ∈ I, which we model as a function I : R2 → R3 (for 3 color channels). Naively, one could try to construct a representation ¯T : SE(3) × I → I such that the perspective projection π : S → I is an equivariant map: ¯T (g)◦ π = π ◦ T (g), but this is not possible. The reason is that a 3D motion can bring entirely new structures into the image. In classical computer vision, the solution is sought in strong assumptions on the scene geometry, such as the assumption that the scene is planar, in which case one obtains a representation of the projective group on the image plane. This assumption leads to neat formulas but real scenes are not ﬂat. A better approach to the problem of partial observability is to model all variability that is not caused by the linear action of a low-dimensional Lie group as being caused by the action of the inﬁnite-dimensional group of diffeomorphisms (Bruna et al., 2013; Soatto, 2012). The scattering representations of Bruna & Mallat (2013) achieve simultaneous insensitivity to trans- lations and diffeomorphisms, and this method achieves very good performance on texture recog- nition and 2D pattern recognition (e.g. MNIST). However, diffeomorphisms are not an entirely satisfactory model of projected 3D motions either, because they are invertible by deﬁnition while  4  Published as a conference paper at ICLR 2015  zn  gn,v  xn,v  θ, σx  V  N  Figure 1: Graphical model description of the latent linear SO(3) representation.  projected motions are not. Furthermore, arbitrarily small scene motions can bring arbitrarily bright structures into the image, so scattering representations are not Lipschitz continuous to scene mo- tions. What we can do instead (at least in principle) is to learn a prior over scenes and a generative model of images given scenes, and then perform inference over scenes given images. By requiring that the latent scene transforms as a representation of a symmetry group, we bias the model towards representing latent properties of the scene as opposed to properties of the image (Kanatani, 1990; Soatto, 2012). By further requiring that the latent scene transforms irreducibly, we may also obtain a simple dependency structure in the latent space (by theorem 1).  5 A LATENT GROUP REPRESENTATION  In this section we deﬁne a simple model that demonstrates the idea of a latent group representation concretely. Let X n = [xn,1, . . . , xn,V ] be a matrix of V views of the same object instance n. We model such a set of views using a single latent vector zn and one latent transformation gn,v ∈ SO(3) per view, which we collect in a matrix Gn = [gn,1, . . . , gn,V ]. In order to generate xn,v, we ﬁrst compute zn,v = ˆT (gn,v)zn (this computation is explained in section 5.2) and then pass this transformed latent scene to a neural network. The conditional p(xn,v | zn, gn,v) is given by a normal distribution, centered on the output of a generative neural network fθ : RDz → RDx that maps z to x-space:  p(xn,v | zn, gn,v) = N (xn,v | fθ( ˆT (gn,v) zn), σ2 x)  (6)  We use a standard normal prior on z, and a uniform distribution over SO(3) for g. The complete graphical model is shown in ﬁgure 1. For regularization, we use a zero-mean Gaussian prior on the neural network weights and on ln σx (with precision β and α, respectively). The complete log joint probability for a single instance is then given by:  ln p(X n, Gn, zn, θ, σx) = − V(cid:88) −(cid:107)zn(cid:107)2  v=1  (cid:107)xn,v − fθ( ˆT (gn,v)zn)(cid:107)2  2σ2 x − α  (cid:107)θ(cid:107)2 2  − β  (ln σx)2  2  2  − V Dx 2  ln σx  (7)  The gradients of which are easily computed using backpropagation (in our own implementation, we compute gradients automatically using Theano (Bergstra et al., 2010)).  5.1 REPRESENTATION THEORY OF SO(3)  In order to compute the transformed latent scene zn,v = ˆT (gn,v)zn, we must understand the struc- ture of the unitary representation ˆT , and ﬁnd out how to compute it. Since any representation is  5  Published as a conference paper at ICLR 2015  (cid:88)  l(cid:88)  l≥0  m=−l  equivalent to a block diagonal one, we take ˆT to be block-diagonal:   ˆT l1 (g)  ˆT (g) =  . . .   ,  ˆT lN (g) where ˆT l is the matrix of an irreducible representation of index l. The complete set of irreducible unitary representations of SO(3) (the unitary dual) can be obtained by decomposing what is called the regular representation of SO(3) acting on functions on the sphere S2. In this case, the representation space is the Hilbert space H of square-integrable functions on the sphere, and the representation is deﬁned as T (g)x(p) = x(g−1p) for g ∈ SO(3), x ∈ H, p ∈ S2. A function x ∈ H can be decomposed as a sum of so-called real spherical harmonic functions Ylm : S2 → R (for l ≥ 0,|m| ≤ l). That is, for any x ∈ H we can write  (8)  x(p) =  clmYlm(p),  (9)  for some x-dependent coefﬁcients clm (comparable to ordinary Fourier coefﬁcients of a periodic function on the line). The matrix elements of the representation T in this basis are given by Ylm(p)Yln(g−1p)dp.  mn(g) = (cid:104)Ylm, T (g)Yln(cid:105) = ˆT l  Ylm(p)[T (g)Yln](p)dp =  (10)  (cid:90)  S2  (cid:90)  S2  this representation ˆT l(g), which maps coefﬁcients It can be shown (Sugiura, 1990) that (cl,−l, . . . , cl,l) to coefﬁcients (c(cid:48) l,−l, . . . , c(cid:48) l,l) corresponding to the expansion of the rotated function x(cid:48)(p) = x(g−1p), is irreducible. Furthermore, all irreducible unitary representations of SO(3) are equivalent to some ˆT l obtained in this way. To get an intuitive understanding of the transformation properties of the spherical harmonics, con- sider ﬁgure 2a: the basis functions on each row (corresponding to one value for l) can be linearly combined, and any rotation of the resulting function can again be expressed as a linear combination of only those basis functions. That is, each row corresponds to a representation. For the experiments detailed in section 6, we will be interested in the action of SO(3) on 3D objects, which could be represented as functions of some compact region of 3D space. Such a function can be decomposed by using multiple copies of each irreducible representation, as was done in Skibbe et al. (2009) in the context of rotation invariant shape descriptors.  5.2 COMPUTATION OF THE REPRESENTATION MATRICES We now turn to the computation of the transformation x → ˆT (g)x. Due to the block-structure of ˆT , this computation breaks up into a large number of relatively small matrix multiplies. The matrix elements ˆT l mn of the irreducible representations of SO(3) are known as the Wigner D-functions. The formulae given for these matrix elements by Wigner (1959) involve numerically unstable sums of many elements with large coefﬁcients. Quite surprisingly, given the prominence of these ma- trices in physical theories and their long history, a relatively recent paper introduced a novel and very fast method for computing the representation matrices in the basis of real spherical harmon- ics (Pinchon & Hoggan, 2007). The authors of this paper show that in the basis of real spheri- cal harmonics, a rotation speciﬁed by ZYZ-Euler angles g = (g1, g2, g3)T can be computed as ˆT l(g) = ˆT l z(g1), where J l is a precomputed symmetric orthogonal block matrix that exchanges the Y and Z axes, and T l z represents a z-axis rotation, which takes the simple form:  z(g3)J l ˆT l  z(g2)J l ˆT l  cos(lα)  cos((l − 1)α)  sin((l − 1)α)  sin(lα)  ˆT l z(α) =  − sin(lα)  − sin((l − 1)α)  . . .  . . .  1  . . .  . . .  6  cos((l − 1)α)    (11)  cos(lα)    Published as a conference paper at ICLR 2015  (a) Real Spherical Harmonics  (b) Real Wigner-D matrices for T35(π/8, 0, 0) = T35  z (π/8), T35(0, π/8, 0), and T35(π/8, π/8, π/8)  Figure 2: Real spherical harmonics and Wigner D-Matrices  Figure 2b shows the matrix ˆT 35 corresponding to weight l = 35 for three values of g. Naively implemented, this method has computational complexity O(l3) in dimension 2l + 1, due to the matrix multiplications. However, it is possible to apply the matrix ˆT l(g) to a vector without explicitly constructing it, using associativity: x(cid:48) = ˆT (g)x = ˆTz(g3)(J( ˆTz(g2)(J( ˆTz(g1)x)))). The sparse multiplication ˆTzx takes linear time, while Jx takes quadratic time. In practice, we use many copies of relatively low-dimensional representations, so the values of l are much smaller than the dimensionality of the latent space, and hence the quadratic complexity is not a concern.  5.3 LEARNING  We train the model using a stochastic hard EM algorithm, which involves alternating between the following steps:  1. In hard EM, the E-step consists of partial maximization with respect to zn and gn,v (v = 1, . . . , V ) for a single instance n while keeping the parameters θ, σx ﬁxed. We initialize the latent variables at the state of the last iteration and perform one step of gradient ascent on ln p(X n, Gn, zn, θ, σx).  2. The M-step consists of a maximization with respect to the parameters θ, σx while holding latent variables ﬁxed. In our stochastic algorithm, we perform a single gradient step on ln p(X n, Gn, zn, θ, σx).  We use adagrad for all optimization (Duchi et al., 2011).  6 EXPERIMENTS  We trained the model on the NORB dataset (LeCun & Bottou, 2004). This dataset consists of objects in 5 generic categories: four-legged animals, human ﬁgures, airplanes, trucks, and cars. Each category contains 10 instances, of which we used the last 5 for training. Each instance is imaged at 9 camera elevations (30 to 70 degrees from horizontal, in 5 degree increments) and 18 azimuths (0 to 340 degrees in 20 degree increments). Finally, there are 6 lighting conditions for each instance, yielding a total of 5 · 5 · 6 · 9 · 18 = 24300 images. The data was made zero mean, contrast normalized and then PCA whitened, retaining 95% of the variance. We used a neural network fθ with one hidden layer containing 550 hidden units. The group representation ˆT is determined by a choice of li; i = 1, . . . , L which we chose to be: [0] × 20 + [1]× 15 + [2]× 10 + [3]× 10 + [4]× 10 + [5]× 9 + [6]× 8 + [7]× 7 + [8]× 6 + [9]× 5, where the number in brackets represents li and the multiplier denotes its multiplicity. The regularization parameters were set to β = 0.1, α = 0.1. In ﬁgure 3, we show that the model is able to generate reasonable images for angles it has never seen before. The model is only trained on images that are off by 20 azimuthal degrees, but the model can produce images off by much smaller angles. In ﬁgure 4, we show that the model is able to extrapolate to unseen angles of a known object. That is, we train the model only on the azimuthal angle larger than 40 degrees from the reference ﬁgure  7  Published as a conference paper at ICLR 2015  Figure 3: Interpolation over 40 degrees for various objects.  Figure 4: Extrapolation: the ﬁrst two images in each sequence of 4 were not part of the training set.  (i.e. rotation 0), but produce a mean ﬁgure from the network at angles 0, 20, 40, 60. The model gives reasonable images that retain the object identity for poses in which it has not seen that object before.  7 RELATED WORK  Our work is related to the idea of transforming auto-encoders or “capsules” by Hinton et al. (2011). A transforming auto-encoder consists of many capsules, each of which learns to recognize a visual entity and predict its pose. The pose variables g are thus explicitly represented in the model, and act linearly on other pose variables, as is the case in our model. Unlike our model, a transforming auto- encoder represents the scene content as a set of probabilities, each of which indicates the likelihood of the preferred visual entity being present. The binary recognition unit used by a capsule for object z corresponds to an orbit O(z) = { ˆT (g)z | g ∈ G} in our model. Having a single latent space shared by multiple visual entities may aid in generalization, and makes it possible to compute metric relations between different ob- jects. Our approach should also deal better with (approximately) symmetric objects, for which it is not possible to unambiguously estimate pose and motion (what is the pose of a circle?). For the case of translational motion of an edge-like structure, this is known as the aperture problem (Memisevic, 2012). Instead of trying to estimate the motion anyway, our model would represent the edge as a vector whose orbit has reduced dimensionality compared to non-symmetric objects. That said, the fully connected generative network and hard-EM algorithm used in our current model are not suitable for dealing with large images, and so we consider the current model as only a proof of concept. A more scalable linear representation learning system could be based on a group-invariant convolutional network (e.g. Gens & Domingos (2014); Mallat (2012)) that generates a distributed representation that at each point locally describes the scene content, while transforming in a (locally) covariant manner.  8  Published as a conference paper at ICLR 2015  8 CONCLUSION  As the problem of object recognition in static images is steadily approaching the “solved” status, we should start looking towards the next frontier. One of the central challenges is to move away from the idea that images are i.i.d. draws from an underlying distribution (as is the case only in current day benchmark datasets), and begin to model the dynamics of the visual world. Another challenge is to generalize effectively from few examples, which necessitates the exploitation of symmetries of the data distribution. Both of these problems require us to take a closer look at the transformation properties of learned visual representations. In this paper, we have theoretically studied the consequences of assuming a linear representation of a symmetry group in the observed or latent representation space. We have shown that the entire class of such models can be understood mathematically (they are all direct sums of irreducible representations), and have shown how the theory specializes for the case of the 3D rotation group. Furthermore, we have shown that under uniform sampling of orbits, the geometrical objective of learning a linear, unitary and irreducible representation leads to decorrelated representations, thereby shedding new light on this common learning objective.  ACKNOWLEDGMENTS  This work was supported by NWO, grant number NAI.14.108.  REFERENCES Anselmi, Fabio, Leibo, Joel Z, Rosasco, Lorenzo, Mutch, Jim, Tacchetti, Andrea, and Poggio, Tomaso. Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning? Technical Report 001, MIT Center for Brains, Minds and Machines, 2014.  Bengio, Y., Courville, A., and Vincent, P. Representation Learning: A Review and New Perspec- tives. IEEE transactions on pattern analysis and machine intelligence, pp. 1–30, February 2013.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- Farley, D., and Bengio, Y. Theano: A CPU and GPU math compiler in Python. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy), pp. 1–7, 2010.  Bruna, Joan and Mallat, St´ephane. Invariant scattering convolution networks. IEEE transactions on pattern analysis and machine intelligence, 35(8):1872–86, August 2013. ISSN 1939-3539. doi: 10.1109/TPAMI.2012.230.  Bruna, Joan, Szlam, Arthur, and LeCun, Yann. Learning Stable Group Invariant Representations with Convolutional Networks. In International Conference on Learning Representations (ICLR), January 2013.  Cohen, T. and Welling, M. Learning the Irreducible Representations of Commutative Lie Groups.  In International Conference on Machine Learning (ICML), volume 32, 2014.  Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.  Gens, Robert and Domingos, Pedro. Deep Symmetry Networks. In Advances in neural information  processing systems, 2014.  Henriques, Joao F., Martins, Pedro, Caseiro, Rui, and Batista, Jorge. Fast Training of Pose Detectors  in the Fourier Domain. In Advances in neural information processing systems, 2014.  Hinton, GE, Krizhevsky, A, and Wang, SD. Transforming auto-encoders. ICANN-11: International  Conference on Artiﬁcial Neural Networks, Helsinki, 2011.  Kanatani, K. Group Theoretical Methods in Image Understanding. Springer-Verlag New York, Inc.,  Secaucus, NJ, USA, 1990. ISBN 0387512535.  9  Published as a conference paper at ICLR 2015  Kazhdan, Michael, Funkhouser, Thomas, and Rusinkiewicz, Szymon. Rotation invariant spheri- cal harmonic representation of 3D shape descriptors. In Eurographics Symposium on Geometry Processing, 2003.  LeCun, Y. and Bottou, L. Learning methods for generic object recognition with invariance to pose and lighting. Proceedings of the 2004 IEEE Computer Society Conference on Computer Vi- sion and Pattern Recognition, 2004. CVPR 2004., 2:97–104, 2004. doi: 10.1109/CVPR.2004. 1315150.  Mallat, Stephane. Group Invariant Scattering. Communications in Pure and Applied Mathematics,  65(10):1331–1398, 2012.  Memisevic, R. On multi-view feature learning. International Conference on Machine Learning,  2012.  Miao, X. and Rao, R. P. N. Learning the Lie groups of visual invariance. Neural computation, 19  (10):2665–93, October 2007.  Michalski, Vincent, Memisevic, Roland, and Konda, K. Modeling Deep Temporal Dependencies with Recurrent Grammar Cells. In Advances in neural information processing systems, pp. 1–9, 2014.  Pinchon, Didier and Hoggan, Philip E. Rotation matrices for real spherical harmonics: general rota- tions of atomic orbitals in space-ﬁxed axes. Journal of Physics A: Mathematical and Theoretical, 40(7):1597–1610, February 2007. ISSN 1751-8113.  Rao, R. P. N. and Ruderman, D. L. Learning Lie groups for invariant visual perception. Advances  in neural information processing systems, 816:810–816, 1999.  Skibbe, Henrik, Wang, Qing, and Reisert, Marco. Fast computation of 3d spherical fourier har- monic descriptors - a complete orthonormal basis for a rotational invariant representation of three- In IEEE International Workshop on 3-D Digital Imaging and Modeling dimensional objects. (3DIM 2009), in conjunction with the ICCV 2009, pp. 1863—-1869, 2009.  Soatto, Stefano. Steps Toward a Theory of Visual Information: Active Perception, Signal-to-Symbol  Conversion and the Interplay Between Sensing and Control. CoRR, abs/1110.2, 2012.  Sohl-Dickstein, J., Wang, J. C., and Olshausen, B. A. An unsupervised algorithm for learning lie  group transformations. arXiv preprint, 2010.  Sugiura, Mitsuo. Unitary Representations and Harmonic Analysis. John Wiley & Sons, New York,  London, Sydney, Toronto, 2nd edition, 1990.  Wang, CM, Shol-Dickstein, J, Tosic, Ivana, and Olshausen, Bruno A. Lie Group Transformation  Models for Predictive Video Coding. Data Compression Conference, pp. 83–92, 2011.  Wigner, E. P. Group Theory and Its Application to the Quantum Mechanics of Atomic Spectra,  1959. ISSN 00029505.  10  Published as a conference paper at ICLR 2015  9 APPENDIX  9.1 PROOF OF THEOREM 1  We have ˆx = ˆT (g)ˆτ, and g distributed uniformly. Let µ(g) denote the normalized Haar measure on G.  Ep(g)  m ˆxl(cid:48) ˆxl m(cid:48)  ˆT l mn(g)ˆτ l n  ˆT l(cid:48) m(cid:48)n(cid:48)(g)ˆτ l(cid:48) n(cid:48)  dµ(g)  (cid:33)  (cid:32)(cid:88)  n(cid:48)  ·  (cid:33)  mn(g) ˆT l(cid:48) ˆT l  m(cid:48)n(cid:48)(g)dµ(g)  δll(cid:48)δmm(cid:48)δnn(cid:48)  dim ˆT l  (12)  (cid:104)  (cid:105)  (cid:90)  G  n  G  (cid:32)(cid:88) (cid:90) (cid:88) (cid:88) (cid:88)  n ˆτ l(cid:48) ˆτ l n(cid:48)  n ˆτ l(cid:48) ˆτ l n(cid:48)  nn(cid:48)  nn(cid:48)  ˆτ l n ˆτ l n (cid:107)ˆτ l(cid:107)2 dim ˆT l  n  =  =  =  =  =  δll(cid:48)δmm(cid:48) dim ˆT l  δll(cid:48)δmm(cid:48)  Where we used the orthogonality of matrix elements of irreducible representations (Sugiura, 1990):  (cid:104) ˆT l  mn, ˆT l(cid:48)  m(cid:48)n(cid:48)(cid:105) =  δll(cid:48)δmm(cid:48)δnn(cid:48)  dim ˆT l  ,  (13)  and where dim ˆT l denotes the dimension of the representation indexed by l.  11  ",
1412.7028,2015,Joint RNN-Based Greedy Parsing and Word Composition,"['Joint RNN-Based Greedy Parsing and Word Composition', 'Joël Legrand and Ronan Collobert']",https://arxiv.org/pdf/1412.7028,"5 1 0 2    r p A 0 1         ]  G L . s c [      4 v 8 2 0 7  .  2 1 4 1 : v i X r a  PublishedasaconferencepaperatICLR2015JOINTRNN-BASEDGREEDYPARSINGANDWORDCOMPOSITIONJo¨elLegrandIdiapResearchInstitute,Martigny,Switzerland´EcolePolytechniqueF´ed´eraledeLausanne(EPFL),LausanneSwitzerlandjoel.legrand@idiap.chRonanCollobert∗FacebookAIResearch,MenloPark,CA,USAIdiapResearchInstitute,Martigny,Switzerlandronan@collobert.comABSTRACTThispaperintroducesagreedyparserbasedonneuralnetworks,whichleveragesanewcompositionalsub-treerepresentation.Thegreedyparserandthecom-positionalprocedurearejointlytrained,andtightlydependsoneach-other.Thecompositionprocedureoutputsavectorrepresentationwhichsummarizessyntac-tically(parsingtags)andsemantically(words)sub-trees.Compositionandtag-gingisachievedovercontinuous(wordortag)representations,andrecurrentneu-ralnetworks.WereachF1performanceonparwithwell-knownexistingparsers,whilehavingtheadvantageofspeed,thankstothegreedynatureoftheparser.Weprovideafullyfunctionalimplementationofthemethoddescribedinthispaper1.1INTRODUCTIONInNaturalLanguageProcessing(NLP),theparsingtaskaimsatanalysingtheunderlyingsyntacticstructureofanaturallanguagesequenceofwords(asentence).Theanalysisisexpressedasatreeofsyntacticrelationsbetweensub-constituentsofthesentence.Inthelinguisticworld,Chomsky(1956)ﬁrstintroducedformallytheparsingtask,bydeﬁningthenaturallanguagesyntaxasasetofcontext-freegrammarrules(aparticulartypeofformalgrammar),combinedwithtransformationsrules.Automatedsyntacticparsingbecamerapidlyakeytaskincomputationallinguistic.Aparsetreenotonlycarriessyntaxinformation,butmightalsoembedsomesemanticinformation(inthesensethatitcandisambiguatedifferentinterpretationsofagivensentence).Inthatrespect,parsingithasbeenwidelyusedasaninputfeatureforseveralotherNLPtaskssuchasmachinetransla-tion(Zollmann&Venugopal,2006),informationretrieval(Alonsoetal.,2002),orSemanticRoleLabeling(Punyakanoketal.,2008).Thispaperintroducesagreedyparserwhichleveragesanewcompositionapproachtokeepanhis-toryofwhathasbeenpredictedsofar.Thecompositionperformsasyntacticandsemanticsummaryofthecontentsofasub-treeintheformofavectorrepresentation.Thecompositionisperformedalongthetree:bottomtreenoderepresentationsareobtainedbycomposingcontinuouswordvectorrepresentations,andproducesvectorrepresentationswhichareinturncomposedtogetherinsubse-quentnodesofthetree.ThecompositionoperationaswellastreenodetaggingandpredictionsareachievedwithaRecurrentNeuralNetwork(RNN).Boththecompositionandnodepredictionaretrainedjointly.Section2presentsseveralrelatedapproaches.Section3detailsourparsingarchitecture.Anempir-icalevaluationofourmodelsaswellasourcompositionalvectorsisgiveninSection4.∗AllresearchwasconductedattheIdiapResearchInstitute,beforeRonanCollobertjoinedFacebookAIResearch.1Theparsercanbedownloadedatjoel-legrand.fr/parser.1PublishedasaconferencepaperatICLR20152RELATEDWORKTheﬁrstattemptstoautomaticallyparsenaturallanguageweremainlyconductedusinggenerativemodels.Awiderangeofparserwere,andstillare,basedonProbabilisticcontext-freegrammar(PCFGs)(Magerman,1995;Collins,2003;Charniak,2000).Thesetypesofparsersmodelthesyn-tacticgrammarbycomputingstatisticsofsimplegrammarrules(overparsingtags)occurringinatrainingcorpus.However,manylanguageambiguitiescannotbecaughtwithsimpletag-basedPCFGrules.AkeyelementinthesuccessofPCFGsistoreﬁnetheruleswithawordlexicon.ThisisusuallyachievedbyattachingtoPCFGsalexicalinformationcalledthehead-word.Sev-eralhead-wordvariantsexist,buttheyallrelyonadeterministicprocedurewhichleveragescleverlinguisticknowledge.Parsinginferenceismostlyachievedusingsimplebottom-upchartparser(Kasami,1965;Earley,1970;Kay,1986).Thesemethodsfaceaclassicallearningdilemma:ononehandPCFGruleshavetobereﬁnedenoughtoavoidanyambiguitiesintheprediction.Ontheotherhand,toomuchreﬁnementintheserulesimpliesloweroccurrencesinthetrainingset,andthusapossiblegeneralizationissue.PCFGs-basedparsersarethusjudiciouslycomposedwithcarefullychosenPCFGrulesandcleverregularizationtricks.2.1STATE-OF-THE-ARTDiscriminativeapproachesfromHenderson(2004);Charniak&Johnson(2005)outperformstan-dardPCFG-basedgenerativeparsers,butonlybydiscriminativelyre-rankingtheK-bestpredictedtreescomingoutofagenerativeparser.Toourknowledge,thestateoftheartinsyntacticparsingisstillheldbyMcCloskyetal.(2006),wholeveragesdiscriminativere-ranking,aswellasself-trainingoverunlabeledcorpora:are-rankeristrainedoveragenerativemodelwhichisthenusedtolabeltheunlabeleddataset.Theoriginalparseristhenre-trainedwiththisnew“labeled”corpus.Petrov&Klein(2007)introducedamethodtoautomaticalyreﬁnePCFGrulesbyiterativelysplitingthem.Thismethodleveragesanefﬁcientcoarse-to-ﬁneproceduretospeedupthedecodingprocess.Morerecently,Finkeletal.(2008);Petrov&Klein(2008)proposedPCFG-baseddiscriminativeparsersreachingtheperformanceoftheirgenerativecounterparts.ConditionalRandomFields(CRFs)areatthecoreofsuchapproaches.Carrerasetal.(2008)currentlyholdsthestate-of-the-artamongthe(non-reranking)discriminativeparsers.Theirparserleveragesaglobal-linearmodel(insteadofaCRF)withPCFGs,togetherwithvariousnewadvancedfeatures.Z.etal.(2010)showedthatjointlyusingmultipleself-trainedgrammarscanachievehigheraccuracythananindividualgrammar.Incontrasttotheseexistingapproaches,ourparserdoesnotrelyonPCFGs,noronreﬁnedfeatureslikehead-words.Taggingnodesisachievedinagreedymanner,usingonlyrawwordsandpart-of-speech(POS)asfeatures.Treenodehistoryismaintainedasavectorrepresentationobtainedinarecurrentfashion,bycomposingpastnoderepresentationsandtagpredictions.2.2GREEDYPARSINGManydiscriminativeparsersfollowsagreedystrategybecauseofthelack(ortheintractability)ofaglobaltreescoreforanentirederivationpathwhichwouldcombineindependentnodedecisions.Adoptingagreedystrategythatmaximizelocalscoresforindividualdecisionsisthenasolutionworthinvestigating.Oneoftheﬁrstsuccessfuldiscriminativeparsers(Ratnaparkhi,1999)wasbasedonMaxEntclassiﬁers(trainedoveralargenumberofdifferentfeatures)andpoweredagreedyshift-reducestrategy.Henderson(2003)introducedagenerativeleft-cornerparserwheretheprobabilityofaderivationgiventhederivationhistoricwasapproximatedusingaSimpleSynchronyNetworks(SNN),whichisaneuralnetworkspeciﬁcallydesignedforprocessingstructures.Turian&Melamed(2006)laterproposedabottom-upgreedyalgorithmfollowingaleft-to-rightoraright-toleftstrategyandusingusingafeatureboostingapproach.Inthisapproach,greedydecisionsregardingthetreeconstructionaremadeusingdecisiontreeclassiﬁers.Theirmodelwasneverthelesslimitedtoshortlengthsentences.Zhuetal.(2013)proposedashift-reduceparserwhichachievesresultscomparabletotheirchart-basedcounterparts.Thisisdonebyleveragingseveralunsuperviselytrainedfeatures(wordBrown2PublishedasaconferencepaperatICLR2015clustering,dependencyrelations,dependencylanguagemodel)combinedwithasmartbeamsearchstrategy.2.3PARSINGWITHRECURRENTNEURALNETWORKSRecurrentNeuralNetworks(RNNs)wereseenveryearly(Elman,1991)asawaytotackletheproblemofparsing,astheycannaturallyrecuralongtheparsetree.AﬁrstpracticalapplicationofRNNonsyntacticparsingwereproposedbyCostaetal.(2002).Theirapproachwasbasedonaleft-to-rightincrementalparser,wherearecursiveneuralnetworkwasusedtore-rankpossiblephraseattachments.Thegoaloftheircontributionwas,intheirownterms,theassessmentofamethodologyratherthanafullyfunctionalsystem.TheydemonstratedthatRNNswereabletocaptureenoughinformationtomakecorrectparsingdecisions.Collobert(2011)proposedapurelydiscriminativeparserbasedonneuralnetworks.ThismodelleveragedcontinuousvectorrepresentationsfromCollobert&Weston(2008),andbuildsthefullparsingtreeinabottom-upmanner.Todealwiththerecursivestructureinherenttosyntacticparsing,averysimplehistorywasgiventothenetworkasanewvectorfeature(correspondingtothenearesttagspanningthewordbeingtagged).Socheretal.(2011)alsoleveragedcontinuousvectorsfromCollobert&Weston(2008),combiningthemtobuildatreeinagreedymanner.However,thisworkdidnottacklethefullparsetreeproblem,butwasrestrictedtounlabeledbracketing.Socheretal.(2013)introducedtheCompositionalVectorGrammar(CVG)whichcombinesPCFGswithaSyntacticallyUntiedRecursiveNeuralNetwork(SU-RNN).Compositionisperformedoverabinarytree,thenusedtoscoretheK-besttreescomingoutofagenerativeparser.Foragiven(parent)nodeofthetree,theauthorsapplyacompositionoperationoveritschildnodes,conditionedwiththeirsyntaxinformation.Incontrast,wecomposephrases(notlimitedtotwowords).Boththewordsandsyntaxinformationofthechildnodesarefedtoeachcompositionoperation,leadingtoavectorrepresentationofeachtreenodecarryingbothsomesemanticandsyntacticinformation.Wealsodonotrelyonanygenerativeparserasourmodeljointlytrainsthetaskofnodeprediction,andthetaskofnodecomposition.Legrand&Collobert(2014)proposedagreedyRNN-basedparser.Theneuralnetworkwasrecur-rentonlyinthesenseitusedpreviouslypredictedtagstoproducenexttreenodetags.ContrarytoSocheretal.(2013),itdidnotinvolvecomposingsub-treerepresentations.Instead,head-wordswereusedasakeyfeature.Ourapproachsharessomesimilaritieswith(Legrand&Collobert,2014),asitisalsoagreedyparserbasedonRNNs.However,insteadofrelyingonhead-words(whichcouldbeseenasasimplisticrepresentationsofsub-trees),weleveragecompositionalsub-treevectorrep-resentationstrainedjointlywiththeparser.Thisapproachleadstomuchbetterparsingaccuracy,whilerelyingonlyonafewsimplefeatures(wordsandPOStags).Ourmodelhasalsotheabilityofproducingphraseembeddings,whichmayrepresentavaluablefeatureforotherNLPtasks.Chen&Manning(2014)proposedagreedytransition-baseddependencyparserbasedonneuralnetworks,fedwithdensewordandtagvectorrepresentations.Incontrasttoourapproach,itdoesnotintegrateacompositionalprocedureoversentencesub-trees.Thenetworkisonlyinvolvedinpredictingcorrecttransitionsateachstepoftheparsingprocess.3GREEDYRNNPARSINGOurparserisbasedonaneuralnetworktagger,andperformparsinginagreedyrecurrentway.Ourapproachisabottom-upiterativeprocedure:thetreeisconstructedstartingfromtheterminalnodes(sentencewords),asshowninFigure1.Ateachiteration,1.Welookforallpossiblenewtreenodesmerginginputconstituents(i.e.,headsofthetreespredictedsofarorleaveswhichhavenotbeencomposedsofar).Forthatpurpose,weapplyaneuralnetwork(seeFigure3)slidingwindowtaggeroverinputconstituentsX1,...,XN.ConsideringanarbitraryruleA→Xi,Xi+1,...,Xj3PublishedasaconferencepaperatICLR2015IW:Lookaroundandchooseyourownground.IT:VBRPCCVBPRP$JJNN.O:OS-PRTOOB-NPI-NPE-NPOIW:LookR1andchooseR2.IT:VBPRTCCVBNP.O:B-VPE-VPOB-VPE-VPOIW:R3andR4.IT:VPCCVP.O:B-VPI-VPE-VPOIW:R5.IT:VP.O:B-SE-SFigure1:Greedyparsingalgorithm,onthesentence“Lookaroundandchooseyourownground.”.IW,ITandOstandforinputwords(orcomposedwordrepre-sentationsRi),inputsyntactictags(parsingorpart-of-speech)andoutputtags(pars-ing),respectively.SeeFigure2andSection3.2forthewordcompositionprocedure.Thetreeproducedafter4greedyiterations(asshownhere)canbereconstructedasthefollowing:(S(VP(VP(VBLook)(PRT(RParound)))(CCand)(VP(VBchoose)(NP(PRP$your)(JJown)(NNground))))(..)).deﬁninganewnodewithtagA,thetaggerwillproducepreﬁxedtagsB-A,I-A,...E-A,respectivelyforconstituentsXi,Xi+1,...,Xj,followingaclassicalBIOESpreﬁxingscheme2.2.Asimpledynamicprogrammingisperformed,onlytoinsurethecoherenceofthetagprediction(e.g.,aB-AcanbefollowedonlybyaI-AoraE-A).3.A(neuralnetwork)compositionmodulecomputesvectorrepresentationsofthenewnodes,accordingtotherepresentationsofthemergedconstituents,aswellasthetagpredictions(seeFigure2).4.Newpredictednodesbecomeinputconstituentsandwegobackto1(seeFigure1).Oursystemisrecurrentintwoways:newlypredictedparsingnodelabelsaswellasvectorrepresen-tationsobtainedbycomposingthesepredictednodes,areusedinthenextiterationofouralgorithm.Wewilldetailourarchitectureinthefollowing.3.1WORDEMBEDDINGSFollowingtheworkfromCollobert&Weston(2008)onvariousNLPtasks,ourparserreliesonrawwords.EachwordinaﬁnitedictionaryW,isassignedacontinuousvectorrepresentation.Theserepresentationsasallparametersofourarchitecturearetrainedbyback-propagation.Moreformally,givenasentenceofNwords,w1,w2,...,wN,eachwordwn∈WisﬁrstembeddedinaD-dimensionalvectorspacebyapplyingalookup-tableoperation:LTW(wn)=Wwn,wherethematrixW∈RD×|W|representstheparameterstobetrainedinthislookuplayer.EachcolumnWn∈RDcorrespondstothevectorembeddingofthenthwordinourdictionaryW.Inthiswork,twokindoffeaturesareusedtofeedthenetworks:words(orwordcompositions)andPOStagsT(orparsingtagsP).Asforwords,alookup-tableassociateseachtagtintheﬁnitesetoftagT∪PwithacontinuousvectorrepresentationofsizeT.Theoutputvectorsofthedifferentlookup-tablesaresimplyconcatenatedtoformtheinputofthenextlayer.2Begin,Intermediate,Other,End,Single.ThisapproachisveryoftenusedinNLP,whenonewantstorewriteachunk(herenode)predictionproblemintoawordtaggingproblem.4PublishedasaconferencepaperatICLR2015chooseVByourPRP$ownJJgroundNNC3R2C2R4Figure2:Recurrentcompositionofthesub-tree(VP(VBchoose)(NP(PRP$your)(JJown)(NNground))).TherepresentationR2isﬁrstcomputedusingthe3-inputsmoduleC3withyour/PRP$own/JJground/NNasinput.R4isob-tainedbyusingthe2-inputsmoduleC2withchoose/VBR1/NPasinputXi−2Xi−1XiXi+1Xi+2Concath(M1×.)M2×.s1s2s|˜P|...Figure3:AconstituentXiistaggedbycon-sideringaﬁxedsizecontextwindowofsizeK(hereK=5).Theconcatenatedoutputofthecompositionalhistoryandconstituenttagsisfedasinputtothetagger.ItoutputsascoreforeachBIOES-preﬁxedparsingtag.Thetaggerisastandardtwo-layerneuralnet-work.Tagsforthecurrentsequenceofcon-stituentsX1,...,XNisobtainedbysimplyslidingthisnetworkoverthesequence.Usingcontinuouswordvectorsasinputallowsustotakeadvantageofunsuperviselypre-trainedwordembeddings.Lotofworkonthisdomainhasbeendoneinrecentyear,includingCollobert&Weston(2008),Mikolovetal.(2013).Inthispaper,wechosetousetherepresentationsfromLebret&Collobert(2014),obtainedbyasimplePCAonamatrixofwordco-occurrences.3.2WORD-TAGCOMPOSITIONAteachstepoftheparsingprocedure,werepresentseachnodeofthetreeasavectorrepresentation,whichsummarizesboththesyntax(predictedPOSorparsingtags)andthesemantic(words)ofthesub-treecorrespondingtothegivennode.AsshowninFigure2,thevectorrepresentationisobtainedbyasimplerecurrentprocedure,whichinvolvesseveralcomponents:•Wordvectorrepresentationsfortheleaves(comingoutfromalookuptable)(dimensionD).•Tag(POSfortheleaves,predictedtagsotherwise)vectorrepresentations(alsocomingoutforanotherlookuptable,asexplainedinSection3.1)(dimensionT).•CompositionalnetworksCk().EachofthemcancompresstherepresentationofachunkofsizekintoaD-dimensionalvector.Compositionalnetworkstakeasinputboththemergednoderepresentationsandpredictedtagrep-resentations.ThereisonedifferentnetworkCkforeachpossiblenodewithanumberofkmergedconstituent.Inpracticemosttreenodesdonotmergemorethanafewconstituents3.Inourcase,denotingz∈R(D+T)×ktheconcatenationofthemergedconstituentrepresentations(kvectorsoftagsandconstituentrepresentations),thecompositionalnetworkissimplyamatrix-vectoroperationfollowedbyanon-linearityCk(z)=h(Mkz),whereMk∈RD×(k(D+T))isamatrixofparameterstobetrained,andh()isasimplenon-linearitysuchasapointwisehyperbolictangent.3Taking1≤k≤5coversalready98.6%ofthenodesintheWallStreetJournaltrainingcorpus,and1≤k≤7covers99.8%.5PublishedasaconferencepaperatICLR2015Notethatnodeandwordrepresentationsareembeddedinthesamespace.Thisway,thecom-positionalnetworksCkcancompressindifferentlyinformationcomingfromleavesorsub-trees.Implementation-wise,onecanstorenewnoderepresentationsintothewordlookup-tableasthetreeiscreated,suchthatsubsequentcompositionortaggingoperationscanbeachievedinanefﬁcientmanner.3.3SLIDINGWINDOWBIOESTAGGERThetaggingmoduleofourarchitecture(seeFigure3)isatwo-layerneuralnetworkwhichappliesaslidingwindowovertheinputconstituentrepresentations(ascomputedinSection3.2),aswellastheinputconstituenttagrepresentations.ConsideringNinputconstituentsX1,...,XN,ifweassumetheirrespectiverepresentationshasbeenstoredsofarinlookuptables,thenthwindowisdeﬁnedasun=[LT(Xn−K−12),...,LT(Xn),...,LT(Xn+K−12)],whereKisthesizeofwindow.Denoting˜PthesetofBIOES-preﬁxedparsingtagsfromP,themoduleoutputsavectorofscoress(un)=[s1,...,s|˜P|](wherestisthescoreoftheBIOES-preﬁxedparsingtagt∈˜PfortheconstituentXn).Theconstituentwithindicesexceedingtheinputboundaries(n−(K−1)/2<1orn+(K−1)/2>N)aremappedtoaspecialpaddingvector(whichisalsolearned).Asanyclassicaltwo-layerneuralnetwork,ourarchitectureperformsseveralmatrix-vectoroperationsonitsinputs,interleavedwithsomenon-lineartransferfunctionh(·),s(un)=M2h(M1un),wherethematricesM1∈RH×K|D|andM1∈R|˜P|×Harethetrainedparametersofthenetwork,andh()isapointwisenon-linearfunctionsuchasthehyperbolictangent.ThenumberofhiddenunitsHisahyper-parametertobetuned.3.4COHERENTBIOESPREDICTIONSThenextmoduleofourarchitectureaggregatestheBIOES-preﬁxedparsingtagsfromourtaggermoduleinacoherentmanner.ItisimplementedasaViterbidecodingalgorithmoveraconstrainedgraphG,whichencodesallthepossiblevalidsequencesofBIOES-preﬁxedtagsoverconstituents:e.g.B-AtagscanonlybefollowedbyI-AorE-Atags,foranyparsinglabelA.Eachnodeofthegraphisassignedascoreproducedbythepreviousneuralnetworkmodule(scoreforeachBIOES-preﬁxedtag,andforeachword).ThescoreS([t]N1,[X]N1,θ)forasequenceoftags[t]N1inthelatticeGissimplyobtainedbysummingscoresalongthepath([X]N1beingtheinputsequenceofconstituentsandθalltheparametersofthemodel).Wefollowedtheexactsameapproachasin(Legrand&Collobert,2014),exceptthattransitionscores(edgesonthegraph)wereallsettozero.Indeed,weobservedinempiricalexperimentsthataddingtransitionsscoresdoesnotimproveF1-scoreperformance.Thisdecodingisthuspresentonlytoinsurecoherenceinthepredictedsequenceoftags.3.5TRAININGPROCEDUREBoththecompositionnetworkandtaggingnetworksaretrainedbymaximizingalikelihoodoverthetrainingdatausingstochasticgradientascent.Weperformedallpossibleiterations,overalltrainingsentences,ofthegreedyprocedurepresentedinFigure1constrainedwiththeprovidedlabeledparsetree.Thisleadstoourtrainingsetofsequencesoftreenodes.Whilethisprocedureissimilarto(Legrand&Collobert,2014),itisworthmentioningthatitimpliesthesystemisonlytrainedoncorrectsequencesoftreenodes.Inthatrespect,itisnottrainedtorecoverfrompastmistakesitcouldhavemadeduringtherecurrentprocess.Foreverytreenode,thesub-trees(structureandtags)werealsoextractedduringthisprocedure.Trainingthesystemconsistsinrepeatingthefollowingsteps:•Pickarandomsequenceofnodesextractedinthetrainingset,asdescribedabove.Considertheassociatedsub-treesforeachnodewhichisnotaleaf.•Performaforwardpassoftheword-tagcomposer(seeSection3.2)alongthesesub-trees.6PublishedasaconferencepaperatICLR2015•Forallnodesinthesequence,performaforwardpassofthetaggeraccordingtoword(orsub-tree)representations,aswellasconstituenttags.•ComputealikelihoodoftherightsequenceofBIOS-preﬁxedtags(seebelow),giventhescoresofthetagger.•Backwardgradientthroughthetaggeruptotheword(orsub-tree)andtagrepresentations.•Backwardgradientthroughtheword-tagcomposeruptothewordandtagrepresentation.•Updateallmodelparameters(fromcompositionalnetworksCi,taggernetwork,andlookuptables)withaﬁxedlearningrate.Detailsaboutthetraininglikelihoodcanbefoundin(Legrand&Collobert,2014).ThescoreS([t]N1,[X]N1,θ)ofthetruesequenceofBIOS-preﬁxedtags[t]N1,giventheinputnodesequence[X]N1canbeinterpretedasaconditionalprobabilitybyexponentiatingthisscore(thusmakingitpositive)andnormalizingitwithrespecttoallpossiblepathscores.Thelog-probabilityofase-quenceoftags[t]N1fortheinputsequenceofconstituents[X]N1isgivenby:logP([t]N1|[X]N1,θ)=S([t]N1,[X]N1,θ)(1)−logX∀[t0]N1expS([t0]N1,[X]N1,θ).Thesecondtermofthisequation(whichcorrespondtothenormalisationterm)canbecomputedinlineartimethankstoarecursionsimilartotheViterbialgorithm(seeRabiner,1989).Similartrainingprocedureshavebeenproposedinthepastforstructureddata(Denker&Burges,1995;Bottouetal.,1997;Laffertyetal.,2001).4EXPERIMENTS4.1CORPUSExperimentswereconductedusingthestandardEnglishPennTreebankdataset(Marcusetal.,1993).Weadoptedtheclassicalsetup,withsections02-21fortrain,section22forvalidation,andsection23fortest.Thevalidationcorpuswasusedtoselectourhyper-parametersandbestmodels.Wepre-processedthedataonlywithasubsetofoperationswhichareachievedinstandardparsers:(1)functionallabels,traceswereremoved,(2)thePRTlabelwasreplacedbyADVP(Magerman,1995).(3)Wetackledtheunarychainissue-non-terminalswithasinglenon-terminalchild-bymergingthenodestogetherandassigningastagtheconcatenationofthemergednodetags.Thiswasdoneinordertoavoidloopingissuesintheparsingalgorithm(e.g.anodebeingrepetitivelytaggedwithtwodifferenttagsinouriterativeprocess)andensuretheconvergenceoftheparsingprocess.Onlyconcatenatedlabelswhichoccurredatleast30times(correspondingtothelowestnumberofoccurrencesofthelesscommonoriginalparsingtag)werekept,leadingto11additionalparsingtags.Addedtotheoriginal26parsingtags,thisresultedin161tagsproducedbyourparser.Attesttime,theinverseoperationisperformed:concatenatedtagnodesaresimplyexpandedintotheiroriginalform.4.2DETAILEDSETUPOursystemsweretrainedusingastochasticgradientdescentovertheavailabletrainingdataun-tilconvergenceonthevalidationset.Hyper-parameterswerechosenaccordingtothevalidation.Lookup-tablesizesforthewordsandtags(part-of-speechandparsing)are200and20,respectively.ThewindowsizeforthetaggerisK=7(3neighboursfromeachside).Thesizeofthetagger’shiddenlayerisH=500.WeusedthewordembeddingsobtainedfromLebret&Collobert(2014)toinitializethewordlookup-table.Theseembeddingswerethenﬁne-tunedduringthetrainingpro-cess.Weﬁxedthelearningratetoλ=0.15duringthestochasticgradientprocedure.AssuggestedinPlaut&Hinton(1987),thelearningratewasdividedbythesizeoftheinputvectorofeachlayer.Thepart-of-speechtagswereobtainedusingthefreelyavailablesoftwareSENNA4.4http://ml.nec-labs.com/senna7PublishedasaconferencepaperatICLR2015051015859095100IterationsF1-scoretrain(standard)valid(standard)train(dropout)valid(dropout)Figure4:TrainandvalidationF1-score,ac-cordingtothenumberoftrainingiterations,withandwithoutthe“dropout”procedure.0102030405060708090100SentencelengthValidationF1ValidationF1#sentences0200400600#sentencesFigure5:ValidationF1andnumberofsen-tences,accordingthethesentencelength.4.3WORDEMBEDDINGDROPOUTREGULARIZATIONWefoundthatoursystemwaseasilysubjecttooverﬁtting(trainingF1-scoreincreasingwhilethevalidationcurvewaseventuallydecreasingasshowninFigure4).Asthecapacityofournetworkmainlyliesonthewordsandtagembeddings,weadoptedadropoutregularizationstrategy(seeHintonetal.,2012)forthelookuptables.Thekeyideaofthedropoutregularizationistorandomlydropunits(alongwiththeirconnections)fromtheneuralnetworkduringtraining.Thispreventsunitsfromco-adaptingtoomuch.Inourcase,duringthetrainingphase,a“dropoutmask”isappliedtotheoutputofthelookup-tables:eachelementoftheoutputissetto0withaprobability0.25.Attesttime,nopatchisappliedbuttheoutputisre-weighted,scalingitby0.75.WeobservedagoodimprovementinF1-scoreperformance,asshowninFigure4.4.4PERFORMANCECOMPARISONF1performancescoresarereportedinTable1.ScoreswereobtainedusingtheEvalbimplemen-tation5.Wecomparedoursystemiscomparedwitharangeofdifferentstate-of-the-artparsers.Inadditiontothethefourmaingenerativeparsers,wereportthescoresofwellknownre-rankingparsers(includingthestate-of-the-artfromMcCloskyetal.(2006))aswellasfortwomajorpurelydiscriminativeparsers.DetailederroranalysiscomparedagainstasubsetoftheseparsersisreportedinTable2,usingthecodeprovidedbyKummerfeldetal.(2012).PerformancewithrespecttosentencelengthisreportedinFigure5.Weincludedavotingprocedureusingseveralmodelstrainedstartingfromdifferentrandominitial-izations.Thevotingprocedureisachievedinthefollowingway:ateachiterationofthegreedyparsingprocedure,giventheinputsequenceofconstituents,(1)noderepresentationsarecomputedforeachmodelbycomposingthesub-treerepresentationscorrespondingtothegivenmodelandus-ingitsowncompositionalnetwork(2)eachmodelcomputestagscoresusingitsowntaggernetwork(3)tagscoresareaveraged(4)acoherentpathoftagispredictedusingtheViterbialgorithm.Finally,wereportabriefquantitativeevaluationofourcompositionalrepresentationsinTable3.RandomphraseswerepickedintheWSJcorpus,andclosestneighbors(accordingtotheEuclideandistance)withotherphrasesofthecorpusarereported.5Availableathttp://nlp.cs.nyu.edu/evalb8PublishedasaconferencepaperatICLR2015Table1:Performancecomparisonofdifferentstate-of-the-artparsers,intermsofPrecision(P),Recall(R),andF1score,forsentencesofsize≤40words,andonthefullWSJtestset.Vxdenotesavotingprocedurewithxmodels.Thereportedtime(inseconds)isthetimetoparsethefullWSJtestcorpus.<40FULLMODEL(R)(P)F1(R)(P)F1TIMEMAGERMAN(1995)84.684.984.8GENERATIVECOLLINS(1999)88.588.788.688.188.388.21247CHARNIAK(2000)90.190.190.189.689.589.6PETROV&KLEIN(2007)90.790.590.690.298.990.1307GENERATIVEHENDERSON(2004)89.890.490.1WITHCHARNIAK&JOHNSON(2005)92.091.1RE-RANKINGMCCLOSKYETAL(2006)92.1SOCHERETAL(2013)91.190.4390CARRERASETAL.(2008)90.791.491.1DISCRIMINATIVELEGRAND&COLLOBERT(2014)(V10)90.090.190.189.689.789.6LEGRAND&COLLOBERT(2014)+DROPOUT(V10)90.690.190.490.289.789.9THISWORK88.889.189.088.288.688.4THISWORK+DROPOUT89.790.39089.189.989.530THISWORK+DROPOUT(V4)90.590.890.790.190.490.3120Table2:Detailedparsercomparison.Wereporttheaveragenumberofbracketerrorspersentencefordifferenterrorcategories.PPCLAUSEDIFFMODNP1-WORDNPATTACHATTACHLABELATTACHATTACHCO-ORDSPANUNARYINT.OTHERMCCLOSKYETAL(2006)0.600.380.310.250.250.230.200.140.140.50SOCHERETAL(2013)0.790.430.290.270.310.320.310.220.190.41LEGRANDETAL(2014)0.740.450.270.250.340.380.240.220.200.57THISWORK+DROPOUT0.780.440.290.270.360.420.240.210.200.60THISWORK+DROPOUT(V4)0.710.430.250.240.350.380.230.210.190.565CONCLUSIONInthispaper,weintroducedanewparsingarchitecturewhichleveragesRNN-basedcompositionalrepresentationofparsingsub-trees,bothencodingthesyntactic(tags)andsemantic(words)infor-mation.Theparsingprocedureistightlyintegratedwiththecompositionoperation,andallowsustoreachperformanceofverywell-knownparserswhile(1)adoptingagreedyandfastprocedure(2)avoidstandardreﬁnedfeaturessuchasheadwords.ACKNOWLEDGMENTSPartofthisworkwassupportedbyNECLaboratoriesAmerica.REFERENCESAlonso,M.A.,Vilares,J.,andDarriba,V.M.Ontheusefulnessofextractingsyntacticdependenciesfortextindexing.InArtiﬁcialIntelligenceandCognitiveScience.2002.Bottou,L.,LeCun,Y.,andBengio,Y.Globaltrainingofdocumentprocessingsystemsusinggraphtransformernetworks.InProc.ofComputerVisionandPatternRecognition,1997.Carreras,X.,Collins,M.,andKoo,T.TAG,dynamicprogramming,andtheperceptronforefﬁ-cient,feature-richparsing.InProceedingsoftheTwelfthConferenceonComputationalNaturalLanguageLearning,2008.Charniak,E.Amaximum-entropy-inspiredparser.InProceedingsofthe1stNorthAmericanChap-teroftheAssociationforComputationalLinguisticsConference,2000.Charniak,E.andJohnson,M.Coarse-to-ﬁneN-bestparsingandMaxEntdiscriminativereranking.InProceedingsofthe43rdAnnualMeetingonAssociationforComputationalLinguistics,2005.9PublishedasaconferencepaperatICLR2015Table3:Nearestneighbors(intermsofvectorrepresentationEuclideandistance)forseveralphrasesintheWSJcorpus.Foreverynodeinthecorpus,thesub-treerepresentationswerecomputed.Then,fortheselectedphrases,wecomputedallEuclideandistances.Wereportbelowthe5topclosestotherphrasesinWSJ.brendanbarba,chairmanofthemoonachie,n.j.,makerofplasticﬁlmproductsedmundedelman,chairmanofthelosangelescountyboardofsupervisorsestherdyson,editorofrelease0.0,anindustrynewsletterthatspotsnewdevelopmentsmichaelslater,editorofthemicroprocessorreport,anindustrynewsletterbrucemiller,presidentofartfundingcorp.,anartlenderjeffreynichols,presidentofapmscanada,torontopreciousmetalsadvisers,elililly&co.,indianapolis,johnkinnard&co.,minneapolis,procter&gambleco.,cincinnati,anbinvestmentmanagementco.,chicago,scimedlifesystemsinc.,minneapolis,rjrnabiscoinc.’sfrenchcrackersubsidiary,belin,mr.engelken’ssister,martha,whowasborntwodaysbeforethehomerun,thecompany’spresident,n.j.nicholas,whowilleventuallybeco-chiefexecutiveoftimewarneralongsidemr.ross,claudio’ssister,isabella,anovitiateinaconvent,herdaughter,elizabeth,anattorneywhoisvicechairman,hisbrother,parkhaji,whoseheadisswathedinagorgeouscrimsonturban,mrs.coleman’shusband,joseph,aphysician,chairmanandchiefexecutiveofﬁcerpresidentandchiefexecutiveofﬁcerpresidentandchiefoperatingofﬁcerchairmanandchiefexecutiveexecutivevicepresidentandchiefﬁnancialofﬁcerexecutivevicepresidentandchiefoperatingofﬁcer10PublishedasaconferencepaperatICLR2015Chen,D.andManning,C.D.Afastandaccuratedependencyparserusingneuralnetworks.InProceedingsofEMNLP,2014.Chomsky,N.Threemodelsforthedescriptionoflanguage.IRETransactionsonInformationTheory,1956.Collins,M.Head-drivenstatisticalmodelsfornaturallanguageparsing.Comput.Linguist.,2003.Collobert,R.Deeplearningforefﬁcientdiscriminativeparsing.InAISTATS,2011.Collobert,R.andWeston,J.Auniﬁedarchitecturefornaturallanguageprocessing:Deepneuralnetworkswithmultitasklearning.InInternationalConferenceonMachineLearning,ICML,2008.Costa,F.,Frasconi,P.,Lombardo,V.,andSoda,G.Towardsincrementalparsingofnaturallanguageusingrecursiveneuralnetworks,2002.Denker,J.S.andBurges,C.J.C.Imagesegmentationandrecognition.InInTheMathematicsofInduction,1995.Earley,J.Anefﬁcientcontext-freeparsingalgorithm.1970.Elman,J.L.Distributedrepresentations,simplerecurrentnetworks,andgrammaticalstructure.Mach.Learn.,1991.Finkel,J.R.,Kleeman,A.,andManning,C.D.Efﬁcient,feature-based,conditionalrandomﬁeldparsing.InInProc.ACL/HLT,2008.Henderson,J.Inducinghistoryrepresentationsforbroadcoveragestatisticalparsing.InProceedingsofthe2003ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguisticsonHumanLanguageTechnology-Volume1,2003.Henderson,J.Discriminativetrainingofaneuralnetworkstatisticalparser.InProceedingsofthe42NdAnnualMeetingonAssociationforComputationalLinguistics,2004.Hinton,GE.,Srivastava,N.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors.CoRR,2012.Kasami,T.Anefﬁcientrecognitionandsyntaxanalysisalgorithmforcontext-freelanguages.Tech-nicalreport,1965.Kay,M.Readingsinnaturallanguageprocessing.chapterAlgorithmSchemataandDataStructuresinSyntacticProcessing.1986.Kummerfeld,J.K.,Hall,D.,Curran,J.R.,andKlein,D.Parsershowdownatthewallstreetcorral:Anempiricalinvestigationoferrortypesinparseroutput.InProceedingsofthe2012JointConferenceonEmpiricalMethodsinNaturalLanguageProcessingandComputationalNaturalLanguageLearning,2012.Lafferty,J.,McCallum,A.,andPereira,F.Conditionalrandomﬁelds:Probabilisticmodelsforsegmentingandlabelingsequencedata.InEighteenthInternationalConferenceonMachineLearning,ICML,2001.Lebret,R.andCollobert,R.WordembeddingsthroughhellingerPCA.InProceedingsofthe14thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics,2014.Legrand,J.andCollobert,R.Recurrentgreedyparsingwithneuralnetworks.InEuropeanConfer-enceonMachineLearning,2014.Magerman,D.M.Statisticaldecision-treemodelsforparsing.InInProceedingsofthe33rdAnnualMeetingoftheAssociationforComputationalLinguistics,1995.Marcus,M.P.,Santorini,B.,andMarcinkiewicz,M.A.Buildingalargeannotatedcorpusofenglish:Thepenntreebank.ComputationalLinguistics,1993.11",
1412.6980,2015,Adam: A Method for Stochastic Optimization,"['Adam: A Method for Stochastic Optimization', 'Jimmy Ba and Diederik Kingma']",https://arxiv.org/pdf/1412.6980,"7 1 0 2     n a J    0 3      ]  G L . s c [      9 v 0 8 9 6  .  2 1 4 1 : v i X r a  PublishedasaconferencepaperatICLR2015ADAM:AMETHODFORSTOCHASTICOPTIMIZATIONDiederikP.Kingma*UniversityofAmsterdam,OpenAIdpkingma@openai.comJimmyLeiBa∗UniversityofTorontojimmy@psi.utoronto.caABSTRACTWeintroduceAdam,analgorithmforﬁrst-ordergradient-basedoptimizationofstochasticobjectivefunctions,basedonadaptiveestimatesoflower-ordermo-ments.Themethodisstraightforwardtoimplement,iscomputationallyefﬁcient,haslittlememoryrequirements,isinvarianttodiagonalrescalingofthegradients,andiswellsuitedforproblemsthatarelargeintermsofdataand/orparameters.Themethodisalsoappropriatefornon-stationaryobjectivesandproblemswithverynoisyand/orsparsegradients.Thehyper-parametershaveintuitiveinterpre-tationsandtypicallyrequirelittletuning.Someconnectionstorelatedalgorithms,onwhichAdamwasinspired,arediscussed.Wealsoanalyzethetheoreticalcon-vergencepropertiesofthealgorithmandprovidearegretboundontheconver-genceratethatiscomparabletothebestknownresultsundertheonlineconvexoptimizationframework.EmpiricalresultsdemonstratethatAdamworkswellinpracticeandcomparesfavorablytootherstochasticoptimizationmethods.Finally,wediscussAdaMax,avariantofAdambasedontheinﬁnitynorm.1INTRODUCTIONStochasticgradient-basedoptimizationisofcorepracticalimportanceinmanyﬁeldsofscienceandengineering.Manyproblemsintheseﬁeldscanbecastastheoptimizationofsomescalarparameter-izedobjectivefunctionrequiringmaximizationorminimizationwithrespecttoitsparameters.Ifthefunctionisdifferentiablew.r.t.itsparameters,gradientdescentisarelativelyefﬁcientoptimizationmethod,sincethecomputationofﬁrst-orderpartialderivativesw.r.t.alltheparametersisofthesamecomputationalcomplexityasjustevaluatingthefunction.Often,objectivefunctionsarestochastic.Forexample,manyobjectivefunctionsarecomposedofasumofsubfunctionsevaluatedatdifferentsubsamplesofdata;inthiscaseoptimizationcanbemademoreefﬁcientbytakinggradientstepsw.r.t.individualsubfunctions,i.e.stochasticgradientdescent(SGD)orascent.SGDproveditselfasanefﬁcientandeffectiveoptimizationmethodthatwascentralinmanymachinelearningsuccessstories,suchasrecentadvancesindeeplearning(Dengetal.,2013;Krizhevskyetal.,2012;Hinton&Salakhutdinov,2006;Hintonetal.,2012a;Gravesetal.,2013).Objectivesmayalsohaveothersourcesofnoisethandatasubsampling,suchasdropout(Hintonetal.,2012b)regularization.Forallsuchnoisyobjectives,efﬁcientstochasticoptimizationtechniquesarerequired.Thefocusofthispaperisontheoptimizationofstochasticobjectiveswithhigh-dimensionalparametersspaces.Inthesecases,higher-orderoptimizationmethodsareill-suited,anddiscussioninthispaperwillberestrictedtoﬁrst-ordermethods.WeproposeAdam,amethodforefﬁcientstochasticoptimizationthatonlyrequiresﬁrst-ordergra-dientswithlittlememoryrequirement.Themethodcomputesindividualadaptivelearningratesfordifferentparametersfromestimatesofﬁrstandsecondmomentsofthegradients;thenameAdamisderivedfromadaptivemomentestimation.Ourmethodisdesignedtocombinetheadvantagesoftworecentlypopularmethods:AdaGrad(Duchietal.,2011),whichworkswellwithsparsegra-dients,andRMSProp(Tieleman&Hinton,2012),whichworkswellinon-lineandnon-stationarysettings;importantconnectionstotheseandotherstochasticoptimizationmethodsareclariﬁedinsection5.SomeofAdam’sadvantagesarethatthemagnitudesofparameterupdatesareinvarianttorescalingofthegradient,itsstepsizesareapproximatelyboundedbythestepsizehyperparameter,itdoesnotrequireastationaryobjective,itworkswithsparsegradients,anditnaturallyperformsaformofstepsizeannealing.∗Equalcontribution.AuthororderingdeterminedbycoinﬂipoveraGoogleHangout.1PublishedasaconferencepaperatICLR2015Algorithm1:Adam,ourproposedalgorithmforstochasticoptimization.Seesection2fordetails,andforaslightlymoreefﬁcient(butlessclear)orderofcomputation.g2tindicatestheelementwisesquaregt(cid:12)gt.Gooddefaultsettingsforthetestedmachinelearningproblemsareα=0.001,β1=0.9,β2=0.999and(cid:15)=10−8.Alloperationsonvectorsareelement-wise.Withβt1andβt2wedenoteβ1andβ2tothepowert.Require:α:StepsizeRequire:β1,β2∈[0,1):ExponentialdecayratesforthemomentestimatesRequire:f(θ):StochasticobjectivefunctionwithparametersθRequire:θ0:Initialparametervectorm0←0(Initialize1stmomentvector)v0←0(Initialize2ndmomentvector)t←0(Initializetimestep)whileθtnotconvergeddot←t+1gt←∇θft(θt−1)(Getgradientsw.r.t.stochasticobjectiveattimestept)mt←β1·mt−1+(1−β1)·gt(Updatebiasedﬁrstmomentestimate)vt←β2·vt−1+(1−β2)·g2t(Updatebiasedsecondrawmomentestimate)bmt←mt/(1−βt1)(Computebias-correctedﬁrstmomentestimate)bvt←vt/(1−βt2)(Computebias-correctedsecondrawmomentestimate)θt←θt−1−α·bmt/(√bvt+(cid:15))(Updateparameters)endwhilereturnθt(Resultingparameters)Insection2wedescribethealgorithmandthepropertiesofitsupdaterule.Section3explainsourinitializationbiascorrectiontechnique,andsection4providesatheoreticalanalysisofAdam’sconvergenceinonlineconvexprogramming.Empirically,ourmethodconsistentlyoutperformsothermethodsforavarietyofmodelsanddatasets,asshowninsection6.Overall,weshowthatAdamisaversatilealgorithmthatscalestolarge-scalehigh-dimensionalmachinelearningproblems.2ALGORITHMSeealgorithm1forpseudo-codeofourproposedalgorithmAdam.Letf(θ)beanoisyobjec-tivefunction:astochasticscalarfunctionthatisdifferentiablew.r.t.parametersθ.Wearein-terestedinminimizingtheexpectedvalueofthisfunction,E[f(θ)]w.r.t.itsparametersθ.Withf1(θ),...,,fT(θ)wedenotetherealisationsofthestochasticfunctionatsubsequenttimesteps1,...,T.Thestochasticitymightcomefromtheevaluationatrandomsubsamples(minibatches)ofdatapoints,orarisefrominherentfunctionnoise.Withgt=∇θft(θ)wedenotethegradient,i.e.thevectorofpartialderivativesofft,w.r.tθevaluatedattimestept.Thealgorithmupdatesexponentialmovingaveragesofthegradient(mt)andthesquaredgradient(vt)wherethehyper-parametersβ1,β2∈[0,1)controltheexponentialdecayratesofthesemovingaverages.Themovingaveragesthemselvesareestimatesofthe1stmoment(themean)andthe2ndrawmoment(theuncenteredvariance)ofthegradient.However,thesemovingaveragesareinitializedas(vectorsof)0’s,leadingtomomentestimatesthatarebiasedtowardszero,especiallyduringtheinitialtimesteps,andespeciallywhenthedecayratesaresmall(i.e.theβsarecloseto1).Thegoodnewsisthatthisinitializationbiascanbeeasilycounteracted,resultinginbias-correctedestimatesbmtandbvt.Seesection3formoredetails.Notethattheefﬁciencyofalgorithm1can,attheexpenseofclarity,beimproveduponbychangingtheorderofcomputation,e.g.byreplacingthelastthreelinesintheloopwiththefollowinglines:αt=α·p1−βt2/(1−βt1)andθt←θt−1−αt·mt/(√vt+ˆ(cid:15)).2.1ADAM’SUPDATERULEAnimportantpropertyofAdam’supdateruleisitscarefulchoiceofstepsizes.Assuming(cid:15)=0,theeffectivesteptakeninparameterspaceattimesteptis∆t=α·bmt/√bvt.Theeffectivestepsizehastwoupperbounds:|∆t|≤α·(1−β1)/√1−β2inthecase(1−β1)>√1−β2,and|∆t|≤α2PublishedasaconferencepaperatICLR2015otherwise.Theﬁrstcaseonlyhappensinthemostseverecaseofsparsity:whenagradienthasbeenzeroatalltimestepsexceptatthecurrenttimestep.Forlesssparsecases,theeffectivestepsizewillbesmaller.When(1−β1)=√1−β2wehavethat|bmt/√bvt|<1therefore|∆t|<α.Inmorecommonscenarios,wewillhavethatbmt/√bvt≈±1since|E[g]/pE[g2]|≤1.Theeffectivemagnitudeofthestepstakeninparameterspaceateachtimestepareapproximatelyboundedbythestepsizesettingα,i.e.,|∆t|/α.Thiscanbeunderstoodasestablishingatrustregionaroundthecurrentparametervalue,beyondwhichthecurrentgradientestimatedoesnotprovidesufﬁcientinformation.Thistypicallymakesitrelativelyeasytoknowtherightscaleofαinadvance.Formanymachinelearningmodels,forinstance,weoftenknowinadvancethatgoodoptimaarewithhighprobabilitywithinsomesetregioninparameterspace;itisnotuncommon,forexample,tohaveapriordistributionovertheparameters.Sinceαsets(anupperboundof)themagnitudeofstepsinparameterspace,wecanoftendeducetherightorderofmagnitudeofαsuchthatoptimacanbereachedfromθ0withinsomenumberofiterations.Withaslightabuseofterminology,wewillcalltheratiobmt/√bvtthesignal-to-noiseratio(SNR).WithasmallerSNRtheeffectivestepsize∆twillbeclosertozero.Thisisadesirableproperty,sinceasmallerSNRmeansthatthereisgreateruncertaintyaboutwhetherthedirectionofbmtcorrespondstothedirectionofthetruegradient.Forexample,theSNRvaluetypicallybecomescloserto0towardsanoptimum,leadingtosmallereffectivestepsinparameterspace:aformofautomaticannealing.Theeffectivestepsize∆tisalsoinvarianttothescaleofthegradients;rescalingthegradientsgwithfactorcwillscalebmtwithafactorcandbvtwithafactorc2,whichcancelout:(c·bmt)/(√c2·bvt)=bmt/√bvt.3INITIALIZATIONBIASCORRECTIONAsexplainedinsection2,Adamutilizesinitializationbiascorrectionterms.Wewillherederivethetermforthesecondmomentestimate;thederivationfortheﬁrstmomentestimateiscompletelyanalogous.Letgbethegradientofthestochasticobjectivef,andwewishtoestimateitssecondrawmoment(uncenteredvariance)usinganexponentialmovingaverageofthesquaredgradient,withdecayrateβ2.Letg1,...,gTbethegradientsatsubsequenttimesteps,eachadrawfromanunderlyinggradientdistributiongt∼p(gt).Letusinitializetheexponentialmovingaverageasv0=0(avectorofzeros).Firstnotethattheupdateattimesteptoftheexponentialmovingaveragevt=β2·vt−1+(1−β2)·g2t(whereg2tindicatestheelementwisesquaregt(cid:12)gt)canbewrittenasafunctionofthegradientsatallprevioustimesteps:vt=(1−β2)tXi=1βt−i2·g2i(1)WewishtoknowhowE[vt],theexpectedvalueoftheexponentialmovingaverageattimestept,relatestothetruesecondmomentE[g2t],sowecancorrectforthediscrepancybetweenthetwo.Takingexpectationsoftheleft-handandright-handsidesofeq.(1):E[vt]=E""(1−β2)tXi=1βt−i2·g2i#(2)=E[g2t]·(1−β2)tXi=1βt−i2+ζ(3)=E[g2t]·(1−βt2)+ζ(4)whereζ=0ifthetruesecondmomentE[g2i]isstationary;otherwiseζcanbekeptsmallsincetheexponentialdecayrateβ1can(andshould)bechosensuchthattheexponentialmovingaverageassignssmallweightstogradientstoofarinthepast.Whatisleftistheterm(1−βt2)whichiscausedbyinitializingtherunningaveragewithzeros.Inalgorithm1wethereforedividebythistermtocorrecttheinitializationbias.Incaseofsparsegradients,forareliableestimateofthesecondmomentoneneedstoaverageovermanygradientsbychosingasmallvalueofβ2;howeveritisexactlythiscaseofsmallβ2wherealackofinitialisationbiascorrectionwouldleadtoinitialstepsthataremuchlarger.3PublishedasaconferencepaperatICLR20154CONVERGENCEANALYSISWeanalyzetheconvergenceofAdamusingtheonlinelearningframeworkproposedin(Zinkevich,2003).Givenanarbitrary,unknownsequenceofconvexcostfunctionsf1(θ),f2(θ),...,fT(θ).Ateachtimet,ourgoalistopredicttheparameterθtandevaluateitonapreviouslyunknowncostfunctionft.Sincethenatureofthesequenceisunknowninadvance,weevaluateouralgorithmusingtheregret,thatisthesumofallthepreviousdifferencebetweentheonlinepredictionft(θt)andthebestﬁxedpointparameterft(θ∗)fromafeasiblesetXforalltheprevioussteps.Concretely,theregretisdeﬁnedas:R(T)=TXt=1[ft(θt)−ft(θ∗)](5)whereθ∗=argminθ∈XPTt=1ft(θ).WeshowAdamhasO(√T)regretboundandaproofisgivenintheappendix.Ourresultiscomparabletothebestknownboundforthisgeneralconvexonlinelearningproblem.Wealsousesomedeﬁnitionssimplifyournotation,wheregt,∇ft(θt)andgt,iastheithelement.Wedeﬁneg1:t,i∈Rtasavectorthatcontainstheithdimensionofthegradientsoveralliterationstillt,g1:t,i=[g1,i,g2,i,···,gt,i].Also,wedeﬁneγ,β21√β2.Ourfollowingtheoremholdswhenthelearningrateαtisdecayingatarateoft−12andﬁrstmomentrunningaveragecoefﬁcientβ1,tdecayexponentiallywithλ,thatistypicallycloseto1,e.g.1−10−8.Theorem4.1.Assumethatthefunctionfthasboundedgradients,k∇ft(θ)k2≤G,k∇ft(θ)k∞≤G∞forallθ∈RdanddistancebetweenanyθtgeneratedbyAdamisbounded,kθn−θmk2≤D,kθm−θnk∞≤D∞foranym,n∈{1,...,T},andβ1,β2∈[0,1)satisfyβ21√β2<1.Letαt=α√tandβ1,t=β1λt−1,λ∈(0,1).Adamachievesthefollowingguarantee,forallT≥1.R(T)≤D22α(1−β1)dXi=1pTbvT,i+α(1+β1)G∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+dXi=1D2∞G∞√1−β22α(1−β1)(1−λ)2OurTheorem4.1implieswhenthedatafeaturesaresparseandboundedgradients,thesum-mationtermcanbemuchsmallerthanitsupperboundPdi=1kg1:T,ik2<<dG∞√TandPdi=1pTbvT,i<<dG∞√T,inparticulariftheclassoffunctionanddatafeaturesareintheformofsection1.2in(Duchietal.,2011).TheirresultsfortheexpectedvalueE[Pdi=1kg1:T,ik2]alsoapplytoAdam.Inparticular,theadaptivemethod,suchasAdamandAdagrad,canachieveO(logd√T),animprovementoverO(√dT)forthenon-adaptivemethod.Decayingβ1,ttowardszeroisimpor-tantinourtheoreticalanalysisandalsomatchespreviousempiricalﬁndings,e.g.(Sutskeveretal.,2013)suggestsreducingthemomentumcoefﬁcientintheendoftrainingcanimproveconvergence.Finally,wecanshowtheaverageregretofAdamconverges,Corollary4.2.Assumethatthefunctionfthasboundedgradients,k∇ft(θ)k2≤G,k∇ft(θ)k∞≤G∞forallθ∈RdanddistancebetweenanyθtgeneratedbyAdamisbounded,kθn−θmk2≤D,kθm−θnk∞≤D∞foranym,n∈{1,...,T}.Adamachievesthefollowingguarantee,forallT≥1.R(T)T=O(1√T)ThisresultcanbeobtainedbyusingTheorem4.1andPdi=1kg1:T,ik2≤dG∞√T.Thus,limT→∞R(T)T=0.5RELATEDWORKOptimizationmethodsbearingadirectrelationtoAdamareRMSProp(Tieleman&Hinton,2012;Graves,2013)andAdaGrad(Duchietal.,2011);theserelationshipsarediscussedbelow.OtherstochasticoptimizationmethodsincludevSGD(Schauletal.,2012),AdaDelta(Zeiler,2012)andthenaturalNewtonmethodfromRoux&Fitzgibbon(2010),allsettingstepsizesbyestimatingcurvature4PublishedasaconferencepaperatICLR2015fromﬁrst-orderinformation.TheSum-of-FunctionsOptimizer(SFO)(Sohl-Dicksteinetal.,2014)isaquasi-Newtonmethodbasedonminibatches,but(unlikeAdam)hasmemoryrequirementslinearinthenumberofminibatchpartitionsofadataset,whichisofteninfeasibleonmemory-constrainedsystemssuchasaGPU.Likenaturalgradientdescent(NGD)(Amari,1998),Adamemploysapreconditionerthatadaptstothegeometryofthedata,sincebvtisanapproximationtothediagonaloftheFisherinformationmatrix(Pascanu&Bengio,2013);however,Adam’spreconditioner(likeAdaGrad’s)ismoreconservativeinitsadaptionthanvanillaNGDbypreconditioningwiththesquarerootoftheinverseofthediagonalFisherinformationmatrixapproximation.RMSProp:AnoptimizationmethodcloselyrelatedtoAdamisRMSProp(Tieleman&Hinton,2012).Aversionwithmomentumhassometimesbeenused(Graves,2013).Thereareafewimpor-tantdifferencesbetweenRMSPropwithmomentumandAdam:RMSPropwithmomentumgener-atesitsparameterupdatesusingamomentumontherescaledgradient,whereasAdamupdatesaredirectlyestimatedusingarunningaverageofﬁrstandsecondmomentofthegradient.RMSPropalsolacksabias-correctionterm;thismattersmostincaseofavalueofβ2closeto1(requiredincaseofsparsegradients),sinceinthatcasenotcorrectingthebiasleadstoverylargestepsizesandoftendivergence,aswealsoempiricallydemonstrateinsection6.4.AdaGrad:AnalgorithmthatworkswellforsparsegradientsisAdaGrad(Duchietal.,2011).Itsbasicversionupdatesparametersasθt+1=θt−α·gt/qPti=1g2t.Notethatifwechooseβ2tobeinﬁnitesimallycloseto1frombelow,thenlimβ2→1bvt=t−1·Pti=1g2t.AdaGradcorrespondstoaversionofAdamwithβ1=0,inﬁnitesimal(1−β2)andareplacementofαbyanannealedversionαt=α·t−1/2,namelyθt−α·t−1/2·bmt/plimβ2→1bvt=θt−α·t−1/2·gt/qt−1·Pti=1g2t=θt−α·gt/qPti=1g2t.NotethatthisdirectcorrespondencebetweenAdamandAdagraddoesnotholdwhenremovingthebias-correctionterms;withoutbiascorrection,likeinRMSProp,aβ2inﬁnitesimallycloseto1wouldleadtoinﬁnitelylargebias,andinﬁnitelylargeparameterupdates.6EXPERIMENTSToempiricallyevaluatetheproposedmethod,weinvestigateddifferentpopularmachinelearningmodels,includinglogisticregression,multilayerfullyconnectedneuralnetworksanddeepconvolu-tionalneuralnetworks.Usinglargemodelsanddatasets,wedemonstrateAdamcanefﬁcientlysolvepracticaldeeplearningproblems.Weusethesameparameterinitializationwhencomparingdifferentoptimizationalgorithms.Thehyper-parameters,suchaslearningrateandmomentum,aresearchedoveradensegridandtheresultsarereportedusingthebesthyper-parametersetting.6.1EXPERIMENT:LOGISTICREGRESSIONWeevaluateourproposedmethodonL2-regularizedmulti-classlogisticregressionusingtheMNISTdataset.Logisticregressionhasawell-studiedconvexobjective,makingitsuitableforcomparisonofdifferentoptimizerswithoutworryingaboutlocalminimumissues.Thestepsizeαinourlogisticregressionexperimentsisadjustedby1/√tdecay,namelyαt=α√tthatmatcheswithourtheorat-icalpredictionfromsection4.Thelogisticregressionclassiﬁestheclasslabeldirectlyonthe784dimensionimagevectors.WecompareAdamtoacceleratedSGDwithNesterovmomentumandAdagradusingminibatchsizeof128.AccordingtoFigure1,wefoundthattheAdamyieldssimilarconvergenceasSGDwithmomentumandbothconvergefasterthanAdagrad.Asdiscussedin(Duchietal.,2011),Adagradcanefﬁcientlydealwithsparsefeaturesandgradi-entsasoneofitsmaintheoreticalresultswhereasSGDislowatlearningrarefeatures.Adamwith1/√tdecayonitsstepsizeshouldtheoraticallymatchtheperformanceofAdagrad.WeexaminethesparsefeatureproblemusingIMDBmoviereviewdatasetfrom(Maasetal.,2011).Wepre-processtheIMDBmoviereviewsintobag-of-words(BoW)featurevectorsincludingtheﬁrst10,000mostfrequentwords.The10,000dimensionBoWfeaturevectorforeachreviewishighlysparse.Assug-gestedin(Wang&Manning,2013),50%dropoutnoisecanbeappliedtotheBoWfeaturesduring5PublishedasaconferencepaperatICLR2015051015202530354045iterations over entire dataset0.20.30.40.50.60.7training costMNIST Logistic RegressionAdaGradSGDNesterovAdam020406080100120140160iterations over entire dataset0.200.250.300.350.400.450.50training costIMDB BoW feature Logistic RegressionAdagrad+dropoutRMSProp+dropoutSGDNesterov+dropoutAdam+dropoutFigure1:LogisticregressiontrainingnegativeloglikelihoodonMNISTimagesandIMDBmoviereviewswith10,000bag-of-words(BoW)featurevectors.trainingtopreventover-ﬁtting.Inﬁgure1,AdagradoutperformsSGDwithNesterovmomentumbyalargemarginbothwithandwithoutdropoutnoise.AdamconvergesasfastasAdagrad.TheempiricalperformanceofAdamisconsistentwithourtheoreticalﬁndingsinsections2and4.Sim-ilartoAdagrad,AdamcantakeadvantageofsparsefeaturesandobtainfasterconvergenceratethannormalSGDwithmomentum.6.2EXPERIMENT:MULTI-LAYERNEURALNETWORKSMulti-layerneuralnetworkarepowerfulmodelswithnon-convexobjectivefunctions.Althoughourconvergenceanalysisdoesnotapplytonon-convexproblems,weempiricallyfoundthatAdamoftenoutperformsothermethodsinsuchcases.Inourexperiments,wemademodelchoicesthatareconsistentwithpreviouspublicationsinthearea;aneuralnetworkmodelwithtwofullyconnectedhiddenlayerswith1000hiddenunitseachandReLUactivationareusedforthisexperimentwithminibatchsizeof128.First,westudydifferentoptimizersusingthestandarddeterministiccross-entropyobjectivefunc-tionwithL2weightdecayontheparameterstopreventover-ﬁtting.Thesum-of-functions(SFO)method(Sohl-Dicksteinetal.,2014)isarecentlyproposedquasi-Newtonmethodthatworkswithminibatchesofdataandhasshowngoodperformanceonoptimizationofmulti-layerneuralnet-works.WeusedtheirimplementationandcomparedwithAdamtotrainsuchmodels.Figure2showsthatAdammakesfasterprogressintermsofboththenumberofiterationsandwall-clocktime.Duetothecostofupdatingcurvatureinformation,SFOis5-10xslowerperiterationcom-paredtoAdam,andhasamemoryrequirementthatislinearinthenumberminibatches.Stochasticregularizationmethods,suchasdropout,areaneffectivewaytopreventover-ﬁttingandoftenusedinpracticeduetotheirsimplicity.SFOassumesdeterministicsubfunctions,andindeedfailedtoconvergeoncostfunctionswithstochasticregularization.WecomparetheeffectivenessofAdamtootherstochasticﬁrstordermethodsonmulti-layerneuralnetworkstrainedwithdropoutnoise.Figure2showsourresults;Adamshowsbetterconvergencethanothermethods.6.3EXPERIMENT:CONVOLUTIONALNEURALNETWORKSConvolutionalneuralnetworks(CNNs)withseverallayersofconvolution,poolingandnon-linearunitshaveshownconsiderablesuccessincomputervisiontasks.Unlikemostfullyconnectedneuralnets,weightsharinginCNNsresultsinvastlydifferentgradientsindifferentlayers.AsmallerlearningratefortheconvolutionlayersisoftenusedinpracticewhenapplyingSGD.WeshowtheeffectivenessofAdamindeepCNNs.OurCNNarchitecturehasthreealternatingstagesof5x5convolutionﬁltersand3x3maxpoolingwithstrideof2thatarefollowedbyafullyconnectedlayerof1000rectiﬁedlinearhiddenunits(ReLU’s).Theinputimagearepre-processedbywhitening,and6PublishedasaconferencepaperatICLR2015050100150200iterations over entire dataset10-210-1training costMNIST Multilayer Neural Network + dropoutAdaGradRMSPropSGDNesterovAdaDeltaAdam(a)(b)Figure2:TrainingofmultilayerneuralnetworksonMNISTimages.(a)Neuralnetworksusingdropoutstochasticregularization.(b)Neuralnetworkswithdeterministiccostfunction.Wecomparewiththesum-of-functions(SFO)optimizer(Sohl-Dicksteinetal.,2014)0.00.51.01.52.02.53.0iterations over entire dataset0.51.01.52.02.53.0training costCIFAR10 ConvNet First 3 EpochesAdaGradAdaGrad+dropoutSGDNesterovSGDNesterov+dropoutAdamAdam+dropout051015202530354045iterations over entire dataset10-410-310-210-1100101102training costCIFAR10 ConvNetAdaGradAdaGrad+dropoutSGDNesterovSGDNesterov+dropoutAdamAdam+dropoutFigure3:Convolutionalneuralnetworkstrainingcost.(left)Trainingcostfortheﬁrstthreeepochs.(right)Trainingcostover45epochs.CIFAR-10withc64-c64-c128-1000architecture.dropoutnoiseisappliedtotheinputlayerandfullyconnectedlayer.Theminibatchsizeisalsosetto128similartopreviousexperiments.Interestingly,althoughbothAdamandAdagradmakerapidprogressloweringthecostintheinitialstageofthetraining,showninFigure3(left),AdamandSGDeventuallyconvergeconsiderablyfasterthanAdagradforCNNsshowninFigure3(right).Wenoticethesecondmomentestimatebvtvanishestozerosafterafewepochsandisdominatedbythe(cid:15)inalgorithm1.ThesecondmomentestimateisthereforeapoorapproximationtothegeometryofthecostfunctioninCNNscomparingtofullyconnectednetworkfromSection6.2.Whereas,reducingtheminibatchvariancethroughtheﬁrstmomentismoreimportantinCNNsandcontributestothespeed-up.Asaresult,Adagradconvergesmuchslowerthanothersinthisparticularexperiment.ThoughAdamshowsmarginalimprovementoverSGDwithmomentum,itadaptslearningratescalefordifferentlayersinsteadofhandpickingmanuallyasinSGD.7PublishedasaconferencepaperatICLR2015β1=0β1=0.9β2=0.99β2=0.999β2=0.9999β2=0.99β2=0.999β2=0.9999(a) after 10 epochs(b) after 100 epochslog10(α)LossFigure4:Effectofbias-correctionterms(redline)versusnobiascorrectionterms(greenline)after10epochs(left)and100epochs(right)ontheloss(y-axes)whenlearningaVariationalAuto-Encoder(VAE)(Kingma&Welling,2013),fordifferentsettingsofstepsizeα(x-axes)andhyper-parametersβ1andβ2.6.4EXPERIMENT:BIAS-CORRECTIONTERMWealsoempiricallyevaluatetheeffectofthebiascorrectiontermsexplainedinsections2and3.Discussedinsection5,removalofthebiascorrectiontermsresultsinaversionofRMSProp(Tiele-man&Hinton,2012)withmomentum.Wevarytheβ1andβ2whentrainingavariationalauto-encoder(VAE)withthesamearchitectureasin(Kingma&Welling,2013)withasinglehiddenlayerwith500hiddenunitswithsoftplusnonlinearitiesanda50-dimensionalsphericalGaussianlatentvariable.Weiteratedoverabroadrangeofhyper-parameterchoices,i.e.β1∈[0,0.9]andβ2∈[0.99,0.999,0.9999],andlog10(α)∈[−5,...,−1].Valuesofβ2closeto1,requiredforrobust-nesstosparsegradients,resultsinlargerinitializationbias;thereforeweexpectthebiascorrectiontermisimportantinsuchcasesofslowdecay,preventinganadverseeffectonoptimization.InFigure4,valuesβ2closeto1indeedleadtoinstabilitiesintrainingwhennobiascorrectiontermwaspresent,especiallyatﬁrstfewepochsofthetraining.Thebestresultswereachievedwithsmallvaluesof(1−β2)andbiascorrection;thiswasmoreapparenttowardstheendofoptimizationwhengradientstendstobecomesparserashiddenunitsspecializetospeciﬁcpatterns.Insummary,AdamperformedequalorbetterthanRMSProp,regardlessofhyper-parametersetting.7EXTENSIONS7.1ADAMAXInAdam,theupdateruleforindividualweightsistoscaletheirgradientsinverselyproportionaltoa(scaled)L2normoftheirindividualcurrentandpastgradients.WecangeneralizetheL2normbasedupdateruletoaLpnormbasedupdaterule.Suchvariantsbecomenumericallyunstableforlargep.However,inthespecialcasewhereweletp→∞,asurprisinglysimpleandstablealgorithmemerges;seealgorithm2.We’llnowderivethealgorithm.Let,incaseoftheLpnorm,thestepsizeattimetbeinverselyproportionaltov1/pt,where:vt=βp2vt−1+(1−βp2)|gt|p(6)=(1−βp2)tXi=1βp(t−i)2·|gi|p(7)8PublishedasaconferencepaperatICLR2015Algorithm2:AdaMax,avariantofAdambasedontheinﬁnitynorm.Seesection7.1fordetails.Gooddefaultsettingsforthetestedmachinelearningproblemsareα=0.002,β1=0.9andβ2=0.999.Withβt1wedenoteβ1tothepowert.Here,(α/(1−βt1))isthelearningratewiththebias-correctiontermfortheﬁrstmoment.Alloperationsonvectorsareelement-wise.Require:α:StepsizeRequire:β1,β2∈[0,1):ExponentialdecayratesRequire:f(θ):StochasticobjectivefunctionwithparametersθRequire:θ0:Initialparametervectorm0←0(Initialize1stmomentvector)u0←0(Initializetheexponentiallyweightedinﬁnitynorm)t←0(Initializetimestep)whileθtnotconvergeddot←t+1gt←∇θft(θt−1)(Getgradientsw.r.t.stochasticobjectiveattimestept)mt←β1·mt−1+(1−β1)·gt(Updatebiasedﬁrstmomentestimate)ut←max(β2·ut−1,|gt|)(Updatetheexponentiallyweightedinﬁnitynorm)θt←θt−1−(α/(1−βt1))·mt/ut(Updateparameters)endwhilereturnθt(Resultingparameters)Notethatthedecaytermishereequivalentlyparameterisedasβp2insteadofβ2.Nowletp→∞,anddeﬁneut=limp→∞(vt)1/p,then:ut=limp→∞(vt)1/p=limp→∞ (1−βp2)tXi=1βp(t−i)2·|gi|p!1/p(8)=limp→∞(1−βp2)1/p tXi=1βp(t−i)2·|gi|p!1/p(9)=limp→∞ tXi=1(cid:16)β(t−i)2·|gi|(cid:17)p!1/p(10)=max(cid:0)βt−12|g1|,βt−22|g2|,...,β2|gt−1|,|gt|(cid:1)(11)Whichcorrespondstotheremarkablysimplerecursiveformula:ut=max(β2·ut−1,|gt|)(12)withinitialvalueu0=0.Notethat,convenientlyenough,wedon’tneedtocorrectforinitializationbiasinthiscase.AlsonotethatthemagnitudeofparameterupdateshasasimplerboundwithAdaMaxthanAdam,namely:|∆t|≤α.7.2TEMPORALAVERAGINGSincethelastiterateisnoisyduetostochasticapproximation,bettergeneralizationperformanceisoftenachievedbyaveraging.PreviouslyinMoulines&Bach(2011),Polyak-Ruppertaveraging(Polyak&Juditsky,1992;Ruppert,1988)hasbeenshowntoimprovetheconvergenceofstandardSGD,where¯θt=1tPnk=1θk.Alternatively,anexponentialmovingaverageovertheparameterscanbeused,givinghigherweighttomorerecentparametervalues.Thiscanbetriviallyimplementedbyaddingonelinetotheinnerloopofalgorithms1and2:¯θt←β2·¯θt−1+(1−β2)θt,with¯θ0=0.Initalizationbiascanagainbecorrectedbytheestimatorbθt=¯θt/(1−βt2).8CONCLUSIONWehaveintroducedasimpleandcomputationallyefﬁcientalgorithmforgradient-basedoptimiza-tionofstochasticobjectivefunctions.Ourmethodisaimedtowardsmachinelearningproblemswith9PublishedasaconferencepaperatICLR2015largedatasetsand/orhigh-dimensionalparameterspaces.Themethodcombinestheadvantagesoftworecentlypopularoptimizationmethods:theabilityofAdaGradtodealwithsparsegradients,andtheabilityofRMSProptodealwithnon-stationaryobjectives.Themethodisstraightforwardtoimplementandrequireslittlememory.Theexperimentsconﬁrmtheanalysisontherateofcon-vergenceinconvexproblems.Overall,wefoundAdamtoberobustandwell-suitedtoawiderangeofnon-convexoptimizationproblemsintheﬁeldmachinelearning.9ACKNOWLEDGMENTSThispaperwouldprobablynothaveexistedwithoutthesupportofGoogleDeepmind.WewouldliketogivespecialthankstoIvoDanihelka,andTomSchaulforcoiningthenameAdam.ThankstoKaiFanfromDukeUniversityforspottinganerrorintheoriginalAdaMaxderivation.ExperimentsinthisworkwerepartlycarriedoutontheDutchnationale-infrastructurewiththesupportofSURFFoundation.DiederikKingmaissupportedbytheGoogleEuropeanDoctorateFellowshipinDeepLearning.REFERENCESAmari,Shun-Ichi.Naturalgradientworksefﬁcientlyinlearning.Neuralcomputation,10(2):251–276,1998.Deng,Li,Li,Jinyu,Huang,Jui-Ting,Yao,Kaisheng,Yu,Dong,Seide,Frank,Seltzer,Michael,Zweig,Geoff,He,Xiaodong,Williams,Jason,etal.Recentadvancesindeeplearningforspeechresearchatmicrosoft.ICASSP2013,2013.Duchi,John,Hazan,Elad,andSinger,Yoram.Adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.TheJournalofMachineLearningResearch,12:2121–2159,2011.Graves,Alex.Generatingsequenceswithrecurrentneuralnetworks.arXivpreprintarXiv:1308.0850,2013.Graves,Alex,Mohamed,Abdel-rahman,andHinton,Geoffrey.Speechrecognitionwithdeeprecurrentneuralnetworks.InAcoustics,SpeechandSignalProcessing(ICASSP),2013IEEEInternationalConferenceon,pp.6645–6649.IEEE,2013.Hinton,G.E.andSalakhutdinov,R.R.Reducingthedimensionalityofdatawithneuralnetworks.Science,313(5786):504–507,2006.Hinton,Geoffrey,Deng,Li,Yu,Dong,Dahl,GeorgeE,Mohamed,Abdel-rahman,Jaitly,Navdeep,Senior,Andrew,Vanhoucke,Vincent,Nguyen,Patrick,Sainath,TaraN,etal.Deepneuralnetworksforacousticmodelinginspeechrecognition:Thesharedviewsoffourresearchgroups.SignalProcessingMagazine,IEEE,29(6):82–97,2012a.Hinton,GeoffreyE,Srivastava,Nitish,Krizhevsky,Alex,Sutskever,Ilya,andSalakhutdinov,RuslanR.Im-provingneuralnetworksbypreventingco-adaptationoffeaturedetectors.arXivpreprintarXiv:1207.0580,2012b.Kingma,DiederikPandWelling,Max.Auto-EncodingVariationalBayes.InThe2ndInternationalConferenceonLearningRepresentations(ICLR),2013.Krizhevsky,Alex,Sutskever,Ilya,andHinton,GeoffreyE.Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks.InAdvancesinneuralinformationprocessingsystems,pp.1097–1105,2012.Maas,AndrewL,Daly,RaymondE,Pham,PeterT,Huang,Dan,Ng,AndrewY,andPotts,Christopher.Learningwordvectorsforsentimentanalysis.InProceedingsofthe49thAnnualMeetingoftheAssociationforComputationalLinguistics:HumanLanguageTechnologies-Volume1,pp.142–150.AssociationforComputationalLinguistics,2011.Moulines,EricandBach,FrancisR.Non-asymptoticanalysisofstochasticapproximationalgorithmsformachinelearning.InAdvancesinNeuralInformationProcessingSystems,pp.451–459,2011.Pascanu,RazvanandBengio,Yoshua.Revisitingnaturalgradientfordeepnetworks.arXivpreprintarXiv:1301.3584,2013.Polyak,BorisTandJuditsky,AnatoliB.Accelerationofstochasticapproximationbyaveraging.SIAMJournalonControlandOptimization,30(4):838–855,1992.10PublishedasaconferencepaperatICLR2015Roux,NicolasLandFitzgibbon,AndrewW.Afastnaturalnewtonmethod.InProceedingsofthe27thInternationalConferenceonMachineLearning(ICML-10),pp.623–630,2010.Ruppert,David.Efﬁcientestimationsfromaslowlyconvergentrobbins-monroprocess.Technicalreport,CornellUniversityOperationsResearchandIndustrialEngineering,1988.Schaul,Tom,Zhang,Sixin,andLeCun,Yann.Nomorepeskylearningrates.arXivpreprintarXiv:1206.1106,2012.Sohl-Dickstein,Jascha,Poole,Ben,andGanguli,Surya.Fastlarge-scaleoptimizationbyunifyingstochas-ticgradientandquasi-newtonmethods.InProceedingsofthe31stInternationalConferenceonMachineLearning(ICML-14),pp.604–612,2014.Sutskever,Ilya,Martens,James,Dahl,George,andHinton,Geoffrey.Ontheimportanceofinitializationandmomentumindeeplearning.InProceedingsofthe30thInternationalConferenceonMachineLearning(ICML-13),pp.1139–1147,2013.Tieleman,T.andHinton,G.Lecture6.5-RMSProp,COURSERA:NeuralNetworksforMachineLearning.Technicalreport,2012.Wang,SidaandManning,Christopher.Fastdropouttraining.InProceedingsofthe30thInternationalConfer-enceonMachineLearning(ICML-13),pp.118–126,2013.Zeiler,MatthewD.Adadelta:Anadaptivelearningratemethod.arXivpreprintarXiv:1212.5701,2012.Zinkevich,Martin.Onlineconvexprogrammingandgeneralizedinﬁnitesimalgradientascent.2003.11PublishedasaconferencepaperatICLR201510APPENDIX10.1CONVERGENCEPROOFDeﬁnition10.1.Afunctionf:Rd→Risconvexifforallx,y∈Rd,forallλ∈[0,1],λf(x)+(1−λ)f(y)≥f(λx+(1−λ)y)Also,noticethataconvexfunctioncanbelowerboundedbyahyperplaneatitstangent.Lemma10.2.Ifafunctionf:Rd→Risconvex,thenforallx,y∈Rd,f(y)≥f(x)+∇f(x)T(y−x)TheabovelemmacanbeusedtoupperboundtheregretandourproofforthemaintheoremisconstructedbysubstitutingthehyperplanewiththeAdamupdaterules.Thefollowingtwolemmasareusedtosupportourmaintheorem.Wealsousesomedeﬁnitionssim-plifyournotation,wheregt,∇ft(θt)andgt,iastheithelement.Wedeﬁneg1:t,i∈Rtasavectorthatcontainstheithdimensionofthegradientsoveralliterationstillt,g1:t,i=[g1,i,g2,i,···,gt,i]Lemma10.3.Letgt=∇ft(θt)andg1:tbedeﬁnedasaboveandbounded,kgtk2≤G,kgtk∞≤G∞.Then,TXt=1sg2t,it≤2G∞kg1:T,ik2Proof.WewillprovetheinequalityusinginductionoverT.ThebasecaseforT=1,wehaveqg21,i≤2G∞kg1,ik2.Fortheinductivestep,TXt=1sg2t,it=T−1Xt=1sg2t,it+sg2T,iT≤2G∞kg1:T−1,ik2+sg2T,iT=2G∞qkg1:T,ik22−g2T+sg2T,iTFrom,kg1:T,ik22−g2T,i+g4T,i4kg1:T,ik22≥kg1:T,ik22−g2T,i,wecantakesquarerootofbothsideandhave,qkg1:T,ik22−g2T,i≤kg1:T,ik2−g2T,i2kg1:T,ik2≤kg1:T,ik2−g2T,i2pTG2∞Rearrangetheinequalityandsubstitutetheqkg1:T,ik22−g2T,iterm,G∞qkg1:T,ik22−g2T+sg2T,iT≤2G∞kg1:T,ik212PublishedasaconferencepaperatICLR2015Lemma10.4.Letγ,β21√β2.Forβ1,β2∈[0,1)thatsatisfyβ21√β2<1andboundedgt,kgtk2≤G,kgtk∞≤G∞,thefollowinginequalityholdsTXt=1bm2t,iptbvt,i≤21−γ1√1−β2kg1:T,ik2Proof.Undertheassumption,√1−βt2(1−βt1)2≤1(1−β1)2.WecanexpandthelastterminthesummationusingtheupdaterulesinAlgorithm1,TXt=1bm2t,iptbvt,i=T−1Xt=1bm2t,iptbvt,i+p1−βT2(1−βT1)2(PTk=1(1−β1)βT−k1gk,i)2qTPTj=1(1−β2)βT−j2g2j,i≤T−1Xt=1bm2t,iptbvt,i+p1−βT2(1−βT1)2TXk=1T((1−β1)βT−k1gk,i)2qTPTj=1(1−β2)βT−j2g2j,i≤T−1Xt=1bm2t,iptbvt,i+p1−βT2(1−βT1)2TXk=1T((1−β1)βT−k1gk,i)2qT(1−β2)βT−k2g2k,i≤T−1Xt=1bm2t,iptbvt,i+p1−βT2(1−βT1)2(1−β1)2pT(1−β2)TXk=1T(cid:18)β21√β2(cid:19)T−kkgk,ik2≤T−1Xt=1bm2t,iptbvt,i+TpT(1−β2)TXk=1γT−kkgk,ik2Similarly,wecanupperboundtherestofthetermsinthesummation.TXt=1bm2t,iptbvt,i≤TXt=1kgt,ik2pt(1−β2)T−tXj=0tγj≤TXt=1kgt,ik2pt(1−β2)TXj=0tγjForγ<1,usingtheupperboundonthearithmetic-geometricseries,Pttγt<1(1−γ)2:TXt=1kgt,ik2pt(1−β2)TXj=0tγj≤1(1−γ)2√1−β2TXt=1kgt,ik2√tApplyLemma10.3,TXt=1bm2t,iptbvt,i≤2G∞(1−γ)2√1−β2kg1:T,ik2Tosimplifythenotation,wedeﬁneγ,β21√β2.Intuitively,ourfollowingtheoremholdswhenthelearningrateαtisdecayingatarateoft−12andﬁrstmomentrunningaveragecoefﬁcientβ1,tdecayexponentiallywithλ,thatistypicallycloseto1,e.g.1−10−8.Theorem10.5.Assumethatthefunctionfthasboundedgradients,k∇ft(θ)k2≤G,k∇ft(θ)k∞≤G∞forallθ∈RdanddistancebetweenanyθtgeneratedbyAdamisbounded,kθn−θmk2≤D,13PublishedasaconferencepaperatICLR2015kθm−θnk∞≤D∞foranym,n∈{1,...,T},andβ1,β2∈[0,1)satisfyβ21√β2<1.Letαt=α√tandβ1,t=β1λt−1,λ∈(0,1).Adamachievesthefollowingguarantee,forallT≥1.R(T)≤D22α(1−β1)dXi=1pTbvT,i+α(β1+1)G∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+dXi=1D2∞G∞√1−β22α(1−β1)(1−λ)2Proof.UsingLemma10.2,wehave,ft(θt)−ft(θ∗)≤gTt(θt−θ∗)=dXi=1gt,i(θt,i−θ∗,i)Fromtheupdaterulespresentedinalgorithm1,θt+1=θt−αtbmt/pbvt=θt−αt1−βt1(cid:18)β1,t√bvtmt−1+(1−β1,t)√bvtgt(cid:19)Wefocusontheithdimensionoftheparametervectorθt∈Rd.Subtractthescalarθ∗,iandsquarebothsidesoftheaboveupdaterule,wehave,(θt+1,i−θ∗,i)2=(θt,i−θ∗,i)2−2αt1−βt1(β1,tpbvt,imt−1,i+(1−β1,t)pbvt,igt,i)(θt,i−θ∗,i)+α2t(bmt,ipbvt,i)2WecanrearrangetheaboveequationanduseYoung’sinequality,ab≤a2/2+b2/2.Also,itcanbeshownthatpbvt,i=qPtj=1(1−β2)βt−j2g2j,i/p1−βt2≤kg1:t,ik2andβ1,t≤β1.Thengt,i(θt,i−θ∗,i)=(1−βt1)pbvt,i2αt(1−β1,t)(cid:18)(θt,i−θ∗,t)2−(θt+1,i−θ∗,i)2(cid:19)+β1,t(1−β1,t)bv14t−1,i√αt−1(θ∗,i−θt,i)√αt−1mt−1,ibv14t−1,i+αt(1−βt1)pbvt,i2(1−β1,t)(bmt,ipbvt,i)2≤12αt(1−β1)(cid:18)(θt,i−θ∗,t)2−(θt+1,i−θ∗,i)2(cid:19)pbvt,i+β1,t2αt−1(1−β1,t)(θ∗,i−θt,i)2pbvt−1,i+β1αt−12(1−β1)m2t−1,ipbvt−1,i+αt2(1−β1)bm2t,ipbvt,iWeapplyLemma10.4totheaboveinequalityandderivetheregretboundbysummingacrossallthedimensionsfori∈1,...,dintheupperboundofft(θt)−ft(θ∗)andthesequenceofconvexfunctionsfort∈1,...,T:R(T)≤dXi=112α1(1−β1)(θ1,i−θ∗,i)2pbv1,i+dXi=1TXt=212(1−β1)(θt,i−θ∗,i)2(pbvt,iαt−pbvt−1,iαt−1)+β1αG∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+αG∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+dXi=1TXt=1β1,t2αt(1−β1,t)(θ∗,i−θt,i)2pbvt,i14PublishedasaconferencepaperatICLR2015Fromtheassumption,kθt−θ∗k2≤D,kθm−θnk∞≤D∞,wehave:R(T)≤D22α(1−β1)dXi=1pTbvT,i+α(1+β1)G∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+D2∞2αdXi=1tXt=1β1,t(1−β1,t)ptbvt,i≤D22α(1−β1)dXi=1pTbvT,i+α(1+β1)G∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+D2∞G∞√1−β22αdXi=1tXt=1β1,t(1−β1,t)√tWecanusearithmeticgeometricseriesupperboundforthelastterm:tXt=1β1,t(1−β1,t)√t≤tXt=11(1−β1)λt−1√t≤tXt=11(1−β1)λt−1t≤1(1−β1)(1−λ)2Therefore,wehavethefollowingregretbound:R(T)≤D22α(1−β1)dXi=1pTbvT,i+α(1+β1)G∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+dXi=1D2∞G∞√1−β22αβ1(1−λ)215",
1409.0473,2015,Neural Machine Translation by Jointly Learning to Align and Translate,"['Neural Machine Translation by Jointly Learning to Align and Translate', 'Dzmitry Bahdanau', 'Kyunghyun Cho', 'and Yoshua Bengio']",https://arxiv.org/pdf/1409.0473,"6 1 0 2     y a M 9 1         ] L C . s c [      7 v 3 7 4 0  .  9 0 4 1 : v i X r a  Published as a conference paper at ICLR 2015  NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE  Dzmitry Bahdanau Jacobs University Bremen, Germany  KyungHyun Cho Universit´e de Montr´eal  Yoshua Bengio∗  ABSTRACT  Neural machine translation is a recently proposed approach to machine transla- tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu- ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec- ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.  1  INTRODUCTION  Neural machine translation is a newly emerging approach to machine translation, recently proposed by Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Unlike the traditional phrase-based translation system (see, e.g., Koehn et al., 2003) which consists of many small sub-components that are tuned separately, neural machine translation attempts to build and train a single, large neural network that reads a sentence and outputs a correct translation. Most of the proposed neural machine translation models belong to a family of encoder– decoders (Sutskever et al., 2014; Cho et al., 2014a), with an encoder and a decoder for each lan- guage, or involve a language-speciﬁc encoder applied to each sentence whose outputs are then com- pared (Hermann and Blunsom, 2014). An encoder neural network reads and encodes a source sen- tence into a ﬁxed-length vector. A decoder then outputs a translation from the encoded vector. The whole encoder–decoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence. A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a ﬁxed-length vector. This may make it difﬁcult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus. Cho et al. (2014b) showed that indeed the performance of a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases. In order to address this issue, we introduce an extension to the encoder–decoder model which learns to align and translate jointly. Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.  ∗CIFAR Senior Fellow  1  Published as a conference paper at ICLR 2015  The most important distinguishing feature of this approach from the basic encoder–decoder is that it does not attempt to encode a whole input sentence into a single ﬁxed-length vector. Instead, it en- codes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a ﬁxed-length vector. We show this allows a model to cope better with long sentences. In this paper, we show that the proposed approach of jointly learning to align and translate achieves signiﬁcantly improved translation performance over the basic encoder–decoder approach. The im- provement is more apparent with longer sentences, but can be observed with sentences of any length. On the task of English-to-French translation, the proposed approach achieves, with a single model, a translation performance comparable, or close, to the conventional phrase-based system. Furthermore, qualitative analysis reveals that the proposed model ﬁnds a linguistically plausible (soft-)alignment between a source sentence and the corresponding target sentence.  2 BACKGROUND: NEURAL MACHINE TRANSLATION  From a probabilistic perspective, translation is equivalent to ﬁnding a target sentence y that max- imizes the conditional probability of y given a source sentence x, i.e., arg maxy p(y | x). In neural machine translation, we ﬁt a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a translation model, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability. Recently, a number of papers have proposed the use of neural networks to directly learn this condi- tional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al., 2014; Cho et al., 2014b; Forcada and ˜Neco, 1997). This neural machine translation approach typ- ically consists of two components, the ﬁrst of which encodes a source sentence x and the second decodes to a target sentence y. For instance, two recurrent neural networks (RNN) were used by (Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a ﬁxed-length vector and to decode the vector into a variable-length target sentence. Despite being a quite new approach, neural machine translation has already shown promising results. Sutskever et al. (2014) reported that the neural machine translation based on RNNs with long short- term memory (LSTM) units achieves close to the state-of-the-art performance of the conventional phrase-based machine translation system on an English-to-French translation task.1 Adding neural components to existing translation systems, for instance, to score the phrase pairs in the phrase table (Cho et al., 2014a) or to re-rank candidate translations (Sutskever et al., 2014), has allowed to surpass the previous state-of-the-art performance level.  2.1 RNN ENCODER–DECODER  Here, we describe brieﬂy the underlying framework, called RNN Encoder–Decoder, proposed by Cho et al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns to align and translate simultaneously. In the Encoder–Decoder framework, an encoder reads the input sentence, a sequence of vectors x = (x1,··· , xTx ), into a vector c.2 The most common approach is to use an RNN such that  ht = f (xt, ht−1)  (1)  and  c = q ({h1,··· , hTx}) ,  where ht ∈ Rn is a hidden state at time t, and c is a vector generated from the sequence of the hidden states. f and q are some nonlinear functions. Sutskever et al. (2014) used an LSTM as f and q ({h1,··· , hT}) = hT , for instance.  1 We mean by the state-of-the-art performance, the performance of the conventional phrase-based system  without using any neural network-based component.  2 Although most of the previous works (see, e.g., Cho et al., 2014a; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013) used to encode a variable-length input sentence into a ﬁxed-length vector, it is not necessary, and even it may be beneﬁcial to have a variable-length vector, as we will show later.  2  Published as a conference paper at ICLR 2015  T(cid:89)  The decoder is often trained to predict the next word yt(cid:48) given the context vector c and all the previously predicted words {y1,··· , yt(cid:48)−1}. In other words, the decoder deﬁnes a probability over the translation y by decomposing the joint probability into the ordered conditionals:  where y =(cid:0)y1,··· , yTy  p(y) =  p(yt | {y1,··· , yt−1} , c),  (cid:1). With an RNN, each conditional probability is modeled as  t=1  (2)  p(yt | {y1,··· , yt−1} , c) = g(yt−1, st, c),  (3) where g is a nonlinear, potentially multi-layered, function that outputs the probability of yt, and st is the hidden state of the RNN. It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used (Kalchbrenner and Blunsom, 2013).  3 LEARNING TO ALIGN AND TRANSLATE  In this section, we propose a novel architecture for neural machine translation. The new architecture consists of a bidirectional RNN as an encoder (Sec. 3.2) and a decoder that emulates searching through a source sentence during decoding a translation (Sec. 3.1).  3.1 DECODER: GENERAL DESCRIPTION  In a new model architecture, we deﬁne each conditional probability in Eq. (2) as:  p(yi|y1, . . . , yi−1, x) = g(yi−1, si, ci),  (4)  where si is an RNN hidden state for time i, computed by  si = f (si−1, yi−1, ci).  It should be noted that unlike the existing encoder–decoder ap- proach (see Eq. (2)), here the probability is conditioned on a distinct context vector ci for each target word yi. The context vector ci depends on a sequence of annotations (h1,··· , hTx ) to which an encoder maps the input sentence. Each annotation hi contains information about the whole input sequence with a strong focus on the parts surrounding the i-th word of the input sequence. We explain in detail how the annotations are com- puted in the next section. The context vector ci is, then, computed as a weighted sum of these annotations hi:  ci =  αijhj.  (5)  Tx(cid:88)  j=1  Figure 1: The graphical illus- tration of the proposed model trying to generate the t-th tar- get word yt given a source sentence (x1, x2, . . . , xT ).  The weight αij of each annotation hj is computed by  (cid:80)Tx  exp (eij) k=1 exp (eik)  αij =  ,  (6)  where  eij = a(si−1, hj)  is an alignment model which scores how well the inputs around position j and the output at position i match. The score is based on the RNN hidden state si−1 (just before emitting yi, Eq. (4)) and the j-th annotation hj of the input sentence. We parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system. Note that unlike in traditional machine translation,  3  x1x2x3xT+αt,1αt,2αt,3αt,Tyt-1yth1h2h3hTh1h2h3hTst-1stPublished as a conference paper at ICLR 2015  the alignment is not considered to be a latent variable. Instead, the alignment model directly com- putes a soft alignment, which allows the gradient of the cost function to be backpropagated through. This gradient can be used to train the alignment model as well as the whole translation model jointly. We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation, where the expectation is over possible alignments. Let αij be a probability that the target word yi is aligned to, or translated from, a source word xj. Then, the i-th context vector ci is the expected annotation over all the annotations with probabilities αij. The probability αij, or its associated energy eij, reﬂects the importance of the annotation hj with respect to the previous hidden state si−1 in deciding the next state si and generating yi. Intuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a ﬁxed- length vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.  3.2 ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES  The usual RNN, described in Eq. (1), reads an input sequence x in order starting from the ﬁrst symbol x1 to the last one xTx. However, in the proposed scheme, we would like the annotation of each word to summarize not only the preceding words, but also the following words. Hence, we propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been successfully used recently in speech recognition (see, e.g., Graves et al., 2013).  A BiRNN consists of forward and backward RNN’s. The forward RNN as it is ordered (from x1 to xTx) and calculates a sequence of forward hidden states ( The backward RNN sequence of backward hidden states (  −→ f reads the input sequence −→ h Tx ). ←− f reads the sequence in the reverse order (from xTx to x1), resulting in a  −→ We obtain an annotation for each word xj by concatenating the forward hidden state h j and the backward one . In this way, the annotation hj contains the summaries of both the preceding words and the following words. Due to the tendency of RNNs to better represent recent inputs, the annotation hj will be focused on the words around xj. This sequence of annotations is used by the decoder and the alignment model later to compute the context vector (Eqs. (5)–(6)). See Fig. 1 for the graphical illustration of the proposed model.  ←− h j, i.e., hj =  −→ h 1,··· ,  ←− h 1,··· ,  ←− h Tx ).  (cid:104)−→  (cid:105)(cid:62)  ←− h (cid:62)  h (cid:62) j ;  j  4 EXPERIMENT SETTINGS  We evaluate the proposed approach on the task of English-to-French translation. We use the bilin- gual, parallel corpora provided by ACL WMT ’14.3 As a comparison, we also report the perfor- mance of an RNN Encoder–Decoder which was proposed recently by Cho et al. (2014a). We use the same training procedures and the same dataset for both models.4  4.1 DATASET  WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively, totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).5 We do not use any monolingual data other than the mentioned parallel corpora, although it may be possible to use a much larger monolingual corpus to pretrain an encoder. We concatenate news-test-  3 http://www.statmt.org/wmt14/translation-task.html 4 Implementations are available at https://github.com/lisa-groundhog/GroundHog. 5 Available online at http://www-lium.univ-lemans.fr/˜schwenk/cslm_joint_paper/.  4  Published as a conference paper at ICLR 2015  Figure 2: The BLEU scores of the generated translations on the test set with respect to the lengths of the sen- tences. The results are on the full test set which in- cludes sentences having un- known words to the models.  2012 and news-test-2013 to make a development (validation) set, and evaluate the models on the test set (news-test-2014) from WMT ’14, which consists of 3003 sentences not present in the training data. After a usual tokenization6, we use a shortlist of 30,000 most frequent words in each language to train our models. Any word not included in the shortlist is mapped to a special token ([UNK]). We do not apply any other special preprocessing, such as lowercasing or stemming, to the data.  4.2 MODELS  We train two types of models. The ﬁrst one is an RNN Encoder–Decoder (RNNencdec, Cho et al., 2014a), and the other is the proposed model, to which we refer as RNNsearch. We train each model twice: ﬁrst with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then with the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50). The encoder and decoder of the RNNencdec have 1000 hidden units each.7 The encoder of the RNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000 hidden units. Its decoder has 1000 hidden units. In both cases, we use a multilayer network with a single maxout (Goodfellow et al., 2013) hidden layer to compute the conditional probability of each target word (Pascanu et al., 2014). We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model. Each SGD update direction is computed using a minibatch of 80 sen- tences. We trained each model for approximately 5 days. Once a model is trained, we use a beam search to ﬁnd a translation that approximately maximizes the conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al., 2013). Sutskever et al. (2014) used this approach to generate translations from their neural machine translation model. For more details on the architectures of the models and training procedure used in the experiments, see Appendices A and B.  5 RESULTS  5.1 QUANTITATIVE RESULTS  In Table 1, we list the translation performances measured in BLEU score. It is clear from the table that in all the cases, the proposed RNNsearch outperforms the conventional RNNencdec. More importantly, the performance of the RNNsearch is as high as that of the conventional phrase-based translation system (Moses), when only the sentences consisting of known words are considered. This is a signiﬁcant achievement, considering that Moses uses a separate monolingual corpus (418M words) in addition to the parallel corpora we used to train the RNNsearch and RNNencdec.  6 We used the tokenization script from the open-source machine translation package, Moses. 7 In this paper, by a ’hidden unit’, we always mean the gated hidden unit (see Appendix A.1.1).  5  0102030405060Sentencelength051015202530BLEUscoreRNNsearch-50RNNsearch-30RNNenc-50RNNenc-30Published as a conference paper at ICLR 2015  (a)  (b)  (c)  (d)  Figure 3: Four sample alignments found by RNNsearch-50. The x-axis and y-axis of each plot correspond to the words in the source sentence (English) and the generated translation (French), respectively. Each pixel shows the weight αij of the annotation of the j-th source word for the i-th target word (see Eq. (6)), in grayscale (0: black, 1: white). (a) an arbitrary sentence. (b–d) three randomly selected samples among the sentences without any unknown words and of length between 10 and 20 words from the test set.  One of the motivations behind the proposed approach was the use of a ﬁxed-length context vector in the basic encoder–decoder approach. We conjectured that this limitation may make the basic encoder–decoder approach to underperform with long sentences. In Fig. 2, we see that the perfor- mance of RNNencdec dramatically drops as the length of the sentences increases. On the other hand, both RNNsearch-30 and RNNsearch-50 are more robust to the length of the sentences. RNNsearch- 50, especially, shows no performance deterioration even with sentences of length 50 or more. This superiority of the proposed model over the basic encoder–decoder is further conﬁrmed by the fact that the RNNsearch-30 even outperforms RNNencdec-50 (see Table 1).  6  TheagreementontheEuropeanEconomicAreawassignedinAugust1992.<end>L'accordsurlazoneéconomiqueeuropéenneaétésignéenaoût1992.<end>Itshouldbenotedthatthemarineenvironmentistheleastknownofenvironments.<end>Ilconvientdenoterquel'environnementmarinestlemoinsconnudel'environnement.<end>DestructionoftheequipmentmeansthatSyriacannolongerproducenewchemicalweapons.<end>Ladestructiondel'équipementsignifiequelaSyrienepeutplusproduiredenouvellesarmeschimiques.<end>""Thiswillchangemyfuturewithmyfamily,""themansaid.<end>""Celavachangermonaveniravecmafamille"",aditl'homme.<end>Published as a conference paper at ICLR 2015  Model  RNNencdec-30 RNNsearch-30 RNNencdec-50 RNNsearch-50 RNNsearch-50(cid:63)  Moses  All 13.93 21.50 17.82 26.75 28.45 33.30  No UNK◦  24.19 31.44 26.71 34.16 36.15 35.63  Table 1: BLEU scores of the trained models com- puted on the test set. The second and third columns show respectively the scores on all the sentences and, on the sentences without any unknown word in them- selves and in the reference translations. Note that RNNsearch-50(cid:63) was trained much longer until the performance on the development set stopped improv- ing. (◦) We disallowed the models to generate [UNK] tokens when only the sentences having no unknown words were evaluated (last column).  5.2 QUALITATIVE ANALYSIS  5.2.1 ALIGNMENT  The proposed approach provides an intuitive way to inspect the (soft-)alignment between the words in a generated translation and those in a source sentence. This is done by visualizing the annotation weights αij from Eq. (6), as in Fig. 3. Each row of a matrix in each plot indicates the weights associated with the annotations. From this we see which positions in the source sentence were considered more important when generating the target word. We can see from the alignments in Fig. 3 that the alignment of words between English and French is largely monotonic. We see strong weights along the diagonal of each matrix. However, we also observe a number of non-trivial, non-monotonic alignments. Adjectives and nouns are typically ordered differently between French and English, and we see an example in Fig. 3 (a). From this ﬁgure, we see that the model correctly translates a phrase [European Economic Area] into [zone ´economique europ´een]. The RNNsearch was able to correctly align [zone] with [Area], jumping over the two words ([European] and [Economic]), and then looked one word back at a time to complete the whole phrase [zone ´economique europ´eenne]. The strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance, from Fig. 3 (d). Consider the source phrase [the man] which was translated into [l’ homme]. Any hard alignment will map [the] to [l’] and [man] to [homme]. This is not helpful for translation, as one must consider the word following [the] to determine whether it should be translated into [le], [la], [les] or [l’]. Our soft-alignment solves this issue naturally by letting the model look at both [the] and [man], and in this example, we see that the model was able to correctly translate [the] into [l’]. We observe similar behaviors in all the presented cases in Fig. 3. An additional beneﬁt of the soft align- ment is that it naturally deals with source and target phrases of different lengths, without requiring a counter-intuitive way of mapping some words to or from nowhere ([NULL]) (see, e.g., Chapters 4 and 5 of Koehn, 2010).  5.2.2 LONG SENTENCES  As clearly visible from Fig. 2 the proposed model (RNNsearch) is much better than the conventional model (RNNencdec) at translating long sentences. This is likely due to the fact that the RNNsearch does not require encoding a long sentence into a ﬁxed-length vector perfectly, but only accurately encoding the parts of the input sentence that surround a particular word. As an example, consider this source sentence from the test set:  An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital.  The RNNencdec-50 translated this sentence into:  Un privil`ege d’admission est le droit d’un m´edecin de reconnaˆıtre un patient `a l’hˆopital ou un centre m´edical d’un diagnostic ou de prendre un diagnostic en fonction de son ´etat de sant´e.  7  Published as a conference paper at ICLR 2015  The RNNencdec-50 correctly translated the source sentence until [a medical center]. However, from there on (underlined), it deviated from the original meaning of the source sentence. For instance, it replaced [based on his status as a health care worker at a hospital] in the source sentence with [en fonction de son ´etat de sant´e] (“based on his state of health”). On the other hand, the RNNsearch-50 generated the following correct translation, preserving the whole meaning of the input sentence without omitting any details:  Un privil`ege d’admission est le droit d’un m´edecin d’admettre un patient `a un hˆopital ou un centre m´edical pour effectuer un diagnostic ou une proc´edure, selon son statut de travailleur des soins de sant´e `a l’hˆopital.  Let us consider another sentence from the test set:  This kind of experience is part of Disney’s efforts to ”extend the lifetime of its series and build new relationships with audiences via digital platforms that are becoming ever more important,” he added.  The translation by the RNNencdec-50 is  Ce type d’exp´erience fait partie des initiatives du Disney pour ”prolonger la dur´ee de vie de ses nouvelles et de d´evelopper des liens avec les lecteurs num´eriques qui deviennent plus complexes.  As with the previous example, the RNNencdec began deviating from the actual meaning of the source sentence after generating approximately 30 words (see the underlined phrase). After that point, the quality of the translation deteriorates, with basic mistakes such as the lack of a closing quotation mark. Again, the RNNsearch-50 was able to translate this long sentence correctly:  Ce genre d’exp´erience fait partie des efforts de Disney pour ”prolonger la dur´ee de vie de ses s´eries et cr´eer de nouvelles relations avec des publics via des plateformes num´eriques de plus en plus importantes”, a-t-il ajout´e.  In conjunction with the quantitative results presented already, these qualitative observations con- ﬁrm our hypotheses that the RNNsearch architecture enables far more reliable translation of long sentences than the standard RNNencdec model. In Appendix C, we provide a few more sample translations of long source sentences generated by the RNNencdec-50, RNNsearch-50 and Google Translate along with the reference translations.  6 RELATED WORK  6.1 LEARNING TO ALIGN  A similar approach of aligning an output symbol with an input symbol was proposed recently by Graves (2013) in the context of handwriting synthesis. Handwriting synthesis is a task where the model is asked to generate handwriting of a given sequence of characters. In his work, he used a mixture of Gaussian kernels to compute the weights of the annotations, where the location, width and mixture coefﬁcient of each kernel was predicted from an alignment model. More speciﬁcally, his alignment was restricted to predict the location such that the location increases monotonically. The main difference from our approach is that, in (Graves, 2013), the modes of the weights of the annotations only move in one direction. In the context of machine translation, this is a severe limi- tation, as (long-distance) reordering is often needed to generate a grammatically correct translation (for instance, English-to-German). Our approach, on the other hand, requires computing the annotation weight of every word in the source sentence for each word in the translation. This drawback is not severe with the task of translation in which most of input and output sentences are only 15–40 words. However, this may limit the applicability of the proposed scheme to other tasks.  8  Published as a conference paper at ICLR 2015  6.2 NEURAL NETWORKS FOR MACHINE TRANSLATION  Since Bengio et al. (2003) introduced a neural probabilistic language model which uses a neural net- work to model the conditional probability of a word given a ﬁxed number of the preceding words, neural networks have widely been used in machine translation. However, the role of neural net- works has been largely limited to simply providing a single feature to an existing statistical machine translation system or to re-rank a list of candidate translations provided by an existing system. For instance, Schwenk (2012) proposed using a feedforward neural network to compute the score of a pair of source and target phrases and to use the score as an additional feature in the phrase-based statistical machine translation system. More recently, Kalchbrenner and Blunsom (2013) and Devlin et al. (2014) reported the successful use of the neural networks as a sub-component of the existing translation system. Traditionally, a neural network trained as a target-side language model has been used to rescore or rerank a list of candidate translations (see, e.g., Schwenk et al., 2006). Although the above approaches were shown to improve the translation performance over the state- of-the-art machine translation systems, we are more interested in a more ambitious objective of designing a completely new translation system based on neural networks. The neural machine trans- lation approach we consider in this paper is therefore a radical departure from these earlier works. Rather than using a neural network as a part of the existing system, our model works on its own and generates a translation from a source sentence directly.  7 CONCLUSION  The conventional approach to neural machine translation, called an encoder–decoder approach, en- codes a whole input sentence into a ﬁxed-length vector from which a translation will be decoded. We conjectured that the use of a ﬁxed-length context vector is problematic for translating long sen- tences, based on a recent empirical study reported by Cho et al. (2014b) and Pouget-Abadie et al. (2014). In this paper, we proposed a novel architecture that addresses this issue. We extended the basic encoder–decoder by letting a model (soft-)search for a set of input words, or their annotations com- puted by an encoder, when generating each target word. This frees the model from having to encode a whole source sentence into a ﬁxed-length vector, and also lets the model focus only on information relevant to the generation of the next target word. This has a major positive impact on the ability of the neural machine translation system to yield good results on longer sentences. Unlike with the traditional machine translation systems, all of the pieces of the translation system, including the alignment mechanism, are jointly trained towards a better log-probability of producing correct translations. We tested the proposed model, called RNNsearch, on the task of English-to-French translation. The experiment revealed that the proposed RNNsearch outperforms the conventional encoder–decoder model (RNNencdec) signiﬁcantly, regardless of the sentence length and that it is much more ro- bust to the length of a source sentence. From the qualitative analysis where we investigated the (soft-)alignment generated by the RNNsearch, we were able to conclude that the model can cor- rectly align each target word with the relevant words, or their annotations, in the source sentence as it generated a correct translation. Perhaps more importantly, the proposed approach achieved a translation performance comparable to the existing phrase-based statistical machine translation. It is a striking result, considering that the proposed architecture, or the whole family of neural machine translation, has only been proposed as recently as this year. We believe the architecture proposed here is a promising step toward better machine translation and a better understanding of natural languages in general. One of challenges left for the future is to better handle unknown, or rare words. This will be required for the model to be more widely used and to match the performance of current state-of-the-art machine translation systems in all contexts.  9  Published as a conference paper at ICLR 2015  ACKNOWLEDGMENTS  The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Qu´ebec, Compute Canada, the Canada Research Chairs and CIFAR. Bah- danau thanks the support from Planet Intelligent Systems GmbH. We also thank Felix Hill, Bart van Merri´enboer, Jean Pouget-Abadie, Coline Devin and Tae-Ho Kim.  REFERENCES Axelrod, A., He, X., and Gao, J. (2011). Domain adaptation via pseudo in-domain data selection. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 355–362. Association for Computational Linguistics.  Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient  descent is difﬁcult. IEEE Transactions on Neural Networks, 5(2), 157–166.  Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic language model.  J. Mach. Learn. Res., 3, 1137–1155.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- In  Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2013). Audio chord recognition with  recurrent neural networks. In ISMIR.  Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. (2014a). Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014). to appear.  Cho, K., van Merri¨enboer, B., Bahdanau, D., and Bengio, Y. (2014b). On the properties of neural machine translation: Encoder–Decoder approaches. In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. to appear.  Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. In Association for Computational Linguistics.  Forcada, M. L. and ˜Neco, R. P. (1997). Recursive hetero-associative memories for translation. In J. Mira, R. Moreno-D´ıaz, and J. Cabestany, editors, Biological and Artiﬁcial Computation: From Neuroscience to Technology, volume 1240 of Lecture Notes in Computer Science, pages 453–462. Springer Berlin Heidelberg.  Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). Maxout net- works. In Proceedings of The 30th International Conference on Machine Learning, pages 1319– 1327.  Graves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the  29th International Conference on Machine Learning (ICML 2012).  Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv:1308.0850  [cs.NE].  Graves, A., Jaitly, N., and Mohamed, A.-R. (2013). Hybrid speech recognition with deep bidirec- tional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Work- shop on, pages 273–278.  10  Published as a conference paper at ICLR 2015  Hermann, K. and Blunsom, P. (2014). Multilingual distributed representations without word align- ment. In Proceedings of the Second International Conference on Learning Representations (ICLR 2014).  Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut  f¨ur Informatik, Lehrstuhl Prof. Brauer, Technische Universit¨at M¨unchen.  Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8),  1735–1780.  Kalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700–1709. Association for Computational Linguistics.  Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press, New York, NY,  USA.  Koehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Stroudsburg, PA, USA. Association for Computational Linguistics.  Pascanu, R., Mikolov, T., and Bengio, Y. (2013a). On the difﬁculty of training recurrent neural  networks. In ICML’2013.  Pascanu, R., Mikolov, T., and Bengio, Y. (2013b). On the difﬁculty of training recurrent neural In Proceedings of the 30th International Conference on Machine Learning (ICML  networks. 2013).  Pascanu, R., Gulcehre, C., Cho, K., and Bengio, Y. (2014). How to construct deep recurrent neural networks. In Proceedings of the Second International Conference on Learning Representations (ICLR 2014).  Pouget-Abadie, J., Bahdanau, D., van Merri¨enboer, B., Cho, K., and Bengio, Y. (2014). Overcoming In  the curse of sentence length for neural machine translation using automatic segmentation. Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. to appear.  Schuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. Signal Processing,  IEEE Transactions on, 45(11), 2673–2681.  Schwenk, H. (2012). Continuous space translation models for phrase-based statistical machine translation. In M. Kay and C. Boitet, editors, Proceedings of the 24th International Conference on Computational Linguistics (COLIN), pages 1071–1080. Indian Institute of Technology Bombay.  Schwenk, H., Dchelotte, D., and Gauvain, J.-L. (2006). Continuous space language models for statistical machine translation. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 723–730. Association for Computational Linguistics.  Sutskever, I., Vinyals, O., and Le, Q. (2014). Sequence to sequence learning with neural networks.  In Advances in Neural Information Processing Systems (NIPS 2014).  Zeiler, M. D. (2012). ADADELTA: An adaptive learning rate method.  [cs.LG].  arXiv:1212.5701  11  Published as a conference paper at ICLR 2015  A MODEL ARCHITECTURE  A.1 ARCHITECTURAL CHOICES  The proposed scheme in Section 3 is a general framework where one can freely deﬁne, for instance, the activation functions f of recurrent neural networks (RNN) and the alignment model a. Here, we describe the choices we made for the experiments in this paper.  A.1.1 RECURRENT NEURAL NETWORK  For the activation function f of an RNN, we use the gated hidden unit recently proposed by Cho et al. (2014a). The gated hidden unit is an alternative to the conventional simple units such as an element-wise tanh. This gated unit is similar to a long short-term memory (LSTM) unit proposed earlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn long-term dependencies. This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1. These paths allow gradients to ﬂow backward easily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013a). It is therefore possible to use LSTM units instead of the gated hidden unit described here, as was done in a similar context by Sutskever et al. (2014). The new state si of the RNN employing n gated hidden units8 is computed by  si = f (si−1, yi−1, ci) = (1 − zi) ◦ si−1 + zi ◦ ˜si,  where ◦ is an element-wise multiplication, and zi is the output of the update gates (see below). The proposed updated state ˜si is computed by  ˜si = tanh (W e(yi−1) + U [ri ◦ si−1] + Cci) ,  where e(yi−1) ∈ Rm is an m-dimensional embedding of a word yi−1, and ri is the output of the reset gates (see below). When yi is represented as a 1-of-K vector, e(yi) is simply a column of an embedding matrix E ∈ Rm×K. Whenever possible, we omit bias terms to make the equations less cluttered. The update gates zi allow each hidden unit to maintain its previous activation, and the reset gates ri control how much and what information from the previous state should be reset. We compute them by  zi = σ (Wze(yi−1) + Uzsi−1 + Czci) , ri = σ (Wre(yi−1) + Ursi−1 + Crci) ,  where σ (·) is a logistic sigmoid function. At each step of the decoder, we compute the output probability (Eq. (4)) as a multi-layered func- tion (Pascanu et al., 2014). We use a single hidden layer of maxout units (Goodfellow et al., 2013) and normalize the output probabilities (one for each word) with a softmax function (see Eq. (6)).  A.1.2 ALIGNMENT MODEL The alignment model should be designed considering that the model needs to be evaluated Tx × Ty times for each sentence pair of lengths Tx and Ty. In order to reduce computation, we use a single- layer multilayer perceptron such that  a(si−1, hj) = v(cid:62)  a tanh (Wasi−1 + Uahj) ,  where Wa ∈ Rn×n, Ua ∈ Rn×2n and va ∈ Rn are the weight matrices. Since Uahj does not depend on i, we can pre-compute it in advance to minimize the computational cost.  8 Here, we show the formula of the decoder. The same formula can be used in the encoder by simply  ignoring the context vector ci and the related terms.  12  Published as a conference paper at ICLR 2015  A.2 DETAILED DESCRIPTION OF THE MODEL  A.2.1 ENCODER  In this section, we describe in detail the architecture of the proposed model (RNNsearch) used in the experiments (see Sec. 4–5). From here on, we omit all bias terms in order to increase readability. The model takes a source sentence of 1-of-K coded word vectors as input  x = (x1, . . . , xTx), xi ∈ RKx and outputs a translated sentence of 1-of-K coded word vectors y = (y1, . . . , yTy ), yi ∈ RKy ,  where Kx and Ky are the vocabulary sizes of source and target languages, respectively. Tx and Ty respectively denote the lengths of source and target sentences. First, the forward states of the bidirectional recurrent neural network (BiRNN) are computed:  0  h i  (1 − −→z i) ◦ −→ (cid:16)−→ (cid:16)−→ (cid:16)−→  h i−1 + −→z i ◦ −→ (cid:104)−→r i ◦ −→ (cid:17) (cid:17)  , if i > 0 , if i = 0  (cid:105)(cid:17)  where  h i−1  W zExi +  −→ h i = tanh −→z i =σ −→r i =σ  W Exi + −→ U z −→ U r −→ W ,  −→ U −→ h i−1 −→ h i−1 W rExi + −→ −→ −→ E ∈ Rm×Kx is the word embedding matrix. U r ∈ Rn×n are W r ∈ Rn×m, W z, weight matrices. m and n are the word embedding dimensionality and the number of hidden units, respectively. σ(·) is as usual a logistic sigmoid function. The backward states ( E between the forward and backward RNNs, unlike the weight matrices. We concatenate the forward and backward states to to obtain the annotations (h1, h2,··· , hTx ), where  ←− h Tx ) are computed similarly. We share the word embedding matrix  ←− h 1,··· ,  −→ U z,  −→ U ,  .  (cid:40)  −→ h i =  (7)  (cid:35)  (cid:34) −→  h i←− h i  hi =  A.2.2 DECODER  The hidden state si of the decoder given the annotations from the encoder is computed by  si =(1 − zi) ◦ si−1 + zi ◦ ˜si,  where  ˜si = tanh (W Eyi−1 + U [ri ◦ si−1] + Cci) zi =σ (WzEyi−1 + Uzsi−1 + Czci) ri =σ (WrEyi−1 + Ursi−1 + Crci)  E is the word embedding matrix for the target language. W, Wz, Wr ∈ Rn×m, U, Uz, Ur ∈ Rn×n, and C, Cz, Cr ∈ Rn×2n are weights. Again, m and n are the word embedding dimensionality and the number of hidden units, respectively. The initial hidden state s0 is computed by s0 = tanh  , where Ws ∈ Rn×n.  ←− h 1  (cid:16)  (cid:17)  Ws  The context vector ci are recomputed at each step by the alignment model:  ci =  αijhj,  Tx(cid:88)  j=1  13  Published as a conference paper at ICLR 2015  Model  RNNenc-30 RNNenc-50 RNNsearch-30 RNNsearch-50 RNNsearch-50(cid:63)  Updates (×105) Epochs Hours 109 108 113 111 252  8.46 6.00 4.71 2.88 6.67  6.4 4.5 3.6 2.2 5.0  GPU  Train NLL Dev. NLL  TITAN BLACK Quadro K-6000 TITAN BLACK Quadro K-6000 Quadro K-6000  28.1 44.0 26.7 40.7 36.7  53.0 43.6 47.2 38.1 35.2  Table 2: Learning statistics and relevant information. Each update corresponds to updating the parameters once using a single minibatch. One epoch is one pass through the training set. NLL is the average conditional log-probabilities of the sentences in either the training set or the development set. Note that the lengths of the sentences differ.  where  (cid:80)Tx  exp (eij) k=1 exp (eik)  αij = eij =v(cid:62)  a tanh (Wasi−1 + Uahj) ,  and hj is the j-th annotation in the source sentence (see Eq. (7)). va ∈ Rn(cid:48) , Wa ∈ Rn(cid:48)×n and Ua ∈ Rn(cid:48)×2n are weight matrices. Note that the model becomes RNN Encoder–Decoder (Cho et al., 2014a), if we ﬁx ci to With the decoder state si−1, the context ci and the last generated word yi−1, we deﬁne the probability of a target word yi as  −→ h Tx.  where  (cid:1) ,  i Woti  p(yi|si, yi−1, ci) ∝ exp(cid:0)y(cid:62) ti =(cid:2)max(cid:8)˜ti,2j−1, ˜ti,2j (cid:9)(cid:3)(cid:62)  j=1,...,l  and ˜ti,k is the k-th element of a vector ˜ti which is computed by  ˜ti =Uosi−1 + VoEyi−1 + Coci.  Wo ∈ RKy×l, Uo ∈ R2l×n, Vo ∈ R2l×m and Co ∈ R2l×2n are weight matrices. This can be under- stood as having a deep output (Pascanu et al., 2014) with a single maxout hidden layer (Goodfellow et al., 2013).  A.2.3 MODEL SIZE  For all the models used in this paper, the size of a hidden layer n is 1000, the word embedding dimensionality m is 620 and the size of the maxout hidden layer in the deep output l is 500. The number of hidden units in the alignment model n(cid:48) is 1000.  B TRAINING PROCEDURE  B.1 PARAMETER INITIALIZATION  −→ U r as random or- We initialized the recurrent weight matrices U, Uz, Ur, thogonal matrices. For Wa and Ua, we initialized them by sampling each element from the Gaussian distribution of mean 0 and variance 0.0012. All the elements of Va and all the bias vectors were ini- tialized to zero. Any other weight matrix was initialized by sampling from the Gaussian distribution of mean 0 and variance 0.012.  −→ U z and  ←− U z,  ←− U r,  −→ U ,  ←− U ,  B.2 TRAINING  We used the stochastic gradient descent (SGD) algorithm. Adadelta (Zeiler, 2012) was used to automatically adapt the learning rate of each parameter ((cid:15) = 10−6 and ρ = 0.95). We explicitly  14  Published as a conference paper at ICLR 2015  normalized the L2-norm of the gradient of the cost function each time to be at most a predeﬁned threshold of 1, when the norm was larger than the threshold (Pascanu et al., 2013b). Each SGD update direction was computed with a minibatch of 80 sentences. At each update our implementation requires time proportional to the length of the longest sentence in a minibatch. Hence, to minimize the waste of computation, before every 20-th update, we retrieved 1600 sentence pairs, sorted them according to the lengths and split them into 20 minibatches. The training data was shufﬂed once before training and was traversed sequentially in this manner. In Tables 2 we present the statistics related to training all the models used in the experiments.  C TRANSLATIONS OF LONG SENTENCES  Source  Reference  RNNenc-50  An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital. Le privil`ege d’admission est le droit d’un m´edecin, en vertu de son statut de membre soignant d’un hˆopital, d’admettre un patient dans un hˆopital ou un centre m´edical aﬁn d’y d´elivrer un diagnostic ou un traitement. Un privil`ege d’admission est le droit d’un m´edecin de reconnaˆıtre un patient `a l’hˆopital ou un centre m´edical d’un diagnostic ou de prendre un diagnostic en fonction de son ´etat de sant´e.  RNNsearch-50 Un privil`ege d’admission est le droit d’un m´edecin d’admettre un patient `a un hˆopital ou un centre m´edical pour effectuer un diagnostic ou une proc´edure, selon son statut de travailleur des soins de sant´e `a l’hˆopital. Un privil`ege admettre est le droit d’un m´edecin d’admettre un patient dans un hˆopital ou un centre m´edical pour effectuer un diagnostic ou une proc´edure, fond´ee sur sa situation en tant que travailleur de soins de sant´e dans un hˆopital.  Google Translate  Source  Reference  RNNenc-50  RNNsearch-50  Google Translate  Source  Reference  RNNenc-50  RNNsearch-50  Google Translate  This kind of experience is part of Disney’s efforts to ”extend the lifetime of its series and build new relationships with audiences via digital platforms that are becoming ever more important,” he added. Ce type d’exp´erience entre dans le cadre des efforts de Disney pour ”´etendre la dur´ee de vie de ses s´eries et construire de nouvelles relations avec son public grˆace `a des plateformes num´eriques qui sont de plus en plus importantes”, a-t-il ajout´e. Ce type d’exp´erience fait partie des initiatives du Disney pour ”prolonger la dur´ee de vie de ses nouvelles et de d´evelopper des liens avec les lecteurs num´eriques qui deviennent plus com- plexes. Ce genre d’exp´erience fait partie des efforts de Disney pour ”prolonger la dur´ee de vie de ses s´eries et cr´eer de nouvelles relations avec des publics via des plateformes num´eriques de plus en plus importantes”, a-t-il ajout´e. Ce genre d’exp´erience fait partie des efforts de Disney `a “´etendre la dur´ee de vie de sa s´erie et construire de nouvelles relations avec le public par le biais des plates-formes num´eriques qui deviennent de plus en plus important”, at-il ajout´e.  In a press conference on Thursday, Mr Blair stated that there was nothing in this video that might constitute a ”reasonable motive” that could lead to criminal charges being brought against the mayor. En conf´erence de presse, jeudi, M. Blair a afﬁrm´e qu’il n’y avait rien dans cette vid´eo qui puisse constituer des ”motifs raisonnables” pouvant mener au d´epˆot d’une accusation criminelle contre le maire. Lors de la conf´erence de presse de jeudi, M. Blair a dit qu’il n’y avait rien dans cette vid´eo qui pourrait constituer une ”motivation raisonnable” pouvant entraˆıner des accusations criminelles port´ees contre le maire. Lors d’une conf´erence de presse jeudi, M. Blair a d´eclar´e qu’il n’y avait rien dans cette vid´eo qui pourrait constituer un ”motif raisonnable” qui pourrait conduire `a des accusations criminelles contre le maire. Lors d’une conf´erence de presse jeudi, M. Blair a d´eclar´e qu’il n’y avait rien dans cette vido qui pourrait constituer un ”motif raisonnable” qui pourrait mener `a des accusations criminelles portes contre le maire.  Table 3: The translations generated by RNNenc-50 and RNNsearch-50 from long source sentences (30 words or more) selected from the test set. For each source sentence, we also show the gold- standard translation. The translations by Google Translate were made on 27 August 2014.  15  ",
1406.3269,2015,Scheduled denoising autoencoders,"['Scheduled denoising autoencoders', 'Krzysztof Geras and Charles Sutton']",https://arxiv.org/pdf/1406.3269,"5 1 0 2    r p A 0 1         ]  G L . s c [      3 v 9 6 2 3  .  6 0 4 1 : v i X r a  Published as a conference paper at ICLR 2015  SCHEDULED DENOISING AUTOENCODERS  Krzysztof J. Geras School of Informatics University of Edinburgh k.j.geras@sms.ed.ac.uk  Charles Sutton School of Informatics University of Edinburgh csutton@inf.ed.ac.uk  ABSTRACT  We present a representation learning method that learns features at multiple dif- ferent levels of scale. Working within the unsupervised framework of denoising autoencoders, we observe that when the input is heavily corrupted during train- ing, the network tends to learn coarse-grained features, whereas when the input is only slightly corrupted, the network tends to learn ﬁne-grained features. This motivates the scheduled denoising autoencoder, which starts with a high level of noise that lowers as training progresses. We ﬁnd that the resulting representation yields a signiﬁcant boost on a later supervised task compared to the original in- put, or to a standard denoising autoencoder trained at a single noise level. After supervised ﬁne-tuning our best model achieves the lowest ever reported error on the CIFAR-10 data set among permutation-invariant methods.  1  INTRODUCTION  In most applications of representation learning, we wish to learn features at different levels of scale. For example, in image data, some edges will span only a few pixels, whereas others, such as a boundary between foreground and background, will span a large portion of the image. Similarly, in text data, some features in the representation might model specialized topics that use only a few words. For example a topic about electronics would often use words such as “big”, “screen” and “tv”. Other features model more general topics that use many different words. Good representations should model both of these phenomena, containing features at different levels of granularity. Denoising autoencoders (Vincent et al., 2008; 2010; Glorot et al., 2011a) provide a particularly natural framework in which to formalise this intuition. In a denoising autoencoder, the network is trained so as to be able to reconstruct each data point from a corrupted version. The noise process used to perform the corruption is chosen by the modeller, and is an important tuning parameter that affects the ﬁnal representation. On a digit recognition task, Vincent et al. (2010) noticed that using a low level of noise leads to learning blob detectors, while increasing it results in obtaining detectors of strokes or parts of digits. They also recognise that either too low or too high level of noise harms the representation learnt. The relationship between the level of noise and spatial extent of the ﬁlters was also noticed by Karklin and Simoncelli (2011) for a different feature learning model. Despite impressive practical results with denoising autoencoders, e.g. Glorot et al. (2011b), Mesnil et al. (2012), the choice of noise distribution is a tuning parameter whose effects are not fully understood. In this paper, we introduce scheduled denoising autoencoders (ScheDA), which are based on the intuition that by training the same network at multiple noise levels, we can encourage it to learn features at different scales. The network is trained with a schedule of gradually decreasing noise levels. At the initial, high noise levels, the training data is highly corrupted, which forces the network to learn more global, coarse-grained features of the data. At low noise levels, the network is able to learn features for reconstructing ﬁner details of the training data. At the end of the schedule, the network will include a combination of both coarse-grained and ﬁne-grained features.  1  Published as a conference paper at ICLR 2015  This idea is reminiscent of continuation methods, which have also been applied to neural networks (Bengio et al., 2009). The motivation of this work is signiﬁcantly different though. Our goal is to encourage the network to learn a more diverse set of features, some which are similar to features learnt at the initial noise level, and others which are similar to features learnt at the ﬁnal noise level. In Section 4.1.3, we verify quantitatively that this happens. Experimentally, we ﬁnd on both image and text data that scheduled denoising autoencoders learn better representations than standard denoising autoencoders, as measured by the features’ perfor- mance on a supervised task. On both classiﬁcation tasks, the representation from ScheDA yields lower test error than that from a denoising autoencoder trained at the best single noise level. Af- ter supervised ﬁne-tuning our best ScheDA model achieves the lowest ever reported error on the CIFAR-10 data set among permutation-invariant methods.  2 BACKGROUND  The core idea of learning a representation by learning to reconstruct artiﬁcially corrupted training data dates back at least to the work of Seung (1998), who suggested using a recurrent neural net- work for this purpose. Using unsupervised layer-wise learning of representations for classiﬁcation purposes appeared later in the work of Bengio et al. (2007) and Hinton et al. (2006). The denoising autoencoder (DA) (Vincent et al., 2008) is based on the same intuition as the work of Seung (1998) that that a good representation should contain enough information to reconstruct corrupted versions of an original input. Let x ∈ Rd be the input to the network. The output of the network is a hidden representation y ∈ Rd(cid:48) , which is simply computed as fθ(x) = s(Wx + b), where the matrix W ∈ Rd(cid:48)×d and the vector b ∈ Rd(cid:48) are the parameters of the network, and s is a typically nonlinear transfer function, such as a sigmoid. We write θ = (W, b). The function f is called an encoder because it maps the input to a hidden representation. In an autoencoder, we have also a decoder that “reconstructs” the input vector from the hidden representation, which is used when training the network. The decoder has a similar form to the encoder, namely, gθ(cid:48)(y) = t(W(cid:48)y + b(cid:48)), except that here W(cid:48) ∈ Rd×d(cid:48) and b(cid:48) ∈ Rd. It can be useful to allow the transfer function t for the decoder to be different from that for the encoder. Typically, W and W(cid:48) are constrained by W(cid:48) = WT by analogy to the interpretation of principal components analysis as a linear encoder and decoder. During training, our objective is to learn the encoder parameters W and b. As a byproduct, we will need to learn the decoder parameters b(cid:48) as well. We do this by deﬁning a noise distribution p(˜x|x, ν). The amount of corruption is controlled by a parameter ν. We train the autoencoder weights to be able to reconstruct a random input from the training distribution x from its corrupted version ˜x by running the encoder and the decoder in sequence. Formally, this process is described by the objective function  (cid:104)  (cid:16)  (cid:17)(cid:105)  L  X, gθ(cid:48)(fθ( ˜X))  ,  (1)  θ∗, θ(cid:48)∗  = arg min  θ,θ(cid:48)  E (X, ˜X)  where L is a loss function over the input space, such as squared error. Typically we minimize this objective function using stochastic gradient descent with mini-batches, where at each iteration we sample new values for both the uncorrupted and corrupted inputs. In the absence of noise, this model is known simply as an autoencoder or autoassociator. A classic result (Baldi and Hornik, 1989) states that when d(cid:48) < d, then under certain conditions, an autoen- coder learns the same subspace as PCA. If the dimensionality of the hidden representation is too large, i.e., if d(cid:48) > d, then the autoencoder can obtain zero reconstruction error simply by learning the identity map. In a denoising autoencoder, in contrast, the noise forces the model to learn inter- esting structure even when there are a large number of hidden units. Indeed, in practical denoising autoencoders, often the best results are found with overcomplete representations for which d(cid:48) > d. There are several tuning parameters here, including the noise distribution, the transformations s and t and the loss function L. For the loss function L, for continuous x, squared error can be used. For binary x or x ∈ [0, 1], as we consider in this paper, it is common to use the cross entropy loss,  L(x, z) = − D(cid:88)  (xi log zi + (1 − xi) log (1 − zi)) .  i=1  2  Published as a conference paper at ICLR 2015  For the transfer functions, common choices include the sigmoid s(v) = 1 and decoder, or to use a rectiﬁer s(v) = max(0, v) in the encoder paired with sigmoid decoder. One of the most important parameters in a denoising autoencoder is the noise distribution p. For continuous x, Gaussian noise p(˜x|x, ν) = N (˜x; x, ν) can be used. For binary x or x ∈ [0, 1], it is most common to use masking noise, that is, for each i ∈ 1, 2, . . . d, we sample ˜xi independently as (2)  (cid:26)0 with probability ν,  1+e−v for both the encoder  p(˜xi|xi, ν) =  xi  otherwise.  In either case, the level of noise ν affects the degree of corruption of the input. If ν is high, the inputs are more heavily corrupted during training. The noise level has a signiﬁcant effect on the representations learnt. For example, if the input data are images, masking only a few pixels will bias the process of learning the representation to deal well with local corruptions. On the other hand, masking very many pixels will push the algorithm to use information from more distant regions. It is also possible to train multiple layers of representations with denoising autoencoders by training a denoising autoencoder with data mapped to a representation learnt by another denoising autoen- coder. This model is known as the stacked denoising autoencoder (Vincent et al., 2008; 2010).  3 SCHEDULED DENOISING AUTOENCODERS  Our goal is to learn a single representation that combines the best aspects of representations learnt at different levels of noise. The scheduled denoising autoencoder (ScheDA) aims to do this by training a single DA sequentially using a schedule of noise levels, such that ν0 > ··· > νT ≥ 0. The initial noise level ν0 is chosen to be a high noise level that corrupts most of the input. The ﬁnal noise level νT is chosen to be lower than the optimal noise level for a standard DA, i.e., chosen via a held-out validation set or by cross-validation. In pseudocode,  while θ not converged do  Take a stochastic gradient step on (1), using noise level ν0.  end while for t in 1, . . . , T do νt := νt−1 − ∆ν for K steps do  end for  end for  Take a stochastic gradient step on (1), using noise level νt.  This method is reminiscent of deterministic annealing (Rose, 1998), which has been applied to clustering problems, in which a sequence of clustering problems are solved at a gradually lowered noise level. However, the meaning of “noise level” is very different. In deterministic annealing, the noise is added to the mapping between inputs and cluster labels. This is to encourage data points to move between cluster centroids early in the optimization process. ScheDA is also conceptually related to curriculum learning (Bengio et al., 2009) and continuation methods more generally (Allgower and Georg, 1980). In curriculum learning, the network is trained on a sequence of learning problems that have the property that the earlier tasks are “easier” than later tasks. In ScheDA, it is less obvious that the earlier tasks are easier since the lowest achievable reconstruction error is actually higher at the earlier high noise levels than at the later low noise levels. We observe this in practice (cf. Figure 1). On the other hand, we found that, for a given learning rate, the reconstruction error converges to a local minimum faster with large ν’s (cf. the right panel of Figure 1). Thus, even though the problems that ScheDA starts with are harder in absolute terms, ﬁnding the local minima for these problems is easier. This can be understood given the insight provided by the work of Vincent (2011), who has shown that, for a DA trained with Gaussian noise and squared error, minimising reconstruction error is equivalent to matching the score (with respect to the input) of a nonparametric Parzen density estimator of the data, which depends on the level of noise. An implication of this viewpoint is that if the density learnt by the Parzen density estimator is harder to represent, it makes the DA learning problem harder too. Convolving the data with a high level of noise transforms the data generating distribution into a much smoother one, which is easier to capture. As the noise level is reduced, the density becomes more multimodal and harder to represent.  3  Published as a conference paper at ICLR 2015  Table 1: Test errors on CIFAR-10 data set. Each ScheDA is characterised by the sequence of noise levels it was trained with and the number of epochs for which it was trained at each level of noise after the ﬁrst noise level switch. Each row shows the best DA and the best ScheDA for a given number of hidden units, choosing the learning rate, the number of training epochs and the noise level using the error on the validation set. best DA test error hidden units 45.34% 41.95% 38.64%  0.7→0.65→ . . . →0.2→0.15, K=100  0.2→0.15→0.1→0.05, K=50  test error 43.01% 40.1% 36.77%  0.4→0.3→0.2, K=50  best ScheDA  1000 2000 5000  0.4 0.3 0.1  Figure 1: Experimental results with CIFAR-10 for 2000 hidden units. Test errors (left) and recon- struction errors on training set (right) as a function of the number of epochs. Dashed lines indicate a point when the level of noise was changed. Test errors were measured every 100 training epochs initially (during the ﬁrst 2000 epochs). After each change of the noise level, test error was measured after the ﬁrst, the third, the ﬁfth epoch and then after every ten epochs. Reconstruction errors were measured after each training epoch for data corrupted with the noise level used for training at that epoch. For clarity, we do not show the results of DA (0.2), DA (0.4) and DA (0.6), which yield higher test error than DA (0.3).  4 EXPERIMENTS  We evaluate ScheDA on two different data sets, an image classiﬁcation data set (CIFAR-10), and a text classiﬁcation data set (Amazon product reviews, results in the supplementary material). We use a procedure similar to one used, for example, by Coates et al. (2011)1. That is, in all experiments, we ﬁrst learn the representation in an unsupervised fashion and then use the learnt representation within a linear classiﬁer as a measure of its quality. In both experiments, in the unsupervised feature learning stage, we use masking noise as the corruption process, a sigmoid encoder and decoder and cross entropy loss (Equation 2)2 following Vincent et al. (2008; 2010). All experiments with learning the representations were implemented using the Theano library (Bergstra et al., 2010). To do optimisation, we use stochastic gradient descent with mini-batches. For the classiﬁcation step, we use L2-regularised logistic regression implemented in LIBLINEAR (Fan et al., 2008), with the regularisation parameter chosen to minimise the validation error.  4.1  IMAGE RECOGNITION  We use the CIFAR-10 (Krizhevsky, 2009) data set for experiments with vision data. This data set consists of 60000 colour images spread evenly between ten classes. There are 50000 training and validation images and 10000 test images. Each image has a size of 32x32 pixels and each pixel has three colour channels, which are represented with a number in {0, . . . , 255}. We divide the training and validation set into 45000 training instances and 5000 validation instances. The only preprocessing step we use is dividing the intensity of every pixel by 255 to get numbers in [0, 1].  1We do not use any form of pooling, keeping our setup invariant to the permutation of the features. 2We also tried a rectiﬁed linear encoder combined with sigmoid decoder on the Amazon data set. The  results were very similar, so we do not show them here.  4  10050010001500200025003000404244464850epochtest error  0.10.30.50.70.7−0.65−...−0.050.7−0.6−...−0.110050010001500200025003000170017101720173017401750176017701780epochreconstruction error on training dataPublished as a conference paper at ICLR 2015  DA (0.7)  ScheDA (0.7→0.6→0.5)  ScheDA (0.7→0.6→...→0.1) Figure 2: A sample of ﬁlters (rows of the matrix W) learnt from CIFAR-10 with DA (0.7) and ScheDAs starting with ν0 = 0.7. All sets of ﬁlters are similar, but those that were post-trained with low level of noise are sharper. With schedules that end at a lower level of noise, the ﬁlters become more local but not as much as when only training with a low level of noise (cf. Figure 3).  ScheDA (0.7→0.6→...→0.3)  To get the strongest possible DAs trained with a single noise level, we choose the noise level, learning rate and number of training epochs in order to minimise classiﬁcation error on the val- idation set. We try all combinations of the following values of the parameters: noise level ∈ {0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05}, learning rate ∈ {0.002, 0.01, 0.05}, number of training epochs ∈ {100, 200, . . . , 2000}. We choose these parameters separately for each size of the hidden layer ∈ {1000, 2000, 5000}. To train ScheDA models, we ﬁrst pick the best DA for each level of noise we consider, optimising the learning rate and the number of training epochs with respect to the validation error. Starting from the best DA for given ν0, we continue the training, lowering the level of noise from νt−1 to νt := νt−1 − ∆ν and training the model for K epochs. We repeat this noise reduction T times. In our experiments we consider ∆ν ∈ {0.05, 0.1} and K ∈ {50, 100}. We use the learning rate of 0.01 for this stage as it turned out to always be optimal or very close to optimal for the standard DA3. We pick the combination of parameters (ν0, ∆ν, K) and the number of noise reduction steps, T , using the validation error of a classiﬁer after the last training epoch at each level of noise νt. We denote a DA trained with the level of noise ν by DA (ν) and ScheDA trained with a schedule of noise levels ν0, ν1, ..., νT by ScheDA (ν0→ν1→...→νT ). The error obtained by the classiﬁer trained with raw pixel data equals 59.78%. A summary of the performance of DAs and ScheDAs for each number of hidden units can be found in Table 1. For each size of the hidden layer we tried, ScheDA easily outperforms DA, with a relative error reduction of about 5%. Our best model achieves the error of 36.77%. Interestingly, our method is very robust to the parameters (ν0, ∆ν, K) of the schedule. See Section 4.1.1 for more details. Those results do not use supervised ﬁne-tuning. Supervised ﬁne-tuning of our best model yielded the error of 35.7%, which, to our knowledge, is the lowest ever reported error for permutation invariant CIFAR- 10, outperforming Le et al. (2013) who achieved the error of 36.9% and Memisevic et al. (2014), who achieved the error of 36.1%. We describe the details of our supervised ﬁne-tuning procedure in the supplementary material. Figure 1 shows the test errors and reconstruction errors on the training data as a function of the training epoch for selected DAs and ScheDAs with 2000 hidden units. It is worth noting that, even though the schedules exhibiting the best performance go below the optimal ν for DA, training for many iterations with a level of noise that is too low hurts performance (see the ﬁnal parts of the schedules shown in Figure 1). This may be due to the fact that structures learnt at low noise levels are too local to help generalisation. The performance of our method does not appear to be solely due to better optimisation of the training objective. For example, DA (0.1) trained for 3000 epochs has a lower reconstruction error on the training data than the ScheDA (0.7→0.6→...→0.1) shown in Figure 1, while the test error it yields is higher by about 5%. The features learnt with ScheDA are visibly noticeably different from those learnt with any single level of noise as they contain a mixture of features that could be found for various values of ν.  3Note that tuning this parameter could only help ScheDAs and would not affect the baselines.  5  Published as a conference paper at ICLR 2015  DA (0.1)  ScheDA (0.7→0.6→0.5→0.4→0.3→0.2→0.1)  Figure 3: Samples of ﬁlters (rows of the matrix W) learnt from CIFAR-10 with a low ﬁxed noise level (left) and ﬁlters learnt with an initially high level of noise and post-trained with a schedule of lower levels of noise (right). These two sets of ﬁlters are visually very different. There are fewer edge detector ﬁlters among these learnt only with a low level of noise and those that are edge detectors are more local.  1000 hidden units  2000 hidden units  5000 hidden units  Figure 4: Comparison of different schedules for ScheDA. For each size of hidden layer, the vertical line indicates the optimal level of noise for a DA, i.e., the level of noise which allowed to train representation yielding the lowest error on the validation set, while the horizontal line indicates the test error obtained with this representation.  Figure 2 and Figure 3 display visualisations of the ﬁlters learnt by a DA and ScheDA. It can be seen in Figure 2 that when training ScheDA the features across the consecutive levels of noise are similar, which indicates that it is the initial training with a higher level of noise that puts the optimisation procedure in a basin of attraction of a different local minimum, which would not be otherwise achievable. This is shown in Figure 3, which visualises features learnt by a DA trained only with a low noise level, DA (0.1), and those learnt with ScheDA (0.7→0.6→...→0.1). The set of features learnt by DA (0.1) contains more noisy features and very few edge detectors, which are all very local. In contrast, features learnt with the schedule contain a more diverse set of edge detectors which can be learnt with high noise level (Figure 2) as well as some blob detectors which can be learnt with a low noise level (Figure 3).  4.1.1 ROBUSTNESS TO THE CHOICE OF SCHEDULE  Our method is very robust to the choice of the parameters of the schedule, ν0, ∆ν and K. Figure 4 shows the performance of ScheDA for different values of those parameters. For 1000 and 2000 hidden units for all schedules ScheDA performed better than the best DA, as long as the initial level of noise ν0 was not lower than the level of noise yielding the best DA. For 5000 hidden units, ScheDA also performed better than DA, except for the model trained with ν0 = 0.7. These results suggest than ScheDA’s performance is superior to DA as long as the initial level of noise is not too large and not below the optimal level of noise for DA.  6  0.10.20.30.40.50.60.7444648505254test errorν00.10.20.30.40.50.60.738404244464850ν00.10.20.30.40.50.60.736384042444648ν0  ∆ν = 0.1, K = 50∆ν = 0.1, K = 100∆ν = 0.05, K = 50∆ν = 0.05, K = 100Published as a conference paper at ICLR 2015  We also examined whether it is necessary for the schedule to be decreasing. To investigate this, we trained ScheDAs using the same procedure as before, except that the levels of noise were increasing. The networks had 2000 hidden units and they started with the best DA (over the learning rate and the number of training epochs) for each ν0. We considered ν0 ∈ {0.1, 0.2, 0.3, 0.4}, K ∈ {50, 100} and ∆ν ∈ {0.05, 0.1}. The largest possible ﬁnal noise level νT was 0.7. To evaluate all combinations of these hyperparameters (ν0, νT , ∆ν and K) we used the validation set. We set the learning rate to 0.01 as it worked optimally or very close to optimally in all previous experiments and we also used this value in the experiments with decreasing schedules. The best model we obtained this way used the schedule 0.3→0.35 and K = 100. Its test error was 41.97%, just a little worse than a DA trained with ν = 0.3 (achieving the test error of 41.95%). For comparison, ScheDA (0.1→0.2→0.3) with K = 100 yielded the test error of 44.99% and ScheDA (0.3→0.4→0.5→0.6→0.7) with K = 100 yielded the test error of 46.7%. These results provide some evidence that the initial noise levels puts the optimisation procedure in a basin of attraction of a local minimum that can be favourable, as we observe for ScheDA when starting training with higher noise levels, or detrimental, as we see here.  4.1.2 CONCATENATING SETS OF FEATURES LEARNT WITH DIFFERENT NOISE LEVELS  To explain the results above, we examine whether features learnt with different noise levels contain different information about the data. To explore this, we trained two sets of representations with 2000 hidden units independently with a standard DA. DAs in the ﬁrst set were initialised with a randomly drawn set of parameters θ1 and DAs in the second set were initialised with a different ran- domly drawn set of parameters θ2. Each set contained representations learnt with ν = 0.1, ν = 0.2, ..., ν = 0.7. Then we gathered all 49 possible pairs of representations between the two sets and con- catenated representations within each pair, creating representations with 4000 features. The errors yielded by classiﬁers using these representations can be found in Figure 5. The important obser- vation here is that, even though concatenating two representations learnt with the same ν but with different initialisations results in a better representation (cf. Figure 1), concatenating representations with different ν’s yields even lower errors. This is another piece of evidence strengthening our hypothesis that having both local and global features in the representation learnt with ScheDA helps classiﬁcation. Note, however, that even though concatenating representations learnt with different ν helps, ScheDA is clearly a better model. For a fair comparison, we trained ScheDA with 4000 units using, which matches the number of hidden units in the concatenated architecture. While the best concatenated DA achieved 39.33% (cf. Figure 5), the best ScheDA achieved 37.77%.  4.1.3 COMPARING SETS OF FEATURES  Figure 5: Test errors yielded by representa- tions constructed by concatenating representa- tions learnt with various levels of noise. This allows representations that are otherwise weak separately to achieve low test errors, e.g. for ν = 0.1 and ν = 0.5 (cf. Figure 1).  Having conﬁrmed that using both features learnt with different levels of noise indeed helps classi- ﬁcation, we experimentally verify the hypothesis that the ﬁnal representation trained with ScheDA (0.7→0.6→...→0.1) contains both features similar to those learnt with low levels of noise (local fea- tures) and high levels of noise (global features). Intuitively, two features are similar if they are active for the same set of inputs. We deﬁne the activation vector ai for feature i as the vector containing the activation of the feature over all the data points. More formally, if wi is the weight vector for feature i, bi is the bias for feature i and xn is a data item, the activation vector is ai = [ai1, . . . , aiN ], where ain = sigmoid(wixn + bi). Here N is the total number of data items, the total number of features is I. We compute the activation vector for all features from eight different autoencoders: DA (0.1), DA (0.2), ..., DA (0.7) and ScheDA (0.7→0.6→...→0.1). We denote the resulting activation vectors a0.1 , i , respectively. Now for each feature in ScheDA (0.7→0.6→...→0.1) we can ﬁnd the ..., a0.7  and aS  i  i  7  0.10.20.30.40.50.60.70.10.20.30.40.50.60.7noise level, the first set of representationsnoise level, the second set of representations43.6741.2240.0540.1139.8240.4841.1540.7740.0339.3539.3339.3739.4339.934039.7440.3540.2139.9839.8240.3540.239.5340.1540.3740.7440.5441.2539.7339.5540.0140.6441.3242.3342.1940.1639.5740.1140.4441.6442.0943.2541.3640.2740.4341.1442.1243.0644.35Published as a conference paper at ICLR 2015  Table 2: Comparison of features of ScheDA and DA. The ﬁrst row shows how many ScheDA (0.7→0.6→...→0.1) features, out of 2000 in total, were closest to a feature learnt by DA (0.1), ..., DA (0.7). It demonstrates that ScheDA combines information that would be learnt from DAs at varying noise levels. The second and third row are baselines for comparison (see text for details).  ScheDA DA* (0.1) DA* (0.7)  DA (0.1) DA (0.2) DA (0.3) DA (0.4) DA (0.5) DA (0.6) DA (0.7)  374 1247 25  550 465 30  444 167 72  299 54 165  169 21 308  92 12 587  72 7 813  i , a0.1  i , a0.7  (cid:80)I  j ), cos(aS  j ), ..., cos(aS  i=1 1[maxjcos(aS  i , a0.2 i , a0.1 j ) > {maxjcos(aS  closest feature among those learnt with DA (0.1), DA (0.2), ..., DA (0.7). To do this, we compute cosine similarities cos(aS j ) for all pairs (i, j). Finally, we compute C0.1, the number of ScheDA features that are closest to a feature from DA (0.1) as C0.1 = j )}] and similarly for C0.2, C0.3, ..., C0.7. To see how much ScheDA differs in that respect from the standard DA trained only at the ﬁnal level of noise for ScheDA, we also performed the same pro- cedure as described above, but comparing to features learnt by DA* (0.1), which is the same as DA (0.1) but starting from a different random initialisation. We found that ScheDA contains more features similar to those learnt with higher noise levels than DA* (0.1) (see Table 2). This conﬁrms our expectation that the ScheDA representation retains a large number of more global features from the earlier noise levels. We also put the same numbers for DA* (0.7) for comparison.  j ), ..., maxjcos(aS  j ), maxjcos(aS  i , a0.3  i , a0.7  i , a0.2  5 COMPOSITE DENOISING AUTOENCODER  The observation that more diverse representations lead to a better discriminative performance can be exploited more explicitly than in ScheDA. Instead of training all of the hidden units with a sequence of noise levels, we can partition the hidden units, training each subset of units with a different noise level. This can be done by deﬁning the hidden representation and the reconstruction to be y = [f (˜xν1W1 + b1) , . . . , f (˜xνS WS + bS)] and z = g  (cid:16)(cid:80)S  s + b(cid:48)(cid:17)  s=1 f (˜xνsWs + bs) WT  ,  where ˜xνs denotes an input x corrupted with the level of noise νs. We call this a composite DA. Our pre- liminary experiments show that, even when using only two noise levels, it outperforms a standard DA and per- forms on par with ScheDA. Successful learning of the parameters is more complicated though. We found that standard SGD (updating all parameters at each epoch) performs much worse than a version of the SGD al- ternating between updating parameters associated with the two levels of noise. See Figure 6.  6 DISCUSSION  Figure 6: Test errors for a composite de- noising autoencoder using two levels of noise, ν = 0.2 and ν = 0.4, and with 2000 hidden units divided equally between the two levels of noise. Dashed lines indicate the epochs when optimisation switched be- tween updating different sets of parame- ters.  We have introduced a simple, yet powerful extension of an important and commonly used model for learn- ing representations and justiﬁed its superior perfor- mance by its ability to learn a more diverse set of features than the standard denoising autoencoder. Instead of learning a denoising autoencoder with just a single level of noise, we exploit the fact that various levels of noise yield different features, which are more global for large values of ν. Start- ing the training with a high level of noise enables the algorithm to learn these global features ﬁrst, which are partially retained when the level of noise is lower and the model is learning more local dependencies. Erhan et al. (2010) investigated why unsupervised pretraining helps learning a deep neural network and found that the set of functions learnt by pretrained sigmoid neural networks is very different from  8  010002000300040004041424344454647training epochtest error  standard SGDalternating SGDPublished as a conference paper at ICLR 2015  the ones that are learnt without unsupervised pretraining. In fact, we have investigated a related question, why does unsupervised pretraining help unsupervised pretraining? Or, more precisely, since we are getting a large boost of performance even without supervised ﬁne-tuning, why does unsupervised pretraining help unsupervised training? One of their conclusions was that, when using their architecture, unsupervised pretaining puts the optimisation procedure in a basin of attraction of a local minimum that would not otherwise be found. This is very similar to what we observe in our experiments. We often ﬁnd that a DA trained with a given level of noise ν can have a lower reconstruction error than ScheDA trained with the ﬁnal level of noise ν, yet ScheDA is performing better in terms of classiﬁcation error. The ﬁlters at the minima for DA and ScheDA also look very different (cf. Figure 3). This way of training a denosing autoencoder is related to walkback training (Bengio et al., 2013) in the sense that at the initial stages of training both methods attempt to correctly reconstruct corrupted examples that lie further from the data manifold. It is different though as we do not require the loss to be interpretable as log-likelihood and we do not perform any sampling from the denoising autoen- coder. Additionally, Chandra and Sharma (2014) independently tried an idea similar to ScheDA, but they were unable to show consistent improvement over the results of Vincent et al. (2010). There is a number of ways this work can be extended. Primarily, ScheDA can be stacked, which would likely improve our results. More generally, our results suggest that large improvements can be achieved by combining diverse representations, which we aim to exploit in composite denoising autoencoders. Finally, we would like to point out that the main observation we make, namely, that it is beneﬁcial for the feature learning algorithm to learn more global features ﬁrst and then to proceed to learning more local ones, is very general and it is likely that scheduling is applicable to other approaches to feature learning. Indeed, in the case of dropout (Hinton et al., 2014), Rennie et al. (2014) have, independently from our work, explored the use of a schedule to decrease the dropout rate.  ACKNOWLEDGMENTS  We thank Amos Storkey, Vittorio Ferrari, Iain Murray, Chris Williams and Ruslan Salakhutdinov for insightful comments on this work.  REFERENCES Eugene L. Allgower and Kurt Georg. Numerical continuation methods. An introduction. Springer-  Verlag, 1980.  Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from  examples without local minima. Neural Networks, 2, 1989.  Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training  of deep networks. In NIPS, 2007.  Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In  ICML, 2009.  Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders  as generative models. In NIPS, 2013.  James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler. SciPy, 2010.  John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, Bollywood, boom-boxes and  blenders: Domain adaptation for sentiment classiﬁcation. In ACL, 2007.  B. Chandra and Rajesh Kumar Sharma. Adaptive noise schedule for denoising autoencoder.  Neural Information Processing, volume 8834 of Lecture Notes in Computer Science. 2014.  In  Adam Coates, Andrew Y. Ng, and Honglak Lee. An analysis of single-layer networks in unsuper-  vised feature learning. In AISTATS, 2011.  9  Published as a conference paper at ICLR 2015  Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and  Samy Bengio. Why does unsupervised pre-training help deep learning? JMLR, 11, 2010.  Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR:  A library for large linear classiﬁcation. JMLR, 9, 2008.  Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer networks. In AISTATS,  2011a.  Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment  classiﬁcation: A deep learning approach. In ICML, 2011b.  Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief  nets. Neural Computation, 18(7), 2006.  Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.  Improving neural networks by preventing co-adaptation of feature detectors. JMLR, 15, 2014.  Yan Karklin and Eero P. Simoncelli. Efﬁcient coding of natural images with a population of noisy  linear-nonlinear neurons. In NIPS, 2011.  Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University  of Toronto, 2009.  Quoc Le, Tam´as Sarl´os, and Alexander Smola. Fastfood-computing Hilbert space expansions in  loglinear time. In ICML, 2013.  Roland Memisevic, Kishore Konda, and David Krueger. Zero-bias autoencoders and the beneﬁts of  co-adapting features. arXiv:1402.3337v2, 2014.  Gr´egoire Mesnil, Yann Dauphin, Xavier Glorot, Salah Rifai, Yoshua Bengio, Ian J. Goodfellow, Er- ick Lavoie, Xavier Muller, Guillaume Desjardins, David Warde-Farley, Pascal Vincent, Aaron C. Courville, and James Bergstra. Unsupervised and transfer learning challenge: a deep learning approach. In ICML Unsupervised and Transfer Learning Workshop, 2012.  Steven Rennie, Vaibhava Goel, and Samuel Thomas. Annealed dropout training of deep networks.  IEEE Workshop on Spoken Language Technology, 2014.  Kenneth Rose. Deterministic annealing for clustering, compression, classiﬁcation, regression, and  related optimization problems. Proceedings of the IEEE, 86:2210–2239, 1998.  H. Sebastian Seung. Learning continuous attractors in recurrent networks. In NIPS, 1998.  Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Compu-  tation, 23, 2011.  Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and  composing robust features with denoising autoencoders. In ICML, 2008.  Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. JMLR, 2010.  10  Published as a conference paper at ICLR 2015  SUPPLEMENTARY MATERIAL  SAMPLING THE LEVEL OF NOISE  As an alternative to a schedule, which sequentially changes the level of noise, we tried to sample a different ν for each mini-batch. We tried two variants of this idea: sampling ν uniformly from a continuous interval [0.1, 0.7], and sampling ν from a discrete distribution over the values in the set {0.1, 0.2, . . . , 0.7}. Replicating the setup described in Section 4.1 for a DA with 2000 hidden units, the ﬁrst method obtained test error of 44.85% and the second one obtained the test error 46.83%. Thus, both have performed much worse than DA (0.3). The result of this experiment provides evidence that training a denoising autoencoder with a sequence of noise levels is important for the success of our method.  SUPERVISED FINE-TUNING  For completeness, we also tried training a supervised single-layer neural network using param- eters of the encoder as the initialisation of the parameters of the hidden layer of the network. We did that for all models in Table 1. That is, for each size of the hidden layer, we take the best DA and the best ScheDA trained in an unsupervised manner and perform supervised ﬁne- tuning of their parameters. The learning rate, the same for all parameters, was chosen from the set {0.00125, 0.00125· 2−1, . . . , 0.00125· 2−4} and the maximum number of training epochs was 2000 (we computed the validation error after each epoch). We report the test error for the combination of the learning rate and the number of epochs yielding the lowest validation error. The numbers are shown in Table 3. Fine-tuning makes the performance of DA and ScheDA much more similar, but the advantage of ScheDA is consistent and its magnitude grows with the size of the hidden layer.  Table 3: Test errors on CIFAR-10 data set for the best DA and ScheDA models trained without supervised ﬁne-tuning and their ﬁne-tuned versions.  hidden units  1000 2000 5000  DA  no ﬁne-tuning  45.34% 41.95% 38.64%  ﬁne-tuning  39.55% 36.85% 36.47%  ScheDA  no ﬁne-tuning  ﬁne-tuning  43.01% 40.1% 36.77%  39.44% 36.22% 35.7%  SENTIMENT CLASSIFICATION  We also evaluate our idea on a data set of product reviews from Amazon (Blitzer et al., 2007), adapting the experimental setting used with the CIFAR-10 data set. The version of the data set we are using contains reviews of products from six domains4 corresponding to high-level categories on Amazon.com. The goal is to classify whether a review is positive or negative based on the review text. For computational reasons, we keep only 3000 most popular words in the entire data set, transforming each example into a binary vector indicating presence or absence of a word. We divide the data set into a training set of 10000 labelled examples and 35000 unlabelled examples, a validation set of 10000 labelled examples and a test set of 10000 labelled examples, each of them consisting equal fractions of positive and negative labelled examples. The six domains are mixed among training, validation and test examples. We set the number of hidden units to 2000. The baseline, logistic regression trained with raw data obtains the test error of 14.79%, while the best DA (0.6) yields 13.61% and the best ScheDA (0.7→0.6) yields 13.41% error. The relative error reduction is smaller than on the image data, which is not surprising since the raw features are here a much stronger baseline and the improvement obtained by the standard DA is relatively smaller too. Smaller relative error reduction can be explained by the fact that the DA performance varies less with the level of noise for this data set. While the test error for the best set of features learnt by DA (0.6) was 13.61%, the worst, DA (0.1), yielded the error of 13.9%. This result suggests a simple diagnostic for whether ScheDA is likely to be effective, namely, to check whether the DA validation error is sensitive to the noise level.  4books, dvd, electronics, kitchen & housewares, music, video  11  ",
1412.6575,2015,Embedding Entities and Relations for Learning and Inference in Knowledge Bases,"['Embedding Entities and Relations for Learning and Inference in Knowledge Bases', 'Bishan Yang', 'Scott Yih', 'Xiaodong He', 'Jianfeng Gao', 'and Li Deng']",https://arxiv.org/pdf/1412.6575,"5 1 0 2     g u A 9 2         ] L C . s c [      4 v 5 7 5 6  .  2 1 4 1 : v i X r a  Published as conference paper at ICLR 2015  EMBEDDING ENTITIES AND RELATIONS FOR LEARN- ING AND INFERENCE IN KNOWLEDGE BASES  Bishan Yang1˚, Wen-tau Yih2, Xiaodong He2, Jianfeng Gao2 & Li Deng2 1Department of Computer Science, Cornell University, Ithaca, NY, 14850, USA bishan@cs.cornell.edu 2Microsoft Research, Redmond, WA 98052, USA {scottyih,xiaohe,jfgao,deng}@microsoft.com  ABSTRACT  We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a uniﬁed learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel ap- proach that utilizes the learned relation embeddings to mine logical rules such as BornInCitypa, bq ^ CityInCountrypb, cq ùñ N ationalitypa, cq. We ﬁnd that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is char- acterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of- the-art conﬁdence-based rule mining approach in mining Horn rules that involve compositional reasoning.  1  INTRODUCTION  Recent years have witnessed a rapid growth of knowledge bases (KBs) such as Freebase1, DBPe- dia (Auer et al., 2007), and YAGO (Suchanek et al., 2007). These KBs store facts about real-world entities (e.g. people, places, and things) in the form of RDF triples2 (i.e. (subject, predicate, ob- ject)). Today’s KBs are large in size. For instance, Freebase contains millions of entities and billions of facts (triples) involving a large variety of predicates (relation types). Such large-scale multi- relational data provide an excellent potential for improving a wide range of tasks, from information retrieval, question answering to biological data mining. Recently, much effort has been invested in relational learning methods that can scale to large knowl- edge bases. Tensor factorization (e.g. (Nickel et al., 2011; 2012)) and neural-embedding-based models (e.g. (Bordes et al., 2013a;b; Socher et al., 2013)) are two popular kinds of approaches that learn to encode relational information using low-dimensional representations of entities and rela- tions. These representation learning methods have shown good scalability and reasoning ability in terms of validating unseen facts given the existing KB. In this work, we focus on the study of neural-embedding models, where the representations are learned using neural networks with energy-based objectives. Recent embedding models TransE (Bordes et al., 2013b) and NTN (Socher et al., 2013) have shown state-of-the-art predic- tion performance compared to tensor factorization methods such as RESCAL (Nickel et al., 2012). They are similar in model forms with slight differences on the choices of entity and relation rep- resentations. Without careful comparison, it is not clear how different design choices affect the  ˚Work conducted while interning at Microsoft Research. 1http://freebase.com 2http://www.w3.org/TR/rdf11-concepts/  1  Published as conference paper at ICLR 2015  learning results. In addition, the performance of the embedding models are evaluated on the link prediction task (i.e. predicting the correctness of unseen triples). This only indirectly shows the meaningfulness of low-dimensional embeddings. It is hard to explain what relational properties are being captured and to what extent they are captured during the embedding process. We make three main contributions in this paper. (1) We present a general framework for multi- relational learning that uniﬁes most multi-relational embedding models developed in the past, in- cluding NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b). (2) We empirically evaluate different choices of entity representations and relation representations under this framework on the canonical link prediction task and show that a simple bilinear formulation achieves new state-of- the-art results for the task (a top-10 accuracy of 73.2% vs. 54.7% by TransE when evaluated on Freebase). (3) We propose and evaluate a novel approach that utilizes the learned embeddings to mine logical rules such as BornInCitypa, bq ^ CityOf Countrypb, cq ùñ N ationalitypa, cq. We show that such rules can be effectively extracted by modeling the composition of relation embeddings, and that the embeddings learned from the bilinear objective are particularly good at capturing the compositional semantics of relations via matrix multiplication. Furthermore, we demonstrate that our embedding-based approach outperforms a state-of-the-art rule mining system AMIE (Gal´arraga et al., 2013) on mining rules that involve compositional reasoning. The rest of this paper is structured as follows. Section 2 discusses related work. Section 3 presents the general framework for learning multi-relational representations. Sections 4 and 5 present two inference tasks: a canonical link prediction task and a novel rule extraction task where the learned embeddings are empirically evaluated. Section 6 concludes the paper.  2 RELATED WORK  Multi-relational learning has been an active research area for the past couple of years. Traditional statistical learning approaches (Getoor & Taskar, 2007) such as Markov-logic networks (Richard- son & Domingos, 2006) usually suffer from scalability issues. More recently, various types of representation learning methods have been proposed to embed multi-relational knowledge into low- dimensional representations of entities and relations, including tensor/matrix factorization (Singh & Gordon, 2008; Nickel et al., 2011; 2012), Bayesian clustering framework (Kemp et al., 2006; Sutskever et al., 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al., 2013a;b; Socher et al., 2013). Our work focuses on the study of neural-embedding models as they have shown good scalability and strong generalizability on large-scale KBs. Existing neural embedding models (Bordes et al., 2013a;b; Socher et al., 2013) all represent entities as low-dimensional vectors and represent relations as operators that combine the representations of two entities. They differ in different parametrization of relation operators. For instance, given two entity vectors, the model of Neural Tensor Network (NTN) (Socher et al., 2013) represents each rela- tion as a bilinear tensor operator followed by a linear matrix operator. The model of TransE (Bordes et al., 2013b), on the other hand, represents each relation as a single vector that linearly interacts with the entity vectors. Likewise, variations on entity representations also exist. Most methods rep- resent each entity as a unit vector while NTN (Socher et al., 2013) represent entities as an average of word vectors and initializing word vectors with pre-trained vectors from external text corpora. There has not been work that closely examines the effectiveness of these different design choices. Our work on embedding-based rule extraction presented in part of this paper is related to the ear- lier study on logical inference with learned continuous-space representations. Much existing work along this line focuses on learning logic-based representations for natural language sentences. For example, Socher et al. (2012) builds a neural network that recursively combines word representa- tions based on parse tree structures and shows that such neural network can simulate the behavior of conjunction and negation. Bowman (2014) further demonstrates that recursive neural network can capture certain aspects of natural logical reasoning on examples involving quantiﬁers like some and all. Recently, Grefenstette (2013) shows that in theory most aspects of predicate logic can be sim- ulated using tensor calculus. Rockt¨aschel et al. (2014) further implements the idea by introducing a supervised objective that trains embeddings to be consistent with given logical rules. The evalua- tion was conducted on toy data and uses limited logical forms. Different from these earlier studies, we propose a novel approach to utilizing embeddings learned without explicit logical constraints to directly mine logical rules from KBs. We demonstrate that the learned embeddings of relations  2  Published as conference paper at ICLR 2015  can capture the compositional semantics of relations. Moreover, we systematically evaluate our ap- proach and compare it favorably with a state-of-the-art rule mining approach on the rule extraction task on Freebase.  3 MULTI-RELATIONAL REPRESENTATION LEARNING  In this section, we present a general neural network framework for multi-relational representation learning. We discuss different design choices for the representations of entities and relations which will be empirically compared in Section 4. Given a KB that is represented as a list of relation triplets pe1, r, e2q (denoting e1 (the subject) and e2 (the object) that are in a certain relationship r), we want to learn representations for entities and relations such that valid triplets receive high scores (or low energies). The embeddings can be learned via a neural network. The ﬁrst layer projects a pair of input entities to low dimensional vectors, and the second layer combines these two vectors to a scalar for comparison via a scoring function with relation-speciﬁc parameters.  3.1 ENTITY REPRESENTATIONS  Each input entity corresponds to a high-dimensional vector, either a “one-hot” index vector or a “n-hot” feature vector. Denote by xe1 and xe2 the input vectors for entity e1 and e2, respectively. Denote by W the ﬁrst layer projection matrix. The learned entity representations, ye1 and ye2 can be written as  ˘  ˘  `  `  ye1 “ f  Wxe1  , ye2 “ f  Wxe2  where f can be a linear or non-linear function, and W is a parameter matrix, which can be randomly initialized or initialized using pre-trained vectors. Most existing embedding models adopt the “one-hot” input vectors except for NTN (Socher et al., 2013) which represents each entity as an average of its word vectors. This can be viewed as adopting “bag-of-words” vectors as input and learning a projection matrix consisting of word vectors.  3.2 RELATION REPRESENTATIONS  The choice of relation representations reﬂects in the form of the scoring function. Most of the existing scoring functions in the literature can be uniﬁed based on a basic linear transformation ga r , a bilinear transformation gb  r or their combination, where ga  r are deﬁned as  ˆ  ˙  r and gb rpye1 , ye2q “ yT  e1  and gb  Brye2,  (1)  rpye1, ye2q “ AT ga  r  ye1 ye2  which Ar and Br are relation-speciﬁc parameters.  Models  Distance (Bordes et al., 2011)  Single Layer (Socher et al., 2013)  TransE (Bordes et al., 2013b)  NTN (Socher et al., 2013)  Br - - I Tr  ` ` ` `  AT r1 ´QT r QT r2 QT r1 QT r ´VT r2 VT r r1 QT QT r2  ˘ ˘ ˘ ˘  Scoring Function ´||ga rpye1 , ye2q||1 ` r tanhpga uT rpye1 , ye2q ´ 2gb rpye1 , ye2q ` gb ga r tanh  ˘ rpye1 , ye2qq rpye1 , ye2q ` ||Vr||2 2q  rpye1 , ye2q  ´p2ga uT  Table 1: Comparisons among several multi-relational models in their scoring functions.  In Table 1, we summarize several popular scoring functions in the literature for a relation triplet pe1, r, e2q, reformulated in terms of the above two functions. Denote by ye1, ye2 P Rn two entity vectors. Denote by Qr1, Qr2 P Rnˆm and Vr P Rn matrix or vector parameters r . Denote by Tr P Rnˆnˆm tensor parameters for bilinear trans- for linear transformation ga r. I P Rn is an identity matrix. ur P Rm is an additional parameter for rela- formation gb 2 “ tion r. The scoring function for TransE (L2 formulation) is derived from ||ye1 ´ ye2 ` Vr||2 2, where ye1 and ye2 are unit vectors. 2V T Note that NTN is the most expressive model as it contains both linear and bilinear relation operators as special cases. In terms of the number of parameters, TransE is the simplest model which only parametrizes the linear relation operators with one-dimensional vectors.  r pye1 ´ ye2q ´ 2yT  ye2 ` ||Vr||2  2 ` ||ye1||2  2 ` ||ye2||2  e1  3  Published as conference paper at ICLR 2015  In this paper, we also consider the basic bilinear scoring function:  rpye1, ye2q “ yT gb  e1  Mrye2  (2)  which is a special case of NTN without the non-linear layer and the linear operator, and uses a 2-d matrix operator Mr P Rnˆn instead of a tensor operator. Such bilinear formulation has been used in other matrix factorization models such as in (Nickel et al., 2011; Jenatton et al., 2012; Garc´ıa-Dur´an et al., 2014) with different forms of regularization. Here, we consider a simple way to reduce the number of relation parameters by restricting Mr to be a diagonal matrix. This results in the same number of relation parameters as TransE. Our experiments in Section 4 demonstrate that this simple formulation enjoys the same scalable property as TransE and it achieves superior performance over TransE and other more expressive models on the task of link prediction. This general framework for relationship modeling also applies to the recent deep-structured semantic model (Huang et al., 2013; Shen et al., 2014a;b; Gao et al., 2014; Yih et al., 2014), which learns the relevance or a single relation between a pair of word sequences. The framework above applies when using multiple neural network layers to project entities and using a relation-independent scoring “ cosrye1pWrq, ye2pWrqs. The cosine scoring function is a special case of function Gr r with normalized ye1, ye2 and with Br “ I. gb  ye1 , ye2  `  ˘  3.3 PARAMETER LEARNING  The neural network parameters of all the models discussed above can be learned by minimizing a margin-based ranking objective , which encourages the scores of positive relationships (triplets) to be higher than the scores of any negative relationships (triplets). Usually only positive triplets are observed in the data. Given a set of positive triplets T , we can construct a set of “negative” triplets T 1 by corrupting either one of the relation arguments, T 1 “ tpe1 1, r, e2q R Tu Y tpe1, r, e1 2q R Tu. Denote the scoring function for triplet pe1, r, e2q as Spe1,r,e2q. The training objective is to minimize the margin-based ranking loss  1, r, e2q|e1  1 P E,pe1  2q|e1  maxtSpe1  2q ´ Spe1,r,e2q ` 1, 0u  1,r,e1  (3)  2 P E,pe1, r, e1 ÿ  LpΩq “  ÿ  pe1,r,e2qPT  pe1 1,r,e1  2qPT 1  4  INFERENCE TASK I: LINK PREDICTION  We ﬁrst conduct a comparison study of different embedding models on the canonical link predic- tion task, which is to predict the correctness of unseen triplets. As in (Bordes et al., 2013b), we formulate link prediction as an entity ranking task. For each triplet in the test data, we treat each entity as the target entity to be predicted in turn. Scores are computed for the correct entity and all the corrupted entities in the dictionary and are ranked in descending order. We consider Mean Re- ciprocal Rank (MRR) (an average of the reciprocal rank of an answered entity over all test triplets), HITS@10 (top-10 accuracy), and Mean Average Precision (MAP) (as used in (Chang et al., 2014)) as the evaluation metrics. We examine ﬁve embedding models in decreasing order of complexity: (1) NTN with 4 tensor slices as in (Socher et al., 2013); (2) Bilinear+Linear, NTN with 1 tensor slice and without the non-linear layer; (3) TransE, a special case of Bilinear+Linear (see Table 1); (4) Bilinear: using scoring function in Eq. (2); (5) Bilinear-diag: a special case of Bilinear where the relation matrix is a diagonal matrix.  Datasets We used the WordNet (WN) and Freebase (FB15k) datasets introduced in (Bordes et al., 2013b). WN contains 151, 442 triplets with 40, 943 entities and 18 relations, and FB15k consists of 592, 213 triplets with 14, 951 entities and 1345 relations. We use the same training/validation/test split as in (Bordes et al., 2013b). We also consider a subset of FB15k (FB15k-401) containing only frequent relations (relations with at least 100 training examples). This results in 560, 209 triplets with 14, 541 entities and 401 relations.  Implementation details All the models were implemented in C# and using GPU. Training was implemented using mini-batch stochastic gradient descent with AdaGrad (Duchi et al., 2011). At each gradient step, we sampled for each positive triplet two negative triplets, one with a corrupted  4  Published as conference paper at ICLR 2015  subject entity and one with a corrupted object entity. The entity vectors are renormalized to have unit length after each gradient step (it is an effective technique that empirically improved all the models). For the relation parameters, we used standard L2 regularization. For all models, we set the number of mini-batches to 10, the dimensionality of the entity vector d “ 100, the regularization parameter 0.0001, and the number of training epochs T “ 100 on FB15k and FB15k-401 and T “ 300 on WN (T was determined based on the learning curves where the performance of all models plateaued.) The learning rate was initially set to 0.1 and then adapted during training by AdaGrad.  4.1 RESULTS  NTN  Blinear+Linear  TransE (DISTADD)  Bilinear  Bilinear-diag (DISTMULT)  FB15k  FB15k-401  WN  MRR HITS@10 MRR HITS@10 MRR HITS@10 0.25 0.30 0.32 0.31 0.35  41.4 49.0 53.9 51.9 57.7  0.24 0.30 0.32 0.32 0.36  40.5 49.4 54.7 52.2 58.5  66.1 91.6 90.9 92.8 94.2  0.53 0.87 0.38 0.89 0.83  Table 2: Performance comparisons among different embedding models  Table 2 shows the results of all compared methods on all the datasets. In general, we observe that the performance increases as the complexity of the model decreases on FB. NTN, the most complex model, provides the worst performance on both FB and WN, which suggests overﬁtting. Compared to the previously published results of TransE (Bordes et al., 2013b), our implementation achieves much better results (53.9% vs. 47.1% on FB15k and 90.9% vs. 89.2% on WN) using the same evaluation metric (HITS@10). We attribute such discrepancy mainly to the different choice of SGD optimization: AdaGrad vs. constant learning rate. We also found that Bilinear consistently provides comparable or better performance than TransE, especially on WN. Note that WN contains much more entities than FB, it may require the parametrization of relations to be more expressive to better handle the richness of entities. Interestingly, we found that a simple variant of Bilinear – BILINEAR-DIAG, clearly outperforms all baselines on FB and achieves comparable performance to Bilinear on WN. Note that BILINEAR-DIAG has the limitation of encoding the difference between a relation and its inverse. Still, as there is a large variety of relations in FB and the average number of training examples seen by each relation is relatively small (compared to WN), the simple form of BILINEAR-DIAG is able to provide good prediction performance. Multiplicative vs. Additive Interactions Note that BILINEAR-DIAG and TRANSE have the same number of model parameters and their difference can be viewed as the operational choices of the composition of two entity vectors – BILINEAR-DIAG uses weighted element-wise dot product (mul- tiplicative operation) and TRANSE uses element-wise subtraction with a bias (additive operation). To highlight the difference, here we use DISTMULT and DISTADD to refer to BILINEAR-DIAG and TRANSE, respectively. Comparisons between these two models can provide us more insights on the effect of two common choices of compositional operations – multiplication and addition for model- ing entity relations. Overall, we observed superior performance of DISTMULT on all the datasets in Table 2. Table 3 shows the HITS@10 score on four types of relation categories (as deﬁned in (Bordes et al., 2013b)) on FB15k-401 when predicting the subject entity and the object entity respectively. We can see that DISTMULT signiﬁcantly outperforms DISTADD in almost all the categories.  Predicting subject entities  1-to-1 70.0 75.5  1-to-n 76.7 85.1  n-to-1 21.1 42.9  n-to-n 53.9 55.2  DISTADD DISTMULT  Predicting object entities  1-to-1 68.7 73.7  1-to-n 17.4 46.7  n-to-1 83.2 81.0  n-to-n 57.5 58.8  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many  Initialization of Entity Vectors In the following, we examine the learning of entity representations and introduce two further improvements: using non-linear projection and initializing entity vectors with pre-trained vectors. We focus on DISTMULT as our baseline and compare it with the two modiﬁcations DISTMULT-tanh (using f “ tanh for entity projection ) and DISTMULT-tanh-EV-init  5  Published as conference paper at ICLR 2015  (initializing the entity parameters with the 1000-dimensional pre-trained entity vectors released by word2vec (Mikolov et al., 2013)) on FB15k-401. We also reimplemented the initialization technique introduced in (Socher et al., 2013) – each entity is represented as an average of its word vectors and the word vectors are initialized using the 300-dimensional pre-trained word vectors released by word2vec. We denote this method as DISTMULT-tanh-WV-init. Inspired by (Chang et al., 2014), we design a new evaluation setting where the predicted entities are automatically ﬁltered according to “entity types” (entities that appear as the subjects/objects of a relation have the same type deﬁned by that relation). This provides us with better understanding of the model performance when some entity type information is provided.  DISTMULT  DISTMULT-tanh  DISTMULT-tanh-WV-init DISTMULT-tanh-EV-init  MRR HITS@10 MAP (w/ type checking) 0.36 0.39 0.28 0.42  58.5 63.3 52.5 73.2  64.5 76.0 65.5 88.2  Table 4: Evaluation with pre-trained vectors  In Table 4, we can see that DISTMULT-tanh-EV-init provides the best performance on all the metrics. Surprisingly, we observed performance drops by DISTMULT-tanh-WV-init. We suspect that this is because word vectors are not appropriate for modeling entities described by non-compositional phrases (more than 73% of the entities in FB15k-401 are person names, locations, organizations and ﬁlms). The promising performance of DISTMULT-tanh-EV-init suggests that the embedding model can greatly beneﬁt from pre-trained entity-level vectors using external textual resources.  5  INFERENCE TASK II: RULE EXTRACTION  In this section, we focus on a complementary inference task, where we utilize the learned embed- dings to extract logical rules from the KB. For example, given the fact that a person was born in New York and New York is a city of the United States, then the person’s nationality is the United States:  BornInCitypa, bq ^ CityOf Countrypb, cq ùñ N ationalitypa, cq  Such logical rules can serve four important purposes. First, they can help deduce new facts and complete the existing KBs. Second, they can help optimize data storage by storing only rules instead of large amounts of extensional data, and generate facts only at inference time. Third, they can support complex reasoning. Lastly, they can provide explanations for inference results, e.g. we may infer that people’s professions usually involve the specialization of the ﬁeld they study, etc. The key problem of extracting Horn rules like the aforementioned example is how to effectively explore the search space. Traditional rule mining approaches directly operate on the KB graph – they search for possible rules (i.e. closed-paths in the graph) by pruning rules with low statistical signiﬁcance and relevance (Schoenmackers et al., 2010). These approaches often fail on large KB graphs due to scalability issues. In the following, we introduce a novel embedding-based rule mining approach whose efﬁciency is not affected by the size of the KB graph but rather by the number of distinct types of relations in the KB (which is usually relatively small). It can also mine better rules due to its strong generalizability.  5.1 BACKGROUND AND NOTATIONS For a better illustration, we adopt the graph view of KB. Each binary relation rpa, bq is a directed edge from node a to node b and with link type r. We are interested in extracting Horn rules that consist of a head relation H and a sequence of body relations B1, ..., Bn:  B1pa1, a2q ^ B2pa2, a3q ^ ... ^ Bnpan, an`1q ùñ Hpa1, an`1q  (4) where ai are variables that can be substituted by entities. We constrain the body relations B1, ..., Bn to form a directed path in the graph and the head relation H to from a directed edge that close the path (from the start of the path to the end of the path). We denote such property as the closed-path property. For consecutive relations that share one variable but do not form a path,  6  Published as conference paper at ICLR 2015  e,g, Bi´1pa, bq ^ Bipa, cq, we can replace one of the relations with its inverse relation, so that the i´1pb, aq ^ Bipa, cq. We are interested relations are connected by an object and an subject, e.g. B´1 in mining rules that reﬂect relationships among different relation types, therefore we also constrain B1, ..., Bn, H to have distinct relation types. A rule is instantiated when all variables are substi- tuted by entities. We denote the length of the rule as the number of body relations. In general longer rules are harder to extract due to the exponential search space. In our experiments, we focus on extracting rules of length 2 and 3. In KBs, entities usually have types and relations often can only take arguments of certain types. For example, BornInCity relation can only take a person as the subject and a location as the object. For each relation r, we denote the domain of its subject argument (the set of entities that can appear in the subject position) as Xr and similarly the domain of its object argument as Yr. Such domain information can be extremely useful in restricting the search space of logical rules.  5.2 EMBEDDING-BASED RULE EXTRACTION  For simplicity, we consider Horn rules of length 2 (longer rules can be easily derived from this case):  B1pa, bq ^ B2pb, cq ùñ Hpa, cq  b M2 « yT  a M is still a unit vector 3, then we have the property that yT a pM1M2q « yT c .  (5) Note that the body of the rule can be viewed as the composition of relations B1 and B2, which is a new relation that has the property that entities a and c are in a relation if and only if there is an entity b which simultaneously satisﬁes two relations B1pa, bq and B2pb, cq. We model relation composition as multiplication or addition of two relation embeddings. Here we focus on relation embeddings that are in the form of vectors (as in TRANSE) and matrices (as in BILINEAR and its variants). The composition results in a new embedding that lies in the same relation space. Speciﬁcally, we use addition for relation vector embeddings and multiplication for relation matrix embeddings. This is inspired by two different properties: (1) if a relation corresponds to a translation vector V and assume ya`V´yb « 0 when Bpa, bq holds, then we have the property that ya ` V1 « yb and yb ` V2 « yc implies ya `pV1 ` V2q « yc; (2) if a relation corresponds a Myb « 1 when Bpa, bq holds, also ya to a matrix M in the bilinear transformation and assume yT a M1 « yT and yb are unit vectors and yT and yT c implies yT To simulate the implication in 5, we want the composition result of relation B1 and B2 to demon- strate similar behavior to the embedding of relation H. We assume the similarity between relation embeddings is related to the Euclidean distance if the embeddings are vectors and to the Frobe- nius norm if the embeddings are matrices. This distance metric allows us to rank possible pairs of relations with respect to the relevance of their composition to the target relation. Note that we do not need to enumerate all possible pairs of relations in the KB. For example, if the relation in the head is r, then we are only interested in relation pairs pp, qq that satisfy the type constraints, namely: (1) Yp XXq ‰ H; (2) Xp XXr ‰ H; (3) Yq XYr ‰ H. As mentioned before, the arguments (entities) of relations are usually strongly typed in KBs. Applying such domain constraints can effectively reduce the search space. In Algorithm 1, we describe our rule extraction algorithm for general closed-path Horn rules as in Eq. (4). In Step 7, ˝ denotes vector addition or matrix multiplication. We apply a global threshold value δ in our experiments to ﬁlter candidate sequences for each relation r, and then automatically select the top remaining sequences by applying a heuristic thresholding strategy based on the differ- ence of the distance scores: sort the sequences by increasing distance d1, ..., dT and set the cut-off point to be the j-th sequence where j “ arg maxipdi`1 ´ diq.  b  5.3 EXPERIMENTS  We evaluate our rule extraction method (denoted as EMBEDRULE) on the FB15k-401 dataset. In our experiments, we remove the equivalence relations and relations whose domains have cardinality  3These assumptions may not hold in our implementations. The intuition still leads to surprisingly good em- pirical performance on Horn rule extraction. How to effectively enforce these constraints is worth investigating in future work.  7  Published as conference paper at ICLR 2015  Algorithm 1 EMBEDRULE 1: Input: KB “ tpe1, r, e2qu, relation set R 2: Output: Candidate rules Q 3: for each r in R do 4: 5: 6: 7: 8: 9: 10: end for  Select the set of start relations S “ ts : Xs X Xr ‰ Hu Select the set of end relations T “ tt : Yt X Yr ‰ Hu Find all possible relation sequences Select the K-NN sequences P 1 Ď P for r based on distpMr, Mp1 ˝ ¨¨¨ ˝ Mpnq Form candidate rules using P 1 where r is the head relation and p P P 1 is the body in a rule Add the candidate rules into Q  1 since rules involving these relations are not interesting. This results in training data that contains 485,741 facts, 14,417 entities, and 373 relations. Our EMBEDRULE algorithm identiﬁes 60,020 possible length-2 relation sequences and 2,156,391 possible length-3 relation sequences. We then apply the thresholding method described in Section 5.2 to further select top „3.9K length-2 rules and „2K length-3 rules 4. By default all the extracted rules are ranked by decreasing conﬁdence, which is computed as the ratio of the correct predictions to the total number of predictions, where predictions are triplets that are derived from the instantiated rules where the body relations are observed. We implemented four versions of EMBEDRULE using embeddings trained from TRANSE (DIS- TADD), BILINEAR, BILINEAR-DIAG (DISTMULT) and DISTMULT-tanh-EV-init with correspond- ing composition functions. We also compare our approaches to AMIE (Gal´arraga et al., 2013), a state-of-the-art rule mining system that can efﬁciently search for Horn rules in large-scale KBs by using novel measurements of support and conﬁdence. The system extracts close rules – a super- set of the rules we consider in this paper: every relation in the body is connected to the following relation by sharing an entity variable, and every entity variable in the rule appears at least twice. We run AMIE with the default setting on the same training set. It extracts 2,201 possible length-1 rules and 46,975 possible length-2 rules, among which 3,952 rules have the close-path property. We compare these length-2 rules with the similar number of length-2 rules extracted by EMBEDRULE. By default AMIE ranks rules by PCA conﬁdence (a normalized conﬁdence that takes into account the incompleteness of KBs). However we found that ranking by the standard conﬁdence gives better performance than the PCA conﬁdence on the Freebase dataset we use. For computational cost, EmbedRule mines length-2 rules in 2 minutes and mines length-3 rules in 20 minutes (the computational time is similar when using different types of embeddings). AMIE mines rules of length ď 2 in 9 minutes. All methods are evaluated on a machine with a 64-bit processor, 2 CPUs and 8GB memory. We consider precision as the evaluation metric, which is the ratio of predictions that are in the test (unseen) data to all the generated unseen predictions. Note that this is an estimation, since a prediction is not necessarily “incorrect” if it is not seen. Gal´arraga et al. (2013) suggested to identify incorrect predictions based on the functional property of relations. However, we ﬁnd that most relations in our datasets are not functional. For a better estimation, we manually labeled the top 30 unseen facts predicted by each method by checking Wikipedia. We also remove rules where the head relations are hard to justiﬁed due to dynamic factors (i.e. involving the word “current”).  5.4 RESULTS  Figure 1 compares the predictions generated by the length-2 rules extracted by different meth- ods. We plot the aggregated precision of the top rules that produce up to 10K predictions in to- tal. From left to right, the n-th data point represents the total number of predictions of the top n rules and the estimated precision of these predictions. We can see that EMBEDRULE that uses embeddings trained from the bilinear objective (BILINEAR, DISTMULT and DISTMULT-TANH-  4We consider K=100 nearest-neighbor sequences for each method, and set δ to 9.2, 36.3, 1.9 and 3.4 for DISTMULT-TANH-EV-INIT, DISTMULT, BILINEAR and DISTADD respectively for length-2 rules, and set it to 9.1, 48.8, 2.9, and 1.1 for lengh-3 rules.  8  Published as conference paper at ICLR 2015  EV-INIT) consistently outperforms AMIE. This suggests that the bilinear embeddings contain good amount of information about relations which allows for effective rule selection without look- ing at the entities. For example, AMIE fails to extract T V P rogramCountryof Originpa, bq ^ CountryOf f icialLanguagepb, cq ùñ T V P rogramLanguagepa, cq by relying on the instan- tiations of the rule occurred in the observed KB while all the bilinear variants of EMBEDRULE successfully extract the rule purely based on the embeddings of the three involved relations. We can also see that in general, using multiplicative composition of matrix embeddings (from DIST- MULT and BILINEAR) results in better performance compared to using additive composition of vector embeddings (from DISTADD). We found many examples where DISTADD fails to retrieve rules because it assigns large distance between the composition of the body relations and the head relation in the embedding space while its multiplicative counterpart DISTMULT ranks the composi- tion result much closer to the head relation. For example, DISTADD prunes the possible composition F ilmDistributorInRegion^RegionGDP Currency for relation F ilmBudgetCurrency while DISTMULT ranks the composition as the nearest neighbor of F ilmBudgetCurrency.  Figure 1: Aggregated precision of top length-2 rules extracted by different methods  Figure 2: Aggregated precision of top length-3 rules extracted by different methods  We also look at the results for length-3 rules generated by different implementations of EMBEDRULE in Figure 2. We can see that the initial length-3 rules extracted by EMBEDRULE can provide very good precision in general. We can also see that BILINEAR consistently outperforms DISTMULT and DISTADD on the top 1K predictions and DISTMULT-TANH-EV-INIT tends to outperform the other methods as more predictions are generated. We think that the fact that BILINEAR starts to show more advantage over DISTMULT in extracting longer rules conﬁrm the limitation of representing relations by diagonal matrices, as longer rules requires the modeling of more complex relation semantics.  9  Published as conference paper at ICLR 2015  6 CONCLUSION  In this paper, we present a general framework for learning representations of entities and relations in KBs. Under the framework, we empirically evaluate different embedding models on knowledge inference tasks. We show that a simple formulation of bilinear model can outperform the state-of- the-art embedding models for link prediction on Freebase. Furthermore, we examine the learned embeddings by utilizing them to extract logical rules from KBs. We show that embeddings learned from the bilinear objective can capture compositional semantics of relations and be successfully used to extract Horn rules that involve compositional reasoning. For future work, we aim to exploit deep structure in the neural network framework. As learning representations using deep networks has shown great success in various applications (Hinton et al., 2012; Vinyals et al., 2012; Deng et al., 2013), it may also help capturing hierarchical structure hidden in the multi-relational data. Further, tensor constructs have been usefully applied to some deep learning architectures (Yu et al., 2013; Hutchinson et al., 2013). Related constructs and architectures may help improve multi-relational learning and inference.  APPENDIX  A EXAMPLES OF THE EXTRACTED HORN RULES  Examples of length-2 rules extracted by EMBEDRULE with embeddings learned from DISTMULT- tanh-EV-init: AwardInCeremanypa, bq ^ CeremanyEventT ypepb, cq ùñ AwardInEventT ypepa, cq  AtheleteP layInT eampa, bq ^ T eamP laySportpb, cq ùñ AtheleteP laySportpa, cq  T V P rogramInT V N etworkpa, bq^T V N etworkServiceLanguagepb, cq ùñ T V P rogramLanguagepa, cq  LocationInStatepa, bq ^ StateInCountrypb, cq ùñ LocationInCountrypa, cq  BornInLocationpa, bq ^ LocationInCountrypb, cq ùñ N ationalitypa, cq  Examples of length-3 rules extracted by EMBEDRULE with embeddings learned from DISTMULT- tanh-EV-init: SportP layByT eampa, bq^T eamInClubpb, cq^ClubHasP layerpc, dq ùñ SportP layByAtheletepa, dq M usicT rackP erf ormerpa, bq^P eerInf luencepb, cq^P erf ormRolepc, dq ùñ M usicT rackRolepa, dq F ilmHasActorpa, bq^CelebrityF riendshippb, cq^P ersonLanguagepc, dq ùñ F ilmLanguagepa, dq  B VISUALIZATION OF THE RELATION EMBEDDINGS  Visualization of the relation embeddings learned by DISTMULT and DISTADD using t-SNE (see ﬁgure 3 and 4). We selected 189 relations in the FB15k-401 dataset. The embeddings learned by DISTMULT nicely reﬂect the clustering structures among these relations (e.g. /ﬁlm/release region is closed to /ﬁlm/country); whereas the embeddings learned by DISTADD present structure that is harder to interpret.  REFERENCES Auer, S¨oren, Bizer, Christian, Kobilarov, Georgi, Lehmann, Jens, Cyganiak, Richard, and Ives, In The semantic web, pp. 722–735.  Zachary. Dbpedia: A nucleus for a web of open data. Springer, 2007.  Bordes, Antoine, Weston, Jason, Collobert, Ronan, and Bengio, Yoshua. Learning structured em-  beddings of knowledge bases. In AAAI, 2011.  Bordes, Antoine, Glorot, Xavier, Weston, Jason, and Bengio, Yoshua. A semantic matching energy  function for learning with multi-relational data. Machine Learning, pp. 1–27, 2013a.  Bordes, Antoine, Usunier, Nicolas, Garcia-Duran, Alberto, Weston, Jason, and Yakhnenko, Oksana.  Translating embeddings for modeling multi-relational data. In NIPS, 2013b.  10  Published as conference paper at ICLR 2015  Figure 3: Relation embeddings (DISTADD)  Figure 4: Relation embeddings (DISTMULT)  Bowman, Samuel R. Can recursive neural tensor networks learn logical reasoning? In ICLR, 2014.  Chang, Kai-Wei, Yih, Wen-tau, Yang, Bishan, and Meek, Chris. Typed tensor decomposition of  knowledge bases for relation extraction. In EMNLP, 2014.  Deng, Li, Hinton, G., and Kingsbury, B. New types of deep neural network learning for speech  recognition and related applications: An overview. In in ICASSP, 2013.  Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning  and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.  Gal´arraga, Luis Antonio, Teﬂioudi, Christina, Hose, Katja, and Suchanek, Fabian. Amie: associa-  tion rule mining under incomplete evidence in ontological knowledge bases. In WWW, 2013.  Gao, Jianfeng, Pantel, Patrick, Gamon, Michael, He, Xiaodong, Deng, Li, and Shen, Yelong. Mod-  eling interestingness with deep neural networks. In EMNLP, 2014.  Garc´ıa-Dur´an, Alberto, Bordes, Antoine, and Usunier, Nicolas. Effective blending of two and three- way interactions for modeling multi-relational data. In Machine Learning and Knowledge Dis- covery in Databases, pp. 434–449. Springer, 2014.  Getoor, Lise and Taskar, Ben (eds.). Introduction to Statistical Relational Learning. The MIT Press,  2007.  Grefenstette, Edward. Towards a formal distributional semantics: Simulating logical calculi with  tensors. In *SEM, 2013.  11  Published as conference paper at ICLR 2015  Hinton, Geoff, Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. Deep neural networks for acoustic modeling in speech recognition. IEEE Sig. Proc. Mag., 29:82–97, 2012.  Huang, Po-Sen, He, Xiaodong, Gao, Jianfeng, Deng, Li, Acero, Alex, and Heck, Larry. Learning  deep structured semantic models for Web search using clickthrough data. In CIKM, 2013.  Hutchinson, B, Deng, L., and Yu, D. Tensor deep stacking networks. IEEE Transactions on Pattern  Analysis and Machine Intelligence, 35(8):1944–1957, 2013.  Jenatton, Rodolphe, Le Roux, Nicolas, Bordes, Antoine, and Obozinski, Guillaume. A latent factor  model for highly multi-relational data. In NIPS, 2012.  Kemp, Charles, Tenenbaum, Joshua B, Grifﬁths, Thomas L, Yamada, Takeshi, and Ueda, Naonori. Learning systems of concepts with an inﬁnite relational model. In AAAI, volume 3, pp. 5, 2006. Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed represen-  tations of words and phrases and their compositionality. In NIPS, pp. 3111–3119, 2013.  Nickel, Maximilian, Tresp, Volker, and Kriegel, Hans-Peter. A three-way model for collective learn-  ing on multi-relational data. In ICML, pp. 809–816, 2011.  Nickel, Maximilian, Tresp, Volker, and Kriegel, Hans-Peter. Factorizing YAGO: scalable machine  learning for linked data. In WWW, pp. 271–280, 2012.  Paccanaro, Alberto and Hinton, Geoffrey E. Learning distributed representations of concepts using IEEE Transactions on Knowledge and Data Engineering, 13(2):  linear relational embedding. 232–244, 2001.  Richardson, Matthew and Domingos, Pedro. Markov logic networks. Machine learning, 62(1-2):  107–136, 2006.  Rockt¨aschel, Tim, Boˇsnjak, Matko, Singh, Sameer, and Riedel, Sebastian. Low-dimensional em-  beddings of logic. In ACL Workshop on Semantic Parsing, 2014.  Schoenmackers, Stefan, Etzioni, Oren, Weld, Daniel S, and Davis, Jesse. Learning ﬁrst-order horn  clauses from web text. In EMNLP, 2010.  Shen, Yelong, He, Xiaodong, Gao, Jianfeng, Deng, Li, and Mesnil, Gregoire. A latent semantic  model with convolutional-pooling structure for information retrieval. In CIKM, 2014a.  Shen, Yelong, He, Xiaodong, Gao, Jianfeng, Deng, Li, and Mesnil, Gr´egoire. Learning semantic In WWW, pp. 373–374,  representations using convolutional neural networks for Web search. 2014b.  Singh, Ajit P and Gordon, Geoffrey J. Relational learning via collective matrix factorization. In  KDD, pp. 650–658. ACM, 2008.  Socher, Richard, Huval, Brody, Manning, Christopher D., and Ng, Andrew Y. Semantic composi-  tionality through recursive matrix-vector spaces. In EMNLP-CoNLL, 2012.  Socher, Richard, Chen, Danqi, Manning, Christopher D., and Ng, Andrew Y. Reasoning with neural  tensor networks for knowledge base completion. In NIPS, 2013.  Suchanek, Fabian M, Kasneci, Gjergji, and Weikum, Gerhard. Yago: a core of semantic knowledge.  In WWW, 2007.  Sutskever, Ilya, Tenenbaum, Joshua B, and Salakhutdinov, Ruslan. Modelling relational data using  Bayesian clustered tensor factorization. In NIPS, pp. 1821–1828, 2009.  Vinyals, O., Jia, Y., Deng, L., and Darrell, T. Learning with recursive perceptual representations. In  NIPS, 2012.  Yih, Wen-tau, He, Xiaodong, and Meek, Christopher. Semantic parsing for single-relation question  answering. In ACL, 2014.  Yu, D., Deng, L., and Seide, F. The deep tensor neural network with applications to large vocabulary  speech recognition. IEEE Trans. Audio, Speech and Language Proc., 21(2):388 –396, 2013.  12  ",
1412.6626,2015,The local low-dimensionality of natural images,"['The local low-dimensionality of natural images', 'Olivier Henaff', 'Johannes Balle', 'Neil Rabinowitz', 'and Eero Simoncelli']",https://arxiv.org/pdf/1412.6626,"5 1 0 2    r a     M 4 2      ]  V C . s c [      4 v 6 2 6 6  .  2 1 4 1 : v i X r a  Published as conference paper at ICLR 2015  THE LOCAL LOW-DIMENSIONALITY OF NATURAL IMAGES  Olivier J. H´enaff, Johannes Ball´e, Neil C. Rabinowitz & Eero P. Simoncelli Howard Hughes Medical Institute, and Center for Neural Science New York University New York, NY 10003, USA {ojh221, jb4726, nr66, eero.simoncelli}@nyu.edu  ABSTRACT  We develop a new statistical model for photographic images, in which the local responses of a bank of linear ﬁlters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of ﬁlters so as to minimize the nuclear norm of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a ﬂexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters opti- mized according to this objective are oriented and band-pass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local ﬁlter response covariance alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.  1  INTRODUCTION  Many problems in computer vision and image processing can be formulated in terms of statistical inference, based on probabilistic models of natural, photographic images. Whereas natural images themselves are complex, locally structured, and high-dimensional, the vision community has traditionally sought probabilistic models of images that are simple, global, and low-dimensional. For exam- ple, in the classical spectral model, the coefﬁcients of the Fourier transform are assumed independent and Gaussian, with variance falling with frequency; in block-based PCA, a set of orthogonal ﬁlters are used to decompose each block into components that are modeled as independent and Gaussian; and in ICA, ﬁlters are cho- sen so as to optimize for non-Gaussian (heavy-tailed, or “sparse”) projections (Bell & Sejnowski (1997); ﬁgure 1). To add local structure to these models, a simple observation has proved very useful: the local variance in natural images ﬂuctu- ates over space (Ruderman, 1994; Simoncelli, 1997). This has been made explicit in the Gaussian Scale Mixture model, which represents neighborhoods of individual ﬁlter coefﬁcients as a sin- gle global Gaussian combined with a locally-varying multiplica- tive scale factor (Wainwright & Simoncelli, 2000). The Mixture of GSMs model builds upon this by modeling the local density as a sum of such scale mixtures (Guerrero-Col´on et al., 2008). Here, we extend this concept to a richer and more powerful model by making another simple ob- servation about the local structure of natural images: the covariance of ﬁlter coefﬁcients at a given location can vary smoothly with spatial position, and these local covariances tend to be highly elon-  Figure 1: Global responses of oriented band-pass ﬁlters to natural images are heavy tailed.  1  Published as conference paper at ICLR 2015  gated, i.e. they lie close to low-dimensional subspaces. In section 2, we motivate the model, showing that these properties hold for a pair of simple oriented ﬁlters. We ﬁnd that the low-dimensionality of local covariances depends on both the ﬁlter choice, and the content of the images—speciﬁcally, it does not hold for phase-randomized ﬁlters or images. In section 3, we use this distinctive property to optimize a set of ﬁlters for a measure of low-dimensionality over natural images. Finally, in sec- tion 4, we demonstrate that the local low-dimensional covariance description captures most of the visual appearance of images, by synthesizing images with matching local covariance structure.  2 ANALYSING LOCAL IMAGE STATISTICS  Figure 2: Locally, we can approximate the responses of a pair of oriented band-pass ﬁlters to pho- tographic images as jointly Gaussian, with a covariance that changes continuously across space. In regions with oriented content, these responses are low-dimensional, as indicated by a high correla- tion between ﬁlter responses.  To understand the statistics of local image patches, we begin with a simple example, based on analysis with two orthogonal, oriented band-pass ﬁlters. If we aggregate the two ﬁlters’ responses over the whole image, the resulting joint distribution is approximately spherically symmetric but the marginal distributions are heavy-tailed (Zetzsche & Krieger (1999); Lyu & Simoncelli (2009); ﬁgure 1). However, if we aggregate only over local neighborhoods, the distributions are more ir- regular, with covariance structure that varies in scale (contrast/energy), aspect ratio, and orientation (ﬁgure 2). Our interest here is in the second property, which provides a measure of the dimensional- ity of the data. Speciﬁcally, a covariance with large aspect ratio (i.e., ratio of eigenvalues) indicates that the ﬁlter responses lie close to a one-dimensional subspace (line). In the case of two ﬁlters, we can construct a simple, continuous measure of local dimensionality by calculating the correlation coefﬁcient between ﬁlter responses in local neighborhoods. The dis- tribution of correlation coefﬁcient magnitudes across image patches is very skewed (ﬁgure 3, left): in many locations, the responses are correlated, i.e. the local Gaussians are low-dimensional. In contrast, if we repeat the same experiment with spectrally-matched noise images rather than a pho- tograph (ﬁgure 3, center), the correlations are typically lower, i.e. the local Gaussians are more high-dimensional. The spectral properties of natural images alone are thus insufﬁcient to produce local low-dimensional structure. Similarly, if we analyze a photograph with phase-randomized ﬁl- ters (ﬁgure 3, right), we do not ﬁnd the same local low-dimensionality. We take this as evidence that local low-dimensional structure is a property that emerges from the combination of local band-pass ﬁlters and photographic images.  2  |r| = 0.78|r| = 0.10|r| = 0.37|r| = 0.48Published as conference paper at ICLR 2015  Figure 3: Local low-dimensional structure is not a guaranteed property of all ﬁlter responses to all images. For each panel, the pair of ﬁlters in the top left corner (enlarged 15×) are applied to the image in the top right, and the histogram of local correlation coefﬁcient magnitudes across locations is plotted below. Oriented ﬁlters analyzing natural images (left) exhibit locally low-dimensional responses, but when oriented ﬁlters are applied to spectrally matched noise images (center) or phase- randomized ﬁlters are applied to photographic images (right) this behavior vanishes.  3 LOCAL LOW DIMENSIONALITY AS A LEARNING OBJECTIVE  3.1 OBJECTIVE FUNCTION  We now ask whether these oriented band-pass ﬁlters are the best ﬁlters, or indeed the only ﬁlters, for producing representations of natural images that are locally low-dimensional. Just as marginal sparsity has been used as an objective for optimizing ﬁlters for representation of natural images (Ol- shausen & Field, 1996; Bell & Sejnowski, 1997), we aim to derive a ﬁlter bank that minimizes a measure of the local dimensionality of responses to natural images. Here we describe the construc- tion of the objective function and the motivation behind its components; in section 3.2 we cover some technical details relating to the optimization; in section 3.3 we present the results. To begin, suppose we have a convolutional ﬁlter bank f, and an image x. We compute a map of ﬁlter responses yi(t) = (fi∗x)(t), with the index t indicating spatial position, and consider this map in terms of a set of overlapping patches. For each patch P , we can form a matrix YP = [y(t)]t∈P composed of all the response vectors in that patch. Next, we need to deﬁne an appropriate measure of dimensionality for the local covariance on each patch. The correlation coefﬁcient presented in section 2 does not extend beyond the simple two- dimensional case. Instead, we choose to measure the nuclear norm of YP , i.e. the sum of its singular values. This is the convex envelope of the matrix rank function, so it provides a continuous measure of dimensionality; unlike the rank, it is robust to small perturbations of ﬁlter responses away from true subspaces. The local low-dimensionality component of the objective is thus:  Elocal dim =(cid:88)P (cid:107)YP(cid:107)∗  To ensure that the ﬁlter responses provide a reasonable representation of the original image, we reconstruct the image from them via the ﬁlter bank transpose ˜f (t) = f (−t), and penalize for reconstruction error:  Erecons =(cid:88)t(cid:16)x(t) −(cid:88)i  ( ˜fi ∗ yi)(t)(cid:17)2  Finally, in order to ensure that all ﬁlters are used and the ﬁlter responses are sufﬁciently diverse, we include a term that makes the global distribution of ﬁlter responses high-dimensional. One way to achieve this would be to form the matrix Y of all response vectors from the ensemble, and maximize its nuclear norm:  Eglobal dim = −(cid:107)Y (cid:107)∗  3   0 1 correlation 0 1 correlation 0 1 correlationPublished as conference paper at ICLR 2015  Together with the reconstruction penalty, this tends to whiten the ﬁlter responses (i.e. Cov[y] ∝ I). In practice however, this term allows degenerate solutions with ﬁlters that are related to each other by a phase shift. This is best understood in the Fourier domain: with the Parseval identity,  Cov[y] =(cid:20)(cid:90) yi(t)yj(t) dt(cid:21)i,j =(cid:20)(cid:90) ˆyi(ω)ˆyj(ω) dω(cid:21)i,j =(cid:20)(cid:90) |ˆx(ω)|2 ˆfi(ω) ˆfj(ω) dω(cid:21)i,j  where ˆa = F[a] is the Fourier transform of a. Two ﬁlters with identical Fourier magnitudes but different phases can make this expectation zero. To eliminate this degeneracy, we can maximize the dimensionality of the ﬁlter reconstructions zi = ˜fi∗fi∗x, rather than the ﬁlter responses yi = fi∗x. As  Cov[z] =(cid:20)(cid:90) |ˆx(ω)|2| ˆfi(ω)|2| ˆfj(ω)|2 dω(cid:21)i,j  maximizing the nuclear norm of Z = [z(t)]t pushes Cov[z] towards a multiple of the identity and hence penalizes any overlap between ﬁlter Fourier magnitudes. Since this tends to insist that ﬁlters have hard edges in the Fourier domain, we relax this constraint by only penalizing for overlaps between blurred versions of the ﬁlters’ Fourier magnitudes. Using a Gaussian blurring window h, we compute modulated ﬁlter reconstructions ˜zi = (h ˜fi) ∗ (hfi) ∗ x, and whiten Cov[˜z] =(cid:20)(cid:90) |ˆx(ω)|2|h ∗ ˆfi(ω)|2|h ∗ ˆfj(ω)|2 dω(cid:21)i,j  by maximizing the dimensionality of ˜Z = [˜z(t)]t via the term  Summarizing, we optimize our ﬁlter bank for  E = Elocal dim + λ Erecons  + µ Eglobal dim  Eglobal dim = −(cid:13)(cid:13)(cid:13) ˜Z(cid:13)(cid:13)(cid:13)∗ + λ(cid:13)(cid:13)(cid:13)x −(cid:88)i zi(cid:13)(cid:13)(cid:13) 2 − µ(cid:13)(cid:13)(cid:13) ˜Z(cid:13)(cid:13)(cid:13)∗  2  =(cid:88)P (cid:107)YP(cid:107)∗  We now describe the practical details of the learning procedure and our results.  3.2 OPTIMIZATION  3.2.1 MODEL SPECIFICATIONS  We trained the model described above on the van Hateren dataset (van Hateren & van der Schaaf, 1998) using the Torch machine learning library (Collobert et al., 2011). We used 20×20 pixel ﬁlter kernels, varying in number from 4 to 12, and estimated local dimensionality over neighborhoods of size 16× 16 pixels, weighted by a Gaussian window with a standard devia- tion of 3 pixels. We ﬁxed the blurring window h to be Gaussian with a standard deviation of 3 pixels, such that it only becomes negligible at the kernel boundary. The hyperparameters λ and µ are easily tuned: we increased the reconstruction weight λ until a desired reconstruction level was reached (e.g. a relative L2 error of 1%) and increased the diversity weight µ until none of the ﬁlters were zero nor perfectly correlated with another. In the experiments below they were set to 3500 and 100 respectively. We optimized our ﬁlter bank using stochastic gradient descent with a ﬁxed learning rate, chosen as high as possible without causing any instability.  Figure 4: Gradient scaling. Left: Input spectrum ranges from 1 to 100. Right: Value of objective over time, with and without gradient scaling.  4   0 5unscaledscaled log10(t)162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215UnderreviewasconferencepaperatICRL2015from**to**.WeﬁxedtheblurringwindowhtobeGaussianwithastandarddeviationof**,suchthatitonlybecomesnegligeableatthekernelboundary.Thehyperparametersλandµareveryeasilytuned,inthatthereconstructionweightλcanbeincreaseduntiladesiredreconstructionlevelisreached(inourcasearelativeL2errorof1%)andthediversityweightµcanbeincreaseduntilnoneoftheﬁltersarenulnorperfectlycorrelatedwithoneanother.Intheexperimentsbelowtheyweresetto3500and100respectively.Weoptimizedourﬁlterbankusingstochasticgradientdescentwithaﬁxedlearningrate,chosenashighaspossiblewithoutcausinganyinstability.3.2.2ACCELERATEDLEARNINGWITHGRADIENTSCALINGWedeveloppedamethodtoscalegradientsaccordingtotheinputspectrum,andfoundthatitcon-siderablyacceleratedtheoptimizationprocedure.Givenagradient∂E∂yioftheobjectivefunctionwithrespecttoﬁlteractivationsyi=fi∗x,thebackpropagationrule(citeBP?)statesthattheﬁltercomponentfi(t)receivesthegradient∂E∂fi(t)=∂E∂yi∗˜x(t)where˜xisahorizontally-andvertically-ﬂippedcopyoftheimagex.Thisimpliesthatthefrequencycomponentoftheﬁlterˆfi(ω)receivesthegradient**DETAILEDPROOF?**∂E∂ˆfi(ω)=ˆx(ω)∂E∂ˆyi(ω)Giventhehyperbolicshapeofˆx(ω)fornaturalimages(seeﬁgure*****),thelowerfrequencycomponentsoftheﬁltereffectivelyreceivesubstantiallylargergradientsthanthehighfrequencyones.Wecorrectforthisbydividingthegradient∂E∂ˆfi(ω)bythecorrespondingmeanimagefrequencymagnitude.∂E∂ˆfi(ω)←1E|ˆx(w)|∂E∂ˆfi(ω)=ˆx(ω)E|ˆx(w)|∂E∂ˆfi(ω)Sinceourﬁlterkernelsandgradientsaredeﬁnedinthespatialdomain,wecompletethisoperationwithtwoFouriertransforms∂E∂fi(t)←F−1(cid:31)1E|ˆx(w)|F(cid:31)∂E∂fi(cid:30)(ω)(cid:30)(t)Weestimatethemeanfrequencymagnitudeofthedatasetonce,thendivideallgradientsbythismeanratherthanbythelocalfrequencymagnitudeˆx(ω)inordertoavoidanyinstabilityduetosmallvaluesofˆx(ω),andtoreducethecomplexityoftheoperation.Thisenablesustousemuchlargerlearningratesandweestimatethatitaccelerateslearningbyafactorof10to100(refﬁgure).Werepeatedourexperimentswithoutthegradientscalingmethodandfoundnodifferenceintheﬁnalresult.FIGUREshowingobjectiveasafunctionoftimefortwoidenticalnetworks,oneoptimizedwithgradientscaling,theothernot.Nexttoitshow2dplotofmeanimagespectrum,onalogscale.3.3RESULTSOurresults(ﬁgure2)showthattheoptimalﬁlterbankforexhibitingthelowdimensionalstructureofnaturalimagesiscomposedofalow-pass,ahigh-pass,andasetoforientedband-passﬁlters.Asweincreasethenumberofﬁlters,weobtainaﬁnerpartitionofscalesandorientations:8ﬁlterspartitionFourierspaceintoalow-passband,ahigh-passresidual,and2scalesand3orientations.Thisﬁlterbankalsobecomessensitivetoaliasingintheinputimages,asindicatedbythecircularboundariesofcertainﬁlterspectra.4LOCALLOWDIMENSIONALITYASAPRIORFORNATURALIMAGESHavinglearnedalineartransformationthatuncoversthelowdimensionalstructureofnaturalim-ages,weconstructanon-linearrepresentationwhichparametrizeslocalsubspaces.Wedosoby4log102Published as conference paper at ICLR 2015  3.2.2 ACCELERATED LEARNING WITH GRADIENT SCALING  We developed a method to scale gradients according to the input spectrum, and found that it consid- erably accelerated the optimization procedure. In ordinary gradient descent, the descent direction for the ﬁlter fi(t) is the negative gradient. Using the chain rule, it can be expressed in terms of the ﬁlter responses yi = fi ∗ x:  ∆fi(t) = −  ∂E  ∂fi(t)  In the Fourier domain, this is  = −(cid:18) ∂E  ∂yi ∗ ˜x(cid:19) (t)  ∆ ˆfi(ω) = −  ∂E  ∂ ˆfi(ω)  = −ˆx(ω)  ∂E  ∂ ˆyi(ω)  Due to the hyperbolic spectrum of ˆx(ω) (ﬁgure 4, left panel), the low frequency components of the gradient are substantially larger than the high frequency ones. We offset this imbalance by dividing the gradient by the corresponding mean image frequency magnitude. The modiﬁed descent direction is thus  ∆ ˆfi(ω) =  −1  (cid:112)E [|ˆx(w)|2]  ∂E  ∂ ˆfi(ω)  =  −ˆx(ω)  (cid:112)E [|ˆx(w)|2]  ∂E  ∂ ˆyi(ω)  where the expectation is over the ensemble of all images. The gradient scaling algorithm accelerates convergence by a factor of at least 10 (ﬁgure 4, right panel) and does not affect the ﬁnal result.  3.3 RESULTS  Figure 5: Examples of optimized ﬁlter banks of size 4, 7 and 12. Top row: spatial ﬁlters, bottom row: Fourier magnitudes (zero frequency is in center).  Even though our objective function is not convex with respect to the ﬁlter bank, we found empir- ically that different initializations lead to qualitatively similar results. The optimized ﬁlter bank for uncovering the local low-dimensional structure of natural images is composed of a low-pass, a high-pass, and a set of oriented band-pass ﬁlters (ﬁgure 5). As we increase the number of ﬁlters, we obtain a ﬁner partition of scales and orientations: 7 ﬁlters divide the band-pass region into 2 radial sub-bands, with 3 orientations in the mid-low band and 2 orientations in the mid-high band. With 12 ﬁlters, the mid-low band remains mostly unchanged, while the mid-high band is partitioned into 6 orientations.  4 REPRESENTING NATURAL IMAGES AS LOCAL COVARIANCE MAPS  Having optimized a linear transform to reveal the local low-dimensional covariance structure of natural images, we now ask what information these local covariances actually capture about an im- age. More precisely, we construct a nonlinear representation of an image by ﬁltering it through the learned ﬁlter bank, estimating the local covariances and subsampling the resulting covariance  5  Published as conference paper at ICLR 2015  (a) Original image  #pixels: 21600  (b) neighborhood: 8×8 subsampling: 2×2 #measurements: 54000 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 1.5%  (c) neighborhood: 16×16 subsampling: 4×4 #measurements: 13500 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 5.7%  (d) neighborhood: 24×24 subsampling: 4×4 #measurements: 13500 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 11.1%  Figure 6: Synthesized images, matched for local covariance maps of a bank of 4 optimized ﬁlters (ﬁgure 5), are almost indistinguishable from the original. As the neighborhood over which the covariance is estimated increases, the errors increase but are still far less visible than equivalent amounts of additive white noise. Top row: original image x and synthetic images. Middle row: pixelwise magnitude of difference with original ∆x. Each difference image is individually scaled to full dynamic range for display. Bottom row: original image corrupted with additive Gaussian noise, such that the relative error ((cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2) is the same.  map. To explore the power of this representation, we synthesize new images with the same covari- ance representation. This method of image synthesis is useful for probing the equivalence class of images that are identical with respect to an analysis model, thereby exhibiting its selectivities and invariances (Portilla & Simoncelli, 2000). The procedure for these experiments is as follows. We ﬁrst build a representation of the original image by estimating the covariance matrix of ﬁlter responses in each local neighborhood P :  CP (x) =(cid:104)(cid:88)t∈P  w(t)yi(t)yj(t)(cid:105)i,j  where w is a spatial windowing function. The local covariance map φ of the original (target) image is then:  To synthesize a new image with the same local covariance map, we start with a white noise image x, and perform gradient descent on the objective  φ(xtarget) = [CP (xtarget)]P  E(x) = (cid:107)Vec (φ(x) − φ(xtarget))(cid:107)1  where Vec(a) is a vector composed of the elements of the multi-dimensional array a. We choose an L1 penalty in order to obtain a piecewise quadratic error term, and use a harmonically decaying gradient step to ensure convergence.  6  Published as conference paper at ICLR 2015  4.1 PERCEPTUAL RELEVANCE OF LOCAL COVARIANCES  We distinguish two regimes which lead to very different synthesis results: overcomplete and un- dercomplete. When φ(x) is overcomplete, the solution of the synthesis problem at x = xtarget is often unique. However, even if this holds, ﬁnding this solution can be difﬁcult or expensive as the original image must be recovered from a set of quadratic measurements (Bruna et al., 2013). When φ(x) is undercomplete, many images are represented with the same φ(x), and synthesis amounts to sampling from this equivalence class of solutions (Portilla & Simoncelli, 2000). In an overcomplete setting (ﬁgure 6b), the simple synthesis algorithm is able to reconstruct the image almost perfectly from the local covariance map. Surprisingly, as we move into the undercomplete regime by further subsampling the covariance map (ﬁgure 6c), the synthetic images retain excellent perceptual quality. In the undercomplete regime, the diversity amongst solutions reveals information which is lost by this representation. When we subsample the covariance maps by a factor of 4 in each direction (ﬁgure 6c), samples include slight phase shifts in high frequency content. When we estimate the covariance over an even larger neighborhood (ﬁgure 6d), these phase shifts get larger as indicated by the white lines in the difference image (ﬁgure 6, middle row). Nevertheless, despite the large differences in mean squared error, the synthetic images are almost indistinguishable from the orig- inal, especially when compared to images corrupted by additive white noise of equivalent variance (ﬁgure 6, bottom row). As a control, we compared these results against syntheses from a representation of natural images in terms of local variance maps. These correspond to a subset of the parameters in local covari- ance maps, namely the diagonals of the local covariances. To offset the fact that the local variance maps have fewer parameters (the off-diagonal terms of each local covariance being discarded), we increased the number of ﬁlters to match the cardinality of the covariance maps. We expected that the results would be similar, as the covariance between two ﬁlters can be expressed as the variance of their sum, subtracted by their respective variances. However, the reconstructions from the variance maps are substantially worse (ﬁgure 8), both in terms of mean squared error and perceptual quality. This is because successful synthesis from variance maps requires the ﬁlters to satisfy a stringent set of properties (Bruna et al., 2013). Synthesis from covariance maps appears to be more robust, both in the oversampled and undersampled regimes. Having constructed a representation which captures the shape, scale, and orientation of local dis- tributions of ﬁlter responses, and tested its perceptual relevance, we now investigate the role of the shape of these distributions.  4.2 PERCEPTUAL RELEVANCE OF LOCAL LOW-DIMENSIONAL COVARIANCES  In section 2, we found that natural images distinguish themselves from noise by the proximity of their local distributions to low-dimensional subspaces. We can now ask if these subspaces carry the important information of natural images. Speciﬁcally, if we project the representation onto local subspaces, how much of the original image is preserved? We answer this question by synthesizing from a map of local covariances from which we have discarded information corresponding to the smallest eigenvalues. For every covariance matrix, we compute its eigendecomposition, threshold its eigenvalues at a ﬁxed value, and synthesize from the resulting covariance map. Truncating the distribution of eigenvalues results in the removal of high frequency noise as well as low-contrast edges (ﬁgure 8, 2nd column). Since a ﬁxed threshold does not distinguish between scale and shape, we repeated the experiment with a threshold value that was scaled adaptively by the local energy (sum of all eigenvalues but the ﬁrst, which corresponds to the mean luminance). This modiﬁes the shape of local distributions regardless of their scale. Projecting local distributions onto their local subspaces enhances the image by removing noise while preserving any oriented structure (ﬁgure 8, 3rd column). On the other hand, making the image locally high-dimensional by raising eigenvalues to a power less than one degrades it by texturizing it artiﬁcally (ﬁgure 8, 4th column).  7  Published as conference paper at ICLR 2015  (a) Original image  #pixels: 21600  (b) neighborhood: 8×8 subsampling: 2×2 #measurements: 54000 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 8.4%  (c) neighborhood: 16×16 subsampling: 4×4 #measurements: 13500 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 15.4%  (d) neighborhood: 24×24 subsampling: 4×4 #measurements: 13500 (cid:107)∆x(cid:107)2 /(cid:107)x(cid:107)2 = 20.7%  Figure 7: Synthesized images matched for local variance maps fail to capture the relevant structure of the original. Local variances are computed from 10 ﬁlters, matching the cardinality of the full covariance representation used in ﬁgure 6.  Figure 8: Effects of various modiﬁcations of the local covariance map. Top row: histograms of (log) eigenvalues of local covariance matrices. Applying a ﬁxed threshold to the corresponding singular values (2nd column) removes non-oriented content but also low-contrast edges, whereas adaptive thresholding (3rd column) preserves oriented structure regardless of contrast. Dimensionality ex- pansion (4th column) corrupts the image with artiﬁcial texture.  8  Published as conference paper at ICLR 2015  5 CONCLUSION  We have arrived at a representation of natural images in terms of a spatial map of local covariances. We apply a bank of ﬁlters to the image, and view the vector of responses at each location as a sample from a multivariate Gaussian distribution with zero mean and a covariance that varies smoothly across space. We optimize the ﬁlter bank to minimize the dimensionality of these local covariances; this yields ﬁlters that are local and oriented. We used synthesis to demonstrate the power of this representation. Via a descent method, we im- pose the covariance map estimated from an original image onto a second, noise image. The result- ing image is nearly identical to the original image, and appears to degrade gracefully as we blur and undersample the covariance map. Visually, these reconstructions are vastly better than those obtained from variance maps of equivalent cardinality. The low-dimensionality of the covariances appears to be a crucial factor: when we squeeze the local covariances to make them even more low- dimensional, images retain much of their perceptual quality; when we do the opposite, and make the local covariances more high dimensional, images are corrupted with artiﬁcial textures. A related line of work by Karklin and Lewicki has studied the statistical ﬂuctuations of natural im- ages. Their initial model describes the variance of ﬁlter responses as linear combinations of a sparse set of latent coefﬁcients, thereby approximating the joint distributions of local ﬁlter responses as sep- arable (Karklin & Lewicki, 2005). More recently, they examined the invariance and discriminability of local ﬁlter response distributions, and modeled the log-covariance of image patches as a sparse sum of outer products drawn from a learned dictionary (Karklin & Lewicki, 2009). Ultimately, as in traditional sparse models such as sparse coding, ICA, ISA, or K-SVD (Olshausen & Field, 1996; Bell & Sejnowski, 1997; Hyv¨arinen & Hoyer, 2000; Rubinstein et al., 2013), these higher order coefﬁcients are assumed to be sparse along the axes of a ﬁxed, ﬁnite basis. On the contrary, the ﬁlter responses in our model are not required to be sparse along ﬁxed axes, but along the axes speciﬁed adaptively by the eigenvectors of the local covariance matrix. Approximat- ing an arbitrary subspace in a conventional sparse model would require dictionaries of a size that scales exponentially in the dimensionality of the input space (the so-called “curse of dimensional- ity”). In addition to its computational cost, the high coherence of such an overcomplete dictionary would make the inference of sparse coefﬁcients infeasible with convex relaxations (Donoho, 2006). Our model circumvents these problems by continuously parameterizing orientation in feature space. Similarly, continuous parametrizations of translation have been successfully embedded into sparse optimization problems (Ekanadham et al., 2011). These models avoid the brittleness of conventional sparse representations, which exhibit discontinuities when switching coefﬁcients on or off as a signal smoothly varies across space or time. Our model appears promising for a number of image processing applications. The property of local low-dimensionality provides a means of discriminating between natural images and noise, and thus offers a potentially powerful prior for denoising. Our synthesis experiments indicate that undercomplete or thresholded representations of the covariance map are sufﬁcient to reconstruct the original image with a high perceptual quality, suggesting that lossy compression schemes might make use of this representation to produce less visually salient distortions. Finally, we believe that local covariance map representations offer a natural path for extension into a hierarchical model. As an example, scattering networks have developed decompositions based on alternations of linear ﬁlters and local variance maps for applications such as texture synthesis and invariant object recognition (Bruna & Mallat, 2012). Hierarchical models of alternating linear ﬁlters and nonlinear pooling have also been proposed as an approximate description of computation along the visual cortical hierarchy in mammals (Riesenhuber & Poggio, 1999; Jarrett et al., 2009). Our synthesis experiments suggest that a similar infrastructure which recursively stacks linear decompo- sitions and covariance maps, with an objective of reducing local dimensionality, could offer a new canonical description for the biological visual hierarchy, and an unsupervised architecture for use in machine inference, synthesis, and recognition tasks.  ACKNOWLEDGMENTS  We would like to thank Joan Bruna for helpful discussions on the use of the nuclear norm and technical aspects of the phase-recovery problem.  9  Published as conference paper at ICLR 2015  REFERENCES Bell, Anthony J. and Sejnowski, Terrence J. The ‘independent components’ of natural scenes are  edge ﬁlters. 37(23):3327–3338, 1997.  Bruna, Joan and Mallat, St´ephane. Invariant Scattering Convolution Networks. arXiv.org, cs.CV,  March 2012.  Bruna, Joan, Szlam, Arthur, and LeCun, Yann. Signal Recovery from Pooling Representations.  arXiv.org, stat.ML, November 2013.  Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement. Torch7: A matlab-like environment  for machine learning. BigLearn, 2011.  Donoho, David. For most large underdetermined systems of equations, the minimal l1-norm near- solution approximates the sparsest near-solution. Communications on Pure and Applied Mathe- matics, 59(7):907–934, 2006.  Ekanadham, Chaitanya, Tranchina, Daniel, and Simoncelli, Eero P. Recovery of sparse translation- invariant signals with continuous basis pursuit. IEEE transactions on signal processing : a pub- lication of the IEEE Signal Processing Society, 59(10), October 2011.  Guerrero-Col´on, Jose A., Simoncelli, Eero P., and Portilla, Javier. Image denoising using mixtures  of Gaussian scale mixtures. pp. 565–568, 2008.  Hyv¨arinen, Aapo and Hoyer, Patrik.  by Decomposition  tures http://dx.doi.org/10.1162/089976600300015312, 2000.  of Natural  Emergence of Phase- and Shift-Invariant Fea- Images Independent Feature Subspaces.  into  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09). IEEE, 2009.  Karklin, Yan and Lewicki, Michael S. A Hierarchical Bayesian Model for Learning Nonlinear Statistical Regularities in Nonstationary Natural Signals. Neural Computation, 17(2):397–423, February 2005.  Karklin, Yan and Lewicki, Michael S. Emergence of complex cell properties by learning to gener-  alize in natural scenes. Nature, 457(7225):83–86, January 2009.  Lyu, Siwei and Simoncelli, Eero P. Nonlinear extraction of ‘independent components’ of natural images using radial Gaussianization. Neural Computation, 21(6):1485–1519, Jun 2009. doi: 10.1162/neco.2009.04-08-773.  Olshausen, Bruno A. and Field, David J. Emergence of simple-cell receptive ﬁeld properties by  learning a sparse code for natural images. Nature, 381(6583):607–609, 1996.  Portilla, Javier and Simoncelli, Eero P. A Parametric Texture Model Based on Joint Statistics of  Complex Wavelet Coefﬁcients. International Journal of Computer Vision, 40(1):49–70, 2000.  Riesenhuber, Maximilian and Poggio, Tomaso. Hierarchical models of object recognition in cortex.  Nature Neuroscience, 2:1019–1025, 1999.  Rubinstein, Ron, Peleg, Tomer, and Elad, Michael. Analysis K-SVD: a dictionary-learning algo- rithm for the analysis sparse model. IEEE transactions on signal processing : a publication of the IEEE Signal Processing Society, 61(3):661–677, 2013.  Ruderman, Daniel L. The statistics of natural images. Network: computation in neural systems,  1994.  Simoncelli, Eero P. Statistical models for images: Compression, restoration and synthesis. 1:673–  678, 1997.  10  Published as conference paper at ICLR 2015  van Hateren, J. H. and van der Schaaf, A. Independent component ﬁlters of natural images compared with simple cells in primary visual cortex. Proceedings: Biological Sciences, 265(1394):359–366, Mar 1998.  Wainwright, Martin J. and Simoncelli, Eero P. Scale mixtures of Gaussians and the statistics of natural images. In Solla, S. A., Leen, T. K., and M¨uller, K.-R. (eds.), Adv. Neural Information Processing Systems (NIPS*99), volume 12, pp. 855–861, Cambridge, MA, May 2000. MIT Press.  Zetzsche, C. and Krieger, G. The atoms of vision: Cartesian or polar? 16(7), July 1999.  11  ",
1412.6572,2015,Explaining and Harnessing Adversarial Examples,"['Explaining and Harnessing Adversarial Examples', 'Ian Goodfellow', 'Jon Shlens', 'and Christian Szegedy']",https://arxiv.org/pdf/1412.6572,"5 1 0 2    r a     M 0 2      ] L M  . t a t s [      3 v 2 7 5 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES  Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy Google Inc., Mountain View, CA {goodfellow,shlens,szegedy}@google.com  ABSTRACT  Several machine learning models, including neural networks, consistently mis- classify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed in- put results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to ad- versarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Us- ing this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.  INTRODUCTION  1 Szegedy et al. (2014b) made an intriguing discovery: several machine learning models, including state-of-the-art neural networks, are vulnerable to adversarial examples. That is, these machine learning models misclassify examples that are only slightly different from correctly classiﬁed exam- ples drawn from the data distribution. In many cases, a wide variety of models with different archi- tectures trained on different subsets of the training data misclassify the same adversarial example. This suggests that adversarial examples expose fundamental blind spots in our training algorithms. The cause of these adversarial examples was a mystery, and speculative explanations have suggested it is due to extreme nonlinearity of deep neural networks, perhaps combined with insufﬁcient model averaging and insufﬁcient regularization of the purely supervised learning problem. We show that these speculative hypotheses are unnecessary. Linear behavior in high-dimensional spaces is suf- ﬁcient to cause adversarial examples. This view enables us to design a fast method of generating adversarial examples that makes adversarial training practical. We show that adversarial training can provide an additional regularization beneﬁt beyond that provided by using dropout (Srivastava et al., 2014) alone. Generic regularization strategies such as dropout, pretraining, and model averaging do not confer a signiﬁcant reduction in a model’s vulnerability to adversarial examples, but changing to nonlinear model families such as RBF networks can do so. Our explanation suggests a fundamental tension between designing models that are easy to train due to their linearity and designing models that use nonlinear effects to resist adversarial perturbation. In the long run, it may be possible to escape this tradeoff by designing more powerful optimization methods that can succesfully train more nonlinear models. 2 RELATED WORK Szegedy et al. (2014b) demonstrated a variety of intriguing properties of neural networks and related models. Those most relevant to this paper include:  • Box-constrained L-BFGS can reliably ﬁnd adversarial examples. • On some datasets, such as ImageNet (Deng et al., 2009), the adversarial examples were so close to the original examples that the differences were indistinguishable to the human eye. • The same adversarial example is often misclassiﬁed by a variety of classiﬁers with different  architectures or trained on different subsets of the training data.  1  Published as a conference paper at ICLR 2015  • Shallow softmax regression models are also vulnerable to adversarial examples. • Training on adversarial examples can regularize the model—however, this was not practical  at the time due to the need for expensive constrained optimization in the inner loop.  These results suggest that classiﬁers based on modern machine learning techniques, even those that obtain excellent performance on the test set, are not learning the true underlying concepts that determine the correct output label. Instead, these algorithms have built a Potemkin village that works well on naturally occuring data, but is exposed as a fake when one visits points in space that do not have high probability in the data distribution. This is particularly disappointing because a popular approach in computer vision is to use convolutional network features as a space where Euclidean distance approximates perceptual distance. This resemblance is clearly ﬂawed if images that have an immeasurably small perceptual distance correspond to completely different classes in the network’s representation. These results have often been interpreted as being a ﬂaw in deep networks in particular, even though linear classiﬁers have the same problem. We regard the knowledge of this ﬂaw as an opportunity to ﬁx it. Indeed, Gu & Rigazio (2014) and Chalupka et al. (2014) have already begun the ﬁrst steps toward designing models that resist adversarial perturbation, though no model has yet succesfully done so while maintaining state of the art accuracy on clean inputs. 3 THE LINEAR EXPLANATION OF ADVERSARIAL EXAMPLES We start with explaining the existence of adversarial examples for linear models. In many problems, the precision of an individual input feature is limited. For example, digital images often use only 8 bits per pixel so they discard all information below 1/255 of the dynamic range. Because the precision of the features is limited, it is not rational for the classiﬁer to respond differently to an input x than to an adversarial input ˜x = x + η if every element of the perturbation η is smaller than the precision of the features. Formally, for problems with well-separated classes, we expect the classiﬁer to assign the same class to x and ˜x so long as ||η||∞ < (cid:15), where (cid:15) is small enough to be discarded by the sensor or data storage apparatus associated with our problem. Consider the dot product between a weight vector w and an adversarial example ˜x:  w(cid:62) ˜x = w(cid:62)x + w(cid:62)η.  The adversarial perturbation causes the activation to grow by w(cid:62)η.We can maximize this increase subject to the max norm constraint on η by assigning η = sign(w). If w has n dimensions and the average magnitude of an element of the weight vector is m, then the activation will grow by (cid:15)mn. Since ||η||∞ does not grow with the dimensionality of the problem but the change in activation caused by perturbation by η can grow linearly with n, then for high dimensional problems, we can make many inﬁnitesimal changes to the input that add up to one large change to the output. We can think of this as a sort of “accidental steganography,” where a linear model is forced to attend exclusively to the signal that aligns most closely with its weights, even if multiple signals are present and other signals have much greater amplitude. This explanation shows that a simple linear model can have adversarial examples if its input has suf- ﬁcient dimensionality. Previous explanations for adversarial examples invoked hypothesized prop- erties of neural networks, such as their supposed highly non-linear nature. Our hypothesis based on linearity is simpler, and can also explain why softmax regression is vulnerable to adversarial examples. 4 LINEAR PERTURBATION OF NON-LINEAR MODELS The linear view of adversarial examples suggests a fast way of generating them. We hypothesize that neural networks are too linear to resist linear adversarial perturbation. LSTMs (Hochreiter & Schmidhuber, 1997), ReLUs (Jarrett et al., 2009; Glorot et al., 2011), and maxout networks (Good- fellow et al., 2013c) are all intentionally designed to behave in very linear ways, so that they are easier to optimize. More nonlinear models such as sigmoid networks are carefully tuned to spend most of their time in the non-saturating, more linear regime for the same reason. This linear behavior suggests that cheap, analytical perturbations of a linear model should also damage neural networks.  2  Published as a conference paper at ICLR 2015  + .007 ×  =  x  “panda”  57.7% conﬁdence  sign(∇xJ(θ, x, y))  “nematode”  8.2% conﬁdence  x +  (cid:15)sign(∇xJ(θ, x, y))  “gibbon”  99.3 % conﬁdence  Figure 1: A demonstration of fast adversarial example generation applied to GoogLeNet (Szegedy et al., 2014a) on ImageNet. By adding an imperceptibly small vector whose elements are equal to the sign of the elements of the gradient of the cost function with respect to the input, we can change GoogLeNet’s classiﬁcation of the image. Here our (cid:15) of .007 corresponds to the magnitude of the smallest bit of an 8 bit image encoding after GoogLeNet’s conversion to real numbers.  Let θ be the parameters of a model, x the input to the model, y the targets associated with x (for machine learning tasks that have targets) and J(θ, x, y) be the cost used to train the neural network. We can linearize the cost function around the current value of θ, obtaining an optimal max-norm constrained pertubation of  η = (cid:15)sign (∇xJ(θ, x, y)) .  We refer to this as the “fast gradient sign method” of generating adversarial examples. Note that the required gradient can be computed efﬁciently using backpropagation. We ﬁnd that this method reliably causes a wide variety of models to misclassify their input. See Fig. 1 for a demonstration on ImageNet. We ﬁnd that using (cid:15) = .25, we cause a shallow softmax classiﬁer to have an error rate of 99.9% with an average conﬁdence of 79.3% on the MNIST (?) test set1. In the same setting, a maxout network misclassiﬁes 89.4% of our adversarial examples with an average conﬁdence of 97.6%. Similarly, using (cid:15) = .1, we obtain an error rate of 87.15% and an average probability of 96.6% assigned to the incorrect labels when using a convolutional maxout network on a preprocessed version of the CIFAR-10 (Krizhevsky & Hinton, 2009) test set2. Other simple methods of generating adversarial examples are possible. For example, we also found that rotating x by a small angle in the direction of the gradient reliably produces adversarial examples. The fact that these simple, cheap algorithms are able to generate misclassiﬁed examples serves as evidence in favor of our interpretation of adversarial examples as a result of linearity. The algorithms are also useful as a way of speeding up adversarial training or even just analysis of trained networks. 5 ADVERSARIAL TRAINING OF LINEAR MODELS VERSUS WEIGHT DECAY Perhaps the simplest possible model we can consider is logistic regression. In this case, the fast gradient sign method is exact. We can use this case to gain some intuition for how adversarial examples are generated in a simple setting. See Fig. 2 for instructive images.  If we train a single model to recognize labels y ∈ {−1, 1} with P (y = 1) = σ(cid:0)w(cid:62)x + b(cid:1) where  σ(z) is the logistic sigmoid function, then training consists of gradient descent on  Ex,y∼pdata ζ(−y(w(cid:62)x + b))  where ζ(z) = log (1 + exp(z)) is the softplus function. We can derive a simple analytical form for training on the worst-case adversarial perturbation of x rather than x itself, based on gradient sign  1This is using MNIST pixel values in the interval [0, 1]. MNIST data does contain values other than 0 or 1, but the images are essentially binary. Each pixel roughly encodes “ink” or “no ink”. This justiﬁes expecting the classiﬁer to be able to handle perturbations within a range of width 0.5, and indeed human observers can read such images without difﬁculty.  2 See https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/scripts/  papers/maxout. for the preprocessing code, which yields a standard deviation of roughly 0.5.  3  Published as a conference paper at ICLR 2015  (a)  (b)  (c)  (d)  Figure 2: The fast gradient sign method applied to logistic regression (where it is not an approxi- mation, but truly the most damaging adversarial example in the max norm box). a) The weights of a logistic regression model trained on MNIST. b) The sign of the weights of a logistic regression model trained on MNIST. This is the optimal perturbation. Even though the model has low capacity and is ﬁt well, this perturbation is not readily recognizable to a human observer as having anything to do with the relationship between 3s and 7s. c) MNIST 3s and 7s. The logistic regression model has a 1.6% error rate on the 3 versus 7 discrimination task on these examples. d) Fast gradient sign adversarial examples for the logistic regression model with (cid:15) = .25. The logistic regression model has an error rate of 99% on these examples.  perturbation. Note that the sign of the gradient is just −sign(w), and that w(cid:62)sign(w) = ||w||1. The adversarial version of logistic regression is therefore to minimize Ex,y∼pdataζ(y((cid:15)||w||1 − w(cid:62)x − b)).  This is somewhat similar to L1 regularization. However, there are some important differences. Most signiﬁcantly, the L1 penalty is subtracted off the model’s activation during training, rather than added to the training cost. This means that the penalty can eventually start to disappear if the model learns to make conﬁdent enough predictions that ζ saturates. This is not guaranteed to happen—in the underﬁtting regime, adversarial training will simply worsen underﬁtting. We can thus view L1 weight decay as being more “worst case” than adversarial training, because it fails to deactivate in the case of good margin. If we move beyond logistic regression to multiclass softmax regression, L1 weight decay becomes even more pessimistic, because it treats each of the softmax’s outputs as independently perturbable, when in fact it is usually not possible to ﬁnd a single η that aligns with all of the class’s weight vectors. Weight decay overestimates the damage achievable with perturbation even more in the case of a deep network with multiple hidden units. Because L1 weight decay overestimates the amount of damage an adversary can do, it is necessary to use a smaller L1 weight decay coefﬁcient than the (cid:15) associated with the precision of our features. When training maxout networks on MNIST, we obtained good results using adversarial training with (cid:15) = .25. When applying L1 weight decay to the ﬁrst layer, we found that even a coefﬁcient of .0025 was too large, and caused the model to get stuck with over 5% error on the training set. Smaller weight decay coefﬁcients permitted succesful training but conferred no regularization beneﬁt. 6 ADVERSARIAL TRAINING OF DEEP NETWORKS The criticism of deep networks as vulnerable to adversarial examples is somewhat misguided, be- cause unlike shallow linear models, deep networks are at least able to represent functions that resist adversarial perturbation. The universal approximator theorem (Hornik et al., 1989) guarantees that a neural network with at least one hidden layer can represent any function to an arbitary degree of accuracy so long as its hidden layer is permitted to have enough units. Shallow linear models are not able to become constant near training points while also assigning different outputs to different training points. Of course, the universal approximator theorem does not say anything about whether a training al- gorithm will be able to discover a function with all of the desired properties. Obviously, standard supervised training does not specify that the chosen function be resistant to adversarial examples. This must be encoded in the training procedure somehow.  4  Published as a conference paper at ICLR 2015  Szegedy et al. (2014b) showed that by training on a mixture of adversarial and clean examples, a neural network could be regularized somewhat. Training on adversarial examples is somewhat dif- ferent from other data augmentation schemes; usually, one augments the data with transformations such as translations that are expected to actually occur in the test set. This form of data augmenta- tion instead uses inputs that are unlikely to occur naturally but that expose ﬂaws in the ways that the model conceptualizes its decision function. At the time, this procedure was never demonstrated to improve beyond dropout on a state of the art benchmark. However, this was partially because it was difﬁcult to experiment extensively with expensive adversarial examples based on L-BFGS. We found that training with an adversarial objective function based on the fast gradient sign method was an effective regularizer:  ˜J(θ, x, y) = αJ(θ, x, y) + (1 − α)J(θ, x + (cid:15)sign (∇xJ(θ, x, y)) .  In all of our experiments, we used α = 0.5. Other values may work better; our initial guess of this hyperparameter worked well enough that we did not feel the need to explore more. This approach means that we continually update our supply of adversarial examples, to make them resist the current version of the model. Using this approach to train a maxout network that was also regularized with dropout, we were able to reduce the error rate from 0.94% without adversarial training to 0.84% with adversarial training. We observed that we were not reaching zero error rate on adversarial examples on the training set. We ﬁxed this problem by making two changes. First, we made the model larger, using 1600 units per layer rather than the 240 used by the original maxout network for this problem. Without adversarial training, this causes the model to overﬁt slightly, and get an error rate of 1.14% on the test set. With adversarial training, we found that the validation set error leveled off over time, and made very slow progress. The original maxout result uses early stopping, and terminates learning after the validation set error rate has not decreased for 100 epochs. We found that while the validation set error was very ﬂat, the adversarial validation set error was not. We therefore used early stopping on the adversarial validation set error. Using this criterion to choose the number of epochs to train for, we then retrained on all 60,000 examples. Five different training runs using different seeds for the random number generators used to select minibatches of training examples, initialize model weights, and generate dropout masks result in four trials that each had an error rate of 0.77% on the test set and one trial that had an error rate of 0.83%. The average of 0.782% is the best result reported on the permutation invariant version of MNIST, though statistically indistinguishable from the result obtained by ﬁne-tuning DBMs with dropout (Srivastava et al., 2014) at 0.79%. The model also became somewhat resistant to adversarial examples. Recall that without adversarial training, this same kind of model had an error rate of 89.4% on adversarial examples based on the fast gradient sign method. With adversarial training, the error rate fell to 17.9%. Adversarial examples are transferable between the two models but with the adversarially trained model showing greater robustness. Adversarial examples generated via the original model yield an error rate of 19.6% on the adversarially trained model, while adversarial examples generated via the new model yield an error rate of 40.9% on the original model. When the adversarially trained model does misclassify an adversarial example, its predictions are unfortunately still highly conﬁdent. The average conﬁdence on a misclassiﬁed example was 81.4%. We also found that the weights of the learned model changed signiﬁcantly, with the weights of the adversarially trained model being signiﬁcantly more localized and interpretable (see Fig. 3). The adversarial training procedure can be seen as minimizing the worst case error when the data is perturbed by an adversary. That can be interpreted as learning to play an adversarial game, or as minimizing an upper bound on the expected cost over noisy samples with noise from U (−(cid:15), (cid:15)) added to the inputs. Adversarial training can also be seen as a form of active learning, where the model is able to request labels on new points. In this case the human labeler is replaced with a heuristic labeler that copies labels from nearby points. We could also regularize the model to be insensitive to changes in its features that are smaller than the (cid:15) precision simply by training on all points within the (cid:15) max norm box, or sampling many points within this box. This corresponds to adding noise with max norm (cid:15) during training. However, noise with zero mean and zero covariance is very inefﬁcient at preventing adversarial examples. The expected dot product between any reference vector and such a noise vector is zero. This means that in many cases the noise will have essentially no effect rather than yielding a more difﬁcult input.  5  Published as a conference paper at ICLR 2015  Figure 3: Weight visualizations of maxout networks trained on MNIST. Each row shows the ﬁlters for a single maxout unit. Left) Naively trained model. Right) Model with adversarial training. In fact, in many cases the noise will actualy result in a lower objective function value. We can think of adversarial training as doing hard example mining among the set of noisy inputs, in order to train more efﬁciently by considering only those noisy points that strongly resist classiﬁcation. As control experiments, we trained training a maxout network with noise based on randomly adding ±(cid:15) to each pixel, or adding noise in U (−(cid:15), (cid:15)) to each pixel. These obtained an error rate of 86.2% with conﬁdence 97.3% and an error rate of 90.4% with a conﬁdence of 97.8% respectively on fast gradient sign adversarial examples. Because the derivative of the sign function is zero or undeﬁned everywhere, gradient descent on the adversarial objective function based on the fast gradient sign method does not allow the model to anticipate how the adversary will react to changes in the parameters. If we instead adversarial examples based on small rotations or addition of the scaled gradient, then the perturbation process is itself differentiable and the learning can take the reaction of the adversary into account. However, we did not ﬁnd nearly as powerful of a regularizing result from this process, perhaps because these kinds of adversarial examples are not as difﬁcult to solve. One natural question is whether it is better to perturb the input or the hidden layers or both. Here the results are inconsistent. Szegedy et al. (2014b) reported that adversarial perturbations yield the best regularization when applied to the hidden layers. That result was obtained on a sigmoidal network. In our experiments with the fast gradient sign method, we ﬁnd that networks with hidden units whose activations are unbounded simply respond by making their hidden unit activations very large, so it is usually better to just perturb the original input. On saturating models such as the Rust model we found that perturbation of the input performed comparably to perturbation of the hidden layers. Perturbations based on rotating the hidden layers solve the problem of unbounded activations growing to make additive perturbations smaller by comparison. We were able to succesfully train maxout networks with rotational perturbations of the hidden layers. However, this did not yield nearly as strong of a regularizing effect as additive perturbation of the input layer. Our view of adversarial training is that it is only clearly useful when the model has the capacity to learn to resist adversarial examples. This is only clearly the case when a universal approximator theorem applies. Because the last layer of a neural network, the linear-sigmoid or linear-softmax layer, is not a universal approximator of functions of the ﬁnal hidden layer, this suggests that one is likely to encounter problems with underﬁtting when applying adversarial perturbations to the ﬁnal hidden layer. We indeed found this effect. Our best results with training using perturbations of hidden layers never involved perturbations of the ﬁnal hidden layer. 7 DIFFERENT KINDS OF MODEL CAPACITY One reason that the existence of adversarial examples can seem counter-intuitive is that most of us have poor intuitions for high dimensional spaces. We live in three dimensions, so we are not used to small effects in hundreds of dimensions adding up to create a large effect. There is another way that our intuitions serve us poorly. Many people think of models with low capacity as being unable to make many different conﬁdent predictions. This is not correct. Some models with low capacity do exhibit this behavior. For example shallow RBF networks with  p(y = 1 | x) = exp(cid:0)(x − µ)(cid:62)β(x − µ)(cid:1)  are only able to conﬁdently predict that the positive class is present in the vicinity of µ. Elsewhere, they default to predicting the class is absent, or have low-conﬁdence predictions.  6  Published as a conference paper at ICLR 2015  RBF networks are naturally immune to adversarial examples, in the sense that they have low con- ﬁdence when they are fooled. A shallow RBF network with no hidden layers gets an error rate of 55.4% on MNIST using adversarial examples generated with the fast gradient sign method and (cid:15) = .25. However, its conﬁdence on mistaken examples is only 1.2%. Its average conﬁdence on clean test examples is 60.6%. We can’t expect a model with such low capacity to get the right an- swer at all points of space, but it does correctly respond by reducing its conﬁdence considerably on points it does not “understand.” RBF units are unfortunately not invariant to any signiﬁcant transformations so they cannot generalize very well. We can view linear units and RBF units as different points on a precision-recall tradeoff curve. Linear units achieve high recall by responding to every input in a certain direction, but may have low precision due to responding too strongly in unfamiliar situations. RBF units achieve high precision by responding only to a speciﬁc point in space, but in doing so sacriﬁce recall. Motivated by this idea, we decided to explore a variety of models involving quadratic units, including deep RBF networks. We found this to be a difﬁcult task—very model with sufﬁcient quadratic inhibition to resist adversarial perturbation obtained high training set error when trained with SGD. 8 WHY DO ADVERSARIAL EXAMPLES GENERALIZE? An intriguing aspect of adversarial examples is that an example generated for one model is often misclassiﬁed by other models, even when they have different architecures or were trained on dis- joint training sets. Moreover, when these different models misclassify an adversarial example, they often agree with each other on its class. Explanations based on extreme non-linearity and over- ﬁtting cannot readily account for this behavior—why should multiple extremely non-linear model with excess capacity consistently label out-of-distribution points in the same way? This behavior is especially surprising from the view of the hypothesis that adversarial examples ﬁnely tile space like the rational numbers among the reals, because in this view adversarial examples are common but occur only at very precise locations. Under the linear view, adversarial examples occur in broad subspaces. The direction η need only have positive dot product with the gradient of the cost function, and (cid:15) need only be large enough. Fig. 4 demonstrates this phenomenon. By tracing out different values of (cid:15) we see that adversarial examples occur in contiguous regions of the 1-D subspace deﬁned by the fast gradient sign method, not in ﬁne pockets. This explains why adversarial examples are abundant and why an example misclassiﬁed by one classiﬁer has a fairly high prior probability of being misclassiﬁed by another classiﬁer. To explain why mutiple classiﬁers assign the same class to adversarial examples, we hypothesize that neural networks trained with current methodologies all resemble the linear classiﬁer learned on the same training set. This reference classiﬁer is able to learn approximately the same classiﬁcation weights when trained on different subsets of the training set, simply because machine learning algo- rithms are able to generalize. The stability of the underlying classiﬁcation weights in turn results in the stability of adversarial examples. To test this hypothesis, we generated adversarial examples on a deep maxout network and classiﬁed these examples using a shallow softmax network and a shallow RBF network. On examples that were misclassiﬁed by the maxout network, the RBF network predicted the maxout network’s class assignment only 16.0% of the time, while the softmax classiﬁer predict the maxout network’s class correctly 54.6% of the time. These numbers are largely driven by the differing error rate of the different models though. If we exclude our attention to cases where both models being compared make a mistake, then softmax regression predict’s maxout’s class 84.6% of the time, while the RBF network is able to predict maxout’s class only 54.3% of the time. For comparison, the RBF network can predict softmax regression’s class 53.6% of the time, so it does have a strong linear component to its own behavior. Our hypothesis does not explain all of the maxout network’s mistakes or all of the mistakes that generalize across models, but clearly a signiﬁcant proportion of them are consistent with linear behavior being a major cause of cross-model generalization. 9 ALTERNATIVE HYPOTHESES We now consider and refute some alternative hypotheses for the existence of adversarial examples. First, one hypothesis is that generative training could provide more constraint on the training pro-  7  Published as a conference paper at ICLR 2015  Figure 4: By tracing out different values of (cid:15), we can see that adversarial examples occur reliably for almost any sufﬁciently large value of (cid:15) provided that we move in the correct direction. Correct classiﬁcations occur only on a thin manifold where x occurs in the data. Most of Rn consists of adversarial examples and rubbish class examples (see the appendix). This plot was made from a naively trained maxout network. Left) A plot showing the argument to the softmax layer for each of the 10 MNIST classes as we vary (cid:15) on a single input example. The correct class is 4. We see that the unnormalized log probabilities for each class are conspicuously piecewise linear with (cid:15) and that the wrong classiﬁcations are stable across a wide region of (cid:15) values. Moreover, the predictions become very extreme as we increase (cid:15) enough to move into the regime of rubbish inputs. Right) The inputs used to generate the curve (upper left = negative (cid:15), lower right = positive (cid:15), yellow boxes indicate correctly classiﬁed inputs).  cess, or cause the model to learn what to distinguish “real” from “fake” data and be conﬁdent only on “real” data. The MP-DBM (Goodfellow et al., 2013a) provides a good model to test this hy- pothesis. Its inference procedure gets good classiﬁcation accuracy (an 0.88% error rate) on MNIST. This inference procedure is differentiable. Other generative models either have non-differentiable inference procedures, making it harder to compute adversarial examples, or require an additional non-generative discriminator model to get good classiﬁcation accuracy on MNIST. In the case of the MP-DBM, we can be sure that the generative model itself is responding to adversarial exam- ples, rather than the non-generative classiﬁer model on top. We ﬁnd that the model is vulnerable to adversarial examples. With an (cid:15) of 0.25, we ﬁnd an error rate of 97.5% on adversarial examples generated from the MNIST test set. It remains possible that some other form of generative training could confer resistance, but clearly the mere fact of being generative is not alone sufﬁcient. Another hypothesis about why adversarial examples exist is that individual models have strange quirks but averaging over many models can cause adversarial examples to wash out. To test this hy- pothesis, we trained an ensemble of twelve maxout networks on MNIST. Each network was trained using a different seed for the random number generator used to initialize the weights, generate dropout masks, and select minibatches of data for stochastic gradient descent. The ensemble gets an error rate of 91.1% on adversarial examples designed to perturb the entire ensemble with (cid:15) = .25. If we instead use adversarial examples designed to perturb only one member of the ensemble, the error rate falls to 87.9%. Ensembling provides only limited resistance to adversarial perturbation. 10 SUMMARY AND DISCUSSION As a summary, this paper has made the following observations:  They are a result of models being too linear, rather than too nonlinear.  • Adversarial examples can be explained as a property of high-dimensional dot products. • The generalization of adversarial examples across different models can be explained as a result of adversarial perturbations being highly aligned with the weight vectors of a model, and different models learning similar functions when trained to perform the same task. • The direction of perturbation, rather than the speciﬁc point in space, matters most. Space is not full of pockets of adversarial examples that ﬁnely tile the reals like the rational numbers. • Because it is the direction that matters most, adversarial perturbations generalize across  different clean examples.  8  15105051015†20001500100050005001000argument to softmax0123456789Published as a conference paper at ICLR 2015  regularization than dropout.  • We have introduced a family of fast methods for generating adversarial examples. • We have demonstrated that adversarial training can result in regularization; even further • We have run control experiments that failed to reproduce this effect with simpler but less • Models that are easy to optimize are easy to perturb. • Linear models lack the capacity to resist adversarial perturbation; only structures with a hidden layer (where the universal approximator theorem applies) should be trained to resist adversarial perturbation.  efﬁcient regularizers including L1 weight decay and adding noise.  • RBF networks are resistant to adversarial examples. • Models trained to model the input distribution are not resistant to adversarial examples. • Ensembles are not resistant to adversarial examples.  Some further observations concerning rubbish class examples are presented in the appendix:  • Rubbish class examples are ubiquitous and easily generated. • Shallow linear models are not resistant to rubbish class examples. • RBF networks are resistant to rubbish class examples.  Gradient-based optimization is the workhorse of modern AI. Using a network that has been designed to be sufﬁciently linear–whether it is a ReLU or maxout network, an LSTM, or a sigmoid network that has been carefully conﬁgured not to saturate too much– we are able to ﬁt most problems we care about, at least on the training set. The existence of adversarial examples suggests that being able to explain the training data or even being able to correctly label the test data does not imply that our models truly understand the tasks we have asked them to perform. Instead, their linear responses are overly conﬁdent at points that do not occur in the data distribution, and these conﬁdent predictions are often highly incorrect. This work has shown we can partially correct for this problem by explic- itly identifying problematic points and correcting the model at each of these points. However, one may also conclude that the model families we use are intrinsically ﬂawed. Ease of optimization has come at the cost of models that are easily misled. This motivates the development of optimization procedures that are able to train models whose behavior is more locally stable.  ACKNOWLEDGMENTS  We would like to thank Geoffrey Hinton and Ilya Sutskever for helpful discussions. We would also like to thank Jeff Dean, Greg Corrado, and Oriol Vinyals for their feedback on drafts of this article. We would like to thank the developers of Theano(Bergstra et al., 2010; Bastien et al., 2012), Pylearn2(Goodfellow et al., 2013b), and DistBelief (Dean et al., 2012). REFERENCES Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guil- laume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expres- sion compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy), June 2010. Oral Presentation.  Chalupka, K., Perona, P., and Eberhardt, F. Visual Causal Feature Learning. ArXiv e-prints, December 2014. Dean, Jeffrey, Corrado, Greg S., Monga, Rajat, Chen, Kai, Devin, Matthieu, Le, Quoc V., Mao, Mark Z., Ranzato, MarcAurelio, Senior, Andrew, Tucker, Paul, Yang, Ke, and Ng, Andrew Y. Large scale distributed deep networks. In NIPS, 2012.  Deng, Jia, Dong, Wei, Socher, Richard, jia Li, Li, Li, Kai, and Fei-fei, Li. Imagenet: A large-scale hierarchical  image database. In In CVPR, 2009.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectiﬁer neural networks. In JMLR W&CP: Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011), April 2011.  9  Published as a conference paper at ICLR 2015  Goodfellow, Ian J., Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Multi-prediction deep Boltzmann  machines. In Neural Information Processing Systems, December 2013a.  Goodfellow, Ian J., Warde-Farley, David, Lamblin, Pascal, Dumoulin, Vincent, Mirza, Mehdi, Pascanu, Razvan, Bergstra, James, Bastien, Fr´ed´eric, and Bengio, Yoshua. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013b.  Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout net- works. In Dasgupta, Sanjoy and McAllester, David (eds.), International Conference on Machine Learning, pp. 1319–1327, 2013c.  Gu, Shixiang and Rigazio, Luca. Towards deep neural network architectures robust to adversarial examples. In  NIPS Workshop on Deep Learning and Representation Learning, 2014.  Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997. Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert. Multilayer feedforward networks are universal  approximators. Neural Networks, 2:359–366, 1989.  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pp. 2146–2153. IEEE, 2009.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images. Technical  report, University of Toronto, 2009.  Nguyen, A., Yosinski, J., and Clune, J. Deep Neural Networks are Easily Fooled: High Conﬁdence Predictions  for Unrecognizable Images. ArXiv e-prints, December 2014.  Rust, Nicole, Schwartz, Odelia, Movshon, J. Anthony, and Simoncelli, Eero. Spatiotemporal elements of  macaque V1 receptive ﬁelds. Neuron, 46(6):945–956, 2005.  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15 (1):1929–1958, 2014.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du- mitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. Technical report, arXiv preprint arXiv:1409.4842, 2014a.  Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian J., ICLR, abs/1312.6199, 2014b. URL http:  Intriguing properties of neural networks.  and Fergus, Rob. //arxiv.org/abs/1312.6199.  A RUBBISH CLASS EXAMPLES A concept related to adversarial examples is the concept of examples drawn from a “rubbish class.” These examples are degenerate inputs that a human would classify as not belonging to any of the categories in the training set. If we call these classes in the training set “the positive classes,” then we want to be careful to avoid false positives on rubbish inputs–i.e., we do not want to classify a degenerate input as being something real. In the case of separate binary classiﬁers for each class, we want all classes output near zero probability of the class being present, and in the case of a multinoulli distribution over only the positive classes, we would prefer that the classiﬁer output a high-entropy (nearly uniform) distribution over the classes. The traditional approach to reducing vulnerability to rubbish inputs is to introduce an extra, constant output to the model representing the rubbish class (?). Nguyen et al. (2014) recently re-popularized the concept of the rubbish class in the context of computer vision under the name fooling images. As with adversarial examples, there has been a misconception that rubbish class false positives are hard to ﬁnd, and that they are primarily a problem faced by deep networks. Our explanation of adversarial examples as the result of linearity and high dimensional spaces also applies to analyzing the behavior of the model on rubbish class examples. Linear models produce more extreme predictions at points that are far from the training data than at points that are near the training data. In order to ﬁnd high conﬁdence rubbish false positives for such a model, we need only generate a point that is far from the data, with larger norms yielding more conﬁdence. RBF networks, which are not able to conﬁdently predict the presence of any class far from the training data, are not fooled by this phenomenon. We generated 10,000 samples from N (0, I784) and fed them into various classiﬁers on the MNIST dataset. In this context, we consider assigning a probability greater than 0.5 to any class to be an error. A naively trained maxout network with a softmax layer on top had an error rate of 98.35% on Gaussian rubbish examples with an average conﬁdence of 92.8% on mistakes. Changing the top layer to independent sigmoids dropped the error rate to 68% with an average conﬁdence on mistakes of 87.9%. On CIFAR-10, using 1,000 samples from N (0, I3072), a convolutional maxout net obtains an error rate of 93.4%, with an average conﬁdence of 84.4%.  10  Published as a conference paper at ICLR 2015  Figure 5: Randomly generated fooling images for a convolutional network trained on CIFAR- 10. These examples were generated by drawing a sample from an isotropic Gaussian, then taking a gradient sign step in the direction that increases the probability of the “airplane” class. Yellow boxes indicate samples that successfully fool the model into believing an airplane is present with at least 50% conﬁdence. “Airplane” is the hardest class to construct fooling images for on CIFAR-10, so this ﬁgure represents the worst case in terms of success rate. These experiments suggest that the optimization algorithms employed by Nguyen et al. (2014) are overkill (or perhaps only needed on ImageNet), and that the rich geometric structure in their fooling images are due to the priors encoded in their search procedures, rather than those structures being uniquely able to cause false positives. Though Nguyen et al. (2014) focused their attention on deep networks, shallow linear models have the same problem. A softmax regression model has an error rate of 59.8% on the rubbish examples, with an average conﬁdence on mistakes of 70.8%. If we use instead an RBF network, which does not behave like a linear function, we ﬁnd an error rate of 0%. Note that when the error rate is zero the average conﬁdence on a mistake is undeﬁned. Nguyen et al. (2014) focused on the problem of generating fooling images for a speciﬁc class, which is a harder problem than simply ﬁnding points that the network conﬁdently classiﬁes as belonging to any one class despite being defective. The above methods on MNIST and CIFAR-10 tend to have a very skewed distribution over classes. On MNIST, 45.3% of a naively trained maxout network’s false positives were classiﬁed as 5s, and none were classiﬁed as 8s. Likewise, on CIFAR-10, 49.7% of the convolutional network’s false positives were classiﬁed as frogs, and none were classiﬁed as airplanes, automobiles, horses, ships, or trucks. To solve the problem introduced by Nguyen et al. (2014) of generating a fooling image for a particular class, we propose adding (cid:15)∇xp(y = i | x) to a Gaussian sample x as a fast method of generating a fooling image classiﬁed as class i. If we repeat this sampling process until it succeeds, we a randomized algorithm with variable runtime. On CIFAR-10, we found that one sampling step had a 100% success rate for frogs and trucks, and the hardest class was airplanes, with a success rate of 24.7% per sampling step. Averaged over all ten classes, the method has an average per-step success rate of 75.3%. We can thus generate any desired class with a handful of samples and no special priors, rather than tens of thousands of generations of evolution. To conﬁrm that the resulting examples are indeed fooling images, and not images of real classes rendered by the gradient sign method, see Fig. 5. The success rate of this method in terms of generating members of class i may degrade for datasets with more classes, since the risk of inadvertently increasing the activation of a different class j increases in that case. We found that we were able to train a maxout network to have a zero percent error rate on Gaussian rubbish examples (it was still vulnerable to rubbish examples generated by applying a fast gradient sign step to a Gaussian sample) with no negative impact on its ability to classify clean examples. Unfortunately, unlike training on adversarial examples, this did not result in any signiﬁcant reduction of the model’s test set error rate. In conclusion, it appears that a randomly selected input to deep or shallow models built from linear parts is overwhelmingly likely to be processed incorrectly, and that these models only behave reasonably on a very thin manifold encompassing the training data.  11  ",
1412.6577,2015,Modeling Compositionality with Multiplicative Recurrent Neural Networks,"['Modeling Compositionality with Multiplicative Recurrent Neural Networks', 'Ozan Irsoy and Claire Cardie']",https://arxiv.org/pdf/1412.6577,"5 1 0 2     y a M 2         ]  G L . s c [      3 v 7 7 5 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  MODELING COMPOSITIONALITY WITH MULTIPLICATIVE RECURRENT NEURAL NETWORKS  Ozan ˙Irsoy & Claire Cardie Department of Computer Science Cornell University Ithaca, NY {oirsoy,cardie}@cs.cornell.edu  ABSTRACT  We present the multiplicative recurrent neural network as a general model for com- positional meaning in language, and evaluate it on the task of ﬁne-grained senti- ment analysis. We establish a connection to the previously investigated matrix- space models for compositionality, and show they are special cases of the mul- tiplicative recurrent net. Our experiments show that these models perform com- parably or better than Elman-type additive recurrent neural networks and outper- form matrix-space models on a standard ﬁne-grained sentiment analysis corpus. Furthermore, they yield comparable results to structural deep models on the re- cently published Stanford Sentiment Treebank without the need for generating parse trees.  1  INTRODUCTION  Recent advancements in neural networks and deep learning have provided fruitful applications for natural language processing (NLP) tasks. One important such advancement was the invention of word embeddings that represent a single word as a dense, low-dimensional vector in a meaning space (Bengio et al., 2001) from which numerous problems in NLP have beneﬁted (Collobert & Weston, 2008; Collobert et al., 2011). The natural next question, then, was how to properly map larger phrases into such dense representations for NLP tasks that require properly capturing their meaning. Most existing methods take a compositional approach by deﬁning a function that com- poses multiple word vector representations into a phrase representation (e.g. Mikolov et al. (2013b), Socher et al. (2013), Yessenalina & Cardie (2011)). Compositional matrix-space models (Rudolph & Giesbrecht, 2010; Yessenalina & Cardie, 2011), for example, represent phrase-level meanings in a vector space and represent words as matrices that act on this vector space. Therefore, a matrix assigned to a word should capture how it transforms the meaning space (e.g. negation or intensiﬁcation). Meaning representations for longer phrases are simply computed as a multiplication of word matrices in sequential order (left-to-right, for English). Their representational power, however, is accompanied by a large number of parameters — a matrix for every word in the vocabulary. Thus, learning can be difﬁcult. But sequential composition of words into phrases is not the only mechanism for tackling semantic composition. Recursive neural networks (Pollack, 1990), for example, employ a structural ap- proach to compositionality: the composition function for a phrase operates on its two children in a binary parse tree of the sentence. Single words are represented in a vector-space. Different ways of deﬁning the composition function lead to different variants of the recursive neural network. In Socher et al. (2011), a simple additive afﬁne function with an additional nonlinearity is used. The matrix-vector recursive neural network of Socher et al. (2012) extends this by assigning an addi- tional matrix to each word, similar to the aforementioned matrix-space models; and the composition function involves a matrix-vector multiplication of sibling representations. More recently, Socher et al. (2013) deﬁnes a bilinear tensor multiplication as the composition function — to capture mul- tiplicative interactions between siblings. On the other hand, recurrent neural networks (RNNs), a neural network architecture with se- quential prediction capabilities, implicitly model compositionality when applied to natural language  1  Published as a conference paper at ICLR 2015  sentences. Representation of a phrase can be conceptualized as a nonlinear function that acts on the network’s hidden layer (memory), which results from repeated function composition over the hidden layer and the next word in the phrase/sentence (see Section 3.2). Unfortunately, it is possible that conventional additive recurrent networks are not powerful enough to accommodate some of the more complex effects in language, as suggested in previous work on (multiplicative and additive variants of) recursive neural networks (e.g. Socher et al. (2013)). More speciﬁcally, even though ad- ditive models can theoretically model arbitrary functions when combined with a nonlinearity, they might require a very large number of hidden units, and learnability of large parameter sets from data might pose an issue. To this end we investigate the multiplicative recurrent neural network as a model for composi- tional semantic effects in language. Previously, this type of multiplicative sequential approach has been applied to a character-level text generation task (Sutskever et al., 2011). In this work, we inves- tigate its capacity for recognizing the sentiment of a sentence or a phrase represented as a sequence of dense word vectors. Like the matrix-space models, multiplicative RNNs are sequential models of language; and as a type of recurrent NN, they implicitly model compositionality. Like the very successful multiplicative recursive neural networks, multiplicative RNNs can capture the same types of sibling interactions, but are much simpler. In particular, no parse trees are required, so sequential computations replace the associated recursive computations and performance does not depend on the accuracy of the parser. We also show a connection between the multiplicative RNN and compositional matrix-space mod- els, which have also been applied to sentiment analysis (Rudolph & Giesbrecht, 2010; Yessenalina & Cardie, 2011). In particular, matrix-space models are effectively a special case of multiplicative RNNs in which a word is represented as a large “one-hot” vector instead of a dense small one. Thus, these networks carry over the idea of matrix-space models from a one-hot sparse representation to dense word vectors. They can directly employ word vector representations, which makes them better suited for semi-supervised learning given the plethora of word vector training schemes. Mul- tiplicative recurrent networks can be considered to unify these two views of distributed language processing — the operator semantics view of matrix-space models in which a word is interpreted as an operator acting on the meaning representation, and the sequential memory processing view of recurrent neural networks. Our experiments show that multiplicative RNNs provide comparable or better performance than conventional additive recurrent nets and matrix-space models in terms of ﬁne-grained sentiment detection accuracy. Furthermore, although the absence of parse tree information puts an additional learning burden on multiplicative RNNs, we ﬁnd that they can reach comparable performance to the recursive neural network variants that require parse tree annotations for each sentence.  2 RELATED WORK  Vector Space Models. In natural language processing, a common way of representing a single token as a vector is to use a “one-hot” vector per token, with a dimensionality of the vocabulary size. This results in a very high dimensional, sparse representation. Additionally, every word is put at an equal distance to one another, disregarding their syntactic or semantic similarities. Alternatively, a distributed representation maps a token to a real-valued dense vector of smaller size (usually on the order of 100 dimensions). Generally, these representations are learned in an unsupervised manner from a large corpus, e.g. Wikipedia. Various architectures have been explored to learn these embeddings (Bengio et al., 2001; Collobert & Weston, 2008; Mnih & Hinton, 2007; Mikolov et al., 2013a) which might have different generalization capabilities depending on the task (Turian et al., 2010). The geometry of the induced word vector space might have interesting semantic properties (king - man + woman ≈ queen) (Mikolov et al., 2013a;b). In this work, we employ such word vector representations as the initial input representation when training neural networks. Matrix Space Models. An alternative approach is to embed words into a matrix space, by assigning matrices to words. Intuitively, a matrix embedding of a word is desired in order to capture operator semantics: the embedding should model how a word transforms meaning when it is applied to a context. Baroni & Zamparelli (2010) partially apply this idea to model adjectives as matrices that act on noun vectors. In their theoretical work, Rudolph & Giesbrecht (2010) deﬁne a proper matrix space model by assigning every word to a matrix; representations for longer phrases are computed  2  Published as a conference paper at ICLR 2015  by matrix multiplication. They show that matrix space models generalize vector space models and argue that they are neurologically and psychologically plausible. Yessenalina & Cardie (2011) apply this model to ﬁne-grained sentiment detection. Socher et al. (2012) use a structural approach in which every word is assigned a matrix-vector pair, where the vector captures the meaning of the word in isolation and the matrix captures how it transforms meaning when applied to a vector. Compositionality in Vector and Matrix Spaces. Commutative vector operations such as addition (e.g. bag-of-words) or element-wise multiplication along with negation (Widdows, 2003) provide simple composition schemes (Mitchell & Lapata, 2010; Zanzotto et al., 2010). Even though they ignore the order of the words, they might prove effective depending on the length of the phrases, and on the task (Mikolov et al., 2013b). Other models for compositional distributional semantics emulate formal semantics by representing functions as tensors and arguments as vectors (e.g. (Clark, 2008; Coecke et al., 2010; Grefenstette et al., 2013)) for which (Grefenstette et al., 2013) generalise the tensor-learning approach of (Baroni & Zamparelli, 2010). More complex non-commutative composition functions can be modeled via sequential or structural models of the sentence. In par- ticular, compositionality in recurrent neural networks can be considered as tranformations on the memory (hidden layer) applied by successive word vectors in order. Recursive neural networks employ a structural setting where compositions of smaller phrases into larger ones are determined by their parent-children relationship in the associated binary parse tree (Socher et al., 2011; 2012; 2013). In matrix space models, compositionality is naturally modeled via function composition in sequence (Rudolph & Giesbrecht, 2010; Yessenalina & Cardie, 2011). Sentiment Analysis. Sentiment analysis has been a very active area among NLP researchers, at various granularities such as the word-, phrase-, sentence- or document-level (Pang & Lee, 2008). Besides preexisting work that tried to formulate the problem as binary classiﬁcation, recently ﬁne- grained approaches were explored (Yessenalina & Cardie, 2011; Socher et al., 2013). Ultimately, the vast majority of approaches do not tackle the task compositionally, and in addition to bag-of-words features, they incorporate engineered features to account for negators, intensiﬁers and contextual valence shifters (Polanyi & Zaenen, 2006; Wilson et al., 2005; Kennedy & Inkpen, 2006; Shaikh et al., 2007).  3 PRELIMINARIES  3.1 MATRIX-SPACE MODELS  A matrix-space model models a single word as a square matrix that transforms a meaning (state) vector to another vector in the same meaning space. Intuitively, a word is viewed as a function, or an operator (in this particular case, linear) that acts on the meaning representation. Therefore, a phrase (or any sequence of words) is represented as successive application of the individual operators inside the phrase. Let s = w1, w2, . . . , wT be a sequence of words of length T and let Mw ∈ Rm×m denote the matrix representation of a word w ∈ V where V is the vocabulary. Then, the representation of s is simply (1)  M (s) = Mw1Mw2 . . . MwT  which yields another linear transformation in the same space. Observe that this representation re- spects word order (unlike, e.g. a bag of words). Note that even though M (s) is modeled as a linear operator on the meaning space, M (s) as a function of {Mwi}i=1..T is not linear, since it constitutes multiplications of those terms. Applying this representation to a task is simply applying the function to an initial empty meaning vector h0, which results in a transformed, ﬁnal meaning vector h that then is used to make a decision on the phrase s. In the case of sentiment detection, a sentiment score y(s) can be assigned to s as follows:  y(s) = h(cid:62)u = h(cid:62)  (2) In such a supervised task, matrix-space model parameters {Mw}w∈V , h0, u are learned from data. h0 and u can be ﬁxed (without reducing the representative power of the model) to reduce the degrees of freedom during training.  0 M (s)u  3  Published as a conference paper at ICLR 2015  Figure 1: Vector x (blue) and tensor A (red) sliced along the dimension of x. Left. Dense word vector x computes a weighted sum over base matrices to get a square matrix, which then is used to transform the meaning vector. Right. One-hot word vector x with the same computation, which is equivalent to selecting one of the base matrices and falls back to a matrix-space model.  3.2 RECURRENT NEURAL NETWORKS  A recurrent neural network (RNN) is a class of neural network that has recurrent connections, which allow a form of memory. This makes them applicable for sequential prediction tasks of arbitrary spatio-temporal dimension. They model the conditional distribution of a set (or a sequence) of output variables, given an input sequence. In this work, we focus our attention on only Elman-type networks (Elman, 1990). In the Elman-type network, the hidden layer ht at time step t is computed from a nonlinear trans- formation of the current input layer xt and the previous hidden layer ht−1. Then, the ﬁnal output yt is computed using the hidden layer ht. One can interpret ht as an intermediate representation summarizing the past so far. More formally, given a sequence of vectors {xt}t=1..T , an Elman-type RNN operates by computing the following memory and output sequences:  ht = f (W xt + V ht−1 + b) yt = g(U ht + c)  (3) (4)  where f is a nonlinearity, such as the element-wise sigmoid function, g is the output nonlinearity, such as the softmax function, W and V are weight matrices between the input and hidden layer, and among the hidden units themselves (connecting the previous intermediate representation to the current one), respectively, while U is the output weight matrix, and b and c are bias vectors connected to hidden and output units, respectively. When yt is a scalar (hence, U is a row vector) and g is the sigmoid function, yt is simply the probability of a positive label, conditioned on {xτ}τ =1..t. For tasks requiring a single label per sequence (e.g. single sentiment score per sentence), we can discard intermediate outputs {yt}t=1..(T−1) and use the output of the last time step yT , where T is the length of the sequence. This also means that during training, external error is only incurred at the ﬁnal time step. In general, supervision can be applied at any intermediate time step whenever there are labels available in the dataset, even if intermediate time step labels are not to be used at the testing phase, since this makes training easier.  4 METHODOLOGY  4.1 MULTIPLICATIVE RECURRENT NEURAL NETWORK  A property of recurrent neural networks is that input layer activations and the hidden layer activations of the previous time step interact additively to make up the activations for hidden layers at the current  4  Published as a conference paper at ICLR 2015  time step. This might be rather restrictive for some applications, or difﬁcult to learn for modeling more complex input interactions. On the other hand, a multiplicative interaction of those layers might provide a better representation for some semantic analysis tasks. For sentiment detection, for example, “not” might be considered as a negation of the sentiment that comes after it, which might be more effectively modeled with multiplicative interactions. To this end, we investigate the multiplicative recurrent neural network (or the recurrent neural tensor network) for the sentiment analysis task that is the main focus of this paper (Sutskever et al., 2011). mRNNs retain the same interpretation of memory as RNNs, the only difference being the recursive deﬁnition of h:  ht = f (x(cid:62) yt = g(U ht + c)  t A[1..dh]ht−1 + W xt + V ht−1 + b)  (5) (6) where A is a dh × dx × dh tensor, and the bilinear operation x(cid:62)Ay deﬁnes another vector as (x(cid:62)Ay)i = x(cid:62)A[i]y where the right-hand side represents the standard vector matrix multiplications and A[i] is a single slice (matrix) of the tensor A. This means that a single entry of ht,i is not only a linear combination of entries xt,j and ht−1,k, but also includes multiplicative terms in the form of jkxt,jht−1,k. ai We can simplify Equation 5 and 6 by adding bias units to x and h:  (7) (8) where x(cid:48) = [x; 1] and h(cid:48) = [h; 1]. With this notation, W , V and b become part of the tensor A(cid:48) and c becomes part of the matrix U(cid:48).  t−1)  ht = f (x(cid:48)(cid:62) yt = g(U(cid:48)h(cid:48) t)  t A(cid:48)[1..dh]h(cid:48)  4.2 ORDINAL REGRESSION WITH NEURAL NETWORKS  Since ﬁne-grained sentiment labels denote intensity in addition to polarity, our class labels are or- dinal in nature. Therefore, we use an ordinal regression scheme for neural networks, as described in Cheng et al. (2008). Intuitively, each sentiment class denotes a threshold for which the instances belonging to the class have sentiment values less than or equal to. If an instance s belongs to class k, it automatically belongs to the lower order classes 1, . . . , k − 1, as well. Therefore, the target vector for instance s is r = [1, . . . , 1, 0, . . . , 0](cid:62) where ri = 1 if i < k and ri = 0 otherwise. This way, we can consider the output vector as a cumulative probability distribution on classes. Because of the way class labels are deﬁned, output response is not subject to normalization. There- fore, output layer nonlinearity in this case is the elementwise sigmoid function ( 1+exp(−xi) ) instead of the softmax function (  (cid:80) j exp(xj ) ) which is traditionally used for multiclass classiﬁcation.  exp(xi)  1  Note that with this scheme, output of the network is not necessarily consistent. To decode an output vector, we ﬁrstly binarize each entry, by assigning 0 if the entry is less than 0.5 and 1 otherwise, as in conventional binary classiﬁcation. Then we simply start from the entry with the lowest index, and whenever we observe a 0, we assume all of the entries with higher indices are also 0, which ensures that the resulting target vector has the proper ordinal form. As an example, [1, 0, 1, 0](cid:62) is mapped to [1, 0, 0, 0](cid:62). Then ﬁnally, we assign the corresponding integer label.  4.3 RELATIONSHIP TO MATRIX-SPACE MODEL  In this section we will show the connection between mRNNs and matrix space model. Let us assume a purely multiplicative mRNN, without the bias units in the input and hidden layers (equivalently, W = V = b = 0). In such an mRNN, we compute the hidden layer (memory) as follows:  ht = f (x(cid:62)  t Aht−1)  (9)  Furthermore, assume f = I is the identity mapping, rather than a nonlinearity function. We can view the tensor multiplication in two parts: A vector xt multiplied by a tensor A, resulting in a  5  Published as a conference paper at ICLR 2015  Figure 2: Hidden layer vectors reduced to 2 dimensions for various phrases. Left. Recurrent neural network. Right. Purely multiplicative recurrent neural tensor network. In mRNN, handling of negation is more nonlinear and correctly shifts the sentiment.  matrix which we will denote as M (wt), to make the dependence of the resulting matrix on the word wt explicit. Then the matrix-vector multiplication M (wt)ht−1 resulting in the vector ht. Therefore, we can write the same equation as:  ht = (x(cid:62)  t A)ht−1 = M (wt)ht−1  (10)  and unfolding the recursion, we have  (11) If we are interested in a scalar response for the whole sequence, we apply the output layer to the hidden layer at the ﬁnal time step:  ht = M (wt)M (wt−1) . . . M (w1)h0  yT = u(cid:62)hT = u(cid:62)M (wT ) . . . M (w1)h0  (12) which is the matrix space model if individual M (wt) were to be associated with the matrices of their corresponding words (Equation 2). Therefore, we can view mRNNs as a simpliﬁcation to matrix- space models in which we have a tensor A to extract a matrix for a word w from its associated word vector, rather than associating a matrix with every word. This can be viewed as learning a matrix-space model with parameter sharing. This reduces the number of parameters greatly: instead of having a matrix for every word in the vocabulary, we have a vector per word, and a tensor to extract matrices. Another interpretation of this is the following: instead of learning an individual linear operator Mw per word as in matrix-space models, mRNN learns dx number of base linear operators. mRNN, then, represents each word as a weighted sum of these base operators (weights given by the word vector x). Note that if x is a one-hot vector representation of a word instead of a dense word embedding (which means dx = |V|), then we have |V| matrices as the base set of operators, and x simply selects one of these matrices, essentially falling back to an exact matrix-space model (see Figure 1). Therefore mRNNs provide a natural transition of the matrix-space model from a one-hot sparse word representation to a low dimensional dense word embedding. Besides a reduction in the number of parameters, another potential advantage of mRNNs over matrix-space models is that the matrix-space model is task-dependent: for each task, one has to learn one matrix per word in the whole vocabulary. On the other hand, mRNNs can make use of task-independent word vectors (which can be learned in an unsupervised manner) and only the pa- rameters for the network would have to be task-dependent. This allows easier extension to multitask learning or transfer learning settings.  5 EXPERIMENTS  5.1 SETTING  Data. For experimental evaluation of the models, we use the manually annotated MPQA corpus (Wiebe et al., 2005) that contains 535 newswire documents annotated with phrase-level subjectivity  6  Published as a conference paper at ICLR 2015  Table 1: Average ranking losses (MPQA)  Table 2: Average accuracies (SST)  Method PRank Bag-of-words LogReg Matrix-spaceRand (dh = 3) Matrix-spaceBOW (dh = 3) RNN+ vec (dh = 315) mRNNI Rand (dh = 2) mRNNI vec (dh = 25) mRNN+ vec (dh = 25) mRNNtanh vec  (dh = 25)  Loss 0.7808 0.6665 0.7417 0.6375 0.5265 0.6799 0.5278 0.5232 0.5147  Method Bag-of-words NB Bag-of-words SVM Bigram NB VecAvg Recursivetanh MV-Recursivetanh mRecursivetanh Recurrent+ mRecurrent+  vec (dh = 315) vec (dh = 20)  Acc (%)  41.0 40.7 41.9 32.7 43.2 44.4 45.7 43.1 43.5  and intensity. We use the same scheme as Yessenalina & Cardie (2011) to preprocess and extract individual phrases from the annotated documents, and convert the annotations to an integer ordinal label {0, 1, 2, 3, 4} denoting a sentiment score from negative to positive. After preprocessing, we have 8022 phrases in total with an average length of 2.83. We use the training-validation-test set partitions provided by the authors to apply 10-fold CV and report average performance over ten folds. Additionally, we use the recently published Stanford Sentiment Treebank (SST) (Socher et al., 2013), which includes labels for 215,154 phrases in the parse trees of 11,855 sentences, with an average sentence length of 19.1. Similarly, real-valued sentiment labels are converted to an integer ordinal label in {0, . . . , 4} by simple thresholding. We use the single training-validation-test set partition provided by the authors. We do not make use of the parse trees in the treebank since our approach is not structural; however, we include the phrase-level supervised labels (at the internal nodes of the parse trees) as labels for partial sentences. Problem formulation. For experiments on the MPQA corpus, we employ an ordinal regression setting. For experiments on SST, we employ a simple multiclass classiﬁcation setting, to make the models directly comparable to previous work. In the classiﬁcation setting, output nonlinearity g is the softmax function, and the output y is a vector (cid:80) valued response with the class probabilities. Ordinal regression setting is as described in Section 4.2. Evaluation metrics. For experiments using the MPQA corpus, we use the ranking loss as in Yesse- i |yi − ri| where y and r are predicted and true scores nalina & Cardie (2011), deﬁned as 1 n i 1(yi = ri) as in Socher et al. respectively. For experiments using SST, we use accuracy, 1 n (2013). Word vectors. We experiment with both randomly initialized word vectors (RAND) and pretrained word vector representations (VEC). For pretrained word vectors, we use publicly available 300 dimensional word vectors by Mikolov et al. (2013b), trained on part of Google News dataset (∼100B words). When using pretrained word vectors, we do not ﬁnetune them to reduce the degree of freedom of our models. Additionally, matrix-space models are initialized with random matrices (RAND) or a bag-of-words regression model weights (BOW) as described in Yessenalina & Cardie (2011).  (cid:80)  5.2 RESULTS  Quantitative results on the MPQA corpus are reported in Table 1. The top group shows previous results from Yessenalina & Cardie (2011) and the bottom group shows our results. We observe that mRNN does slightly better that RNN with approximately the same number of pa- rameters (0.5232 vs. 0.5265). This suggests that multiplicative interactions improve the model over additive interactions. Even though the difference is not signiﬁcant in the test set, it is signiﬁcant in the development set. We partially attribute this effect to the test set variance. This also suggests that multiplicative models are indeed more powerful, but require more careful regularization, because early stopping with a high model variance might tend to overﬁt to the development set.  7  Published as a conference paper at ICLR 2015  The randomly initialized mRNN outperforms its equivalent randomly initialized matrix-space model (0.6799 vs. 0.7417), which suggests that more compact representations with shared parameters learned by mRNN indeed generalize better. The mRNN and RNN that use pretrained word vectors get the best results, which suggests the importance of good pretraining schemes, especially when supervised data is limited. This is also conﬁrmed by our preliminary experiments (which are not shown here) using other word vector training methods such as CW embeddings (Collobert & Weston, 2008) or HLBL (Mnih & Hinton, 2007), which yielded a signiﬁcant difference (about 0.1 − 0.2) in ranking loss. To test the effect of different nonlinearities, we experiment with the identity, rectiﬁer and tanh func- tions with mRNNs. Experiments show that there is small but consistent improvement as we use rectiﬁer or tanh over not using extra nonlinearity. The differences between rectiﬁer and identity, and tanh and rectiﬁer are not signiﬁcant; however, the difference between tanh and identity is sig- niﬁcant, suggesting a performance boost from using a nonlinear squashing function. Nonetheless, not using any nonlinearity is only marginally worse. A possible explanation is that since the squash- ing function is not the only source of nonlinearity in mRNNs (multiplicativeness is another source of nonlinearity), it is not as crucial. Results on the Stanford Sentiment Treebank are shown in Table 2. Again, the top group shows baselines from Socher et al. (2013) and the bottom group shows our results. Both RNN and mRNN outperform the conventional SVM and Naive Bayes baselines. We observe that RNN can get very close to the performance of Recursive Neural Network, which can be con- sidered its structural counterpart. mRNN further improves over RNN and performs better than the recursive net and worse than the matrix-vector recursive net. Note that none of the RNN-based methods employ parse trees of sentences, unlike their recursive neural network variants.  6 CONCLUSION AND DISCUSSION  In this work, we explore multiplicative recurrent neural networks as a model for the compositional interpretation of language. We evaluate on the task of ﬁne-grained sentiment analysis, in an ordinal regression setting and show that mRNNs outperform previous work on MPQA, and get compara- ble results to previous work on Stanford Sentiment Treebank without using parse trees. We also describe how mRNNs effectively generalize matrix-space models from a sparse 1-hot word vector representation to a distributed, dense representation. One beneﬁt of mRNNs over matrix-space models is their separation of task-independent word rep- resentations (vectors) from task-dependent classiﬁers (tensor), making them very easy to extend for semi-supervised learning or transfer learning settings. Slices of the tensor can be interpreted as base matrices of a simpliﬁed matrix-space model. Intuitively, every meaning factor (a dimension of the dense word vector) of a word has a separate operator acting on the meaning representation which we combine to get the operator of the word itself. From a parameter sharing perspective, mRNNs provide better models. For matrix-space models, an update over a sentence affects only the word matrices that occur in that particular sentence. On the other hand, in an mRNN, an update over a sentence affects the global tensor as well. With such an update, the network alters its operation for similar words towards a similar direction. One drawback of mRNNs over conventional additive RNNs is their increased model variance, re- sulting from multiplicative interactions. This can be tackled by a stricter regularization. Another future direction is to explore sparsity constraints on word vectors, which would mean that every word would select only a few base operators to act on the meaning representation.  ACKNOWLEDGMENTS  This work was supported in part by NSF grant IIS-1314778 and DARPA DEFT Grant FA8750- 13-2-0015. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of NSF, DARPA or the U.S. Government.  8  Published as a conference paper at ICLR 2015  REFERENCES Baroni, Marco and Zamparelli, Roberto. Nouns are vectors, adjectives are matrices: Representing adjective- In Proceedings of the 2010 Conference on Empirical Methods in  noun constructions in semantic space. Natural Language Processing, pp. 1183–1193. Association for Computational Linguistics, 2010.  Bengio, Yoshua, Ducharme, Rjean, Vincent, Pascal, Jauvin, Christian, K, Jaz, Hofmann, Thomas, Poggio, Tomaso, and Shawe-taylor, John. A neural probabilistic language model. In In Advances in Neural Infor- mation Processing Systems, 2001.  Cheng, Jianlin, Wang, Zheng, and Pollastri, Gianluca. A neural network approach to ordinal regression. In Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE Interna- tional Joint Conference on, pp. 1279–1284. IEEE, 2008.  Clark, Stephen. A compositional distributional model of meaning. In Proceedings of the Second Quantum  Interaction Symposium (QI-2008), 2008.  Coecke, B., Sadrzadeh, M., and S., Clark. Mathematical foundations for a compositional distributional model  of meaning. Linguistic Analysis, 36:345–384, 2010.  Collobert, Ronan and Weston, Jason. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pp. 160–167. ACM, 2008.  Collobert, Ronan, Weston, Jason, Bottou, L´eon, Karlen, Michael, Kavukcuoglu, Koray, and Kuksa, Pavel. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November 2011. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=1953048.2078186.  Elman, Jeffrey L. Finding structure in time. Cognitive science, 14(2):179–211, 1990.  Grefenstette, E., Dinu, G., Zhang, Y., Sadrzadeh, M., and Baroni, M. Multi-step regression learning for com- positional distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers, pp. 131–142, Potsdam, Germany, March 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W13-0112.  Kennedy, Alistair and Inkpen, Diana. Sentiment classiﬁcation of movie reviews using contextual valence  shifters. Computational Intelligence, 22(2):110–125, 2006.  Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efﬁcient estimation of word representations in  vector space. arXiv preprint arXiv:1301.3781, 2013a.  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp. 3111–3119, 2013b.  Mitchell, Jeff and Lapata, Mirella. Composition in distributional models of semantics. Cognitive Science, 34  (8):1388–1439, 2010.  Mnih, Andriy and Hinton, Geoffrey. Three new graphical models for statistical language modelling. In Pro-  ceedings of the 24th international conference on Machine learning, pp. 641–648. ACM, 2007.  Pang, Bo and Lee, Lillian. Opinion mining and sentiment analysis. Foundations and trends in information  retrieval, 2(1-2):1–135, 2008.  Polanyi, Livia and Zaenen, Annie. Contextual valence shifters. In Computing attitude and affect in text: Theory  and applications, pp. 1–10. Springer, 2006.  Pollack, J. B. Recursive distributed representations. Artiﬁcial Intelligence, 1:77–105, 1990.  Rudolph, Sebastian and Giesbrecht, Eugenie. Compositional matrix-space models of language. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 907–916. Association for Computational Linguistics, 2010.  Shaikh, Mostafa Al Masum, Prendinger, Helmut, and Mitsuru, Ishizuka. Assessing sentiment of text by se- mantic dependency and contextual valence analysis. In Affective Computing and Intelligent Interaction, pp. 191–202. Springer, 2007.  Socher, Richard, Lin, Cliff C, Ng, Andrew, and Manning, Chris. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 129–136, 2011.  9  Published as a conference paper at ICLR 2015  Socher, Richard, Huval, Brody, Manning, Christopher D, and Ng, Andrew Y. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 1201–1211. Associa- tion for Computational Linguistics, 2012.  Socher, Richard, Perelygin, Alex, Wu, Jean Y, Chuang, Jason, Manning, Christopher D, Ng, Andrew Y, and Potts, Christopher. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’13, 2013.  Sutskever, Ilya, Martens, James, and Hinton, Geoffrey E. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 1017–1024, 2011.  Turian, Joseph, Ratinov, Lev, and Bengio, Yoshua. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 384–394. Association for Computational Linguistics, 2010.  Widdows, Dominic. Orthogonal negation in vector spaces for modelling word-meanings and document re- trieval. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 136– 143, Sapporo, Japan, July 2003. Association for Computational Linguistics. doi: 10.3115/1075096.1075114. URL http://www.aclweb.org/anthology/P03-1018.  Wiebe, Janyce, Wilson, Theresa, and Cardie, Claire. Annotating expressions of opinions and emotions in  language. Language resources and evaluation, 39(2-3):165–210, 2005.  Wilson, Theresa, Wiebe, Janyce, and Hoffmann, Paul. Recognizing contextual polarity in phrase-level senti- ment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pp. 347–354. Association for Computational Linguistics, 2005.  Yessenalina, Ainur and Cardie, Claire. Compositional matrix-space models for sentiment analysis. In Proceed- ings of the Conference on Empirical Methods in Natural Language Processing, pp. 172–182. Association for Computational Linguistics, 2011.  Zanzotto, Fabio Massimo, Korkontzelos, Ioannis, Fallucchi, Francesca, and Manandhar, Suresh. Estimating In Proceedings of the 23rd International Con- linear models for compositional distributional semantics. ference on Computational Linguistics (Coling 2010), pp. 1263–1271, Beijing, China, August 2010. Coling 2010 Organizing Committee. URL http://www.aclweb.org/anthology/C10-1142.  10  ",
1409.1556,2015,Very Deep Convolutional Networks for Large-Scale Image Recognition,"['Very Deep Convolutional Networks for Large-Scale Image Recognition', 'Karen Simonyan and Andrew Zisserman']",https://arxiv.org/pdf/1409.1556,"5 1 0 2    r p A 0 1         ]  V C . s c [      6 v 6 5 5 1  .  9 0 4 1 : v i X r a  Published as a conference paper at ICLR 2015  VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION  Karen Simonyan∗ & Andrew Zisserman+ Visual Geometry Group, Department of Engineering Science, University of Oxford {karen,az}@robots.ox.ac.uk  ABSTRACT  In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisa- tion and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili- tate further research on the use of deep visual representations in computer vision.  1  INTRODUCTION  Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale im- age and video recognition (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014) which has become possible due to the large public image reposito- ries, such as ImageNet (Deng et al., 2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al., 2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recog- nition Challenge (ILSVRC) (Russakovsky et al., 2014), which has served as a testbed for a few generations of large-scale image classiﬁcation systems, from high-dimensional shallow feature en- codings (Perronnin et al., 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky et al., 2012) (the winner of ILSVRC-2012).  With ConvNets becoming more of a commodity in the computer vision ﬁeld, a number of at- tempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC- 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) utilised smaller receptive window size and smaller stride of the ﬁrst convolutional layer. Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet et al., 2014; Howard, 2014). In this paper, we address another important aspect of ConvNet architecture design – its depth. To this end, we ﬁx other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution ﬁlters in all layers. As a result, we come up with signiﬁcantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classiﬁcation and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classiﬁed by a linear SVM without ﬁne-tuning). We have released our two best-performing models1 to facilitate further research. The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet conﬁgurations. The details of the image classiﬁcation training and evaluation are then presented in Sect. 3, and the  ∗current afﬁliation: Google DeepMind +current afﬁliation: University of Oxford and Google DeepMind 1http://www.robots.ox.ac.uk/˜vgg/research/very_deep/  1  Published as a conference paper at ICLR 2015  conﬁgurations are compared on the ILSVRC classiﬁcation task in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B. Finally, Appendix C contains the list of major paper revisions.  2 CONVNET CONFIGURATIONS  To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer conﬁgurations are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012). In this section, we ﬁrst describe a generic layout of our ConvNet conﬁgurations (Sect. 2.1) and then detail the speciﬁc conﬁgurations used in the evaluation (Sect. 2.2). Our design choices are then discussed and compared to the prior art in Sect. 2.3.  2.1 ARCHITECTURE  During training, the input to our ConvNets is a ﬁxed-size 224 × 224 RGB image. The only pre- processing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use ﬁlters with a very small receptive ﬁeld: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the conﬁgurations we also utilise 1 × 1 convolution ﬁlters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is ﬁxed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers. Spatial pooling is carried out by ﬁve max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2. A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the ﬁrst two have 4096 channels each, the third performs 1000- way ILSVRC classiﬁcation and thus contains 1000 channels (one for each class). The ﬁnal layer is the soft-max layer. The conﬁguration of the fully connected layers is the same in all networks.  All hidden layers are equipped with the rectiﬁcation (ReLU (Krizhevsky et al., 2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory con- sumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al., 2012).  2.2 CONFIGURATIONS  The ConvNet conﬁgurations, evaluated in this paper, are outlined in Table 1, one per column. In the following we will refer to the nets by their names (A–E). All conﬁgurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the ﬁrst layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512. In Table 2 we report the number of parameters for each conﬁguration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive ﬁelds (144M weights in (Sermanet et al., 2014)).  2.3 DISCUSSION  Our ConvNet conﬁgurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al., 2014). Rather than using relatively large receptive ﬁelds in the ﬁrst conv. lay- ers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al., 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al., 2014)), we use very small 3 × 3 receptive ﬁelds throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 × 3 conv. layers (without spatial pooling in between) has an effective receptive ﬁeld of 5 × 5; three  2  Published as a conference paper at ICLR 2015  Table 1: ConvNet conﬁgurations (shown in columns). The depth of the conﬁgurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as “convhreceptive ﬁeld sizei-hnumber of channelsi”. The ReLU activation function is not shown for brevity.  ConvNet Conﬁguration  A  11 weight  layers  A-LRN 11 weight  layers  B  C  D  E  13 weight  layers  16 weight  layers  16 weight  layers  19 weight  layers  input (224 × 224 RGB image)  conv3-64  conv3-64  LRN  conv3-64 conv3-64  conv3-64 conv3-64  conv3-64 conv3-64  conv3-64 conv3-64  conv3-128  conv3-128  conv3-128 conv3-128  conv3-128 conv3-128  conv3-128 conv3-128  conv3-128 conv3-128  maxpool  conv3-256 conv3-256  conv3-256 conv3-256  conv3-256 conv3-256  conv3-256 conv3-256 conv1-256  conv3-256 conv3-256 conv3-256  maxpool  conv3-512 conv3-512  conv3-512 conv3-512  conv3-512 conv3-512  conv3-512 conv3-512 conv1-512  conv3-512 conv3-512 conv3-512  maxpool  conv3-512 conv3-512  conv3-512 conv3-512  conv3-512 conv3-512  conv3-512 conv3-512 conv1-512  conv3-512 conv3-512 conv3-512  maxpool  conv3-256 conv3-256 conv3-256 conv3-256  conv3-512 conv3-512 conv3-512 conv3-512  conv3-512 conv3-512 conv3-512 conv3-512  maxpool FC-4096 FC-4096 FC-1000 soft-max  Table 2: Number of parameters (in millions).  Network Number of parameters  A,A-LRN  133  B 133  C 134  D 138  E 144  such layers have a 7 × 7 effective receptive ﬁeld. So what have we gained by using, for instance, a stack of three 3 × 3 conv. layers instead of a single 7 × 7 layer? First, we incorporate three non-linear rectiﬁcation layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by 3 (cid:0)32C 2(cid:1) = 27C 2 weights; at the same time, a single 7 × 7 conv. layer would require 72C 2 = 49C 2 parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7 × 7 conv. ﬁlters, forcing them to have a decomposition through the 3 × 3 ﬁlters (with non-linearity injected in between). The incorporation of 1 × 1 conv. layers (conﬁguration C, Table 1) is a way to increase the non- linearity of the decision function without affecting the receptive ﬁelds of the conv. layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectiﬁcation function. It should be noted that 1 × 1 conv. layers have recently been utilised in the “Network in Network” architecture of Lin et al. (2014).  Small-size convolution ﬁlters have been previously used by Ciresan et al. (2011), but their nets are signiﬁcantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classiﬁcation task, was developed independently of our work, but is similar in that it is based on very deep ConvNets  3  Published as a conference paper at ICLR 2015  (22 weight layers) and small convolution ﬁlters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions). Their network topology is, however, more complex than ours, and the spatial reso- lution of the feature maps is reduced more aggressively in the ﬁrst layers to decrease the amount of computation. As will be shown in Sect. 4.5, our model is outperforming that of Szegedy et al. (2014) in terms of the single-network classiﬁcation accuracy.  3 CLASSIFICATION FRAMEWORK  In the previous section we presented the details of our network conﬁgurations. In this section, we describe the details of classiﬁcation ConvNet training and evaluation.  3.1 TRAINING  The ConvNet training procedure generally follows Krizhevsky et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al., 1989)) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the L2 penalty multiplier set to 5 · 10−4) and dropout regularisation for the ﬁrst two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to 10−2, and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al., 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. ﬁlter sizes; (b) pre-initialisation of certain layers.  The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the conﬁguration A (Table 1), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the ﬁrst four convolutional layers and the last three fully- connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and 10−2 variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010).  To obtain the ﬁxed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal ﬂipping and random RGB colour shift (Krizhevsky et al., 2012). Training image rescaling is explained below.  Training image size. Let S be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to S as the training scale). While the crop size is ﬁxed to 224 × 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for S ≫ 224 the crop will correspond to a small part of the image, containing a small object or an object part.  We consider two approaches for setting the training scale S. The ﬁrst is to ﬁx S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi- scale image statistics). In our experiments, we evaluated models trained at two ﬁxed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014)) and S = 384. Given a ConvNet conﬁguration, we ﬁrst trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of 10−3. The second approach to setting S is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [Smin, Smax] (we used Smin = 256 and Smax = 512). Since objects in images can be of different size, it is beneﬁcial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single  4  Published as a conference paper at ICLR 2015  model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by ﬁne-tuning all layers of a single-scale model with the same conﬁguration, pre-trained with ﬁxed S = 384.  3.2 TESTING  At test time, given a trained ConvNet and an input image, it is classiﬁed in the following way. First, it is isotropically rescaled to a pre-deﬁned smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect. 4, using several values of Q for each S leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sermanet et al., 2014). Namely, the fully-connected layers are ﬁrst converted to convolutional layers (the ﬁrst FC layer to a 7 × 7 conv. layer, the last two FC layers to 1 × 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a ﬁxed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal ﬂipping of the images; the soft-max class posteriors of the original and ﬂipped images are averaged to obtain the ﬁnal scores for the image.  Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efﬁcient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szegedy et al. (2014), can lead to improved accuracy, as it results in a ﬁner sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive ﬁeld, so more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 ﬂips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014).  3.3  IMPLEMENTATION DETAILS  Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of signiﬁcant modiﬁcations, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU.  While more sophisticated methods of speeding up ConvNet training have been recently pro- posed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.  4 CLASSIFICATION EXPERIMENTS  Dataset. In this section, we present the image classiﬁcation results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 chal- lenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The clas- siﬁcation performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classiﬁcation error, i.e. the proportion of incorrectly classiﬁed images; the latter is the  5  Published as a conference paper at ICLR 2015  main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories.  For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the ofﬁcial ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al., 2014).  4.1 SINGLE SCALE EVALUATION  We begin with evaluating the performance of individual ConvNet models at a single scale with the layer conﬁgurations described in Sect. 2.2. The test image size was set as follows: Q = S for ﬁxed S, and Q = 0.5(Smin + Smax) for jittered S ∈ [Smin, Smax]. The results of are shown in Table 3. First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B–E).  Second, we observe that the classiﬁcation error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the conﬁguration C (which contains three 1 × 1 conv. layers), performs worse than the conﬁguration D, which uses 3 × 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. ﬁlters with non-trivial receptive ﬁelds (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneﬁcial for larger datasets. We also compared the net B with a shallow net with ﬁve 5 × 5 conv. layers, which was derived from B by replacing each pair of 3 × 3 conv. layers with a single 5 × 5 conv. layer (which has the same receptive ﬁeld as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which conﬁrms that a deep net with small ﬁlters outperforms a shallow net with larger ﬁlters.  Finally, scale jittering at training time (S ∈ [256; 512]) leads to signiﬁcantly better results than training on images with ﬁxed smallest side (S = 256 or S = 384), even though a single scale is used at test time. This conﬁrms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics.  Table 3: ConvNet performance at a single test scale.  ConvNet conﬁg. (Table 1)  smallest image side train (S) test (Q)  top-1 val. error (%)  top-5 val. error (%)  A A-LRN B  C  D  E  256 256 256 256 384  [256;512]  256 384  [256;512]  256 384  [256;512]  256 256 256 256 384 384 256 384 384 256 384 384  29.6 29.7 28.7 28.1 28.1 27.3 27.0 26.8 25.6 27.3 26.9 25.5  10.4 10.5 9.9 9.4 9.3 8.8 8.8 8.7 8.1 9.0 8.7 8.0  4.2 MULTI-SCALE EVALUATION  Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with ﬁxed S were evaluated over three test image sizes, close to the training one: Q = {S − 32, S, S + 32}. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable S ∈ [Smin; Smax] was evaluated over a larger range of sizes Q = {Smin, 0.5(Smin + Smax), Smax}.  6  Published as a conference paper at ICLR 2015  The results, presented in Table 4, indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table 3). As before, the deepest conﬁgurations (D and E) perform the best, and scale jittering is better than training with a ﬁxed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table 4). On the test set, the conﬁguration E achieves 7.3% top-5 error.  Table 4: ConvNet performance at multiple test scales.  ConvNet conﬁg. (Table 1)  smallest image side test (Q)  train (S)  top-1 val. error (%)  top-5 val. error (%)  B  C  D  E  256 256 384  [256; 512]  256 384  [256; 512]  256 384  [256; 512]  224,256,288 224,256,288 352,384,416 256,384,512 224,256,288 352,384,416 256,384,512 224,256,288 352,384,416 256,384,512  28.2 27.7 27.8 26.3 26.6 26.5 24.8 26.9 26.7 24.8  9.6 9.2 9.2 8.2 8.6 8.6 7.5 8.7 8.6 7.5  4.3 MULTI-CROP EVALUATION  In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2 for de- tails). We also assess the complementarity of the two evaluation techniques by averaging their soft- max outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions.  Table 5: ConvNet evaluation techniques comparison. In all experiments the training scale S was sampled from [256; 512], and three test scales Q were considered: {256, 384, 512}.  ConvNet conﬁg. (Table 1)  Evaluation method  top-1 val. error (%)  top-5 val. error (%)  D  E  4.4 CONVNET FUSION  dense  multi-crop  multi-crop & dense  dense  multi-crop  multi-crop & dense  24.8 24.6 24.4 24.8 24.6 24.4  7.5 7.5 7.2 7.5 7.4 7.1  Up until now, we evaluated the performance of individual ConvNet models. In this part of the exper- iments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014).  The results are shown in Table 6. By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by ﬁne-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (conﬁgurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table 5).  4.5 COMPARISON WITH THE STATE OF THE ART  Finally, we compare our results with the state of the art in Table 7. In the classiﬁcation task of ILSVRC-2014 challenge (Russakovsky et al., 2014), our “VGG” team secured the 2nd place with  7  Published as a conference paper at ICLR 2015  Combined ConvNet models  Table 6: Multiple ConvNet fusion results.  Error  top-1 val top-5 val top-5 test  ILSVRC submission  (D/256/224,256,288), (D/384/352,384,416), (D/[256;512]/256,384,512) (C/256/224,256,288), (C/384/352,384,416) (E/256/224,256,288), (E/384/352,384,416)  post-submission  (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), dense eval. (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop & dense eval.  24.7  24.0 23.9 23.7  7.5  7.1 7.2 6.8  7.3  7.0 - 6.8  7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models. As can be seen from Table 7, our very deep ConvNets signiﬁcantly outperform the previous gener- ation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competi- tions. Our result is also competitive with respect to the classiﬁcation task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models – signiﬁcantly less than used in most ILSVRC submissions. In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart from the classical ConvNet architecture of LeCun et al. (1989), but improved it by substantially increasing the depth.  top-1 val. error (%) top-5 val. error (%) top-5 test error (%)  23.7 24.4 24.7  Table 7: Comparison with the state of the art in ILSVRC classiﬁcation. Our method is denoted as “VGG”. Only the results obtained without outside training data are reported. Method VGG (2 nets, multi-crop & dense eval.) VGG (1 net, multi-crop & dense eval.) VGG (ILSVRC submission, 7 nets, dense eval.) GoogLeNet (Szegedy et al., 2014) (1 net) GoogLeNet (Szegedy et al., 2014) (7 nets) MSRA (He et al., 2014) (11 nets) MSRA (He et al., 2014) (1 net) Clarifai (Russakovsky et al., 2014) (multiple nets) Clarifai (Russakovsky et al., 2014) (1 net) Zeiler & Fergus (Zeiler & Fergus, 2013) (6 nets) Zeiler & Fergus (Zeiler & Fergus, 2013) (1 net) OverFeat (Sermanet et al., 2014) (7 nets) OverFeat (Sermanet et al., 2014) (1 net) Krizhevsky et al. (Krizhevsky et al., 2012) (5 nets) Krizhevsky et al. (Krizhevsky et al., 2012) (1 net)  8.1 9.1 11.7 12.5 14.8 16.1 13.6  36.0 37.5 34.0 35.7 38.1 40.7  14.7 16.0 13.2 14.2 16.4 18.2  - 9.1 - -  6.8 7.1 7.5  6.8 7.0 7.3  7.9 6.7  27.9  16.4  - - -  - -  -  -  5 CONCLUSION  In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large- scale image classiﬁcation. It was demonstrated that the representation depth is beneﬁcial for the classiﬁcation accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again conﬁrm the importance of depth in visual representations.  ACKNOWLEDGEMENTS  This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.  8  Published as a conference paper at ICLR 2015  REFERENCES  Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context  database. CoRR, abs/1412.0623, 2014.  Chatﬁeld, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep  into convolutional nets. In Proc. BMVC., 2014.  Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional ﬁlter banks for texture recognition and segmentation.  CoRR, abs/1411.6836, 2014.  Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance  convolutional neural networks for image classiﬁcation. In IJCAI, pp. 1237–1242, 2011.  Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang,  K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In NIPS, pp. 1232–1240, 2012.  Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image  database. In Proc. CVPR, 2009.  Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional  activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013.  Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual  object classes challenge: A retrospective. IJCV, 111(1):98–136, 2015.  Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An In IEEE CVPR Workshop of Generative  incremental bayesian approach tested on 101 object categories. Model Based Vision, 2004.  Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection  and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014.  Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604,  2014.  Glorot, X. and Bengio, Y. Understanding the difﬁculty of training deep feedforward neural networks. In Proc.  AISTATS, volume 9, pp. 249–256, 2010.  Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street  view imagery using deep convolutional neural networks. In Proc. ICLR, 2014.  Grifﬁn, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California  Institute of Technology, 2007.  He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual  recognition. CoRR, abs/1406.4729v2, 2014.  Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014. Howard, A. G. Some improvements on deep convolutional neural network based image classiﬁcation. In Proc.  ICLR, 2014.  Jia, Y.  Caffe:  An open source  convolutional  architecture  for  fast  feature  embedding.  http://caffe.berkeleyvision.org/, 2013.  Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR,  abs/1412.2306, 2014.  Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural  language models. CoRR, abs/1411.2539, 2014.  Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014. Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classiﬁcation with deep convolutional neural net-  works. In NIPS, pp. 1106–1114, 2012.  LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropa-  gation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.  Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. ICLR, 2014. Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR,  abs/1411.4038, 2014.  Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations  using Convolutional Neural Networks. In Proc. CVPR, 2014.  Perronnin, F., S´anchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classiﬁcation. In  Proc. ECCV, 2010.  Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline  for Recognition. CoRR, abs/1403.6382, 2014.  9  Published as a conference paper at ICLR 2015  Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., ImageNet large scale visual recognition challenge. CoRR,  Bernstein, M., Berg, A. C., and Fei-Fei, L. abs/1409.0575, 2014.  Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition,  Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014.  Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR,  abs/1406.2199, 2014. Published in Proc. NIPS, 2014.  Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,  A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.  Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR,  abs/1406.5726, 2014.  Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901,  2013. Published in Proc. ECCV, 2014.  A LOCALISATION  In the main body of the paper we have considered the classiﬁcation task of the ILSVRC challenge, and performed a thorough evaluation of ConvNet architectures of different depth. In this section, we turn to the localisation task of the challenge, which we have won in 2014 with 25.3% error. It can be seen as a special case of object detection, where a single object bounding box should be predicted for each of the top-5 classes, irrespective of the actual number of objects of the class. For this we adopt the approach of Sermanet et al. (2014), the winners of the ILSVRC-2013 localisation challenge, with a few modiﬁcations. Our method is described in Sect. A.1 and evaluated in Sect. A.2.  A.1 LOCALISATION CONVNET  To perform object localisation, we use a very deep ConvNet, where the last fully connected layer predicts the bounding box location instead of the class scores. A bounding box is represented by a 4-D vector storing its center coordinates, width, and height. There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR (Sermanet et al., 2014)) or is class-speciﬁc (per-class regression, PCR). In the former case, the last layer is 4-D, while in the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers and was found to be the best-performing in the classiﬁcation task (Sect. 4).  Training. Training of localisation ConvNets is similar to that of the classiﬁcation ConvNets (Sect. 3.1). The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth. We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 submission). Training was initialised with the corresponding classiﬁcation models (trained on the same scales), and the initial learning rate was set to 10−3. We explored both ﬁne-tuning all layers and ﬁne-tuning only the ﬁrst two fully-connected layers, as done in (Sermanet et al., 2014). The last fully-connected layer was initialised randomly and trained from scratch.  Testing. We consider two testing protocols. The ﬁrst is used for comparing different network modiﬁcations on the validation set, and considers only the bounding box prediction for the ground truth class (to factor out the classiﬁcation errors). The bounding box is obtained by applying the network only to the central crop of the image.  The second, fully-ﬂedged, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classiﬁcation task (Sect. 3.2). The difference is that instead of the class score map, the output of the last fully-connected layer is a set of bounding box predictions. To come up with the ﬁnal prediction, we utilise the greedy merging procedure of Sermanet et al. (2014), which ﬁrst merges spatially close predictions (by averaging their coor- dinates), and then rates them based on the class scores, obtained from the classiﬁcation ConvNet. When several localisation ConvNets are used, we ﬁrst take the union of their sets of bounding box predictions, and then run the merging procedure on the union. We did not use the multiple pooling  10  Published as a conference paper at ICLR 2015  offsets technique of Sermanet et al. (2014), which increases the spatial resolution of the bounding box predictions and can further improve the results.  A.2 LOCALISATION EXPERIMENTS  In this section we ﬁrst determine the best-performing localisation setting (using the ﬁrst test proto- col), and then evaluate it in a fully-ﬂedged scenario (the second protocol). The localisation error is measured according to the ILSVRC criterion (Russakovsky et al., 2014), i.e. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5.  Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the ﬁndings of Sermanet et al. (2014), where PCR was outperformed by SCR. We also note that ﬁne-tuning all layers for the lo- calisation task leads to noticeably better results than ﬁne-tuning only the fully-connected layers (as done in (Sermanet et al., 2014)). In these experiments, the smallest images side was set to S = 384; the results with S = 256 exhibit the same behaviour and are not shown for brevity.  Table 8: Localisation error for different modiﬁcations with the simpliﬁed testing protocol: the bounding box is predicted from a single central image crop, and the ground-truth class is used. All ConvNet layers (except for the last one) have the conﬁguration D (Table 1), while the last layer performs either single-class regression (SCR) or per-class regression (PCR). Fine-tuned layers regression type GT class localisation error  1st and 2nd FC  all  SCR PCR PCR  36.4 34.3 33.1  Fully-ﬂedged evaluation. Having determined the best localisation setting (PCR, ﬁne-tuning of all layers), we now apply it in the fully-ﬂedged scenario, where the top-5 class labels are predicted us- ing our best-performing classiﬁcation system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014). As can be seen from Ta- ble 9, application of the localisation ConvNet to the whole image substantially improves the results compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of the ground truth. Similarly to the classiﬁcation task (Sect. 4), testing at several scales and combining the predictions of multiple networks further improves the performance.  Table 9: Localisation error  smallest image side test (Q)  train (S)  256 384 384  256 384  352,384  fusion: 256/256 and 384/352,384  test.  top-5 localisation error (%) val. 29.5 28.2 27.5 26.9  25.3  26.7  -  -  Comparison with the state of the art. We compare our best localisation result with the state of the art in Table 10. With 25.3% test error, our “VGG” team won the localisation challenge of ILSVRC-2014 (Russakovsky et al., 2014). Notably, our results are considerably better than those of the ILSVRC-2013 winner Overfeat (Sermanet et al., 2014), even though we used less scales and did not employ their resolution enhancement technique. We envisage that better localisation per- formance can be achieved if this technique is incorporated into our method. This indicates the performance advancement brought by our very deep ConvNets – we got better results with a simpler localisation method, but a more powerful representation.  B GENERALISATION OF VERY DEEP FEATURES  In the previous sections we have discussed training and evaluation of very deep ConvNets on the ILSVRC dataset. In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature  11  Published as a conference paper at ICLR 2015  Table 10: Comparison with the state of the art in ILSVRC localisation. Our method is denoted as “VGG”.  top-5 val. error (%) top-5 test error (%)  Method VGG GoogLeNet (Szegedy et al., 2014) OverFeat (Sermanet et al., 2014) Krizhevsky et al. (Krizhevsky et al., 2012)  26.9  -  30.0  -  25.3 26.7 29.9 34.2  extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-ﬁtting. Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al., 2013; Razavian et al., 2014; Chatﬁeld et al., 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin. Following that line of work, we investigate if our models lead to better performance than more shallow models utilised in the state-of-the-art methods. In this evaluation, we consider two models with the best classiﬁcation performance on ILSVRC (Sect. 4) – conﬁgurations “Net-D” and “Net-E” (which we made publicly available).  To utilise the ConvNets, pre-trained on ILSVRC, for image classiﬁcation on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classiﬁcation), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales. The resulting image descriptor is L2-normalised and combined with a linear SVM classiﬁer, trained on the target dataset. For simplicity, pre-trained ConvNet weights are kept ﬁxed (no ﬁne-tuning is performed).  Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure (Sect. 3.2). Namely, an image is ﬁrst rescaled so that its smallest side equals Q, and then the net- work is densely applied over the image plane (which is possible when all weight layers are treated as convolutional). We then perform global average pooling on the resulting feature map, which produces a 4096-D image descriptor. The descriptor is then averaged with the descriptor of a hori- zontally ﬂipped image. As was shown in Sect. 4.2, evaluation over multiple scales is beneﬁcial, so we extract features over several scales Q. The resulting multi-scale features can be either stacked or pooled across scales. Stacking allows a subsequent classiﬁer to learn how to optimally combine image statistics over a range of scales; this, however, comes at the cost of the increased descriptor dimensionality. We return to the discussion of this design choice in the experiments below. We also assess late fusion of features, computed using two networks, which is performed by stacking their respective image descriptors.  Table 11: Comparison with the state of the art in image classiﬁcation on VOC-2007, VOC-2012, Caltech-101, and Caltech-256. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (2000 classes). Method  Caltech-101  Caltech-256  VOC-2007 VOC-2012 (mean AP) (mean AP)  (mean class recall) (mean class recall)  Zeiler & Fergus (Zeiler & Fergus, 2013) Chatﬁeld et al. (Chatﬁeld et al., 2014) He et al. (He et al., 2014) Wei et al. (Wei et al., 2014) VGG Net-D (16 layers) VGG Net-E (19 layers) VGG Net-D & Net-E  -  82.4 82.4  79.0 83.2  -  81.5 (85.2∗) 81.7 (90.3∗)  89.3 89.3 89.7  89.0 89.0 89.3  86.5 ± 0.5 88.4 ± 0.6 93.4 ± 0.5  -  91.8 ± 1.0 92.3 ± 0.5 92.7 ± 0.5  74.2 ± 0.3 77.6 ± 0.1  - -  85.0 ± 0.2 85.1 ± 0.3 86.2 ± 0.3  Image Classiﬁcation on VOC-2007 and VOC-2012. We begin with the evaluation on the image classiﬁcation task of PASCAL VOC-2007 and VOC-2012 benchmarks (Everingham et al., 2015). These datasets contain 10K and 22.5K images respectively, and each image is annotated with one or several labels, corresponding to 20 object categories. The VOC organisers provide a pre-deﬁned split into training, validation, and test data (the test data for VOC-2012 is not publicly available; instead, an ofﬁcial evaluation server is provided). Recognition performance is measured using mean average precision (mAP) across classes.  Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs sim-  12  Published as a conference paper at ICLR 2015  ilarly to the aggregation by stacking. We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-speciﬁc seman- tics which a classiﬁer could exploit. Since averaging has a beneﬁt of not inﬂating the descrip- tor dimensionality, we were able to aggregated image descriptors over a wide range of scales: Q ∈ {256, 384, 512, 640, 768}. It is worth noting though that the improvement over a smaller range of {256, 384, 512} was rather marginal (0.3%). The test set performance is reported and compared with other approaches in Table 11. Our networks “Net-D” and “Net-E” exhibit identical performance on VOC datasets, and their combination slightly improves the results. Our methods set the new state of the art across image representations, pre- trained on the ILSVRC dataset, outperforming the previous best result of Chatﬁeld et al. (2014) by more than 6%. It should be noted that the method of Wei et al. (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, semantically close to those in VOC datasets. It also beneﬁts from the fusion with an object detection-assisted classiﬁcation pipeline.  Image Classiﬁcation on Caltech-101 and Caltech-256. In this section we evaluate very deep fea- tures on Caltech-101 (Fei-Fei et al., 2004) and Caltech-256 (Grifﬁn et al., 2007) image classiﬁcation benchmarks. Caltech-101 contains 9K images labelled into 102 classes (101 object categories and a background class), while Caltech-256 is larger with 31K images and 257 classes. A standard eval- uation protocol on these datasets is to generate several random splits into training and test data and report the average recognition performance across the splits, which is measured by the mean class recall (which compensates for a different number of test images per class). Following Chatﬁeld et al. (2014); Zeiler & Fergus (2013); He et al. (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training images per class (and the rest is used for testing). In each split, 20% of training images were used as a validation set for hyper-parameter selection.  We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multi- ple scales, performs better than averaging or max-pooling. This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are se- mantically different (capturing the whole object vs. object parts), and stacking allows a classiﬁer to exploit such scale-speciﬁc representations. We used three scales Q ∈ {256, 384, 512}. Our models are compared to each other and the state of the art in Table 11. As can be seen, the deeper 19-layer Net-E performs better than the 16-layer Net-D, and their combination further improves the performance. On Caltech-101, our representations are competitive with the approach of He et al. (2014), which, however, performs signiﬁcantly worse than our nets on VOC-2007. On Caltech-256, our features outperform the state of the art (Chatﬁeld et al., 2014) by a large margin (8.6%).  Action Classiﬁcation on VOC-2012. We also evaluated our best-performing image representa- tion (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classiﬁcation task (Everingham et al., 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action. The dataset contains 4.6K training im- ages, labelled into 11 classes. Similarly to the VOC-2012 object classiﬁcation task, the performance is measured using the mAP. We considered two training settings: (i) computing the ConvNet fea- tures on the whole image and ignoring the provided bounding box; (ii) computing the features on the whole image and on the provided bounding box, and stacking them to obtain the ﬁnal representation. The results are compared to other approaches in Table 12.  Our representation achieves the state of art on the VOC action classiﬁcation task even without using the provided bounding boxes, and the results are further improved when using both images and bounding boxes. Unlike other approaches, we did not incorporate any task-speciﬁc heuristics, but relied on the representation power of very deep convolutional features.  Other Recognition Tasks. Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, consistently outperform- ing more shallow representations. For instance, Girshick et al. (2014) achieve the state of the object detection results by replacing the ConvNet of Krizhevsky et al. (2012) with our 16-layer model. Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been ob-  13  Published as a conference paper at ICLR 2015  Table 12: Comparison with the state of the art in single-image action classiﬁcation on VOC- 2012. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (1512 classes).  Method (Oquab et al., 2014) (Gkioxari et al., 2014) (Hoai, 2014) VGG Net-D & Net-E, image-only VGG Net-D & Net-E, image and bounding box  VOC-2012 (mean AP)  70.2∗ 73.6 76.3 79.2 84.0  served in semantic segmentation (Long et al., 2014), image caption generation (Kiros et al., 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al., 2014; Bell et al., 2014).  C PAPER REVISIONS  Here we present the list of major paper revisions, outlining the substantial changes for the conve- nience of the reader. v1 Initial version. Presents the experiments carried out before the ILSVRC submission. v2 Adds post-submission ILSVRC experiments with training set augmentation using scale jittering, which improves the performance. v3 Adds generalisation experiments (Appendix B) on PASCAL VOC and Caltech image classiﬁca- tion datasets. The models used for these experiments are publicly available. v4 The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple crops for classiﬁcation. v6 Camera-ready ICLR-2015 conference paper. Adds a comparison of the net B with a shallow net and the results on PASCAL VOC action classiﬁcation benchmark.  14  ",
1412.6553,2015,Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition,"['Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition', 'Vadim Lebedev', 'Yaroslav Ganin', 'Victor Lempitsky', 'Maksim Rakhuba', 'and Ivan Oseledets']",https://arxiv.org/pdf/1412.6553,"Published as a conference paper at ICLR 2015  SPEEDING-UP CONVOLUTIONAL NEURAL NETWORKS USING FINE-TUNED CP-DECOMPOSITION  Vadim Lebedev1,2, Yaroslav Ganin1, Maksim Rakhuba1,3, Ivan Oseledets1,4, and Victor Lempitsky1  1Skolkovo Institute of Science and Technology (Skoltech), Moscow, Russia  2Yandex, Moscow, Russia  3Moscow Institute of Physics and Technology, Moscow Region, Russia  4Institute of Numerical Mathematics RAS, Moscow, Russia  5 1 0 2    r p A 4 2         ]  V C . s c [      3 v 3 5 5 6  .  2 1 4 1 : v i X r a  ABSTRACT  We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discrim- inative ﬁne-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is ﬁne-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it is competitive with pre- vious approaches, leading to higher obtained CPU speedups at the cost of lower accuracy drops for the smaller of the two networks. Thus, for the 36-class char- acter classiﬁcation CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1% from 91% to 90%). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of 1% increase of the overall top-5 classiﬁcation error.  1  INTRODUCTION  Over the course of two years, Convolutional neural networks (CNNs) (LeCun et al., 1989) have revolutionized computer vision and became ubiquituous through a range of computer vision ap- plications. In many ways, this breakthrough has become possible through the acceptance of new computational tools, most notably GPUs (Krizhevsky et al., 2012), but also CPU clusters (Dean et al., 2012) and FPGAs (Farabet et al., 2011). On the other hand, there is a rising interest to deploy- ing CNNs on low-end architectures, such as standalone desktop/laptop CPUs, mobile processors, and CPU in robotics. On such processors, the computational cost of applying, yet alone training a CNN might pose a problem, especially when real-time operation is required. The key layer of CNNs that distinguishes them from other neural networks and enables their re- cent success is the convolution operation. Convolutions dominate the computation cost associated with training or testing a CNN (especially for newer architectures such as (Simonyan & Zisserman, 2014), which tend to add more and more convolutional layers often at the expense of fully-connected layers). Consequently, there is a strong interest to the task of improving the efﬁciency of this opera- tion (Chintala, 2014; Mathieu et al., 2013; Chellapilla et al., 2006). An efﬁcient implementation of the convolution operation is one of the most important elements of all major CNN packages. A group of recent works have achieved signiﬁcant speed-ups of CNN convolutions by applying ten- sor decompositions. In more detail, recall that a typical convolution in a CNN takes as an input a 3D tensor (array) with the dimensions corresponding to the two spatial dimensions, and to different image maps. The output of a convolution is another similarly-structured 3D tensor. The convolution kernel itself constitutes a 4D tensor with dimensions corresponding to the two spatial dimensions, the input image maps, and the output image maps. Exploiting this tensor structure, previous works  1  Published as a conference paper at ICLR 2015  (Denton et al., 2014; Jaderberg et al., 2014a) have suggested different tensor decomposition schemes to speed-up convolutions within CNNs. These schemes are applied to the kernel tensor and gener- alize previous 2D ﬁlter approximations in computer vision like (Rigamonti et al., 2013). In this work, we further investigate tensor decomposition in the context of speeding up convolutions within CNNs. We use a standard (in tensor algebra) low-rank CP-decomposition (also known as PARAFAC or CANDECOMP) and an efﬁcient optimization approach (non-linear least squares) to decompose the full kernel tensor. We demonstrate that the use of the CP-decomposition on a full kernel tensor has the following important advantages:  • Ease of the decomposition implementation. CP-decomposition is one of the standard tools in tensor linear algebra with well understood properties and mature implementations. Consequently, one can use existing efﬁcient algorithms such as NLS to compute it efﬁ- ciently. • Ease of the CNN implementation. CP-decomposition approximates the convolution with a 4D kernel tensor by the sequence of four convolutions with small 2D kernel tensors. All these lower dimensional convolutions represent standard CNN layers, and are therefore easy to insert into a CNN using existing CNN packages (no custom layer implementation is needed). • Ease of ﬁne-tuning. Related to the previous item, once a convolutional layer is approxi- mated and replaced with a sequence of four convolutional layers with smaller kernels, it is straight-forward to ﬁne-tune the entire network on training data using back-propagation. • Efﬁciency. Perhaps most importantly, we found that the simple combination of the full kernel CP-decomposition and global ﬁne-tuning can result in better speed-accuracy trade- off for the approximated networks than the previous methods.  The CNNs obtained in our method are somewhat surprisingly accurate, given that the number of parameters is reduced several times compared to the initial networks. Practically, this reduction in the number of parameters means more compact networks with reduced memory footprint, which can be important for architectures with limited RAM or storage memory. Such memory savings can be especially valuable for feed-forward networks that include convolutions with spatially-varying kernels (“locally-connected” layers). On the theoretical side, these results conﬁrm the intuition that modern CNNs are over-parameterized, i.e. that the sheer number of parameters in the modern CNNs are not needed to store the information about the classiﬁcation task but, rather, serve to facilitate convergence to good local minima of the loss function. Below, we ﬁrst discuss previous works of (Rigamonti et al., 2013; Jaderberg et al., 2014a; Denton et al., 2014) and outline the differences between them and our approach in Section 2. We then review the CP-decomposition and give the details of our approach in Section 3. The experiments on two CNNs, namely the character classiﬁcation CNN from (Jaderberg et al., 2014a) and (Jaderberg et al., 2014b) and AlexNet from the Caffe package (Jia et al., 2014), follow in Section 4. We conclude with the summary and the discussion in Section 5.  2 RELATED WORK  Decompositions for convolutions. Using low-rank decomposition to accelerate convolution was suggested by Rigamonti et al. (2013) in the context of codebook learning. In (Rigamonti et al., 2013), a bank of 2D or 3D ﬁlters X is decomposed into linear combinations of a shared bank of separable (decomposable) ﬁlters Y . The decompositions of ﬁlters within Y are independent (components are not shared). Jaderberg et al. (2014a) evaluated the decomposition (Rigamonti et al., 2013) in the context of CNNs and furthermore suggested a more efﬁcient decomposition (Figure 1b) that effectively approximates the 4D kernel tensor as a composition (product) of two 3D tensors. In the experiments, Jaderberg et al. (2014a) have demonstrated the advantage of this scheme over (Rigamonti et al., 2013). In a sequel, when refering to (Jaderberg et al., 2014a) we imply this two-component decomposition.  2  Published as a conference paper at ICLR 2015  (a) Full convolution  (b) Two-component decomposition (Jaderberg et al., 2014a)  (c) CP-decomposition  Figure 1: Tensor decompositions for speeding up a generalized convolution. Gray boxes corre- spond to 3D tensors (map stacks) within a CNN, with the two frontal sides corresponding to spatial dimensions. Arrows show linear mappings and demonstrate how scalar values on the right (small boxes corresponding to single elements of the target tensor) are computed. Initial full convolu- tion (a) computes each element of the target tensor as a linear combination of the elements of a 3D subtensor that spans a spatial d × d window over all input maps. Jaderberg et al. (2014a) (b) approximate the initial convolution as a composition of two linear mappings with the intermediate map stack having R maps (where R is the rank of the decomposition). Each of the two mappings computes each target value based on a spatial window of size 1×d or d×1 in all input maps. Finally, CP-decomposition (c) used in our approach approximates the convolution as a composition of four convolutions with small kernels, so that a target value is computed based on a 1D-array that spans either one pixel in all input maps, or a 1D spatial window in one input map.  Once the decomposition is computed, Jaderberg et al. (2014a) perform “local” ﬁne-tuning that min- imizes the deviation between the full and the approximated convolutions outputs on the training data. Differently from Jaderberg et al. (2014a), our method ﬁne-tunes the entire network based on the original discriminative criterion. While (Jaderberg et al., 2014a) reported that such discrimi- native ﬁne-tuning was inefﬁcient for their scheme, we found that in our case it works well, even when CP-decomposition has large approximation error. Below, we provide a theoretical complexity comparison and empirical comparison of our scheme with (Jaderberg et al., 2014a). In the work that is most related to ours, Denton et al. (2014) have suggested a scheme based on the CP-decomposition of parts of the kernel tensor obtained by biclustering (alongside with a different decompositions for the ﬁrst convolutional layer and the fully-connected layers). Biclustering of (Denton et al., 2014) splits the two non-spatial dimensions into subgroups, and reduces the effective ranks in the CP-decomposition. CP-decompositions of the kernel tensor parts in (Denton et al., 2014) have been computed with the greedy approach1. Our approach essentially simpliﬁes that of Denton et al. (2014) in that we do not perform biclustering and apply CP-decomposition directly to the full convolution kernel tensor. On the other hand, we replace greedy computation of CP-decomposition with non-linear least squares. Finally, as discussed above, we ﬁne-tune the complete network by backpropagation, whereas Denton et al. (2014) only ﬁne-tunes the layers above the approximated one.  1Note that the alternating least squares process mentioned in (Denton et al., 2014) refers to computing the  best next rank-1 tensor, but the outer process of adding rank-1 tensors is still greedy.  3  R(cid:88)  r=1  Published as a conference paper at ICLR 2015  3 METHOD  Overall our method is a conceptually simple two-step approach: (1) take a convolutional layer and decompose its kernel using CP-decomposition, (2) ﬁne-tune the entire network using backpropaga- tion. If necessary, proceed to another layer. Below, we review the CP-decomposition, which is at the core of our method, and provide the details for the two steps of our approach.  3.1 CP-DECOMPOSITION REVIEW  Tensor decompositions are a natural way to generalize low-rank approach to multidimensional case. Recall that a low-rank decomposition of a matrix A of size n × m with rank R is given by:  A(i, j) =  A1(i, r)A2(j, r),  i = 1, n,  j = 1, m,  (1)  R(cid:88)  and leads to the idea of separation of variables. The most straightforward way to separate variables in case of many dimensions is to use the canonical polyadic decomposition (CP-decomposition, also called as CANDECOMP/PARAFAC model) (Kolda & Bader, 2009). For a d-dimensional array A of size n1 × ··· × nd a CP-decomposition has the following form  A(i1, . . . , id) =  A1(i1, r) . . . Ad(id, r),  (2)  r=1  where the minimal possible R is called canonical rank. The proﬁt of this decomposition is that we need to store only (n1 + ··· + nd)R elements instead of the whole tensor with n1...nd elements. In two dimensions, the low-rank approximation can be computed in a stable way by using singular value decomposition (SVD) or, if the matrix is large, by rank-revealing algorithms. Unfortunately, this is not the case for the CP-decomposition when d > 2, as there is no ﬁnite algorithm for deter- mining canonical rank of a tensor (Kolda & Bader, 2009). Therefore, most algorithms approximate a tensor with different values of R until the approximation error becomes small enough. This leads to the point that for ﬁnding good low-rank CP-decomposition certain tricks have to be used. A detailed survey of methods for computing low-rank CP-decompositions can be found in (Tomasi & Bro, 2006). As a software package to calculate the CP-decomposition we used Tensorlab (Sorber et al., 2014). For our purposes, we chose non-linear least squares (NLS) method, which minimizes the L2-norm of the approximation residual (for a user-deﬁned ﬁxed R) using Gauss-Newton opti- mization. Such NLS optimization is capable of obtaining much better approximations than the strategy of greedily ﬁnding best rank-1 approximation of the residual vectors used in Denton et al. (2014). The fact that the greedy rank-1 algorithm may increase tensor rank can be found in (Stegeman & Comon, 2010; Koﬁdis & Regalia, 2002). We also give a simple example highlighting this advantage of the NLS in the Appendix.  3.2 KERNEL TENSOR APPROXIMATION  CNNs (LeCun et al., 1989) are feed-forward multi-layer architectures that map the input images to certain output vectors using a sequence of operations. The units within CNN are organized as a sequence of 3D tensors (“map stacks”) with two spatial dimensions and the third dimension corre- sponding to different “maps” or ”channels”2. The most “important” and time-consuming operation within modern CNNs is the generalized con- volution that maps an input tensor U (·,·,·) of size X×Y ×S into an output tensor V (·,·,·) of size (X−d+1)×(Y −d+1)×T using the following linear mapping:  V (x, y, t) =  K(i − x + δ, j − y + δ, s, t) U (i, j, s)  (3)  i=x−δ  j=y−δ  s=1  2These tensors are 4D if/when a CNN is applied to a batch of images, with the fourth dimension correspond- ing to different images in a batch. This extra dimension does not affect the derivation below and is therefore disregarded.  4  x+δ(cid:88)  y+δ(cid:88)  S(cid:88)  Published as a conference paper at ICLR 2015  Here, K(·,·,·,·) is a 4D kernel tensor of size d×d×S×T with the ﬁrst two dimensions correspond- ing to the spatial dimensions, the third dimension corresponding to different input channels, the fourth dimension corresponding to different output channels. The spatial width and height of the kernel are denoted as d, while δ denotes “half-width” (d − 1)/2 (for simplicity we assume square shaped kernels and even d). The rank-R CP-decomposition (2) of the 4D kernel tensor has the form:  K(i, j, s, t) =  K x(i − x + δ, r) K y(j − y + δ, r) K s(s, r) K t(t, r) ,  (4)  where K x(·,·), K y(·,·), K s(·,·), K t(·,·) are the four components of the composition representing 2D tensors (matrices) of sizes d×R, d×R, S×R, and T×R respectively. Substituting (4) into (3) and performing permutation and grouping of summands gives the following expression for the approximate evaluation of the convolution (3):   y+δ(cid:88)  j=y−δ  (cid:32) S(cid:88)  s=1  (cid:33)  V (x, y, t) =  K t(t, r)  K x(i − x + δ, r)  K y(j − y + δ, r)  K s(s, r) U (i, j, s)  (5) Based on (5), the output tensor V (·,·,·) can be computed from the input tensor U (·,·,·) via a se- quence of four convolutions with smaller kernels (Figure 1c):  R(cid:88)  r=1   x+δ(cid:88)  i=x−δ  R(cid:88)  r=1  s=1  S(cid:88) y+δ(cid:88) x+δ(cid:88) R(cid:88)  i=x−δ  j=y−δ  U s(i, j, r) =  U sy(i, y, r) =  U syx(x, y, r) =  V (x, y, t) =  K s(s, r) U (i, j, s)  K y(j − y + δ, r) U s(i, j, r)  K x(i − x + δ, r) U sy(i, y, r)  K t(t, r) U syx(x, y, r) ,  (6)  (7)  (8)  (9)  where U s(·,·,·), U sy(·,·,·), and U syx(·,·,·) are intermediate tensors (map stacks).  r=1  IMPLEMENTATION AND FINE-TUNING  3.3 Computing U s(·,·,·) from U (·,·,·) in (6) as well as V (·,·,·) from U syx(·,·,·) in (9) represent so- called 1×1 convolutions (also used within “network-in-network” approach (Lin et al., 2013)) that essentially perform pixel-wise linear re-combination of input maps. Computing U sy(·,·,·) from U s(·,·,·) and U syx(·,·,·) from U sy(·,·,·) in (7) and (8) are “standard” convolutions with small kernels that are “ﬂat” in one of the two spatial dimensions. We use the popular Caffe package (Jia et al., 2014) to implement the resulting architecture, utiliz- ing standard convolution layers for (7) and (8), and an optimized 1×1 convolution layers for (6) and (9). The resulting architecture is ﬁne-tuned through standard backpropagation (with momentum) on training data. All network layers including layers above the approximated layer, layers below the approximated layer, and the four inserted convolutional layers participate in the ﬁne-tuning. We found, however, that the gradients within the inserted layers are prone to gradient explosion, so one should either be careful to keep the learning rate low, or ﬁx some or all of the inserted layers, while still ﬁne-tuning layers above and below.  3.4 COMPLEXITY ANALYSIS  Initial convolution operation is deﬁned by ST d2 parameters (number of elements in the kernel ten- sor) and requires the same number of “multiplication+addition” operations per pixel.  5  Published as a conference paper at ICLR 2015  For (Jaderberg et al., 2014a) this number changes to Rd(S + T ), where R is the rank of the de- composition (see Figure 1b and Jaderberg et al. (2014a)). While the two numbers are not directly comparable, assuming that the required rank is comparable or several times smaller than S and T (e.g. taking R ≈ ST S+T ), the scheme (Jaderberg et al., 2014a) gives a reduction in the order of d times compared to the initial convolution. For (Denton et al., 2014) in the absence of bi-clustering as well as in the case of our approach, the complexity is R(S + 2d + T ) (again, both for the number of parameters and for the number of “multiplications+additions” per output pixel). Almost always, d (cid:28) T , which for the same rank gives a further factor of d improvement in complexity over (Jaderberg et al., 2014a) (and an order of d2 improvement over the initial convolution when R ≈ ST The bi-clustering in (Denton et al., 2014) makes a “theoretical” comparison with the complexity of our approach problematic, as on the one hand bi-clustering increases the number of tensors to be approximated, but on the other hand, reduces the required ranks considerably (so that assuming the same R would not be reasonable). We therefore restrict ourselves to the empirical comparison.  S+T ).  4 EXPERIMENTS  In this section we test our approach on two network architectures, small character-classiﬁcation CNN and a bigger net trained for ILSVRC. Most of our experiments are devoted to the approximation of single layers, when other layers remain intact apart from the ﬁne-tuning. We make several measurements to evaluate our models. After the approximation of the kernel tensor with the CP-decomposition, we calculate the accuracy of this decomposition, i.e. (cid:107)K(cid:48) − K(cid:107)/(cid:107)K(cid:107), where K is the original tensor and K(cid:48) is the obtained approximation. The difference between the original tensor and our approximation may disturb data propagation in CNN, resulting in the drop of classiﬁcation accuracy. We measure this drop before and after the ﬁne-tuning of CNN. Furthermore, we record the CPU timings for our models and report the speed-up compared to the CPU timings of the original model (all timings are based on Caffe code run in the CPU mode on image batches of size 64). Finally, we report the reduction in the number of parameters resulting from the low-rank approximation. All results are reported for a number of ranks R.  4.1 CHARACTER-CLASSIFICATION CNN  We use use CNN described in (Jaderberg et al., 2014b) for our experiments. The network has four convolutional layers with maxout nonlinearities between them and a softmax output. It was trained to classify 24 × 24 image patches into one of 36 classes (10 digits plus 26 characters). Our Caffe port of the publicly available pre-trained model (refered below as CharNet) achieves 91.2% accuracy on test set (very similar to the original). As in (Jaderberg et al., 2014a), we consider only the second and the third convolutional layers, which constitute more then 90% of processing time. Layer 2 has 48 input and 128 output channels and ﬁlters of size 9 × 9, layer 3 has 64 input and 512 output channels, ﬁlter size is 8 × 8. The results of separate approximation of layers 2 and 3 are shown in ﬁgures 2a and 2b. Tensor approximation error diminishes with the growth of approximation rank, and when the approximation rank becomes big enough, it is possible to approximate weight tensor accurately. However, our experiments showed that accurate approximation is not required for the network to function properly. For example, while approximating layer 3, network classiﬁcation accuracy is unaffected even if tensor approximation error is as big as 78%. Combining approximations. We have applied our methods to two layers of the network using the following procedure. Firstly, layer 2 was approximated with rank 64. After that, the drop in accuracy was made small by ﬁne-tuning of all layers but the new ones. Finally, layer 3 was approximated with rank 64, and for this layer such approximation does not result in signiﬁcant drop of network prediction accuracy, so there is no need to ﬁne-tune the network one more time. The network derived by this procedure is 8.5 times faster than original model, while classiﬁcation accuracy drops by 1% to 90.2%. Comparing with (Jaderberg et al., 2014a), we achieve almost two  6  Published as a conference paper at ICLR 2015  times bigger speedup for the same loss of accuracy, ((Jaderberg et al., 2014a) incurs 1% accuracy loss for the speedup of 4.2x and 5% accuracy loss for the speedup of 6x).  4.2 ALEXNET  Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in (Jaderberg et al., 2014b) it is sufﬁcient to take 200 components, which also gives a superior layer speed-up (3.6× vs. 2× achieved by Scheme 2 of (Jaderberg et al., 2014b)). The running time of the conv2 can be further reduced if we allow for slightly more misclassiﬁcations: rank 140 approximation leads to 4.5× speed-up at the cost of ≈ 1% accuracy loss surpassing the results of (Denton et al., 2014). Along with conventional full-network ﬁne-tuning we tried to reﬁne the obtained tensor approxima- tion by applying the data reconstruction approach from (Jaderberg et al., 2014b). Unfortunately, we failed to ﬁnd a good SGD learning rate: larger values led to the exploding gradients, while the smaller ones did not allow to sensibly reduce the reconstruction loss. We suspect that this effect is due to the instability of the low-rank CP-decomposition (De Silva & Lim, 2008). One way to circumvent the issue would be to alternate the components learning (i.e. not optimizing all of them at once), which is the scope of our future work. Our latest experiments showed that while our approach is superior for smaller architectures (as in character classiﬁcation), it is not the best one for large nets such as AlexNet. Although (Jader- berg et al., 2014b) mentions that application of their approach to the second layer of the Over- Feat architecture yields a 2× speed-up, our colleagues at Yandex have discovered that a far greater speed-up with (Jaderberg et al., 2014b) can be reached, at least for AlexNet. In particular, our exper- iments with (Jaderberg et al., 2014b) on AlexNet have showed that the second convolutional layer of AlexNet can be accelerated by the factor of 6.6× at the cost of ≈ 1% accuracy loss via Scheme 2 and data optimization as described in (Jaderberg et al., 2014b) (as compared to 4.5× for similar accuracy loss obtainable with our method).  4.3 NLS VS. GREEDY  One of our main contributions is pointing out that greedy CP-decomposition works worse than more advanced algorithms such as non-linear least squares (NLS), and evaluate this degradation in the context of speeding up CNNs. We perform comparisons on the second layers of both CharNet and AlexNet. For CharNet, we also evaluate the combination with ﬁne-tuning. Furthermore, for CharNet, we also tried initializing the layers randomly (using the scheme of (Glorot & Bengio, 2010)). The results in Table 1 clearly demonstrate two related things. Firstly, NLS decomposition leads to signiﬁcantly higher accuracy whether with ﬁne-tuning or without. The advantage is greater for the more complex network (AlexNet). Secondly, the output of the ﬁne-tuning clearly depends on the quality of the approximation. This observation concurs with the hypothesis that the large number of parameters in CNN is needed to avoid poor local minima. Indeed, CP-decomposition radically decreases the number of parameters in a layer. While good minima may still exist (e.g. the NLS+FT result), optimization is prone to stucking in a much worse minima (e.g. the Random+FT result).  5 DISCUSSION  We have demonstrated that a rather straightforward application of a modern tensor decomposition method (NLS-based low-rank CP-decomposition) combined with the discriminative ﬁne-tuning of the entire network can achieve considerable speedups with minimal loss in accuracy.  7  Published as a conference paper at ICLR 2015  (a) CharNet conv2  (b) CharNet conv3  (c) AlexNet conv2  Figure 2: Various properties and performance metrics of different approximated CNNs plotted as functions of the approximation rank. First row: kernel tensor approximation error. Second row: drop of the classiﬁcation accuracy of the full model with approximated layers w.r.t the ac- curacy of the original model; dashed lines correspond to the non-tuned CNNs, solid lines depict the performance after the ﬁne-tuning. Note the log-scale. Cases, where the accuracy has actually improved are plotted at the bottom line. Third row: empirical speed-up of the approximated layer w.r.t. the original layer. Fourth row: ratio between the numbers of parameters in the original and the approximated layers.  8  416642560.20.40.60.81Approximationerror416642560.010.111050Accuracydrop(%)noFTFT41664256200400RankParametersreduction(×)4166425620406080100Speed-up(×)416642560.20.40.60.81416642560.010.111050noFTFT41664256200400Rank41664256204060801001001402002503000.20.40.60.81100140200250300110100noFTFT1001402002503005101520Rank100140200250300246Published as a conference paper at ICLR 2015  Table 1: Accuracy drop for the greedy and the non-linear least-squares (NLS) CP- decomposition. The results are given for the second layers of CharNet and AlexNet, for dif- ferent decomposition ranks R, and the numbers correspond to the accuracy drop of the entire CNN. Original networks achieve 91.24% (CharNet) and 79.95% (AlexNet). For CharNet we also evaluate the effect of ﬁne-tuning (FT), and also for the random initialization. NLS computation of the CP-decomposition invariably leads to better performance, and the advantage persists through ﬁne-tuning.  CharNet, R=16 CharNet, R=64 CharNet, R=256 AlexNet AlexNet AlexNet NO FT  R=140  R=200  R=300  NO FT  NO FT  –  RANDOM GREEDY 24.15 18.96  NLS  FT 9.70 2.64 2.16  –  4.99 1.93  FT 7.64 0.46 0.09  –  1.14 0.31  FT 6.13 -0.31 -0.52  –  65.04 3.21  –  7.29 0.97  –  4.76 0.30  In the preliminary comparisons, this approach outperforms the previous methods of (Denton et al., 2014) and (Jaderberg et al., 2014a), for the character classiﬁcation CNN. However, more compar- isons especially with a more related work of (Denton et al., 2014) are needed. In particular, it is to be determined whether bi-clustering is useful, when non-linear least squares are used for CP- decomposition. Another avenue of research are layers with spatially-varying kernels, such as used e.g. in (Taigman et al., 2014). Firstly, these layers would greatly beneﬁt from the reduction in the number of param- eters. Secondly, the spatial variation of the kernel might be embedded into extra tensor dimensions, which may open up further speed-up possibilities. Finally, similarly to (Denton et al., 2014), we note that low-rank decompositions seems to have a regularizing effects allowing to slightly improve the overall accuracy for higher rank.  ACKNOWLEDGMENTS  The work presented in Section 3 was supported by Russian Science Foundation grant 14-11-00659.  REFERENCES Chellapilla, Kumar, Puri, Sidd, and Simard, Patrice. High performance convolutional neural net- works for document processing. In Tenth International Workshop on Frontiers in Handwriting Recognition, 2006.  Chintala,  Soumith.  Convnet-benchmarks.  convnet-benchmarks, 2014. Accessed: 2014-12-19.  https://github.com/soumith/  De Silva, Vin and Lim, Lek-Heng. Tensor rank and the ill-posedness of the best low-rank approxi-  mation problem. SIAM J. Matrix Anal. Appl., 30(3):1084–1127, 2008.  Dean, Jeffrey, Corrado, Greg, Monga, Rajat, Chen, Kai, Devin, Matthieu, Mao, Mark, Senior, An- In  drew, Tucker, Paul, Yang, Ke, Le, Quoc V, et al. Large scale distributed deep networks. Advances in Neural Information Processing Systems, pp. 1223–1231, 2012.  Denton, Emily, Zaremba, Wojciech, Bruna, Joan, LeCun, Yann, and Fergus, Rob. Exploiting linear structure within convolutional networks for efﬁcient evaluation. arXiv preprint arXiv:1404.0736, 2014.  Farabet, Cl´ement, LeCun, Yann, Kavukcuoglu, Koray, Culurciello, Eugenio, Martini, Berin, Ak- selrod, Polina, and Talay, Selcuk. Large-scale FPGA-based convolutional networks. Machine Learning on Very Large Data Sets, 2011.  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and  9  Published as a conference paper at ICLR 2015  Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp. 249–256, 2010.  Jaderberg, Max, Vedaldi, Andrea, and Zisserman, Andrew. Speeding up convolutional neural net- In Proceedings of the British Machine Vision Conference  works with low rank expansions. (BMVC), 2014a.  Jaderberg, Max, Vedaldi, Andrea, and Zisserman, Andrew. Deep features for text spotting.  Computer Vision–ECCV 2014, pp. 512–528. Springer, 2014b.  In  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Koﬁdis, Eleftherios and Regalia, Phillip A. On the best rank-1 approximation of higher-order super-  symmetric tensors. SIAM J. Matrix Anal. Appl., 23(3):863–884, 2002.  Kolda, T. G. and Bader, B. W. Tensor decompositions and applications. SIAM Rev., 51(3):455–500,  2009.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  LeCun, Yann, Boser, Bernhard, Denker, John S, Henderson, Donnie, Howard, Richard E, Hubbard, Wayne, and Jackel, Lawrence D. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. arXiv preprint arXiv:1312.4400,  2013.  Mathieu, Michael, Henaff, Mikael, and LeCun, Yann. Fast training of convolutional networks  through FFTs. arXiv preprint arXiv:1312.5851, 2013.  Rigamonti, Roberto, Sironi, Amos, Lepetit, Vincent, and Fua, Pascal. Learning separable ﬁlters. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pp. 2754–2761. Ieee, 2013.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.  Sorber, L., Van Barel, M., and De Lathauwer, L. Tensorlab v2.0. Available online, 2014. URL  http://tensorlab.net.  Stegeman, Alwin and Comon, Pierre. Subtracting a best rank-1 approximation may increase tensor  rank. Linear Algebra Appl., 433(7):1276–1300, 2010.  Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf, Lior. Deepface: Closing the gap In Computer Vision and Pattern Recognition  to human-level performance in face veriﬁcation. (CVPR), 2014 IEEE Conference on, pp. 1701–1708. IEEE, 2014.  Tomasi, Giorgio and Bro, Rasmus. A comparison of algorithms for ﬁtting the parafac model. Comp.  Stat. Data An., 50(7):1700–1734, 2006.  10  Published as a conference paper at ICLR 2015  APPENDIX  In all our experiments, we consistently found that non-linear least squares (NLS) with the imple- mentation from (Sorber et al., 2014) yield better CP-decompositions with smaller ranks (for the same approximation accuracy) than the greedy approach of ﬁnding the next best rank-one tensor and adding it to the decomposition. Such advantage can be highlighted by the following example of 2 × 2 × 2 tensor G of rank two, whose frontal slices are deﬁned by:  (cid:18)1  0  (cid:19)  0 1  (cid:18)1  0  (cid:19)  1 2  .  G1 =  , G2 =  We checked numerically that a sequence of two best rank-one approximations fails to approximate the tensor G – relative error was 0.35. In contrast to that, the best rank-two approximation (with 10−7 error in this case) was successfully obtained by the NLS optimization.  11  ",
1412.6632,2015,Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN),"['Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)', 'Junhua Mao', 'Wei Xu', 'Yi Yang', 'Jiang Wang', 'and Alan Yuille']",https://arxiv.org/pdf/1412.6632,"5 1 0 2     n u J    1 1      ]  V C . s c [      5 v 2 3 6 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  DEEP CAPTIONING WITH MULTIMODAL RECURRENT NEURAL NETWORKS (M-RNN)  Junhua Mao University of California, Los Angeles; Baidu Research mjhustc@ucla.edu  Wei Xu & Yi Yang & Jiang Wang & Zhiheng Huang Baidu Research {wei.xu,yangyi05,wangjiang03,huangzhiheng}@baidu.com  Alan Yuille University of California, Los Angeles yuille@stat.ucla.edu  ABSTRACT  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are gen- erated according to this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves signiﬁcant performance improvement over the state-of-the-art methods which di- rectly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/˜junhua.mao/m-RNN.html. 1  1  INTRODUCTION  Obtaining sentence level descriptions for images is becoming an important task and it has many ap- plications, such as early childhood education, image retrieval, and navigation for the blind. Thanks to the rapid development of computer vision and natural language processing technologies, recent work has made signiﬁcant progress on this task (see a brief review in Section 2). Many previous methods treat it as a retrieval task. They learn a joint embedding to map the features of both sen- tences and images to the same semantic space. These methods generate image captions by retrieving them from a sentence database. Thus, they lack the ability of generating novel sentences or describ- ing images that contain novel combinations of objects and scenes. In this work, we propose a multimodal Recurrent Neural Networks (m-RNN) model 2 to address both the task of generating novel sentences descriptions for images, and the task of image and sentence retrieval. The whole m-RNN model contains a language model part, a vision part and a multimodal part. The language model part learns a dense feature embedding for each word in the  1Most recently, we adopt a simple strategy to boost the performance of image captioning task signiﬁcantly. More details are shown in Section 8. The code and related data (e.g. reﬁned image features and hypotheses sentences generated by the m-RNN model) are available at https://github.com/mjhucla/mRNN-CR. 2A previous version of this work appears in the NIPS 2014 Deep Learning Workshop with the title “Explain Images with Multimodal Recurrent Neural Networks” http://arxiv.org/abs/1410.1090 (Mao et al. (2014)). We observed subsequent arXiv papers which also use recurrent neural networks in this topic and cite our work. We gratefully acknowledge them.  1  Published as a conference paper at ICLR 2015  Figure 1: Examples of the generated and two top-ranked retrieved sentences given the query image from IAPR TC-12 dataset. The sentences can well describe the content of the images. We show a failure case in the fourth image, where the model mistakenly treats the lake as the sky and misses all the people. More examples from the MS COCO dataset can be found on the project page: www.stat.ucla.edu/˜junhua.mao/m-RNN.html.  dictionary and stores the semantic temporal context in recurrent layers. The vision part contains a deep Convolutional Neural Network (CNN) which generates the image representation. The multi- modal part connects the language model and the deep CNN together by a one-layer representation. Our m-RNN model is learned using a log-likelihood cost function (see details in Section 4). The errors can be backpropagated to the three parts of the m-RNN model to update the model parameters simultaneously. In the experiments, we validate our model on four benchmark datasets: IAPR TC-12 (Grubinger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO (Lin et al. (2014)). We show that our method achieves state-of-the-art performance, signiﬁcantly outperforming all the other methods for the three tasks: generating novel sentences, retrieving im- ages given a sentence and retrieving sentences given an image. Our framework is general and can be further improved by incorporating more powerful deep representations for images and sentences.  2 RELATED WORK  Deep model for computer vision and natural language. The methods based on the deep neural network developed rapidly in recent years in both the ﬁeld of computer vision and natural lan- guage. For computer vision, Krizhevsky et al. (2012) propose a deep Convolutional Neural Net- works (CNN) with 8 layers (denoted as AlexNet) and outperform previous methods by a large margin in the image classiﬁcation task of ImageNet challenge (Russakovsky et al. (2014)). This network structure is widely used in computer vision, e.g. Girshick et al. (2014) design a object de- tection framework (RCNN) based on this work. Recently, Simonyan & Zisserman (2014) propose a CNN with over 16 layers (denoted as VggNet) and performs substantially better than the AlexNet. For natural language, the Recurrent Neural Network (RNN) shows the state-of-the-art performance in many tasks, such as speech recognition and word embedding learning (Mikolov et al. (2010; 2011; 2013)). Recently, RNNs have been successfully applied to machine translation to extract semantic information from the source sentence and generate target sentences (e.g. Kalchbrenner & Blunsom (2013), Cho et al. (2014) and Sutskever et al. (2014)). Image-sentence retrieval. Many previous methods treat the task of describing images as a retrieval task and formulate the problem as a ranking or embedding learning problem (Hodosh et al. (2013); Frome et al. (2013); Socher et al. (2014)). They ﬁrst extract the word and sentence features (e.g. Socher et al. (2014) uses dependency tree Recursive Neural Network to extract sentence features) as well as the image features. Then they optimize a ranking cost to learn an embedding model that maps both the sentence feature and the image feature to a common semantic feature space. In this way, they can directly calculate the distance between images and sentences. Recently, Karpathy et al. (2014) show that object level image features based on object detection results can generate better results than image features extracted at the global level.  2  Retr.Gen.1. Tourists are sitting at a long table with beer bottles on it in a rather dark restaurant and are raising their bierglaeser; 2. Tourists are sitting at a long table with a white table-cloth in a somewhat dark restaurant;Tourists are sitting at a long table with a white table cloth and are eating;1. Top view of the lights of a city at night, with a well-illuminated square in front of a church in the foreground;2. People on the stairs in front of an illuminated cathedral with two towers at night;A square with burning street lamps and a street in the foreground;1. A dry landscape with light brown grass and green shrubs and trees in the foreground and large reddish-brown rocks and a blue sky in the background;2. A few bushes at the bottom and a clear sky in the background; A dry landscape with green trees and bushes and light brown grass in the foreground and reddish-brown round rock domes and a blue sky in the background;1. Group picture of nine tourists and one local on a grey rock with a lake in the background;2. Five people are standing and four are squatting on a brown rock in the foreground;A blue sky in the background;Published as a conference paper at ICLR 2015  Figure 2: Illustration of the simple Recurrent Neural Network (RNN) and our multimodal Recurrent Neural Network (m-RNN) architecture. (a). The simple RNN. (b). Our m-RNN model. The inputs of our model are an image and its corresponding sentence descriptions. w1, w2, ..., wL represents the words in a sentence. We add a start sign wstart and an end sign wend to all the training sentences. The model estimates the probability distribution of the next word given previous words and the image. It consists of ﬁve layers (i.e. two word embedding layers, a recurrent layer, a multimodal layer and a softmax layer) and a deep CNN in each time frame. The number above each layer indicates the dimension of the layer. The weights are shared among all the time frames. (Best viewed in color)  Generating novel sentence descriptions for images. There are generally three categories of meth- ods for this task. The ﬁrst category assumes a speciﬁc rule of the language grammar. They parse the sentence and divide it into several parts (Mitchell et al. (2012); Gupta & Mannem (2012)). Each part is associated with an object or an attribute in the image (e.g. Kulkarni et al. (2011) uses a Con- ditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. The second category retrieves similar captioned images, and generates new descriptions by generalizing and re-composing the re- trieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and topic models (Barnard et al. (2003); Jia et al. (2011)). They generate sentences with richer and more ﬂexible structure than the ﬁrst group. The probability of generating sentences using the model can serve as the afﬁnity metric for retrieval. Our method falls into this category. More closely related to our tasks and method is the work of Kiros et al. (2014b), which is built on a Log-BiLinear model (Mnih & Hinton (2007)) and use AlexNet to extract visual features. It needs a ﬁxed length of context (i.e. ﬁve words), whereas in our model, the temporal context is stored in a recurrent architecture, which allows arbitrary context length. Shortly after Mao et al. (2014), several papers appear with record breaking results (e.g. Kiros et al. (2014a); Karpathy & Fei-Fei (2014); Vinyals et al. (2014); Donahue et al. (2014); Fang et al. (2014); Chen & Zitnick (2014)). Many of them are built on recurrent neural networks. It demonstrates the effectiveness of storing context information in a recurrent layer. Our work has two major difference from these methods. Firstly, we incorporate a two-layer word embedding system in the m-RNN network structure which learns the word representation more efﬁciently than the single-layer word embedding. Secondly, we do not use the recurrent layer to store the visual information. The image representation is inputted to the m-RNN model along with every word in the sentence description. It utilizes of the capacity of the recurrent layer more efﬁciently, and allows us to achieve state-of- the-art performance using a relatively small dimensional recurrent layer. In the experiments, we show that these two strategies lead to better performance. Our method is still the best-performing approach for almost all the evaluation metrics.  3 MODEL ARCHITECTURE 3.1 SIMPLE RECURRENT NEURAL NETWORK  We brieﬂy introduce the simple Recurrent Neural Network (RNN) or Elman network (Elman (1990)). Its architecture is shown in Figure 2(a). It has three types of layers in each time frame:  3  (b). The m-RNN modelEmbedding IEmbedding IIRecurrentMultimodalSoftMaxwstartImageCNNw1ImageCNNwLImageCNN...Predictw1Predictw2Predictwend(a). The simple RNN modelw(t)r(t-1)r(t)y(t)w(t-1)...y(t-1)Input Word Layer wRecurrentLayer rOutputLayer yunfoldThe m-RNN model for one time frame128256256512Published as a conference paper at ICLR 2015  the input word layer w, the recurrent layer r and the output layer y. The activation of input, re- current and output layers at time t is denoted as w(t), r(t), and y(t) respectively. w(t) denotes the current word vector, which can be a simple 1-of-N coding representation h(t) (i.e. the one-hot representation, which is binary and has the same dimension as the vocabulary size with only one non-zero element) Mikolov et al. (2010). y(t) can be calculated as follows:  x(t) = [w(t) r(t − 1)]; r(t) = f1(U · x(t)); y(t) = g1(V · r(t));  (1) where x(t) is a vector that concatenates w(t) and r(t− 1), f1(.) and g1(.) are element-wise sigmoid and softmax function respectively, and U, V are weights which will be learned. The size of the RNN is adaptive to the length of the input sequence. The recurrent layers connect the sub-networks in different time frames. Accordingly, when we do backpropagation, we need to propagate the error through recurrent connections back in time (Rumelhart et al. (1988)).  3.2 OUR M-RNN MODEL  The structure of our multimodal Recurrent Neural Network (m-RNN) is shown in Figure 2(b). It has ﬁve layers in each time frame: two word embedding layers, the recurrent layer, the multimodal layer, and the softmax layer). The two word embedding layers embed the one-hot input into a dense word representation. It en- codes both the syntactic and semantic meaning of the words. The semantically relevant words can be found by calculating the Euclidean distance between two dense word vectors in embedding layers. Most of the sentence-image multimodal models (Karpathy et al. (2014); Frome et al. (2013); Socher et al. (2014); Kiros et al. (2014b)) use pre-computed word embedding vectors as the initialization of their model. In contrast, we randomly initialize our word embedding layers and learn them from the training data. We show that this random initialization is sufﬁcient for our architecture to generate the state-of-the-art result. We treat the activation of the word embedding layer II (see Figure 2(b)) as the ﬁnal word representation, which is one of the three direct inputs of the multimodal layer. After the two word embedding layers, we have a recurrent layer with 256 dimensions. The calcula- tion of the recurrent layer is slightly different from the calculation for the simple RNN. Instead of concatenating the word representation at time t (denoted as w(t)) and the recurrent layer activation at time t− 1 (denoted as r(t− 1)), we ﬁrst map r(t− 1) into the same vector space as w(t) and add them together:  r(t) = f2(Ur · r(t − 1) + w(t));  (2) where “+” represents element-wise addition. We set f2(.) to be the Rectiﬁed Linear Unit (ReLU), inspired by its the recent success when training very deep structure in computer vision ﬁeld (Nair & Hinton (2010); Krizhevsky et al. (2012)). This differs from the simple RNN where the sigmoid function is adopted (see Section 3.1). ReLU is faster, and harder to saturate or overﬁt the data than non-linear functions like the sigmoid. When the backpropagation through time (BPTT) is conducted for the RNN with sigmoid function, the vanishing or exploding gradient problem appears since even the simplest RNN model can have a large temporal depth 3. Previous work (Mikolov et al. (2010; 2011)) use heuristics, such as the truncated BPTT, to avoid this problem. The truncated BPTT stops the BPTT after k time steps, where k is a hand-deﬁned hyperparameter. Because of the good properties of ReLU, we do not need to stop the BPTT at an early stage, which leads to better and more efﬁcient utilization of the data than the truncated BPTT. After the recurrent layer, we set up a 512 dimensional multimodal layer that connects the language model part and the vision part of the m-RNN model (see Figure 2(b)). This layer has three inputs: the word-embedding layer II, the recurrent layer and the image representation. For the image rep- resentation, here we use the activation of the 7th layer of AlexNet (Krizhevsky et al. (2012)) or 15th layer of VggNet (Simonyan & Zisserman (2014)), though our framework can use any image fea- tures. We map the activation of the three layers to the same multimodal feature space and add them together to obtain the activation of the multimodal layer:  m(t) = g2(Vw · w(t) + Vr · r(t) + VI · I);  (3)  3We tried Sigmoid and Scaled Hyperbolic Tangent function as the non-linear functions for RNN in the  experiments but they lead to the gradient explosion problem easily.  4  Published as a conference paper at ICLR 2015  where “+” denotes element-wise addition, m denotes the multimodal layer feature vector, I denotes the image feature. g2(.) is the element-wise scaled hyperbolic tangent function (LeCun et al. (2012)):  g2(x) = 1.7159 · tanh(  2 3  x)  (4)  This function forces the gradients into the most non-linear value range and leads to a faster training process than the basic hyperbolic tangent function. Both the simple RNN and m-RNN models have a softmax layer that generates the probability dis- tribution of the next word. The dimension of this layer is the vocabulary size M, which is different for different datasets.  4 TRAINING THE M-RNN  To train our m-RNN model we adopt a log-likelihood cost function. It is related to the Perplexity of the sentences in the training set given their corresponding images. Perplexity is a standard measure for evaluating language model. The perplexity for one word sequence (i.e. a sentence) w1:L is calculated as follows:  log2 PPL(w1:L|I) = − 1 L  log2 P (wn|w1:n−1, I)  (5)  where L is the length of the word sequence, PPL(w1:L|I) denotes the perplexity of the sentence w1:L given the image I. P (wn|w1:n−1, I) is the probability of generating the word wn given I and previous words w1:n−1. It corresponds to the activation of the SoftMax layer of our model. The cost function of our model is the average log-likelihood of the words given their context words and corresponding images in the training sentences plus a regularization term. It can be calculated by the perplexity:  L(cid:88)  n=1  Li · log2 PPL(w(i)  1:Li  |I(i)) + λθ · (cid:107)θ(cid:107)2  2  (6)  Ns(cid:88)  i=1  C =  1 N  where Ns and N denotes the number of sentences and the number of words in the training set receptively, Li denotes the length of ith sentences, and θ represents the model parameters. Our training objective is to minimize this cost function, which is equivalent to maximize the proba- bility of generating the sentences in the training set using the model. The cost function is differen- tiable and we use backpropagation to learn the model parameters.  5 SENTENCE GENERATION, IMAGE RETRIEVAL AND SENTENCE RETRIEVAL  We use the trained m-RNN model for three tasks: 1) Sentences generation, 2) Image retrieval (re- trieving most relevant images to the given sentence), 3) Sentence retrieval (retrieving most relevant sentences to the given image). The sentence generation process is straightforward. Starting from the start sign wstart or arbitrary number of reference words (e.g. we can input the ﬁrst K words in the reference sentence to the model and then start to generate new words), our model can calculate the probability distribution of the next word: P (wn|w1:n−1, I). Then we can sample from this probability distribution to pick the next word. In practice, we ﬁnd that selecting the word with the maximum probability performs slightly better than sampling. After that, we input the picked word to the model and continue the process until the model outputs the end sign wend. For the retrieval tasks, we use our model to calculate the probability of generating a sentence w1:L n P (wn|w1:n−1, I). The probability can be treated as an afﬁnity  given an image I: P (w1:L|I) =(cid:81)  measurement between sentences and images. For the image retrieval task, given the query sentence wQ ing to the probability P (wQ perplexity-based image retrieval in Kiros et al. (2014b).  1:L, we rank the dataset images ID accord- 1:L|ID) and retrieved the top ranked images. This is equivalent to the  5  Published as a conference paper at ICLR 2015  The sentence retrieval task is trickier because there might be some sentences that have high proba- bility or perplexity for any image query (e.g. sentences consist of many frequently appeared words). To solve this problem, Kiros et al. (2014b) uses the perplexity of a sentence conditioned on the averaged image feature across the training set as the reference perplexity to normalize the original perplexity. Different from them, we use the normalized probability where the normalization factor is the marginal probability of wD  1:L);  P (wD  P (wD  1:L denotes the sentence in the dataset, IQ denotes the query image, and I  (7) are images where wD sampled from the training set. We approximate P (I ) by a constant and ignore this term. This strategy leads to a much better performance than that in Kiros et al. (2014b) in the experiments. The normalized probability is equivalent to the probability P (IQ|wD 1:L), which is symmetric to the probability P (wQ  1:L|ID) used in the image retrieval task.  (cid:48) P (wD I  )  (cid:48)  (cid:48)  (cid:48)  1:L|I  ) · P (I  (cid:48)  1:L:  1:L|IQ)/P (wD  1:L) =(cid:80)  6 LEARNING OF SENTENCE AND IMAGE FEATURES  The architecture of our model allows the gradients from the loss function to be backpropagated to both the language modeling part (i.e. the word embedding layers and the recurrent layer) and the vision part (e.g. the AlexNet or VggNet). For the language part, as mentioned above, we randomly initialize the language modeling layers and learn their parameters. For the vision part, we use the pre-trained AlexNet (Krizhevsky et al. (2012)) or the VggNet (Simonyan & Zisserman (2014)) on ImageNet dataset (Russakovsky et al. (2014)). Recently, Karpathy et al. (2014) show that using the RCNN object detection results (Girshick et al. (2014)) combined with the AlexNet features performs better than simply treating the image as a whole frame. In the experiments, we show that our method performs much better than Karpathy et al. (2014) when the same image features are used, and is better than or comparable to their results even when they use more sophisticated features based on object detection. We can update the CNN in the vision part of our model according to the gradient backpropagated from the multimodal layer. In this paper, we ﬁx the image features and the deep CNN network in the training stage due to a shortage of data. In future work, we will apply our method on large datasets (e.g. the complete MS COCO dataset, which has not yet been released) and ﬁnetune the parameters of the deep CNN network in the training stage. The m-RNN model is trained using Baidu’s internal deep learning platform PADDLE, which allows us to explore many different model architectures in a short period. The hyperparameters, such as layer dimensions and the choice of the non-linear activation functions, are tuned via cross-validation on Flickr8K dataset and are then ﬁxed across all the experiments. It takes 25 ms on average to generate a sentence (excluding image feature extraction stage) on a single core CPU.  7 EXPERIMENTS  7.1 DATASETS  We test our method on four benchmark datasets with sentence level annotations: IAPR TC-12 (Grub- inger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO (Lin et al. (2014)). IAPR TC-12. This dataset consists of around 20,000 images taken from different locations around the world. It contains images of different sports and actions, people, animals, cities, landscapes, etc. For each image, it provides at least one sentence annotation. On average, there are about 1.7 sentence annotations for one image. We adopt the standard separation of training and testing set as previous works (Guillaumin et al. (2010); Kiros et al. (2014b)) with 17,665 images for training and 1962 images for testing. Flickr8K. This dataset consists of 8,000 images extracted from Flickr. For each image, it provides ﬁve sentence annotations. We adopt the standard separation of training, validation and testing set provided by the dataset. There are 6,000 images for training, 1,000 images for validation and 1,000 images for testing.  6  Published as a conference paper at ICLR 2015  Flickr30K. This dataset is a recent extension of Flickr8K. For each image, it also provides ﬁve sentences annotations. It consists of 158,915 crowd-sourced captions describing 31,783 images. The grammar and style for the annotations of this dataset is similar to Flickr8K. We follow the previous work (Karpathy et al. (2014)) which used 1,000 images for testing. This dataset, as well as the Flick8K dataset, were originally used for the image-sentence retrieval tasks. MS COCO. The current release of this recently proposed dataset contains 82,783 training images and 40,504 validation images. For each image, it provides ﬁve sentences annotations. We randomly sampled 4,000 images for validation and 1,000 images for testing from their currently released validation set. The dataset partition of MS COCO and Flickr30K is available in the project page 4.  7.2 EVALUATION METRICS  Sentence Generation. Following previous works, we use the sentence perplexity (see Equ. 5) and BLEU scores (i.e. B-1, B-2, B-3, and B-4) (Papineni et al. (2002)) as the evaluation metrics. BLEU scores were originally designed for automatic machine translation where they rate the quality of a translated sentences given several reference sentences. Similarly, we can treat the sentence gener- ation task as the “translation” of the content of images to sentences. BLEU remains the standard evaluation metric for sentence generation methods for images, though it has drawbacks. For some images, the reference sentences might not contain all the possible descriptions in the image and BLEU might penalize some correctly generated sentences. Please see more details of the calcula- tion of BLEU scores for this task in the supplementary material section 10.3 5. Sentence Retrieval and Image Retrieval. We adopt the same evaluation metrics as previous works (Socher et al. (2014); Frome et al. (2013); Karpathy et al. (2014)) for both the tasks of sentences retrieval and image retrieval. We use R@K (K = 1, 5, 10) as the measurement. R@K is the recall rate of a correctly retrieved groundtruth given top K candidates. Higher R@K usually means better retrieval performance. Since we care most about the top-ranked retrieved results, the R@K scores with smaller K are more important. The Med r is another metric we use, which is the median rank of the ﬁrst retrieved groundtruth sentence or image. Lower Med r usually means better performance. For IAPR TC-12 datasets, we use additional evaluation metrics to conduct a fair comparison with previous work (Kiros et al. (2014b)). Please see the details in the supplementary material section 10.3.  7.3 RESULTS ON IAPR TC-12  The results of the sentence generation task6 are shown in Table 1. Ours-RNN-Base serves as a baseline method for our m-RNN model. It has the same architecture as m-RNN except that it does not have the image representation input. To conduct a fair comparison, we follow the same experimental settings of Kiros et al. (2014b) to calculate the BLEU scores and perplexity. These two evaluation metrics are not necessarily correlated to each other for the following reasons. As mentioned in Section 4, perplexity is calculated according to the conditional probability of the word in a sentence given all of its previous reference words. Therefore, a strong language model that successfully captures the distributions of words in sentences can have a low perplexity without the image content. But the content of the generated sentences might be uncorrelated to images. From Table 1, we can see that although our baseline method of RNN generates a low perplexity, its BLEU score is low, indicating that it fails to generate sentences that are consistent with the content of images. Table 1 shows that our m-RNN model performs much better than our baseline RNN model and the state-of-the-art methods both in terms of the perplexity and BLEU score.  4www.stat.ucla.edu/˜junhua.mao/m-RNN.html 5The BLEU outputted by our implementation is slightly lower than the recently released MS COCO caption evaluation toolbox (Chen et al. (2015)) because of different tokenization methods of the sentences. We re- evaluate our method using the toolbox in the current version of the paper.  6Kiros et al. (2014b) further improved their results after the publication. We compare our results with their  updated ones here.  7  Published as a conference paper at ICLR 2015  LBL, Mnih & Hinton (2007) MLBLB-AlexNet, Kiros et al. (2014b) MLBLF-AlexNet, Kiros et al. (2014b) Gupta et al. (2012) Gupta & Mannem (2012) Ours-RNN-Base Ours-m-RNN-AlexNet  PPL 9.29 9.86 9.90  - -  7.77 6.92  B-1 0.321 0.393 0.387 0.15 0.33 0.307 0.482  B-2 0.145 0.211 0.209 0.06 0.18 0.177 0.357  B-3 0.064 0.112 0.115 0.01 0.07 0.096 0.269  B-4 - - - - -  0.043 0.208  Table 1: Results of the sentence generation task on the IAPR TC-12 dataset. “B” is short for BLEU.  Sentence Retrival (Image to Text) R@1 R@5 R@10 Med r  Ours-m-RNN 20.9  43.8  54.4  8  Image Retrival (Text to Image) R@1 R@5 R@10 Med r 13.2  31.2  40.8  21  Table 2: R@K and median rank (Med r) for IAPR TC-12 dataset.  Random SDT-RNN-AlexNet Socher-avg-RCNN DeViSE-avg-RCNN DeepFE-AlexNet DeepFE-RCNN Ours-m-RNN-AlexNet  Sentence Retrival (Image to Text) R@1 R@5 R@10 Med r 631 0.1 4.5 32 23 6.0 28 4.8 34 5.9 14 12.6 14.5 11  0.5 18.0 22.7 16.5 19.2 32.9 37.2  1.0 28.6 34.0 27.3 27.3 44.0 48.5  Image Retrival (Text to Image) R@1 R@5 R@10 Med r 500 0.1 6.1 29 25 6.6 29 5.9 32 5.2 15 9.7 11.5 15  0.5 18.5 21.6 20.1 17.6 29.6 31.0  1.0 29.0 31.7 29.6 26.5 42.5 42.4  Table 3: Results of R@K and median rank (Med r) for Flickr8K dataset. “-AlexNet” denotes the image representation based on AlexNet extracted from the whole image frame. “-RCNN” denotes the image representation extracted from possible objects detected by the RCNN algorithm.  For the retrieval tasks, since there are no publicly available results of R@K and Med r in this dataset, we report R@K scores of our method in Table 2 for future comparisons. The result shows that 20.9% top-ranked retrieved sentences and 13.2% top-ranked retrieved images are groundtruth. We also adopt additional evaluation metrics to compare our method with Kiros et al. (2014b), see sup- plementary material Section 10.2.  7.4 RESULTS ON FLICKR8K  This dataset was widely used as a benchmark dataset for image and sentence retrieval. The R@K and Med r of different methods are shown in Table 3. We compare our model with several state-of- the-art methods: SDT-RNN (Socher et al. (2014)), DeViSE (Frome et al. (2013)), DeepFE (Karpathy et al. (2014)) with various image representations. Our model outperforms these methods by a large margin when using the same image representation (e.g. AlexNet). We also list the performance of methods using more sophisticated features in Table 3. “-avg-RCNN” denotes methods with features of the average CNN activation of all objects above a detection conﬁdence threshold. DeepFE-RCNN Karpathy et al. (2014) uses a fragment mapping strategy to better exploit the object detection results. The results show that using these features improves the performance. Even without the help from the object detection methods, however, our method performs better than these methods in almost all the evaluation metrics. We will develop our framework using better image features based on object detection in the future work. The PPL, B-1, B-2, B-3 and B-4 of the generated sentences using our m-RNN-AlexNet model in this dataset are 24.39, 0.565, 0.386, 0.256, and 0.170 respectively.  8  Published as a conference paper at ICLR 2015  Sentence Retrival (Image to Text) R@1 R@5 R@10 Med r  Image Retrival (Text to Image) R@1 R@5 R@10 Med r  Flickr30K  Random DeViSE-avg-RCNN DeepFE-RCNN RVR MNLM-AlexNet MNLM-VggNet NIC LRCN DeepVS Ours-m-RNN-AlexNet Ours-m-RNN-VggNet  Random DeepVS-RCNN Ours-m-RNN-VggNet  0.1 4.8 16.4 12.1 14.8 23.0 17.0 14.0 22.2 18.4 35.4  0.1 29.4 41.0  0.6 16.5 40.2 27.8 39.2 50.7 56.0 34.9 48.2 40.2 63.8  0.6 62.0 73.0  1.1 27.3 54.7 47.8 50.9 62.9  -  47.0 61.4 50.9 73.7  1.1 75.9 83.5  MS COCO  631 28 8 11 10 5 7 11 4.8 10 3  631 2.5 2  0.1 5.9 10.3 12.7 11.8 16.8 17.0  -  15.2 12.6 22.8  0.5 20.1 31.4 33.1 34.0 42.0 57.0  -  37.7 31.2 50.7  1.0 29.6 44.5 44.9 46.3 56.5  - -  50.5 41.5 63.1  0.1 20.9 29.0  0.5 52.8 42.2  1.0 69.2 77.0  500 29 13 12.5 13 8 7 - 9.2 16 5  500 4 3  Table 4: Results of R@K and median rank (Med r) for Flickr30K dataset and MS COCO dataset.  Flickr30K  PPL B-1 B-2 B-3 B-4 PPL B-1 B-2 B-3 B-4 0.19  0.13  -  -  -  -  -  MS COCO  -  - -  0.47 0.21 0.09 21.20 0.50 0.30 0.15  RVR DeepVS-AlexNet DeepVS-VggNet NIC LRCN DMSM Ours-m-RNN-AlexNet 35.11 0.54 0.36 0.23 0.15 Ours-m-RNN-VggNet  0.66 0.59 0.39 0.25 0.16  - - -  - - -  -  -  -  -  -  -  - -  - - - -  0.53 0.28 0.15 19.64 0.57 0.37 0.19  -  -  0.67 0.63 0.44 0.31 0.21 0.21  - -  - -  - -  - - -  -  20.72 0.60 0.41 0.28 0.19 13.60 0.67 0.49 0.35 0.25  Table 5: Results of generated sentences on the Flickr 30K dataset and MS COCO dataset.  RNN Dim. LSTM  Our m-RNN MNLM NIC 512 Yes  256 No  300 Yes  LRCN 1000 (×4)  Yes  RVR DeepVS 100 300-600 No  No  Table 6: Properties of the recurrent layers for the ﬁve very recent methods. LRCN has a stack of four 1000 dimensional LSTM layers. We achieves state-of-the-art performance using a relatively small dimensional recurrent layer. LSTM (Hochreiter & Schmidhuber (1997)) can be treated as a sophisticated version of the RNN.  7.5 RESULTS ON FLICKR30K AND MS COCO  We compare our method with several state-of-the-art methods in these two recently released dataset (Note that the last six methods appear very recently, we use the results reported in their papers): DeViSE (Frome et al. (2013)), DeepFE (Karpathy et al. (2014)), MNLM (Kiros et al. (2014a)), DMSM (Fang et al. (2014)), NIC (Vinyals et al. (2014)), LRCN (Donahue et al. (2014)), RVR (Chen & Zitnick (2014)), and DeepVS (Karpathy & Fei-Fei (2014)). The results of the retrieval tasks and the sentence generation task 7 are shown in Table 4 and Table 5 respectively. We also summarize some of the properties of the recurrent layers adopted in the ﬁve very recent methods in Table 6.  7We only select the word with maximum probability each time in the sentence generation process in Table 5 while many comparing methods (e.g. DMSM, NIC, LRCN) uses a beam search scheme that keeps the best K candidates. The beam search scheme will lead to better performance in practice using the same model.  9  Published as a conference paper at ICLR 2015  m-RNN-greedy-c5 m-RNN-greedy-c40 m-RNN-beam-c5 m-RNN-beam-c40  B1 0.668 0.845 0.680 0.865  B2 0.488 0.730 0.506 0.760  B3 0.342 0.598 0.369 0.641  B4 0.239 0.473 0.272 0.529  CIDEr ROUGE L METEOR 0.729 0.740 0.791 0.789  0.489 0.616 0.499 0.640  0.221 0.291 0.225 0.304  Table 7: Results of the MS COCO test set evaluated by MS COCO evaluation server  Our method with VggNet image representation (Simonyan & Zisserman (2014)) outperforms the state-of-the-art methods, including the very recently released methods, in almost all the evaluation metrics. Note that the dimension of the recurrent layer of our model is relatively small compared to the competing methods. It shows the advantage and efﬁciency of our method that directly inputs the visual information to the multimodal layer instead of storing it in the recurrent layer. The m- RNN model with VggNet performs better than that with AlexNet, which indicates the importance of strong image representations in this task. 71% of the generated sentences for MS COCO datasets are novel (i.e. different from training sentences). We also validate our method on the test set of MS COCO by their evaluation server (Chen et al. (2015)). The results are shown in Table 7. We evaluate our model with greedy inference (select the word with the maximum probability each time) as well as with the beam search inference. “- c5” represents results using 5 reference sentences and “-c40” represents results using 40 reference sentences. To further validate the importance of different components of the m-RNN model, we train sev- eral variants of the original m-RNN model and compare their performance. In particular, we show that the two-layer word embedding system outperforms the single-layer version and the strategy of directly inputting the visual information to the multimodal layer substantially improves the perfor- mance (about 5% for B-1). Due to the limited space, we put the details of these experiments in Section 10.1 in the supplementary material after the main paper.  8 NEAREST NEIGHBOR AS REFERENCE  Recently, Devlin et al. (2015b) proposed a nearest neighbor approach that retrieves the captions of the k nearest images in the training set, ranks these captions according to the consensus of the caption w.r.t. to the rest of the captions, and output the top ranked one. Inspired by this method, we ﬁrst adopt the m-RNN model with the transposed weight sharing strat- egy (Mao et al. (2015), denoted as m-RNN-shared) to generate n hypotheses using a beam search scheme. Speciﬁcally, we keep the n best candidates in the sentence generation process until the model generates the end sign wend. These n best candidates are approximately the n most probable sentences generated by the model, and can be treated as the n hypotheses. In our experiments, we set n = 10 since it gives us a diversiﬁed set of hypotheses without too much outliers on our validation set. 8 After generating the hypotheses of a target image, we retrieve its nearest neighbors in the image feature space on the training set (see details in Section 8.1). Then we calculate the “consensus” scores (Devlin et al. (2015a)) of the hypotheses w.r.t. to the groundtruth captions of the nearest neighbor images, and rerank the hypotheses according to these scores (see details in Section 8.2).  8.1  IMAGE FEATURES FOR THE NEAREST NEIGHBOR IMAGE SEARCH  We try two types of image features for the nearest neighbor image search 9. The ﬁrst one is the original image features extracted by the VggNet (Simonyan & Zisserman (2014)). We ﬁrst resize the image so that its short side is 256 pixels. Then we extract features on ten 224 × 224 windows  8If we directly output the top hypotheses generated by the model, then n = 5 gives us the best performance.  But if we want to rerank the hypotheses, then n = 10 gives us a better result on the validation set.  9We release both types of the features on MS COCO 2014 train, val and test sets. Please refer to the readme  ﬁle at https://github.com/mjhucla/mRNN-CR to see how to download and use them.  10  Published as a conference paper at ICLR 2015  Figure 3: The sample images and their nearest neighbors retrieved by two types of features. Com- pared to the original VggNet features, the features reﬁned by the m-RNN model are better for cap- turing richer and more accurate visual information.  (the four corners, the center and their mirrored versions) on the resized image. Finally, we average pool the ten features to make it a 4,096 dimensional feature. The second type is the feature reﬁned by our m-RNN model. It can be calculated as: Ir = g2(VI·I), where VI is the weight matrix between the image representation and the multimodal layer (see Equation 3), and g2(.) is the scaled hyperbolic tangent function. We show the sample images and their nearest neighbors in Figure 3. We ﬁnd that compared to the original VggNet features, the features reﬁned by the m-RNN model capture richer and more accurate visual information. E.g., the target image in the second row contains an old woman with a bunch of bananas. The original VggNet features do not retrieve images with bananas in them.  8.2 CONSENSUS RERANKING  Suppose we have get the k nearest neighbor images in the training set as the reference. We follow Devlin et al. (2015a) to calculate the consensus score of a hypotheses. The difference is that Devlin et al. (2015a) treat the captions of the k nearest neighbor images as the hypotheses while our hy- potheses are generated by the m-RNN model. More speciﬁcally, for each hypothesis, we calculate the mean similarity between this hypothesis and all the captions of the k nearest neighbor images.  MS COCO val for consensus reranking  B4 CIDEr ROUGE L METEOR  B2  B1 m-RNN-shared 0.686 0.511 0.375 0.280 0.842 m-RNN-shared-NNref-BLEU 0.718 0.550 0.409 0.305 0.909 m-RNN-shared-NNref-CIDEr 0.714 0.543 0.406 0.304 0.938 m-RNN-shared-NNref-BLEU-Orcale 0.792 0.663 0.543 0.443 1.235 m-RNN-shared-NNref-CIDEr-Oracle 0.784 0.648 0.529 0.430 1.272  B3  m-RNN-shared m-RNN-shared-NNref-BLEU m-RNN-shared-NNref-CIDEr  MS COCO 2014 test server  B3  B2  B1 0.685 0.512 0.376 0.279 0.819 0.720 0.553 0.410 0.302 0.886 0.716 0.545 0.404 0.299 0.917  B4 CIDEr ROUGE L METEOR  0.500 0.519 0.519 0.602 0.593  0.228 0.235 0.239 0.287 0.287  0.504 0.524 0.521  0.229 0.238 0.242  Table 8: Results of m-RNN-shared model after applying consensus reranking using nearest neigh- bors as references (m-RNN-shared-NNref), compared with those of the original m-RNN model on our validation set and MS COCO test server.  11  Target ImageNearest Five Neighbors In Terms of m-RNN Refined FeatureNearest Five Neighbors In Terms of Original VGG FeaturePublished as a conference paper at ICLR 2015  The consensus score of this hypothesis is the mean similarity score of the m nearest captions. The similarity between a hypothesis and one of its nearest neighbor reference captions is deﬁned by a sentence-level BLEU score (Papineni et al. (2002)) or a sentence-level CIDEr (Vedantam et al. (2014)). We cross-validate the hyperparamters k and m. For the BLEU-based similarity, the opti- mal k and m are 60 and 175 respectively. For the CIDEr-based similarity, the optimal k and m are 60 and 125 respectively.  8.3 EXPERIMENTS  We show the results of our model on our validation set and the MS COCO testing server in Table 8. For BLEU-based consensus reranking, we get an improvement of 3.5 points on our validation set and 3.3 points on the MS COCO test 2014 set in terms of BLEU4 score. For the CIDEr-based consensus reranking, we get an improvement of 9.4 points on our validation set and 9.8 points on the MS COCO test 2014 set in terms of CIDEr.  8.4 DISCUSSION  We show the rank of the ten hypotheses before and after reranking in Figure 4. Although the hy- potheses are similar to each other, there are some variances among them (E.g., some of them capture more details of the images. Some of them might be partially wrong). The reranking process is able to improve the rank of good captions. We also show the oracle performance of the ten hypotheses, which is the upper bound of the con- sensus reranking. More speciﬁcally, for each image in our validation set, we rerank the hypotheses according to the scores (BLEU or CIDEr) w.r.t to the groundtruth captions. The results of this oracle reranking are shown in Table 8 (see rows with “-oracle”). The oracle performance is surprisingly high, indicating that there is still room for improvement, both for the m-RNN model itself and the reranking strategy.  9 CONCLUSION  We propose a multimodal Recurrent Neural Network (m-RNN) framework that performs at the state-of-the-art in three tasks: sentence generation, sentence retrieval given query image and image  Figure 4: The original rank of the hypotheses and the rank after consensus reranking (CIDEr).  12  Original After Reranking (C(Der) 1. a piece of cake on a plate on a table 2. a piece of cake on a white plate 3. a piece of cake sitting on top of a white plate 4. a piece of cake sitting on top of a plate 5. a piece of cake on a plate with a fork 6. a close up of a piece of cake on a plate 7. a piece of chocolate cake on a plate 8. a piece of cake sitting on a plate 9. a slice of cake on a white plate 10. a slice of cake on a plate with a fork 1. a piece of cake on a plate with a fork 2. a slice of cake on a plate with a fork 3. a close up of a piece of cake on a plate 4. a piece of cake on a plate on a table 5. a piece of cake on a white plate 6. a piece of cake sitting on top of a plate 7. a piece of cake sitting on top of a white plate 8. a piece of chocolate cake on a plate 9. a piece of cake sitting on a plate 10. a slice of cake on a white plate 1. a black and white photo of a black bear 2. a black and white photo of a bear 3. a black bear laying on top of a rock 4. a black bear sitting on top of a wooden bench 5. a black bear sitting on top of a rock 6. a black bear laying on top of a wooden bench 7. a black and white photo of a dog 8. a black bear laying on top of a wooden floor 9. a close up of a black and white dog 10. a close up of a black and white cat 1. a black bear sitting on top of a rock 2. a black bear laying on top of a rock 3. a black bear sitting on top of a wooden bench 4. a black bear laying on top of a wooden bench 5. a black bear laying on top of a wooden floor 6. a black and white photo of a black bear 7. a black and white photo of a bear 8. a close up of a black and white dog 9. a black and white photo of a dog 10. a close up of a black and white cat 1. a group of people standing next to each other 2. a group of people standing around a train 3. a group of people standing in a room 4. a group of people in a room with luggage 5. a group of people that are standing in a room 6. a group of people standing next to a train 7. a group of people standing in front of a train 8. a group of people sitting on a bench 9. a group of people standing in a room with luggage 10. a group of people standing next to each other on a train 1. a group of people standing in a room with luggage 2. a group of people in a room with luggage 3. a group of people standing next to a train 4. a group of people standing in front of a train 5. a group of people standing around a train 6. a group of people standing next to each other on a train 7. a group of people standing in a room 8. a group of people standing next to each other 9. a group of people that are standing in a room 10. a group of people sitting on a bench Published as a conference paper at ICLR 2015  retrieval given query sentence. The model consists of a deep RNN, a deep CNN and these two sub-networks interact with each other in a multimodal layer. Our m-RNN is powerful of connecting images and sentences and is ﬂexible to incorporate more complex image representations and more sophisticated language models.  ACKNOWLEDGMENTS  We thank Andrew Ng, Kai Yu, Chang Huang, Duohao Qin, Haoyuan Gao, Jason Eisner for useful discussions and technical support. We also thank the comments and suggestions of the anonymous reviewers from ICLR 2015 and NIPS 2014 Deep Learning Workshop. We acknowledge the Center for Minds, Brains and Machines (CBMM), partially funded by NSF STC award CCF-1231216, and ARO 62250-CS.  REFERENCES Barnard, Kobus, Duygulu, Pinar, Forsyth, David, De Freitas, Nando, Blei, David M, and Jordan,  Michael I. Matching words and pictures. JMLR, 3:1107–1135, 2003.  Chen, X., Fang, H., Lin, TY, Vedantam, R., Gupta, S., Dollr, P., and Zitnick, C. L. Microsoft coco  captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.  Chen, Xinlei and Zitnick, C Lawrence. Learning a recurrent visual representation for image caption  generation. arXiv preprint arXiv:1411.5654, 2014.  Cho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.  Devlin, Jacob, Cheng, Hao, Fang, Hao, Gupta, Saurabh, Deng, Li, He, Xiaodong, Zweig, Geoffrey, and Mitchell, Margaret. Language models for image captioning: The quirks and what works. arXiv preprint arXiv:1505.01809, 2015a.  Devlin, Jacob, Gupta, Saurabh, Girshick, Ross, Mitchell, Margaret, and Zitnick, C Lawrence. Ex- ploring nearest neighbor approaches for image captioning. arXiv preprint arXiv:1505.04467, 2015b.  Donahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio, Rohrbach, Marcus, Venugopalan, Sub- hashini, Saenko, Kate, and Darrell, Trevor. Long-term recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389, 2014.  Elman, Jeffrey L. Finding structure in time. Cognitive science, 14(2):179–211, 1990.  Fang, Hao, Gupta, Saurabh, Iandola, Forrest, Srivastava, Rupesh, Deng, Li, Doll´ar, Piotr, Gao, Jianfeng, He, Xiaodong, Mitchell, Margaret, Platt, John, et al. From captions to visual concepts and back. arXiv preprint arXiv:1411.4952, 2014.  Farhadi, Ali, Hejrati, Mohsen, Sadeghi, Mohammad Amin, Young, Peter, Rashtchian, Cyrus, Hock- enmaier, Julia, and Forsyth, David. Every picture tells a story: Generating sentences from images. In ECCV, pp. 15–29. 2010.  Frome, Andrea, Corrado, Greg S, Shlens, Jon, Bengio, Samy, Dean, Jeff, Mikolov, Tomas, et al.  Devise: A deep visual-semantic embedding model. In NIPS, pp. 2121–2129, 2013.  Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object  detection and semantic segmentation. In CVPR, 2014.  Grubinger, Michael, Clough, Paul, M¨uller, Henning, and Deselaers, Thomas. The iapr tc-12 bench- In International Workshop  mark: A new evaluation resource for visual information systems. OntoImage, pp. 13–23, 2006.  Guillaumin, Matthieu, Verbeek, Jakob, and Schmid, Cordelia. Multiple instance metric learning  from automatically labeled bags of faces. In ECCV, pp. 634–647, 2010.  13  Published as a conference paper at ICLR 2015  Gupta, Ankush and Mannem, Prashanth. From image annotation to image description. In ICONIP,  2012.  Gupta, Ankush, Verma, Yashaswi, and Jawahar, CV. Choosing linguistics over vision to describe  images. In AAAI, 2012.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Hodosh, Micah, Young, Peter, and Hockenmaier, Julia. Framing image description as a ranking  task: Data, models and evaluation metrics. JAIR, 47:853–899, 2013.  Jia, Yangqing, Salzmann, Mathieu, and Darrell, Trevor. Learning cross-modality similarity for  multinomial data. In ICCV, pp. 2407–2414, 2011.  Kalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. In EMNLP, pp.  1700–1709, 2013.  Karpathy, Andrej and Fei-Fei, Li. Deep visual-semantic alignments for generating image descrip-  tions. arXiv preprint arXiv:1412.2306, 2014.  Karpathy, Andrej, Joulin, Armand, and Fei-Fei, Li. Deep fragment embeddings for bidirectional  image sentence mapping. In arXiv:1406.5679, 2014.  Kiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S. Unifying visual-semantic embeddings  with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014a.  Kiros, Ryan, Zemel, R, and Salakhutdinov, Ruslan. Multimodal neural language models. In ICML,  2014b.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, pp. 1097–1105, 2012.  Kulkarni, Girish, Premraj, Visruth, Dhar, Sagnik, Li, Siming, Choi, Yejin, Berg, Alexander C, and  Berg, Tamara L. Baby talk: Understanding and generating image descriptions. In CVPR, 2011.  Kuznetsova, Polina, Ordonez, Vicente, Berg, Tamara L, and Choi, Yejin. Treetalk: Composition and compression of trees for image descriptions. Transactions of the Association for Computational Linguistics, 2(10):351–362, 2014.  LeCun, Yann A, Bottou, L´eon, Orr, Genevieve B, and M¨uller, Klaus-Robert. Efﬁcient backprop. In  Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012.  Lin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Doll´ar, Piotr, and Zitnick, C Lawrence. Microsoft coco: Common objects in context. arXiv preprint arXiv:1405.0312, 2014.  Mao, Junhua, Xu, Wei, Yang, Yi, Wang, Jiang, and Yuille, Alan L. Explain images with multimodal  recurrent neural networks. NIPS DeepLearning Workshop, 2014.  Mao, Junhua, Xu, Wei, Yang, Yi, Wang, Jiang, Huang, Zhiheng, and Yuille, Alan. Learning like a child: Fast novel visual concept learning from sentence descriptions of images. arXiv preprint arXiv:1504.06692, 2015.  Mikolov, Tomas, Karaﬁ´at, Martin, Burget, Lukas, Cernock`y, Jan, and Khudanpur, Sanjeev. Recur-  rent neural network based language model. In INTERSPEECH, pp. 1045–1048, 2010.  Mikolov, Tomas, Kombrink, Stefan, Burget, Lukas, Cernocky, JH, and Khudanpur, Sanjeev. Exten-  sions of recurrent neural network language model. In ICASSP, pp. 5528–5531, 2011.  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed represen-  tations of words and phrases and their compositionality. In NIPS, pp. 3111–3119, 2013.  14  Published as a conference paper at ICLR 2015  Mitchell, Margaret, Han, Xufeng, Dodge, Jesse, Mensch, Alyssa, Goyal, Amit, Berg, Alex, Ya- maguchi, Kota, Berg, Tamara, Stratos, Karl, and Daum´e III, Hal. Midge: Generating image descriptions from computer vision detections. In EACL, 2012.  Mnih, Andriy and Hinton, Geoffrey. Three new graphical models for statistical language modelling.  In ICML, pp. 641–648. ACM, 2007.  Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines.  In ICML, pp. 807–814, 2010.  Papineni, Kishore, Roukos, Salim, Ward, Todd, and Zhu, Wei-Jing. Bleu: a method for automatic  evaluation of machine translation. In ACL, pp. 311–318, 2002.  Rashtchian, Cyrus, Young, Peter, Hodosh, Micah, and Hockenmaier, Julia. Collecting image anno-  tations using amazon’s mechanical turk. In NAACL-HLT workshop 2010, pp. 139–147, 2010.  Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning representations by  back-propagating errors. Cognitive modeling, 1988.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge, 2014.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. arXiv preprint arXiv:1409.1556, 2014.  Socher, Richard, Le, Q, Manning, C, and Ng, A. Grounded compositional semantics for ﬁnding and  describing images with sentences. In TACL, 2014.  Srivastava, Nitish and Salakhutdinov, Ruslan. Multimodal learning with deep boltzmann machines.  In NIPS, pp. 2222–2230, 2012.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural net-  works. In NIPS, pp. 3104–3112, 2014.  Vedantam, Ramakrishna, Zitnick, C Lawrence, and Parikh, Devi. Cider: Consensus-based image  description evaluation. arXiv preprint arXiv:1411.5726, 2014.  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural  image caption generator. arXiv preprint arXiv:1411.4555, 2014.  Young, Peter, Lai, Alice, Hodosh, Micah, and Hockenmaier, Julia. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In ACL, pp. 479–488, 2014.  10 SUPPLEMENTARY MATERIAL  10.1 EFFECTIVENESS OF THE DIFFERENT COMPONENTS OF THE M-RNN MODEL  m-RNN m-RNN-NoEmbInput m-RNN-OneLayerEmb m-RNN-EmbOneInput m-RNN-visInRnn m-RNN-visInRnn-both m-RNN-visInRnn-both-shared  B-1 0.600 0.592 0.594 0.590 0.466 0.546 0.478  B-2 0.412 0.408 0.406 0.406 0.267 0.333 0.279  B-3 0.278 0.277 0.274 0.274 0.157 0.191 0.171  B-4 0.187 0.188 0.184 0.185 0.101 0.120 0.110  Table 9: Performance comparison of different versions of m-RNN models on the Flickr30K dataset. All the models adopt VggNet as the image representation. See Figure 5 for details of the models.  15  Published as a conference paper at ICLR 2015  Figure 5: Illustration of the seven variants of the m-RNN models.  In this section, we compare different variants of our m-RNN model to show the effectiveness of the two-layer word embedding and the strategy to input the visual information to the multimodal layer. The word embedding system. Intuitively, the two word embedding layers capture high-level se- mantic meanings of words more efﬁciently than the single layer word embedding. As an input to the multimodal layer, it offers useful information for predicting the next word distribution. To validate its efﬁciency, we train three different m-RNN networks: m-RNN-NoEmbInput, m-RNN- OneLayerEmb, m-RNN-EmbOneInput. They are illustrated in Figure 5. “m-RNN-NoEmbInput” denotes the m-RNN model whose connection between the word embedding layer II and the mul- timodal layer is cut off. Thus the multimodal layer has only two inputs: the recurrent layer and the image representation. “m-RNN-OneLayerEmb” denotes the m-RNN model whose two word embedding layers are replaced by a single 256 dimensional word-embedding layer. There are much more parameters of the word-embedding layers in the m-RNN-OneLayerEmb than those in the original m-RNN (256 · M v.s. 128 · M + 128 · 256) if the dictionary size M is large. “m-RNN- EmbOneInput” denotes the m-RNN model whose connection between the word embedding layer II and the multimodal layer is replaced by the connection between the word embedding layer I and the multimodal layer. The performance comparisons are shown in Table 9. Table 9 shows that the original m-RNN model with the two word embedding layers and the con- nection between word embedding layer II and multimodal layer performs the best. It veriﬁes the effectiveness of the two word embedding layers. How to connect the vision and the language part of the model. We train three variants of m-RNN models where the image representation is inputted into the recurrent layer: m-RNN-VisualInRNN, m-RNN-VisualInRNN-both, and m-RNN-VisualInRNN-Both-Shared. For m-RNN-VisualInRNN, we only input the image representation to the word embedding layer II while for the later two mod- els, we input the image representation to both the multimodal layer and word embedding layer II.  16  Embedding IEmbedding IIRecurrentMultimodalSoftMaxwstartImageCNNPredictw1The Original m-RNN model for one time frame128256256512wstartImageCNNPredictw1128256256512m-RNN-NoEmbInputwstartImageCNNPredictw1256256512m-RNN-OneLayerEmbwstartImageCNNPredictw1128256256512m-RNN-EmbOneInputwstartImageCNNPredictw1128256256512wstartImageCNNPredictw1128256256512VI(1)VI(2)wstartImageCNNPredictw1128256256512VIVIm-RNN-VisualInRnnm-RNN-VisualInRnn-Bothm-RNN-VisualInRnn-Both-SharedPublished as a conference paper at ICLR 2015  I  I  , V (2)  The weights of the two connections V (1) are shared for m-RNN-VisualInRNN-Both-Shared. Please see details of these models in Figure 5. Table 9 shows that the original m-RNN model performs much better than these models, indicating that it is effective to directly input the visual information to the multimodal layer. In practice, we ﬁnd that it is harder to train these variants than to train the original m-RNN model and we have to keep the learning rate very small to avoid the exploding gradient problem. Increasing the dimension of the recurrent layer or replacing RNN with LSTM (a sophisticated version of RNN Hochreiter & Schmidhuber (1997)) might solve the problem. We will explore this issue in future work.  10.2 ADDITIONAL RETRIEVAL PERFORMANCE COMPARISONS ON IAPR TC-12  For the retrieval results in this dataset, in addition to the R@K and Med r, we also adopt exactly the same evaluation metrics as Kiros et al. (2014b) and plot the mean number of matches of the retrieved groundtruth sentences or images with respect to the percentage of the retrieved sentences or images for the testing set. For the sentence retrieval task, Kiros et al. (2014b) uses a shortlist of 100 images which are the nearest neighbors of the query image in the feature space. This shortlist strategy makes the task harder because similar images might have similar descriptions and it is often harder to ﬁnd subtle differences among the sentences and pick the most suitable one. The recall accuracy curves with respect to the percentage of retrieved images (sentence retrieval task) or sentences (sentence retrieval task) are shown in Figure 6. The ﬁrst method, bowdecaf, is a strong image based bag-of-words baseline (Kiros et al. (2014b)). The second and the third models (Kiros et al. (2014b)) are all multimodal deep models. Our m-RNN model signiﬁcantly outperforms these three methods in this task.  10.3 THE CALCULATION OF BLEU SCORE  The BLEU score was proposed by Papineni et al. (2002) and was originally used as a evaluation metric for machine translation. To calculate BLEU-N (i.e. B-N in the paper where N=1,2,3,4) score, we ﬁrst compute the modiﬁed n-gram precision (Papineni et al. (2002)), pn. Then we compute the geometric mean of pn up to length N and multiply it by a brevity penalty BP:  (8)  (9) where r is the length of the reference sentence and c is the length of the generated sentence. We use the same strategy as Fang et al. (2014) where pn, r, and c are computed over the whole testing corpus. When there are multiple reference sentences, the length of the reference that is closest (longer or shorter) to the length of the candidate is used to compute r.  n=1 log pn  BP = min(1, e1− r c ) (cid:80)N  B-N = BP · e  1 N  (a) Image to Text Curve  (b) Text to Image Curve  Figure 6: Retrieval recall curve for (a). Sentence retrieval task (b). Image retrieval task on IAPR TC-12 dataset. The behavior on the far left (i.e. top few retrievals) is most important.  17  0.010.020.050.1 0.250.5 1   00.10.20.30.40.50.60.70.80.91  Ours−mRNNbow−decafMLBL−F−decafMLBL−B−decaf0.00050.001 0.002 0.005 0.01  0.02  0.05  0.1   0.25  0.5   1     00.10.20.30.40.50.60.70.80.91  Ours−mRNNbow−decafMLBL−F−decafMLBL−B−decaf",
1412.5903,2015,Deep Structured Output Learning for Unconstrained Text Recognition,"['Deep Structured Output Learning for Unconstrained Text Recognition', 'Max Jaderberg', 'Karen Simonyan', 'Andrea Vedaldi', 'and Andrew Zisserman']",https://arxiv.org/pdf/1412.5903,"5 1 0 2    r p A 0 1         ]  V C . s c [      5 v 3 0 9 5  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  DEEP STRUCTURED OUTPUT LEARNING FOR UNCONSTRAINED TEXT RECOGNITION  Max Jaderberg∗, Karen Simonyan*, Andrea Vedaldi & Andrew Zisserman+ Visual Geometry Group, Department of Engineering Science, University of Oxford {max,karen,vedaldi,az}@robots.ox.ac.uk  ABSTRACT  We develop a representation suitable for the unconstrained recognition of words in natural images, where unconstrained means that there is no ﬁxed lexicon and words have unknown length. To this end we propose a convolutional neural network (CNN) based architecture which incorporates a Conditional Random Field (CRF) graphical model, taking the whole word image as a single input. The unaries of the CRF are provided by a CNN that predicts characters at each position of the output, while higher order terms are provided by another CNN that detects the presence of N-grams. We show that this entire model (CRF, character predictor, N-gram predictor) can be jointly optimised by back-propagating the structured output loss, essentially requiring the system to perform multi-task learning, and training requires only synthetically generated data. The resulting model is a more accurate system on standard real-world text recognition benchmarks than character prediction alone, setting a benchmark for systems that have not been trained on a particular lexicon. In addition, our model achieves state-of-the-art accuracy in lexicon-constrained scenarios, without being speciﬁcally modelled for constrained recognition. To test the generalisation of our model, we also perform experiments with random alpha-numeric strings to evaluate the method when no visual language model is applicable.  1  INTRODUCTION  In this work we tackle the problem of unconstrained text recognition – recognising text in natural images without restricting the words to a ﬁxed lexicon or dictionary. Usually this problem is de- composed into a word detection stage followed by a word recognition stage. The word detection stage generates bounding boxes around words in an image, while the word recognition stage takes the content of these bounding boxes and recognises the text within. This paper focuses on the text recognition stage, developing a model based on deep convolutional neural networks (CNNs) (LeCun et al. (1998)). Previous methods using CNNs for word recognition (discussed in more detail in section Section 2) has either constrained (Jaderberg et al. (2014b)) or heavily weighted (Bissacco et al. (2013)) the recognition results to be from a dictionary of known words. This works very well when training and testing are limited to a ﬁxed vocabulary, but does not generalise to where previously unseen or non-language based text must be recognised – for example for generic alpha-numeric strings such as number plates or phone numbers. The shift of focus towards a model which performs accurately without a ﬁxed dictionary increases the complexity of the text recognition problem. To solve this, we propose a novel CNN architecture (Figure 2) employing a Conditional Random Field (CRF) whose unary terms are outputs of a CNN character predictor, which are position-dependent, and whose higher order terms are outputs of a CNN N-gram predictor, which are position-independent. The recognition result is then obtained by ﬁnding the character sequence that maximises the CRF score, enforcing the consistency of the individual predictions.  ∗Current afﬁliation Google DeepMind. +Current afﬁliation University of Oxford and Google DeepMind.  1  Published as a conference paper at ICLR 2015  The CRF model builds on our previous work where we explored dictionary-based recognition (Jader- berg et al. (2014a)) for two scenarios: the ﬁrst was to train a different CNN character classiﬁer for each position in the word being recognised, using the whole image of the word as input to each classiﬁer (an idea also expored by Goodfellow et al. (2013)); the second was to construct a CNN predictor to detect the N-grams contained in the word, effectively encoding the text as a bag-of-N- grams. The dictionary-free joint model proposed here is trained by deﬁning a structured output learning problem, and back-propagating the corresponding structured output loss. This formulation results in multi-task learning of both the character and N-gram predictors, and additionally learns how to combine their representations in the CRF, resulting in more accurate text recognition. The result is a highly ﬂexible text recognition system that achieves excellent unconstrained text recognition performance as well as state-of-the-art recognition performance when using standard dictionary constraints. While performance is measured on real images as contained in standard text recognition benchmarks, all results are obtained by training the model purely on synthetic data. The model is evaluated on this synthetic data as well in order to study its performance under different scenarios. Section 2 outlines work related to ours. Section 3.1 reviews the character sequence model and Section 3.2 the bag-of-N-grams model. Section 4 shows how these predictors can be combined to form a joint CRF model and formulates the training of the latter as structured-output learning. Section 5 evaluates these models extensively and Section 6 summarises our ﬁndings.  2 RELATED WORK  We concentrate here on text recognition methods, recognising from a cropped image of a single word, rather than the text detection stages of scene text recognition (‘text spotting’) that generate the word detections. Traditional text recognition methods are based on sequential character classiﬁcation, ﬁnding char- acters by sliding window methods (Wang et al. (2011; 2012); Jaderberg et al. (2014c), after which a word prediction is made by integrating character classiﬁer predictions in a left-to-right manner. The character classiﬁers include random ferns (Ozuysal et al. (2007)) in Wang et al. (2011), and CNNs in Wang et al. (2012); Jaderberg et al. (2014c). Both Wang et al. (2011) and Wang et al. (2012) use a small ﬁxed lexicon as a language model to constrain word recognition. More recent works such as Bissacco et al. (2013); Alsharif & Pineau (2014) make use of over- segmentation methods, guided by a supervised classiﬁer, to generate candidate character proposals in a single-word image, which are subsequently classiﬁed as true or false positives. For example, PhotoOCR (Bissacco et al. (2013)) uses binarization and a sliding window classiﬁer to generate can- didate character regions, with words recognised through a beam search driven by classiﬁer scores and static N-gram language model, followed by a re-ranking using a dictionary of 100k words. Jader- berg et al. (2014c) uses the convolutional nature of CNNs to generate response maps for characters and bigrams which are integrated to score lexicon words. In contrast to these approaches based on character classiﬁcation, the work by Almaz´an et al. (2014); Gordo (2014); Goel et al. (2013); Rodriguez-Serrano et al. (2013); Novikova et al. (2012); Mishra et al. (2012) instead uses the notion of holistic word recognition. Mishra et al. (2012); Novikova et al. (2012) still rely on explicit character classiﬁers, but construct a graph to infer the word, pool- ing together the full word evidence. Rodriguez-Serrano et al. (2013) use aggregated Fisher Vec- tors (Perronnin et al. (2010)) and a Structured SVM framework to create a joint word-image and text embedding. Almaz´an et al. (2014) and more recently Gordo (2014) also formluate joint embed- ding spaces, achieving impressive results with minimal training data. Goel et al. (2013) use whole word-image features to recognize words by comparing to simple black-and-white font-renderings of lexicon words. In our own previous work (Jaderberg et al. (2014a;b)) we use large CNNs acting on the full word image region to perform 90k-way classiﬁcation to a dictionary word. It should be noted that all the methods make use of strong static language models, either relying on a constrained dictionary or re-ranking mechanism.  2  Published as a conference paper at ICLR 2015  (a)  (b)  Figure 1: (a) The character sequence model. A word image is recognised by predicting the character at each position in the output, spelling out the text character by character. Each positional classiﬁer is learnt independently but shares a jointly optimised set of features. (b) The N-gram encoding model. The recognised text is represented by its bag-of-N-grams. This can be thought of as 10k independently trained binary classiﬁers using a shared set of jointly learnt features, trained to detect the presence of a particular N-gram.  Goodfellow et al. (2013) had great success using a CNN with multiple position-sensitive character classiﬁer outputs (closely related to the character sequence model in Section 3.1) to perform street number recognition. This model was extended to CAPTCHA sequences (up to 8 characters long) where they demonstrated impressive performance using synthetic training data for a synthetic prob- lem (where the generative model is known), but we show that synthetic training data can be used for a real-world data problem (where the generative model is unknown). There have been previous uses of graphical models with back-propagated loss functions for neural networks, such as the early text recognition work of LeCun et al. (1998) to combine character clas- siﬁer results on image segmentations. Another example is the recent work of Tompson et al. (2014) for human pose estimation, where an MRF-like model over the distribution of spatial locations for each body part is constructed, incorporating a single round of message-passing.  3 CNN TEXT RECOGNITION MODELS  We now review the component CNN models, originally presented in our tech report Jaderberg et al. (2014a), that form the basis of our joint model in Section 4.  3.1 CHARACTER SEQUENCE MODEL REVIEW  In this section we describe our character sequence model. This model encodes the character at each position in the word and so predicts the sequence of characters in an image region (hereafter we simply refer to the image region as an image). Each position in the word is modelled by an independent classiﬁer acting on a shared set of features from a single CNN. By construction, this model makes no assumptions about the underlying language and allows completely unconstrained recognition. A word w of length N is modelled as a sequence of characters such that w = (c1, c2, . . . , cN ) where each ci ∈ C = {1, 2, . . . , 36} represents a character at position i in the word, from the set of 10 digits and 26 letters. Each ci can be predicted with a single classiﬁer, one for each character in the word. However, since words have variable length N which is unknown at test time, we ﬁx the number of characters to Nmax (here set to 23), the maximum length of a word in the training set, and introduce a null character class. Therefore a word is represented by a string w ∈ (C ∪ {φ})Nmax. For a given input image x, we want to return the estimated word w∗ which maximises P (w∗|x). Since we seek an unconstrained recognition system with this model, we assume independence be- tween characters leading to  w∗ = arg max  w  P (w|x) = arg max  c1,c2,...,cNmax  P (ci|Φ(x))  (1)  where P (ci|Φ(x)) is given by the classiﬁer for the i-th position acting on a single set of shared CNN features Φ(x). The word w∗ can be computed by taking the most probable character at each position i = arg maxci∈C∪{φ} P (ci|Φ(x)). c∗  3  Nmax(cid:89)  i=1  Published as a conference paper at ICLR 2015  The CNN (Figure 1 (a)) takes the whole word image x as input. Word images can be of different sizes, in particular due to the variable number of characters in the image. However, our CNN requires a ﬁxed size input for all input images. This problem is overcome by simply resampling the original word image to a canonical height and width, without regard to preserving the aspect ratio, producing a ﬁxed size input x. The base CNN has a number of convolutional layers followed by a series of fully connected layers, giving Φ(x). The full details of the network architecture are given in Section 5.2. Φ(x) is fed to Nmax separate fully connected layers with 37 neurons each, one for each character class including the null character. These fully connected layers are independently softmax normalised and can be interpreted as the probabilities P (ci|Φ(x)) of the width-resized input image x. The CNN is trained with multinomial logistic regression loss, back-propagation, and stochastic gra- dient descent (SGD) with dropout regularisation similar to Hinton et al. (2012).  3.2 BAG-OF-N-GRAMS MODEL REVIEW  This section describes our second word recognition model, which exploits compositionality to rep- resent words. In contrast to the sequential character encoding of Section 3.1, words can be seen as a composition of an unordered set of character N-grams, a bag-of-N-grams. In the follow- ing, if s ∈ CN and w ∈ CM are two strings, the symbol s ⊂ w indicates that s is a sub- string of w. An N-gram of word w is a substring s ⊂ w of length |s| = N. We will de- note with GN (w) = {s : s ⊂ w ∧ |s| ≤ N} the set of all N-grams of word w of length up to N and with GN = ∪w∈W GN (w) the set of all such grams in the language. For example, G3(spires) = {s, p, i, r, e, sp, pi, ir, re, es, spi, pir, ire, res}. This method of encod- ing variable length sequences is similar to the Wickelphone phoneme-encoding methods (Wickelgran (1969)). Even for small values of N, GN (w) encodes each word w ∈ W nearly uniquely. For example, with N = 4, this map has only 7 collisions out of a dictionary of 90k words. The encoding GN (w) can be represented as a |GN|-dimensional binary vector of N-gram occurrences. This vector is very sparse, as on average |GN (w)| ≈ 22 whereas |GN| = 10k. Using a CNN we can predict GN (w) for a word w depicted in the input image x. We can use the same architecture as in Section 3.1, but now have a ﬁnal fully connected layer with GN neurons to represent the encoding vector. The scores from the fully connected layer can be interpreted as probabilities of an N-gram being present in the image by applying the logistic function to each neuron. The CNN is therefore learning to recognise the presence of each N-gram somewhere within the input image, so is an N-gram detector. With the applied logistic function, the training problem becomes that of |GN| separate binary clas- siﬁcation tasks, and so we back-propagate the logistic regression loss with respect to each N-gram class independently. To jointly train a whole range of N-grams, some of which occur very frequently and some barely at all, we have to scale the gradients for each N-gram class by the inverse frequency of their appearance in the training word corpus. We also experimented with hinge loss and simple regression to train but found frequency weighted binary logistic regression was superior. As with the other model, we use dropout and SGD. In this model we exploit the statistics of our underlying language in choosing a subset of |GN| N- grams from the space of all possible N-grams to be modelled. This can be seen as using a language model to compress the representation space of the encoding, but is not restraining the predictive capability for unconstrained recognition. While the encoding GN (w) is almost always unique for words from natural language, non-language words often contain much fewer N-grams from the modelled set GN leading to more ambiguous and non-unique encodings.  4  JOINT MODEL  imising the log-score log P (w|x) = S(w, x) =(cid:80)Nmax  In Section 3.1, maximising the posterior probability of a character sequence (1) is equivalent to max- c(ci, x) = log P (ci|Φ(x)) is the logarithm of the posterior probability of the character at position i in the sequence. The graph  c(ci, x) where Si  i=1 Si  4  Published as a conference paper at ICLR 2015  Figure 2: An illustration of the construction of the path score S(camel, x) for the word camel. The unary and edge terms used for the score are selected by the path through the graph of character positions shown in the upper right corner. The values of these terms, Sc(ci, x) and Se(s, x), where s ⊂ w, are given by the outputs of the character sequence model CNN (CHAR CNN) and the N-gram encoding CNN (NGRAM CNN).  associated with this function is a set of nodes, one for each unary term Si contain any edges. Hence maximising the function reduces to maximising each term individually. The model can now be extended to incorporate the N-gram predictors of Section 3.2, encoding the presence of N-grams in the word image x. The N-gram scoring function Se(s, x) assigns a score to each string s of length |s| ≤ N, where N is the maximum order of N-gram modelled. Note that, c deﬁned before, the function Se is position-independent. However, differently from the functions Si it is applied repeatedly at each position i in the word:  c(ci, x), and does not  Nmax(cid:88)  |w|(cid:88)  min(N,|w|−i+1)(cid:88)  S(w, x) =  Si  c(ci, x) +  Se(cici+1 . . . ci+n−1, x).  (2)  i=1  i=1  n=1  c(ci, x) are obtained from the CNN character predictors of As illustrated in Figure 2, the scores Si Section 3.1 whereas the score Se(s, x) is obtained from the CNN N-gram predictor of Section 3.2; note that the N-gram scoring function is only deﬁned for the subset GN of N-grams modelled in the CNN; if s (cid:54)∈ GN , the score Se(s, x) = 0 is deﬁned to be zero. The graph associated with the function (2) has cliques of order N; hence, when N is even moderately large, we resort to beam search (Russel et al. (1994)) to maximise (2) and ﬁnd the predicted word w∗. Also, the score (2) can be interpreted as a potential function deﬁning a word posterior probability as before; however, evaluating this probability would require computing a normalisation factor, which is non-trivial. Instead, the function is trained discriminatively, as explained in the next section.  Structured Output Loss. The unary and edge score functions Si c(ci, x) and Se(s, x), should in- corporate the outputs of the character sequence model and N-gram encoding model respectively. A simple way to do this is to apply a weighting to the output of the CNNs after removing the softmax normalisation and the logistic loss:  Nmax(cid:88)  |w|(cid:88)  min(N,|w|−i+1)(cid:88)  i=1  i=1  n=1  S(w, x) =  αi ci  f i ci  (x) +  βcici+1...ci+n−1gcici+1...ci+n−1 (x),  (3)  ci  (x) is the output of the character sequence CNN for character ci at position i and gs(x) If desired, the character weights } and edge weights β = {βs} can be constrained to be shared across different characters,  where f i ci is the output of the N-gram encoding CNN for the N-gram s. α = {αi character positions, different N-grams of the same order, or across all N-grams. The sets of weights α and β in Equation 3, or any weight-constrained variant of Equation 3, can be learnt in a structured output learning framework, encouraging the score of the ground-truth word wgt to be greater than or equal to the highest scoring incorrect word prediction plus a margin, i.e. S(wgt, x) ≥ µ + S(w∗, x) where S(w∗, x) = maxw(cid:54)=wgt S(w, x). Enforcing this as a soft- constraint results in the convex loss  max(0, µ + S(w, x) − S(wgt,i, xi))  (4)  L(xi, wgt,i, S) = max w(cid:54)=wgt,i  and averaging over M example pairs (xi, wgt,i) results in the regularised empirical risk objective  L(xi, wgt,i, S).  (5)  M(cid:88)  i=1  E(S) =  (cid:107)α(cid:107)2 +  λα 2  λβ 2  (cid:107)β(cid:107)2 +  1 M  5  Nmax(cid:88)  (cid:26)1  Published as a conference paper at ICLR 2015  Figure 3: The architecture for training the joint model, comprising of the character sequence model (CHAR) and and the N-gram encoding model (NGRAM) with structured output loss. The Path Select Layer generates the score S(wgt, x) by summing the inputs of the groundtruth word. The Beam Search Layer uses beam search to try to select the path with the largest score S(w∗, x) from the inputs. The hinge loss implements a ranking loss, constraining the highest scoring path to be the groundtruth path, and can be back-propagated through the entire network to jointly learn all the parameters.  However, in the general scenario of Equation 3, the weights can be incorporated into the CNN functions f and g, resulting in the score  |w|(cid:88)  min(N,|w|−i+1)(cid:88)  (cid:26)1  0  |w|−|s|+1(cid:88)  S(w, x) =  f i ci  (x) +  gcici+1...ci+n−1(x),  (6)  i=1  i=1  n=1  The functions f and g are deﬁned by CNNs and so we can optimise the parameters of them to reduce the cost in Equation 5. This can be done through standard back-propagation and SGD. Differentiating the loss L with respect to S gives  ∂L(x, wgt, S) ∂S(w∗, x)  =  (7) where z = maxw(cid:54)=wgt,i µ + S(w, x) − S(wgt, x). Differentiating the score function of Equation 6 ci and gs gives with respect to the character sequence model and N-gram encoding model outputs f i  ∂S(wgt, x)  =  0  0  ∂L(x, wgt, S)  if z > 0 otherwise  if z > 0 otherwise  (cid:26)−1  ∂S(w, x)  =  ∂f i c  if ci = c otherwise ,  ∂S(w, x)  =  ∂gs  1{cici+1...ci+|s|−1=s}  (8)  i=1 This allows errors to be back-propagated to the entire network. Intuitively, the errors are back- propagated through the CNN outputs which are responsible for margin violations, since they con- tributed to form an incorrect score. Using this structured output loss allows the parameters of the entire model to be jointly optimised within the structure imposed by Equation 6. Figure 3 shows the training architecture used. Due to the presence of high order scores in Equation 6, it is too expensive to exhaustively search the space of all possible paths to ﬁnd w∗, even with dynamic programming, so instead we use beam search to ﬁnd the approximate highest scoring path. The structured output loss described in this section bares resemblance to the discriminative Viterbi training introduced by LeCun et al. (1998). However, our model includes higher-order terms, terms of a different nature (N-grams), and uses a structured-output formulation. Furthermore, our method incorporates only a very weak language model, limited to assigning a score of 0 to all N-grams outside a target set GN . Note that this does not mean that these N-grams cannot be recognised (this would require assigning to them a score of −∞); instead, it is a smoothing technique that assigns a nominal score to infrequent N-grams.  5 EVALUATION  In this section we evaluate the three models introduced in the previous sections. The datasets used for training and testing are described in Section 5.1, the implementation details given in Section 5.2, and the results of experiments reported in Section 5.3.  6  Published as a conference paper at ICLR 2015  5.1 DATASETS  We evaluate our models on a number of standard datasets – ICDAR 2003, ICDAR 2013, Street View Text, and IIIT5k, whereas for training, as well as testing across a larger vocabulary, we turn to the synthetic Synth90k and SynthRand datasets. ICDAR 2003 (Lucas et al. (2003)) is a scene text recognition dataset, with the test set containing 251 full scene images and 860 groundtruth cropped images of the words contained with the full images. We follow the standard evaluation protocol deﬁned by Wang et al. (2011) and perform recognition on the words containing only alphanumeric characters and at least three characters. The test set of 860 cropped word images is referred to as IC03. The lexicon of all test words is IC03-Full (563 words), and the per-image 50 word lexicons deﬁned by Wang et al. (2011) and used in a number of works (Wang et al. (2011; 2012); Alsharif & Pineau (2014)) are referred to as IC03-50. ICDAR 2013 (Karatzas et al. (2013)) test dataset contains 1015 groundtruth cropped word images from scene text. Much of the data is inherited from the ICDAR 2003 datasets. We refer to the 1015 groundtruth cropped words as IC13. Street View Text (Wang et al. (2011)) is a more challenging scene text dataset than the ICDAR datasets. It contains 250 full scene test images downloaded from Google Street View. The test set of 647 groundtruth cropped word images is referred to as SVT. The lexicon of all test words is SVT- Full (4282 words), and the smaller per-image 50 word lexicons deﬁned by Wang et al. (2011) and used in previous works (Wang et al. (2011; 2012); Alsharif & Pineau (2014); Bissacco et al. (2013)) are referred to as SVT-50. IIIT 5k-word (Mishra et al. (2012)) test dataset contains 3000 cropped word images of scene text downloaded from Google image search. Each image has an associated 50 word lexicon (IIIT5k-50) and 1k word lexicon (IIIT5k-1k). Synth90k1 (Jaderberg et al. (2014a;b)) is a dataset of 9 million cropped word images that have been synthetically generated. The synthetic data is highly realistic and can be used to train on and as a challenging test benchmark. The dataset covers 90k different English words, and there are predeﬁned training and test splits with approximately 8 million training images and 900k test images. In addition, we use the same synthetic text engine from Jaderberg et al. (2014a;b) to generate word images with completely random strings of up to 10 uniformly sampled alphanumeric characters. We refer to this dataset as SynthRand. The training set consists of 8 million training images and the test set of 900k images. In this corpus there are very few word repetitions (in addition to the random rendering variations). There is a wide range of difﬁculty in this dataset, from perfectly readable text to almost impossible to read samples.  5.2  IMPLEMENTATION DETAILS  In the following, the character sequence model is referred to as CHAR, the N-gram encoding model as NGRAM, and the joint model as JOINT. The CHAR and NGRAM models both have the same base CNN architecture. The base CNN has ﬁve convolutional layers and two fully connected layers. The input is a 32 × 100 greyscale image obtained by resizing the word image (ignoring its aspect ratio) and then subtracting its mean and dividing by its standard deviation. Rectiﬁed linear units are used throughout after each weight layer except for the last one. In forward order, the convolutional layers have 64, 128, 256, 512, and 512 square ﬁlters with an edge size of 5, 5, 3, 3, and 3. Convolutions are performed with stride 1 and there is input feature map padding to preserve spatial dimensionality. 2× 2 max-pooling follows the ﬁrst, second and third convolutional layers. The fully connected layers have 4096 units. On top of this base CNN, the CHAR model has 23 independent fully connected layers with 37 units, allowing recognition of words of up to Nmax = 23 characters long. The NGRAM model operates on a selection of 10k frequent N-grams of order N ≤ 4 (identiﬁed as the ones that occur at least 10 times in the Synth90k word corpus, resulting in 36 1-grams, 522 2-grams, 3965 3-grams, and 5477 4-grams). This requires a ﬁnal fully connected layer on top of the base CNN with 10k units. Therefore, the graph of function (6) has cliques of sizes at most 4. Beam search uses a width of  1http://www.robots.ox.ac.uk/˜vgg/data/text/  7  Published as a conference paper at ICLR 2015  Training Data  Test Data  Synth90k-train  Synth1-72k Synth1-45k SynthRand  Synth90k-test IC03 SVT IC13 Synth72k-90k Synth45k-90k SynthRand  CHAR JOINT 91.0 87.3 89.6 85.9 71.7 68.0 81.8 79.5 89.7 82.4 89.1 80.3 80.7 79.5  Table 1: Left: The accuracy (%) of the character sequence model, CHAR, and the joint model, JOINT. Differ- ent combinations of training and test data are evaluated. Synthx-y refers to a subset of the Synth90k that only contains words in the label interval [x, y] (word label indices are in random, non-alphabetical order). Train- ing and testing on completely distinct words demonstrates the power of a general, unconstrained recognition model. Right: Some results of the CHAR model on the SynthRand test dataset. Letters in red have been predicted incorrectly with the groundtruth (GT) shown below. Notice the range in difﬁculty of the SynthRand data. 5 during training and of 10 during testing. If a lexicon is used to constrain the output, instead of performing beam search, the paths associated with the lexicon words are scored with Equation 6, and the word with the maximum score is selected as the ﬁnal result. The three models are all trained with SGD and dropout regularisation. The learning rates are dynam- ically decreased as training progresses. The JOINT model is initialised with the pre-trained CHAR and NGRAM network weights and the convolutional layers’ weights are frozen during training.  5.3 EXPERIMENTS  We evaluate our models on a combination of real-world test data and synthetic data to highlight different operating characteristics.  N-gram Encoding Results. The NGRAM model predicts the N-grams contained in input word image. Due to the highly unbalanced nature of this problem (where only 10-20 N-grams are con- tained in any given image), results are reported as the maximum achieved F-score, computed as the harmonic mean of precision and recall. The latter are computed by sweeping the threshold prob- ability for an N-gram to be classiﬁed as present in the word. The maximum achieved F-score on Synth90k is 87.0% and on IC03 is 87.1%. This demonstrates that, while not perfect, the NGRAM model accurately models the presence of N-grams in word images.  Character Sequence and Joint Model Results. The CHAR and JOINT models are evaluated on standard as well as synthetic benchmarks (Table 1), but both models are trained on Synth90k. While the CHAR model achieves good performance, it is consistently outperformed by the JOINT model; the accuracy improvement is as much as +4% on IC03 and SVT, despite the difﬁculty of the latter. Figure 4 shows some example results using the JOINT model. Next, we evaluate the ability of our model to generalise by recognising words unseen during training. This effectively amounts to zero-shot learning and is a key contribution compared to Jaderberg et al. (2014a;b). In order to do so, the training vocabulary is split into two parts, with one part (50% or 80%) used for training and the other one for evaluation (50% or 20%). In this case the CHAR model is signiﬁcantly penalised, but the JOINT model can recover most of the performance. For instance, on the 50/50 split, the JOINT model accuracy is 89.1%, only -2% compared to the 91.0% obtained when the training and testing vocabularies are equal. The ﬁnal test pushes generalisation by training and testing on completely random strings from Syn- thRand. As this dataset is a lot less regular than a natural language, the performance of the CHAR model suffers, dropping to 80.7% accuracy. Furthermore, as could be expected form the absence of common N-grams in the random language, the JOINT model performs slightly worse at 79.5% accuracy. However this drop is very small because N-grams are not used as hard constraints on the predicted words, but rather to nudge the word scores based on further visual cues.  Comparison to the state-of-the-art. Table 2 compares the accuracy of CHAR and JOINT to previous works. Whereas these works make use of strong language models, our models make min-  8  Published as a conference paper at ICLR 2015  (a)  (b)  Figure 4: Results where the unary terms of the JOINT model cannot solely recognise the word correctly, but the addition of the edge scores result in correct recognition, from SVT (a,b) and IC13 (c). The input image is shown in the top left corner. The unary scores for characters (rows) at each position (columns, up to 12 out of 23 characters) are shown, with the selected path using only the unary score term Si c (orange) and when edge scores Se are incorporated (cyan). The bars show the NGRAM strengths, with lighter colour representing larger values.  (c)  Model  IC03-50*  IC03-Full*  IC03-50k*  IC03 SVT-50* SVT IC13 IIIT5k-50*  IIIT5k-1k*  Baseline ABBYY (Wang et al. (2011)) Wang et al. (2011) Mishra et al. (2012) Novikova et al. (2012) Wang et al. (2012) Goel et al. (2013) Bissacco et al. (2013) Alsharif & Pineau (2014) Almaz´an et al. (2014) Yao et al. (2014) Jaderberg et al. (2014c) Gordo (2014) DICT Jaderberg et al. (2014a;b) CHAR JOINT  56.0 76.0 81.8 82.8 90.0 89.7  93.1  88.5 96.2  98.7 98.5 97.8  -  -  -  55.0 62.0 67.8  84.0  88.6  80.3 91.5  98.6 96.7 97.0  -  - -  -  -  - - - - - - -  - - - -  85.1  93.3 92.3 93.4  - - - - - - - - - - - -  93.1 85.9 89.6  35.0 57.0 73.2 72.9 70.0 77.3 90.4 74.3 89.2 75.9 86.1 90.7 95.4 93.5 93.2  - - - - - -  - - - - -  - - - - - -  - - - - -  78.0  87.6  80.7 68.0 71.7  90.8 79.5 81.8  - -  - - - -  -  24.3  64.1  91.2 80.2  93.3 97.1 95.0 95.5  - - -  - - - -  -  57.5  82.1 69.3  86.6 92.7 89.3 89.6  Table 2: Comparison to previous methods. The baseline method is from a commercially available OCR system. Note that the training data for DICT includes the lexicons of the test sets, so it has the capacity to recognise all test words. *Results are constrained to the lexicons described in Section 5.1.  imal assumptions about the language. In the constrained lexicon cases (the starred columns of Ta- ble 2), both CHAR and JOINT are very close to the state-of-the-art DICT model of Jaderberg et al. (2014a;b). Furthermore, if the same 90k dictionary used in by the DICT model is used to constrain the output of the JOINT model, the performance is identical at 93.1% accuracy on IC03. While in the constrained lexicon experiments the lexicon is limited at test time, these results are still remark- able because, differently from DICT, CHAR and JOINT are not trained on a speciﬁc dictionary. In particular, DICT would not be able to operate on random strings. The recognition results without a lexicon are still behind that of some constrained models, however the JOINT model provides competitive performance and is far more ﬂexible to recognise unseen words than previous works, while still achieving state-of-the-art performance if a lexicon is then applied as a constraint at test time. Figure 4 shows some example results where the CHAR model does not recognise the word correctly but the JOINT model succeeds.  6 CONCLUSION  In this paper we have introduced a new formulation for word recognition, designed to be used iden- tically in language and non-language scenarios. By modelling character positions and the presence of common N-grams, we can deﬁne a joint graphical model. This can be trained effectively by back propagating structured output loss, and results in a more accurate word recognition system than pre- dicting characters alone. We show impressive results for unconstrained text recognition with the ability to generalise recognition to previously unseen words, and match state-of-the-art accuracy when comparing in lexicon constrained scenarios.  Acknowledgments. This work was supported by the EPSRC and ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.  9  Published as a conference paper at ICLR 2015  REFERENCES  Almaz´an, J., Gordo, A., Forn´es, A., and Valveny, E. Word spotting and recognition with embedded attributes.  In TPAMI, 2014.  Alsharif, O. and Pineau, J. End-to-End Text Recognition with Hybrid HMM Maxout Models. In Proc. ICLR,  2014.  Bissacco, A., Cummins, M., Netzer, Y., and Neven, H. PhotoOCR: Reading text in uncontrolled conditions. In  Proc. ICCV, 2013.  Goel, V., Mishra, A., Alahari, K., and Jawahar, C. V. Whole is greater than sum of parts: Recognizing scene  text words. In ICDAR, pp. 398–402, 2013.  Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street  view imagery using deep convolutional neural networks. arXiv:1312.6082, 2013.  Gordo, A. Supervised mid-level features for word image representation. ArXiv e-prints, Oct 2014. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Improving neural networks  by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.  Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman, A. Synthetic data and artiﬁcial neural networks for  natural scene text recognition. NIPS Deep Learning Workshop 2014, 2014a.  Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman, A. Reading text in the wild with convolutional neural  networks. arXiv pre-print, 2014b.  Jaderberg, M., Vedaldi, A, and Zisserman, A. Deep features for text spotting. In European Conference on  Computer Vision, 2014c.  Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., Mestre, S. R., i Bigorda, L. G., Mas, J., Mota, D. F., Almazan, J., and de las Heras, L. P. ICDAR 2013 robust reading competition. In ICDAR, pp. 1484–1493, 2013.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, 1998.  Lucas, S. M., Panaretos, A., Sosa, L., Tang, A., Wong, S., and Young, R. ICDAR 2003 robust reading compe-  titions. In ICDAR, pp. 682–687, 2003.  Mishra, A., Alahari, K., and Jawahar, C. Scene text recognition using higher order language priors. 2012. Novikova, T., Barinova, O., Kohli, P., and Lempitsky, V. Large-lexicon attribute-consistent text recognition in  natural images. In Proc. ECCV, pp. 752–765. Springer, 2012.  Ozuysal, M., Fua, P., and Lepetit, V. Fast keypoint recognition in ten lines of code. In Proc. CVPR, 2007. Perronnin, F., Liu, Y., S´anchez, J., and Poirier, H. Large-scale image retrieval with compressed ﬁsher vectors.  In Proc. CVPR, 2010.  Rodriguez-Serrano, J. A., Perronnin, F., and Meylan, F. Label embedding for text recognition. In Proc. BMVC.,  2013.  Russel, Stuart, Norvig, Peter, et al. Artiﬁcial intelligence: A modern approach, 1995. Cited on, pp. 20, 1994. Tompson, Jonathan J, Jain, Arjun, LeCun, Yann, and Bregler, Christoph. Joint training of a convolutional network and a graphical model for human pose estimation. In Advances in Neural Information Processing Systems, pp. 1799–1807, 2014.  Wang, K., Babenko, B., and Belongie, S. End-to-end scene text recognition. In Proc. ICCV, pp. 1457–1464.  IEEE, 2011.  Wang, T., Wu, D. J, Coates, A., and Ng, A. Y. End-to-end text recognition with convolutional neural networks.  In ICPR, pp. 3304–3308. IEEE, 2012.  Wickelgran, Wayne A. Context-sensitive coding, associative memory, and serial order in (speech) behavior.  Psychological Review, 76(1):1, 1969.  Yao, Cong, Bai, Xiang, Shi, Baoguang, and Liu, Wenyu. Strokelets: A learned multi-scale representation for scene text recognition. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 4042–4049. IEEE, 2014.  10  ",
1402.3337,2015,Zero-bias autoencoders and the benefits of co-adapting features,"['Zero-bias autoencoders and the benefits of co-adapting features', 'Kishore Konda', 'Roland Memisevic', 'and David Krueger']",https://arxiv.org/pdf/1402.3337,"5 1 0 2    r p A 8         ] L M  . t a t s [      5 v 7 3 3 3  .  2 0 4 1 : v i X r a  Published as a conference paper at ICLR 2015  ZERO-BIAS AUTOENCODERS AND THE BENEFITS OF CO-ADAPTING FEATURES  Kishore Konda Goethe University Frankfurt Germany konda.kishorereddy@gmail.com  Roland Memisevic University of Montreal Canada roland.memisevic@umontreal.ca  David Krueger University of Montreal Canada david.krueger@umontreal.ca  ABSTRACT  Regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. We show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. We then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. We also propose a new activation function that decouples the two roles of the hidden layer and that allows us to learn representations on data with very high intrinsic dimensionality, where standard autoencoders typi- cally fail. Since the decoupled activation function acts like an implicit regularizer, the model can be trained by minimizing the reconstruction error of training data, without requiring any additional regularization.  1  INTRODUCTION  Autoencoders are popular models used for learning features and pretraining deep networks. In their simplest form, they are based on minimizing the squared error between an observation, x, and a non-linear reconstruction deﬁned as  (cid:88)  h(cid:0)wT  (cid:1)wk + c  r(x) =  k x + bk  (1)  k  where wk and bk are weight vector and bias for hidden unit k, c is a vector of visible biases, and h(·) is a hidden unit activation function. Popular choices of activation function are the sigmoid  h(a) =(cid:0)1 + exp(−a)(cid:1)−1, or the rectiﬁed linear (ReLU) h(a) = max(0, a). Various regularization  schemes can be used to prevent trivial solutions when using a large number of hidden units. These include corrupting inputs during learning Vincent et al. (2008), adding a “contraction” penalty which forces derivatives of hidden unit activations to be small Rifai et al. (2011), or using sparsity penalties Coates et al. (2011). This work is motivated by the empirical observation that across a wide range of applications, hid- den biases, bk, tend to take on large negative values when training an autoencoder with one of the mentioned regularization schemes. In this work, we show that negative hidden unit biases are at odds with some desirable properties of the representations learned by the autoencoder. We also show that negative biases are a simple  1  Published as a conference paper at ICLR 2015  Figure 1: Left: Filters learned by a sigmoid contractive autoencoder Rifai et al. (2011) (contraction strength 1.0; left) and a ReLU denoising autoencoder Vincent et al. (2008) (zeromask-noise 0.5; right) from CIFAR-10 patches, and resulting histograms over learned hidden unit biases. Right: Classication accuracy on permutation invariant CIFAR-10 data using cAE with multiple different inference schemes. All plots in this paper are best viewed in color.  consequence of the fact that hidden units in the autoencoder have the dual function of (1) selecting which weight vectors take part in reconstructing a given training point, and (2) representing the coefﬁcients with which the selected weight vectors get combined to reconstruct the input (cf., Eq. 1). To overcome the detrimental effects of negative biases, we then propose a new activation function that allows us to disentangle these roles. We show that this yields features that increasingly outper- form regularized autoencoders in recognition tasks of increasingly high dimensionality. Since the regularization is “built” into the activation function, it allows us to train the autoencoder without ad- ditional regularization, like contraction or denoising, by simply minimizing reconstruction error. We also show that using an encoding without negative biases at test-time in both this model and a con- tractive autoencoder achieves state-of-the-art performance on the permutation-invariant CIFAR-10 dataset.1  1.1 RELATED WORK  Our analysis may help explain why in a network with linear hidden units, the optimal number of units tends to be relatively small Ba & Frey (2013); Makhzani & Frey (2013). Training via thresholding, which we introduce in Section 3, is loosely related to dropout Hinton et al. (2012), in that it forces features to align with high-density regions. In contrast to dropout, our thresholding scheme is not stochastic. Hidden activations and reconstructions are a deterministic function of the input. Other related work is the work by Goroshin & LeCun (2013) who introduce a variety of new activation functions for training autoencoders and argue for shrinking non-linearities (see also Hyv¨arinen et al. (2004)), which set small activations to zero. In contrast to that work, we show that it is possible to train autoencoders without additional regularization, when using the right type of shrinkage function. Our work is also loosely related to Martens et al. (2013) who discuss limitations of RBMs with binary observations.  2 NEGATIVE BIAS AUTOENCODERS  This work is motivated by the observation that regularized training of most common autoencoder models tends to yield hidden unit biases which are negative. Figure 1 shows an experimental demon- stration of this effect using whitened 6 × 6-CIFAR-10 color patches Krizhevsky & Hinton (2009). Negative biases and sparse hidden units have also been shown to be important for obtaining good features with an RBM Lee et al. (2008); Hinton (2010).  1An example implementation of the zero-bias autoencoder in python is available at http://www.iro.  umontreal.ca/˜memisevr/code/zae/.  2  05001000150020002500300035004000number of features4045505560657075Percentage accuracycAE Relu with biascAE RelucAE SigmoidPublished as a conference paper at ICLR 2015  2.1 NEGATIVE BIASES ARE REQUIRED FOR LEARNING AND BAD IN THE ENCODING  Negative biases are arguably important for training autoencoders, especially overcomplete ones, because they help constrain capacity and localize features. But they can have several undesirable consequences on the encoding as we shall discuss. Consider the effect of a negative bias on a hidden unit with “one-sided activation functions”, such as ReLU or sigmoid (i.e. activations which asymptote at zero for increasingly negative preactivation): On contrast-normalized data, it will act like a selection function, zeroing out the activities for points whose inner product with the weight vector wk is small. As a result, the region on the hypersphere that activates a hidden unit (ie. that yields a value that is signiﬁcantly different from 0) will be a spherical cap, whose size is determined by the size of the weight vector and the bias. When activa- tions are deﬁned by spherical caps, the model effectively deﬁnes a radial basis function network on the hypersphere. (For data that is not normalized, it will still have the effect of limiting the number of training examples for which the activation function gets active.) As long as the regions where weight vectors become active do not overlap this will be equivalent to clustering. In contrast to clustering, regions may of course overlap for the autoencoder. However, as we show in the following on the basis of an autoencoder with ReLU hidden units and negative biases, even where active regions merge, the model will resemble clustering, in that it will learn a point attractor to represent that region. In other words, the model will not be able to let multiple hidden units “collaborate” to deﬁne a multidimensional region of constant density.  2.1.1 MODES OF THE DENSITY LEARNED BY A RELU AUTOENCODER  We shall focus on autoencoders with ReLU activation function in the following. We add an approx- imate argument about sigmoid autoencoders in Section 2.1.2 below. Consider points x with r(x) = x which can be reconstructed perfectly by the autoencoder. The set of such points may be viewed as the mode of the true data generating density, or the true “manifold” the autoencoder has to ﬁnd. For an input x, deﬁne the active set (see also Razvan Pascanu (2014)) as the set of hidden units which yield a positive response: S(x) = {k : wT k x + bk > 0}. Let WS(x) denote the weight matrix restricted to the active units. That is, WS(x) contains in its columns the weight vectors associated with active hidden units for data point x. The ﬁxed point condition r(x) = x for the ReLU autoencoder can now be written  or equivalently,  WS(x)(W T  S(x)x + b) = x,  (WS(x)W T  S(x) − I)x = −WS(x)b  (2)  (3)  This is a set of inhomogeneous linear equations, whose solutions are given by a speciﬁc solution S(x) − I). The null-space is given by the eigenvectors cor- plus the null-space of M = (WS(x)W T responding to the unit eigenvalues of WS(x)W T S(x). The number of unit eigenvalues is equal to the number of orthonormal weight vectors in WS(x). S(x)x(cid:107)2, would enforce Although minimizing the reconstruction error without bias, (cid:107)x − WS(x)W T orthogonality of WS(x) for those hidden units that are active together, learning with a ﬁxed, non-zero b will not: it amounts to minimizing the reconstruction error between x and a shifted projection, S(x)x + WS(x)b(cid:107)2, for which the orthonormal solution is no longer optimal (it has (cid:107)x − WS(x)W T to account for the non-zero translation WS(x)b).  2.1.2 SIGMOID ACTIVATIONS  The case of a sigmoid activation function is harder to analyze because the sigmoid is never exactly zero, and so the notion of an active set cannot be used. But we can characterize the manifold learned by a sigmoid autoencoder (and thereby an RBM, which learns the same density model k x)+  (Kamyshanska & Memisevic (2013)) approximately using the binary activation hk(x) =(cid:0)(wT  3  Published as a conference paper at ICLR 2015  bk ≥ 0(cid:1). The reconstruction function in this case would be (cid:88) k x)+bk≥0(cid:1) wk k:(cid:0)(wT  r(x) =  which is simply the superposition of active weight vectors (and hence not a multidimensional mani- fold either).  2.1.3 ZERO-BIAS ACTIVATIONS AT TEST-TIME  This analysis suggests that even though negative biases are required to achieve sparsity, they may have a detrimental effect in that they make it difﬁcult to learn a non-trivial manifold. The observation that sparsity can be detrimental is not new, and has already been discussed, for example, in Ranzato et al. (2007); Kavukcuoglu et al. (2010), where the authors give up on sparsity at test-time and show that this improves recognition performance. Similarly, Coates et al. (2011); Saxe et al. (2011) showed that very good classiﬁcation performance can be achieved using a linear classiﬁer applied to a bag of features, using ReLU activation without bias. They also showed how this classiﬁcation scheme is robust wrt. the choice of learning method used for obtaining features (in fact, it even works with random training points as features, or using K-means as the feature learning method).2 In Figure 1 we conﬁrm this ﬁnding, and we show that it is still true when features represent whole CIFAR-10 images (rather than a bag of features). The ﬁgure shows the classiﬁcation performance of a standard contractive autoencoder with sigmoid hidden units trained on the permutation-invariant CIFAR-10 training dataset (ie. using the whole images not patches for training), using a linear classiﬁer applied to the hidden activations. It shows that much better classiﬁcation performance (in fact better than the previous state-of-the-art in the permutation invariant task) is achieved when replacing the sigmoid activations used during training with a zero-bias ReLU activation at test-time (see Section 4.3 for more details).  3 LEARNING WITH THRESHOLD-GATED ACTIVATIONS  In light of the preceding analysis, hidden units should promote sparsity during learning, by becoming active in only a small region of the input space, but once a hidden unit is active it should use a linear not afﬁne encoding. Furthermore, any sparsity-promoting process should be removed at test time. To satisfy these criteria we suggest separating the selection function, which sparsiﬁes hiddens, from the encoding, which deﬁnes the representation, and should be linear. To this end, we deﬁne the autoencoder reconstruction as the product of the selection function and a linear representation:  (cid:88)  h(cid:0)wT k x(cid:1)(cid:0)wT  k x(cid:1)wk  r(x) =  (4)  k  The selection function, h(·), may use a negative bias to achieve sparsity, but once active, a hidden unit uses a linear activation to deﬁne the coefﬁcients in the reconstruction. This activation function is reminiscent of spike-and-slab models (for example, Courville et al. (2011)), which deﬁne probability distributions over hidden variables as the product of a binary spike variable and a real-valued code. In our case, the product does not come with a probabilistic interpretation and it only serves to deﬁne a deterministic activation function which supports a linear encoding. The activation function is differentiable almost everywhere, so one can back-propagate through it for learning. The activation function is also related to adaptive dropout (Ba & Frey (2013)), which however is not differentiable and thus cannot be trained with back-prop.  3.1 THRESHOLDING LINEAR RESPONSES In this work, we propose as a speciﬁc choice for h(·) the boolean selection function  h(cid:0)wT k x(cid:1) =(cid:0)wT  k x > θ(cid:1)  (5)  2In Coates et al. (2011) the so-called “triangle activation” was used instead of a ReLU as the inference method for K-means. This amounts to setting activations below the mean activation to zero, and it is almost identical to a zero-bias ReLU since the mean linear preactivation is very close to zero on average.  4  Published as a conference paper at ICLR 2015  k x · (wT wT  k x > θ)  k x · (|wT wT  k x| > θ)  θ  wT  k x  −θ  θ  wT  k x  k x > θ)wT  to the unit’s net input, wT  k x > θ) · 1 + 0 · wT  k x = (cid:0)wT  Figure 2: Activation functions for training autoencoders: thresholded rectiﬁed (left); thresholded linear (right).  zero, it follows that the derivative of the activation function wrt.  With this choice, the overall activation function is(cid:0)wT From the product rule, and the fact that the derivative of the boolean expression(cid:0)wT (cid:0)wT  k x. It is shown in Figure 2 (left). k x > θ) is k x, is k x > θ). Unlike for ReLU, the non-differentiability of the activation function at θ is also a non-continuity. As common with ReLU activations, we train with (minibatch) stochastic gradient descent and ignore the non-differentiability during the optimization. We will refer to this activation function as Truncated Rectiﬁed (TRec) in the following. We set θ to 1.0 in most of our experiments (and all hiddens have the same threshold). While this is unlikely to be optimal, we found it to work well and often on par with, or better than, traditional regularized autoencoders like the denoising or contractive autoencoder. Truncation, in contrast to the negative- bias ReLU, can also be viewed as a hard-thresholding operator, the inversion of which is fairly well-understood Boche et al. (2013). Note that the TRec activation function is simply a peculiar activation function that we use for train- ing. So training amounts to minimizing squared error without any kind of regularization. We drop the thresholding for testing, where we use simply the rectiﬁed linear response. We also experiment with autoencoders that use a “subspace” variant of the TRec activation function (Rozell et al. (2008)), given by  (cid:88)  h(cid:0)(wT  k x)2(cid:1)(cid:0)wT  k x(cid:1)wk  r(x) =  (6)  k  It performs a linear reconstruction when the preactivation is either very large or very negative, so the active region is a subspace rather than a convex cone. To contrast it with the rectiﬁed version, we refer to this activation function as thresholded linear (TLin) below, but it is also known as hard- thresholding in the literature Rozell et al. (2008). See Figure 2 (right plot) for an illustration. Both the TRec and TLin activation functions allow hidden units to use a linear rather than afﬁne encoding. We shall refer to autoencoders with these activation functions as zero-bias autoencoder (ZAE) in the following.  3.2 PARSEVAL AUTOENCODERS  For overcomplete representations orthonormality can no longer hold. However, if the weight vectors span the data space, they form a frame (eg. Kovacevic & Chebira (2008)), so analysis weights ˜wi exist, such that an exact reconstruction can be written as  (cid:88)  (cid:0) ˜wT k x(cid:1)wk  r(x) =  (7)  k∈S(x)  5  Published as a conference paper at ICLR 2015  The vectors ˜wi and wi are in general not identical, but they are related through a matrix multipli- cation: wk = S ˜wk. The matrix S is known as frame operator for the frame {wk}k given by the weight vectors wk, and the set { ˜wk}k is the dual frame associated with S (Kovacevic & Chebira (2008)). The frame operator may be the identity in which case wk = ˜wk (which is the case in an autoencoder with tied weights.) Minimizing reconstruction error will make the frames {wk}k and { ˜wk}k approximately duals of one another, so that Eq. 7 will approximately hold. More interestingly, for an autoencoder with tied weights (wk = ˜wk), minimizing reconstruction error would let the frame approximate a Parseval = (cid:107)x(cid:107)2.  frame (Kovacevic & Chebira (2008)), such that Parseval’s identity holds(cid:80)  k x(cid:1)2 (cid:0)wT  k∈S(x)  4 EXPERIMENTS  4.1 CIFAR-10  We chose the CIFAR-10 dataset (Krizhevsky & Hinton (2009)) to study the ability of various models to learn from high dimensional input data. It contains color images of size 32 × 32 pixels that are assigned to 10 different classes. The number of samples for training is 50, 000 and for testing is 10, 000. We consider the permutation invariant recognition task where the method is unaware of the 2D spatial structure of the input. We evaluated several other models along with ours, namely contractive autoencoder, standout autoencoder (Ba & Frey (2013)) and K-means. The evaluation is based on classiﬁcation performance. The input data of size 3 × 32 × 32 is contrast normalized and dimensionally reduced us- ing PCA whitening retaining 99% variance. We also evaluated a second method of di- mensionality reduction using PCA without whitening (denoted NW below). By whitening we mean normalizing the variances, i.e., dividing each dimension by the square-root of the eigenvalues after PCA projection. The number of features for each of the model is set to 200, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000. All models are trained with stochastic gradi- ent descent. For all the experiments in this section we chose a learning rate of 0.0001 for a few (e.g. 3) initial training epochs, and then increased it to 0.001. This is to ensure that scaling issues in the initializing are dealt with at the outset, and to help avoid any blow-ups during training. Each model is trained for 1000 epochs in total with a ﬁxed momentum of 0.9. For inference, we use rectiﬁed linear units without bias for all the models. We classify the resulting representation using logistic regression with weight decay for classiﬁcation, with weight cost parameter estimated using cross-validation on a subset of the training samples of size 10000. The threshold parameter θ is ﬁxed to 1.0 for both the TRec and TLin autoencoder. For the cAE we tried the regularization strengths 1.0, 2.0, 3.0,−3.0; the latter being “uncontraction”. In the case of the Standout AE we set α = 1,β = −1. The results are reported in the plots of Figure 4. Learned ﬁlters are shown in Figure 3. From the plots in Figure 4 it is observable that the results are in line with our discussions in the earlier sections. Note, in particular that the TRec and TLin autoencoders perform well even with very few hidden units. As the number of hidden units increases, the performance of the models which tend to “tile” the input space tends to improve. In a second experiment we evaluate the impact of different input sizes on a ﬁxed number of features. For this experiment the training data is given by image patches of size P cropped from the center of each training image from the CIFAR-10 dataset. This yields for each patch size P a training set of 50000 samples and a test set of 10000 samples. The different patch sizes that we evaluated are 10, 15, 20, 25 as well as the original image size of 32. The number of features is set to 500 and 1000. The same preprocessing (whitening/no whitening) and classiﬁcation procedure as above are used to report performance. The results are shown in Figure 4. When using preprocessed input data directly for classiﬁcation, the performance increased with in- creasing patch size P , as one would expect. Figure 4 shows that for smaller patch sizes, all the models perform equally well. The performance of the TLin based model improves monotonically as the patch size is increased. All other model’s performances suffer when the patch size gets too large. Among these, the ZAE model using TRec activation suffers the least, as expected.  6  Published as a conference paper at ICLR 2015  (a) TLin AE  (b) TRec AE  (c) cAE RS=3  (d) cAE RS=-3  (e) TLin AE NW  (f) TRec AE NW  (g) cAE RS=3 NW  (h) cAE RS=-3 NW  Figure 3: Features of different models trained on CIFAR-10 data. Top: PCA with whitening as pre- processing. Bottom: PCA with no whitening as preprocessing. RS denotes regularization strength.  Figure 4: Top row: Classiﬁcation accuracy on permutation invariant CIFAR-10 data as a function of number of features. PCA with whitening (left) and without whitening (right) is used for prepro- ceesing. Bottom row: Classiﬁcation accuracy on CIFAR-10 data for 500 features (left) 1000 features (right) as a function of input patch size. PCA with whitening is used for preprocessing.  We also experimented by initializing a neural network with features from the trained models. We use a single hidden layer MLP with ReLU units where the input to hidden weights are initialized with features from the trained models and the hidden to output weights from the logistic regression mod- els (following Krizhevsky & Hinton (2009)). A hyperparameter search yielding 0.7 as the optimal threshold, along with supervised ﬁne tuning helps increase the best performance in the case of the  7  05001000150020002500300035004000number of features304050607080Percentage accuracyTRec AE NWTLin AE NWcAE NW RS=3cAE NW RS=-3K-means NWStandout AE NW05001000150020002500300035004000number of features304050607080Percentage accuracyTRec AETLin AEcAE RS=3cAE RS=-3K-meansStandout AE51015202530354045Patchsize30354045505560Percentage accuracyTRec AETLin AEcAE RS=1cAE RS=2cAE RS=3K-means51015202530354045Patchsize30354045505560Percentage accuracyTRec AETLin AEcAE RS=1cAE RS=2cAE RS=3K-meansPublished as a conference paper at ICLR 2015  TRec AE to 63.8. The same was not observed in the case of the cAE where the performance went slightly down. Thus using the TRec AE followed by supervised ﬁne-tuning with dropout regular- ization yields 64.1% accuracy and the cAE with regularization strength of 3.0 yields 63.9%. To the best of our knowledge both results beat the current state-of-the-art performance on the permutation invariant CIFAR-10 recognition task (cf., for example, Le et al. (2013)), with the TRec slightly out- performing the cAE. In both cases PCA without whitening was used as preprocessing. In contrast to Krizhevsky & Hinton (2009) we do not train on any extra data, so none of these models is provided with any knowledge of the task beyond the preprocessed training set.  4.2 VIDEO DATA  An dataset with very high intrinsic dimensionality are videos that show transforming random dots, as used in Memisevic & Hinton (2010) and subsequent work: each data example is a vectorized video, whose ﬁrst frame is a random image and whose subsequent frames show transformations of the ﬁrst frame. Each video is represented by concatenating the vectorized frames into a large vector. This data has an intrinsic dimensionality which is at least as high as the dimensionality of the ﬁrst frame. So it is very high if the ﬁrst frame is a random image. It is widely assumed that only bi-linear models, such as Memisevic & Hinton (2010) and related models, should be able to learn useful representations of this data. The interpretation of this data in terms of high intrinsic dimensionality suggests that a simple autoencoder may be able to learn reasonable features, as long as it uses a linear activation function so hidden units can span larger regions. We found that this is indeed the case by training the ZAE on rotating random dots as proposed in Memisevic & Hinton (2010). The ZAE model with 100 hiddens is trained on vectorized 10-frame random dot videos with 13 × 13 being the size of each frame. Figure 5 depicts ﬁlters learned and shows that the model learns to represent the structure in this data by developing phase-shifted rotational Fourier components as discussed in the context of bi-linear models. We were not able to learn features that were distinguishable from noise with the cAE, which is in line with existing results (eg. Memisevic & Hinton (2010)).  Model TRec AE TLin AE covAE Memisevic (2011) GRBM Taylor et al. (2010) K-means contractive AE  Average precision  50.4 49.8 43.3 46.6 41.0 45.2  Figure 5: Top: Subset of ﬁlters learned from rotating random dot movies (frame 2 on the left, frame 4 on the right). Bottom: Average precision on Hollywood2.  8  Published as a conference paper at ICLR 2015  We then chose activity recognition to perform a quantitative evaluation of this observation. The intrinsic dimensionality of real world movies is probably lower than that of random dot movies, but higher than that of still images. We used the recognition pipeline proposed in Le et al. (2011); Konda et al. (2014) and evaluated it on the Hollywood2 dataset Marszałek et al. (2009). The dataset consists of 823 training videos and 884 test videos with 12 classes of human actions. The models were trained on PCA-whitened input patches of size 10 × 16 × 16 cropped randomly from training videos. The number of training patches is 500, 000. The number of features is set to 600 for all models. In the recognition pipeline, sub blocks of the same size as the patch size are cropped from 14×20×20 super-blocks, using a stride of 4. Each super block results in 8 sub blocks. The concatenation of sub block ﬁlter responses is dimensionally reduced by performing PCA to get a super block descriptor, on which a second layer of K-means learns a vocabulary of spatio-temporal words, that get classiﬁed with an SVM (for details, see Le et al. (2011); Konda et al. (2014)). In our experiments we plug the features learned with the different models into this pipeline. The performances of the models are reported in Figure 5 (right). They show that the TRec and TLin autoencoders clearly outperform the more localized models. Surprisingly, they also outperform more sophisticated gating models, such as Memisevic & Hinton (2010).  4.3 RECTIFIED LINEAR INFERENCE  In previous sections we discussed the importance of (unbiased) rectiﬁed linear inference. Here we experimentally show that using rectiﬁed linear inference yields the best performance among different inference schemes. We use a cAE model with a ﬁxed number of hiddens trained on CIFAR-10 images, and evaluate the performance of  1. Rectiﬁed linear inference with bias (the natural preactivation for the unit): [W T X + b]+ 2. Rectiﬁed linear inference without bias: [W T X]+ 3. natural inference: sigmoid(W T X + b)  The performances are shown in Figure 1 (right), conﬁrming and extending the results presented in Coates et al. (2011); Saxe et al. (2011).  5 DISCUSSION  Quantizing the input space with tiles proportional in quantity to the data density is arguably the best way to represent data given enough training data and enough tiles, because it allows us to approx- imate any function reasonably well using only a subsequent linear layer. However, for data with high intrinsic dimensionality and a limited number of hidden units, we have no other choice than to summarize regions using responses that are invariant to some changes in the input. Invariance, from this perspective, is a necessary evil and not a goal in itself. But it is increasingly important for increasingly high dimensional inputs. We showed that linear not afﬁne hidden responses allow us to get invariance, because the density deﬁned by a linear autoencoder is a superposition of (possibly very large) regions or subspaces. After a selection is made as to which hidden units are active for a given data example, linear coef- ﬁcients are used in the reconstruction. This is very similar to the way in which gating and square pooling models (eg., Olshausen et al. (2007); Memisevic & Hinton (2007; 2010); Ranzato et al. (2010); Le et al. (2011); Courville et al. (2011)) deﬁne their reconstruction: The response of a hid- den unit in these models is deﬁned by multiplying the ﬁlter response or squaring it, followed by a non-linearity. To reconstruct the input, the output of the hidden unit is then multiplied by the ﬁlter response itself, making the model bi-linear. As a result, reconstructions are deﬁned as the sum of feature vectors, weighted by linear coefﬁcients of the active hiddens. This may suggest interpreting the fact that these models work well on videos and other high-dimensional data as a result of using linear, zero-bias hidden units, too.  9  Published as a conference paper at ICLR 2015  ACKNOWLEDGMENTS  This work was supported by an NSERC Discovery grant, a Google faculty research award, and the German Federal Ministry of Education and Research (BMBF) in the project 01GQ0841 (BFNT Frankfurt).  REFERENCES Ba, Jimmy and Frey, Brendan. Adaptive dropout for training deep neural networks. In Advances in Neural  Information Processing Systems, pp. 3084–3092, 2013.  Boche, Holger, Guillemard, Mijail, Kutyniok, Gitta, and Philipp, Friedrich. Signal recovery from thresholded  frame measurements. In SPIE 8858, Wavelets and Sparsity XV, August 2013.  Coates, Adam, Lee, Honglak, and Ng, A. Y. An analysis of single-layer networks in unsupervised feature  learning. In Artiﬁcial Intelligence and Statistics, 2011.  Courville, Aaron C, Bergstra, James, and Bengio, Yoshua. A spike and slab restricted boltzmann machine. In  International Conference on Artiﬁcial Intelligence and Statistics, pp. 233–241, 2011.  Goroshin, Rotislav and LeCun, Yann. Saturating auto-encoders.  Representations (ICLR2013), April 2013.  In International Conference on Learning  Hinton, Geoffrey. A Practical Guide to Training Restricted Boltzmann Machines. Technical report, University  of Toronto, 2010.  Hinton, Geoffrey E., Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Improv-  ing neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.  Hyv¨arinen, Aapo, Karhunen, Juha, and Oja, Erkki. Independent component analysis, volume 46. John Wiley  & Sons, 2004.  Kamyshanska, Hanna and Memisevic, Roland. On autoencoder scoring. In Proceedings of the 30th Interna-  tional Conference on Machine Learning (ICML 2013), 2013.  Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. Fast inference in sparse coding algorithms  with applications to object recognition. arXiv preprint arXiv:1010.3467, 2010.  Konda, Kishore Reddy, Memisevic, Roland, and Michalski, Vincent. The role of spatio-temporal synchrony in  the encoding of motion. In International Conference on Learning Representations (ICLR2014), 2014.  Kovacevic, J. and Chebira, A. An Introduction to Frames. Foundations and trends in signal processing. Now  Publishers, 2008.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images. Master’s thesis,  Department of Computer Science, University of Toronto, 2009.  Le, Quoc, Sarlos, Tamas, and Smola, Alex. Fastfood - approximating kernel expansions in loglinear time. In  30th International Conference on Machine Learning (ICML), 2013.  Le, Q.V., Zou, W.Y., Yeung, S.Y., and Ng, A.Y. Learning hierarchical invariant spatio-temporal features for  action recognition with independent subspace analysis. In CVPR, 2011.  Lee, Honglak, Ekanadham, Chaitanya, and Ng, Andrew. Sparse deep belief net model for visual area v2. In  Advances in Neural Information Processing Systems 20, 2008.  Makhzani, Alireza and Frey, Brendan. k-sparse autoencoders. CoRR, abs/1312.5663, 2013.  Marszałek, Marcin, Laptev, Ivan, and Schmid, Cordelia. Actions in context. In IEEE Conference on Computer  Vision & Pattern Recognition, 2009.  Martens, James, Chattopadhyay, Arkadev, Pitassi, Toniann, and Zemel, Richard. On the representational efﬁ-  ciency of restricted boltzmann machines. In Neural Information Processing Systems (NIPS) 2013, 2013.  Memisevic, Roland. Gradient-based learning of higher-order image features. In ICCV, 2011.  Memisevic, Roland and Hinton, Geoffrey. Unsupervised learning of image transformations. In CVPR, 2007.  10  Published as a conference paper at ICLR 2015  Memisevic, Roland and Hinton, Geoffrey E. Learning to represent spatial transformations with factored higher-  order boltzmann machines. Neural Computation, 22(6):1473–1492, June 2010. ISSN 0899-7667.  Olshausen, Bruno, Cadieu, Charles, Culpepper, Jack, and Warland, David. Bilinear models of natural images.  In SPIE Proceedings: Human Vision Electronic Imaging XII, San Jose, 2007.  Ranzato, M, Huang, Fu Jie, Boureau, Y-L, and LeCun, Yann. Unsupervised learning of invariant feature In Computer Vision and Pattern Recognition, 2007.  hierarchies with applications to object recognition. CVPR’07. IEEE Conference on, pp. 1–8. IEEE, 2007.  Ranzato, Marc’Aurelio, Krizhevsky, Alex, and Hinton, Geoffrey E. Factored 3-Way Restricted Boltzmann  Machines For Modeling Natural Images. In Artiﬁcial Intelligence and Statistics, 2010.  Razvan Pascanu, Guido Montufar, Yoshua Bengio. On the number of inference regions of deep feed forward  networks with piece-wise linear activations. CoRR, arXiv:1312.6098, 2014.  Rifai, Salah, Vincent, Pascal, Muller, Xavier, Glorot, Xavier, and Bengio, Yoshua. Contractive Auto-Encoders:  Explicit Invariance During Feature Extraction. In ICML, 2011.  Rozell, Christopher J, Johnson, Don H, Baraniuk, Richard G, and Olshausen, Bruno A. Sparse coding via  thresholding and local competition in neural circuits. Neural computation, 20(10):2526–2563, 2008.  Saxe, Andrew, Koh, Pang Wei, Chen, Zhenghao, Bhand, Maneesh, Suresh, Bipin, and Ng, Andrew. On random weights and unsupervised feature learning. In Proceedings of the 28th International Conference on Machine Learning, 2011.  Taylor, Graham W., Fergus, Rob, LeCun, Yann, and Bregler, Christoph. Convolutional learning of spatio- temporal features. In Proceedings of the 11th European conference on Computer vision: Part VI, ECCV’10, 2010.  Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, 2008.  11  ",
1412.6598,2015,Automatic Discovery and Optimization of Parts for Image Classification,"['Automatic Discovery and Optimization of Parts for Image Classification', 'Sobhan Naderi Parizi', 'Andrea Vedaldi', 'Andrew Zisserman', 'and Pedro Felzenszwalb']",https://arxiv.org/pdf/1412.6598,"Published as a conference paper at ICLR 2015  AUTOMATIC DISCOVERY AND OPTIMIZATION OF PARTS FOR IMAGE CLASSIFICATION  Sobhan Naderi Parizi Andrea Vedaldi Andrew Zisserman Pedro Felzenszwalb Brown University pff@brown.edu  {vedaldi, az}@robots.ox.ac.uk  Brown University sobhan@brown.edu  University of Oxford  5 1 0 2    r p A 1 1         ]  V C . s c [      2 v 8 9 5 6  .  2 1 4 1 : v i X r a  ABSTRACT  Part-based representations have been shown to be very useful for image classiﬁ- cation. Learning part-based models is often viewed as a two-stage problem. First, a collection of informative parts is discovered, using heuristics that promote part distinctiveness and diversity, and then classiﬁers are trained on the vector of part responses. In this paper we unify the two stages and learn the image classiﬁers and a set of shared parts jointly. We generate an initial pool of parts by randomly sampling part candidates and selecting a good subset using (cid:96)1/(cid:96)2 regularization. All steps are driven directly by the same objective namely the classiﬁcation loss on a training set. This lets us do away with engineered heuristics. We also introduce the notion of negative parts, intended as parts that are negatively correlated with one or more classes. Negative parts are complementary to the parts discovered by other methods, which look only for positive correlations.  1  INTRODUCTION  Computer vision makes abundant use of the concept of “part”. There are at least three key reasons why parts are useful for representing objects or scenes. One reason is the existence of non-linear and non-invertible nuisance factors in the generation of images, including occlusions. By breaking an object or image into parts, at least some of these may be visible and recognizable. A second reason is that parts can be recombined in a model to express a combinatorial number of variants of an object or scene. For example parts corresponding to objects (e.g. a laundromat and a desk) can be rearranged in a scene, and parts of objects (e.g. the face and the clothes of a person) can be replaced by other parts. A third reason is that parts are often distinctive of a particular (sub)category of objects (e.g. cat faces usually belong to cats). Discovering good parts is a difﬁcult problem that has recently raised considerable interest (Juneja et al. (2013); Doersch et al. (2013); Sun & Ponce (2013)). The quality of a part can be deﬁned in different ways. Methods such as (Juneja et al. (2013); Doersch et al. (2013)) decouple learning parts and image classiﬁers by optimizing an intermediate objective that is only heuristically related to classiﬁcation. Our ﬁrst contribution is to learn instead a system of discriminative parts jointly with the image classiﬁers, optimizing the overall classiﬁcation performance on a training set. We propose a uniﬁed framework for training all of the model parameters jointly (Section 3). We show that joint training can substantially improve the quality of the models (Section 5).  Figure 1: Part ﬁlters before (left) and after joint training (right) and top scoring detections for each.  1  Published as a conference paper at ICLR 2015  Random Part Initialization 1. Extract feature from a patch  at random image and location.  2. Whiten the feature. 3. Repeat to construct a pool of  candidate parts.  Part Selection  Joint Training  1. Train part weights u with  (cid:96)1/(cid:96)2 regularization.  2. Discard parts that are not used  according to u.  1. Train part weights u keeping  part ﬁlters w ﬁxed.  2. Train part ﬁlters w keeping  part weights u ﬁxed.  3. Repeat until convergence.  Figure 2: Our pipeline. Part selection and joint training are driven by classiﬁcation loss. Part selection is important because joint training is computationally demanding.  A fundamental challenge in part learning is a classical chicken-and-egg problem: without an ap- pearance model, examples of a part cannot be found, and without having examples an appearance model cannot be learned. To address this methods such as (Juneja et al. (2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between ﬁnding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classiﬁers using (cid:96)1/(cid:96)2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts. This translates directly to test time speedup. We present experiments with both HOG (Dalal & Triggs (2005)) and CNN (Krizhevsky et al. (2012)) features and improve the state-of-the-art results on the MIT-indoor dataset (Quattoni & Torralba (2009)) using CNN features. A ﬁnal contribution of our paper is the introduction of the concept of negative parts, i.e. parts that are negatively correlated with respect to a class (Section 2). These parts are still informative as “counter-evidence” for the class. In certain formulations, negative parts are associated to negative weights in the model and in others with negative weight differences.  1.1 RELATED WORK  Related ideas in part learning have been recently explored in (Singh et al. (2012); Juneja et al. (2013); Sun & Ponce (2013); Doersch et al. (2013)). The general pipeline in all of these approaches is a two-stage procedure that involves pre-training a set of discriminative parts followed by training a classiﬁer on top of the vector of the part responses. The differences in these methods lay in the details of how parts are discovered. Each approach uses a different heuristic to ﬁnd a collection of parts such that each part scores high on a subset of categories (and therefore is discriminative) and, collectively, they cover a large area of an image after max-pooling (and therefore are descriptive). Our goal is similar, but we achieve part diversity, distinctiveness, and coverage as natural byproducts of optimizing the “correct” objective function, i.e. the ﬁnal image classiﬁcation performance. Reconﬁgurable Bag of Words (RBoW) model Naderi et al. (2012) is another part-based model used for image classiﬁcation. RBoW uses latent variables to deﬁne a mapping from image regions to part models. In contrast, the latent variables in our model deﬁne a mapping from parts to image regions. It has been shown before (Girshick & Malik (2013)) that joint training is important for the success of part-based models in object detection. Differently from them, however, we share parts among multiple classes and deﬁne a joint optimization in which multiple classiﬁers are learned concurrently. In particular, the same part can vote strongly for a subset of the classes and against another subset. The most closely related work to ours is (Lobel et al. (2013)). Their model has two sets of param- eters; a dictionary of visual words θ and a set of weights u that speciﬁes the importance the visual words in each category. Similar to what we do here, Lobel et al. (2013) trains u and θ jointly (visual words would be the equivalent of part ﬁlters in our terminology). However, they assume that u is non-negative. This assumption does not allow for “negative parts” as we describe in Section 2.  2  Published as a conference paper at ICLR 2015  The concept of negative parts and relative attributes (Parikh & Grauman (2011)) are related in that both quantify the relative strength of visual patterns. Our parts are trained jointly using using im- age category labels as the only form of suppervision, whereas the relative attributes in (Parikh & Grauman (2011)) were trained independently using labeled information about the strength of hand picked attributes in training images.  2 PART-BASED MODELS AND NEGATIVE PARTS  We model an image class using a collection of parts. A part may capture the appearance of an entire object (e.g. bed in a bedroom scene), a part of an object (e.g. drum in the laundromat scene), a rigid composition of multiple objects (e.g. rack of clothes in a closet scene), or a region type (e.g. ocean in a beach scene). Let x be an image. We use H(x) to denote the space of latent values for a part. In our experiments H(x) is a set of positions and scales in a scale pyramid. To test if the image x contains part j at location zj ∈ H(x), we extract features ψ(x, zj) and take the dot product of this feature vector with a part ﬁlter wj. Let s(x, zj, wj) denote the response of part j at location zj in x. Since the location of a part is unknown, it is treated as a latent variable which is maximized over. This deﬁnes the response r(x, wj) of a part in an image,  s(x, zj, wj) = wj · ψ(x, zj),  r(x, wj) = max zj∈H(x)  s(x, zj, wj).  (1)  Given a collection of m parts w = (w1, . . . , wm), their responses are collected in an m-dimensional vector of part responses r(x, w) = (r(x, w1); . . . ; r(x, wm)). In practice, ﬁlter responses are pooled within several distinct spatial subdivisions (Lazebnik et al. (2006)) to encode weak geometry. In this case we have R pooling regions and r(x, w) is an mR-dimensional vector maximizing part responses in each pooling region. For the rest of the paper we assume a single pooling region to simplify notation. Part responses can be used to predict the class of an image. For example, high response for “bed” and “lamp” would suggest the image is of a “bedroom” scene. Binary classiﬁers are often used for multi-class classiﬁcation with a one-vs-all setup. DPMs (Felzenszwalb et al. (2010)) also use binary classiﬁers to detect objects of each class. For a binary classiﬁer we can deﬁne a score function fβ(x) for the foreground hypothesis. The score combines part responses using a vector of part weights u, (2)  fβ(x) = u · r(x, w),  The binary classiﬁer predicts y = +1 if fβ(x) ≥ 0, and y = −1 otherwise. Negative parts in a binary classiﬁer: If uj > 0 we say part j is a positive part for the foreground class and if uj < 0 we say part j is a negative part for the foreground class. Intuitively, a negative part provides counter-evidence for the foureground class; i.e. r(x, wj) is negatively correlated with fβ(x). For example, since cows are not usually in bedrooms a high response from a cow ﬁlter should penalize the score of a bedroom classiﬁer. Let β = (u, w) be the parameters of a binary classiﬁer. We can multiply wj and divide uj by a positive value α to obtain an equivalent model. If we use α = |uj| we obtain a model where u ∈ {−1, +1}m. However, in general it is not possible to transform a model where uj is negative into a model where uj is positive because of the max in (1). We note that, when u is non-negative the score function fβ(x) is convex in w. On the other hand, if there are negative parts, fβ(x) is no longer convex in w. If u is non-negative then (2) reduces to the scoring function of a latent SVM, and a special case of a DPM. By the argument above when u is non-negative we can assume u = 1 and (2) reduces to  β = (u, w).  fβ(x) =  max zj∈H(x)  wj · ψ(x, zj) = max z∈Z(x)  w · Ψ(x, z),  (3)  m(cid:88)  j=1  where Z(x) = H(x)m, and Ψ(x, z) = (ψ(x, z1); . . . ; ψ(x, zm)). In the case of a DPM, the feature vector Ψ(x, z) and the model parameters contain additional terms capturing spatial relationships between parts. In a DPM all part responses are positively correlated with the score of a detection. Therefore DPMs do not use negative parts.  3  Published as a conference paper at ICLR 2015  2.1 NEGATIVE PARTS IN MULTI-CLASS SETTING  In the previous section we showed that certain one-vs-all part-based classiﬁers, including DPMs, cannot capture counter-evidence from negative parts. This limitation can be addressed by using more general models with two sets of parameters β = (u, w) and a scoring function fβ(x) = u · r(x, w), as long as we allow u to have negative entries. Now we consider the case of a multi-class classiﬁer where part responses are weighted differently for each category but all categories share the same set of part ﬁlters. A natural consequence of part sharing is that a positive part for one class can be used as a negative part for another class. Let Y = {1, . . . , n} be a set of n categories. A multi-class part-based model β = (w, u) is deﬁned by m part ﬁlters w = (w1, . . . , wm) and n vectors of part weights u = (u1, . . . , un) with uy ∈ Rm. The shared ﬁlters w and the weight vector uy deﬁne parameters βy = (w, uy) for a scoring function for class y. For an input x the multi-class classiﬁer selects the class with highest score  ˆyβ(x) = arg max  y∈Y  fβy (x) = arg max  y∈Y  uy · r(x, w)  (4)  We can see u as an n×m matrix. Adding a constant to a column of u does not change the differences between scores of two classes fβa (x) − fβb (x). This implies the function ˆy is invariant to such transformations. We can use a series of such transformations to make all entries in u non-negative, without changing the classiﬁer. Thus, in a multi-class part-based model, unlike the binary case, it is not a signiﬁcant restriction to require the entries in u to be non-negative. In particular the sign of an entry in uy does not determine the type of a part (positive or negative) for class y. Negative parts in a multi-class classiﬁer: If ua,j > ub,j we say part j is a positive part for class a relative to b. If ua,j < ub,j we say part j is a negative part for class a relative to b. Although adding a constant to a column of u does not affect ˆy, it does impact the norms of the part weight vectors uy. For an (cid:96)2 regularized model the columns of u will sum to zero. Otherwise we can subtract the column sums from each column of u to decrease the (cid:96)2 regularization cost without changing ˆy and therefore the classiﬁcation loss. We see that in the multi-class part-based model constrainig u to have non-negative entries only affects the regularization of the model.  3  JOINT TRAINING  In this section we propose an approach for joint training of all parameters β = (w, u) of a multi- class part-based model. Training is driven directly by classiﬁcation loss. Note that a classiﬁcation loss objective is sufﬁcient to encourage diversity of parts. In particular joint training encourages part ﬁlters to complement each other. We have found that joint training leads to a substantial improve- ment in performance (see Section 5). The use of classiﬁcation loss to train all model parameters also leads to a simple framework that does not rely on multiple heuristics. Let D = {(xi, yi)}k i=1 denote a training set of labeled examples. We train β using (cid:96)2 regularization for both the part ﬁlters w and the part weights u (we think of each as a single vector) and the multi-class hinge loss, resulting in the objective function:  O(u, w) = λw||w||2 + λu||u||2 +  = λw||w||2 + λu||u||2 +  (cid:26) (cid:26)  k(cid:88) k(cid:88)  i=1  i=1  max  0, 1 + (max y(cid:54)=yi  max  0, 1 + max y(cid:54)=yi  (cid:27) uy · r(xi, w)) − uyi · r(xi, w)  (cid:27) (uy − uyi) · r(xi, w)  (5)  (6)  We use block coordinate descent for training, as summarized in Algorithm 1. This alternates between (Step 1) optimizing u while w is ﬁxed and (Step 2) optimizing w while u is ﬁxed. The ﬁrst step reduces to a convex structural SVM problem (line 3 of Algorithm 1). If u is non-negative the second step could be reduced to a latent structural SVM problem deﬁned by (5). We use a novel approach that allows u to be negative (lines 4-7 of Algorithm 1) described below.  4  Published as a conference paper at ICLR 2015  u := arg minu(cid:48) O(u(cid:48), w) (deﬁned in Equation 6) repeat  Algorithm 1 Joint training of model parameters by optimizing O(u, w) in Equation 6. 1: initialize the part ﬁlters w = (w1, . . . , wm) 2: repeat 3: 4: 5: 6: 7: 8: until convergence 9: output β = (u, w)  wold := w w := arg minw(cid:48) Bu(w(cid:48), wold) (deﬁned in Equation 7)  until convergence  STEP 1: LEARNING PART WEIGHTS (LINE 3 OF ALGORITHM 1) This involves computing arg minu(cid:48) O(u(cid:48), w). Since w is ﬁxed λw||w||2 and r(xi, w) are constant. This makes the optimization problem equivalent to training a multi-class SVM where the i-th train- ing example is represented by an m-dimensional vector of part responses r(xi, w). This is a convex problem that can be solved using standard methods.  STEP 2: LEARNING PART FILTERS (LINES 4-7 OF ALGORITHM 1) This involves computing arg minw(cid:48) O(u, w(cid:48)). Since u is ﬁxed λu||u||2 is constant. While r(xi, wj) is convex in w (it is a maximum of linear functions) the coefﬁcients uy,j − uyi,j may be negative. This makes the objective function (6) non-convex. Lines 4-7 of Algorithm 1 implement the CCCP algorithm (Yuille & Rangarajan (2003)). In each iteration we construct a convex bound using the previous estimate of w and update w to be the minimizer of the bound. Let s(x, z, w) = (s(x, z1, w1); . . . ; s(x, zm, wm)) to be the vector of part responses in image x when the latent variables are ﬁxed to z = (z1, . . . , zm). We construct a convex upper bound on O by replacing r(xi, wj) with s(xi, zi,j, wj) in (6) when uy,j − uyi,j < 0. We make the bound tight for the last estimate of the part ﬁlters wold by selecting zi,j = arg maxzj∈H(xi) s(xi, zj, wold j ). Then a convex upper bound that touches O at wold is given by λu||u||2 + Bu(w, wold) with  (uy−uyi )·(cid:2)Sy,yir(xi, w)+ ¯Sy,yis(xi, zi, w)(cid:3)(cid:27)  (7)  Bu(w, wold) = λw||w||2+  (cid:26)  max  0, 1+max y(cid:54)=yi  k(cid:88)  i=1  Here, for a pair of categories (y, y(cid:48)), Sy,y(cid:48) and ¯Sy,y(cid:48) are m × m diagonal 0-1 matrices such that ¯Sy,y(cid:48)(j, j) = 1 − Sy,y(cid:48)(j, j) and Sy,y(cid:48)(j, j) = 1 if and only if uy,j − uy(cid:48),j ≥ 0. The matrices S and ¯S select r(xi, wj) when uy,j −uyi,j ≥ 0 and s(xi, zi,j, wj) when uy,j −uyi,j < 0. This implements the convex upper-bound outlined above. Line 6 of the algorithm updates the part ﬁlters by minimizing Bu(w, wold). Optimizing this function requires signiﬁcant computational and memory resources. In the supplementary material (Section A) we give details of how our optimization method works in practice.  4 PART GENERATION AND SELECTION  The joint training objective in (6) is non-convex making Algorithm 1 sensitive to initialization. Thus, the choice of initial parts can be crucial in training models that perform well in practice. We devote the ﬁrst two steps of our pipeline to ﬁnding good initial parts (Figure 2). We then use those parts to initialize the joint training procedure of Section 3. In the ﬁrst step of our pipeline we randomly generate a large pool of initial parts. Generating a part involves picking a random training image (regardless of the image category labels) and extracting features from a random subwindow of the image followed by whitening (Hariharan et al. (2012)). To whiten a feature vector f we use Σ−1(f − µ) where µ and Σ are the mean and covariance of all patches in all training images. We estimate µ and Σ from 300,000 random patches. We use the norm of the whitened features to estimate discriminability of a patch. Patches with large whitened feature norm are farther from the mean of the background distribution in the whitened space and,  5  Published as a conference paper at ICLR 2015  hence, are expected to be more discriminative. Similar to (Aubry et al. (2013)) we discard the 50% least discriminant patches from each image prior to generating random parts. Our experimental results with HOG features (Figure 3) show that randomly generated parts using the procedure described here perform better than or comparable to previous methods that are much more involved (Juneja et al. (2013); Doersch et al. (2013); Sun & Ponce (2013)). When using CNN features we get very good results using random parts alone, even before part-selection and training of the part ﬁlters (Figure 4). Random part generation may produce redundant or useless parts. In the second step of our pipeline we train image classiﬁers u using (cid:96)1/(cid:96)2 regularization (a.k.a. group lasso) to select a subset of parts from the initial random pool. We group entries in each column of u. Let ρj denote the (cid:96)2-norm of  the j-th column of u. The (cid:96)1/(cid:96)2 regularization is deﬁned by Rg(u) = λ(cid:80)m  j=1 ρj.  If part j is not uninformative or redundant ρj (and therefore all entries in the j-th column of u) will be driven to zero by the regularizer. We train models using different values for λ to generate a target number of parts. The number of selected parts decreases monotonically as λ increases. Figure 8 in the supplement shows this. We found it important to retrain u after part selection using (cid:96)2 regularization to obtain good classiﬁcation performance.  5 EXPERIMENTS  We evaluate our methods on the MIT-indoor dataset (Quattoni & Torralba (2009)). We compare performance of models with randomly generated parts, selected parts, and jointly trained parts. We also compare performance of HOG and CNN features. The dataset has 67 indoor scene classes. There are about 80 training and 20 test images per class. Recent part-based methods that do well on this dataset (Juneja et al. (2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400). HOG features: We resize images (maintaining aspect ratio) to have about 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs (2005); Felzenszwalb et al. (2010)) at multiple scales. Our HOG pyramid has 3 scales per octave. This yields about 11,000 patches per image. Each part ﬁlter wj models a 6×6 grid of HOG features, so wj and ψ(x, zj) are both 1152-dimensional. CNN features: We extract CNN features at multiple scales from overlapping patches of ﬁxed size 256×256 and with stride value 256/3 = 85. We resize images (maintaining aspect ratio) to have about 5M pixels in the largest scale. We use a scale pyramid with 2 scales per octave. This yields about 1200 patches per image. We extract CNN features using Caffe (Jia et al. (2014)) and the hybrid neural network from (Zhou et al. (2014)). The hybrid network is pre-trained on images from ImageNet (Deng et al. (2009)) and PLACES (Zhou et al. (2014)) datasets. We use the output of the 4096 units in the penultimate fully connected layer of the network (fc7). We denote these features by HP in our plots. Part-based representation: Our ﬁnal image representation is an mR-dimensional vector of part responses where m is the number of shared parts and R is the number of spatial pooling regions. We use R = 5 pooling regions arranged in a 1×1 + 2×2 grid. To make the ﬁnal representation invariant to horizontal image ﬂips we average the mR-dimensional vector of part responses for image x and its right-to-left mirror image x(cid:48) to get [r(x, w) + r(x(cid:48), w)] /2 as in (Doersch et al. (2013)). We ﬁrst evaluate the performance of random parts. Given a pool of randomly initialized parts (Sec- tion 4), we train the part weights u using a standard (cid:96)2-regularized linear SVM; we then repeat the experiment by selecting few parts from a large pool using (cid:96)1/(cid:96)2 regularization (Section 4). Finally, we evaluate joint training (Section 3). While joint training signiﬁcantly improves performance, it comes at a signiﬁcantly increased computational cost. Figure 3 shows performance of HOG features on the MIT-indoor dataset. Because of the high dimensionality of the HOG features and the large space of potential placements in a HOG pyramid we consider a 10-class subset of the dataset for experiments with a large number of parts using HOG features. The subset comprises bookstore, bowling, closet, corridor, laundromat, library, nursery, shoeshop, staircase, and winecellar. Performance of random parts increases as we use more parts. Flip invariance and part selection consistently improve results. Joint training improves the performance even further by a large margin achieving the same level of performance as the  6  Published as a conference paper at ICLR 2015  Figure 3: Performance of HOG features on 10-class subset (left) and full MIT-indoor dataset (right).  selected parts using much fewer parts. On the full dataset, random parts already outperform the results from Juneja et al. (2013), ﬂip invariance boosts the performance beyond Sun & Ponce (2013). Joint training dominates other methods. However, we could not directly compare with the best performance of Doersch et al. (2013) due to the very large number of parts they use. Figure 4 shows performance of CNN features on MIT-indoor dataset. As a baseline we extract CNN features from the entire image (after resizing to 256×256 pixels) and train a multi-class linear SVM. This obtains 72.3% average performance. This is a strong baseline. Razavian et al. (2014) get 58.4% using CNN trained on ImageNet. They improve the result to 69% after data augmentation. We applied PCA on the 4096 dimensional features to make them more compact. This is essential for making the joint training tractable both in terms of running time and memory footprint. Figure 4-left shows the effect of PCA dimensionality reduction. It is surprising that we lose only 1% in accuracy with 160 PCA coefﬁcients and only 3.5% with 60 PCA coefﬁcients. We also show how performance changes when a random subset of dimensions is used. For joint training we use 60 PCA coefﬁcients. Figure 4-right shows performance of our part-based models using CNN features. For comparison with HOG features we also plot result of Doersch et al. (2013). Note that part-based representation improves over the CNN extracted on the entire image. With 13400 random parts we get 77.1% (vs 72.3% for CNN on the entire image). The improvement is from 68.2% to 72.4% when we use 60 PCA coefﬁcients. Interestingly, the 60 PCA coefﬁcients perform better than the full CNN features when only a few parts are used (up to 1000). The gap increases as the number of parts decreases. We do part selection and joint training using 60 PCA coefﬁcients of the CNN features. We select parts from an initial pool of 1000 random parts. Part selection is most effective when a few parts are used. Joint training improves the quality of the selected parts. With only 372 jointly trained parts we obtain 73.3% classiﬁcation performance which is even better than 13400 random parts (72.4%). The signiﬁcance of our results is two fold: 1) we demonstrate a very simple and fast to train pipeline for image classiﬁcation using randomly generated parts; 2) we show that using part selection and joint training we can obtain similar or higher performance using much fewer parts. The gain is largest for CNN features (13400/372 ≈ 36 times). This translates to 36x speed up in test time. See Section D of the supplement for detailed run-time analysis of our method.  5.1 VISUALIZATION OF THE MODEL  Figure 5 shows the part weight matrix after joint training a model with 52 parts on the full MIT- indoor dataset. This model uses 60 PCA coefﬁcients from the HP CNN features. Figure 6 shows top scoring patches for a few parts before and after joint training. The parts correspond to the model illustrated in Figure 5. The beneﬁt of joint training is clear. The part detections are more consistent and “clean” after joint training. The majority of the detections of part 25 before joint training are seats. Joint training ﬁlters out most of the noise (mostly coming from bed and sofa) in this part. Part 46 consistently ﬁres on faces even before joint training. After joint training, however, the part becomes more selective to a single face and the detections become more localized. Figure 7 illustrates selectivity of a few parts. Each row shows the highest scoring detections of a par- ticular part on test images. The part indices in the ﬁrst column match those of Figure 5. Even though  7  10110210335404550556065707580# partsPerformance %  Random parts (no flipping)Random parts (flip invariant)Selected parts (from 10K)Jointly trained parts1021031042030405060# partsPerformance %  Random parts (no flipping)Random parts (flip invariant)Selected parts (from 5K)Jointly trained partsJuneja et al.Doersch et al.Sun et al.Published as a conference paper at ICLR 2015  Figure 4: Performance of CNN features on the full MIT-indoor dataset. HP denotes the hybrid features from Zhou et al. (2014). Left: the effect of dimensionality reduction on performance of the CNN features extracted from the entire image. Two approaches are compared; random selection over 5 trials (blue curve) and PCA (red curve). Right: part-based models with random parts (blue curves), selected parts from 1K random parts (red curve), and jointly trained parts (black curve).  Figure 5: Part weights after joint training a model with 52 parts on the full dataset. Here patches are represented using 60 PCA coefﬁcients on CNN features. Although the model uses 5 pooling regions (corresponding to cells in 1×1 + 2×2 grids) here we show the part weights only for the ﬁrst pooling region corresponding to the entire image.  most detections look consistently similar the images usually belong to multiple classes demonstrat- ing part sharing across categories. For example, while part 17 appears to capture bed the images belong to hospitalroom, childrensroom, and bedroom classes. While part 25 appears to capture seats the images belong to waitingroom, library, auditorium, and insidebus. Conversely, multiple parts may capture the same semantic concept. For example, parts 3, 16, and 35 appear to capture shelves but they seem to be tuned speciﬁcally to shelves in pantry, store, and book-shelves respectively. Some parts respond to a part of an object; e.g. part 40 and 46 respond to leg and face. Others ﬁnd entire objects or even composition of multiple objects. For example, parts 6, 17, 37, 43 detect laundromats, beds, cabinets, and monitor. Part 29 detects composition of seats-and-screen.  8  102103203040506070Feature dimension (log scale)Performance %  HP random dimensionsHP PCA coefficientsHP full10110210310420304050607080# partsPerformance %  Doersch et al.Random parts on HP fullRandom parts on HP PCA 60Selected parts on HP PCA (from 1K)Jointly trained parts on HP PCA 60  246810121416182022242628303234363840424446485052airport_insideartstudioauditoriumbakerybarbathroombedroombookstorebowlingbuffetcasinochildren_roomchurch_insideclassroomcloisterclosetclothingstorecomputerroomconcert_hallcorridordelidentalofficedining_roomelevatorfastfood_restaurantfloristgameroomgaragegreenhousegrocerystoregymhairsalonhospitalroominside_businside_subwayjewelleryshopkindergardenkitchenlaboratorywetlaundromatlibrarylivingroomlobbylocker_roommallmeeting_roommovietheatermuseumnurseryofficeoperating_roompantrypoolinsideprisoncellrestaurantrestaurant_kitchenshoeshopstairscasestudiomusicsubwaytoystoretrainstationtv_studiovideostorewaitingroomwarehousewinecellar−0.4−0.200.20.40.6Published as a conference paper at ICLR 2015  Top scoring patches on test images (multiple patches per image)  e r o f e B  5 2 t r a P  r e t f  A  e r o f e B  6 4 t r a P  r e t f  A  Figure 6: Top detections of two parts are shown before and after joint training on test images of the full MIT-indoor dataset. The numbers in the ﬁrst column match the part indices in Figure 5.  The part weight matrix u (Figure 5) helps us better understand how parts assists classiﬁcation. Part 6 has signiﬁcantly high weight for class laundromat and it appears to be a good detector thereof. Part 27 ﬁres strongly on game/sports-related scenes. The weight matrix reveals that this part is strongly correlated with gameroom, casino, and poolinside. Part 17 ﬁres strongly on bed and it has the highest weight for hospitalroom, children room, bedroom, and operating room. Weight matrix also identiﬁes negative parts. An interesting example is part 46 (the face detector). It has the lowest weight for buffet, classroom, computerroom, and garage. This suggests that part 46 is a negative part for these classes relative to others. This is rather surprising because one would expect to ﬁnd people in scenes such as classroom and computerroom. We examined all training images of these classes and found no visible faces in them except for 1 image in classroom and 3 images in computerroom with hardly visible faces and 1 image in garage with a clear face in it.  6 CONCLUSIONS  We presented a simple pipeline to train part-based models for image classiﬁcation. All model pa- rameters are trained jointly in our framework; this includes shared part ﬁlters and class-speciﬁc part weights. All stages of our training pipeline are driven directly by the same objective namely the classiﬁcation performance on a training set. In particular, our framework does not rely on ad- hoc heuristics for selecting discriminative and/or diverse parts. We also introduced the concept of “negative parts” for part-based models. Models based on our randomly generated parts perform better than almost all previously published work despite the profound simplicity of the method. Using CNN features and random parts we obtain 77.1% accuracy on the MIT-indoor dataset, improving the state-of-the-art. We also showed that part selection and joint training can be used to train a model that achieves better or the same level of performance as a system with randomly generated parts while using much fewer parts. Joint training alternates between training part weights and updating part ﬁlters. This process can be initiated before the ﬁrst or the second step leading to two different initialization schemes. Currently we use random examples to initialize the part ﬁlters. It would also be possible to initialize the entries in u based on how a hypothetical part is correlated with a class; negatively, irrelevant, or positively. Training the part ﬁlters would then learn part models that ﬁt this description.  9  Published as a conference paper at ICLR 2015  REFERENCES Aubry, Mathieu, Russell, Bryan, and Sivic, Josef. Painting-to-3D model alignment via discrimina-  tive visual elements. ACM Transactions on Graphics, 2013.  Dalal, Navneet and Triggs, Bill. Histograms of oriented gradients for human detection. In CVPR,  2005.  Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale  hierarchical image database. In CVPR, 2009.  Doersch, Carl, Gupta, Abhinav, and Efros, Alexei. Mid-level visual element discovery as discrimi-  native mode seeking. In NIPS, 2013.  Endres, Ian, Shih, Kevin, Jiaa, Johnston, and Hoiem, Derek. Learning collections of part models for  object recognition. In CVPR, 2013.  Felzenszwalb, Pedro, Girshick, Ross, McAllester, David, and Ramanan, Deva. Object detection  with discriminatively trained part based models. PAMI, 2010.  Girshick, Ross and Malik, Jitendra. Training deformable part models with decorrelated features. In  ICCV, 2013.  Hariharan, Bharath, Malik, Jitendra, and Ramanan, Deva. Discriminative decorrelation for cluster-  ing and classication. In ECCV, 2012.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv:1408.5093, 2014.  Joachims, Thorsten, Finley, Thomas, and Yu, Chun-Nam John. Cutting-plane training of structural  svms. Machine Learning, 2009.  Juneja, Mayank, Vedaldi, Andrea, Jawahar, C. V., and Zisserman, Andrew. Blocks that shout:  Distinctive parts for scene classiﬁcation. In CVPR, 2013.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey. Imagenet classication with deep convolu-  tional neural networks. In NIPS, 2012.  Lazebnik, Svetlana, Schmid, Cordelia, and Ponce, Jean. Beyond bag of features: Spatial pyramid  matching for recognizing natural scene categories. In CVPR, 2006.  Lobel, Hans, Vidal, Rene, and Soto, Alvaro. Hierarchical joint max-margin learning of mid and top  level representations for visual recognition. In ICCV, 2013.  Naderi, Sobhan, Oberlin, John, and Felzenszwalb, Pedro. Reconﬁgurable models for scene recog-  nition. In CVPR, 2012.  Parikh, Devi and Grauman, Kristen. Relative attributes. In ICCV, 2011.  Quattoni, Ariadna and Torralba, Antonio. Recognizing indoor scenes. In CVPR, 2009.  Razavian, Ali Sharif, Azizpour, Hossein, Sullivan, Josephine, and Carlsson, Stefan. Cnn features  off-the-shelf: an astounding baseline for recognition. In CVPR DeepVision workshop, 2014.  Singh, Saurabh, Gupta, Abhinav, and Efros, Alexei. Unsupervised discovery of mid-level discrimi-  native patches. In ECCV, 2012.  Sun, Jian and Ponce, Jean. Learning discriminative part detectors for image classiﬁcation and coseg-  mentation. In ICCV, 2013.  Yuille, Alan and Rangarajan, Anand. The concave-convex procedure. In NIPS, 2003.  Zhou, Bolei, Lapedriza, Agata, Xiao, Jianxiong, Torralba, Antonio, and Oliva, Aude. Learning deep  features for scene recognition using places database. In NIPS, 2014.  10  Published as a conference paper at ICLR 2015  3 t r a P  6 t r a P  6 1  t r a P  7 1 t r a P  0 2  t r a P  5 2  t r a P  7 2  t r a P  9 2  t r a P  5 3  t r a P  7 3  t r a P  0 4  t r a P  3 4  t r a P  6 4  t r a P  Figure 7: Top detections of parts on test images of the full dataset. The numbers in the ﬁrst column match the part indices in Figure 5. Part detection is done in a multi-scale sliding window fashion and using a 256×256 window. For visualization purposes images are stretched to have the same size.  11  Published as a conference paper at ICLR 2015  APPENDIX  A NOTES ON OPTIMIZATION OF THE BOUND Bu  The joint training procedure outlined in Section 3 is computationally expensive because of two reasons. Firstly, joint training involves optimizing the model parameters for all categories simulta- neously. This includes all the shared part ﬁlters as well as all class-speciﬁc part weights. Secondly, learning part ﬁlters requires convolving them with training images repeatedly which is a slow pro- cess. Similar to (Felzenszwalb et al. (2010)) we use a caching mechanism to make this process tractable (Section A.1). It works by breaking the problem of training parts into a series of smaller optimization problems. The solution to each sub-problem optimizes the part ﬁlters on a limited set of candidate locations (instead of all possible locations in a scale pyramid). To expedite optimization of the parts over a cache even further we use the cutting-plane method (Section A.2).  A.1 CACHING HARD EXAMPLES  We copy the bound Bu from (7) here:  Bu(w, wold) = λw||w||2+  max  0, 1+max y(cid:54)=yi  (cid:26)  k(cid:88)  i=1  (uy−uyi )·(cid:2)Sy,yir(xi, w)+ ¯Sy,yis(xi, zi, w)(cid:3)(cid:27)  (8)  Recall that Sy,y(cid:48) and ¯Sy,y(cid:48) are m×m diagonal 0-1 matrices that select s(xi, zi,j, wj) when uy,j−uyi,j < 0 and r(xi, wj) otherwise. The two functions r and s are deﬁned in (1). Minimizing Bu(w, wold) over w requires repeatedly computing the vector of ﬁlter responses r(xi, w) from the entire scale hierarchy of each image which is very expensive; see (1). To make the minimization of Bu(w, wold) tractable we use an iterative “caching” mechanism. In each iteration, we update the content of the cache and ﬁnd the optimal w subject to the data in the cache. This pro- cedure is guaranteed to converge to the global minimum of Bu(w, wold). For each part, the cache stores only a few number of active part locations from the scale hierarchy of each image. Thus, ﬁnding the highest responding location for each part among the active entries in the cache requires only a modest amount of computation. Note that here, unlike (Felzenszwalb et al. (2010)), there is no notion of hard negatives because we use a multi-class classiﬁcation objective. Instead we have hard examples. A hard example is a training example along with the best assignment of its latent variables (with respect to a given model) that either has non-zero loss or lies on the decision boundary. In the following we explain our caching mechanism and prove it converges to the unique global minimum of the bound. Let Zi = {(y, z) : y ∈ Y, z ∈ H(xi)m} be the set of all possible latent conﬁgurations of m parts on image xi. Also let Φ(x, z, ¯z, a, ¯a) = (a1ψ(x, z1) + ¯a1ψ(x, ¯z1); . . . ; amψ(x, zm) + ¯amψ(x, ¯zm)) be some features extracted from placed parts. The feature function takes in an image x, two part placement vectors z, ¯z ∈ H(x)m, and two m-dimensional part weight vectors a, ¯a as input and outputs an md-dimensional feature vector. Deﬁne  ay,y(cid:48) = (uy − uyi)T ¯Sy,yi ¯ay,y(cid:48) = (uy − uyi)T ¯Sy,yi  (cid:54)= 0 then ¯ay,y(cid:48),j = 0 and vice versa.  Note that if ay,y(cid:48),j = arg maxzj∈H(xi) s(xi, zj, wj) to be the best placement of part j on image xi using the part ﬁlter deﬁned by w. We use this notation and rewrite Bu(w, wold) as follows,  Finally, we deﬁne z(i,w)  j  Bu(w, wold) = λw||w||2 +  max (y,z)∈Zi  wT Φ(xi, z, z(i,wold), ay,yi, ¯ay,yi)) + ∆(yi, y)  (9)  When optimizing Bu(w, wold), we deﬁne a cache C to be a set of triplets (i, f, δ) where i indicates the i-th training example and f and δ indicate the feature vector Φ(xi, z, z(i,wold), ay,yi, ¯ay,yi) and  12  k(cid:88)  i=1  Published as a conference paper at ICLR 2015  Algorithm 2 Fast optimization of the convex bound Bu(w, wold) using hard example mining 1: Input: wold 2: C0 := {(i, 0, 0)|1 ≤ i ≤ k} 3: t := 0 4: while H(wt, wold,D) (cid:54)⊆ Ct do Ct := Ct \ E(wt, wold,D) 5: Ct := Ct ∪ H(wt, wold,D) 6: wt+1 := arg minw BCt(w) 7: 8: t := t + 1 9: end while 10: output wt  k(cid:88)  the loss value ∆(yi, y) associated to a particular (y, z) ∈ Zi respectively. The bound Bu with respect to a cache C can be written as follows:  BC(w) = BC(w; u, wold) = λw||w||2 +  (10) Note that BCA (w; u, wold) = Bu(w, wold) when CA includes all possible latent conﬁgurations; that is CA = {(i, f, δ)|∀i ∈ {1, . . . , k},∀(y, z) ∈ Zi, f = Φ(xi, z, z(i,wold), ay,yi, ¯ay,yi), δ = ∆(yi, y)}. We denote the set of hard and easy examples of a dataset D with respect to w and wold by H(w, wold,D) and E(w, wold,D), respectively, and deﬁne them as follows:  wT f + δ  (i,f,δ)∈C  max  i=1  H(w, wold,D) ={(i, Φ(xi, z, z(i,wold), ay,yi, ¯ay,yi), ∆(yi, y))|1 ≤ i ≤ k,  (y, z) = arg max (ˆy,ˆz)∈Zi  wT Φ(xi, ˆz, z(i,wold), aˆy,yi, ¯aˆy,yi) + ∆(yi, ˆy)}  (11)  E(w, wold,D) ={(i, Φ(xi, z, z(i,wold), ay,yi, ¯ay,yi), ∆(yi, y))|1 ≤ i ≤ k, (y, z) ∈ Zi,  wT Φ(xi, z, z(i,wold), ay,yi, ¯ay,yi) + ∆(yi, y) < 0}  (12) Note that if y = yi the term in the arg max in (11) is zero, regardless of the value of ˆz. That is because ∀y ∈ Y : ay,y = ¯ay,y = 0. So, wT f + δ is non-negative for any (i, f, δ) ∈ H(w, wold,D). We use the caching procedure outlined in Algorithm 2 to optimize the bound Bu. The beneﬁt of this algorithm is that instead of direct optimization of Bu (which is quickly becomes intractable as the size of the problem gets large) it solves several tractable auxiliary optimization problems (line 7 of Algorithm 2). The algorithm starts with the initial cache C0 = {(i, 0, 0)|1 ≤ i ≤ k} where 0 is the all-zero vector. This corresponds to the set of correct classiﬁcation hypotheses; one for each training image. It then alternates between updating the cache and ﬁnding the w∗ that minimizes BC until the cache does not change. To update the cache we remove all the easy examples and add new hard examples to it (lines 5,6 of Algorithm 2). Note that C0 ⊆ C at all times. Depending on the value of λw, in practice, Algorithm 2 may take up to 10 iterations to converge. However, one can save most of these cache-update rounds by retaining the cache content after con- vergence and using it to warm-start the next call to the algorithm. With this trick, except for the ﬁrst call, Algorithm 2 takes only 2-3 rounds to converge. The reason is that many cache entries remain active even after wold is updated; this happens, in particular, as we get close to the last iterations of the joint training loop (lines 2-8 of Algorithm 1). Note that to employ this trick one has to modify the feature values (i.e. the f ﬁeld in the triplets (i, f, δ)) of the entries in the retained cache according to the updated value of wold. The following theorem shows that the caching mechanism of Algorithm 2 works; meaning that it converges to w∗ = arg minw(cid:48) Bu(w(cid:48), wold) for any value of u, wold. Theorem 1. The caching mechanism converges to w∗ = arg minw(cid:48) Bu(w(cid:48), wold). Proof. Let CA be the cache that contains all possible latent conﬁgurations on D. Assume that Algorithm 2 converges, after T iterations, to w† = arg minw BCT (w). Then since the algorithm  13  Published as a conference paper at ICLR 2015  converged CA \ CT ⊆ E(w†, wold,D). Consider a small ball around w† such that for any w in this ball H(w, wold,D) ⊆ CT . The two functions BCA(w) and Bu(w, wold) are equal in this ball and w† is a local minimum inside this region. This implies that w† is the global minimum of Bu(w, wold) because it is a strictly convex function and therefore has a unique local minimum. To complete the proof we only need to show that the algorithm does in fact converge. The key idea is to note that the algorithm does not visit the same cache more than once. So, it has to converge in a ﬁnite number of iterations because the number of possible caches is ﬁnite. A.2 OPTIMIZING BC VIA CUTTING-PLANE METHOD In this section we review a fast method that implements line 7 of Algorithm 2. The goal is to solve the following optimization problem:  w∗ C = arg min  w  BC(w)  = arg min  w  λw||w||2 +  k(cid:88)  i=1  max  (i,f,δ)∈C  wT f + δ  (13)  One approach is to treat this as an unconstrained optimization problem and search for the optimal w in the entire space that w lives in. Although the form of the objective function (13) is complicated (it is piecewise quadratic), one can use (stochastic) gradient descent to optimize it. This boils down to starting from an arbitrary point w, computing (or estimating) the sub-gradient vector and updating w in the appropriate direction accordingly. Each gradient step requires ﬁnding the cache-entry with the highest value (because of the max operation in Equation 13) and convergence requires numerous gradient steps. We found this to be prohibitively slow in practice. Another approach optimizes a sequence auxiliary objective functions instead. The auxiliary objec- tive functions are simple but constrained. The approach proceeds by gradually adding constraints to make the solution converge to that of the original objective function. We can turn the complex objective function (13) into a simple quadratic function but it comes at the price of introducing an extremely large set of constraints. However, our experiments show that the optimization problem becomes tractable by using the 1-slack formulation of Joachims et al. (2009) and maintaining a working set of active constraints. Let W = {(e1, e2, . . . , ek) : ei = (i, f, δ) ∈ C} be the set of constraints to be satisﬁed. Each con- straint ω ∈ W is an k-tuple whose i-th element is a cache-entry ei = (i, f, δ) ∈ C that corresponds to the i-th training example. Note that each constraint ω speciﬁes a complete latent conﬁguration on the set of training samples (that is the category label and all part locations for all training samples). Therefore, each constraint is a linear function of w. There is a total training loss value associated to i=1 wT fω,i + δω,i where fω,i and δω,i are given by eω,i = (i, fω,i, δω,i). Solving the unconstrained minimization problem in (13) is equivalent to solving the following quadratic programming (QP) problem subject to a set of constraints speciﬁed by W:  each constraint ω = (eω,1, . . . , eω,k) which we refer to as loss(ω, w) =(cid:80)k  w∗ C = arg min  λw||w||2 + ξ  (14)  w  s.t. ∀ω ∈ W : loss(ω, w) ≤ ξ  In practice, we cannot optimize 14 over the entire constraint set explicitly since |W| is too large. Instead, we optimize it over an active subset of the constraint set W ⊆ W. More speciﬁcally, we start from W = ∅, solve the QP with respect to the current W , add the most violated constraint1 to W , and repeat until addition of no new constraint increases the value of the objective function by more than a given (cid:15). Joachims et al. (2009) showed that the number of iterations it takes for this algorithm to converge is inversely proportional to the value of epsilon and is also independent of the size of the training set. They also showed that the dual of the QP in 14 has an extremely sparse solution in the sense that most of the dual variables turn out to be zero. Since dual variables correspond to constraints in the primal form the observation implies that only a tiny fraction of the primal constraints will be active. This is the key behind efﬁciency of the algorithm in practice.  1The most violated constraint is the one with the highest loss value i.e. arg maxω∈W loss(ω, w).  14  Published as a conference paper at ICLR 2015  Algorithm 3 Fast QP solver for optimizing BC 1: Input: Cache C, convergence precision (cid:15) 2: W := ∅ 3: repeat 4: 5: 6: 7: 8: 10: until loss(ω∗, w) ≤ ξ + (cid:15) 11: output w  (cid:80) ω∈W α∗ 9: W := W(cid:83) {ω∗}  construct |W| × |W| matrix M and vector b (see text) solve the QP in Equation 15 to ﬁnd α∗ and compute ξ w := −1 2λw prune W ﬁnd most violated constraint ω∗ = arg maxω∈W loss(ω, w)  i=1 fω,i (see Equation 16)  (cid:80)k  ω  A.3 DUAL FORMULATION Let M be a |W|×|W| kernel matrix for the feature function φ(ω) = (2λw)− 1  2(cid:80)k  i=1 fω,i. Also, let i=1 δω,i for ω ∈ W . We derive the dual of 14 and write  it in the following simple form:  b be a vector of length |W| where bω = −(cid:80)k s.t. (cid:88)  α∗ = arg min αω≥0  The solution of the dual and primal are related through the following equation:  ω∈W  1 αT M α + αT b 2 αω ≤ 1  (cid:88)  ω∈W  k(cid:88)  i=1  α∗  ω  fω,i  w∗ = − 1 2λw  (15)  (16)  Since we start from an empty set of constraints W = ∅ and gradually add constraints to it, after enough iterations, many of the αω’s become (and remain) zero for the rest of the optimization process. This happens in particular for the constraints that were added in the earlier rounds of growing W . This observation suggests that we can prune the constraint set W in each iteration by discarding ω’s for which αω has remained zero for a certain number of consecutive iterations. We use Algorithm 3 to solve the optimization problem of Equation 13 in practice.  B PART SELECTION  As mentioned in Section 4 of the paper, we use group sparsity to select useful parts from a pool of randomly initialized parts. We use the same formulation as in Sun & Ponce (2013). Part selection is done by optimizing the following objective function:  max{0, max y(cid:54)=yi  (uy − uyi ) · r(xi, w) + 1}  (17)  m(cid:88)  k(cid:88)  λ  ρj +  j=1  i=1  (cid:113)(cid:80)  y u2  a small step in the opposite direction of a sub-gradient of the function. Let Rg(u) = λ(cid:80)m  y,j is the (cid:96)2-norm of the column of u that corresponds to part j. This objective where ρj = function is convex. We minimize it using stochastic gradient descent. This requires repeatedly taking j=1 ρj. The explodes as ρj goes to zero. Thus, we round the ρj’s as they approach  partial derivative ∂Rg ∂uy zero. We denote the rounded version by τj and deﬁne them as follows  = uy ρj  (cid:40) ρj  τj =  ρ2 2(cid:15) + (cid:15) j  2  if ρj > (cid:15) if ρj ≤ (cid:15)  15  Published as a conference paper at ICLR 2015  Figure 8: Effect of λ on part norms. Each plot shows sorted ρj values for a particular choice of λ.  The constants in the second case are set so that τj is continuous; that is ρ2 2 = ρj when ρj = (cid:15). In summary, part selection from an initial pool of parts w = (w1, . . . , wm) involves optimizing the following objective function:  2(cid:15) + (cid:15)  j  m(cid:88)  k(cid:88)  λ  τj +  j=1  i=1  max{0, max y(cid:54)=yi  (uy − uyi) · r(xi, w) + 1}  (18)  We can control the sparsity of the solution to this optimization problem by changing the value of λ. In Figure 8 we plot ρj for all parts in decreasing order. Each curve corresponds to the result obtained with a different λ value. These plots suggest that the number of selected parts (i.e. parts whose ρj is larger than a threshold that depends on (cid:15)) decreases monotonically as λ increases. We adjust λ to obtain a target number of selected parts.  C MORE ON VISUALIZATION OF THE MODEL  We complement Section 5.1 of the paper by providing more visualizations of jointly trained parts. Figure 9 shows the part ﬁlters and the weight matrix after joint training a model with 52 parts on the 10-class subset of MIT-indoor dataset. This model uses HOG features. The part weight matrix determines whether a part is positive or negative with respect to two categories. For example, part 42 is positive for bookstore and library relative to laundromat. Part 29 is positive for laundromat relative to bookstore and library. Part 37 is positive for library relative to bookstore so it can be used in combination with the other two parts to distinguish between all three categories bookstore, library, and laundromat. Figure 10 illustrates the top scoring patches for these three parts. Figure 11 complements Figure 7 of the paper. The part indices in the ﬁrst column match those of Figure 5. The rows show the highest scoring detections of a particular part on test images. Part 1 ﬁres on clothing-rack, part 22 appear to ﬁnd container, and part 33 detects table-top. There are parts that capture low-level features such as the mesh pattern of part 31 and the high-frequency horizontal stripes of part 41. Also, there are parts that are selective for certain colors. For example, part 9 appears to respond to speciﬁc red patterns (in particular fruits and ﬂowers). Part 51 appears to ﬁre on yellow-food dishes. Part 48 is very well tuned to ﬁnding text. According to the weight matrix (see Figure 5 in the paper) Part 14 is highly weighted for nursery and staircase classes and it appears to detect a row of vertical-bars. Part 21 is highly weighted for laundromat, library, and cloister and it appears to respond strongly to arch. Also note that part 21 is a strong negative part for bookstore relative to library. Presence of an arch, in fact, is a very sensible differentiating pattern that could tell library apart from bookstore.  D PROCESSING TIME  Test time: the test procedure of our models involves three simple steps: 1) convolving part ﬁlters with the test image, 2) computing the part-based representation 3) ﬁnding the class with the highest  16  010020030040050060070000.20.40.60.81Part indexPart norm  λ=1e−3λ=2e−3λ=3e−3λ=5e−3λ=7e−3λ=10e−3λ=12e−3λ=15e−3λ=17e−3λ=20e−3λ=22e−3λ=25e−3λ=30e−3Published as a conference paper at ICLR 2015  classiﬁcation score. Step 1 takes O(mhd) time where m, h, and d are the number of parts, latent locations, and dimensionality of the patch features. Step 2 takes O(hR) time where R is the number of pooling regions. Step 3 takes O(nmR) time where n is the number of classes. The bottleneck in test time is step 1 and 3 both of which depend on the number of parts m. So, a decrease in m directly affects the test time. Note that both of these two steps are embarrassingly parallel processes. Training time: the training procedure involves two main steps: 1) learning part weights (line 3 in Algorithm 1) and 2) learning part ﬁlters (lines 4-7 in Algorithm 1). The ﬁrst step is a standard multi-class SVM problem and is relatively fast to train. The bottleneck in training is the second step. Learning part ﬁlters involves multiple nested loops: 1) joint training loop (lines 2-8 in Algorithm 1), 2) relabeling loop (lines 4-7 in Algorithm 1), 3) cache update loop (lines 4-9 in Algorithm 2), and 4) the constraint generation loop of the QP solver (lines 3-10 in Algorithm 3). The number of iterations each loop takes depends on the training data and the hyper parameters of the model (i.e. λw and λu). We report the running time of our joint training algorithm separately for one experiment that uses HOG features and one that uses CNN features as the dimensionality of the features and the number of latent locations they consider is different. In our current implementation it takes 5 days to do joint training with 120 shared parts on the full MIT-indoor dataset on a 16-core machine using HOG features. It takes 2.5 days to do joint training with 372 parts on the full dataset on a 8 core machine using 60-dimensional PCA-reduced CNN features. Note that these time include learning all shared part ﬁlters and all 67 class-speciﬁc part weight vectors on a single machine. In both experiments ﬁnding the most violated constraint (line 8 in Algorithm 3) takes more than half of the total running time. The second bottleneck for HOG features is growing the caches (line 6 in Algorithm 2). This involves convolving the part ﬁlters (1152 dimensional HOG templates) with all training images (each containing 11000 candidate locations). With the CNN features, however, the second bottleneck becomes the QP solver (line 7 in Algorithm 2). The QP solver that we use only uses a single core. In both cases the ratio of the time taken by the ﬁrst bottleneck to the second one is 4 to 1. The pipeline in previous methods such as (Juneja et al. (2013); Sun & Ponce (2013); Doersch et al. (2013)) has several steps. For example, to discover parts, Juneja et al. (2013) applies multiple superpixel segmentations on the image to ﬁnd initial seeds, trains exemplar LDA for each seed, enhances the candidate parts by harvesting similar patches in the dataset, and computes the entropy of the top-50 detections of each part over categories. They discard parts with high entropy as well as duplicates. Despite using several heuristics these methods are slow too. Doersch et al. (2013) do not comment on the processing time of their method in the paper but we know from personal correspondence that their code takes a long time to run. However, most of the steps in their method are independent; e.g. they start their method from multiple initial points to ﬁnd the discriminative modes, they train 1-vs-all classiﬁers, etc. So, they distribute the processing load on a big cluster in order to run their experiments. Our experimental results showed that we can obtain better performance than Juneja et al. (2013) and Sun & Ponce (2013) using a pool of randomly initialized parts (see Figure 3). Note that creating a pool of random parts is very straightforward and fast. It only takes extracting features from random subwindow of a random image and applying a simple feature transformation on them (see Section 4).  17  Published as a conference paper at ICLR 2015  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  Figure 9: Part ﬁlters (top) and part weights (bottom) after joint training a model with 52 parts on the 10-class dataset. Here we use HOG features. Although the model uses 5 pooling regions (corresponding to cells in 1×1 + 2×2 grids) here we show the part weights only for the ﬁrst pooling region corresponding the entire image.  Top scoring patches on test images (1 patch per image)  2 4  t r a P  7 3  t r a P  9 2  t r a P  Figure 10: Top detections of three parts on test images of the 10-class dataset. The numbers in the ﬁrst column match the part indices in Figure 9. Patches from bookstore, laundromat, and library images are highlighted in red, green, and blue respectively (best viewed in color).  18    246810121416182022242628303234363840424446485052bookstorebowlingclosetcorridorlaundromatlibrarynurseryshoeshopstaircasewinecellar−0.200.20.4Published as a conference paper at ICLR 2015  1 t r a P  5 t r a P  9  t r a P  4 1 t r a P  9 1  t r a P  1 2  t r a P  2 2  t r a P  1 3  t r a P  3 3  t r a P  1 4  t r a P  8 4  t r a P  1 5 t r a P  Figure 11: Top detections of parts on test images of the full dataset. The numbers in the ﬁrst column match the part indices in Figure 5. Part detection is done in a multi-scale sliding window fashion and using a 256×256 window. For visualization purposes images are stretched to have the same size.  19  ",
1410.1165,2015,Understanding Locally Competitive Networks,"['Understanding Locally Competitive Networks', 'Rupesh Srivastava', 'Jonathan Masci', 'Faustino Gomez', 'and Juergen Schmidhuber']",https://arxiv.org/pdf/1410.1165,"5 1 0 2    r p A 9         ] E N . s c [      3 v 5 6 1 1  .  0 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  UNDERSTANDING LOCALLY COMPETITIVE NETWORKS  Rupesh Kumar Srivastava, Jonathan Masci, Faustino Gomez & J¨urgen Schmidhuber Istituto Dalle Molle di studi sull’Intelligenza Artiﬁciale (IDSIA) Scuola universitaria professionale della Svizzera italiana (SUPSI) Universit`a della Svizzera italiana (USI) Lugano, Switzerland {rupesh, jonathan, tino, juergen}@idsia.ch  ABSTRACT  Recently proposed neural network activation functions such as rectiﬁed linear, maxout, and local winner-take-all have allowed for faster and more effective train- ing of deep neural architectures on large and complex datasets. The common trait among these functions is that they implement local competition between small groups of computational units within a layer, so that only part of the network is activated for any given input pattern. In this paper, we attempt to visualize and understand this self-modularization, and suggest a uniﬁed explanation for the beneﬁcial properties of such networks. We also show how our insights can be directly useful for efﬁciently performing retrieval over large datasets using neural networks.  INTRODUCTION  1 Recently proposed activation functions for neural networks such as rectiﬁed linear (ReL (Glorot et al., 2011)), maxout (Goodfellow et al., 2013a) and LWTA (Srivastava et al., 2013) are quite unlike sigmoidal activation functions. These functions depart from the conventional wisdom in that they are not continuously differentiable (and sometimes non-continuous) and are piecewise linear. Nevertheless, many researchers have found that such networks can be trained faster and better than sigmoidal networks, and they are increasingly in use for learning from large and complex datasets (Krizhevsky et al., 2012; Zeiler et al., 2013). Past research has shown observational evidence that such networks have beneﬁcial properties such as not requiring unsupervised training for weight initialization (Glorot et al., 2011), better gradient ﬂow (Goodfellow et al., 2013a) and mitigation of catastrophic forgetting (Srivastava et al., 2013; Goodfellow et al., 2014). Recently, the expressive power of deep networks with such functions has been theoretically analyzed (Pascanu et al., 2013). However, we are far from a complete understanding of their behavior and advantages over sigmoidal networks, especially during learning. This paper sheds additional light on the properties of such networks by interpreting them as models of models. A common theme among the ReL, maxout and LWTA activation functions is that they are locally competitive. Maxout and LWTA utilize explicit competition between units in small groups within a layer, while in the case of the rectiﬁed linear function, the weighted input sum competes with a ﬁxed value of 0. Networks with such functions are often trained with the dropout regularization technique (Hinton et al., 2012) for improved generalization. We start from the observation that in locally competitive networks, a subnetwork of units has non- zero activations for each input pattern. Instead of treating a neural network as a complex function approximator, the expressive power of the network can be interpreted to be coming from its ability to activate different subsets of linear units for different patterns. We hypothesize that the network acts as a model that can switch between “submodels” (subnetworks) such that similar submodels respond to similar patterns. As evidence of this behavior, we analyze the activated subnetworks for a large subset of a dataset (which is not used for training) and show that the subnetworks activated for different examples exhibit a structure consistent with our hypothesis. These observations provide a uniﬁed explanation for improved credit assignment in locally competitive networks during training, which is believed to be the main reason for their success. Our new point of view suggests a link between these networks and competitive learning approaches of the past decades. We also show that  1  Published as a conference paper at ICLR 2015  Figure 1: Comparison of rectiﬁed linear units (ReLUs), local winner-take-all (LWTA), and maxout activation functions. The pre- and post- synaptic activations of the units are shown on the left and right side of the units respectively. The shaded units are ‘active’ – non-zero activa- tions and errors ﬂow through them. The main difference between maxout and LWTA is that the post-synaptic activation can ﬂow through connections with different weight depending on the winning unit in LWTA. For maxout, the out- going weight is the same for all units in a block.  Figure 2: Subnetworks for 100 examples for 10 ReLUs. The examples activate many different possible subsets of the units, shown in dark. In this case, unit number 3 is inactive for all exam- ples.  a simple encoding of which units in a layer are activated for a given example (its subnetwork) can be used to represent the example for retrieval tasks. Experiments on MNIST, CIFAR-10, CIFAR-100 and the ImageNet dataset show that promising results are obtained for datasets of varying size and complexity.  2 LOCALLY COMPETITIVE NEURAL NETWORKS Neural networks with activation functions like rectiﬁed linear, maxout and LWTA are locally com- petitive. This means that local competition among units in the network decides which parts of it get activated or trained for a particular input example. For each unit, the total input or presynaptic acti- vation z is ﬁrst computed as z = wx + b, where x is the vector of inputs to the unit, w is a trainable weight vector, and b is a trainable bias. For the rectiﬁed linear function, the output or postsynaptic activation of each unit is simply max(z, 0), which can be interpreted as competition with a ﬁxed value of 0. For LWTA, the units in a layer are considered to be divided into blocks of a ﬁxed size. Then the output of each unit is Iz where I is an indicator which is 1 if the unit has the maximum z in its group and 0 otherwise. In maxout, the inputs from a few units compete using a max operation, and the block output is the maximum z among the units1. A maxout block can also be interpreted as an LWTA block with shared outgoing weights among the units. A comparison of the 3 activation functions is shown in Figure 1. In each of the three cases, there is a local gating mechanism which allows non-zero activations (and errors during training) to propagate only through part of the network, i.e. a subnetwork. Consider the activation of a neural network with rectiﬁed linear units (ReLUs) in a single hidden layer. For each input pattern, the subset of units with non-zero activations in the hidden layer form a subnetwork, and an examination of the subnetworks activated for several examples shows that a large number of different subnetworks are activated (Figure 2). The result of training the network can interpreted in the following way: when training a single network with a local gating mechanism, a large number of linear subnetworks are trained on the dataset such that different examples are gated to different subnetworks, each getting trained to produce the desired output. At test time, the system generalizes in the sense that the appropriate subnetwork for a given example is activated.  3 SUBNETWORK ANALYSIS This section investigates how the model of models that is implemented though local competition self-organizes due to training. In order to visualize the organization of subnetworks as a result of training, they are encoded as bit strings called submasks. For the input pattern i, the submask  1In our terminology, the terms unit and block correspond to the terms ﬁlter and units in Goodfellow et al.  (2013a).  2  020406080100Examples10987654321UnitsPublished as a conference paper at ICLR 2015  (a)  (b)  Figure 3: 2-D visualization of submasks from the penultimate layer of a 3 hidden layer network with ReLUs on the MNIST test set. (a) shows the submasks from an untrained network layer which lacks any discernable structure. (b) shows submasks from a trained network layer, showing clearly demarcated clusters relevant to the supervised learning task. ‘Mistakes’ made by the network can also be observed, such as mistaking ‘4’s for ‘9’s.  si ∈ {0, 1}u, where u is the number of units in the full network, represents the corresponding subnetwork by having a 0 in position j, j = 1..u, if the corresponding unit has zero activation, and 1 otherwise. The submasks uniquely and compactly encode each subnetwork in a format that is amenable to analysis through clustering, and, as we show in Section 4.2, facilitates efﬁcient data retrieval. In what follows, the subnetworks that emerge during training are ﬁrst visualized using the t- SNE (Van der Maaten & Hinton, 2008) algorithm. This dimensionality reduction technique en- ables a good visualization of the relationship between submasks for several examples in a dataset by preserving the local structure. Later in this section, we examine the evolution of subnetworks during training, and show that the submasks obtained from a trained network can directly be used for classiﬁcation using a simple nearest neighbors approach. All experiments in this section are performed on the MNIST (LeCun et al., 1998) dataset. This familiar dataset was chosen because it is relatively easy, and therefore provides a tractable setting in which to verify the repeatability of our results. Larger, more interesting datasets are used in section 4 to demonstrate the utility of techniques developed in this section for classiﬁcation and retrieval. 3.1 VISUALIZATION THROUGH DIMENSIONALITY REDUCTION For visualizing the relationship between submasks for a large number of input patterns, we trained multiple networks with different activation functions on the MNIST training set, stopping when the error on a validation set did not improve. The submasks for the entire test set (10K examples) were then extracted and visualized using t-SNE. Since the competition between subnetworks is local and not global, subsets of units in deeper (closer to the output) layers are activated based on information extracted in the shallow layers. Therefore, like unit activations, submasks from deeper layers are expected to be better related to the task since deeper layers code for higher level abstractions. For this reason, we use only submasks extracted from the penultimate network layers in this paper, which considerably reduces the size of submasks to consider. Figure 3b shows a 2D visualization of the submasks from a 3 hidden layer ReL network. Each submask is a bitstring of length 1000 (the size of the network’s penultimate layer). Ten distinct clusters are present corresponding to the ten MNIST classes. It is remarkable that, irrespective of the actual activation values, the subnetworks which are active for the testing examples can be used to visually predict class memberships based on their similarity to each other. The visualization conﬁrms that the subnetworks active for examples of the same class are much more similar to each other compared to the ones activated for the examples of different classes. Visualization of submasks from the same layer of a randomly initialized network does not show any structure (Figure 3a), but we observed some structure for the untrained ﬁrst hidden layer (Appendix A.1). For trained networks, similar clustering is observed in the submasks from shallow layers in  3  01234567890123456789Published as a conference paper at ICLR 2015  Network ReL (no dropout) LWTA (dropout) Maxout (dropout)  No. test errors kNN Softmax 158 154 131  161 142 116  Table 1: Some examples of classiﬁcation re- sults on the permutation invariant MNIST test set using softmax layer outputs vs. kNN on the submasks. All submasks are ex- tracted from the penultimate layer. kNN re- sults are close to the softmax results in each case. The maxout network was additionally trained on the validation set. Results vary slightly across experimental runs and were not cherry-picked for reporting.  Figure 4: The plot shows mean of the fraction of ex- amples (total 10K) for which units in the layer ﬂip (turn from being active to inactive or vice-versa) af- ter every pass through the training set. The units ﬂip for upto 20% of the examples on average in the ﬁrst few epochs, but quickly settle down to less than 5%.  the network, though the clusters appear to be less separated and tight. The visualization also shows many instances where the network makes mistakes. The submasks for some examples lie in the cluster of submasks for the wrong class, indicating that the ‘wrong’ subnetwork was selected for these examples. The experiments in the next sections show that the organization of subnetworks is indicative of the classiﬁcation performance of the full network. Other locally competitive activation functions such as LWTA and maxout result in similar clustering of submasks (visualizations included in Appendix A.1). For LWTA layers, the submasks can be directly constructed from the activations because there is no subsampling when going from presy- naptic to postsynaptic activations, and it is reasonable to expect a subnetwork organization similar to that of ReL layers. Indeed, in a limited qualitative analysis, it has been shown previously (Srivastava et al., 2013) that in trained LWTA nets there are more units in common between subnetworks for examples of the same class than those for different class examples. For maxout layers, the situation is trickier at a ﬁrst glance. The unit activations get pooled before being propagated to the next layer, so it is possible that the maximum activation value plays a much more important role than the identity of the winning units. However, using the same basic principle of credit assignment to subnetworks, we can construct submasks from maxout layers by binarizing the unit activations such that only the units producing the maximum activation are represented by a 1. Separation of subnetworks is necessary to gain the advantages of local competition during learning, and the visualization of the generated submasks produces results similar to those for ReLU and LWTA (included in Appendix A.1). 3.2 BEHAVIOR DURING TRAINING In order to measure how the subnetworks evolve over the course of training, the submasks of each sample in the training set were recorded at each epoch. Figure 4 characterizes the change in the subnets over time by counting the number of input patterns for which a unit ﬂips from being on to being off, or vice-versa, from one epoch to the next. The curve in the ﬁgure shows the fraction of patterns for which an inter-epoch ﬂip occurred, averaged across all units in the network. Higher values indicate that the assignment of subnets to patterns is not stable. The batch size for this experiment was 100, which means that each pass over the training set consists of 500 weight updates. For the run shown, the average fraction of ﬂips starts at 0.2, but falls quickly below 0.05 and keeps falling as training proceeds, indicating that, the assignment of subnetworks to individual examples stabilizes quickly. In this case, after a brief (∼3 epochs) transient period, a ﬁne-tuning period follows where the assigned subnetworks keep getting trained on their corresponding examples with little re- assignment. 3.3 EVALUATING SUBMASKS Since the visualization of submasks for the test set shows task-relevant structure, it is natural to ask: how well can the submask represent the data that produced it? If the submasks for similar examples are similar, perhaps they can be used as data descriptors for tasks such as similarity-based  4  01020304050Epochs0.000.050.100.150.20AverageﬂipsperunitPublished as a conference paper at ICLR 2015  retrieval. Sparse binary codes enable efﬁcient storage and retrieval for large and complex datasets due to which learning to produce them is an active research area (Gong et al., 2013; Masci et al., 2014b;a; Grauman & Fergus, 2013). This would make representative submasks very attractive since no explicit training for retrieval would be required to generate them. To evaluate if examples producing similar binary codes are indeed similar, we train locally competi- tive networks for classiﬁcation and use a simple k nearest neighbors (kNN) algorithm for classifying data using the generated submasks. This approach is a simple way to examine the amount of infor- mation contained in the submasks (without utilizing the actual activation values). We trained networks with fully connected layers on the MNIST training set, and selected the value of k with the lowest validation error to perform classiﬁcation on the test set. Results are shown in Table 1. In each case, the kNN classiﬁcation results are close to the classiﬁcation result obtained using the network’s softmax layer. If we use the (non-pooled) unit activations from the maxout network instead of submasks for kNN classiﬁcation, we obtain 121 errors. Submasks can also be obtained from convolutional layers. Using a convolutional maxout network, we obtained 52 errors on the MNIST test set when we reproduced the model from Goodfellow et al. (2013a). Since the penultimate layer in this model is convolutional, the submasks were constructed using the presynaptic unit activations from this layer for all convolutional maps. Visualization of these submasks showed similar structure to that obtained from fully connected layers, kNN classi- ﬁcation on the submasks resulted in 65 errors. As seen before, for a well-trained network the kNN performance is close to the performance of the network’s softmax layer. 3.4 EFFECT OF DROPOUT The dropout (Hinton et al., 2012) regularization technique has proven to be very useful and efﬁcient at improving generalization for large models, and is often used in combination with locally com- petitive activation functions (Krizhevsky et al., 2012; Goodfellow et al., 2013a; Zeiler et al., 2013). We found that networks which were trained with dropout (and thus produced lower test set error) also yielded better submasks in terms of kNN classiﬁcation performance. To observe the effect of dropout in more detail, we trained a 3 hidden layer network with 800 ReLUs in each hidden layer without dropout on MNIST starting from 5 different initializations until the validation set error did not improve. The networks were then trained again from the same initialization with dropout until the validation error matched or fell below the lowest validation error from the non-dropout case. In both cases, minibatch gradient descent with momentum was used for training the networks. A com- parison of kNN classiﬁcation error for the dropout and non-dropout cases showed that when dropout training is stopped at a point when validation error is similar to a no-dropout network, the submasks from both cases give similar results, but as dropout training continues (lowers validation set error), the submasks yield improved results. This supports the interpretation of dropout as a regularization technique which prevents “co-adaptation of feature detectors” (units) (Hinton et al., 2012), leading to better representation of data by the subnetworks. Another way to look at this effect can be that dropout improves generalization by injecting noise in the organization of subnetworks, making them more robust.  4 EXPERIMENTAL RESULTS The following experiments apply the methods described in the previous section to more challenging benchmark problems: CIFAR-10, CIFAR-100, and ImageNet. For the CIFAR experiments, we used the models described in Goodfellow et al. (2013a) since they use locally competitive activations (maxout), are trained with dropout, and good hyperparameter settings for them are available (Good- fellow et al., 2013b). We report the classiﬁcation error on the test set obtained using the softmax output layer, as well kNN classiﬁcation on the penultimate layer unit activations and submasks. The best value of k is obtained using a validation set, though we found that k = 5 with distance weighting usually worked well. 4.1 CIFAR-10 & CIFAR-100 CIFAR-10 and CIFAR-100 are datasets of 32×32 color images of 10 classes. The results obtained on the test sets for these datasets are summarized in Table 2. We ﬁnd that when comparing nearest neighbor classiﬁcation performance with submasks to unit activation values, we lose an accuracy of 1.25% on the CIFAR-10 dataset, and 2.26% on the CIFAR-100 dataset on average. Figure 5a shows the 2-D visualization of the test set submasks for CIFAR-10. Some classes can be seen to have  5  Published as a conference paper at ICLR 2015  (a)  (b)  Figure 5: 2-D visualizations of the submasks from the penultimate layer of the trained maxout networks reported in Goodfellow et al. (2013a). (a) The CIFAR10 test set. The 10-cluster structure is visible, although the clusters are not as well separated as in the case of MNIST. This corresponds with the higher error rates obtained using both kNN and the full network. (b) The CIFAR100 test set. It is difﬁcult to visualize any dataset with 100 classes, but several clusters are still visible. The separation between clusters is much worse, which is reﬂected in the high classiﬁcation error.  Dataset  CIFAR-10 CIFAR-100  Network error 9.94 ± 0.31% 34.49 ± 0.22%  kNN (activations) 9.63 ± 0.21% 37.54 ± 0.14%  kNN (pre-activations) 10.11 ± 0.16% 41.37 ± 0.26%  kNN (submasks) 11.36 ± 0.22% 43.63 ± 0.18%  Table 2: Classiﬁcation errors on CIFAR datasets comparing maxout network performance, kNN on activation values, kNN on pre-activations (before maximum pooling) and kNN on binary submasks. Results are reported over 5 runs.  IMAGENET  highly representative submasks, while confusion between classes in the lower half is observed. The clusters of subnetworks are not as well-separated as in the case of MNIST, reﬂecting the relatively worse classiﬁcation performance of the full network. Submask visualization for CIFAR-100 (Figure 5b) reﬂects the high error rate for this dataset. Although any visualization with 100 classes can be hard to interpret, many small clusters of submasks can still be observed. 4.2 The results of kNN classiﬁcation and t-SNE visualization using submasks on small datasets of vary- ing complexities show that the submasks contain substantial information relevant for image clas- siﬁcation. In this section, the utility of the submasks obtained for a large convolutional network trained on the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC-2012) (Deng et al., 2012) dataset is evaluated. Our results show that submasks retain a large amount of information on this difﬁcult large scale task, while greatly improving storage efﬁciency. For instance, 4096-dimensional submasks for the full ILSVRC-2012 training set can be stored in about 0.5 GB. Our experiments also indicate that submasks obtained from a better trained network result in better performance (Table 3). Krizhevsky et al. (2012) suggested that the activations from a trained convolutional network can be compressed to binary codes using auto-encoders. We show here that the submasks can be directly utilized for efﬁcient retrieval of data based on high level similarity even though no pair-wise loss was used during training. We compare to DiffHash, a supervised similarity-preserving hashing approach proposed by Strecha et al. (2012), trained on the non-binarized features from the network. Supervision is represented in terms of similar and dissimilar pairs of points, for which a ground-truth similarity measure is known, i.e. sharing the same class or not. While it is beyond the scope of this paper to provide an  2https://github.com/torontodeeplearning/convnet/  6  Published as a conference paper at ICLR 2015  Network DeCAF Convnet2  Network error  19.2% 13.5%  kNN on submasks  29.2% 20.38%  Table 3: Top-5 Classiﬁcation accuracy on validation set when performance of two different networks on ImageNet is compared to performance of submasks obtained from each of them. Note that as network accuracy improves by about 6%, submask accuracy improves by about 10%.  Technique mAP@5 mAP@10 mAP@100 Submasks Diffhash  58.3 61.0  46.7 49.5  56.7 59.3  Figure 6: Comparison of precision-recall curves on ILSVRC-2012 when using binary codes ob- tained using different techniques. The perfor- mance of submasks is competitive and decays only for high recall values where supervised hash- ing obtains a better ranking of the results due to the pair-wise supervision.  Table 4: Comparison of mean average precisions at various thresholds using binary codes obtained using different techniques on the ILSVRC-2012 dataset. Submasks are obtained directly from net- works trained for classiﬁcation without any further training. Up to mAP@100 the submasks show a con- stant performance degradation of about 3 points.  exhaustive comparison or to introduce a new approach to supervised hashing, we nevertheless show (cid:80)R very competitive performance w.r.t. a dedicated algorithm devised for this task. Precision-recall curves are shown in Figure 6 while Table 4 reports results for mean average precision; mAP = r=1 P (r) · rel(r), where rel(r) indicates the relevance of a result at a given rank r, P (r) the precision at r, and R the number of retrieved results. DiffHash learns a linear projection, which is one of the reason we decided to use it to limit impact of supervision. Thus we attribute the small performance gap to the input features already being very discriminative which left little room for improvement. For the purpose of this comparison, we did not investigate more sophisticated techniques which would have steered the focus to conventional hashing approaches. Sample retrieval results for examples from the ILSVRC-2012 dataset are shown in Figure 7.  5 DISCUSSION Training a system of many networks on a dataset such that they specialize to solve simpler tasks can be quite difﬁcult without combining them into a single network with locally competitive units. Without such local competition, one needs to have a global gating mechanism as in Jacobs et al. (1991). The training algorithm and the objective function also need modiﬁcations such that compe- tition between networks is encouraged, and the system becomes hard to train. On the other hand, a locally competitive neural network can behave like a model composed of many subnetworks, and massive sharing of parameters between subnetworks enables better training. Stochastic gradient de- scent can be used to minimize the desired loss function, and the implementation is so simple that one does not even realize that a model of models is being trained. Figure 4 suggests that during optimization, the subnetworks get organized during an early transient phase such that subnetworks responding to similar examples have more parameters in common than those responding to dissimilar examples. This allows for better training of subnetworks due to reduced interference from dissimilar examples and shared parameters for similar examples. In the later ﬁne-tuning phase, the parameters of subnetworks get adjusted to improve classiﬁcation and much less re-assignment of subnetworks is needed. In this way, the gating mechanism induced by locally competitive activation functions accomplishes the purpose of global competition efﬁciently and no modiﬁcations to the error function are required. We believe that due to above advantages locally competitive networks have allowed easier and faster training on complex pattern recognition tasks compared to networks with sigmoidal or similar ac- tivation functions. These ﬁndings provide indirect evidence that low interference between subnet- works is a beneﬁcial property for training large networks. The nature of organization of subnetworks is reminiscent of the data manifold hypothesis for classiﬁcation (Rifai et al., 2011). Just like data  7  Recall00.20.40.60.81Precision0.30.40.50.60.70.80.91DiffHashSubmasksPublished as a conference paper at ICLR 2015  Figure 7: Retrieval based on subnetworks on the ILSVRC-2012 dataset. The ﬁrst image in each row is the query image; the remaining 5 are the responses retrieved using submasks.  points of different classes are expected to concentrate along sub-manifolds, we expect that the orga- nization of subnetworks that respond to the data points reﬂects the data manifold being modeled. An important take-away from these results is the unifying theme between locally competitive archi- tectures, which is related to past work on competitive learning. Insights from past literature on this topic may be utilized to develop improved learning algorithms and neural architectures. This paper, to the best of our knowledge, is the ﬁrst to show that useful binary data descriptors can be obtained directly from a neural network trained for classiﬁcation without any additional training. These de- scriptors are not just results of a thresholding trick or unique to a particular activation function, but arise as a direct result of the credit assignment process. Our experiments on datasets of increasing complexity show that when the network performance improves, the performance gap to submask- based classiﬁcation closes. This suggests that in the near future, as training techniques continue to advance and yield lower errors on larger datasets, submasks will perform as well as activation values for retrieval and transfer learning tasks. Importantly, these binary representations will always be far more efﬁcient for storage and retrieval than continuous activation vectors.  REFERENCES Batra, D., Agrawal, H., Banik, P., Chavali, N., and Alfadda, A. CloudCV: Large-Scale distributed computer vision as a cloud service. http://www.cloudcv.org, September 2013. URL http://www.cloudcv.org. Deng, Jia, Berg, Alex, Satheesh, Sanjeev, Hao, Su, Khosla, Aditya, and Li, Fei-Fei. ImageNet large scale visual recognition competition 2012 (ILSVRC2012). http://www.image-net.org/challenges/LSVRC/2012/, 2012. URL http://www.image-net.org/challenges/LSVRC/2012/.  Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. DeCAF: a deep convolutional activation feature for generic visual recognition. arXiv:1310.1531 [cs], Octo- ber 2013. URL http://arxiv.org/abs/1310.1531.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectiﬁer networks.  In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics. JMLR W&CP, volume 15, pp. 315–323, 2011. URL http://eprints.pascal-network.org/archive/00008596/.  8  Published as a conference paper at ICLR 2015  Gong, Yunchao, Kumar, Sanjiv, Rowley, Henry A., and Lazebnik, Svetlana. Learning binary codes for high- In Computer Vision and Pattern Recognition (CVPR), 2013 dimensional data using bilinear projections. IEEE Conference on, pp. 484–491. IEEE, 2013. URL http://ieeexplore.ieee.org/xpls/abs_ all.jsp?arnumber=6618913.  Goodfellow, Ian, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout net- works. In Proceedings of The 30th International Conference on Machine Learning, pp. 1319–1327, 2013a. URL http://jmlr.org/proceedings/papers/v28/goodfellow13.html.  Goodfellow, Ian J., Warde-Farley, David, Lamblin, Pascal, Dumoulin, Vincent, Mirza, Mehdi, Pascanu, Raz- van, Bergstra, James, Bastien, Fr´ed´eric, and Bengio, Yoshua. Pylearn2: a machine learning research library. arXiv:1308.4214 [cs, stat], August 2013b. URL http://arxiv.org/abs/1308.4214.  Goodfellow, Ian J., Mirza, Mehdi, Da, Xiao, Courville, Aaron, and Bengio, Yoshua. An empirical investiga- tion of catastrophic forgetting in gradient-based neural networks. In International Conference on Learning Representations, 2014. URL http://arxiv.org/abs/1312.6211.  Grauman, Kristen and Fergus, Rob. Learning binary hash codes for large-scale image search.  In Machine Learning for Computer Vision, pp. 49–87. Springer, 2013. URL http://link.springer.com/ chapter/10.1007/978-3-642-28661-2_3.  Hinton, Geoffrey E., Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im- proving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580 [cs], July 2012. URL http://arxiv.org/abs/1207.0580.  Jacobs, Robert A., Jordan, Michael I., Nowlan, Steven J., and Hinton, Geoffrey E. Adaptive mixtures of local experts. Neural computation, 3(1):79–87, 1991. URL http://www.mitpressjournals.org/doi/ abs/10.1162/neco.1991.3.1.79.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images. Computer Science Department, University of Toronto, Tech. Rep, 2009. URL http://citeseerx.ist.psu. edu/viewdoc/download?doi=10.1.1.222.9220&rep=rep1&type=pdf.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012. URL http://books. nips.cc/papers/files/nips25/NIPS2012_0534.pdf.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to doc- ument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. URL http://ieeexplore. ieee.org/xpls/abs_all.jsp?arnumber=726791.  Masci, Jonathan, Bronstein, Alex M., Bronstein, Michael M., and Schmidhuber, J¨urgen. Multimodal Similarity-  Preserving Hashing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(4), 2014a.  Masci, Jonathan, Bronstein, Alex M., Bronstein, Michael M., Sprechmann, Pablo, and Sapiro, Guillermo. In International Conference on Learning Representations, 2014b.  Sparse similarity-preserving hashing. URL http://arxiv.org/abs/1312.5479. arXiv: 1312.5479.  Pascanu, Razvan, Montufar, Guido, and Bengio, Yoshua. On the number of response regions of deep feed forward networks with piece-wise linear activations. arXiv:1312.6098 [cs], December 2013. URL http: //arxiv.org/abs/1312.6098. arXiv: 1312.6098.  Rifai, Salah, Dauphin, Yann, Vincent, Pascal, Bengio, Yoshua, and Muller, Xavier. The manifold tangent classiﬁer. In Advances in Neural Information Processing Systems, pp. 2294–2302, 2011. URL https: //papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf.  Salakhutdinov, Ruslan and Hinton, Geoffrey. Semantic hashing.  International Journal of Approximate Reasoning, 50(7):969–978, July 2009. doi: 10.1016/j.ijar.2008.11.006. URL http://linkinghub. elsevier.com/retrieve/pii/S0888613X08001813.  Srivastava, Rupesh K., Masci, Jonathan, Kazerounian, Sohrob, Gomez, Faustino, and Schmidhuber, J¨urgen. Compete to compute. In Advances in Neural Information Processing Systems, pp. 2310–2318, 2013. URL http://papers.nips.cc/paper/5059-compete-to-compute.  Strecha, Christop, Bronstein, Alex M., Bronstein, Michael M., and Fua, Pascal. LDAHash: Improved Matching  with Smaller Descriptors. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(1), 2012.  Van der Maaten, Laurens and Hinton, Geoffrey. Visualizing data using t-SNE. Journal of Machine Learning  Research, 9(11), 2008.  Zeiler, Matthew D., Ranzato, M., Monga, R., Mao, M., Yang, K., Le, Q. V., Nguyen, P., Senior, A., In IEEE International Vanhoucke, V., and Dean, J. On rectiﬁed linear units for speech processing. Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3517–3521. IEEE, 2013. URL http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6638312.  9  Published as a conference paper at ICLR 2015  A SUPPLEMENTARY MATERIALS A.1 EXTRA VISUALIZATIONS  (a) Trained LWTA layer.  (b) Trained Maxout layer.  Figure 8: 2-D visualization of submasks from the penultimate layer of 3 hidden layer LWTA and maxout networks on MNIST test set. Organization of submasks into distinct class speciﬁc clusters similar to ReL networks is observed.  (a) Untrained 1st LWTA layer.  (b) Untrained 1st ReL layer.  Figure 9: 2-D visualization of submasks obtained before training from the 1st (closest to the input) hidden layer of 3 hidden layer LWTA and ReL networks on MNIST test set.  B DATASET DESCRIPTIONS B.1 CIFAR-10 AND CIFAR-100 CIFAR-10 is a dataset of 32×32 color images of 10 classes split into a training set of size 50,000 and testing set of size 10,000 (6000 images per class) (Krizhevsky & Hinton, 2009). CIFAR-100 is a similar dataset of color images but with 100 classes and 600 images per class, making it more challenging. The models from Goodfellow et al. (2013a) for these dataset utilize preprocessing using global contrast normalization and ZCA whitening as well as data augmentation using translational and horizontal reﬂections. B.2 ILSVRC-2012 is a dataset of over a million natural images split into 1000 classes. An implementation of the network in Krizhevsky et al. (2012), with some minor differences (Donahue et al., 2013), is available publicly. For the experiments in this section, the penultimate-layer activations obtained using this model were downloaded from CloudCV Batra et al. (2013). The activations were obtained using the center-only option, meaning that only the activations for the central, 224×224 crop of each image were used.  IMAGENET (ILSVRC-2012)  10  0123456789012345678901234567890123456789Published as a conference paper at ICLR 2015  For each validation set example, 100 examples from the training set with the closest submasks were weighted by the inverse of the distance, then the classes with top-1 or top-5 weighted sums were returned as predictions.  C NOTE ON SIGMOIDAL NETWORKS In this paper we focused on improving our understanding of neural networks with locally competitive activation functions. We also obtained binary codes for efﬁcient retrieval directly from neural networks trained for clas- siﬁcation, but this was not the primary aim of our study. When this is the aim, we note here that it possible to use sigmoidal activation functions to obtain binary codes by thresholding the activation values after supervised or unsupervised (Salakhutdinov & Hinton, 2009) training. However it should be noted that:  • The thresholding is somewhat arbitrary and the best threshold needs to be selected by trying various values. For locally competitive networks, the binarization is natural and inherent to the nature of credit assignment in these networks. • Since sigmoidal networks are hard and slow to train, the approach of thresholding their activations is impractical for large datasets which are common for retrieval tasks. Locally competitive networks have been crucial for the successful application of neural networks to such datasets.  11  ",
1412.6334,2015,Leveraging Monolingual Data for Crosslingual Compositional Word Representations,"['Leveraging Monolingual Data for Crosslingual Compositional Word Representations', 'Hubert Soyer', 'Pontus Stenetorp', 'and Akiko Aizawa']",https://arxiv.org/pdf/1412.6334,"5 1 0 2     g u A 2 2         ] L C . s c [      4 v 4 3 3 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  LEVERAGING MONOLINGUAL DATA FOR CROSSLIN- GUAL COMPOSITIONAL WORD REPRESENTATIONS  Hubert Soyer National Institute of Informatics, Tokyo, Japan soyer@nii.ac.jp  Pontus Stenetorp∗ University of Tokyo, Tokyo, Japan pontus@stenetorp.se  Akiko Aizawa National Institute of Informatics, Tokyo, Japan aizawa@nii.ac.jp  ABSTRACT  In this work, we present a novel neural network based architecture for induc- ing compositional crosslingual word representations. Unlike previously proposed methods, our method fulﬁlls the following three criteria; it constrains the word- level representations to be compositional, it is capable of leveraging both bilingual and monolingual data, and it is scalable to large vocabularies and large quantities of data. The key component of our approach is what we refer to as a monolingual inclusion criterion, that exploits the observation that phrases are more closely se- mantically related to their sub-phrases than to other randomly sampled phrases. We evaluate our method on a well-established crosslingual document classiﬁ- cation task and achieve results that are either comparable, or greatly improve upon previous state-of-the-art methods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for the English to German and German to English sub-tasks respectively. The former advances the state of the art by 0.9% points of accuracy, the latter is an absolute improvement upon the previous state of the art by 7.7% points of accuracy and an improvement of 33.0% in error reduction.  1  INTRODUCTION  Dense vector representations (embeddings) of words and phrases, as opposed to discrete feature templates, have recently allowed for notable advances in the state of the art of Natural Language Processing (NLP) (Socher et al., 2013; Baroni et al., 2014). These representations are typically in- duced from large unannotated corpora by predicting a word given its context (Collobert & Weston, 2008). Unlike discrete feature templates, these representations allow supervised methods to readily make use of unlabeled data, effectively making them semi-supervised (Turian et al., 2010).  A recent focus has been on crosslingual, rather than monolingual, representations. Crosslingual representations are induced to represent words, phrases, or documents for more than one language, where the representations are constrained to preserve representational similarity or can be trans- formed between languages (Klementiev et al., 2012; Mikolov et al., 2013b; Hermann & Blunsom, 2014). In particular, crosslingual representations can be helpful for tasks such as translation or to leverage training data in a source language when little or no training data is available for a target language. Examples of such transfer learning tasks are crosslingual sentiment analysis (Wan, 2009) and crosslingual document classiﬁcation (Klementiev et al., 2012).  Mikolov et al. (2013b) induced language-speciﬁc word representations, learned a linear mapping be- tween the language-speciﬁc representations using bilingual word pairs and evaluated their approach  ∗Currently at the University College London.  1  Published as a conference paper at ICLR 2015  for single word translation. Klementiev et al. (2012) used automatically aligned sentences and words to constrain word representations across languages based on the number of times a given word in one language was aligned to a word in another language. They also introduced a dataset for crosslingual document classiﬁcation and evaluated their work on this task. Hermann & Blunsom (2014) intro- duced a method to induce compositional crosslingual word representations from sentence-aligned bilingual corpora. Their method is trained to distinguish the sentence pairs given in a bilingual corpus from randomly generated pairs. The model represents sentences as a function of their word representations, encouraging the word representations to be compositional. Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar.  from one or more of  these previous methods all suffer  However, three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al. (2014) all learn their representations using a word-level monolingual objective. This effectively means that compositionality is not en- couraged by the monolingual objective, which may be problematic when composing word repre- sentations for a phrase or document-level task. While the method of Hermann & Blunsom (2014) allows for arbitrary composition functions, they are limited to using sentence-aligned bilingual data and it is not immediately obvious how their method can be extended to make use of monolingual data. Lastly, while the method of Chandar A P et al. (2014) suffers from neither of the above issues, their method represents each sentence as a bag of words vector with the size of the whole vocabulary. This leads to computational scaling issues and necessitates a vocabulary cut-off which may hamper performance for compounding languages such as German.  The question that we pose is thus, can a single method  1. Constrain the word-level representations to be compositional.  2. Leverage both monolingual and bilingual data.  3. Scale to large vocabulary sizes without greatly impacting training time.  In this work, we propose a neural network based architecture for creating crosslingual compositional word representations. The method is agnostic to the choice of composition function and combines a bilingual training objective with a novel way of training monolingual word representations. This enables us to draw from a plethora of unlabeled monolingual data, while our method is efﬁcient enough to be trained using roughly seven million sentences in about six hours on a single-core desktop computer. We evaluate our method on a well-established document classiﬁcation task and achieve results for both sub-tasks that are either comparable or greatly improve upon the previous state of the art. For the German to English sub-task our method achieves 84.4% in accuracy, an error reduction of 33.0% in comparison to the previous state of the art.  2 MODEL  2.1  INDUCING CROSSLINGUAL WORD REPRESENTATIONS  For any task involving crosslingual word representations we distinguish between two kinds of errors  1. Transfer errors occur due to transferring representations between languages. Ideally, ex- pressions of the same meaning (words, phrases, or documents) should be represented by the same vectors, regardless of the language they are expressed in. The more different these representations are from language 1 (l1) to language 2 (l2), the larger the transfer error.  2. Monolingual errors occur because the word, phrase or document representations within the same language are not expressive enough. For example, in the case of classiﬁcation this would mean that the representations do not possess enough discriminative power for a classiﬁer to achieve high accuracy.  2  Published as a conference paper at ICLR 2015  The way to attain high performance for any task that involves crosslingual word representations is to keep both transfer errors and monolingual errors to a minimum using representations that are both expressive and constrained crosslingually.  2.2 CREATING REPRESENTATIONS FOR PHRASES AND DOCUMENTS  Following the work of Klementiev et al. (2012); Hermann & Blunsom (2014); Gouws et al. (2014) we represent each word as a vector and use separate word representations for each language. Like Hermann & Blunsom (2014), we look up the vector representations for all words of a given sentence in the corresponding lookup table and apply a composition function to transform these word vectors into a sentence representation. To create document representations, we apply the same composition function again, this time to transform the representations of all sentences in a document to a doc- ument representation. For the majority of this work we will make use of the addition composition function, which can be written as the sum of all word representations wi in a given phrase  a([w1, w2, · · · , wl]) =  l  X  i=1  wi  (1)  To give an example of another possible candidate composition function, we also use the bigram based addition (Bi) composition function, formalized as  b([w1, w2, · · · , wl]) =  l  X  i=2  tanh(wi−1, wi)  (2)  where the hyperbolic tangent (tanh) is wrapped around every word bigram to produce intermediate results that are then summed up. By introducing a non-linear function the Bi composition is no longer a bag-of-vectors function and takes word order into account.  Given that neither of the above composition functions involve any additional parameters, the only parameters of our model are in fact the word representations that are shared globally across all training samples.  2.3 OBJECTIVE  Following Klementiev et al. (2012) we split our objective into two sub-objectives, a bilingual objec- tive minimizing the transfer errors and a monolingual objective minimizing the monolingual errors for l1 and l2. We formalize the loss over the whole training set as  Ltotal =  Nbi  X  i=1  Lbi(vl1  i , vl2  i ) +  Nmono1  X  i=1  Lmono(xl1  i ) +  Nmono2  X  i=1  Lmono(yl2  i ) + λkθk2  (3)  i  from corpora in language 1 and Nmono2 sentences yl2  where Lbi is the bilingual loss for two aligned sentences, vi is a sample from the set of Nbi aligned sentences in language 1 and 2, Lmono is the monolingual loss which we sum over Nmono1 sentences xl1 from corpora in language 2. We learn the parameters θ, which represent the whole set of word representations for both l1 and l2. The pa- rameters are used in a shared fashion to construct sentence representations for both the monolingual corpora and the parts of the bilingual corpus corresponding to each language. We regularize θ using the squared euclidean norm and scale the contribution of the regularizer by λ.  i  Both objectives operate on vectors that represent composed versions of phrases and are agnostic to how a phrase is transformed into a vector. The objective can therefore be used with arbitrary composition functions.  An illustration of our proposed method can be found in Figure 1.  2.3.1 BILINGUAL OBJECTIVE  Given a pair of aligned sentences, sl1 vl1 1 and vl2  1 in l2, we ﬁrst compute their vector representations 1 using the composition function. Since the sentences are either translations of each other  1 in l1 and sl2  3  Published as a conference paper at ICLR 2015  German  Figure 1: An illustration of our method.  or at least very close in meaning, we require their vector representations to be similar and express this as minimizing the squared euclidean distance between vl1  1 . More formally, we write  1 and vl2  Lbi(vl1 , vl2 ) = kvl1 − vl2k2  (4)  for any two vector representations vl1 and vl2 corresponding to the sentences of an aligned transla- tion pair.  The bilingual objective on its own is degenerate, since setting the vector representations of all sen- tences to the same value poses a trivial solution. We therefore combine this bilingual objective with a monolingual objective.  2.3.2 MONOLINGUAL OBJECTIVE  The choice of the monolingual objective greatly inﬂuences the generality of models for crosslingual word representations. Klementiev et al. (2012) use a neural language model to leverage monolin- gual data. However, this does not explicitly encourage compositionality of the word representations. Hermann & Blunsom (2014) achieve good results with a noise-contrastive objective, discriminating aligned translation pairs from randomly sampled pairs. However, their approach can only be trained using sentence aligned data, which makes it difﬁcult to extend to leverage unannotated monolingual data. Gouws et al. (2014) introduced BilBOWA combining a bilingual objective with the Skip-Gram model proposed by Mikolov et al. (2013a) which predicts the context of a word given the word itself. They achieve high accuracy on the German → English sub-task of the crosslingual document classiﬁcation task introduced by Klementiev et al. (2012). Chandar A P et al. (2014) presented a bag-of-words auto-encoder model which is the current state of the art for the English → German sub-task for the same task. Both the auto-encoder based model and BilBOWA require a sentence- aligned bilingual corpus, but in addition are capable of leveraging monolingual data. However, due to their bag-of-words based nature, their architectures implicitly restrict how sentence representa- tions are composed from word representations.  We extend the idea of the noise-contrastive objective given by Hermann & Blunsom (2014) to the monolingual setting and propose a framework that, like theirs, is agnostic to the choice of compo- sition function and operates on the phrase level. However, our framework, unlike theirs, is able to leverage monolingual data. Our key novel idea is based on the observation that phrases are typically more similar to their sub-phrases than to randomly sampled phrases. We leverage this insight using the hinge loss as follows  Lmono(aouter, ainner, bnoise) = [max(0, m + kaouter  c  k2 − kaouter  c  Lmono(a) = Lmono(aouter, ainner, bnoise) − ainner {z len(ainner) len(aouter)  | + kaouter  − ainner  hinge loss  k2] ·  c  c  c  (5) (6)  − bnoise  c  k2) }  4  Published as a conference paper at ICLR 2015  Figure 2: Examples illustrating the inclusion criterion which we use to leverage monolingual text.  where m is a margin, aouter is a phrase sampled from a sentence, ainner is a sub-phrase of aouter and bnoise is a phrase extracted from a sentence that was sampled uniformly from the corpus. The start and end positions of both phrases and the sub-phrase were chosen uniformly at random within their context and constrained to guarantee a minimum length of 3 words. Subscript c denotes that a phrase has been transformed into its vector representation. We add kaouter k2 to the hinge loss to reduce the inﬂuence of the margin as a hyperparameter and to make sure that the we retain an error signal even after the hinge loss objective is satisﬁed. To compensate for differences in phrase and sub-phrase length we scale the error by the ratio between the number of words in the outer phrase and the inner phrase. Minimizing this objective captures the intuition stated above; a phrase should generally be closer to its sub-phrases, than to randomly sampled phrases.  − ainner  c  c  The examples in Figure 2 seek to further clarify this observation. In both examples, the blue area represents the outer phrase (aouter), the red area covers the inner sub-phrase (ainner), and the gray area marks a randomly selected phrase in a randomly sampled noise sentence. The inner workings of the monolingual inclusion objective only become clear when more than one example is considered. In Example 1, ainner is embedded in the same context as in Example 2, while in both examples aouter is contrasted with the same noise phrase. Minimizing the objective brings the representations of both likes to drink beer and likes to eat chips closer to the phrase they are embedded in and makes them less similar to the same noise sentence. Since in both examples the outer phrases are very similar, this causes likes to drink beer and likes to eat chips to be similar. While we picked idealized sentences for demonstration purposes, this relative notion still holds in practice to varying degrees depending on the choice of sentences.  In contrast to many recently introduced log-linear models, like the Skip-Gram model, where word vectors are similar if they appear as the center of similar word windows, our proposed objective, using addition for composition, encourages word vectors to be similar if they tend to be embedded in similar phrases. The major difference between these two formulations manifests itself for words that appear close or next to each other very frequently. These word pairs are not usually the center of the same word windows, but they are embedded together in the same phrases.  For example: the two word central context of “eat” is “to” and “chips”, whereas the context of “chips” would be “eat” and “when”. Using the Skip-Gram model this would cause “chips” and “eat” to be less similar, with “chips” probably being similar to other words related to food and “eat” being similar to other verbs. Employing the inclusion objective, the representations for “eat” and “chips” will end up close to each other since they tend to be embedded in the same phrases. This causes the word representations induced by the inclusion criterion to be more topical in nature. We hypothesize that this property is particularly useful for document classiﬁcation.  3 EXPERIMENTS  3.1 CROSSLINGUAL DOCUMENT CLASSIFICATION  Crosslingual document classiﬁcation constitutes a task where a classiﬁer is trained to classify doc- uments in one language (l1) and is later applied to documents in a different language (l2). This requires either transforming the classiﬁer itself to ﬁt the new language or transforming/sharing rep- resentations of the text for both languages. The crosslingual word and document representations  5  Published as a conference paper at ICLR 2015  Table 1: Statistics from the corpora used to induce crosslingual word representations listing type, frequency threshold for turning tokens into UNKs, the number of sentences, the number of tokens and the vocabulary size. The statistics were calculated on the preprocessed versions of the corpora.  EuroParl (EN) EuroParl (DE) Reuters (EN) Reuters (DE)  Type bilingual bilingual monolingual monolingual  UNK threshold 2 2 5 3  #sentences 1.66 million 1.66 million 4.5 million 0.9 million  #tokens 46 million 48 million 120 million 18 million  |V | 51,000 163,000 114,000 117,000  induced using the approach proposed in this work present an intuitive way to tackle crosslingual document classiﬁcation.  Like previous work, we evaluate our method on the crosslingual document classiﬁcation task intro- duced by Klementiev et al. (2012). The goal is to correctly classify news articles taken from the English and German sections of the RCV1 and RCV2 corpus (Lewis et al., 2004) into one of four categories: Economics, Government/Social, Markets, or Corporate. Maintaining the original setup, we train an averaged perceptron (Collins, 2002) for 10 iterations on representations of documents in one language (English/German) and evaluate its performance on representations of documents in the corresponding other language (German/English).  We use the original data and the original implementation of the averaged perceptron used by Klementiev et al. (2012) to evaluate the document representations created by our method. There are different versions of the training set of varying sizes, ranging from 100 to 10,000 documents, and the test sets for both languages contain 5,000 documents. Most related work only reports results using the 1,000 documents sized training set. Following previous work, we tune the hyperparameters of our model on held out documents in the same language that the model was trained on.  3.2  INDUCING CROSSLINGUAL WORD REPRESENTATIONS  To induce representations using the method proposed in this work, we require at least a bilingual corpus of aligned sentences. In addition, our model allows the representations to draw upon mono- lingual data from either or both languages. Like Klementiev et al. (2012) we choose EuroParl v7 (Koehn, 2005) as our bilingual corpus and leverage the English and German parts of the RCV1 and RCV2 corpora as monolingual resources. To avoid a testing bias, we exclude all documents that are part of the crosslingual classiﬁcation task. We detect sentence boundaries using pre-trained models of the Punkt tokenizer (Kiss & Strunk, 2006) shipped with NLTK1 and perform tokenization and lowercasing with the scripts deployed with the cdec decoder2. Following Turian et al. (2010) we re- move all English sentences (and their German correspondences in EuroParl) that have a ratio of less than 0.9. This affects mainly headlines and reports with numbers. In total it reduces the number of sentences in EuroParl by about 255, 000 and the English part of the Reuters corpus by about 8 million. Since German features more upper case characters than English we set the cut- off ratio to 0.7, which reduces the number of sentences by around 620, 000. Further, we replace words that occur less than a certain threshold by an UNK token. Corpus statistics and thresholds are reported in Table 1.  nonlowercase  lowercase  We initialize all word representations with noise samples from a Gaussian with µ = 0, σ = 0.1 and optimize them in a stochastic setting to minimize the objective deﬁned in Equation 3. To speed up the convergence of training we use AdaGrad (Duchi et al., 2011). We tuned all hyperparameters of our model and explored learning rates around 0.2, mini-batch sizes around 40,000, hinge loss margins around 40 (since our vector dimensionality is 40) and λ (regularization) around 1.0. We trained all versions that use the full monolingual data for 25 iterations (= 25 × 4.5 million samples) and the versions only involving bilingual data for 100 iterations on their training sets. Training our model3, implemented in a high-level, dynamic programming language (Bezanson et al., 2012),  1http://www.nltk.org/ 2http://www.cdec-decoder.org/ 3Our implementation is available at https://github.com/ogh/binclusion  6  Published as a conference paper at ICLR 2015  Table 2: Results for our proposed models, baselines, and related work. All results are reported for a training set size of 1,000 documents for each language. We refer to our proposed method as Binclusion.  Training Data  EN → DE DE → EN  Method Machine Translation Glossed Majority Class I-Matrix ADD BAE-cr BAE-cr BAE-cr BilBOWA Binclusion Binclusion Binclusion Binclusion (reduced vocabulary) Binclusion Binclusion (Bi)  (Klementiev et al., 2012) EuroFullReuters  (Hermann & Blunsom, 2014) Euro500k (Chandar A P et al., 2014) Euro500k (Chandar A P et al., 2014) Euro500kReuters (Chandar A P et al., 2014) EuroFullReuters  (Gouws et al., 2014) Euro500k Euro500k EuroFull Euro500kReuters Euro500kReuters EuroFullReuters EuroFullReuters  68.1 65.1 46.8 77.6 83.7 86.1 87.9 91.8 86.5 86.8 87.8 92.7 92.6 90.8 89.8  67.4 68.6 46.8 71.1 71.4 68.8 76.7 74.2 75.0 76.7 75.7 84.4 82.8 79.5 80.1  for the largest set of data takes roughly six hours on a single-core desktop computer. This can be compared to for example Chandar A P et al. (2014) which train their auto-encoder model for 3.5 days.  4 RESULTS  4.1 CROSSLINGUAL DOCUMENT CLASSIFICATION  We compare our method to various architectures introduced in previous work. As these meth- ods differ in their ability to handle monolingual data, we evaluate several versions of our model using different data sources and sizes for training. Also, we follow the lines of previous work and use 40-dimensional word representations. We report results when using the ﬁrst 500,000 sen- tence pairs of EuroParl (Euro500k), the full EuroParl corpus (EuroFull), the ﬁrst 500,000 sentence pairs of EuroParl and the German and English text from the Reuters corpus as monolingual data (Euro500kReuters), and one version using the full EuroParl and Reuters corpus (EuroFullReuters). Table 2 shows results for all these conﬁgurations. The result table includes previous work as well as the Glossed, the machine translation and the majority class baselines from Klementiev et al. (2012).  Our method achieves results that are comparable or improve upon the previous state of the art for all dataset conﬁgurations. It advances the state of the art for the EN → DE sub-task by 0.9% points of accuracy and greatly outperforms the previous state of the art for the DE → EN sub-task, where it yields an absolute improvement of 7.7% points of accuracy. The latter corresponds to an error reduction of 33.0% in comparison to the previous state of the art. An important observation is that including monolingual data is strongly beneﬁcial for the classiﬁca- tion accuracy. We found increases in performance to 80.6% for DE → EN and 88.6% accuracy for EN → DE, even when using as little as 5% of the monolingual data. We hypothesize that the key cause of this effect is domain adaptation. From this observation it is also worth pointing out that our method is on par with the previous state of the art for the DE → EN sub-task using no monolingual training data and would improve upon it using as little as 5% of the monolingual data. To show that our method achieves high accuracy even with a reduced vocabulary, we discard representations for infrequent terms and report results using our best setup with the same vocabulary size as Klementiev et al. (2012).  7  Published as a conference paper at ICLR 2015  Table 3: German nearest neighbors for English words that only appear in the monolingual data.  English  German  soybeans mais alkoholherstellung silomais genmais gluten  forex drachme liquidit¨atsfalle bankenliquidit¨at abnutzung pf¨andung  s&p ratings ratingindustrie ratingbranche ratingstiftung kreditratingagenturen minderheitenaktion¨aren  stockholders aktion¨arsschutz minderheitenaktion¨are aktion¨arsrechte aktion¨are  Table 4: English words and their nearest neighbors in the induced space, demonstrating the topical nature of the word representations.  Query  Neighbors  intel pentium  kabul taliban talibans microprocessor taleban masood microprocessors dostum netscape  ibm  transport trafﬁc transporting transports dockworkers transportation  ﬂy air ﬂying ﬂight airspace naval  4.2  INTERESTING PROPERTIES OF THE INDUCED CROSSLINGUAL WORD REPRESENTATIONS  For a bilingual word representation model that uses monolingual data, the most difﬁcult cases to resolve are words appearing in the monolingual data, but not in the bilingual data. Since the model does not have any kind of direct signal regarding what translations these words should correspond to, their location in the vector space is entirely determined by how the monolingual objective arranges them. Therefore, looking speciﬁcally at these difﬁcult examples presents a good way to get an impression of how well the monolingual and bilingual objective complement each other.  In Table 3, we list some of the most frequently occurring words that are present in the monolingual data but not in the bilingual data. The nearest neighbors are topically strongly related to their cor- responding queries. For example, the credit-rating agency Standard & Poor’s (s&p) is matched to rating-related words, soybeans is proximal to crop and food related terms, forex features a list of currency related terms, and the list for stockholders, includes aktion¨are, its correct German transla- tion. This speaks strongly in favor of how our objectives complement each other, even though these words were only observed in the monolingual data they relate sensibly across languages.  To convey an impression of how the induced representations behave, not interlingually, but within the same language, we list some examples in Table 4. The semi-conductor chip maker intel, is very close to IT-related companies like ibm or netscape and also to microprocessor-related terms. For the verb ﬂy, the nearest neighbors not only include forms like ﬂying, but also related nouns like airspace or air, underlining the topical nature of our proposed objective.  5 CONCLUSION AND FUTURE WORK  In this work we introduced a method that can induce compositional crosslingual word representa- tions while scaling to large datasets. Our novel approach for learning monolingual representations integrates naturally with our bilingual objective and allows us to make use of sentence-aligned bilin- gual corpora as well as monolingual data. The method is agnostic to the choice of composition func- tion, enabling more complex (e.g. preserving word order information) ways to compose phrase rep- resentations from word representations. For crosslingual document classiﬁcation (Klementiev et al., 2012) our models perform comparably or greatly improve upon previously reported results.  To increase the expressiveness of our method we plan to investigate more complex composition functions, possibly based on convolution to preserve word order information. We consider the mono- lingual inclusion objective worthy of further research on its own and will evaluate its performance in comparison to related methods when learning word representations from monolingual data.  8  Published as a conference paper at ICLR 2015  ACKNOWLEDGEMENTS  This work was supported by the Data Centric Science Research Commons Project at the Research Organization of Information and Systems and by the Japan Society for the Promotion of Science KAKENHI Grant Number 13F03041.  REFERENCES Baroni, Marco, Dinu, Georgiana, and Kruszewski, Germ´an. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL, pp. 238–247, 2014.  Bezanson, Jeff, Karpinski, Stefan, Shah, Viral B., and Edelman, Alan.  Julia: A Fast Dynamic  Language for Technical Computing. arXiv, abs/1209.5145, September 2012.  Chandar A P, Sarath, Lauly, Stanislas, Larochelle, Hugo, Khapra, Mitesh, Ravindran, Balaraman, Raykar, Vikas C, and Saha, Amrita. An Autoencoder Approach to Learning Bilingual Word Representations. In NIPS, pp. 1853–1861. 2014.  Collins, Michael. Discriminative Training Methods for Hidden Markov Models: Theory and Exper-  iments with Perceptron Algorithms. In EMNLP, pp. 1–8, 2002.  Collobert, Ronan and Weston, Jason. A Uniﬁed Architecture for Natural Language Processing:  Deep Neural Networks with Multitask Learning. In ICML, pp. 160–167, 2008.  Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive Subgradient Methods for Online Learning  and Stochastic Optimization. JMLR, 12:2121–2159, 2011.  Gouws, Stephan, Bengio, Yoshua, and Corrado, Greg. BilBOWA: Fast Bilingual Distributed Repre-  sentations without Word Alignments. arXiv, abs/1410.2455, 2014.  Hermann, Karl Moritz and Blunsom, Phil. Multilingual Models for Compositional Distributed Se-  mantics. In ACL, pp. 58–68, 2014.  Kiss, Tibor and Strunk, Jan. Unsupervised Multilingual Sentence Boundary Detection. CL, 32(4):  485–525, 2006.  Klementiev, Alexandre, Titov, Ivan, and Bhattarai, Binod. Inducing Crosslingual Distributed Rep-  resentations of Words. In COLING, pp. 1459–1474, 2012.  Koehn, Philipp. Europarl: A parallel corpus for statistical machine translation.  volume 5, pp. 79–86, 2005.  In MT summit,  Lewis, David D., Yang, Yiming, Rose, Tony G., and Li, Fan. RCV1: A New Benchmark Collection  for Text Categorization Research. JMLR, 5:361–397, 2004.  Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efﬁcient Estimation of Word Repre-  sentations in Vector Space. In ICLR Workshop, 2013a.  Mikolov, Tomas, Le, Quoc V., and Sutskever, Ilya. Exploiting Similarities among Languages for  Machine Translation. arXiv, abs/1309.4168, 2013b.  Socher, Richard, Perelygin, Alex, Wu, Jean, Chuang, Jason, Manning, Christopher D., Ng, Andrew, and Potts, Christopher. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In EMNLP, pp. 1631–1642, 2013.  Turian, Joseph, Ratinov, Lev-Arie, and Bengio, Yoshua. Word representations: A simple and general  method for semi-supervised learning. In ACL, pp. 384–394, 2010.  Wan, Xiaojun. Co-training for Cross-lingual Sentiment Classiﬁcation. In ACL, pp. 235–243, 2009.  9  ",
1412.6564,2015,Move Evaluation in Go Using Deep Convolutional Neural Networks,"['Move Evaluation in Go Using Deep Convolutional Neural Networks', 'Chris Maddison', 'Aja Huang', 'Ilya Sutskever', 'and David Silver']",https://arxiv.org/pdf/1412.6564,"5 1 0 2    r p A 0 1         ]  G L . s c [      2 v 4 6 5 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  MOVE EVALUATION IN GO USING DEEP CONVOLUTIONAL NEURAL NETWORKS  Chris J. Maddison University of Toronto cmaddis@cs.toronto.edu  Aja Huang1, Ilya Sutskever2, David Silver1 Google DeepMind1, Google Brain2 {ajahuang,ilyasu,davidsilver}@google.com  ABSTRACT  The game of Go is more challenging than other board games, due to the difﬁculty of constructing a position or move evaluation function. In this paper we investi- gate whether deep convolutional networks can be used to directly represent and learn this knowledge. We train a large 12-layer convolutional neural network by supervised learning from a database of human professional games. The network correctly predicts the expert move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used di- rectly to play games of Go, without any search, it beat the traditional-search pro- gram GnuGo in 97% of games, and matched the performance of a state-of-the-art Monte-Carlo tree search that simulates two million positions per move.  1  INTRODUCTION  The most frequently cited reason for the difﬁculty of Go, compared to games such as Chess, Scrabble or Shogi, is the difﬁculty of constructing an evaluation function that can differentiate good moves from bad in a given position. The combination of an enormous state space of 10170 positions, combined with sharp tactics that lead to steep non-linearities in the optimal value function, has led many researchers to conclude that representing and learning such a function is impossible (M¨uller, 2002). In previous years, the most successful methods have sidestepped this problem altogether using Monte-Carlo search, which dynamically evaluates a position through random sequences of self-play. Such programs have led to strong amateur level performance, but a considerable gap still remains between top professional players and the strongest computer programs. The majority of recent progress has been due to increased quantity and quality of prior knowledge, which is used to bias the search towards more promising states in both the search tree and during rollouts (Coulom, 2007; Gelly & Silver, 2011; Enzenberger et al., 2010; Baudiˇs & Gailly, 2012; Huang et al., 2011), and it is widely believed that this knowledge is the major bottleneck towards further progress (Huang & M¨uller, 2013). However, this knowledge again is ultimately compiled into an evaluation function or distribution that expresses a preference over moves. In this paper we address these fundamental questions of representation and learning of Go knowl- edge, by using a deep convolutional neural network (CNN). Although CNNs have previously been applied to the game of Go, with modest success (Schraudolph et al., 1994; Enzenberger, 1996; Sutskever & Nair, 2008), previous architectures have typically been limited to one hidden layer of relatively small size, and have not exploited recent advances in computational power. In this paper we use much deeper and larger CNNs of 12 hidden layers and several billion connections to repre- sent and learn Go knowledge. We ﬁnd that this increase in depth and size leads to a qualitative jump in performance, suggesting that contrary to previous beliefs, a strong move evaluation function for Go can indeed be represented and learnt by such architectures. We focus on a supervised learning setup, in which the network is trained to predict expert human moves, using a large database of professional 19 × 19 Go games. The predictive accuracy of the  1  Published as a conference paper at ICLR 2015  CNN on a held-out set of positions reaches 55%, which is a signiﬁcant improvement over the 35% and 39% predictive accuracy reported for some of the strongest Go programs, and comparable to the performance of the 6 dan author on the same data set. Furthermore, when the CNN was used to play games by directly selecting the move recommended by the network output, without any search, it equalled the performance of state-of-the-art Monte-Carlo search programs, such as Pachi (Baudiˇs & Gailly, 2012), that are given 10,000 rollouts per move (i.e., programs that combine handcrafted or shallow prior knowledge with a search that simulates two million positions), and the ﬁrst strong Monte-Carlo search program MoGo with 100,000 rollouts per move (Gelly & Silver, 2007). In addition, direct move selection using the CNN beat GnuGo (a traditional search program) in 97% of games.1 Finally, we demonstrate that the Go knowledge embodied by the CNN can be effectively combined with Monte-Carlo tree search, by using a delayed prior knowledge procedure. In this approach, the CNN is evaluated asynchronously on a GPU, and results are incorporated into the main search procedure once available. Using 100,000 rollouts per move, the overall search defeats the raw CNN in 87% of games.  2 PRIOR WORK  Convolutional neural networks have a long history in the game of Go. Schraudolph Schraudolph et al. (1994) trained a simple CNN (exploiting rotational, reﬂectional, and colour inversion sym- metries) to predict ﬁnal territory, by reinforcement learning from games of self-play. The resulting program beat a simplistic handcrafted program called Wally. NeuroGo (Enzenberger, 1996) used a more sophisticated architecture to predict ﬁnal territory, eyes, and connectivity, again exploiting symmetries; and used a connectivity pathﬁnder to propagate information across weakly connected groups of stones. Enzenberger’s program also used reinforcement learning from self-play. When combined with an alpha-beta search, NeuroGo equalled the performance of GnuGo on 9 × 9 Go, and reached around 13 kyu on 19 × 19 Go. Sutskever & Nair (2008) applied convolutional net- works to supervised learning of expert moves, but using a small 1 hidden layer CNN; this matched the state-of-the-art prediction performance, achieving 34.6% accuracy, but this was not sufﬁcient to play Go at any reasonable level. The most successful current programs in Go are based on Monte-Carlo tree search (Kocsis & Szepesv´ari, 2006). The basic algorithm was augmented in MoGo to use prior knowledge to bootstrap value estimates in the search tree (Gelly & Silver, 2007); and to use abstractions over subtrees to accelerate the search (Gelly & Silver, 2011). The strongest current programs such as CrazyStone ap- ply supervised learning to construct a move selection policy; this is then used to bias the exploration during search; a faster policy is also learned that selects moves during rollouts (Coulom, 2007). CrazyStone achieved a 35% move prediction accuracy by extracting a large database of common patterns from expert games, and combining them into a large linear softmax. Recent work in image recognition has demonstrated considerable advantages of deep convolutional networks over alternative architectures. Krizhevsky et al. (2012) were the ﬁrst to achieve a very large performance gain with large and deep convolutional neural networks over traditional computer vi- sion systems. Improved convolutional neural network architectures (primarily in the form of deeper networks) (Simonyan & Zisserman, 2014) provided another substantial improvement, culminating with Szegedy et al. (2014), who reduced the error rate of Krizhevsky et al. (2012) from 15.3% top-5 error to 7.0%. The power and generality of large and deep convolutional neural networks suggests that they may do well on other “visual” domains, such as computer Go.  3 DATA  The dataset used in this work comes from the KGS Go Server. It consists of sequences of board positions st for complete games played between humans of varying rank. Board state information includes the position of all stones on the 19x19 board and the sequence allows one to determine the  1 Since we performed this research, we have learned that Clark & Storkey (2014) independently adopted a similar approach using a smaller 8-layer CNN to achieve 44% move prediction accuracy; and defeated GnuGo in 86% of games.  2  Published as a conference paper at ICLR 2015  Feature Black / white / empty Liberties Liberties after move Legality Turns since Capture size Ladder move KGS rank  Planes Description Stone colour  3 4 Number of liberties (empty adjacent points) 6 Number of liberties after this move is played 1 Whether point is legal for current player 5 How many turns since a move was played 7 How many opponent stones would be captured 1 Whether a move at this point is a successful ladder capture 9 Rank of current player  Table 1: Features used as inputs to the CNN.  sequence of moves; a move at is encoded as a 1 of 361 indicator for each position on the 19x19 board. We collected 29.4 million board-state next-move pairs (st, at) corresponding to 160,000 games. Each position st was preprocessed into a set of 19 × 19 feature planes φ(st), that serve as input to the neural network. The features that we use come directly from the raw representation of the game rules (stones, liberties, captures, legality, turns since). In addition, we have one simple tactical feature representing a basic common pattern in Go known as ladders; in practice this adds a small performance beneﬁt, but the results that we report would be qualitatively similar even without these features. Many of the features are split into multiple planes of binary values, for example in the case of liberties there are separate binary features representing whether each intersection has 1 liberty, 2 liberties, 3 liberties, >= 4 liberties. The feature planes are listed in Table 1.2 Finally, we used the following minor innovation. Our dataset consists of games from players of different strengths. Speciﬁcally, the KGS data contains more games by lower dan players, and fewer games by higher dan players. As a result, a naive approach to training on the KGS data will result in a network that primarily imitates weaker players. Alternatively, training only on games by stronger players would result in a massive reduction of training data. To mitigate this, we provided the network with an additional global inputs indicating the player’s rank. Speciﬁcally we add 9 feature planes each indicating a speciﬁc rank. This is like a 1 of 9 encoding that represents the strength of the current player. That is, if the network is learning to predict a move made by a d dan player, the dth rank feature plane is ﬁlled with 1s and the remaining 8 planes are ﬁlled with 0s. This has the effect of providing a dynamic bias to the network that depends on rank. Because every Go game is symmetric under reﬂections and rotations, we augmented the dataset by sampling uniformly from one of the 8 symmetric boards as we ﬁlled minibatches in gradient descent. The dataset was split into a training set of 27.4 million board-state next-move pairs and a test set of 2 million. This split was done before shufﬂing, so this corresponds to a test set with distinct games.  4 ARCHITECTURE & TRAINING  In this section we describe the precise network architecture and the details of the training procedure. We used a deep convolutional neural network with 12 weight matrices for each of 12 layers and rectiﬁed linear non-linearities. The ﬁrst hidden layer’s ﬁlters were of size 5×5 and the remainder were of size 3×3, with a stride of 1. Every layer operated on a 19× 19 input space, with no pooling; outputs were zero-padded back up up to 19 × 19. The number of ﬁlters in each layer ranged from 64 to 192. In addition to convolutions, we also used position-dependent biases (following Sutskever & Nair (2008)). Our best model has 2.3 million parameters, 630 million connections, and 550,000 hidden units. The output layer of the CNN was also convolutional with position dependent biases, but with only two ﬁlters. Each produced a 19 × 19 plane, corresponding to inputs to two softmax distributions of size 361. The ﬁrst softmax is the distribution over the next move if it is the black player’s turn, and  2Due to the computational cost of running extensive experiments, it is possible that some of these features  are unnecessary or redundant.  3  Published as a conference paper at ICLR 2015  the second softmax is the distribution over the next move if it is the white player’s move. Although both players may often prefer the same move, in general the optimal policy may select different moves for each player. We also experimented with weight symmetries Schraudolph et al. (1994). Given that the board is symmetric, it makes sense to force the ﬁlters and biases to be rotationally and reﬂectionally symmet- ric, by aggregating weight updates over the 8-fold symmetry group between connections. This type of symmetry is stronger than the symmetric data augmentation described above, since it enforces local symmetry of all ﬁlters at all locations on the board, not just global symmetry of the entire board. For training the network, we used asynchronous stochastic gradient descent (Dean et al., 2012) with 50 replicas each on its own GPU. All parameters were initialized randomly from a uniform[-0.05, 0.05]. Each replica was trained for 25 epochs with a batchsize of 128, a ﬁxed learning rate of 0.128 normalized by batchsize, and no momentum. The network was then ﬁne-tuned on a single GPU with vanilla SGD for 3 epochs with an annealed learning rate, beginning at half the learning rate for the asynchronous setting and halved again every epoch. After augmenting the dataset with random symmetries overﬁtting was very minor — our 10 layer network overﬁt by under 1% achieving 55% on the training set and 54.5% on the test set. Even at the end of training errors on the test set did not increase. This suggests that we are currently operating in an underﬁtting regime suggesting that further improvement is possible. All reported accuracies are on a held out test set.  5 RESULTS  5.1  INVESTIGATION OF WEIGHT SYMMETRIES  We evaluated the effect of weight symmetries on a smaller CNN with 3 and 6 layers respectively. These networks were trained on a reduced feature set, excluding rank, liberties after move, capture size, ladder move, and only including a history of one move. The results are given in the table below:  model 3 layer, 64 ﬁlters 3 layer, 64 ﬁlters, symmetric 6 layer, 192 ﬁlters 6 layer, 192 ﬁlters, symmetric  % Accuracy  43.3 44.3 49.6 49.4  These results suggest that, perhaps surprisingly, weight symmetries have a strong effect on move prediction for small and shallow networks, but the effect appeared to disappear completely in larger and deeper networks.  5.2 ACCURACY AND PLAYING STRENGTH  To understand how the performance depends on network depth, we trained several networks of different depths. Each CNN used the same architecture as described above, except that the number of 3 × 3 layers was restricted to 3, 6, 10 and 12 respectively. We measured the prediction accuracy on the test set, and also the playing strength of the CNN when it was used to directly select moves. This was achieved by inputting the current position into the network, and selecting the action with maximum probability in the softmax output for the current player. Unless otherwise speciﬁed the KGS rank feature was set to its maximum setting. Performance was evaluated against the benchmark program GnuGo 3.8, running at its highest level 10. Comparisons are given with reported values for the 3 dan Monte-Carlo search program Aya3; simultaneously published results on a somewhat shallower CNN Clark & Storkey (2014)4; and also with the prediction accuracy of a 6 dan human (the second author) on randomly sampled positions  3http://computer-go.org/pipermail/computer-go/2014-December/007018.html 4It should be noted that Clark & Storkey (2014) did not use the highly-predictive turns since feature, because they believed that it would hurt the network’s play. This is an interesting hypothesis, which this work does not address.  4  Published as a conference paper at ICLR 2015  Figure 1: Probability that the expert’s move is within the top-n predictions of the network. The 10 layer CNN was omitted for clarity, but it’s performance is only slightly worse than 12 layer. Note y-axis begins at 0.30.  from the test set. All games were scored using Chinese rules, refereed by GnuGo; duplicate games were excluded from results. It is apparent from the results that larger and deeper networks have qualitatively better performance than shallow networks, reaching 97% winning rate against GnuGo for a large 12-layer network compared to 3.4% for a small 3-layer network. Furthermore, the accuracy on the supervised learning task is clearly strongly correlated with playing performance, demonstrating that the knowledge learnt by the network generalises effectively to the real task of evaluating moves.  Size % Accuracy % Wins vs. GnuGo  Depth 16 ﬁlters 3 layer 128 ﬁlters 3 layer 128 ﬁlters 6 layer 10 layer 128 ﬁlters 128 ﬁlters 12 layer 8 layer (Clark & Storkey, 2014)4 ≤ 64 ﬁlters Aya 2014 Human 6 dan  37.5 48.0 51.2 54.5 55.2 44.4 38.8 52 ±5.8  stderr 3.4 ± 1.1 61.8 ± 2.6 84.4 ± 1.9 94.7 ± 1.2 97.2 ± 0.9 86 ± 2.5 6 ± 1.0  100  It is also valuable to know that the correct move is within the network’s n most conﬁdent predictions. If n can be kept small, then this knowledge can be used to reduce the program’s effective search space. We ﬁnd that the top-n performance of our network is quite strong; in particular, the network is able to predict the correct expert move 94% of the time when n = 10. Next, we compared how the CNN performed when asked to imitate players of different strengths. We used the same CNN, trained on KGS data of all ranks, and asked it to select moves as if it was playing according to a speciﬁed rank. The opponent was a ﬁxed 10 layer, 128 ﬁlter CNN trained without the KGS rank feature. The results clearly show that the network plays signiﬁcantly better when it is asked to imitate a stronger player.  KGS rank % wins vs. 10-layer CNN stderr 49.2 ± 3.6 1 dan 60.1 ± 1.6 5 dan 67.9 ± 5.0 9 dan  Finally, we evaluated the overall strength of the 12-layer CNN when used for move selection, by playing against several publicly available benchmark programs. All programs were played at the strongest available settings, and a ﬁxed number of rollouts per move, as speciﬁed in the table.  5  151015202530354045n0.30.40.50.60.70.80.91.0% accuracy12-layer CNN6-layer CNN3-layer CNN3-layer, 16-filters CNNPublished as a conference paper at ICLR 2015  Opponent Rollouts per move Games won by CNN stderr 97.2 ± 0.9 GnuGo 45.9 ± 4.5 MoGo 11.0 ± 2.1 Pachi 12.5 ± 5.8 Fuego 47.4 ± 3.7 Pachi 23.3 ± 7.8 Fuego  100,000 100,000 100,000 10,000 10,000  The neural network is considerably stronger than the traditional search-based program GnuGo, and its performance is on a par with MoGo with 100,000 rollouts per move (Gelly & Silver, 2007), and Pachi running a somewhat reduced search of 10,000 rollouts per move (a search that visits It wins more than 10% of games against Fuego (latest svn approximately 2 million positions). revision 1966) (Enzenberger et al., 2010) and Pachi 10.99 playing at a strong level (using 100,000 rollouts per move over 16 threads).5  6 SEARCH  The overarching goal of this work is to build a strong Go playing program. To this end, we attempted to integrate our move prediction network with Monte Carlo Tree Search (MCTS). Combining MCTS with a large deep neural network is far from trivial, since the CNN is slower than the natural speed of the search, and it is not feasible to evaluate every node with the neural network. The 12-layer network takes 0.15s to evaluate a minibatch of size 128.6 We address this problem by using asynchronous node evaluation. In asynchronous node evaluation, MCTS builds its search tree and tracks the new nodes that are added into the search tree. When the number of new nodes equals the minibatch size, all these new positions are submitted to the CNN for evaluation on a GPU. The GPU computes the move recommendations, while the search continues in parallel. Once the GPU computation is complete, the prior knowledge in the new nodes is updated to contain move evaluations from the CNN. The network evaluates the nodes in a FIFO order, in order to maximally inﬂuence the search tree. By using a single machine with Intel® Xeon® CPU E5-2643 v2 @ 3.50GHz and GeForce GTX Titan Black GPU, we are able to maintain a MCTS search at approximately 47,000 rollouts per second, without dropping CNN evaluations. However, it should be noted that the performance of asynchronous node evaluation is signiﬁcantly less than a fully synchronous and serial implementation, since new information from the search is only utilised after a signiﬁcant lag (around 0.15s in our case), due to the GPU computation. In addition, the MCTS engine utilised standard heuristics for computer Go: RAVE (Gelly & Silver, 2011), a UCT exploration strategy similar to Chaslot et al. (2008), and very simple rollouts based solely on 3 × 3 patterns (Huang et al., 2011). We measured the performance of the search-based program by playing games between the 12-layer CNN with MCTS, and a baseline 12-layer CNN without any search. Using 100,000 rollouts per move, the search-based program beats the baseline CNN in 87% of games.  Rollouts per move % wins against baseline 100,000 10,000  stderr 86.7 ± 3.5 67.6 ± 2.6  7 DISCUSSION  In this work, we showed that large deep convolutional neural networks can predict the next move made by Go experts with an accuracy that exceeds previous methods by a large margin, approxi- mately matching human performance. Furthermore, this predictive accuracy translates into much  5The 8-layer network of Clark & Storkey (2014) won 12% of games against the older version Fuego 1.1 at 10 seconds per move on 2 × 1.6 GHz cores. We tested our 12-layer CNN against Fuego 1.1 at 5 and 10 seconds per move on 2 × 3.1GHz cores, winning 56% and 33% respectively.  6Reducing the minibatch size does not signiﬁcantly speed up end-to-end computation time in our GPU  implementation.  6  Published as a conference paper at ICLR 2015  Figure 2: A game played between the 12-layer CNN (without any search) and Fuego (using 100k roll- outs/move). The CNN plays white.  stronger move evaluation and playing strength than has previously been possible. Without any search, the network is able to outperform traditional search based programs such as GnuGo, and compete with state-of-the-art MCTS programs such as Pachi and Fuego. In Figure 2 we present a sample game played by the 12-layer CNN (with no search) versus Fuego (searching 100K rollouts per move) which was won by the neural network player. It is clear that the neural network has implicitly understood many sophisticated aspects of Go, including good shape (patterns that maximise long term effectiveness of stones), Fuseki (opening sequences), Joseki (corner patterns), Tesuji (tactical patterns), Ko ﬁghts (intricate tactical battles involving repeated recapture of the same stones), territory (ownership of points), and inﬂuence (long-term potential for territory). It is remarkable that a single, uniﬁed, straightforward architecture can master these elements of the game to such a degree, and without any explicit lookahead. On the other hand, we note that the network still has weaknesses: notably it sometimes fails to under- stand the global picture, behaving as if the life and death status of large groups has been incorrectly assessed. Interestingly, it is precisely these global aspects of the game for which Monte-Carlo search excels, suggesting that these two techniques may be largely complementary. We have provided a preliminary proof-of-concept that MCTS and deep neural networks may be combined effectively. It appears that we now have two core elements that scale effectively with increased computational re- source: scalable planning, using Monte-Carlo search; and scalable evaluation functions, using deep neural networks. In the future, as parallel computation units such as GPUs continue to increase in performance, we believe that this trajectory of research will lead to considerably stronger programs than are currently possible.  7  123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657586061636466676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137139140142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178180181182183184185186187189192193196198199200201203204206207208209210211214216217219221222224225226227228229231232233234235237238239240242243244246247248249250251252253254255256257259261265266268270271272273274275276277278279280281282283285286287288289290292294295296297298299300301595162506551138128141133179771881761901281917719417619513319777202168205199212168213128215199218168220176223772301762362212417724519925816826017626221626377264219267199269208284168291199293168301end ()B+Resign(komi: 7.5)12layerCNN (?)Fuego100k (?)Powered by TCPDF (www.tcpdf.org)Published as a conference paper at ICLR 2015  REFERENCES Baudiˇs, Petr and Gailly, Jean-loup. Pachi: State of the art open source go program. In Advances in  Computer Games, pp. 24–38. Springer, 2012.  Chaslot, Guillaume M. J-B., Winands, Mark H. M., van den Herik, H. Jaap, Uiterwijk, Jos W. H. M., and Bouzy, Bruno. Progressive strategies for Monte-Carlo tree search. New Mathematics and Natural Computation, 4:343–357, 2008. doi: 10.1142/S1793005708001094.  Clark, Christopher and Storkey, Amos. Teaching deep convolutional neural networks to play Go.  arXiv preprint arXiv:1412.3409, 2014.  Coulom, R´emi. Efﬁcient selectivity and backup operators in Monte-Carlo tree search. In Computers  and games, pp. 72–83. Springer, 2007.  Dean, Jeffrey, Corrado, Greg, Monga, Rajat, Chen, Kai, Devin, Matthieu, Mao, Mark, aure- lio Ranzato, Marc’, Senior, Andrew, Tucker, Paul, Yang, Ke, Le, Quoc V., and Ng, An- drew Y. Large scale distributed deep networks. In Pereira, F., Burges, C.J.C., Bottou, L., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 25, pp. 1223–1231. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/ 4687-large-scale-distributed-deep-networks.pdf.  Enzenberger, Markus. The integration of a priori knowledge into a Go playing neural network. URL:  http://www. markus-enzenberger. de/neurogo. html, 1996.  Enzenberger, Markus, M¨uller, Martin, Arneson, Broderick, and Segal, R. Fuego - an open-source framework for board games and Go engine based on monte carlo tree search. IEEE Trans. Comput. Intellig. and AI in Games, 2(4):259–270, 2010.  Gelly, S. and Silver, D. Combining online and ofﬂine learning in UCT.  Conference on Machine Learning, pp. 273–280, 2007.  In 17th International  Gelly, S. and Silver, D. Monte-Carlo tree search and rapid action value estimation in computer Go.  Artiﬁcial Intelligence, 175:1856–1875, 2011.  Huang, Shih-Chieh and M¨uller, Martin. Investigating the limits of Monte-Carlo tree search methods in computer Go. In Computers and Games - 8th International Conference, CG 2013, Yokohama, Japan, August 13-15, 2013, Revised Selected Papers, pp. 39–48, 2013.  Huang, Shih-Chieh, Coulom, R´emi, and Lin, Shun-Shii. Monte-Carlo simulation balancing in prac- tice. In Proceedings of the 7th International Conference on Computers and Games, pp. 81–92. Springer-Verlag, 2011.  Kocsis, Levente and Szepesv´ari, Csaba. Bandit based Monte-Carlo planning. In Machine Learning:  ECML 2006, pp. 282–293. Springer, 2006.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  M¨uller, Martin. Computer Go. Artif. Intell., 134(1-2):145–179, 2002.  Schraudolph, Nicol N, Dayan, Peter, and Sejnowski, Terrence J. Temporal difference learning of position evaluation in the game of Go. Advances in Neural Information Processing Systems, pp. 817–817, 1994.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. arXiv preprint arXiv:1409.1556, 2014.  Sutskever, Ilya and Nair, Vinod. Mimicking Go experts with convolutional neural networks.  Artiﬁcial Neural Networks-ICANN 2008, pp. 101–110. Springer, 2008.  In  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.  8  ",
1412.7580,2015,Fast Convolutional Nets With fbfft: A GPU Performance Evaluation,"['Fast Convolutional Nets With fbfft: A GPU Performance Evaluation', 'Nicolas Vasilache', 'Jeff Johnson', 'Michael Mathieu', 'Soumith Chintala', 'Serkan Piantino', 'and Yann LeCun']",https://arxiv.org/pdf/1412.7580,"5 1 0 2    r p A 0 1         ]  G L . s c [      3 v 0 8 5 7  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  FAST CONVOLUTIONAL NETS WITH fbfft : A GPU PERFORMANCE EVALUATION  Nicolas Vasilache, Jeff Johnson, Michael Mathieu, Soumith Chintala, Serkan Piantino & Yann LeCun Facebook AI Research 770 Broadway, New York, NY 10003, USA {ntv,jhj,myrhev,soumith,spiantino,yann}@fb.com  ABSTRACT  We examine the performance proﬁle of Convolutional Neural Network (CNN) training on the current generation of NVIDIA Graphics Processing Units (GPUs). We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA’s cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides signiﬁcant speedups over cuFFT (over 1.5×) for whole CNNs. Both of these convolution implementations are avail- able in open source, and are faster than NVIDIA’s cuDNN implementation for many common convolutional layers (up to 23.5× for a synthetic kernel conﬁgura- tion). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hard- ware speciﬁcs in the implementation of fbfft are also provided.  1  INTRODUCTION  Deep convolutional neural networks (CNNs) have emerged as one of the most promising techniques to tackle large scale learning problems, whether in image and face recognition, audio and speech processing or natural language understanding. A convolutional layer within these networks pro- vides useful properties such as translation equivariance of activations. A limiting factor for use of convolutional nets on large data sets was, until recently, their computational expense.  Krizhevsky et al. (2012) demonstrated that training of large CNNs with millions of weights and massive data sets is tractable when graphics processing units (GPUs) are properly put to use. Since then, renewed interest in CNNs insufﬂated a fresh breath in various frameworks and implemen- tations, including Torch (Collobert et al. (2011a)), Theano (Bergstra et al. (2010)), cuda-convnet (Krizhevsky (2014)) and Caffe (Jia et al. (2014)). Many of these frameworks are based around codes for NVIDIA GPUs using CUDA (Garland et al. (2008)).  We discuss our contributions to convolution performance on these GPUs, namely using Fast Fourier Transform (FFT) implementations within the Torch framework. We summarize the theory behind training convolutional layers both in the time and frequency domain in Section 2. We then detail our implementations. The ﬁrst is based on NVIDIA’s cuFFT and cuBLAS libraries (Section 3). We evaluate our relative performance to NVIDIA’s cuDNN library (Chetlur et al. (2014)) on over 8, 000 different conﬁgurations (Section 4). We signiﬁcantly outperform cuDNN and other time domain convolution implementations for a wide range of problem sizes.  Our second implementation is motivated by limitations in using a black box library such as cuFFT in our application domain, which we describe. In reaction, we implemented a from-scratch open- source implementation of batched 1-D FFT and batched 2-D FFT, called Facebook FFT (fbfft), which achieves over 1.5× speedup over cuFFT for the sizes of interest in our application domain. This implementation achieves GPU efﬁciency ratios of over 75% in certain cases. We describe an on- going effort to further improve the performance of our solution based on algorithmic tiling (Section 6) before we conclude. Our implementation is released as part of the fbcuda and fbcunn open- source libraries at http://github.com/facebook.  1  Published as a conference paper at ICLR 2015  2 CONVOLUTION  Discrete convolution and cross-correlation are used in CNNs. We quickly summarize these and their implementation, with a formulation mirroring Mathieu et al. (2013). Forward propagation (fprop) inputs are a set f of input feature planes xi, i ∈ f . These are cross-correlated1 with f ′ × f different ﬁlter kernel weights w(j,i), j ∈ f ′, i ∈ f , producing output feature planes yj, j ∈ f ′. Each input and output feature can be part of a minibatch S, so we have x(s,i) and y(s,j), i ∈ f, j ∈ f ′, s ∈ S:  y(s,j) =Xi∈f  x(s,i) ⋆ w(j,i)  The feature planes f are reduced (summed) pointwise. For back-propagation (bprop), the gradient of the loss with respect to outputs are convolved with the kernels:  ∂L  ∂x(s,i)  ∂L  ∂y(s,j)  = Xj∈f ′  ∗ w(j,i)  Reduction is over f ′ here. Finally, the kernel weights are updated using the gradient of the loss with respect to the weights (accGrad):  ∂L  ∂w(j,i)  ∂L  ∂y(s,j)  =Xs∈S  ⋆ x(s,i)  Reduction is over S here. For purposes of this paper, we use set symbols interchangeably to refer to their size: each input plane is a 2-D matrix of size h × w, and each ﬁlter kernel is a 2-D matrix of 2. The output planes y(s,i) are of size (h − kh + 1) × (w − kw + 1), and implement size kh × kw valid-only convolution, as per MATLAB terminology. Input zero padding and input mirror padding around the margins of the input (ph, pw) can be optionally added.3 A popular convolution implementation is to unroll the data until the computation is in the form of a large matrix multiplication (Chellapilla et al. (2006)). This is the strategy followed by many imple- mentors, since matrix multiplication is a well-tuned linear algebra primitive available on virtually any platform. While it is possible to provide instances of direct calculation that are faster than matrix unrolling (e.g., for large S, Krizhevsky (2014)), it is challenging to provide an implementation that is faster for more than just a small subset of possible convolution problems. Introducing strides in this form of convolution (i.e., performing the convolution at every dh, dw-th offset) is a popular way to reduce the computational cost at the expense of precision. The memory accesses required are very similar but with fewer reuse opportunities. On the other hand, by the convolution theorem, a convolution of two discrete signals can be performed with lower asymptotic complexity by performing the multiplication in the frequency domain. Applied to the forward pass, it becomes:  y(s,j) =Xi∈f  x(s,i) ⋆ w(j,i) =Xi∈f  F −1(cid:0)F (x(s,i)) ◦ F (w(j,i))∗(cid:1)  where ∗ denotes complex conjugation and ◦ is the pointwise product. The discrete Fourier basis used is the largest of the two components convolved and the output.4 Linearity of the DFT allows one to perform the sum above in the Fourier domain if desired. Applying the FFT then yields a O(Sf f ′n2 + (Sf + f f ′ + Sf ′)n2 log n) procedure in lieu of the original O(Sf f ′n2k2), n = h = w, k = kh = kw. Similar transformations apply for the other two passes. We call this a frequency domain convolution, in contrast to time domain convolution via direct computation.  1Torch practice is that the forward pass is cross-correlation, hence the ⋆. 22-D can be extended to n-D, n ≥ 1. 3Input size (h + ph) × (w + pw), output size (h + ph − kh + 1) × (w + pw − kw + 1). 4(h × w)-dimensional or even bigger for performance (Section 3.2).  2  Published as a conference paper at ICLR 2015  Strided convolutions via FFT can be implemented efﬁciently to obtain good performance Brosch & Tam (2015). We do not consider those in this paper.  3 CUFFT CONVOLUTION IMPLEMENTATION  In this section we discuss implementation strategies using the NVIDIA cuFFT libraries and their efﬁciency.  3.1 FFT CONVOLUTION DETAILS  We described the general formulation for the three types of convolutions in section 2. Here, we borrow the Torch naming convention: input for x(s,i); weight for w(j,i); output for y(s,j); gradOutput for ∂L/∂y(s,j); gradInput for ∂L/∂x(s,i); and gradWeight for ∂L/∂w(j,i). All are stored as single- precision ﬂoating point 4-D tensors in row-major layout, and are stored in memory using the so- called BDHW format. This is explicit in the expression InS×f ×h×w, with input image row data as the innermost or most varying dimension.  Table 1 describes the in-order operations for FFT computation of the forward pass, using the F F T 2D and IF F T 2D operators and Cgemm matrix multiplication. Similar implementations fol- low for the other two passes. The G preﬁx denotes gradients. The F sufﬁx denotes C-valued fre- quency domain tensors; the rest are over R. The T sufﬁx denotes transposed tensors.  Table 1: Implementation detail for forward propagation  INPUT  OUTPUT  InS×f ×h×w  W eif ′×f ×kh×kw InFS×f ×(h+ph)×(⌊ w+pw W eiFf ′×f ×(h+ph)×(⌊ w+pw  2  2  ( InF T(h+ph)×(⌊ w+pw  W eiF T ∗  (h+ph)×(⌊ w+pw  2  2  ⌋+1)  ⌋+1)  ⌋+1)×S×f  ⌋+1)×f ′×f  OutF T(h+ph)×(⌊ w+pw OutFS×f ′×(h+ph)×(⌊ w+pw  2  2  ⌋+1)  ⌋+1)×S×f ′  F F T 2D −−−−−→ InFS×f ×(h+ph)×(⌊ w+pw F F T 2D −−−−−→ W eiFf ′×f ×(h+ph)×(⌊ w+pw T rans2D −−−−−−→ InF T(h+ph)×(⌊ w+pw T rans2D −−−−−−→ W eiF T(h+ph)×(⌊ w+pw  2  2  2  ⌋+1)  ⌋+1)  ⌋+1)×S×f  ⌋+1)×f ′×f  2  Cgemm −−−→  OutF T(h+ph)×(⌊ w+pw  2  ⌋+1)×S×f ′  T rans2D −−−−−−→ OutFS×f ′×(h+ph)×(⌊ w+pw ⌋+1) IF F T 2D −−−−−−→ OutS×f ′×(h−kh+1)×(w−kw+1)  2  Exact tensor dimensions are also given above. By taking advantage of the Hermitian symmetry property of the 2-D DFT for R-valued inputs we only store about half the complex entries; the remaining can be obtained by complex conjugation. This results in array sizes such as ⌊ w+pw ⌋ + 1. We also perform interpolation by zero-padding, which serves multiple purposes. First, it is necessary to handle boundary conditions.5 Second, it is required to interpolate all operands over the same Fourier basis.6 Finally, padding has an impact on the FFT algorithm used in practice, as well as on the ﬂoating point operation count of non-FFT operations (Section 3.2).  2  Following the conversion into frequency domain, we perform transpositions to prepare the tensors for Cgemm matrix multiplication library calls. The transposition converts the BDHW layout into HWBD. The transposition is currently out-of-place and implemented using the Cgeam routine; we are also considering our own, in-place transposition routine. Cgemm library calls are performed on transposed tensors in the frequency domain. Casting the operation as a Cgemm call allows us to beneﬁt from the heavily tuned cuBLAS routine. Eventually, we transpose the result back into the BDHW format and perform a 2-D inverse FFT. At this point, the resulting real tensor, always  5In this case, we typically have ph = ⌊ k 6All tensors are zero-padded to (h + ph) × (w + pw) before F F T 2D.  2 ⌋ and pw = ⌊ kw  2 ⌋.  h  3  Published as a conference paper at ICLR 2015  (h + ph) × (w + pw), is clipped to the appropriate ﬁnal size: (h − kh + 1) × (w − kw + 1) for fprop, h × w for bprop, kh × kw for accGrad.  3.2 CUFFT DESIGN SPACE  We now discuss implementation aspects we explored. Multiple factors inﬂuence the computational efﬁciency of FFTs: transform size n, n’s prime factor decomposition, and whether batched or it- erated single transforms are applied. In the deep learning domain, it is commonplace to deal with small sizes, n 6= 2k. If n has undesirable properties, efﬁciency can drop by an order of magnitude.7 cuFFT implements FFTs with the ubiquitous Cooley-Tukey algorithm (Cooley & Tukey (1965)) which takes advantage of trigonometric equalities to recursively decompose and reuse computa- tions. This is further discussed in the Supplement. Decomposition is built on specialized kernels of ﬁxed sizes which correspond to the prime factor decomposition of n. cuFFT implements specialized building blocks for radix sizes 2, 3, 5, 7, and for sizes n where 4|n, it can use more efﬁcient kernels exploiting the conjugate symmetry property. When n does not admit a prime factor decomposition using those radices only, the expensive Bluestein algorithm is used (Bluestein (1970)). Because our results are used in the time domain, we can in fact zero-pad the image and kernel to perform the FFT at any larger size that may be handled more efﬁciently. Exploiting more efﬁcient, larger sizes should be balanced against the extra cost introduced in the subsequent transposition and matrix multiplica- tion steps. Table 4’s last case is one in which the best tradeoff is not easily guessed. cuFFT also has batched mode optimizations when multiple FFTs of the same size are being performed.  3.3 CUBLAS DESIGN SPACE  The cuBLAS library also comes with different implementations for batched and single operation modes. We had the choice between 3 implementation options:  • for larger batches over small matrices, the cublasCgemmBatched library call; • for smaller batches over larger matrices, multiple cublasCgemm calls from the host; • for intermediate batch and matrix sizes, devices of compute capability 3.5 and higher sup- port dynamic parallelism which allows CUDA kernels to launch other kernels. This can be beneﬁcial for many launches over small matrices.  Note that the discussion above applies to multiplications after transposition. So the matrix size is either S × f , S × f ′ or f × f ′ and the number of such matrices is h × w. Vendor libraries are usually optimized for throughput and not latency, so we expect it to be more efﬁcient for larger sizes along critical dimensions (i.e., image size for the batch case and S × f , S × f ′ or f × f ′ for the multiple kernel case). Due to build system limitations we were not able to experiment with the dynamic parallelism strategy; we leave this for future work.  At the system level, we use CUDA streams and buffering of all CUDA resources and intermediate buffers to remove synchronization points across convolutions. We are mindful of memory consump- tion; to address this we keep one single buffered copy of each type of tensor involved. This behavior is tailored for a bulk synchronous execution of layers on a GPU and is not adapted for multiple asynchronous convolutions on the same GPU. The buffers are automatically expanded as required and reused as much as possible.  3.4 AUTOTUNING  We combine the above implementation with a simple autotuning strategy. We devise a strategy selection mechanism that runs once for each problem size and caches the fastest strategy out of a few dozen for later reuse. The autotuning strategy explores different possible Fourier basis sizes that can be decomposed in powers for which cuFFT has an efﬁcient implementation. In other words, for an FFT dimension of size n, we explore the sizes i ∈ [n, 2⌊log2 n⌋] where i = 2a3b5c7d. When the input size is a power of 2, the search space is reduced to a single point. In addition to Fourier basis sizes, we weigh in various cuBLAS calls and asynchronous modes.  7http://docs.nvidia.com/cuda/cufft/index.html#accuracy-and-performance  4  Published as a conference paper at ICLR 2015  4 CUFFT CONVOLUTION PERFORMANCE  4.1 PERFORMANCE VERSUS CUDNN: 8,232 CONFIGURATIONS  We compare our cuFFT convolution results against NVIDIA’s cuDNN 1.0 library (Chetlur et al. (2014)), which contains one of the fastest, general purpose convolution methods for the GPU, using matrix unrolling. It has decent performance for many problem sizes thanks to heavy autotuning of cuBLAS codes for different problems. It is a strong baseline for this reason.  Image CNNs to date have for the most part used square input images and ﬁlters, though rectan- gular ﬁlters are valid for other problems (notably text CNNs, Collobert et al. (2011b)). Thus, we restrict ourselves to a 5-D problem domain {S, f, f ′, n(= h = w), k(= kh = kw)}. Much of this space is not used in practice. Some areas are perhaps over-emphasized (large S, small k) due to current engineering concerns. We evaluate cuDNN vs cuFFT-based convolution for Table 2’s 8, 232 conﬁgurations.8  Table 2: Conﬁguration elements evaluated  DIMENSION  SIZES EVALUATED  Minibatch size (S) Input ﬁlters (f ) Output ﬁlters (f ′) Kernel h/w (k = kh = kw) Output h/w (y = h − kh + 1 = w − kw + 1)  1, 16, 64, 128 1, 4, 16, 64, 96, 128, 256 1, 4, 16, 64, 96, 128, 256 3, 5, 7, 9, 11, 13 1, 2, 4, 8, 16, 32, 64  Figures 1-6 are performance summaries of cuFFT convolution versus cuDNN on a NVIDIA Tesla K40m, averaged across all three passes. The y-axis problem size corresponds to the minibatch size multiplied by number of input and output planes (Sf f ′); each one of these is a pass reduction dimension. Many possible combinations of S, f, f ′ may map to the same problem size. cuDNN per- formance varies to a greater degree than cuFFT across passes. This is due to the asymmetry of convolution sizes in each pass, and the fact that a larger convolution kernel (as seen with gradient accumulation) is essentially free in the Fourier domain. Averaging the three passes together pro- vides a proxy for overall performance. The x-axis corresponds to output height/width. For deeper layers in image CNNs, output size will decrease while f, f ′ will increase, so depth corresponds to moving from the upper right to the lower left of the graph. Black areas in the chart are due to failed cuFFT runs, due to memory pressure or undetermined potential cuFFT 6.5 issues.  FFT convolutions make large kernel sizes inexpensive, which make the performance of all three passes roughly equal (Table 4). On the other hand, zero-padding kh × kw to h × w penalizes smaller kernels compared to cuDNN. For 3 × 3 kernels (Figure 1), cuFFT performance is poor compared to cuDNN. The overhead of multiple kernel launches, streaming memory in and out multiple times, and zero-padding to the input size often outweigh the algorithmic advantage of FFT. However, for the largest problem sizes, 3 × 3 convolution via FFT can still be advantageous, with top speed 1.84× faster than cuDNN. 5 × 5 kernels (Figure 2) show an increasing dominance of the FFT strategy, with top speed 5.33× faster. The tendency is conﬁrmed for larger kernel sizes: at 13 × 13, maximum speedup is 23.54× over cuDNN.  8Parameterized on output rather than input size h, w because the implied h = y + kh − 1, w = y + kw − 1  will be valid for any choice of kh, kw.  5  Published as a conference paper at ICLR 2015  1/16x  16x  speedup  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  e z i s    m e l b o r p  1 2 4 8  6 1  2 3  4 6  1 2 4 8  6 1  2 3  4 6  output size  output size  Figure 1: 3 × 3 kernel (K40m)  Figure 2: 5 × 5 kernel (K40m)  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  e z i s    m e l b o r p  1 2 4 8  6 1  2 3  4 6  1 2 4 8  6 1  2 3  4 6  output size  output size  Figure 3: 7 × 7 kernel (K40m)  Figure 4: 9 × 9 kernel (K40m)  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  e z i s    m e l b o r p  1 2 4 8  6 1  2 3  4 6  1 2 4 8  6 1  2 3  4 6  output size  output size  Figure 5: 11 × 11 kernel (K40m)  Figure 6: 13 × 13 kernel (K40m)  6  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  1 96 512 4096 12288 32768 49152 65536 98304 131072 147456 196608 262144 393216 524288 589824 786432 1048576 1179648 1572864 2097152 3145728 4194304 8388608  e z i s    m e l b o r p  e z i s    m e l b o r p  e z i s    m e l b o r p  Published as a conference paper at ICLR 2015  4.2 CNN PERFORMANCE  In table 3, we show performance for real CNNs, AlexNet (Krizhevsky et al. (2012)) and OverFeat fast (Sermanet et al. (2014)), comparing against cuDNN and cuda-convnet2 (ccn2) kernels in Torch. The ﬁrst layer uses cuDNN for the cuFFT runs because it is strided, but all other layers use cuFFT. The timings include all convolutional layers of the network.  Table 3: AlexNet and OverFeat fast performance (K40, ms)  NETWORK KERNEL FPROP BPROP ACCGRAD TOTAL  AlexNet  OverFeat fast  cuFFT cuDNN ccn2  cuFFT cuDNN ccn2  94.34 147.32 99.03  96.69 167.79 104.59  93.20 153.96 103.29  375.65 459.06 433.11  460.48 634.26 398.87  397.85 508.02 450.82  284.23 469.07 306.91  1233.98 1601.35 1282.80  Table 4 shows the performance of the cuDNN and our cuFFT convolution implementation for some representative layer sizes, assuming all the data is present on the GPU. Our speedups range from 1.4× to 14.5× over cuDNN. Unsurprisingly, larger h, w, smaller S, f, f ′, kh, kw all contribute to reduced efﬁciency with the FFT. More surprisingly, we experience noticeable speedups on small 3 × 3 kernels as long as the input tensor remains of small size. The optimal FFT sizes that au- totuning ﬁnds are reported in columns 2 and 3; note L5 padding being found by the autotuner. Column 7 has the trillion equivalent time-domain reductions per second (single-precision ﬂoating point multiply-adds) achieved by our implementation on a NVIDIA Tesla K40m on CUDA 6.5. This number represents the throughput a time-domain kernel needs to achieve in order to match our implementation; it is computed as (Sf f ′khkw(h − kh + 1)(w − kw + 1))/time. This is a metric to compare relative efﬁciency across problem and padding sizes. In the cases L2, L3 and L4, a time domain convolution would need to exceed the K40m peak of 4.29 Tﬂop/sec in order to match our throughput.  5 fbfft IMPLEMENTATION  This section presumes familiarity with GPU architecture. Refer to the Supplement for details.  When designing high-performance libraries, multiple objectives must be balanced against each other: memory latency/bandwidth tradeoffs, maximizing locality without sacriﬁcing too much par- allelism, good instruction mix, register usage and mapping strategy of computation and data to memories and compute elements. A key principle is to design a set of leaf kernels with well-tuned in-register performance and reduce the larger problem to a combination of these kernels by data and loop tiling (Irigoin & Triolet (1988)) and recursive decompositions (Gunnels et al. (2001)). Since vendors have to sustain high performance for a large class of application domains, there exist pa- rameter conﬁgurations for which a carefully tuned approach signiﬁcantly outperforms vendor-tuned libraries (Shin et al. (2010)). For common deep learning use, convolutional layers consist of many batched small 2-D convolutions. These are tiny relative to DSP and HPC standards and put us in a regime where (a) we fall outside of the highly tuned regime, (b) feature dimensions are often smaller than GPU warp sizes and can often ﬁt exclusively in registers rather than in shared memory (SMEM), and (c) we are very sensitive to latencies. We determined that it is possible to obtain better efﬁciency than the existing batched cuFFT mode for CNNs.  5.1 LIMITATIONS OF CUFFT  Because the cuFFT library is a black box, zero-padding9 has to be explicitly embedded in the input and output arrays. The consequence is that one may need to allocate a duplicate, larger memory  9This is different from the FFTW compatibility padding mode for in-place transforms.  7  Published as a conference paper at ICLR 2015  Table 4: Representative layer performance (S = 128, K40m)  LAYER h + ph w + pw  cuDNN  cuFFT  SPEEDUP TRED/s  L1 fprop bprop accGrad L2 fprop bprop accGrad L3 fprop bprop accGrad L4 fprop bprop accGrad L5 fprop bprop accGrad  64 64 64  128 128 128  46.44 ms 46.25 ms 47.03 ms  80.98 ms 66.49 ms 69.63 ms  125.11 ms 153.39 ms 155.07 ms  354.83 ms 579.37 ms 416.34 ms  Params: f = 3, f ′ = 96, h = w = 128, kh = kw = 11 1.54× 128 2.30× 128 128 2.22× Params: f = 64, f ′ = 64, h = w = 64, kh = kw = 9 7.64× 64 12.5× 64 64 8.85× Params: f = 128, f ′ = 128, h = w = 32, kh = kw = 9 32 32 32 Params: f = 128, f ′ = 128, h = w = 16, kh = kw = 7 16 16 16 Params: f = 384, f ′ = 384, h = w = 13, kh = kw = 3 13 13 13  130.89 ms 245.57 ms 154.96 ms  17.77 ms 16.97 ms 17.00 ms  15.13 ms 20.80 ms 18.17 ms  39.82 ms 28.33 ms 47.84 ms  21.35 ms 20.22 ms 21.26 ms  4.88 ms 4.71 ms 4.70 ms  7.36× 14.5× 9.29×  3.10× 4.41× 3.86×  1.86× 1.40× 2.25×  32 32 32  16 16 16  14 14 14  0.9 1.1 1.05  7.49 7.52 7.40  9.90 10.37 10.34  5.54 5.76 5.75  1.34 1.42 1.35  region (only once) and copy data from non-padded tensors to padded tensors. This memory con- sumption and spurious copies affect latency signiﬁcantly. Instead, we devised an implementation for batched 1-D FFT and 2-D FFT of sizes 2-256 and reaches up to 78% efﬁciency at 97.5% occupancy. We also implemented an IFFT kernel based on our FFT kernel.  In our implementation we use clipping to conditionally load a value if reading within bounds or a constant (0) otherwise. This is an approach used in automatic code generation tools such as Halide (Ragan-Kelley et al. (2013)) and relies on aggressive if-conversion properties of the CUDA compiler. It allows for more efﬁcient control ﬂow rather than using explicit loop prologues and epilogues. This mechanism does not require any additional memory allocation and is zero-copy; this is particularly desirable in the latency sensitive mode.  Additionally, since cuFFT and cuBLAS are closed source, it is impossible to take advantage of algo- rithmic simpliﬁcations that may be available. For instance, in the forward pass of our computation as shown in Table 1, the result of the ﬁrst cuFFT call is of the form S×f ×(h+ph)×(⌊(w+pw)/2⌋+1). With fbfft we return it in the form S × f × (⌊(w + pw)/2⌋ + 1) × (h + ph) where the two inner- most data dimensions are transposed. This allows us to remove a full data transposition from each of the FFT kernels. Another domain-speciﬁc optimization we have yet to explore is eliminating bit reversal portions of the FFT and IFFT. This can be done by performing the FFT with decimation in frequency (DIF) and the IFFT with decimation in time (DIT), discussed in the Supplement.  5.2 WARP-LEVEL 1-D FFT AND 2-D FFT FOR SIZE n ≤ 32  For batched FFT of power of two sizes we view a single warp as a small distributed system with lockstep collective communication capabilities and we program it in a bulk-synchronous fashion (Valiant (1990)). We implement DIF and enforce the following invariants for the log2 n steps:  • each warp thread originally loads one real element of the input vector and locally computes  one complex twiddle factor (i.e. a root of unity);  • at each step, all warp threads exchange data with another thread in the warp in parallel and  produce a new value;  8  Published as a conference paper at ICLR 2015  • then, all warp threads exchange twiddle factors with another thread in the warp in parallel,  and produce a new value.  The two bulk-synchronous exchanges can be written each with one warp-wide instruction. After the log2 n steps, the FFT is computed and stored in a distributed and bit reversed manner within 1 register across a warp. For sizes n ≤ 32, bit reversal can be implemented with a single warp shufﬂe. We either load twiddle factors from device memory or compute them with the sincosf function only once, and subsequently swap them within registers. This greatly reduces the reliance on either memory bandwidth or on the special functional unit at the expense of a few additional registers. The decision between explicitly loading twiddle factors from device memory or computing them is a tradeoff between arithmetic intensity and memory bandwidth. For sizes 16 and 32 the arithmetic pipeline is the bottleneck. Loading twiddle factors from memory for these two special sizes results in a performance increase of 15% and 20% respectively. The discussion above applies to 1-D FFT and to each independent FFT within a larger 2-D FFT. A n-D Fourier transform is separable and can be implemented with sets of multiple 1-D FFT with transpositions between each of these sets. In 2-D FFT R-to-C, the ﬁrst set comprises n FFTs and the second set comprises n/2 + 1 FFTs by Hermitian symmetry. Following standard techniques Lyons (1996) we further pack 2 real FFTs into a single complex FFT . The extra 1 term in the quantity n/2+ 1 makes the computation ill-balanced and can bring down performance by lowering occupancy. We chose to dimension our kernels to have size n×(n/2) and introduce additional control ﬂow to handle the border case. This results in 30% additional performance. We implement the transposition in SMEM across warps following Ruetsch & Micikevicius (2009). Data is already resident in registers so our main concerns are limiting SMEM usage to keep occupancy high, and limiting load/stores by using vector instructions to avoid saturating the load-store unit (LSU).  5.3  1-D FFT AND 2-D FFT FOR SIZE 32 < n ≤ 256  With size 32 as our building block, we extend our strategy to larger sizes. We use the same single warp approach to compute a full 1-D FFT. The main difference is that the computation is now dis- tributed across multiple registers across threads in a warp (⌈n/32⌉ Fourier coefﬁcients and twiddle factors in registers per thread). Because we perform a full FFT per warp, a performance cross-over where cuFFT wins happens after register usage limits occupancy too much. We outperform 1-D cuFFT for n ≤ 256, with a hard register limit at n = 512 (128 and 256 similarly for 2-D FFT). This is still well within our application domain. The following modiﬁcations handle multiple registers per thread:  • Hermitian symmetry allows us to perform half the computation. There is a tradeoff be- tween adding control-ﬂow divergence and performing less work. At n ≥ 64, beneﬁts from reduced computations dominate divergence losses;  • we take advantage of trigonometric symmetries and twiddle factor distribution to compute only a fraction of the roots of unity needed for each FFT, distributed with register to register copies;  • twiddle factor re-balancing across a warp and across registers requires a different imple-  mentation. We managed to implement it fully within registers;  • bit reversal occurs across registers and across warps. The high-order bits represent the reg- ister while the low-order bits represent the warp. Without a sophisticated implementation, this results in indirect addressing of registers which is costly. We implement a simple bit reversal in SMEM, which is an occupancy bottleneck at n ≥ 256 for 1-D FFT.  In the 2-D FFT case, the intermediate transpose becomes signiﬁcantly more expensive. We exper- imented with various strategies to keep occupancy high, including partial transpositions within a warp to use minimal amounts of SMEM.  5.4 DISCUSSION  We report the relative performance of our implementation fbfft compared to cuFFT for various batch and input sizes of interest. The number of batches to consider depends on the dimension of  9  Published as a conference paper at ICLR 2015  CNN layers as well as any multi-GPU parallelization strategy that may be involved. At typical sizes of interest, fbfft is between 1.5× and 5× faster. We tried up to 4 million batches and at larger sizes gains stabilize around 1.4× but efﬁciency goes down as more and more memory is used.  p u d e e p S   T F F B F  p u d e e p S   T F F B F  4.5  4.0  3.5  3.0  2.5  2.0  1.5  1.0  0.5  0.0  4.5  4.0  3.5  3.0  2.5  2.0  1.5  1.0  0.5  0.0  FBFFT-1D Speedup at various sizes and batches  8 16 32 64 128 256  4  32  128  1024  4096  16384  65536  Number of batches  p u d e e p S   T F F I B F  4.5  4.0  3.5  3.0  2.5  2.0  1.5  1.0  0.5  0.0  FBIFFT-1D Speedup at various sizes and batches  8 16 32 64 128 256  4  32  128  1024  4096  16384  65536  Number of batches  Figure 7: fbfft-1D FFT and IFFT (K40m, cuFFT 6.5 @ 1x)  FBFFT-2D Speedup at various sizes and batches  8 16 32 64 128  4  32  128  1024  4096  16384  65536  Number of batches  p u d e e p S   T F F I B F  4.5  4.0  3.5  3.0  2.5  2.0  1.5  1.0  0.5  0.0  FBIFFT-2D Speedup at various sizes and batches  8 16 32 64 128  4  32  128  1024  4096  16384  65536  Number of batches  Figure 8: fbfft-2D FFT and IFFT (K40m, cuFFT 6.5 @ 1x)  Figure 7 shows the performance in the 1-D case. These numbers do not exercise our implicit zero- copy padding, so we expect additional gains when we incorporate our FFT in the convolution. Our implementation outperforms cuFFT for all cases of interest, more dramatically so for smaller batch sizes. Small batch sizes also correspond to the latency sensitive regime in Figures 1-6 for which the cuFFT based implementation performs quite worse than cuDNN. We achieve 78% efﬁciency at 97.5% occupancy for size 64 at batch size 16, 384, as reported by nvvp. Figure 8 shows the performance in the 2-D case. Relative performance gains for sizes 64 are more modest than in the 1-D case, even losing to cuFFT at size 128 and small batch sizes. The magnitude of the relative gains at various batch sizes drops faster than in the 1-D case. Looking at the perfor- mance of the 32 × 32 FFT, we obtain 1.6× speedup over cuFFT at 1, 024 batches. The same ratio is not obtained until 16, 384 batches in 1-D FFT.10 When coupled with the tiling strategy in Section 6, we emphasize that the sizes of interest are actually 8-64, and depend on kh, kw but not input h, w. Batch sizes can vary on the whole spectrum.  We interfaced fbfft into our convolution module and ran experiments with 3 × 3 kernels for the 3 different convolution passes over inputs of sizes x = h = w, x ∈ {13, 16, 27, 32, 57, 64}. For problem size, we used p = S = f = f ′, p ∈ {16, 32, 64, 128}. By swapping our FFT implemen- tation we observed an overall mean speedup of 1.51× with standard deviation 0.21 and geometric mean 1.49×. The minimum speedup was 1.21×, despite sometimes performing more computations  10This is not unexpected because these two computations perform the same number of ﬂops when accounting for Hermitian symmetry, plus the fact that the efﬁciency of cuFFT increases while fbfft remains high but almost constant.  10  Published as a conference paper at ICLR 2015  with fbfft which can only interpolate to a power of 2. These experiments exercise the zero-copy padding and lower memory footprints of fbfft compared to cuFFT but do not yet reﬂect additional optimizations such as tiling and bit twiddling elision.  6 CURRENT LIMITATIONS AND FUTURE WORK  In our current implementation, fbfft heavily relies on shufﬂe instructions. In spite of a good efﬁciency, we only utilize 60% of the available memory bandwidth. This is due to the load and store instructions in our kernel competing with the shufﬂe instructions for the Load-Store Unit (LSU). As a consequence, our ﬁrst bottleneck is the number of instructions issued on the LSU. For instance, on Kepler (capability 3.5), the throughput for 32-bit ﬂoating point multiply-add operations is 192 per cycle but the throughput for shufﬂes is only 32. In the future we will investigate and release faster implementations as they become available.  Temporary memory overhead requirements are a common issue when performing convolutions in the Fourier domain. In this ﬁrst implementation, we introduced the following memory buffers to support our implementation:  • for each of input, output and weight tensors we store 1 buffer for the frequency array and 1 buffer for its complex transpose. These buffers store the Fourier representation and are generally limited by the weight tensor which is independent of the mini-batch size. Because of the global memory pressure we introduce, we reuse buffers at each layer and pass on the opportunity to (1) reuse 2 FFT results in each hidden layer, reducing the cost of forward FFTs by 33%; and (2) asynchronously precompute FFTs of the weight tensors and their gradients to better ﬁll the gpu utilization pipeline,  • when using cuFFT we additionally pad the input, weight and output tensors explicitly to  the best performing common fft size  • when using cuFFT additional temporary memory is reserved by each cufftPlan • with fbfft padding is implicit but and no temporary memory buffer is needed until we reach size 64. On the other hand, fbfft only supports square convolutions whose size is a power of 2. As a consequence, too much padding could occur and adversely affect both performance and memory consumption. The tiling strategy we describe next is a good way to circumvent the problem.  Additionally, we recently developed an in-place transposed batched CGEMM which permits the removal of the complex transposed buffer. For this problem, a tool like MaxAS Lavin (2015) could be valuable.  fbfft provides the most gains over cuFFT at sizes 8-64. A tiling strategy for the input can be used to exploit this advantage. When the kernel is signiﬁcantly smaller than the input, we can decompose a large convolution into several smaller ones. For simplicity, we consider 1D convolution on a single input plane, as it can trivially be extended. Let x be an input of size n, c a kernel of size w and y = x ⋆ c. We write x[i,j] for the vector formed by contiguous elements of x: {xi, xi+1, ..., xj−1}. Let d ≤ n. From the deﬁnition of the convolution, we have:  y[i,i+d] = x[i,i+d+w] ⋆ c  So the convolution of the input of size n can be computed with ⌊n/d⌋ convolutions with inputs of size d + w. The cost of the convolution goes down from O(n log(n)) to O(⌊n/d⌋(d + w) log(d + w)) = O((n + w/d) log(d + w)). From this formula, we see that the optimal d is of the order of w, to get the complexity O(n log(w)). This strategy allows us to speed up forward and backward propagation. Tiling can also be used to reduce memory cost for temporary storage by not running all the tiles in parallel (just the tiles which do run in parallel need their scratch space), at the potential expense of parallelism or efﬁciency.  For the gradient accumulation, we cannot reuse this strategy, since it involves a larger convolution between an input x of size n and a kernel z = ∂L ∂y of size n − w + 1. However, we have a similar formula:  11  Published as a conference paper at ICLR 2015  ∂c(cid:19)j (cid:18) ∂L  =  n−1  Xi=0  xj+i · zi =  ⌊n/d⌋−1  Xk=0  d−1  Xi=0  xj+i+kd · zi+kd +  n−1  Xi=d⌊n/d⌋  xj+i · zi  And so  (cid:18) ∂L ∂c(cid:19) =  ⌊n/d⌋−1  Xk=0  x[dk,(d+1)k+w−1] ⋆ z[dk,(d+1)k] + x[d⌊n/d⌋,n] ⋆ z[d⌊n/d⌋,n−w+1]  We have a few other optimizations that are planned as well. Since much of the data we have is al- ready available in registers or in shared memory, we are implementing our own in-place, in-register transpose via recursive decomposition. The pointwise multiplications in the Fourier domain, es- pecially with tiling, are rather small, so our own matrix multiplication routines integrated with the rest of the convolution kernel code might win over cuBLAS, and prevent the need for multiple CUDA kernel launches and their associated overhead. Finally, as mentioned earlier, bit reversal portions can be eliminated with the FFT using DIF and the IFFT using DIT.  7 CONCLUSION  To summarize, we achieve signiﬁcant gains in CNNs using FFTs, with a cuFFT convolution im- plementation achieving 1.4 × −14.5× speedups over cuDNN for common sizes. In reaction to cuFFT and cuBLAS limitations in the context of our speciﬁc application domain, we developed our own FFT implementation, fbfft, which is more suited to deep learning problem sizes (large batches, small feature planes). fbfft itself is ≥ 1.4× faster than cuFFT transforms for these prob- lems of interest. For convolution, it is faster than the cuFFT as well, with a mean of 1.51× for sizes that we wish to exploit.  Given our new efﬁcient primitive for size 8-64 convolution, we are continuing work on bit twiddling, transposition and pointwise multiplication optimizations, and continuing work on tiling to make the computational advantage at that size apply to larger convolution problems. These will all allow for reduced training time and use of ever larger and deeper CNNs.  ACKNOWLEDGMENTS  We would like to thank Julien Demouth from NVIDIA who suggested further improvements are still possible by virtue of the current implementation being LSU throughput-bound rather than memory- bound.  REFERENCES Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Bluestein, Leo I. A linear ﬁltering approach to the computation of discrete Fourier transform. Audio and Electroacoustics, IEEE Transactions on, 18(4):451–455, December 1970. ISSN 0018-9278.  Brosch, Tom and Tam, Roger C. Efﬁcient training of convolutional deep belief networks in the frequency domain for application to high-resolution 2d and 3d images. Neural Computation, 27 (1):211–227, 2015. doi: 10.1162/NECO a 00682. URL http://dx.doi.org/10.1162/ NECO_a_00682.  Burrus, C. Sidney. Fast fourier transforms, 2008. URL http://cnx.org/contents/  16e8e5e8-4f22-4b53-9cd6-a15b14f01ce4@5.6:16/Fast_Fourier_ Transforms_(6x9_V.  Chellapilla, Kumar, Puri, Sidd, and Simard, Patrice. High Performance Convolutional Neural Net- works for Document Processing. In Lorette, Guy (ed.), Tenth International Workshop on Frontiers  12  Published as a conference paper at ICLR 2015  in Handwriting Recognition, La Baule (France), October 2006. Universit´e de Rennes 1, Suvisoft. URL https://hal.inria.fr/inria-00112631. http://www.suvisoft.com.  Chetlur, Sharan, Woolley, Cliff, Vandermersch, Philippe, Cohen, Jonathan, Tran, John, Catanzaro, Bryan, and Shelhamer, Evan. cudnn: Efﬁcient primitives for deep learning. CoRR, abs/1410.0759, 2014. URL http://arxiv.org/abs/1410.0759.  Collobert, R., Kavukcuoglu, K., and Farabet, C. Torch7: A matlab-like environment for machine  learning. In BigLearn, NIPS Workshop, 2011a.  Collobert, Ronan, Weston, Jason, Bottou, L´eon, Karlen, Michael, Kavukcuoglu, Koray, and Kuksa, Pavel. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November 2011b. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id= 1953048.2078186.  Cooley, James W. and Tukey, John W. An algorithm for the machine calculation of complex fourier  series. Mathematics of computation, 19(90):297–301, 1965.  Garland, Michael, Le Grand, Scott, Nickolls, John, Anderson, Joshua, Hardwick, Jim, Morton, Scott, Phillips, Everett, Zhang, Yao, and Volkov, Vasily. Parallel computing experiences with cuda. IEEE Micro, 28(4):13–27, July 2008. ISSN 0272-1732. doi: 10.1109/MM.2008.57. URL http://dx.doi.org/10.1109/MM.2008.57.  Giles, Mike. Course on cuda programming on nvidia gpus, lecture 3, 2014. URL http:  //people.maths.ox.ac.uk/gilesm/cuda/lecs/lec3.pdf.  Gunnels, John A., Henry, Greg M., and van de Geijn, Robert A. A family of high-performance matrix multiplication algorithms. In Proceedings of the International Conference on Computa- tional Sciences-Part I, ICCS ’01, pp. 51–60, London, UK, UK, 2001. Springer-Verlag. ISBN 3-540-42232-3. URL http://dl.acm.org/citation.cfm?id=645455.653765.  Irigoin, F. and Triolet, R. Supernode partitioning.  In Proceedings of the 15th ACM SIGPLAN- SIGACT Symposium on Principles of Programming Languages, POPL ’88, pp. 319–329, New York, NY, USA, 1988. ACM. ISBN 0-89791-252-7. doi: 10.1145/73560.73588. URL http: //doi.acm.org/10.1145/73560.73588.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Krizhevsky, Alex.  convnet2/.  cuda-convnet2, 2014. URL https://code.google.com/p/cuda-  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.  Imagenet classiﬁcation with deep convolutional neural networks. In Pereira, F., Burges, C.J.C., Bottou, L., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 25, pp. 1097–1105. Cur- ran Associates, Inc., 2012. URL http://papers.nips.cc/paper/4824-imagenet- classification-with-deep-convolutional-neural-networks.pdf.  Lavin, Andrew. maxdnn: An efﬁcient convolution kernel for deep learning with maxwell gpus.  CoRR, abs/1501.06633, 2015. URL http://arxiv.org/abs/1501.06633.  Lyons, Richard G. Understanding Digital Signal Processing. Addison-Wesley Longman Publishing  Co., Inc., Boston, MA, USA, 1st edition, 1996. ISBN 0201634678.  Mathieu, Micha¨el, Henaff, Mikael, and LeCun, Yann. Fast training of convolutional networks  through ffts. CoRR, abs/1312.5851, 2013. URL http://arxiv.org/abs/1312.5851.  Ragan-Kelley, Jonathan, Barnes, Connelly, Adams, Andrew, Paris, Sylvain, Durand, Fr´edo, and Amarasinghe, Saman P. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. In ACM SIGPLAN Conference on Program- ming Language Design and Implementation, PLDI ’13, Seattle, WA, USA, June 16-19, 2013, pp. 519–530, 2013. doi: 10.1145/2462156.2462176. URL http://doi.acm.org/10.1145/ 2462156.2462176.  13  Published as a conference paper at ICLR 2015  Ruetsch, Greg and Micikevicius, Paulius. Optimizing matrix transpose in cuda. Technical report,  NVIDIA Corp., January 2009.  Sermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Michael, Fergus, Rob, and LeCun, Yann. In Overfeat: Integrated recognition, localization and detection using convolutional networks. International Conference on Learning Representations (ICLR 2014). CBLS, April 2014. URL http://openreview.net/document/d332e77d-459a-4af8-b3ed-55ba.  Shin, Jaewook, Hall, Mary W., Chame, Jacqueline, Chen, Chun, Fischer, Paul F., and Hovland, Paul D. Speeding up nek5000 with autotuning and specialization. In Proceedings of the 24th ACM International Conference on Supercomputing, ICS ’10, pp. 253–262, New York, NY, USA, 2010. ACM. ISBN 978-1-4503-0018-6.  Valiant, Leslie G. A bridging model for parallel computation. Commun. ACM, 33(8):103–111, August 1990. ISSN 0001-0782. doi: 10.1145/79173.79181. URL http://doi.acm.org/ 10.1145/79173.79181.  Volkov, V. Better performance at lower occupancy. In GPU Technology Conference, 2010. URL  http://www.cs.berkeley.edu/˜volkov/volkov10-GTC.pdf.  14  Published as a conference paper at ICLR 2015  8 SUPPLEMENT  8.1 CUFFT CONVOLUTION PERFORMANCE BREAKDOWN  We show a breakdown of cuFFT convolution performance for the steps indicated in Table 1. The timings do not add up to 100% of the reported performance in the previous table because we do not report additional copies needed for zero-padding here. We also enforce force extra synchronizations to isolate the contribution of each operation. Abstracting from these details, the FFT and IFFT take up a signiﬁcant amount of compute resources, which we address in Section 5.  Table 5: cuFFT convolution performance breakdown (K40m, ms)  LAYER FFT A TRANS. A FFT B TRANS. B CGEMM TRANS. C IFFT C  L1 fprop bprop accGrad L2 fprop bprop accGrad L3 fprop bprop accGrad L4 fprop bprop accGrad L5 fprop bprop accGrad  0.86 0.86 1.14  2.99 2.99 5.94  3.07 3.08 3.07  0.84 0.83 0.84  7.07 7.07 2.40  0.24 0.24 0.32  0.98 0.98 2.04  0.89 0.89 0.89  0.24 0.24 0.24  1.58 1.59 0.51  1.13 34.55 34.60  0.32 10.26 10.26  15.13 12.62 12.37  12.67 0.39 0.26  36.46 1.19 0.91  5.91 5.92 5.93  3.08 3.07 3.06  0.83 0.83 0.82  2.39 2.40 2.38  2.03 2.03 2.02  0.89 0.90 0.89  0.24 0.24 0.24  0.51 0.51 0.52  8.92 8.85 8.38  4.40 4.05 4.03  1.21 1.13 1.10  6.23 5.59 6.18  1.67 1.67 0.83  0.87 0.86 0.87  0.24 0.23 0.24  0.50 0.51 1.54  6.24 6.23 3.15  3.49 3.48 3.48  0.95 0.94 0.95  2.54 2.54 7.51  In the particular case of L1, the FFTs take more than 50% of the runtime. This is due to the wasteful interpolation of the kernel tensor from a 11 × 11 up to 128 × 128, which is the minimal size to compute the FFT of the input array without interpolation loss. In such cases, the tiling strategy we are developing (see section 6) will result in large additional performance gains.  8.2 FFT : DECIMATION IN TIME VS FREQUENCY  A Fourier transform projects R and C-valued functions onto a harmonic orthogonal basis. The discrete Fourier transform of a vector {xk}, k ∈ [0, n − 1] is the vector:  {Xk} =   n−1  Xj=0  xjwkj  n   , k ∈ [0, n − 1]  where wj recursively decomposes the computation between an odd and even part:  n = e−2πij/n is the jth n-root of unity. The traditional radix-2 Cooley-Tukey algorithm  {Xk} =   (n−1)/2  (n−1)/2  xjwk(2j)  n  +  Xj=0  Xj=0  15  x2j+1wk(2j+1)  n    , k ∈ [1, n]  Published as a conference paper at ICLR 2015  Figure 9: DIT output ordered (left); DIF input ordered (right) (Burrus (2008))  This decomposition is called decimation in time (DIT). An alternate decomposition performs deci- mation in frequency (DIF):  {Xk} =   (n−1)/2  Xj=0  xjwkj  n +  n  Xj=(n−1)/2  xjwkj  n   , k ∈ [1, n]  When n is a power of 2, both decimations recursively decompose into a perfectly balanced tree and take advantage of the symmetry properties of the roots of unity. The dataﬂow graph for the radix-2 FFT has a butterﬂy shape and is a good way of visualizing the computations. There is a symmetry between DIT and DIF in both the order of operations applied and in whether the input or the output order is shufﬂed (Figure 9).  8.3 GPU PROGRAMMING  There are a variety of references available that describe CUDA and NVIDIA’s various GPU architec- tures (Garland et al. (2008)) which we won’t discuss in detail, but the implementation of fbfft very much depends upon speciﬁcs of the Kepler GPU architecture.  NVIDIA GPUs execute code at the granularity of a warp which is deﬁned as a set of 32 threads in all existing architectures; each thread is assigned a lane within the warp. These threads execute in a SIMT (single instruction, multiple thread) fashion, meaning that a warp is an atomic unit of execution. It holds a single program counter (PC) and can thus only execute a single instruction at a time across all of its threads. Collections of warps are brought together in blocks or CTAs, which together share a region of fast shared memory resident on chip. Blocks themselves can only exchange data via much slower global memory, resident on the GPU or in the host CPU’s address space.  Individual threads within a warp are free to take divergent paths, but since a single PC is present, each branch in the execution will be serialized. Threads that aren’t participating in the branch in question are disabled. In other words, if all 32 threads were to take divergent code paths, we would obtain only 1/32× of the computational efﬁciency. Divergent code paths are hard to avoid, but the NVIDIA instruction set has means to reduce their cost (Giles (2014)). One is with predicated instructions, which are used for small branches, in which all warp threads execute both parts of the branch, with non-participating threads having no side effects.  Block threads have access to a register ﬁle, with up to 255 registers per thread for Kepler. Registers are allocated statically by the CUDA compiler. An important performance factor when writing CUDA kernels is that data should be kept in registers as much as possible to avoid communications.  16  Published as a conference paper at ICLR 2015  Registers in CUDA are “addressable”: it is possible to declare a static array within registers and operate on its elements. The limitation is that all addressing should be performed using statically determined constants so the compiler can translate these accesses to a register number known at compile time. Indirect addressing is also supported but results in copies to a local region within global memory, which essentially constitutes register spilling. Even with the presence of caches, using local memory usually comes with a performance hit.11 As a consequence, we design our kernels using aggressive inlining, template parameters and unrolling directives to make all register accesses statically addressable.  The Kepler architecture introduced specialized shufﬂe instructions to exchange data between regis- ters within a warp synchronously, which avoids round-trips to shared or global memory. Interest- ingly, these shufﬂe instructions allow the dynamic indexing of an array held in registers, as long as the array is distributed in a cyclic fashion across registers in each thread within a warp.  float arr[3]; ... // This simulates a linear array float realArr[96]: // arr[0] holds elements 0-31 (lane i holds element i) // arr[1] holds elements 32-63 (lane i holds element 32 + i) // arr[2] holds elements 64-95 (lane i holds element 64 + i) // Example: all warp threads read value held at realArr[34] float val = __shfl(arr[1], 2); // ‘1‘ must be statically known  // ‘2‘ can be dynamic  Many warps run in parallel and can be switched by the GPU hardware at each cycle. When enough parallelism is available (measured in occupancy of the GPU as a ﬁrst approximation), long latency operations are hidden thanks to fast context switching. Registers and shared memory come in ﬁnite quantities on each GPU compute multiprocessor. These limited resources are partitioned by the compiler and the hardware amongst computations at the level of a CUDA kernel. Increased usage of registers or of shared memory can reduce GPU occupancy, which limits the ability to hide long latency operations. Reduced occupancy does not necessarily result in performance loss (Volkov (2010)). There are often non-obvious performance tradeoffs in increasing or decreasing threads per block, shared memory per block or registers per thread that are hard to discover. This problem is one of the many reasons why designing a one-size-ﬁts-all implementation that aims to be efﬁcient for any problem is difﬁcult.  11There are bleeding edge cases where a little local memory consumption helps performance; for instance,  when restricting the number of registers per thread to increase occupancy.  17  ",
1412.6623,2015,Word Representations via Gaussian Embedding,"['Word Representations via Gaussian Embedding', 'Luke Vilnis and Andrew McCallum']",https://arxiv.org/pdf/1412.6623,"5 1 0 2     y a M 1         ] L C . s c [      4 v 3 2 6 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  WORD REPRESENTATIONS VIA GAUSSIAN EMBEDDING  Luke Vilnis, Andrew McCallum School of Computer Science University of Massachusetts Amherst Amherst, MA 01003 luke@cs.umass.edu, mccallum@cs.umass.edu  ABSTRACT  Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representa- tion and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distribu- tions. We compare performance on various word embedding benchmarks, inves- tigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.  1  INTRODUCTION  In recent years there has been a surge of interest in learning compact distributed representations or embeddings for many machine learning tasks, including collaborative ﬁltering (Koren et al., 2009), image retrieval (Weston et al., 2011), relation extraction (Riedel et al., 2013), word semantics and language modeling (Bengio et al., 2006; Mnih & Hinton, 2008; Mikolov et al., 2013), and many others. In these approaches input objects (such as images, relations or words) are mapped to dense vectors having lower-dimensionality than the cardinality of the inputs, with the goal that the ge- ometry of his low-dimensional latent embedded space be smooth with respect to some measure of similarity in the target domain. That is, objects associated with similar targets should be mapped to nearby points in the embedded space. While this approach has proven powerful, representing an object as a single point in space carries some important limitations. An embedded vector representing a point estimate does not naturally express uncertainty about the target concepts with which the input may be associated. Point vec- tors are typically compared by dot products, cosine-distance or Euclean distance, none of which provide for asymmetric comparisons between objects (as is necessary to represent inclusion or en- tailment). Relationships between points are normally measured by distances required to obey the triangle inequality. This paper advocates moving beyond vector point representations to potential functions (Aizerman et al., 1964), or continuous densities in latent space. In particular we explore Gaussian function embeddings (currently with diagonal covariance), in which both means and variances are learned from data. Gaussians innately represent uncertainty, and provide a distance function per object. KL- divergence between Gaussian distributions is straightforward to calculate, naturally asymmetric, and has a geometric interpretation as an inclusion between families of ellipses. There is a long line of previous work in mapping data cases to probability distributions, perhaps the most famous being radial basis functions (RBFs), used both in the kernel and neural network literature. We draw inspiration from this work to propose novel word embedding algorithms that embed words directly as Gaussian distributional potential functions in an inﬁnite dimensional func- tion space. This allows us to map word types not only to vectors but to soft regions in space, modeling uncertainty, inclusion, and entailment, as well as providing a rich geometry of the latent space.  1  Published as a conference paper at ICLR 2015  Figure 1: Learned diagonal vari- ances, as used in evaluation (Section 6), for each word, with the ﬁrst let- ter of each word indicating the po- sition of its mean. We project onto generalized eigenvectors between the mixture means and variance of query word Bach. Nearby words to Bach are other composers e.g. Mozart, which lead to similar pictures.  After discussing related work and presenting our algorithms below we explore properties of our al- gorithms with multiple qualitative and quantitative evaluation on several real and synthetic datasets. We show that concept containment and speciﬁcity matches common intuition on examples concern- ing people, genres, foods, and others. We compare our embeddings to Skip-Gram on seven standard word similarity tasks, and evaluate the ability of our method to learn unsupervised lexical entail- ment. We also demonstrate that our training method also supports new styles of supervised training that explicitly incorporate asymmetry into the objective.  2 RELATED WORK  This paper builds on a long line of work on both distributed and distributional semantic word vec- tors, including distributional semantics, neural language models, count-based language models, and, more broadly, the ﬁeld of representation learning. Related work in probabilistic matrix factorization (Mnih & Salakhutdinov, 2007) embeds rows and columns as Gaussians, and some forms of this do provide each row and column with its own vari- ance (Salakhutdinov & Mnih, 2008). Given the parallels between embedding models and matrix factorization (Deerwester et al., 1990; Riedel et al., 2013; Levy & Goldberg, 2014), this is relevant to our approach. However, these Bayesian methods apply Bayes’ rule to observed data to infer the latent distributions, whereas our model works directly in the space of probability distributions and discriminatively trains them. This allows us to go beyond the Bayesian approach and use arbitrary (and even asymmetric) training criteria, and is more similar to methods that learn kernels (Lanckriet et al., 2004) or function-valued neural networks such as mixture density networks (Bishop, 1994). Other work in multiplicative tensor factorization for word embeddings (Kiros et al., 2014) and met- ric learning (Xing et al., 2002) learns some combinations of representations, clusters, and a distance metric jointly; however, it does not effectively learn a distance function per item. Fitting Gaussian mixture models on embeddings has been done in order to apply Fisher kernels to entire documents (Clinchant & Perronnin, 2013b;a). Preliminary concurrent work from Kiyoshiyo et al. (2014) de- scribes a signiﬁcantly different model similar to Bayesian matrix factorization, using a probabilistic Gaussian graphical model to deﬁne a distribution over pairs of words, and they lack quantitative experiments or evaluation. In linguistic semantics, work on the distributional inclusion hypothesis (Geffet & Dagan, 2005), uses traditional count-based vectors to deﬁne regions in vector space (Erk, 2009) such that subordinate concepts are included in these regions. In fact, one strength of our proposed work is that we extend these intuitively appealing ideas (as well as the ability to use a variety of asymmetric distances between vectors) to the dense, low-dimensional distributed vectors that are now gaining popularity.  3 BACKGROUND Our goal is to map every word type w in some dictionary D and context word type c in a dictionary C to a Gaussian distribution over a latent embedding space, such that linguistic properties of the words  2  Published as a conference paper at ICLR 2015  are captured by properties of and relationships between the distributions. For precision, we call an element of the dictionary a word type, and a particular observed token in some context a word token. This is analogous to the class vs. instance distinction in object-oriented programming. In unsupervised learning of word vectors, we observe a sequence of word tokens {t(w)i} for each type w, and their contexts (sets of nearby word tokens), {c(w)i}. The goal is to map each word type w and context word type c to a vector, such that types that appear in similar contexts have similar vectors. When it is unambiguous, we also use the variables w and c to denote the vectors associated to that given word type or context word type. An energy function (LeCun et al., 2006) is a function Eθ(x, y) that scores pairs of inputs x and outputs y, parametrized by θ. The goal of energy-based learning is to train the parameters of the energy function to score observed positive input-output pairs higher (or lower, depending on sign conventions) than negative pairs. This is accomplished by means of a loss function L which deﬁnes which pairs are positive and negative according to some supervision, and provides gradients on the parameters given the predictions of the energy function. In prediction-based (energy-based) word embedding models, the parameters θ correspond to our learned word representations, and the x and y input-output pairs correspond to word tokens and their contexts. These contexts can be either positive (observed) or negative (often randomly sampled). In the word2vec Skip-Gram (Mikolov et al., 2013) word embedding model, the energy function takes the form of a dot product between the vectors of an observed word and an observed context w(cid:62)c. The loss function is a binary logistic regression classiﬁer that treats the score of a word and its observed context as the score of a positive example, and the score of a word and a randomly sampled context as the score of a negative example. Backpropagating (Rumelhart et al., 1986) this loss to the word vectors trains them to be predictive of their contexts, achieving the desired effect (words in similar contexts have similar vectors). In recent work, word2vec has been shown to be equivalent to factoring certain types of weighted pointwise mutual information matrices (Levy & Goldberg, 2014). In our work, we use a slightly different loss function than Skip-Gram word2vec embeddings. Our energy functions take on a more limited range of values than do vector dot products, and their dynamic ranges depend in complex ways on the parameters. Therefore, we had difﬁculty using the word2vec loss that treats scores of positive and negative pairs as positive and negative examples to a binary classiﬁer, since this relies on the ability to push up on the energy surface in an absolute, rather than relative, manner. To avoid the problem of absolute energies, we train with a ranking-based loss. We chose a max-margin ranking objective, similar to that used in Rank-SVM (Joachims, 2002) or Wsabie (Weston et al., 2011), which pushes scores of positive pairs above negatives by a margin:  Lm(w, cp, cn) = max(0, m − E(w, cp) + E(w, cn))  In this terminology, the contribution of our work is a pair of energy functions for training Gaussian distributions to represent word types.  4 WARMUP: EMPIRICAL COVARIANCES  Given a pre-trained set of word embeddings trained from contexts, there is a simple way to construct variances using the empirical variance of a word type’s set of context vectors. For a word w with N word vector sets {c(w)i} representing the words found in its contexts, and window size W , the empirical variance is  N(cid:88)  W(cid:88)  i  j  Σw =  1  N W  (c(w)ij − w)(c(w)ij − w)(cid:62)  This is an estimator for the covariance of a distribution assuming that the mean is ﬁxed at w. In practice, it is also necessary to add a small ridge term δ > 0 to the diagonal of the matrix to regularize and avoid numerical problems when inverting. However, in Section 6.2 we note that the distributions learned by this empirical estimator do not possess properties that we would want from Gaussian distributional embeddings, such as unsuper- vised entailment represented as inclusion between ellipsoids. By discriminatively embedding our  3  Published as a conference paper at ICLR 2015  predictive vectors in the space of Gaussian distributions, we can improve this performance. Our models can learn certain forms of entailment during unsupervised training, as discussed in Section 6.2 and exempliﬁed in Figure 1.  5 ENERGY-BASED LEARNING OF GAUSSIANS  As discussed in Section 3, our architecture learns Gaussian distributional embeddings to predict words in context given the current word, and ranks these over negatively sampled words. We present two energy functions to train these embeddings.  5.1 SYMMETRIC SIMILARITY: EXPECTED LIKELIHOOD OR PROBABILITY PRODUCT  KERNEL  While the dot product between two means of independent Gaussians is a perfectly valid measure of similarity (it is the expected dot product), it does not incorporate the covariances and would not enable us to gain any beneﬁt from our probabilistic model. The most logical next choice for a symmetric similarity function would be to take the inner product between the distributions themselves. Recall that for two (well-behaved) functions f, g ∈ Rn → R, a standard choice of inner product is  (cid:90)  i.e. the continuous version of(cid:80)  f (x)g(x)dx  x∈Rn  i figi = (cid:104)f, g(cid:105) for discrete vectors f and g.  (cid:90)  This idea seems very natural, and indeed has appeared before – the idea of mapping data cases w into probability distributions (often over their contexts), and comparing them via integrals has a history under the name of the expected likelihood or probability product kernel (Jebara et al., 2004). For Gaussians, the inner product is deﬁned as  E(Pi, Pj) =  x∈Rn  N (x; µi, Σi)N (x; µj, Σj)dx = N (0; µi − µj, Σi + Σj)  The proof of this identity follows from simple calculus. This is a consequence of the broader fact that the Gaussian is a stable distribution, i.e. the convolution of two Gaussian random variables is another Gaussian. Since we aim to discriminatively train the weights of the energy function, and it is always positive, we work not with this quantity directly, but with its logarithm. This has two motivations: ﬁrstly, we plan to use ranking loss, and ratios of densities and likelihoods are much more commonly worked with than differences – differences in probabilities are less interpretable than an odds ratio. Secondly, it is easier numerically, as otherwise the quantities can get exponentially small and harder to deal with. The logarithm of the energy (in d dimensions) is log N (0; µi−µj, Σi+Σj) = − 1 2  log(2π). ∂A log det A = A−1, and the gradient Recalling that the gradient of the log determinant is ∂A x(cid:62)A−1y = −A−(cid:62)xy(cid:62)A−(cid:62) (Petersen, 2006) we can take the gradient of this energy function ∂ with respect to the means µ and covariances Σ:  (µi−µj)(cid:62)(Σi+Σj)−1(µi−µj)− d 2  log det(Σi+Σj)− 1 2  ∂  ∂ log E(Pi, Pj)  ∂µi  = − ∂ log E(Pi, Pj)  ∂µj  = −∆ij  ∂ log E(Pi, Pj)  ∂Σi  ∂ log E(Pi, Pj)  = where ∆ij = (Σi + Σj)−1(µi − µj)  ∂Σj  =  1 2  (∆ij∆(cid:62)  ij − (Σi + Σj)−1)  For diagonal and spherical covariances, these matrix inverses are trivial to compute, and even in the full-matrix case can be solved very efﬁciently for the small dimensionality common in embedding  4  Published as a conference paper at ICLR 2015  models. If the matrices have a low-rank plus diagonal structure, they can be computed and stored even more efﬁciently using the matrix inversion lemma. This log-energy has an intuitive geometric interpretation as a similarity measure. Gaussians are measured as close to one another based on the distance between their means, as measured through the Mahalanobis distance deﬁned by their joint inverse covariance. Recalling that log det A + const. is equivalent to the log-volume of the ellipse spanned by the principle components of A, we can interpret this other term of the energy as a regularizer that prevents us from decreasing the distance by only increasing joint variance. This combination pushes the means together while encouraging them to have more concentrated, sharply peaked distributions in order to have high energy.  5.2 ASYMMETRIC SIMILARITY: KL DIVERGENCE  Training vectors through KL-divergence to encode their context distributions, or even to incorporate more explicit directional supervision re: entailment from a knowledge base or WordNet, is also a sensible objective choice. We optimize the following energy function (which has a similarly tractable closed form solution for Gaussians):  −E(Pi, Pj) = DKL(Nj||Ni) =  x∈Rn  N (x; µi, Σi) log  N (x; µj, Σj) N (x; µi, Σi) (µi − µj) − d − log  dx  det(Σj) det(Σi)  )  =  1 2  (tr(Σ−1  i Σj) + (µi − µj)(cid:62)Σ−1  i  (cid:90)  Note the leading negative sign (we deﬁne the negative energy), since KL is a distance function and not a similarity. KL divergence is a natural energy function for representing entailment between concepts – a low KL divergence from x to y indicates that we can encode y easily as x, implying that y entails x. This can be more intuitively visualized and interpreted as a soft form of inclusion between the level sets of ellipsoids generated by the two Gaussians – if there is a relatively high expected log-likelihood ratio (negative KL), then most of the mass of y lies inside x. Just as in the previous case, we can compute the gradients for this energy function in closed form:  ∂E(Pi, Pj)  ∂µi  ∂E(Pi, Pj)  ∂Σi  ∂E(Pi, Pj)  ∂Σj  (Σ−1  =  i + ∆(cid:48)  ij∆  ij − Σ−1 (cid:48)(cid:62)  i  )  = − ∂E(Pi, Pj)  = −∆(cid:48)  ij  ∂µj i ΣjΣ−1 j − Σ−1 ) ij = Σ−1  i  i  (Σ−1 = where ∆(cid:48)  1 2 1 2  (µi − µj)  using the fact that ∂ 2006).  ∂A tr(X(cid:62)A−1Y ) = −(A−1Y X(cid:62)A−1)(cid:62) and ∂  ∂A tr(XA) = X(cid:62) (Petersen,  5.3 UNCERTAINTY OF INNER PRODUCTS  Another beneﬁt of embedding objects as probability distributions is that we can look at the distribu- tion of dot products between vectors drawn from two Gaussian representations. This distribution is not itself a one-dimensional Gaussian, but it has a ﬁnite mean and variance with a simple structure in the case where the two Gaussians are assumed independent (Brown & Rutemiller, 1977). For the distribution P (z = x(cid:62)y), we have  y Σyµy + tr(ΣxΣy)  this means we can ﬁnd e.g. a lower or upper bound on the dot products of random samples from these distributions, that should hold some given percent of the time. Parametrizing this energy by some number of standard deviations c, we can also get a range for the dot product as:  x µy ± c µ(cid:62)  µ(cid:62) x Σxµx + µ(cid:62)  y Σyµy + tr(ΣxΣy)  µz = µ(cid:62) Σz = µ(cid:62)  x µy x Σxµx + µ(cid:62) (cid:113)  5  Published as a conference paper at ICLR 2015  We can choose c in a principled using an (incorrect) Gaussian approximation, or more general con- centration bounds such as Chebyshev’s inequality.  5.4 LEARNING  To learn our model, we need to pick an energy function (EL or KL), a loss function (max-margin), and a set of positive and negative training pairs. As the landscape is highly nonconvex, it is also helpful to add some regularization. We regularize the means and covariances differently, since they are different types of geometric objects. The means should not be allowed to grow too large, so we can add a simple hard constraint to the (cid:96)2 norm:  (cid:107)µi(cid:107)2 ≤ C, ∀i  However, the covariance matrices need to be kept positive deﬁnite as well as reasonably sized. This is achieved by adding a hard constraint that the eigenvalues λi lie within the hypercube [m, M ]d for constants m and M.  mI ≺ Σi ≺ M I, ∀i  For diagonal covariances, this simply involves either applying the min or max function to each element of the diagonal to keep it within the hypercube, Σii ← max(m, min(M, Σii)). Controlling the bottom eigenvalues of the covariance is especially important when training with expected likelihood, since the energy function includes a log det term that can give very high scores to small covariances, dominating the rest of the energy. We optimize the parameters using AdaGrad (Duchi et al., 2011) and stochastic gradients in small minibatches containing 20 sentences worth of tokens and contexts.  6 EVALUATION  We evaluate the representation learning algorithms on several qualitative and quantitative tasks, including modeling asymmetric and linguistic relationships, uncertainty, and word similarity. All Gaussian experiments are conducted with 50-dimensional vectors, with diagonal variances except where noted otherwise. Unsupervised embeddings are learned on the concatenated ukWaC and WaCkypedia corpora (Baroni et al., 2009), consisting of about 3 billion tokens. This matches the experimental setup used by Baroni et al. (2012), aside from leaving out the small British National Corpus, which is not publicly available and contains only 100 million tokens. All word types that appear less than 100 times in the training set are dropped, leaving a vocabulary of approximately 280 thousand word types. When training word2vec Skip-Gram embeddings for baselines, we follow the above training setup (50 dimensional embeddings), using our own implementation of word2vec to change as little as possible between the two models, only the loss function. We train both models with one pass over the data, using separate embeddings for the input and output contexts, 1 negative sample per positive example, and the same subsampling procedure as in the word2vec paper (Mikolov et al., 2013). The only other difference between the two training regimes is that we use a smaller (cid:96)2 regularization constraint when using the word2vec loss function, which improves performance vs. the diagonal Gaussian model which does better with “spikier” mean embeddings with larger norms (see the comment in Section 6.4). The original word2vec implementation uses no (cid:96)2 constraint, but we saw better performance when including it in our training setup.  6.1 SPECIFICITY AND UNCERTAINTY OF EMBEDDINGS  In Figure 2, we examine some of the 100 nearest neighbors of several query words as we sort from largest to smallest variance, as measured by determinant of the covariance matrix, using diagonal Gaussian embeddings. Note that more speciﬁc words, such as joviality and electroclash have smaller variance, while polysemous words or those denoting broader concepts have larger variances, such as mix, mind, and graph. This is not merely an artifact of higher frequency words getting more variance – when sorting by those words whose rank by frequency and rank by variance are most dissimilar, we see that genres with names like chillout, avant, and shoegaze overindex their variance compared  6  Published as a conference paper at ICLR 2015  Query Word Nearby Words, Descending Variance rock  food feeling  algebra  mix sound blue folk jazz rap avant hardcore chillout shoegaze powerpop electroclash drink meal meat diet spice juice bacon soya gluten stevia sense mind mood perception compassion sadness coldness sincerity perplexity difﬁdence joviality theory graph equivalence ﬁnite predicate congruence topology quaternion symplectic homomorphism  Figure 2: Elements of the top 100 nearest neighbor sets for chosen query words, sorted by descend- ing variance (as measured by determinant of covariance matrix). Note that less speciﬁc and more ambiguous words have greater variance.  Test  Model Baroni et al. (2012) E E Empirical (D) E Empirical (D) Empirical (S) E E Empirical (S) E Learned (D) E Learned (D) E Learned (S) Learned (S) E  Similarity Best F1 AP balAPinc KL Cos KL Cos KL Cos KL Cos  75.1 70.05 76.24 71.18 76.24 79.01 76.99 79.34 77.36  – .68 .71 .69 .71 .80 .73 .78 .73  Figure 3: Entailment: We compare empirical and learned variances, both diagonal (D) and spherical (S). E is the dataset of Baroni et al. (2012). Measures of similarity are symmetric (cosine between means) and asymmetric (KL) divergence for Gaussians. balAPinc is an asymmetric similarity mea- sure speciﬁc to sparse, distributional count-based representations.  to how frequent they are, since they appear in different contexts. Similarly, common emotion words like sadness and sincerity have less variance than their frequency would predict, since they have fairly ﬁxed meanings. Another emotion word, coldness, is an uncommon word with a large variance due to its polysemy.  6.2 ENTAILMENT  As can be seen qualitatively in Figure 1, our embeddings can learn some forms of unsupervised entailment directly from the source data. We evaluate quantitatively on the Entailment dataset of Baroni et al. (2012). Our setup is essentially the same as theirs but uses slightly less data, as men- tioned in the beginning of this section. We evaluate with Average Precision and best F1 score. We include the best F1 score (by picking the optimal threshold at test) because this is used by Baroni et al. (2012), but we believe AP is better to demonstrate the correlation of various asymmetric and symmetric measures with the entailment data. In Figure 3, we compare variances learned jointly during embedding training by using the expected likelihood objective, with empirical variances gathered from contexts on pre-trained word2vec-style embeddings. We compare both diagonal (D) and spherical (S) variances, using both cosine similarity between means, and KL divergence. Baseline asymmetric measurements, such as the difference between the sizes of the two embeddings, did worse than the cosine. We see that KL divergence between the entailed and entailing word does not give good performance for the empirical variances, but beats the count-based balAPinc measure when used with learned variances. For the baseline empirical model to achieve reasonable performance when using KL divergence, we regularized the covariance matrices, as the unregularized matrices had very small entries. We regularized the empirical covariance by adding a small ridge δ to the diagonal, which was tuned to maximize performance, to give the largest possible advantage to the baseline model. Interestingly, the empirical variances do worse with KL than the symmetric cosine similarity when predicting en- tailment. This appears to be because the empirically learned variances are so small that the choice is  7  Published as a conference paper at ICLR 2015  Figure 4: Synthetic experiments on embedding two simple hierarchies in two dimensions directly using KL divergence. The embedding model captures all of the hierarchical relationships present in the tree. Sibling leaves are pushed into overlapping areas by the objective function.  between either leaving them small, making it very difﬁcult to have one Gaussian located “inside” an- other Gaussian, or regularizing so much that their discriminative power is washed out. Additionally, when examining the empirical variances, we noted that common words like “such,” which receive very large variances in our learned model, have much smaller empirical variances relative to rarer words. A possible explanation is that the contrastive objective forces variances of commonly sam- pled words to spread out to avoid loss, while the empirical variance sees only “positive examples” and has no penalty for being close to many contexts at once. While these results indicate that we can do as well or better at unsupervised entailment than previ- ous distributional semantic measures, we would like to move beyond purely unsupervised learning. Although certain forms of entailment can be learned in an unsupervised manner from distributional data, many entailing relationships are not present in the training text in the form of lexical substitu- tions that reﬂect the is-a relationship. For example, one might see phrases such as “look at that bird,” “look at that eagle,” “look at that dog,” but rarely “look at that mammal.” One appealing aspect of our models versus count-based ones is that they can be directly discriminatively trained to embed hierarchies.  6.3 DIRECTLY LEARNING ASYMMETRIC RELATIONSHIPS  In Figure 4, we see the results of directly embedding simple tree hierarchies as Gaussians. We embed nodes as Gaussians with diagonal variances in two-dimensional space using gradient descent on the KL divergence between parents and children. We create a Gaussian for each node in the tree, and randomly initialize means. Negative contexts come from randomly sampled nodes that are neither ancestors nor descendents, while positive contexts come from ancestors or descendents using the appropriate directional KL divergence. Unlike our experiments with symmetric energy, we must use the same set of embeddings for nodes and contexts, or else the objective function will push the variances to be unboundedly large. Our training process captures the hierarchical relationships, although leaf-level siblings are not differentiated from each other by this objective function. This is because out of all the negative examples that a leaf node can receive, only one will push it away from its sibling node.  6.4 WORD SIMILARITY BENCHMARKS  We evaluate the embeddings on seven different standard word similarity benchmarks (Rubenstein & Goodenough, 1965; Szumlanski et al., 2013; Hill et al., 2014; Miller & Charles, 1991; Bruni et al., 2014; Yang & Powers, 2006; Finkelstein et al., 2001). A comparison to all of the state of the art word-embedding numbers for different dimensionalities as in (Baroni et al., 2014) is out of the scope of this evaluation. However, we note that the overall performance of our 50-dimensional embeddings matches or beats reported numbers on these datasets for the 80-dimensional Skip-Gram vectors at wordvectors.org (Faruqui & Dyer, 2014), as well as our own Skip-Gram implementation. Note that the numbers are not directly comparable since we use a much older version of Wikipedia (circa 2009) in our WaCkypedia dataset, but this should not give us an edge.  8  Published as a conference paper at ICLR 2015  SG (50d) Dataset 29.39 SimLex 59.89 WordSim WordSim-S 69.86 WordSim-R 53.03 70.27 MEN 63.96 MC 70.01 RG YP 39.34 49.14 Rel-122  SG (100d) LG/50/m/S LG/50/d/S LG/50/m/D LG/50/d/D 31.13 59.33 70.19 54.64 70.70 66.76 69.38 35.76 51.26  31.25 62.12 74.64 54.44 71.30 67.01 70.41 36.05 52.28  32.23 65.49 76.15 58.96 71.31 70.41 71.00 41.50 53.74  29.84 62.03 73.92 54.37 69.65 69.17 74.76 42.55 51.09  30.50 61.00 72.79 53.36 70.18 68.50 77.00 39.30 53.54  Figure 5: Similarity: We evaluate our learned Gaussian embeddings (LG) with spherical (S) and diagonal (D) variances, on several word similarity benchmarks, compared against standard Skip- Gram (SG) embeddings on the trained on the same dataset. We evaluate Gaussian embeddings with both cosine between means (m), and cosine between the distributions themselves (d) as deﬁned by the expected likelihood inner product.  While it is good to sanity-check that our embedding algorithms can achieve standard measures of distributional quality, these experiments also let us compare the different types of variances (spher- ical and diagonal). We also compare against Skip-Gram embeddings with 100 latent dimensions, since our diagonal variances have 50 extra parameters. We see that the embeddings with spherical covariances have an overall slight edge over the embed- dings with diagonal covariances in this case, in a reversal from the entailment experiments. This could be due to the diagonal variance matrices making the embeddings more axis-aligned, making it harder to learn all the similarities and reducing model capacity. To test this theory, we plotted the absolute values of components of spherical and diagonal variance mean vectors on a q-q plot and noted a signiﬁcant off-diagonal shift, indicating that diagonal variance embedding mean vectors have “spikier” distributions of components, indicating more axis-alignment. We also see that the distributions with diagonal variances beneﬁt more from including the variance in the comparison (d) than the spherical variances. Generally, the data sets in which the cosine between distributions (d) outperforms cosine between means (m) are similar for both spherical and diagonal covariances. Using the cosine between distributions never helped when using empirical variances, so we do not include those numbers.  7 CONCLUSION AND FUTURE WORK  In this work we introduced a method to embed word types into the space of Gaussian distribu- tions, and learn the embeddings directly in that space. This allows us to represent words not as low-dimensional vectors, but as densities over a latent space, directly representing notions of uncer- tainty and enabling a richer geometry in the embedded space. We demonstrated the effectiveness of these embeddings on a linguistic task requiring asymmetric comparisons, as well as standard word similarity benchmarks, learning of synthetic hierarchies, and several qualitative examinations. In future work, we hope to move beyond spherical or diagonal covariances and into combinations of low rank and diagonal matrices. Efﬁcient updates and scalable learning is still possible due to the Sherman-Woodbury-Morrison formula. Additionally, going beyond diagonal covariances will enable us to keep our semantics from being axis-aligned, which will increase model capacity and expressivity. We also hope to move past stochastic gradient descent and warm starting and be able to learn the Gaussian representations robustly in one pass from scratch by using e.g. proximal or block coordinate descent methods. Improved optimization strategies will also be helpful on the highly nonconvex problem of training supervised hierarchies with KL divergence. Representing words and concepts as different types of distributions (including other elliptic distri- butions such as the Student’s t) is an exciting direction – Gaussians concentrate their density on a thin spherical ellipsoidal shell, which can lead to counterintuitive behavior in high dimensions. Multimodal distributions represent another clear avenue for future work. Combining ideas from  9  Published as a conference paper at ICLR 2015  kernel methods and manifold learning with deep learning and linguistic representation learning is an exciting frontier. In other domains, we want to extend the use of potential function representations to other tasks requiring embeddings, such as relational learning with the universal schema (Riedel et al., 2013). We hope to leverage the asymmetric measures, probabilistic interpretation, and ﬂexible training criteria of our model to tackle tasks involving similarity-in-context, comparison of sentences and paragraphs, and more general common sense reasoning.  8 ACKNOWLEDGEMENTS  This work was supported in part by the Center for Intelligent Information Retrieval, in part by IARPA via DoI/NBC contract #D11PC20152, and in part by NSF grant #CNS-0958392 The U.S. Govern- ment is authorized to reproduce and distribute reprint for Governmental purposes notwithstanding any copyright annotation thereon. Any opinions, ﬁndings and conclusions or recommendations ex- pressed in this material are those of the authors and do not necessarily reﬂect those of the sponsor.  REFERENCES Aizerman, M. A., Braverman, E. A., and Rozonoer, L. Theoretical foundations of the potential function method in pattern recognition learning. In Automation and Remote Control,, number 25 in Automation and Remote Control,, pp. 821–837, 1964.  Baroni, Marco, Bernardini, Silvia, Ferraresi, Adriano, and Zanchetta, Eros. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209–226, 2009.  Baroni, Marco, Bernardi, Raffaella, Do, Ngoc-Quynh, and Shan, Chung-chieh. Entailment above the word level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, pp. 23–32, Stroudsburg, PA, USA, 2012. Association for Computational Linguistics.  Baroni, Marco, Dinu, Georgiana, and Kruszewski, Germ´an. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 238–247. Association for Computational Linguistics, 2014.  Bengio, Yoshua, Schwenk, Holger, Sen´ecal, Jean-S´ebastien, Morin, Fr´ederic, and Gauvain, Jean- Luc. Neural probabilistic language models. In Innovations in Machine Learning, pp. 137–186. Springer, 2006.  Bishop, Christopher M. Mixture density networks, 1994.  Brown, Gerald G and Rutemiller, Herbert C. Means and variances of stochastic vector products with  applications to random linear models. Management Science, 24(2):210–216, 1977.  Bruni, Elia, Tran, Nam-Khanh, and Baroni, Marco. Multimodal distributional semantics. JAIR,  2014.  Clinchant, St´ephane and Perronnin, Florent. Textual similarity with a bag-of-embedded-words model. In Proceedings of the 2013 Conference on the Theory of Information Retrieval, ICTIR ’13, pp. 25:117–25:120, New York, NY, USA, 2013a. ACM.  Clinchant, St´ephane and Perronnin, Florent. Aggregating continuous word embeddings for infor-  mation retrieval. ACL 2013, pp. 100, 2013b.  Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., and Harshman, R.A. Indexing by latent semantic analysis. Journal of the American Society for Information Science 41, pp. 391–407, 1990.  Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning  and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.  10  Published as a conference paper at ICLR 2015  Erk, Katrin. Representing words as regions in vector space. In Proceedings of the Thirteenth Confer- ence on Computational Natural Language Learning, pp. 57–65. Association for Computational Linguistics, 2009.  Faruqui, Manaal and Dyer, Chris. Community evaluation and exchange of word vectors at word- vectors.org. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Baltimore, USA, June 2014. Association for Computational Linguistics.  Finkelstein, Lev, Gabrilovich, Evgeniy, Matias, Yossi, Rivlin, Ehud, Solan, Zach, Wolfman, Gadi, and Ruppin, Eytan. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pp. 406–414. ACM, 2001.  Geffet, Maayan and Dagan, Ido. The distributional inclusion hypotheses and lexical entailment. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pp. 107–114. Association for Computational Linguistics, 2005.  Hill, Felix, Reichart, Roi, and Korhonen, Anna. Simlex-999: Evaluating semantic models with  (genuine) similarity estimation. arXiv preprint arXiv:1408.3456, 2014.  Jebara, Tony, Kondor, Risi, and Howard, Andrew. Probability product kernels. The Journal of  Machine Learning Research, 5:819–844, 2004.  Joachims, Thorsten. Optimizing search engines using clickthrough data.  In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 133–142. ACM, 2002.  Kiros, Ryan, Zemel, Richard, and Salakhutdinov, Ruslan R. A multiplicative model for learning  distributed text-based attribute representations. In NIPS, 2014.  Kiyoshiyo, Shimaoka, Masayasu, Muraoka, Futo, Yamamoto, Watanabe, Yotaro, Okazaki, Naoaki, and Inui, Kentaro. Distribution representation of the meaning of words and phrases by a gaussian distribution. In Language Processing Society 20th Annual Conference (In Japanese), 2014.  Koren, Yehuda, Bell, Robert, and Volinsky, Chris. Matrix factorization techniques for recommender  systems. Computer, 42(8):30–37, August 2009. ISSN 0018-9162.  Lanckriet, Gert RG, Cristianini, Nello, Bartlett, Peter, Ghaoui, Laurent El, and Jordan, Michael I. Learning the kernel matrix with semideﬁnite programming. The Journal of Machine Learning Research, 5:27–72, 2004.  LeCun, Yann, Chopra, Sumit, and Hadsell, Raia. A tutorial on energy-based learning. 2006.  Levy, Omer and Goldberg, Yoav. Neural word embedding as implicit matrix factorization.  Advances in Neural Information Processing Systems, pp. 2177–2185, 2014.  In  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed repre- sentations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp. 3111–3119, 2013.  Miller, George A and Charles, Walter G. Contextual correlates of semantic similarity. Language  and cognitive processes, 6(1):1–28, 1991.  Mnih, Andriy and Hinton, Geoffrey E. A scalable hierarchical distributed language model.  Advances in neural information processing systems, pp. 1081–1088, 2008.  In  Mnih, Andriy and Salakhutdinov, Ruslan. Probabilistic matrix factorization. In Advances in neural  information processing systems, pp. 1257–1264, 2007.  Petersen, Kaare Brandt. The matrix cookbook. 2006.  Riedel, Sebastian, Yao, Limin, McCallum, Andrew, and Marlin, Benjamin M. Relation extraction  with matrix factorization and universal schemas. 2013.  11  Published as a conference paper at ICLR 2015  Rubenstein, Herbert and Goodenough, John B. Contextual correlates of synonymy. Commun. ACM,  8(10):627–633, October 1965. ISSN 0001-0782.  Rumelhart, D.E., Hintont, G.E., and Williams, R.J. Learning representations by back-propagating  errors. Nature, 323(6088):533–536, 1986.  Salakhutdinov, Ruslan and Mnih, Andriy. Bayesian probabilistic matrix factorization using markov chain monte carlo. In Proceedings of the 25th international conference on Machine learning, pp. 880–887. ACM, 2008.  Szumlanski, Sean R, Gomez, Fernando, and Sims, Valerie K. A new set of norms for semantic  relatedness measures. In ACL, 2013.  Weston, Jason, Bengio, Samy, and Usunier, Nicolas. Wsabie: Scaling up to large vocabulary image  annotation. In IJCAI, volume 11, pp. 2764–2770, 2011.  Xing, Eric P, Jordan, Michael I, Russell, Stuart, and Ng, Andrew Y. Distance metric learning with In Advances in neural information processing  application to clustering with side-information. systems, pp. 505–512, 2002.  Yang, Dongqiang and Powers, David M. W. Verb similarity on the taxonomy of wordnet. In In the  3rd International WordNet Conference (GWC-06), Jeju Island, Korea, 2006.  12  ",
1412.6544,2015,Qualitatively characterizing neural network optimization problems,"['Qualitatively characterizing neural network optimization problems', 'Ian Goodfellow and Oriol Vinyals']",https://arxiv.org/pdf/1412.6544,"5 1 0 2     y a M 1 2         ] E N . s c [      6 v 4 4 5 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  QUALITATIVELY CHARACTERIZING NEURAL NETWORK OPTIMIZATION PROBLEMS  Ian J. Goodfellow∗ & Oriol Vinyals∗ & Andrew M. Saxe∗∗ ∗Google Inc., Mountain View, CA ∗∗Department of Electrical Engineering, Stanford University, Stanford, CA {goodfellow,vinyals}@google.com, asaxe@stanford.edu  ABSTRACT  Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difﬁcult, with fear of local minima and other obstacles motivating a variety of schemes to improve opti- mization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct train- ing with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We ﬁnd that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any signiﬁcant obstacles.  1  INTRODUCTION  Neural networks are generally regarded as difﬁcult to optimize. The objective functions we must optimize in order to train them are non-convex and there are not many theoretical guarantees about the performance of the most popular algorithms on these problems. Nevertheless, neural networks are commonly trained successfully and obtain state of the art results on many tasks. In this paper, we present a variety of simple experiments designed to roughly characterize the objec- tive functions involved in neural network training. These experiments are not intended to measure any speciﬁc quantitative property of the objective function, but rather to answer some simple qual- itative questions. Do neural networks enter and escape a series of local minima? Do they move at varying speed as they approach and then pass a variety of saddle points? Answering these questions deﬁnitively is difﬁcult, but we present evidence strongly suggesting that the answer to all of these questions is no. We show that there exists a linear subspace in which neural network training could proceed by descending a single smooth slope with no barriers. Early symmetry breaking is the most conspicuous example of non-convexity. One important question is what happens after SGD leaves this well-behaved linear subspace. The main text of this article is restricted to experiments that were peer-reviewed prior to ICLR 2014, but the appendix presents additional experiments added after the review process ended. These experi- ments show that in some cases SGD does encounter obstacles, such as a ravine that shapes its path, but we never found evidence that local minima or saddle points slowed the SGD trajectory. This suggests that less exotic problems such as poor conditioning and variance in the gradient estimate are the primary difﬁculties in training neural networks. In all cases, we examine the total cost function (added up across all training examples). SGD of course only ever acts on unbiased stochastic approximations to this loss function. The structure of these stochastic approximations could be different from the global loss functions that we examine here, so it remains possible that neural networks are difﬁcult to train due to exotic structures in individual terms of the total cost function, or due to the noise induced by sampling minibatches of these terms. The results of our linear subspace experiments were qualitatively the same for all seven models we examined, which were drawn from a variety of categories, including fully-connected supervised feed-forward networks (Rumelhart et al., 1986) with a variety of activation functions, supervised  1  Published as a conference paper at ICLR 2015  convolutional networks (LeCun et al., 2010), unsupervised models, recurrent models of sequences, and analytically tractable factored linear models. (The additional experiments in the appendix found some qualitatively unique behavior outside this linear subspace for two of our models, but the re- mainder have the same qualitative behavior as factored linear models) Our models were all chosen because they performed well on competitive benchmark tasks. More research is needed to determine whether one should interpret our results as implying that SGD never encounters exotic obstacles when training neural networks, or as implying that SGD only works well when it does not encounter these structures.  2 LINEAR PATH EXPERIMENTS  Training a neural network consists of ﬁnding the optimal set of parameters θ. These are initialized to some set of small, random, initial parameters θ = θi. We then train using stochastic gradient de- scent (usually with extra features such as momentum) to minimize J(θ) until reaching convergence (usually some early stopping criterion). At the end of training, θ = θf . The trajectory that SGD follows from θi to θf is complicated and high-dimensional. It is difﬁcult to summarize such a trajectory meaningfully in a two-dimensional visualization. Simple learning curves showing the value of the objective function over time do not convey some fairly simple information. For example, when a learning curve bounces up and down repeatedly, we do not know whether the objective function is highly bumpy or whether SGD is rapidly changing direction due to noise in the stochastic, minibatch-based, estimate of the gradient. When the objective function remains constant for long periods of time, we do not know whether the parameters are stuck in a ﬂat region, oscillating around a local minimum, or tracing their way around the perimeter of a large obstacle. In this paper, we introduce a simple technique for qualitatively analyzing objective functions. We simply evaluate J(θ) at a series of points θ = (1− α)θ0 + αθ1 for varying values of α. This sweeps out a line in parameter space. We can see whether the cross-section of the objective function along this line is well-behaved. When we set θ0 = θi and θ1 = θf , we ﬁnd that the objective function has a simple, approximately convex shape along this cross-section. In other words, if we knew the correct direction, a single coarse line search could do a good job of training a neural network. These results are consistent with recent empirical and theoretical work arguing that local minima are not a signiﬁcant problem for training large neural networks (Saxe et al., 2013; Dauphin et al., 2014; Choromanska et al., 2014).  3 FEED-FORWARD FULLY CONNECTED NETWORKS  We begin our investigation with the simplest kind of neural network, the deterministic, feed-forward, fully-connected supervised network. For these experiments we use the MNIST dataset (LeCun et al., 1998). When not using dataset augmentation, the best result in this category is a maxout network (Goodfellow et al., 2013c) regularized with dropout (Srivastava et al., 2014) and adversarial training (Goodfellow et al., 2014), and trained using SGD with momentum. See the appendix of this paper for a full speciﬁcation of the architecture and training algorithm for this and all subsequent experiments. This conﬁguration results in an average of 78.2 mistakes on the MNIST test set, out of 10,000 examples total. Without adversarial training, the model also performs very well, with only 94 mistakes. Running the linear interpolation experiment on this problem, we ﬁnd in Fig. 1 that the 1-D subspace spanning the initial parameters and ﬁnal parameters is very well-behaved, and that SGD spends most of its time exploring the ﬂat region at the bottom of the valley. Maxout units do not saturate (they can saturate with respect to their input, but not with respect to their parameters), so perhaps it should not be too surprising that optimization is simple in this case. To determine whether the hard zero saturation of ReLUs (Jarrett et al., 2009; Glorot et al., 2011) or the two-sided saturation of logistic sigmoids can induce additional difﬁculties, we ran the linear interpolation experiment for both of these activation functions. The results are presented in Fig. 2 and Fig. 3. Again, we ﬁnd that the 1-D subspace spanning the initial and ﬁnal parameters contains no difﬁcult, exotic structures.  2  Published as a conference paper at ICLR 2015  Figure 1: Experiments with maxout on MNIST. Top row) The state of the art model, with adversarial training. Bottom row) The previous best maxout network, without adversarial training. Left column) The linear interpolation experiment. This experiment shows that the objective function is fairly smooth within the 1-D subspace spanning the initial and ﬁnal parameters of the model. Apart from the ﬂattening near α = 0, it appears nearly convex in this subspace. If we chose the initial direction correctly, we could solve the problem with a coarse line search. Right column) The progress of the actual SGD algorithm over time. The vast majority of learning happens in the ﬁrst few epochs. Thereafter, the algorithm struggles to make progress.  Figure 2: The linear interpolation curves for fully connected networks with different activation functions. Left) Sigmoid activation function. Right) ReLU activation function.  3  0.00.51.01.52.0α0.00.51.01.52.02.5J(θ)Linear interpolation of adversarially trained maxout on MNISTJ(θ) trainJ(θ) validation0100200300400500600time (epochs)0.51.01.52.02.5J(θ)SGD training of adversarial maxout on MNISTJ(θ) trainJ(θ) validation0.00.51.01.52.0α0.00.51.01.52.02.5J(θ)Linear interpolation of maxout on MNISTJ(θ) trainJ(θ) validation050100150200time (epochs)0.00.51.01.52.0J(θ)SGD training of maxout on MNISTJ(θ) trainJ(θ) validation0.00.51.01.52.0α0.00.51.01.52.02.53.0J(θ)Linear interpolation of sigmoids on MNISTJ(θ) trainJ(θ) validation0.00.51.01.52.0α0.00.51.01.52.02.5J(θ)Linear interpolation of ReLUs on MNISTJ(θ) trainJ(θ) validationPublished as a conference paper at ICLR 2015  Figure 3: The linear interpolation experiment for maxout, ReLUs, and sigmoids on MNIST, all plotted on the same axis for comparison. For this plot, we put the y axis in log scale, otherwise differences at the bottom of the curve are difﬁcult to see.  (a)  (c)  (b)  (d)  Figure 4: Higher resolution linear interpolation experiments. a) Tiling the interval [0, 1] with 200 values of α. b) A zoomed-in view of the same plot. c) Tiling the interval [0, .01] with 200 values of α, to see whether the initial symmetry breaking causes difﬁcult structures. d) Tiling the interval [.99, 1.] with 200 values of α, to see if the behavior of the objective function is more exotic in regions where the parameters encode fully learned intermediate concepts. We do not show the validation set objective here because it is too widely separated from the training set objective and would require zooming out the plot too far.  One possible objection to these results is that we have explored α with too coarse of a resolution to expose local non-convex structures. We therefore ran a variety of higher-resolution experiments, presented in Fig. 4. For these experiments, we did not use dropout, because the resolution we use here is high enough to expose artifacts induced by the Monte Carlo approximation to the true dropout loss function, which involves a sum over all (exponentially many) dropout masks. Maxout tends to overﬁt on MNIST if used without dropout, so we used ReLUs for these experiments. We found that increased resolution did not expose any small, difﬁcult structures.  4  0.00.51.01.52.0α10-210-1100101J(θ)Comparison of activation functions on MNISTJ(θ) train (maxout)J(θ) train (ReLU)J(θ) train (sigmoid)0.00.20.40.60.81.0α0.00.51.01.52.02.5J(θ)High resolution ReLU interpolation on MNISTJ(θ) trainJ(θ) validation0.50.60.70.80.91.0α0.000.050.100.150.200.250.300.35J(θ)High resolution ReLU interpolation on MNISTJ(θ) trainJ(θ) validation0.0000.0020.0040.0060.0080.010α2.2942.2962.2982.3002.3022.304J(θ)Initial portion of high resolution ReLU curveJ(θ) trainJ(θ) validation0.9900.9920.9940.9960.9981.000α0.001840.001860.001880.001900.00192J(θ)Final portion of high resolution ReLU curveJ(θ) trainPublished as a conference paper at ICLR 2015  Figure 5: Here we use linear interpolation to search for local minima. Left) By interpolating between two different SGD solutions, we show that each solution is a different local minimum within this 1-D subspace. Right) If we interpolate between a random point in space and an SGD solution, we ﬁnd no local minima besides the SGD solution, suggesting that local minima are rare.  Figure 6: The linear interpolation experiment for a convolutional maxout network on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009). Left) At a global scale, the curve looks very well-behaved. Right) Zoomed in near the initial point, we see there is a shallow barrier that SGD must navigate.  There are of course multiple minima in neural network optimization problems, and the shortest path between two minima can contain a barrier of higher cost. We can ﬁnd two different solutions by us- ing different random seeds for the random number generators used to initialize the weights, generate dropout masks, and select examples for SGD minibatches. (It is possible that these solutions are not minima but saddle points that SGD failed to escape) We do not ﬁnd any local minima within this subspace other than solution points, and these different solutions appear to correspond to different choices of how to break the symmetry of the saddle point at the origin, rather than to fundamentally different solutions of varying quality. See Fig. 5.  4 ADVANCED NETWORKS  Having performed experiments to understand the behavior of neural network optimization on su- pervised feedforward networks, we now verify that the same behavior occurs for more advanced networks. In the case of convolutional networks, we ﬁnd that there is a single barrier in the objective function, near where the network is initialized. This may simply correspond to the network being initialized with too large of random weights. This barrier is reasonably wide but not very tall. See Fig. 6 for details. To examine the behavior of SGD on generative models, we experimented with an MP-DBM (Good- fellow et al., 2013a). This model is useful for our purposes because it gets good performance as a generative model and as a classiﬁer, and its objective function is easy to evaluate (no MCMC business). Here we ﬁnd a secondary local minimum with high error, but a visualization of the SGD trajectory reveals that SGD passed far enough around the anomaly to avoid having it affect learning.  5  0.00.51.01.52.0α02468101214J(θ)Linear interpolation of maxout on MNIST between two solutionsJ(θ) trainJ(θ) validation0.00.51.01.52.0α02000400060008000100001200014000J(θ)Linear interpolation of maxout on MNIST from random starting pointJ(θ) trainJ(θ) validation0.00.51.01.52.0α0510152025J(θ)Linear interpolation of convolutional maxout on CIFARJ(θ) train0.000.050.100.150.200.250.30α2.222.242.262.282.302.322.34J(θ)Linear interpolation of convolutional maxout on CIFARJ(θ) trainPublished as a conference paper at ICLR 2015  Figure 7: Experiments with the MP-DBM. Left) The linear interpolation experiment reveals a sec- ondary local minimum with high error. Right) On the two horizonal axes, we plot components of θ that capture the extrema of θ throughout the learning process. On the vertical axis, we plot the objective function. Each point is another epoch of actual SGD learning. This plot allows us to see that SGD did not pass near this anomaly.  Figure 8: The linear interpolation experiment for an LSTM trained on the Penn Treebank dataset.  See Fig. 7. The MP-DBM was initialized with very large, sparse, weights, which may have con- tributed to our visualization technique exploring more non-convex areas, e.g. due to saturation of sigmoidal units. Finally, we performed the linear interpolation experiment for an LSTM regularized with dropout (Hochreiter & Schmidhuber, 1997; Zaremba et al., 2014) on the Penn Treebank dataset (Marcus et al., 1993). See Fig. 8. This experiment did not ﬁnd any difﬁcult structures, showing that the exotic features of non-convex optimization do not appear to cause difﬁculty even for recurrent models of sequences.  5 DEEP LINEAR NETWORKS  Saxe et al. (2013) have advocated developing a mathematical theory of deep networks by studying simpliﬁed mathematical models of these networks. Deep networks are formed by composing an alternating series of learned afﬁne transformations and ﬁxed non-linearities. One simpliﬁed way to model these functions is to compose only a series of learned linear transformations. The composition of a series of linear transformations is itself a linear transformation, so this mathematical model lacks the expressive capacity of a general deep network. However, because the weights of such a model are factored, its learning dynamics resemble those of the deep network. The output of the model is linear in the input to the model, but non-linear as a function of the model parameters. In particular, while ﬁtting linear regression parameters is a convex problem, ﬁtting deep linear regression parameters is a non-convex problem. Deep linear regression suffers from saddle points but does not suffer from local minima of varying quality. All minima are global minima, and are linked to each other in a continuous manifold.  6  0.00.51.01.52.0α0.040.060.080.100.120.140.160.180.200.22J(θ)Linear interpolation of an MP-DBM on MNISTJ(θ) trainJ(θ) validation1952002052102152202250204060800.060.080.100.120.140.160.180.200.00.20.40.60.81.0α345678910J(θ)Linear interpolation of an LSTM on Penn TreebankJ(θ) trainJ(θ) validationPublished as a conference paper at ICLR 2015  Figure 9: Linear interpolation from a small random initialization point to a solution for a linear regression model of depth 2. This shows the same qualitative features as our linear interpolation experiments for neural networks: a ﬂattening of the objective function near the saddle point at the origin, and only one minimum within this 1-D subspace.  Figure 10: Left) Interpolation between two solutions to deep linear regression. Though these two solutions lie on connected manifold of globally minimal values, the straight line between them en- counters a barrier of higher cost. The curve for the low dimensional linear model has all the same qualitative characteristics as the curve for the high dimensional non-linear networks we studied. Right) Interpolation between a random point with large norm and an solution to deep linear regres- sion. As with the neural network, this search does not encounter any minima other than the solution used to initialize the search.  Our linear interpolation experiments can be carried out analytically rather than experimentally in the case of deep linear regression. The results are strikingly similar to our results with deep non-linear networks. Speciﬁcally, we show that the problem of training y = w1w2x to output 1 when x = 1 using mean squared error is sufﬁcient to produce all of the qualitative features of neural network training that our linear interpolation experiments have exposed. See Fig. 9 and Fig. 10.  6 DISCUSSION  The reason for the success of SGD on a wide variety of tasks is now clear: these tasks are relatively easy to optimize. Finding a good direction in a high-dimensional space is a difﬁcult problem, but it is not nearly as difﬁcult as navigating an error surface that has complicated obstacles within multiple different low-dimensional subspaces. This work has only considered neural networks that perform very well. It is possible that these neural networks perform well because extensive hyperparameter search has found problems that SGD is able to optimize easily, but that other hyperparameters correspond to optimization problems that are too hard. In particular, it seems likely that very large neural networks are easier to ﬁt to a particular task.  7  0.00.51.01.52.0α0123456789J(θ)Linear interpolation of a deep linear modelJ(θ) train0.00.20.40.60.81.01.21.4α0.00.20.40.60.81.01.21.41.6J(θ)Linear interpolation between two solutions for a deep linear modelJ(θ) train0.00.20.40.60.81.01.21.4α0.00.20.40.60.81.01.2J(θ)Interpolating from a random point to a solution for a deep linear modelJ(θ) train, trial 0J(θ) train, trial 1J(θ) train, trial 2J(θ) train, trial 3J(θ) train, trial 4Published as a conference paper at ICLR 2015  Future work should aim to characterize the set of problems that are easy for SGD, to understand why SGD is able to avoid the obstacles that are present, and to determine why the training of large models remains slow despite the scarcity of obstacles. More advanced optimization algorithms could reduce the computational cost of deploying neural networks by enabling small networks to reach good performance, and could reduce the cost of training large networks by reducing the amount of time required to reach their solution.  ACKNOWLEDGMENTS  We would like to thank J¨org Bornschein, Eric Drexler, and Yann Dauphin for helpful discussions. We would like to thank Yaroslav Bulatov, Chung-Cheng Chiu, Greg Corrado, and Jeff Dean for their reviews of drafts of this work. We would like to thank the developers of Theano(Bergstra et al., 2010; Bastien et al., 2012) and Pylearn2(Goodfellow et al., 2013b).  REFERENCES Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Choromanska, A., Henaff, M., Mathieu, M., Ben Arous, G., and LeCun, Y. The Loss Surface of  Multilayer Networks. ArXiv e-prints, November 2014.  Dauphin, Yann N, Pascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, Ganguli, Surya, and Ben- gio, Yoshua. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 27, pp. 2933–2941. Curran Asso- ciates, Inc., 2014.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectiﬁer neural networks. In JMLR W&CP: Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011), April 2011.  Goodfellow, Ian, Shlens, Jonathon, and Szegedy, Christian. Explaining and harnessing adversarial  examples. 2014. URL http://arxiv.org/abs/1412.6572.  Goodfellow, Ian J., Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Multi-prediction deep  Boltzmann machines. In Neural Information Processing Systems, December 2013a.  Goodfellow, Ian J., Warde-Farley, David, Lamblin, Pascal, Dumoulin, Vincent, Mirza, Mehdi, Pas- canu, Razvan, Bergstra, James, Bastien, Fr´ed´eric, and Bengio, Yoshua. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013b.  Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout networks. In Dasgupta, Sanjoy and McAllester, David (eds.), International Conference on Machine Learning, pp. 1319–1327, 2013c.  Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 9(8):1735–1780,  1997.  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pp. 2146–2153. IEEE, 2009.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images.  Technical report, University of Toronto, 2009.  8  Published as a conference paper at ICLR 2015  LeCun, Yann, Bottou, Leon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998.  LeCun, Yann, Kavukcuoglu, Koray, and Farabet, Cl´ement. Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pp. 253–256. IEEE, 2010.  Marcus, Mitchell P., Santorini, Beatrice, and Marcinkiewicz, Mary Ann. Building a large annotated corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330, 1993. Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning internal representations by error  propagation. volume 1, chapter 8, pp. 318–362. MIT Press, Cambridge, 1986.  Saxe, Andrew M., McClelland, James L., and Ganguli, Surya. Exact solutions to the nonlinear  dynamics of learning in deep linear neural networks. In ICLR, 2013.  Srivastava, Nitish.  Improving Neural Networks with Dropout. Master’s thesis, University of  Toronto, Toronto, Canada, January 2013.  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15(1):1929–1958, 2014.  Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol. Recurrent neural network regularization.  CoRR, abs/1409.2329, 2014. URL http://arxiv.org/abs/1409.2329.  A EXPERIMENT DETAILS  All of our experiments except for the sigmoid network were using hyperparameters taken directly from the literature. We fully specify each of them here. Adversarially trained maxout network: This model is the one used by Goodfellow et al. (2014). There is no public conﬁguration for it, but the paper describes how to modify the previous best maxout network to obtain it. Maxout network: This model was retrained using the publicly available implementation used by Goodfellow et al. (2013c). The code is available at: https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/scripts/ papers/maxout/mnist_pi.yaml ReLU network with dropout: This model is intended to nearly reproduce the the dropout ReLU network described by Srivastava (2013). It is a standard reference implementation provided by Pylearn2 (Goodfellow et al., 2013b) and the speciﬁc ﬁle is available here: https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/scripts/ papers/dropout/mnist_valid.yaml ReLU network without dropout: We simply removed the dropout from the preceding conﬁguration ﬁle. Sigmoid network: We simply replaced the ReLU non-linearities with sigmoids. This performs ac- ceptably for a sigmoid network; it gets a test set error rate of 1.66%. Convolutional network: We used the best convolutional network available in Pylearn2 for the CIFAR-10 dataset, speciﬁcally the one developed by Goodfellow et al. (2013c). to reduce the computational cost of computing the training set objective, In order we used the the variant without data augmentation. The conﬁguration ﬁle is avail- able here: https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/ scripts/papers/maxout/cifar10_no_aug_valid.yaml MP-DBM: We used the best MP-DBM for classifying MNIST, as described by Goodfellow et al. (2013a). Dropout LSTM: We used the conﬁguration described in the paper introducing this method (Zaremba et al., 2014).  9  Published as a conference paper at ICLR 2015  Figure 11: Plots of the projection along the axis from initialization to solution versus the norm of the residual of this projection for random walks of varying dimension. Each plot is formed by using 1,000 steps. We designate step 900 as being the “solution” and continue to plot 100 more steps, in order to simulate the way neural network training trajectories continue past the point that early stopping on a validation set criterion chooses as being the solution. Each step is made by incrementing the current coordinate by a sample from a Gaussian distribution with zero mean and unit covariance. Because the dimensionality of the space forces most trajectories to have this highly regular shape, this kind of plot is not a meaningful way of investigating how SGD behaves as it moves away from the 1-D subspace we study in this paper.  B STRAYING FROM THE PATH  We have shown that, for seven different models of practical interest, there exists a straight path from initialization to solution along which the objective function decreases smoothly and monotonically, at least up to the resolution that our experiments investigated. Stochastic gradient descent does not actually follow this path. We know that SGD matches this path at the beginning and at the end. One might naturally want to plot the norm of the residual of the parameter value after projecting the parameters at each point in time into the 1-D subspace we have identiﬁed. SGD begins at θi, the solution point chosen by early stopping is θf , and SGD visits θ(t) at time t. If we deﬁne u to be a unit vector pointing in the direction θf −θi, then the primary subspace we have investigated so far is the line θi + α(t)u where α(t) = (θ(t) − θi)(cid:62)u. We can plot the coordinate within this subspace, α(t), on the horizontal axis. We can then deﬁne a second unit vector v pointed in the direction θ(t) − (θi + α(t)u). The projection into this subspace is β(t) = (θ(t) − θi − α(t)u)(cid:62)v. In other words, β(t) is the norm of the residual of the projection of θi onto the line spanning initialization and solution. We can plot β on the vertical axis. In the next section, we will see that the shape of the objective function in terms of α and β coor- dinates has interesting features that vary from one problem to another. However, it is important to understand that this kind of plot does not tell us much about the shape of the SGD trajectory. All that it tells us is how far SGD strays from the primary linear subspace. This is because in high dimensional spaces, the shape of this curve does not convey very much information. See Fig. 11 for a demonstration of how this plot converges to a simple geometric shape as the dimensionality of a random walk increases.  10  10010203040506070Distance along main ray0510152025L2 norm of residualDivergence of random walk from the main ray (2 dimensional)20020406080100120140Distance along main ray01020304050L2 norm of residualDivergence of random walk from the main ray (10 dimensional)02004006008001000Distance along main ray0100200300400500L2 norm of residualDivergence of random walk from the main ray (1000 dimensional)0200040006000800010000Distance along main ray010002000300040005000L2 norm of residualDivergence of random walk from the main ray (100000 dimensional)Published as a conference paper at ICLR 2015  Figure 12: To show the effect of different learning rates (cid:15) and momentum coefﬁcients µ, we plot the projection and residual norm of gradient descent with several different hyperparameters. In this case, to make the plots comparable, we use the true solution to a synthetic, convex problem as the endpoint for all trajectories. (In the non-convex case, different trajectories could go to different solutions, and this would confound our interpretation of the differences between them) Because this problem is 100,000 dimensional, the curves all have a very simple shape, and the primary quantity distinguishing them is the maximum norm of the residual.  Figure 13: This plot examines how far off the linear path SGD strays when training a maxout network on MNIST. The x axis is the projection along the linear path from initialization to solution. The y axis is the norm of the residual. The plot uses the Lp norm with p = 2, also known as the Euclidean norm. It is not the squared norm.  Plots of the residual norm of the projection for SGD trajectories converge to a very similar geometric shape in high dimensional spaces. See Fig. 12 for an example of several different runs of SGD on the same problem. However, we can still glean some information from this kind of plot by looking at the maximum norm of the residual and comparing this to the maximum norm of the parameter vector as a whole. We show this same kind of plot for a maxout network in Fig. 13. Keep in mind that the shape of the trajectory is not interesting, but the ratio of the norm of the residual to the total norm of the parameter vector at each point does give us some idea of how much information the 1-D projection discards. We see from this plot that our linear subspace captures at least 2/3 the norm of the parameter vector at all points in time.  C THREE-DIMENSIONAL VISUALIZATIONS  A natural question is whether there exist obstacles in between the well-behaved linear subspace we have described and the path followed by SGD. One way to investigate this is to introduce an additional direction of exploration. Speciﬁcally, we would like to explore the line passing through each SGD point and its projection on the primary 1-D subspace we have investigated so far. If this line contains obstacles, it could explain why SGD does not exploit the simple behavior within this subspace.  11  0100200300400500600Distance along main ray051015L2 norm of residualDivergence of SGD from the main ray†=0.0001,µ=0.†=0.0001,µ=0.5†=0.0001,µ=0.9†=0.0005,µ=0.†=0.0005,µ=0.5†=0.0005,µ=0.9†=0.001,µ=0.†=0.001,µ=0.5†=0.001,µ=0.9100102030405060Distance along main ray05101520L2 norm of residualDivergence of SGD path from the main rayPublished as a conference paper at ICLR 2015  Figure 14: The error surface (1 − w1w2)2 of a factored linear model with two layers and one unit per layer. This error surface shows a saddle point at the origin and a non-linear manifold of global solutions. The SGD trajectory from initialization to solution encounters negative curvature near its initial point and positive curvature near its ﬁnal point, but does not encounter any exotic obstacles.  For our factored linear model, where the training loss is just (1 − w1w2)2, we can accomplish this by viewing a heatmap of the cost function. See Fig 14. This ﬁgure predicts many properties that we expect to see for our more complicated neural network models: negative curvature at initialization, positive curvature surrounding the solution, a lack of obstacles separating the SGD trajectory from the well-behaved region of the function, and a connected manifold of globally optimal points. In this case, the connected manifold is the hyperbola w2 = 1/w1. For neural networks, the pattern of equivalent solutions will be different. For example, we can take a deep rectiﬁed linear network and obtain an equivalent network by rescaling its parameters. If we modify the parameters of layer i by multiplying bias j and column j of the weight matrix by γ, then we can preserve the function of the input that the network respesents by dividing row j of the weights for layer i + 1 by γ. In the case of the factored linear model, the hyperbolic shape of this manifold means that linearly interpolating between two solution points reveals a region of high cost in the middle, even though the two solutions are connected by a manifold of other solutions. This kind of 3-D plot is not directly achievable for problems with more than two parameters. We must summarize the parameters with a 2-D coordinate somehow. In the remainder of this section, we summarize the parameters via their projection in to the line spanning initialization and solution (the α(t) coordinate deﬁned in the previous section) and their projection into the line orthogonal to this subspace that passes through the SGD point θ(t), which we deﬁned as the coordinate β(t) in the previous section.  12  w1w2  −2−1.5−1−0.500.511.52−2−1.5−1−0.500.511.52GradientManifold of global minimaSaddle pointGradient descent pathLinear pathInterpolation between minima0.511.522.533.544.5Student Version of MATLABPublished as a conference paper at ICLR 2015  Figure 15: As a canonical example of the deep factored linear model of deep network training, we trained a linear model with mean squared error on MNIST. Speciﬁcally, we multiply together ten matrices, the ﬁrst nine of which are square and have the same dimension as the MNIST input, and the last of which has only ten columns. The output is thus a ten dimensional vector. The mean squared error encourages element i of this vector to have value close to 1 and the other elements to have zero when the true class is i. This 3-D plot shows negative curvature near the initialization point, positive curvature near the solution point, and a general lack of obstacles.  If we plot the cost as a function of α and β, we see that the cost function of a deep factored linear model (Fig. 15) has roughly the same structure as the cost function of our LSTM (Fig. 16) and as most of our feedforward networks (we show one in Fig. 17). These models show only structures predicted by the factored linear model of learning dynamics. However, for the adversarially trained maxout network, we ﬁnd that an obstacle that is small in height yet very steep constrains SGD into a narrow canyon, preventing it from accessing the subspace studied in the main text of this paper (Fig. 18 and Fig. 19). Finally, recall that the MP-DBM had a local minimum within the primary 1-D subspace (which could be a local minimum or a saddle point in high dimensional space). With our 3-D plot (Fig. 20), we see that SGD passed far around the plateau surrounding this point. Note that in most of these visualizations we can see signﬁcant negative curvature in the early part of the SGD trajectory, and that SGD does not seem to have any difﬁculty escaping the saddle point near the origin. One possible explanation for this behavior is that one model of SGD with sufﬁciently small step size naturally avoids saddle points. Consider the SGD trajectory as a function of time, θ(t). As an analytical model of SGD with small step size, we can consider the continuous-time dt θ = −∇θJ(θ). If we make a second-order Taylor series expansion gradient descent process with d  13  Projection01234567Residual2.01.51.00.50.00.51.01.52.02.50.00.20.40.60.8Published as a conference paper at ICLR 2015  Figure 16: The 3-D visualization of the LSTM cost reveals a simple structure, qualitatively the same as that of the deep factored linear model.  14  Projection0100200300400500Residual200100010020024681012Published as a conference paper at ICLR 2015  Figure 17: Most of our 3-D visualizations feedforward networks had shapes that were qualitatively the same as the factored linear network. Here we show the adversarially trained ReLU network as a representative sample.  15  Projection020406080100Residual40200204060800.00.51.01.52.0Published as a conference paper at ICLR 2015  Figure 18: The 3-D visualization technique applied to adversarially trained maxout reveals some obstacles.  16  Projection0102030405060Residual3020100102030400.00.51.01.52.0Published as a conference paper at ICLR 2015  Figure 19: The same as Fig. 18, but zoomed in to show detail near the end of learning.  in time  it simpliﬁes to  θ(t) ≈ θ(0) − t  d dt  θ(t) +  1 2  t2 d2  dt2 θ(t)  θ(t) ≈ θ(0) − t∇θ(0)J(θ(0)) +  t2H(0)∇θ(0)J(θ(0))  1 2  where H is the Hessian matrix of J(θ(0)) with respect to θ(0). This view shows that a second-order approximation in time of continuous-time gradient descent incorporates second-order information in space via the Hessian matrix. Speciﬁcally, the second-order term of the Taylor series expansion is equivalent to ascending the gradient of ||∇θJ(θ)||2. In other words, the ﬁrst-order term says to go downhill, while the second-order term says to make the gradient get bigger. The latter term encourages SGD to exploit directions of negative curvature.  D CONTROL VISUALIZATIONS  Visualization has not typically been used as a tool for understanding the structure of neural net- work objective functions. This is mostly because neural network objective functions are very high- dimensional and visualizations are by necessity fairly low dimensional. In this section, we include a few “control” visualizations as a reminder of the need to interpret any low-dimensional visualization carefully. Most of our visualizations showed rich structure in the cost function and a relatively simple shape in the SGD trajectory. It’s important to remember that our 3-D visualizations are not showing a 2-D linear subspace. Instead, they are showing multiply 1-D subspaces rotated to be parallel to each other. Our particular choice of subspaces was intended to capture a lot of variation in the cost function, and as a side effect it discards all variation in a high-dimensional trajectory, reducing most trajectories to semi-circles. If as a control we instead plot a randomly selected 2-D linear subspace  17  Projection404244464850525456Residual3020100102030400.140.160.180.200.220.240.26Published as a conference paper at ICLR 2015  Figure 20: The 3-D visualization of the MP-DBM contains a plateau, but SGD avoided it.  intersecting the solution point, then we see that there is almost no variation in the cost function within this subspace, and the SGD trajectory is quite noisy. See Fig. 21. As an intermediate control, we generated the plot for the MP-DBM, with α(t) on one axis, and the other axis being a random linear projection. This allows us to see a true 2-D linear subspace that has signiﬁcant variation in the cost function due to the choice of the ﬁrst axis, but also allows us to see that the SGD trajectory is not a semi-circle. See Fig. 22  18  Projection020406080100120140160180Residual500500.000.050.100.150.20Published as a conference paper at ICLR 2015  Figure 21: As a control experiment, we plot a random 2-D subspace intersecting the solution point. In this subspace, we see a complicated SGD trajectory and essentially no variation in the cost func- tion value. This visualization is useful as a reminder that the visualizations presented in this paper are designed to expose variation in the cost function and discard variation in the shape of SGD tra- jectory. Not all directions in the cost function have high variability and SGD trajectories do vary greatly.  19  0.040.030.020.010.000.010.020.030.040.050.140.120.100.080.060.040.020.000.020.00.51.01.52.02.5Published as a conference paper at ICLR 2015  Figure 22: A control experiment with α(t) on one axis and the other axis being a random projection.  20  500501001502000.070.060.050.040.030.020.010.000.010.00.10.20.30.40.5",
1410.3916,2015,Memory Networks,"['Memory Networks', 'Jason Weston', 'Sumit Chopra', 'and Antoine Bordes']",https://arxiv.org/pdf/1410.3916,"5 1 0 2     v o N 9 2         ] I  A . s c [      1 1 v 6 1 9 3  .  0 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  MEMORY NETWORKS  Jason Weston, Sumit Chopra & Antoine Bordes Facebook AI Research 770 Broadway New York, USA {jase,spchopra,abordes}@fb.com  ABSTRACT  We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term mem- ory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to an- swer questions that require understanding the intension of verbs.  1  INTRODUCTION  Most machine learning models lack an easy way to read and write to part of a (potentially very large) long-term memory component, and to combine this seamlessly with inference. Hence, they do not take advantage of one of the great assets of a modern day computer. For example, consider the task of being told a set of facts or a story, and then having to answer questions on that subject. In principle this could be achieved by a language modeler such as a recurrent neural network (RNN) (Mikolov et al., 2010; Hochreiter & Schmidhuber, 1997) as these models are trained to predict the next (set of) word(s) to output after having read a stream of words. However, their memory (en- coded by hidden states and weights) is typically too small, and is not compartmentalized enough to accurately remember facts from the past (knowledge is compressed into dense vectors). RNNs are known to have difﬁculty in performing memorization, for example the simple copying task of outputting the same input sequence they have just read (Zaremba & Sutskever, 2014). The situation is similar for other tasks, e.g., in the vision and audio domains a long term memory is required to watch a movie and answer questions about it.  In this work, we introduce a class of models called memory networks that attempt to rectify this problem. The central idea is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. The model is then trained to learn how to operate effectively with the memory component. We introduce the general framework in Section 2, and present a speciﬁc implementation in the text domain for the task of question answering in Section 3. We discuss related work in Section 4, describe our experiments in 5, and ﬁnally conclude in Section 6.  2 MEMORY NETWORKS  A memory network consists of a memory m (an array of objects1 indexed by mi) and four (poten- tially learned) components I, G, O and R as follows:  I: (input feature map) – converts the incoming input to the internal feature representation.  1For example an array of vectors or an array of strings.  1  Published as a conference paper at ICLR 2015  G: (generalization) – updates old memories given the new input. We call this generalization as there is an opportunity for the network to compress and generalize its memories at this stage for some intended future use.  O: (output feature map) – produces a new output (in the feature representation space), given  the new input and the current memory state.  R: (response) – converts the output into the response format desired. For example, a textual  response or an action.  Given an input x (e.g., an input character, word or sentence depending on the granularity chosen, an image or an audio signal) the ﬂow of the model is as follows:  1. Convert x to an internal feature representation I(x). 2. Update memories mi given the new input: mi = G(mi, I(x), m), ∀i. 3. Compute output features o given the new input and the memory: o = O(I(x), m). 4. Finally, decode output features o to give the ﬁnal response: r = R(o).  This process is applied at both train and test time, if there is a distinction between such phases, that is, memories are also stored at test time, but the model parameters of I, G, O and R are not updated. Memory networks cover a wide class of possible implementations. The components I, G, O and R can potentially use any existing ideas from the machine learning literature, e.g., make use of your favorite models (SVMs, decision trees, etc.).  I component: Component I can make use of standard pre-processing, e.g., parsing, coreference and entity resolution for text inputs. It could also encode the input into an internal feature represen- tation, e.g., convert from text to a sparse or dense feature vector.  G component: The simplest form of G is to store I(x) in a “slot” in the memory:  mH(x) = I(x),  (1) where H(.) is a function selecting the slot. That is, G updates the index H(x) of m, but all other parts of the memory remain untouched. More sophisticated variants of G could go back and update earlier stored memories (potentially, all memories) based on the new evidence from the current input x. If the input is at the character or word level one could group inputs (i.e., by segmenting them into chunks) and store each chunk in a memory slot.  If the memory is huge (e.g., consider all of Freebase or Wikipedia) one needs to organize the memo- ries. This can be achieved with the slot choosing function H just described: for example, it could be designed, or trained, to store memories by entity or topic. Consequently, for efﬁciency at scale, G (and O) need not operate on all memories: they can operate on only a retrieved subset of candidates (only operating on memories that are on the right topic). We explore a simple variant of this in our experiments.  If the memory becomes full, a procedure for “forgetting” could also be implemented by H as it chooses which memory is replaced, e.g., H could score the utility of each memory, and overwrite the least useful. We have not explored this experimentally yet.  O and R components: The O component is typically responsible for reading from memory and performing inference, e.g., calculating what are the relevant memories to perform a good response. The R component then produces the ﬁnal response given O. For example in a question answering setup O ﬁnds relevant memories, and then R produces the actual wording of the answer, e.g., R could be an RNN that is conditioned on the output of O. Our hypothesis is that without conditioning on such memories, such an RNN will perform poorly.  3 A MEMNN IMPLEMENTATION FOR TEXT  One particular instantiation of a memory network is where the components are neural networks. We refer to these as memory neural networks (MemNNs). In this section we describe a relatively simple implementation of a MemNN with textual input and output.  2  Published as a conference paper at ICLR 2015  3.1 BASIC MODEL  In our basic architecture, the I module takes an input text. Let us ﬁrst assume this to be a sentence: either the statement of a fact, or a question to be answered by the system (later we will consider word-based input sequences). The text is stored in the next available memory slot in its original form2, i.e., S(x) returns the next empty memory slot N : mN = x, N = N + 1. The G module is thus only used to store this new memory, so old memories are not updated. More sophisticated models are described in subsequent sections.  The core of inference lies in the O and R modules. The O module produces output features by ﬁnding k supporting memories given x. We use k up to 2, but the procedure is generalizable to larger k. For k = 1 the highest scoring supporting memory is retrieved with:  o1 = O1(x, m) = arg max i=1,...,N  sO(x, mi)  (2)  where sO is a function that scores the match between the pair of sentences x and mi. For the case k = 2 we then ﬁnd a second supporting memory given the ﬁrst found in the previous iteration:  o2 = O2(x, m) = arg max i=1,...,N  sO([x, mo1 ], mi)  (3)  where the candidate supporting memory mi is now scored with respect to both the original in- put and the ﬁrst supporting memory, where square brackets denote a list3. The ﬁnal output o is [x, mo1 , mo2], which is input to the module R. Finally, R needs to produce a textual response r. The simplest response is to return mok, i.e., to output the previously uttered sentence we retrieved. To perform true sentence generation, one can instead employ an RNN. In our experiments we also consider an easy to evaluate compromise approach where we limit textual responses to be a single word (out of all the words seen by the model) by ranking them:  r = argmaxw∈W sR([x, mo1 , mo2], w)  (4)  where W is the set of all words in the dictionary, and sR is a function that scores the match. An example task is given in Figure 1. In order to answer the question x = “Where is the milk now?”, the O module ﬁrst scores all memories, i.e., all previously seen sentences, against x to retrieve the most relevant fact, mo1 = “Joe left the milk” in this case. Then, it would search the memory again to ﬁnd the second relevant fact given [x, mo1], that is mo2 = “Joe travelled to the ofﬁce” (the last place Joe went before dropping the milk). Finally, the R module using eq. (4) would score words given [x, mo1 , mo2] to output r = “ofﬁce”. In our experiments, the scoring functions sO and sR have the same form, that of an embedding model:  s(x, y) = Φx(x)⊤U ⊤U Φy(y).  (5) where U is a n × D matrix where D is the number of features and n is the embedding dimension. The role of Φx and Φy is to map the original text to the D-dimensional feature space. The simplest feature space to choose is a bag of words representation, we choose D = 3|W | for sO, i.e., every word in the dictionary has three different representations: one for Φy(.) and two for Φx(.) depending on whether the words of the input arguments are from the actual input x or from the supporting memories so that they can be modeled differently.4 Similarly, we used D = 3|W | for sR as well. sO and sR use different weight matrices UO and UR.  2Technically, we will be using an embedding model to represent text, so we could store the incoming input using its learned embedding vector in memory instead. The downside of such a choice is that during learning the embedding parameters are changing, and hence the stored vectors would go stale. However, at test time (where the parameters are not changing) storing as embedding vectors could make sense, as this is faster than reading the original words and then embedding them repeatedly.  3As we will use a bag-of-words model where both x and mo1 are represented in the bag (but with two differ- ent dictionaries) this is equivalent to using the sum sO(x, mi) + sO(mo1 , mi), however a more sophisticated modeling of the inputs (e.g., with nonlinearities) may not separate into a sum.  4Experiments with only a single dictionary and linear embeddings performed worse (not shown). In order to model with only a single dictionary, one could consider deeper networks that transform the words dependent on their context. We leave this to future work.  3  Published as a conference paper at ICLR 2015  Figure 1: Example “story” statements, questions and answers generated by a simple simulation. Answering the question about the location of the milk requires comprehension of the actions “picked up” and “left”. The questions also require comprehension of the time elements of the story, e.g., to answer “where was Joe before the ofﬁce?”.  Joe went to the kitchen. Fred went to the kitchen. Joe picked up the milk. Joe travelled to the ofﬁce. Joe left the milk. Joe went to the bathroom. Where is the milk now? A: ofﬁce Where is Joe? A: bathroom Where was Joe before the ofﬁce? A: kitchen  Training We train in a fully supervised setting where we are given desired inputs and responses, and the supporting sentences are labeled as such in the training data (but not in the test data, where we are given only the inputs). That is, during training we know the best choice of both max functions in eq. (2) and (3)5. Training is then performed with a margin ranking loss and stochastic gradient descent (SGD). Speciﬁcally, for a given question x with true response r and supporting sentences mo1 and mo2 (when k = 2), we minimize over model parameters UO and UR:  max(0, γ − sO(x, mo1 ) + sO(x, ¯f )) +  P¯f 6=mo1 max(0, γ − sO([x, mo1], mo2]) + sO([x, mo1 ], ¯f ′])) +  max(0, γ − sR([x, mo1 , mo2], r) + sR([x, mo1 , mo2], ¯r]))  P¯f ′6=mo2 P¯r6=r  (6)  (7)  (8)  where ¯f , ¯f ′ and ¯r are all other choices than the correct labels, and γ is the margin. At every step of SGD we sample ¯f , ¯f ′, ¯r rather than compute the whole sum for each training example, following e.g., Weston et al. (2011).  In the case of employing an RNN for the R component of our MemNN (instead of using a single word response as above) we replace the last term with the standard log likelihood used in a language modeling task, where the RNN is fed the sequence [x, o1, o2, r]. At test time we output its prediction r given [x, o1, o2]. In contrast the absolute simplest model, that of using k = 1 and outputting the located memory mo1 as response r, would only use the ﬁrst term to train. In the following subsections we consider some extensions of our basic model.  3.2 WORD SEQUENCES AS INPUT  If input is at the word rather than sentence level, that is words arrive in a stream (as is often done, e.g., with RNNs) and not already segmented as statements and questions, we need to modify the approach we have so far described. We hence add a “segmentation” function, to be learned, which takes as in- put the last sequence of words that have so far not been segmented and looks for breakpoints. When the segmenter ﬁres (indicates the current sequence is a segment) we write that sequence to memory, and can then proceed as before. The segmenter is modeled similarly to our other components, as an embedding model of the form:  seg(c) = W ⊤  segUSΦseg(c)  (9)  where Wseg is a vector (effectively the parameters of a linear classiﬁer in embedding space), and c is the sequence of input words represented as bag of words using a separate dictionary. If seg(c) > γ, where γ is the margin, then this sequence is recognised as a segment. In this way, our MemNN has a learning component in its write operation. We consider this segmenter a ﬁrst proof of concept: of course, one could design something much more sophisticated. Further details on the training mechanism are given in Appendix B.  5 However, note that methods like RNNs and LSTMs cannot easily use this information.  4  Published as a conference paper at ICLR 2015  3.3 EFFICIENT MEMORY VIA HASHING  If the set of stored memories is very large it is prohibitively expensive to score all of them as in equations (2) and (3). Instead we explore hashing tricks to speed up lookup: hash the input I(x) into one or more buckets and then only score memories mi that are in the same buckets. We investigated two ways of doing hashing: (i) via hashing words; and (ii) via clustering word embeddings. For (i) we construct as many buckets as there are words in the dictionary, then for a given sentence we hash it into all the buckets corresponding to its words. The problem with (i) is that a memory mi will only be considered if it shares at least one word with the input I(x). Method (ii) tries to solve this by clustering instead. After training the embedding matrix UO, we run K-means to cluster word vectors (UO)i, thus giving K buckets. We then hash a given sentence into all the buckets that its individual words fall into. As word vectors tend to be close to their synonyms, they cluster together and we thus also will score those similar memories as well. Exact word matches between input and memory will still be scored by deﬁnition. Choosing K controls the speed-accuracy trade-off.  3.4 MODELING WRITE TIME  We can extend our model to take into account when a memory slot was written to. This is not important when answering questions about ﬁxed facts (“What is the capital of France?”) but is important when answering questions about a story, see e.g., Figure 1. One obvious way to implement this is to add extra features to the representations Φx and Φy that encode the index j of a given memory mj, assuming that j follows write time (i.e., no memory slot rewriting). However, that requires dealing with absolute rather than relative time. We had more success empirically with the following procedure: instead of scoring input, candidate pairs with s as above, learn a function on triples sOt (x, y, y′):  sOt (x, y, y′) = Φx(x)⊤UOt  ⊤UOt(cid:16)Φy(y) − Φy(y′) + Φt(x, y, y′)(cid:17).  (10) Φt(x, y, y′) uses three new features which take on the value 0 or 1: whether x is older than y, x is older than y′, and y older than y′. (That is, we extended the dimensionality of all the Φ embeddings by 3, and set these three dimensions to zero when not used.) Now, if sOt (x, y, y′) > 0 the model prefers y over y′, and if sOt (x, y, y′) < 0 it prefers y′. The argmax of eq. (2) and (3) are replaced by a loop over memories i = 1, . . . , N , keeping the winning memory (y or y′) at each step, and always comparing the current winner to the next memory mi. This procedure is equivalent to the argmax before if the time features are removed. More details are given in Appendix C.  3.5 MODELING PREVIOUSLY UNSEEN WORDS  Even for humans who have read a lot of text, new words are continuously introduced. For example, the ﬁrst time the word “Boromir” appears in Lord of The Rings (Tolkien, 1954). How should a machine learning model deal with this? Ideally it should work having seen only one example. A possible way would be to use a language model: given the neighboring words, predict what the word should be, and assume the new word is similar to that. Our proposed approach takes this idea, but incorporates it into our networks sO and sR, rather than as a separate step. Concretely, for each word we see, we store a bag of words it has co-occurred with, one bag for the left context, and one for the right. Any unknown word can be represented with such features. Hence, we increase our feature representation D from 3|W | to 5|W | to model these contexts (|W | features for each bag). Our model learns to deal with new words during training using a kind of “dropout” technique: d% of the time we pretend we have not seen a word before, and hence do not have a n-dimensional embedding for that word, and represent it with the context instead.  3.6 EXACT MATCHES AND UNSEEN WORDS  Embedding models cannot efﬁciently use exact word matches due to the low dimensionality n. One solution is to score a pair x, y with  (11) instead. That is, add the “bag of words” matching score to the learned embedding score (with a mixing parameter λ). Another, related way, that we propose is to stay in the n-dimensional em- bedding space, but to extend the feature representation D with matching features, e.g., one per  Φx(x)⊤U ⊤U Φy(y) + λΦx(x)⊤Φy(y)  5  Published as a conference paper at ICLR 2015  word. A matching feature indicates if a word occurs in both x and y. That is, we score with Φx(x)⊤U ⊤U Φy(y, x) where Φy is actually built conditionally on x: if some of the words in y match the words in x we set those matching features to 1. Unseen words can be modeled similarly by using matching features on their context words. This then gives a feature space of D = 8|W |.  4 RELATED WORK  Classical QA methods use a set of documents as a kind of memory, and information retrieval meth- ods to ﬁnd answers, see e.g., (Kolomiyets & Moens, 2011) and references therein. More recent methods try instead to create a graph of facts – a knowledge base (KB) – as their memory, and map questions to logical queries (Berant et al., 2013; 2014). Neural network and embedding approaches have also been recently explored (Bordes et al., 2014a; Iyyer et al., 2014; Yih et al., 2014). Com- pared to recent knowledge base approaches, memory networks differ in that they do not apply a two-stage strategy: (i) apply information extraction principles ﬁrst to build the KB; followed by (ii) inference over the KB. Instead, extraction of useful information to answer a question is performed on-the-ﬂy over the memory which can be stored as raw text, as well as other choices such as embed- ding vectors. This is potentially less brittle as the ﬁrst stage of building the KB may have already thrown away the relevant part of the original data.  Classical neural network memory models such as associative memory networks aim to provide content-addressable memory, i.e., given a key vector to output a value vector, see e.g., Haykin (1994) and references therein. Typically this type of memory is distributed across the whole network of weights of the model rather than being compartmentalized into memory locations. Memory-based learning such as nearest neighbor, on the other hand, does seek to store all (typically labeled) exam- ples in compartments in memory, but only uses them for ﬁnding closest labels. Memory networks combine compartmentalized memory with neural network modules that can learn how to (poten- tially successively) read and write to that memory, e.g., to perform reasoning they can iteratively read salient facts from the memory.  However, there are some notable models that have attempted to include memory read and write operations from the 90s. In particular (Das et al., 1992) designed differentiable push and pop actions called a neural network pushdown automaton. The work of Schmidhuber (1992) incorporated the concept of two neural networks where one has very fast changing weights which can potentially be used as memory. Schmidhuber (1993) proposed to allow a network to modify its own weights “self- referentially” which can also be seen as a kind of memory addressing. Finally two other relevant works are the DISCERN model of script processing and memory (Miikkulainen, 1990) and the NARX recurrent networks for modeling long term dependencies (Lin et al., 1996).  Our work was submitted to arxiv just before the Neural Turing Machine work of Graves et al. (2014), which is one of the most relevant related methods. Their method also proposes to perform (sequence) prediction using a “large, addressable memory” which can be read and written to. In their experi- ments, the memory size was limited to 128 locations, whereas we consider much larger storage (up to 14M sentences). The experimental setups are notably quite different also: whereas we focus on language and reasoning tasks, their paper focuses on problems of sorting, copying and recall. On the one hand their problems require considerably more complex models than the memory network de- scribed in Section 3. On the other hand, their problems have known algorithmic solutions, whereas (non-toy) language problems do not.  There are other recent related works. RNNSearch (Bahdanau et al., 2014) is a method of machine translation that uses a learned alignment mechanism over the input sentence representation while predicting an output in order to overcome poor performance on long sentences. The work of (Graves, 2013) performs handwriting recognition by dynamically determining “an alignment between the text and the pen locations” so that “it learns to decide which character to write next”. One can view these as particular variants of memory networks where in that case the memory only extends back a single sentence or character sequence.  6  Published as a conference paper at ICLR 2015  Table 1: Results on the large-scale QA task of (Fader et al., 2013).  Method (Fader et al., 2013) (Bordes et al., 2014b) MemNN (embedding only) MemNN (with BoW features)  F1 0.54 0.73 0.72 0.82  Table 2: Memory hashing results on the large-scale QA task of (Fader et al., 2013).  Method MemNN (no hashing) MemNN (word hash) MemNN (cluster hash)  Embedding F1 Embedding + BoW F1 Candidates (speedup)  0.72 0.63 0.71  0.82 0.68 0.80  14M (0x) 13k (1000x) 177k (80x)  5 EXPERIMENTS  5.1 LARGE-SCALE QA  We perform experiments on the QA dataset introduced in Fader et al. (2013). It consists of 14M statements, stored as (subject, relation, object) triples, which are stored as memories in the MemNN model. The triples are REVERB extractions mined from the ClueWeb09 corpus and cover di- verse topics such as (milne, authored, winnie-the-pooh) and (sheep, be-afraid-of, wolf). Following Fader et al. (2013) and Bordes et al. (2014b), training combines pseudo-labeled QA pairs made of a question and an associated triple, and 35M pairs of paraphrased questions from WikiAnswers like “Who wrote the Winnie the Pooh books?” and “Who is poohs creator?”.  We performed experiments in the framework of re-ranking the top returned candidate answers by several systems measuring F1 score over the test set, following Bordes et al. (2014b). These answers have been annotated as right or wrong by humans, whereas other answers are ignored at test time as we do not know their label. We used a MemNN model of Section 3 with a k = 1 supporting memory, which ends up being similar to the approach of Bordes et al. (2014b).6 We also tried adding the bag of words features of Section 3.6 as well. Time and unseen word modeling were not used. Results are given in Table 1. The results show that MemNNs are a viable approach for large scale QA in terms of performance. However, lookup is linear in the size of the memory, which with 14M facts is slow. We therefore implemented the memory hashing techniques of Section 3.3 using both hashing of words and clustered embeddings. For the latter we tried K = 1000 clusters. The results given in Table 2 show that one can get signiﬁcant speedups (∼80x) while maintaining similar performance using the cluster-based hash. The string hash on the other hand loses performance (whilst being a lot faster) because answers which share no words are now no longer matched.  5.2 SIMULATED WORLD QA  Similar to the approach of Bordes et al. (2010) we also built a simple simulation of 4 characters, 3 objects and 5 rooms – with characters moving around, picking up and dropping objects. The actions are transcribed into text using a simple automated grammar, and labeled questions are generated in a similar way. This gives a QA task on simple “stories” such as in Figure 1. The overall difﬁculty of the task is that multiple statements have to be used to do inference when asking where an object is, e.g. to answer where is the milk in Figure 1 one has to understand the meaning of the actions “picked up” and “left” and the inﬂuence of their relative order. We generated 7k statements and 3k questions from the simulator for training7, and an identical number for testing and compare MemNNs to RNNs and LSTMs (long short term memory RNNs (Hochreiter & Schmidhuber, 1997)) on this task. To  6We use a larger 128 dimension for embeddings, and no ﬁne tuning, hence the result of MemNN slightly  differs from those reported in Bordes et al. (2014b).  7Learning curves with different numbers of training examples are given in Appendix D.  7  Published as a conference paper at ICLR 2015  Table 3: Test accuracy on the simulation QA task.  Method RNN LSTM MemNN k = 1 MemNN k = 1 (+time) MemNN k = 2 (+time)  actor w/o before  Difﬁculty 1 actor 60.9% 64.8% 31.0% 60.2% 100%  Difﬁculty 5  actor+object  27.9% 49.1% 24.0% 42.5% 100%  actor 23.8% 35.2% 21.9% 60.8% 100%  actor+object  17.8% 29.0% 18.5% 44.4% 99.9%  100% 100% 97.8% 99.9% 100%  test with sequences of words as input (Section 3.2) the statements are joined together again with a simple grammar8, to produce sentences that may contain multiple statements, see e.g., Figure 2. We control the complexity of the task by setting a limit on the number of time steps in the past the entity we ask the question about was last mentioned. We try two experiments: using a limit of 1, and of 5, i.e., if the limit is 5 then we pick a random sentence between 1-5 time steps in the past. If this chosen sentence only mentions an actor, e.g., “Bill is in the kitchen” then we generate the question “where is Bill?” or “where was Bill before the kitchen?”. If the sentence mentions an object, e.g., “Bill dropped the football” then we ask the question “where is the football?”. For the answers we consider two options: (i) single word answers; and (ii) a simple grammar for generating true answers in sentence form, e.g., “kitchen” for (i) and “He is in the kitchen I believe” (and other variants) for (ii). More details on the dataset generation are given in Appendix A. Note that in the object case the supporting statements necessary to deduce the answer may not lie in the last 5 sentences, e.g., in this example the answer depends on other sentences to ﬁnd out where Bill actually was when he dropped the football. In fact, in the dataset we generated necessary supporting statements can be up to 65 sentences before (but are usually closer). For that reason, we also conducted two further types of experiments: where we only ask questions about actors (easier) and about actors and objects (harder). We also consider the actor-based questions without the “before” questions for the simplest possible task (i.e. “where is Bill?” but not “where was Bill before the kitchen?” questions).  Figure 2: Sample test set predictions (in red) for the simulation in the setting of word-based input and where answers are sentences and an LSTM is used as the R component of the MemNN.  Joe went to the garden then Fred picked up the milk; Joe moved to the bathroom and Fred dropped the milk, and then Dan moved to the living room. Where is Dan? A: living room I believe Where is Joe? A: the bathroom Fred moved to the bedroom and Joe went to the kitchen then Joe took the milk there and Dan journeyed to the bedroom; Joe discarded the milk. Where is the milk now ? A: the milk is in the kitchen Where is Dan now? A: I think he is in the bedroom Joe took the milk there, after that Mike travelled to the ofﬁce, then Joe went to the living room, next Dan went back to the kitchen and Joe travelled to the ofﬁce. Where is Joe now? A: I think Joe is in the ofﬁce  For the baseline RNN and LSTM systems we perform language modeling with backpropagation through time (Mikolov et al., 2010), but where we backprop only on answer words9. We optimized the hyperparameters: size of the hidden layer, bptt steps, and learning rate for each dataset. For MemNNs we ﬁxed the embedding dimension to 100, learning rate to 0.01 and margin γ to 0.1 and 10 epochs of training in all experiments.  Results The results for the single word answer setting (i) are given in Table 3. For the actor-only tasks, RNN and LSTMs solve the simpler difﬁculty level 1 task without before questions (“w/o  8We also tried the same kind of experiments with sentence-level rather than word-sequence input, without  joining sentences, giving results with similar overall conclusions, see Appendix E.  9We tried using standard language modeling on the questions as well, with slightly worse results.  8  Published as a conference paper at ICLR 2015  Figure 3: An example story with questions correctly answered by a MemNN. The MemNN was trained on the simulation described in Section 5.2 and had never seen many of these words before, e.g., Bilbo, Frodo and Gollum.  Bilbo travelled to the cave. Gollum dropped the ring there. Bilbo took the ring. Bilbo went back to the Shire. Bilbo left the ring there. Frodo got the ring. Frodo journeyed to Mount-Doom. Frodo dropped the ring there. Sauron died. Frodo went back to the Shire. Bilbo travelled to the Grey-havens. The End. Where is the ring? A: Mount-Doom Where is Bilbo now? A: Grey-havens Where is Frodo now? A: Shire  Figure 4: An example dialogue with a MemNN system trained on both the simulation data and the large-scale QA data. The system is able to (attempt to) answer questions about general world knowledge and about speciﬁc story-based statements in the dialogue.  Fred went to the kitchen. Fred picked up the milk. Fred travelled to the ofﬁce. Where is the milk ? A: ofﬁce Where does milk come from ? A: milk come from cow What is a cow a type of ? A: cow be female of cattle Where are cattle found ? A: cattle farm become widespread in brazil What does milk taste like ? A: milk taste like milk What does milk go well with ? A: milk go with coffee Where was Fred before the ofﬁce ? A: kitchen  before”), but perform worse with before questions, and even worse on the difﬁculty 5 tasks. This demonstrates that the poor performance of the RNN is due to its failure to encode long(er)-term memory. This would likely deteriorate even further with higher difﬁculty levels (distances). LSTMs are however better than RNNs, as expected, as they are designed with a more sophisticated memory model, but still have trouble remembering sentences too far in the past. MemNNs do not have this memory limitation and its mistakes are instead due to incorrect usage of its memory, when the wrong statement is picked by sO. Time features are necessary for good performance on before questions or difﬁculty > 1 (i.e., when the answer is not in the last statement), otherwise sO can pick a statement about a person’s whereabouts but they have since moved. Finally, results on the harder actor+object task indicate that MemNN also successfully perform 2-stage inference using k = 2, whereas MemNNs without such inference (with k = 1) and RNNs and LSTMs fail. We also tested MemNNs in the multi-word answer setting (ii) with similar results, whereby MemNNs outperform RNNs and LSTMs, which are detailed in Appendix F. Example test prediction output demonstrating the model in that setting is given in Figure 2.  5.2.1 QA WITH PREVIOUSLY UNSEEN WORDS  We then tested the ability of MemNNs to deal with previously unseen words at test time using the unseen word modeling approach of Sections 3.5 and 3.6. We trained the MemNN on the same sim- ulated dataset as before and test on the story given in Figure 3. This story is generated using similar structures as in the simulation data, except that the nouns are unknowns to the system at training time. Despite never seeing any of the Lord of The Rings speciﬁc words before (e.g., Bilbo, Frodo, Sauron, Gollum, Shire and Mount-Doom), MemNNs are able to correctly answer the questions.  MemNNs can discover simple linguistic patterns based on verbal forms such as (X, dropped, Y), (X, took, Y) or (X, journeyed to, Y) and can successfully generalize the meaning of their instantiations using unknown words to perform 2-stage inference. Without the unseen word modeling described in Section 3.5, they completely fail on this task.  9  Published as a conference paper at ICLR 2015  5.3 COMBINING SIMULATED DATA AND LARGE-SCALE QA  Combining simulated world learning with real-world data might be one way to show the power and generality of the models we design. We implemented a naive setup towards that goal: we took the two models from Sections 5.1 and 5.2, trained on large-scale QA and simulated data respectively, and built an ensemble of the two. We present the input to both systems and then for each question simply output the response of the two choices with the highest score. This allows us to perform simple dialogues with our combined MemNN system. The system is then capable of answering both general knowledge questions and speciﬁc statements relating to the previous dialogue. An example dialogue trace is given in Fig. 4. Some answers appear ﬁne, whereas others are nonsensical. Future work should combine these models more effectively, for example by multitasking directly the tasks with a single model.  6 CONCLUSIONS AND FUTURE WORK  In this paper we introduced a powerful class of models, memory networks, and showed one instanti- ation for QA. Future work should develop MemNNs for text further, evaluating them on harder QA and open-domain machine comprehension tasks (Richardson et al., 2013). For example, large scale QA tasks that require multi-hop inference such as WebQuestions should also be tried Berant et al. (2013). More complex simulation data could also be constructed in order to bridge that gap, e.g., requiring coreference, involving more verbs and nouns, sentences with more structure and requiring more temporal and causal understanding. More sophisticated architectures should also be explored in order to deal with these tasks, e.g., using more sophisticated memory management via G and more sophisticated sentence representations. Weakly supervised settings are also very important, and should be explored, as many datasets only have supervision in the form of question answer pairs, and not supporting facts as well as we used here. Finally, we believe this class of models is much richer than the one speciﬁc variant we detail here, and that we have currently only explored one speciﬁc variant of memory networks. Memory networks should be applied to other text tasks, and other domains, such as vision, as well.  ACKNOWLEDGMENTS  We thank Tomas Mikolov for useful discussions.  REFERENCES Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly  learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.  Berant, Jonathan, Chou, Andrew, Frostig, Roy, and Liang, Percy. Semantic parsing on freebase from  question-answer pairs. In EMNLP, pp. 1533–1544, 2013.  Berant, Jonathan, Srikumar, Vivek, Chen, Pei-Chun, Huang, Brad, Manning, Christopher D, Van- der Linden, Abby, Harding, Brittany, and Clark, Peter. Modeling biological processes for reading comprehension. In Proc. EMNLP, 2014.  Bordes, Antoine, Usunier, Nicolas, Collobert, Ronan, and Weston, Jason. Towards understanding  situated natural language. In AISTATS, 2010.  Bordes, Antoine, Chopra, Sumit, and Weston, Jason. Question answering with subgraph embed-  dings. In Proc. EMNLP, 2014a.  Bordes, Antoine, Weston, Jason, and Usunier, Nicolas. Open question answering with weakly su-  pervised embedding models. ECML-PKDD, 2014b.  Das, Sreerupa, Giles, C Lee, and Sun, Guo-Zheng. Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana University, 1992.  Fader, Anthony, Zettlemoyer, Luke, and Etzioni, Oren. Paraphrase-driven learning for open question  answering. In ACL, pp. 1608–1618, 2013.  10  Published as a conference paper at ICLR 2015  Graves, Alex.  Generating sequences with recurrent neural networks.  arXiv:1308.0850, 2013.  Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural turing machines.  arXiv:1410.5401, 2014.  Haykin, Simon. Neural networks: A comprehensive foundation. 1994.  arXiv preprint  arXiv preprint  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Iyyer, Mohit, Boyd-Graber, Jordan, Claudino, Leonardo, Socher, Richard, and III, Hal Daum´e. A neural network for factoid question answering over paragraphs. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pp. 633–644, 2014.  Kolomiyets, Oleksandr and Moens, Marie-Francine. A survey on question answering technology  from an information retrieval perspective. Information Sciences, 181(24):5412–5434, 2011.  Lin, Tsungnam, Horne, Bil G, Tiˇno, Peter, and Giles, C Lee. Learning long-term dependencies in narx recurrent neural networks. Neural Networks, IEEE Transactions on, 7(6):1329–1338, 1996.  Miikkulainen, Risto. {DISCERN}:{A} distributed artiﬁcial neural network model of script process-  ing and memory. 1990.  Mikolov, Tomas, Karaﬁ´at, Martin, Burget, Lukas, Cernock`y, Jan, and Khudanpur, Sanjeev. Recur-  rent neural network based language model. In Interspeech, pp. 1045–1048, 2010.  Richardson, Matthew, Burges, Christopher JC, and Renshaw, Erin. Mctest: A challenge dataset for  the open-domain machine comprehension of text. In EMNLP, pp. 193–203, 2013.  Schmidhuber, J¨urgen. Learning to control fast-weight memories: An alternative to dynamic recur-  rent networks. Neural Computation, 4(1):131–139, 1992.  Schmidhuber, J¨urgen. A self-referentialweight matrix. In ICANN93, pp. 446–450. Springer, 1993.  Tolkien, John Ronald Reuel. The Fellowship of the Ring. George Allen & Unwin, 1954.  Weston, Jason, Bengio, Samy, and Usunier, Nicolas. Wsabie: Scaling up to large vocabulary im- age annotation. In Proceedings of the Twenty-Second international joint conference on Artiﬁcial Intelligence-Volume Volume Three, pp. 2764–2770. AAAI Press, 2011.  Yih, Wen-Tau, He, Xiaodong, and Meek, Christopher. Semantic parsing for single-relation question answering. In Proceedings of ACL. Association for Computational Linguistics, June 2014. URL http://research.microsoft.com/apps/pubs/default.aspx?id=214353.  Zaremba, Wojciech and Sutskever, Ilya. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.  11  Published as a conference paper at ICLR 2015  A SIMULATION DATA GENERATION  Aim We have built a simple simulation which behaves much like a classic text adventure game. The idea is that generating text within this simulation allows us to ground the language used.  Some comments about our intent:  • Firstly, while this currently only encompasses a very small part of the kind of language and understanding we want a model to learn to move towards full language understanding, we believe it is a prerequisite that models should perform well on this kind of task for them to work on real-world environments.  • Secondly, our aim is to make this simulation more complex and to release improved ver- sions over time. Hopefully it can then scale up to evaluate more and more useful properties.  Currently, tasks within the simulation are restricted to question answering tasks about the location of people and objects. However, we envisage other tasks should be possible, including asking the learner to perform actions within the simulation (“Please pick up the milk”, “Please ﬁnd John and give him the milk”) and asking the learner to describe actions (”What did John just do?”).  Actions The underlying actions in the simulation consist of the following:  go <location>, get <object>, put <object1> in/on <object2>, drop <object>,  look,  inventory,  get <object1> from <object2>, give <object> to <actor>,  examine <object>.  There are a set of constraints on those actions. For example an actor cannot get something that they or someone else already has, they cannot go to a place they are already at, cannot drop something they do not already have, and so on.  Executing Actions and Asking Questions Using the underlying actions and their constraints, there is then a (hand-built) model that deﬁnes how actors act. Currently this is very simple: they try to make a random valid action, at the moment restricted to go or go, get and drop depending on the which of two types of experiments we are running: (i) actor; or (ii) actor + object.  If we write these actions down in text form this gives us a very simple “story” which is executable by the simulation, e.g., joe go kitchen; fred go kitchen; joe get milk; joe go ofﬁce; joe drop milk; joe go bathroom. This example corresponds to the story given in Figure 1. The system can then ask questions about the state of the simulation e.g., where milk?, where joe?, where joe before ofﬁce? It is easy to calculate the true answers for these questions as we have access to the underlying world. What remains is to convert both the statements and the questions to look more like natural language.  Simple Grammar For Generating Language In order to produce more natural looking text with lexical variety we built a simple automated grammar. Each verb is assigned a set of synonyms, e.g., the simulation command get is replaced with either picked up, got, grabbed or took, and drop is replace with either dropped, left, discarded or put down. Similarly, each object and actor can have a set of replacement synonyms as well, although currently there is no ambiguity there in our experiments, we simply add articles or not. We do add lexical variation to questions, e.g., “Where is John ?” or “Where is John now ?”.  Joining Statements Finally, for the word sequence training setting, we join the statements above into compound sentences. To do this we simply take the set of statements and then join them randomly with one of the following: “.”, “and”, “then”, “, then”, “;”, “, later”, “, after that”, “, and then”, or “, next”. Example output can be seen in Figure 2.  Issues There are a great many aspects of language not yet modeled. For example, currently coref- erence is not modeled (e.g., “He picked up the milk”) and similarly there are no compound noun phrases (“John and Fred went to the kitchen”). Some of these seem easy to add to the simulation. The hope is that adding these complexities will help evaluate models in a controlled way, within the simulated environment, which is hard to do with real data. Of course, this is not a substitute for real data which our models should be applied to as well, but does serve as a useful testbed.  12  Published as a conference paper at ICLR 2015  B WORD SEQUENCE TRAINING  For segmenting an input word stream as generated in Appendix A we use a segmenter of the form:  seg(c) = W ⊤  segUSΦseg(c)  where Wseg is a vector (effectively the parameters of a linear classiﬁer in embedding space). As we are already in the fully supervised setting, where for each question in the training set we are given the answer and the supporting facts from the input stream, we can also use that supervision for the segmenter as well. That is, for any known supporting fact, such as “Bill is in the Kitchen” for the question “Where is Bill?” we wish the segmenter to ﬁre for such a statement, but not for unﬁnished statements such as “Bill is in the”. We can thus write our training criterion for segmentation as the minimization of:  X  f ∈F  max(0, γ − seg(f )) + X  ¯f ∈ ¯F  max(0, γ + seg( ¯f ))  (12)  where F are all known supporting segments in the labeled training set, and ¯F are all other segments in the training set.  C WRITE TIME FEATURE TRAINING  The training procedure to take into account modeling write time is slightly different to that described in Section 3.1. Write time features are important so that the MemNN knows when each memory was written, and hence knows the ordering of statements that comprise a story or dialogue. Note that this is different to time information described in the text of a statement, such as the tense of a statement, or statements containing time expressions, e.g., “He went to the ofﬁce yesterday”. For such cases, write time features are not directly necessary, and they could (potentially) be modeled directly from the text.  As was described in Section 3.4 we add three write time features to the model and score triples using:  sOt (x, y, y′) = Φx(x)⊤UOt  ⊤UOt(cid:16)Φy(y) − Φy(y′) + Φt(x, y, y′)(cid:17).  (13)  If sO(x, y, y′) > 0 the model prefers y over y′, and if sO(x, y, y′) < 0 it prefers y′. The argmax of eq. (2) and (3) are replaced by a loop over memories i = 1, . . . , N , keeping the winning memory (y or y′) at each step, and always comparing the current winner to the next memory mi. That is, at inference time, for a k = 2 model the arg max functions of eq. (2) and (3) are replaced with o1 = Ot(x, m) and o2 = Ot([x, mo1 ], m) where Ot is deﬁned in Algorithm 1 below.  Algorithm 1 Ot replacement to arg max when using write time features  function Ot(q, m)  t ← 1 for i = 2, . . . , N do  if sOt (q, mi, mt) > 0 then  t ← i  end if  end for return t end function  Φt(x, y, y′) uses three new features which take on the value 0 or 1: whether x is older than y, x is older than y′, and y older than y′. When ﬁnding the second supporting memory (computing Ot([x, mo1], m)) we encode whether mo1 is older than y, mo1 is older than y′, and y older than y′ to capture the relative age of the ﬁrst supporting memory w.r.t. the second one in the ﬁrst two features. Note that when ﬁnding the ﬁrst supporting memory (i.e., for Ot(x, m)) the ﬁrst two features are useless as x is the last thing in the memory and hence y and y′ are always older.  13  Published as a conference paper at ICLR 2015  To train our model with write time features we need to replace the hinge loss in eqs. (6)-(7) with a loss that matches Algorithm 1. To do this, we instead minimize:  max(0, γ − sOt (x, mo1 , ¯f )) + P¯f 6=mo1 P¯f 6=mo1 max(0, γ − sOt ([x, mo1], mo2 , ¯f ′)) + P¯f ′6=mo2  P¯f ′6=mo2  max(0, γ + sOt (x, ¯f , mo1)) +  max(0, γ + sOt ([x, mo1], ¯f ′, mo2) +  max(0, γ − sR([x, mo1 , mo2], r) + sR([x, mo1 , mo2], ¯r]))  P¯r6=r  The last term is the same as in eq. (8) and is for the ﬁnal ranking of words to return a response, which remains unchanged (as usual, this can also be replaced by an RNN for a more sophisticated model). Terms 1-4 replace eqs. (6)-(7) by considering triples directly. For both mo1 and mo2 we need to have two terms considering them as the second or third argument to SOt as they may appear on either side during inference (via Algorithm 1). As before, at every step of SGD we sample ¯f , ¯f ′, ¯r rather than compute the whole sum for each training example.  D WORD-SEQUENCE LEARNING CURVE EXPERIMENTS  We computed the test accuracy of MemNNs k = 2 (+ time) for varying amounts of training data: 100, 500, 1000 and 3000 training questions. The results are given in Table 4. These results can be compared with RNNs and LSTMs on the full data (3000 examples) by comparing with Figure 3. For example, on the difﬁculty 5 actor and actor + object tasks MemNNs outperform LSTMs even using 30 times less training examples.  Table 4: Test accuracy of MemNNs k = 2 (+time) on the word-sequence simulation QA task for differing numbers of training examples (number of questions).  Num. training questions 100 500 1000 3000  Difﬁculty 1 actor  actor  + object 73.8% 64.9% 99.9% 99.2% 99.9% 100% 100% 100%  Difﬁculty 5  actor  actor  + object 74.4% 49.8% 99.8% 95.1% 100% 98.4% 100% 99.9%  E SENTENCE-LEVEL EXPERIMENTS  We conducted experiments where input was at the sentence-level, that is the data was already pre- segemented into statements and questions as input to the MemNN (as opposed to being input as a stream of words). Results comparing RNNs with MemNNs are given in Table 5. The conclusions are similar to those at the word level from Section 5.2. That is, MemNNs outperform RNNs, and that inference that ﬁnds k = 2 supporting statements and time features are necessary for the actor w/o before + object task.  Table 5: Test accuracy on the sentence-level simulation QA task.  Method RNN MemNN k = 1 MemNN k = 1 (+time) MemNN k = 2 (+time)  Difﬁculty 1  Difﬁculty 5  actor  w/o before  actor w/o before  + object  actor  w/o before  actor w/o before  + object  29% 46% 100% 100%  17% 21% 73% 99.4%  100% 90% 100% 100%  58% 9% 73%  99.95%  14  Published as a conference paper at ICLR 2015  F MULTI-WORD ANSWER SETTING EXPERIMENTS  We conducted experiments for the simulation data in the case where the answers are sentences (see Appendix A and Figure 2). As the single word answer model can no longer be used, we simply compare MemNNs using either RNNs or LSTMs for the response module R. As baselines we can still use RNNs and LSTMs in the standard setting of being fed words only including the statements and the question as a word stream. In contrast, the MemNN RNN and LSTMs are effectively fed the output of the O module (see Section 3.1). In these experiments we only consider the difﬁculty 5 actor+object setting in the case of MemNNs with k = 2 iterations (eq. (3)), which means the module R is fed the features [x, mo1 , mo2] after the modules I, G and O have run. The sentence generation is performed on the test data, and the evaluation we chose is as follows. A correct generation has to contain the correct location answer, and can optionally contain the subject or a correct pronoun referring to it. For example the question “Where is Bill?” allows the correct answers “Kitchen”, “In the kitchen”, “Bill is in the kitchen”, “He is in the kitchen” and “I think Bill is in the kitchen”. However incorrect answers contain an incorrect location or subject reference, for example “Joe is in the kitchen”, “It is in the kitchen” or “Bill is in the bathroom I believe”. We can then measure the percentage of text examples that are correct using this metric.  The numerical results are given in Table 6, and example output is given in Figure 2. The results indicate that MemNNs with LSTMs perform quite strongly, outperforming MemNNs using RNNs. However, both MemNN variant outperform both RNNs and LSTMs by some distance.  Table 6: Test accuracy on the multi-word answer simulation QA task. We compare conventional RNN and LSTMs with MemNNs using an RNN or LSTM module R (i.e., where R is fed features [x, mo1 , mo2] after the modules I, G and O have run).  Model MemNN: IGO features [x, mo1 , mo2] Word features RNN LSTM  68.83% 90.98%  13.97% 14.01%  15  ",
1412.6296,2015,Generative Modeling of Convolutional Neural Networks,"['Generative Modeling of Convolutional Neural Networks', 'Jifeng Dai', 'Yang Lu', 'and Ying-Nian Wu']",https://arxiv.org/pdf/1412.6296,"5 1 0 2    r p A 9         ]  V C . s c [      2 v 6 9 2 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  GENERATIVE MODELING OF CONVOLUTIONAL NEU- RAL NETWORKS  Jifeng Dai Microsoft Research jifdai@microsoft.com  Yang Lu and Ying Nian Wu University of California, Los Angeles {yanglv, ywu}@stat.ucla.edu  ABSTRACT  This paper investigates generative modeling of the convolutional neural networks (CNNs). The main contributions include: (1) We construct a generative model for CNNs in the form of exponential tilting of a reference distribution. (2) We pro- pose a generative gradient for pre-training CNNs by a non-parametric importance sampling scheme, which is fundamentally different from the commonly used dis- criminative gradient, and yet has the same computational architecture and cost as the latter. (3) We propose a generative visualization method for the CNNs by sam- pling from an explicit parametric image distribution. The proposed visualization method can directly draw synthetic samples for any given node in a trained CNN by the Hamiltonian Monte Carlo (HMC) algorithm, without resorting to any extra hold-out images. Experiments on the ImageNet benchmark show that the pro- posed generative gradient pre-training helps improve the performances of CNNs, and the proposed generative visualization method generates meaningful and var- ied samples of synthetic images from a large and deep CNN.  1  INTRODUCTION  Recent years have witnessed the triumphant return of the feedforward neural networks, especially the convolutional neural networks (CNNs) (LeCun et al., 1989; Krizhevsky et al., 2012; Girshick et al., 2014). Despite the successes of the discriminative learning of CNNs, the generative aspect of CNNs has not been thoroughly investigated. But it can be very useful for the following reasons: (1) The generative pre-training has the potential to lead the network to a better local optimum; (2) Samples can be drawn from the generative model to reveal the knowledge learned by the CNN. Although many generative models and learning algorithms have been proposed (Hinton et al., 2006a;b; Rifai et al., 2011; Salakhutdinov & Hinton, 2009), most of them have not been applied to learning large and deep CNNs. In this paper, we study the generative modeling of the CNNs. We start from deﬁning probability distributions of images given the underlying object categories or class labels, such that the CNN with a ﬁnal logistic regression layer serves as the corresponding conditional distribution of the class labels given the images. These distributions are in the form of exponential tilting of a reference distribution, i.e., exponential family models or energy-based models relative to a reference distribution. With such a generative model, we proceed to study it along two related themes, which differ in how to handle the reference distribution or the null model. In the ﬁrst theme, we propose a non-parametric generative gradient for pre-training the CNN, where the CNN is learned by the stochastic gradient algorithm that seeks to minimize the log-likelihood of the generative model. The gradient of the log- likelihood is approximated by the importance sampling method that keeps reweighing the images that are sampled from a non-parametric implicit reference distribution, such as the distribution of all the training images. The generative gradient is fundamentally different from the commonly used discriminative gradient, and yet in batch training, it shares the same computational architecture as well as computational cost as the discriminative gradient. This generative learning scheme can be  1  Published as a conference paper at ICLR 2015  used in a pre-training stage that is to be followed by the usual discriminative training. The generative log-likelihood provides stronger driving force than the discriminative criteria for stochastic gradient by requiring the learned parameters to explain the images instead of their labels. Experiments on the MNIST (LeCun et al., 1998) and the ImageNet (Deng et al., 2009) classiﬁcation benchmarks show that this generative pre-training scheme helps improve the performance of CNNs. The second theme in our study of generative modeling is to assume an explicit parametric form of the reference distribution, such as the Gaussian white noise model, so that we can draw synthetic images from the resulting probability distributions of images. The sampling can be accomplished by the Hamiltonian Monte Carlo (HMC) algorithm (Neal, 2011), which iterates between a bottom- up convolution step and a top-down deconvolution step. The proposed visualization method can directly draw samples of synthetic images for any given node in a trained CNN, without resorting to any extra hold-out images. Experiments show that meaningful and varied synthetic images can be generated for nodes of a large and deep CNN discriminatively trained on ImageNet.  2 PAST WORK  The generative model that we study is an energy-based model. Such models include ﬁeld of ex- perts (Roth & Black, 2009), product of experts (Hinton, 2002), Boltzmann machines (Hinton et al., 2006a), model based on neural networks (Hinton et al., 2006b), etc. However, most of these gener- ative models and learning algorithms have not been applied to learning large and deep CNNs. The relationship between the generative models and the discriminative approaches has been exten- sively studied (Jordan, 2002; Liang & Jordan, 2008). The usefulness of generative pre-training for deep learning has been studied by Erhan et al. (2010) etc. However, this issue has not been thor- oughly investigated for CNNs. As to visualization, our work is related to Erhan et al. (2009); Le et al. (2012); Girshick et al. (2014); Zeiler & Fergus (2013); Long et al. (2014). In Girshick et al. (2014); Long et al. (2014), the high- scoring image patches are directly presented. In Zeiler & Fergus (2013), a top-down deconvolution process is employed to understand what contents are emphasized in the high-scoring input image patches. In Erhan et al. (2009); Le et al. (2012); Simonyan et al. (2014), images are synthesized by maximizing the response of a given node in the network. In our work, a generative model is formally deﬁned. We sample from the well-deﬁned probability distribution by the HMC algorithm, generating meaningful and varying synthetic images, without resorting to a large collection of hold- out images (Girshick et al., 2014; Zeiler & Fergus, 2013; Long et al., 2014).  3 GENERATIVE MODEL BASED ON CNN  3.1 PROBABILITY DISTRIBUTIONS ON IMAGES  Suppose we observe images from many different object categories. Let x be an image from an object category y. Consider the following probability distribution on x,  py(x; w) =  1  Zy(w)  exp (fy(x; w)) q(x),  (1)  where q(x) is a reference distribution common to all the categories, fy(x; w) is a scoring func- tion for class y, w collects the unknown parameters to be learned from the data, and Zy(w) =  Eq[exp(fy(x; w))] =(cid:82) exp(fy(x; w))q(x)dx is the normalizing constant or partition function. The  distribution py(x; w) is in the form of an exponential tilting of the reference distribution q(x), and can be considered an energy-based model or an exponential family model. In Model (1), the ref- erence distribution q(x) may not be unique. If we change q(x) to q1(x), then we can change fy(x; w) to fy(x; w) − log[q1(x)/q(x)], which may correspond to a fy(x; w1) for a different w1 if the parametrization of fy(x, w) is ﬂexible enough. We want to choose q(x) so that either q(x) is reasonably close to py(x; w) as in our non-parametric generative gradient method, or the resulting py(x; w) based on q(x) is easy to sample from as in our generative visualization method.  2  Published as a conference paper at ICLR 2015  (cid:80)  For an image x, let y be the underlying object category or class label, so that p(x|y; w) = py(x; w). Suppose the prior distribution on y is p(y) = ρy. The posterior distribution of y given x is  ,  p(y|x, w) =  exp(fy(x; w) + αy) y exp(fy(x; w) + αy)  (2) where αy = log ρy − log Zy(w). p(y|x, w) is in the form of a multi-class logistic regression, where αy can be treated as an intercept parameter to be estimated directly if the model is trained discriminatively. Thus for notational simplicity, we shall assume that the intercept term αy is already absorbed into w for the rest of the paper. Note that fy(x; w) is not unique in (2). If we change fy(x; w) to fy(x; w) − g(x) for a g(x) that is common to all the categories, we still have the same p(y|x; w). This non-uniqueness corresponds to the non-uniqueness of q(x) in (1) mentioned above. Given a set of labeled data {(xi, yi)}, equations (1) and (2) suggest two different methods to estimate i log p(xi|yi, w), i log p(xi, yi|w), where the prior prob- ability of ρy can be estimated by class frequency of category y. The other is to maximize the i log p(yi|xi, w). For the discriminative model (2), a pop- ular choice of fy(x; w) is multi-layer perceptron or CNN, with w being the connection weights, and the top-layer is a multi-class logistic regression. This is the choice we adopt throughout this paper.  the parameters w. One is to maximize the generative log-likelihood lG(w) = (cid:80) which is the same as maximizing the full log-likelihood(cid:80) discriminative log-likelihood lD(w) =(cid:80)  (3)  (4)  (5)  (6)  where αy is absorbed into w as mentioned above, and the expectation for discriminative gradient is  3.2 GENERATIVE GRADIENT  The gradient of the discriminative log-likelihood is calculated according to  (cid:20) ∂  ∂w  fy(xi; w)  ,  exp(fy(xi; w)) y exp(fy(xi; w))  .  (cid:21)  (cid:21)  (cid:21)  ∂ ∂w  (cid:88)  y  fyi (xi; w) − ED (cid:80) (cid:20) ∂  fy(xi; w)  fyi(xi; w) − EG  ∂ ∂w  log p(yi|xi, w) =  ∂ ∂w  ED  fy(xi; w)  =  (cid:20) ∂  ∂w  ∂ ∂w  (cid:20) ∂  ∂w  (cid:21)  ∂ ∂w  (cid:90) ∂  ∂w  The gradient of the generative log-likelihood is calculated according to  log pyi(xi; w) =  fyi(x; w)  ,  ∂w  where the expectation for generative gradient is  EG  fyi(x; w)  =  fyi(x; w)  exp(fyi(x; w))q(x),  1  Zyi (w)  which can be approximated by importance sampling. Speciﬁcally, let {˜xj}m j=1 be a set of samples from q(x), for instance, q(x) is the distribution of images from all the categories. Here we do not attempt to model q(x) parametrically, instead, we treat it as an implicit non-parametric distribution. Then by importance sampling,  (cid:20) ∂  ∂w  EG  (cid:21)  fyi(x; w)  ≈(cid:88) fyi (xi; w) −(cid:88)  j  ∂ ∂w  fyi(˜xj; w)Wj,  (7)  where the importance weight Wj ∝ exp(fyi(˜xj; w)) and is normalized to have sum 1. Namely,  ∂ ∂w  log pyi(xi; w) ≈ ∂ ∂w  ∂ ∂w  j  fyi(˜xj; w)  exp(fyi(˜xj; w)) k exp(fyi(˜xk; w))  .  (8)  (cid:80)  The discriminative gradient and the generative gradient differ subtly and yet fundamentally in calcu- lating E[∂fy(x; w)/∂w], whose difference from the observed ∂fyi (xi; w)/∂w provides the driving force for updating w. In the discriminative gradient, the expectation is with respect to the posterior distribution of the class label y while the image xi is ﬁxed, whereas in the generative gradient, the expectation is with respect to the distribution of the images x while the class label yi is ﬁxed. In  3  Published as a conference paper at ICLR 2015  general, it is easier to adjust the parameters w to predict the class labels than to reproduce the fea- tures of the images. So it is expected that the generative gradient provides stronger driving force for updating w. The non-parametric generative gradient can be especially useful in the beginning stage of training or what can be called pre-training, where w is small, so that the current py(x; w) for each category y is not very separated from q(x), which is the overall distribution of x. In this stage, the importance weights Wj are not very skewed and the effective sample size for importance sampling can be large. So updating w according to the generative gradient can provide useful pre-training with the potential to lead w toward a good local optimum. If the importance weights Wj start to become skewed and the effective sample size starts to dwindle, then this indicates that the categories py(x; w) start to separate from q(x) as well as from each other, so we can switch to discriminative training to further separate the categories.  3.3 BATCH TRAINING AND GENERATIVE LOSS LAYER  At ﬁrst glance, the generative gradient appears computationally expensive due to the need to sample from q(x). In fact, with q(x) being the collection of images from all the categories, we may use each batch of samples as an approximation to q(x) in the batch training mode. Speciﬁcally, let {(xi, yi)}n  (cid:80) i log pyi(xi; w) via generative gradient. In the calculation of ∂ log pyi(xi; w)/∂w, {xj}n  i=1 be a batch set of training examples, and we seek to maximize j=1 can be used as samples from q(x). In this way, the computational cost of the generative gradient is about the same as that of the discriminative gradient. Moreover, the computation of the generative gradient can be induced to share the same back prop- agation architecture as the discriminative gradient. Speciﬁcally, the calculation of the generative gradient can be decoupled into the calculation at a new generative loss layer and the calculation at lower layers. To be more speciﬁc, by replacing {˜xj}m j=1, we can rewrite (8) in the following form:  j=1 in (8) by the batch sample {xj}n  log pyi(xi; w) ≈(cid:88)  y,j  ∂ ∂w  ∂ log pyi(xi; w)  ∂fy(xj; w)  ∂fy(xj; w)  ∂w  ,  (9)  where ∂ log pyi(xi; w)/∂fy(xj; w) is called the generative loss layer (to be deﬁned below, with fy(xj; w) being treated here as a variable in the chain rule), while the calculation of ∂fy(xj; w)/∂w is exactly the same as that in the discriminative gradient. This decoupling brings simplicity to programming. We use the notation ∂ log pyi(xi; w)/∂fy(xj; w) for the top generative layer mainly to make it conformal to the chain rule calculation. According to (8), ∂ log pyi(xi; w)/∂fy(xj; w) is deﬁned by  ∂ log pyi(xi; w)  ∂fy(xj; w)  =  (cid:80) (cid:80)  0 1 − exp(fyi(xj; w))  k exp(fyi (xk; w))  − exp(fyi(xj; w))  k exp(fyi(xk; w))  y (cid:54)= yi;  y = yi, j = i;  y = yi, j (cid:54)= i.  (10)    3.4 GENERATIVE VISUALIZATION  Recently, researchers are interested in understanding what the machine learns. Suppose we care about the node at the top layer (the idea can be applied to the nodes at any layer). We consider generating samples from py(x; w) with w already learned by discriminative training (or any other methods). For this purpose, we need to assume a parametric reference distribution q(x), such as Gaussian white noise distribution. After discriminatively learning fy(x; w) for all y, we can sample from the corresponding py(x; w) by Hamiltonian Monte Carlo (HMC) (Neal, 2011). Speciﬁcally, for any category y, we can write py(x; w) ∝ exp(−U (x)), where U (x) = −fy(x; w)+ |x|2/(2σ2) (σ is the standard deviation of q(x)). In physics context, x is a position vector and U (x) is the potential energy function. To implement Hamiltonian dynamics, we need to introduce  4  Published as a conference paper at ICLR 2015  Iteration 0  Iteration 10  Iteration 50  Iteration 100  Figure 1: The sequence of images sampled from the “Starﬁsh, sea star” category of the “AlexNet” network (Krizhevsky et al., 2012) discriminatively trained on ImageNet ILSVRC-2012.  Table 1: Error rates on the MNIST test set of different training approaches utilizing the “LeNet” network (LeCun et al., 1998).  Training approaches Error rates  DG 1.03  GG 0.85  GG+DG 0.78  an auxiliary momentum vector φ and the corresponding kinetic energy function K(φ) = |φ|2/2m, where m denotes the mass. Thus, a ﬁctitious physical system described by the canonical coordinates (x, φ) is deﬁned, and its total energy is H(x, φ) = U (x) + K(φ). Each iteration of HMC draws a random sample from the marginal Gaussian distribution of φ, and then evolve according to the Hamiltonian dynamics that conserves the total energy. A key step in the leapfrog algorithm is the computation of the derivative of the potential energy function ∂U/∂x, which includes calculating ∂fy(x; w)/∂x. The computation of ∂fy(x; w)/∂x involves bottom-up convolution and max-pooling, followed by top-down deconvolution and arg-max un-pooling. The max-pooling and arg-max un-pooling are applied to the current synthesized image (not the input image, which is not needed by our method). The top-down derivative computation is derived from HMC, and is different from Zeiler & Fergus (2013). The visualization sequence of a category is shown in Fig. 1.  4 EXPERIMENTS  4.1 GENERATIVE PRE-TRAINING  In generative pre-training experiments, three different training approaches are studied: i) discrimina- tive gradient (DG); ii) generative gradient (GG); iii) generative gradient pre-training + discriminative gradient reﬁning (GG+DG). We build algorithms on the code of Caffe (Jia et al., 2014) and the ex- periment settings are identical to Jia et al. (2014). Experiments are performed on two commonly used image classiﬁcation benchmarks: MNIST (LeCun et al., 1998) handwritten digit recognition and ImageNet ILSVRC-2012 (Deng et al., 2009) natural image classiﬁcation. MNIST handwritten digit recognition. We ﬁrst study generative pre-training on the MNIST dataset. The “LeNet” network (LeCun et al., 1998) is utilized, which is default for MNIST in Caffe. Although higher accuracy can be achieved by utilizing deeper networks, random image distortion etc, here we stick to the baseline network for fair comparison and experimental efﬁciency. Network training and testing are performed on the train and test sets respectively. For all the three training ap- proaches, stochastic gradient descent is performed in training with a batch size of 64, a base learning rate of 0.01, a weight decay term of 0.0005, a momentum term of 0.9, and a max epoch number of 25. For GG+DG, the pre-training stage stops after 16 epochs and the discriminative gradient tuning stage starts with a base learning rate of 0.003. The experimental results are presented in Table 1. The error rate of LeNet trained by discriminative gradient is 1.03%. When trained by generative gradient, the error rate reduces to 0.85%. When generative gradient pre-training and discriminative gradient reﬁning are both applied, the error rate further reduces to 0.78%, which is 0.25% (24% relatively) lower than that of discriminative gradient.  5  Published as a conference paper at ICLR 2015  Table 2: Top-1 classiﬁcation error rates on the ImageNet ILSVRC-2012 val set of different training approaches.  Training approaches AlexNet ZeilerFergusNet (fast)  DG 40.7 38.4  GG 45.8 44.3  GG+DG 39.6 37.4  Figure 2: Samples from the nodes at the ﬁnal fully-connected layer in the fully trained LeNet model, which correspond to different handwritten digits.  ImageNet ILSVRC-2012 natural image classiﬁcation. In experiments on ImageNet ILSVRC- 2012, two networks are utilized, namely “AlexNet” (Krizhevsky et al., 2012) and “ZeilerFergusNet” (fast) (Zeiler & Fergus, 2013). Network training and testing are performed on the train and val sets respectively. In training, a single network is trained by stochastic gradient descent with a batch size of 256, a base learning rate of 0.01, a weight decay term of 0.0005, a momentum term of 0.9, and a max epoch number of 70. For GG+DG, the pre-training stage stops after 45 epochs and the discriminative gradient tuning stage starts with a base learning rate of 0.003. In testing, top-1 classiﬁcation error rates are reported on the val set by classifying the center and the four corner crops of the input images. As shown in Table 2, the error rates of discriminative gradient training applied on AlexNet and ZeilerFergusNet are 40.7% and 38.4% respectively, while the error rates of generative gradient are 45.8% and 44.3% respectively. Generative gradient pre-training followed by discriminative gradient reﬁning achieves error rates of 39.6% and 37.4% respectively, which are 1.1% and 1.0% lower than those of discriminative gradient. Experiment results on MNIST and ImageNet ILSVRC-2012 show that generative gradient pre- training followed by discriminative gradient reﬁning improves the classiﬁcation accuracies for vary- ing networks. At the beginning stage of training, updating network parameters according to the generative gradient provides useful pre-training, which leads the network parameters toward a good local optimum. As to the computational cost, generative gradient is on par with discriminative gradient. The compu- tational cost of the generative loss layer itself is ignorable in the network compared to the computa- tion at the convolutional layers and the fully-connected layers. The total epoch numbers of GG+DG is on par with that of DG.  (a) conv1  (b) conv2  (c) conv3  (d) conv4  Figure 3: Samples from the nodes at the intermediate convolutional layers (conv1 to conv5) in the fully trained AlexNet model.  (e) conv5  6  Published as a conference paper at ICLR 2015  (a) Hen  (b) Ostrich  (c) Fish  Figure 4: Samples from the nodes at the ﬁnal fully-connected layer (fc8) in the fully trained AlexNet model. More examples are included in the supplementary materials.  (d) Horse cart  7  Published as a conference paper at ICLR 2015  4.2 GENERATIVE VISUALIZATION  In the generative visualization experiments, we visualize the nodes of the LeNet network and the AlexNet network trained by discriminative gradient on MNIST and ImageNet ILSVRC-2012 re- spectively. The algorithm can visualize networks trained by generative gradient as well. We ﬁrst visualize the nodes at the ﬁnal fully-connected layer of LeNet. In the experiments, we delete the drop-out layer to avoid unnecessary noise for visualization. At the beginning of visualization, x is initialized by Gaussian distribution with standard deviation 10. The HMC iteration number, the leapfrog step size, the leapfrog step number, the standard deviation of the reference distribution σ, and the particle mass are set to be 300, 0.0001, 100, 10, and 0.0001 respectively. The visualization results are shown in Fig. 2. We further visualize the nodes in AlexNet, which is a much larger network compared to LeNet. Both nodes from the intermediate convolutional layers (conv1 to conv5) and the ﬁnal fully-connected layer (fc8) are visualized. To visualize the intermediate layers, for instance the layer conv2 with 256 ﬁlters, all layers above conv2 are removed other than the generative visualization layer. The size of the synthesized images are designed so that the dimension of the response from conv2 is 1×1×256. We can visualize each ﬁlter by assigning label from 1 to 256. The leapfrog step size, the leapfrog step number, the standard deviation of the reference distribution σ, and the particle mass are set to be 0.000003, 50, 10, and 0.00001 respectively. The HMC iteration numbers are 100 and 500 for nodes from the intermediate convolutional and the ﬁnal fully-connected layer respectively. The synthesized images for the ﬁnal layer are initialized from the zero image. The samples from the intermediate convolutional layers and the ﬁnal fully-connected layer of AlexNet are shown in Fig. 3 and 4 respectively. The HMC algorithm produces meaningful and varied samples, which reveals what is learned by the nodes at different layers of the network. Note that such samples are generated from the trained model directly, without using a large hold-out collection of images as in Girshick et al. (2014); Zeiler & Fergus (2013); Long et al. (2014). As to the computational cost, it varies for nodes at different layers within different networks. On a desktop with GTX Titian, it takes about 0.4 minute to draw a sample for nodes at the ﬁ- nal fully-connected layer of LeNet. In AlexNet, for nodes at the ﬁrst convolutional layer and at the ﬁnal fully-connected layer, it takes about 0.5 minute and 12 minute to draw a sample respectively. The code can be downloaded at http://www.stat.ucla.edu/˜yang.lu/ Project/generativeCNN/main.html  5 CONCLUSION  Given the recent successes of CNNs, it is worthwhile to explore their generative aspects. In this work, we show that a simple generative model can be constructed based on the CNN. The generative model helps to pre-train the CNN. It also helps to visualize the knowledge of the learned CNN. The proposed visualizing scheme can sample from the generative model, and it may be turned into a parametric generative learning algorithm, where the generative gradient can be approximated by samples generated by the current model.  ACKNOWLEDGEMENT  The work is supported by NSF DMS 1310391, ONR MURI N00014-10-1-0933, DARPA MSEE FA8650-11-1-7149.  REFERENCES Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248–255. IEEE, 2009.  Erhan, Dumitru, Bengio, Yoshua, Courville, Aaron, and Vincent, Pascal. Visualizing higher-layer  features of a deep network. Dept. IRO, Universit´e de Montr´eal, Tech. Rep, 2009.  8  Published as a conference paper at ICLR 2015  Erhan, Dumitru, Bengio, Yoshua, Courville, Aaron, Manzagol, Pierre-Antoine, Vincent, Pascal, and Bengio, Samy. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660, 2010.  Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik, Jitendra. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 580–587. IEEE, 2014.  Hinton, Geoffrey. Training products of experts by minimizing contrastive divergence. Neural com-  putation, 14(8):1771–1800, 2002.  Hinton, Geoffrey, Osindero, Simon, and Teh, Yee-Whye. A fast learning algorithm for deep belief  nets. Neural computation, 18(7):1527–1554, 2006a.  Hinton, Geoffrey, Osindero, Simon, Welling, Max, and Teh, Yee-Whye. Unsupervised discovery of nonlinear structure using contrastive backpropagation. Cognitive science, 30(4):725–731, 2006b.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Jordan, A. On discriminative vs. generative classiﬁers: A comparison of logistic regression and  naive bayes. Advances in neural information processing systems, 14:841, 2002.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Le, Quoc V., Monga, Rajat, Devin, Matthieu, Chen, Kai, Corrado, Greg S., Dean, Jeff, and Ng, An- drew Y. Building high-level features using large scale unsupervised learning. In In International Conference on Machine Learning, 2012. 103, 2012.  LeCun, Yann, Boser, Bernhard, Denker, John S, Henderson, Donnie, Howard, Richard E, Hubbard, Wayne, and Jackel, Lawrence D. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  Liang, Percy and Jordan, Michael I. An asymptotic analysis of generative, discriminative, and In Proceedings of the 25th international conference on Machine  pseudolikelihood estimators. learning, pp. 584–591. ACM, 2008.  Long, Jonathan L, Zhang, Ning, and Darrell, Trevor. Do convnets learn correspondence? In Ad-  vances in Neural Information Processing Systems, pp. 1601–1609, 2014.  Neal, Radford M. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2,  2011.  Rifai, Salah, Vincent, Pascal, Muller, Xavier, Glorot, Xavier, and Bengio, Yoshua. Contractive auto- encoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 833–840, 2011.  Roth, Stefan and Black, Michael J. Fields of experts. International Journal of Computer Vision, 82  (2):205–229, 2009.  Salakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltzmann machines. In International Con-  ference on Artiﬁcial Intelligence and Statistics, pp. 448–455, 2009.  Simonyan, Karen, Vedaldi, Andrea, and Zisserman, Andrew. Deep inside convolutional networks: Visualising image classiﬁcation models and saliency maps. Workshop at International Conference on Learning Representations, 2014.  Zeiler, Matthew D and Fergus, Rob. Visualizing and understanding convolutional neural networks.  arXiv preprint arXiv:1311.2901, 2013.  9  Published as a conference paper at ICLR 2015  SUPPLEMENTARY MATERIALS  A. DISCRIMINATIVE VS GENERATIVE LOG-LIKELIHOOD AND GRADIENT FOR BATCH TRAINING During training, on a batch of training examples, {(xi, yi), i = 1, ..., n}, the generative log- likelihood is  ≈(cid:88)  i  (cid:80)  log  exp (fyi(xi; w))  Zyi(w)  exp (fyi(xi; w)) k exp (fyi(xk; w)) /n  .  (cid:88)  i  lG(w) =  log p(xi|yi, w) =  The gradient with respect to w is  (cid:34)  (cid:88)  i  ∂ ∂w  l(cid:48) G(w) =  The discriminative log-likelihood is  The gradient with respect to w is  lD(w) =  (cid:34)  (cid:88)  i  l(cid:48) D(w) =  i  log  (cid:88) fyi(xi; w) −(cid:88) (cid:88) fyi(xi; w) −(cid:88)  log p(yi|xi, w) =  j  i  ∂ ∂w  y  (cid:80)  exp(fyi(xj; w)) k exp(fyi(xk; w))  fyi(xj; w)  ∂ ∂w  (cid:88)  i  log  (cid:80)  exp(fyi (xi; w)) y exp(fy(xi; w))  .  ∂ ∂w  fy(xi; w)  (cid:80)  exp(fy(xi; w)) y exp(fy(xi; w))  (cid:35)  .  (cid:35)  .  D, the summation is over  G, the summation is over example xj while yi is ﬁxed.  G are similar in form but different in the summation operations. In l(cid:48)  D and l(cid:48) l(cid:48) category y while xi is ﬁxed, whereas in l(cid:48) In the generative gradient, we want fyi to assign high score to xi as well as those observations that belong to yi, but assign low scores to those observations that do not belong to yi. This constraint is for the same fyi, regardless of what other fy do for y (cid:54)= yi. In the discriminative gradient, we want fy(xi) to work together for all different y, so that fyi assigns high score to xi than other fy for y (cid:54)= yi. Apparently, the discriminative constraint is weaker because it involves all fy, and the generative constraint is stronger because it involves single fy. After generative learning, these fy are well behaved and then we can continue to reﬁne them (including the intercepts for different y) to satisfy the discriminative constraint.  B. MORE GENERATIVE VISUALIZATION EXAMPLES  More generative visualization examples for the nodes at the ﬁnal fully-connected layer in the fully trained AlexNet model are shown in Fig. B1, Fig. B2 and Fig. B3.  10  Published as a conference paper at ICLR 2015  (a) Boat  (b) Peacock  (c) Panda  (d) Orange  Figure B1: More samples from the nodes at the ﬁnal fully-connected layer (fc8) in the fully trained AlexNet model, which correspond to different object categories (part 1).  11  Published as a conference paper at ICLR 2015  (a) Lotion bottle  (b) Hook  (c) Lawn mower  (d) Hourglass  Figure B2: More samples from the nodes at the ﬁnal fully-connected layer (fc8) in the fully trained AlexNet model, which correspond to different object categories (part 2).  12  Published as a conference paper at ICLR 2015  (a) Knot  (b) Nail  (c) Academic gown  Figure B3: More samples from the nodes at the ﬁnal fully-connected layer (fc8) in the fully trained AlexNet model, which correspond to different object categories (part 3).  (d) Goose  13  ",
1412.7489,2015,A Unified Perspective on Multi-Domain and Multi-Task Learning,"['A Unified Perspective on Multi-Domain and Multi-Task Learning', 'Yongxin Yang and Timothy Hospedales']",https://arxiv.org/pdf/1412.7489,"5 1 0 2    r a     M 6 2      ] L M  . t a t s [      3 v 9 8 4 7  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  A UNIFIED PERSPECTIVE ON MULTI-DOMAIN AND MULTI-TASK LEARNING  Yongxin Yang & Timothy M. Hospedales Electronic Engineering and Computer Science Queen Mary, University of London {yongxin.yang, t.hospedales}@qmul.ac.uk  ABSTRACT  In this paper, we provide a new neural-network based perspective on multi-task learning (MTL) and multi-domain learning (MDL). By introducing the concept of a semantic descriptor, this framework uniﬁes MDL and MTL as well as encom- passing various classic and recent MTL/MDL algorithms by interpreting them as different ways of constructing semantic descriptors. Our interpretation pro- vides an alternative pipeline for zero-shot learning (ZSL), where a model for a novel class can be constructed without training data. Moreover, it leads to a new and practically relevant problem setting of zero-shot domain adaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model for an unseen do- main can be generated by its semantic descriptor. Experiments across this range of problems demonstrate that our framework outperforms a variety of alternatives.  1  INTRODUCTION  Multi-task and multi-domain learning are established strategies to improve learning by sharing knowledge across different but related tasks or domains. Multi-domain learning refers to sharing information about the same problem across different contextual domains, while multi-task learn- ing addresses sharing information about different problems in the same domain. Because the do- main/task distinction is sometimes subtle, and some methods proposed for MTL can also address MDL and vice-versa, the two settings are sometimes loosely used interchangeably. However, it is useful to distinguish them clearly: Domain relates to some covariate, such as the bias implicitly captured in a particular dataset (Torralba & Efros, 2011), or the speciﬁc data capture device. For example the Ofﬁce Dataset (Saenko et al., 2010) contains three domains related to image source: Amazon, webcam, and DSLR. A multi-domain learning problem can then be posed by training a particular object recogniser across these three domains (same task, different domains). In contrast, a multi-task problem would be to share information across the recognisers for individual object cat- egories (same domain, different tasks). The issue of simultaneously addressing multiple tasks and multiple domains seems to be un-addressed in the literature to our knowledge. In this paper, we propose a neural network framework that addresses both multi-domain and multi- task learning, and can perform simultaneous multi-domain multi-task learning. A key concept in our framework is the idea of a multivariate “semantic descriptor” for tasks and domains. Such a descriptor is often available as metadata, and can be exploited to improve information sharing for MTL and MDL. We show that various classic and recent MTL/MDL methods are special cases of our framework that make particular assumptions about this descriptor: Existing algorithms typically implicitly assume categorical domains/tasks, which is less effective for information sharing when more detailed task/domain metadata is available. For example, the classic “school dataset” poses a task of predicting students’ grades, and is typically interpreted as containing a domain correspond- ing to each school. However, since each school has three year groups, representing domains by a semantic descriptor tuple (school-id, year-group) is better for information sharing. Our framework exploits such multi-variate semantic descriptors effectively, while existing MTL/MDL algorithms would struggle to do so, as they implicitly consider tasks/domains to be atomic. Going beyond information sharing for known tasks, an exciting related paradigm for task-transfer is “zero-shot” learning (ZSL) (Larochelle et al., 2008; Lampert et al., 2009; Fu et al., 2014). This  1  Published as a conference paper at ICLR 2015  setting addresses automatically constructing a test-time classiﬁer for categories which are unseen at training time. Our neural-network framework provides an alternative pipeline for ZSL. More inter- estingly, it leads to the novel problem setting of zero-shot domain adaptation (ZSDA): Synthesising a model appropriate for a new unseen domain given only its semantic descriptor. For example, suppose we have an audio recogniser trained for a variety of acoustic playback environments, and for a variety of microphone types: Can we synthesise a recogniser for an arbitrary environment- microphone combination? To our knowledge, this is the ﬁrst time that zero-shot domain adaptation has been addressed speciﬁcally.  2 RELATED WORK  2.1 MULTI-TASK LEARNING  Multi-Task Learning (MTL) aims to jointly learn a set of tasks by discovering and exploiting task similarity. Various assumptions have been made to achieve this. An early study (Evgeniou & Pontil, 2004) assumed a linear model for ith task can be written as wi := w0 + vi where w0 can be considered as the shared knowledge which beneﬁts all tasks and vi is the task-speciﬁc knowledge. Another common assumption of MTL is that the predictors (task parameters) lie in a low dimen- sional subspace (Argyriou et al., 2008). Imposing the (2,1)-norm on the predictor matrix W , where each column is a task, results in a low-rank W , which implicitly encourages parameter sharing. However, this assumes that all tasks are related, which is likely violated in practice. Forcing predic- tors to be shared across unrelated tasks can signiﬁcantly degrade the performance – a phenomenon called negative transfer (Rosenstein et al., 2005). A task grouping framework is thus proposed by Kang et al. (2011) that partitions all tasks into disjoint groups where each group shares a low dimen- sional structure. This partially alleviates the unrelated task problem, but misses any fundamental information shared by all tasks, as there is no overlap between the subspaces of each group. As a middle ground, the GO-MTL algorithm (Kumar & Daum´e III, 2012) allows information to be shared between different groups, by representing the model of each task as a linear combination of latent predictors. Thus the concept of grouping is no longer explicit, but determined by the coefﬁcients of the linear combination. Intuitively, model construction can be thought of as: W = LS where L is the matrix of which each column is a latent predictor (shared knowledge), and S = [s1, s2, . . . , sM ] where si is a coefﬁcient vector that cues how to construct the model for the ith task (task-speciﬁc knowledge). It is worth noting that this kind of predictor matrix factorisation approach – W = LS – can explain several models: Kumar & Daum´e III (2012) is L1/L2 regularised decomposition, Passos et al. (2012) is linear Gaussian model with IBP prior and an earlier study (Xue et al., 2007) assumes si are unit vectors generated by a Dirichlet Process (DP). Most MTL methods in literature assume that each task is an atomic entity indexed by a single categorical variable. Some recent studies (Romera-paredes et al., 2013; Kishan Wimalawarne & Tomioka, 2014) noticed a drawback – this strategy can not represent a task with more structured metadata, e.g., (school-id, year-group) for school dataset. Thus they replace predictor matrix W with a tensor so that the linear models across more than one categorical variable can be placed in the tensor. Then they follow the line of Argyriou et al. (2008) to impose a variety of regularisations on the mentioned tensor, such as sum of the ranks of the matriciations of the tensors (Romera-paredes et al., 2013) and scaled latent trace norm (Kishan Wimalawarne & Tomioka, 2014). However, this again suffers from the strong assumption that all tasks are related.  2.2 MULTI-DOMAIN LEARNING  Domain Adaptation There has been extensive work on domain adaptation (DA) (Beijbom, 2012). A variety of studies have proposed both supervised (Saenko et al., 2010; Duan et al., 2012) and unsupervised (Gong et al., 2012; Sun & Saenko, 2014) methods. As we have mentioned, the typ- ical assumption is that domains are indexed by a single categorical variable: For example a data source such as Amazon/DSLR/Webcam (Saenko et al., 2010), a benchmark dataset such as PAS- CAL/ImageNet/Caltech (Gong et al., 2012), or a modality such as image/video (Duan et al., 2012). Despite the majority of research with the categorical assumption on domains, it has recently been generalised by studies considering domains with a (single) continuous parameter such as time (Hoff-  2  Published as a conference paper at ICLR 2015  man et al., 2014) or viewing angle (Qiu et al., 2012). In this paper, we take an alternative approach to generalising the conventional categorical formulation of domains, and instead investigate infor- mation sharing with domains described by a vector of discrete parameters.  Multi-Domain Learning Multi-Domain Learning (MDL) (Dredze et al., 2010; Joshi et al., 2012) shares properties of both domain adaptation and multi-task learning. In conventional domain adap- tation, there is an explicit pair of source and target domain, and the knowledge transfer is one way Source→Target. In contrast, MDL encourages knowledge sharing in both directions. Although some existing MTL algorithms reviewed in previous section tackle MDL as well, we distinguish them by the key difference during testing time: MDL makes prediction for same problem (binary classiﬁcation like “is laptop”) across multiple domains (e.g., datasets or camera type), but MTL handles different problems (such as “is laptop” versus “is mouse”).  2.3 ZERO-SHOT LEARNING  Zero-Shot Learning (ZSL) aims to eliminate the need for training data for a particular task. It has been widely studied in different areas, such as character (Larochelle et al., 2008) and object recognition (Lampert et al., 2009; Socher et al., 2013; Fu et al., 2014). Typically for ZSL, the label space of training and test data are disjoint, so no data has been seen for test-time categories. Instead, test-time classiﬁers are constructed given some mid-level information. Although diverse in other ways, most existing ZSL methods follow the pipeline in Palatucci et al. (2009): X → Z → Y where Z is some “semantic descriptor”, which refers to attributes (Lampert et al., 2009) or semantic word vectors (Socher et al., 2013). Our work can be considered as an alternative pipeline, which is more similar to Larochelle et al. (2008) and Frome et al. (2013) in the light of the following illustration: Z Going beyond conventional ZSL, we generalise the notion of zero-shot learning of tasks to zero-shot learning of domains. In this context, zero-shot means no training data has been seen for the target domain prior to testing. The challenge is to construct a good model for a novel test domain based solely on its semantic descriptor. The closest work to our zero-shot domain adaptation setting is Ding et al. (2014), which addresses the issue of a missing modality with the help of the partially overlapped modalities that have been previously seen. However they use a single ﬁxed modality pair, rather than synergistically exploiting an arbitrary number of auxiliary domains in a multi- domain way as in our framework. Note that despite the title, Blitzer et al. (2009) actually considers unsupervised domain adaptation without target domain labels, but with target data.  (cid:47) Y .  X  3 MODEL  3.1 GENERAL FRAMEWORK  j , z(i)}j=1,2,··· ,Ni}i=1,2,··· ,M and the corresponding label as {{y(i)  Assume that we have M domains (tasks), and the ith domain has Ni instances. We denote the feature vector of the jth instance in the ith domain (task) and its associated semantic descriptor by the pair {{x(i) j }j=1,2,··· ,Ni}i=1,2,··· ,M . Note that, in multi-domain or multi-task learning, all the instances are effectively associated with a semantic descriptor indicating their domain (task). Without loss of generality, we propose an objective function that minimises the empirical risk for all domains (tasks),  (cid:18) 1  M(cid:88)  Ni(cid:88)  L(cid:16)  i=1  Ni  j=1  arg min  P,Q  1 M  (cid:17)(cid:19)  ˆy(i) j  , y(i) j  , where  ˆy(i) j = fP (x(i)  j ) · gQ(z(i))  (1)  This model can be understood as a two-sided neural network illustrated by Figure 1. One can see it contains two learning processes: the left-hand side is representation learning fP (·), starting with the original feature vector x; and the right-hand side is model construction gQ(·), starting with an associated semantic descriptor z. P and Q are the weights to train for each side. To train P and Q, standard back propagation can be performed by the loss L(·) calculated between ground truth y and the prediction ˆy.  3  (cid:43) (cid:43) (cid:47) Published as a conference paper at ICLR 2015  y  . . .  . . .  P  x  . . .  . . .  Q  z  Figure 1: Two-sided Neural Network for Multi-Task/Multi-Domain Learning  With this neural network interpretation, the two sides can be arbitrarily complex but we ﬁnd that one inner product layer for each is sufﬁcient to unify some existing MDL/MTL algorithms and demonstrate the efﬁcacy of the approach. In this case, P is a D-by-K matrix and Q is a B-by-K matrix, where K is the number of units in the middle layer; D and B is the length of feature vector x and semantic descriptor z respectively. The prediction is then based on (x(i)  j P )(z(i)Q)(cid:48).  3.2 UNIFICATION OF EXISTING ALGORITHMS  We next demonstrate how a variety of existing algorithms1 are special cases of our general frame- work. For clarity we show this in an MDL/MTL setting with M = 3 domains/tasks. Observe that RMTL (Evgeniou & Pontil, 2004), FEDA2 (Daum´e III, 2007), MTFL (Argyriou et al., 2008) and GO-MTL (Kumar & Daum´e III, 2012) each assume speciﬁc settings of Z, P and Q(cid:48) as in Table 1.  Table 1: A Unifying Review of Some Existing MTL/MDL Algorithms  (cid:35) (cid:35)  0 0  (cid:34)1 (cid:34)1 (cid:34)1 (cid:34)1  0 0  0 0  0 0  Z  0 1 0  0 1 0  0 0 1  0 0 1  0 1 0 0 1 0  1 1 1  1 1 1  (cid:35) (cid:35)  0 0 1 0 0 1  RMTL  FEDA2  MTFL  GO-MTL  P  Norm on P  Identity  None  a ⊗ b  None  Identity  None  L  Frobenius  (cid:34) |  0  Q(cid:48)  v1 |  | v2 | 0 w1 0 0 w2 0  | | v3 w0 | | 0 w0 0 0 0 0 0 w3 0  (cid:35)   W  S  Norm on Q(cid:48)  None  None  (2, 1)-Norm  Entry-wise (cid:96)1  The notion used is kept same with the original paper, e.g., P here is analogous to L in Kumar & Daum´e III (2012). Each row of the matrices in the second (Z) column is the corresponding domain’s semantic descriptor in different methods. These methods are implicitly assuming a single categorical domain/task index: with 1-of-N encoding as semantic descriptor (sometimes with a constant term). We argue that more structured domain/task-metadata is often available, and with our framework it can be directly exploited to improve information sharing compared to simple categorical indices. For example, suppose two categorical variables (A,B) describe a domain, and each of them has two states (1,2), then four distinct domains can be encoded by Z in a distributed fashion (Table 2 left) in contrast to the 1-of-N form used by traditional multi-task learning methods (Table 2 right). The ability to exploit more structured domain/task descriptors Z where available, improves information sharing compared to existing MTL/MDL methods. In our experiments, we will demonstrate ex- amples of problems with multivariate domain/task metadata, and its efﬁcacy to improve learning.  1RMTL: Regularized Multi–Task Learning, FEDA: Frustratingly Easy Domain Adaptation, MTFL: Multi–  Task Feature Learning and GO-MTL: Grouping and Overlap for Multi–Task Learning matrix, and ⊗ denotes Kronecker product. w0, w1, w2, w3 and 0 in Q(cid:48) are D-dimensional column vectors.  2a is an (M+1)-dimensional row vector with all ones, e.g., a = [1, 1, 1, 1] when M=3, b is a D-by-D identity  4  Published as a conference paper at ICLR 2015    Domain-1 Domain-2 Domain-3 Domain-4  Table 2: Illustration for Distributed Coding and 1-of-N Coding A-1 A-2 B-1 B-2 0 1 1 1 0 0 0 1  Domain-1 Domain-2 Domain-3 Domain-4  0 0 1 1  1 0 1 0  1 0 0 0  0 1 0 0  0 0 1 0  A-1-B-1 A-1-B-2 A-2-B-1 A-2-B-2    0 0 0 1      3.3 LEARNING SETTINGS  Multi-domain multi-task (MDMT) Existing frameworks have focused on either MDL or MTL settings but not considered both together. Our interpretation provides a simple means to exploit them both simultaneously for better information sharing when multiple tasks in multiple domains are available. If z(d) and z(t) are the domain and task descriptors respectively, then MDMT learning can be performed by simply concatenating the descriptors [z(d), z(t)] corresponding to the domain and task of each individual instance during learning. Zero-shot learning (ZSL) As mentioned, the dominant zero-shot (task) learning pipeline is X → Z → Y . At train time, the X → Z mapping is learned by classiﬁer/regressor, where Z is a task descriptor, such as a binary attribute vector (Lampert et al., 2009; Fu et al., 2014), or a continuous word-vector describing the task name (Socher et al., 2013; Fu et al., 2014). At testing time, the “prototype” semantic vector for a novel class z is presented, and zero-shot recognition is performed by matching the X → Z estimate and prototype z, e.g., by nearest neighbour (Fu et al., 2014). In our framework, ZSL is achieved by presenting each novel semantic vector z∗ j (each testing cate- gory is indexed by j) in turn along with novel category instances x∗. Zero-shot recognition then is given by: j∗ = arg maxj fP (x∗) · fQ(z∗ j ). Zero-shot domain adaptation (ZSDA) The zero-shot domain adaptation task can also be ad- dressed by our framework. With a distributed rather than 1-of-N encoded domain descriptor, only a subset of domains is necessary to effectively learn Q. Thus a model suitable for data from a novel held-out domain can be constructed by applying its semantic descriptor z∗ along with data x∗.  4 EXPERIMENTS  log(D) leads to satisfactory solutions for all datasets.  We demonstrate our framework on ﬁve experimental settings: MDL, ZSDA, MTL, ZSL and MDMT. Implementation: We implement the model with the help of Caffe framework (Jia et al., 2014). Though we don’t place regularisation terms on P or Q, a non-linear function σ(x) = max(0, x) (i.e., ReLU activation function) is placed to encourage sparse models gQ(z(i)) = σ(z(i)Q). The choice of loss function for regression and classiﬁcation is Euclidean loss and Hinge loss respectively. Preliminary experiments show K = D MTL/MDL Baselines: We compare the proposed method with a single task learning baseline – linear or logistic regression with (cid:96)2 regularisation (LR), and four multi-task learning methods: (i) RMTL (Evgeniou & Pontil, 2004), (ii) FEDA (Daum´e III, 2007), (iii) MTFL (Argyriou et al., 2008) and (iv) GO-MTL (Kumar & Daum´e III, 2012). Note that these methods are re-implemented within the proposed framework. We have veriﬁed our implementations with the original ones and found that the performance difference is not signiﬁcant. Baseline methods use traditional 1-of-N encoding, while we use a distributed descriptor encoding based on metadata for each problem. Zero-Shot Domain Adaptation: We follow the MDL setting to learn P and Q except that one domain is held out each time. We construct test-time models for held out domains using their se- mantic descriptor. We evaluate against two baselines: (i) Blind-transfer (LR): learning a single lin- ear/logistic regression model on aggregated data from all seen domains. To ensure fair comparison, distributed semantic descriptors are concatenated with the feature vectors for baselines, i.e., they are included as a plain feature. (ii) Tensor-completion (TC): we use a tensor W ∈ RD,p1,p2,··· ,pN to store all the linear models trained by SVM where N is the number of categorical variables and pi is the number of states in the ith categorical variable (p1 + p2 + ··· + pN = B in our context and p1 ∗ p2 ∗ ··· ∗ pN = M if there is always a domain for each of possible combinations). ZSDA can be formalised by setting the model parameters for the held-out domain to missing values, and recovering them by a low-rank tensor completion algorithm (Kressner et al., 2014). This low-rank strategy corresponds to our implementation of Romera-paredes et al. (2013).  5  Published as a conference paper at ICLR 2015  4.1 SCHOOL DATASET - MDL AND ZSDA  Data This classic dataset3 collects exam grades of 15,362 students from 139 schools. Given the 23 features4, a regression problem is to predict each student’s exam grade. There are 139 schools and three year groups. School IDs and year groups naturally form multivariate domains. Note that 64 of 139 schools have the data of students for all three year groups, and we also choose the school of which each year group has more than 50 students so that each domain has sufﬁcient training data. Finally there are 23 × 3 = 69 distinct domains given these two categorical variables. Settings and Results For MDL we learn all domains together, and for ZSDA we use a leave-one- domain-out strategy, constructing the test-time model based on the held-out domain’s descriptor with P and Q learned from the training domains. In each case the training/test split is 50%/50%. Note that the test sets for MDL and ZSDA are the same. The results in Table 3 are averages over the test set for all domains (MDL), and averages over the held-out domain performance when holding out each of the 69 domains in turn (ZSDA). Our method outperforms the alternatives in each case.  Table 3: School Dataset (RMSE)  LR RMTL FEDA MTFL GO-MTL 10.00 MDL 9.51 ZSDA 10.35 -  10.75 -  10.22 -  9.46 -  TC - 12.41  Ours 9.37 10.19  4.2 AUDIO RECOGNITION - MDL AND ZSDA  Audio analysis tasks are affected by a variety of covariates, notably the playback device / envi- ronment (e.g., studio recording versus live concert hall), and the listening device (e.g., smartphone versus professional microphone). Directly applying a model trained in one condition/domain to an- other will result in poor performance. Moreover, as the covariates/domains are combinatorial: (i) models cannot be trained for all situations, and (ii) even applying conventional domain adaptation is not scalable. Zero-shot domain adaptation has potential to address this, because a model could be calibrated on the ﬂy for a given environment. Data We investigate recognition in a complex set of noise domains: covering both acoustic en- vironment and microphone type. We consider a music-speech discrimination task introduced by Tzanetakis & Cook (2002), which includes 64 music and speech tracks. Two categorical variables are smartphone microphone and live concert hall environment, and each of them has two states: on or off. Then the four domains are generated as: (i) Original (ii) Live Recording (LR) (iii) Smart- phone Recording (SR) and (iv) smartphone in a live hall (LRSR). The noises are synthesised by Audio Degradation Toolbox (Mauch & Ewert, 2013). Settings and Results We use MFCC to extract audio features and K-means to build a K = 64 bag-of-words representation. We split the data 50%/50% for training and test and keep test sets same for MDL and ZSDA. The results in Table 4 break down the results by each domain and overall (MDL), and each domain when held-out (ZSDA). In each case our method is best or joint-best due to better exploiting the semantic descriptor (recall that it does not have any additional information; for fairness the descriptor is also given to the other methods as a regular feature). The only exception is the least practical case of ZSDA recognition in a noise free environment given prior training only in noisy environments. The ZSDA result here generally demonstrates that models can be synthe- sised to deal effectively with new multivariate domains / covariate combinations without needing to exhaustively see data and explicitly train models for all, as would be conventionally required. 4.3 ANIMAL WITH ATTRIBUTES - MTL AND ZSL  Animal with Attributes (Lampert et al., 2009) includes images from 50 animal categories, each with an 85-dimensional binary attribute vector. The attributes, such as “black”, “furry”, “stripes”, describe an animal semantically, and provide a unique mapping from a combination of attributes to an animal. The original setting of ZSL with AwA is to split the 50 animals into 40 for training  3Available at http://multilevel.ioe.ac.uk/intro/datasets.html 4The original dataset has 26 features, but 3 that indicate student year group are used in semantic descriptors.  6  Published as a conference paper at ICLR 2015  Table 4: Audio Recognition: Music versus Speech (Error Rate)  L D M  LR RMTL FEDA MTFL GO-MTL Ours  A LR TC D S Ours Z  Origin 3.13 6.25 7.81 6.25 3.13 3.13 32.81 46.88 35.94  LR 18.75 18.75 18.75 21.88 17.19 17.19 28.13 21.88 9.38  SR LRSR 17.19 6.25 6.25 17.19 18.75 9.38 14.06 9.38 18.75 6.25 4.69 14.06 23.44 14.06 59.38 26.56 12.50 18.75  Avg 11.33 12.11 13.67 12.89 11.33 9.77 24.61 38.67 19.14  and hold out 10 for testing. We evaluate this condition to investigate: (i) if multi-task learning of attributes and classes improves over the STL approaches typically taken when analysing AwA, (ii) if it helps to use the attributes as an MTL semantic task descriptor against the traditional setting of MTL where semantic descriptor is a 1-of-N unit vector indexing tasks. For MTL training on AwA, we decompose the multi-class problem with C categories to C one-vs-rest binary classiﬁcation tasks. Note that in this case the semantic descriptor reveals the label, so it is not given during testing. We run all one-vs-rest classiﬁers on each instance and rank the scores to produce the label. Multi-Task Learning We use the recently released DeCAF feature (Donahue et al., 2015) for AwA. For MTL, we pick ﬁve animals from the training set with moderately overlapped attributes, and use the ﬁrst half of the images for training then test on the rest. The results in Table 5 show limited improvement by existing MTL approaches over the standard STL. However, our attribute- descriptor approach to encoding tasks for MTL improves the accuracy by about 2% over STL.  Table 5: AwA: MTL Multi-Class Accuracy  LR RMTL FEDA MTFL GO-MTL Ours  antelope 92.31 86.08 92.31 92.67 91.21 93.41  killer whale 87.08 71.22 83.39 85.61 84.87 91.51  otter walrus 89.26 75.60 61.90 80.99 79.17 88.15 79.76 90.36 80.36 89.81 94.21 79.76  blue whale 82.44 96.18 89.31 87.02 84.73 79.39  Avg 85.34 79.28 86.47 87.09 86.20 87.66  Zero-Shot Learning For ZSL, we adopt the training/testing split in Lampert et al. (2009). The blind-transfer baseline is not meaningful because there are different binary classiﬁcation problems, and aggregating does not lead to anything. Also, tensor-completion is not practical because of its exponential space (D ∗ 285) against D ∗ 40 observations. Our method achieves 43.79% multi-class accuracy, compared to 41.03% from direct-attribute prediction (DAP) approach (Lampert et al., 2009) using DeCAF features. A recent result using DeCAF feature is 44.20% in Deng et al. (2014), but this uses additional higher order attribute correlation information. Given that we did not design a solution for AwA speciﬁcally, or exploit this higher order correlation cue, the result is encouraging.  4.4 RESTAURANT & CONSUMER DATASET - MDMT  The restaurant & Consumer Dataset, introduced by Vargas-Govea et al. (2011) contains 1161 customer-to-restaurant scoring records, where each record has 43 features and three scores: food, service and overall. We build a multi-domain multi-task problem as follows: (i) a domain refers to a restaurant, (ii) a task is a regression problem to predict one of the three scores given an instance and (iii) an instance is a 43-dimensional feature vector based on customer’s and restaurant’s proﬁle. The 1161 records cover 130 restaurants but most of them just have few scores, so we just pick 8 most frequently scored ones, and we split training and test sets equally. The semantic descriptor is constructed by concatenating 8-bit domain and 3-bit task indicator. Conventional MTL interpre- tations of this dataset consider 8 × 3 = 24 atomic tasks. Thus the task overlap across domain or  7  Published as a conference paper at ICLR 2015  domain overlap across task is ignored. Results in Table 6 shows that our approach outperforms this traditional MTL setting by better representing it as a distributed MDMT problem.  Table 6: Restaurant & Consumer Dataset (RMSE) LR RMTL FEDA MTFL GO-MTL Ours 0.78 2.32  1.17  1.06  1.23  1.13  5 CONCLUSION  In this paper we proposed a uniﬁed framework for multi-domain and multi-task learning. The core concept is a semantic descriptor for tasks or domains. This can be used to unify and improve on a variety of existing multi-task learning algorithms. Moreover it naturally extends the use of a single categorical variable to index domains/tasks to the multivariate case, which enables better information sharing where additional metadata is available. Beyond multi task/domain learning, it enables the novel task of zero-shot domain adaptation and provides an alternative pipeline for zero-shot learning. Neural networks have also been used to address MTL/MDL by learning shared invariant features (Donahue et al., 2015). Our contribution is complementary to this (as demonstrated e.g., with AwA) and the approaches are straightforward to combine by placing more complex structure on left-hand side fP (·). Our future directions are: (i) The current semantic descriptor is formed by discrete variables. We want to extend this to continuous and periodic variable like the pose, brightness and time. (ii) We assume the semantic descriptor (task/domain) is always observed, an improvement for dealing with a missing descriptor is also of interest. Acknowledgements We gratefully acknowledge the support of NVIDIA Corporation for the dona- tion of the GPUs used for this research.  REFERENCES Argyriou, A., Evgeniou, T., and Pontil, M. Convex multi-task feature learning. Mach. Learn., 73  (3):243–272, December 2008.  Beijbom, O. Domain adaptations for computer vision applications. Technical report, UCSD, 2012.  Blitzer, J., Foster, D. P., and Kakade, S. M. Zero-shot domain adaptation: A multi-view approach.  Technical report, 2009.  Daum´e III, H. Frustratingly easy domain adaptation. In ACL, 2007.  Deng, J., Ding, N., Jia, Y., Frome, A., Murphy, K., Bengio, S., Li, Y., Neven, H., and Adam, H.  Large-scale object classiﬁcation using label relation graphs. In ECCV, pp. 48–64, 2014.  Ding, Z., Ming, S., and Fu, Y. Latent low-rank transfer subspace learning for missing modality  recognition. In AAAI, pp. 1192–1198, 2014.  Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep  convolutional activation feature for generic visual recognition. In ICML, 2015.  Dredze, M., Kulesza, A., and Crammer, K. Multi-domain learning by conﬁdence-weighted param-  eter combination. Machine Learning, 79(1-2):123–149, 2010.  Duan, L., Xu, D., and Chang, S.-F. Exploiting web images for event recognition in consumer videos:  A multiple source domain adaptation approach. In CVPR, 2012.  Evgeniou, T. and Pontil, M. Regularized multi–task learning. In KDD, 2004.  Frome, A., Corrado, G., Shlens, J., Bengio, S., Dean, J., Ranzato, M., and Mikolov, T. Devise: A  deep visual-semantic embedding model. In NIPS, 2013.  Fu, Y., Hospedales, T., Xiang, T., Fu, Z., and Gong, S. Transductive multi-view embedding for  zero-shot recognition and annotation. In ECCV, 2014.  8  Published as a conference paper at ICLR 2015  Gong, B., Shi, Y., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adapta-  tion. In CVPR, 2012.  Hoffman, J., Darrell, T., and Saenko, K. Continuous manifold based adaptation for evolving visual  domains. In CVPR, 2014.  Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., and Darrell, T. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.  Joshi, M., Dredze, M., Cohen, W. W., and Ros´e, C. P. Multi-domain learning: When do domains  matter? In EMNLP, 2012.  Kang, Z., Grauman, K., and Sha, F. Learning with whom to share in multi-task feature learning. In  ICML, 2011. ISBN 978-1-4503-0619-5.  Kishan Wimalawarne, M. S. and Tomioka, R. Multitask learning meets tensor factorization: task  imputation via convex optimization. In NIPS, 2014.  Kressner, D., Steinlechner, M., and Vandereycken, B. Low-rank tensor completion by riemannian  optimization. Technical Report 2, 2014.  Kumar, A. and Daum´e III, H. Learning task grouping and overlap in multi-task learning. In ICML,  2012.  Lampert, C. H., Nickisch, H., and Harmeling, S. Learning to detect unseen object classes by  between-class attribute transfer. In CVPR, 2009.  Larochelle, H., Erhan, D., and Bengio, Y. Zero-data learning of new tasks. In AAAI, 2008. Mauch, M. and Ewert, S. The audio degradation toolbox and its application to robustness evaluation.  In ISMIR, 2013.  Palatucci, M., Pomerleau, D., Hinton, G., and Mitchell, T. Zero-shot learning with semantic output  codes. In Neural Information Processing Systems (NIPS), 2009.  Passos, A., Rai, P., Wainer, J., and Daum´e III, H. Flexible modeling of latent task structures in  multitask learning. In ICML, 2012.  Qiu, Q., Patel, V. M., Turaga, P., and Chellappa, R. Domain adaptive dictionary learning. In ECCV,  2012. ISBN 978-3-642-33764-2.  Romera-paredes, B., Aung, H., Bianchi-berthouze, N., and Pontil, M. Multilinear multitask learning.  In ICML, 2013.  Rosenstein, M. T., Marx, Z., Kaelbling, L. P., and Dietterich, T. G. To transfer or not to transfer. In  In NIPS05 Workshop, Inductive Transfer: 10 Years Later, 2005.  Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains.  In ECCV, 2010.  Socher, R., Ganjoo, M., Manning, C. D., and Ng, A. Y. Zero-shot learning through cross-modal  transfer. In NIPS, pp. 935–943, 2013.  Sun, B. and Saenko, K. From virtual to reality: Fast adaptation of virtual object detectors to real  domains. In BMVC, 2014.  Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR, 2011. Tzanetakis, G. and Cook, P. Musical genre classiﬁcation of audio signals. IEEE Transactions on  Speech and Audio Processing, 10(5):293–302, July 2002.  Vargas-Govea, B., Gonz´alez-Serna, G., and Ponce-Medellın, R. Effects of relevant contextual fea-  tures in the performance of a restaurant recommender system. ACM RecSys, 11, 2011.  Xue, Y., Liao, X., Carin, L., and Krishnapuram, B. Multi-task learning for classiﬁcation with  dirichlet process priors. Journal of Machine Learning Research, 8:35–63, 2007.  9  ",
1412.6856,2015,Object detectors emerge in Deep Scene CNNs,"['Object detectors emerge in Deep Scene CNNs', 'Bolei Zhou', 'Aditya Khosla', 'Agata Lapedriza', 'Aude Oliva', 'and Antonio Torralba']",https://arxiv.org/pdf/1412.6856,"5 1 0 2    r p A 5 1         ]  V C . s c [      2 v 6 5 8 6  .  2 1 4 1 : v i X r a  Published as a conference paper at ICLR 2015  OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS  Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba Computer Science and Artiﬁcial Intelligence Laboratory, MIT {bolei,khosla,agata,oliva,torralba}@mit.edu  ABSTRACT  With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classiﬁcation. As scenes are composed of objects, the CNN for scene classiﬁca- tion automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.  1  INTRODUCTION  Current deep neural networks achieve remarkable performance at a number of vision tasks surpass- ing techniques based on hand-crafted features. However, while the structure of the representation in hand-crafted features is often clear and interpretable, in the case of deep networks it remains unclear what the nature of the learned representation is and why it works so well. A convolutional neural network (CNN) trained on ImageNet (Deng et al., 2009) signiﬁcantly outperforms the best hand crafted features on the ImageNet challenge (Russakovsky et al., 2014). But more surprisingly, the same network, when used as a generic feature extractor, is also very successful at other tasks like object detection on the PASCAL VOC dataset (Everingham et al., 2010). A number of works have focused on understanding the representation learned by CNNs. The work by Zeiler & Fergus (2014) introduces a procedure to visualize what activates each unit. Recently Yosinski et al. (2014) use transfer learning to measure how generic/speciﬁc the learned features are. In Agrawal et al. (2014) and Szegedy et al. (2013), they suggest that the CNN for ImageNet learns a distributed code for objects. They all use ImageNet, an object-centric dataset, as a training set. When training a CNN to distinguish different object classes, it is unclear what the underlying repre- sentation should be. Objects have often been described using part-based representations where parts can be shared across objects, forming a distributed code. However, what those parts should be is unclear. For instance, one would think that the meaningful parts of a face are the mouth, the two eyes, and the nose. However, those are simply functional parts, with words associated with them; the object parts that are important for visual recognition might be different from these semantic parts, making it difﬁcult to evaluate how efﬁcient a representation is. In fact, the strong internal conﬁgu- ration of objects makes the deﬁnition of what is a useful part poorly constrained: an algorithm can ﬁnd different and arbitrary part conﬁgurations, all giving similar recognition performance. Learning to classify scenes (i.e., classifying an image as being an ofﬁce, a restaurant, a street, etc) using the Places dataset (Zhou et al., 2014) gives the opportunity to study the internal representation learned by a CNN on a task other than object recognition. In the case of scenes, the representation is clearer. Scene categories are deﬁned by the objects they contain and, to some extent, by the spatial conﬁguration of those objects. For instance, the important parts of a bedroom are the bed, a side table, a lamp, a cabinet, as well as the walls, ﬂoor and ceiling. Objects represent therefore a distributed code for scenes (i.e., object classes are shared across different scene categories). Importantly, in scenes, the spatial conﬁguration of objects,  1  Published as a conference paper at ICLR 2015  Table 1: The parameters of the network architecture used for ImageNet-CNN and Places-CNN. fc7 Layer Units 4096 Feature  conv2 256 27×27  conv3 384 13×13  conv5 256 13×13  conv1 55×55  pool1 27×27  pool2 256 13×13  conv4 384 13×13  pool5 256 6×6  fc6 4096  96  96  1  1  although compact, has a much larger degree of freedom. It is this loose spatial dependency that, we believe, makes scene representation different from most object classes (most object classes do not have a loose interaction between parts). In addition to objects, other feature regularities of scene categories allow for other representations to emerge, such as textures (Renninger & Malik, 2004), GIST (Oliva & Torralba, 2006), bag-of-words (Lazebnik et al., 2006), part-based models (Pandey & Lazebnik, 2011), and ObjectBank (Li et al., 2010). While a CNN has enough ﬂexibility to learn any of those representations, if meaningful objects emerge without supervision inside the inner layers of the CNN, there will be little ambiguity as to which type of representation these networks are learning. The main contribution of this paper is to show that object detection emerges inside a CNN trained to recognize scenes, even more than when trained with ImageNet. This is surprising because our results demonstrate that reliable object detectors are found even though, unlike ImageNet, no supervision is provided for objects. Although object discovery with deep neural networks has been shown before in an unsupervised setting (Le, 2013), here we ﬁnd that many more objects can be naturally discovered, in a supervised setting tuned to scene classiﬁcation rather than object classiﬁcation. Importantly, the emergence of object detectors inside the CNN suggests that a single network can support recognition at several levels of abstraction (e.g., edges, texture, objects, and scenes) without needing multiple outputs or a collection of networks. Whereas other works have shown that one can detect objects by applying the network multiple times in different locations (Girshick et al., 2014), or focusing attention (Tang et al., 2014), or by doing segmentation (Grangier et al., 2009; Farabet et al., 2013), here we show that the same network can do both object localization and scene recognition in a single forward-pass. Another set of recent works (Oquab et al., 2014; Bergamo et al., 2014) demonstrate the ability of deep networks trained on object classiﬁcation to do localization without bounding box supervision. However, unlike our work, these require object-level supervision while we only use scenes.  2  IMAGENET-CNN AND PLACES-CNN  Convolutional neural networks have recently obtained astonishing performance on object classiﬁ- cation (Krizhevsky et al., 2012) and scene classiﬁcation (Zhou et al., 2014). The ImageNet-CNN from Jia (2013) is trained on 1.3 million images from 1000 object categories of ImageNet (ILSVRC 2012) and achieves a top-1 accuracy of 57.4%. With the same network architecture, Places-CNN is trained on 2.4 million images from 205 scene categories of Places Database (Zhou et al., 2014), and achieves a top-1 accuracy of 50.0%. The network architecture used for both CNNs, as proposed in (Krizhevsky et al., 2012), is summarized in Table 11. Both networks are trained from scratch using only the speciﬁed dataset. The deep features from Places-CNN tend to perform better on scene-related recognition tasks com- pared to the features from ImageNet-CNN. For example, as compared to the Places-CNN that achieves 50.0% on scene classiﬁcation, the ImageNet-CNN combined with a linear SVM only achieves 40.8% on the same test set2 illustrating the importance of having scene-centric data. To further highlight the difference in representations, we conduct a simple experiment to identify the differences in the type of images preferred at the different layers of each network: we create a set of 200k images with an approximately equal distribution of scene-centric and object-centric images3, and run them through both networks, recording the activations at each layer. For each layer, we obtain the top 100 images that have the largest average activation (sum over all spatial locations for  1We use unit to refer to neurons in the various layers and features to refer to their activations. 2Scene recognition demo of Places-CNN is available at http://places.csail.mit.edu/demo. html. The demo has 77.3% top-5 recognition rate in the wild estimated from 968 anonymous user responses. 3100k object-centric images from the test set of ImageNet LSVRC2012 and 108k scene-centric images from  the SUN dataset (Xiao et al., 2014).  2  Published as a conference paper at ICLR 2015  Figure 1: Top 3 images producing the largest activation of units in each layer of ImageNet-CNN (top) and Places-CNN (bottom).  a given layer). Fig. 1 shows the top 3 images for each layer. We observe that the earlier layers such as pool1 and pool2 prefer similar images for both networks while the later layers tend to be more specialized to the speciﬁc task of scene or object categorization. For layer pool2, 55% and 47% of the top-100 images belong to the ImageNet dataset for ImageNet-CNN and Places-CNN. Starting from layer conv4, we observe a signiﬁcant difference in the number of top-100 belonging to each dataset corresponding to each network. For fc7, we observe that 78% and 24% of the top-100 images belong to the ImageNet dataset for the ImageNet-CNN and Places-CNN respectively, illustrating a clear bias in each network. In the following sections, we further investigate the differences between these networks, and focus on better understanding the nature of the representation learned by Places-CNN when doing scene classiﬁcation in order to clarify some part of the secret to their great performance.  3 UNCOVERING THE CNN REPRESENTATION  The performance of scene recognition using Places-CNN is quite impressive given the difﬁculty of the task. In this section, our goal is to understand the nature of the representation that the network is learning.  3.1 SIMPLIFYING THE INPUT IMAGES  Simplifying images is a well known strategy to test human recognition. For example, one can remove information from the image to test if it is diagnostic or not of a particular object or scene (for a review see Biederman (1995)). A similar procedure was also used by Tanaka (1993) to understand the receptive ﬁelds of complex cells in the inferior temporal cortex (IT). Inspired by these approaches, our idea is the following: given an image that is correctly classiﬁed by the network, we want to simplify this image such that it keeps as little visual information as possible while still having a high classiﬁcation score for the same category. This simpliﬁed image (named minimal image representation) will allow us to highlight the elements that lead to the high classiﬁcation score. In order to do this, we manipulate images in the gradient space as typically done in computer graphics (P´erez et al., 2003). We investigate two different approaches described below. In the ﬁrst approach, given an image, we create a segmentation of edges and regions and remove segments from the image iteratively. At each iteration we remove the segment that produces the smallest decrease of the correct classiﬁcation score and we do this until the image is incorrectly classiﬁed. At the end, we get a representation of the original image that contains, approximately, the minimal amount of information needed by the network to correctly recognize the scene category. In Fig. 2 we show some examples of these minimal image representations. Notice that objects seem to contribute important information for the network to recognize the scene. For instance, in the case of bedrooms these minimal image representations usually contain the region of the bed, or in the art gallery category, the regions of the paintings on the walls. Based on the previous results, we hypothesized that for the Places-CNN, some objects were crucial for recognizing scenes. This inspired our second approach: we generate the minimal image repre- sentations using the fully annotated image set of SUN Database (Xiao et al., 2014) (see section 4.1 for details on this dataset) instead of performing automatic segmentation. We follow the same pro- cedure as the ﬁrst approach using the ground-truth object segments provided in the database. This led to some interesting observations: for bedrooms, the minimal representations retained the bed in 87% of the cases. Other objects kept in bedrooms were wall (28%) and window (21%).  3  ImageNet-CNNPlaces-CNNpool1pool2pool5fc7conv4conv3Published as a conference paper at ICLR 2015  Figure 2: Each pair of images shows the original image (left) and a simpliﬁed image (right) that gets classiﬁed by the Places-CNN as the same scene category as the original image. From top to bottom, the four rows show different scene categories: bedroom, auditorium, art gallery, and dining room.  Figure 3: The pipeline for estimating the RF of each unit. Each sliding-window stimuli contains a small randomized patch (example indicated by red arrow) at different spatial locations. By compar- ing the activation response of the sliding-window stimuli with the activation response of the original image, we obtain a discrepancy map for each image (middle top). By summing up the calibrated discrepancy maps (middle bottom) for the top ranked images, we obtain the actual RF of that unit (right).  For art gallery the minimal image representations contained paintings (81%) and pictures (58%); in amusement parks, carousel (75%), ride (64%), and roller coaster (50%); in bookstore, bookcase (96%), books (68%), and shelves (67%). These results suggest that object detection is an impor- tant part of the representation built by the network to obtain discriminative information for scene classiﬁcation.  3.2 VISUALIZING THE RECEPTIVE FIELDS OF UNITS AND THEIR ACTIVATION PATTERNS  In this section, we investigate the shape and size of the receptive ﬁelds (RFs) of the various units in the CNNs. While theoretical RF sizes can be computed given the network architecture (Long et al., 2014), we are interested in the actual, or empirical size of the RFs. We expect the empirical RFs to be better localized and more representative of the information they capture than the theoretical ones, allowing us to better understand what is learned by each unit of the CNN. Thus, we propose a data-driven approach to estimate the learned RF of each unit in each layer. It is simpler than the deconvolutional network visualization method (Zeiler & Fergus, 2014) and can be easily extended to visualize any learned CNNs4. The procedure for estimating a given unit’s RF, as illustrated in Fig. 3, is as follows. As input, we use an image set of 200k images with a roughly equal distribution of scenes and objects (similar to Sec. 2). Then, we select the top K images with the highest activations for the given unit.  4More visualizations are available at http://places.csail.mit.edu/visualization  4  receptive ﬁeld sliding-window stimuli calibrated discrepancy maps discrepancy maps for top 10 images  Published as a conference paper at ICLR 2015  Figure 4: The RFs of 3 units of pool1, pool2, conv4, and pool5 layers respectively for ImageNet- and Places-CNNs, along with the image patches corresponding to the top activation regions inside the RFs.  Table 2: Comparison of the theoretical and empirical sizes of the RFs for Places-CNN and ImageNet-CNN at different layers. Note that the RFs are assumed to be square shaped, and the sizes reported below are the length of each side of this square, in pixels.  Theoretic size Places-CNN actual size ImageNet-CNN actual size  19  pool1 17.8± 1.6 17.9± 1.6  67  pool2 37.4± 5.9 36.7± 5.4  conv3  99  52.1±10.6 51.1±9.9  conv4 131  60.0± 13.7 60.4± 16.0  pool5 195  72.0± 20.0 70.3± 21.6  For each of the K images, we now want to identify exactly which regions of the image lead to the high unit activations. To do this, we replicate each image many times with small random occluders (image patches of size 11×11) at different locations in the image. Speciﬁcally, we generate occlud- ers in a dense grid with a stride of 3. This results in about 5000 occluded images per original image. We now feed all the occluded images into the same network and record the change in activation as compared to using the original image. If there is a large discrepancy, we know that the given patch is important and vice versa. This allows us to build a discrepancy map for each image. Finally, to consolidate the information from the K images, we center the discrepancy map around the spatial location of the unit that caused the maximum activation for the given image. Then we average the re-centered discrepancy maps to generate the ﬁnal RF. In Fig. 4 we visualize the RFs for units from 4 different layers of the Places-CNN and ImageNet- CNN, along with their highest scoring activation regions inside the RF. We observe that, as the layers go deeper, the RF size gradually increases and the activation regions become more semantically meaningful. Further, as shown in Fig. 5, we use the RFs to segment images using the feature maps of different units. Lastly, in Table 2, we compare the theoretical and empirical size of the RFs at different layers. As expected, the actual size of the RF is much smaller than the theoretical size, especially in the later layers. Overall, this analysis allows us to better understand each unit by focusing precisely on the important regions of each image.  Figure 5: Segmentation based on RFs. Each row shows the 4 most conﬁdent images for some unit.  5  (cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:1)(cid:2)(cid:2)(cid:3)(cid:5)(cid:1)(cid:2)(cid:2)(cid:3)(cid:6)(cid:7)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)(cid:3)(cid:12)(cid:7)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:17)(cid:18)(cid:19)(cid:12)(cid:20)(cid:13)(cid:17)(cid:13)(cid:21)(cid:15)(cid:16)(cid:17)(cid:17)pool1Places-CNNpool2conv4pool5ImageNet-CNNPublished as a conference paper at ICLR 2015  Figure 6: AMT interface for unit concept annotation. There are three tasks in each annotation.  3.3  IDENTIFYING THE SEMANTICS OF INTERNAL UNITS  In Section 3.2, we found the exact RFs of units and observed that activation regions tended to become more semantically meaningful with increasing depth of layers. In this section, our goal is to understand and quantify the precise semantics learned by each unit. In order to do this, we ask workers on Amazon Mechanical Turk (AMT) to identify the common theme or concept that exists between the top scoring segmentations for each unit. We expect the tags provided by naive annotators to reduce biases. Workers provide tags without being constrained to a dictionary of terms that could bias or limit the identiﬁcation of interesting properties. Speciﬁcally, we divide the task into three main steps as shown in Fig. 6. We show workers the top 60 segmented images that most strongly activate one unit and we ask them to (1) identify the concept, or semantic theme given by the set of 60 images e.g., car, blue, vertical lines, etc, (2) mark the set of images that do not fall into this theme, and (3) categorize the concept provided in (1) to one of 6 semantic groups ranging from low-level to high-level: simple elements and colors (e.g., horizontal lines, blue), materials and textures (e.g., wood, square grid), regions ans surfaces (e.g., road, grass), object parts (e.g., head, leg), objects (e.g., car, person), and scenes (e.g., kitchen, corridor). This allows us to obtain both the semantic information for each unit, as well as the level of abstraction provided by the labeled concept. To ensure high quality of annotation, we included 3 images with high negative scores that the work- ers were required to identify as negatives in order to submit the task. Fig. 7 shows some example annotations by workers. For each unit, we measure its precision as the percentage of images that were selected as ﬁtting the labeled concept. In Fig. 8.(a) we plot the average precision for ImageNet- CNN and Places-CNN for each layer. In Fig. 8.(b-c) we plot the distribution of concept categories for ImageNet-CNN and Places-CNN at each layer. For this plot we consider only units that had a precision above 75% as provided by the AMT workers. Around 60% of the units on each layer where above that threshold. For both networks, units at the early layers (pool1, pool2) have more units responsive to simple elements and colors, while those at later layers (conv4, pool5) have more high-level semantics (responsive more to objects and scenes). Furthermore, we observe that conv4 and pool5 units in Places-CNN have higher ratios of high-level semantics as compared to the units in ImageNet-CNN. Fig. 9 provides a different visualization of the same data as in Fig. 8.(b-c). This plot better reveals how different levels of abstraction emerge in different layers of both networks. The vertical axis indicates the percentage of units in each layer assigned to each concept category. ImageNet-CNN has more units tuned to simple elements and colors than Places-CNN while Places-CNN has more objects and scenes. ImageNet-CNN has more units tuned to object parts (with the maximum around conv4). It is interesting to note that Places-CNN discovers more objects than ImageNet-CNN despite having no object-level supervision.  6  Published as a conference paper at ICLR 2015  Figure 7: Examples of unit annotations provided by AMT workers for 6 units from pool5 in Places- CNN. For each unit the ﬁgure shows the label provided by the worker, the type of label, the images selected as corresponding to the concept (green box) and the images marked as incorrect (red box). The precision is the percentage of correct images. The top three units have high performance while the bottom three have low performance (< 75%).  7  Pool5, unit 77; Label:legs; Type: object part; Precision: 96%Pool5, unit 76; Label: ocean; Type: scene; Precision: 93%Pool5, unit 13; Label: Lamps; Type: object; Precision: 84%Pool5, unit 22; Label: dinner table; Type: scene; Precision: 60%Pool5, unit 112; Label: pool table; Type: object; Precision: 70%Pool5, unit 168; Label: shrubs; Type: object; Precision: 54%Published as a conference paper at ICLR 2015  Figure 8: (a) Average precision of all the units in each layer for both networks as reported by AMT workers. (b) and (c) show the number of units providing different levels of semantics for ImageNet- CNN and Places-CNN respectively.  Figure 9: Distribution of semantic types found for all the units in both networks. From left to right, each plot corresponds to the distribution of units in each layer assigned to simple elements or colors, textures or materials, regions or surfaces, object parts, objects, and scenes. The vertical axis is the percentage of units with each layer assigned to each type of concept.  4 EMERGENCE OF OBJECTS AS THE INTERNAL REPRESENTATION  As shown before, a large number of units in pool5 are devoted to detecting objects and scene- regions (Fig. 9). But what categories are found? Is each category mapped to a single unit or are there multiple units for each object class? Can we actually use this information to segment a scene?  4.1 WHAT OBJECT CLASSES EMERGE?  To answer the question of why certain objects emerge from pool5, we tested ImageNet-CNN and Places-CNN on fully annotated images from the SUN database (Xiao et al., 2014). The SUN database contains 8220 fully annotated images from the same 205 place categories used to train Places-CNN. There are no duplicate images between SUN and Places. We use SUN instead of COCO (Lin et al., 2014) as we need dense object annotations to study what the most informative object classes for scene categorization are, and what the natural object frequencies in scene images are. For this study, we manually mapped the tags given by AMT workers to the SUN categories. Fig. 10(a) shows the distribution of objects found in pool5 of Places-CNN. Some objects are detected by several units. For instance, there are 15 units that detect buildings. Fig. 11 shows some units from the Places-CNN grouped by the type of object class they seem to be detecting. Each row shows the top ﬁve images for a particular unit that produce the strongest activations. The segmentation shows the regions of the image for which the unit is above a certain threshold. Each unit seems to be selective to a particular appearance of the object. For instance, there are 6 units that detect lamps, each unit detecting a particular type of lamp providing ﬁner-grained discrimination; there are 9 units selective to people, each one tuned to different scales or people doing different tasks.  8  pool1pool2conv3conv4pool5020406080100Places−CNNpool1pool2conv3conv4pool5020406080100ImageNet−CNN  simple elements & colortexture materialsregion surfaceobject partobjectscenepool1pool2conv3conv4pool550556065707580Average precision per layer  Places−CNNImageNet−CNNNumber of units (p>75%)Number of units (p>75%)a)c)b)01020304050percent units (perf>75%)0246810024681012024681012051015200246810places-CNNimagenet-CNNpool1pool2conv4pool5conv3Simple elements & colorsTexture materialsObject partRegion or surfaceObjectScenepool1pool2conv4pool5conv3pool1pool2conv4pool5conv3pool1pool2conv4pool5conv3pool1pool2conv4pool5conv3pool1pool2conv4pool5conv3Published as a conference paper at ICLR 2015  Figure 10: Object counts of CNN units discovering each object class for (a) Places-CNN and (b) ImageNet-CNN.  Fig. 10(b) shows the distribition of objects found in pool5 of ImageNet-CNN. ImageNet has an abundance of animals among the categories present: in the ImageNet-CNN, out of the 256 units in pool5, there are 15 units devoted to detecting dogs and several more detecting parts of dogs (body, legs, ...). The categories found in pool5 tend to follow the target categories in ImageNet. Why do those objects emerge? One possibility is that the objects that emerge in pool5 correspond to the most frequent ones in the database. Fig. 12(a) shows the sorted distribution of object counts in the SUN database which follows Zipf’s law. Fig. 12(b) shows the counts of units found in pool5 for each object class (same sorting as in Fig. 12(a)). The correlation between object frequency in the database and object frequency discovered by the units in pool5 is 0.54. Another possibility is that the objects that emerge are the objects that allow discriminating among scene categories. To measure the set of discriminant objects we used the ground truth in the SUN database to measure the classiﬁcation performance achieved by each object class for scene classiﬁcation. Then we count how many times each object class appears as the most informative one. This measures the number of scene categories a particular object class is the most useful for. The counts are shown in Fig. 12(c). Note the similarity between Fig. 12(b) and Fig. 12(c). The correlation is 0.84 indicating that the network is automatically identifying the most discriminative object categories to a large extent. Note that there are 115 units in pool5 of Places-CNN not detecting objects. This could be due to incomplete learning or a complementary texture-based or part-based representation of the scenes. Therefore, although objects seem to be a key part of the representation learned by the network, we cannot rule out other representations being used in combination with objects.  4.2 OBJECT LOCALIZATION WITHIN THE INNER LAYERS  Places-CNN is trained to do scene classiﬁcation using the output of the ﬁnal layer of logistic regres- sion and achieves state-of-the-art performance. From our analysis above, many of the units in the inner layers could perform interpretable object localization. Thus we could use this single Places- CNN with the annotation of units to do both scene recognition and object localization in a single forward-pass. Fig. 13 shows an example of the output of different layers of the Places-CNN using the tags provided by AMT workers. Bounding boxes are shown around the areas where each unit is activated within its RF above a certain threshold. In Fig. 14 we provide the segmentation performance of the objects discovered in pool5 using the SUN database. The performance of many units is very high which provides strong evidence that they are indeed detecting those object classes despite being trained for scene classiﬁcation.  5 CONCLUSION  We ﬁnd that object detectors emerge as a result of learning to classify scene categories, showing that a single network can support recognition at several levels of abstraction (e.g., edges, textures, objects, and scenes) without needing multiple outputs or networks. While it is common to train a network to do several tasks and to use the ﬁnal layer as the output, here we show that reliable outputs can be extracted at each layer. As objects are the parts that compose a scene, detectors tuned to the objects that are discriminant between scenes are learned in the inner layers of the network. Note  9  051015051015building    tree    grass    floor    mountain    person    plant    water    window    ceiling lamp    pitch    road    arcade    bridge    cabinet    chair    food    lighthouse    path    sky    tower    wall    water tower    bed    bookcase    car    ceiling    cementery    column    desk    desk lamp    field    grandstand    ground    iceberg    phone booth    railing    river    rocks    sand    screen    sea    seats    showcase    snowy ground    street    swimming pool    tent    text    wardrobe    waterfall    windmilldog    bird    person    wheel    animal body    flower    ground    head    legs    animal face    animal head    building    car    cat    ceiling    face    human face    leg    monkey    plant    plants    pot    road    sea    tower    tree    water    windowCountsCountsa)b)Published as a conference paper at ICLR 2015  Figure 11: Segmentations using pool5 units from Places-CNN. Many classes are encoded by several units covering different object appearances. Each row shows the 5 most conﬁdent images for each unit. The number represents the unit number in pool5.  that only informative objects for speciﬁc scene recognition tasks will emerge. Future work should explore which other tasks would allow for other object classes to be learned without the explicit supervision of object labels.  ACKNOWLEDGMENTS  This work is supported by the National Science Foundation under Grant No. 1016862 to A.O, ONR MURI N000141010933 to A.T, as well as MIT Big Data Initiative at CSAIL, Google and Xerox Awards, a hardware donation from NVIDIA Corporation, to A.O and A.T.  REFERENCES Agrawal, Pulkit, Girshick, Ross, and Malik, Jitendra. Analyzing the performance of multilayer  neural networks for object recognition. ECCV, 2014.  Bergamo, Alessandro, Bazzani, Loris, Anguelov, Dragomir, and Torresani, Lorenzo. Self-taught  object localization with deep networks. arXiv preprint arXiv:1409.3964, 2014.  Biederman, Irving. Visual object recognition, volume 2. MIT press Cambridge, 1995.  Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale  hierarchical image database. In CVPR, 2009.  Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. The pascal visual  object classes challenge. IJCV, 2010.  Farabet, Clement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical  features for scene labeling. TPAMI, 2013.  10  120) arcade116) bed18) billard table155) bookcase8) bridge56) buildingBuildingsOutdoor objectsNaturePeopleIndoor objectsFurniture119) building123) building38) cabinet87) car55) ceiling lamp174) ceiling lampLighting223) ceiling lamp145) cementery85) chair13) desk lamp182) foodScenes9) lighthouse195) grass89) iceberg140) mountain3) person49) person100) person138) person46) painting159) sand61) road106) screen53) staircase127) street96) swimming pool107) wardrobe28) water tower6) windmill218) pitchPublished as a conference paper at ICLR 2015  Figure 12: (a) Object frequency in SUN (only top 50 objects shown), (b) Counts of objects discov- ered by pool5 in Places-CNN. (c) Frequency of most informative objects for scene classiﬁcation.  Figure 13: Interpretation of a picture by different layers of the Places-CNN using the tags provided by AMT workers. The ﬁrst shows the ﬁnal layer output of Places-CNN. The other three show detection results along with the conﬁdence based on the units’ activation and the semantic tags.  Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object  detection and semantic segmentation. 2014.  Grangier, D., Bottou, L., and Collobert, R. Deep convolutional networks for scene parsing. TPAMI,  2009.  Jia, Yangqing. Caffe: An open source convolutional architecture for fast feature embedding, 2013.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, 2012.  Lazebnik, Svetlana, Schmid, Cordelia, and Ponce, Jean. Beyond bags of features: Spatial pyramid  matching for recognizing natural scene categories. In CVPR, 2006.  Le, Quoc V. Building high-level features using large scale unsupervised learning. In ICASSP, 2013.  11  Object counts in SUN050001000015000Object counts of most informative objects for scene recognitionCounts of CNN units discovering each object class.b)c)a)051015200102030    wall    window    chair    building    floor    tree    ceiling lamp    cabinet    ceiling    person    plant    cushion    sky    picture    curtain    painting    door    desk lamp    side table    table    bed    books    pillow    mountain    car    pot    armchair    box    vase    flowers    road    grass    bottle    shoes    sofa    outlet    worktop    sign    book    sconce    plate    mirror    column    rug    basket    ground    desk    coffee table    clock    shelves    wall    window    chair    building    floor    tree    ceiling lamp    cabinet    ceiling    person    plant    cushion    sky    picture    curtain    painting    door    desk lamp    side table    table    bed    books    pillow    mountain    car    pot    armchair    box    vase    flowers    road    grass    bottle    shoes    sofa    outlet    worktop    sign    book    sconce    plate    mirror    column    rug    basket    ground    desk    coffee table    clock    shelves    wall    window    chair    building    floor    tree    ceiling lamp    cabinet    ceiling    person    plant    cushion    sky    picture    curtain    painting    door    desk lamp    side table    table    bed    books    pillow    mountain    car    pot    armchair    box    vase    flowers    road    grass    bottle    shoes    sofa    outlet    worktop    sign    book    sconce    plate    mirror    column    rug    basket    ground    desk    coffee table    clock    shelvesPublished as a conference paper at ICLR 2015  Figure 14: (a) Segmentation of images from the SUN database using pool5 of Places-CNN (J = Jaccard segmentation index, AP = average precision-recall.) (b) Precision-recall curves for some discovered objects. (c) Histogram of AP for all discovered object classes.  Li, Li-Jia, Su, Hao, Fei-Fei, Li, and Xing, Eric P. Object bank: A high-level image representation  for scene classiﬁcation & semantic feature sparsiﬁcation. In NIPS, pp. 1378–1386, 2010.  Lin, Tg-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Dollr, Piotr, and Zitnick, C. Lawrence. Microsoft COCO: Common objects in context. In ECCV, 2014. Long, Jonathan, Zhang, Ning, and Darrell, Trevor. Do convnets learn correspondence? In NIPS,  2014.  Oliva, A. and Torralba, A. Building the gist of a scene: The role of global image features in recog-  nition. Progress in Brain Research, 2006.  Oquab, Maxime, Bottou, L´eon, Laptev, Ivan, Sivic, Josef, et al. Weakly supervised object recogni-  tion with convolutional neural networks. In NIPS. 2014.  Pandey, M. and Lazebnik, S. Scene recognition and weakly supervised object localization with  deformable part-based models. In ICCV, 2011.  P´erez, Patrick, Gangnet, Michel, and Blake, Andrew. Poisson image editing. ACM Trans. Graph.,  2003.  Renninger, Laura Walker and Malik, Jitendra. When is scene identiﬁcation just texture recognition?  Vision research, 44(19):2301–2311, 2004.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge, 2014.  Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian, and Fergus, Rob. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.  Tanaka, Keiji. Neuronal mechanisms of object recognition. Science, 262(5134):685–688, 1993. Tang, Yichuan, Srivastava, Nitish, and Salakhutdinov, Ruslan R. Learning generative models with  visual attention. In NIPS. 2014.  Xiao, J, Ehinger, K A., Hays, J, Torralba, A, and Oliva, A. SUN database: Exploring a large  collection of scene categories. IJCV, 2014.  Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod. How transferable are features in  deep neural networks? In NIPS, 2014.  Zeiler, M. and Fergus, R. Visualizing and understanding convolutional networks. In ECCV, 2014. Zhou, Bolei, Lapedriza, Agata, Xiao, Jianxiong, Torralba, Antonio, and Oliva, Aude. Learning deep  features for scene recognition using places database. In NIPS, 2014.  12  Fireplace (J=5.3%, AP=22.9%)Wardrobe (J=4.2%, AP=12.7%)Billiard table (J=3.2%, AP=42.6%)Bed (J=24.6%, AP=81.1%)Mountain (J=11.3%, AP=47.6%)Sofa (J=10.8%, AP=36.2%)Building (J=14.6%, AP=47.2%)Washing machine (J=3.2%, AP=34.4%)0246810120102030405060708090100102030405060708090100SofaDesk lampSwimming poolBedCarPrecisionRecallCountsAverage precision (AP)0102030405060708090100a)b)c)",
1412.7272,2015, Learning Non-deterministic Representations with Energy-based Ensembles,"['Learning Non-deterministic Representations with Energy-based Ensembles', 'Maruan Al-Shedivat', 'Emre Neftci', 'and Gert Cauwenberghs']",https://arxiv.org/pdf/1412.7272,"AcceptedasaworkshopcontributionatICLR2015LEARNINGNON-DETERMINISTICREPRESENTATIONSWITHENERGY-BASEDENSEMBLESMaruanAl-Shedivat∗Computer,ElectricalandMathematicalSciencesandEngineeringDivisionKingAbdullahUniversityofScienceandTechnology(KAUST)Thuwal23955-6900,SaudiArabiamaruan.shedivat@kaust.edu.saEmreNeftciandGertCauwenberghsInstituteforNeuralComputationUniversityofCalifornia,SanDiegoLaJolla,CA92093,USA{nemre,gert}@ucsd.eduABSTRACTThegoalofagenerativemodelistocapturethedistributionunderlyingthedata,typicallythroughlatentvariables.Aftertraining,thesevariablesareoftenusedasanewrepresentation,moreeffectivethantheoriginalfeaturesinavarietyoflearn-ingtasks.However,therepresentationsconstructedbycontemporarygenerativemodelsareusuallypoint-wisedeterministicmappingsfromtheoriginalfeaturespace.Thus,evenwithrepresentationsrobusttoclass-speciﬁctransformations,statisticallydrivenmodelstrainedonthemwouldnotbeabletogeneralizewhenthelabeleddataisscarce.Inspiredbythestochasticityofthesynapticconnectionsinthebrain,weintroduceEnergy-basedStochasticEnsembles.Theseensemblescanlearnnon-deterministicrepresentations,i.e.,mappingsfromthefeaturespacetoafamilyofdistributionsinthelatentspace.Thesemappingsareencodedinadistributionovera(possiblyinﬁnite)collectionofmodels.Byconditionallysamplingmodelsfromtheensemble,weobtainmultiplerepresentationsforeveryinputexampleandeffectivelyaugmentthedata.WeproposeanalgorithmsimilartocontrastivedivergencefortrainingrestrictedBoltzmannstochasticensembles.Finally,wedemonstratetheconceptofthestochasticrepresentationsonasyn-theticdatasetaswellastestthemintheone-shotlearningscenarioonMNIST.1INTRODUCTIONLearningdatarepresentationsisapowerfultechniquethathasbeenwidelyadoptedinmanyﬁeldsofartiﬁcialintelligence(Bengio,2009).Itsmaingoalisusuallytolearntransformationsofthedatathatdisentangledifferentclassesinthenewspace,thatarerobusttothenoiseintheinputandtoleranttothevariationsalongtheclass-speciﬁcmanifolds(DiCarlo&Cox,2007).Awidelyusedtechniqueofconstructingrepresentationsisbasedonusingprobabilisticgenerativemodelsofthedata.Latentvariablesinsuchmodelscancapturehigh-ordercorrelationsbetweenthesamplesandaresuccessfulasnewrepresentationsinavarietyoftasks(Bengio,2009).However,evenhighqualityrepresenta-tionsdonotsolvetheproblemofgeneralization:statisticallyderiveddiscriminativemodelsrequireasufﬁcientnumberoflabeledtrainingexamplesinordertoexhibitgoodperformancewhentested.Thestandardwayofamelioratingtheproblemofoverﬁttingduetothelimitedtrainingdataisbasedonenforcingaregularization(Bishop,2006).Maatenetal.(2013)recentlydemonstratedthattheartiﬁcialdataaugmentationviafeaturecorruptioneffectivelyplaystheroleofadata-adaptiveregu-larization.Wageretal.(2013)alsoshowedthatthedropouttechniquesappliedtogeneralizedlinearmodelsresultintoanadaptiveregularization.Inotherwords,theseapproachesconﬁrmthathavingmore(evencorrupted)trainingdataisequivalenttoregularizingthemodel.∗Correspondingauthor:maruan.alshedivat.com.1arXiv:1412.7272v2  [cs.LG]  22 Apr 2015AcceptedasaworkshopcontributionatICLR2015Ontheotherhand,insteadofcorruptingfeaturesonecantryperformregularizationbyperturbingthelearningmodelitself.Parameterperturbationleadstothenotionof(possiblyinﬁnite)collectionofmodelsthatwecallstochasticensemble.TheDropout(Hintonetal.,2012)andDropConnect(Wanetal.,2013)techniquesaresuccessfulexamplesofusingaparticularformofstochasticensembleinthecontextoffeedforwardneuralnetworks.Auniﬁedframeworkforacollectionofperturbedmodels(whichalsoencompassesthedatacorruptionmethods)wasrecentlyintroducedbyBachmanetal.(2014):Anarbitrarynoiseprocesswasusedtoperturbaparametricparentmodeltogenerateanensembleofchildmodels.Thetrainingwasperformedbyminimizingthemarginalizedlossfunction,andyieldedaneffectivelyregularizedparentmodelthatwasrobusttotheintroducednoise.Theabovementionedapproachesuseﬁxedcorruptionprocessestoregularizetheoriginalmodelandimproveitsgeneralization.Injectionofarbitrarynoiseimprovestherobustnessofthemodeltothecorruptionprocess(Maatenetal.,2013),butitdoesnotnecessarilycapturetheinformationaboutthegenerativeprocessbehindtheactualdata.Inordertocapturethevarianceofthedatamanifold,weproposetolearnthenoisethatperturbsthemodel.Ourworkisinspiredbytheadaptivestochasticityofthesynapticconnectionsinthebrain.Accordingtotheexperimentaldata,synapsesbetweencorticalneuronsarehighlyunreliable(Branco&Staras,2009).Moreover,thistypeofstochasticityisadaptiveandadjustedbythelearning(Stevens&Wang,1994).Inthispaper,weintroduceenergy-basedstochasticensembles(EBSEs)whichcanbetrainedinunsupervisedfashiontoﬁtadatadistribution.Theseensemblesresultfromenergy-basedmodels(EBMs)withstochasticparameters.TheEBSElearningprocedureoptimizesthelog-likelihoodoftheensemblebytuningaparametricdistributionoverthemodels.Thisdistributionisﬁrstarbitrarilyinitialized,andthentunedbyanexpectation-maximization(EM)likeprocedure:IntheE-step,weperformsamplingtoestimatethenecessaryexpectations,whileintheM-stepwemaximizethelog-likelihoodwithrespecttotheensembleparameters.WefurtherdevelopanalgorithmforlearningrestrictedBoltzmannstochasticensembles(RBSE)similartocontrastivedivergence.Usingapre-trainedensemble,wefurtherintroducethenotionofnon-deterministicrepresentations:insteadofconstructingpoint-wisemappingsfromtheoriginalfeaturespacetoanewlatentspace,weproposeusingstochasticmappings(Figure1a).Thestochasticityoftherepresentationsisbasedonthedistributionstoredintheensemble.Foreveryinputobjectwecansamplemultiplemodels,andhenceperformnon-deterministictransformationstoobtainasetofdifferentrepresentations.Theensembleistunedtocapturethevarianceofthedata,hencethestochasticrepresentationsarelikelytobettercapturethedatamanifold.Wedemonstratetheseconceptsvisuallybyperformingexperimentsonatwo-dimensionalsyntheticdataset.Finally,wetrainaclassiﬁerontherepresenta-tionsoftheMNISThand-writtendigitsgeneratedbyanRBSEandobservethatthegeneralizationcapabilityintheone-shotlearningscenarioimproves.2ENERGY-BASEDSTOCHASTICENSEMBLESThedistributionofthebinarydatavectorsvvv∈{0,1}Dcanbeencodedwiththefollowingenergy-basedmodel(EBM)thathassomebinaryhiddenvariableshhh∈{0,1}K:P(vvv,hhh;θ)=e−E(vvv,hhh;θ)Z(θ),(1)whereθdenotesthemodelparameters,E(vvv,hhh;θ)isaparametricscalarenergyfunction(LeCunetal.,2006),andZ(θ)isthenormalizingcoefﬁcient(partitionfunction).Ifweimposeadistributionoverthemodelparameters(i.e.,perturbθaccordingtosomenoisegenerationprocess),weobtainanenergy-basedstochasticensemble(EBSE).Inordertooptimizethedistributionoverthemodelsintheensemble,weparametrizethenoisegenerationprocesswithα:P(vvv,hhh,θ;α)=P(vvv,hhh|θ)P(θ;α)=e−E(vvv,hhh,θ)Z(θ)P(θ;α)=e−E(vvv,hhh,θ)−φ(θ;α)ζ(α),(2)whereφ(θ;α)isanadditionalenergypotential,andζ(α)isanewpartitionfunction.EBSEcanbetrainedbymaximizingthedatalog-likelihoodlogP(VVV;α)withrespecttoparametersα,whereVVVdenotestheentiredataset.Theintroducedformofthemodel(2)allowstothinkofP(θ;α)asaprior.Hence,wecanrelatetheoptimizationofEBSEwiththeBayesianinferenceframeworkforastandardEBMbytaking2AcceptedasaworkshopcontributionatICLR2015Point-wise representationsStochastic representationsNon-linear mappings(a)Point-wisevs.stochasticrepresentationsvhWbcKDN(b)RBMvWhbcαWαbαcKDN(c)RBSEFigure1:(a)Thedifferencebetweenpoint-wiseandstochasticrepresentations.Whilebothmap-pingsdisentanglethemanifoldsofdifferentclasses,stochasticityadditionallycapturesthedataman-ifoldintherepresentationspace.(b)RBMand(c)RBSEgraphicaldiagrams.Theshadednodesarethevisiblevariable,theblacknodesarethelatentvariables,literalsoutsideofnodesareconstants(modelparameters).Platesdenotevariablescopeswithsizesindicatedatthebottomrightcorners.expectationovertheposteriorP(θ|VVV;α):logP(VVV;α)=ZP(θ|VVV;α)logP(VVV|θ)dθ−DKL[P(θ|VVV;α)kP(θ;α)]=E[logP(VVV|θ)]P(θ|VVV;α)|{z}ExpectedEBMlog-likelihood−DKL[P(θ|VVV;α)kP(θ;α)]|{z}KLdivergenceofposteriorandprior.(3)SincetheKLdivergenceisnon-negative,byoptimizingthelog-likelihoodofEBSE,weeffectivelymaximizealowerboundontheEBMlog-likelihoodaveragedovertheposteriorP(θ|VVV;α).Thisrelatesourapproachtothefeaturecorruptionmethod(Maatenetal.,2013,Eq.2)whichoptimizestheexpectedlossdirectly,butoverasimpleposteriorfeaturecorruptiondistributionP(˜vvv|vvv).Eventually,oncewetrainedthestochasticensembleofenergy-basedgenerativemodels,wegetnewparametersˆαthatmakeP(θ;ˆα)bettertunedtocapturethevarianceofthedata.Below,wederivethegradientoftheEBSElog-likelihood(3)forthegeneralenergycase.Then,wefocusontherestrictedBoltzmannstochasticensembles(RBSE),analyzetheirstructure,andproposeanefﬁcientlearningalgorithm.2.1LOG-LIKELIHOODOPTIMIZATIONThegradientofthelog-likelihoodfunction(3)canbewritteninthefollowingway:∂logP(vvv;α)∂α=−∂F(vvv;α)∂α−1ζ(α)∂ζ(α)∂α,(4)whereF(vvv;α)=−log(cid:0)Re−E(vvv,hhh,θ)−φ(θ;α)dhhhdθ(cid:1)iscalledfreeenergy.Itiseasytoshowthatthegradientsofthefreeenergyandthepartitionfunctionhavethefollowingform:∂F(vvv;α)∂α=ZP(θ|vvv;α)(cid:18)∂φ(θ;α)∂α(cid:19)dθ,1ζ(α)∂ζ(α)∂α=−ZP(θ;α)(cid:18)∂φ(θ;α)∂α(cid:19)dθ.(5)Theﬁnalexpressionforthegradienthasacontrastivedivergencelikeform(Hinton,2002):∂logP(vvv;α)∂α=E(cid:20)∂φ(θ;α)∂α(cid:21)P(θ;α)−E(cid:20)∂φ(θ;α)∂α(cid:21)P(θ|vvv;α),(6)wheretheexpectationsE[·]P(θ;α)andE[·]P(θ|vvv;α)aretakenoverthemarginalandconditionaldistributionsoverthemodels,respectively.BasedonthepropertiesoftheP(θ;α)distribution,theseexpectationscanbeeitherfullyestimatedbyMonteCarlosampling,orpartlycomputedanalytically.3AcceptedasaworkshopcontributionatICLR2015Algorithm1Expectation-maximizationk-stepcontrastivedivergenceforRBSEInput:SSS—training(mini-)batch;learningrateλ;se-RBM(vvv,hhh,WWW,bbb,ccc)Output:gradientapproximation∆α[dependsontheparametrizationofφ(WWW,bbb,ccc;α)]1:initialize∆α=02:forallthevvv∈SSSdo3:vvv(0)←vvv4:hhh(0)←persistentstate(orsample)5:sampleWWW(0),bbb(0),ccc(0)∼P(WWW,bbb,ccc|vvv(0),hhh(0))6:fort=0,...,kdo.CD-kforsamplingfromP(vvv,hhh)7:sampleWWW(t),bbb(t),ccc(t)∼P(WWW,bbb,ccc|vvv(t),hhh(t))8:samplevvv(t)∼P(vvv|hhh(t),WWW(t),bbb(t),ccc(t))9:samplehhh(t)∼P(hhh|vvv(t),WWW(t),bbb(t),ccc(t))10:∆αm=Eh∂φ∂α|vvv(k),hhh(k)i11:fort=0,...,kdo.CD-kforsamplingfromP(hhh|vvv)12:sampleWWW(t),bbb(t),ccc(t)∼P(WWW,bbb,ccc|vvv,hhh(t))13:samplehhh0(t)∼P(hhh0|vvv,WWW(t),bbb(t),ccc(t))14:∆αc=Eh∂φ∂α|vvv,hhh0(k)i15:∆α←∆α+λ(∆αm−∆αc).EstimateSGDstep16:∆α=∆α/cord(SSS)Sincetheexpectationsdependontheparameterα,EBSEtrainingisreminiscentofexpectation-maximization(EM):Afterinitializingα,intheE-stepofthealgorithm,weestimateE[·]P(θ;α)andE[·]P(θ|vvv;α)usingthecurrentvalueofα.IntheM-step,wemaximizethelog-likelihoodbyusinggradient-basedoptimization.2.2MODELSTRUCTUREWefurtherconsideraspeciﬁcenergy-basedstochasticensemblecomposedofrestrictedBoltzmannmachines(RBM).TheenergyfunctionoftheRBMislinearineachofthevariablesvvv,hhh,andθ:E(vvv,hhh,θ)=−(vvvTWWWhhh+bbbTvvv+cccThhh)=−(Xi,jWijvihj+Xibivi+Xjcjhj),(7)whereparametersθarerepresentedbyatuple(WWW,bbb,ccc),andWWW∈RD×K,bbb∈RD,ccc∈RK.RBMisanundirectedgraphicalmodelthatcanberepresentedbytwolayersofinterconnectedprobabilisticunits(Figure1b).Theseunitscanbeseenasneuronswithaprobabilisticsigmoidactivationfunction,andthegraphicalmodelcanberepresentedbyatwo-layerneuralnetwork.Inthiscase,theparametersWWWplaytheroleofconnectionstrengthsbetweentheneurons.Byimposinganoisedistributionontheparameters,connectionstrengthsbecomestochastic.Fromagraphicalmodelperspective,thisisequivalenttointroducingnewlatentvariables(Figure1c).Noticethatthemodelbecomesmixeddirectedandundirected—anensembleofundirectedRBMsgeneratedbyasimpleBayesiannetworkcomposedof(WWW,bbb,ccc)randomhiddenvariables.Inordertofullydeﬁnethemodel,weneedtochooseaspeciﬁcformofthemarginaldistributionP(θ;α)thatgeneratestheensemble.First,tomaketheexpectationscomputationallyfeasible,wesupposethatWWW,bbb,cccareallmarginallyindependent:P(θ;α)=YijP(Wij;αWij)YiP(bi;αbi)YjP(cj;αcj).(8)Then,weconsidertwocases:(a)Bernoullidistributedparameters.InthiscaseWijcanbeeitherzerowithprobability1−pij,orequaltosome¯Wijwithprobabilitypij.Thetunableparametersareαij={pij,¯Wij}.ThiscaseissimilartoDropConnect(Wanetal.,2013)techniquebutwithadaptivedistributionsovertheconnectionsbetweenvisibleandhiddenlayersinRBM.(b)Inthesecondcase,parametersarenormallydistributed,i.e.,Wij∼N(µij,σij),andαWij={µij,σij}.Inbothcases,bbbandcccareparametrizedsimilarlyasWWW.ThenumberofparametersintheRBSEistwicetheoriginalnumberfortheRBM.4AcceptedasaworkshopcontributionatICLR2015Thisstructureofthemodel(Figure1c)impliesthefollowingsetofindependencies:vi⊥⊥vj|hhh,hi⊥⊥hj|vvv,Wij⊥⊥Wlk,Wij6⊥⊥Wik|vi,Wij6⊥⊥Wlj|hi.(9)TheseexpressionsshowthemarginalindependencieswepurposefullyincorporatedintoP(θ;α)andconditionaldependencesbetweenthecomponentsofWWW.Duetothesedependenciesbetweenstochasticconnectionsthoughvisibleandhiddenunits,themodelisabletocapturethedatavarianceinthedistributionoverthestochasticparametersθ.Importantly,eventhoughthecomponentsofWWWaredependentoneachothergivenvvvandhhh,wearestillabletofactorizetheconditionaldistributionP(θ|vvv,hhh)usingtherenormalizationprocedure(seethesupplementarymaterialfordetails).2.3TRAININGWeproposetheexpectation-maximizationk-stepcontrastivedivergencealgorithmfortrainingRBSE(summarizedinAlgorithm1)withtwodifferenttypesofstochasticconnections—BernoulliandGaussian—betweenthevisibleandthehiddenlayers.TocarryouttheE-step,weneedtocomputetheexpectationsin(6).WeuseMonteCarloestimatesoftheseexpectations:E[·]P(θ;α)=ZdvvvdhhhP(vvv,hhh;α)ZdθP(θ|vvv,hhh;α)[·]≈1MX˜vvv,˜hhh∼P(vvv,hhh;α)ZdθP(θ|˜vvv,˜hhh;α)[·],E[·]P(θ|vvv;α)≈1MXˆhhh∼P(hhh|vvv;α)ZdθP(θ|vvv,ˆhhh;α)[·],(10)whereMisthenumberofsamplesusedtoapproximateadistribution.Thestates(˜vvv,˜hhh)shouldbesampledfromthemarginalmodeldistributionP(vvv,hhh;α),andˆhhhsampledfromthemarginalmodelconditionaldistributionP(hhh|vvv;α).ThiscanbeachievedbyrunningaGibbssamplingprocedureasinstandardcontrastivedivergencealgorithm(seeAlgorithm1).Lastly,weneedtocomputetheexpectationovertheposteriorP(θ|vvv,hhh;α)givenavisibleandahiddenstate.InbothBernoulliandGaussiancases,duetothestructureoftheensembledistributionP(θ;α)discussedinsection2.2,theseexpectationscanbecomputedanalytically.Asanexample,wegetthefollowingexpressionsforthegradientupdateoftheGaussianRBSE(fortheBernoullicase,thenotationandotherdetailsseethesupplementarymaterial):∂logP(vvv;α)∂¯Wij=hvihiidata−hvihiirecon,∂logP(vvv;α)∂¯σij=(cid:10)v2ih2jσij(cid:11)data−(cid:10)v2ih2jσij(cid:11)recon.Forthemodelswheretheexpectationsovertheensemblearenotanalyticallytractable,wecanestimatethemusingMonteCarloMarkovchainsamplingaswell.Themainbottleneckofthealgorithmisthenumberofsamplingspergradientupdate.Sincealltheconnectionsbetweenthevisibleandthehiddenunitsareprobabilistic,therandomvariablesneedtobesampledaquadraticnumberoftimescomparedtolearninganRBM.However,duetotheindependenciesintroducedinEq.(9),allthevariablesWij,bi,andcjcanbesampledinparallel.3EXPERIMENTSWeintroducetheconceptofstochasticrepresentationsbyconsideringasemi-supervisedlearningscenariowherealargeamountofdataisavailablebutwithonlyafewlabeledexamplesperclass.RBSEisagenerativemodel,andhenceitcanbetrainedinanunsupervisedfashionontheentiredataset.OncetheensembledistributionP(θ;α)istunedtothedata,foreverylabeledtrainingexample,wecanconditionallysamplemodelsθ∼P(θ|vvv)andthenuseeachmodeltogeneratearepresentationbasedontheactivationofthehiddenunitshhh.Inotherwords,thegenerativestochasticensemblescanbeusedtostoretheinformationabouttheentiredatasetandeffectivelyaugmentthenumberoflabeledexamplesbygeneratingmultiplestochasticrepresentations.Thisisanalogoustotheconceptofcorruptedfeatures(Maatenetal.,2013;Wageretal.,2013).ThemaindifferenceisthatRBSEscanlearnfromtheunlabeledpartofthedatasethowtocorruptthedataproperly.Totesttheconcept,weimplementedBernoulliandaGaussianRBSEusingtheTheanolibrary(Bergstraetal.,2010).Wetrainedourgenerativemodelsontwo-dimensionalsyntheticdatasetsandonMNISTdigits.Thefollowingsectionsprovidethedetailsoneachoftheexperiments.5AcceptedasaworkshopcontributionatICLR20150.00.20.40.60.81.0X10.00.20.40.60.81.0X2(a)RBMperformsdeterministicmapping.0.00.20.40.60.81.0X10.00.20.40.60.81.0(b)RBSEcapturesthetrainingdatavariance.0.00.20.40.60.81.0X10.00.20.40.60.81.0X2(c)Isolatedpointsareattractedbythetrainingdata.0.00.20.40.60.81.0X10.00.20.40.60.81.0(d)RBSEcapturesthewholedatamanifold.Figure2:Visualizationoftheexperimentswiththesynthetictwodimensionaldata.Onallpanels,theblackpointsrepresentthetrainingdata,andtheredpointsarethetesting.Bestviewedincolor.(a)TheclassicalRBMcanperformonlydeterministicmapping.(b)RBSEmapsthetestingpointstomultiplerepresentationswhichmapbackwardstodifferentpointsintheoriginalspace.(c)RBSEattractstherepresentationsoftheisolatedpointstowardsthetrainingdatamanifold.(d)Stochasticrepresentationscancapturethevarianceoftheentiremanifoldfromafewexamples.3.1SYNTHETICDATAInordertovisuallytesthowastochasticensemblecancapturethetrainingdatamanifold,wegener-atedseveralsimpletwo-dimensionaldatasets{x1,x2}∈[0,1]2(Figure2).WetrainedanordinaryRBMwith2visibleand8hiddenunitsandanRBSEofthesamestructure.Usingthesemodels,wemappedthetestingpointstothelatentspacemultipletimes,andthenbacktotheoriginalspace.ForRBM,weusedthemeanﬁeldactivationsfornewrepresentations:hhhnew=P(hhh|vvv)andvvvnew=P(vvv|hhhnew).Unsurprisingly,thetwoconsecutivetransformations,fromvisibletohid-denandbacktovisiblespace,performedbyaproperlytrainedRBMalwaysmappedthetestingpointstothemselves(Figure2a).NoticethatthisholdsnotonlyforRBMsbutforanydetermin-isticmodel:Otherpoint-wisedeterministicrepresentationlearningtechniques,e.g.,autoencoders(Bengio,2009),exhibitthesamebehavior.WeperformedasimilarproceduremultipletimesforRBSE:ﬁrst,wesampledamodelfromthecon-ditionaldistributionP(θ|vvv),thenusingP(hhh|vvv,˜θ),wetransformedtestingpointsfromthevisibletothehiddenspace,thenmappedbackwardsusingtheaveragemodelfromtheensemble.Stochasticrepresentationsofthetestingdata,whenmappedbacktothevisiblespace,weredistributedalongthetrainingdatamanifoldnearthetestingpointstheybelongedto(Figure2b).Theexperimentsalsodemonstratedthattherepresentationsofanisolatedtestingpoint(anoutlier)willbeattractedtowardsthemanifoldcapturedbyRBSE(Figure2c),whichcannotbedonebytheRBM.Finally,anentiremanifoldcanbecapturedbythegeneratedstochasticrepresentationsforonlyasmallnumberofinitialdatapoints(Figure2d).TheseexperimentsconﬁrmedthecapabilityofRBSEstoreﬁneitsinternaldistributionoverthemodelsandtocapturethevarianceofthedata.6AcceptedasaworkshopcontributionatICLR2015(a)RBSEﬁlters(b)RBSEprobabilitiesPixelsRBMRBM +DropConnectRBSE0.350.400.450.500.550.600.65Accuracy(c)One-shotaccuracyon10-classclassiﬁcationFigure3:AftertrainingonMNISTforseveralepochs:(a)RBSEﬁlters,i.e.,the¯Wijvalues.(b)RBSEconnectionprobabilities,i.e.,thepijvalues—thedarkpixelsarecloseto0;thelightpixelsarecloseto1.(c)Performanceofalogisticregressionclassiﬁertrainedondifferentrepresentationsofthedataundertheone-shotconstraints.Therefore,thestochasticityofsuchtunableensemblesshouldprovidebetterregularizationthanjustarbitrarynoiseinjectionand,asweshowfurther,improvetheperfomranceinone-shotlearning.3.2ONE-SHOTLEARNINGOFMNISTDIGITSToteststochasticrepresentationsinthesemi-supervisedsettingwhereonlyafewlabeleddataareavailable(oneexampleperclass),weperformedexperimentsonMNISTdigitsdataset.WetrainedaBernoulliRBSEmodelwith784visibleand400hiddenunitson50,000unlabeledMNISTdigitsusingtheproposedAlgorithm1withanumberofMCMCstepsk=1.Thelearnedﬁlters(i.e.,¯Wij)andtheBernoulliconnectionprobabilities(i.e.,pij)arrangedintotilesarepresentedonFigure3.Noticethattheconnectionprobabilitiesencodealotofstructure(Figure3b).Forthepurposeofcomparison,wealsotrainedanRBMofthesameconﬁgurationonthesameunlabeledMNISTdata.Next,wesampled1000objects(100perclass)fromtheremaining20,000MNISTdigits:1train-ingand99testingexamplesperclass.Foreverytrainingsampleweuseddifferentapproachestoconstructtherepresentations:(a)imagepixels,(b)deterministicrepresentationsconstructedbyapre-trainedRBM,(c)stochasticrepresentationsconstructedbyanRBMwithBernoulliprobabilities0.5foreveryconnection(equivalenttoDropConnect),and(d)RBSE-generatedstochasticrepresen-tations.Inthe(c)and(d)cases,weconstructed10representationsforeveryobject.Finally,wetrainedandtestedalogisticregressionclassiﬁer(withnoadditionalregularization)onthetheserepresentationsunderone-shotlearningconstraints.Theclassiﬁcationexperimentsweredonemultipletimesfordifferentlysampledobjects.TheresultsarepresentedonFigure3c.About10%improvementinclassiﬁcationaccuracyisduetobetterrepresentationslearnedbyRBM(disentan-glementoftheclasses).WhenweregularizetheclassiﬁerbygeneratingstochasticrepresentationswithDropConnectnoiseappliedtotrainedRBM,theperformanceslightlydrops.Onthecontrary,whentheclassiﬁerisregularizedthroughtherepresentationsgeneratedbyRBSE,wegetaboutanother5%increaseinaccuracyonaverage.4DISCUSSIONInthispaper,weintroducedtheconceptnon-deterministicrepresentationsthatcanbelearnedbyEnergy-basedStochasticEnsemblestunedtothedatainunsupervisedfashion.Theensemblestochasticitycancapturethedatavarianceandbeusedtoadaptivelyregularizediscriminativemod-elsandimprovetheirperformanceinsemi-supervisedsettings.Theactuallearningofapropermodelperturbationfromthedataistheconceptualnoveltycomparedtopreviouswork(Bachmanetal.,2014;Maatenetal.,2013;Wageretal.,2013).Weillustratedthepowerofstochasticen-semblesvisuallyonsynthetictwo-dimensionaldataanddemonstrateditquantitativelyonone-shotlearningoftheMNISThand-writtendigits.7AcceptedasaworkshopcontributionatICLR2015Theinspirationfromsynapticstochasticityobservedinbiologicalnervecellsprovidesanumberofinsightsandhypothesesforexperimentalneuroscience,whichwewillreportseparatelyelsewhere.Fromtheartiﬁcialneuralnetworksperspective,theproposedapproachofusingstochasticconnec-tionsbetweenneuralunitsisinterestingaswell.Forexample,similarstochasticensemblesoffeed-forwardneuralnetworksshouldbeabletocapturecomplexmulti-modaldatamanifolds(Tang&Salakhutdinov,2013).Also,recentlyproposedGenerativeStochasticNetworks(GSNs),whichareusedtoencodeprobabilitiesintheirsamplingbehavior(Bengio&Thibodeau-Laufer,2013;Bengioetal.,2013),canbenaturallyendorsedwithnon-deterministicconnectionsandmightpotentiallyrealizericherfamiliesofdistributions.Interestingly,biologicalinspirationalsosuggeststhatneuromorphiccomputersthatoperateinamassivelyparallelfashion,whileconsumingafactionofthepowerofdigitalcomputers(Merollaetal.,2014)canbeleveraged.TheyoftennativelysupportBernoullisynapticstochasticity(Gold-bergetal.,2001),andneuromorphicvariantsofRBMscanbeefﬁcientlyimplementedandtrained(Neftcietal.,2014).ThissuggeststhatthedisadvantagesassociatedtothecomputationaloverheadofRBSEscanbenulliﬁedbyusinganappropriatecomputationalsubstrate.ACKNOWLEDGMENTSTheauthorsthankCianO’Donnellforhelpfuldiscussions.E.N.andG.C.weresupportedinpartbytheNationalScienceFoundation(NSFEFRI-1137279)andtheOfﬁceofNavalResearch(ONRMURI14-13-1-0205).M.A.wassupportedbyKAUSTGraduateFellowship.REFERENCESBachman,Phil,Alsharif,Ouais,andPrecup,Doina.Learningwithpseudo-ensembles.InAdvancesinNeuralInformationProcessingSystems,pp.3365–3373,2014.Bengio,Yoshua.LearningdeeparchitecturesforAI.FoundationsandtrendsR(cid:13)inMachineLearn-ing,2(1):1–127,2009.Bengio,YoshuaandThibodeau-Laufer,Eric.Deepgenerativestochasticnetworkstrainablebybackprop.arXivpreprintarXiv:1306.1091,2013.Bengio,Yoshua,Yao,Li,Alain,Guillaume,andVincent,Pascal.Generalizeddenoisingauto-encodersasgenerativemodels.InAdvancesinNeuralInformationProcessingSystems,pp.899–907,2013.Bergstra,James,Breuleux,Olivier,Bastien,Fr´ed´eric,Lamblin,Pascal,Pascanu,Razvan,Des-jardins,Guillaume,Turian,Joseph,Warde-Farley,David,andBengio,Yoshua.Theano:acpuandgpumathexpressioncompiler.InProceedingsofthePythonforscientiﬁccomputingconfer-ence(SciPy),volume4,pp.3,2010.Bishop,ChristopherM.Patternrecognitionandmachinelearning,volume1.Springer,2006.Branco,TiagoandStaras,Kevin.Theprobabilityofneurotransmitterrelease:variabilityandfeed-backcontrolatsinglesynapses.NatureReviewsNeuroscience,10(5):373–383,2009.DiCarlo,JamesJandCox,DavidD.Untanglinginvariantobjectrecognition.Trendsincognitivesciences,11(8):333–341,2007.Goldberg,D.H.,Cauwenberghs,G.,andAndreou,A.G.Probabilisticsynapticweightinginarecon-ﬁgurablenetworkofVLSIintegrate-and-ﬁreneurons.NeuralNetworks,14(6–7):781–793,Sep2001.Hinton,GeoffreyE.Trainingproductsofexpertsbyminimizingcontrastivedivergence.Neuralcomputation,14(8):1771–1800,2002.Hinton,GeoffreyE,Srivastava,Nitish,Krizhevsky,Alex,Sutskever,Ilya,andSalakhutdinov,Rus-lanR.Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors.arXivpreprintarXiv:1207.0580,2012.8AcceptedasaworkshopcontributionatICLR2015LeCun,Yann,Chopra,Sumit,Hadsell,Raia,Ranzato,M,andHuang,F.Atutorialonenergy-basedlearning.Predictingstructureddata,2006.Maaten,Laurens,Chen,Minmin,Tyree,Stephen,andWeinberger,KilianQ.Learningwithmarginalizedcorruptedfeatures.InProceedingsofthe30thInternationalConferenceonMa-chineLearning(ICML-13),pp.410–418,2013.Merolla,PaulA,Arthur,JohnV,Alvarez-Icaza,Rodrigo,Cassidy,AndrewS,Sawada,Jun,Akopyan,Filipp,Jackson,BryanL,Imam,Nabil,Guo,Chen,Nakamura,Yutaka,etal.Amillionspiking-neuronintegratedcircuitwithascalablecommunicationnetworkandinterface.Science,345(6197):668–673,2014.Neftci,E.,Das,S.,Pedroni,B.,Kreutz-Delgado,K.,andCauwenberghs,G.Event-drivencontrastivedivergenceforspikingneuromorphicsystems.FrontiersinNeuroscience,7(272),2014.Stevens,CharlesFandWang,Yanyan.Changesinreliabilityofsynapticfunctionasamechanismforplasticity.Nature,371(6499):704–707,1994.Tang,YichuanandSalakhutdinov,Ruslan.Learningstochasticfeedforwardneuralnetworks.InAdvancesinNeuralInformationProcessingSystems,pp.530–538,2013.Wager,Stefan,Wang,Sida,andLiang,Percy.Dropouttrainingasadaptiveregularization.InAdvancesinNeuralInformationProcessingSystems,pp.351–359,2013.Wan,Li,Zeiler,Matthew,Zhang,Sixin,Cun,YannL,andFergus,Rob.Regularizationofneuralnetworksusingdropconnect.InProceedingsofthe30thInternationalConferenceonMachineLearning(ICML-13),pp.1058–1066,2013.9UnderreviewasaconferencepaperatICLR2015SUPPLEMENTARYMATERIALAPOSTERIORBERNOULLIANDGAUSSIANDISTRIBUTIONSThissectionprovidesdetailsontheposteriordistributionsforBernoulliandGaussianstochasticensembles.RBMhasthreesetsofparameters:connectionstrengthsbetweenthevisibleandhiddenlayersWWW∈RD×K,biasbbb∈RDforvisible,andbiasccc∈RKforhiddenunits.WeuseitoindexDvisibledimensions,andjforKhiddendimensions.Wedenoteunnormalizedmeasuresby˜P(·).WhentheequationsforWij,bi,andcjhavethesameform,werefertothesecomponentsasθk.Forsomeﬁxed(vvv,hhh)weknowthatRBSE’senergyisalinearfunctionofθ.SincethepriorP(θ;α)factorizesoverallthecomponentsofθaccordingtoourdeﬁnition.Thus,wecanfactorizeunnor-malized˜P(θ,vvv,hhh;α)overallθk:˜P(Wij,vi,hj;αij)=P(Wij;αij)exp(vihjWij),˜P(bi,vi;αi)=P(bi;αi)exp(vibi),˜P(cj,hj;αj)=P(cj;αj)exp(hjcj).(A1)Wecanfurtherrenormalize(A1)andobtainthefollowingposteriordistribution:P(WWW,bbb,ccc|vvv,hhh;α)=QijP(Wij,vi,hj;αij)QiP(bi,vi;αi)QjP(cj,hj;αj)RQijP(Wij,vi,hj;αij)dWWWQiP(bi,vi;αi)dbbbQjP(cj,hj;αj)dccc=YijP(Wij,vi,hj;αij)RP(Wij,vi,hj;αij)dWijYiP(bi,vi;αi)RP(bi,vi;αi)dbiYjP(cj,hj;αj)RP(cj,hj;αj)dcj=YijP(Wij|vi,hj;αij)YiP(bi|vi;αi)YjP(cj|hj;αj).(A2)Aswesee,ifthepriordistributionP(θ;α)factorizesoverthecomponentsofθ,theposteriordistri-butionP(θ|vvv,hhh;α)isalsofactorisable.Finally,weneedtoﬁndtheposteriordistributionforeachofthecomponentsfortheBernoulliandGaussianensembles.ForBernoullicase,apriori,everycomponentθkcaneithertakesomenon-zerovalue¯θkwithproba-bility(1−pk),orbeequaltozerowithprobabilitypk.Then,thedistributionP(θ;α)isparametrizedbyα=(¯θk,pk)andhasthefollowingform:P(θ;α)=YkP(θk;αk)=Yk(cid:2)δ(θk)(1−pk)+δ(θk−¯θk)pk(cid:3),(A3)whereδ(·)istheDiracdeltafunction.TheposteriorforWij(similarforbiandcj)willbeP(Wij|vi,hj;αij)=(cid:0)δ(Wij)(1−pij)+δ(Wij−¯Wij)pij(cid:1)exp(vihjWij)1−pij+pijexp(vihjWij)(A4)ForGaussiancase,componentsθkapriorihaveaGaussiandistributionwiththemean¯θkandvari-anceσ2k,i.e.,αk=(¯θk,σk).Omittingthedetailsofintegration,theposteriorforWijwillhavethefollowingform(similarexpressionsareeasytoderiveforbiandcj):P(Wij|vi,hi;αij)=1√2πσijexp−""Wij−(¯Wij+vihiσ2ij)√2σij#2.(A5)InbothBernoulliandGaussiancases,duetothelinearstructureoftheRBSEenergyfunctionandthestructureofthepriorP(θ;α),weareabletogetanalyticalexpressionsfortheposteriorofeachcomponentofθ.Moreover,thederivationswillbethesamenotonlyforBernoulliandGaussiancases,butforanyP(θ;α)thatiscompletelyfactorisableover{θk}.1arXiv:1412.7272v2  [cs.LG]  22 Apr 2015UnderreviewasaconferencepaperatICLR2015BEXPECTATIONSOVERTHEPOSTERIORHereweprovidedetailsontheanalyticalcomputationoftheexpectedstochasticgradientupdateforthetwotypesofstochasticensembles:BernoulliandGaussian.Forthesakeofspace,asinAppendixA,whenthereisnoambiguity,wedenotethecomponentsofWWW,bbb,cccgenerallyasθk.Thegeneralformofthegradientexpectationisthefollowing(seeEq.10inthemaintext):ZΘP(θ|vvv,hhh;α)∂φ(θ;α)∂αdθ.(B1)Now,westartwiththeBernoullicase,whereαk=(θk,pk).Thepriordistributionandtheφpotentialhavethefollowingform:P(θk;αk)≡e−φ(θk;αk)=δ(θk)(1−pk)+δ(θk−¯θk)pk,∂φ∂pk=δ(θk)−δ(θk−¯θk)e−φ(θk;αk),∂φ∂¯θk=δ0(θk−¯θk)pke−φ(θk;αk),(B2)whereδ(·)istheDiracdeltafunction,andδ0(·)isitsderivative.Whenweplug(A2)and(B2)into(B1),weobtainthefollowingexpressionsfortheexpectationoverWij(similarforbiandci):E(cid:20)∂φ∂pij|vi,hi(cid:21)=1−pij1−pij+pijexp(vihi¯Wij)(cid:16)1−evihi¯Wij(cid:17),E(cid:20)∂φ∂¯Wij|vi,hi(cid:21)=pij1−pij+pijevihi¯WijZδ0(Wij−¯Wij)exp(vihiWij)dWij=[integratingbyparts]=pijevihi¯Wij1−pij+pijexp(vihi¯Wij)(−vihi).(B3)Eventually,wegetthefollowingexpressionsforthegradientcomponentsofthelog-likelihood:∂logP(vvv;α)∂¯Wij=hP(Wij6=0|vi,hi)vihiidata−hP(Wij6=0|vi,hi)vihiirecon,∂logP(vvv;α)∂¯pij=DP(Wij=0|vi,hi)(cid:16)1−evihi¯Wij(cid:17)Edata−DP(Wij=0|vi,hi)(cid:16)1−evihi¯Wij(cid:17)Erecon.(B4)Thegradientover¯Wijresemblestheoriginalcontrastivedivergencebuthasadditionalposteriorprobabilitymultipliers.Similarly,wedothesamederivationsfortheGaussiancase,whereαk=(¯θk,σk).Thepriorandtheφderivativeshavethefollowingform:P(θk;αk)≡e−φ(θk;αk)=1√2πσkexp""−(cid:18)θk−¯θk√2σk(cid:19)2#,∂φ∂¯θk=¯θk−θkσ2k,∂φ∂σk=1σk""1−(cid:18)θk−¯θkσk(cid:19)2#.(B5)Aftermarginalization,weobtainthefollowingexpressionsforWij(similarforbiandcj):E(cid:20)∂φ∂¯Wij|vi,hi(cid:21)=−vihi,E(cid:20)∂φ∂σij|vi,hi(cid:21)=−v2ih2iσij,(B6)andgetthefollowingexpressionsforthegradientofthelog-likelihoodcomponents:∂logP(vvv;α)∂¯Wij=hvihiidata−hvihiirecon,∂logP(vvv;α)∂¯σij=(cid:10)v2ih2jσij(cid:11)data−(cid:10)v2ih2jσij(cid:11)recon.(B7)2UnderreviewasaconferencepaperatICLR2015NoticethatwhenWWW,bbb,cccaredeterministic(i.e.,pij=0andσk=0forBernoulliandGaussianensembles,respectively),theexpectedderivativesover¯Wij(thesameisfor¯biand¯cj)givethesameexpressionsasthestandardcontrastivedivergence.Thus,RBMisthedeterministiclimitofRBSE.Anotherpointisrelatedtotheimplementation:Duringtheoptimization,thealgorithmstepsmightbesuboptimal,hencewerestrictprobabilitiesbeintherange[(cid:15),1−(cid:15)]forsome(cid:15)(cid:28)1.Inotherwords,wedonotallowthemodeloccasionallyturnintoclassicRBMbykeepingsome(cid:15)-stochasticityintheconnectionstrengths.3",
1412.7063,2015, Diverse Embedding Neural Network Language Models,"['Diverse Embedding Neural Network Language Models', 'Kartik Audhkhasi', 'Abhinav Sethy', 'and Bhuvana Ramabhadran']",https://arxiv.org/pdf/1412.7063,"Accepted as workshop contribution at ICLR 2015  DIVERSE EMBEDDING NEURAL NETWORK LANGUAGE MODELS  Kartik Audhkhasi∗, Abhinav Sethy∗ & Bhuvana Ramabhadran IBM T. J. Watson Research Center Yorktown Heights, NY 10598, USA {kaudhkha,asethy,bhuvana}@us.ibm.com ∗Kartik Audhkhasi and Abhinav Sethy are joint ﬁrst authors.  ABSTRACT  We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vec- tor onto multiple diverse low-dimensional sub-spaces instead of a single higher- dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an aug- mented loss function. Our language modeling experiments on the Penn Treebank data set show the performance beneﬁt of using a DENNLM.  1  INTRODUCTION  Diversity of systems trained to perform a given machine learning task is crucial to obtaining a performance improvement upon fusion (Kuncheva (2004)). The problem of language modeling, that aims to design predictive models of text, is no exception. Several language models (LMs) have been proposed over many years of research (Rosenﬁeld (2000)). Simple N-gram models esti- mate the conditional probability of the i-th word wi in a sentence given the previous N − 1 words (wi−N +1, . . . , wi−1) as  ˆPN-gram(wi|wi−N +1, . . . , wi−1) =  C(wi, wi−1, . . . , wi−N +1)  C(wi−1, . . . , wi−N +1)  (1)  where C(.) computes the count of a given word phrase or N-gram in the training text1. More com- plex LMs such as feed-forward neural networks (NNs) (Bengio et al. (2006)) estimate this probabil- ity as a non-linear function  ˆPNN(wi|wi−N +1, . . . , wi−1) = f (wi−1, . . . , wi−N +1; Θ)  (2) parameterized by Θ2. Researchers have found that fusing different kind of n-gram language models together (Goodman (2001), Mikolov et al. (2011)) often signiﬁcantly improves performance. Table 1 shows the perplexity3 of 4-gram and NNLMs on a standard split of the Penn Treebank data set (Marcus et al. (1993)). Interpolation of a NNLM with a 4-gram LM gives a 16.9% reduction in perplexity over a single NNLM even though the two LMs have relatively close perplexities. We use the correlation coefﬁcient between the posterior probabilities of the predicted word over the test set from the two models as a simple measure to predict whether the models are diverse. If the posterior probabilities are highly correlated, then the models are less diverse and smaller gains are expected from fusion. The posteriors from the N-gram and NNLM have a correlation coefﬁcient of 0.869 which is signiﬁcantly lower than the correlation coefﬁcient of 0.988 for a pair of randomly initialized NNLMs. This higher diversity of the NNLM and N-gram LM combination results in signiﬁcant perplexity improvement upon interpolation.  5 1 0 2    r p A 5 1         ] L C . s c [      5 v 3 6 0 7  .  2 1 4 1 : v i X r a  1Almost all N-gram models are smoothed in various ways to assign non-zero probability estimates for word phrases unseen in training data. We use Kneser-Ney smoothing (Kneser & Ney (1995)) for all N-grams models in this paper.  2We will describe the typical architecture of a NNLM in the next section 3Perplexity is a standard measure of LM performance and is 2−L where L is the negative average log-  likelihood of the test set text estimated by the LM.  1  Accepted as workshop contribution at ICLR 2015  Table 1: This table shows the test set perplexities of several LMs on the Penn Treebank test set. A (X,Y) N-gram NNLM projects the one-hot vector5of the previous N-1 words onto an X-dimensional linear sub-space. It then inputs this projected vector into a NN with one hidden layer with Y neu- rons that outputs the posterior probability of all words in the vocabulary. K NN-RandInit denotes interpolation of K randomly initialized and independently-trained NNLMs. The last column shows the average correlation coefﬁcient between posteriors of the test set words estimated by the LMs in the ensemble.  LM Name  4-gram 1 NN  4 NN-RandInit  4-gram + 1 NN  4-gram + 4 NN-RandInit  Description  Perplexity  Posterior Corr. Coeff.  4-gram Kneser-Ney smoothed  1x(600,800) 4gm NN-LM  Randomly initialized  4x(150,200) 4gm NN-LMs  Interpolation Interpolation  142.04  140.06 (-1.4%) 134.80 (-5.1%)  116.33 (-18.1%) 116.17 (-18.2%)  N/A N/A 0.99  0.87 0.88  Random initialization and modifying the neural net topology in terms of the embedding and hidden layer size can be used to build diverse NNLM models. As we can see from Table 1, 4 randomly ini- tialized NNLMs (4 NN-RandInit) when fused together provide a 5% improvement in perplexity over the baseline. Recurrent NNLM models of different topologies can be fused to get signiﬁcant gains as well as demonstrated in (Mikolov et al. (2011)). The remarkable beneﬁt of such simple diversity- promoting strategies leads us to the central question of this paper - Is there a way to explicitly enforce diversity during NNLM model training? We show that modifying the NNLM architecture and aug- menting the training loss function achieves that. The fact that a NNLM learns a low-dimensional continuous space embedding of input words motivates the architecture and training of our proposed model - Diverse Embedding Neural Network (DENN) LM. We ﬁrst give an overview of conventional NNLMs in the next section. Section 3 presents the DENNLM architecture and its training loss function. We presents experiments and results in Sec- tion 4 and conclude the paper in Section 5.  2 FEED-FORWARD NEURAL NETWORK LANGUAGE MODEL (NNLM)  A feed-forward neural network LM (NNLM) converts the one-hot encoding of each word in the history to a continuous low-dimensional vector representation (Bengio et al. (2006)). Figure 1 shows the schematic diagram of a typical NNLM. Let wi, . . . , wi−N +1 denote the 1-in-V vectors of the history words. Let RH denote the D × V matrix that projects a history word vectors onto D- dimensional vectors RH wi, . . . , RH wi−N +1. D is typically much smaller than the size V of the vocabulary with typical values being V = 10, 000 and D = 500. This resulting (N − 1)D-dimensional continuous representation of input words feeds into a neural network with one hidden layer that estimates the 1-in-V vector of the target word wi. The hid- den neuron activation function is hyperbolic tangent or logistic sigmoid while the output neuron activation function is a V -way softmax. The back-propagation algorithm (Rumelhart et al. (1986)) trains an NNLM, often using stochastic gradient descent where the gradient is computed over several random subsets or batches of N-grams from the input training data. Researchers have proposed several variants of the feed forward NNLM, especially to model a longer input word history. Prominent examples include the recurrent neural network LM (Mikolov et al. (2010)) and bidirectional long-short term memory (LSTM) LM (Sundermeyer et al. (2012)). These models offer improved performance but can be slower and more difﬁcult to train. Even though we restrict our attention to feed forward NNLMs in this paper, the general principles proposed are applicable to other NNLM architectures as well. The next section introduces the diverse embedding NNLM (DENNLM).  5A one-hot vector of the i-th word in the vocabulary contains 1 at index i and 0 everywhere else.  2  Accepted as workshop contribution at ICLR 2015  Figure 1: This ﬁgure shows the schematic diagram of a 3-gram NNLM. The matrix RH projects the input history word vectors onto a continuous space. These representations then pass through a NN with one hidden layer to predict the next word.  3 DIVERSE EMBEDDING NNLM  A diverse embedding NNLM (DENNLM) aims to learn multiple diverse representations of the in- put words rather than a single representation. It is ﬁrst important to understand the intuition of a representation in the context of a NNLM. Given a set of N input word vectors wi−1, . . . , wi−N +1, consider the set of D-dimensional vectors RH wi−1, . . . , RH wi−N +1. The pairwise distances be- tween vectors of this set constitute the representation of the input words. A good representation captures the contextual and semantic similarity between pairs of words. Similar words are located close to each other in representation while dissimilar words are located far apart. A NNLM uses this representation of the input words to predict the next word. The most natural way to ensure diversity of two NNLMs is through the diversity in the representation itself (representational diversity). The next section discusses an intuitive score to capture representational diversity.  3.1 DIVERSITY BETWEEN NNLM EMBEDDINGS  Maximizing the representational diversity between two NNLMs ﬁrst requires an objective score to capture this diversity. Consider the representation RH wi−1, . . . , RH wi−N +1 of the N − 1 input words to the NNLM. Since this representation lies in a D-dimensional Euclidean space, the set of all (N − 1)(N − 2)/2 pairwise angles between N − 1 words are sufﬁcient to uniquely deﬁne the representation even under any afﬁne transformation such as translation, rotation, and scaling. The matrix of pairwise cosine angles between all pairs of points in the representation deﬁned by RH1 is    C(RH1) =  1  wT  i−2RT  H1RH1wi−1  ||RH1wi−2|| ||RH1wi−2||  ...  wT  i−N +1RT  H1RH1wi−1  ||RH1wi−N +1|| ||RH1wi−1||  . . .  . . . ... . . .  wT  i−1RT  H1RH1wi−N +1  ||RH1wi−1|| ||RH1wi−N +1|| ||RH1wi−2|| ||RH1wi−N +1||  H1RH1wi−N +1  i−2RT  wT  ...  1   .  This matrix completely deﬁnes the representation of the word history produced by RH. It is also independent of the dimensionality of the representation, i.e. the number of rows of RH. This is useful when comparing two representations with different dimensionality of the same set of input data points. Given two such representations computed using matrices RH1 and RH2, we deﬁne the  3  Accepted as workshop contribution at ICLR 2015  (cid:16)  (cid:17)T  (cid:16)  (cid:17)  representational diversity as the negative correlation between cosine angles  dRep(C(RH1), C(RH2)) = −vec  C(RH1)  vec  C(RH2)  (3)  across the two representations, where vec raster-scans a matrix into a column vector. We note that dRep is bounded because the cosine is bounded between −1 and 1. Representational diversity between RH1 and RH2 increases as dRep decreases. In our implementation of distance diversity, for computation efﬁciency reasons we consider distances over a randomly chosen set of 500 words in each minibatch instead of the full vocabulary. Our experiments show that using the entire vocabulary for diversity computation gave only minor improvements in perplexity at the expense of much longer training time. We are currently exploring several other potential ways to compute diversity between two NNLMs beyond the representational diversity score presented in this paper. The next section discusses the DENNLM architecture and training loss function.  3.2 DENNLM ARCHITECTURE AND TRAINING LOSS FUNCTION  As discussed earlier, a DENNLM attempts to learn multiple diverse low-dimensional representations of the input word history. Figure 2 shows the schematic diagram of a DENNLM with two diverse representations. The two representations pass through two separate NNs and produce separate pre- dictions of the next word. The model merges the two predictions to produce the ﬁnal prediction.  Figure 2: This ﬁgure shows the schematic diagram of a 3-gram DENNLM with two diverse repre- sentations.  It is important to note that the DENNLM is equivalent to a single NNLM with the following block- structured representation matrix   R1H (cid:19)  R2H  0   (cid:18) R1P  0  RH =  0 R1H  0 R2H  (cid:18) W1  W =  0 0 W2  RP =  0 0 R2P  (cid:19)  .  and block-diagonal connection weight matrices  The presence of zero entries in the above matrices implies that a DENNLM has a smaller number of parameters than a comparable NNLM. The equivalence between a DENNLM and a conventional NNLM makes the implementation of a DENNLM easy within conventional NNLM toolkits.  4  Accepted as workshop contribution at ICLR 2015  Merely constraining the architecture of a NNLM does not guarantee that it learns multiple diverse representations. Hence we augment the training loss function of a conventional NNLM with addi- tional terms to promote diversity. We use the negative log-likelihood or cross entropy for V -way classiﬁcation as the loss function for a conventional NNLM. A DENNLM instead uses the following augmented loss function:  T(cid:88)  i=1  (cid:16) M(cid:88) M(cid:88)  m=1  log  T(cid:88)  (cid:16)  LDENNLM = −β  αmpm(wi|wi−1, . . . , wi−N +1)  − (1 − β) − γdRep(C(RH1), . . . , C(RHM ))  αm log  m=1  i=1  pm(wi|wi−1, . . . , wi−N +1)  (cid:17)  (cid:17)  (4)  The ﬁrst term of the above loss function is a mixture model loss that ensures that the fused pre- diction is accurate. Using only the ﬁrst term trains a mixture of neural networks. However, this will not ensure high discrimination ability of the representations learned by the individual networks because the different representations will only capture the modes of the data distribution. We thus include the second term, motivated by a recent work on deeply supervised neural networks (Lee et al. (2014)), where the authors augment the conventional loss at the ﬁnal layer with discriminative loses computed at previous layers. The second term in (4) plays a similar role and makes the individual representations discriminative as well. The ﬁnal term is the representational diversity of the NNLM as described in Section 3.1. Minimizing the DENNLM loss function in (4) gives representations that are jointly discriminative, individually discriminative, and diverse. We can add L1 and/or L2 regularization penalties to the loss function as well, which are often useful to prevent over-ﬁtting in case the network size is large compared to the number of training set N- grams. The next section discusses the experimental setup and results.  4 EXPERIMENTS AND RESULTS  We conducted language modeling experiments on a standard split of the Penn Treebank (UPenn) data set (Marcus et al. (1993)), as used in previous works such as (Mikolov et al. (2011)). The UPenn data set has a vocabulary of 10K words, and contains approximately 1M N-grams in the training set. We used the UPenn data set since it is well-studied for language modeling and also small enough to conduct several experiments for understanding the DENNLM model better. We implemented the DENNLM in Theano (Bergstra et al. (2011)) and trained it by optimizing the augmented loss function in (4) using Root-Mean Square Propagation (RMSProp) (Tieleman & Hinton (2012)), a variant of stochastic gradient descent. We tuned all hyper-parameters on the standard UPenn held- out set. Table 4 shows the test set perplexities of the baseline and the DENNLMs. We kept the NN model size comparable by reducing the size of each component NNLM. Our results show a signiﬁcant improvement in perplexity by using a DENNLM over the 4-gram model, a single NNLM, and in- terpolation of randomly initialized NNLMs. The posterior correlation coefﬁcients are signiﬁcantly less than 0.99, which is the correlation coefﬁcient for randomly initialized NNLMs. Note that the DENNLM signiﬁcantly outperforms a standard NNLM of size (600,800) which has similar number of parameters. It is also clearly better then a randomly initialized set of 4 NNLM models. The perplexity results in Table 4 for our diverse feed-forward NNLMs are especially encour- aging given the fact that the more advanced recurrent neural network (RNN) LM gives a perplexity of 124.7 by itself and 105.7 upon interpolation with a 5-gram LM on the same task (Mikolov et al. (2011)).  4.1 SENSITIVITY TO HYPERPARAMETERS  We further studied the dependence of DENNLM performance on some sets of hyper-parameters and list our observations below:  5  Accepted as workshop contribution at ICLR 2015  Table 2: This table shows the test set perplexities of baseline and DENNLMs on the Penn Treebank test set.  LM Name  Description  Perplexity  Posterior Corr. Coeff.  4-gram 1 NN  4 NN-RandInit  4 DENN  8 DENN  4-gram + 1 NN  4-gram + 4 DENN 4-gram + 8 DENN  4-gram Kneser-Ney smoothed  1x(600,800) 4gm NN-LM  Randomly initialized  4x(150,200) 4gm NN-LMs  Diversely trained  4x(150,200) 4gm NN-LMs  Diversely trained  8x(75,100) 4gm NN-LMs  Interpolation Interpolation Interpolation  142.04  137.32 (-3.3%) 134.85 (-5.1%)  122.69 (-13.6%)  116.41 (-18.0%)  116.33 (-18.1%) 112.79 (-20.6%) 109.32 (-23.0%)  N/A N/A 0.99  0.89  0.83  0.87 0.88 0.92  • Diversity parameters: The performance was fairly stable over a range of β between 0.3 to 0.7 with minor differences of around 2 points in perplexity. The performance with respect to diversity weight γ was also stable in the range 1 to 3 • Model parameters: We explored different model sizes for both the single model and DENNLM by changing the DENNLM topology (number of hidden neurons, layers etc.). We observed that in general, a DENNLM with the same total size as a single NNLM works well. We can achieve smaller gains of around 3 to 4 points in perplexity by increasing the size of the DENNLM. • Optimization parameters: We observed that the choice of the optimization algorithm can have a signiﬁcant impact on the diversity of models generated with random initialization. We found that using RMSProp with a signiﬁcantly higher learning rate, gradient clipping, and scaling of bias updates while training four randomly-initialized NNLMs leads to a lower posterior posterior correlation of 0.95 compared to 0.99 with our standard optimiza- tion setting. This higher diversity between models translates to a lower perplexity of 120 with the interpolated NNLM which is comparable to our best results in Table 4. This indi- cates that the choice of hyper-parameters and optimizer settings for building diverse models via random initialization can be different from the ones used for training the models indi- vidually.  To further understand the impact of hyper-parameters on DENNLM diversity and perplexity, we performed a thorough grid search of β and γ in (4), the learning rate of RMSProp, and the weight of an L2 penalty on the DENNLM connection weights. We then computed the log perplexity and av- erage cross-correlation between posteriors of individual models in a DENNLM. Figure 3 shows the scatter plot between average posterior cross-correlation and percent improvement of the DENNLM log perplexity over the best model’s perplexity. A strong negative correlation of −0.97 in this ﬁgure shows that more diverse models give a bigger improvement in log perplexity upon interpolation. This highlights the merit of training diverse NNLM and the fact that one can achieve this diver- sity by either informed objective functions such as the one in (4) or an exhaustive hyper-parameter search. The latter becomes especially tedious for deep and complex neural networks trained on big data sets.  5 CONCLUSION  In this work, we introduced a neural network architecture and training objective function that en- courages the individual models to be diverse in terms of their output distribution as well as the underlying word representations. We demonstrated its usefulness on the well-studied UPenn lan- guage modeling task. The proposed training criterion is general enough and does not constrain the NNLM models to be of any speciﬁc architecture. Given the promising results, our next step is to evaluate the improved language models on speech recognition and spoken-term detection tasks. We  6  Accepted as workshop contribution at ICLR 2015  Figure 3: This ﬁgure shows the scatter plot between average posterior cross-correlation and percent improvement of the DENNLM log perplexity over the best model’s log perplexity. We observed a strong correlation coefﬁcient of -0.97 between the two variables indicating that more diverse NNLMs give a bigger reduction in perplexity upon interpolation.  also plan to explore the utility of these diverse representations to measure semantic similarity and for sentence completion, where word embedding-based models have been shown to be effective.  6 ACKNOWLEDGEMENT  This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via De- partment of Defense U.S. Army Research Laboratory (DoD / ARL) contract number W911NF-12- C-0012. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government.  REFERENCES Bengio, Y., Schwenk, H., Sen´ecal, J., Morin, F., and Gauvain, J. Neural probabilistic language  models. In Innovations in Machine Learning, pp. 137–186. Springer, 2006.  Bergstra, J., Bastien, F., Breuleux, O., Lamblin, P., Pascanu, R., Delalleau, O., Desjardins, G., Warde-Farley, D., Goodfellow, I. J., Bergeron, A., and Bengio, Y. Theano: Deep learning on gpus with python. In Big Learn workshop, NIPS’11, 2011.  Goodman, J. T. A bit of progress in language modeling. Technical report, 2001.  Kneser, R. and Ney, H. Improved backing-off for m-gram language modeling. In Proc. ICASSP,  volume 1, pp. 181–184. IEEE, 1995.  Kuncheva, L. I. Combining pattern classiﬁers: methods and algorithms. John Wiley & Sons, 2004.  Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., and Tu, Z. Deeply-Supervised Nets. ArXiv e-prints,  September 2014.  Marcus, M. P., Marcinkiewicz, M. A., and Santorini, B. Building a large annotated corpus of En-  glish: The Penn Treebank. Computational linguistics, 19(2):313–330, 1993.  7  Accepted as workshop contribution at ICLR 2015  Mikolov, T., Karaﬁ´at, M., Burget, L., Cernock`y, J., and Khudanpur, S. Recurrent neural network  based language model. In Interspeech, pp. 1045–1048, 2010.  Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernock`y, J. Empirical evaluation and  combination of advanced language modeling techniques. In Interspeech, pp. 605–608, 2011.  Rosenﬁeld, R. Two decades of statistical language modeling: Where do we go from here? Proceed-  ings of the IEEE, 88, 2000.  Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning representations by back-propagating  errors. Nature, 323:533–536, 1986.  Sundermeyer, M., Schl¨uter, R., and Ney, H. LSTM neural networks for language modeling.  Interspeech, 2012.  In  Tieleman, T. and Hinton, G. Lecture 6.5—RmsProp: Divide the gradient by a running average of its  recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  8  ",
1412.6599,2015, Hot Swapping for Online Adaptation of Optimization Hyperparameters,"['Hot Swapping for Online Adaptation of Optimization Hyperparameters', 'Kevin Bache', 'Dennis Decoste', 'and Padhraic Smyth']",https://arxiv.org/pdf/1412.6599,"5 1 0 2    r p A 3 1         ]  G L . s c [      3 v 9 9 5 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  HOT SWAPPING FOR ONLINE ADAPTATION OF OPTIMIZATION HYPERPARAMETERS  Kevin M. Bache & Padhraic Smyth Department of Computer Science University of California, Irvine Irvine, CA 92697, USA {kbache,smyth}@ics.uci.edu  Dennis DeCoste Machine Learning Group eBay Research Labs San Jose, CA 95125, USA ddecoste@ebay.com  ABSTRACT  We describe a general framework for online adaptation of optimization hyper- parameters by ‘hot swapping’ their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.  1  INTRODUCTION  In this paper, we introduce a new stochastic gradient method with adaptive learning rate selection based on the insight that optimization hyperparameters may be freely ‘hot swapped’ in the middle of the learning process1. Where existing adaptive learning rate algorithms are based on running curvature estimates of the local loss surface (Schaul et al., 2012; Zeiler, 2012), we present a procedure which recasts learning rate selection as an explore-exploit problem which can be addressed using existing solutions to multi-armed bandit problems. This method is straightforward to implement, retains the runtime characteristics and memory footprint of stochastic gradient, and outperforms existing methods on a common benchmark task.  2 ALGORITHM  The basis of the proposed algorithm is the observation that optimization hyperparameters such as learning rate and momentum may be freely ‘hot swapped’ during optimization runs. This is in contrast to model hyperparameters, such as hidden layer size or unit type, which cannot be changed so easily during learning. This approach can also be contrasted to traditional hyperparameter search strategies such as grid search, random search, or Bayesian optimization which set optimization hyperparameters in an outer loop and treat learning as an inner loop (Bergstra and Bengio, 2012; Snoek et al., 2012). Instead, we propose to observe the optimization process under a variety of hyperparameter settings and to preferentially continue to use those settings which have performed best in the past. We do this 1The notion of model-based hot swapping of algorithms or their parameters has been previously considered the study of the ‘dynamic algorithm selection’ or ‘dynamic algorithm conﬁguration’ problems (Gagliolo and Schmidhuber, 2006), of which learning rate selection may be considered a speciﬁc instantiation. Most past work in this area has been focused on combinatorial optimization problems, where here we consider a continuous optimization problem.  1  Accepted as a workshop contribution at ICLR 2015  by maintaining a meta-model of hyperparameter performance. The general hot swapped optimization procedure is deﬁned in algorithm 1. Many meta-models may work well for this task, but in this work we cast the problem of learning rate selection into an explore-exploit framework and choose a discounted upper conﬁdence bound (DUCB) model for hyper-parameter selection. In brief, we seek an algorithm that ‘explores’ the space of possible learning rates—in order to learn which ones perform best on the given problem—while reserving most of its time to ‘exploit’ the best performing rates—by repeatedly using them to update model parameters. The upper conﬁdence bound algorithm is a common choice for tackling explore- exploit problems, and its discounted form achieves the optimal regret bound up to a logaritmic factor for rapidly shifting reward distributions (Garivier and Moulines, 2008). The procedure for hot swapped optimization with a DUCB model is listed in algorithm 2 with the full details given in algorithm 3. We assume that we have a ﬁnite set of learning rates to select from, (cid:126)α ≡ {α1, ..., αK}, a postive objective function to be minimized f ((cid:126)θ; B), along with its gradient g((cid:126)θ; B), both of which can be evaluated at a point in parameter space (cid:126)θ for a given data batch B. We deﬁne the ‘reward’ granted to a given learning rate αk as r = log(f ((cid:126)θ0; B)) − log(f ((cid:126)θk; B)), where (cid:126)θ0 represents the (non-hyper) parameters at the beggining of the current iteration and (cid:126)θk ≡ (cid:126)θ0−αkg((cid:126)θ0, B) represents the parameters obtained by choosing learing rate αk. We use a logarithmic scaling of the rewards which treats multiplicative reductions of the objective function f as equally valuable, a useful feature given the exponential slowdown of optimization progress which is often observed in practice. The α value chosen at each step is selected by the DUCB algorithm in the usual way (see the function GETDUCBSUGGESTEDINDEX in algorithm 3 for details). The DUCB model will periodically seek to explore different learning rates as the optimization procedure progresses. This introduces the potential to take catastrophically large steps which could discard progress made up to the current time. To prevent this, we perform a line search across learning rates to ﬁnd the best learning rate value for each minibatch. The line search starts from the learning rate proposed by the DUCB algorithm and decreases through the other available learning rates until it ﬁnds one which lowers the current minibatch’s objective function value below the value it held before the current update. Because the line search is only performed on the current minibatch of data, it still takes linear time in batch size and problem dimension, just like simple SGD2. In practice, these two processes work well together. The line search prevents the bandit algorithm’s tendency to explore catastrophically large step sizes, while the bandit algorithm’s carefully chosen initial step sizes reduce the number of minibatch objective function evaluations from the large number required by a vanilla minibatch line search to a much smaller quantity.  3  INITIAL RESULTS  We test the efﬁcacy of this procedure3 on a neural network based on the MNIST dataset. MNIST is comprised of 60,000 28x28 pixel black and white images of handwritten digits. The task is to classify each image as a number ‘0’ through ‘9’. We show results from fully connected feed-forward network with 500 sigmoidal units in the ﬁrst hidden layer, 300 sigmoidal hidden units in the second hidden layer, and a ﬁnal 10-way softmax output. We trained on the ﬁrst 50,000 images of the training set. We perform an exhaustive search of SGD hyperparameters for comparison. We consider all combi- nations of the following parameters: initial learning rates in α0 ∈ {1.0, 0.3, 0.1, 0.03, 0.01, 0.003}, per-epoch learning rate multipliers of η ∈ {0.99, 0.995, 1.0}, momentum coefﬁcients µ ∈ {0.0, 0.5, 0.7, 0.9} and batch sizes ∈ {64, 128, 256, 512, 1024} for a total of 360 SGD settings.  2Minibatch line search is not without precedent (Ngiam et al., 2011; Roux et al., 2012), though it has received  3This experiment was conducted using Theano, PyLearn2, and a cluster of computers with NVIDIA GRID  little direct attention in the past  K520 GPUs  2  Accepted as a workshop contribution at ICLR 2015  We also compare against AdaDelta, another widely used adaptive learning rate algorithm, using the hyperparameters described in Zeiler (2012) for MNIST: (cid:15) = 10−6 and decay rate factor of 0.95 across batch sizes ∈ {64, 128, 256, 512, 1024}. Finally, we tested hot swapped optimization with a DUCB step size model and batch sizes ∈ {64, 128, 256, 512, 1024}4. We ran each algorithm for 500 epochs using three random weight initializations. Figure 1 shows the spread of training and test performances of all 380 algorithms after 200 and 500 training epochs. Each dot represents the median performance of a single set of hyperparameters with error bars indicating min and max performance across initializations. The best performing algorithms will be in the bottom left. At convergence, all 15 hot swap DUCB algorithms (red) achieve the lowest training objective (negative log likelihood) of every other algorithm tested (green and blue) Furthermore, while test performance isn’t the direct target of an optimization process—test perfor- mance is also heavily linked to regularization quality, a factor to which optimization algorithms are agnostic—the DUCB algorithm obtained the lowest median test error rate across all of its vari- ations (1.85% vs 1.97% for AdaDelta and 2.35% for SGD) and the best single test performance overall (1.63% error rate), despite the fact that SGD instantiations outnumbered hot swapped DUCB instantiations by a factor of 75:1. The performance of SGD algorithms vary widely across hyperparameter settings. The AdaDelta algorithms consistently exhibit performance in the top 30% of the SGD algorithms, through as with hot swapped DUCB, they vary across batch size.  (a) Epoch 200  (b) Epoch 500  Figure 1: Best viewed in color. Training objective (training set negative log likelihood) and test performance (test set misclassiﬁcation rate) for various algorithms on MNIST 784-500-300-10 with sigmoidal units. Each dot represents one algorithm with one set of hyperparameters. The best performing algorithms are in the bottom left. The only parameter varied for DUCB and AdaDelta are batch size. SGD algorithms were varied across initial learning rate, learning rate decrease schedule, momentum and batch size. Error bars represent performance of a single algorithm and hyperparameter set across 3 random weight initializations. All 15 instantiations of the hot swap DUCB achieve better training likelihood than all 1125 other algorithms we compare against.  Figure 1 shows the variations in performance of different algorithms after a ﬁxed number of training epochs, but obscures the differences in the time it takes each algorithm to complete one training epoch. Speciﬁcally, algorithms with small batch sizes are slower because they need to perform more updates per epoch than algorithms with large batch sizes, and the DUCB algorithms are slower per iteration than the competing algorithms (see below for details). Figure 2 gives a direct comparison of training time vs. various measures of performance for one instantiation each of the ﬁve DUCB hot swapping algorithms (one for each batch size), the ﬁve 4It’s worth noting that batch size is the one signiﬁcant hyperparameter for the hot swapping DUCB algorithm. It is similar in this regard to other adaptive learning rate methods such as AdaDelta and No More Pesky Learning Rates.  3  Accepted as a workshop contribution at ICLR 2015  Figure 2: Best viewed in color. Performance of hot swapping with DUCB model, AdaDelta, and the two best-performing SGD algorithms (out of a total of 1080) on MNIST 784-500-300-10 with sigmoidal units.  AdaDelta algorithms, and the two best-performing SGD algorithms (chosen retrospectivlye out of a total of 1080 SGD algorithms; ‘best performing’ measured in ﬁnal training objective and test misclassiﬁcation). The top plot in ﬁgure 2 shows test error rate vs. wall clock time. Despite the fact that DUCB hot swapping with a batch size of 64 takes the most time per iteration, it makes sufﬁcient progress in each step that by 1000 seconds into the training run it has attained the best test error of any algorithm we tested5. The second plot shows training objective over time (training set negative log likelihood), with all of the DUCB algorithms making signiﬁcant training progress well after the SGD and AdaDelta algorithms have plateaued and ﬁnding lower optima. The third plot suggests that the DUCB algorithms naturally learn to decrease their learning rates as they near local optima6. The fourth plot shows a running average of gradient norms over time, with the DUCB algorithms all ﬁnding ﬂatter optima than competing algorithms. On other problems in which we have tested the hot swapping DUCB procedure, we have observed similarly strong training performance with greater variation on test performance that we observed here. This suggests that the hot swapped DUCB procedure is a strong optimization algorithm that requires similarly strong regularization not to overﬁt.  5A single iteration of DUCB hot swapping takes between 1.6 and 3.6 times as long as a comparable iteration of SGD or AdaDelta because it performs several line search iterations per step. However, it makes considerably greater progress per iteration than SGD and continues to do so well after SGD and AdaDelta have plateaued, converging to better optima than the existing algorithms. For additional details on the relative timing of each method, see Appendix 1: Timing.  6The AdaDelta algorithms are not included in the learning rate plots because AdaDelta maintains a different  learning rate for each paramter.  4  Accepted as a workshop contribution at ICLR 2015  4 CONCLUSION  We have introduced a new adaptive learning rate algorithm for stochastic gradient that is built to hot swap an optimization hyperparameter over the course of a learning run. Preliminary results indicate that the proposed method consistently outperforms competing methods on several measures. Numerous extensions of this basic procedure are possible, including using different meta-models, swapping new optimization or regularization hyper parameters, swapping multiple parameters at once, and reducing the frequency of line search to speed performance. We are currently working to test this and other hot swapping procedures on a wide variety of problems.  ACKNOWLEDGMENTS  This material is based upon work partially supported by a National Science Foundation Graduate Fellowship (KB), by NSF via award number IIS-1320527 (PS), and by the Ofﬁce of Naval Research under MURI grant N00014-08-1-1015 (PS). A portion of this work was performed by KB while a summer intern at eBay Research Labs.  REFERENCES Bergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization. The Journal of  Machine Learning Research, 13(1):281–305.  Gagliolo, M. and Schmidhuber, J. (2006). Learning dynamic algorithm portfolios. Annals of  Mathematics and Artiﬁcial Intelligence, 47(3-4):295–328.  Garivier, A. and Moulines, E. (2008). On upper-conﬁdence bound policies for non-stationary bandit  problems. arXiv:0805.3415 [math, stat]. arXiv: 0805.3415.  Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., Le, Q. V., and Ng, A. Y. (2011). On optimization methods for deep learning. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 265–272.  Roux, N. L., Schmidt, M., and Bach, F. R. (2012). A stochastic gradient method with an exponential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems, pages 2663–2671. 00059.  Schaul, T., Zhang, S., and LeCun, Y. (2012). No more pesky learning rates. arXiv preprint  arXiv:1206.1106.  Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical bayesian optimization of machine learning algorithms. In Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q., editors, Advances in Neural Information Processing Systems 25, pages 2951–2959. Curran Associates, Inc.  Zeiler, M. D. (2012). ADADELTA: An adaptive learning rate method.  arXiv:1212.5701.  arXiv preprint  5  Accepted as a workshop contribution at ICLR 2015  APPENDIX 1: TIMING  This section contains details of the relative timings of SGD, AdaDelta, and the hot swapped DUCB algorithm for the model detailed in section 3. DUCB tends to require more line search iterations per step as it nears a local optima, meaning that it takes less time per minibatch earlier in the optimization process and more time per minibatch later in the optimizaion process. This effect is signiﬁcantly mitigated with larger batch sizes as they exhibit less variance across minibatches. This helps the DUCB model to predict which step size will perform optimally for a given problem, which limits the number of line search iterations required per minibatch and yields epoch timings that are closer to SGD and AdaDelta. Overall, hot swapped DUCB takes between 1.6 and 3.6 times as long per minibatch as SGD and AdaDelta, however it makes more progress per iteration than either of these competing algorithms and converges to better optima (see ﬁgure 2 and the discussion in section 3).  Batch Size:  64: 128: 256: 512: 1024:  Milliseconds per Minibatch  SGD AdaDelta DUCB, epoch 100 DUCB, epoch 300 DUCB, epoch 500 11.9 42.4 43.7 13.0 49.2 16.8 56.2 24.7 39.2 63.3  12.1 13.2 16.7 25.5 40.8  30.1 20.3 26.4 37.9 60.2  40.2 38.8 39.7 38.2 60.4  Table 1: Milliseconds per minibatch for SGD, AdaDelta, and the hot swapped DUCB algorithm. Timing varies over the course of the optimization run for the hot swapped DUCB algorithm, and so its average timing is listed after 100, 300, and 500 epochs.  Batch Size:  64: 128: 256: 512: 1024:  Seconds per Epoch  SGD AdaDelta DUCB, epoch 100 DUCB, epoch 300 DUCB, epoch 500 33.2 17.1 9.6 5.5 3.1  23.5 7.9 5.2 3.7 3.0  31.4 15.2 7.8 3.7 3.0  9.3 5.1 3.3 2.4 1.9  9.4 5.2 3.3 2.5 2.0  Table 2: Seconds per epoch for SGD, AdaDelta, and the hot swapped DUCB algorithm. Timing varies over the course of the optimization run for the hot swapped DUCB algorithm, and so its average timing is listed after 100, 300, and 500 epochs.  6  Accepted as a workshop contribution at ICLR 2015  APPENDIX 2: FULL ALGORITHM DESCRIPTION  Algorithm 1 General hot swapped optimization Require: (cid:126)θ, the parameters to be optimized, X, a dataset which may be broken into batches denoted B, f (θ; X) the objevtive function to be optimized, A, a set of optimization hyperparameter values to consider, M, some model of optimization hyperparameter performance, U (θ; B, α), an update step for the parameters (cid:126)θ given a batch of data points and optimization hyperparameter value, a convergence criteria B ← a new batch of data α ← the best optimization hyperparameters α ∈ A as judged by M θ ← U (θ; B, α) M observes performance of α  1: while not converged do 2: 3: 4: 5: 6: end while  Algorithm 2 Hot Swapped Stochastic Optimization with DUCB model Require: (cid:126)θ, f, g, (cid:126)α array, a dataset, convergence criteia 1: maxIndex ← maximum index in the αs array 2: rewards ← array of 0s of same length as (cid:126)α 3: counts ← array of 0s of same length as (cid:126)α 4: t ← −1 5: while not converged do 6: 7: 8: 9: 10: end while  t ← t + 1 B ← new batch of data startIndex ← INITIALALPHAINDEX(rewards, counts, t, maxIndex) θ ←BACKTRACKINGLINESEARCHWITHREWARDS(f, g, B, θ, (cid:126)α, startIndex)  7  Accepted as a workshop contribution at ICLR 2015  if t < maxIndex then return t elsereturn GETDUCBINDEX(rewards, counts) end if  conf Intervals ←(cid:112)exploreConst ∗ log(n)/counts  maxIndex ← the maximum index in the (cid:126)α array fstart ← f ((cid:126)θ; B) fcurrent ← fstart fbest ← fstart αbest ← startIndex haveF oundBetterT hanStart ← F alse for all index ← startIndex : maxIndex do  rewards ← γ ∗ rewards counts ← γ ∗ counts means ← rewards/counts n ← sum(counts) ucbs ← means + conf Intervals return arg max(ucbs)  Algorithm 3 DUCB Helper Functions 1: 2: function INITIALALPHAINDEX(rewards, counts, t, maxIndex) 3: 4: 5: 6: end function 7: 8: function GETDUCBSUGGESTEDINDEX(rewards, counts) 9: 10: 11: 12: 13: 14: 15: end function 16: 17: function BACKTRACKINGLINESEARCHWITHREWARDS(f, g, B, (cid:126)θ, (cid:126)α, startIndex) 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: end function 40: 41: function OBJECTIVEATALPHA((cid:126)θ, f, g, B, α) return f ((cid:126)θ − αg((cid:126)θ; B); B) 42: end function 43: 44: function GRANTREWARD(index, rewards, counts, fstart, fcurrent) rewards[index] ← rewards[index] + log(fstart) − log(fcurrent) 45: counts[index] ← counts[index] + 1 46: 47: end function  fprev ← fcurrent α ← (cid:126)α[index] fcurrent ← OBJECTIVEATALPHA((cid:126)θ, f, g, B, α) GRANTREWARD(index, rewards, counts, fstart, fcurrent) if fcurrent < fbest then  fbest ← fcurrent αbest ← α haveF oundBetterT hanStart ← True  end if if haveF oundBetterT hanStart and fcurrent > fprev then  end for (cid:126)θ ← (cid:126)θ − αbestg((cid:126)θ; B) return (cid:126)θ  Break Loop  end if  8  ",
1412.7156,2015, Representation Learning for cold-start recommendation,"['Representation Learning for cold-start recommendation', 'Gabriella Contardo', 'Ludovic Denoyer', 'and Thierry Artieres']",https://arxiv.org/pdf/1412.7156,"5 1 0 2    n u J    2 2      ]  R  I . s c [      5 v 6 5 1 7  .  2 1 4 1 : v i X r a  Accepted as workshop contribution at ICLR 2015  REPRESENTATION LEARNING FOR COLD-START RECOMMENDATION  Gabriella Contardo∗, Ludovic Denoyer∗,Thierry Arti`eresφ ∗ Sorbonne Universit´es, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, Paris, France φ Ecole Centrale Marseille - Laboratoire d’Informatique Fondamentale (Aix-Marseille Univ.), France  ABSTRACT  A standard approach to Collaborative Filtering (CF), i.e. prediction of user rat- ings on items, relies on Matrix Factorization techniques. Representations for both users and items are computed from the observed ratings and used for prediction. Unfortunatly, these transductive approaches cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few ini- tialization ratings. This paper presents a model to tackle this twofold problem of (i) ﬁnding good questions to ask, (ii) building efﬁcient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets showing its ability to improve baseline performance in both cases.  1  INTRODUCTION  Most of the successful machine learning algorithms rely on data representation, i.e a way to disen- tangle and extract useful information from the data, which will help the model in its objective task. As highlighted by Bengio et al. (2013), designing models able to learn these representations from (raw) data instead of manual pre-processing seems crucial to go further in Artiﬁcial Intelligence, and representation learning has gain a surge of interest in machine learning. In parallel, recommender systems have became an active ﬁeld of research and are now used in an increasing variety of appli- cations, such as e-commerce, social networks or participative platforms. They aim to suggest the most relevant items (e.g products) to each user, in order to facilitate their experience. To recommend such relevant items, recommender systems can rely on different types of data, such as users’ explicit and/or implicit feedbacks (e.g rating a movie on a scale of stars, buying an item or listening to a song), or informative features about users (age, post code) or items (type of movie, actors). One of the most common approach to recommendation is Collaborative Filtering (CF) which consists in making recommendation only based on the ratings provided by users over a set of items (i.e without using any additional features). Within CF context, a popular and efﬁcient family of methods are Latent Factor Models, which rely on matrix factorization-based techniques 1. These approaches treat the recommender problem as a representation learning one, by computing representations for users and items in a common latent space. More formally, let us consider a set U of U known users and a set I of I items. Let ru,i denote the rating of user u ∈ U for item i ∈ I. A rating is usually a discrete value between 1 and 5, that can be binarized (-1/1) with a proper threshold (often 3). The U × I matrix R = {ru,i} is the rating matrix which is incomplete since all ratings are not known. We will denote O the set of observed pairs (u, i) such that a rating on item i has been made by user u. Let us denote N the dimension of the latent representation space of users and items, pu ∈ RN being the (learned) representation of user u and qi ∈ RN denoting the (learned) representation of item i. Given these representations, classical approaches are able to compute missing ratings made by a user u over an item i as the dot product between pu and qi. In other words, the more similar the user and the item representations are, the higher the predicted rating will be. Let us denote ˜ru,i this predicted rating, we have:  ˜ru,i = qT  i pu  (1)  1Other families of approaches are detailed in Section 4.  1  Accepted as workshop contribution at ICLR 2015  The representation pu and qi are usually learned on the sparse input rating matrix R by minimizing an objective loss function over L(p, q) which measures the difference between observed ratings ru,i and predicted ones. p is the set of users representations, q being the representation of items. The loss is usually deﬁned as a L2 objective:  L(p, q) =  (ru,i − qT  i pu)2 + λ(  ||qi||2 +  ||pu||2)  (2)  (cid:88)  (cid:88)  i  u  (cid:88)  (u,i)∈O  The coefﬁcient λ is a manually deﬁned regularization coefﬁcient. This loss corresponds to a matrix decomposition in latent factors and different optimization algorithms have been proposed as alter- nated least squares or stochastic gradient descent (Koren et al. (2009)). Note that this models is a transductive model since it allows one to compute representations over a set of a priori known users and items. The transductive nature of Matrix Factorization approaches makes them well adapted when the sets of users and of items are ﬁxed. Yet in practical applications, new items and new users regularly appear in the system. This requires often retraining the whole system which is time consuming and also makes the system behavior unstable. Furthermore, one main limitation of transductive Matrix Factorization approaches is that they strongly rely on a certain amount of data to build relevant representations, e.g. one must have enough ratings from a new user to construct an accurate repre- sentation. Indeed, facing new users, MF methods (and more generally CF-based approaches) have to wait for this user to interact with the system and to provide ratings before being able to make recommendations for this user. These methods are thus not well-suited to propose recommendation at the beginning of the process. We propose to focus on the user cold-start problem2 by interview method which consists in build- ing a set of items on which ratings are asked to any new user. Then, recommendations are made based on this list of (incomplete) ratings. We consider a representation-learning approach which is an original approach in this context and which simultaneously learns which items to use in the interview, but also how to use these ratings for building relevant user representations. Our method is based on an inductive model whose principle is to code ratings on items as translations in the latent representation space, allowing to easily integrate different opinions at a low computational cost. The contributions of this paper are thus the following: (i) We propose a generic representation-learning formalism for user cold-start recommendation. This formalism integrates the representation build- ing function as part of the objective loss, and restriction over the number of items to consider in the interview process. (ii) We present a particular representation-learning model called Inductive Additive Model (IAM) which is based on simple assumptions about the nature of users’ represen- tations to build and that we are able to optimize using classical gradient-descent algorithms. (iii) We perform experiments on four datasets in the classical CF context as well as in the user cold-start con- text. Quantitative results show the effectiveness of our approach in both contexts while qualitative results show the relevancy of learned representations. The paper is organized as follow: in Section 2, we propose the generic formulation of the represen- tation learning problem for user cold-start, and the particular instance of model we propose. The Section 3 presents the experiments and Section 4 discusses the related work in the collaborative ﬁltering domain. Section 5 proposes perspectives to this contribution.  2 PROPOSED APPROACH  We now rewrite the objective function detailed in Equation 2 in a more general form that will allow us to integrate the user cold-start problem as a representation-learning problem. As seen above, we still consider that each item will have its own learned representation denoted qi ∈ RN and focus on building a user representation. When facing any new user, our model will ﬁrst collect a set of ratings by asking a set of queries during an interview process. This process is composed by a set of items3 that are selected during the training phase. For each item in the interview, the new user  2The integration of new items which is less critical in practical applications is not the focus of this paper but  is discussed in the conclusion.  3The article focuses on a static interview process i.e interview where the set of items is the same for all  incoming users. A discussion on that point is provided in Section 4  2  Accepted as workshop contribution at ICLR 2015  can provide a rating, but can also choose not to provide this rating when he has no opinion. This is typically the case for example with recommendation of movies, where users are only able to provide ratings on movies they have seen. The model will thus have to both select relevant items to include in the interview, but also to learn how (incomplete) collected ratings will be used to build a user representation. Let us denote Q ⊂ I the subset of items that will be used in the interview. The representation of a new incoming user u will thus depend on the ratings of u over Q that we will note Q(u). This representation will be given by a function fΨ(Q(u)) whose parameters, to be optimized, are denoted Ψ. These Ψ parameters are global, i.e shared by all users. The objective function of the cold-start problem (ﬁnding the parameters Ψ, the items’ representations and the interview questions conjointly) can then be written as: Lcold(q, Ψ,Q) = (ru,i−qT  ||fΨ(Q(u))||2)+λ2#Q (3)  i fΨ(Q(u)))2 +λ1(  ||qi||2 +  (cid:88)  (cid:88)  (cid:88)  (u,i)∈O  i  u  The difference between this loss and the classical CF loss is twofold: (i) ﬁrst, the learned represen- tations pu are not free parameters, but computed by using a parametric function fΨ(Q(u)), whose parameters Ψ are learned; (ii) the loss includes an additional term λ2#Q which measures the bal- ance between the quality of the prediction, and the size of the interview, #Q denoting the number of items of the interview; λ1 and λ2 are manually chosen hyper-parameters - by changing their values, the user can obtain more robust models, and models with more or less interview questions. Note that solving this problem aims at simultaneously learning the items representations, the set of items in the interview, and the parameters of the representation building function.  2.1  INDUCTIVE ADDITIVE MODEL (IAM)  The generic formulation presented above cannot easily be optimized with any representation func- tion. Particularly, the use of a transductive model in this context is not trivial and, when using MF-based approaches in that case, we only obtained very complex solutions with a high computa- tion complexity. We thus need to use a more appropriate representation-learning function fΨ that is described below. The Inductive Additive Model (IAM) is based on two simple ideas concerning the representation of users we want to build: (i) First, one has to be able to provide good recommenda- tion to any user that does not provide ratings during the interview process Q. (ii) Second we want the user representation to be easily enriched as new ratings are available. This feature makes our approach suitable for the particular cold-start setting but also for the standard CF setting as well. Based on the ﬁrst idea, IAM considers that any user without answers will be mapped to a represen- tation denoted Ψ0 ∈ RN . Moreover, the second idea naturally led us to build an additive model where a user representation is deﬁned as a sum of the particular items representations. This means that providing a rating will yield a translation of the user representation in the latent space. This translation will depend on the item i but also on the rating value. This translation will be learned for each possible rating value and item and denoted Ψr i where r is the value of the rating. More precisely, in case of binary ratings like and dislike, the like over a particular item will correspond to a particular translation Ψ+1 . The fact that the two rating values correspond to two different unrelated translations is interesting since, for some items, the dislike rating can provide no additional information represented by a null translation, while the like rating can be very informative, modifying the user representation - see Section 3 for a qualitative study over Ψ. The resulting model fΨ can thus be written as:  , and a dislike to the translation Ψ−1  i  i  fΨ(u,Q) = Ψ0 +  Ψru,i  i  (4)  (cid:88)  (u,i)∈O/i∈Q  where the set {(u, i) ∈ O/i ∈ Q} is the set of items selected in the interview on which user u has provided a rating.  2.1.1 CONTINUOUS LEARNING PROBLEM  Now, let us describe how the objective function described in Equation 3 with IAM model described in Equation 4 can be optimized. The optimization problem consisting in minimizing Lcold(q, Ψ,Q) over q, Ψ and Q is a combinatorial problem since Q is a subset of the items. This combinatorial nature prevents us from using classical optimization methods such as gradient-descent methods and  3  Accepted as workshop contribution at ICLR 2015  DataSet Users ML1M 5,954 Flixter 35,657 Jester 48,481 Yahoo 15,397  Items 2,955 10,247  100 1000  Ratings 991,656 7,499,706 3,519,324 311,672  (a) Description of the datasets  DataSet MF Jester 0.723 ML1M 0.689 0.675 Yahoo 0.766 Flixter  IAM ItemKNN 0.737 0.727 0.719 0.758  0.725 0.675 0.726 NA  (b) Accuracy of the different models in the classical CF context (without cold-start). NA (Not Available) means that, due to the complexity of ItemKNN, results were not computed over the Flixter dataset.  Table 1: Datasets description and performance of different models.  involves an intractable number of possible combinations of items. We propose to use a L1 relaxation in order to transform this problem in a continuous one. Let us denote α ∈ RI a weight vector, one weight per item, such that if αi = 0 then item i will not be in the interview. The cold-start loss can be rewritten with α’s as:  Lcold(q, Ψ, α) =  (ru,i − qT  i fΨ(u, α))2 + λ|α|  (5)  (cid:88)  (u,i)∈O  Note that the L2 regularization term over the computed representation of users and items is removed here for sake of clarity. The representation of a user thus depends on the ratings made by this user for items i that have a non-null weight αi, restricting our model to compute its prediction on a subset of items which compose the interview. If we rewrite the proposed model as:  (cid:88)  (u,i)∈O  αiΨru,i  i  (cid:88)  (u,i)∈O  fΨ(u, α) = Ψ0 +  (cid:88)  (u,i)∈O  then we obtain the following loss function:  Lcold(q, Ψ, α) =  (ru,i − qT  i (Ψ0 +  αiΨru,i  i  ))2 + λ|α|  which is now continuous. Note that, in that case, the translation resulting from a rating over an item corresponds to αiΨru,i  rather than to Ψru,i  .  i  i  This objective loss can be optimized by using stochastic gradient-descent methods. Since it contains a L1 term which is not derivable on all the points, we propose to use the same idea than proposed in Carpenter (2008) which consists in ﬁrst making a gradient step without considering the L1 term, and then applying the L1 penalty to the weight to the extent that it does not change its sign. In other words, a weight αi is clipped when it crosses zero.  2.2  IAM AND CLASSICAL COLLABORATIVE FILTERING  The IAM, which is particularly well-ﬁtted for user cold-start recommendation, can also be used in the classical collaborative ﬁltering problem, without constraining the set of items. In that case, the objective function can be written as: Lwarm(q, Ψ) =  (cid:88)  (cid:88)  (ru,i − qT  Ψru,i  (8)  ))2  i (Ψ0 +  i  (u,i)∈O  (u,i)∈O  which can be easily optimized through gradient descent. This model is a simple alternative to matrix factorization-based approaches, which is also evaluated in the experimental section. This model have some nice properties in comparison to transductive techniques, mainly it can easily update users’ representations when faced with new incoming ratings, but this is not the topic of this article.  3 EXPERIMENTS  We evaluate our models on four benchmark datasets - Table 1a - of various size in terms of number of users, of items or regarding the sparsity of ratings. The datasets are classical datasets used in the literature (Zhou et al. (2011); Golbandi et al. (2010)). ML1M corresponds to the MovieLens 1 millon dataset and Yahoo corresponds to the Yahoo! Music benchmark. Flixter and Jester are  4  (6)  (7)  Accepted as workshop contribution at ICLR 2015  (a) RMSE performance  (b) Accuracy performance  Figure 1: Accuracy and RMSE evaluation on Yahoo dataset for all models, regarding the size of the interview (number of questions/items asked).  classical datasets. As our main goal is mainly to evaluate the quality of our approach in the context of new users arriving in the system, we deﬁne the following protocol in order to simulate a realistic interview process on incoming users, and to evaluate different models. We proceed as follow: (i) We randomly divide each dataset along users, to have a pool of training users denoted U train, composed of 50% of the users of the complete dataset, on which we learn our model. The remaining users are split in two sets ( representing each 25% of initial users) for validation and testing. The interview process will be applied on each of these two subsets. (ii) The U test and U valid sets are then randomly split in two subsets of ratings to simulate the possible known answers : 50% of the ratings of a set are used as the possible answers to the interview questions (Answer Set). The 50% of ratings left will be used for evaluating our models (Evaluation Set). Ratings have been binarized for each datasets, a rating of -1 (resp. 1) being considered a dislike, (resp. like). The quality of the different models is evaluated by two different measures. The root mean squared error (RMSE) measures the average ratings’ prediction precision measured as the difference be- tween predicted and actual ratings (ˆru,i − ru,i)2. As we work with binary ratings, we also use the accuracy as a performance evaluation. In this context, it means that we focus on the overall predic- tion, i.e on the fact that the system has rightly predicted like or dislike, rather than on its precision regarding the ”true” rating. The accuracy is calculated as the average ”local” accuracy along users. These measures are computed over the set of missing ratings i.e the Evaluation Set. We explore the quality of our approach on both the classical CF context using the IAM Model (Equation 8) and on the cold-start problem using the CS-IAM model deﬁned in Equation 7. We compare our models with two baseline collaborative ﬁltering methods: Matrix Factorization (MF) that we presented earlier, and the Item-KNN with Pearson correlation measure (Koren (2010)) which does not compute representations for users nor items but is a state-of-the-art CF method. Note that the inductive models (IAM and CS-IAM) are trained using only the set of training users U train. The ratings in the Answer Set are only used as inputs during the testing phase, but not during training. Transductive models are trained using both the training users U train, but also the Answer set of ratings deﬁned over the testing users. It is a crucial difference as our model has signiﬁcantly less information during training. Each model has its own hyper-parameters to be tuned: the learning-rate of the gradient descent procedure, the size N of the latent space, the different regularization coefﬁcients... The evaluation is thus made as follows: models are evaluated for several hyper-parameters values using a grid-search procedure, the performance being averaged over 3 different randomly initialized runs. The models with the best average performance are presented in the next ﬁgures and tables. All models have been evaluated over the same datasets splits.  3.1 COLLABORATIVE FILTERING  First, we evaluate the ability of our model to learn relevant representations in a classical CF context. In that case, the IAM model directly predicts ratings based on the ratings provided by a user. Results for the four different datasets are presented in Table 1b. We can observe that, despite having much  5  01020304050#items selected0.880.900.920.940.960.981.001.021.04RMSE<function title at 0x7f57a5eed938>CSIAMIKNN-HelfIKNN-PopMF-HelfMF-Pop01020304050#items selected0.580.600.620.640.660.680.70Accuracy<function title at 0x7f57a5eed938>CSIAMIKNN-HelfIKNN-PopMF-HelfMF-PopAccepted as workshop contribution at ICLR 2015  (a) Accuracy performance on Jester  (b) Visualization of some αiΨi after a PCA  Figure 2: Performance on Jester, and visualization  less information during the learning phase, IAM obtains competitive results, attesting the ability of the additive model to generalize to new users. More precisely, IAM is better than MF on three out of four datasets. For example, on the MovieLens-1M dataset, IAM obtains 72.7% in terms of accuracy while MF’s accuracy is only 68.9%. Similar scores are observed for Jester and Yahoo. Although Item-KNN model gives slightly better results for two datasets, one should note that this method do not rely on nor provide any representations for users or items and belongs to a different family of approach. Moreover, ItemKNN - which is based on a KNN-based method - has a high complexity, and is thus very slow to use, and unable to deal with large scale datasets like Flixter on which many days are needed in order to compute performance. Beyond its nice performance IAM is able to predict over a new user in a very short-time, on the contrary to MF and ItemKNN.  3.2 COLD-START SETTING  We now study the ability of our approach to predict ratings in a realistic cold-start situation. As MF and ItemKNN do not provide a way to select a set of items for the interview, we use two benchmark selection methods used in the literature (Rashid et al. (2002)). The POP method select the most popular items - i.e the items with the highest number of ratings in the training set - and the HELF (Harmonic mean of Entropy and Logarithm of rating Frequency) method which select items based on both their popularity but also using an entropy criterion, which focus on the informativeness of items (e.g a controversial movie can be more informative than a movie liked by everyone) (Rashid et al. (2008)). Our model is learned solely on the U train set. Baselines are computed on a dataset composed of the original U train ratings with the additional ratings of the AnswerSet of U test that lie into the set of items selected by the POP or the HELF approach. Transductive approaches use more information during training that our inductive model. The number of items selected by the CS-IAM model directly depends on the value of the L1 reg- ularization coefﬁcient and several values have been evaluated. In CS-IAM, the number of selected items correspond to the number of non-null αi parameters. The number of items selected by POP and HELF is manually chosen. Figure 1 shows accuracy and RMSE results for all models on the Yahoo dataset as a function of the interview size. It ﬁrst illustrates that ItemKNN approach does not provide good results for RMSE- evaluation, as it is not a regression-based method, but is better than MF in terms of accuracy. It also shows that HELF criterion does not seem to be speciﬁcally better on this dataset than the POP criterion. For both evaluations, CS-IAM gives better results, for all sizes of interview. It can also be noted that CS-IAM also gives good results when no item is selected due to the Ψ0 parameters that correspond to the learned default representation. The model with 0 items also expresses the base performance obtained on users unable to provide ratings during the interview. Detailed accuracy results for the four datasets are summarized in Table 2, for different reasonable sizes of interview. Similar observations can be made on the results, where CS-IAM managed to have the best or competitive accuracy for all datasets and all number of questions allowed, while using less information in train. At last, when comparing the performance of CS-IAM with a version of IAM where items have been selected by the POP criterion -IAM-Pop, Figure 2a - one can see that the CS-IAM outperforms the  6  68101214161820#items selected0.600.620.640.660.680.700.72Accuracy<function title at 0x7f57a5eed938>CSIAMIAM-PopIKNN-PopMF-Pop0.000.010.020.030.040.80.60.40.20.0Awakenings Awakenings Schindler's List Schindler's List Star Wars: Episode IV - A New Hope Star Wars: Episode IV - A New Hope Saving Private Ryan Saving Private Ryan LikeDislikeAccepted as workshop contribution at ICLR 2015  DataSet  NbItems MF POP MF HELF  IKNN POP  Jester  MovieLens 1M  Yahoo  Flixter  5 10 20 5 10 20 5 10 20 5 10 20  0.603 0.613 0.665 0.629 0.634 0.648 0.590 0.601 0.621 0.719 0.720 0.727  0.589 0.609 0.641 0.617 0.620 0.621 0.594 0.610 0.623 0.722 0.726 0.739  0.608 0.640 0.688 0.649 0.651 0.663 0.623 0.633 0.654 NA NA NA  IKNN HELF CS-IAM 0.667 0.686 0.701 0.690 0.695 0.696 0.638 0.647 0.665 0.723 0.727 0.735  0.634 0.608 0.676 0.647 0.653 0.638 0.624 0.634 0.654 NA NA NA  Table 2: Accuracy performance of models on four datasets regarding the number of questions asked. NA (Not Available) means that, due to the complexity of ItemKNN, results were not computed over the Flixter dataset. Bold results corresponds to best accuracy. other approaches. It interestingly shows that (i) IAM managed to give better results than MF with the same information selection strategy (POP) (ii) CS-IAM with all its parameters learned, managed to select more useful items for the interview process, illustrating that the performance of this model is due to both, its expressive power, but also on its ability to simultaneously learn representations, and select relevant items. We have shown that our approach gives signiﬁcantly good quantitative results. We now focus our interest on a qualitative analysis of the results performed over the MovieLens dataset. First, we com- pare the items selected by the three selection methods (CS-IAM, POP and HELF). These items are presented in Table 3. First, when using the POP criterion, one can see that many redundant movies are selected - i.e the three last episodes of Star Wars on which the ratings are highly correlated: a user likes or dislikes Star Wars, not only some episodes. The same effect seems to appear also with CS-IAM which selects Back to the future I and Back to the future III. But, in fact, the situation is different since the ratings on these two movies have less correlations. Half of the users that like Back to the future I dislike Back to the future III. Figure 2b shows the translations αiΨi after having performed a PCA in order to obtain 2D repre- sentations. What we can see is that depending on the movie, the fact of having a positive rating or a negative rating does not have the same consequences in term of representation: For example, liking or disliking Saving Private Ryan is different than liking or disliking Star Wars; the translation concerning these two movies are almost perpendicular and thus result in a very different modiﬁca- tion of the representation of the user. Schindler’s List has less consequences concerning the user representation i.e the norm of αiΨr  i is lower than the others.  3.3 MIXING COLD-START AND WARM RECOMMENDATION  Our model can also allow one to smoothly move from a cold-start to a warm context : after having answered the interview, the user will start interacting with the system, providing new ratings, which will be easily integrated with our inductive translation model to update his representation and thus, the resulting recommendations. To do so, we simply change the learning strategy: (i) The model is learned in the warm setting described in Equation (8), i.e we learn each item’s representation qi and the translations on representations (the Ψr i parameters). (ii) We select the most relevant items for the interview process by learning the α’s weights using a L1 regularization as explained in Equation (7). In this phase, we only learn the α-values which will allow us to choose which items to use during the interview, following Equation (6). After the interview, each new incoming rating modiﬁes the user representation as explained in Equation (4), resulting in a system that is naturally able to take into account new information. Note that, in this setting, the use of an hyperbolic tangent function on the representation, which will limit its norm, improves the quality of the system. This model has been evaluated on the Yahoo dataset with the following experimental protocol: First the model is evaluated in its cold-start setting using the item with non-null α’s values. Then, we evaluate the performance of this model when adding different amount of ”new” ratings sampled uniformly from the set of items. The results are illustrated in Figure 3 which shows that the perfor- mance of this strategy increases as new ratings are added and almost reaches the one obtain for the  7  Accepted as workshop contribution at ICLR 2015  CS-IAM American Beauty, Being John Malkovich, Lion King, Ghost, Superman, Back to the Future, Fargo, Armageddon, Get Shorty, Splash, 20 000 Leagues Under the Sea, Back to the Future Part III, Outbreak  Popularity Star American Beauty, Episode I, Star Wars: Wars: Episode V, Star Wars: Episode IV, Star Wars: Episode VI, Jurassic Park, Terminator 2, Ma- trix, Back to the Future, Saving Ryan, Silence of the Lambs, Men in Black, Raiders of the Lost Ark, Fargo  Private  Figure 3: Accuracy regarding percent- age of ratings added after the interview  Table 3: MovieLens 1M - Selected items for the interview process by the three selection methods  classical warm setting (see Table 1b). Curves for different sizes of initial interviews are shown. We think that this extension of our approach which makes the link between the cold-start and the warm settings is an original and promising feature.  4 RELATED WORK  The recommendation problem has been studied under various assumptions. We focus on Collab- orative Filtering (CF) methods, which only use the past ratings observed on the users and items, but other families of approaches exists, as Content-Based methods, which use informative features on users and items (Pazzani & Billsus (2007)), and hybrid methods that mix ratings and informative features (Basilico & Hofmann (2004)). CF techniques can be distinguished into two categories. Memory-based methods, such as Neighbor- based CF Resnick et al. (1994), calculate weights between pairs of items (Sarwar et al. (2001)) or users (Herlocker et al. (1999)), based on similarities or correlations between them. Model-based methods, such as Latent Factor Models, have rather a representation learning approach, where representations vectors for each user and item are inferred from the matrix of ratings with matrix factorization techniques (Koren et al. (2009)). Collaborative ﬁltering models have a major limitation when there is no history for a user or an item. A classical approach in this case is to use an interview process with a few questions asked to the new user as it is done in this paper. Several papers have proposed different methods to choose which questions to select. Static approaches (see Rashid et al. (2002) for a comparative study), construct a static seed set of questions (ﬁxed for all users) following a selection criterion like measures of popularity, entropy or coverage while Golbandi et al. (2010) also proposed a greedy algorithm that aims to minimize the prediction error performed with the seed set. Adaptive approaches have also been proposed, where the interview process considers the user’s answers to choose the next question. For example, Rashid et al. (2008) ﬁts a decision tree to ﬁnd a set of clusters of users, while Golbandi et al. (2011) uses a ternary tree where each node is an item and branch corresponds to eventual answers (like,dislike,unknown). Zhou et al. (2011) presents functional matrix factorization, a decision tree based method which also associate a latent proﬁle to each nodes of the tree. The closest model to our approach is Sun et al. (2013), who learn a ternary tree allowing multiple questions at each node, each node containing a (learned) regressor and trans- lations functions on selected items. Our model can be seen as one node of their tree. However, their approach does not seem to allow a bridge between cold start and warm context as ours does. It is also interesting to note that while usually more efﬁcient, one drawback of such adaptive approaches is that users usually dislike having to rate item one by one and prefer rating several items in one shot (Golbandi et al. (2011); Rashid et al. (2002)).  5 CONCLUSION AND PERSPECTIVES  We have proposed a new representation-based model for collaborative ﬁltering. This inductive model (IAM) directly computes the representation of a user by cumulative translations in the la-  8  020406080100Percent of additional ratings used0.620.640.660.680.70AccuracyAccuracy along sparsitySize of interview :  14Size of interview :  23Size of interview :  1Accepted as workshop contribution at ICLR 2015  tent space, each translation depending on a rating value on a particular item. We have also proposed a generic formulation of the user cold-start problem as a representation learning problem and shown that the IAM method can be instantiated in this framework allowing one to learn both which items to use in order to build a preliminary interview for incoming users, but also how to use these ratings for recommendation. The results obtained over four datasets show the ability of our approach to outperform baseline methods. Different research directions are opened by this work: (i) ﬁrst, the model can certainly be extended to deal with both incoming users, but also new items. In that last case, the interview process would consist in asking reviews for any new item to a particular subset of relevant users. (ii) While we have studied the problem of building a static interview - i.e the opin- ions on a ﬁxed set of items is asked to any new user - we are currently investigating how to produce personalized interviews by using sequential learning models i.e reinforcement learning techniques.  ACKNOWLEDGEMENTS This article has been supported within the Labex SMART supported by French state funds managed by the ANR within the Investissements d’Avenir programme under reference ANR-11-LABX-65. Part of this work has beneﬁted from a grant from program DGA-RAPID, project LuxidX. REFERENCES Basilico, Justin and Hofmann, Thomas. Unifying collaborative and content-based ﬁltering.  In Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 9. ACM, 2004.  Bengio, Yoshua, Courville, Aaron, and Vincent, Pascal. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798– 1828, 2013.  Carpenter, Bob. Lazy sparse stochastic gradient descent for regularized multinomial logistic regres-  sion. Alias-i, Inc., Tech. Rep, pp. 1–20, 2008.  Golbandi, Nadav, Koren, Yehuda, and Lempel, Ronny. On bootstrapping recommender systems. In  Proceedings of the 19th ACM CIKM, pp. 1805–1808. ACM, 2010.  Golbandi, Nadav, Koren, Yehuda, and Lempel, Ronny. Adaptive bootstrapping of recommender systems using decision trees. In Proceedings of the fourth ACM international conference on Web search and data mining, pp. 595–604. ACM, 2011.  Herlocker, Jonathan L, Konstan, Joseph A, Borchers, Al, and Riedl, John. An algorithmic framework In Proceedings of the 22nd annual international ACM  for performing collaborative ﬁltering. SIGIR, pp. 230–237. ACM, 1999.  Koren, Yehuda. Factor in the neighbors: Scalable and accurate collaborative ﬁltering. ACM Trans-  actions on Knowledge Discovery from Data (TKDD), 4(1):1, 2010.  Koren, Yehuda, Bell, Robert, and Volinsky, Chris. Matrix factorization techniques for recommender  systems. Computer, 42(8):30–37, 2009.  Pazzani, Michael J and Billsus, Daniel. Content-based recommendation systems. In The adaptive  web, pp. 325–341. Springer, 2007.  Rashid, Al Mamunur, Albert, Istvan, Cosley, Dan, Lam, Shyong K, McNee, Sean M, Konstan, Joseph A, and Riedl, John. Getting to know you: learning new user preferences in recommender systems. In Proceedings of the 7th IUI, pp. 127–134. ACM, 2002.  Rashid, Al Mamunur, Karypis, George, and Riedl, John. Learning preferences of new users in rec- ommender systems: an information theoretic approach. ACM SIGKDD Explorations Newsletter, 10(2):90–100, 2008.  Resnick, Paul, Iacovou, Neophytos, Suchak, Mitesh, Bergstrom, Peter, and Riedl, John. Grouplens: In Proceedings of the 1994 ACM  an open architecture for collaborative ﬁltering of netnews. conference on Computer supported cooperative work, pp. 175–186. ACM, 1994.  Sarwar, Badrul, Karypis, George, Konstan, Joseph, and Riedl, John. Item-based collaborative ﬁlter-  ing recommendation algorithms. In Proceedings of WWW, pp. 285–295. ACM, 2001.  9  Accepted as workshop contribution at ICLR 2015  Sun, Mingxuan, Li, Fuxin, Lee, Joonseok, Zhou, Ke, Lebanon, Guy, and Zha, Hongyuan. Learning multiple-question decision trees for cold-start recommendation. In Proceedings of the sixth ACM international conference on Web search and data mining, pp. 445–454. ACM, 2013.  Zhou, Ke, Yang, Shuang-Hong, and Zha, Hongyuan. Functional matrix factorizations for cold-start recommendation. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pp. 315–324. ACM, 2011.  10  ",
1406.2080,2015, Training Convolutional Networks with Noisy Labels,"['Training Convolutional Networks with Noisy Labels', 'Sainbayar Sukhbaatar', 'Joan Bruna', 'Manohar Paluri', 'Lubomir Bourdev', 'and Rob Fergus']",https://arxiv.org/pdf/1406.2080,"5 1 0 2    r p A 0 1         ]  V C . s c [      4 v 0 8 0 2  .  6 0 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  TRAINING CONVOLUTIONAL NETWORKS WITH NOISY LABELS  Sainbayar Sukhbaatar Department of Computer Science, New York University sainbayar@cs.nyu.edu  Joan Bruna, Manohar Paluri, Lubomir Bourdev & Rob Fergus Facebook AI Research {joanbruna,mano,lubomir,robfergus}@fb.com  ABSTRACT  The availability of large labeled datasets has allowed Convolutional Network mod- els to achieve impressive recognition results. However, in many settings manual annotation of the data is impractical; instead our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the net- work which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modiﬁcations to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experi- ments on the ImageNet classiﬁcation benchmark.  1  INTRODUCTION  In recent years, Convolutional Networks (Convnets) (LeCun et al., 1989; Lecun et al., 1998) have shown impressive results on image classiﬁcation tasks (Krizhevsky et al., 2012; Simonyan & Zisser- man, 2014). However, this achievement relies on the availability of large amounts of labeled images, e.g. ImageNet (Deng et al., 2009). Labeling images by hand is a laborious task and impractical for many problems. An alternative approach is to use labels that can be obtained easily, such as user tags from social network sites, or keywords from image search engines. The catch is that these labels are not reliable so they may contain misleading information that will subvert the model during training. But given the abundance of tasks where noisy labels are available, it is important to understand the consequences of training a Convnet on them, and this is one of the contributions of our paper. For image classiﬁcation in real-world settings, two types of label noise dominate: (i) label ﬂips, where an example has erroneously been given the label of another class within the dataset and (ii) outliers, where the image does not belong to any of the classes under consideration, but mistakenly has one of their labels. Fig. 1 shows examples of these two cases. We consider both scenarios and explore them on a variety of noise levels and datasets. Contrary to expectations, a standard Convnet model (from Krizhevsky et al. (2012)) proves to be surprisingly robust to both types of noise. But inevitably, at high noise levels signiﬁcant performance degradation occurs. Consequently, we propose a novel modiﬁcation to a Convnet that enables it to be effectively trained on data with high level of label noise. The modiﬁcation is simply done by adding a constrained linear “noise” layer on top of the softmax layer which adapts the softmax output to match the noise distribution. We demonstrate that this model can handle both label ﬂip and outlier noise. As it is a linear layer, both it and the rest of the model can be trained end-to-end with conventional back- propagation, thus automatically learning the noise distribution without supervision. The model is also easy to implement with existing Convnet libraries (Krizhevsky, 2012; Jia et al., 2014; Collobert et al., 2011) and can readily scale to ImageNet-sized problems.  1  Accepted as a workshop contribution at ICLR 2015  Figure 1: A toy classiﬁcation example with 3 classes, illustrating the two types of label noise encoun- tered on real datasets. In the label ﬂip case, the images all belong to the 3 classes, but sometimes the labels are confused between them. In the outlier case, some images are unrelated to the classiﬁcation task but possess one of the 3 labels.  2 RELATED WORK  In any classiﬁcation model, a degradation in performance is inevitable when there is noise in the training data (Nettleton et al., 2010; Pechenizkiy et al., 2006). Especially, noise in labels is more harmful than noise in input features (Zhu & Wu, 2004). Label noise itself is a complex phenomenon. There are several types of noise on labels (Frenay & Verleysen, 2014). Also, noise source can be very different. For example, label noise can be caused by unreliable labeling by cheap and fast framework such as Amazon Mechanical Turk (http://www.mturk.com) (Ipeirotis et al., 2010), or noise can be introduced to labels intentionally to protect people privacy (van den Hout & van der Heijden, 2002). A simple approach to handle noisy labels is a data preprocessing stage, where labels suspected to be incorrect are removed or corrected (Barandela & Gasca, 2000; Brodley & Friedl, 1999). However, a weakness of this approach is the difﬁculty of distinguishing informative hard samples from harmful mislabeled ones (Guyon et al., 1996). Instead, in this paper, we focus on models robust to presence of label noise. The effects of label noise are well studied in common classiﬁers (e.g. SVMs, kNN, logistic regression), and robust variants have been proposed (Frenay & Verleysen, 2014; Bootkrajang & Kabn, 2012). Recently, Natarajan et al. (2013) proposed a generic unbiased estimator for binary classiﬁcation with noisy labels. They employed a surrogate cost function that can be expressed by a weighted sum of the original cost functions, and gave theoretical bounds on the performance. Considering the recent success of deep learning (Krizhevsky et al., 2012; Taigman et al., 2014; Sermanet et al., 2014), there is relatively little work on their application to noisy data. In Mnih & Hinton (2012) and Larsen et al. (1998), noise modeling is incorporated to neural network in the same way as our proposed model. However, only binary classiﬁcation is considered in Mnih & Hinton (2012), and Larsen et al. (1998) assumed symmetric label noise (i.e. noise is independent of the true label). Therefore, there is only a single noise parameter, which can be tuned by cross-validation. In this paper, we consider multi-class classiﬁcation and assume more realistic asymmetric label noise, which makes it impossible to use cross-validation to adjust noise parameters (for k = 103 classes, there are 106 parameters). Unsupervised pre-training of deep models has received much attention (Hinton & Salakhutdinov, 2006; Erhan et al., 2010). Particularly relevant to our work is Le (2013) and Lee et al. (2009) who use auto-encoders to layer-wise pre-train the models. However, their performance has been eclipsed by purely discriminative Convnet models, trained on large labeled set (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014). In the paradigm we consider, all the data has noisy labels, with an unknown fraction being trust- worthy. We do not assume the availability of any clean labels, e.g. those provided by a human. This contrasts with semi-supervised learning (SSL) (Zhu, 2008), where some fraction of the data has high quality labels but the rest are either unlabeled or have unreliable labels. Although closely related, in fact the two approaches are complementary to one another. Given a large set of data with noisy labels, SSL requires us to annotate a subset. But which ones should we choose? In the absence of external information, we are forced to pick at random. However, this is an inefﬁcient use of labeler resources since only a fraction of points lie near decision boundaries and a random sample is unlikely to contain many of them. Even in settings where there is a non-uniform prior on the labels, picking informative examples to label is challenging. For example, taking high ranked images returned by an image search engine might seem a good strategy but is likely to result in prototypical images, rather than borderline cases. In light of this, our approach can be regarded as a natural precursor to SSL. By ﬁrst applying our method to the noisy data, we can train a Convnet that will identify the subset of difﬁcult examples that should be presented to the human annotator.  2  horsedogcathorsecatdogcatdoghorsecatcathorsecatdogdogcatLabel ﬂip noiseOutlier noiseAccepted as a workshop contribution at ICLR 2015  Moreover, in practical settings SSL has several drawbacks that make it impractical to apply, unlike our method. Many popular approaches are based on spectral methods (Zhu et al., 2003; Zhou et al., 2004; Zhu & Lafferty, 2005) that have O(n3) complexity, problematic for datasets in the n = 106 range that we consider. Fergus et al. (2009) use an efﬁcient O(n) spectral approach but make strong independence assumptions that may be unrealistic in practice. Nystrom methods (Talwalkar et al., 2008) can scale to large n, but do so by drastically sub-sampling ﬁrst, resulting in a loss of ﬁne structure within the problem. By contrast, our approach is O(k2) complexity, in the number of classes k, since we model the aggregate noise statistics between classes rather than estimating per- example weights.  3 LABEL NOISE MODELING  3.1 LABEL FLIP NOISE n) where y∗ denotes the Let us ﬁrst describe the scenario of label ﬂip. Given training data (xn, y∗ true labels ∈ 1, . . . K, we deﬁne a noisy label distribution ˜y given by p(˜y = j|y∗ = i) = q∗ j,i, parametrized by a K × K probability transition matrix Q∗ = (q∗ ji). We thus assume here that label ﬂips are independent of x. However, this model has the capacity to model asymmetric label noise distributions, as opposed to uniform label ﬂips models (Larsen et al., 1998). For example, we can model that a cat image is more likely to mislabeled as “dog” than “tree”. The probability that an (cid:88) input x is labeled as j in the noisy data can be computed using Q∗  (cid:88)  ∗  ∗  p(˜y = j|x) =  p(˜y = j|y  = i)p(y  i  = i|x) =  i  ∗ q jip(y  ∗  = i|x).  (1)  In the same way, we can modify a classiﬁcation model using a probability matrix Q that modiﬁes its prediction to match the label distribution of the noisy data. Let ˆp(y∗ |x, θ) be the prediction probability of true labels by the classiﬁcation model. Then, the prediction of the combined model will be given by  ˆp(˜y = j|x, θ, Q) =  = i|x, θ).  (2)  ∗  qji ˆp(y  (cid:88)  i  This combined model consist of two parts: the base model parameterized by θ and the noise model parameterized by Q. The combined model is trained by maximizing the cross entropy between the noisy labels ˜y and the model prediction given by Eqn. 2. The cost function to minimize is  L(θ, Q) = −  1 N  log ˆp(˜y = ˜yn|xn, θ, Q) = −  1 N  ∗  q˜yni ˆp(y  = i|xn, θ)  ,  (3)  N(cid:88)  (cid:32)(cid:88)  log  n=1  i  N(cid:88)  n=1  (cid:33)  (a)  (b)  Figure 2: (a) Label noise is modeled by a constrained linear layer inserted between softmax and cost layers. Noise distribution Q becomes the weight matrix of this layer. It changes output probabilities from the base model into a distribution that better matches the noisy labels. (b) The training sequence when learning from noisy data. The noise matrix Q (red) is initially set to the identity, while the base model (green) is trained, inadvertently learning the noise in the data. Then we start updating Q also (with regularization) and this captures the noise properties of the data, leaving the model to make “clean” predictions.  3  !""#$%&&&&&’&()*+,-&.!""/,0&.,1/0&&&&&&&’&20)((&/""%0)#1&2)(%&&&&&&&&&&&&&&&’&&base model noise model "")!(1&.,3/.&&&&&&’&x,˜y)j|x,3,(/&""/%4)05&&&&’&6.!""/,072)""870/.$9&.Qrandomsamples,,,cannotL(θ,Q)=,,,cannotL(θ,QandCtime/epochs identity uniform true fixed less noisy more noisy !""#$""%&’(#)*+%%%%%%,-"".%/0#11%""$#23%24/""%4$%,3-+.""%(32#5%6.3%7#/3%04(31%13#$*/%*4-/3%-*%(#""#%matricesQ,matricesQ,matricesQ,matrix:QCwecannottrueQ∗.6.3%*4-/3%1#53$%#7/4$7/%"".3%*4-/3%Accepted as a workshop contribution at ICLR 2015  n=1  N(cid:88) K(cid:88) K(cid:88) (cid:80) k q∗ (cid:88)  k=1  i=1  where N is the number of training samples. However, the ultimate goal is to predict true labels y∗, not the noisy labels ˜y. This can be achieved if we can make the base model predict the true labels accurately. One way to quantify this is to use its confusion matrix C = {cij} deﬁned by  = i|xn, θ),  (4)  (cid:88)  n∈Sj  ∗  ˆp(y  cij :=  1 |Sj|  where Sj is the set of training samples that have true label y∗ = j. If we manage to make C equal to identity, that means the base model perfectly predicts the true labels in training data. Note that this is the prediction before being modiﬁed by the noise model. We can also deﬁne the confusion matrix (cid:88) ˜C = {˜cij} for the combined model in the same way  (5)  ˜cij :=  1 |Sj|  n∈Sj  ˆp(˜y = i|xn, θ, Q).  Using Eqn. 2, it follows that ˜C = QC. Note that we cannot actually measure C and ˜C in reality, unless we know the true labels. Let us show that minimizing the training objective in Eqn. 3 forces the predicted distribution from the combined model to be as close as possible to the noisy label distribution of the training data, asymptotically. As N → ∞, the objective in Eqn. 3 becomes  L(θ, Q) = −  1 N  log ˆp(˜y = ˜yn|xn) = −  K(cid:88)  (cid:88)  k=1  n∈Sk  1 N  ∗ n = k)  log ˆp(˜y = ˜yn|xn, y  K(cid:88)  K(cid:88)  N→∞ −−−−→ −  (cid:80) k q∗  q  ∗  ∗ ik log ˆp(˜y = i|x, y k = H(q∗), and with equality in the last equation only when ik. In other words, the model tries to match the confusion matrix ˜C of the  = k) ≥ −  k log q∗  ∗ ik log q  ∗ ik ,  (6)  k=1  i=1  q  since − k log ˆpk ≥ − ˆp(˜y = i|x, y∗ = k) = q∗ combined model to the true noise distribution Q∗ of the noisy data  ˜cik = 1/|Sk|  n∈Sk  ˆp(˜y = i|xn, y  ∗ n = k) → q  ∗ ∗ ik =⇒ ˜C = QC → Q  .  (7)  If we know the true noise distribution Q∗ and it is non-singular, then from Eqn. 7, setting Q = Q∗ would force C to converge to identity. Therefore, training to predict the noisy labels using the combined model parameterized by Q∗ directly forces the base model to predict the true labels. If the base model is a Convnet network with a softmax output layer, then the noise model is a linear layer, constrained to be a probability matrix, that sits on top of the softmax layer, as shown in Figure 2a. The role of this noise layer is to implement Eqn. 2 on the output of the softmax layer. Therefore, the noise layer is a linear layer with no bias and weights set to matrix Q∗. Since this is the only modiﬁcation to the network, we can still perform back-propagation for training.  3.2 LEARNING THE NOISE DISTRIBUTION In the previous section, we showed that setting Q = Q∗ in the noise model is optimal for making the base model accurately predict true labels. In practice, the true noise distribution Q∗ is often unknown to us. In this case, we have to infer it from the noisy data itself. Fortunately, the noise model is a constrained linear layer in our network, which means its weights Q can be updated along with other weights in the network. This is done by back-propagating the cross-entropy loss through the Q matrix, down into the base model. After taking a gradient step with the Q and the model weights, we project Q back to the subspace of probability matrices because it represents conditional probabilities. Unfortunately, simply minimizing the loss in Eqn. 3 will not give us the desired solution. From (6), it follows that QC = ˜C → Q∗ as the training progresses, where ˜C is the confusion matrix of the combined model and Q∗ is the true noise distribution of data. However, this alone cannot guarantee Q → Q∗ and C → IK. For example, given enough capacity, the base model can start learning the noise distribution and hence C → Q∗, which implies that Q → IK. Actually, there are inﬁnitely many solutions for QC = Q∗ where Q (cid:54)= Q∗.  4  Accepted as a workshop contribution at ICLR 2015  In order to force Q → Q∗, we add a regularizer on the probability matrix Q which forces it to diffuse, such as a trace norm or a ridge regression. This regularizer effectively transfers the label noise distribution from the base model to the noise model, encouraging Q to converge to Q∗. Such regularization is reminiscent of blind deconvolution algorithms. Indeed, the noise distribution acts on our system by diffusing the predictions of the base model. When the diffusion kernel is unknown, it is necessary to regularize the ill-posed inverse problem by pushing the estimates away from the trivial identity kernel (Levin et al., 2009). Under some strong assumptions, we can actually prove that tr(Q) takes the smallest value only when Q = Q∗. Let us assume QC = Q∗ holds true, and Q and Q∗ have large diagonal elements (i.e. qii > qij and q∗  ii > q∗  (cid:88) (cid:88) ij for ∀i, j (cid:54)= i). Then, it follows qiicji) = (  (cid:88)  (cid:88)  (  (cid:88)  qijcji) ≤  (cid:88)  i  j  i  j  i  j  i  (cid:88)  ∗ tr(Q  ) = tr(QC) =  qii(  cji) =  qii = tr(Q) .  This shows that tr(Q∗) is a lower bound for tr(Q), and the equality will hold true only when C = IK and Q = Q∗. Therefore, minimizing tr(Q) is a sensible way to make the base model accurately predict clean labels. Although the above proof is true under strong assumptions, we show empirically that it works well in practice. Also, we use weight decay on Q instead of minimizing tr(Q) in practice since it is already implemented in most deep learning packages and has similar effect of diffusing Q. Let us ﬁnally describe the learning procedure, illustrated in Figure 2b. In the beginning of training, QC and Q∗ are very different in general, and the confusion matrix C does not necessarily have large elements on its diagonal. This makes learning Q difﬁcult. Therefore, we ﬁx Q = IK at the start of training. This is illustrated in the left part of Figure 2b. At that point, the base model could have learned the noise in the training data (i.e. C = Q∗), which is not what we want. Therefore, we start updating Q along with the rest of the network, using weight decay to push Q away from the identity and towards Q∗. As Q starts to diffuse, it starts absorbing the noise from the base model, thus making the base model more accurate. However, too large weight decay would make Q more diffused than the true Q∗, which may hurt the performance. In case there is no clean data, we cannot tune this weight decay parameters with validation. In the experiments in this paper, we ﬁx the weight decay parameter for Q to 0.1 (in some cases where the training classiﬁcation cost signiﬁcantly increased because of the weight decay on Q, we used smaller weight decay parameter). If we want to make prediction or test the model on clear data, the noise layer should be removed (or set to identity I).  3.3 OUTLIER NOISE Another important setting is the case where some training samples do not belong to any of the existing signal classes. In that case, we can create an additional “outlier” class, which enables us to apply the previously described noise model. Let K be the number of the existing classes. Then, the base network should output K + 1 probabili- ties now, where the last one represents the probability of a sample being an outlier. If the labels given to outlier samples are uniformly distributed across classes, then the corresponding noise distribution Q∗ becomes a K + 1 × K + 1 matrix ∗ Q  1/K 1/K  =  (8)  .  0 1 . . . 0 0  · · · · · · . . . · · · · · ·  0 0 . . . 1 0  . . .  1/K  0  1  0 . . . 0 0  Unfortunately, this matrix is singular and would map two different network outputs y1 = (0, ..., 0, 1)T and y2 = (1/K, ..., 1/K, 0) to the exact same point. A simple solution to this problem is to add some extra outlier images with label K + 1 in the training data, which would make Q∗ non- singular (in most cases, it is cheap to obtain such extra outlier samples). Now, the noise distribution becomes   , where α = |outliers labeled “K + 1”|  |total outliers|  0 1 . . . 0 0  · · · · · · . . . · · · · · ·  0 0 . . . 1 0  (1 − α)/K (1 − α)/K  . . .  (1 − α)/K  α  1  0 . . . 0 0  ∗ Q  =  .  (9)  5  Accepted as a workshop contribution at ICLR 2015  Note that in this setting there is no learning: Q matrix is ﬁxed to Q∗ given in Eqn. 9. The fraction of outliers in the training set, required to compute α, is a hyper-parameter that must be set manually (since there is no principled way to estimate it). However, we experimentally demonstrate that the algorithm is not sensitive to the exact value.  4 EXPERIMENTS  In this section, we empirically examine the robustness of deep networks with and without noise modeling. First, we perform controlled experiments by deliberately adding two types of label noise to clean datasets: label ﬂip noise and outlier noise. Then we show more realistic experiments using two datasets with inherent label noise, where we do not know the true distribution of noisy labels.  4.1 DATA AND MODEL ARCHITECTURE We use three different image classiﬁcation datasets in our experiments. The ﬁrst dataset is the Google street-view house number dataset (SVHN) (Netzer et al., 2011), which consists of 32x32 images of house number digits captured from Google Streetview. It has about 600k images for train- ing and 26k images for testing. The second one is more challenging dataset CIFAR10 (Krizhevsky & Hinton, 2009), a subset of 80 million Tiny Images dataset (Torralba et al., 2008) of 32x32 natural images labeled into 10 object categories. There are 50k training images and 10k test images. The last dataset is ImageNet (Deng et al., 2009), a large scale dataset of 1.2M images, labeled with 1000 classes. For all datasets, the only preprocessing step is mean subtraction, except for SVHN where we also perform contrast normalization. For ImageNet, we use data augmentation by taking random crop of 227x227 at random locations, as well as horizontal ﬂips with probability 0.5. For SVHN and CIFAR-10 we use the model architecture and hyperparameter settings given by the CudaConv (Krizhevsky, 2012) conﬁguration ﬁle layers-18pct.cfg, which implements a network with three convolutional layers. These settings were kept ﬁxed for all experiments using these datasets. For ImageNet, we use the model architecture described in Krizhevsky et al. (2012) (AlexNet).  4.2 LABEL FLIP NOISE  We synthesize noisy data from clean data by stochastically changing some of the labels: an original label i is randomly changed to j with ﬁxed probability q∗ ji. Figure 3c shows the noise distribution Q∗ = {q∗ ji} used in our experiments. We can alter this distribution by changing the probability on the diagonal to generate datasets with different overall noise levels. The labels of test images are left unperturbed. SVHN: When training a noise model with SVHN data, we ﬁx Q to the identity for the ﬁrst ﬁve epochs. Thereafter, Q is updated with weight decay 0.1 for 95 epochs. Figure 3a and 3b shows the test errors for different training set sizes and different noise levels. These plots show the normal model coping with up to 30% noise, but then degrading badly. By contrast, the addition of our noise layer allows the model to operate with up to 70%. Beyond this, the method breaks down as the false labels overwhelm the correct ones. Overall, the addition of the noise model consistently achieves better accuracy, compared with a normal deep network. The ﬁgure also shows error rates for a noise model trained using the true noise distribution Q∗. We see that it performs as well as the learned Q, showing that our proposed method for learning the noise distribution from data is effective. Figure 3c shows an example of a learned Q alongside the ground truth Q∗ used to generate the noisy data. We can see that the difference between them is negligible. Figure 4a shows the effects of label ﬂip noise in more detail. The color in the ﬁgure shows the test errors (brighter means better), and the contour lines indicates the same accuracy. Without the noise model, the performance drops quickly as the number of incorrect labels increases. In contrast, the convnet with the noise model shows greater robustness to incorrect labels. CIFAR-10: We perform the same experiments as for SVHN on the CIFAR-10 dataset, varying the training data size and noise level. We ﬁx Q to identity for the ﬁrst 30 epochs of training and then run for another 70 epochs updating Q with weight decay 0.1. The results are shown in Figure 4b. Again, using the noise model is more robust to label noise, compared to the unmodiﬁed model. The difference is especially large for high noise levels and large training sets, which shows the scalability of the noise model.  6  Accepted as a workshop contribution at ICLR 2015  (a)  (b)  (c)  Figure 3: (a) Test errors on SVHN dataset when the noise level is 50% for differing overall training set sizes. (b) Test errors when trained on 100k samples, as the noise level varies. Note that the performance for learned Q is very close to a model trained with Q ﬁxed to the true noise distribution Q∗. (c) The ground truth noise distribution Q∗ (left) and Q learned from noisy data (right).  Without noise model  With learned noise model  Without noise model  With learned noise model  (a) SVHN  (b) CIFAR10  Figure 4: The noise model is compared to the baseline model on different amount of training data and varying noise levels. The plots show test errors (%), where brighter color indicates better accuracy.  ImageNet: In this experiment, we deliberately ﬂip half of the training labels in ImageNet dataset to test the scalability of our noise model to a 1000 class problem. We explore two different noise distributions: (i) random and (ii) adversarial, applied to the ImageNet 2012 training set. In the random case, labels are ﬂipped with a non-uniform probability using a pre-deﬁned matrix Q∗, which has around 10 thousand non-zero values at random off-diagonal locations. I.e for each class, 50% of the labels are correct, with the other 50% being distributed over 10 other randomly chosen classes. In the adversarial case, we use a noise distribution where labels are changed to other classes that are more likely to be confused with the true class (e.g. simulating human mislabeling). First, we train a normal convnet on clean data and measure its confusion matrix. This matrix gives us a good metric of which classes more likely to be confused. Then this matrix is used for constructing Q∗ so that 40% of labels are randomly ﬂipped to other similar labels. Using the AlexNet architecture (Krizhevsky et al., 2012), we train three models for each noise case: (i) a standard model with no noise layer; (ii) a model with a learned Q matrix and (iii) a model with Q ﬁxed to the ground truth Q∗. Table 1a shows top-1 classiﬁcation error on the ImageNet 2012 validation set for models trained on the random noise distribution. It is clear that the noise hurts performance signiﬁcantly. The model with learned Q shows a clear gain (8.5%) over the unaltered model, but is still 3.8% behind the model that used the ground truth Q∗. The learned Q model is superior to training an unaltered model on the subset of clean labels, showing that the noisy examples carry useful information that can be accessed by our model. Table 1b shows errors for the adversarial noise situation. Here, the overall performance is worse (despite a lower noise level than Table 1a, but the learned noise model is still superior to the unaltered model. 4.3 OUTLIER NOISE CIFAR-10: Here, we simulate outlier noise by deliberately polluting CIFAR10 training data with randomly chosen images from the Tiny Images dataset (Torralba et al., 2008). Those random images can be considered as outliers because Tiny Images dataset covers ∼75,000 classes, thus the chance of belonging to 10 CIFAR classes is very small. Our training data consists of a random mix of inlier images with true labels and outlier images with random labels (n.b. the model has no knowledge  7  2040608010081012141618202224training data size (K)test error (%)  normal modelnoise model (true Q)noise model (learned Q)0204060805101520% of false labelstest error (%)  normal modelnoise model (true Q)noise model (learned Q)# of correct labels# of incorrect labels  12345x 104012345x 1041016254063# of correct labels# of incorrect labels  12345x 104012345x 10410162540size of training data% of incorrect labels  12345x 104010203040506070202532405063size of training data% of incorrect labels  12345x 104010203040506070202532405063Accepted as a workshop contribution at ICLR 2015  Training Noise size model 1.2M None 0.6M None None 1.2M Learned Q 1.2M True Q∗ 1.2M  Noise % 0 0 50 50 50  Valid. Error 39.8% 48.5% 53.7% 45.2% 41.4%  (a) random label ﬂip noise  Training Noise size model None 1.2M Learned Q 1.2M True Q∗ 1.2M  Noise % 40 40 40  Valid. error 50.5% 46.7% 43.3%  (b) adversarial label ﬂip noise  Table 1: Effect of label ﬂip noise using the ImageNet dataset  of which are which). As described in Section 3.3, the outlier model requires a small set of known outlier images. In this case, we use 10k examples randomly picked from Tiny Images. For testing, we use the original (clean) CIFAR10 test data. Figure 5(left) shows the classiﬁcation performance of a model trained on different amounts of inlier and outlier images. Interestingly, a large amount of outlier noise does not signiﬁcantly reduce the accuracy of the normal model without any noise modeling. Nevertheless, when the noise model is used the effect of outlier noise is reduced, particularly for small training sets. In this experiment, we set hyper-parameter α in Eqn. 9 using the true number of outliers but, as shown in Fig. 6, model is not sensitive to the precise value. Figure 5(right) explores the ability of the trained models to distinguish inlier from outlier in a held- out noisy test set. For the normal model, we use the entropy of the softmax output as a proxy for outlier conﬁdence. For our outlier model, we use the conﬁdence of the k + 1th class output. The ﬁgure shows precision recall curves for both models trained varying training set sizes with 50% outliers. The average precision for the outlier model is consistently superior to the normal model. ImageNet: We added outlier noise to ImageNet dataset (1.2M images, 1K categories) using images randomly chosen from the entire ImageNet Fall 2011 release (15M images, 22K categories). Each outlier images is randomly labeled as one of 1K categories and added to the training set. We ﬁx the number of inlier images Nin to be 0.6M (half of the original ImageNet training data). We increase the number of outlier images up to Nout = 1.2M, mixed into the training data. For training the noise model, we added 20K outlier images, labeled as outlier to the training data. We trained a normal AlexNet model on the data, along with a version using the outlier model. We used three different α values (see Eqn. 9) in these experiments. One run used α set using the true percentage of outliers in the training data. The other two runs perturbed this value by ±15%, to explore the sensitivity of the noise model to this hyper-parameter. The results are shown in Fig. 6 (the error bars show ±1σ) for differing amounts of outliers. Although the normal deep network is quite robust to large outlier noise, using the noise model further reduces the effect of noise.  4.4 REAL LABEL NOISE  Tiny Images: The Tiny Images dataset (Torralba et al., 2008) is a realistic source of noisily labeled data, having been gathered from Internet search engines. We apply our outlier model to a 200k  Figure 5: CIFAR-10 outlier experiments. Left: The effect of the outlier noise on the classiﬁcation performance on CIFAR10 with and without the noise model. Here, Nin and Nout are the number of inlier and outlier images in the training data, respectively. Right: Precision recall curve for detecting inliers in test data.  8  01220253035Nout / Nintest error (%)  normal Nin=10Knormal Nin=20Knormal Nin=50Knoise model Nin=10Knoise model Nin=20Knoise model Nin=50K00.20.40.60.810.20.30.40.50.60.70.80.91recallprecision  normal Nin=10Knoise model Nin=10Knormal Nin=20Knoise model Nin=20Knormal Nin=50Knoise model Nin=50KAccepted as a workshop contribution at ICLR 2015  Figure 6: ImageNet outlier experiments with varying ratios of outlier/inliers.  subset, taken from the 10 classes from which the CIFAR-10 dataset was extracted. The data contains a mix of inlier images, as well as totally unrelated outlier images. After a cursory examination of the data, we estimated the outlier fraction to be 0.5. Using this ratio, along with 5k known outliers (randomly drawn Tiny Images), α was set to 0.05. For evaluation we used the CIFAR-10 test set1. Training on this data using an unmodiﬁed convnet produced a test error of 19.2%. A second model, trained with an outlier layer, gave a test error of 18.8%, a relative gain of 2.1%. Web Images + ImageNet: We also apply our approach to a more challenging noisy real world problem based around the 1000 classes used in ImageNet. We collected a new noisy web image dataset, using each of the thousand category names as input into Internet image search engines. All available images for each class were downloaded, taking care to delete any that also appeared in the ImageNet dataset. This dataset averages 900 examples/class for a total of 0.9M images. The noise is low for the highly ranked images, but signiﬁcant for the later examples of each class. The precise noise level is unknown, but after browsing some of the images we set α = 0.08, assuming 20% of images are outlier. We trained an AlexNet model (Krizhevsky et al., 2012) with and without the noise adaption layer on this web image dataset and evaluated the performance on the ImageNet 2012 validation set. One complication is that the distribution of inliers in the web images differs somewhat from the ImageNet evaluation set, creating a problem of domain shift. To reduce this effect (so that the noise adaptation effects of our approach can be fairly tested), we added 0.3M ImageNet training images to our web data. This ensures that the model learns a representation for each class that is consistent with the test data. Table 2 shows three Alexnet models applied to the data: (i) an unaltered model, (ii) a model with learned label-ﬂip noise matrix (weigh decay parameter for Q set to 0.02 because larger value signif- icantly increased the training classiﬁcation cost) and (iii) a model with an outlier noise matrix (with α set to 0.08). The results show the label-ﬂip model boosting performance by 0.6%.  Method Normal Convnet Label-ﬂip model Outlier model  Valid. error  48.8% 48.2% 48.5%  Table 2: Evaluation on our real-world Web image + ImageNet noisy dataset.  5 CONCLUSION  In this paper we explored how convolutional networks can be trained on data with noisy labels. We proposed two simple models for improving noise robustness, focusing different types of noise. We explored both approaches in a variety of settings: small and large-scale datasets, as well as synthesized and real label noise. In the former case, both approaches gave signiﬁcant performance gains over a standard model. On real data, then gains were smaller. However, both approaches can be implemented with minimal effort in existing deep learning implementations, so add little overhead to any training procedure.  1We carefully removed from the training set any images that were present in the CIFAR-10 test set.  9  00.511.524850525456Nout / Nintest error (%)  normalnoise modelAccepted as a workshop contribution at ICLR 2015  REFERENCES Barandela, Ricardo and Gasca, Eduardo. Decontamination of training samples for supervised pattern recogni- tion methods. In Advances in Pattern Recognition, volume 1876 of Lecture Notes in Computer Science, pp. 621–630. Springer, 2000.  Bootkrajang, Jakramate and Kabn, Ata. Label-noise robust logistic regression and its applications. In Machine Learning and Knowledge Discovery in Databases, volume 7523 of Lecture Notes in Computer Science, pp. 143–158. Springer, 2012.  Brodley, Carla E. and Friedl, Mark A. Identifying mislabeled training data. Journal of Artiﬁcial Intelligence  Research, 11:131–167, 1999.  Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement. Torch7: A matlab-like environment for machine  learning. In BigLearn, NIPS Workshop, 2011.  Deng, Jia, Dong, Wei, Socher, R., Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248–255, June 2009.  Erhan, Dumitru, Bengio, Yoshua, Courville, Aaron, Manzagol, Pierre-Antoine, Vincent, Pascal, and Bengio, Samy. Why does unsupervised pre-training help deep learning? J. Mach. Learn. Res., 11:625–660, March 2010.  Fergus, Rob, Weiss, Yair, and Torralba, Antonio. Semi-supervised learning in gigantic image collections. In Bengio, Y., Schuurmans, D., Lafferty, J.D., Williams, C.K.I., and Culotta, A. (eds.), Advances in Neural Information Processing Systems 22, pp. 522–530. 2009.  Frenay, B. and Verleysen, M. Classiﬁcation in the presence of label noise: A survey. Neural Networks and  Learning Systems, IEEE Transactions on, 25(5):845–869, May 2014.  Guyon, Isabelle, Matic, Nada, and Vapnik, Vladimir. Discovering informative patterns and data cleaning. In  Advances in Knowledge Discovery and Data Mining, pp. 181–203. 1996.  Hinton, G. E. and Salakhutdinov, R. R. Reducing the dimensionality of data with neural networks. Science,  313(5786):504–507, 2006.  Ipeirotis, Panagiotis G., Provost, Foster, and Wang, Jing. Quality management on amazon mechanical turk. In Proceedings of the ACM SIGKDD Workshop on Human Computation, HCOMP ’10, pp. 64–67. ACM, 2010.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.  Krizhevsky, Alex. cuda-convnet. https://code.google.com/p/cuda-convnet/, 2012.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images. Computer  Science Department, University of Toronto, Tech. Rep, 2009.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convolutional  neural networks. In Advances in Neural Information Processing Systems 25, pp. 1097–1105, 2012.  Larsen, J., Nonboe, L., Hintz-Madsen, M., and Hansen, L.K. Design of robust neural network classiﬁers. In Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference on, volume 2, pp. 1205–1208 vol.2, May 1998.  Le, Q.V. Building high-level features using large scale unsupervised learning. In Acoustics, Speech and Signal  Processing (ICASSP), 2013 IEEE International Conference on, pp. 8595–8598, May 2013.  LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Back- propagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 2014/11/13 1989.  Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, Nov 1998.  Lee, Honglak, Grosse, Roger, Ranganath, Rajesh, and Ng, Andrew Y. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual Interna- tional Conference on Machine Learning, ICML ’09, pp. 609–616. ACM, 2009.  10  Accepted as a workshop contribution at ICLR 2015  Levin, A., Weiss, Y., Durand, F., and Freeman, W. T. Understanding and evaluating blind deconvolution  algorithms. In CVPR, 2009.  Mnih, Volodymyr and Hinton, Geoffrey. Learning to label aerial images from noisy data. In Proceedings of the  29th International Conference on Machine Learning (ICML-12), pp. 567–574, 2012.  Natarajan, Nagarajan, Dhillon, Inderjit, Ravikumar, Pradeep, and Tewari, Ambuj. Learning with noisy labels.  In Advances in Neural Information Processing Systems 26, pp. 1196–1204. 2013.  Nettleton, David, Orriols-Puig, Albert, and Fornells, Albert. A study of the effect of different types of noise on  the precision of supervised learning techniques. Artiﬁcial Intelligence Review, 33(4):275–306, 2010.  Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.  Pechenizkiy, M., Tsymbal, A., Puuronen, S., and Pechenizkiy, O. Class noise and supervised learning in medical domains: The effect of feature extraction. In Computer-Based Medical Systems, 2006. CBMS 2006. 19th IEEE International Symposium on, pp. 708–713, 2006.  Sermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Michael, Fergus, Rob, and LeCun, Yann. Overfeat: Integrated recognition, localization and detection using convolutional networks. In International Conference on Learning Representations (ICLR 2014), April 2014.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition.  arXiv preprint arXiv:1409.1556, 2014.  Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf, Lior. DeepFace: Closing the Gap to Human- Level Performance in Face Veriﬁcation. Conference on Computer Vision and Pattern Recognition (CVPR), 2014.  Talwalkar, A., Kumar, S., and Rowley, H. Large-scale manifold learning. In CVPR, 2008.  Torralba, A., Fergus, R., and Freeman, W.T. 80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(11):1958– 1970, Nov 2008.  van den Hout, Ardo and van der Heijden, Peter G.M. Randomized response, statistical disclosure control and  misclassiﬁcatio: a review. International Statistical Review, 70(2):269–288, 2002.  Zhou, D., Bousquet, O., Lal, T., Weston, J., and Scholkopf, B. Learning with local and global consistency. In  NIPS, 2004.  Zhu, X. and Lafferty, J. Harmonic mixtures: combining mixture models and graph-based methods for inductive  and scalable semi-supervised learning. In ICML, 2005.  Zhu, X., Ghahramani, Z., and Laffery, J. Semi-supervised learning using gaussian ﬁelds and harmonic func-  tions. In ICML, 2003.  Zhu, Xiaojin. Semi-supervised learning literature survey. In Computer Sciences TR 1530, University of Wis-  consin Madison, 2008.  Zhu, Xingquan and Wu, Xindong. Class noise vs. attribute noise: A quantitative study. Artiﬁcial Intelligence  Review, 22(3):177–210, 2004.  11  ",
1412.6806,2015, Striving for Simplicity:  The All Convolutional Net,"['Striving for Simplicity:  The All Convolutional Net', 'Alexey Dosovitskiy', 'Jost Tobias Springenberg', 'Thomas Brox', 'and Martin Riedmiller']",https://arxiv.org/pdf/1412.6806,"5 1 0 2    r p A 3 1         ]  G L . s c [      3 v 6 0 8 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  STRIVING FOR SIMPLICITY: THE ALL CONVOLUTIONAL NET  Jost Tobias Springenberg∗, Alexey Dosovitskiy∗, Thomas Brox, Martin Riedmiller Department of Computer Science University of Freiburg Freiburg, 79110, Germany {springj, dosovits, brox, riedmiller}@cs.uni-freiburg.de  ABSTRACT  Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling lay- ers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We ﬁnd that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this ﬁnding – and building on other recent work for ﬁnding simple network struc- tures – we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recog- nition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the “deconvolution approach” for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.  1  INTRODUCTION AND RELATED WORK  The vast majority of modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: They use alternating convolution and max-pooling layers followed by a small number of fully connected layers (e.g. Jarrett et al. (2009); Krizhevsky et al. (2012); Ciresan et al.). Within each of these layers piecewise-linear activation functions are used. The networks are typically parameterized to be large and regularized during training using dropout. A considerable amount of research has over the last years focused on improving the performance of this basic pipeline. Among these two major directions can be identiﬁed. First, a plethora of extensions were recently proposed to enhance networks which follow this basic scheme. Among these the most notable directions are work on using more complex activation functions (Goodfellow et al., 2013; Lin et al., 2014; Srivastava et al., 2013) techniques for improving class inference (Stollenga et al., 2014; Srivastava & Salakhutdinov, 2013) as well as procedures for improved regularization (Zeiler & Fergus, 2013; Springenberg & Riedmiller, 2013; Wan et al., 2013) and layer-wise pre-training using label information (Lee et al., 2014). Second, the success of CNNs for large scale object recognition in the ImageNet challenge (Krizhevsky et al., 2012) has stimulated research towards experimenting with the different architectural choices in CNNs. Most notably the top entries in the 2014 ImageNet challenge deviated from the standard design principles by either introducing multiple convolutions in between pooling layers (Simonyan & Zisserman, 2014) or by building heterogeneous modules performing convolutions and pooling at multiple scales in each layer (Szegedy et al., 2014). Since all of these extensions and different architectures come with their own parameters and training procedures the question arises which components of CNNs are actually necessary for achieving state of the art performance on current object recognition datasets. We take a ﬁrst step towards answering this question by studying the most simple architecture we could conceive: a homogeneous network solely consisting of convolutional layers, with occasional dimensionality reduction by using a stride of 2. Surprisingly, we ﬁnd that this basic architecture – trained using vanilla stochastic gradient  ∗Both authors contributed equally to this work.  1  Accepted as a workshop contribution at ICLR 2015  descent with momentum – reaches state of the art performance without the need for complicated activation functions, any response normalization or max-pooling. We empirically study the effect of transitioning from a more standard architecture to our simpliﬁed CNN by performing an ablation study on CIFAR-10 and compare our model to the state of the art on CIFAR-10, CIFAR-100 and the ILSVRC-2012 ImageNet dataset. Our results both conﬁrm the effectiveness of using small con- volutional layers as recently proposed by Simonyan & Zisserman (2014) and give rise to interesting new questions about the necessity of pooling in CNNs. Since dimensionality reduction is performed via strided convolution rather than max-pooling in our architecture it also naturally lends itself to studying questions about the invertibility of neural networks (Estrach et al., 2014). For a ﬁrst step in that direction we study properties of our network using a deconvolutional approach similar to Zeiler & Fergus (2014).  2 MODEL DESCRIPTION - THE ALL CONVOLUTIONAL NETWORK  The models we use in our experiments differ from standard CNNs in several key aspects. First – and most interestingly – we replace the pooling layers, which are present in practically all modern CNNs used for object recognition, with standard convolutional layers with stride two. To understand why this procedure can work it helps to recall the standard formulation for deﬁning convolution and pooling operations in CNNs. Let f denote a feature map produced by some layer of a CNN. It can be described as a 3-dimensional array of size W × H × N where W and H are the width and height and N is the number of channels (in case f is the output of a convolutional layer, N is the number of ﬁlters in this layer). Then p-norm subsampling (or pooling) with pooling size k (or half-length k/2) and stride r applied to the feature map f is a 3-dimensional array s(f ) with the following entries:   (cid:98)k/2(cid:99)(cid:88)  (cid:98)k/2(cid:99)(cid:88)  h=−(cid:98)k/2(cid:99)  w=−(cid:98)k/2(cid:99)  1/p  si,j,u(f ) =  |fg(h,w,i,j,u)|p  ,  (1)  where g(h, w, i, j, u) = (r · i + h, r · j + w, u) is the function mapping from positions in s to positions in f respecting the stride, p is the order of the p-norm (for p → ∞, it becomes the commonly used max pooling). If r > k, pooling regions do not overlap; however, current CNN architectures typically include overlapping pooling with k = 3 and r = 2. Let us now compare the pooling operation deﬁned by Eq. 1 to the standard deﬁnition of a convolutional layer c applied to feature map f given as:   (cid:98)k/2(cid:99)(cid:88)  (cid:98)k/2(cid:99)(cid:88)  N(cid:88)  ci,j,o(f ) = σ  h=−(cid:98)k/2(cid:99)  w=−(cid:98)k/2(cid:99)  u=1   ,  θh,w,u,o · fg(h,w,i,j,u)  (2)  where θ are the convolutional weights (or the kernel weights, or ﬁlters), σ(·) is the activation func- tion, typically a rectiﬁed linear activation ReLU σ(x) = max(x, 0), and o ∈ [1, M ] is the number of output feature (or channel) of the convolutional layer. When formalized like this it becomes clear that both operations depend on the same elements of the previous layer feature map. The pooling layer can be seen as performing a feature-wise convolution 1 in which the activation function is replaced by the p-norm. One can therefore ask the question whether and why such special layers need to be introduced into the network. While a complete answer of this question is not easy to give (see the experiments and discussion for further details and remarks) we assume that in general there exist three possible explanations why pooling can help in CNNs: 1) the p-norm makes the represen- tation in a CNN more invariant; 2) the spatial dimensionality reduction performed by pooling makes covering larger parts of the input in higher layers possible; 3) the feature-wise nature of the pooling operation (as opposed to a convolutional layer where features get mixed) could make optimization easier. Assuming that only the second part – the dimensionality reduction performed by pooling – is crucial for achieving good performance with CNNs (a hypothesis that we later test in our experi- ments) one can now easily see that pooling can be removed from a network without abandoning the spatial dimensionality reduction by two means:  1. We can remove each pooling layer and increase the stride of the convolutional layer that  preceded it accordingly.  1That is, a convolution where θh,w,u,o = 1 if u equals o and zero otherwise.  2  Accepted as a workshop contribution at ICLR 2015  2. We can replace the pooling layer by a normal convolution with stride larger than one (i.e. for a pooling layer with k = 3 and r = 2 we replace it with a convolution layer with corresponding stride and kernel size and number of output channels equal to the number of input channels)  The ﬁrst option has the downside that we signiﬁcantly reduce the overlap of the convolutional layer that preceded the pooling layer. It is equivalent to a pooling operation in which only the top-left feature response is considered and can result in less accurate recognition. The second option does not suffer from this problem, since all existing convolutional layers stay unchanged, but results in an increase of overall network parameters. It is worth noting that replacing pooling by convolution adds inter-feature dependencies unless the weight matrix θ is constrained. We emphasize that that this replacement can also be seen as learning the pooling operation rather than ﬁxing it; which has previously been considered using different parameterizations in the literature 2 (LeCun et al., 1998; G¨ulc¸ehre et al., 2014; Jia et al., 2012). We will evaluate both options in our experiments, ensuring a fair comparison w.r.t. the number of network parameters. Although we are not aware of exist- ing studies containing such controlled experiments on replacing pooling with convolution layers it should be noted that the idea of removing pooling is not entirely unprecedented: First, the nomencla- ture in early work on CNNs LeCun et al. (1998) (referring to pooling layers as subsampling layers already) suggests the usage of different operations for subsampling. Second, albeit only consider- ing small networks, experiments on using only convolution layers (with occasional subsampling) in an architecture similar to traditional CNNs already appeared in work on the “neural abstraction pyramid”Behnke (2003). The second difference of the network model we consider to standard CNNs is that – similar to mod- els recently used for achieving state-of-the-art performance in the ILSVRC-2012 competition (Si- monyan & Zisserman, 2014; Szegedy et al., 2014) – we make use of small convolutional layers with k < 5 which can greatly reduce the number of parameters in a network and thus serve as a form of regularization. Additionally, to unify the architecture further, we make use of the fact that if the image area covered by units in the topmost convolutional layer covers a portion of the image large enough to recognize its content (i.e. the object we want to recognize) then fully connected layers can also be replaced by simple 1-by-1 convolutions. This leads to predictions of object classes at different positions which can then simply be averaged over the whole image. This scheme was ﬁrst described by Lin et al. (2014) and further regularizes the network as the one by one convolution has much less parameters than a fully connected layer. Overall our architecture is thus reduced to consist only of convolutional layers with rectiﬁed linear non-linearities and an averaging + softmax layer to produce predictions over the whole image.  Table 1: The three base networks used for classiﬁcation on CIFAR-10 and CIFAR-100.  Model  A 5 × 5 conv. 96 ReLU  B C Input 32 × 32 RGB image 5 × 5 conv. 96 ReLU 3 × 3 conv. 96 ReLU 1 × 1 conv. 96 ReLU 3 × 3 conv. 96 ReLU 3 × 3 max-pooling stride 2 5 × 5 conv. 192 ReLU 5 × 5 conv. 192 ReLU 3 × 3 conv. 192 ReLU 1 × 1 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 max-pooling stride 2 3 × 3 conv. 192 ReLU 1 × 1 conv. 192 ReLU 1 × 1 conv. 10 ReLU  global averaging over 6 × 6 spatial dimensions  10 or 100-way softmax  2Although in order to implement “proper pooling” in the same sense as commonly considered in the litera- ture a special nonlinearity (e.g. a squaring operation) needs to be considered. A simple convolution layer with rectiﬁed linear activation cannot by itself implement a p-norm computation.  3  Accepted as a workshop contribution at ICLR 2015  3 EXPERIMENTS  In order to quantify the effect of simplifying the model architecture we perform experiments on three datasets: CIFAR-10, CIFAR-100 (Krizhevsky & Hinton, 2009) and ILSVRC-2012 ImageNet (Deng et al., 2009) . Speciﬁcally, we use CIFAR-10 to perform an in-depth study of different models, since a large model on this dataset can be trained with moderate computing costs of ≈ 10 hours on a modern GPU. We then test the best model found on CIFAR-10 and CIFAR-100 with and without augmentations and perform a ﬁrst preliminary experiment on the ILSVRC-2012 ImageNet dataset. We performed all experiments using the Caffe (Jia et al., 2014) framework.  3.1 EXPERIMENTAL SETUP  In experiments on CIFAR-10 and CIFAR-100 we use three different base network models which are intended to reﬂect current best practices for setting up CNNs for object recognition. Architectures of these networks are described in Table 1. Starting from model A (the simplest model) the depth and number of parameters in the network gradually increases to model C. Several things are to be noted here. First, as described in the table, all base networks we consider use a 1-by-1 convolution at the top to produce 10 outputs of which we then compute an average over all positions and a softmax to produce class-probabilities (see Section 2 for the rationale behind this approach). We performed additional experiments with fully connected layers instead of 1-by-1 convolutions but found these models to consistently perform 0.5% − 1% worse than their fully convolutional counterparts. This is in line with similar ﬁndings from prior work (Lin et al., 2014). We hence do not report these numbers here to avoid cluttering the experiments. Second, it can be observed that model B from the table is a variant of the Network in Network architecture proposed by Lin et al. (2014) in which only one 1-by-1 convolution is performed after each “normal” convolution layer. Third, model C replaces all 5 × 5 convolutions by simple 3 × 3 convolutions. This serves two purposes: 1) it uniﬁes the architecture to consist only of layers operating on 3 × 3 spatial neighborhoods of the previous layer feature map (with occasional subsampling); 2) if max-pooling is replaced by a convolutional layer, then 3 × 3 is the minimum ﬁlter size to allow overlapping convolution with stride 2. We also highlight that model C resembles the very deep models used by Simonyan & Zisserman (2014) in this years ImageNet competition.  Table 2: Model description of the three networks derived from base model C used for evaluating the importance of pooling in case of classiﬁcation on CIFAR-10 and CIFAR-100. The derived models for base models A and B are built analogously. The higher layers are the same as in Table 1 .  Strided-CNN-C 3 × 3 conv. 96 ReLU 3 × 3 conv. 96 ReLU with stride r = 2  Model ConvPool-CNN-C Input 32 × 32 RGB image 3 × 3 conv. 96 ReLU 3 × 3 conv. 96 ReLU 3 × 3 conv. 96 ReLU 3 × 3 max-pooling stride 2  3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 max-pooling stride 2  with stride r = 2  ...  All-CNN-C 3 × 3 conv. 96 ReLU 3 × 3 conv. 96 ReLU 3 × 3 conv. 96 ReLU with stride r = 2 3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU 3 × 3 conv. 192 ReLU  with stride r = 2  For each of the base models we then experiment with three additional variants. The additional (derived) models for base model C are described in in Table 2. The derived models for base models A and B are built analogously but not shown in the table to avoid cluttering the paper. In general the additional models for each base model consist of:  • A model in which max-pooling is removed and the stride of the convolution layers pre- ceding the max-pool layers is increased by 1 (to ensure that the next layer covers the same spatial region of the input image as before). This is column “Strided-CNN-C” in the table.  4  Accepted as a workshop contribution at ICLR 2015  CNN-C” in the table.  • A model in which max-pooling is replaced by a convolution layer. This is column “All- • A model in which a dense convolution is placed before each max-pooling layer (the ad- ditional convolutions have the same kernel size as the respective pooling layer). This is model “ConvPool-CNN-C” in the table. Experiments with this model are necessary to en- sure that the effect we measure is not solely due to increasing model size when going from a “normal” CNN to its “All-CNN” counterpart.  Finally, to test whether a network solely using convolutions also performs well on a larger scale recognition problem we trained an up-scaled version of ALL-CNN-B on the ILSVRC 2012 part of the ImageNet database. Although we expect that a larger network using only 3 × 3 convolutions and having stride 1 in the ﬁrst layer (and thus similar in style to Simonyan & Zisserman (2014)) would perform even better on this dataset, training it would take several weeks and could thus not be completed in time for this manuscript.  3.2 CLASSIFICATION RESULTS  3.2.1 CIFAR-10  Table 3: Comparison between the base and derived models on the CIFAR-10 dataset.  CIFAR-10 classiﬁcation error  Error (%)  12.47% 13.46%  # parameters Model without data augmentation ≈ 0.9 M Model A ≈ 0.9 M Strided-CNN-A ConvPool-CNN-A 10.21% ≈ 1.28 M ≈ 1.28 M ALL-CNN-A 10.30% ≈ 1 M Model B 10.20% ≈ 1 M Strided-CNN-B 10.98% ≈ 1.35 M ConvPool-CNN-B 9.33% ≈ 1.35 M ALL-CNN-B 9.10% ≈ 1.3 M Model C 9.74% ≈ 1.3 M Strided-CNN-C 10.19% ≈ 1.4 M ConvPool-CNN-C 9.31% ≈ 1.4 M ALL-CNN-C 9.08%  In our ﬁrst experiment we compared all models from Section 3.1 on the CIFAR-10 dataset without using any augmentations. All networks were trained using stochastic gradient descent with ﬁxed momentum of 0.9. The learning rate γ was adapted using a schedule S = e1, e2, e3 in which γ is multiplied by a ﬁxed multiplier of 0.1 after e1.e2 and e3 epochs respectively. To keep the amount of computation necessary to perform our comparison bearable 3 we only treat γ as a changeable hyper- parameter for each method. The learning rate schedule and the total amount of training epochs were determined in a preliminary experiment using base model A and then ﬁxed for all other experiments. We used S = [200, 250, 300] and trained all networks for a total of 350 epochs. It should be noted that this strategy is not guaranteed to result in the best performance for all methods and thus care must be taken when interpreting the the following results from our experiments. The learning rate γ was individually adapted for each model by searching over the ﬁxed set γ ∈ [0.25, 0.1, 0.05, 0.01]. In the following we only report the results for the best γ for each method. Dropout (Hinton et al., 2012) was used to regularize all networks. We applied dropout to the input image as well as af- ter each pooling layer (or after the layer replacing the pooling layer respectively). The dropout probabilities were 20% for dropping out inputs and 50% otherwise. We also experimented with additional dropout (i.e. dropout on all layers or only on the 1 × 1 convolution layer) which however did not result in increased accuracy4 . Additionally all models were regularized with weight decay  3Training one network on CIFAR-10 can take up to 10 hours on a modern GPU. 4In the case were dropout of 0.5 is applied to all layers accuracy even dropped, suggesting that the gradients  become too noisy in this case  5  Accepted as a workshop contribution at ICLR 2015  λ = 0.001. In experiments with data augmentation we perform only the augmentations also used in previous work (Goodfellow et al., 2013; Lin et al., 2014) in order to keep our results comparable. These include adding horizontally ﬂipped examples of all images as well as randomly translated versions (with a maximum translation of 5 pixels in each dimension). In all experiments images were whitened and contrast normalized following Goodfellow et al. (2013). The results for all models that we considered are given in Table 3. Several trends can be observed from the table. First, conﬁrming previous results from the literature (Srivastava et al., 2014) the sim- plest model (model A) already performs remarkably well, achieving 12.5% error. Second, simply removing the max-pooling layer and just increasing the stride of the previous layer results in dimin- ished performance in all settings. While this is to be expected we can already see that the drop in performance is not as dramatic as one might expect from such a drastic change to the network archi- tecture. Third, surprisingly, when pooling is replaced by an additional convolution layer with stride r = 2 performance stabilizes and even improves on the base model. To check that this is not only due to an increase in the number of trainable parameters we compare the results to the “ConvPool” versions of the respective base model. In all cases the performance of the model without any pooling and the model with pooling on top of the additional convolution perform about on par. Surprisingly, this suggests that while pooling can help to regularize CNNs, and generally does not hurt perfor- mance, it is not strictly necessary to achieve state-of-the-art results (at least for current small scale object recognition datasets). In addition, our results conﬁrm that small 3 × 3 convolutions stacked after each other seem to be enough to achieve the best performance. Perhaps even more interesting is the comparison between the simple all convolutional network de- rived from base model C and the state of the art on CIFAR-10 shown in Table 4 , both with and without data augmentation. In both cases the simple network performs better than the best previ- ously reported result. This suggests that in order to perform well on current benchmarks “almost all you need” is a stack of convolutional layers with occasional stride of 2 to perform subsampling.  Table 4: Test error on CIFAR-10 and CIFAR-100 for the All-CNN compared to the state of the art from the literature. The All-CNN is the version adapted from base model C (i.e. All-CNN-C). The other results are from: [1] (Goodfellow et al., 2013), [2] (Lin et al., 2014), [3] (Lee et al., 2014), [4] (Stollenga et al., 2014), [5] (Srivastava & Salakhutdinov, 2013), [6] (Graham, 2015). The number of parameters is given in million parameters.  CIFAR-10 classiﬁcation error  Method without data augmentation Maxout [1] Network in Network [2] Deeply Supervised [3] ALL-CNN (Ours) with data augmentation Maxout [1] DropConnect [2] dasNet [4] Network in Network [2] Deeply Supervised [3] ALL-CNN (Ours)  Error (%)  # params  11.68% 10.41% 9.69% 9.08%  9.38% 9.32% 9.22% 8.81% 7.97% 7.25%  > 6 M ≈ 1 M ≈ 1 M ≈ 1.3 M  > 6 M - > 6 M ≈ 1 M ≈ 1 M ≈ 1.3 M  CIFAR-100 classiﬁcation error  Method CNN + tree prior [5] Network in Network [2] Deeply Supervised [3] Maxout (larger) [4] dasNet [4] ALL-CNN (Ours) Fractional Pooling (1 test) [6] Fractional Pooling (12 tests) [6]  Error (%) 36.85% 35.68% 34.57% 34.54% 33.78% 33.71% 31.45% 26.39%  CIFAR-10 classiﬁcation error  Method with large data augmentation Spatially Sparse CNN [6] Large ALL-CNN (Ours) Fractional Pooling (1 test) [6] Fractional Pooling (100 tests) [6]  Error (%)  4.47% 4.41% 4.50% 3.47%  3.2.2 CIFAR-100  We performed an additional experiment on the CIFAR-100 dataset to conﬁrm the efﬁcacy of the best model (the All-CNN-C) found for CIFAR-10. As is common practice we used the same model as on CIFAR-10 and also kept all hyperparameters (the learning rate as well as its schedule) ﬁxed. Again note that this does not necessarily give the best performance. The results of this experiment are given in Table 4 (right). As can be seen, the simple model using only 3 × 3 convolutions again  6  Accepted as a workshop contribution at ICLR 2015  performs comparable to the state of the art for this dataset even though most of the other methods either use more complicated training schemes or network architectures. It is only outperformed by the fractional max-pooling approach (Graham, 2015) which uses a much larger network (on the order of 50M parameters).  3.2.3 CIFAR-10 WITH ADDITIONAL DATA AUGMENTATION  After performing our experiments we became aware of recent results by Graham (2015) who report a new state of the art on CIFAR-10/100 with data augmentation. These results were achieved using very deep CNNs with 2 × 2 convolution layers in combination with aggressive data augmentation in which the 32 × 32 images are placed into large 126 × 126 pixel images and can hence be heavily scaled, rotated and color augmented. We thus implemented the Large-All-CNN, which is the all convolutional version of this network (see Table 5 in the appendix for details) and report the results of this additional experiment in Table 4 (bottom right). As can be seen, Large-All-CNN achieves performance comparable to the network with max-pooling. It is only outperformed by the fractional max-pooling approach when performing multiple passes through the network. Note that these net- works have vastly more parameters (> 50 M) than the networks from our previous experiments. We are currently re-training the Large-All-CNN network on CIFAR-100, and will include the results in Table 4 once training is ﬁnished.  3.3 CLASSIFICATION OF IMAGENET  We performed additional experiments using the ILVRC-2012 subset of the ImageNet dataset. Since training a state of the art model on this dataset can take several weeks of computation on a mod- ern GPU, we did not aim for best performance, but rather performed a simple ’proof of concept’ experiment. To test if the architectures performing best on CIFAR-10 also apply to larger datasets, we trained an upscaled version of the All-CNN-B network (which is also similar to the architecture proposed by Lin et al. (2014)). It has 12 convolutional layers (conv1-conv12) and was trained for 450, 000 iterations with batches of 64 samples each, starting with a learning rate of γ = 0.01 and dividing it by 10 after every 200, 000 iterations. A weight decay of λ = 0.0005 was used in all layers. The exact architecture used is given in Table 6 in the Appendix. This network achieves a Top-1 validation error of 41.2% on ILSVRC-2012, when only evaluat- ing on the center 224 × 224 patch, – which is comparable to the 40.7% Top-1 error reported by Krizhevsky et al. (2012) – while having less than 10 million parameters (6 times less than the net- work of Krizhevsky et al. (2012)) and taking roughly 4 days to train on a Titan GPU. This supports our intuition that max-pooling may not be necessary for training large-scale convolutional networks. However, a more thorough analysis is needed to precisely evaluate the effect of max-pooling on ImageNet-scale networks. Such a complete quantitative analysis using multiple networks on Ima- geNet is extremely computation-time intensive and thus out of the scope of this paper. In order to still gain some insight into the effects of getting rid of max-pooling layers, we will try to analyze the representation learned by the all convolutional network in the next section.  3.4 DECONVOLUTION  In order to analyze the network that we trained on ImageNet – and get a ﬁrst impression of how well the model without pooling lends itself to approximate inversion – we use a ’deconvolution’ approach. We start from the idea of using a deconvolutional network for visualizing the parts of an image that are most discriminative for a given unit in a network, an approach recently proposed by Zeiler & Fergus (2014). Following this initial attempt – and observing that it does not always work well without max-pooling layers – we propose a new and efﬁcient way of visualizing the concepts learned by higher network layers. The deconvolutional network (’deconvnet’) approach to visualizing concepts learned by neurons in higher layers of a CNN can be summarized as follows. Given a high-level feature map, the ’deconvnet’ inverts the data ﬂow of a CNN, going from neuron activations in the given layer down to an image. Typically, a single neuron is left non-zero in the high level feature map. Then the resulting reconstructed image shows the part of the input image that is most strongly activating this neuron (and hence the part that is most discriminative to it). A schematic illustration of this procedure is shown in Figure 1 a). In order to perform the reconstruction through max-pooling layers, which  7  Accepted as a workshop contribution at ICLR 2015  are in general not invertible, the method of Zeiler and Fergus requires ﬁrst to perform a forward pass of the network to compute ’switches’ – positions of maxima within each pooling region. These switches are then used in the ’deconvnet’ to obtain a discriminative reconstruction. By using the switches from a forward pass the ’deconvnet’ (and thereby its reconstruction) is hence conditioned on an image and does not directly visualize learned features. Our architecture does not include max- pooling, meaning that in theory we can ’deconvolve’ without switches, i.e. not conditioning on an input image. This way we get insight into what lower layers of the network learn. Visualizations of  Figure 1: Schematic of visualizing the activations of high layer neurons. a) Given an input image, we perform the forward pass to the layer we are interested in, then set to zero all activations except one and propagate back to the image to get a reconstruction. b) Different methods of propagating back through a ReLU nonlinearity. c) Formal deﬁnition of different methods for propagating a output activation out back through a ReLU unit in layer l; note that the ’deconvnet’ approach and guided backpropagation do not compute a true gradient but rather an imputed version.  features from the ﬁrst three layers are shown in Figure 2 . Interestingly, the very ﬁrst layer of the network does not learn the usual Gabor ﬁlters, but higher layers do. For higher layers of our network the method of Zeiler and Fergus fails to produce sharp, recogniz- able, image structure. This is in agreement with the fact that lower layers learn general features with limited amount of invariance, which allows to reconstruct a single pattern that activates them. However, higher layers learn more invariant representations, and there is no single image maximally activating those neurons. Hence to get reasonable reconstructions it is necessary to condition on an input image. An alternative way of visualizing the part of an image that most activates a given neuron is to use a simple backward pass of the activation of a single neuron after a forward pass through the network; thus computing the gradient of the activation w.r.t. the image. The backward pass is, by design, partially conditioned on an image through both the activation functions of the network and the max- pooling switches (if present). The connections between the deconvolution and the backpropagation  conv1  conv2  conv3  Figure 2: Visualizations of patterns learned by the lower layers (conv1-conv3) of the network trained on ImageNet. Each single patch corresponds to one ﬁlter. Interestingly, Gabor ﬁlters only appear in the third layer.  8  a)b)c)activation:backpropagation: backward 'deconvnet':       guided backpropagation:Accepted as a workshop contribution at ICLR 2015  approach were recently discussed in Simonyan et al. (2014). In short the both methods differ mainly in the way they handle backpropagation through the rectiﬁed linear (ReLU) nonlinearity. In order to obtain a reconstruction conditioned on an input image from our network without pooling layers we propose a modiﬁcation of the ’deconvnet’, which makes reconstructions signiﬁcantly more accurate, especially when reconstructing from higher layers of the network. The ’deconvolution’ is equivalent to a backward pass through the network, except that when propagating through a nonlin- earity, its gradient is solely computed based on the top gradient signal, ignoring the bottom input. In case of the ReLU nonlinearity this amounts to setting to zero certain entries based on the top gradi- ent. The two different approaches are depicted in Figure 1 b), rows 2 and 3. We propose to combine these two methods: rather than masking out values corresponding to negative entries of the top gra- dient (’deconvnet’) or bottom data (backpropagation), we mask out the values for which at least one of these values is negative, see row 4 of Figure 1 b). We call this method guided backpropagation, because it adds an additional guidance signal from the higher layers to usual backpropagation. This prevents backward ﬂow of negative gradients, corresponding to the neurons which decrease the ac- tivation of the higher layer unit we aim to visualize. Interestingly, unlike the ’deconvnet’, guided backpropagation works remarkably well without switches, and hence allows us to visualize interme- diate layers (Figure 3) as well as the last layers of our network (Figures 4 and 5 in the Appendix). In a sense, the bottom-up signal in form of the pattern of bottom ReLU activations substitutes the switches. To compare guided backpropagation and the ’deconvnet’ approach, we replace the stride in our network by 2 × 2 max-pooling after training, which allows us to obtain the values of switches. We then visualize high level activations using three methods: backpropagation, ’deconvnet’ and guided backpropagation. A striking difference in image quality is visible in the feature visualizations of the highest layers of the network, see Figures 4 and 5 in the Appendix. Guided backpropagation works equally well with and without switches, while the ’deconvnet’ approach fails completely in the absence of switches. One potential reason why the ’deconvnet’ underperforms in this experiment is that max-pooling was only ’artiﬁcially’ introduced after training. As a control Figure 6 shows visualizations of units in the fully connected layer of a network initially trained with max-pooling. Again guided backpropagation produces cleaner visualizations than the ’deconvnet’ approach.  4 DISCUSSION  To conclude, we highlight a few key observations that we made in our experiments:  • With modern methods of training convolutional neural networks very simple architec- tures may perform very well: a network using nothing but convolutions and subsampling matches or even slightly outperforms the state of the art on CIFAR-10 and CIFAR-100. A similar architecture shows competitive results on ImageNet.  • In particular, as opposed to previous observations, including explicit (max-)pooling op- erations in a network does not always improve performance of CNNs. This seems to be especially the case if the network is large enough for the dataset it is being trained on and can learn all necessary invariances just with convolutional layers.  • We propose a new method of visualizing the representations learned by higher layers of a convolutional network. While being very simple, it produces sharper visualizations of descriptive image regions than the previously known methods, and can be used even in the absence of ’switches’ – positions of maxima in max-pooling regions.  We want to emphasize that this paper is not meant to discourage the use of pooling or more sophisti- cated activation functions altogether. It should rather be understood as an attempt to both search for the minimum necessary ingredients for recognition with CNNs and establish a strong baseline on often used datasets. We also want to stress that the results of all models evaluated in this paper could potentially be improved by increasing the overall model size or a more thorough hyperparameter search. In a sense this fact makes it even more surprising that the simple model outperforms many existing approaches.  9  Accepted as a workshop contribution at ICLR 2015  deconv  guided backpropagation  corresponding image crops  deconv  guided backpropagation  corresponding image crops  Figure 3: Visualization of patterns learned by the layer conv6 (top) and layer conv9 (bottom) of the network trained on ImageNet. Each row corresponds to one ﬁlter. The visualization using “guided backpropagation” is based on the top 10 image patches activating this ﬁlter taken from the ImageNet dataset. Note that image sizes are not preserved (in order to save space).  ACKNOWLEDGMENTS  We acknowledge funding by the ERC Starting Grant VideoLearn (279401); the work was also partly supported by the BrainLinks-BrainTools Cluster of Excellence funded by the German Research Foundation (DFG, grant number EXC 1086).  REFERENCES Behnke, Sven. Hierarchical neural networks for image interpretation. PhD thesis, 2003.  Ciresan, Dan C., Meier, Ueli, Masci, Jonathan, Gambardella, Luca M., and Schmidhuber, J¨urgen. High-performance neural networks for visual object classiﬁcation. In arxiv:cs/arXiv:1102.0183. URL http://arxiv.org/abs/1102.0183.  Deng, Jia, Dong, Wei, Socher, Richard, jia Li, Li, Li, Kai, and Fei-fei, Li. Imagenet: A large-scale  hierarchical image database. In CVPR, 2009.  Estrach, Joan B., Szlam, Arthur, and Lecun, Yann. Signal recovery from pooling representations. In  ICML, 2014.  Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.  Maxout networks. In ICML, 2013.  Graham, Benjamin. Fractional max-pooling. In arxiv:cs/arXiv:1412.6071, 2015.  G¨ulc¸ehre, C¸ aglar, Cho, KyungHyun, Pascanu, Razvan, and Bengio, Yoshua. Learned-norm pooling  for deep feedforward and recurrent neural networks. In ECML, 2014.  Hinton, Geoffrey E., Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Rus- lan R. Improving neural networks by preventing co-adaptation of feature detectors. 2012. pre- print, arxiv:cs/1207.0580v3.  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. What is the best  multi-stage architecture for object recognition? In ICCV, 2009.  10  Accepted as a workshop contribution at ICLR 2015  Jia, Yangqing, Huang, Chang, and Darrell, Trevor. Beyond spatial pyramids: Receptive ﬁeld learn-  ing for pooled image features. In CVPR, 2012.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. 2009.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, pp. 1106–1114, 2012.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document  recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998.  Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang, Zhengyou, and Tu, Zhuowen. Deeply su-  pervised nets. In Deep Learning and Representation Learning Workshop, NIPS, 2014.  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. In ICLR: Conference Track,  2014.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. In arxiv:cs/arXiv:1409.1556, 2014.  Simonyan, Karen, Vedaldi, Andrea, and Zisserman, Andrew. Deep inside convolutional networks: Visualising image classiﬁcation models and saliency maps. In 1312.6034, also appeared at ICLR Workshop 2014, 2014. URL http://arxiv.org/abs/1312.6034.  Springenberg, Jost Tobias and Riedmiller, Martin.  Improving deep neural networks with proba- bilistic maxout units. In arXiv:1312.6116, also appeared at ICLR: Workshop Track, 2013. URL http://arxiv.org/abs/1312.6116.  Srivastava, Nitish and Salakhutdinov, Ruslan. Discriminative transfer learning with tree-based pri-  ors. In NIPS. 2013.  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research (JMLR), 15:1929–1958, 2014.  Srivastava, Rupesh K, Masci, Jonathan, Kazerounian, Sohrob, Gomez, Faustino, and Schmidhuber,  J¨urgen. Compete to compute. In NIPS. 2013.  Stollenga, Marijn F, Masci, Jonathan, Gomez, Faustino, and Schmidhuber, J¨urgen. Deep networks  with internal selective attention through feedback connections. In NIPS, 2014.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. In arxiv:cs/arXiv:1409.4842, 2014.  Wan, Li, Zeiler, Matthew D., Zhang, Sixin, LeCun, Yann, and Fergus, Rob. Regularization of neural  networks using dropconnect. In International Conference on Machine Learning (ICML), 2013.  Zeiler, Matthew D. and Fergus, Rob. Stochastic pooling for regularization of deep convolutional  neural networks. In ICLR, 2013.  Zeiler, Matthew D. and Fergus, Rob. Visualizing and understanding convolutional networks.  ECCV, 2014.  In  11  Accepted as a workshop contribution at ICLR 2015  APPENDIX  A LARGE ALL-CNN MODEL FOR CIFAR-10  The complete model architecture for the large All-CNN derived from the spatially sparse network of Benjamin Graham (see Graham (2015) for an explanation) is givenin Table 5 . Note that the network uses leaky ReLU units instead of ReLUs as we found these to speed up training. As can be seen it also requires a much larger input size in which the 32 × 32 pixel image is centered (and then potentially augmented by applying multiple transformations such as scaling). As a result the subsampling performed by the convolutional layers with stride 2 can hence be applied much more slowly. Also note that this network only consists of 2× 2 convolutions with occasional subsampling until the spatial dimensionality is reduced to 1× 1. It does hence not employ global average pooling at the end of the network. In a sense this architecture hence represents the most simple convolutional network usable for this task.  Table 5: Architecture of the Large All-CNN network for CIFAR-10.  Layer name  input conv1 conv2 conv3 conv4 conv5 conv6 conv7 conv8 conv9 conv10 conv11 conv12 conv13 conv14 conv15 conv16 conv17 softmax  Large All-CNN for CIFAR-10 Layer description  Input 126 × 126 RGB image  2 × 2 conv. 320 LeakyReLU, stride 1 2 × 2 conv. 320 LeakyReLU, stride 1 2 × 2 conv. 320 LeakyReLU, stride 2  2 × 2 conv. 960 LeakyReLU, stride 2  2 × 2 conv. 640 LeakyReLU, stride 2  2 × 2 conv. 640 LeakyReLU, stride 1, dropout 0.1 2 × 2 conv. 640 LeakyReLU, stride 1, dropout 0.1 2 × 2 conv. 960 LeakyReLU, stride 1, dropout 0.2 2 × 2 conv. 960 LeakyReLU, stride 1, dropout 0.2 2 × 2 conv. 1280 LeakyReLU, stride 1, dropout 0.3 2 × 2 conv. 1280 LeakyReLU, stride 1, dropout 0.3 2 × 2 conv. 1600 LeakyReLU, stride 1, dropout 0.4 2 × 2 conv. 1600 LeakyReLU, stride 1, dropout 0.4 2 × 2 conv. 1920 LeakyReLU, stride 1, dropout 0.5 1 × 1 conv. 1920 LeakyReLU, stride 1, dropout 0.5  2 × 2 conv. 1280 LeakyReLU, stride 2  2 × 2 conv. 1600 LeakyReLU, stride 2  10-way softmax  B IMAGENET MODEL  The complete model architecture for the network trained on the ILSVRC-2102 ImageNet dataset is given in Table 6 .  12  Accepted as a workshop contribution at ICLR 2015  Table 6: Architecture of the ImageNet network.  Layer name  input conv1 conv2 conv3 conv4 conv5 conv6 conv7 conv8 conv9 conv10 conv11 conv12  global pool  softmax  11 × 11 conv. 96 ReLU units, stride 4  Layer description  ImageNet model Input 224 × 224 RGB image 1 × 1 conv. 96 ReLU, stride 1 3 × 3 conv. 96 ReLU, stride 2 5 × 5 conv. 256 ReLU, stride 1 1 × 1 conv. 256 ReLU, stride 1 3 × 3 conv. 256 ReLU, stride 2 3 × 3 conv. 384 ReLU, stride 1 1 × 1 conv. 384 ReLU, stride 1 3 × 3 conv. 1024 ReLU, stride 1 1 × 1 conv. 1024 ReLU, stride 1 1 × 1 conv. 1000 ReLU, stride 1 global average pooling (6 × 6)  1000-way softmax  3 × 3 conv. 384 ReLU, stride 2, dropout 50 %  C ADDITIONAL VISUALIZATIONS  Additional visualizations of the features learned by the last convolutional layer ’conv12’ as well as the pre-softmax layer ’global pool’ are depicted in Figure 4 and Figure 5 respectively. To al- low fair comparison of ’deconvnet’ and guided backpropagation, we additionally show in Figure 6 visualizations from a model with max-pooling trained on ImageNet.  backpropagation  ’deconvnet’  guided backpropagation  with  pooling + switches  without pooling  Figure 4: Visualization of descriptive image regions with different methods from the single largest activation in the last convolutional layer conv12 of the network trained on ImageNet. Reconstruc- tions for 4 different images are shown.  13  Accepted as a workshop contribution at ICLR 2015  backpropagation  ’deconvnet’  guided backpropagation  with  pooling + switches  without pooling  Figure 5: Visualization of descriptive image regions with different methods from the single largest activation in the pre-softmax layer global pool of the network trained on ImageNet.  backpropagation  ’deconvnet’  guided backpropagation  Figure 6: Visualization of descriptive image regions with different methods from the single largest activation in the last layer fc8 of the Caffenet reference network (Jia et al., 2014) trained on Ima- geNet. Reconstructions for 4 different images are shown.  14  ",
1412.7110,2015, Learning linearly separable features for speech recognition using convolutional neural networks,"['Learning linearly separable features for speech recognition using convolutional neural networks', 'Dimitri Palaz', 'Mathew Magimai Doss', 'and Ronan Collobert']",https://arxiv.org/pdf/1412.7110,"5 1 0 2    r p A 6 1         ]  G L . s c [      6 v 0 1 1 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  LEARNING LINEARLY SEPARABLE FEATURES FOR SPEECH RECOGNITION USING CONVOLUTIONAL NEU- RAL NETWORKS  Dimitri Palaz Idiap Research Institute, Martigny, Switzerland Ecole Polytechnique F´ed´erale de Lausanne (EPFL), Lausanne, Switzerland dimitri.palaz@idiap.ch  Mathew Magimai.-Doss & Ronan Collobert Idiap Research Institute, Martigny, Switzerland mathew@idiap.ch, ronan@collobert.com  ABSTRACT  Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as, speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabil- ities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale contin- uous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classiﬁer in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system us- ing cepstral-based features as input.  1  INTRODUCTION  State-of-the-art automatic speech recognition (ASR) systems typically divide the task into several sub-tasks, which are optimized in an independent manner (Bourlard & Morgan, 1994). In a ﬁrst step, the data is transformed into features, usually composed of a dimensionality reduction phase and an information selection phase, based on the task-speciﬁc knowledge of the phenomena. These two phases have been carefully hand-crafted, leading to state-of-the-art features such as mel frequency cepstral coefﬁcients (MFCCs) or perceptual linear prediction cepstral features (PLPs). In a second step, the likelihood of subword units such as, phonemes is estimated using generative models or discriminative models. In a ﬁnal step, dynamic programming techniques are used to recognize the word sequence given the lexical and syntactical constraints. Recently, in the hybrid HMM/ANN framework (Bourlard & Morgan, 1994), there has been growing interests in using “intermediate” representations, like short-term spectrum, instead of conventional features, such as cepstral-based features. Representations such as Mel ﬁlterbank output or log spec- trum have been proposed in the context of deep neural networks (Hinton et al., 2012). In our recent study (Palaz et al., 2013), it was shown that it is possible to estimate phoneme class conditional probabilities by using temporal raw speech signal as input to convolutional neural networks (LeCun, 1989) (CNNs). This system yielded similar or better results on TIMIT phoneme recognition task with standard hybrid HMM/ANN systems. We also showed that this system is scalable to large vo- cabulary speech recognition task (Palaz et al., 2015). In this case, the CNN-based system was able to outperform the HMM/ANN system with less parameters. In this paper, we investigate the features learning capability of the CNN based system with simple classiﬁers. More speciﬁcally, we replace the classiﬁcation stage of the CNN based system, which was a non-linear multi-layer perceptron, by a linear single layer perceptron. Thus, the features  1  Accepted as a workshop contribution at ICLR 2015  (a) MFCC and PLP extraction pipelines  (b) Typical CNN based pipeline using Mel ﬁlterbank (Sainath et al., 2013a; Swietojanski et al., 2014)  (c) Proposed approach  Figure 1: Illustration of several features extraction pipelines. p(i|x) denotes the conditional proba- bilities for each input frame x, for each label i.  learned by the CNNs are trained to be linearly separable. We evaluate the proposed approach on phoneme recognition task on the TIMIT corpus and on large vocabulary continuous speech recog- nition on the WSJ corpus. We compare our approach with conventional HMM/ANN system using cepstral-based features. Our studies show that the CNN-based system using a linear classiﬁer yields similar or better performance than the ANN-based approach using MFCC features, with fewer pa- rameters. The remainder of the paper is organized as follows. Section 2 presents the motivation of this work. Section 3 presents the architecture of the proposed system. Section 4 presents the experimental setup and Section 5 presents the results. Section 6 presents the discussion and conclude the paper.  2 MOTIVATION  In speech recognition, designing relevant features is not a trivial task, mainly due to the fact that the speech signal is non-stationary and that relevant information is present at different level, namely spectral level and temporal level. Inspired by speech coding studies, feature extraction typically in- volves modeling the envelop of the short-term spectrum. The two most common features along that line are Mel frequency cepstral coefﬁcient (MFCC) (Davis & Mermelstein, 1980) and perceptual linear prediction cepstral coefﬁcient (PLP) (Hermansky, 1990). These features are both based on obtaining a good representation of the short-term power spectrum. They are computed following a series of steps, as presented in Figure 1(a). The extraction process consists of (1) transforming the temporal data in the frequency domain, (2) ﬁltering the spectrum based on critical bands analysis, which is derived from speech perception knowledge, (3) applying a non-linear operation and (4) applying a transformation to get reduced dimension decorrelated features. This process only models the local spectral level information, on a short time window. To model the temporal variation in- trinsic in the speech signal, dynamic features are computed by taking the ﬁrst and second derivative of the static features on the longer time window, and concatenate them together. These resulting features are then fed to the acoustic modeling part of the speech recognition system, which can be based on Gaussian mixture model (GMM) or artiﬁcial neural networks (ANN). In the case of neural networks, the classiﬁer outputs the conditional probabilities p(i|x), with x denoting the input feature and i the class. In recent years, deep neural network (DNN) based and deep belief network (DBN) based approaches have been proposed (Hinton et al., 2006), which yield state-of-the-art results in speech recognition using neural networks composed of many hidden layers. In the case of DBN, the networks are  2  speechsignalFFTCriticalbandsﬁlteringNon-linearoperationDCTlog(·)ARmodeling3√·MFCCPLPDerivatives∆+∆∆Derivatives∆+∆∆++NNclassiﬁerNNclassiﬁerp(i|x)p(i|x)xxspeechsignalFFTCriticalbandsﬁlteringDerivatives∆+∆∆+CNNNNclassiﬁerp(i|x)xspeechsignalCNNNNclassiﬁerp(i|x)xAccepted as a workshop contribution at ICLR 2015  has  also  input  using  based  neural  systems  network  spectrum as  initialized in an unsupervised manner. While this original work relied on MFCC features, several approaches have been proposed to use ‘intermediate” representations (standing between raw signal and “classical” features such as cepstral-based features) as input. In other words, these are approaches that discard several operations in the extraction pipeline of the conventional features (see Figure 1(b)). For instance, Mel ﬁlterbank energies were used as input of convolutional neural net- works based systems (Abdel-Hamid et al., 2012; Sainath et al., 2013a; Swietojanski et al., 2014). Deep been proposed (Mohamed et al., 2012; Lee et al., 2009; Sainath et al., 2013b). Combination of dif- ferent features has also been investigated (Bocchieri & Dimitriadis, 2013). Learning features directly from the raw speech signal using neural networks-based systems has been investigated. In Jaitly & Hinton (2011), the learned features by a DBN are post-processed by adding their temporal derivatives and used as input for another neural network. A recent study investigated acoustic modeling using raw speech as input to a DNN T¨uske et al. (2014). The study showed that raw speech based system is outperformed by spectral feature based system. In our recent stud- ies (Palaz et al., 2013; 2015), we showed that it is possible to estimate phoneme class conditional probabilities by using temporal raw speech signal as input to convolutional neural networks (see Figure 1(c)). This system is composed of several ﬁlter stages, which performs the features learn- ing step and which are implemented by convolution and max-pooling layers, and of a classiﬁcation stage, implemented by a multi-layer perceptron. Both stages are trained jointly. On phoneme recog- nition and on large vocabulary continuous speech recognition task, we showed that the system is able to learn features from the raw speech signal, and yielded performance similar or better than conventional ANN based system that takes cepstral features as input. The proposed system needed less parameters to yield similar performance with conventional systems, suggesting that the learned features seems to be somehow more efﬁcient than cepstral-based features. Motivated by these studies, the goal of the present paper is to ascertain the capability of the con- volutional neural network based system to learn linearly separable features in a data-driven man- ner. To this aim, we replace the classiﬁer stage of the CNN-based system, which was a non-linear multi-layer perceptron, by a linear single layer perceptron. Our objective is not to show that the proposed approach yields state-of-the-art performance, rather show that learning features in a data- driven manner together with the classiﬁer leads to ﬂexible features. Using these features as input for a linear classiﬁer yields better performance than SLP-based baseline system and almost reach the performance of MLP-based system.  3 CONVOLUTIONAL NEURAL NETWORKS  This section presents the architecture used in the paper. in (Palaz et al., 2013), and is presented here for the sake of clarity.  It is similar to the one presented  3.1 ARCHITECTURE  Our network (see Figure 2) is given a sequence of raw input signal, split into frames, and outputs a score for each classes, for each frame. The network architecture is composed of several ﬁlter stages, followed by a classiﬁcation stage. A ﬁlter stage involves a convolutional layer, followed by a temporal pooling layer and a non-linearity (tanh()). Processed signal coming out of these stages are fed to a classiﬁcation stage, which in our case can be either a multi-layer perceptron (MLP) or a linear single layer perceptron (SLP). It outputs the conditional probabilities p(i|x) for each class i, for each frame x.  3.2 CONVOLUTIONAL LAYER  While “classical” linear layers in standard MLPs accept a ﬁxed-size input vector, a convolution layer is assumed to be fed with a sequence of T vectors/frames: X = {x1 x2 . . . xT}. A convolutional layer applies the same linear transformation over each successive (or interspaced by dW frames) windows of kW frames. For example, the transformation at frame t is formally written  3  Accepted as a workshop contribution at ICLR 2015  Figure 2: Convolutional neural network based architecture, which estimates the conditional prob- abilities p(i|x) for each class i, for each frame x. Several stages of convolution/pooling/tanh might be considered. The classiﬁcation stage can be a multi-layer perceptron or a single layer perceptron.  as:  M   xt−(kW−1)/2  xt+(kW−1)/2  ...   ,  (1)  where M is a dout × din matrix of parameters. In other words, dout ﬁlters (rows of the matrix M) are applied to the input sequence.  3.3 MAX-POOLING LAYER  These kind of layers perform local temporal max operations over an input sequence. More formally, the transformation at frame t is written as:  t−(kW−1)/2≤s≤t+(kW−1)/2  max  xd s  ∀d  (2)  with x being the input, kW the kernel width and d the dimension. These layers increase the robust- ness of the network to minor temporal distortions in the input.  3.4 SOFTMAX LAYER  The Sof tmax (Bridle, 1990) layer interprets network output scores fi(x) as conditional probabili- ties, for each class label i:  3.5 NETWORK TRAINING  j  The network parameters θ are learned by maximizing the log-likelihood L, given by:  efi(x)(cid:88)  efj (x)  p(i|x) =  N(cid:88)  n=1  L(θ) =  log(p(in|xn, θ))  for each input x and label i, over the whole training set (composed of N examples), with re- spect to the parameters of each layer of the network. Deﬁning the logsumexp operation as:  logsumexpi(zi) = log((cid:80)  i ezi), the likelihood can be expressed as: L = log(p(i|x)) = fi(x) − logsumexp  j  (fj(x))  where fi(x) described the network score of input x and class i. The log-likelihood is maximized using the stochastic gradient ascent algorithm (Bottou, 1991).  4 EXPERIMENTAL SETUP  In this paper, we investigate using the CNN-based approach on a phoneme recognition task and on a large vocabulary continuous speech recognition task. In this section, we present the two tasks, the databases, the baselines and the hyper-parameters of the networks.  4  (3)  (4)  (5)  RawspeechinputxConvolutionMaxpoolingtanh(·)Filterstage(featurelearning)×NMLP/SLPClassiﬁcationstage(acousticmodeling)p(i|x)Accepted as a workshop contribution at ICLR 2015  4.1 TASKS  4.1.1 PHONEME RECOGNITION  As a ﬁrst experiment, we propose a phoneme recognition study, where the CNN-based system is used to estimate phoneme class conditional probabilities. The decoder is a standard HMM decoder, with constrained duration of 3 states, and considering all phoneme equally probable.  4.1.2 LARGE VOCABULARY SPEECH RECOGNITION  We evaluate the scalability of the proposed system on a large vocabulary speech recognition task on the WSJ corpus. The CNN-based system is used to compute the posterior probabilities of context- dependent phonemes. The decoder is an HMM. The scaled likelihoods are estimated by dividing the posterior probability by the prior probability of each class, estimated by counting on the training set. The hyper parameters such as, language scaling factor and the word insertion penalty are determined on the validation set.  4.2 DATABASES  For the phoneme recognition task, we use the TIMIT acoustic-phonetic corpus. It consists of 3,696 training utterances (sampled at 16kHz) from 462 speakers, excluding the SA sentences. The cross- validation set consists of 400 utterances from 50 speakers. The core test set was used to report the results. It contains 192 utterances from 24 speakers, excluding the validation set. The 61 hand labeled phonetic symbols are mapped to 39 phonemes with an additional garbage class, as presented in (Lee & Hon, 1989). The the large vocabulary speech recognition task, we use the the SI-284 set of the Wall Street Journal (WSJ) corpus (Woodland et al., 1994). It is formed by combining data from WSJ0 and WSJ1 databases, sampled at 16 kHz. The set contains 36416 sequences, representing around 80 hours of speech. Ten percent of the set was taken as validation set. The Nov’92 set was selected as test set. It contains 330 sequences from 10 speakers. The dictionary was based on the CMU phoneme set, 40 context-independent phonemes. 2776 tied-states were used in the experiment. They were derived by clustering context-dependent phones in HMM/GMM framework using decision tree state tying. The dictionary and the bigram language model provided by the corpus were used. The vocabulary contains 5000 words.  4.3 FEATURE INPUT  For the CNN-based system, we use raw features as input. They are simply composed of a window of the temporal speech signal (hence, din = 1 for the ﬁrst convolutional layer). The speech samples in the window are normalized to have zero mean and unit variance. We also performed several baseline experiments, with MFCC as input features. They were computed (with HTK (Young et al., 2002)) using a 25 ms Hamming window on the speech signal, with a shift of 10 ms. The signal is represented using 13th-order coefﬁcients along with their ﬁrst and second derivatives, computed on a 9 frames context.  4.4 BASELINE SYSTEMS  We compare our approach with the standard HMM/ANN system using cepstral features. We train a multi-layer perceptron with one hidden layer, referred to as MLP, and a linear single layer per- ceptron, referred to as SLP. The system inputs are MFCC with several frames of preceding and following context. We do not pre-train the network. The MLP baseline performance is consistent with other works (Fosler & Morris, 2008).  4.5 NETWORKS HYPER-PARAMETERS  The hyper-parameters of the network are: the input window size win, corresponding to the context taken along with each example, the kernel width kWn, the shift dWn and the number of ﬁlters dn of the nth convolution layer, and the pooling width kWmp. We train the CNN based system with  5  Accepted as a workshop contribution at ICLR 2015  several ﬁlter stages (composed of convolution and max-pooling layers). We use between one and ﬁve ﬁlter stages. In the case of linear classiﬁer, the capacity of the system cannot be tuned directly. It depends on the size of the input of the classiﬁer, which can be adjusted by manually tuning the hyper-parameters of the ﬁlter stages. The hyper-parameters were tuned by early-stopping on the frame level classiﬁcation accuracy on the validation set. Ranges which were considered for the grid search are reported in Table 1. A ﬁxed learning rate or 10−4 was used. Each example has a duration of 10 ms. The experiments were implemented using the torch7 toolbox (Collobert et al., 2011). On the TIMIT corpus, using 2 ﬁlter stages, the best performance was found with: 310 ms of context, 30 samples width for the ﬁrst convolution, 7 frames kernel width for the second convolution, 80 and 60 ﬁlters and 3 pooling width. Using 3 ﬁlter stages, the best performance was found with: 310 ms of context, 30 samples width for the ﬁrst convolution, 7 and 7 frames kernel width for the other convolutions, 80, 60 and 60 ﬁlters and 3 pooling width. Using 4 ﬁlter stages, the best performance was found with: 310 ms of context, 30 samples width for the ﬁrst convolution, 7, 7 and 7 frames kernel width for the other convolutions, 80, 60, 60 and 60 ﬁlters and 3 pooling width. We also set the hyper-parameters to have a ﬁxed classiﬁer input. They are presented in Table 2. For the baselines, the MLP uses 500 nodes for the hidden layer and 9 frames as context. The SLP based system uses 9 frames as context. On the WSJ corpus, using 1 ﬁlter stage, the best performance was found with: 210 ms of context, 30 samples width for the ﬁrst convolution, 80 ﬁlters and 50 pooling width. Using 2 ﬁlter stages, the best performance was found with: 310 ms of context, 30 samples width for the ﬁrst convolution, 7 frames kernel width for the other convolutions, 80 and 40 ﬁlters and 7 pooling width. Using 3 ﬁlter stages, the best performance was found with: 310 ms of context, 30 samples width for the ﬁrst convolution, 7 and 7 frames kernel width for the other convolutions, 80, 60 and 60 ﬁlters and 3 pooling width. We also ran experiments using hyper-parameters outside the ranges considered previously using 4 ﬁlter stages. This experiment has the following hyper-parameters: 310 ms of context, 30 samples width for the ﬁrst convolution, 25, 25 and 25 frames kernel width for the other convolutions, 80, 60 and 39 ﬁlters and 2 pooling width. For the baselines, the MLP uses 1000 nodes for the hidden layer and 9 frames as context. The SLP based system uses 9 frames as context.  Table 1: Network hyper-parameters ranges considered for tuning on the validation set.  Hyper-parameter  Units  Range  Input window size (win) Kernel width of the ﬁrst conv. (kW1) Kernel width of the nth conv. (kWn) Number of ﬁlters per kernel (dout) Max-pooling kernel width (kWmp)  ms samples frames ﬁlters frames  100-700 10-90 1-11 20-100 2-6  Table 2: Network hyper-parameters for a ﬁxed output size  # conv. layer win 310 310 430 510 310  1 2 3 4 5  kW1  3 3 3 3 3  kW2 na 7 5 5 5  kW3 na na 5 3 7  kW4 na na na 3 7  kW5 na na na na 7  dn 39 39 39 39 39  kWmp  50 7 4 3 2  # output  351 351 351 351 351  5 RESULTS  The results for the phoneme recognition task on the TIMIT corpus are presented in Table 3. The performance is expressed in terms of phone error rate (PER). The number of parameters in the classiﬁer and in the ﬁlter stages are also presented. Using a linear classiﬁer, the proposed CNN-based system outperforms the MLP based baseline with three or more ﬁlter stages. It can be observed that  6  Accepted as a workshop contribution at ICLR 2015  the performance of the CNN-based system improves with increase in number of convolution layers and almost approaches the case where a MLP (with 60 more parameters) is used in the classiﬁcation stages. Furthermore, it can be observed that the complexity of the classiﬁcation stage decreases drastically with the increase in the number of convolution layers. The results for the proposed system with a ﬁxed output size is presented in Table 4, along with the baseline performance and the number of the parameters in the classiﬁer and ﬁlter stages. The proposed CNN based system outperforms the SLP based baseline with the same number of parameters in the classiﬁer. Fixing the output size seems to degrade the performance compared to Table 3. This indicate that it is better to treat the feature size also as a hyper-parameter and learn it on the data.  Table 3: Results on the TIMIT core testset # conv. layers  # conv. param. Classiﬁer  param.  # classiﬁer  na 3 na 2 3 4  na 61k na 36k 61k 85k  MLP MLP SLP SLP SLP SLP  200k 470k 14k 124k 36k 7k  PER  33.3 % 29.6 % 51.5 % 38.0 % 31.5 % 30.2 %  Features  MFCC RAW MFCC RAW RAW RAW  Table 4: Results for a ﬁxed output on the TIMIT core testset  Features  # conv. layers  # conv. param. Classiﬁer  # classiﬁer  param.  MFCC RAW RAW RAW RAW RAW  na 1 2 3 4 5  na 1.2k 24k 152k 270k 520k  SLP SLP SLP SLP SLP SLP  14k 14k 14k 14k 14k 14k  PER  51.5 % 49.3% 38.0 % 33.4 % 34.6 % 33.1 %  The results for the large vocabulary continuous speech recognition task on the WSJ corpus are presented in Table 5. The performance is expressed in term of word error rate (WER). We observe a similar trend to the TIMIT results, i.e. with the increase in number of convolution layers the performance of the system improves. More speciﬁcally, it can be observed that with only two convolution layers the proposed system is able to achieve performance comparable to SLP-based system with MFCC as input. With three convolution layers the proposed system is approaching the MLP-based systems. With four convolution layers, the system is able to yield similar performance with the MLP baseline using MFCC as input. Overall, it can be observed that the CNN-based approach can lead to systems with simple classiﬁers, i.e. with a small number of parameters, thus shifting the system capacity to the feature learning stage of the system. On the phoneme recognition study (see Table 3), the proposed approach even leads to a system where most parameters lie in the feature learning stage rather than in the classiﬁcation stage. This system yields performance similar to or better than baselines system. On the continuous speech recognition study, it can be observed that the four convolution layers experiment has ﬁve times less parameters in the classiﬁer than the three layers experiment and still yields better performance. This four layers experiement is also able to yield similar performance to the MLP-based baseline with two times less parameters.  6 DISCUSSION AND CONCLUSION  Traditionally in speech recognition systems, feature extraction and acoustic modeling (classiﬁer training) are dealt in two separate steps, where feature extraction is knowledge-driven, and classiﬁer  7  Accepted as a workshop contribution at ICLR 2015  Table 5: Results on the Nov’92 testset of the WSJ corpus.  Features  # conv. layers  # conv. param. Classiﬁer  # classiﬁer  param.  MFCC RAW MFCC RAW RAW RAW RAW  na 3 na 1 2 3 4  na 55k na 5k 27k 64k 180k  MLP MLP SLP SLP SLP SLP SLP  3M 3M 1M 1.3M 1M 2.4M 410k  WER  7.0 % 6.7 % 10.9 % 15.5 % 10.5 % 7.6 % 6.9 %  training in data-driven. In the CNN-based approach with raw speech signal as input, both feature extraction and classiﬁer training is data-driven. Such an approach allows the features to be ﬂexible as they are learned along with the classiﬁer. It also allows to shift the system capacity from the clas- siﬁer stage to the feature extraction stage of the system. Our studies indicate that these empirically learned features can be linearly separable and could yield systems that perform similar to or better than standard spectral-based systems. This can have potential implication for low resource speech recognition. This is part of our future investigation.  ACKNOWLEDGMENTS  This work was supported by the HASLER foundation (www.haslerstiftung.ch) through the grant “Universal Spoken Term Detection with Deep Learning” (DeepSTD). The authors also thank their colleague Ramya Rasipuram for providing the HMM setup for WSJ.  REFERENCES Abdel-Hamid, O., Mohamed, A., Jiang, H., and Penn, G. Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition. In Proc. of ICASSP, pp. 4277–4280, 2012.  Bocchieri, E. and Dimitriadis, D.  Investigating deep neural network based transforms of robust  audio features for lvcsr. In Proc. of ICASSP, pp. 6709–6713, 2013.  Bottou, L. Stochastic gradient learning in neural networks.  Nimes, France, 1991. EC2.  In Proceedings of Neuro-Nmes 91,  Bourlard, H. and Morgan, N. Connectionist speech recognition: a hybrid approach, volume 247.  Springer, 1994.  Bridle, J.S. Probabilistic interpretation of feedforward classiﬁcation network outputs, with rela- tionships to statistical pattern recognition. In Neuro-computing: Algorithms, Architectures and Applications, pp. 227–236. 1990.  Collobert, R., Kavukcuoglu, K., and Farabet, C. Torch7: A matlab-like environment for machine  learning. In BigLearn, NIPS Workshop, 2011.  Davis, S. and Mermelstein, P. Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. IEEE Transactions on Acoustics, Speech and Sig- nal Processing, 28(4):357–366, 1980.  Fosler, E.L. and Morris, J. Crandem systems: Conditional random ﬁeld acoustic models for hid- In Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE  den markov models. International Conference on, pp. 4049 –4052, April 2008.  Hermansky, H. Perceptual linear predictive (plp) analysis of speech. The Journal of the Acoustical  Society of America, 87:1738, 1990.  8  Accepted as a workshop contribution at ICLR 2015  Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., and Sainath, T. N. Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):8297, 2012.  Hinton, G. E., Osindero, S., and Teh, Y. W. A fast learning algorithm for deep belief nets. Neural  computation, 18(7):1527–1554, 2006.  Jaitly, N. and Hinton, G. Learning a better representation of speech soundwaves using restricted  boltzmann machines. In Proc. of ICASSP, pp. 5884–5887, 2011.  LeCun, Y. Generalization and network design strategies. In Pfeifer, R., Schreter, Z., Fogelman, F.,  and Steels, L. (eds.), Connectionism in Perspective, Zurich, Switzerland, 1989. Elsevier.  Lee, H., Pham, P., Largman, Y., and Ng, A. Y. Unsupervised feature learning for audio classiﬁcation using convolutional deep belief networks. In Advances in Neural Information Processing Systems 22, pp. 1096–1104, 2009.  Lee, K. F and Hon, H. W. Speaker-independent phone recognition using hidden markov models.  IEEE Transactions on Acoustics, Speech and Signal Processing, 37(11):1641–1648, 1989.  Mohamed, A., Dahl, G.E., and Hinton, G. Acoustic modeling using deep belief networks. IEEE  Transactions on Audio, Speech, and Language Processing, 20(1):14 –22, jan. 2012.  Palaz, D., Collobert, R., and Magimai.-Doss, M. Estimating phoneme class conditional probabilities  from raw speech signal using convolutional neural networks. In Proc. of Interspeech, 2013.  Palaz, D., Magimai.-Doss, M., and Collobert, R. Convolutional neural networks-based continuous  speech recognition using raw speech signal. In Proc. of ICASSP, preprint, April 2015.  Sainath, T. N., Mohamed, A., Kingsbury, B., and Ramabhadran, B. Deep convolutional neural  networks for lvcsr. In Proc. of ICASSP, pp. 8614–8618, 2013a.  Sainath, T.N., Kingsbury, B., Mohamed, A.-R., and Ramabhadran, B. Learning ﬁlter banks within  a deep neural network framework. In Proc. of ASRU, pp. 297–302, December 2013b.  Swietojanski, P., Ghoshal, A., and Renals, S. Convolutional neural networks for distant speech  recognition. Signal Processing Letters, IEEE, 21(9):1120–1124, September 2014.  T¨uske, Z., Golik, P., Schl¨uter, R., and Ney, H. Acoustic modeling with deep neural networks using  raw time signal for lvcsr. In Interspeech, pp. 890–894, Singapore, September 2014.  Woodland, P.C., Odell, J.J., Valtchev, V., and Young, S.J. Large vocabulary continuous speech  recognition using htk. In Proc. of ICASSP, volume ii, pp. II/125 –II/128 vol.2, apr 1994.  Young, S., Evermann, G., Kershaw, D., Moore, G., Odell, J., Ollason, D., Valtchev, V., and Wood-  land, P. The htk book. Cambridge University Engineering Department, 3, 2002.  9  ",
1412.6596,2015, Training Deep Neural Networks on Noisy Labels with Bootstrapping,"['Training Deep Neural Networks on Noisy Labels with Bootstrapping', 'Scott Reed', 'Honglak Lee', 'Dragomir Anguelov', 'Christian Szegedy', 'Dumitru Erhan', 'and Andrew Rabinovich']",https://arxiv.org/pdf/1412.6596,"5 1 0 2    r p A 5 1         ]  V C . s c [      3 v 6 9 5 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  TRAINING DEEP NEURAL NETWORKS ON NOISY LABELS WITH BOOTSTRAPPING  Scott E. Reed & Honglak Lee Dept. of Electrical Engineering and Computer Science, University of Michigan Ann Arbor, MI, USA {reedscot,honglak}@umich.edu  Dragomir Anguelov, Christian Szegedy, Dumitru Erhan & Andrew Rabinovich Google, Inc. Mountain View, CA, USA {dragomir,szegedy,dumitru,amrabino}@google.com  ABSTRACT  Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overﬁtting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction ob- jective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on sev- eral datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also beneﬁt from unlabeled face images with no modiﬁcation to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.  1  INTRODUCTION  Currently the predominant systems for visual object recognition and detection (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Girshick et al., 2013; Sermanet et al., 2013; Szegedy et al., 2014) use purely supervised training with regularization such as dropout (Hinton et al., 2012) to avoid overﬁtting. These systems do not account for missing labels, subjective labeling or inexhaustively- annotated images. However, this assumption often does not hold, especially for very large datasets and in high-resolution images with complex scenes. For example, in recognition, the class labels may be missing; in detection, the objects in the image may not all be localized; in subjective tasks such as facial emotion recognition, humans may not even agree on the class label. As training sets for deep networks become larger (as they should), the problem of missing and noisy labels becomes more acute, and so we argue it is a fundamental problem for scaling up vision. In this work we propose a simple approach to hande noisy and incomplete labeling in weakly- supervised deep learning, by augmenting the usual prediction objective with a notion of perceptual consistency. We consider a prediction consistent if the same prediction is made given similar per- cepts, where the notion of similarity incorporates features learned by the deep network. One interpretation of the perceptual consistency objective is that the learner makes use of its rep- resentation of the world (implicit in the network parameters) to match incoming percepts to known  1  Accepted as a workshop contribution at ICLR 2015  categories, or in general structured outputs. This provides the learner justiﬁcation to “disagree” with a perceptually-inconsistent training label, and effectively re-label the data while training. More accurate labels may lead to a better model, which allows further label clean-up, and the learner bootstraps itself in this way. Of course, too much skepticism of the labels carries the risk of ending up with a delusional agent, so it is important to balance the trade-off between prediction and the learner’s perceptual consistency. In our experiments we demonstrate that our approach yields substantial robustness to several types of label noise on several datasets. On MNIST handwritten digits (LeCun & Cortes, 1998) we show that our model is robust to label corruption. On the Toronto Face Database (Susskind et al., 2010) we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the-art results, and can also beneﬁt from unlabeled face images with no modiﬁcation to our method. On the ILSVRC2014 detection challenge data (Russakovsky et al., 2014), we show that our approach improves single-shot person detection using a MultiBox network (Erhan et al., 2014), and also improves performance in full 200-way detection using MultiBox for region proposal and a deep CNN for post-classiﬁcation. In section 2 we discuss related work, in section 3 we describe our method along with a probabilistic interpretation and in section 4 we present our results.  2 RELATED WORK  The literature on semi-supervised and weakly-supervised learning is vast (see Zhu (2005) for a survey), and so in this section we focus on the key previous papers that inspired this work and on other papers on weakly- and semi-supervised deep learning. The notion of bootstrapping, or “self-training” a learning agent was proposed in (Yarowsky, 1995) as a way to do word-sense disambiguation with only unlabeled examples and a small list of seed example sentences with labels. The algorithm proceeds by building an initial classiﬁer using the seed examples, and then iteratively classifying unlabeled examples, extracting new seed rules for the classiﬁer using the now expanded training data, and repeating these steps until convergence. The algorithm was analyzed by Abney (Abney, 2004) and more recently by (Haffari & Sarkar, 2012). Co-training (Blum & Mitchell, 1998; Nigam et al., 2000; Nigam & Ghani, 2000) was similarly- motivated but used a pair of classiﬁers with separate views of the data to iteratively learn and gen- erate additional training labels. Whitney & Sarkar (2012) proposed bootstrapping labeled training examples with graph-based label propagation. Brodley & Friedl (1999) developed statistical meth- ods for identifying mislabeled training data. Rosenberg et al. (2005) also trained an object detection system in a weakly-supervised manner using self-training, and demonstrated that their proposed model achieved comparable performance to models trained with a much larger set of labels. However, that approach works as a wrapper around an existing detection system, whereas in this work we integrate a consistency objective for bootstrapping into the training of the deep network itself. Our work shares a similar motivation to these earlier works, but instead of explicitly generating new training labels and adding new examples to the training set in an outer loop, we incorporate our consistency objective directly into the model. In addition, we consider not only the case of learning from unlabeled examples, but also from noisy labels and inexhaustively-annotated examples. Mnih & Hinton (2012) developed deep neural networks for improved labeling of aerial images, with robust loss functions to handle label omission and registration errors. This work shares a similar motivation of robustness to noisy labels, but rather than formulating loss functions for speciﬁc types of noise, we add a generic consistency objective to the loss to achieve robustness. Minimum entropy regularization, proposed in (Grandvalet & Bengio, 2005; 2006), performs semi- supervised learning by augmenting cross-entropy loss with a term encouraging the classiﬁer to make predictions with high conﬁdence on the unlabeled examples1. This is notable because in their ap- proach training on unlabeled examples does not require a generative model, which is beneﬁcial for training on high-resolution images and other sensory data. We take a similar approach by side-  1see eq. 9.7 in (Grandvalet & Bengio, 2006)  2  Accepted as a workshop contribution at ICLR 2015  stepping the difﬁculty of fully-generative models of high-dimensional sensory data. However, we extend beyond shallow models to deep networks, and to structured output prediction. Never ending language learning (NELL) (Carlson et al., 2010) and never ending image learning (NEIL) (Chen et al., 2013; 2014) are lifelong-learning systems for language and image understand- ing, respectively. They continuously bootstrap themselves using a cycle of data collection, propa- gation of labels to the newly collected data, and self-improvement by training on the new data. Our work is complementary to these efforts, and focuses on building robustness to noisy and missing labels into the model for weakly-supervised deep learning. Larochelle & Bengio (2008) developed an RBM for classiﬁcation that uses a hybrid generative and discriminative training objective. Deep Boltmann Machines (Salakhutdinov & Hinton, 2009) can also be trained in a semi-supervised manner with labels connected to the top layer. More recently, multi-prediction DBM training (Goodfellow et al., 2013) and Generative Stochastic Networks (Ben- gio & Thibodeau-Laufer, 2013) improved the performance and simpliﬁed the training of deep gener- ative models, enabling training via backpropagation much like in standard deep supervised networks. However, fully-generative unsupervised training on high-dimensional sensory data, e.g. ImageNet images, is still far behind supervised methods in terms of performance, and so in this work we do not follow the generative approach directly. Instead, this work focuses on a way to beneﬁt from unlabeled and weakly-labeled examples with minimal modiﬁcation to existing deep supervised net- works. We demonstrate increased robustness to label noise and performance improvements from unlabeled data for a minimal engineering effort. More recently, the problem of deep learning from noisy labels has begun to receive attention. Lee (2013) also followed the idea of minimum entropy regularization, and proposed generating “pseudo- labels” as training targets for unlabeled data, and showed improved performance on MNIST with few labeled examples. Sukhbaatar & Fergus (2014) developed two deep learning techniques for handling noisy labels, learning to model the noise distribution in a top-down and bottom-up fashion. In this work, we push further by extending beyond class labels to structured outputs, and we achieve state-of-the-art scalable detection performance on ILSVRC2014, despite the fact that our method does not require explicitly modeling the noise distribution.  3 METHOD  In this section we describe two approaches: section 3.1 uses reconstruction error as a consistency objective and explicitly models the noise distribution as a matrix mapping model predictions to training labels. A reconstruction loss is added to promote top-down consistency of model predictions with the observations, which allows the model to discover the pattern of noise in the data. The method presented in section 3.2 (bootstrapping) uses a convex combination of training labels and the current model’s predictions to generate the training targets, and thereby avoids directly modeling the noise distribution. This property is well-suited to the case of structured outputs, for which modeling dense interactions among all pairs of output units may be neither practical nor useful. These two approaches are compared empirically in section 4. In section 3.3 we show how to apply our bootstrapping approach to structured outputs by using the MultiBox (Erhan et al., 2014) region proposal network to handle the case of inexhaustive structured output labeling for single-shot person detection and for class-agnostic region proposal.  3.1 CONSISTENCY IN MULTI-CLASS PREDICTION VIA RECONSTRUCTION  Let x ∈ {0, 1}D be the data (or deep features computed from the data) and t ∈ {0, 1}L,(cid:80) as a latent multinomial variable q ∈ {0, 1}L,(cid:80)  k tk = 1 the observed noisy multinomial labels. The standard softmax regresses x onto t without taking into account noisy or missing labels. In addition to optimizing the conditional log-likelihood log P (t|x), we add a regularization term encouraging the class prediction to be perceptually consistent. We ﬁrst introduce into our model the “true” class label (as opposed to the noisy label observations) j qj = 1. Our deep feed-forward network models  3  L(cid:88)  j=1  L(cid:88)  Accepted as a workshop contribution at ICLR 2015  the posterior over q using the usual softmax regression:  P (qj = 1|x) =  (cid:80)L  ˜P (qj = 1|x) j(cid:48)=1  ˜P (qj(cid:48) = 1|x)  =  exp((cid:80)D (cid:80)L j(cid:48)=1 exp((cid:80)D  i=1 W (1)  ij xi + b(1)  i  )  i=1 W (1)  ij(cid:48) xi + b(1)  i  (1)  )  where ˜P denotes the unnormalized probability distribution. Given the true label q, the label noise can be modeled using another softmax with logits as follows:  log ˜P (tk = 1|q) =  W (2)  kj qj + b(2)  k  (2)  L(cid:88)  Roughly, W (2) kj observation x, we can marginalize over q to compute the posterior of target t given x.  learns the log-probability of observing true label j as noisy label k. Given only an  P (tk = 1|x) =  P (tk = 1, qj = 1|x) =  P (tk = 1|qj = 1)P (qj = 1|x)  (3)  j=1  j=1  where the label noise distribution and posterior over true labels are deﬁned above. We can per- form discriminative training by gradient ascent on log P (t|x). However, this purely discriminative training does not yet incorporate perceptual consistency, and there is no explicit incentive for the model to treat q as the “true” label; it can be viewed as another hidden layer, with the multinomial constraint resulting in an information bottleneck. In unpublished work2, Hinton & Mnih (2009) developed a Restricted Boltzmann Machine (RBM) (Smolensky, 1986) variant with hidden multinomial output unit q and observed noisy la- bel unit t as described above. The associated energy function can be written as  E(x, t, q) = − D(cid:88)  L(cid:88)  ij xiqj − L(cid:88)  W (1)  L(cid:88)  kj tkqj − L(cid:88)  j qj − L(cid:88)  b(1)  k tk − D(cid:88)  b(2)  W (2)  b(3) i xi (4)  i=1  j=1  k=1  j=1  j=1  k=1  i=1  Due to the bipartite structure of the RBM, t and x are conditionally independent given q, and so the energy function in eq. (4) leads to a similar form of the posterior as in eq. (3), marginalizing out the hidden multinomial unit. The probability distribution arising from (4) is given by  where Z = (cid:80) likelihood P (xi = 1|q) = σ((cid:80)L  x,t,q exp(−E(x, t, q)) is the partition function. The model can be trained with a generative objective, e.g. by approximate gradient ascent on log P (x, t) via contrastive diver- gence (Hinton, 2002). Generative training naturally provides a notion of consistency between obser- vations x and predictions q because the model learns to draw sample observations via the conditional  j=1 W (1)  ij qj + b(3)  i  ), assuming binary observations.  (cid:80) q exp(−E(x, t, q))  Z  P (x, t) =  Figure 1: Left: Restricted Boltzmann Machine with hidden multinomial output unit. Right: Analo- gous feed-forward autoencoder version.  2Known from personal correspondence.  4  True output labels qObserved data xObserved noisy label tTrue output labels qObserved data xObserved noisy label tData reconstructionAccepted as a workshop contribution at ICLR 2015  However, fully-generative training is complicated by the fact that the exact likelihood gradient is intractable due to computing the partition function Z, and in practice MCMC is used. Generative training is further complicated (though certainly still possible) in cases where the features x are non- binary. To avoid these complications, and to make our approach rapidly applicable to existing deep networks using rectiﬁed linear activations, and trainable via exact gradient descent, we propose an analogous autoencoder version. Figure 1 compares the RBM and autoencoder approaches to the multiclass prediction problem with perceptual consistency. The overall objective in the feed-forward version is as follows:  (5) where q(x)j = P (qj = 1|x) as in equation 1. The parameter β can be found via cross-validation. Experimental results using this method are presented in sections 4.1 and 4.2.  tk log P (tk = 1|x) + β||x − W (2)q(x)||2  k=1  2  Lrecon(x, t) = − L(cid:88)  3.2 CONSISTENCY IN MULTI-CLASS PREDICTION VIA BOOTSTRAPPING  In this section we develop a simple consistency objective that does not require an explicit noise distribution or a reconstruction term. The idea is to dynamically update the targets of the prediction objective based on the current state of the model. The resulting targets are a convex combination of (1) the noisy training label, and (2) the current prediction of the model. Intuitively, as the learner improves over time, its predictions can be trusted more. This mitigates the damage of incorrect labeling, because incorrect labels are likely to be eventually highly inconsistent with other stimuli predicted to have the same label by the model. By paying less heed to inconsistent labels, the learner can develop a more coherent model, which further improves its ability to evaluate the consistency of noisy labels. We refer to this approach as “bootstrapping”, in the sense of pulling oneself up by one’s own bootstraps, and also due to inspiration from the work of Yarowsky (1995) which is also referred to as bootstrapping. Concretely, we use a cross-entropy objective as before, but generate new regression targets for each SGD mini-batch based on the current state of the model. We empirically evaluated two types of boot- strapping. “Soft” bootstrapping uses predicted class probabilities q directly to generate regression targets for each batch as follows:  Lsof t(q, t) =  [βtk + (1 − β)qk] log(qk)  (6)  In fact, it can be shown that the resulting objective is equivalent to softmax regression with minimum entropy regularization, which was previously studied in (Grandvalet & Bengio, 2006). Intuitively, minimum entropy regularization encourages the model to have a high conﬁdence in predicting labels (even for the unlabeled examples, which enables semi-supervised learning). “Hard” bootstrapping modiﬁes regression targets using the MAP estimate of q given x, which we denote as zk := 1[k = argmax qi, i = 1...L]:  Lhard(q, t) =  [βtk + (1 − β)zk] log(qk)  (7)  When used with mini-batch stochastic gradient descent, this leads to an EM-like algorithm: In the E-step, estimate the “true” conﬁdence targets as a convex combination of training labels and model predictions; in the M-step, update the model parameters to better predict those generated targets. Both hard and soft bootstrapping can be viewed as instances of a more general approach in which model-generated regression targets are modulated by a softmax temperature parameter T ; i.e.  P (qj = 1|x) =  (8) Setting T = 1 recovers soft boostrapping, and T = ∞ recovers hard bootstrapping. We only use these two operating points in our experiments, but it may be worthwhile to explore other values for T , and learning T for each dataset.  i=1 W (1)  j(cid:48) ))  i=1 W (1)  ij xi + b(1)  j )) ij(cid:48) xi + b(1)  exp(T · ((cid:80)D j(cid:48)=1 exp(T · ((cid:80)D (cid:80)L  5  L(cid:88)  k=1  L(cid:88)  k=1  Accepted as a workshop contribution at ICLR 2015  3.3 CONSISTENCY WITH STRUCTURED OUTPUT PREDICTION  Noisy labels also occur in structured output prediction problems such as object detection. Current state-of-the-art object detection systems train on images annotated with bounding box labels of the relevant objects in each image, and the class label for each box. However, it is expensive to exhaus- tively annotate each image, and for some commonly-appearing categories the data may be prone to missing annotations. In this section, we modify the training objective of the MultiBox (Erhan et al., 2014) network for object detection to incorporate a notion of perceptual consistency into the loss. In the MultiBox approach, ground-truth bounding boxes are clustered and the resulting centroids are used as “priors” for predicting object location. A deep neural network is trained to predict, for each groundtruth object in an image, a residual of that groundtruth bounding box to the best-matching bounding box prior. The network also outputs a logistic conﬁdence score for each prior, indicating the model’s belief of whether or not an object appears in the corresponding location. Because MultiBox gives proposals with conﬁdence scores, it enables very efﬁcient runtime-quality tradeoffs for detection via thresholding the top-scoring proposals within budget. Thus it is an attractive target for further quality improvements, as we pursue in this section. Denote the conﬁdence score training targets as t ∈ {0, 1}L and the predicted conﬁdence scores as c ∈ [0, 1]L. The objective for MultiBox 3 can be written as the following cross-entropy loss:  (tk log(ck) + (1 − tk) log(1 − ck))  (9)  Lmultibox(c, t) = − L(cid:88)  k=1  Note that the sum here is over object locations, not class labels as in the case of sections 3.1 and 3.2. If there is an object at location k, but tk = 0 due to inexhaustive annotation, the model pays a large cost for correctly predicting ck = 1. Training naively on the noisy labels leads to perverse learning situations such as the following: two objects of the same category (potentially within the same image) appear in the training data, but only one of them is labeled. To reduce the loss, the conﬁdence prediction layer must learn to distinguish the two objects, which is exactly contrary to the objective of visual invariance to category-preserving differences. To incorporate a notion of perceptual consistency into the loss, we follow the same approach as in the case of multi-class classiﬁcation: augment the regression targets using the model’s current state. In the “hard” case, MAP estimates can be obtained by thresholding ck > 1/2.  k=1  Lmultibox−hard(c, t) = − L(cid:88) − L(cid:88) Lmultibox−sof t(c, t) = − L(cid:88) − L(cid:88)  k=1  k=1  [βtk + (1 − β)1ck>0.5] log(ck)  [β(1 − tk) + (1 − β)(1 − 1ck>0.5)] log(1 − ck)  [βtk + (1 − β)ck] log(ck)  [β(1 − tk) + (1 − β)(1 − ck)] log(1 − ck)  (10)  (11)  k=1  With the bootstrap variants of the MultiBox objective, unlabeled positives pose less of a problem because penalties for large ck are down-scaled by factor β in the ﬁrst term and (1− ck) in the second term. By mitigating penalties due to missing positives in the data, our approach allows the model to learn to predict ck with high conﬁdence even if the objects at location k are often unlabeled.  4 EXPERIMENTS  We perform experiments on three image understanding tasks: MNIST handwritten digits recogni- tion, Toroto Faces Database facial emotion recognition, and ILSVRC2014 detection. In all tasks,  3We omit the bounding box regression term for simplicity, see (Erhan et al., 2014) for full details.  6  Accepted as a workshop contribution at ICLR 2015  we train a deep neural network with our proposed consistency objective. In our ﬁgures, “bootstrap- recon” refers to training as described in section 3.1, using reconstruction as a consistency objective. “bootstrap-soft” and “bootstrap-hard” refer to our method described in sections 3.2 and 3.3.  4.1 MNIST WITH NOISY LABELS  In this section we train using our reconstruction-based objective (detailed in section 3.1) on MNIST handwritten digits with varying degrees of noise in the labels. Speciﬁcally, we used a ﬁxed random permutation of the labels as visualized in ﬁgure 2, and we perform control experiments while varying the probability of applying the label permutation to each training example. All models were trained with mini-batch SGD, with the same architecture: 784-500-300-10 neural network with rectiﬁed linear units. We used L2 weight decay of 0.0001. We found that β = 0.8 worked best for bootstrap-hard, 0.95 for bootstrap-soft, and 0.005 for bootstrap-recon. We initialize W (2) to the identity matrix. For the network trained with our proposed consistency objective, we initialized the network layers from the baseline prediction-only model. It is also possible to initialize from scratch using our approach, but we found that with pre-training we could use a larger β and more quickly converge to a good result. Intuitively, this is similar to the initial collection of “seed” rules in the original bootstrapping algorithm of (Yarowsky, 1995). During the ﬁne-tuning training phase, all network weights are updated by backpropagating gradients through the layers. Figure 2 shows that our bootstrapping method provides a very signiﬁcant beneﬁt in the case of permuted labels. The bootstrap-recon method performs the best, and bootstrap-hard nearly as well. The bootstrap-soft method provides some beneﬁt in the high-noise regime, but only slightly better than the baseline overall.  Figure 2: Left: Digit recognition accuracy versus percent corrupted labels. Middle: a visualization of the noise pattern. If there is a white entry in row r, column c, then label c is mapped to to label r with some probability during training. Right: a visualization of P (tr = 1|qc = 1) learned by our bootstrap-recon model (as parameters W (2)  rc ) trained with 40% label noise.  Figure 2 also shows that bootstrap-recon effectively learns the noise distribution P (t|q) via the parameters W (2) kj . Intuitively, the loss from the reconstruction term provides the learner a basis on which predictions q may disagree with training labels t. Since x must be able to be reconstructed from q in bootstrap-recon, learning a non-identity W (2) allows q to ﬂexibly vary from t to better reconstruct x without incurring a penalty from prediction error. However, it is interesting to note that bootstrap-hard achieves nearly equivalent performance without explicitly parameterizing the noise distribution. This is useful because reconstruction may be chal- lenging in many cases, such as when x is drawn from a complicated, high-dimensional distribution, and bootstrap-hard is trivial to implement on top of existing deep supervised networks.  7  01234567890    1    2   3   4    5    6   7   8  901234567890    1    2   3   4    5    6   7   8  9Label noise patternLearned Noisy LabelTrue LabelNoisy LabelTrue LabelAccepted as a workshop contribution at ICLR 2015  4.2 TORONTO FACES DATABASE EMOTION RECOGNITION  In this section we present results on emotion recognition. The Toronto Faces Database has 112,234 images, 4,178 of which have emotion labels. In all experiments we ﬁrst extracted spatial-pyramid- pooled OMP-1 features as described in (Coates & Ng, 2011) to get 3200-dimensional features. We then trained a 3200-1000-500-7 network to predict the 1-of-7 emotion labels for each image. As in the case for our MNIST experiments, we initialize our model from the network pre-trained with prediction only, and the ﬁne-tuned all layers with our hybrid objective. Figure 3 summarizes our TFD results. As in the case of MNIST, bootstrap-recon and bootstrap-hard perform the best, signiﬁcantly outperforming the softmax baseline, and bootstrap-soft provides a more modest improvement.  Training baseline bootstrap-recon bootstrap-hard bootstrap-soft disBM a CDA+CCA b  Accuracy (%)  85.3 86.8 86.8 85.6 85.4 85.0  Table 1: Emotion recognition results on Toronto Faces Database compared to state-of-the-art methods.  Figure 3: Predicted-to-noisy emotion label con- nection W (2) learned by our model.  a(Reed et al., 2014) b(Rifai et al., 2012)  There is signiﬁcant off-diagonal weight in W (2) learned by bootstrap-recon on TFD, suggesting that the model learns to “hedge” its emotion prediction during training by spreading probability mass from the predicted class to commonly-confused classes, such as “afraid” and “surprised”. The strongest off diagonals are in the “happy” column, which may be due to the fact that “happy” is the most common expression, or perhaps that happy expressions have a large visual diversity. Our method improves the emotion recognition performance, which to our knowledge is state-of-the-art. The performance improvement from all three bootstrap methods on TFD suggests that our approach can be useful not just for mistaken labels, but also for semi-supervised learning (missing labels) and learning from weak labels such as emotion categories.  4.3  ILSVRC 2014 FAST SINGLE-SHOT PERSON DETECTION  In this section we apply our method to detecting persons using a MultiBox network built on top of the Inception architecture proposed in (Szegedy et al., 2014). We ﬁrst pre-trained MultiBox on class- agnostic localization using the full ILSVRC2014 training set, since there are only several thousand images labeled with persons in ILSVRC, and then ﬁne-tuned on person images only. An important point for comparison is the top-K bootstrapping heuristic introduced for MultiBox training in (Szegedy et al., 2014) for person detection in the presence of missing annotations. In that approach, the top-K largest conﬁdence predictions are dropped from the loss (which we show here in eq. (9)), and the setting used was K = 4. In other words, there is no gradient coming from the top-K most conﬁdent location predictions. In fact, it can be viewed as a form of bootstrapping where only the top-K most conﬁdent locations modify their targets, which become the predictions themselves. In this work, we aim to achieve similar or better performance in a more general way that can be applied to MultiBox and other discriminative models. The precision-recall curves in ﬁgure 4 show that our proposed bootstrapping improves substantially over the prediction-only baseline. At the high-precision end of the PR curve, the approaches intro- duced in this paper perform better, while the top-K heuristic is slightly better at high-recall.  8  AngerDisgustAfraidHappySadSurprisedNeutralAngerDisgustAfraidHappySadSurprisedNeutral Noisy (Weak)  Label Predicted True  LabelAccepted as a workshop contribution at ICLR 2015  Training baseline top-K heuristic bootstrap-hard bootstrap-soft  AP (%) Recall @ 60p  30.9 44.1 44.6 43.6  14.3 43.4 43.9 42.8  Table 2: Our proposed bootstrap-hard and bootstrap-soft, and the top-K heuristic signiﬁ- cantly improve average precision compared to the prediction-only baseline. Recall@60p is the recall achievable at 60% precision.  Figure 4: Precision-recall curves for our meth- ods compared to the baseline prediction-only network, and the top-K heuristic.  4.4  ILSVRC 2014 200-CATEGORY DETECTION  In this section we apply our method to the case of object detection on the large-scale ImageNet data. Our proposed method is applied in two ways: ﬁrst to the MultiBox network for region proposal, and second to the classiﬁer network that predicts labels for each cropped image region. We follow the approach in (Szegedy et al., 2014) and combine image crops from MultiBox region proposals with deep network context features as the input to the classiﬁer for each proposed region. We trained the MultiBox network as described in 3.3, and the post-classiﬁer network as described in section 3.2. We found that “hard” performed better than the “soft” form of bootstrapping.  MultiBox baseline baseline bootstrap-hard bootstrap-hard - -  Postclassiﬁer baseline bootstrap-hard baseline bootstrap-hard GoogLeNet single modela DeepID-Net single modelb  mAP (%) Recall @ 60p  39.8 40.0 40.3 40.3 38.8 40.1  38.4 38.6 39.3 39.1  - -  Using our proposed bootstrapping method, we observe modest improvement on the ILSVRC2014 detection “val2” data4, mainly attributable to bootstrapping in MultiBox training.  5 CONCLUSIONS  In this paper we developed novel training methods for weakly-supervised deep learning, and demon- strated the effectiveness of our approach on multi-class prediction and structured output prediction for several datasets. Our method is exceedingly simple and can be applied with very little engi- neering effort to existing networks trained using a purely-supervised objective. The improvements that we show even with very simple methods, suggest that moving beyond purely-supervised deep learning is worthy of further research attention. In addition to achieving better performance with the data we already have, our results suggest that performance gains may be achieved from collecting more data at a cheaper price, since image annotation need not be as exhasutive and mistaken labels are not as harmful to the performance. In future work, it may be promising to consider learning a time-dependent policy for tuning β, the scaling factor between prediction and perceptual consistency objectives, and also to extend our approach to the case of a situated agent. Another promising direction is to augment large-scale training for detection (e.g. ILSVRC) with unlabeled and more weakly-labeled images, to further beneﬁt from our proposed perceptual consistency objective.  4In a previous version of this draft we included numbers on the full validation set. However, we discovered  that context and post-classiﬁer models used “val1” data, so we re-ran experiments only on the “val2” subset  9  00.10.20.30.40.50.60.70.800.10.20.30.40.50.60.70.80.91ILSVRC Person DetectionRecallPrecision  baselinetop−K heuristicbootstrap−hardbootstrap−softAccepted as a workshop contribution at ICLR 2015  REFERENCES Abney, Steven. Understanding the yarowsky algorithm. Computational Linguistics, 30(3):365–395, 2004. 2  Bengio, Yoshua and Thibodeau-Laufer, Eric. Deep generative stochastic networks trainable by backprop. arXiv  preprint arXiv:1306.1091, 2013. 3  Blum, Avrim and Mitchell, Tom. Combining labeled and unlabeled data with co-training. In Proceedings of  the eleventh annual conference on Computational learning theory, pp. 92–100. ACM, 1998. 2  Brodley, Carla E and Friedl, Mark A. Identifying mislabeled training data. Journal of Artiﬁcial Intelligence  Research, 11:131–167, 1999. 2  Carlson, Andrew, Betteridge, Justin, Kisiel, Bryan, Settles, Burr, Hruschka Jr, Estevam R, and Mitchell, Tom M.  Toward an architecture for never-ending language learning. In AAAI, volume 5, pp. 3, 2010. 3  Chen, Xinlei, Shrivastava, Abhinav, and Gupta, Abhinav. Neil: Extracting visual knowledge from web data. In  Computer Vision (ICCV), 2013 IEEE International Conference on, pp. 1409–1416. IEEE, 2013. 3  Chen, Xinlei, Shrivastava, Abhinav, and Gupta, Abhinav. Enriching visual knowledge bases via object discov-  ery and segmentation. CVPR, 2014. 3  Coates, Adam and Ng, Andrew Y. The importance of encoding versus training with sparse coding and vector quantization. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 921–928, 2011. 8  Erhan, Dumitru, Szegedy, Christian, Toshev, Alexander, and Anguelov, Dragomir. Scalable Object Detection  Using Deep Neural Networks. In CVPR, pp. 2155–2162, 2014. 2, 3, 6  Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik, Jitendra. Rich feature hierarchies for accurate object  detection and semantic segmentation. arXiv preprint arXiv:1311.2524, 2013. 1  Goodfellow, Ian, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Multi-prediction deep boltzmann  machines. In Advances in Neural Information Processing Systems, pp. 548–556, 2013. 3  Grandvalet, Yves and Bengio, Yoshua. Semi-supervised learning by entropy minimization. In Advances in  Neural Information Processing Systems, pp. 529–536, 2005. 2  Grandvalet, Yves and Bengio, Yoshua. 9 entropy regularization. 2006. 2, 5  Haffari, Gholam Reza and Sarkar, Anoop. Analysis of semi-supervised learning with the yarowsky algorithm.  arXiv preprint arXiv:1206.5240, 2012. 2  Hinton, Geoffrey E. Training products of experts by minimizing contrastive divergence. Neural computation,  14(8):1771–1800, 2002. 4  Hinton, Geoffrey E and Mnih, Volodymyr. Restricted boltzmann machine with hidden multinomial output unit.  Unpublished work, 2009. 4  Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im- proving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. 1  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convolutional  neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012. 1  Larochelle, Hugo and Bengio, Yoshua. Classiﬁcation using discriminative restricted boltzmann machines. In  Proceedings of the 25th international conference on Machine learning, pp. 536–543. ACM, 2008. 3  LeCun, Yann and Cortes, Corinna. The mnist database of handwritten digits, 1998. 2  Lee, Dong-Hyun. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural  networks. In Workshop on Challenges in Representation Learning, ICML, 2013. 3  Mnih, Volodymyr and Hinton, Geoffrey E. Learning to label aerial images from noisy data. In Proceedings of  the 29th International Conference on Machine Learning (ICML-12), pp. 567–574, 2012. 2  Nigam, Kamal and Ghani, Rayid. Analyzing the effectiveness and applicability of co-training. In Proceedings of the ninth international conference on Information and knowledge management, pp. 86–93. ACM, 2000. 2  10  Accepted as a workshop contribution at ICLR 2015  Nigam, Kamal, McCallum, Andrew Kachites, Thrun, Sebastian, and Mitchell, Tom. Text classiﬁcation from  labeled and unlabeled documents using em. Machine learning, 39(2-3):103–134, 2000. 2  Ouyang, Wanli, Luo, Ping, Zeng, Xingyu, Qiu, Shi, Tian, Yonglong, Li, Hongsheng, Yang, Shuo, Wang, Zhe, Xiong, Yuanjun, Qian, Chen, et al. Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection. arXiv preprint arXiv:1409.3505, 2014.  Reed, Scott, Sohn, Kihyuk, Zhang, Yuting, and Lee, Honglak. Learning to disentangle factors of variation with  manifold interaction. In Proceedings of The 31st International Conference on Machine Learning, 2014. 8  Rifai, Salah, Bengio, Yoshua, Courville, Aaron, Vincent, Pascal, and Mirza, Mehdi. Disentangling factors of variation for facial expression recognition. In Computer Vision–ECCV 2012, pp. 808–822. Springer, 2012. 8  Rosenberg, Chuck, Hebert, Martial, and Schneiderman, Henry. Semi-supervised self-training of object detec-  tion models. 2005. 2  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge, 2014. 2  Salakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltzmann machines. In International Conference on  Artiﬁcial Intelligence and Statistics, pp. 448–455, 2009. 3  Sermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Micha¨el, Fergus, Rob, and LeCun, Yann. Over- arXiv preprint  Integrated recognition, localization and detection using convolutional networks.  feat: arXiv:1312.6229, 2013. 1  Smolensky, Paul. Information processing in dynamical systems: Foundations of harmony theory. 1986. 4  Sukhbaatar, Sainbayar and Fergus, Rob. Learning from noisy labels with deep neural networks. arXiv preprint  arXiv:1406.2080, 2014. 3  Susskind, Josh M, Anderson, Adam K, and Hinton, Geoffrey E. The toronto face database. Department of  Computer Science, University of Toronto, Toronto, ON, Canada, Tech. Rep, 2010. 2  Szegedy, C., Reed, S., Erhan, D., and Anguelov, D. Scalable, High-Quality Object Detection. ArXiv e-prints,  December 2014. 1, 8, 9  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du- mitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014. 8  Whitney, Max and Sarkar, Anoop. Bootstrapping via graph propagation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pp. 620–628. Association for Computational Linguistics, 2012. 2  Yarowsky, David. Unsupervised word sense disambiguation rivaling supervised methods.  In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pp. 189–196. Association for Computational Linguistics, 1995. 2, 5, 7  Zeiler, Matthew D and Fergus, Rob. Visualizing and understanding convolutional neural networks. arXiv  preprint arXiv:1311.2901, 2013. 1  Zhu, Xiaojin. Semi-supervised learning literature survey. 2005. 2  11  ",
1412.5896,2015, On the Stability of Deep Networks,"['On the Stability of Deep Networks', 'Raja Giryes', 'Guillermo Sapiro', 'and Alex Bronstein']",https://arxiv.org/pdf/1412.5896,"5 1 0 2     n u J    3      ] L M  . t a t s [      3 v 6 9 8 5  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  ON THE STABILITY OF DEEP NETWORKS  Raja Giryes & Guillermo Sapiro Duke University {raja.giryes, guillermo.sapiro}@duke.edu  Alex M. Bronstein Tel Aviv University bron@eng.tau.ac.il  ABSTRACT  In this work we study the properties of deep neural networks (DNN) with random weights. We formally prove that these networks perform a distance-preserving embedding of the data. Based on this we then draw conclusions on the size of the training data and the networks’ structure. A longer version of this paper with more results and details can be found in (Giryes et al., 2015). In particular, we formally prove in (Giryes et al., 2015) that DNN with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data.  1  INTRODUCTION  Deep neural nets (DNN) have led to a revolution in the areas of machine learning, audio analysis, and computer vision. Many state-of-the-art results have been achieved using these architectures. In this work we study the properties of these architectures with random weights. We prove that DNN preserve the distances in the data along their layers and that this property allows stably recovering the original data from the features calculated by the network. Our results provide insights into the outstanding empirically observed performance of DNN and the size of the training data.  Our motivation for studying networks with random weights is threefold. First, one of the differences between the networks used two decades ago and state-of-the-art training strategies is the usage of random initialization of the weights. Second, a series of works (Pinto et al., 2009; Saxe et al., 2011; Cox & Pinto, 2011) empirically showed successful DNN learning techniques based on randomiza- tion. Third, recent works that studied the optimization aspect in the training of deep networks also have done so via randomization (Saxe et al., 2014; Dauphin et al., 2014; Choromanska et al., 2015).  Bruna et al. (2013) show that the pooling stage in DNN causes a shift invariance property. Bruna et al. (2014) interpret this step as the removal of phase from a complex signal and show how the signal may be recovered after a pooling stage using phase retrieval methods. In this short note, and for presentation purposes, we do not consider the previously studied pooling step, assuming the data to be properly aligned. We focus on the roles of the layers of a linear operation followed by an element-wise non-linear activation function.  2 STABLE EMBEDDING OF A SINGLE LAYER  We assume the input data to belong to a manifold K with Gaussian mean width  ω(K) := E[ sup  x,y∈Khg, x − yi],  (1)  where the expectation is taken over g with normal i.i.d. elements. In Section 3 we will illustrate this concept and exemplify the results with Gaussian mixture models (GMM). We say that f : R → R is a semi-truncated linear function if it is linear on some (possi- bly, semi-inﬁnite) interval and constant outside of it, f (0) = 0, 0 < f (x) ≤ x,∀x > 0 and  1  Accepted as a workshop contribution at ICLR 2015  0 ≥ f (x) ≥ x,∀x < 0. The popular rectiﬁed linear unit (ReLU), f (x) = max(0, x), is an example of such a function, while the sigmoid functions satisfy this property approximately. The follow- ing theorem shows that each standard DNN layer performs a stable embedding of the data in the Gromov-Hausdorff sense.  Theorem 1 Let M be the linear operator applied at the i-th layer, f the non-linear activation function, and K ⊂ Sn−1 the manifold of the input data for the i-th layer. is a random matrix with i.i.d normally distributed entries with m = O(ω(K)2) being the output dimension, and f is a semi-truncated linear function, then with high probability  If √mM ∈ Rn×m  (2) where d(·,·) is a variant of the Hamming distance that treats the positive values in the vectors as ones. This result implies that the metric of the input data is preserved.  kx − yk2 ≃ d(f (Mx), f (My)), ∀x, y ∈ K,  The proof follows from (Plan & Vershynin, 2014) and Klartag & Mendelson (2005).  Mahendran & Vedaldi (2014) demonstrate that it is possible to recover the input of DNN from their output. The next result provides a theoretical justiﬁcation for their observation by showing that it is possible to recover the input of each layer from its output:  Theorem 2 Under the assumptions of Theorem 1 there exists a program A such that  kx − A(f (Mx))k2 ≤ ǫ,  (3)  where ǫ = O (cid:16) ω(K)  √m (cid:17).  The proof follows from Plan & Vershynin (2014).  3 STABLE EMBEDDING OF THE ENTIRE NETWORK  In order to show that the entire network produces a stable embedding of its input, we need to show that the Gaussian mean width does not grow signiﬁcantly as the data propagate through the layers of the network. Instead of bounding the variation of the Gaussian mean width throughout the net- work, we bound the change in the covering number N (K, ǫ), i.e., the lowest number of ℓ2-balls of radius ǫ that cover K. Having the bound on the covering number, we use Dudleys inequality (Ledoux & Talagrand, 1991), ω(K) ≤ C R ∞ 0 plog N (K, ǫ)dǫ, to bound the Gaussian mean width variation, where C is a constant.  Theorem 3 Under the assumptions of Theorem 1,  N (f (MK), ǫ) ≤ N  K,  ǫ  1 + ω(K) √m    .  (4)  Proof: We now present a sketch of the proof, deferring the full proof that treats also the Gaussian mean width directly to a longer version of the paper. It is not hard to see that since a non-linear activation function shrinks the data, then it can not increase the size of the covering; therefore we focus on the linear part. Following (Klartag & Mendelson, 2005, Theorem 1.4), we have that the √m distances in MK are the same as the ones in K up to a 1+ ω(K) factor. This is sufﬁcient to complete the proof.  (cid:3)  We demonstrate the implication of the above theorem for a GMM, i.e., K consisting of L Gaussians ǫ(cid:1)k for ǫ < 1 and 1 otherwise (see of dimension k in the ℓ2-ball. For this model N (K, ǫ) = L(cid:0)1 + 2 Mendelson et al. (2008)). Therefore we have that ω(K) ≤ C′√k + log L and that at each layer the √k+log L Gaussian mean width grows at most with an order of 1 + √m . Similar results can be shown for other models of union of subspaces and low dimensional manifolds.  2  Accepted as a workshop contribution at ICLR 2015  4 HOW MANY MEASUREMENTS ARE NEEDED TO TRAIN THE NETWORK  An important question in deep learning is what is the amount of labeled training samples needed at training. Using Sudakov minoration (Ledoux & Talagrand, 1991), one may get an upper bound on the size of an ǫ-net in K. We have demonstrated that networks with random Gaussian weights realize a stable embedding; consequently, if a network is trained using the screening technique by selecting the best among many networks generated with random weights as suggested in Pinto et al. (2009); Saxe et al. (2011); Cox & Pinto (2011), then the number of data points needed to be used in order to guarantee that the network represents all the data is O(exp(ω(K)2/ǫ2)). Since ω(K)2 is a proxy for the data dimension (see Plan & Vershynin (2014)), we conclude that the number of training points grows exponentially with the intrinsic dimension of the data.  5 DISCUSSION AND CONCLUSION  We have shown that DNN with random Gaussian weights perform a distance-preserving embedding of the data. This result provides a relationship between the complexity of the input data and the size of the required training set. In addition, it draws a connection between the dimension of the features produced by the network,which still keep the metric information of the original manifold, and the complexity of the data.  Though we have focused here on the case of DNN with linear ﬁlters with random Gaussian entries, it is possible to extend our analysis to distributions such as sub-Gaussian, and to random convolu- tional ﬁlters using proof techniques from (Haupt et al., 2010; Saligrama, 2012; Rauhut et al., 2012; Ai et al., 2014). This and the extension to learned DNN will be presented in an extended version of this note. Acknowledgments: This work is supported by NSF, DoD and ERC StG 335491.  REFERENCES Ai, A., Lapanowski, A., Plan, Y., and Vershynin, R. One-bit compressed sensing with non-gaussian  measurements. to appear in Linear Algebra and its Applications, 2014.  Bruna, J., LeCun, Y., and Szlam, A. Learning stable group invariant representations with convolu-  tional networks. In ICLR Workshop, Jan. 2013.  Bruna, J., Szlam, A., and LeCun, Y. Signal recovery from lp pooling representations. In Int. Conf.  on Machine Learning (ICML), 2014.  Choromanska, A., Henaff, M. B., Mathieu, M., Arous, G. Ben, and LeCun, Y. The loss surfaces of multilayer networks. In International Conference on Artiﬁcial Intelligence and Statistics (AIS- TATS), 2015.  Cox, D. and Pinto, N. Beyond simple features: A large-scale feature search approach to uncon- strained face recognition. In IEEE International Conference on Automatic Face Gesture Recog- nition and Workshops (FG), pp. 8–15, March 2011.  Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y.  Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural Information Processing Systems (NIPS), 2014.  Giryes, R., Saprio, G.,  and Bronstein, A. M.  dom gaussian weights: A universal classiﬁcation strategy? http://arxiv.org/abs/1504.08291.  Deep neural networks with ran- URL  ArXiv, 2015.  Haupt, J., Bajwa, W.U., Raz, G., and Nowak, R. Toeplitz compressed sensing matrices with appli-  cations to sparse channel estimation. IEEE Trans. Inf. Theory, 56(11):5862–5875, Nov. 2010.  Klartag, B. and Mendelson, S. Empirical processes and random projections. Journal of Functional  Analysis, 225(1):229–245, Aug. 2005.  Ledoux, Michel and Talagrand, Michel. Probability in Banach Spaces. Springer-Verlag, 1991.  3  Accepted as a workshop contribution at ICLR 2015  Mahendran, A. and Vedaldi, A. Understanding deep image representations by inverting them. ArXiv,  2014. URL http://arxiv.org/abs/1412.0035.  Mendelson, S., Pajor, A., and Tomczak-Jaegermann, N. Uniform uncertainty principle for Bernoulli  and sub-Gaussian ensembles. Constructive Approximation, 28:277–289, 2008.  Pinto, N., Doukhan, D., DiCarlo, J. J., and Cox, D. D. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS Comput Biol, 5(11): e1000579, 11 2009.  Plan, Y. and Vershynin, R. Dimension reduction by random hyperplane tessellations. Discrete and  Computational Geometry, 51(2):438–461, 2014.  Rauhut, H., Romberg, J., and Tropp, J. A. Restricted isometries for partial random circulant matri-  ces. Appl. Comput. Harmon. Anal., 32(2):242–254, Mar. 2012.  Saligrama, V. Aperiodic sequences with uniformly decaying correlations with applications to com- pressed sensing and system identiﬁcation. IEEE Trans. Inf. Theory, 58(9):6023–6036, Sept. 2012.  Saxe, A., Koh, P. W., Chen, Z., Bhand, M., Suresh, B., and Ng, A. Y. On random weights and unsupervised feature learning. In Int. Conf. on Machine Learning (ICML), pp. 1089–1096, 2011.  Saxe, A., McClelland, J., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in deep linear neural network. In International Conference on Learning Representations (ICLR), 2014.  4  ",
1412.7022,2015, Audio source separation with Discriminative Scattering Networks ,"['Audio source separation with Discriminative Scattering Networks', 'Joan Bruna', 'Yann LeCun', 'and Pablo Sprechmann']",https://arxiv.org/pdf/1412.7022,"5 1 0 2    r p A 8 2         ]  D S . s c [      3 v 2 2 0 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  AUDIO SOURCE SEPARATION WITH DISCRIMINATIVE SCATTERING NETWORKS  Pablo Sprechmann1, Joan Bruna2, Yann Lecun1,2 1 NYU, Courant Institute of Mathematical Sciences, 2 Facebook AI Research.  {pablo,bruna,yann}@cims.nyu.edu  ABSTRACT  In this report we describe an ongoing line of research for solving single-channel source separation problems. Many monaural signal decomposition techniques proposed in the literature operate on a feature space consisting of a time-frequency representation of the input data. A challenge faced by these approaches is to ef- fectively exploit the temporal dependencies of the signals at scales larger than the duration of a time-frame. In this work we propose to tackle this problem by modeling the signals using a time-frequency representation with multiple tem- poral resolutions. The proposed representation consists of a pyramid of wavelet scattering operators, which generalizes Constant Q Transforms (CQT) with extra layers of convolution and complex modulus. We ﬁrst show that learning standard models with this multi-resolution setting improves source separation results over ﬁxed-resolution methods. As study case, we use Non-Negative Matrix Factoriza- tions (NMF) that has been widely considered in many audio application. Then, we investigate the inclusion of the proposed multi-resolution setting into a discrimi- native training regime. We discuss several alternatives using different deep neural network architectures.  1  INTRODUCTION  Monaural Source Separation is a fundamental inverse problem in speech processing (Loizou (2007); H¨ansler & Schmidt (2008)). Successful algorithms rely on models that capture signal regularity while preserving discrimination between different speakers. The decomposition of time-frequency representations, such as the power or magnitude spectrogram in terms of elementary atoms of a dictionary, has become a popularcitet tool in audio processing. Non-negative matrix factorization (NMF) (Lee & Seung (1999)), have been widely adopted in various audio processing tasks, includ- ing in particular source separation, see Smaragdis et al. (2014) for a recent review. There are many works that follow this line in speech separation (Schmidt & Olsson (2006); Shashanka et al. (2007)) and enhancement (Duan et al. (2012); Mohammadiha et al. (2013)).  Although NMF applied on spectral features is highly efﬁcient, it fails to model long range geomet- rical features that characterize speech signals. Increasing the temporal window is not the solution, since it increases signiﬁcantly the dimensionality of the problem and reduces the discriminative power of the model. In order to overcome this limitation, many works have proposed regularized ex- tensions of NMF to promote learned structure in the codes. Examples of these approaches are, tem- poral smoothness of the activation coefﬁcients (F´evotte (2011)), including co-occurrence statistics of the basis functions (Wilson et al. (2008)), and learned temporal dynamics with Kalman ﬁltering like techniques(Mysore & Smaragdis (2011); Han et al. (2012); F´evotte et al. (2013)) or integrat- ing Recurrent Neural Networks (RNN) into the NMF framework (Boulanger-Lewandowski et al. (2014)).  More recently, several works have observed that the efﬁciency of these methods can be improved with discriminative training. Discriminatively trained dictionary learning techniques (Mairal et al. (2012); Gregor & LeCun (2010); Sprechmann et al. (2014); Weninger et al. (2014a)) show the im- portance of adapting the modeling task to become discriminative at the inverse problem at hand.  1  Accepted as a workshop contribution at ICLR 2015  A number of works completely bypass the modeling aspect and approach inverse problems as non-linear regression problems using Deep Neural Networks(DNN) (Sprechmann et al. (2013); Schuler et al. (2014); Dong et al. (2014)) with differet levels of structure ranging from simple frame- by-frame regressors to more sophisticated RNN. Applications include source separation in mu- sic (Sprechmann et al. (2012); Huang et al. (2014b)), speech separation (Huang et al. (2014a)) and speech enhancement (Weninger et al. (2014b)).  The goal of this work is to show that using stable and robust multi-resolution representation of the data can beneﬁt the sources separation algorithms in both discriminative and non-discriminative settings. Previous works have shown that the choice of the input features plays a very important role of on source separation (Weninger et al. (2014b)) and speech recognition (Mohamed et al. (2012)). This work takes this observation a step further to the multi-resolution setting.  We consider a deep representation based on the wavelet scattering pyramid, which produces in- formation at different temporal resolutions and deﬁnes a metric which is increasingly contracting. This representation can be thought as a generalization of the CQT. Discriminative features having longer temporal context can be constructed with the scattering transform (Bruna & Mallat (2013b)) and have been sucessfully applied to audio signals by And´en & Mallat (2013). While these features have shown excellent performance in various classiﬁcation tasks, in the context of source separation we require a representation that not only captures long-range temporal structures, but also preserves as much temporal discriminability as possible.  For the non-discriminative setting, we present an extension of the NMF framework to the pyramid representation. We learn NMF models at different levels of the hierarchy. While NMF dictionaries at the ﬁrst level are very selective to temporally localized energy patterns, deeper layers provide additional modeling of the longer temporal dynamics (Bruna et al. (2014)). For the discriminative setting we discuss a number of baseline models based on neural networks. As a proof of concept, we evaluate both settings on a multi-speaker speech separation task. We observe that in both training regimes the multi-resolution setting leads to better performance with respect to the baselines. We also conﬁrm with experiments the superiority of discriminative approaches.  The paper is organized as follows. In Section 2 we describe the general setting of source separation and review some baseline solutions for both in training regimes. We present the proposed represen- tation in Sections 3 and show how it can be used in the context of source separation in Section 4. We show some initial experimental results in Section 5 and a discussion is given in Section 6.  2 SINGLE-CHANNEL SOURCE SEPARATION  In this work we are intereseted in the families of algorithms that solve source separation on a feature space. This section is dedicated to describing different alternatives that fall in this category. We ﬁrst introduce the general setting in Section 2.1. In Section 2.2 we describe the popular NMF framework and different training regimes employed with it. Finally we discuss purely discriminative approaches based on deep networks in Section 2.3.  2.1 PROBLEM FORMULATION  We consider the setting in which we observe a temporal signal y(t) that is the sum of two sources xi(t), with i = 1, 2,  y(t) = x1(t) + x2(t),  (1)  and we aim at ﬁnding estimates ˆxi(t). We consider the supervised monoaural source separation problem, in which the components xi, i = 1, 2 come from sources for which we have representative training data. In this report we concentrate to the case of speech signals, but other alternatives could be considered, such as noise or music.  Most recent techniques typically operate on a non-negative time-frequency representation. Let us denote as Φ(y) ∈ Rm×n the transformed version of y(t), comprising m frequency bins and n temporal frames. This transform can be thought as a non-linear analysis operator and is typically deﬁned as the magnitude (or power) of a time-frequency representation such as the Short-Time Fourier Transform (STFT). Other robust alternatives have also been explored (Huang et al. (2014a);  2  Accepted as a workshop contribution at ICLR 2015  Weninger et al. (2014b)). In all cases, the temporal resolution of the features is ﬁxed and given by the frame duration.  Performing the separation in the non-linear representation is key to the success of these algorithms. The transformed domain is in general invariant to some irrelevant variability of the signals (such as local shifts), thus relieving the algorithms from learning it. This comes at the expense of in- verting the unmixed estimates in the feature space, normally known as the phase recovery problem (Gerchberg & Saxton (1972)). Speciﬁcally, these algorithms take Φ(y) as input and produce esti- mates for each source, Φ(ˆxi) with i = 1, 2. The phase recovery problem corresponts to ﬁnding signals ˆx′ The most common choice is to use the magintud (or power) STFT as the feature space. In this case, the phase recovery problem can be solved very efﬁciently using soft masks to ﬁlter the mixture signal (Schmidt et al. (2007)). The strategy resembles Wiener ﬁltering and has demonstrated very good results in practice. Speciﬁcally, Φ(y) = |S{y}|, where S{y} ∈ Cm×n is a complex matrix corresponding to the STFT. The estimated unmixed signals are obtained by ﬁltering the mixture,  i such matching the obtained features Φ(ˆxi) and satisfying y = ˆx′  2. 1 + ˆx′  ˆxi = S−1 {Mi ◦ S{y}} , with Mi =  Φ(ˆxi)p  Pl=1,2 Φ(ˆxl)p  ,  (2)  where multiplication denoted ◦, division, and exponentials are element-wise operations. The param- eter p deﬁnes the smoothness of the mask, we use p = 2 in our experiments. Note that this solution automatically imposes the consistency restriction y = ˆx′  2. 1 + ˆx′  2.2 NON-NEGATIVE MATRIX FACTORIZATION  Source separation methods based on matrix factorization approaches have received a lot of attention in the literature in recent years. NMF-based source separation techniques attempt to ﬁnd the non- negative activations Zi ∈ Rq×n, i = 1, 2 best representing the different speech components in two dictionaries Di ∈ Rm×q. Ideally one would want to solve the problem,  min i,Zi≥0  x′  X  i=1,2  D(Φ(x′  i)|DiZi) + λR(Zi)  s.t. y = x′  1 + x′  2 .  (3)  where the ﬁrst term in the optimization objective measures the dissimilarity between the input data and the estimated channels in the feature space. Common choices of D are the squared Euclidean distance, the Kullback-Leibler divergence, and the Itakura-Saito divergence. The second term in the minimization objective is included to promote some desired structure of the activations. This is done using a designed regularization function R, whose relative importance is controlled by the parameters λ. In this work we use D reweighted squared Euclidean distance and the ℓ1 norm as the regularization function R. i and zi. Note Problem (3) could be minimized with an alternating gradient descent between x′ i requires locally inverting the transform Φ, which that ﬁxing zi and minimizing with respect to x′ amounts to solve an overcomplete phase recovery problem. In practice, a greedy proxy of (3) is solved instead. First a separation is obtained in the feature spaces by solving a clasic NMF problem,  min Zi≥0  D(Φ(x)| X  DiZi) + λ X  R(Zi) ,  i=1,2  i=1,2  (4)  for which there exist standard optimization algorithms, see for example F´evotte & Idier (2011). Once the optimal activations are solved for, the spectral envelopes of the speech are estimated as Φ( ˆxi) = DiZi, and the phase recovery is solved using (2). In this supervised setting, the dictionaries are obtained from training data. The classic approach is to build model for each source independently and later use them together at testing time. Many works have observed that sparse coding inference algorithms can be improved in speciﬁc tasks by using discriminative training, i.e. by directly optimizing the parameters of the model on the evaluation cost function. Task-aware (or discriminative) sparse modeling is elegantly described by Mairal et al. (2012), observing that one can back-propagate through the Lasso. These ideas have been used in the context of source separation and enhancement (Sprechmann et al. (2014); Weninger et al. (2014a)). The goal is to obtain dictionaries such that the solution of (4) also minimizes the reconstruction  3  Accepted as a workshop contribution at ICLR 2015  given the ground truth separation,  min  D1≥0,D2≥0  D(Φ(x1)|D1Z ∗  1 ) + αD(Φ(x2)|D2Z ∗  2 ),  (5)  where Z ∗ i are the solutions of (4) (and depend on the dictionaries) and α is a parameter controlling the relative importance of source recovery; typically, one would set α = 0 in a denoising application (where the second signal is noise), and α = 1 in a source separation application where both signals need to be recovered. When the phase recovery can be obtained using the masking scheme described in Section 2.1, it could be included into the objective in order to directly optimize the signal recon- struction in the time domain. While the discriminative setting is a better target, the estimation needs to be computed over the product set rather than each training set independently and the generaliza- tion might be compromised when small training sets are available. It is important to note that the level of supervision is very mild, as in the training of autoencoders. We are artiﬁcially generating the mixtures, and consequently obtaining the ground truth.  The standard NMF approaches treat different time-frames independently, ignoring the temporal dy- namics of the signals. As described in Section 1, many works attempt to change the regularization function R in order integrate several frames into de decomposition. It’s analysis and description is outside the scope of this report.  2.3 PURELY DISCRIMINATIVE SETTINGS  With the mindset of the discriminative learning, one is tempted to simply replace the inference step by a generic neural network architecture, having enough capacity to perform non-linear regression. The systems are trained as to minimize a measure of ﬁtness between the ground truth separation and the output as in (5), being the most common the Mean Squared Error (MSE). Not that this can be performed in the feature space or in the time domain (when the phase recovery is simple). Other alternatives studied in the literature consist of predicting the masks given in (3) as described by Huang et al. (2014a).  The most straight forward choice is to perform the estimation using a DNN on a ﬁxed time scale. Using a short temporal context fails to model long range temporal dependencies on the speech sig- nals, while increasing the context renders the regression problem intractable. One could consider to train a DNN on an set of several frames (Weninger et al. (2014b)). Recent works have explored neu- ral network architectures that exploit temporal context such as RNN and Long Short-Term Memory (LSTM) (Huang et al. (2014a); Weninger et al. (2014b)).  3 PYRAMID WAVELET SCATTERING  In this section we present brieﬂy the proposed wavelet scattering pyramid, which is conceptually similar to standard scattering networks introduced by Mallat (2010), but creates features at different temporal resolutions at every layer.  3.1 WAVELET FILTER BANK  A wavelet ψ(t) is a band-pass ﬁlter with good frequency and spatial localization. We consider a complex wavelet with a quadrature phase, whose Fourier transform satisﬁes F ψ(ω) ≈ 0 for ω < 0. We assume that the center frequency of F ψ is 1 and that its bandwidth is of the order of Q−1. Wavelet ﬁlters centered at the frequencies λ = 2j/Q are computed by dilating ψ: ψλ(t) = λ ψ(λ t), and hence F ψλ(ω) = bψ(λ−1ω). We denote by Λ the index set of λ = 2j/Q over the signal frequency support, with j ≤ J1. The resulting ﬁlter bank has a constant number Q of bands per octave and J1 octaves. Let us deﬁne φ1(t) as a low-pass ﬁlter with bandwidth 2−J1. The wavelet transform of a signal x(t) is  Since the bandwidth of all ﬁlters is at most Q1, we can down-sample its outputs with a stride Q.  W x = {x ∗ φ1(t) , x ∗ ψλ(t)}λ∈Λ .  4  Accepted as a workshop contribution at ICLR 2015  3.2 PYRAMID SCATTERING TRANSFORM  Instead of using a ﬁxed bandwidth smoothing kernel that is applied at all layers, we sample at critical rate in order to preserve temporal locality as much as possible. We start by removing the complex phase of wavelet coefﬁcients in W x with a complex modulus nonlinearity. Then, we arrange these ﬁrst layer coefﬁcients as nodes in the ﬁrst level of a tree. Each node of this tree is down sampled at the critical sampling rate of the layer ∆1, given by the reciprocal of the largest bandwidth present in the ﬁlter bank:  |W 1|x = {x1  i }i=1...1+|Λ| = {x ∗ φ1(∆1n) , |x ∗ ψλ(∆1n)|}λ∈Λ .  These ﬁrst layer coefﬁcients give localized information both in time and frequency, with a trade-off dictated by the Q factor. They are however sensitive to local time-frequency warps, which are often uninformative. In order to increase the robustness of the representation, we transform each of the down sampled signals with a new wavelet ﬁlter bank and take the complex modulus of the oscillatory component. For simplicity, we assume a dyadic transformation, which reduces the ﬁlter bank to a pair of conjugate mirror ﬁlters {φ2, ψ2} (Mallat (1999)), carrying respectively the low-frequencies and high-frequencies of the discrete signal from above the tree:  |W 2|x = {x1  i ∗ φ2(2n) , |x1  i ∗ ψ2(2n)|}i=1...|W 1| .  Every layer thus produces new feature maps at a lower temporal resolution. As shown in Bruna & Mallat (2013b), only coefﬁcients having gone through m ≤ mmax non-linearities are in practice computed, since their energy quickly decays. We ﬁx mmax = 2 in our experiments. We can reapply the same operator as many times k as desired until reaching a temporal con- text T = 2k∆1. If the wavelet ﬁlters are chosen such that they deﬁne a non-expansive mapping Bruna & Mallat (2013b), it results that every layer deﬁnes a metric which is increasingly contract- ing:  k|W k|x − |W k|x′k ≤ k|W k−1|x − |W k−1|x′k ≤ kx − x′k .  Every layer thus produces new feature maps at a lower temporal resolution. In the end we obtain a tree of different representations, Φj(x) = |W j|x with j = 1, . . . , k.  4 SOURCE SEPARATION ALGORITHMS  In this section we show a few examples of how the proposed pyramid scattering features could be used for solving the source separation problem. We present alternatives for both learning paradigms: non-discriminative and discriminative.  4.1 NON-DISCRIMINATIVE TRAINING  In this setting, we try to ﬁnd models for each speaker using the features of the wavelet scattering pyramid. Each layer of the transform produces information with different stability/discriminability trade-offs. Whereas in typical classiﬁcation applications one is mostly interested in choosing a single layer which provides the best trade-off given the intrinsic variability of the dataset, in inverse problems we can leverage signal models at all levels. Let us suppose two different sources X1 and X2, and let us consider for simplicity the features Φj(xi), j = 1, 2, i = 1, 2, xi ∈ Xi, obtained by localizing the scattering features of two different resolutions at their corresponding sampling rates. Therefore, Φ1 carries more discriminative and localized information than Φ2. In the non-discriminative training, we train independent models for each source. Given training examples X t  i from each source, we consider a NMF of each of the features Φj(xt  i):  min i ,Z j  i ≥0  Dj  X  xi∈Xi  1 2  kΦj(xi) − Dj  i Z j  i k2 + λj  i kZ j  i k1,  where here the parameters λj our experiments we used a ﬁxed value for all of them λ2  i control the sparsity-reconstruction trade-off in the sparse coding. In i = λ. At test time, given y = x1 + x2, we  5  Accepted as a workshop contribution at ICLR 2015  estimate ˆx1, ˆx2 as the solution of  min  ˆx1+ˆx2=y,Z j  i ≥0  X  i=1,2  1 2  kΦ1(ˆxi) − D1  i Z 1  i k2  2 + λ1  i kZik1 +  1 2  kΦ2(ˆxi) − D2  i Z 2  i k2  2 + λ2  i kZ 2  i k1 .  (6)  Problem (6) is a coupled phase recovery problem under linear constraints. It can be solved using gradient descent as in Bruna & Mallat (2013a), but in our setting we use a greedy algorithm, which approximates the unknown complex phases using the phase of W1y and W2|W1y| respectively. Similarly as in Weninger et al. (2014b), we simplify the inference by using a stronger version of the linear constraint y = x1 + x2, namely  and therefore that destructive interferences are negligible.  |W 1y|2 = |W 1x1|2 + |W 1x2|2 ,  4.2 DISCRIMINATIVE TRAINING  The pyramid scattering features can also be used to train end-to-end models. The most simple alternative is to train a DNN directly from features having the same temporal context as second layer scattering features. For simplicity, we replace the second layer of complex wavelets and modulus with a simple Haar transform:  Φ2(x) = {|x ∗ ψλ| ∗ hk(∆1n)}λ∈Λ,k=0,...J2 ,  where hk is the Haar wavelet at scale 2k, and we feed this feature into a DNN with the same number of hidden units as before. We do not take the absolute value as in standard scattering to leave the chance to the DNN to recombine coefﬁcients before the ﬁrst non-linearity. We report results for J2 = 5 which corresponds to a temporal context of 130ms. We will refer to this alternative as DNN- multi. As a second example, we also consider a multi-resolution Convolutional Neural Network (CNN), constructed by creating contexts of three temporal frames at resolutions 2j, j = 0 . . . , J2 = 5. We will refer to this alternative as CNN-multi. This setting has the same temporal context as the DNN-multi but rather than imposing separable ﬁlters we leave extra freedom. This architecture can access relatively large temporal context with a small number of learnable parameters. Since the phse recovery problem cannot be approximated with softmax as in (3), we use as the cost function the MSE of the reconstructed feature at all resolutions.  5 EXPERIMENTS  In this section we present some initial experimental evaluation in which we study the use of multi resolution signal representation with both discriminative and non-discriminative training regimes. We compare the performance against some basic baseline settings.  As a proof of concept, we evaluated the different alternatives in a multi-speaker setting in which we aim at separating male and female speech. In each case, we trained two gender-speciﬁc modeles. The training data consists of recordings of a generic group of speakres per gender, none of which were included in the test set. The experiments were carried out on the TIMIT corpus. We adopted the standard test-train division, using all the training recordings (containing 462 different speakers) for building the models and a subset of 12 different speakers (6 males and 6 females) for testing. For each speaker we randomly chose two clips and compared all female-male combinations (144 mixtures). All signals where mixed at 0 dB and resampled to 16 kHz. We used the source-to- distortion ratio (SDR), source-to-interference ratio (SIR), and source-to-artifact ratio (SAR) from the BSS-EVAL metrics (Vincent et al. (2006)). We report the average over the both speakers, as the measure are not symmetric.  Non-discriminative settings: As a basline for the non-discriminative setting we used standard NMF with STFT of frame lengths of 1024 samples and 50% overlap, leading to 513 feature vectors. The dictionaries were chosen with 200 and 400 atoms. We evaluated the proposed scattering features in combination with NMF (as described in Section 4.1) with one and two layers, referred as scatt- NMF1 and scatt-NMF2 respectively. We use complex Morlet wavelets with Q1 = 32 voices per octave in the ﬁrst level, and dyadic Morlet wavelets (Q2 = 1) for the second level, for a review on Morlet wavelets refer to Mallat (1999). The resulting representation had 175 coefﬁcients for the ﬁrst  6  Accepted as a workshop contribution at ICLR 2015  NMF scatt-NMF1 scatt-NMF2 CQT-DNN CQT-DNN-5 CQT-DNN-multi CQT-CNN-multi  SDR  6.1 [2.9] 6.2 [2.8] 6.9 [2.7] 9.4 [3.0] 9.2 [2.8] 9.7 [3.0] 9.9 [3.1]  SIR  14.1 [3.8] 13.5 [3.5] 16.0 [3.5] 17.7 [4.2] 17.4 [4.0] 19.6 [4.4] 19.8 [4.2]  SAR  7.4 [2.1] 7.8 [2.2] 7.9 [2.2] 10.4 [2.6] 10.3 [2.4] 10.4 [2.7] 10.6 [2.8]  Table 1: Source separation results on a multi-speaker settings. Average SDR, SIR and SAR (in dB) for different methods. Standard deviation of each result shown between brackets.  level and around 2000 for the second layer. We used 400 atoms for scatt-NMF1 and 1000 atoms for scatt-NMF2. In all cases, the features were frame-wise normalized and we used λ = 0.1. In all cases, parameters were obtained using cross-validation on a few clips separated from the training as a validation set.  Discriminative settings: We use a single and multi-frame DNNs as a baseline for this training setting.The network architectures consist of two hidden layers using the outputs of the ﬁrst layer of scattering, that is, the CQT coefﬁcients at a given temporal position. It uses RELU’s as in the rest of the architectures and the output is normalize so that it corresponds to the spectral mask discussed in (3). The multi-frame version considers the concatenation of 5 frames as inputs matching the temporal context of the tested multi-resolution versions. We used 512 and 150 units for the single- frame DNN (referred as CQT-DNN) and 1024 and 512 for the multi-frame one (referred as CQT- DNN-5), increasing the number of parameters did not improve the results. We optimize the network to optimize the MSE to each of the sources. We also include the architectures DNN-multi and CNN-multi described in Section 4.2. In all cases the weights are randomly initialized and training is performed using stochastic gradient descent with momentum. We used the GPU-enabled package Matconvnet (Simonyan & Zisserman (2014)).  Table 1 shows the results obtained for the speaker-speciﬁc and multi-speaker settings. In all cases we observe that the one layer scattering transform outperforms the STFT in terms of SDR. Furthermore, there is a tangible gain in including a deeper representation; scatt-NMF2 performs always better than scatt-NMF1. While the gain in the SDR and SAR are relatively small the SIR is 3dB higher. It is thus beneﬁtial to consider a longer temporal context in order to perform the separation sucessfully.  On the other hand, as expected, the discriminative training yields very signiﬁcant improvements. The same reasons that produced the improvements in the non-discriminative setting also have an impact in the discriminative case. Adding enough temporal contexts to the neural regressors improves their performance. The multi-temporal representation plays a key role as simply augmenting the number of frames does not lead to better performance (at least using baseline DNNs). It remains to be seen how these architectures would compare with the alternative RNN models.  6 DISCUSSION  We have observed that the performance of baseline source separation algorithms can be improved by using a temporal multi-resolution representation. The representation is able to integrate infor- mation across longer temporal contexts while removing uninformative variability with a relatively low parameter budget. In line with recent ﬁndings in the literature, we have observed that including discriminative criteria in the training leads to signiﬁcant improvements in the source separation per- formance. However, contrary to standard sparse modeling in which the resulting inference can be readily approximated with a neural network, it remains unclear whether phase-recovery type infer- ence can also be efﬁciently approximated with neural network architectures. We believe there might still be a gap in performance that might be bridged with appropriate discriminative architectures.  While this report presents shows some promising initial results, several interesting comparisons need to be made and are subject of current research. We consider an interesting problem exploring the best way of including the long-term temporal consistency into the estimation. Recent studies have evaluate the use of deep RNN’s for solving the source separation problem Huang et al. (2014a); Weninger et al. (2014b). While Huang et al. (2014a) do not observe signiﬁcant improvements over  7  Accepted as a workshop contribution at ICLR 2015  standard DNN’s in speech separation, Weninger et al. (2014b) obtain signiﬁcant improvements using LSTM-DRNN in speech enhancement. We are currently addressing the question of comparing different neural network architectures that exploit temporal dependancies and assessing whether the use of multi-resolution representation can play a role as in this initial study.  REFERENCES And´en, J. and Mallat, S. Deep scattering spectrum. arXiv preprint arXiv:1304.6763, 2013.  Boulanger-Lewandowski, N., Mysore, G.J., and Hoffman, M. Exploiting long-term temporal depen- dencies in nmf using recurrent neural networks with application to source separation. In ICASSP, pp. 6969–6973, May 2014.  Bruna, J. and Mallat, S. Audio texture synthesis with scattering moments.  arXiv:1311.0407, 2013a.  arXiv preprint  Bruna, J. and Mallat, S. Invariant scattering convolution networks. Pattern Analysis and Machine  Intelligence, IEEE Transactions on, 35(8):1872–1886, 2013b.  Bruna, J., Sprechmann, P., and Lecun, Yann. Source separation with scattering non-negative matrix  factorization. submitted, 2014.  Dong, Chao, Loy, ChenChange, He, Kaiming, and Tang, Xiaoou. Learning a deep convolutional network for image super-resolution. In Fleet, David, Pajdla, Tomas, Schiele, Bernt, and Tuyte- laars, Tinne (eds.), Computer Vision ? ECCV 2014, volume 8692 of Lecture Notes in Computer Science, pp. 184–199. 2014. doi: 10.1007/978-3-319-10593-2 13.  Duan, Z., Mysore, G. J., and Smaragdis, P. Online plca for real-time semi-supervised source sepa-  ration. In LVA/ICA, pp. 34–41, 2012.  F´evotte, C. Majorization-minimization algorithm for smooth itakura-saito nonnegative matrix fac-  torization. In ICASSP, pp. 1980–1983. IEEE, 2011.  F´evotte, C. and Idier, J. Algorithms for nonnegative matrix factorization with the β-divergence.  Neural Computation, 23(9):2421–2456, 2011.  F´evotte, C., Roux, J. Le, and Hershey, J. R. Non-negative dynamical system with application to  speech and audio. In ICASSP, 2013.  Gerchberg, R. W. and Saxton, W. Owen. A practical algorithm for the determination of the phase  from image and diffraction plane pictures. Optik, 35:237–246, 1972.  Gregor, K. and LeCun, Y. Learning fast approximations of sparse coding. In ICML, pp. 399–406,  2010.  Han, J., Mysore, G. J., and Pardo, B. Audio imputation using the non-negative hidden markov  model. In LVA/ICA, pp. 347–355, 2012.  H¨ansler, E. and Schmidt, G. Speech and Audio Processing in Adverse Environments. Springer, 2008.  Huang, P.-S., Kim, M., Hasegawa-Johnson, M., and Smaragdis, P. Deep learning for monaural  speech separation. In ICASSP, pp. 1562–1566, 2014a.  Huang, Po-Sen, Kim, Minje, Hasegawa-Johnson, Mark, and Smaragdis, Paris. Singing-voice sepa-  ration from monaural recordings using deep recurrent neural networks. ISMIR, 2014b.  Lee, D.D. and Seung, H.S. Learning parts of objects by non-negative matrix factorization. Nature,  401(6755):788–791, 1999.  Loizou, P. C. Speech Enhancement: Theory and Practice, volume 30. CRC, 2007.  Mairal, J., Bach, F., and Ponce, J. Task-driven dictionary learning. Pattern Analysis and Machine  Intelligence, IEEE Transactions on, 34(4):791–804, 2012.  8  Accepted as a workshop contribution at ICLR 2015  Mallat, St´ephane. A wavelet tour of signal processing. Academic press, 1999.  Mallat, St´ephane. Recursive interferometric representation. In Proc. of EUSICO conference, Den-  mark, 2010.  Mohamed, Abdel-rahman, Hinton, Geoffrey, and Penn, Gerald. Understanding how deep belief In Acoustics, Speech and Signal Processing (ICASSP),  networks perform acoustic modelling. 2012 IEEE International Conference on, pp. 4273–4276. IEEE, 2012.  Mohammadiha, N., Smaragdis, P., and Leijon, A. Supervised and unsupervised speech enhance- ment using nonnegative matrix factorization. Audio, Speech, and Language Processing, IEEE Transactions on, 21(10):2140–2151, 2013.  Mysore, G. J. and Smaragdis, P. A non-negative approach to semi-supervised separation of speech  from noise with the use of temporal dynamics. In ICASSP, pp. 17–20, 2011.  Schmidt, M. N. and Olsson, R. K. Single-channel speech separation using sparse non-negative  matrix factorization. In INTERSPEECH, Sep 2006.  Schmidt, M. N., Larsen, J., and Hsiao, F.-T. Wind noise reduction using non-negative sparse coding.  In MLSP, pp. 431–436, Aug 2007.  Schuler, Ch., Hirsch, M., Harmeling, S., and Scholkopf, B. Learning to deblur. arXiv preprint  arXiv:1406.7444, 2014.  Shashanka, M. V. S., Raj, B., and Smaragdis, P. Sparse Overcomplete Decomposition for Single  Channel Speaker Separation. In ICASSP, 2007.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. arXiv preprint arXiv:1409.1556, 2014.  Smaragdis, P., Fevotte, C., Mysore, G, Mohammadiha, N., and Hoffman, M. Static and dynamic source separation using nonnegative factorizations: A uniﬁed view. Signal Processing Magazine, IEEE, 31(3):66–75, 2014.  Sprechmann, P., Bronstein, A., Bronstein, M., and Sapiro, G. Learnable low rank sparse models for  speech denoising. In ICASSP, pp. 136–140, 2013.  Sprechmann, P., Bronstein, A. M., and Sapiro, G. Supervised non-euclidean sparse NMF via bilevel  optimization with applications to speech enhancement. In HSCMA, pp. 11–15. IEEE, 2014.  Sprechmann, Pablo, Bronstein, Alexander M, and Sapiro, Guillermo. Real-time online singing voice separation from monaural recordings using robust low-rank modeling. In ISMIR, pp. 67– 72. Citeseer, 2012.  Vincent, E., Gribonval, R., and F´evotte, C. Performance measurement in blind audio source separa-  tion. IEEE Trans. on Audio, Speech, and Lang. Proc., 14(4):1462–1469, 2006.  Weninger, F., Le Roux, J., Hershey, J. R, and Watanabe, S. Discriminative NMF and its application  to single-channel source separation. Proc. of ISCA Interspeech, 2014a.  Weninger, Felix, Le Roux, Jonathan, Hershey, John R., and Schuller, Bj¨orn. Discriminatively trained recurrent neural networks for single-channel speech separation. In Proc. IEEE GlobalSIP 2014 Symposium on Machine Learning Applications in Speech Processing, 2014b.  Wilson, K. W., Raj, B., Smaragdis, P., and Divakaran, A. Speech denoising using nonnegative matrix  factorization with priors. In ICASSP, pp. 4029–4032, 2008.  9  ",
1412.8419,2015, Simple Image Description Generator via a Linear Phrase-Based Model,"['Simple Image Description Generator via a Linear Phrase-Based Model', 'Pedro Pinheiro', 'Rémi Lebret', 'and Ronan Collobert']",https://arxiv.org/pdf/1412.8419,"5 1 0 2    r p A 1 1         ] L C . s c [      3 v 9 1 4 8  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  SIMPLE IMAGE DESCRIPTION GENERATOR VIA A LINEAR PHRASE-BASED MODEL  R´emi Lebret∗ & Pedro O. Pinheiro∗ Idiap Research Institute, Martigny, Switzerland ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL), Lausanne, Switzerland remi@lebret.ch, pedro@opinheiro.com  Ronan Collobert† Facebook AI Research, Menlo Park, CA, USA Idiap Research Institute, Martigny, Switzerland ronan@collobert.com  ABSTRACT  Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sam- ple image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple lan- guage model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the- art models, achieves comparable results on the recently release Microsoft COCO dataset.  1  INTRODUCTION  Being able to automatically generate a description from an image is a fundamental problem in ar- tiﬁcial intelligent, connecting computer vision and natural language processing. The problem is particularly challenging because it requires to correctly recognize different objects in images and also how they interact. Convolutional Neural Networks (CNN) have achieved state of the art results in different computer vision tasks in the last few years. More recently, different authors proposed automatic image sen- tence description approaches based on deep neural networks. All the solutions use the representation of images generated by CNN that was previously trained for object recognition tasks as start point. Vinyals et al. (2014) consider the problem in a similar way as a machine translation problem. The authors propose a encoder/decoder (CNN/LSTM networks) system that is trained to maximize the likelihood of the target description sentence given a training image. Kiros et al. (2014) also consider a encoder/decoder pipeline, but uses a combination of CNN and LSTM networks for encoding and a language model for decoding. Karpathy & Fei-Fei (2014) propose an approach that is a combination of CNN, bidirectional recurrent neural networks over sentences and a structured objective respon- sible for a multimodal embedding. They propose a second recurrent neural network architecture to generate new sentences. Similar to the previous works, Mao et al. (2014) and Donahue et al. (2014) propose a system that uses a CNN to extract image features and a deep recurrent neural network for sentences. The two networks interact with each other in a multimodal common layer.  ∗These two authors contributed equally to this work. †All research was conducted at the Idiap Research Institute, before Ronan Collobert joined Facebook AI  Research.  1  Accepted as a workshop contribution at ICLR 2015  Figure 1: Schematic illustration of our phrase-based model for image descriptions.  Fang et al. (2014) propose a different approach to the problem that does not rely on recurrent neural networks. Their solution can be divided into three steps: (i) visual detector for words that com- monly occur are trained using multiple instance learning, (ii) a set of sentences are generated using a Maximum-Entropy language-model and (iii) the sentences are re-ranked using sentence-level fea- tures and a proposed deep multimodal similarity model. This paper proposes a different approach to the problem. We propose a system that at the same time: (i) automatically generates a sentence describing a given scene and (ii) is relatively simpler than the recently proposed approaches. Our model shares some similarities with previously proposed deep approaches. For instance, we also use a pre-trained CNN to extract image features and we also consider a multimodal embedding. However, thanks to the phrase-based approach, we do not use any complex recurrent network for sentence generation. We represent the ground-truth sentences as a collection of noun, verb and prepositional phrases. Each phrase is represented by the mean of the vector representation of the words that compose it. We then train a simple linear embedding model that transform an image representation into a multimodal space that is common to the image and the phrases that are used to describe them. To automatically generate sentences in inference time, we (i) infer the phrases that correspond to the sample image and (ii) use a simple language model based on the statistics of the ground-truth sentences present in the corpus.  2 PHRASE-BASED MODEL FOR IMAGE DESCRIPTIONS  2.1 UNDERSTANDING STRUCTURES OF IMAGE DESCRIPTIONS  The art of writing sentences can vary a lot according to the domain it is being applied. When reporting news or reviewing an item, not only the choice of the words might vary, but also the general structure of the sentence. Sentence structures used for describing images can therefore be identiﬁed. They possess a very distinct structure, usually describing the different objects present on the scene and how they interact between each other. This interaction among objects is described as actions or relative position between different objects. The sentence can be short or long, but it generally respects this process. This statement is illustrated with the ground-truth sentence descriptions of the image in Figure 1.  Chunking-based approach All the key elements in a given image are usually described with a noun phrase (NP). Interactions between these elements can then be explained using prepositional phrases (PP) or verb phrases (VP). Describing an image is therefore just a matter of identifying these constituents to describe images. We propose to train a model which can predict the phrases which are likely to be in a given image.  2  UA man in a helment skateboarding before an audience.Man riding on edge of an oval ramp with a skate board.A man riding a skateboard up the side of a wooden ramp.A man on a skateboard is doing a trick.A man is grinding a ramp on a skateboard.Va mana wooden rampridingona skate boardis grindingwithNPVPPPAccepted as a workshop contribution at ICLR 2015  Phrase representations Noun phrases or verb phrases are often a combination of several words. Good word vector representations can be obtained very quickly with many different recent ap- proaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Pennington et al., 2014; Lebret & Collobert, 2014). Mikolov et al. (2013a) also showed that simple vector addition can often pro- duce meaningful results, such as king - man + woman ≈ queen. By leveraging the ability of these word vector representations to compose, representations for phrases are easily computed with an element-wise addition.  From phrases to sentence After identifying the most likely constituents of the image, we propose to use a statistical language model to combine them and generate a proper descrip- tion. A general framework is deﬁned to reduce the total number of combination and thus speed up the process for generating sentences. The constrained language model used is illustrated in Figure 2. In general, a noun phrase is always followed by a verb phrase or a prepositional phrase, and both are then followed by another noun phrase. This process is repeated N times until reaching the end of a sentence (character- ized by a period). This heuristic is based on the analysis of syntax if the sentences (see Sec- tion 3.1).  2.2 A MULTIMODAL REPRESENTATION  start  NP  c  VP  PP  .  c  N  Figure 2: The constrained language model for generating description given the predicted phrases for an image.  Image representations For the representa- tion of images, we choose to use a Convolu- tional Neural Network. CNNs have been widely used for many different vision domains and are currently the state-of-the-art in many object recognition tasks. We consider a CNN that has been pre-trained for the task of object classiﬁcation. We use a CNN solely to the purpose of feature extraction, that is, no learning is done in the CNN layers. Learning of a common space for image and phrase representations Let I be the set of training images, D the set of all sentence descriptions for I, C the set of all phrases occuring in D, and θ the trainable parameters of the model. Di is the set of sentences describing a given image i ∈ I, and Cd is the set of phrases which compose a sentence description d ∈ Di. The training objective is to ﬁnd the phrases c that describe the images i by maximizing the log probability:  (cid:88)  (cid:88)  (cid:88)  i∈I  d∈Di  c∈Cd  log p(c|i)  (1)  Each image i ∈ I is represented by a vector xi ∈ Rn thanks to a pre-trained CNN. Each phrase c is composed of K words w which are represented by a vector xw ∈ Rm thanks to another pre-trained model for word representations. A vector representation zc for a phrase c = {w1, . . . , wK} is then calculated by averaging its word vector representations:  k=1  zc =  xwk .  (cid:3) ∈ Rm×|C| .  (cid:2)zc1 , . . . , zc|C|  (2) Vector representations for all phrases c ∈ C can thus be obtained to build a matrix V = In general, m (cid:28) n. An encoding function is therefore deﬁned to map image representations xi ∈ Rn in the same vector space than phrase representations zc ∈ Rm: (3) where U ∈ Rn×m is initialized randomly and trained to encode images in the same vectorial space than the phrases used for their descriptions. Because representations of images and phrases are in a common vector space, similarities between a given image i and all phrases can be calculated:  gθ(i) = xiU ,  K(cid:88)  1 K  fθ(i) = gθ(i)V ,  3  (4)  Accepted as a workshop contribution at ICLR 2015  (a) The number of phrases per sentence.  (b) The 20 most frequent sentence syntactic structures.  Figure 3: Sentence structure statistics of COCO datasets.  where V is ﬁne-tuned to incorporate other features coming from the images. By denoting [fθ(i)]j the score for the jth phrase, this score can be interpreted as the conditional probability p(c = cj|i, θ) by applying a softmax operation over all the phrases:  p(c = cj|i, θ) =  (cid:80)|C|  e[fθ(i)]j k=1 e[fθ(i)]k  .  (5)  In practice, this formulation is often impractical due to the large set of possible phrases C. Training with negative sampling With θ = {U, V } and a negative sampling approach, we instead minimize the following logistic loss function with respect to θ:  θ (cid:55)→(cid:88)  (cid:88)  (cid:88)  (cid:16)  i∈I  d∈Di  cj∈Cd  (cid:16)  (cid:16)  (cid:17)  N(cid:88)  k=1  log  1 + e[fθ(i)]j  +  log  1 + e−[fθ(i)]k  .  (6)  (cid:17)(cid:17)  Thus the task is to distinguish the target phrase from draws from the noise distribution, where there are N negative samples for each data sample. The model is trained using stochastic gradient descent.  3 EXPERIMENTS  3.1 EXPERIMENTAL SETUP  Dataset We validate our model on the recently proposed COCO dataset (Lin et al., 2014), which contains complex images with multiple objects. The dataset contains a total of 123,000 images, each of them with 5 human annotated sentences. The testing images has not yet been released. We thus use two sets of 5,000 images from the validation images for validation and test, as in Karpathy & Fei-Fei (2014)1. We measure the quality of of the generated sentences using the popular, yet controversial, BLEU score (Papineni et al., 2002).  Feature selection Following Karpathy & Fei-Fei (2014), the image features are extracted using VGG CNN (Chatﬁeld et al., 2014). This model generates image representations of dimension 4096 form RGB input images. For sentence features, we extract phrases from the 576,737 training sen- tences with the SENNA software2. Statistics reported in Figure 3 conﬁrm the hypothesis that image  1Available at http://cs.stanford.edu/people/karpathy/deepimagesent/ 2Available at http://ml.nec-labs.com/senna/  4  01234567NPVPPPAppareance frequencies (%)020406080llllllllllllllllllllAppareance frequencies (%)NP VP NP PP NP ONP VP NP ONP PP NP VP NP ONP PP NP PP NP ONP VP NP PP NP PP NP ONP VP NP VP NP ONP PP NP VP NP PP NP ONP PP NP ONP PP NP PP NP PP NP ONP NP VP NP ONP PP NP O NP ONP NP VP NP PP NP ONP VP NP VP NP PP NP ONP VP NP PP NP VP NP ONP PP NP PP NP VP NP ONP VP NP PP NP PP NP PP NP ONP VP NP O NP ONP O NP VP NP ONP PP NP NP VP NP ONP VP NP SBAR VP NP O051015Accepted as a workshop contribution at ICLR 2015  descriptions have a simple syntactic structure. A large majority of sentences contain from two to four noun phrases. Two noun phrases then interact using a verb or prepositional phrase. Only phrases occuring at least ten times in the training set are considered. This results in 11,688 noun phrases, 3,969 verb phrases3 and 219 prepositional phrases. Phrase representations are then computed by averaging vector representations of their words. We obtained word vector representations from the Hellinger PCA of a word co-occurence matrix, following the method described in Lebret & Col- lobert (2014). The word co-occurence matrix is built over the entire English Wikipedia4, with a symmetric context window of ten words coming from the 10,000 most frequent words. Words, and therefore also phrases, are represented in 400-dimensional vectors. Learning multimodal representation The parameters θ are U ∈ R4096×400 and V ∈ R400×15876. The latter is initialized with the phrase representations. They are trained with N = 15 negative samples and a learning rate set to 0.00025.  Generating sentences from the predicted phrases According to the statistics of ground-truth sentence structures, we set N = {2, 3, 4}. As nodes, we consider only the top twenty predicted noun phrases, the top ten predicted verb phrases and the top ﬁve predicted prepositional phrases. A trigram language model is used for the transition probabilities between two nodes. The probability of each lexical phrase is calculated using the previous phrases, p(cj|cj−2, cj−1), and the constraint described in Figure 2. In order to reduce the number of sentences generated, we just consider the transitions which are likely to happen (we discard any sentence which would have a trigram transition probability inferior to 0.01). This thresholding also helps to discard sentences that are semantically incorrect.  Ranking generated sentences Our ﬁnal step consists on ranking the sentences generated and choosing the one with the highest score as the ﬁnal output. For each test image i, we generate a set of M sentence candidates using the proposed language model. For each sentence sm (m ∈ {1, ..., M}), we compute its vector representation zsm by averaging the representation of the phrases zc ∈ V that make the sentence. The ﬁnal score for each sentence sm is computed by doing a dot product between the sentence vector representation and the encoded representation of the sample image i:  fθ(i, m) = gθ(i)zsm .  (7)  The output of the system is the sentence which has the highest score. This ranking helps the system to chose the sentence which is closer to the sample image.  3.2 EXPERIMENTAL RESULTS  Table 1 show our sentence generation results on the COCO dataset. BLEU scores are reported up to 4-grams. Human agreement scores are computed by comparing one of the ground-truth description against the others. For comparison, we include results from recently proposed models. Although we use the same test set as in Karpathy & Fei-Fei (2014), there are slight variations between the test sets chosen in other papers. Our model gives competitive results at all N-gram levels. It is interesting to note that our results are very close to the human agreement scores. Examples of full automatic generated sentences can be found in Figure 4.  4 CONCLUSION AND FUTURE WORKS  In this paper, we propose a simple model that is able to automatically generate sentences from an im- age sample. Our model is considerably simpler than the current state of the art, which uses complex recurrent neural networks. We predict phrase components that are likely to describe a given image and use a simple statistical language model to generate sentences. Our model achieves promising ﬁrst results. Future works include apply the model to different datasets (Flickr8k, Flickr30k and ﬁnal COCO version for benchmarking), do image-sentence ranking experiments and improve the language model used.  3Pre-verbal and post-verbal adverb phrases are merged with verb phrases. 4Available at http://download.wikimedia.org. We took the January 2014 version.  5  Accepted as a workshop contribution at ICLR 2015  Captioning Method  Human agreement  Karpathy & Fei-Fei (2014) Vinyals et al. (2014) Donahue et al. (2014) Fang et al. (2014) Our model  B-1  B-2  B-3  B-4  0.68  0.45  0.30  0.20  0.57 0.67 0.63  -  0.37  0.19  -  -  0.44  0.30  -  -  0.70  0.46  0.30  -  0.21 0.21 0.20  Table 1: Comparison between human agreement scores, state of the art models and our model on the COCO dataset. Note that there are slight variations between the test sets chosen in each paper.  Figure 4: Quantitative results for images on the COCO dataset. Ground-truth annotation (in blue), the NP, VP and PP predicted from the model and generated annotation (in black) are shown for each image. The two last are failure samples.  ACKNOWLEDGEMENTS  This work was supported by the HASLER foundation through the grant “Information and Commu- nication Technology for a Better World 2020” (SmartWorld).  REFERENCES Chatﬁeld, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the Devil in the Details:  Delving Deep into Convolutional Nets. In British Machine Vision Conference, 2014.  Donahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio, Rohrbach, Marcus, Venugopalan, Sub- hashini, Saenko, Kate, and Darrell, Trevor. Long-term recurrent convolutional networks for visual recognition and description. CoRR, abs/1411.4389, 2014.  Fang, Hao, Gupta, Saurabh, Iandola, Forrest N., Srivastava, Rupesh, Deng, Li, Doll´ar, Piotr, Gao, Jianfeng, He, Xiaodong, Mitchell, Margaret, Platt, John C., Zitnick, C. Lawrence, and Zweig, Geoffrey. From captions to visual concepts and back. CoRR, abs/1411.4952, 2014.  6  A man riding skis on a snow covered ski slope. NP: a man, skis, the snow, a person, a woman, a snow covered slope,  a slope, a snowboard, a skier, man. VP: wearing, riding, holding, standing on, skiing down. PP: on, in, of, with, down. A man wearing skis on the snow.A man is doing skateboard tricks on a ramp. NP: a skateboard, a man, a trick, his skateboard, the air, a skateboarder, a ramp, a skate board, a person, a woman. VP: doing, riding, is doing, performing, ﬂying through. PP: on, of, in, at, with. A man riding a skateboard on a ramp.The girl with blue hair stands under the umbrella. NP: a woman, an umbrella, a man, a person, a girl, umbrellas, that, a little girl, a cell phone. VP: holding, wearing, is holding, holds, carrying. PP: with, on, of, in, under. A woman is holding an umbrella.A slice of pizza sitting on top of a white plate. NP:  a plate, a white plate, a table, pizza, it, a pizza, food, a sandwich, top, a close. VP: topped with, has, is, sitting on, is on. PP: of, on, with, in, up. A table with a plate of pizza on a white plate.A person on a surf board in the ocean. NP: a dog, a wave, a person, the water, a man, the ocean, top, that, the snow, a surfboard. VP: riding, standing on, wearing, laying on, sitting on. PP: on, of, in, with, near. A dog standing on top of a wave on the ocean.A cat sitting in a chair staring at a plate on a table. NP: a table, top, a desk, a cat, front, it, that, a laptop, a laptop computer, the table. VP: sitting on, is, sitting in, sitting next to, has. PP: of, on, with, in, next to. A cat sitting on top of a desk with a laptop.Accepted as a workshop contribution at ICLR 2015  Karpathy, Andrej and Fei-Fei, Li. Deep visual-semantic alignments for generating image descrip-  tions. CoRR, abs/1412.2306, 2014.  Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying Visual-Semantic Embeddings with Multi-  modal Neural Language Models. volume abs/1411.2539, 2014.  Lebret, Remi and Collobert, Ronan. Rehabilitation of Count-based Models for Word Vector Repre-  sentations. CoRR, abs/1412.4930, 2014.  Lin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Doll´ar, Piotr, and Zitnick, C. Lawrence. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), 2014.  Mao, Junhua, Xu, Wei, Yang, Yi, Wang, Jiang, and Yuille, Alan L. Explain images with multimodal  recurrent neural networks. CoRR, abs/1410.1090, 2014.  Mikolov, T., Chen, K., Corrado, G., and Dean, Jeff. Efﬁcient Estimation of Word Representations  in Vector Space. ICLR Workshp, 2013a.  Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. Distributed Representations of Words  and Phrases and their Compositionality. In NIPS. 2013b.  Mnih, A. and Kavukcuoglu, Koray. Learning word embeddings efﬁciently with noise-contrastive  estimation. In NIPS. 2013.  Papineni, Kishore, Roukos, Salim, Ward, Todd, and Zhu, Wei-Jing. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, 2002.  Pennington, J., Socher, Richard, and Manning, C. D. GloVe: Global Vectors for Word Representa-  tion. In Proceedings of EMNLP, 2014.  Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural  image caption generator. CoRR, abs/1411.4555, 2014.  7  ",
1412.6515,2015, On Distinguishability Criteria for Estimating Generative Models,"['On Distinguishability Criteria for Estimating Generative Models', 'Ian Goodfellow']",https://arxiv.org/pdf/1412.6515,"5 1 0 2     y a M 1 2         ] L M  . t a t s [      4 v 5 1 5 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  ON DISTINGUISHABILITY CRITERIA FOR ESTIMATING GENERATIVE MODELS  Ian J. Goodfellow Google Inc., Mountain View, CA goodfellow@google.com  ABSTRACT  Two recently introduced criteria for estimation of generative models are both based on a reduction to binary classiﬁcation. Noise-contrastive estimation (NCE) is an estimation procedure in which a generative model is trained to be able to distinguish data samples from noise samples. Generative adversarial networks (GANs) are pairs of generator and discriminator networks, with the generator net- work learning to generate samples by attempting to fool the discriminator network into believing its samples are real data. Both estimation procedures use the same function to drive learning, which naturally raises questions about how they are related to each other, as well as whether this function is related to maximum like- lihood estimation (MLE). NCE corresponds to training an internal data model belonging to the discriminator network but using a ﬁxed generator network. We show that a variant of NCE, with a dynamic generator network, is equivalent to maximum likelihood estimation. Since pairing a learned discriminator with an appropriate dynamically selected generator recovers MLE, one might expect the reverse to hold for pairing a learned generator with a certain discriminator. How- ever, we show that recovering MLE for a learned generator requires departing from the distinguishability game. Speciﬁcally: (i) The expected gradient of the NCE discriminator can be made to match the expected gradient of MLE, if one is allowed to use a non-stationary noise distribution for NCE, (ii) No choice of dis- criminator network can make the expected gradient for the GAN generator match that of MLE, and (iii) The existing theory does not guarantee that GANs will converge in the non-convex case. This suggests that the key next step in GAN re- search is to determine whether GANs converge, and if not, to modify their training algorithm to force convergence.  INTRODUCTION  1 Many machine learning applications involve ﬁtting a probability distribution over a vector of obser- vations x. This is often accomplished by specifying a parametric family of probability distributions indexed by a parameter vector θ. The resulting pmodel(x; θ) can be approximately matched to the reference distribution pdata(x) by using a statistical estimator to ﬁnd a good value of θ. Maximum likelihood estimation (MLE) is the most popular statistical estimator used to accomplish this task. Maximum likelihood estimation works by maximizing the probability of the observed data according to the model. For several models of interest, exact MLE is intractable, and must be approximated using techniques such as Markov chain Monte Carlo methods. As a result, one popular avenue of research is the design of alternative statistical estimators that have lower computation cost. Two recently proposed methods both employ a binary classiﬁer that attempts to discern whether a given x was drawn from the training data or sampled from a “generator” distribution. Both methods are primarily driven by a function we call the distinguishability game value function:  V (pc, pg) = Ex∼pd log pc(y = 1 | x) + Ex∼pg log pc(y = 0 | x)  where pg(x) is a distribution representing a source of “fake” samples, pd(x) is the distribution over the training data (which we can usually access only approximately via an emprical distribution), and pc(y = 1 | x) is the classiﬁer’s probability that x comes from pd rather than pg.  1  Accepted as a workshop contribution at ICLR 2015  The distinguishability game value function is (proportional to) the log likelihood of the classiﬁer on the binary class labeling problem, where the training examples for the classiﬁer are drawn from a uniform mixture of two components: real data labeled with y = 1 and “fake” samples labeled with y = 0. Of the two recently proposed methods, the ﬁrst is noise-contrastive estimation (Gutmann & Hyvari- nen, 2010). Noise-contrastive estimation (NCE) uses an arbitrary, ﬁxed “noise” distribution for pg. The goal of NCE is to learn a model pm(x) that is used to deﬁne the classiﬁer:  pc(y = 1 | x) =  pm(x)  pm(x) + pg(x)  .  (1)  Learning then proceeds by using a standard optimization algorithm to maximize V . More recently, the generative adversarial network (GAN) framework introduced a different approach using the same value function (Goodfellow et al., 2014). Here, the goal is to learn pg, and there is no explicit pm. Instead, pc is parameterized directly. Rather than treating V as an objective function to maximize, it is used to deﬁne a minimax game, with pg trained to minimize the objective and pc trained to maximize it. MLE, NCE, and GANs are all asymptotically consistent, which means that in the limit of inﬁnitely many samples from pd being available, their criteria each have a unique stationary point that corre- sponds to the learned distribution matching the data distribution1. For MLE and NCE, this stationary point is a global maximum of their objective function, while for GANs it corresponds to a saddle point that is a local maximum for the classiﬁer and a local minimum for the generator. Asymp- totic consistency is proven in the space of unconstrained probability distribution functions; when we move to speciﬁc parametric families of distributions a variety of caveats apply: the given function family may not include the true training data distribution, different parameter values may encode the same function and thus introduce identiﬁability issues, and the optimizer may fail to ﬁnd the global optimum. Because GANs and NCE both use the same value function, it is natural to wonder how they are related to each other, and to maximum likelihood. In this paper, we provide some initial answers to each question. We show  • A modiﬁed version of NCE with a dynamic generator is equivalent to MLE. • The existing theoretical work on GANs does not guarantee convergence on practical appli-  cations.  • Because GANs do the model estimation in the generator network, they cannot recover  maximum likelihood using V .  Throughout this paper, we make some weak regularity assumptions. Speciﬁcally, we will assume that all of our models, be they pm or pg, parameterize the probability distribution such that p(x) > 0 for all x. In the case of continuous random variables, we additionally assume that that p and ∇θp are continuous at all x and θ points. 2 SELF-CONTRASTIVE ESTIMATION The performance of NCE is highly dependent on the choice of noise distribution. It is not difﬁcult to discriminate data samples from totally unstructured noise, so models trained with too simplistic of a noise distribution often underﬁt badly. This has motivated a variety of heuristic attempts to design better noise distributions. Gutmann & Hyvarinen (2010) suggest that “one could choose a noise distribution by ﬁrst estimating a preliminary model of the data, and then use this preliminary model as the noise distribution.” Let use consider the extreme version of this approach, where the model is copied and used as the new noise distribution after every step of learning. We call this approach self-contrastive estimation. Here we show that self-contrastive estimation has the same expected gradient as maximum likelihood estimation.  1 By “match” we mean more formally that pd(x) = pm(x) for all x except a set of measure zero.  2  (cid:21)  (cid:20)  Accepted as a workshop contribution at ICLR 2015  Let θ be a parameter of pm. Then  ∂ ∂θ  V (pc, pg) =  ∂ ∂θ  (cid:2)Ex∼pd log pc(y = 1 | x) + Ex∼pg log pc(y = 0 | x)(cid:3)  (2)  Recall from Eq. 1 that in NCE, the classiﬁer is deﬁned by  pc(y = 1 | x) =  pm(x)  pm(x) + pg(x)  .  In the context of SCE, we have copied pm into pg before each step of learning. It may therefore be tempting to make the assumption that pm = pg and simplify the classiﬁer to pc(y = 1 | x) = 1 2. However, this is incorrect, because we have made a deep copy, rather than aliasing pg to pm. It is crucial that pg is not dependent on θ; therefore when calculating derivatives we must consider the effect of θ on pm but not pg. Substituting Eq. 1 into Eq. 2 we obtain  ∂ ∂θ  ∂ ∂θ  pm(x)  Ex∼pd log  V (pc, pg) =  (cid:2)Ex∼pd [log pm(x) − log (pm(x) + pg(x))] + Ex∼pg [log pg(x) − log (pm(x) + pg(x))](cid:3)  (cid:2)Ex∼pd [log pm(x) − log (pm(x) + pg(x))] − Ex∼pg log (pm(x) + pg(x))(cid:3)  pm(x) + pg(x)  pm(x) + pg(x)  + Ex∼pg log  pg(x)  =  ∂ ∂θ  =  ∂ ∂θ  = Ex∼pd  ∂ ∂θ  [log pm(x) − log (pm(x) + pg(x))] − Ex∼pg  ∂ ∂θ  log (pm(x) + pg(x))  (3)  We can show that the term on the right vanishes, as follows.  Ex∼pg  ∂ ∂θ = Ex∼pg  log (pm(x) + pg(x))  ∂ ∂θ pm(x)  pm(x) + pg(x)  .  Because we have assumed our distributions are strictly positive, we can use the trick ∂ ∂θ exp (log p(x)) = p(x) ∂ ∂ Ex∼pg  ∂θ log p(x):  ∂θ log pm(x)  = Ex∼pg  ∂ ∂θ pm(x)  pm(x) ∂  .  pm(x) + pg(x)  pm(x) + pg(x)  ∂θ p(x) =  Crucially, outside of the differentiation sign, we are allowed to exploit the fact that pm(x) and pg(x) are equal in order to simplify pm/(pm + pg) into 1  2. We are left with  Ex∼pg log pg(x).  1 2  We can use the inverse of the exp(log p(x)) trick to observe that this is equal to ∂ ∂θ ∂θ 1 = 0. (In the continuous case one must use an integral rather than a summation) ∂ Plugging this result in Eq. 3, we obtain: V (pc, pg) = Ex∼pd  [log pm(x) − log (pm(x) + pg(x))]  ∂ ∂θ  ∂ ∂θ  (cid:80)  x pg(x) =  Using the same tricks as previously, we can simplify this to  1 2  Ex∼pd 2 the log likelihood gradient, and the 1  log pm(x).  ∂ ∂θ 2 can of course be folded into the learning rate.  This is 1 Note that while the gradients are equivalent, the objective functions are not. MLE maximizes a sin- gle objective function, while SCE changes the objective function at every step. The MLE gradient for a speciﬁc value of θ always matches the gradient at θ of a different SCE objective constructed speciﬁcally for that θ. The value of the SCE objective does not change over time; each new ob- jective function always has value −2 log 2 as a consequence of the model distribution never being distinguishable from the noise distribution.  3  Accepted as a workshop contribution at ICLR 2015  INTERPRETING THE THEORY OF GANS  3 One can prove the asymptotic consistency of MLE, NCE, and GANs by observing that each mini- mizes a convex divergence in function space. In practice, one cannot optimize directly over probabil- ity distribution functions. Instead, one must optimize over parameters indexing a parametric family of functions. Fortunately, estimators that are consistent in function space often prove to perform well in parameter space. In the case of GANs, a subtlety of the theory may have important implications for the behavior of the algorithm in practice. The existing theory justifying GANs uses convexity in function space to prove not only asymptotic consistency but also convergence. Estimating a model by maximimizing the MLE or NCE objective function is guaranteed to converge for smooth functions that are bounded from above regardless of whether these objective functions are convex. It is possible for optimization to get stuck in a local maximum in parameter space, but the optimization process will at least arrive at some critical point. In the case of GANs, the generator is updated while holding the discriminator ﬁxed, and vice versa. In function space this corresponds to performing subgradient descent on a convex problem, and is guaranteed to converge. In the non-convex case, the existing theory does not specify what will happen. To reach the equilibrium point, pg should be trained to minimize maxpc V (pc, pd). Instead, it takes successive steps partially minimizing V (pc, pd) using the current value of pc at each step. Because by deﬁnition V (pc, pd) ≤ maxpc V (pc, pd) this corresponds to taking steps that partially minimize a lower bound. In the non-convex case, it is conceivable that this could merely make the bound looser rather than decrease the underlying objective function as desired. This may result in the learning procedure oscillating rather than converging. Such a process could explain the underﬁtting that has been observed with GANs thus far. No deep generative model has been demonstrated to be able to memorize a rich, complicated training set. For many models, this could be explained by inaccuracies in the approximation of the gradient or too strong of simplifying assumptions for variational learning. For GANs, the failure to memorize the training set is surprising because the gradient can be computed with backpropagation and there are no variational approximations. Non-convergence of gradient-based learning for continuous games stands out as a candidate explanation for why this happens. To be clear, we do not have any positive results identifying non-convergence as the problem. We are merely identifying one way in which the existing theoretical results for GANs fail to guarantee good performance in practice. We suggest that future work could attempt to positively identify non-convergence in GAN learning or apply better algorithms for computing the equilibrium of the game. 4 GANS CANNOT IMPLEMENT MAXIMUM LIKELIHOOD GANs work by learning in the generator, while NCE works by learning in the discriminator (via a generative model that is used to implicitly deﬁne the generator). This turns out to have important re- sults for learning. Namely, each step of learning in a GAN pair consists of decreasing an expectation of a function of samples from the generator:  where f (x) = log pc (y = 0 | x). For a parameter θ of pg, we ﬁnd  Ex∼pg f (x)  ∂ ∂θ  (cid:90)  =  Ex∼pg f (x)  f (x)  ∂ ∂θ  pg(x)  f (x)pg(x)  ∂ ∂θ  log pg(x).  (cid:90)  =  4  Accepted as a workshop contribution at ICLR 2015  Figure 1: The cost the generator pays for sampling a point is a function of the optimal discriminator’s output for that point. This much is true for maximum likelihood, the minimax formulation of the distinguishability game, and a heuristic reformulation used in most experiments by Goodfellow et al. (2014). Where the methods differ is the exact value of that cost. As we can see, the cost for maximum likelihood only has signiﬁcant gradient through the discriminator if the discriminator is fooled with high conﬁdence. Since this is an extremely rare event when sampling from an untrained generator, estimates of the maximum likelihood gradient based on this approach have high variance.  From this vantage point it is clear that to obtain the maximum likelihood derivatives, we need  f (x) = − pd(x) pg(x)  .  (We could also add an arbitrary constant and still obtain the correct result) Suppose our discriminator is given by pc(y = 1 | x) = σ (a(x)) where σ is the logistic sigmoid function. Suppose further that our discriminator has converged to its optimal value for the current generator,  pc(y = 1 | x) =  pd(x)  pg(x) + pd(x)  .  Then f (x) = − exp (a(x)). This is clearly different from the value given by the distinguishability game, which simpliﬁes to f (x) = −ζ (a(x)), where ζ is the softplus function. See this function plotted alongside the GAN cost in Fig 1. In other words, the discriminator gives us the necessary information to compute the maximum like- lihood gradient of the generator, but it requires that we abandon the distinguishability game. In practice, the estimator based on exp (a(x)) has too high of variance. For an untrained model, sam- pling from the generator almost always yields very low values of pd(x) pg(x) . The value of the expectation is dominated by the rare cases where the generator manages to sample something that resembles the data by chance. Empirically, GANs have been able to overcome this problem but it is not entirely clear why. Further study is needed to understand exactly what tradeoff GANs are making.  5  0.00.20.40.60.81.0Discriminator response to generator sample605040302010010Generator costGAN cost (theory)GAN cost (practice)ML costAccepted as a workshop contribution at ICLR 2015  5 DISCUSSION Our analysis has shown a close relationship between noise contrastive estimation and maximum likelihood. We can now interpret noise-contrastive estimation as being a one-sided version of a distinguishability game. Our analysis has also shown that generative adversarial networks are not as closely related to noise contrastive estimation as previously believed. The fact that the primary model is the generator turns out to result in a departure from maximum likelihood even in situations where NCE and MLE are equivalent. Further study is needed to understand exactly what tradeoff is incurred when using adversarial learning. Finally, the problem of non-convergence of indepen- dent SGD in the non-convex case may explain the underﬁtting observed in GANs and suggests the application of better algorithms for solving for the equilibrium strategies of the distinguishability game.  ACKNOWLEDGMENTS  We would like to thank Andrew Dai, Greg Corrado, David Sussillo, Jeff Dean and David Warde- Farley for their feedback on drafts of this article. REFERENCES Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua. Generative adversarial nets. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Sys- tems 27, pp. 2672–2680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/ 5423-generative-adversarial-nets.pdf.  Gutmann, M. and Hyvarinen, A. Noise-contrastive estimation: A new estimation principle for unnormalized  statistical models. 2010.  6  ",
1412.6448,2015, Embedding Word Similarity with Neural Machine Translation,"['Embedding Word Similarity with Neural Machine Translation', 'Felix Hill', 'Kyunghyun Cho', 'Sebastien Jean', 'Coline Devin', 'and Yoshua Bengio']",https://arxiv.org/pdf/1412.6448,"5 1 0 2    r p A 3         ] L C . s c [      4 v 8 4 4 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  EMBEDDING WORD SIMILARITY WITH NEURAL MACHINE TRANSLATION  Felix Hill University of Cambridge felix.hill@cl.cam.ac.uk  KyungHyun Cho Universit´e de Montr´eal  S´ebastien Jean Universit´e de Montr´eal  Coline Devin Harvey Mudd College  Yoshua Bengio Universit´e de Montr´eal, CIFAR Senior Fellow  ABSTRACT  Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models, a recently-developed class of neu- ral language model. We show that embeddings from translation models outper- form those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degrada- tion of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspeciﬁc) inter-word relatedness.  1  INTRODUCTION  It is well known that word representations can be learned from the distributional patterns in corpora. Originally, such representations were constructed by counting word co-occurrences, so that the fea- tures in one word’s representation corresponded to other words (Landauer & Dumais, 1997; Turney & Pantel, 2010). Neural language models, an alternative method for learning word representations, use language data to optimise (latent) features with respect to a language modelling objective. The objective can be to predict either the next word given the initial words of a sentence (Bengio et al., 2003; Mnih & Hinton, 2009; Collobert & Weston, 2008), or simply a nearby word given a single cue word (Mikolov et al., 2013b; Pennington et al., 2014). The representations learned by neural models (sometimes called embeddings) perform very effectively when applied as pre-trained features in a range of NLP applications and tasks (Baroni et al., 2014).  1  Accepted as a workshop contribution at ICLR 2015  Despite these clear results, it is not well understood how the architecture of neural models affects the information encoded in their embeddings. Here we contribute to this understanding by con- sidering the embeddings learned by architectures with a very different objective function: neural machine translation (NMT) models. NMT models have recently emerged as an alternative to statis- tical, phrase-based translation models, and are beginning to achieve impressive translation perfor- mance (Kalchbrenner & Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). We show that NMT models are not only a potential new direction for machine translation, but are also an effective means of learning word embeddings. Speciﬁcally, translation-based embeddings encode information relating to conceptual similarity (rather than non-speciﬁc relatedness or associa- tion) and lexical syntactic role more effectively than embeddings from monolingual neural language models. We demonstrate that these properties persist when translating between different language pairs (English-French and English-German). Further, based on the observation of subtle language- speciﬁc effects in the embedding spaces, we conjecture as to why similarity dominates over other semantic relations in translation embedding spaces. Finally, we discuss a potential limitation of the application of NMT models for embedding learning - the computational cost of training large vo- cabularies of embeddings - and show that a novel method for overcoming this issue preserves the aforementioned properties of translation-based embeddings.  2 LEARNING EMBEDDINGS WITH NEURAL LANGUAGE MODELS  All neural language models, including NMT models, learn real-valued embeddings (of speciﬁed di- mension) for words in some pre-speciﬁed vocabulary, V , covering many or all words in their training corpus. At each training step, a ‘score’ for the current training example (or batch) is computed based on the embeddings in their current state. This score is compared to the model’s objective function, and the error is backpropagated to update both the model weights (affecting how the score is com- puted from the embeddings) and the embedding features themselves. At the end of this process, the embeddings should encode information that enables the model to optimally satisfy its objective.  2.1 MONOLINGUAL MODELS  In the original neural language model (Bengio et al., 2003) and subsequent variants (Collobert & Weston, 2008), training examples consist of an ordered sequence of n words, with the model trained to predict the n-th word given the ﬁrst n−1 words. The model ﬁrst represents the input as an ordered sequence of embeddings, which it transforms into a single ﬁxed length ‘hidden’ representation, generally by concatenation and non-linear projection. Based on this representation, a probability distribution is computed over the vocabulary, from which the model can sample a guess for the next word. The model weights and embeddings are updated to maximise the probability of correct guesses for all sentences in the training corpus. More recent work has shown that high quality word embeddings can be learned via simpler models with no nonlinear hidden layer (Mikolov et al., 2013b; Pennington et al., 2014). Given a single word or unordered window of words in the corpus, these models predict which words will occur nearby. For each word w in V , a list of training cases (w, c) : c ∈ V is extracted from the training corpus according to some algorithm. For instance, in the skipgram approach (Mikolov et al., 2013b), for each ‘cue word’ w the ‘context words’ c are sampled from windows either side of tokens of w in the corpus (with c more likely to be sampled if it occurs closer to w).1 For each w in V , the model initialises both a cue-embedding, representing the w when it occurs as a cue-word, and a context-embedding, used when w occurs as a context-word. For a cue word w, the model uses the corresponding cue-embedding and all context-embeddings to compute a probability distribution over V that reﬂects the probability of a word occurring in the context of w. When a training example (w, c) is observed, the model updates both the cue-word embedding of w and the context-word embeddings in order to increase the conditional probability of c.  1 Subsequent variants use different algorithms for selecting the (w, c) from the training corpus (Hill &  Korhonen, 2014; Levy & Goldberg, 2014)  2  Accepted as a workshop contribution at ICLR 2015  2.2 BILINGUAL REPRESENTATION-LEARNING MODELS  Various studies have demonstrated that word representations can also be effectively learned from bilingual corpora, aligned at the document, paragraph or word level (Haghighi et al., 2008; Vuli´c et al., 2011; Mikolov et al., 2013a; Hermann & Blunsom, 2014; Chandar et al., 2014). These approaches aim to represent the words from two (or more) languages in a common vector space so that words in one language are close to words with similar or related meanings in the other. The resulting multilingual embedding spaces have been effectively applied to bilingual lexicon ex- traction (Haghighi et al., 2008; Vuli´c et al., 2011; Mikolov et al., 2013a) and document classiﬁca- tion (Klementiev et al.; Hermann & Blunsom, 2014; Chandar et al., 2014; Koˇcisk´y et al., 2014). We focus our analysis on two representatives of this class of (non-NMT) bilingual model. The ﬁrst is that of Hermann & Blunsom (2014), whose embeddings improve on the performance of A. Klemen- tiev & Bhattarai (2012) in document classiﬁcation applications. As with the NMT models introduced in the next section, this model can be trained directly on bitexts aligned only at the sentence rather than word level. When training, for aligned sentences SE and SF in different languages, the model computes representations RE and RF by summing the embeddings of the words in SE and SF re- spectively. The embeddings are then updated to minimise the divergence between RE and RF (since they convey a common meaning). A noise-contrastive loss function ensures that the model does not arrive at trivial (e.g. all zero) solutions to this objective. Hermann & Blunsom (2014) show that, despite the lack of prespeciﬁed word alignments, words in the two languages with similar meanings converge in the bilingual embedding space.2 The second model we examine is that of Faruqui & Dyer (2014). Unlike the models described above, Faruqui & Dyer (2014) showed explicitly that projecting word embeddings from two languages (learned independently) into a common vector space can favourably inﬂuence the orientation of word embeddings when considered in their monolingual subspace; i.e relative to other words in their own language. In contrast to the other models considered in this paper, the approach of Faruqui & Dyer (2014) requires bilingual data to be aligned at the word level.  2.3 NEURAL MACHINE TRANSLATION MODELS  The objective of NMT is to generate an appropriate sentence in a target language St given a sentence Ss in the source language (see, e.g., Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014). As a by-product of learning to meet this objective, NMT models learn distinct sets of embeddings for the vocabularies Vs and Vt in the source and target languages respectively. Observing a training case (Ss, St), these models represent Ss as an ordered sequence of embeddings of words from Vs. The sequence for Ss is then encoded into a single representation RS.3 Finally, by referencing the embeddings in Vt, RS and a representation of what has been generated thus far, the model decodes a sentence in the target language word by word. If at any stage the decoded word does not match the corresponding word in the training target St, the error is recorded. The weights and embeddings in the model, which together parameterise the encoding and decoding process, are updated based on the accumulated error once the sentence decoding is complete. Although NMT models can differ in their low-level architecture (Kalchbrenner & Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), the translation objective exerts similar pressure on the em- beddings in all cases. The source language embeddings must be such that the model can combine them to form single representations for ordered sequences of multiple words (which in turn must en- able the decoding process). The target language embeddings must facilitate the process of decoding these representations into correct target-language sentences.  2The models of Chandar et al. (2014) and Hermann & Blunsom (2014) both aim to minimise the divergence between source and target language sentences represented as sums of word embeddings. Because of these similarities, we do not compare with both in this paper.  3Alternatively, subsequences (phrases) of Ss may be encoded at this stage in place of the whole sen-  tence (Bahdanau et al., 2014).  3  Accepted as a workshop contribution at ICLR 2015  3 EXPERIMENTS  To learn translation-based embeddings, we trained two different NMT models. The ﬁrst is the RNN encoder-decoder (RNNenc, Cho et al., 2014), which uses a recurrent-neural-network to encode all of the source sentence into a single vector on which the decoding process is conditioned. The second is the RNN Search architecture (Bahdanau et al., 2014), which was designed to overcome limitations exhibited by the RNN encoder-decoder when translating very long sentences. RNN Search includes a attention mechanism, an additional feed-forward network that learns to attend to different parts of the source sentence when decoding each word in the target sentence.4 Both models were trained on a 348m word corpus of English-French sentence pairs or a 91m word corpus of English-German sentence pairs.5 To explore the properties of bilingual embeddings learned via objectives other than direct trans- lation, we trained the BiCVM model of Hermann & Blunsom (2014) on the same data, and also downloaded the projected embeddings of Faruqui & Dyer (2014), FD, trained on a bilingual cor- pus of comparable size (≈ 300 million words per language).6 Finally, for an initial comparison with monolingual models, we trained a conventional skipgram model (Mikolov et al., 2013b) and its Glove variant (Pennington et al., 2014) for the same number of epochs on the English half of the bilingual corpus. To analyse the effect on embedding quality of increasing the quantity of training data, we then trained the monolingual models on increasingly large random subsamples of Wikipedia text (up to a total of 1.1bn words). Lastly, we extracted embeddings from a full-sentence language model (CW, Collobert & Weston, 2008), which was trained for several months on the same Wikipedia 1bn word corpus. Note that increasing the volume of training data for the bilingual (and NMT) models was not possible because of the limited size of available sentence-aligned bitexts.  3.1 SIMILARITY AND RELATEDNESS MODELLING  As in previous studies (Agirre et al., 2009; Bruni et al., 2014; Baroni et al., 2014), our initial eval- uations involved calculating pairwise (cosine) distances between embeddings and correlating these distances with (gold-standard) human judgements of the strength of relationships between concepts. For this we used three different gold standards: WordSim-353 (Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2014). Importantly, there is a clear distinction between WordSim-353 and MEN, on the one hand, and SimLex-999, on the other, in terms of the semantic relationship that they quantify. For both WordSim-353 and MEN, annotators were asked to rate how related or associated two concepts are. Consequently, pairs such as [clothes-closet], which are clearly related but ontologically dissimilar, have high ratings in WordSim-353 and MEN. In con- trast, such pairs receive a low rating in SimLex-999, where only genuinely similar concepts, such as [coast- shore], receive high ratings. To reproduce the scores in SimLex-999, models must thus distinguish pairs that are similar from those that are merely related. In particular, this requires models to develop sensitivity to the distinc- tion between synonyms (similar) and antonyms (often strongly related, but highly dissimilar).7 Table 1 shows the correlations of NMT (English-French) embeddings, other bilingually-trained embeddings and monolingual embeddings with these three lexical gold-standards. NMT outper- form monolingual embeddings, and, to a lesser extent, the other bilingually trained embeddings, on SimLex-999. However, this clear advantage is not observed on MEN and WordSim-353, where the projected embeddings of Faruqui & Dyer (2014), which were tuned for high performance on WordSim-353, perform best. Given the aforementioned differences between the evaluations, this  4Access to source code and limited GPU time prevent us from training and evaluating the embeddings from other NMT models such as that of (Kalchbrenner & Blunsom, 2013), (Devlin et al., 2014) and Sutskever et al. (2014). The underlying principles of encoding-decoding also apply to these models, and we expect the embeddings would exhibit similar properties to those analysed here.  5These corpora were produced from the WMT ’14 parallel data after conducting the data-selection proce-  dure described by Cho et al. (2014).  6Available from http://www.cs.cmu.edu/˜mfaruqui/soft.html. The available embeddings  were trained on English-German aligned data, but the authors report similar to for English-French. 7For a more detailed discussion of the similarity/relatedness distinction, see (Hill et al., 2014).  4  Accepted as a workshop contribution at ICLR 2015  WordSim-353  SimLex-999 SimLex-333  ρ MEN ρ ρ ρ TOEFL % Syn/antonym %  Monolingual models  Biling. models  NMT models  Skipgram Glove CW FD BiCVM RNNenc RNNsearch 0.58 0.62 0.49 0.45 0.93 0.74  0.69 0.78 0.39 0.24 0.84 0.76  0.52 0.44 0.29 018 0.75 0.69  0.51 0.60 0.28 0.07 0.64 0.75  0.50 0.45 0.36 0.34 0.87 0.70  0.55 0.71 0.32 0.18 0.78 0.72  0.57 0.63 0.52 0.49 0.93 0.79  Table 1: NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding- learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reﬂect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.  teacher  eaten  Britain  Skipgram vocational in-service college spoiled squeezed cooked Northern Great Ireland  Glove student pupil university cooked eat eating Ireland Kingdom Great  CW student tutor mentor baked peeled cooked Luxembourg Belgium Madrid  FD elementary school classroom ate meal salads UK British London  BiCVM RNNenc RNNsearch instructor professor educator ate consumed eat England UK Syria  professor instructor trainer ate consumed tasted UK British America  faculty professors teach eating eat baking UK British England  Table 2: Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previ- ously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.  suggests that bilingually-trained embeddings, and NMT based embeddings in particular, better cap- ture similarity, whereas monolingual embedding spaces are orientated more towards relatedness. To test this hypothesis further, we ran three more evaluations designed to probe the sensitivity of models to similarity as distinct from relatedness or association. In the ﬁrst, we measured perfor- mance on SimLex-Assoc-333 (Hill et al., 2014). This evaluation comprises the 333 most related pairs in SimLex-999, according to an independent empirical measure of relatedness (free associate generation (Nelson et al., 2004)). Importantly, the pairs in SimLex-Assoc-333, while all strongly related, still span the full range of similarity scores.8 Therefore, the extent to which embeddings can model this data reﬂects their sensitivity to the similarity (or dissimilarity) of two concepts, even in the face of a strong signal in the training data that those concepts are related. The TOEFL synonym test is another similarity-focused evaluation of embedding spaces. This test contains 80 cue words, each with four possible answers, of which one is a correct synonym (Lan- dauer & Dumais, 1997). We computed the proportion of questions answered correctly by each model, where a model’s answer was the nearest (cosine) neighbour to the cue word in its vocabu- lary.9 Note that, since TOEFL is a test of synonym recognition, it necessarily requires models to recognise similarity as opposed to relatedness. Finally, we tested how well different embeddings enabled a supervised classiﬁer to distinguish between synonyms and antonyms, since synonyms are necessarily similar and people often ﬁnd antonyms, which are necessarily dissimilar, to be strongly associated. For 744 word pairs hand- selected as either synonyms or antonyms,10 we presented a Gaussian SVM with the concatenation of the two word embeddings. We evaluated accuracy using 10-fold cross-validation.  8The most dissimilar pair in SimLex-Assoc-333 is [shrink,grow] with a score of 0.23. The highest is [van-  ish,disappear] with 9.80.  9To control for different vocabularies, we restricted the effective vocabulary of each model to the intersection  of all model vocabularies, and excluded all questions that contained an answer outside of this intersection.  10Available online at http://www.cl.cam.ac.uk/˜fh295/.  5  Accepted as a workshop contribution at ICLR 2015  As shown in Table 1, with these three additional similarity-focused tasks we again see the same pattern of results. NMT embeddings outperform other bilingually-trained embeddings which in turn outperform monolingual models. The difference is particularly striking on SimLex-Assoc-333, which suggests that the ability to discern similarity from relatedness (when relatedness is high) is perhaps the most clear distinction between the bilingual spaces and those of monolingual models. These conclusions are also supported by qualitative analysis of the various embedding spaces. As shown in Table 2, in the NMT embedding spaces the nearest neighbours (by cosine distance) to concepts such as teacher are genuine synonyms such as professor or instructor. The bilingual ob- jective also seems to orientate the non-NMT embeddings towards semantic similarity, although some purely related neighbours are also oberved. In contrast, in the monolingual embedding spaces the neighbours of teacher include highly related but dissimilar concepts such as student or college.  3.2  IMPORTANCE OF TRAINING DATA QUANTITY  In previous work, monolingual models were trained on corpora many times larger than the English half of our parallel translation corpus. Indeed, the ability to scale to large quantities of training data was one of the principal motivations behind the skipgram architecture (Mikolov et al., 2013b). To check if monolingual models simply need more training data to capture similarity as effectively as bilingual models, we therefore trained them on increasingly large subsets of Wikipedia.11 As shown in Figure 1, this is not in fact the case. The performance of monolingual embeddings on similarity tasks remains well below the level of the NMT embeddings and somewhat lower than the non-MT bilingual embeddings as the amount of training data increases.  Figure 1: The effect of increasing the amount of training data on the quality of monolingual embed- dings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). ET in the legend indicates models trained on the English half of the translation corpus. Wiki indicates models trained on Wikipedia.  3.3 ANALOGY RESOLUTION  Lexical analogy questions have been used as an alternative way of evaluating word representations. In this task, models must identify the correct answer (girl) when presented with analogy questions such as ‘man is to boy as woman is to ?’. It has been shown that Skipgram-style models are surpris- ingly effective at answering such questions (Mikolov et al., 2013b). This is because, if m, b and w are skigram-style embeddings for man, boy and woman respectively, the correct answer is often the nearest neighbour in the vocabulary (by cosine distance) to the vector v = w + b − m. We evaluated embeddings on analogy questions using the same vector-algebra method as Mikolov et al. (2013b). As in the previous section, for fair comparison we excluded questions containing a  11We did not do the same for our translation models because sentence-aligned bilingual corpora of compa-  rable size do not exist.  6  lllllllllll0.30.40.50.65001000Corpus size (million words)CorrelationSimLex−999lllllllllll0.40.50.60.70.85001000Corpus size (million words)MENlllllllllll0.40.50.60.70.85001000Corpus size (million words)CorrelationWordSim−353llRNNencRNNsearchFDBiCVMskipgramETGloveETskipgramWikiGloveWikiCWAccepted as a workshop contribution at ICLR 2015  word outside the intersection of all model vocabularies, and restricted all answer searches to this reduced vocabulary. This left 11,166 analogies. Of these, 7219 are classed as ‘syntactic’, in that they exemplify mappings between parts-of-speech or syntactic roles (e.g. fast is to fastest as heavy is to heaviest), and 3947 are classed as ‘semantic‘ (Ottawa is to Canada as Paris is to France), since successful answering seems to rely on some (world) knowledge of the concepts themselves. As shown in Fig. 2, NMT embeddings yield relatively poor answers to semantic analogy questions compared with monolingual embeddings and the bilingual embeddings FD (which are projections of similar monolingual embeddings).12 It appears that the translation objective prevents the embedding space from developing the same linear, geometric regularities as skipgram-style models with respect to semantic organisation. This also seems to be true of the embeddings from the full-sentence language model CW. Further, in the case of the Glove and FD models this advantage seems to be independent of both the domain and size of the training data, since embeddings from these models trained on only the English half of the translation corpus still outperform the translation embeddings. On the other hand, NMT embeddings are effective for answering syntactic analogies using the vec- tor algebra method. They perform comparably to or even better than monolingual embeddings when trained on less data (albeit bilingual data). It is perhaps unsurprising that the translation objective incentivises the encoding of a high degree of lexical syntactic information, since coherent target- language sentences could not be generated without knowledge of the parts-of-speech, tense or case of its vocabulary items. The connection between the translation objective and the embedding of lex- ical syntactic information is further supported by the fact that embeddings learned by the bilingual model BiCVM do not perform comparably on the syntactic analogy task. In this model, senten- tial semantics is transferred via a bag-of-words representation, presumably rendering the precise syntactic information less important. When considering the two properties of NMT embeddings highlighted by these experiments, namely the encoding of semantic similarity and lexical syntax, it is worth noting that items in the similarity- focused evaluations of the previous section (SimLex-999 and TOEFL) consist of word groups or pairs that have identical syntactic role. Thus, even though lexical semantic information is in general pertinent to conceptual similarity (Levy & Goldberg, 2014), the lexical syntactic and conceptual properties of translation embeddings are in some sense independent of one another.  Figure 2: Translation-based embeddings perform best on syntactic analogies (run,ran: hide, hid). Monolingual skipgram/Glove models are better at semantic analogies (father, man; mother, woman)  4 EFFECT OF TARGET LANGUAGE  To better understand why a translation objective yields embedding spaces with particular properties, we trained the RNN Search architecture to translate from English to German.  12The performance of the FD embeddings on this task is higher than that reported by Faruqui & Dyer (2014)  because we search for answers over a smaller total candidate vocabulary.  7  lllllllllll0.000.250.500.755001000Corpus size (million words)Accuracy (%)Semanticlllllllllll0.000.250.500.755001000Corpus size (million words)llRNNencRNNsearchFDBiCVMskipgramETGloveETskipgramWikiGloveWikiCWSyntacticAccepted as a workshop contribution at ICLR 2015  WordSim-353  SimLex-999 SimLex-Assoc-333  ρ MEN ρ ρ ρ  EN- FR 0.60 0.61 0.49 0.45 TOEFL % 0.90 Syn/antonym % 0.72 Syntactic analogies % 0.73 Semantic analogies % 0.10  EN- DE 0.61 0.62 0.50 0.47 0.93 0.70 0.62 0.11  ‘earned’ gained won  acquired  gained deserved  accummulated  ‘castle’ chateau palace fortress  ‘money’ silver funds cash  chateau palace padlock  funds cash  resources  EN-FR  EN-DE  Table 3: Comparison of embeddings learned by RNN Search models translating between English- French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neigh- bours of selected cue words (right). Bold italics indicate target-language-speciﬁc effects. Evaluation items and vocabulary searches were restricted to words common to both models.  As shown in Table 3 (left side), the performance of the source (English) embeddings learned by this model was comparable to that of those learned by the English-to-French model on all evaluations, even though the English-German training corpus (91 million words) was notably smaller than the English-French corpus (348m words). This evidence shows that the desirable properties of transla- tion embeddings highlighted thus far are not particular to English-French translation, and can also emerge when translating to a different language family, with different word ordering conventions.  5 OVERCOMING THE VOCABULARY SIZE PROBLEM  A potential drawback to using NMT models for learning word embeddings is the computational cost of training such a model on large vocabularies. To generate a target language sentence, NMT models repeatedly compute a softmax distribution over the target vocabulary. This computation scales with vocabulary size and must be repeated for each word in the output sentence, so that training models with large output vocabularies is challenging. Moreover, while the same computational bottleneck does not apply to the encoding process or source vocabulary, there is no way in which a translation model could learn a high quality source embedding for a word if the plausible translations were out- side its vocabulary. Thus, limitations on the size of the target vocabulary effectively limit the scope of NMT models as representation-learning tools. This contrasts with the shallower monolingual and bilingual representation-learning models considered in this paper, which efﬁciently compute a dis- tribution over a large target vocabulary using either a hierarchical softmax (Morin & Bengio, 2005) or approximate methods such as negative sampling (Mikolov et al., 2013b; Hermann & Blunsom, 2014), and thus can learn large vocabularies of both source and target embeddings. A recently proposed solution to this problem enables NMT models to be trained with larger target vocabularies (and hence larger meaningful source vocabularies) at comparable computational cost to training with a small target vocabulary (Jean et al., 2014). The algorithm uses (biased) importance sampling (Bengio & S´en´ecal, 2003) to approximate the probability distribution of words over a large target vocabulary with a ﬁnite set of distributions over subsets of that vocabulary. Despite this element of approximation in the decoder, extending the effective target vocabulary in this way signiﬁcantly improves translation performance, since the model can make sense of more sentences in the training data and encounters fewer unknown words at test time. In terms of representation learning, the method provides a means to scale up the NMT approach to vocabularies as large as those learned by monolingual models. However, given that the method replaces an exact calculation with an approximate one, we tested how the quality of source embeddings is affected by scaling up the target language vocabulary in this way. As shown in Table 4, there is no signiﬁcant degradation of embedding quality when scaling to large vocabularies with using the approximate decoder. Note that for a fair comparison we ﬁltered these evaluations to only include items that are present in the smaller vocabulary. Thus, the numbers do not directly reﬂect the quality of the additional 470k embeddings learned by the extended vocabulary models, which one would expect to be lower since they are words of lower frequency. All embed- dings can be downloaded from http://www.cl.cam.ac.uk/˜fh295/, and the embeddings  8  Accepted as a workshop contribution at ICLR 2015  RNN Search RNN Search RNN Search-LV RNN Search-LV  WordSim-353  SimLex-999 SimLex-Assoc-333  ρ MEN ρ ρ ρ TOEFL % Syn/antonym % Syntactic analogies % Semantic analogies %  EN-FR 0.60 0.61 0.49 0.45 0.90 0.72 0.73 0.10  EN-DE 0.61 0.62 0.50 0.47 0.93 0.70 0.62 0.11  EN-FR 0.59 0.62 0.51 0.47 0.93 0.74 0.71 0.08  EN-DE 0.57 0.61 0.50 0.46 0.98 0.71 0.62 0.13  Table 4: Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.  from the smaller vocabulary models can be interrogated at http://lisa.iro.umontreal. ca/mt-demo/embs/.13  6 HOW SIMILARITY EMERGES  Although NMT models appear to encode both conceptual similarity and syntactic information for any source and target languages, it is not the case that embedding spaces will always be identical. Interrogating the nearest neighbours of the source embedding spaces of the English-French and English-German models reveals occasional language-speciﬁc effects. As shown in Table 3 (right side), the neighbours for the word earned in the English-German model are as one might expect, whereas the neighbours from the English-French model contain the somewhat unlikely candidate won. In a similar vein, while the neighbours of the word castle from the English-French model are unarguably similar, the neighbours from the English-German model contain the word padlock. These infrequent but striking differences between the English-German and English-French source embedding spaces indicate how similarity might emerge effectively in NMT models. Tokens of the French verb gagner have (at least) two possible English translations (win and earn). Since the translation model, which has limited encoding capacity, is trained to map tokens of win and earn to the same place in the target embedding space, it is efﬁcient to move these concepts closer in the source space. Since win and earn map directly to two different verbs in German, this effect is not observed. On the other hand, the English nouns castle and padlock translate to a single noun (Schloss) in German, but different nouns in French. Thus, padlock and castle are only close in the source embeddings from the English-German model. Based on these considerations, we can conjecture that the following condition on the semantic con- ﬁguration between two language is crucial to the effective induction of lexical similarity.  (1)  For s1 and s2 in the source language, there is some t in the target language such that there are sentences in the training data in which s1 translates to t and sentences in which s2 translates to t.  (2)  s1 and s2 are semantically similar.  if and only if  Of course, this condition is not true in general. However, we propose that the extent to which it holds over all possible word pairs corresponds to the quality of similarity induction in the translation embedding space. Note that strong polysemy in the target language, such as gagner = win, earn,  13A different solution to the rare-word problem was proposed by (Luong et al., 2014). We do not evaluate  the effects on the resulting embeddings of this method because we lack access to the source code.  9  Accepted as a workshop contribution at ICLR 2015  can lead to cases in which 1 is satisﬁed but 2 is not. The conjecture claims that these cases are detrimental to the quality of the embedding space (at least with regards to similarity). In practice, qualitative analyses of the embedding spaces and native speaker intuitions suggest that such cases are comparatively rare. Moreover, when such cases are observed, s1 and s2, while perhaps not similar, are not strongly dissimilar. This could explain why related but strongly dissimilar concepts such as antonym pairs do not converge in the translation embedding space. This is also consistent with qualitative evidence presented by (Faruqui & Dyer, 2014) that projecting monolingual embeddings into a bilingual space orientates them to better reﬂect the synonymy/antonymy distinction.  7 CONCLUSION  In this work, we have shown that the embedding spaces from neural machine translation models are orientated more towards conceptual similarity than those of monolingual models, and that transla- tion embedding spaces also reﬂect richer lexical syntactic information. To perform well on similarity evaluations such as SimLex-999, embeddings must distinguish information pertinent to what con- cepts are (their function or ontology) from information reﬂecting other non-speciﬁc inter-concept relationships. Concepts that are strongly related but dissimilar, such as antonyms, are particularly challenging in this regard (Hill et al., 2014). Consistent with the qualitative observation made by Faruqui & Dyer (2014), we suggested how the nature of the semantic correspondence between the words in languages enables NMT embeddings to distinguish synonyms and antonyms and, more generally, to encode the information needed to reﬂect human intuitions of similarity. The language-speciﬁc effects we observed in Section 4 suggest a potential avenue for improving translation and multi-lingual embeddings in future work. First, as the availability of fast GPUs for training grows, we would like to explore the embeddings learned by NMT models that translate between much more distant language pairs such as English-Chinese or English-Arabic. For these language pairs, the word alignment will less monotonic and may result in even more important se- mantic and syntactic information being encoded in the lexical representation. Further, as observed by both Hermann & Blunsom (2014) and Faruqui & Dyer (2014), the bilingual representation learning paradigm can be naturally extended to update representations based on correspondences between multiple languages (for instance by interleaving English-French and English-German training ex- amples). Such an approach should smooth out language-speciﬁc effects, leaving embeddings that encode only language-agnostic conceptual semantics and are thus more generally applicable. An- other related challenge is to develop smaller or less complex representation-learning tools that en- code similarity with as much ﬁdelity as NMT models but without the computational overhead. One promising approach for this is to learn word alignments and word embeddings jointly (Koˇcisk´y et al., 2014). This approach is effective for cross-lingual document classiﬁcation, although the authors do evaluate the monolingual subspace induced by the model.14 Not all word embeddings learned from text are born equal. Depending on the application, those learned by NMT models may have particularly desirable properties. For decades, distributional semantic models have aimed to exploit Firth’s famous distributional hypothesis to induce word meanings from (monolingual) text. However, the hypothesis also betrays the weakness of the mono- lingual distributional approach when it comes to learning humah-quality concept representations. For while it is undeniable that “words which are similar in meaning appear in similar distributional contexts” (Firth, 1957), the converse assertion, which is what really matters, is only sometimes true.  ACKNOWLEDGMENTS  The authors would like to thank the developers of Theano Bergstra et al. (2010); Bastien et al. (2012). We acknowledge the support of the following agencies for research funding and computing support: St John’s College Cambridge, NSERC, Calcul Qu´ebec, Compute Canada, the Canada Research Chairs and CIFAR.  14These embeddings are not publicly available and we were unable to re-train them using the source code.  10  Accepted as a workshop contribution at ICLR 2015  REFERENCES A. Klementiev, I. Titov and Bhattarai, B. Inducing crosslingual distributed representations of words.  In COLING, 2012.  Agirre, Eneko, Alfonseca, Enrique, Hall, Keith, Kravalova, Jana, Pasca, Marius, and Soroa, Aitor. In  A study on similarity and relatedness using distributional and wordnet-based approaches. Proceedings of NAACL-HLT 2009, 2009.  Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly  learning to align and translate. arXiv:1409.0473 [cs.CL], September 2014.  Baroni, Marco, Dinu, Georgiana, and Kruszewski, Germ´an. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, 2014.  Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Bengio, Yoshua and S´en´ecal, Jean-S´ebastien. Quick training of probabilistic neural nets by impor-  tance sampling. In Proceedings of AISTATS 2003, 2003.  Bengio, Yoshua, Ducharme, R´ejean, Vincent, Pascal, and Janvin, Christian. A neural probabilistic  language model. J. Mach. Learn. Res., 3:1137–1155, March 2003.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Bruni, Elia, Tran, Nam-Khanh, and Baroni, Marco. Multimodal distributional semantics. J. Artif.  Intell. Res.(JAIR), 49:1–47, 2014.  Chandar, Sarath, Lauly, Stanislas, Larochelle, Hugo, Khapra, Mitesh M., Ravindran, Balaraman, Raykar, Vikas, and Saha, Amrita. An Autoencoder Approach to Learning Bilingual Word Repre- sentations. In NIPS, 2014.  Cho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP 2014), October 2014. to appear.  Collobert, Ronan and Weston, Jason. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pp. 160–167. ACM, 2008.  Devlin, Jacob, Zbib, Rabih, Huang, Zhongqiang, Lamar, Thomas, Schwartz, Richard, and Makhoul, John. Fast and robust neural network joint models for statistical machine translation. In 52nd Annual Meeting of the Association for Computational Linguistics, Baltimore, MD, USA, June, 2014.  Faruqui, Manaal and Dyer, Chris. Improving vector space word representations using multilingual  correlation. In Proceedings of EACL, volume 2014, 2014.  Firth, J, R. A synopsis of linguistic theory 1930-1955, pp. 1–32. Oxford: Philological Society, 1957.  Haghighi, Aria, Liang, Percy, Berg-Kirkpatrick, Taylor, and Klein, Dan. Learning bilingual lexicons  from monolingual corpora. In ACL, volume 2008, pp. 771–779, 2008.  Hermann, Karl Moritz and Blunsom, Phil. Multilingual Distributed Representations without Word Alignment. In Proceedings of ICLR, April 2014. URL http://arxiv.org/abs/1312. 6173.  11  Accepted as a workshop contribution at ICLR 2015  Hill, Felix and Korhonen, Anna. Learning abstract concepts from multi-modal data: Since you probably can’t see what i mean. In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP 2014), October 2014.  Hill, Felix, Reichart, Roi, and Korhonen, Anna. Simlex-999: Evaluating semantic models with  (genuine) similarity estimation. arXiv preprint arXiv:1408.3456, 2014.  Jean, S´ebastien, Cho, Kyunghyun, Memisevic, Roland, and Bengio, Yoshua. On using very large  target vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007, 2014.  Kalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. Seattle, October  2013. Association for Computational Linguistics.  Klementiev, Alexandre, Titov, Ivan, and Bhattarai, Binod. Inducing crosslingual distributed repre-  sentations of words. COLING.  Koˇcisk´y, Tom´aˇs, Hermann, Karl Moritz, and Blunsom, Phil. Learning Bilingual Word Repre- In Proceedings of ACL, June 2014. URL http:  sentations by Marginalizing Alignments. //arxiv.org/abs/1405.0947.  Landauer, Thomas K and Dumais, Susan T. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211, 1997.  Levy, Omer and Goldberg, Yoav. Dependency-based word embeddings. In Proceedings of the 52nd  Annual Meeting of the Association for Computational Linguistics, volume 2, 2014.  Luong, Thang, Sutskever, Ilya, Le, Quoc V, Vinyals, Oriol, and Zaremba, Wojciech. Addressing the  rare word problem in neural machine translation. arXiv preprint arXiv:1410.8206, 2014.  Mikolov, Tomas, Le, Quoc V, and Sutskever, Ilya. Exploiting similarities among languages for  machine translation. CORR, 2013a.  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed repre- sentations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp. 3111–3119, 2013b.  Mnih, Andriy and Hinton, Geoffrey E. A scalable hierarchical distributed language model.  Advances in neural information processing systems, pp. 1081–1088, 2009.  In  Morin, Frederic and Bengio, Yoshua. Hierarchical probabilistic neural network language model. In  AISTATS, volume 5, pp. 246–252. Citeseer, 2005.  Nelson, Douglas L, McEvoy, Cathy L, and Schreiber, Thomas A. The university of south ﬂorida free association, rhyme, and word fragment norms. Behavior Research Methods, Instruments, & Computers, 36(3):402–407, 2004.  Pennington, Jeffrey, Socher, Richard, and Manning, Christopher. Glove: Global vectors for word In Proceedings of the Empirical Methods in Natural Language Processing  representation. (EMNLP 2014), October 2014.  Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural net-  works. arXiv preprint arXiv:1409.3215, 2014.  Turney, Peter D and Pantel, Patrick. From frequency to meaning: Vector space models of semantics.  Journal of artiﬁcial intelligence research, 37(1):141–188, 2010.  Vuli´c, Ivan, De Smet, Wim, and Moens, Marie-Francine. Identifying word translations from com- parable corpora using latent topic models. In Proceedings of the 49th Annual Meeting of the As- sociation for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pp. 479–484. Association for Computational Linguistics, 2011.  12  ",
1412.6622,2015, Deep metric learning using Triplet network,"['Deep metric learning using Triplet network', 'Elad Hoffer and Nir Ailon']",https://arxiv.org/pdf/1412.6622,"8 1 0 2    c e D 4         ]  G L . s c [      4 v 2 2 6 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  DEEP METRIC LEARNING USING TRIPLET NETWORK  Elad Hoffer Department of Electrical Engineering Technion Israel Institute of Technology ehoffer@tx.technion.ac.il  Nir Ailon ∗ Department of Computer Science Technion Israel Institute of Technology nailon@cs.technion.ac.il  ABSTRACT  Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classiﬁcation task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was deﬁned by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.  1  INTRODUCTION  For the past few years, deep learning models have been used extensively to solve various machine learning tasks. One of the underlying assumptions is that deep, hierarchical models such as con- volutional networks create useful representation of data (Bengio (2009); Hinton (2007)), which can then be used to distinguish between available classes. This quality is in contrast with traditional approaches requiring engineered features extracted from data and then used in separate learning schemes. Features extracted by deep networks were also shown to provide useful representation (Zeiler & Fergus (2013a); Sermanet et al. (2013)) which can be, in turn, successfully used for other tasks (Razavian et al. (2014)). Despite their importance, these representations and their corresponding induced metrics are often treated as side effects of the classiﬁcation task, rather than being explicitly sought. There are also many interesting open question regarding the intermediate representations and their role in disen- tangling and explaining the data (Bengio (2013)). Notable exceptions where explicit metric learning is preformed are the Siamese Network variants (Bromley et al. (1993); Chopra et al. (2005); Hadsell et al. (2006)), in which a contrastive loss over the metric induced by the representation is used to train the network to distinguish between similar and dissimilar pairs of examples. A contrastive loss favours a small distance between pairs of examples labeled as similar, and large distances for pairs labeled dissimilar. However, the representations learned by these models provide sub-par results when used as features for classiﬁcation, compared with other deep learning models including ours. Siamese networks are also sensitive to calibration in the sense that the notion of similarity vs dis- similarity requires context. For example, a person might be deemed similar to another person when a dataset of random objects is provided, but might be deemed dissimilar with respect to the same other person when we wish to distinguish between two individuals in a set of individuals only. In our model, such a calibration is not required. In fact, in our experiments here, we have experienced hands on the difﬁculty in using Siamese networks. We follow a similar task to that of Chechik et al. (2010). For a set of samples P and a chosen rough similarity measure r(x, x(cid:48)) given through a training oracle (e.g how close are two images of objects  ∗The author acknowledges the generous support of ISF grant number 1271/13  1  Accepted as a workshop contribution at ICLR 2015  semantically) we wish to learn a similarity function S(x, x(cid:48)) induced by a normed metric. Unlike Chechik et al. (2010)’s work, our labels are of the form r(x, x1) > r(x, x2) for triplets x, x1, x2 of objects. Accordingly, we try to ﬁt a metric embedding and a corresponding similarity function satisfying:  S(x, x1) > S(x, x2), ∀x, x1, x2 ∈ P for which r(x, x1) > r(x, x2).  In our experiment, we try to ﬁnd a metric embedding of a multi-class labeled dataset. We will always take x1 to be of the same class as x and x2 of a different class, although in general more complicated choices could be made. Accordingly, we will use the notation x+ and x− instead of x1, x2. We focus on ﬁnding an L2 embedding, by learning a function F (x) for which S(x, x(cid:48)) = (cid:107)F (x) − F (x(cid:48))(cid:107)2. Inspired from the recent success of deep learning, we will use a deep network as our embedding function F (x). We call our approach a triplet network. A similar approach was proposed in Wang et al. (2014) for the purpose of learning a ranking function for image retrieval. Compared with the single application proposed in Wang et al. (2014), we make a comprehensive study of the triplet architecture which is, as we shall argue below, interesting in and of itself. In fact, we shall demonstrate below that the triplet approach is a strong competitor to the Siamese approach, its most obvious competitor.  2 THE TRIPLET NETWORK  A Triplet network (inspired by ”Siamese network”) is comprised of 3 instances of the same feed- forward network (with shared parameters). When fed with 3 samples, the network outputs 2 inter- mediate values - the L2 distances between the embedded representation of two of its inputs from the representation of the third. If we will denote the 3 inputs as x, x+ and x−, and the embedded representation of the network as N et(x), the one before last layer will be the vector:  T ripletN et(x, x−, x+) =  (cid:20)(cid:107)N et(x) − N et(x−)(cid:107)2  (cid:107)N et(x) − N et(x+)(cid:107)2  (cid:21)  ∈ R2 + .  In words, this encodes the pair of distances between each of x+ and x− against the reference x.  Figure 1: Triplet network structure  2  kNet(x)−Net(x−)k2kNet(x)−Net(x+)k2x−xx+ComparatorNetNetNetAccepted as a workshop contribution at ICLR 2015  2.1 TRAINING  Training is preformed by feeding the network with samples where, as explained above, x and x+ are of the same class, and x− is of different class. The network architecture allows the task to be expressed as a 2-class classiﬁcation problem, where the objective is to correctly classify which of x+ and x− is of the same class as x. We stress that in a more general setting, where the objective might be to learn a metric embedding, the label determines which example is closer to x. Here we simply interpret “closeness” as “sharing the same label”. In order to output a comparison operator from the model, a SoftMax function is applied on both outputs - effectively creating a ratio measure. Similarly to traditional convolutional-networks, training is done by simple SGD on a negative-log- likelihood loss with regard to the 2-class problem. We later examined that better results are achieved when the loss function is replaced by a simple MSE on the soft-max result, compared to the (0, 1) vector, so that the loss is  Loss(d+, d−) = (cid:107)(d+, d− − 1)(cid:107)2  2 = const · d2  +  where  and  d+ =  d− =  e(cid:107)N et(x)−N et(x+)(cid:107)2  e(cid:107)N et(x)−N et(x+)(cid:107)2 + e(cid:107)N et(x)−N et(x−)(cid:107)2  e(cid:107)N et(x)−N et(x−)(cid:107)2  e(cid:107)N et(x)−N et(x+)(cid:107)2 + e(cid:107)N et(x)−N et(x−)(cid:107)2  .  We note that Loss(d+, d−) → 0 iff (cid:107)N et(x)−N et(x+)(cid:107) (cid:107)N et(x)−N et(x−)(cid:107) → 0, which is the required objective. By using the same shared parameters network, we allow the back-propagation algorithm to update the model with regard to all three samples simultaneously.  3 TESTS AND RESULTS  The Triplet network was implemented and trained using the Torch7 environment (Collobert et al. (2011)).  3.1 DATASETS  We experimented with 4 datasets. The ﬁrst is Cifar10 (Krizhevsky & Hinton (2009)), consisting of 60000 32x32 color images of 10 classes (of which 50000 are used for training only, and 10000 for test only). The second dataset is the original MNIST (LeCun et al. (1998)) consisting of 60000 28x28 gray-scale images of handwritten digits 0-9, and a corresponding set of 10000 test images. The third is the Street-View-House-Numbers (SVHN) of Netzer et al. consisting of 600000 32x32 color images of house-number digits 0-9. The fourth dataset is STL10 of Coates et al. (2011), similar to Cifar10 and consisting of 10 object classes, only with 5000 training images (instead of 50000 in Cifar) and a bigger 96x96 image size. It is important to note that no data augmentation or whitening was applied, and the only prepro- cessing was a global normalization to zero mean and unit variance. Each training instance (for all four datasets) was a uniformly sampled set of 3 images, 2 of which are of the same class (x and x+), and the third (x−) of a different class. Each training epoch consisted of 640000 such instances (randomly chosen each epoch), and a ﬁxed set of 64000 instances used for test. We emphasize that each test instance involves 3 images from the set of test images which was excluded from training.  3.2 THE EMBEDDING NET  For Cifar10 and SVHN we used a convolutional network, consisting of 3 convolutional and 2x2 max-pooling layers, followed by a fourth convolutional layer. A ReLU non-linearity is applied between two consecutive layers. Network conﬁguration (ordered from input to output) consists of ﬁlter sizes {5,3,3,2}, and feature map dimensions {3,64,128,256,128} where a 128 vector is the ﬁnal embedded representation of the network. Usually in convolutional networks, a subsequent fully-connected layer is used for classiﬁcation. In our net this layer is removed, as we are interested in a feature embedding only.  3  Accepted as a workshop contribution at ICLR 2015  The network for STL10 is identical, only with stride=3 for the ﬁrst layer, to allow the bigger input size. The network used for MNIST was a smaller version consisting of smaller feature map sizes {1,32,64,128}.  3.3 RESULTS  Training on all datasets was done by SGD, with initial learning-rate of 0.5 and a learning rate decay regime. We used a momentum value of 0.9. We also used the dropout regularization technique with p = 0.5 to avoid over-ﬁtting. After training on each dataset for 10-30 epochs, the network reached a ﬁxed error over the triplet comparisons. We then used the embedding network to extract features from the full dataset, and trained a simple 1-layer network model on the full 10-class classiﬁcation task (using only training set representations). The test set was then measured for accuracy. These results (Figure 2) are comparable to state-of-the-art results with deep learning models, without us- ing any artiﬁcial data augmentation (Zeiler & Fergus (2013b); Goodfellow et al. (2013); Lin et al. (2013)). Noteworthy is the STL10 dataset, in which the TripletNet achieved the best known result for non-augmented data. We conjecture that data augmentation techniques (such as translations, mirroring and noising) may provide similar beneﬁts to those described in previous works. We also note that similar results are achieved when the embedded representations are classiﬁed using a linear SVM model or KNN classiﬁcation with up to 0.5% deviance from the results in Figure 2. Another side-affect noticed, is that the representation seems to be sparse - about 25% non-zero values. This is very helpful when used later as features for classiﬁcation both computationally and with respect to accuracy, as each class is characterised by only a few non zero elements.  Dataset Mnist Cifar10 SVHN STL10  TripletNet SiameseNet Best known result (with no data augmentation) 99.54±0.08% 97.9±0.1% 99.61% Mairal et al. (2014); Lee et al. (2014)  87.1% 95.37% 70.67%  - - -  90.22% Lee et al. (2014) 98.18% Lee et al. (2014) 67.9% Lin & Kung (2014)  Figure 2: Classiﬁcation accuracy (no data augmentation)  3.4  2D VISUALIZATION OF FEATURES  In order to examine our main premise, which is that the network embeds the images into a repre- sentation with meaningful properties, we use PCA to project the embedding into 2d euclidean space which can be easily visualized (ﬁgures 5 4 5). We can see a signiﬁcant clustering by semantic mean- ing, conﬁrming that the network is useful in embedding images into the euclidean space according to their content. Similarity between objects can be easily found by measuring the distance between their embedding and, as shown in the results, can reach high classiﬁcation accuracy using a simple subsequent linear classiﬁer.  3.5 COMPARISON WITH PERFORMANCE OF THE SIAMESE NETWORK  The Siamese network is the most obvious competitor for our approach. Our implementation of the Siamese network consisted of the same embedding network, but with the use of a contrastive loss between a pair of samples, instead of three (as explained in Chopra et al. (2005)). The generated features were then used for classiﬁcation using a similar linear model as was used for the TripletNet method. We measured lower accuracy on the MNIST dataset compared to results gained using the TripletNet representations 2. We have tried a similar comparison for the other three datasets, but unfortunately could not obtain any meaningful result using a Siamese network. We conjecture that this might be related to the problem of context described above, and leave the resolution of this conjecture to future work.  4  Accepted as a workshop contribution at ICLR 2015  Figure 3: CIFAR10 - Euclidean representation of embedded test data, projected onto top two singu- lar vectors  Figure 4: MNIST - Euclidean representation of embedded test data, projected onto top two singular vectors  4 FUTURE WORK  As the Triplet net model allows learning by comparisons of samples instead of direct data labels, usage as an unsupervised learning model is possible. Future investigations can be performed in several scenarios:  5  −10−8−6−4−20246810−6−4−202468101214xyCifar10 2d Feature Representation − Very Deep Model  airplaneautomobilebirdcatdeerdogfroghorseshiptruck−15−10−50510152025−20−15−10−50510xyMNIST 2d Feature Representation  1234567890Accepted as a workshop contribution at ICLR 2015  Figure 5: SVHN - Euclidean representation of embedded test data, projected onto top two singular vectors  • Using spatial information. Objects and image patches that are spatially near are also expected to be similar from a semantic perspective. Therefore, we could use geometric distance between patches of the same image as a rough similarity oracle r(x, x(cid:48)), in an unsupervised setting.  • Using temporal information. The same is applicable to time domain, where two consecu- tive video frames are expected to describe the same object, while a frame taken 10 minutes later is less likely to do so. Our Triplet net may provide a better embedding and improve on past attempts in solving classiﬁcation tasks in an unsupervised environment, such as that of (Mobahi et al. (2009)).  It is also well known that humans tend to be better at accurately providing comparative labels. Our framework can be used in a crowd sourcing learning environment. This can be compared with Tamuz et al. (2011), who used a different approach. Furthermore, it may be easier to collect data trainable on a Triplet network, as comparisons over similarity measures are much easier to attain (pictures taken at the same location, shared annotations, etc).  5 CONCLUSIONS  In this work we introduced the Triplet network model, a tool that uses a deep network to learn useful representation explicitly. The results shown on various datasets provide evidence that the represen- tations that were learned are useful to classiﬁcation in a way that is comparable with a network that was trained explicitly to classify samples. We believe that enhancement to the embedding network such as Network-in-Network model (Lin et al. (2013)), Inception models (Szegedy et al. (2014)) and others can beneﬁt the Triplet net similarly to the way they beneﬁted other classiﬁcation tasks. Considering the fact that this method requires to know only that two out of three images are sampled from the same class, rather than knowing what that class is, we think this should be inquired further, and may provide us insights to the way deep networks learn in general. We have also shown how this model learns using only comparative measures instead of labels, which we can use in the future to leverage new data sources for which clear out labels are not known or do not make sense (e.g hierarchical labels).  6  −6−4−20246810−8−6−4−202468xySVHN 2d Feature Representation  1234567890Accepted as a workshop contribution at ICLR 2015  ACKNOWLEDGEMENTS  We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan-Z GPU used for this research.  REFERENCES Bengio, Yoshua. Learning Deep Architectures for AI, 2009. ISSN 1935-8237.  Bengio, Yoshua. Deep learning of representations: Looking forward.  In Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), volume 7978 LNAI, pp. 1–37, 2013. ISBN 9783642395925. doi: 10.1007/978-3-642-39593-2\ 1.  Bromley, Jane, Bentz, James W, Bottou, L´eon, Guyon, Isabelle, LeCun, Yann, Moore, Cliff, S¨ackinger, Eduard, and Shah, Roopak. Signature veriﬁcation using a siamese time delay neural network. International Journal of Pattern Recognition and Artiﬁcial Intelligence, 7(04):669–688, 1993.  Chechik, Gal, Sharma, Varun, Shalit, Uri, and Bengio, Samy. Large scale online learning of image  similarity through ranking. The Journal of Machine Learning Research, 11:1109–1135, 2010.  Chopra, Sumit, Hadsell, Raia, and LeCun, Yann. Learning a similarity metric discriminatively, with In Proceedings of the IEEE Computer Society Conference on ISBN 0769523722.  application to face veriﬁcation. Computer Vision and Pattern Recognition, volume 1, pp. 539–546, 2005. doi: 10.1109/CVPR.2005.202.  Coates, Adam, Ng, Andrew Y, and Lee, Honglak. An analysis of single-layer networks in unsuper- vised feature learning. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 215–223, 2011.  Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement. Torch7: A matlab-like environment  for machine learning. In BigLearn, NIPS Workshop, number EPFL-CONF-192376, 2011.  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.  Maxout networks. arXiv preprint arXiv:1302.4389, 2013.  Hadsell, Raia, Chopra, Sumit, and LeCun, Yann. Dimensionality reduction by learning an invariant mapping. In Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pp. 1735–1742. IEEE, 2006.  Hinton, Geoffrey E. Learning multiple layers of representation, 2007. ISSN 13646613.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images.  Computer Science Department, University of Toronto, Tech. Rep, 2009.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang, Zhengyou, and Tu, Zhuowen. Deeply-  supervised nets. arXiv preprint arXiv:1409.5185, 2014.  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. CoRR, abs/1312.4400, 2013.  URL http://arxiv.org/abs/1312.4400.  Lin, Tsung-Han and Kung, HT. Stable and efﬁcient representation learning with nonnegativity constraints. In Proceedings of the 31st International Conference on Machine Learning (ICML- 14), pp. 1323–1331, 2014.  Mairal, Julien, Koniusz, Piotr, Harchaoui, Zaid, and Schmid, Cordelia. Convolutional kernel net-  works. In Advances in Neural Information Processing Systems, pp. 2627–2635, 2014.  7  Accepted as a workshop contribution at ICLR 2015  Mobahi, Hossein, Collobert, Ronan, and Weston, Jason. Deep learning from temporal coherence in video. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 737–744. ACM, 2009.  Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Read-  ing digits in natural images with unsupervised feature learning.  Razavian, Ali Sharif, Azizpour, Hossein, Sullivan, Josephine, and Carlsson, Stefan. CNN Features off-the-shelf: an Astounding Baseline for Recognition. Arxiv, 2014. URL http://arxiv. org/abs/1403.6382.  Sermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Michael, Fergus, Rob, and LeCun, Yann. OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks. arXiv preprint arXiv:1312.6229, pp. 1–15, 2013. URL http://arxiv.org/abs/1312. 6229.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. URL http://arxiv.org/abs/1409.4842.  Tamuz, Omer, Liu, Ce, Belongie, Serge, Shamir, Ohad, and Kalai, Adam. Adaptively learning the crowd kernel. In Getoor, Lise and Scheffer, Tobias (eds.), Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11, pp. 673–680, New York, NY, USA, June 2011. ACM. ISBN 978-1-4503-0619-5.  Wang, Jiang, Song, Yang, Leung, Thomas, Rosenberg, Chuck, Wang, Jingbin, Philbin, James, Chen,  Bo, and Wu, Ying. Learning ﬁne-grained image similarity with deep ranking. In CVPR, 2014.  Zeiler, Matthew D and Fergus, Rob. Visualizing and Understanding Convolutional Networks. arXiv  preprint arXiv:1311.2901, 2013a. URL http://arxiv.org/abs/1311.2901.  Zeiler, Matthew D and Fergus, Rob. Stochastic pooling for regularization of deep convolutional  neural networks. arXiv preprint arXiv:1301.3557, 2013b.  8  ",
1412.6617,2015, Understanding Minimum Probability Flow for RBMs Under Various Kinds of Dynamics,"['Understanding Minimum Probability Flow for RBMs Under Various Kinds of Dynamics', 'Daniel Jiwoong Im', 'Ethan Buchman', 'and Graham Taylor']",https://arxiv.org/pdf/1412.6617,"5 1 0 2    r p A 7         ]  G L . s c [      6 v 7 1 6 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  UNDERSTANDING MINIMUM PROBABILITY FLOW FOR RBMS UNDER VARIOUS KINDS OF DYNAMICS  Daniel Jiwoong Im & Ethan Buchman & Graham W. Taylor School of Engineering University of Guelph Guelph, On, Canada {imj,ebuchman,gwtaylor}@uoguelph.ca  ABSTRACT  Energy-based models are popular in machine learning due to the elegance of their formulation and their relationship to statistical physics. Among these, the Restricted Boltzmann Machine (RBM), and its staple training algorithm contrastive divergence (CD), have been the prototype for some recent advancements in the unsupervised training of deep neural networks. However, CD has limited theoretical motivation, and can in some cases produce undesirable behavior. Here, we investigate the performance of Minimum Probability Flow (MPF) learning for training RBMs. Unlike CD, with its focus on approximating an intractable partition function via Gibbs sampling, MPF proposes a tractable, consistent, objective function deﬁned in terms of a Taylor expansion of the KL divergence with respect to sampling dynamics. Here we propose a more general form for the sampling dynamics in MPF, and explore the consequences of different choices for these dynamics for training RBMs. Experimental results show MPF outperforming CD for various RBM conﬁgurations.  1  INTRODUCTION  A common problem in machine learning is to estimate the parameters of a high-dimensional probabilistic model using gradient descent on the model’s negative log likelihood. For exponential models where p(x) is proportional to the exponential of a negative potential function F (x), the gradient of the data negative log-likelihood takes the form  (cid:32)(cid:88)  −(cid:88)  x  ∇θ =  1 |D|  ∂F (x)  x∈D  ∂θ  p(x)  ∂F (x)  ∂θ  (cid:33)  (1)  where the sum in the ﬁrst term is over the dataset, D, and the sum in the second term is over the entire domain of x. The ﬁrst term has the effect of pushing the parameters in a direction that decreases the energy surface of the model at the training data points, while the second term increases the energy of all possible states. Since the second term is intractable for all but trivial models, we cannot, in practice, accommodate for every state of x, but rather resort to sampling. We call states in the sum in the ﬁrst term positive particles and those in the second term negative particles, in accordance with their effect on the likelihood (opposite their effect on the energy). Thus, the intractability of the second term becomes a problem of negative particle selection (NPS). The most famous approach to NPS is Contrastive Divergence (CD) (Hinton, 2002), which is the centre-piece of unsupervised neural network learning in energy-based models. “CD-k” proposes to sample the negative particles by applying a Markov chain Monte Carlo (MCMC) transition operator k times to each data state. This is in contrast to taking an unbiased sample from the distribution by applying the MCMC operator a large number of times until the distribution reaches equilibrium, which is often prohibitive for practical applications. Much research has attempted to better understand this approach and the reasoning behind its success or failure (Sutskever & Tieleman, 2009; MacKay, 2001), leading to many variations being proposed from the perspective of improving the MCMC chain. Here, we take a more general approach to the problem of NPS, in particular, through the lens of the Minimum Probability Flow (MPF) algorithm (Sohl-Dickstein et al., 2011). MPF works by introducing a continuous dynamical system over the model’s distribution, such that the equilibrium state of the dynamical system is the distribution used to model the data. The objective of learning is to minimize  1  Accepted as a workshop contribution at ICLR 2015  the ﬂow of probability from data states to non-data states after inﬁnitesimal evolution under the model’s dynamics. Inuitively, the less a data vector evolves under the dynamics, the closer it is to an equilibrium point; or from our perspective, the closer the equilibrium distribution is to the data. In MPF, NPS is replaced by a more explicit notion of connectivity between states. Connected states are ones between which probability can ﬂow under the dynamical system. Thus, rather than attempting to approximate an intractable function (as in CD-k), we run a simple optimization over an explicit, continuous dynamics, and actually never have to run the dynamics themselves. Interestingly, MPF and CD-k have gradients with remarkably similar form. In fact, the CD-k gradients can be seen as a special case of the MPF gradients - that is, MPF provides a generalized form which reduces to CD-k under a special dynamics. Moreover, MPF provides a consistent estimator for the model parameters, while CD-k as typically formalized is an update heuristic, that can sometimes do bizarre things like go in circles in parameter space (MacKay, 2001). Thus, in one aspect, MPF solves the problem of contrastive divergence by reconceptualizing it as probability ﬂow under an explicit dynamics, rather than the convenient but biased sampling of an intractable function. The challenge thus becomes one of how to design the dynamical system. This paper makes the following contributions. First, we provide an explanation of MPF that begins from the familiar territory of CD-k, rather than the less familiar grounds of the master equation. While familiar to physicists, the master equation is an apparent obscurity in machine learning, due most likely to its general intractability. Part of the attractiveness of MPF is the way it circumvents that intractability. Second, we derive a generalized form for the MPF transition matrix, which deﬁnes the dynamical system. Third, we provide a Theano (Bastien et al., 2012) based implementation of MPF and a number of variants of MPF that run efﬁciently on GPUs1. Finally, we compare and contrast variants of MPF with those of CD-k, and experimentally demonstrate that variants of MPF outperform CD-k for Restricted Boltzmann Machines trained on MNIST and on Caltech-101.  2 RESTRICTED BOLTZMANN MACHINES  E(v, h; θ) = −(cid:88)  (cid:88)  While the learning methods we discuss apply to undirected probabilistic graphical models in general, we will use the Restricted Boltzmann Machine (RBM) as a canonical example. An RBM is an undirected bipartite graph with visible (observed) variables v ∈ {0, 1}D and hidden (latent) variables h ∈ {0, 1}H (Smolensky, 1986). The RBM is an energy-based model where the energy of state v, h is given by  (2) where θ = {W, b, c} are the parameters of the model. The marginalized probability over visible variables is formulated from the Boltzmann distribution,  cjhj  j  i  Wijvihj −(cid:88) bivi −(cid:88) (cid:32)−1 (cid:88)  1  j  i  (cid:33)  such that Z(θ) = (cid:80)  We can marginalize over the binary hidden states in Equation 2 and re-express in terms of a new energy F (v),  =  p(v; θ) =  τ E(v, h; θ)(cid:1) is a normalizing constant and τ is the thermodynamic temperature. v,h exp(cid:0)−1 (cid:18)−1 (cid:88)  (cid:19)(cid:33)  D(cid:88)  E(v, h; θ)  (cid:32)  (cid:19)  (cid:18)  Z(θ)  exp  (3)  τ  h  log  1 + exp  cj +  viWi,j  (4)  F (v; θ) = − log  p∗(v; θ) Z(θ)  exp  E(v, h)  =  τ  h  p(v; θ) =  D(cid:88) H(cid:88) exp(cid:0) − F (v; θ)(cid:1)  vibi − 1 τ  1 τ  j=1  i  Z(θ)  i  (5)  Following physics, this form of the energy is better known as a free energy, as it expresses the difference between the average energy and the entropy of a distribution, in this case, that of p(h|v). Deﬁning the distribution in terms of free energy as p(v; θ) is convenient since it naturally copes with the presence of latent variables. The key characteristic of an RBM is the simplicity of inference due to conditional independence between visible and hidden states:  (cid:89) (cid:89)  j  p(h|v) =  p(hj|v), p(hj = 1|v) = σ(  p(v|h) =  p(vi|h), p(vi = 1|h) = σ(  (cid:88) (cid:88)  i  Wijvi + cj)  Wijhj + bi)  1https://github.com/jiwoongim/minimum probability ﬂow learning  i  j  2  Accepted as a workshop contribution at ICLR 2015  where σ(z) = 1/(1 + exp (−z)). This leads naturally to a block Gibbs sampling dynamics, used universally for sampling from RBMs. Hence, in an RBM trained by CD-k, the connectivity (NPS) is determined with probability given by k sequential block Gibbs sampling transitions. We can formalize this by writing the learning updates for CD-k as follows  ∆θCD−k ∝ −(cid:88)  (cid:88)  (cid:16) ∂Fj(θ)  j∈D  i(cid:54)∈D  ∂θ  (cid:17)  − ∂Fi(θ)  ∂θ  Tij  (6)  where Tij is the probability of transitioning from state j to state i in k steps of block Gibbs sampling. We can in principle replace Tij by any other transition operator, so long as it preserves the equilibrium distribution. Indeed, this is what alternative methods, like Persistent CD (Tieleman & Hinton, 2009), achieve.  3 MINIMUM PROBABILITY FLOW  The key intuition behind MPF is that NPS can be reformulated in a ﬁrm theoretical context by treating the model distribution as the end point of some explicit continuous dynamics, and seeking to minimize the ﬂow of probability away from the data under those dynamics. In this context then, NPS is no longer a sampling procedure employed to approximate an intractable function, but arises naturally out of the probability ﬂow from data states to non-data states. That is, MPF provides a theoretical environment for the formal treatment of Tij that offers a much more general perspective of that operator than CD-k can. In the same vein, it better formalizes the notion of minimizing divergence between positive and negative particles.  3.1 DYNAMICS OF THE MODEL  The primary mathematical apparatus for MPF is a continuous time Markov chain known as the master equation,  ˙pi =  [Γijp(t)  j − Γjip(t)  i  ]  (7)  (cid:88)  j(cid:54)=i  where j are the data states and i are the non-data states and Γij is the probability ﬂow rate from state j to state i. Note that each state is a full vector of variables, and we are theoretically enumerating all states. ˙pi is the rate of change of the probability of state i, that is, the difference between the probability ﬂowing out of any state j into state i and the probability ﬂowing out of state i to any other state j at time t. We can re-express ˙pi in a simple matrix form as  (8) . We note that if the transition matrix Γ is ergodic, then the model has a unique  ˙p = Γp  by setting Γii = −(cid:80)  i(cid:54)=j Γjip(t)  i  stationary distribution. This is a common model for exploring statistical mechanical systems, but it is unwieldly in practice for two reasons, namely, the continuous time dynamics, and exponential size of the state space. For our purposes, we will actually ﬁnd the former an advantage, and the latter irrelevant. The objective of MPF is to minimize the KL divergence between the data distribution and the distribution after evolving an inﬁnitesimal amount of time under the dynamics:  θM P F = argminθJ(θ), J(θ) = DKL(p(0)||p((cid:15))(θ))  Approximating J(θ) up to a ﬁrst order Taylor expansion with respect to time t, our objective function reduces to  Γij  (9)  (cid:88)  (cid:88)  j∈D  i(cid:54)∈D  J(θ) =  (cid:15) |D|  and θ can be optimized by gradient descent on J(θ). Since Γij captures probability ﬂow from state j to state i, this objective function has the quite elegant interpretation of minimizing the probability ﬂow from data states to non-data states (Sohl-Dickstein et al., 2011).  3  Accepted as a workshop contribution at ICLR 2015  3.2 FORM OF THE TRANSITION MATRIX  MPF does not propose to actually simulate these dynamics. There is, in fact, no need to, as the problem formulation reduces to a rather simple optimization problem with no intractable component. However, we must provide a means for computing the matrix coefﬁcients Γij. Since our target distribution is the distribution deﬁned by the RBM, we require Γ to be a function of the energy, or more particularly, the parameters of the energy function. A sufﬁcient (but not necessary) means to guarantee that the distribution p∞ (θ) is a ﬁxed point of the dynamics is to choose Γ to satisfy detailed balance, that is  Γjip(∞)  i  (θ) = Γijp(∞)  j  (θ).  (10)  The following theorem provides a general form for the transition matrix such that the equilibrium distribution is that of the RBM: Theorem 1. Suppose p(∞) matrix be  is the probability of state j and p(∞)  is the probability of state i. Let the transition  j  (11) such that o(·) is any odd function, where gij is the symmetric connectivity between the states i and j. Then this transition matrix satisﬁes detailed balance in Equation 10.  Γij = gij exp  2  (cid:18) o(Fi − Fj) + 1  i  (cid:19)  (Fj − Fi)  The proof is provided in Appendix A.1. The transition matrix proposed by (Sohl-Dickstein et al., 2011) is thus the simplest case of Theorem 1, found by setting o(·) = 0 and gij = gji:  Γij = gij exp  .  (12)  Given a form for the transition matrix, we can now evaluate the gradient of J(θ)  (cid:17)  2  (Fj(θ) − Fi(θ)  (cid:16) 1 (cid:16) ∂Fj(θ) (cid:88) (cid:88) (cid:0)Fj(θ) − Fi(θ)(cid:1)(cid:17) (cid:16) 1  i(cid:54)∈D  j∈D  ∂θ  (cid:17)  − ∂Fi(θ)  ∂θ  Tij  ∂J(θ)  ∂θ  =  (cid:15) |D|  Tij = gij exp  2  and observe the similarity to the formulation given for the RBM trained by CD-k (Equation 6). Unlike with CD-k, however, this expression was derived through an explicit dynamics and well-formalized minimization objective.  4 PROBABILITY FLOW RATES Γ At ﬁrst glance, MPF might appear doomed, due to the size of Γ, namely 2D × 2D, and the problem of enumerating all of the states. However, the objective function in Equation 9 summing over the Γij’s only considers transitions between data states j (limited in size by our data set) and non-data states i (limited by the sparseness of our design). By specifying Γ to be sparse, the intractability disappears, and complexity is dominated by the size of the dataset. Using traditional methods, an RBM can be trained in two ways, either with sampled negative particles, like in CD-k or PCD (also known as stochastic maximum likelihood) (Hinton, 2002; Tieleman & Hinton, 2009), or via an induc- tive principle, with ﬁxed sets of “fantasy cases”, like in general score matching, ratio matching, or pseudolikelihood (Hyv¨arinen, 2005; Marlin & Freitas, 2011; Besag, 1975). In a similar manner, we can deﬁne Γ by specifying the connectivity function gij either as a distribution from which to sample or as ﬁxed and deterministic. In this section, we examine various kinds of connectivity functions and their consequences on the probability ﬂow dynamics.  4.1  1-BIT FLIP CONNECTIONS  It can be shown that score matching is a special case of MPF in continuous state spaces, where the connectivity function is set to connect all states within a small Euclidean distance r in the limit of r → 0 (Sohl-Dickstein et al.,  4  Accepted as a workshop contribution at ICLR 2015  2011). For simplicity, in the case of a discrete state space (Bernoulli RBM), we can ﬁx the Hamming distance to one instead, and consider that data states are connected to all other states 1-bit ﬂip away:  if state i, j differs by single bit ﬂip  gij =  0, otherwise  (13)  (cid:26)1,  1-bit ﬂip connectivity gives us a sparse Γ with 2DD non-zero terms (rather than a full 22D), and may be seen as NPS where the only negative particles are those which are 1-bit ﬂip away from data states. Therefore, we only ever evaluate |D|D terms from this matrix, making the formulation tractable. This was the only connectivity function pursued in (Sohl-Dickstein et al., 2011) and is a natural starting point for the approach.  Algorithm 1 Minimum probability ﬂow learning with single bit-ﬂip connectivity. Note we leave out all gij since here we are explicit about only connecting states of Hamming distance 1.  • Initialize the parameters θ • for each training example d ∈ D do  1. Compute the list of states, L, with Hamming distance 1 from d 2. Compute the probability ﬂow Γid = exp ( 1  3. The cost function for d is(cid:80)  2 (Fd(θ) − Fi(θ)) for each i ∈ L  i∈L Γid  ∂θ =(cid:80)  (cid:16) ∂Fd(θ) ∂θ − ∂Fi(θ)  ∂θ  (cid:17)  Γid  4. Compute the gradient of the cost function, ∂J(θ) 5. Update parameters via gradient descent with θ ← θ − λ∇J(θ) end for  i∈L  4.2 FACTORIZED MINIMUM PROBABILITY FLOW  to use a probability distribution, such that gij is the probability that state j is connected to state i (i.e.(cid:80)  Previously, we considered connectivity gij as a binary indicator function of both states i and j. Instead, we may wish i gij = 1). Following (Sohl-Dickstein, 2011), we simplify this approach by letting gij = gi, yielding an independence chain (Tierney, 1994). This means the probability of being connected to state i is independent of j, giving us an alternative way of constructing a transition matrix such that the objective function can be factorized:  J(θ) =  gi  1 |D|  (cid:88) (cid:88)  1 (cid:88)  j∈D  i(cid:54)∈D  |D|  j∈D  2  (cid:18) 1  (cid:19) 1 (cid:18) gj (cid:18) 1 (cid:0)Fj(x; θ) + log gj  (cid:0)Fj(x; θ) − Fi(x; θ)(cid:1)(cid:19) (cid:1)(cid:19)(cid:88)  gi exp  exp  gi  2  i(cid:54)∈D  exp  2  =  (cid:16) gj  (cid:17) 1  (cid:18) 1  2  (cid:0) − Fi(x; θ) + log gi  (cid:1)(cid:19)  (14)  (15)  gi  2 is a scaling term required to counterbalance the difference between gi and gj. The independence in where the connectivity function allows us to factor all the j terms in 14 out of the inner sum, leaving us with a product of sums, something we could not achieve with 1-bit ﬂip connectivity since the connection to state i depends on it being a neighbor of state j. Note that, intuitively, learning is facilitated by connecting data states to states that are probable under the model (i.e. to contrast the divergence). Therefore, we can use p(v; θ) to approximate gi. In practice, for each iteration n of learning, we need the gi and gj terms to act as constants with respect to updating θ, and thus we sample them from p(v; θn−1). We can then rewrite the objective function as J(θ) = JD(θ)JS (θ)  (cid:32)  1 |D|  (cid:88)  (cid:20) 1  (cid:0)F (x; θ) − F (x; θn−1)(cid:1)(cid:21)(cid:33)  (cid:32)  1 |S|  (cid:88)  (cid:20) 1  (cid:0) − F (x(cid:48); θ) + F (x(cid:48); θn−1)(cid:1)(cid:21)(cid:33)  exp  JD(θ) = where S is the sampled set from p(v; θn−1), and the normalization terms in log gj and log gi cancel out. Note we use the θn−1 notation to refer to the parameters at the previous iteration, and simply θ for the current iteration.  ; JS (θ) =  x(cid:48)∈S  x∈D  exp  2  2  4.3 PERSISTENT MINIMUM PROBABILITY FLOW There are several ways of sampling “fantasy particles” from p(v; θn−1). Notice that taking the data distribution with respect to θn−1 is necessary for stable learning.  5  Accepted as a workshop contribution at ICLR 2015  Previously, persistent contrastive divergence (PCD) was developed to improve CD-k learning (Tieleman & Hinton, 2009). Similarly, persistence can be applied to sampling in MPF connectivity functions. For each update, we pick a new sample based on a MCMC sampler which starts from previous samples. Then we update θn, which satsiﬁes J(θn) ≤ J(θn−1) (Sohl-Dickstein, 2011). The pseudo-code for persistent MPF is the same as Factored MPF except for drawing new samples, which is indicated by square brackets in Algorithm 2. As we will show, using persistence in MPF is important for achieving faster convergence in learning. While the theoretical formulation of MPF guarantees eventual convergence, the focus on minimizing the initial probability ﬂow will have little effect if the sampler mixes too slowly. In practice, combining the persistent samples and non-persistent samples gave better performance.  Algorithm 2 Factored [Persistent] MPF learning with probabilistic connectivity.  • for each epoch n do  1. Draw a new sample Sn based on S0(cid:2)Sn−1(cid:3) using an MCMC sampler.  2. Compute JS (θ) 3. for each training example d ∈ D do  (a) Compute Jd(θ). The cost function for d is J(θ) = Jd(θ)JS (θ) (b) Compute the gradient of the cost function, x(cid:48)∈S  ∂θ = JS (θ)Jd(θ) ∂Fd(θ)  (c) Update parameters via gradient descent with θ ← θ − λ∇J(θ)  ∂θ + 1|S| Jd  exp(cid:2) 1  (cid:16) ∂F (x(cid:48))  (cid:80)  ∂J(θ)  ∂θ  2  (cid:0)F (x(cid:48); θ) − F (x(cid:48); θn−1)(cid:1)(cid:3)(cid:17)  end for  5 EXPERIMENTS  We conducted the ﬁrst empirical study of MPF under different types of connectivity as discussed in Section 4. We compared our results to CD-k with varying values for K. We analyzed the MPF variants based on training RBMs and assessed them quantitatively and qualitatively by comparing the log-liklihoods of the test data and samples generated from model. For the experiments, we denote the 1-bit ﬂip, factorized, and persistent methods as MPF- 1ﬂip, FMPF, and PMPF, respectively. The goals of these experiments are to  1. Compare the performance between MPF algorithms under different connectivities; and 2. Compare the performance between MPF and CD-k.  In our experiments, we considered the MNIST and CalTech Silhouette datasets. MNIST consists of 60,000 training and 10,000 test images of size 28 × 28 pixels containing handwritten digits from the classes 0 to 9. The pixels in MNIST are binarized based on thresholding. From the 60,000 training examples, we set aside 10,000 as validation examples to tune the hyperparameters in our models. The CalTech Silhouette dataset contains the outlines of objects from the CalTech101 dataset, which are centered and scaled on a 28 × 28 image plane and rendered as ﬁlled black regions on a white background creating a silhouette of each object. The training set consists of 4,100 examples, with at least 20 and at most 100 examples in each category. The remaining instances were split evenly between validation and testing2. Hyperparameters such as learning rate, number of epochs, and batch size were selected from discrete ranges and chosen based on a held-out validation set. The learning rate for FMPF and PMPF were chosen from the range [0.001, 0.00001] and the learning rate for 1-bit ﬂip was chosen from the range [0.2, 0.001].  5.1 MNIST - EXACT LOG LIKELIHOOD  In our ﬁrst experiment, we trained eleven RBMs on the MNIST digits. All RBMs consisted of 20 hidden units and 784 (28×28) visible units. Due to the small number of hidden variables, we calculated the exact value of the partition function by explicitly summing over all visible conﬁgurations. Five RBMs were learned by PCD1, CD1, CD10, CD15, and CD25. Seven RBMs were learned by 1 bit ﬂip, FMPF, and FPMPF3. Block Gibbs sampling is required for FMPF-k and FPMPF-k similar to CD-k training, where the number of steps is given by k.  2More details on pre-processing the CalTech Silhouettes can be found in http://people.cs.umass.edu/ marlin/data.shtml 3FPMPF is the composition of the FMPF and PMPF connectivities.  6  Accepted as a workshop contribution at ICLR 2015  Figure 1: Samples generated from the training set. Samples in each panel are generated by RBMs trained under different paradigms as noted above each image.  Table 1: Experimental results on MNIST using 11 RBMs with 20 hidden units each. The average training and test log-probabilities over 10 repeated runs with random parameter initializations are reported.  Method CD1 PCD MPF-1ﬂip CD10 FMPF10 PMPF10 FPMPF10 CD15 FMPF15 PMPF15 FPMPF15 CD25 FMPF25 PMPF25 FPMPF25  Average log Test Average log Train -145.63 ± 1.30 -146.62 ± 1.72 -136.10 ± 1.21 -137.13 ± 1.21 -143.02 ± 3.96 -141.13 ± 2.01 -136.46 ± 1.18 -135.40 ± 1.21 -136.37 ± 0.17 -137.35 ± 0.19 -141.36 ± 0.35 -142.73 ± 0.35 -134.04 ± 0.12 -135.25 ± 0.11 -135.20 ± 0.84 -134.13 ± 0.82 -136.93 ± 0.18 -135.89 ± 0.19 -138.53 ± 0.23 -139.71 ± 0.23 -133.90 ± 0.14 -135.13 ± 0.14 -134.15 ± 0.08 -133.02 ± 0.08 -135.63 ± 0.07 -134.50 ± 0.08 -135.95 ± 0.13 -137.29 ± 0.13 -132.74 ± 0.13 -133.50 ± 0.11  Time (sec) Batchsize 100 300 75 100 60 25 25 100 60 25 25 100 60 25 25  831 2620 2931 17329 12533 11445 22201 26723 18951 13441 27302 46711 25588 23115 50117  The average log test likelihood values of RBMs with 20 hidden units are presented in Table 1. This table gives a sense of the performance under different types of MPF dynamics when the partition function can be calculated exactly. We observed that PMPF consistently achieved a higher log-likelihood than FMPF. MPF with 1 bit ﬂip was very fast but gave poor performance compared to FMPF and PMPF. We also observed that MPF-1ﬂip outperformed CD1. FMPF always performed slightly worse than CD-k training with the same number of Gibbs steps. However, PMPF always outperformed CD-k. One advantage of FMPF is that it converges much quicker than CD-k or PMPF. This is because we used twice many samples as PMPF as mentioned in Section 4.3. Figure 1 shows initial data and the generated samples after running 100 Gibbs steps from each RBM. PMPF produces samples that are visually more appealing than the other methods.  5.2 MNIST - ESTIMATING LOG LIKELIHOOD  In our second set of experiments, we trained RBMs with 200 hidden units. We trained them exactly as described in Section 5.1. These RBMs are able to generate much higher-quality samples from the data distribution, however, the partition function can no longer be computed exactly. In order to evaluate the model quantitatively, we estimated the test log-likelihood using the Conservative Sampling- based Likelihood estimator (CSL) (Bengio et al., 2013) and annealed importance sampling (AIS) (Salakhutdinov & Murray, 2008). Given well-deﬁned conditional probabilities P (v|h) of a model and a set of latent variable samples S collected from a Markov chain, CSL computes  log ˆf (v) = log meanh∈SP (v|h).  (16)  7  DATAPCDMPF-1flipCD10FMPF10PMPF10CD15FMPF15PMPF15CD25FMPF25PMPF25Accepted as a workshop contribution at ICLR 2015  Figure 2: Samples generated from the training set. Samples in each panel are generated by RBMs trained under different paradigms as noted above each image.  Table 2: Experimental results on MNIST using 11 RBMs with 200 hidden units each. The average estimated train- ing and test log-probabilities over 10 repeated runs with random parameter initializations are reported. Likelihood estimates are made with CSL (Bengio et al., 2013) and AIS (Salakhutdinov & Murray, 2008)  Method CD1 PCD1 MPF-1ﬂip CD10 FMPF10 PMPF10 FPMPF10 CD15 FMPF15 PMPF15 FPMPF15 CD25 FMPF25 PMPF25 FPMPF25  CSL  AIS  Avg. log Test -138.63 ± 0.48 -114.14 ± 0.26 -179.73 ± 0.085 -117.74 ± 0.14 -115.11 ± 0.09 -114.00 ± 0.08 -112.45 ± 0.03 -115.96 ± 0.12 -114.05 ± 0.05 -114.02 ± 0.11 -112.58 ± 0.03 -114.50 ± 0.10 -113.07 ± 0.06 -113.70 ± 0.04 -112.38 ± 0.02  Avg. log Train -138.70 ± 0.45 -114.13 ± 0.28 -179.60 ± 0.07 -117.76 ± 0.13 -115.10 ± 0.07 -113.98 ± 0.09 -112.45 ± 0.03 -115.21 ± 0.12 -114.06 ± 0.05 -114.03 ± 0.09 -112.60 ± 0.02 -114.51 ± 0.10 -113.07 ± 0.07 -113.69 ± 0.04 -112.42 ± 0.02  Avg. log Test -98.75 ± 0.66 -88.82 ± 0.53 -141.95 ± 0.23 -91.94 ± 0.42 -91.21 ± 0.17 -89.26 ± 0.13 -83.83 ± 0.23 -91.32 ± 0.24 -90.72 ± 0.18 -89.25 ± 0.17 -83.27 ± 0.15 -91.36 ± 0.26 -90.43 ± 0.28 -89.21 ± 0.14 -83.25 ± 0.27  Avg. log Train -98.61 ± 0.66 -89.92 ± 0.54 -142.38 ± 0.74 -92.46 ± 0.38 -91.39 ± 0.16 -89.37 ± 0.13 -83.26 ± 0.23 -91.87 ± 0.21 -90.93 ± 0.20 -89.85 ± 0.19 -83.84 ± 0.13 -91.04 ± 0.25 -90.63 ± 0.27 -89.79 ± 0.13 -83.81 ± 0.28  Time (sec) Batchsize 100 100 75 100 25 25 25 100 25 25 25 100 25 25 25  1258 2614 4575 24948 24849 24179 24354 39003 26059 26272 26900 55688 40047 52638 53379  The advantage of CSL is that sampling latent variables h instead of v has the effect of reducing the variance of the estimator. Also, in contrast to annealed importance sampling (AIS) (Salakhutdinov & Murray, 2008), which tends to overestimate, CSL is much more conservative in its estimates. However, most of the time, CSL is far off from the true estimator, so we bound our negative log-likelihood estimate from above and below using both AIS and CSL. Table 2 demonstrates the test log-likelihood of various RBMs with 200 hidden units. The ranking of the differ- ent training paradigms with respect to performance was similar to what we observed in Section 5.1 with PMPF emerging as the winner. However, contrary to the ﬁrst experiment, we observed that MPF with 1 bit ﬂip did not perform well. Moreover, FMPF and PMPF both tended to give higher test log-likelihoods than CD-k training. Smaller batch sizes worked better with MPF when the number of hiddens was increased. Once again, we observed smaller variances compared to CD-k with both forms of MPF, especially with FMPF. We noted that FMPF and PMPF always have smaller variance compared to CD-k. This implies that FMPF and PMPF are less sensitive to random weight initialization. Figure 2 shows initial data and generated samples after running 100 Gibbs steps for each RBM. PMPF clearly produces samples that look more like digits.  5.3 CALTECH 101 SILHOUETTES - ESTIMATING LOG LIKELIHOOD  Finally, we evaluated the same set of RBMs on the Caltech-101 Silhouettes dataset. Compared to MNIST, this dataset contains much more diverse structures with richer correlation among the pixels. It has 10 times more  8  DATAPCDMPF-1flipCD10FMPF10PMPF10CD15FMPF15PMPF15CD25FMPF25PMPF25Accepted as a workshop contribution at ICLR 2015  Figure 3: Random samples generated by RBMs with different training procedures.  Table 3: Experimental results on Caltech-101 Silhouettes using 11 RBMs with 500 hidden units each. The aver- age estimated training and test log-probabilities over 10 repeated runs with random parameter initializations are reported. Likelihood estimates are made with CSL (Bengio et al., 2013) and AIS (Salakhutdinov & Murray, 2008).  Method CD1 PCD1 MPF-1ﬂip CD10 FMPF10 PMPF10 FPMPF10 CD15 FMPF15 PMPF15 FPMPF15 CD25 FMPF25 PMPF25 FPMPF25  CSL  Avg. log Test -251.30 ± 1.80 -199.89 ± 1.53 -281.55 ± 1.68 -207.77 ± 0.92 -211.30 ± 0.84 -203.13 ± 0.12 -200.36 ± 0.16 -205.12 ± 0.87 -210.66 ± 0.24 -201.47 ± 0.13 -198.59 ± 0.17 -201.56 ± 0.11 -206.93 ± 0.13 -199.53 ± 0.11 -198.39 ± 0.0.16  Avg. log Train -252.04 ± 1.56 -199.95 ± 1.31 -283.03 ± 0.60 -207.16 ± 1.18 -211.39 ± 0.90 -203.14 ± 0.10 -200.16 ± 0.16 -204.87 ± 1.13 -210.19 ± 0.30 -201.67 ± 0.10 -198.66 ± 0.17 -201.50 ± 0.13 -206.86 ± 0.11 -199.51 ± 0.12 -198.39 ± 0.17  AIS  Avg. log Test -141.87 ± 8.80 -124.56 ± 0.24 -164.96 ± 0.23 -128.17 ± 0.20 -135.59 ± 0.16 -128.85 ± 0.15 -123.35 ± 0.16 -125.08 ± 0.24 -130.28 ± 0.14 -127.09 ± 0.10 -122.33 ± 0.13 -124.80 ± 0.20 -129.96 ± 0.07 -127.81 ± 020 -122.75 ± 0.13  Avg. log Train -142.88 ± 8.85 -116.56 ± 2.40 -170.92 ± 0.20 -120.65 ± 0.19 -135.57 ± 0.18 -123.06 ± 0.15 -108.81 ± 0.15 -117.09 ± 0.21 -128.57 ± 0.15 -121 ± 0.12 -107.88 ± 0.14 -117.51 ± 0.23 -127.15 ± 0.07 -122.23 ± 0.17 -108.32 ± 0.12  Time (sec) Batchsize 100 100 100 100 20 20 20 100 20 20 20 100 10 10 10  300 784 505 4223 2698 7610 11973 6611 3297 9603 18170 13745 10542 18550 23998  categories, contains less training data per category, and each object covers more of the image. For these reasons, we use 500 hidden units per RBM. The estimated average log-likelihood of train and test data is presented in Table 3. The results for Caltech 101 Silhouettes are consistent with MNIST. In every case, we observed a larger margin between PMPF and CD-k when the number of sampling steps was smaller. In addition, the single bit ﬂip technique was not particularly successful, especially as the number of latent variables grew. We speculate that the reason for this might have to do with the slow rate of convergence for the dynamic system. Moreover, PMPF works better than FMPF for similar reasons. By having persistent samples as the learning progresses, the dynamics always begin closer to equilibrium, and hence converge more quickly. Figure 3 shows initial data and generated samples after running 100 Gibbs steps for each RBM on Caltech28 dataset.  6 CONCLUSION  MPF is an unsupervised learning algorithm that can be employed off-the-shelf to any energy-based model. It has a number of favorable properties but has not seen application proportional to its potential. In this paper, we ﬁrst expounded on MPF and its connections to CD-k training, which allowed us to gain a better understanding and perspective to CD-k. We proved a general form for the transition matrix such that the equilibrium distribution converges to that of an RBM. This may lead to future extensions of MPF based on the choice of o(·) in Equation 11.  9  DATAPCD1MPF 1-bit flipCD10FMPF10PMPF10CD15FMPF15PMPF15CD25FMPF25PMPF25Accepted as a workshop contribution at ICLR 2015  One of the merits of MPF is that the choice of designing a dynamic system by deﬁning a connectivity function is left open as long as it satisﬁes the ﬁxed point equation. We thoroughly explored three different connectivity structures, noting that connectivity can be designed inductively or by sampling. Finally, we showed empirically that MPF, and in particular, PMPF, outperforms CD-k for training generative models. Until now, RBM training was dominated by methods based on CD-k; however, our results indicate that MPF is a practical and effective alternative.  10  Accepted as a workshop contribution at ICLR 2015  REFERENCES Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Bengio, Yoshua, Yao, Li, and Cho, Kyunghyun. Bounding the test log-likelihood of generative models. In Pro-  ceedings of the International Conference of Learning Representations (ICLR), 2013.  Besag, Julian. Statistical analysis of non-lattice data. The Statistician, 24:179–195, 1975.  Hinton, Geoffrey. E. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:  1771–1880, 2002.  Hyv¨arinen, Aapo. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning  Research, 6:695–709, 2005.  MacKay, David J. C. Failures of the one-step learning algorithm, 2001. URL http://www.inference.phy.  cam.ac.uk/mackay/abstracts/gbm.html. Unpublished Technical Report.  Marlin, Benjamin M. and Freitas, Nando de. Asymptotic efﬁciency of deterministic estimators for discrete energy- based models: Ratio matching and pseudolikelihood. In Proceedings of the Uncertainty in Artiﬁcial Intelligence (UAI), 2011.  Salakhutdinov, Ruslan and Murray, Iain. On the quantitative analysis of deep belief networks. In Proceedings of  the International Conference of Machine Learning (ICML), 2008.  Smolensky, Paul.  Information processing in dynamical systems: Foundations of harmony theory.  Distributed Processing: Volume 1: Foundations, pp. 194–281. MIT Press, 1986.  In Parallel  Sohl-Dickstein, Jascha. Persistent minimum probability ﬂow. Technical report, Redwood Centre for Theoretical  Neuroscience, 2011.  Sohl-Dickstein, Jascha, Battaglino, Peter, and DeWeese, Michael R. Minimum probability ﬂow learning.  Proceedings of the International Conference of Machine Learning (ICML), 2011.  In  Sutskever, Ilya and Tieleman, Tijmen. On the convergence properties of contrastive divergence. In Proceedings of  the AI & Statistics (AI STAT), 2009.  Tieleman, Tijmen and Hinton, Geoffrey E. Using fast weights to improve persistent contrastive divergence. In  Proceedings of the International Conference of Machine Learning (ICML), 2009.  Tierney, Luke. Markov chains for exploring posterior distributions. Annals of Statistics, 22:1701–1762, 1994.  11  Accepted as a workshop contribution at ICLR 2015  A MINIMUM PROBABILITY FLOW  A.1 DYNAMICS OF THE MODEL Theorem 1. Suppose p(∞) matrix be  j  is the probability of state j and p(∞)  (cid:18) o(Fi − Fj) + 1  i  (cid:19)  (Fj − Fi)  (17) such that o(·) is any odd function, where gij is the symmetric connectivity between the states i and j. Then this transition matrix satisﬁes detailed balance in Equation 18.  Γij = gij exp  2  is the probability of state i. Let the transition  Proof. By cancalling out the partition function, the detailed balance Equation 18 can be formulated to be  (18) where Fi = F (v = i; θ) We substitute transition matrix deﬁned in Equation 11, then we get the following after straight forward formula manipulation.  Γji exp (−Fi) = Γij exp (−Fj)  (cid:19)  (cid:18) o(Fj − Fi) + 1  Γji exp (−Fi)/Γij exp (−Fj)) = 1  (cid:18) o(Fi − Fj) + 1 (cid:18) o(Fi − Fj) + 1  2  exp  exp 2 o(Fi − Fj) + 1  2 (Fi − Fj)  (cid:18) o(Fi − Fj) + 1 (cid:18) o(Fi − Fj)  2  2  / exp  (Fj − Fi) − Fi (Fj − Fi) − Fi − o(Fj − Fi) + 1 (cid:19)  (Fj − Fi) − Fi − o(Fj − Fi) + 1 − 1  o(Fj − Fi) + 1  2  2  = 0  + o(Fj − Fi)  (cid:19)  2  (Fi − Fj) + Fj = 0  (cid:19)  (Fi − Fj) − Fj  = 1  (cid:19)  (Fi − Fj) + Fj  = 1  (Fi − Fj)  Notice that since o(·) is an odd function, this makes the term(cid:0) o(Fi−Fj )  = 0  +  2  2  2  (cid:1) = 0. Therefore, the detailed  + o(Fj−Fi)  2  balance criterion is satisﬁed.  12  ",
1504.02462,2015, A Group Theoretic Perspective on Unsupervised Deep Learning,"['A Group Theoretic Perspective on Unsupervised Deep Learning', 'Arnab Paul and Suresh Venkatasubramanian']",https://arxiv.org/pdf/1504.02462,"5 1 0 2    r p A 1 2         ]  G L . s c [      3 v 2 6 4 2 0  .  4 0 5 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  A GROUP THEORETIC PERSPECTIVE ON UNSUPER- VISED DEEP LEARNING∗  Arnab Paul Intel Corporation arnab.paul@intel.com  Suresh Venkatasubramanian School of Computing, University of Utah suresh@cs.utah.edu  EXTENDED ABSTRACT  The modern incarnation of neural networks, now popularly known as Deep Learning (DL), accom- plished record-breaking success in processing diverse kinds of signals - vision, audio, and text. In parallel, strong interest has ensued towards constructing a theory of DL. This paper opens up a group theory based approach, towards a theoretical understanding of DL, in particular the unsuper- vised variant. First we establish how a single layer of unsupervised pre-training can be explained in the light of orbit-stabilizer principle, and then we sketch how the same principle can be extended for multiple layers. We focus on two key principles that (amongst others) inﬂuenced the modern DL resurgence.  (P1) Geoff Hinton summed this up as follows. “In order to do computer vision, ﬁrst learn how In other words, if a network learns a good  to do computer graphics”. Hinton (2007). generative model of its training set, then it could use the same model for classiﬁcation.  (P2) Instead of learning an entire network all at once, learn it one layer at a time.  In each round, the training layer is connected to a temporary output layer and trained to learn the weights needed to reproduce its input (i.e to solve P1). This step – executed layer-wise, starting with the ﬁrst hidden layer and sequentially moving deeper – is often referred to as pre-training (see Hinton et al. (2006); Hinton (2007); Salakhutdinov & Hinton (2009); Bengio et al. (in preparation)) and the resulting layer is called an autoencoder. Figure 1(a) shows a schematic autoencoder. Its weight set W1 is learnt by the network. Subsequently when presented with an input f , the network will produce an output f (cid:48) ≈ f . At this point the output units as well as the weight set W2 are discarded. There is an alternate characterization of P1. An autoencoder unit, such as the above, maps an input space to itself. Moreover, after learning, it is by deﬁnition, a stabilizer1 of the input f . Now, input signals are often decomposable into features, and an autoencoder attempts to ﬁnd a succinct set of features that all inputs can be decomposed into. Satisfying P1means that the learned conﬁgurations can reproduce these features. Figure 1(b) illustrates this post-training behavior. If the hidden units learned features f1, f2, . . ., and one of then, say fi, comes back as input, the output must be fi. In other words learning a feature is equivalent to searching for a transformation that stabilizes it. The idea of stabilizers invites an analogy reminiscent of the orbit-stabilizer relationship studied in the theory of group actions. Suppose G is a group that acts on a set X by moving its points around (e.g groups of 2× 2 invertible matrices acting over the Euclidean plane). Consider x ∈ X, and let Ox be the set of all points reachable from x via the group action. Ox is called an orbit2. A subset of the group elements may leave x unchanged. This subset Sx (which is also a subgroup), is the stabilizer of x. If it is possible to deﬁne a notion of volume for a group, then there is an inverse relationship  ∗This research supported in part by the NSF under grant BIGDATA-1251049 1 A transformation T is called a stabilizer of an input f , if f (cid:48) = T ( f ) = f . 2The orbit Ox of an element x ∈ X under the action of a group G, is deﬁned as the set Ox = {g(x) ∈ X|g ∈ G}.  1  Accepted as a workshop contribution at ICLR 2015  (a) General encoder schematic  auto-  (b) post-learning behavior of an auto- encoder  (c) Alternate Decomposition of a Signal  (a) W1 is preserved, W2 discarded  Figure 1: (b) Post-learning, each feature is stabilized (c)Alternate ways of decomposing a signal into simpler features. The neurons could potentially learn features in the top row, or the bottom row. Almost surely, the simpler ones (bottom row) are learned.  between the volumes of Sx and Ox, which holds even if x is actually a subset (as opposed to being a point). For example, for ﬁnite groups, the product of |Ox| and |Sx| is the order of the group. The inverse relationship between the volumes of orbits and stabilizers takes on a central role as we connect this back to DL. There are many possible ways to decompose signals into smaller features. Figure 1(c) illustrates this point: a rectangle can be decomposed into L-shaped features or straight- line edges. All experiments to date suggest that a neural network is likely to learn the edges. But why? To answer this, imagine that the space of the autoencoders (viewed as transformations of the input) form a group. A batch of learning iterations stops whenever a stabilizer is found. Roughly speaking, if the search is a Markov chain (or a guided chain such as MCMC), then the bigger a stabilizer, the earlier it will be hit. The group structure implies that this big stabilizer corresponds to a small orbit. Now intuition suggests that the simpler a feature, the smaller is its orbit. For example, a line-segment generates many fewer possible shapes under linear deformations than a ﬂower-like shape. An autoencoder then should learn these simpler features ﬁrst, which falls in line with most experiments (see Lee et al. (2009)). The intuition naturally extends to a many-layer scenario. Each hidden layer ﬁnding a feature with a big stabilizer. But beyond the ﬁrst level, the inputs no longer inhabit the same space as the training samples. A “simple” feature over this new space actually corresponds to a more complex shape in the space of input samples. This process repeats as the number of layers increases. In effect, each layer learns “edge-like features” with respect to the previous layer, and from these locally simple representations we obtain the learned higher-order representation.  REFERENCES Bengio, Yoshua, Goodfellow, Ian, and Courville, Aaron. Deep learning. In Deep Learning. MIT  Press, in preparation. URL http://www.iro.umontreal.ca/~bengioy/dlbook/.  Hinton, Geoffrey E. To recognize shapes, ﬁrst learn to generate images. Progress in brain research,  165:535–547, 2007.  Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye. A fast learning algorithm for deep belief  nets. Neural Computation, 18:1527–1554, 2006.  Lee, Honglak, Grosse, Roger, Ranganath, Rajesh, and Ng, Andrew Y. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 609–616. ACM, 2009.  Salakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltzmann machines. In International Con-  ference on Artiﬁcial Intelligence and Statistics, pp. 448–455, 2009.  2  ",
1412.7753,2015, Learning Longer Memory in Recurrent Neural Networks,"['Learning Longer Memory in Recurrent Neural Networks', 'Tomas Mikolov', 'Armand Joulin', 'Sumit Chopra', 'Michael Mathieu', ""and Marc'Aurelio Ranzato""]",https://arxiv.org/pdf/1412.7753,"5 1 0 2    r p A 6 1         ] E N . s c [      2 v 3 5 7 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  LEARNING LONGER MEMORY IN RECURRENT NEURAL NETWORKS  Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu & Marc’Aurelio Ranzato Facebook Artiﬁcial Intelligence Research 770 Broadway New York City, NY, 10003, USA {tmikolov,ajoulin,spchopra,myrhev,ranzato}@fb.com  ABSTRACT  Recurrent neural network is a powerful model that learns temporal patterns in se- quential data. For a long time, it was believed that recurrent networks are difﬁcult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modiﬁcation of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming a kind of longer term memory. We evaluate our model on language modeling tasks on benchmark datasets, where we obtain sim- ilar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997).  1  INTRODUCTION  Models of sequential data, such as natural language, speech and video, are the core of many machine learning applications. This has been widely studied in the past with approaches taking their roots in a variety of ﬁelds (Goodman, 2001b; Young et al., 1997; Koehn et al., 2007). In particular, models based on neural networks have been very successful recently, obtaining state-of-the-art performances in automatic speech recognition (Dahl et al., 2012), language modeling (Mikolov, 2012) and video classiﬁcation (Simonyan & Zisserman, 2014). These models are mostly based on two families of neural networks: feedforward neural networks and recurrent neural networks. Feedforward architectures such as time-delayed neural networks usually represent time explicitly with a ﬁxed-length window of the recent history (Rumelhart et al., 1985). While this type of models work well in practice, ﬁxing the window size makes long-term dependency harder to learn and can only be done at the cost of a linear increase of the number of parameters. The recurrent architectures, on the other hand, represent time recursively. For example, in the simple recurrent network (SRN) (Elman, 1990), the state of the hidden layer at a given time is conditioned on its previous state. This recursion allows the model to store complex signals for arbitrarily long time periods, as the state of the hidden layer can be seen as the memory of the model. In theory, this architecture could even encode a “perfect” memory by simply copying the state of the hidden layer over time. While theoretically powerful, these recurrent models were widely considered to be hard to train due to the so-called vanishing and exploding gradient problems (Hochreiter, 1998; Bengio et al., 1994). Mikolov (2012) showed how to avoid the exploding gradient problem by using simple, yet efﬁcient strategy of gradient clipping. This allowed to efﬁciently train these models on large datasets by using only simple tools such as stochastic gradient descent and back-propagation through time (Williams & Zipser, 1995; Werbos, 1988). Nevertheless, simple recurrent networks still suffer from the vanishing gradient problem: as gra- dients are propagated back through time, their magnitude will almost always exponentially shrink. This makes memory of the SRNs focused only on short term patterns, practically ignoring longer  1  Accepted as a workshop contribution at ICLR 2015  term dependencies. There are two reasons why this happens. First, standard nonlinearities such as the sigmoid function have a gradient which is close to zero almost everywhere. This problem has been partially solved in deep networks by using the rectiﬁed linear units (ReLu) (Nair & Hinton, 2010). Second, as the gradient is backpropagated through time, its magnitude is multiplied over and over by the recurrent matrix. If the eigenvalues of this matrix are small (i.e., less than one), the gradient will converge to zero rapidly. Empirically, gradients are usually close to zero after 5 - 10 steps of backpropagation. This makes it hard for simple recurrent neural networks to learn any long term patterns. Many architectures were proposed to deal with the vanishing gradients. Among those, the long short term memory (LSTM) recurrent neural network (Hochreiter & Schmidhuber, 1997) is a modiﬁed version of simple recurrent network which has obtained promising results on hand writing recog- nition (Graves & Schmidhuber, 2009) and phoneme classiﬁcation (Graves & Schmidhuber, 2005). LSTM relies on a fairly sophisticated structure made of gates which control ﬂow of information to hidden neurons. This allows the network to potentially remember information for longer periods. Another interesting direction which was considered is to exploit the structure of the Hessian matrix with respect to the parameters to avoid vanishing gradients. This can be achieved by using second- order methods designed for non-convex objective functions (see section 7 in LeCun et al. (1998)). Unfortunately, there is no clear theoretical justiﬁcation why using the Hessian matrix would help, nor there is, to the best of our knowledge, any conclusive thorough empirical study on this topic. In this paper, we propose a simple modiﬁcation of the SRN to partially solve the vanishing gradient problem. In Section 2, we demonstrate that by simply constraining a part of the recurrent matrix to be close to identity, we can drive some hidden units, called context units to behave like a cache model which can capture long term information similar to the topic of a text (Kuhn & De Mori, 1990). In Section 3, we show that our model can obtain competitive performance compared to the state-of-the-art sequence prediction model, LSTM, on language modeling datasets.  2 MODEL  2.1 SIMPLE RECURRENT NETWORK  (a)  (b)  Figure 1: (a) Simple recurrent network. (b) Recurrent network with context features.  We consider sequential data that comes in the form of discrete tokens, such as characters or words. We assume a ﬁxed dictionary containing d tokens. Our goal is to design a model which is able to predict the next token in the sequence given its past. In this section, we describe the simple recurrent network (SRN) model popularized by Elman (1990), and which is the cornerstone of this work. A SRN consists of an input layer, a hidden layer with a recurrent connection and an output layer (see Figure 1-a). The recurrent connection allows the propagation through time of information about the state of the hidden layer. Given a sequence of tokens, a SRN takes as input the one-hot encoding xt of the current token and predicts the probability yt of next one. Between the current token representation and the prediction, there is a hidden layer with m units which store additional information about the previous tokens seen in the sequence. More precisely, at each time t, the state  2  xthtytARUxthtytstURAB↵VPAccepted as a workshop contribution at ICLR 2015  of the hidden layer ht is updated based on its previous state ht−1 and the encoding xt of the current token, according to the following equation:  ht = σ (Axt + Rht−1) ,  (1) where σ(x) = 1/(1 + exp(x)) is the sigmoid function applied coordinate wise, A is the d × m token embedding matrix and R is the m × m matrix of recurrent weights. Given the state of these hidden units, the network then outputs the probability vector yt of the next token, according to the following equation:  yt = f (U ht) ,  (2) where f is the soft-max function and U is the m × d output matrix. In some cases, the size d of the dictionary can be signiﬁcant (e.g., more than 100K tokens for standard language modeling tasks) and computing the normalization term of the soft-max function is often the bottle-neck of this type √ of architecture. A common trick introduced in Goodman (2001a) is to replace the soft-max function by a hierarchical soft-max. We use a simple hierarchy with two levels, by binning the tokens into d √ clusters with same cumulative word frequency (Mikolov et al., 2011). This reduces the complexity of computing the soft-max from O(hd) to about O(h d), but at the cost of lower performance (around 10% loss in perplexity). We will mention explicitly when we use this approximation in the experiments. The model is trained by using stochastic gradient descent method with back-propagation through time (Rumelhart et al., 1985; Williams & Zipser, 1995; Werbos, 1988). We use gradient renormal- ization to avoid gradient explosion. In practice, this strategy is equivalent to gradient clipping since gradient explosions happen very rarely when reasonable hyper-parameters are used. The details of the implementation are given in the experiment section. It is generally believed that using a strong nonlinearity is necessary to capture complex patterns appearing in real-world data. In particular, the class of mapping that a neural network can learn between the input space and the output space depends directly on these nonlinearities (along with the number of hidden layers and their sizes). However, these nonlinearities also introduce the so- called vanishing gradient problem in recurrent networks. The vanishing gradient problem states that as the gradients get propagated back in time, their magnitude quickly shrinks close to zero. This makes learning longer term patterns difﬁcult, resulting in models which fail to capture the surrounding context. In the next section, we propose a simple extension of SRN to circumvent this problem, yielding a model that can retain information about longer context.  2.2 CONTEXT FEATURES  In this section, we propose an extension of SRN by adding a hidden layer speciﬁcally designed to capture longer term dependencies. We design this layer following two observations: (1) the nonlin- earity can cause gradients to vanish, (2) a fully connected hidden layer changes its state completely at every time step. SRN uses a fully connected recurrent matrix which allows complex patterns to be propagated through time but suffers from the fact that the state of the hidden units changes rapidly at every time step. On the other hand, using a recurrent matrix equal to identity and removing the nonlinear- ity would keep the state of the hidden layer constant, and every change of the state would have to come from external inputs. This should allow to retain information for longer period of time. More precisely, the rule would be:  st = st−1 + Bxt,  (3) where B is the d × s context embedding matrix. This solution leads to a model which cannot be trained efﬁciently. Indeed, the gradient of the recurrent matrix would never vanish, which would require propagation of the gradients up to the beginning of the training set. Many variations around this type of memory have been studied in the past (see Mozer (1993) for an overview of existing models). Most of these models are based on SRN with no off-diagonal  3  Accepted as a workshop contribution at ICLR 2015  recurrent connections between the hidden units. They differ in how the diagonal weights of the recurrent matrix are constrained. Recently, Pachitariu & Sahani (2013) showed that this type of architecture can achieve performance similar to a full SRN when the size of the dataset and of the model are small. This type of architecture can potentially retain information about longer term statistics, such as the topic of a text, but it does not scale well to larger datasets (Pachitariu & Sahani, 2013). Besides, it can been argued that purely linear SRNs with learned self-recurrent weights will perform very similarly to a combination of cache models with different rates of information decay (Kuhn & De Mori, 1990). Cache models compute probability of the next token given a bag- of-words (unordered) representation of longer history. They are well known to perform strongly on small datasets (Goodman, 2001b). Mikolov & Zweig (2012) show that using such contextual features as additional inputs to the hidden layer leads to a signiﬁcant improvement in performance over the regular SRN. However in their work, the contextual features are pre-trained using standard NLP techniques and not learned as part of the recurrent model. In this work, we propose a model which learns the contextual features using stochastic gradient descent. These features are the state of a hidden layer associated with a diagonal recurrent matrix similar to the one presented in Mozer (1993). In other words, our model possesses both a fully con- nected recurrent matrix to produce a set of quickly changing hidden units, and a diagonal matrix that that encourages the state of the context units to change slowly (see the detailed model in Figure 1- b). The fast layer (called hidden layer in the rest of this paper) can learn representations similar to n-gram models, while the slowly changing layer (called context layer) can learn topic information, similar to cache models. More precisely, denoting by st the state of the p context units at time t, the update rules of the model are:  st = (1 − α)Bxt + αst−1, ht = σ (P st + Axt + Rht−1) , yt = f (U ht + V st)  (4) (5) (6) where α is a parameter in (0, 1) and P is a p×m matrix. Note that there is no nonlinearity applied to the state of the context units. The contextual hidden units can be seen as an exponentially decaying bag of words representation of the history. This exponential trace memory (as denoted by Mozer (1993)) has been already proposed in the context of simple recurrent networks (Jordan, 1987; Mozer, 1989). A close idea to our work is to use so-called ”leaky integration” neurons (Jaeger et al., 2007), which also forces the neurons to change their state slowly, however without the structural constraint of SCRN. It was evaluated on the same dataset as we use further (Penn Treebank) by Bengio et al. (2013). Interestingly, the results we observed in our experiments show much bigger gains over stronger baseline using our model, as will be shown later.  Alternative Model Interpretation. If we consider the context units as additional hidden units (with no activation function), we can see our model as a SRN with a constrained recurrent matrix M on both hidden and context units:  (cid:21)  (cid:20) R P  0 αIp  M =  ,  (7)  where Ip is the identity matrix and M is a square matrix of size m + p, i.e., the sum of the number of hidden and context units. This reformulation shows explicitly our structural modiﬁcation of the Elman SRN (Elman, 1990): we constrain a diagonal block of the recurrent matrix to be equal to a reweighed identity, and keep an off-diagonal block equal to 0. For this reason, we call our model Structurally Constrained Recurrent Network (SCRN).  Adaptive Context Features. Fixing the weight α to be constant in Eq. (4) forces the hidden units to capture information on the same time scale. On the other hand, if we allow this weight to be learned for each unit, we can potentially capture context from different time delays (Pachitariu & Sahani, 2013). More precisely, we denote by Q the recurrent matrix of the contextual hidden layer, and we consider the following update rule for the state of the contextual hidden layer st:  4  Accepted as a workshop contribution at ICLR 2015  st = (I − Q)Bxt + Qst−1,  (8)  where Q is a diagonal matrix with diagonal elements in (0, 1). We suppose that these diagonal elements are obtained by applying a sigmoid transformation to a parameter vector β, i.e., diag(Q) = σ(β). This parametrization naturally forces the diagonal weights to stay strictly between 0 and 1. We study in the following section in what situations does learning of the weights help. Interestingly, we show that learning of the self-recurrent weights does not seem to be important, as long as one uses also the standard hidden layer in the model.  3 EXPERIMENTS  We evaluate our model on the language modeling task for two datasets. The ﬁrst dataset is the Penn Treebank Corpus, which consists of 930K words in the training set. The pre-processing of data and division to training, validation and test parts are the same as in (Mikolov et al., 2011). The state-of- the-art performance on this dataset has been achieved by Zaremba et al. (2014), using combination of many big, regularized LSTM recurrent neural network language models. The LSTM networks were ﬁrst introduced to language modeling by Sundermeyer et al. (2012). The second dataset, which is moderately sized, is called Text8. It is composed of a pre-processed version of the ﬁrst 100 million characters from Wikipedia dump. We did split it into training part (ﬁrst 99M characters) and development set (last 1M characters) that we use to report performance. After that, we constructed the vocabulary and replaced all words that occur less than 5 times by <UNK> token. The resulting vocabulary size is about 44K. To simplify reproducibility of our results, we released both the SCRN code and the scripts which construct the datasets 1. In this section we compare the performance of our proposed model against standard SRNs, and LSTM RNNs which are becoming the architecture of choice for modeling sequential data with long-term dependencies.  3.1  IMPLEMENTATION DETAILS.  We used Torch library and implemented our proposed model following the graph given in Figure 1-b. Note that following the alternative interpretation of our model with the recurrent matrix deﬁned in Eq. 8, our model could be simply implemented by modifying a standard SRN. We ﬁx α at 0.95 unless stated otherwise. The number of backpropagation through time (BPTT) steps is set to 50 for our model and was chosen by parameter search on the validation set. For normal SRN, we use just 10 BPTT steps because the gradients vanish faster. We do a stochastic gradient descent after every 5 forward steps. Our model is trained with a batch gradient descent of size 32, and a learning rate of 0.05. We divide the learning rate by 1.5 after each training epoch when the validation error does not decrease.  3.2 RESULTS ON PENN TREEBANK CORPUS.  We ﬁrst report results on the Penn Treebank Corpus using both small and moderately sized models (with respect to the number of hidden units). Table 1 shows that our structurally constrained re- current network (SCRN) model can achieve performance comparable with LSTM models on small datasets with relatively small numbers of parameters. It should be noted that the LSTM models have signiﬁcantly more parameters for the same size of hidden layer, making the comparison somewhat unfair - with the input, forget and output gates, the LSTM has about 4x more parameters than SRN with the same size of hidden layer. Comparison to ”leaky neurons” is also in favor of SCRN: Bengio et al. (2013) report perplexity reduction from 136 (SRN) to 129 (SRN + leaky neurons), while for the same dataset, we observed much bigger improvement, going from perplexity 129 (SRN) down to 115 (SCRN). Table 1 also shows that SCRN outperforms the SRN architecture even with much less parameters. This can be seen by comparing performance of SCRN with 40 hidden and 10 contextual units (test  1The SCRN code can be downloaded at http://github.com/facebook/SCRNNs  5  Accepted as a workshop contribution at ICLR 2015  perplexity 127) versus SRN with 300 hidden units (perplexity 129). This suggests that imposing a structure on the recurrent matrix allows the learning algorithm to capture additional information. To obtain further evidence that this additional information is of a longer term character, we did further run experiments on the Text8 dataset that contains various topics, and thus the longer term information affects the performance on this dataset much more.  #hidden #context Validation Perplexity Test Perplexity  Ngram + cache  Model Ngram  SRN SRN SRN LSTM LSTM LSTM SCRN SCRN SCRN SCRN  - - 50 100 300 50 100 300 40 90 100 300  - - - - - - - - 10 10 40 40  - - 153 137 133 129 120 123 133 124 120 120  141 125 144 129 129 123 115 119 127 119 115 115  Table 1: Results on Penn Treebank Corpus: n-gram baseline, simple recurrent nets (SRN), long short term memory RNNs (LSTM) and structurally constrained recurrent nets (SCRN). Note that LSTM models have 4x more parameters than SRNs for the same size of hidden layer.  3.2.1 LEARNING SELF-RECURRENT WEIGHTS.  We evaluate inﬂuence of learning the diagonal weights of the recurrent matrix for the contextual layer. For the following experiments, we used a hierarchical soft-max with 100 frequency-based classes on the Penn Treebank Corpus to speedup the experiments. In Table 2, we show that when the size of the hidden layer is small, learning the diagonal weights is crucial. This result conﬁrms the ﬁndings in Pachitariu & Sahani (2013). However, as we increase the size of our model and use sufﬁcient number of hidden units, learning of the self-recurrent weights does not give any signiﬁcant improvement. This indicates that learning the weights of the contextual units allows these units to be used as multi-scale representation of the history, i.e., some contextual units can specialize on the very recent history (for example, for α close to 0, the contextual units would be part of a simple bigram language model). With various learned self-recurrent weights, the model can be seen as a combination of cache and bigram models. When the number of standard hidden units is enough to capture short term patterns, learning the self-recurrent weights does not seem crucial anymore. Keeping this observation in mind we ﬁxed the diagonal weights when working with the Text8 corpus.  Fixed weights Adaptive weights  Model #hidden #context SCRN SCRN SCRN SCRN SCRN SCRN  50 25 0 140 100 0  0 25 50 0 40 140  156 150 344 140 127 334  156 145 157 140 127 147  Table 2: Perplexity on the test set of Penn Treebank Corpus with and without learning the weights of the contextual features. Note that in these experiments we used a hierarchical soft-max.  3.3 RESULTS ON TEXT8.  Our next experiment involves the Text8 corpus which is signiﬁcantly larger than the Penn Treebank. As this dataset contains various articles from Wikipedia, the longer term information (such as current topic) plays bigger role than in the previous experiments. This is illustrated by the gains when cache is added to the baseline 5-gram model: the perplexity drops from 309 to 229 (26% reduction).  6  Accepted as a workshop contribution at ICLR 2015  We report experiments with a range of model conﬁgurations, with different number of hidden units. In Table 3, we show that increasing the capacity of standard SRNs by adding the contextual features results in better performance. For example, when we add 40 contextual units to SRN with 100 hidden units, the perplexity drops from 245 to 189 (23% reduction). Such model is also much better than SRN with 300 hidden units (perplexity 202).  Model #hidden SCRN SCRN SCRN  100 300 500  context = 0  245 202 184  context = 10  215 182 177  context = 20  201 172 166  context = 40  189 165 162  context = 80  184 164 161  Table 3: Structurally constrained recurrent nets: perplexity for various sizes of the contextual layer, reported on the development set of Text8 dataset.  In Table 4, we see that when the number of hidden units is small, our model is better than LSTM. Despite the LSTM model with 100 hidden units being larger, the SCRN with 100 hidden and 80 contextual features achieves better performance. On the other hand, as the size of the models in- crease, we see that the best LSTM model is slightly better than the best SCRN (perplexity 156 versus 161). As the perplexity gains for both LSTM and SCRN over SRN are much more signiﬁcant than in the Penn Treebank experiments, it seems likely that both models actually model the same kind of patterns in language.  Model #hidden #context SRN SRN SRN LSTM LSTM LSTM SCRN SCRN SCRN  100 300 500 100 300 500 100 300 500  - - - - - - 80 80 80  Perplexity on development set  245 202 184 193 159 156 184 164 161  Table 4: Comparison of various recurrent network architectures, evaluated on the development set of Text8.  4 CONCLUSION  In this paper, we have shown that learning longer term patterns in real data using recurrent net- works is perfectly doable using standard stochastic gradient descent, just by introducing structural constraint on the recurrent weight matrix. The model can then be interpreted as having quickly changing hidden layer that focuses on short term patterns, and slowly updating context layer that retains longer term information. Empirical comparison of SCRN to Long Short Term Memory (LSTM) recurrent network shows very similar behavior in two language modeling tasks, with similar gains over simple recurrent network when all models are tuned for the best accuracy. Moreover, SCRN shines in cases when the size of models is constrained, and with similar number of parameters it often outperforms LSTM by a large margin. This can be especially useful in cases when the amount of training data is practically unlimited, and even models with thousands of hidden neurons severely underﬁt the training datasets. We believe these ﬁndings will help researchers to better understand the problem of learning longer term memory in sequential data. Our model greatly simpliﬁes analysis and implementation of re- current networks that are capable of learning longer term patterns. Further, we published the code that allows to easily reproduce experiments described in this paper. At the same time, it should be noted that none of the above models is capable of learning truly long term memory, which has a different nature. For example, if we would want to build a model that can  7  Accepted as a workshop contribution at ICLR 2015  store arbitrarily long sequences of symbols and reproduce these later, it would become obvious that this is not doable with models that have ﬁnite capacity. A possible solution is to use the recurrent net as a controller of an external memory which has unlimited capacity. For example in (Joulin & Mikolov, 2015), a stack-based memory is used for such task. However, a lot of research needs to be done in this direction before we will develop models that can successfully learn to grow in complexity and size when solving increasingly more difﬁcult tasks.  REFERENCES Bengio, Yoshua, Simard, Patrice, and Frasconi, Paolo. Learning long-term dependencies with gra-  dient descent is difﬁcult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.  Bengio, Yoshua, Boulanger-Lewandowski, Nicolas, and Pascanu, Razvan. Advances in optimizing  recurrent networks. In ICASSP, 2013.  Dahl, George E, Yu, Dong, Deng, Li, and Acero, Alex. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42, 2012.  Elman, Jeffrey L. Finding structure in time. Cognitive science, 14(2):179–211, 1990.  Goodman, Joshua. Classes for fast maximum entropy training. In Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP’01). 2001 IEEE International Conference on, volume 1, pp. 561–564. IEEE, 2001a.  Goodman, Joshua T. A bit of progress in language modeling. Computer Speech & Language, 15(4):  403–434, 2001b.  Graves, Alex and Schmidhuber, Juergen. Ofﬂine handwriting recognition with multidimensional recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 545–552, 2009.  Graves, Alex and Schmidhuber, J¨urgen. Framewise phoneme classiﬁcation with bidirectional lstm  and other neural network architectures. Neural Networks, 18(5):602–610, 2005.  Hochreiter, Sepp. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02): 107–116, 1998.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Jaeger, Herbert, Lukoˇseviˇcius, Mantas, Popovici, Dan, and Siewert, Udo. Optimization and appli- cations of echo state networks with leaky-integrator neurons. Neural Networks, 20(3):335–352, 2007.  Jordan, Michael I. Attractor dynamics and parallelism in a connectionist sequential machine. Pro- ceedings of the Eighth Annual Conference of the Cognitive Science Society, pp. 531–546, 1987.  Joulin, Armand and Mikolov, Tomas. Inferring algorithmic patterns with stack-augmented recurrent  nets. arXiv preprint arXiv:1503.01007, 2015.  Koehn, Philipp, Hoang, Hieu, Birch, Alexandra, Callison-Burch, Chris, Federico, Marcello, Bertoldi, Nicola, Cowan, Brooke, Shen, Wade, Moran, Christine, Zens, Richard, et al. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meet- ing of the ACL on Interactive Poster and Demonstration Sessions, pp. 177–180. Association for Computational Linguistics, 2007.  Kuhn, Roland and De Mori, Renato. A cache-based natural language model for speech recognition.  Pattern Analysis and Machine Intelligence, IEEE Transactions on, 12(6):570–583, 1990.  LeCun, Yann, Bottou, Leon, Orr, Genevieve, and M¨uller, Klaus. Efﬁcient backprop. Neural Net-  works: Tricks of the Trade, pp. 546–546, 1998.  8  Accepted as a workshop contribution at ICLR 2015  Mikolov, Tom´aˇs. Statistical language models based on neural networks. PhD thesis, Ph. D. thesis,  Brno University of Technology, 2012.  Mikolov, Tomas and Zweig, Geoffrey. Context dependent recurrent neural network language model.  In SLT, pp. 234–239, 2012.  Mikolov, Tomas, Kombrink, Stefan, Burget, Lukas, Cernocky, JH, and Khudanpur, Sanjeev. Exten- sions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pp. 5528–5531. IEEE, 2011.  Mozer, Michael C. A focused back-propagation algorithm for temporal pattern recognition. Complex  systems, 3(4):349–381, 1989.  Mozer, Michael C. Neural net architectures for temporal sequence processing. In Santa Fe Institute Studies in The Sciences of Complexity, volume 15, pp. 243–243. Addison-Wessley Publishing CO, 1993.  Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807– 814, 2010.  Pachitariu, Marius and Sahani, Maneesh. Regularization and nonlinearities for neural language  models: when are they needed? arXiv preprint arXiv:1301.5650, 2013.  Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning internal representations  by error propagation. Technical report, DTIC Document, 1985.  Simonyan, Karen and Zisserman, Andrew. Two-stream convolutional networks for action recogni-  tion in videos. In Advances in Neural Information Processing Systems, pp. 568–576, 2014.  Sundermeyer, Martin, Schl¨uter, Ralf, and Ney, Hermann. Lstm neural networks for language mod-  eling. In INTERSPEECH, 2012.  Werbos, Paul J. Generalization of backpropagation with application to a recurrent gas market model.  Neural Networks, 1(4):339–356, 1988.  Williams, Ronald J and Zipser, David. Gradient-based learning algorithms for recurrent networks and their computational complexity. Back-propagation: Theory, architectures and applications, pp. 433–486, 1995.  Young, Steve, Evermann, Gunnar, Gales, Mark, Hain, Thomas, Kershaw, Dan, Liu, Xunying, Moore, Gareth, Odell, Julian, Ollason, Dave, Povey, Dan, et al. The HTK book, volume 2. En- tropic Cambridge Research Laboratory Cambridge, 1997.  Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol. Recurrent neural network regularization.  arXiv preprint arXiv:1409.2329, 2014.  9  ",
1412.6418,2015, Inducing Semantic Representation from Text by Jointly Predicting and Factorizing Relations,"['Inducing Semantic Representation from Text by Jointly Predicting and Factorizing Relations', 'Ivan Titov and Ehsan Khoddam']",https://arxiv.org/pdf/1412.6418,"5 1 0 2    r p A 6 1         ] L C . s c [      3 v 8 1 4 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  INDUCING SEMANTIC REPRESENTATION FROM TEXT BY JOINTLY PREDICTING AND FACTORIZING RELA- TIONS  Ivan Titov, Ehsan Khoddam Universiteit van Amsterdam Amsterdam, the Netherlands {titov,e.khoddammohammadi}@uva.nl  ABSTRACT  In this work, we propose a new method to integrate two recent lines of work: un- supervised induction of shallow semantics (e.g., semantic roles) and factorization of relations in text and knowledge bases. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument ﬁllers. When the components are estimated jointly to minimize errors in argument reconstruc- tion, the induced roles largely correspond to roles deﬁned in annotated resources. Our method performs on par with most accurate role induction methods on En- glish, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the language.  1  INTRODUCTION  Shallow representations of meaning, and semantic role labels in particular, have a long history in linguistics (Fillmore, 1968). More recently, with an emergence of large annotated resources such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), automatic semantic role labeling (SRL) has attracted a lot of attention (Surdeanu et al., 2008; Hajiˇc et al., 2009; Das et al., 2010). Semantic role representations encode the underlying predicate-argument structure of sentences, or, more speciﬁcally, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). Consider the following sentence:  [Agent The police] charged [P atient the demonstrators] [Instrument with batons].  Here, the police, the demonstrators and with batons are assigned to roles Agent, Patient and Instrument, respectively. Semantic roles have many potential applications in NLP and have been shown to beneﬁt, for example, question answering (Shen and Lapata, 2007; Berant et al., 2014) and textual entailment (Sammons et al., 2009), among others. The scarcity of annotated data has motivated the research into unsupervised learning of seman- tic representations (Lang and Lapata, 2010; 2011a;b; Titov and Klementiev, 2012; F¨urstenau and Rambow, 2012; Garg and Henderson, 2012). The existing methods have a number of serious short- comings. First, they make very strong assumptions, for example, assuming that arguments are con- ditionally independent of each other given the predicate. Second, unlike state-of-the-art supervised parsers, they rely on a very simplistic set of features of a sentence. These factors lead to models be- ing insufﬁciently expressive to capture syntax-semantics interface, inadequate handling of language ambiguity and, overall, introduces an upper bound on their performance. In this work, we propose a method for effective unsupervised estimation of feature-rich models of semantic roles. We demonstrate that reconstruction-error objectives, which have been shown to be effective primarily for training neural networks, are well suited for inducing feature-rich log-linear models of semantics. Our model consists of two components: a log-linear feature rich semantic role  1  Accepted as a workshop contribution at ICLR 2015  Figure 1: (a) An autoencoder from Rm to Rp (typically p < m). reconstruction-error minimization framework.  (b) Modeling roles within the  labeler and a tensor-factorization model which captures interaction between semantic roles and argu- ment ﬁllers. Our method rivals the most accurate semantic role induction methods on English (Titov and Klementiev, 2012; Lang and Lapata, 2011a). Importantly, no prior knowledge about any spe- ciﬁc language was incorporated in our feature-rich model, whereas the clustering counterparts relied on language-speciﬁc argument signatures.  2 APPROACH  At the core of our approach is a statistical model encoding an interdependence between a seman- tic role structure and its realization in a sentence. In the unsupervised learning setting, sentences, their syntactic representations and argument positions (denoted by x) are observable whereas the associated semantic roles r are latent and need to be induced by the model. Crucially, the good r should encode roles rather than some other form of abstraction. In what follows, we will refer to roles using their names, though, in the unsupervised setting, our method, as any other latent variable model, will not yield human-interpretable labels for them. We also focus only on the labeling stage of semantic role labeling. Identiﬁcation, though an important problem, can be tackled with heuris- tics (Lang and Lapata, 2011a), with unsupervised techniques (Abend et al., 2009) or potentially by using a supervised classiﬁer trained on a small amount of data. The model consists of two components. The ﬁrst component is responsible for prediction of argu- ment tuples based on roles and the predicate. In our experiments, in this component, we represent arguments as lemmas of their lexical heads (e.g., baton instead of with batons), and we also restrict ourselves to only verbal predicates. Intuitively, we can think of predicting one argument at a time (see Figure 1(b)): an argument (e.g., demonstrator in our example) is predicted based on the pred- icate lemma (charge), the role assigned to this argument (i.e. Patient) and other role-argument pairs ((Agent, police) and (Instrument, baton)). While learning to predict arguments, the in- ference algorithm will search for role assignments which simplify this prediction task as much as possible. Our hypothesis is that these assignments will correspond to roles accepted in linguistic theories (or, more importantly, useful in practical applications). Why is this hypothesis plausible? Primarily because these semantic representations were introduced as an abstraction capturing cru- cial properties of a relation (or an event). Thus, these representations, rather than surface linguistic details like argument order or syntactic functions, should be crucial for modeling sets of potential argument tuples. The reconstruction component is not the only part of the model. Crucially, what we referred to above as ‘searching for role assignments to simplify argument prediction’ would actually correspond to learning another component: a semantic role labeler which predicts roles relying on a rich set of sentence features. These two components will be estimated jointly in such a way as to minimize errors in recovering arguments. The role labeler will be the end-product of learning: it will be used to process new sentences, and it will be compared to existing methods in our evaluation. Generative modeling is not the only way to learn latent representations. One alternative, popular in the neural network community, is to use autoencoders instead and optimize the reconstruction error (Hinton, 1989; Vincent et al., 2008). The encoding model will be a feature-rich classiﬁer  2   Reconstructed inputEncodingReconstructionInputy2RpLatent representationFeature representation of ""The police charged...  "" (    )Semantic role prediction ( = Encoding)charge(Agent: police,   Patient:  demonstrator,   Instrument: baton)demonstratorArgument prediction( = Reconstruction)Hidden p(r|x,w)Feature-rich model""Argument prediction"" model(a)(b)x2Rm˜x2Rmp(ai|ai,r,v,✓)xAccepted as a workshop contribution at ICLR 2015  which predicts semantic roles for a sentence, and the reconstruction model is the model which predicts an argument given its role, and given the rest of the arguments and their roles. The idea of training linear models with reconstruction error was previously explored by Daum´e III (2009) and very recently by Ammar et al. (2014). However, they do not consider learning factorization models, and they also do not deal with semantics. Tensor and factorization methods used in the context of modeling knoweldge bases (e.g., (Bordes et al., 2011)) are also close in spirit. However, they do not deal with inducing semantics but rather factorize existing relations (i.e. rely on semantics).  2.1 MODELING SEMANTICS WITHIN THE RECONSTRUCTION-ERROR FRAMEWORK  As we mentioned above, we focus on argument labeling: we assume that arguments a = (a1, . . . , aN ), ai ∈ A, are known, and only their roles r = (r1, . . . , rN ), ri ∈ R need to be induced. For the encoder (i.e. the semantic role labeler), we use a log-linear model:  p(r|x, w) ∝ exp(wT g(x, r)),  where g(x, r) is a feature vector encoding interactions between sentence x and the semantic role representation r. Any model can be used here as long as the posterior distributions of roles ri can be efﬁciently computed or approximated. In our experiments, we used a model which factorizes over individual arguments (i.e. independent logistic regression classiﬁers). The reconstruction component predicts an argument (e.g., the ith argument ai) given the semantic roles r, the predicate v and other arguments a−i = (a1, . . . , ai−1, ai+1, . . . , aN ) with a bilinear softmax model:  exp(uT  aiC T  j(cid:54)=i Cv,rj uaj )  v,ri Z(r, v, i)  p(ai|a−i, r, v, C, u) =  (1) ua ∈ Rd (for every a ∈ A) and Cv,r ∈ Rd×k (for every verb v and every role r ∈ R) are model parameters, Z(r, v, i) is the partition function ensuring that the probabilities sum to one. Intuitively, embeddings ua encode semantic properties of an argument: for example, embeddings for the words demonstrator and protestor should be somewhere near each other in Rd space, and further away from that for the word cat. The product Cp,rua is a k-dimensional vector encoding beliefs about other arguments based on the argument-role pair (a, r). In turn, the dot product (Cv,riuai)T Cv,rj uaj is large if the argument pair (ai, aj) is semantically compatible with the predicate, and small otherwise. Intuitively, this objective corresponds to scoring argument tuples according to  ,  (cid:80)  h(a, r, v, C, u) =  uT aiC T  v,riCv,rj uaj ,  i(cid:54)=j  hinting at connections to (coupled) tensor and factorization methods (Yılmaz et al., 2011; Bordes et al., 2011) and distributional semantics (Mikolov et al., 2013; Pennington et al., 2014). Note also that the reconstruction model does not have access to any features of the sentence (e.g., argument order or syntax), forcing the roles to convey all the necessary information. In practice, we smooth the model by using a sum of predicate-speciﬁc and cross-predicate projection matrices (Cv,r + Cr) instead of just Cv,r.  2.2 LEARNING  Parameters of both model components (w, u and C) are learned jointly: the natural objective asso- ciated with every sentence would be the following:  (cid:88)  p(ai|a−i, r, v, C, u)p(r|x, w).  (2)  However optimizing this objective is not practical in its exact form for two reasons: (1) the marginal- ization over r is exponential in the number of arguments; (2) the partition function Z(r, v, i) re- quires summation over the entire set of potential argument lemmas. We use existing techniques to address both challenges. In order to deal with the ﬁrst challenge, we use a basic mean-ﬁeld approximation: instead of marginalization over r we substitute r with their posterior distributions µis = p(ri = s|x, w). To tackle the second problem, the computation of Z(µ, v, i), we use a negative sampling technique (see, e.g., Mikolov et al. (2013)). At test time, only the linear semantic role labeler is used, so the inference is straightforward.  3  N(cid:88)  (cid:88)  log  i=1  r  Accepted as a workshop contribution at ICLR 2015  Table 1: Purity (PU), collocation (CO) and F1 on English (PropBank / CoNLL 2008).  Our Model Bayes Agglom+ RoleOrdering Agglom GraphPart LLogistic SyntF  PU 79.7 89.3 87.9 83.5 88.7 88.6 79.5 81.6  CO 86.2 76.6 75.6 78.5 73.0 70.7 76.5 77.5  F1 82.8 82.5 81.3 80.9 80.1 78.6 78.0 79.5  3 EXPERIMENTS  We followed Lang and Lapata (2010) and used the CoNLL 2008 shared task data (Surdeanu et al., 2008). As in most previous work on unsupervised SRL, we evaluate our model using clustering metrics: purity, collocation and their harmonic mean F1. For the semantic role labeling (encoding) component, we relied on 14 feature patterns used for argument labeling in one of the popular su- pervised role labelers (Johansson and Nugues, 2008), which resulted in a quite large feature space (49,474 feature instantiations for our English dataset). For the reconstruction component, we set the dimensionality of embeddings d, the projection di- mensionality k and the number of negative samples n to 30, 15 and 20, respectively. These hyper- parameters were tuned on held-out data (the CoNLL 2008 development set), d and k were chosen among {10, 15, 20, 30, 50} with constraining d to be always greater than k, n was ﬁxed to 10 and 20. The model was not sensitive to the parameter deﬁning the number of roles as long it was large enough. For training, we used uniform random initialization and AdaGrad (Duchi et al., 2011). Following (Lang and Lapata, 2010), we use a baseline (SyntF) which simply clusters predicate arguments according to the dependency relation to their head. A separate cluster is allocated for each of 20 most frequent relations in the dataset and an additional cluster is used for all other relations. As observed in the previous work (Lang and Lapata, 2011a), this is a hard baseline to beat. We also compare against previous approaches: the latent logistic classiﬁcation model (Lang and Lapata, 2010) (labeled LLogistic), the agglomerative clustering method (Lang and Lapata, 2011a) (Agglom), the graph partitioning approach (Lang and Lapata, 2011b) (GraphPart), the global role ordering model (Garg and Henderson, 2012) (RoleOrdering). We also report results of an improved version of Agglom, recently reported by Lang and Lapata (2014) (Agglom+). The strongest previous model is Bayes: Bayes is the most accurate (‘coupled’) version of the Bayesian model of Titov and Klementiev (2012), estimated from the CoNLL data without relying on any external data. Our model outperforms or performs on par with best previous models in terms of F1 (see Table 1). Interestingly, the purity and collocation balance is very different for our model and for the rest of the systems. In fact, our model induces at most 4-6 roles. On the contrary, Bayes predicts more than 30 roles for the majority of frequent predicates (e.g., 43 roles for the predicate include or 35 for say). Though this tendency reduces the purity scores for our model, this also means that our roles are more human interpretable. For example, agents and patients are clearly identiﬁable in the model predictions.  4 CONCLUSIONS  We introduced a method for inducing feature-rich semantic role labelers from unannoated text. In our approach, we view a semantic role representation as an encoding of a latent relation between a predicate and a tuple of its arguments. We capture this relation with a probabilistic tensor factor- ization model. Our estimation method yields a semantic role labeler which achieves state-of-the-art results on English.  4  Accepted as a workshop contribution at ICLR 2015  REFERENCES Abend, O., Reichart, R., and Rappoport, A. (2009). Unsupervised argument identiﬁcation for semantic role  labeling. In ACL-IJCNLP.  Ammar, W., Dyer, C., and Smith, N. (2014). Conditional random ﬁeld autoencoders for unsupervised structured  prediction. In NIPS.  Baker, C. F., Fillmore, C. J., and Lowe, J. B. (1998). The Berkeley FrameNet project. In ACL-COLING. Berant, J., Srikumar, V., Chen, P., Huang, B., Manning, C. D., Vander Linden, A., Harding, B., and Clark, P.  (2014). Modeling biological processes for reading comprehension. In EMNLP.  Bordes, A., Weston, J., Collobert, R., and Bengio, Y. (2011). Learning structured embeddings of knowledge  bases. In AAAI.  Das, D., Schneider, N., Chen, D., and Smith, N. A. (2010). Probabilistic frame-semantic parsing. In NAACL. Daum´e III, H. (2009). Unsupervised search-based structured prediction. In ICML. Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic  optimization. JMLR, 12:2121–2159.  Fillmore, C. J. (1968). The case for case. In E., B. and R.T., H., editors, Universals in Linguistic Theory, pages  1–88. Holt, Rinehart, and Winston, New York.  F¨urstenau, H. and Rambow, O. (2012). Unsupervised induction of a syntax-semantics lexicon using iterative In the First Joint Conference on Lexical and Computational Semantics-Volume 1: the main  reﬁnement. conference and the shared task, and Volume 2: the Sixth International Workshop on Semantic Evaluation.  Garg, N. and Henderson, J. (2012). Unsupervised semantic role induction with global role ordering. In ACL:  Short Papers-Volume 2.  Hajiˇc, J., Ciaramita, M., Johansson, R., Kawahara, D., Mart´ı, M. A., M`arquez, L., Meyers, A., Nivre, J., Pad´o, S., ˇStˇep´anek, J., Straˇn´ak, P., Surdeanu, M., Xue, N., and Zhang, Y. (2009). The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In CoNLL.  Hinton, G. E. (1989). Connectionist learning procedures. Artiﬁcial intelligence, 40(1):185–234. Johansson, R. and Nugues, P. (2008). Dependency-based syntactic-semantic analysis with PropBank and Nom-  Bank. In CoNLL.  Lang, J. and Lapata, M. (2010). Unsupervised induction of semantic roles. In ACL. Lang, J. and Lapata, M. (2011a). Unsupervised semantic role induction via split-merge clustering. In ACL. Lang, J. and Lapata, M. (2011b). Unsupervised semantic role induction with graph partitioning. In EMNLP. Lang, J. and Lapata, M. (2014). Similarity-driven semantic role induction via graph partitioning. Computational  Linguistics, 40(3):633–669.  Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁcient estimation of word representations in vector  space. arXiv preprint arXiv:1301.3781.  Palmer, M., Gildea, D., and Kingsbury, P. (2005). The proposition bank: An annotated corpus of semantic  roles. Computational Linguistics, 31(1):71–106.  Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation. In  EMNLP.  Sammons, M., Vydiswaran, V., Vieira, T., Johri, N., Chang, M., Goldwasser, D., Srikumar, V., Kundu, G., Tu, Y., Small, K., Rule, J., Do, Q., and Roth, D. (2009). Relation alignment for textual entailment recognition. In Text Analysis Conference (TAC).  Shen, D. and Lapata, M. (2007). Using semantic roles to improve question answering. In EMNLP. Surdeanu, M., Johansson, A. M. R., M`arquez, L., and Nivre, J. (2008). The CoNLL-2008 shared task on joint  parsing of syntactic and semantic dependencies. In CoNLL.  Titov, I. and Klementiev, A. (2012). A bayesian approach to semantic role induction. In EACL. Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and composing robust features  with denoising autoencoders. In ICML.  Yılmaz, K. Y., Cemgil, A. T., and Simsekli, U. (2011). Generalised coupled tensor factorisation. In NIPS.  5  ",
1410.8516,2015, NICE: Non-linear Independent Components Estimation,"['NICE: Non-linear Independent Components Estimation', 'Laurent Dinh', 'David Krueger', 'and Yoshua Bengio']",https://arxiv.org/pdf/1410.8516,"5 1 0 2    r p A 0 1         ]  G L . s c [      6 v 6 1 5 8  .  0 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  NICE: NON-LINEAR INDEPENDENT COMPONENTS ESTIMATION  Laurent Dinh David Krueger Yoshua Bengio∗ D´epartement d’informatique et de recherche op´erationnelle Universit´e de Montr´eal Montr´eal, QC H3C 3J7  ABSTRACT  We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a dis- tribution that is easy to model. For this purpose, a non-linear deterministic trans- formation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in indepen- dent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.  1  INTRODUCTION  One of the central questions in unsupervised learning is how to capture complex data distributions that have unknown structure. Deep learning approaches (Bengio, 2009) rely on the learning of a representation of the data that would capture its most important factors of variation. This raises the question: what is a good representation? Like in recent work (Kingma and Welling, 2014; Rezende et al., 2014; Ozair and Bengio, 2014), we take the view that a good representation is one in which the distribution of the data is easy to model. In this paper, we consider the special case where we ask the learner to ﬁnd a transformation h = f (x) of the data into a new space such that the resulting distribution factorizes, i.e., the components hd are independent:  (cid:89)  d  pH (h) =  pHd (hd).  The proposed training criterion is directly derived from the log-likelihood. More speciﬁcally, we consider a change of variables h = f (x), which assumes that f is invertible and the dimension of h is the same as the dimension of x, in order to ﬁt a distribution pH. The change of variable rule gives us:  pX (x) = pH (f (x))|det  ∂f (x)  |.  ∂x  (1)  where ∂f (x) is the Jacobian matrix of function f at x. In this paper, we choose f such that the determinant of the Jacobian is trivially obtained. Moreover, its inverse f−1 is also trivially obtained, ∂x allowing us to sample from pX (x) easily as follows:  (2) A key novelty of this paper is the design of such a transformation f that yields these two properties of “easy determinant of the Jacobian” and “easy inverse”, while allowing us to have as much capacity  h ∼ pH (h) −1(h) x = f  ∗Yoshua Bengio is a CIFAR Senior Fellow.  1  Accepted as a workshop contribution at ICLR 2015  (a) Inference  (b) Sampling  (c) Inpainting  Figure 1: Computational graph of the probabilistic model, using the following formulas.  (a) Inference: log(pX (x)) = log(pH (f (x))) + log(|det( ∂f (x)  ∂x )|)  (b) Sampling: h ∼ pH (h), x = f−1(h)  maxxH log(pX ((xO, xH ))) = maxxH log(pH (f ((xO, xH )))) + log(|det( ∂f ((xO,xH ))  )|)  ∂x  (c) Inpainting:  as needed in order to learn complex transformations. The core idea behind this is that we can split x into two blocks (x1, x2) and apply as building block a transformation from (x1, x2) to (y1, y2) of the form:  (3) where m is an arbitrarily complex function (a ReLU MLP in our experiments). This building block has a unit Jacobian determinant for any m and is trivially invertible since:  y1 = x1 y2 = x2 + m(x1)  x1 = y1 x2 = y2 − m(y1).  (4)  The details, surrounding discussion, and experimental results are developed below.  2 LEARNING BIJECTIVE TRANSFORMATIONS OF CONTINUOUS  PROBABILITIES  We consider the problem of learning a probability density from a parametric family of densities {pθ, θ ∈ Θ} over ﬁnite dataset D of N examples, each living in a space X ; typically X = RD. Our particular approach consists of learning a continuous, differentiable almost everywhere non- linear transformation f of the data distribution into a simpler distribution via maximum likelihood using the following change of variables formula:  log(pX (x)) = log(pH (f (x))) + log(|det(  ∂f (x)  ∂x  )|)  where pH (h), the prior distribution, will be a predeﬁned density function 1, for example a standard isotropic Gaussian. If the prior distribution is factorial (i.e. with independent dimensions), then we obtain the following non-linear independent components estimation (NICE) criterion, which is simply maximum likelihood under our generative model of the data as a deterministic transform of a factorial distribution:  D(cid:88)  d=1  log(pX (x)) =  log(pHd (fd(x))) + log(|det(  ∂f (x)  ∂x  )|)  where f (x) = (fd(x))d≤D. We can view NICE as learning an invertible preprocessing transform of the dataset. Invertible pre- processings can increase likelihood arbitrarily simply by contracting the data. We use the change of  1Note that this prior distribution does not need to be constant and could also be learned  2  HXHXHXOXHAccepted as a workshop contribution at ICLR 2015  Figure 2: Computational graph of a coupling layer  variables formula (Eq. 1) to exactly counteract this phenomenon and use the factorized structure of the prior pH to encourage the model to discover meaningful structures in the dataset. In this formula, the determinant of the Jacobian matrix of the transform f penalizes contraction and encourages ex- pansion in regions of high density (i.e., at the data points), as desired. As discussed in Bengio et al. (2013), representation learning tends to expand the volume of representation space associated with more “interesting” regions of the input (e.g., high density regions, in the unsupervised learning case). In line with previous work with auto-encoders and in particular the variational auto-encoder (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014), we call f the encoder and its inverse f−1 the decoder. With f−1 given, sampling from the model can proceed very easily by ancestral sampling in the directed graphical model H → X, i.e., as described in Eq. 2.  3 ARCHITECTURE  3.1 TRIANGULAR STRUCTURE  The architecture of the model is crucial to obtain a family of bijections whose Jacobian determinant is tractable and whose computation is straightforward, both forwards (the encoder f) and backwards (the decoder f−1). If we use a layered or composed transformation f = fL ◦ . . . ◦ f2 ◦ f1, the forward and backward computations are the composition of its layers’ computations (in the suited order), and its Jacobian determinant is the product of its layers’ Jacobian determinants. Therefore we will ﬁrst aim at deﬁning those more elementary components. First we consider afﬁne transformations. (Rezende et al., 2014) and (Kingma and Welling, 2014) provide formulas for the inverse and determinant when using diagonal matrices, or diagonal matri- ces with rank-1 correction, as transformation matrices. Another family of matrices with tractable determinant are triangular matrices, whose determinants are simply the product of their diagonal ele- ments. Inverting triangular matrices at test time is reasonable in terms of computation. Many square matrices M can also be expressed as a product M = LU of upper and lower triangular matrices. Since such transformations can be composed, we see that useful components of these compositions include ones whose Jacobian is diagonal, lower triangular or upper triangular. One way to use this observation would be to build a neural network with triangular weight matrices and bijective activation functions, but this highly constrains the architecture, limiting design choices to depth and selection of non-linearities. Alternatively, we can consider a family of functions with triangular Jacobian. By ensuring that the diagonal elements of the Jacobian are easy to compute, the determinant of the Jacobian is also made easy to compute.  3.2 COUPLING LAYER  In this subsection we describe a family of bijective transformation with triangular Jacobian therefore tractable Jacobian determinant. That will serve a building block for the transformation f.  3  =ykeygyciphermxkeyxplainAccepted as a workshop contribution at ICLR 2015  General coupling layer Let x ∈ X , I1, I2 a partition of(cid:74)1, D(cid:75) such that d = |I1| and m a function  deﬁned on Rd, we can deﬁne y = (yI1, yI2 ) where:  where g : RD−d × m(Rd) → RD−d is the coupling law, an invertible map with respect to its ﬁrst argument given the second. The corresponding computational graph is shown Fig 2. If we consider  I1 =(cid:74)1, d(cid:75) and I2 =(cid:74)d, D(cid:75), the Jacobian of this function is: (cid:35)  (cid:34)  yI1 = xI1 yI2 = g(xI2; m(xI1))  ∂y ∂x  =  Id ∂yI2 ∂xI1  0 ∂yI2 ∂xI2  Where Id is the identity matrix of size d. That means that det ∂y can invert the mapping using:  ∂x = det ∂yI2 ∂xI2  . Also, we observe we  xI1 = yI1 xI2 = g  −1(yI2; m(yI1))  We call such a transformation a coupling layer with coupling function m.  Additive coupling layer For simplicity, we choose as coupling law an additive coupling law g(a; b) = a + b so that by taking a = xI2 and b = m(xI1 ): yI2 = xI2 + m(xI1) xI2 = yI2 − m(yI1)  and thus computing the inverse of this transformation is only as expensive as computing the trans- formation itself. We emphasize that there is no restriction placed on the choice of coupling function m (besides having the proper domain and codomain). For example, m can be a neural network with d input units and D − d output units. Moreover, since det ∂yI2 = 1, an additive coupling layer transformation has a unit Jacobian deter- ∂xI2 minant in addition to its trivial inverse. One could also choose other types of coupling, such as a mul- tiplicative coupling law g(a; b) = a(cid:12)b, b (cid:54)= 0 or an afﬁne coupling law g(a; b) = a(cid:12)b1+b2, b1 (cid:54)= 0 if m : Rd → RD−d × RD−d. We chose the additive coupling layer for numerical stability reason as the transformation become piece-wise linear when the coupling function, m, is a rectiﬁed neural network.  Combining coupling layers We can compose several coupling layers to obtain a more complex layered transformation. Since a coupling layer leaves part of its input unchanged, we need to ex- change the role of the two subsets in the partition in alternating layers, so that the composition of two coupling layers modiﬁes every dimension. Examining the Jacobian, we observe that at least three coupling layers are necessary to allow all dimensions to inﬂuence one another. We generally use four.  3.3 ALLOWING RESCALING  As each additive coupling layers has unit Jacobian determinant (i.e. is volume preserving), their composition will necessarily have unit Jacobian determinant too. In order to adress this issue, we include a diagonal scaling matrix S as the top layer, which multiplies the i-th ouput value by Sii: (xi)i≤D → (Siixi)i≤D. This allows the learner to give more weight (i.e. model more variation) on some dimensions and less in others. In the limit where Sii goes to +∞ for some i, the effective dimensionality of the data has been reduced by 1. This is possible so long as f remains invertible around the data point. With such a scaled diagonal last stage along with lower triangular or upper triangular stages for the rest (with the identity in their diagonal), the NICE criterion has the following form:  log(pX (x)) =  [log(pHi(fi(x))) + log(|Sii|)].  D(cid:88)  i=1  4  Accepted as a workshop contribution at ICLR 2015  We can relate these scaling factors to the eigenspectrum of a PCA, showing how much variation is present in each of the latent dimensions (the larger Sii is, the less important the dimension i is). The important dimensions of the spectrum can be viewed as a manifold learned by the algorithm. The prior term encourages Sii to be small, while the determinant term log Sii prevents Sii from ever reaching 0.  3.4 PRIOR DISTRIBUTION  As mentioned previously, we choose the prior distribution to be factorial, i.e.:  D(cid:89)  pH (h) =  pHd (hd)  We generally pick this distribution in the family of standard distribution, e.g. gaussian distribution:  d=1  log(pHd ) = − 1 2  (h2  d + log(2π))  or logistic distribution:  log(pHd) = − log(1 + exp(hd)) − log(1 + exp(−hd))  We tend to use the logistic distribution as it tends to provide a better behaved gradient.  4 RELATED METHODS  Signiﬁcant advances have been made in generative models. Undirected graphical models like deep Boltzmann machines (DBM) (Salakhutdinov and Hinton, 2009) have been very successful and an intense subject of research, due to efﬁcient approximate inference and learning techniques that these models allowed. However, these models require Markov chain Monte Carlo (MCMC) sampling procedure for training and sampling and these chains are generally slowly mixing when the target distribution has sharp modes. In addition, the log-likelihood is intractable, and the best known es- timation procedure, annealed importance sampling (AIS) (Salakhutdinov and Murray, 2008), might yield an overly optimistic evaluation (Grosse et al., 2013). Directed graphical models lack the conditional independence structure that allows DBMs efﬁcient inference. Recently, however, the development of variational auto-encoders (VAE) (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014) - allowed effec- tive approximate inference during training. In constrast with the NICE model, these approaches use a stochastic encoder q(h | x) and an imperfect decoder p(x | h), requiring a reconstruction term in the cost, ensuring that the decoder approximately inverts the encoder. This injects noise into the auto-encoder loop, since h is sampled from q(h | x), which is a variational approximation to the true posterior p(h | x). The resulting training criterion is the variational lower bound on the log-likelihood of the data. The generally fast ancestral sampling technique that directed graphical models provide make these models appealing. Moreover, the importance sampling estimator of the log-likelihood is guaranteed not to be optimistic in expectation. But using a lower bound criterion might yield a suboptimal solution with respect to the true log-likelihood. Such suboptimal solu- tions might for example inject a signiﬁcant amount of unstructured noise in the generation process resulting in unnatural-looking samples. In practice, we can output a statistic of p(x | h), like the expectation or the median, instead of an actual sample. The use of a deterministic decoder can be motivated by the objective of eliminating low-level noise, which gets automatically added at the last stage of generation in models such as the VAE and Boltzmann machines (the visible are considered independent, given the hidden). The NICE criterion is very similar to the criterion of the variational auto-encoder. More speciﬁcally, as the transformation and its inverse can be seen as a perfect auto-encoder pair (Bengio, 2014), the reconstruction term is a constant that can be ignored. This leaves the Kullback-Leibler divergence term of the variational criterion: log(pH (f (x))) can be seen as the prior term, which forces the ∂x |) can be seen code h = f (x) to be likely with respect to the prior distribution, and log(|det ∂f (x) as the entropy term. This entropy term reﬂects the local volume expansion around the data (for the encoder), which translates into contraction in the decoder f−1. In a similar fashion, the entropy  5  Accepted as a workshop contribution at ICLR 2015  term in the variational criterion encourages the approximate posterior distribution to occupy volume, which also translates into contraction from the decoder. The consequence of perfect reconstruction is that we also have to model the noise at the top level, h, whereas it is generally handled by the conditional model p(x | h) in these other graphical models. We also observe that by combining the variational criterion with the reparametrization trick, (Kingma and Welling, 2014) is effectively maximizing the joint log-likelihood of the pair (x, (cid:15)) in a NICE model with two afﬁne coupling layers (where (cid:15) is the auxiliary noise variable) and gaussian prior, see Appendix C. The change of variable formula for probability density functions is prominently used in inverse transform sampling (which is effectively the procedure used for sampling here). Independent com- ponent analysis (ICA) (Hyv¨arinen and Oja, 2000), and more speciﬁcally its maximum likelihood formulation, learns an orthogonal transformation of the data, requiring a costly orthogonalization procedure between parameter updates. Learning a richer family of transformations was proposed in (Bengio, 1991), but the proposed class of transformations, neural networks, lacks in general the structure to make the inference and optimization practical. (Chen and Gopinath, 2000) learns a layered transformation into a gaussian distribution but in a greedy fashion and it fails to deliver a tractable sampling procedure. (Rippel and Adams, 2013) reintroduces this idea of learning those transformations but is forced into a regularized auto-encoder setting as a proxy of log-likelihood maximization due to the lack of bijectivity constraint. A more principled proxy of log-likelihood, the variational lower bound, is used more successfully in nonlinear independent components analysis (Hyv¨arinen and Pajunen, 1999) via ensemble learning (Roberts and Everson, 2001; Lappalainen et al., 2000) and in (Kingma and Welling, 2014; Rezende et al., 2014) using a type of Helmholtz machine (Dayan et al., 1995). Generative adversarial networks (GAN) (Goodfellow et al., 2014) also train a generative model to transform a simple (e.g. factorial) distribution into the data distribution, but do not require an encoder that goes in the other direction. GAN sidesteps the difﬁculties of inference by learning a secondary deep network that discriminates between GAN samples and data. This classiﬁer network provides a training signal to the GAN generative model, telling it how to make its output less distinguishable from the training data. Like the variational auto-encoders, the NICE model uses an encoder to avoid the difﬁculties of in- ference, but its encoding is deterministic. The log-likelihood is tractable and the training procedure does not require any sampling (apart from dequantizing the data). The triangular structure used in NICE to obtain tractability is also present in another family of tractable density models, the neu- ral autoregressive networks (Bengio and Bengio, 2000), which include as a recent and succesful example the neural autoregressive density estimator (NADE) (Larochelle and Murray, 2011). In- deed, the adjacency matrix in the NADE directed graphical model is strictly triangular. However the element-by-element autoregressive schemes make the ancestral sampling procedure computation- ally expensive and unparallelizable for generative tasks on high-dimensional data, such as image data. A NICE model using one coupling layer can be seen as a block version of NADE with two blocks.  5 EXPERIMENTS  5.1 LOG-LIKELIHOOD AND GENERATION  We train NICE on MNIST (LeCun and Cortes, 1998), the Toronto Face Dataset 2 (TFD) (Susskind et al., 2010), the Street View House Numbers dataset (SVHN) (Netzer et al., 2011) and CIFAR-10 (Krizhevsky, 2010). As prescribed in (Uria et al., 2013), we use a dequantized version of the data: we add a uniform noise of 1 256 to the data and rescale it to be in [0, 1]D after dequantization. We add a uniform noise of 1 The architecture used is a stack of four coupling layers with a diagonal positive scaling (parametrized exponentially) exp(s) for the last stage, and with an approximate whitening for TFD and exact ZCA on SVHN and CIFAR-10. We partition the input space between by separating odd (I1) and even (I2)  128 and rescale the data to be in [−1, 1]D for CIFAR-10.  2We train on unlabeled data for this dataset.  6  Accepted as a workshop contribution at ICLR 2015  Dataset  # dimensions Preprocessing # hidden layers # hidden units  Prior  Log-likelihood  MNIST  784 None  5  1000 logistic 1980.50  TFD 2304  Approx. whitening  4  5000  gaussian 5514.71  SVHN 3072 ZCA  4  2000 logistic 11496.55  CIFAR-10  3072 ZCA  4  2000 logistic 5371.78  Figure 3: Architecture and results. # hidden units refer to the number of units per hidden layer.  Model NICE  Deep MFA  GRBM  TFD  5514.71  CIFAR-10 5371.78  5250 2413  3622 2365  Figure 4: Log-likelihood results on TFD and CIFAR-10. Note that the Deep MFA number  correspond to the best results obtained from (Tang et al., 2012) but are actually variational lower  bound.  components, so the equation is:  h(1) I1 = xI1 h(1) I2 = xI2 + m(1)(xI1 )  h(2) I2 = h(1) h(2) I1 = h(1)  I2  I1 + m(2)(xI2)  h(3) I1 = h(2) h(3) I2 = h(2)  I1  I2 + m(3)(xI1)  I2  h(4) I2 = h(3) I1 = h(3) h(4) h = exp(s) (cid:12) h(4)  I1 + m(4)(xI2)  The coupling functions m(1), m(2), m(3) and m(4) used for the coupling layers are all deep rectiﬁed networks with linear output units. We use the same network architecture for each coupling function: ﬁve hidden layers of 1000 units for MNIST, four of 5000 for TFD, and four of 2000 for SVHN and CIFAR-10. A standard logistic distribution is used as prior for MNIST, SVHN and CIFAR-10. A standard normal distribution is used as prior for TFD.  The models are trained by maximizing the log-likelihood log(pH (h)) + (cid:80)D  i=1 si with AdaM (Kingma and Ba, 2014) with learning rate 10−3, momentum 0.9, β2 = 0.01, λ = 1, and (cid:15) = 10−4. We select the best model in terms of validation log-likelihood after 1500 epochs. We obtained a test log-likelihood of 1980.50 on MNIST, 5514.71 on TFD, 11496.55 for SVHN and 5371.78 for CIFAR-10. This compares to the best results that we know of in terms of log- likelihood: 5250 on TFD and 3622 on CIFAR-10 with deep mixtures of factor analysers (Tang et al., 2012) (although it is still a lower bound), see Table 4. As generative models on continuous MNIST are generally evaluated with Parzen window estimation, no fair comparison can be made. Samples generated by the trained models are shown in Fig. 5.  7  Accepted as a workshop contribution at ICLR 2015  (a) Model trained on MNIST  (b) Model trained on TFD  (c) Model trained on SVHN  (d) Model trained on CIFAR-10  Figure 5: Unbiased samples from a trained NICE model. We sample h ∼ pH (h) and we output  x = f−1(h).  8  Accepted as a workshop contribution at ICLR 2015  (a) MNIST test examples  (b) Initial state  (c) MAP inference of the state  Figure 6: Inpainting on MNIST. We list below the type of the part of the image masked per line of the above middle ﬁgure, from top to bottom: top rows, bottom rows, odd pixels, even pixels, left side, right side, middle vertically, middle horizontally, 75% random, 90% random. We clamp the pixels that are not masked to their ground truth value and infer the state of the masked pixels by  projected gradient ascent on the likelihood. Note that with middle masks, there is almost no  information available about the digit.  5.2  INPAINTING  Here we consider a naive iterative procedure to implement inpainting with the trained generative models. For inpainting we clamp the observed dimensions (xO) to their values and maximize log- likelihood with respect to the hidden dimensions (XH ) using projected gradient ascent (to keep the input in its original interval of values) with gaussian noise with step size αi = 10 100+i, where i is the iteration, following the stochastic gradient update:  xH,i+1 = xH,i + αi(  (cid:15) ∼ N (0, I)  ∂ log(pX ((xO, xH,i)))  ∂xH,i  + (cid:15))  where xH,i are the values of the hidden dimensions at iteration i. The result is shown on test exam- ples of MNIST, in Fig 6. Although the model is not trained for this task, the inpainting procedure seems to yield reasonable qualitative performance, but note the occasional presence of spurious modes.  6 CONCLUSION  In this work we presented a new ﬂexible architecture for learning a highly non-linear bijective trans- formation that maps the training data to a space where its distribution is factorized, and a framework to achieve this by directly maximizing log-likelihood. The NICE model features efﬁcient unbiased ancestral sampling and achieves competitive results in terms of log-likelihood. Note that the architecture of our model could be trained using other inductive principles capable of exploiting its advantages, like toroidal subspace analysis (TSA) (Cohen and Welling, 2014). We also brieﬂy made a connection with variational auto-encoders. We also note that NICE can en- able more powerful approximate inference allowing a more complex family of approximate posterior distributions in those models, or a richer family of priors.  ACKNOWLEDGEMENTS  We would like to thank Yann Dauphin, Vincent Dumoulin, Aaron Courville, Kyle Kastner, Dustin Webb, Li Yao and Aaron Van den Oord for discussions and feedback. Vincent Dumoulin provided code for visualization. We are grateful towards the developers of Theano (Bergstra et al., 2011; Bastien et al., 2012) and Pylearn2 (Goodfellow et al., 2013), and for the computational resources  9  Accepted as a workshop contribution at ICLR 2015  provided by Compute Canada and Calcul Qu´ebec, and for the research funding provided by NSERC, CIFAR, and Canada Research Chairs.  REFERENCES Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  Bengio, Y. (1991). Artiﬁcial Neural Networks and their Application to Sequence Recognition. PhD  thesis, McGill University, (Computer Science), Montreal, Canada.  Bengio, Y. (2009). Learning deep architectures for AI. Now Publishers.  Bengio, Y. (2014). How auto-encoders could provide credit assignment in deep networks via target  propagation. Technical report, arXiv:1407.7906.  Bengio, Y. and Bengio, S. (2000). Modeling high-dimensional discrete data with multi-layer neural In Solla, S., Leen, T., and M¨uller, K.-R., editors, Advances in Neural Information  networks. Processing Systems 12 (NIPS’99), pages 400–406. MIT Press.  Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013). Better mixing via deep representations.  In Proceedings of the 30th International Conference on Machine Learning (ICML’13). ACM.  Bergstra, J., Bastien, F., Breuleux, O., Lamblin, P., Pascanu, R., Delalleau, O., Desjardins, G., Warde-Farley, D., Goodfellow, I. J., Bergeron, A., and Bengio, Y. (2011). Theano: Deep learning on gpus with python. In Big Learn workshop, NIPS’11.  Chen, S. S. and Gopinath, R. A. (2000). Gaussianization.  Cohen, T. and Welling, M. (2014). Learning the irreducible representations of commutative lie  groups. arXiv:1402.4437.  Dayan, P., Hinton, G. E., Neal, R., and Zemel, R. (1995). The Helmholtz machine. Neural Compu-  tation, 7:889–904.  Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial networks. Technical Report arXiv:1406.2661, arxiv.  Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., and Bengio, Y. (2013). Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214.  Gregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. (2014). Deep autoregressive  networks. In International Conference on Machine Learning (ICML’2014).  Grosse, R., Maddison, C., and Salakhutdinov, R. (2013). Annealing between distributions by aver-  aging moments. In ICML’2013.  Hyv¨arinen, A. and Oja, E. (2000). Independent component analysis: algorithms and applications.  Neural networks, 13(4):411–430.  Hyv¨arinen, A. and Pajunen, P. (1999). Nonlinear independent component analysis: Existence and  uniqueness results. Neural Networks, 12(3):429–439.  Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980.  Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In Proceedings of the  International Conference on Learning Representations (ICLR).  Krizhevsky, A. (2010). Convolutional deep belief networks on CIFAR-10. Technical report, University of Toronto. Unpublished Manuscript: http://www.cs.utoronto.ca/ kriz/conv-cifar10- aug2010.pdf.  10  Accepted as a workshop contribution at ICLR 2015  Lappalainen, H., Giannakopoulos, X., Honkela, A., and Karhunen, J. (2000). Nonlinear independent component analysis using ensemble learning: Experiments and discussion. In Proc. ICA. Citeseer.  Larochelle, H. and Murray, I. (2011). The Neural Autoregressive Distribution Estimator. In Pro- ceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AIS- TATS’2011), volume 15 of JMLR: W&CP.  LeCun, Y. and Cortes, C. (1998). The mnist database of handwritten digits.  Mnih, A. and Gregor, K. (2014). Neural variational inference and learning in belief networks. In  ICML’2014.  Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading digits in natural images with unsupervised feature learning. Deep Learning and Unsupervised Feature Learning Workshop, NIPS.  Ozair, S. and Bengio, Y. (2014). Deep directed generative autoencoders. Technical report, U.  Montreal, arXiv:1410.0630.  Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate  inference in deep generative models. Technical report, arXiv:1401.4082.  Rippel, O. and Adams, R. P. (2013). High-dimensional probability estimation with deep density  models. arXiv:1302.5125.  Roberts, S. and Everson, R. (2001).  Cambridge University Press.  Independent component analysis: principles and practice.  Salakhutdinov, R. and Hinton, G. (2009). Deep Boltzmann machines. In Proceedings of the Inter-  national Conference on Artiﬁcial Intelligence and Statistics, volume 5, pages 448–455.  Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief networks. In Cohen, W. W., McCallum, A., and Roweis, S. T., editors, Proceedings of the Twenty-ﬁfth International Conference on Machine Learning (ICML’08), volume 25, pages 872–879. ACM.  Susskind, J., Anderson, A., and Hinton, G. E. (2010). The Toronto face dataset. Technical Report  UTML TR 2010-001, U. Toronto.  Tang, Y., Salakhutdinov, R., and Hinton, G. (2012). Deep mixtures of factor analysers. arXiv  preprint arXiv:1206.4635.  Uria, B., Murray, I., and Larochelle, H. (2013). Rnade: The real-valued neural autoregressive  density-estimator. In NIPS’2013.  11  Accepted as a workshop contribution at ICLR 2015  (a) Model trained on MNIST  (b) Model trained on TFD  Figure 7: Sphere in the latent space. These ﬁgures show part of the manifold structure learned by  the model.  A FURTHER VISUALIZATIONS  A.1 MANIFOLD VISUALIZATION  To illustrate the learned manifold, we also take a random rotation R of a 3D sphere S in latent space and transform it to data space, the result f−1(R(S)) is shown in Fig 7.  A.2 SPECTRUM  We also examined the last diagonal scaling layer and looked at its coefﬁcients (Sdd)d≤D. If we −1 consider jointly the prior distribution and the diagonal scaling layer, σd = S dd can be considered as the scale parameter of each independent component. This shows us the importance that the model has given to each component and ultimately how successful the model was at learning manifolds. We sort (σd)d≤D and plot it in Fig 8.  B APPROXIMATE WHITENING  The procedure for learning the approximate whitening is using the NICE framework, with an afﬁne function and a standard gaussian prior. We have:  with L lower triangular and b a bias vector. This is equivalent to learning a gaussian distribution. The optimization procedure is the same as NICE: RMSProp with early stopping and momentum.  z = Lx + b  C VARIATIONAL AUTO-ENCODER AS NICE  We assert here that the stochastic gradient variational Bayes (SGVB) algorithm maximizes the log- likelihood on the pair (x, (cid:15)).(Kingma and Welling, 2014) deﬁne a recognition network:  z = gφ((cid:15) | x), (cid:15) ∼ N (0, I)  For a standard gaussian prior p(z) and conditional p(x | z), we can deﬁne:  x − fθ(z)  σ  ξ =  12  Accepted as a workshop contribution at ICLR 2015  (a) Model trained on MNIST  (b) Model trained on TFD  (c) Model trained on SVHN  (d) Model trained on CIFAR-10  −1 Figure 8: Decay of σd = S dd . The large values correspond to dimensions on which the model chooses to have larger variations, thus highlighting the learned manifold structure from the data. This is the non-linear equivalent of the eigenspectrum in the case of PCA. On the x axis are the  components d sorted by σd (on the y axis).  If we deﬁne a standard gaussian prior on h = (z, ξ). The resulting cost function is:  log(p(x,(cid:15)),(θ,φ)(x, (cid:15))) = log(pH (h)) − DX log(σ) + log(|det  ((cid:15); x)|)  ∂gφ ∂(cid:15)  with DX = dim(X). This is equivalent to: log(p(x,(cid:15)),(θ,φ)(x, (cid:15))) − log(p(cid:15)((cid:15))) = log(pH (h)) − DX log(σ) + log(|det  ((cid:15); x)|) − log(p(cid:15)((cid:15)))  ∂gφ ∂(cid:15) = log(pH (h)) − DX log(σ) − log(qZ|X;φ(z)) = log(pξ(ξ)pZ(z)) − DX log(σ) − log(qZ|X;φ(z)) = log(pξ(ξ)) + log(pZ(z)) − DX log(σ) − log(qZ|X;φ(z)) = log(pξ(ξ)) − DX log(σ) + log(pZ(z)) − log(qZ|X;φ(z)) = log(pX|Z(x | z)) + log(pZ(z)) − log(qZ|X;φ(z))  This is the Monte Carlo estimate of the SGVB cost function proposed in (Kingma and Welling, 2014).  13  ",
1412.6583,2015, Discovering Hidden Factors of Variation in Deep Networks,"['Discovering Hidden Factors of Variation in Deep Networks', 'Brian Cheung', 'Jesse Livezey', 'Arjun Bansal', 'and Bruno Olshausen']",https://arxiv.org/pdf/1412.6583,"5 1 0 2     n u J    7 1      ]  G L . s c [      4 v 3 8 5 6  .  2 1 4 1 : v i X r a  Discovering Hidden Factors of Variation in Deep  Networks  Brian Cheung∗  Jesse A. Livezey∗  Redwood Center for Theoretical Neuroscience  Redwood Center for Theoretical Neuroscience  University of California, Berkeley  Berkeley, CA 94720, USA  bcheung@berkeley.edu  University of California, Berkeley  Berkeley, CA 94720, USA  jesse.livezey@berkeley.edu  Arjun K. Bansal  Nervana Systems, Inc.  San Diego, CA 92121, USA arjun@nervanasys.com  Bruno A. Olshausen  Redwood Center for Theoretical Neuroscience  University of California, Berkeley  Berkeley, CA 94720, USA  baolshausen@berkeley.edu  Abstract  Deep learning has enjoyed a great deal of success because of its ability to learn useful features for tasks such as classiﬁcation. But there has been less explo- ration in learning the factors of variation apart from the classiﬁcation signal. By augmenting autoencoders with simple regularization terms during training, we demonstrate that standard deep architectures can discover and explicitly repre- sent factors of variation beyond those relevant for categorization. We introduce a cross-covariance penalty (XCov) as a method to disentangle factors like hand- writing style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Furthermore, we demonstrate these deep networks can extrapolate ‘hidden’ variation in the su- pervised signal.  1  Introduction  One of the goals of representation learning is to ﬁnd an efﬁcient representation of input data that simpliﬁes tasks such as object classiﬁcation [1] or image restoration [2]. Supervised algorithms approach this problem by learning features which transform the data into a space where different classes are linearly separable. However this often comes at the cost of discarding other variations such as style or pose that may be important for more general tasks. On the other hand, unsuper- vised learning algorithms such as autoencoders seek efﬁcient representations of the data such that the input can be fully reconstructed, implying that the latent representation preserves all factors of variation in the data. However, without some explicit means for factoring apart the different sources of variation the factors relevant for a speciﬁc task such as categorization will be entangled with other factors across the latent variables. Our goal in this work is to combine these two approaches to disentangle class-relevant signals from other factors of variation in the latent variables in a standard deep autoencoder. Previous approaches to separating factors of variation in data, such as content vs. style [3] or form vs. motion [4, 5, 6, 7, 8, 9], have relied upon a bilinear model architecture in which the units representing different factors are combined multiplicatively. Such an approach was recently utilized  ∗Authors contributed equally.  1  to separate facial expression vs. identity using higher-order restricted Boltzmann machines [10]. One downside of bilinear approaches in general is that they require learning an approximate weight tensor corresponding to all three-way multiplicative combinations of units. Despite the impressive results achieved with this approach, the question nevertheless remains as to whether there is a more straightforward way to separate factors of variation using standard nonlinearities in feedforward neural networks. Earlier work by [11] demonstrated class-irrelevant aspects in MNIST (style) can be learned by including additional unsupervised units alongside supervised ones in an autoencoder. However, their model does not disentangle class-irrelevant factors from class-relevant ones. More recently, [12] utilized a variational autoencoder in a semi-supervised learning paradigm which is capable of separating content and style in data. It is this work which is the inspiration for the simple training scheme presented here. Autoencoder models have been shown to be useful for a variety of machine learning tasks [13, 14, 15]. The basic autoencoder architecture can be separated into an encoding stage and a decoding stage. During training, the two stages are jointly optimized to reconstruct the input data from the output of the decoder. In this work, we propose using both the encoding and decoding stages of the autoencoder to learn high-level representations of the factors of variation contained in the data. The high-level representation (or encoder output) is divided into two sets of variables. The ﬁrst set (observed variables) is used in a discriminative task and during reconstruction. The second set (latent variables) is used only for reconstruction. To promote disentangling of representations in an autoencoder, we add two additional costs to the network. The ﬁrst is a discriminative cost on the observed variables. The second is a novel cross-covariance penalty (XCov) between the observed and latent variables across a batch of data. This penalty prevents latent variables from encoding input variations due to class label. [17] proposed a similar penalty over terms in the product between the Jacobians of observed and latent variables with respect to the input. In our penalty, the variables which represent class assignment are separated from those which are encoding other factors of variations in the data. We analyze characteristics of this learned representation on three image datasets. In the absence of standard benchmark task for evaluating disentangling performance, our evaluation here is based on examining qualitatively what factors of variation are discovered for different datasets. In the case of MNIST, the learned factors correspond to style such as slant and size. In the case TFD the factors correspond to identity, and in the case of Multi-PIE identity speciﬁc attributes such as clothing, skin tone, and hair style.  2 Semi-supervised Autoencoder Given an input x ∈ RD and its corresponding class label y ∈ RL for a dataset D, we consider the class label to be a high-level representation of its corresponding input. However, this representation is usually not invertible because it discards much of the variation contained in the input distribution. In order to properly reconstruct x, autoencoders must learn a latent representation which preserves all input variations in the dataset.  Figure 1: The encoder and decoder are combined and jointly trained to reconstruct the inputs and predict the observed variables ˆy.  Using class labels, we incorporate supervised learning to a subset of these latent variables trans- forming them into observed variables, ˆy as shown in Figure 1. In this framework, the remaining the latent variable z must account for the remaining variation of dataset D. We hypothesize this latent variation is a high-level representation of the input complementary to the observed variation. For instance, the class label ‘5’ provided by y would not be sufﬁcient for the decoder to properly recon-  2  h1h2h-3h-2h-1ŷzencoderdecoderxxˆstruct the image of a particular ‘5’. In this scenario, z would encode properties of the digit such as style, slant, width, etc. to provide the decoder sufﬁcient information to reconstruct the original input image. Mathematically, the encoder F and decoder G are deﬁned respectively as:  {ˆy, z} = F (x; θ)  ˆx = G(y, z; φ)  (1) (2)  where θ and φ are the parameters of the encoder and decoder respectively.  2.1 Learning  The objective function to train the network is deﬁned as the sume of three seperate cost terms.  (cid:88)  ˆθ, ˆφ = arg min  θ,φ  {x,y}∈D  (cid:88)  i  ||x − ˆx||2 + β  yilog(ˆyi) + γC.  (3)  The ﬁrst term is a typical reconstruction cost (squared error) for an autoencoder. The second term is a standard supervised cost (cross-entropy). While there are many potential choices for the reconstruc- tion cost depending on the distribution of data vector x, for our experiments we use squared-error for all datasets. For the observed variables, the form of the cost function depends on the type of vari- ables (categorical, binary, continuous). For our experiments, we had categorical observed variables so we parametrized them as one-hot vectors and compute ˆy = sof tmax(Wˆyh2 + bˆy). The third term C is the unsupervised cross-covariance (XCov) cost which disentangles the observed and latent variables of the encoder.  C(ˆy1...N , z1...N ) =  1 2  [  1 N  i − ¯ˆyi)(zn (ˆyn  j − ¯ˆzj)]2.  (4)  (cid:88)  (cid:88)  ij  n  The XCov penalty to disentangle ˆy and z is simply a sum-squared cross-covariance penalty between the activations across samples in a batch of size N where ¯ˆyi and ¯ˆzj denote means over examples. n is an index over examples and i, j index feature dimensions. Unlike the reconstruction and super- vised terms in the objective, XCov is a cost computed over a batch of datapoints. It is possible to approximate this quantity with a moving average during training but we have found that this cost has been robust to small batch sizes and have not found any issues when training with mini-batches as small as N = 50. Its derivative is provided in the supplementary material. This objective function naturally ﬁts a semi-supervised learning framework. For unlabeled data, the multiplier β for the supervised cost is simply set to zero. In general, the choice of β and γ will depend on the intended task. Larger β will lead to better to classiﬁcation performance while larger γ to better separation between latent and observed factors.  3 Experimental Results  We evaluate autoencoders trained to minimize 3 on three datasets of increasing complexity. The net- work is trained using ADADELTA [18] with gradients from standard backpropagation. Models were implemented in a modiﬁed version of Pylearn2 [19] using deconvolution and likelihood estimation code from [20].  3.1 Datasets  MNIST Handwritten Digits Database  The MNIST handwritten digits database [21] consists of 60,000 training and 10,000 test images of handwritten digits 0-9 of size 28x28. Following previous work [22], we split the training set into 50,000 samples for training and 10,000 samples as a validation set for model selection.  3  Toronto Faces Database  The Toronto Faces Database [23] consists of 102,236 grayscale face images of size 48x48. Of these, 4,178 are labeled with 1 of 7 different expressions (anger, disgust, fear, happy, sad, surprise, and neutral). Examples are shown in Figure 2. The dataset also contains 3,784 identity labels which were not used in this paper. The dataset has 5 folds of training, validation and test examples. The three partitions are disjoint and contain no overlap in identities.  Figure 2: Left: Example TFD images from the test set showing 7 expressions with random identity. Right: Example Multi-PIE images from the test set showing 3 of the 19 camera poses with variable lighting and identity.  Multi-PIE Dataset  The Multi-PIE datasets [24] consists of 754,200 high-resolution color images of 337 subjects. Each subject was recorded under 15 camera poses: 13 spaced at 15 degree intervals at head height, and 2 positioned above the subject. For each of these cameras, subjects were imaged under 19 illumination conditions and a variety of facial expressions. We discarded images from the two overhead cameras due to inconsistencies found in their image. Camera pose and illumination data was retained as supervised labels. Only a small subset of the images possess facial keypoint information for each camera pose. To perform a weak registration to appoximately localize the face region, we compute the maximum bounding box created by all available facial keypoint coordinates for a given camera pose. This bounding box is applied to all images for that camera pose. We then resized the cropped images to 48x48 pixels and convert to grayscale. We divide the dataset into 528,060 training, 65,000 validation and 60,580 test examples. Splits were determined by subject id. Therefore, the test set contains no overlap in identities with the training or validation sets. Example images from our test set are shown in Figure 2. The Multi-PIE dataset contains signiﬁcantly more complex factors of variation than MNIST or TFD. Unlike TFD, images in Multi-PIE includes much more of the subject’s body. The weak registration also causes signiﬁcant variation in the subject’s head position and scale.  Table 1: Network Architectures (Softmax (SM), Rectiﬁed Linear (ReLU))  MNIST  TFD  ConvDeconvMultiPIE  500 ReLU 500 ReLU 10 SM, 2 Linear 500 ReLU 500 ReLU 784 Linear  2000 ReLU 2000 ReLU 7 SM, 793 Linear 2000 ReLU 2000 ReLU 2304 Linear  20x20x32 ConvReLU 2000 ReLU 2000 ReLU 13 SM, 19 SM, 793 Linear 2000 ReLU 2000 ReLU 2000 ReLU 2000 ReLU 48x48x1 Deconv  4  angerdisgustfearhappysadsurpriseneutralTable 2: MNIST Classiﬁcation Performance  Model  Accuracy  Model Selection Criterion  MNIST ConvMNIST MaxoutMNIST + dropout Maxout + dropout [22]  98.35 98.71 99.01 99.06  Reconstuction: β = 10, γ = 10 Reconstuction: β = 10, γ = 10 Accuracy: β = 100, γ = 10 Accuracy  Table 3: TFD Classiﬁcation Performance  Model  Accuracy  Model Selection Criterion  TFD ConvTFD disBM [10] CCNET+CDA+SVM [17]  69.4 84.0 85.4 85.0  Reconstuction: β = 10, γ = 1e3 (Fold 0) Accuracy: β = 10, γ = 1e3 (Fold 0) Accuracy Accuracy (Fold 0)  3.2 Model Performace  3.2.1 Classiﬁcation  As a sanity check, we ﬁrst show that the our additional regularization term in the cost negligibly impacts performance for different architectures including convolution, maxout and dropout. Tables 2 and 3 show classiﬁcation results for MNIST and TFD are comparable to previously published results. Details on network architecture for these models can be found in the supplementary material.  3.2.2 Learned Factors of Variation We begin our analysis using the MNIST dataset. We intentionally choose z ∈ R2 for the architecture described in Table 1 for ease in visualization of the latent variables. As shown in Figure 3a, z takes on a suprisingly simple isotropic Normal distribution with mean 0 and standard deviation .35. Visualizing Latent Variables To visualize the transformations that the latent variables are learning, the decoder can be used to create images for different values of z. We vary a single element zi linearly over a set interval with z\i ﬁxed to 0 and y ﬁxed to one-hot vectors corresponding to each class label as shown in Figure 3b and c. Moving across each column for a given row, the digit style is maintained as the class labels varies. This suggests the network has learned a class invariant latent representation. At the center of z-space, (0,0), we ﬁnd the canonical MNIST digit style. Moving away from the center, the digits become more stylized but also less probable. We ﬁnd this result is reliably reproduced without the XCov regularization when the dimensionality of z is relatively small suggesting that the network naturally prefers such a latent representation for factors of variation absent in the supervised signal. With this knowledge, we describe a method to generate samples from such an autoencoder with competative generative performance in the supplementary material. Moving From Latent Space to Image Space Following the layer of observed and latent variables {y, z}, there are two additional layers of activa- tions h−3, h−2 before the output of the model into image space. To visualize the function of these layers, we compute the Jacobian of the output image, ˆx, with respect to the activation of hidden units, hk, in a particular layer. This analysis provides insight into the transformation each unit is applying to the input x to generate ˆx. More speciﬁcally, it is a measure of how a small perturbation of a particular unit in the network affects the output ˆx:  ∆ˆxk  i =  ∂ ˆxi ∂hk j  ∆hj.  5  (5)  Figure 3: a: Histogram of test set z variables. b: Generated MNIST digits formed by setting z2 to zero and varying z1. c: Generated MNIST digits formed by setting z1 to zero and varying z2. σ was calculated from the variation on the test set.  Here, i is the index of a pixel in the output of the network, j is the index of a hidden unit, and k is the layer number. We remove hidden units with zero activation from the Jacobian since their derivatives are not meaningful. A summary of the results are plotted in Figure 4. The Jacobian with respect to the z units shown in Figure 4b locally mirror the transformations seen in Figure 3b and c further conﬁrming the hypothesis that this latent space smoothly controls digit style. The slanted style generated as z2 approaches 2σ in Figure 3c is created by applying a gabor- like ﬁlter to vertically oriented parts of the digit as shown in the second column of Figure 4b. Rather than viewing each unit in the next layer individually, we analyze the singular value spectrum of the Jacobian. For h−3, the spectrum is peaked and thus there are a small number of directions with large effect on the image output, so we plot singular vectors with largest singular value. For all digits besides ‘1’, the ﬁrst component seems to create a template digit and the other componets make small style adjustments. For h−2, the spectrum is more degenerate, so we choose a random set of columns from the Jacobian to plot which will better represent the layer’s function. We notice that for each layer moving from the encoder to the output, their contributions become more spatially localized and less semantically meaningful.  Figure 4: a: Jacobians were taken at activation values that lead to these images. z was set to zero for each digit class. b: Gradients of the decoder output with respect to z. c: Singular vectors from the Jacobian from the activations of the ﬁrst layer after {y, z}. d: Column vectors from the Jacobian from the the activations of the second layer after {y, z}. Note that units in the columns for d are not neccesarily the same unit. e: Plots of the normalized singular values for red: the Jacobians with respect to {y, z}, blue: the Jacobians with respect to the activations of the ﬁrst layer after {y, z} (h−3 in Figure 1), and green: the Jacobians with respect to the activations of the second layer after {y, z} (h−2 in Figure 1).  6  abcz1-2σ2σz2-2σ2σabcde3.3 Generating Expression Transformations  We demonstrate similar manipulations on the TFD dataset which contains substantially more com- plex images than MNIST and has far fewer labeled examples. After training, we ﬁnd the latent representation z encodes the subject’s identity, a major factor of variation in the dataset which is not represented by the expression labels. The autoencoder is able to change the expression while pre- serving identity of faces never before seen by the model. We ﬁrst initialize {ˆy, z} with an example from the test set. We then replace ˆy with a new expression label ˆy(cid:48) feeding {ˆy(cid:48), z} to the decoder. Figure 5 shows the results of this process. Expressions can be changed while leaving other facial features largely intact. Similar to the MNIST dataset, we ﬁnd the XCov penalty is not necessary when the dimensionality of z is low (<10). But convergence during training becomes far more difﬁ- cult with such a bottleneck. We achieve much better reconstruction error with the XCov penalty and a high-dimensional z. The XCov penalty simply prevents expression label variation from ‘leaking’ into the latent representation. Figure 5 shows the decoder is unaffected by changes in y without the XCov penalty because the expression variation is distributed across the hundreds of dimensions of z.  3.4 Extrapolating Observed Variables  Previously, we showed the autoencoder learns a smooth continuous latent representation. We ﬁnd a similar result for the observed expression variables despite only being provided their discrete class labels. In Figure 6, we go a step further. We try values for ˆy well beyond those that the encoder could ever output with a softmax activation (0 to 1). We vary the expression variable given to the decoder from -5 to 5. This results in greatly exagerated expressions when set to extreme positive values as seen in Figure 6. Remarkably, setting the variables to extreme negative values results in ‘negative‘ facial expressions being displayed. These negative facial expressions are abstract oppo- sites of their positive counterparts. When the eyes are open in one extreme, they are closed in the opposite extreme. This is consistent regardless of the expression label and holds true for other ab- stract facial features such as open/closed mouth and smiling/frowning face. The decoder has learned a meaningful extrapolation of facial expression structure not explicitly present in the labeled data, creating a smooth semantically sensible space for values of the observed variables completely absent from the class labels.  Figure 5: Left column: Samples from the test set displaying each of the 7 expressions. The expression-labeled columns are generated by keeping the latent variables z constant and changing y (expression). The rightmost set of faces are from a model with no covarriance cost and showcase the importance of the cost in disentangling expression from the latent z variables.  3.5 Manipulating Multiple Factors of Variation  For Multi-PIE, we use two sets of observed factors (camera pose and illumination). As shown in Table 1, we have two softmax layers at the end of the encoder. The ﬁrst encodes the camera pose of the input image and the second the illumination condition. Due to the increased complexity of these images, we made this network substantially deeper (9 layers).  7  originalangerdisgustfearhappysadsurpriseneutralno covariance costFigure 6: For each column, y is set to a one-hot vector and scaled from 5 to -5 from top to bottom, well outside of the natural range of [0,1]. ‘Opposite’ expressions and more extreme expressions can be made.  In Figure 7, we show the images generated by the decoder while iterating through each camera pose. The network was tied to the illumination and latent variables of images from the test set. Although blurry, the generated images preserve the subject’s illumination and identity (i.e. shirt color, hair style, skin tone) as the camera pose changes. In Figure 8, we instead ﬁx the camera position and iterate through different illumination conditions. We also ﬁnd it possible to interpolate between cam- era and lighting positions by simply linearly interpolating the ˆy between two neighboring camera positions supporting the inherent continuity in the class labels.  Figure 7: Left column: Samples from test set with initial camera pose. The faces on the right were generated by changing the corresponding camera pose.  Figure 8: Left column: Samples from test set. Illumination transformations are shown to the right. Ground truth lighting for the ﬁrst face in each block is in the ﬁrst row.  4 Conclusion  With the addition of a supervised cost and an unsupervised cross-covariance penalty, an autoen- coder can learn to disentangle various transformations using standard feedforward neural network  8  angerdisgustfearhappysadsurpriseneutral+-0components. The decoder implicitly learns to generate novel manipulations of images on multi- ple sets of transformation variables. We show deep feedforward networks are capable of learning higher-order factors of variation beyond the observed labels without the need to explicitly deﬁne these higher-order interactions. Finally, we demonstrate the natural ability of these deep networks to learn a continuum of higher-order factors of variation in both the latent and observed variables. Surprisingly, these networks can extrapolate intrinsic continuous variation hidden in discrete class labels. These results gives insight in the potential of deep learning for the discovery of hidden fac- tors of variation simply by accounting for known variation. This has many potential applications in exploratory data analysis and signal denoising.  Acknowledgments  We would like to acknowledge everyone at the Redwood Center for their helpful discussion and comments. We thank Nervana Systems for supporting Brian Cheung during the summer when this project originated and for their continued collaboration. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPUs used for this research. Bruno Olshausen was supported by NSF grant IIS-1111765.  References [1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in Advances  in neural information processing systems, 2012, pp. 1097–1105.  [2] David Eigen, Dilip Krishnan, and Rob Fergus, “Restoring an image taken through a window covered with dirt or rain,” in Computer  Vision (ICCV), 2013 IEEE International Conference on. IEEE, 2013, pp. 633–640.  [3] Joshua B Tenenbaum and William T Freeman, “Separating style and content with bilinear models,” Neural computation, vol. 12, no. 6,  pp. 1247–1283, 2000.  [4] David B Grimes and Rajesh PN Rao, “Bilinear sparse coding for invariant vision,” Neural computation, vol. 17, no. 1, pp. 47–73, 2005. [5] Bruno A Olshausen, Charles Cadieu, Jack Culpepper, and David K Warland, “Bilinear models of natural images,” in Electronic Imaging  2007. International Society for Optics and Photonics, 2007, pp. 649206–649206.  [6] Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang, “Transforming auto-encoders,” in Artiﬁcial Neural Networks and Machine  Learning–ICANN 2011, pp. 44–51. Springer, 2011.  [7] Roland Memisevic and Geoffrey E Hinton, “Learning to represent spatial transformations with factored higher-order boltzmann ma-  chines,” Neural Computation, vol. 22, no. 6, pp. 1473–1492, 2010.  [8] Pietro Berkes, Richard E Turner, and Maneesh Sahani, “A structured model of video reproduces primary visual cortical organisation,”  PLoS computational biology, vol. 5, no. 9, pp. e1000495, 2009.  [9] Charles F Cadieu and Bruno A Olshausen, “Learning intermediate-level representations of form and motion from natural movies,”  Neural computation, vol. 24, no. 4, pp. 827–866, 2012.  [10] Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee, “Learning to disentangle factors of variation with manifold interaction,” in  Proceedings of The 31st International Conference on Machine Learning. ACM, 2014, p. 14311439.  [11] Ruslan Salakhutdinov and Geoffrey E Hinton, “Learning a nonlinear embedding by preserving class neighbourhood structure,” in  International Conference on Artiﬁcial Intelligence and Statistics, 2007, pp. 412–419.  [12] Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling, “Semi-supervised learning with deep generative  models,” in Advances in Neural Information Processing Systems, 2014, pp. 3581–3589.  [13] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio, “Contractive auto-encoders: Explicit invariance during  feature extraction,” in Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 833–840.  [14] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol, “Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,” The Journal of Machine Learning Research, vol. 11, pp. 3371–3408, 2010.  [15] Quoc V Le, “Building high-level features using large scale unsupervised learning,” in Acoustics, Speech and Signal Processing (ICASSP),  2013 IEEE International Conference on. IEEE, 2013, pp. 8595–8598.  [16] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle, “Greedy layer-wise training of deep networks,” Advances in  neural information processing systems, vol. 19, pp. 153, 2007.  [17] Salah Rifai, Yoshua Bengio, Aaron Courville, Pascal Vincent, and Mehdi Mirza, “Disentangling factors of variation for facial expression  recognition,” in Computer Vision–ECCV 2012, pp. 808–822. Springer, 2012.  [18] Matthew D Zeiler, “Adadelta: An adaptive learning rate method,” arXiv preprint arXiv:1212.5701, 2012. [19]  Ian J Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Fr´ed´eric Bastien, and Yoshua Bengio, “Pylearn2: a machine learning research library,” arXiv preprint arXiv:1308.4214, 2013. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, “Generative adversarial nets,” in Advances in Neural Information Processing Systems, 2014, pp. 2672–2680.  [20]  [21] Yann LeCun and Corinna Cortes, “The mnist database of handwritten digits,” 1998. [22]  Ian J Goodfellow, David Warde-farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio, “Maxout networks,” in Proceedings of the 30th International Conference on Machine Learning. ACM, 2013, pp. 1319–1327.  [23] J. Susskind, A. Anderson, and G. E. Hinton, “The toronto face database,” Tech. Rep., University of Toronto, 2010.  9  [24] Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade, and Simon Baker, “Multi-pie,” Image and Vision Computing, vol. 28, no. 5,  pp. 807–813, 2010.  [25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and  Andrew Rabinovich, “Going deeper with convolutions,” arXiv preprint arXiv:1409.4842, 2014.  [26] Jascha Sohl-Dickstein, Ben Poole, and Surya Ganguli, “Fast large-scale optimization by unifying stochastic gradient and quasi-newton  methods,” in Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 604–612.  [27] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, “Dropout: A simple way to prevent  neural networks from overﬁtting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.  [28] Olivier Breuleux, Yoshua Bengio, and Pascal Vincent, “Quickly generating representative samples from an rbm-derived process,” Neural  Computation, vol. 23, no. 8, pp. 2058–2073, 2011.  [29] Yoshua Bengio, Gregoire Mesnil, Yann Dauphin, and Salah Rifai, “Better mixing via deep representations,” in Proceedings of The 30th  International Conference on Machine Learning, 2013, pp. 552–560.  [30] Yoshua Bengio, Eric Laufer, Guillaume Alain, and Jason Yosinski, “Deep generative stochastic networks trainable by backprop,” in  Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 226–234.  10  ",
1412.7004,2015, Tailoring Word Embeddings for Bilexical Predictions: An Experimental Comparison,"['Tailoring Word Embeddings for Bilexical Predictions: An Experimental Comparison', 'Pranava Swaroop Madhyastha', 'Xavier Carreras', 'and Ariadna Quattoni']",https://arxiv.org/pdf/1412.7004,"5 1 0 2    r p A 0 1         ] L C . s c [      2 v 4 0 0 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  TAILORING WORD EMBEDDINGS FOR BILEXICAL PREDICTIONS: AN EXPERIMENTAL COMPARISON  Pranava Swaroop Madhyastha Universitat Polit`ecnica de Catalunya Campus Nord UPC, 08034 Barcelona pranava@cs.upc.edu  Xavier Carreras Xerox Research Centre Europe 38240 Meylan, France xavier.carreras@xrce.xerox.com  Ariadna Quattoni Xerox Research Centre Europe 38240 Meylan, France ariadna.quattoni@xrce.xerox.com  ABSTRACT  We investigate the problem of inducing word embeddings that are tailored for a particular bilexical relation. Our learning algorithm takes an existing lexical vector space and compresses it such that the resulting word embeddings are good predictors for a target bilexical relation. In experiments we show that task-speciﬁc embeddings can beneﬁt both the quality and efﬁciency in lexical prediction tasks.  1  INTRODUCTION  There has been a large body of work that focuses on learning word representations, either in the form of word clusters (Brown et al., 1992) or vectors (Sahlgren, 2006; Turney & Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Baroni et al., 2014; Bansal et al., 2014) and these have proven useful in many NLP applications (Koo et al., 2008; Turian et al., 2010). An ideal lexical representation should compress the space of lexical words while retaining the es- sential properties of words in order to make predictions that correctly generalize across words. The typical approach is to ﬁrst induce a lexical representation in a task-agnostic setting and then use it in different tasks as features. A different approach is to learn a lexical representation tailored for a certain task. In this work we explore the second approach, and employ the formulation by Mad- hyastha et al. (2014) to induce task-speciﬁc word embeddings. This method departs from a given lexical vector space, and compresses it such that the resulting word embeddings are good predictors for a given lexical relation. Speciﬁcally we learn functions that compute compatibility scores between pairs of lexical items under some linguistic relation. In our work, we refer to these functions as bilexical operators. As an instance of this problem, consider learning a model that predicts the probability that an adjective modiﬁes a noun in a sentence. In this case, we would like the bilexical operator to capture the fact that some adjectives are more compatible with some nouns than others. Given the complexity of lexical relations, one expects that the properties of words that are relevant for some relation are different for another relation. This might affect the quality of an embedding, both in terms of its predictive power and the compression it obtains. If we employ a task-agnostic low-dimensional embedding, will it retain all important lexical properties for any relation? And, given a ﬁxed relation, can we further compress an existing word representation? In this work we present experiments along these lines that conﬁrm that task-speciﬁc embeddings can beneﬁt both the quality and the efﬁciency of lexicalized predictive models.  1  Accepted as a workshop contribution at ICLR 2015  (1)  2 FORMULATION Let V be a vocabulary, and let x ∈ V denote a word. We are interested in modeling a target bilexical relation, that is, a relation between pairs of words without context. For example, in a noun-adjective relation we model what nouns can be assigned to what adjectives. We will denote as Q ⊆ V the set of query words, or words that appear in the left side of the bilexical relation. And we will use C ⊆ V to denote candidate words, appearing in the right side of the relation. In this paper we experiment with the log-linear models by Madhyastha et al. (2014) that given a query word q compute a conditional distribution over candidate words c. The models take the following form:  Pr(c | q; W ) =  (cid:80) exp{φ(q)(cid:62)W φ(c)} c(cid:48) exp{φ(q)(cid:62)W φ(c(cid:48))}  where φ : V → Rn is a distributional representation of words, and W ∈ Rn×n is a bilinear form. The learning problem is to obtain φ and W from data, and we approach it in a semi-supervised fash- ion. There exist many approaches to learn φ from unlabeled data, and in this paper we experiment with two approaches: (a) a simple distributional approach where we represent words with a bag-of- words of contextual words; and (b) the skip-gram model by Mikolov et al. (2013). To learn W we as- sume access to labeled data in the form pairs of compatible examples, i.e. D = {(q, c)1, . . . , (q, c)l}, where q ∈ Q and c ∈ C. The goal is to be able to predict query-candidate pairs that are unseen during training. Recall that we model relations between words without context. Thus the lexical representation φ is essential to generalize to pairs involving unseen words. With φ ﬁxed, we learn W by minimizing the negative log-likelihood of the labeled data using a reg-  ularized objective, L(W ) = −(cid:80)  (q,c)∈D log Pr(c | q; W )+τ ρ(W ), where ρ(W ) is a regularization  penalty and τ is a constant controlling the trade-off. We are interested in regularizers that induce low-rank parameters W , since they lead to task-speciﬁc embeddings. Assume that W has rank k, such that W = U V (cid:62) with U, V ∈ Rn×k. If we consider the product φ(q)(cid:62)U V (cid:62)φ(c), we can now interpret φ(cid:62)U as a k-dimensional embedding of query words, and φ(c)(cid:62)V as a k-dimensional embedding of candidate words. Thus, if we obtain a low- rank W that is highly predictive, we can interpret U and V as task-speciﬁc compressions of the original embedding φ tailored for the target bilexical relation, from n to k dimensions. Since minimizing the rank of a matrix is hard, we employ a convex relaxation based on the nuclear norm of the matrix (cid:96)(cid:63) (that is, the (cid:96)1 norm of the singular values, see Srebro et al. (2005)). In our experiments we compare the low-rank approach to (cid:96)1 and (cid:96)2 regularization penalties, which are common in linear prediction tasks. For all settings we use the forward-backward splitting (FOBOS) optimization algorithm by Duchi & Singer (2009). We note that if we set W to be the identity matrix our model scores are inner products between the query-candidate embeddings, a common approach to evaluate semantic similarity in unsupervised distributional approaches. In general, we can compute a low-dimensional projection of φ down to k dimensions, using SVD, and perform the inner product in the projected space. We refer to this as the unsupervised approach, since the projected embeddings do not use the labeled dataset specifying the target relation.  3 EXPERIMENTS WITH SYNTACTIC RELATIONS  We conducted a set of experiments to test the performance of the learning algorithm with respect to the initial lexical representation φ, for different conﬁgurations of the representation and the learner. We experiment with six bilexical syntactic relations using the Penn Treebank corpus (Marcus et al., 1993), following the experimental setting by Madhyastha et al. (2014). For a relation between queries and candidate words, such as noun-adjective, we partition the query words into train, devel- opment and test queries, thus test pairs are always unseen pairs. To report performance, we measure pairwise accuracy with respect to the efﬁciency of the model in terms of number of active parameters. To measure the efﬁciency of a model we consider the number of double operations that are needed to compute, given a query word, the scores for all candidates in the vocabulary. See (Madhyastha et al., 2014) for details.  2  Accepted as a workshop contribution at ICLR 2015  (a) Adjective-Noun  (b) Noun-Adjective  (c) Verb-Object  (d) Object-Verb  (e) Subject-Verb  (f) Verb-Subject  Figure 1: Pairwise accuracy v/s no. of double operations to compute the distribution over candidate words for a query word. Plots are for noun-adjective, verb-object and verb-subject relations, in both directions. The red curves use distributional representation based on bag-of-words (BoW) and the grey curves use the embeddings of the skip-gram model (SKG).  3  10310410510610710810910101011numberofoperations5060708090pairwiseaccuracyl⇤BoWl2BoWl1BoWUBoWl⇤SKGl2SKGl1SKGUSKG1031041051061071081091010numberofoperations5060708090pairwiseaccuracy1031041051061071081091010numberofoperations5060708090pairwiseaccuracy10310410510610710810910101011numberofoperations5060708090pairwiseaccuracy10310410510610710810910101011numberofoperations5060708090pairwiseaccuracy10310410510610710810910101011numberofoperations5060708090pairwiseaccuracy10310410510610710810910101011numberofoperations5060708090pairwiseaccuracyAccepted as a workshop contribution at ICLR 2015  Type  Obj-Verb  Rel UNS Adj-Noun BoW 85.12 SKG 73.61 BoW 63.85 SKG 64.15 Subj-Verb BoW 58.20 SKG 49.65 Noun-Adj BoW 78.09 SKG 49.65 BoW 66.46 SKG 64.15 Verb-Subj BoW 49.32 SKG 32.34  Verb-Obj  best k  85.99 (80) 91.40 (300) 78.11 (200) 79.98 (50) 78.13 (2) 59.28 (90) 78.11 (70) 59.28 (50) 73.90 (40) 79.99 (30) 71.97 (30) 53.75 (2)  (cid:96)(cid:63)  k = 5 83.99 83.70 73.17 75.45 71.71 53.31 77.48 56.42 73.70 77.05 71.71 53.32  k = 10 84.74 86.27 73.64 78.37 71.73 53.32 77.85 57.19 73.88 78.60 71.23 53.32  (cid:96)2  85.96 91.22 74.08 80.30 78.07 58.24 78.48 58.24 73.30 80.29 72.85 53.47  (cid:96)1  85.63 90.72 73.95 79.89 77.97 58.67 78.85 58.67 73.48 79.89 71.95 53.68  Table 1: Pairwise accuracies for the six relations using the unsupervised, (cid:96)(cid:63), (cid:96)2 and (cid:96)1 models, using either a distributional bag-of-words representation (BoW) or the skip-gram embeddings (SKG) as initial representation. For (cid:96)(cid:63) we show results for the rank that gives best accuracy (with the optimal rank in parenthesis), as well as for ranks k = 5 and 10.  We experiment with two types of initial representations φ. The ﬁrst is a simple high-dimensional distributional representation based on contextual bag-of-words (BoW): each word is represented by the bag of words that appear in contextual windows. In our experiments these were sparse 2,000- dimensional vectors. The second representation are the low-dimensional skip-gram embeddings (SKG) by Mikolov et al. (2013), where we used 300 dimensions. In both cases we induced such representations using the BLIPP corpus (Charniak et al., 2000) and using a context window of size 10 for both. Thus the main difference is that the bag-of-words is an uncompressed representation, while the skip-gram embeddings are a neural-net-style compression of the same contextual windows. As for the bilexical model, we test it under three regularization schemes, namely (cid:96)2, (cid:96)1, and (cid:96)(cid:63). For the ﬁrst two, the efﬁciency of computing predictions is a function of the non-zero entries in W , while for the latter it is the rank k of W , which deﬁnes the dimension of the task-speciﬁc embeddings. We also test a baseline unsupervised approach (UNS).  4 RESULTS AND DISCUSSION  Figure 1 shows the performance of models for noun-adjective, verb-object and verb-subject relations (in both directions). In line with the results by Madhyastha et al. (2014) we observe that the super- vised approach in all cases outperforms the unsupervised case, and that the nuclear norm scheme provides the best performance in terms of accuracy and speed: other regularizers can obtain similar accuracies, but low-rank constraints during learning favor very-low dimensional embeddings that are highly predictive. In terms of starting with bag-of-words vectors or skip-gram embeddings, in three relations the former is clearly better, while in the other three relations the latter is clearly better. We conclude that task- agnostic embeddings do identify useful relevant properties of words, but at the same time not all necessary properties are retained. In all cases, the nuclear norm regularizer successfully compresses the initial representation, even for the embeddings which are already low-dimensional. Table 1 presents the best result for each relation, initial representation and regularization scheme. Plus, for the (cid:96)(cid:63) regularizer we present results at three different ranks, namely 5, 10 or the rank that obtains the best result for each relation. These highly compressed embeddings perform nearly as good as the best performing model for each relation. Table 2 shows a set of query nouns, and two sets of neighbor query nouns, using the embeddings for two different relations to compute the two sets. We can see that, by changing the target relation, the set of close words changes. This suggests that words have a wide range of different behaviors, and different relations might exploit lexical properties that are speciﬁc to the relation.  4  Accepted as a workshop contribution at ICLR 2015  Query city securities board debt law director  noun-adjective  object-verb  province, area, island, township, freeways bonds, mortgage, issuers, debt, loans committee directors, commission, nominees, refusal loan, loans, debts, ﬁnancing, mortgage laws, constitution, code, legislation, immigration assistant, editor, treasurer, postmaster, chairman  residents, towns, marchers, streets, mayor bonds, memberships, equities, certiﬁcates, syndicate slate, membership, committee, appointment, stockholder reinvestment, indebtedness, expenditures, outlay, repayment laws, ordinance, decree, statutes, state ﬁrm, consultant, president, manager, leader  Table 2: Example query words and 5 highest-ranked candidate words for two different bilexical relations: noun-adjective and object-verb.  5 CONCLUSION  We have presented a set of experiments where we compute word embeddings speciﬁc to target linguistic relations. We observe that low-rank penalties favor embeddings that are good both in terms of predictive accuracy and efﬁciency. For example, in certain cases, models using very low- dimensional embeddings perform nearly as good as the best models. In certain tasks, we have shown that we can reﬁne low-dimensional skip-gram embeddings, making them more compressed while retaining their predictive properties. In other tasks, we have shown that our method can improve over skip-gram models when starting from uncompressed distributional representations. This suggests that skip-gram embeddings do not retain all the necessary information of the original words. This motivates future research that aims at general-purpose embeddings that do retain all necessary properties, and can be further compressed in light of speciﬁc lexical relations.  ACKNOWLEDGEMENTS  We thank the reviewers for their helpful comments. This work has been partially funded by the Spanish Government through the SKATER project (TIN2012-38584-C06-01).  REFERENCES Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Tailoring continuous word representations for  dependency parsing. In Proceedings of ACL, 2014.  Baroni, Marco, Dinu, Georgiana, and Kruszewski, Germ´an. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 238–247, Baltimore, Maryland, June 2014. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P14-1023.  Brown, Peter F., deSouza, Peter V., Mercer, Robert L., Pietra, Vincent J. Della, and Lai, Jenifer C. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479, 1992. Charniak, Eugene, Blaheta, Don, Ge, Niyu, Hall, Keith, and Johnson, Mark. BLLIP 1987–89 WSJ  Corpus Release 1, LDC No. LDC2000T43. Linguistic Data Consortium, 2000.  Duchi, John and Singer, Yoram. Efﬁcient online and batch learning using forward backward split-  ting. Journal of Machine Learning Research, 10:2899–2934, 2009.  Koo, Terry, Carreras, Xavier, and Collins, Michael. Simple semi-supervised dependency parsing. In Proceedings of ACL-08: HLT, pp. 595–603, Columbus, Ohio, June 2008. Association for Compu- tational Linguistics. URL http://www.aclweb.org/anthology/P/P08/P08-1068. Madhyastha, Swaroop Pranava, Carreras, Xavier, and Quattoni, Ariadna. Learning task-speciﬁc bilexical embeddings. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pp. 161–171. Dublin City University and As- sociation for Computational Linguistics, 2014. URL http://aclweb.org/anthology/ C14-1017.  Marcus, Mitchell P., Santorini, Beatrice, and Marcinkiewicz, Mary A. Building a Large Annotated  Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993.  5  Accepted as a workshop contribution at ICLR 2015  Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efﬁcient estimation of word repre-  sentations in vector space. arXiv preprint arXiv:1301.3781, 2013.  Pennington, Jeffrey, Socher, Richard, and Manning, Christopher. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pp. 1532–1543. Association for Computational Linguistics, 2014. URL http://aclweb.org/anthology/D14-1162.  Sahlgren, Magnus. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces. PhD thesis, Stock- holm University, 2006.  Srebro, Nathan, Rennie, Jason D. M., and Jaakola, Tommi S. Maximum-margin matrix factorization.  In Advances in Neural Information Processing Systems 17, pp. 1329–1336. MIT Press, 2005.  Turian, Joseph, Ratinov, Lev-Arie, and Bengio, Yoshua. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Associ- ation for Computational Linguistics, pp. 384–394, Uppsala, Sweden, July 2010. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P10-1040.  Turney, Peter D. and Pantel, Patrick. From frequency to meaning: Vector space models of semantics. Journal of Artiﬁcial Intelligence Research, 37(1):141–188, January 2010. ISSN 1076-9757. URL http://dl.acm.org/citation.cfm?id=1861751.1861756.  6  ",
1412.6881,2015, On Learning Vector Representations in Hierarchical Label Spaces,"['On Learning Vector Representations in Hierarchical Label Spaces', 'Jinseok Nam and Johannes Fürnkranz']",https://arxiv.org/pdf/1412.6881,"5 1 0 2    r p A 6 1         ]  G L . s c [      3 v 1 8 8 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  ON LEARNING VECTOR REPRESENTATIONS IN HIERARCHICAL LABEL SPACES  Jinseok Nam1,2, Johannes F¨urnkranz1 1 Knowledge Engineering Group Department of Computer Science Technische Universit¨at Darmstadt 2 Knowledge Discovery in Scientiﬁc Literature German Institute for Educational Research {nam@cs, juffi@ke}.tu-darmstadt.de  ABSTRACT  An important problem in multi-label classiﬁcation is to capture label patterns or underlying structures that have an impact on such patterns. This paper addresses one such problem, namely how to exploit hierarchical structures over labels. We present a novel method to learn vector representations of a label space given a hierarchy of labels and label co-occurrence patterns. Our experimental results demonstrate qualitatively that the proposed method is able to learn regularities among labels by exploiting a label hierarchy as well as label co-occurrences. It highlights the importance of the hierarchical information in order to obtain reg- ularities which facilitate analogical reasoning over a label space. We also ex- perimentally illustrate the dependency of the learned representations on the label hierarchy.  1  INTRODUCTION  Multi-label classiﬁcation is an area of machine learning which aims to learn a function that maps instances to label spaces. In contrast to multi-class classiﬁcation, each instance is assumed to be associated with more than one label. One of the goals in multi-label classiﬁcation is to model underlying structures over a label space because in many such problems, the occurrence of labels is not independent of each other. Many attempts have been made to capture and exploit label structures (F¨urnkranz et al., 2008; Hsu et al., 2009; Read et al., 2011; Dembczy´nski et al., 2012). As the number of possible conﬁgurations of labels grows exponentially with respect to the number of labels, it is required for multi-label classiﬁer to handle many labels efﬁciently (Bi & Kwok, 2013). Not only multi-label classiﬁers but also human annotators need a way to handle a large number of labels efﬁciently. Thus, human experts put a lot of effort into maintaining and updating hierarchies of labels and such hierarchies are used to generate the ground truth for training classiﬁers, and many methods have been developed to use hierarchical output structures in machine learning (Silla Jr. & Freitas, 2011). In particular, several researchers have looked into utilizing the hierarchical structure of the label space for improved predictions in multi-label classiﬁcation (Rousu et al., 2006; Vens et al., 2008; Zimek et al., 2010; Bi & Kwok, 2011). In this work, we present a novel method to efﬁciently learn from hierarchical structures over labels as well as occurrences of labels, and investigate the importance of hierarchical structures to identify internal structures of labels. The rest of the paper is organized as follows. In Section 2, we deﬁne multi-label classiﬁcation and hierarchical structures of labels in a graph. We then introduce our proposed method that learns label spaces while taking into account label hierarchies. In Section 4, we set up our experiments, and present empirical analysis in Section 5. Finally, we discuss ﬁndings from our experiments and provide directions for future work in Section 6.  1  Accepted as a workshop contribution at ICLR 2015  2 MULTI-LABEL CLASSIFICATION WITH LABEL HIERARCHIES  Throughout this work we will present a method to learn representations of labels in multi-label classiﬁcation. Firstly, in this section we deﬁne multi-label classiﬁcation and notation for label hier- archies. Notation. Multi-label classiﬁcation refers to the task of learning a function that maps instances x to label sets Y ⊆ {1, 2,··· , L} given a training dataset D = {(xn,Yn)}N n=1, where N is the number of instances and L is the number of labels. Consider multi-label problems where label hierarchies exist. Label graphs are a natural way to represent such hierarchical structures. Because it is possible for a label to have more than one parent node, we represent a hierarchy of labels in a directed acyclic graph (DAG). Consider a graph G = {V, E} where V denotes a node and E represent a connection between two nodes. Each node corresponds a label and an edge represents a parent-child relationship between the two labels in the hierarchy. A node u ∈ V corresponds to a label. If there exists a directed path from u to v, u is an ancestor of v and v is a descendant of u. The set of all ancestors of v is denoted as SA(v), and the set of all descendants is denoted as SD(u). For large-scale multi-label classiﬁcation problems, high dimensionality of label spaces makes learn- ing classiﬁers difﬁcult. In the literature, several work (Hsu et al., 2009; Tai & Lin, 2012; Chen & Lin, 2012; Yu et al., 2014) have been proposed to tackle the problem by reducing the dimensionality of label spaces in which label co-occurrence patterns play a crucial role. In language modeling, another area of research where high dimensionality issues arise more severely, one can deal with the problem by representing words as dense vectors in Rd and then to learn these representations in a way of maximizing a conditional probability of a word given the context of the word (Bengio et al., 2003). Recently, for learning word representations, Mikolov et al. (2013) have proposed a very efﬁcient log-linear model relying on co-occurrence patterns of words given the ﬁxed-length contexts and have shown interesting properties of word representations by analogical reasoning. In the following Section, we introduce a novel approach to reduce the dimensionality of label spaces by adapting the log-linear models capable of handling the large-scale problems efﬁciently.  3 THE LOG-LINEAR MODEL  As stated in Section 1, there are several work making use of hierarchical structures in label spaces for improving predictive performance of multi-label classiﬁers. This is because label hierarchies are often built and maintained by human experts, and can be used as an additional source of information to identify internal structures. Thus, we propose a log-linear model to reduce the dimensionality of label spaces by exploiting label co-occurrences and hierarchical relations between labels. Formally, the basic idea is to maximize a probability of predicting an ancestor given a particular label in a hierarchy as well as predicting co-occurring labels with it. Given the labels of N train- ing instances DY = {Y1,Y2,··· ,YN}, the objective function L is to maximize the average log probability given by   1  ZA  N(cid:88)  n=1  (cid:88)  (cid:88)  i∈Yn  j∈SA(i)  (cid:88)  (cid:88)  i∈Yn  k∈Yn k(cid:54)=i    L (Θ;DY ) =  log p (yj|yi) +  1 ZN  log p (yk|yi)  (1)  where ZA = |Yn||SA(·)| and ZN = |Yn|(|Yn| − 1), and Θ is a set of parameters (to be introduced shortly). The probability of predicting an ancestor label j of a label i, i.e., p(yj|yi) in Equation 1, can be computed by softmax as follows  p(yj|yi) =  (cid:80)  exp(u(cid:48)T j ui) l∈L exp(u(cid:48)T  l ui)  (2)  j is a j-th column vector of U(cid:48) ∈ Rd×L and ui is a i-th column vector of U ∈ Rd×L. where u(cid:48) Here, U and U(cid:48) correspond to continuous vector representations for input labels and output labels, respectively, and they are used as parameters of Equation 1, that is, Θ = {U, U(cid:48)}. This enables for labels that share the same ancestors or co-occurring labels to have similar representations. The softmax function in Equation 2 can be viewed as an objective function of a neural network consisting  2  Accepted as a workshop contribution at ICLR 2015  of a linear activation function in the hidden layer and two weights {U, U(cid:48)}, where U connects the input layer to the hidden layer while U(cid:48) is used to convey the hidden activations to the output layer. Thus, the log of a probability of predicting a label j given a label i in Eq 2, i.e., ej|i (cid:44) log p(yj|yi), can be obtained in a neural network framework:  hi = Uyi oj|i = U(cid:48)T·jhi ej|i = oj|i − log  (cid:88)  k  exp(ok|i)  (3) (4)  (5)  (t+1) = U(cid:48)  (t) + α∇U(cid:48)  where yi is a L dimensional vector whose i-th element is set to one and zero elsewhere and hi is hidden activation of a label i. Note that since only i-th element of yi is one, hi is equal to ui in Eq 2. The probability of predicting a label k, which is assigned together with a label i to a label set Yn, p(yk|yi) is computed in the same way meaning that it is parameterized by U and U(cid:48) as well. The parameters are updated as follows U(t+1) = U(t) + α∇U(t)L, U(cid:48) (t)L where α denotes a learning rate. Due to computational cost for ∇U(cid:48) (t)L, we use hierarchical softmax (Morin & Bengio, 2005; Mnih & Hinton, 2009) which reduces the gradient computing cost from O(L) to O(log L). Similar to Mikolov et al. (2013), in order to make use of the hierarchical softmax, a binary tree is constructed by Huffman coding giving binary codes with variable length to each label according to |SD(·)|. In other words, the more descendants a label has in a hierarchy, the shorter codes it is assigned. Note that by deﬁnition of the Huffman coding all labels correspond to leaf nodes in the binary tree, namely Huffman tree, and there are L − 1 internal nodes in the Huffman tree. Instead of computing L outputs, the hierarchical softmax computes a probability of (cid:100)log L(cid:101) binary decisions over a path from the root node of the tree to the leaf node corresponding to a target label, say, yj in Equation 2. More speciﬁcally, let C(y) be a codeword of a label y by the Huffman coding, where each bit can be either 0 or 1, and I(C(y)) be the number of bits in the codeword for that label. Cl(y) is the l-th bit in y’s codeword. Unlike the softmax, in computing the hierarchical softmax we use the output label representations U(cid:48) as vector representations for inner nodes in the Huffman tree. The hierarchical softmax is given by  I(C(yj ))(cid:89)  p(yj|yi) =  σ((cid:74)Cl(yj) = 0(cid:75) u(cid:48)T  n(l,yj )ui)  l=1  where σ(·) is the logistic function,(cid:74)·(cid:75) denotes a function taking 1 if its argument is true, otherwise  -1, and u(cid:48) n(l,yj ) is a vector representation for the l-th node in the path from the root node to the node corresponding to the label yj in the Huffman tree. While L inner products are required to compute the normalization term in Equation 2, the hierarchical softmax needs I(C(·)) times com- putations. Hence, the hierarchical softmax allows substantial improvements in computing gradients if E [I(C(·))] (cid:28) L.  (6)  4 EXPERIMENTAL SETUP  We carried out all experiments on the BioASQ Task 2a dataset in which 12 millions of training documents and approximately 27,000 index terms are given with parent-child pairs deﬁned over index terms.1 The index terms are known as Medical Subject Headings (MeSH), which is controlled and updated continually by National Library of Medicine (NLM), and used to index articles for the MEDLINE and PubMed databases. Representing parent-child pairs in a DAG. It is often the case that if we represent parent-child pairs of labels as a graph, in real-world problems, it contains cycles. Sometimes, it is reasonable to assume that these cycles result from wrong annotations due to complex relationship between labels with different abstract levels. Additionally, cycles in a graph also introduce other difﬁculties to learning algorithms. For instance, if we want to visit all ancestors given a node, where the node has a direct edge to one of its ancestors, such graph traversal never ends unless stopping criteria are used. Hence, we remove edges resulting in cycles as follows:  1http://www.bioasq.org  3  Accepted as a workshop contribution at ICLR 2015  Figure 1: Learned representations of 16 major categories in MeSH vocabulary. Projection of label representations into 2D is done by tSNE (Van der Maaten & Hinton, 2008).  • Pick a node that has no parent as a starting node. • Run Depth-First Search (DFS) from the starting node in order to detect edges pointing to • Repeat the above steps until all nodes having no parent are visited.  nodes visited already, then remove such edges.  The above preprocessing steps left us 46,455 parent-child pairs by removing 3,461 ones, from which we obtain a DAG-structured label hierarchy. Training details. We set the dimension of vector representations d to 128 and the learning rate α is ﬁxed to 0.1 during training. It took about 3 hours to iterate 50 times over 12 millions of training instances on Intel Xeon E5-2620 equipped with 24 threads.  5 EXPERIMENTAL RESULTS  The hierarchy of MeSH vocabulary consists of 16 major categories. Each index term belongs to at least one major category. After training the log-linear model on the BioASQ dataset, we selected labels corresponding terminals and belonging to a single major category in order to visualize learned representations. Figure 1 shows that using a hierarchy and co-occurrences of labels the model sepa- rates reasonably well 16 major categories deﬁned in MeSH vocabulary. Please note that whereas we use tSNE for better visualization in Figure 1, Principal Component Analysis is used to project label representations for the rest of ﬁgures in this paper. In the following section, we investigate what label representations learn from a hierarchy and co-occurrences.  5.1 ENCODING HIERARCHICAL STRUCTURES  As an illustration of our results, we will focus on a subgraph related to health care, shown in the top of Figure 2. Consider the leaf nodes in the ﬁgure. According to our objective in Equation 1, Urban Health, Suburban Health and Rural Health are trained to have representations for predicting their common ancestors, i.e., Health, Population Characteristics and Health care category. The child nodes of Population are also trained similarly. Although Urban, Suburban and Rural Health are separated from Urban, Suburban and Rural Population, their representations tend to be similar since they share the same ancestors. In other words, Urban Health and Rural Population, for example, should have somewhat similar representations in part so as to increase a probability of predicting their common ancestors even though they are rarely assigned to the same instance.  4  Accepted as a workshop contribution at ICLR 2015  (a)  (b)  (c)  Figure 2: (a) A part of the hierarchy related to health care. (b) Learned vector representations for the index terms at leaf without the hierarchy (manually rotated). (c) Learned vector representations for the same index terms with the hierarchy.  We did perform an experiment to see whether learning from only the co-occurrences yields mean- ingful structures. In this case, our objective function is limited to the right term in Equation 1. Co-occurrence information enables to learn internal structures (see Figure 2b). If we consider hier- archical relationship between labels as well as co-occurrences of them, a more interesting property of our proposed method can be observed. A major difference between Figure 2b and 2c are the inter- and intra-group relationships among labels at the leaves. Speciﬁcally, all child nodes of Health are located in the left side and those of Population appear in the opposite side (Figure 2c). In addition to such relationship between two groups, relations between labels belonging to Health (on the left) resemble those found among the children of Population (on the right). Figure 3 shows how important it is to use hierarchical information for capturing regularities of the labels. It is likely that label pairs that co-occur frequently are close to each other (see Figure 3a and 3c). In particular, Figure 3a illustrates that Urban Population is close to Urban Health and Rural Population because Urban Population is often assigned together with Urban Population and Rural Population to a document. Likewise, as shown in Figure 3c, each therapy is close by a disorder for which the therapy is an effective treatment. If we make use of hierarchical information during training the model, it reveals more interesting relationship which is not observed from the model trained on only co-occurrences. Figure 3b shows that there is a strong directed relation from Urban to Rural which can be represented by a direction vector pointing upper-left. We can say the learned vector representations has identiﬁed Therapy-Disorders/Diseases relationship (Figure 3d).  5.2 ANALOGICAL REASONING  One way to evaluate representation quality is analogical reasoning as shown in (Mikolov et al., 2013). For example, one could represent the analogy that a king is to a queen like a man to a woman with a qualitative equation such as “king - man + woman ≈ queen.” Upon the observations in Section 5.1, we performed analogical reasoning on both the representations trained with the hierarchy and ones without the hierarchy, speciﬁcally, regarding Therapy-Disorders/Diseases relationship (Table 1). As expected, it seems like the label representations trained with the hierarchy are clearly advan-  5  Population CharacteristicsHealthPopulationDemographySocioeconomic FactorsUrban  HealthSuburban HealthRural Health…UrbanPopulationSuburban PopulationRural PopulationHealth care category0.40.20.00.20.40.30.20.10.00.10.20.30.4Rural HealthSuburban HealthUrban HealthRural PopulationSuburban PopulationUrban PopulationHealthPopulation0.40.20.00.20.40.30.20.10.00.10.20.3Rural HealthSuburban HealthUrban HealthRural PopulationSuburban PopulationUrban PopulationHealthPopulationAccepted as a workshop contribution at ICLR 2015  (a)  (b)  (c)  (d)  Figure 3: (a) & (c) labels’ representations learned from only co-occurrences (b) & (d) from a hier- archy as well as co-occurrences  tageous to ones trained without the hierarchy on analogical reasoning. To be more speciﬁc, consider the ﬁrst example, where we want to know what kinds of therapies are effective on “Respiration Disorders” as the relationship between “Diet Therapy” and “Cardiovascular Diseases.” When we perform such analogical reasoning using learned representations with the hierarchy, the most prob- able answers to this analogy question are therapies that can used to treat “Respiration Disorders” including nutritional therapy. Unlike the learned representations with the hierarchy, it is likely that the label representations learned without the hierarchy perform poorly on this type of tasks. However, we could not observe such regularities from analogical reasoning questions in terms of parent-child relationships. For instance, it was expected that either “Urban Health - Health + Popu- lation” or “Urban Health - Health -Population Characteristics + Population” results in representa- tions close to Urban Population, but the answers of such questions did not end up with something close to Urban Population. We conjecture that this is because our proposed method only encodes one-way hierarchical dependence of labels. In other words, each index term is trained to predict its ancestors, but not to predict its descendants.  5.3 DIFFERENT HIERARCHIES, DIFFERENT REPRESENTATIONS  In the previous section, we have shown our proposed method is capable of capturing hierarchical structures over labels and co-occurrences of them. In this case, it can be expected that the learned representations change when some part of the hierarchy is changed while the label co-occurrences remains same. To answer this question, ﬁrstly, we modiﬁed the original hierarchy (left in Figure 4a) so as to obtain the new hierarchy (right in Figure 4a). Originally, Population Characteristics has two child nodes, Health and Population. Contrary to Population that has only three child nodes, Health has dozens of child nodes. Instead of removing the other nodes from the hierarchy, we kept them as they are in the hierarchy. However, Population was removed from the hierarchy. We then created three new internal nodes, Urban, Suburban and Rural representing types of developed environments. Finally, all nodes of interest (in blue and red) were grouped according to the developed types.  6  0.80.60.40.20.00.20.40.60.81.00.40.20.00.20.4Rural HealthUrban HealthRural PopulationUrban PopulationRural HospitalsUrban Hospitals0.80.60.40.20.00.20.40.60.81.00.40.20.00.20.4Rural HealthUrban HealthRural PopulationUrban PopulationRural HospitalsUrban Hospitals0.80.60.40.20.00.20.40.60.80.80.60.40.20.00.20.40.60.8Respiratory_TherapySpeech_TherapyArt_TherapyAcupuncture_TherapyBehavior_TherapyDiet_TherapyRespiration_DisordersSpeech_DisordersPost_TraumaticStress_DisordersSleep_DisordersMental_DisordersCardiovascular_DiseasesTherapyDisorders/Diseases0.60.40.20.00.20.40.60.80.60.40.20.00.20.40.6Respiratory_TherapySpeech_TherapyArt_TherapyAcupuncture_TherapyBehavior_TherapyDiet_TherapyRespiration_DisordersSpeech_DisordersPost_TraumaticStress_DisordersSleep_DisordersMental_DisordersCardiovascular_DiseasesTherapyDisorders/DiseasesAccepted as a workshop contribution at ICLR 2015  Table 1: Analogical reasoning on learned vector representations of MeSH vocabulary  Analogy questions  Most probable answers  On learned representations using the hierarchy  Cardiovascular Diseases : Diet Therapy ≈ Respiration Disorders : ?  Mental Disorders : Behavior Therapy ≈ Post Traumatic Stress Disorders : ?  Cardiovascular Diseases : Diet Therapy ≈ Respiration Disorders : ?  Mental Disorders : Behavior Therapy ≈ Post Traumatic Stress Disorders : ?  Diet Therapy  Enteral Nutrition  Gastrointestinal Intubation Total Parenteral Nutrition  Parenteral Nutrition Respiratory Therapy Behavior Therapy Cognitive Therapy  Rational-Emotive Psychotherapy  Brief Psychotherapy  Psychologic Desensitization  Implosive Therapy  Respiration Disorders  Respiratory Tract Diseases  Respiratory Sounds Airway Obstruction  Hypoventilation  Croup  Behavior Therapy  Psychologic Desensitization Internal-External Control  Post Traumatic Stress Disorders  Phobic Disorders  Anger  On learned representations without using the hierarchy  Figure 4d shows learned vector representations with modiﬁed hierarchy and compares the learned representations with the previous results.2 Unlike the previous result in Figure 4c, where Health- related nodes and Population-related nodes form clusters, Urban Health and Urban Population are clustered together since they share the common parent node, Urban. We can also observe similar patterns for Suburban and Rural. Besides, relative distance between each Population and Health within a cluster is identical, and the same direction vector from Health to Population or the other way around can be deﬁned. Please note that, in this case, learning the model only from co-occurrences yields always similar label representations since we only modiﬁed the hierarchy (Figure 4b).  6 DISCUSSION AND FUTURE WORK  We have presented a method that learns vector representation of labels taking hierarchical structures into account. The empirical results demonstrate that using hierarchical structures of labels with label co-occurrences have more impact on identifying regularities of labels than using label co- occurrences only. We evaluated label representations qualitatively by observing label representations in 2D. Even though inter- and intra-group relations can be found in the learned label space, it is still desired to evaluate our method using quantitative measures because we have compared and analyzed both learned representations with the very limited number of examples and chosen arbitrarily. Hence, we are currently attempting to evaluate the model quantitatively to check whether the model brings on better prediction results in multi-label classiﬁcation as a means of pre-training a joint embedding space (Weston et al., 2010). We are also interested in extending the model to be capable of analogical reasoning on parent-child relationship.  2For the sake of readability, we used repeated results, the left graph in (a), (b) and (c) taken from Figure 2, in order to clearly show the difference between representations learned from the original hierarchy and from the modiﬁed one as well as learned from only co-occurrences.  7  Accepted as a workshop contribution at ICLR 2015  (a)  (b)  (c)  (d)  Figure 4: (a) A modiﬁed hierarchy (right) from the original one (left) obtained by grouping the same types of developed environments. See text for further explanation. (b) Learned vector representa- tions without using a hierarchy. (c) Learned vector representations using the original hierarchy. (d) Learned vector representations using the modiﬁed hierarchy.  REFERENCES Bengio, Yoshua, Ducharme, R´ejean, Vincent, Pascal, and Jauvin, Christian. A neural probabilistic  language model. Journal of Machine Learning Research, 3:1137–1155, 2003.  Bi, Wei and Kwok, James T. Multilabel classiﬁcation on tree- and DAG-structured hierarchies. In  Proceedings of the 28th International Conference on Machine Learning, pp. 17–24, 2011.  Bi, Wei and Kwok, James T. Efﬁcient multi-label classiﬁcation with many labels. In Proceedings of  the 30th International Conference on Machine Learning, pp. 405–413, 2013.  Chen, Yao-Nan and Lin, Hsuan-Tien. Feature-aware label space dimension reduction for multi-label  classiﬁcation. In Advances in Neural Information Processing Systems, pp. 1529–1537, 2012.  Dembczy´nski, Krzysztof, Waegeman, Willem, Cheng, Weiwei, and H¨ullermeier, Eyke. On label dependence and loss minimization in multi-label classiﬁcation. Machine Learning, 88(1-2):5–45, 2012.  F¨urnkranz, Johannes, H¨ullermeier, Eyke, Loza Menc´ıa, Eneldo, and Brinker, Klaus. Multilabel  classiﬁcation via calibrated label ranking. Machine Learning, 73(2):133–153, 2008.  Hsu, Daniel, Kakade, Sham, Langford, John, and Zhang, Tong. Multi-label prediction via com- In Advances in Neural Information Processing Systems 22, volume 22, pp.  pressed sensing. 772–780, 2009.  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed repre- sentations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pp. 3111–3119. 2013.  Mnih, Andriy and Hinton, Geoffrey E. A scalable hierarchical distributed language model.  Advances in Neural Information Processing Systems 22, pp. 1081–1088, 2009.  In  Morin, Frederic and Bengio, Yoshua. Hierarchical probabilistic neural network language model. In Proceedings of the 10th International Workshop on Artiﬁcial Intelligence and Statistics, pp. 246–252, 2005.  8  Population CharacteristicsUrbanSuburbanUrban  HealthSuburban HealthRural HealthUrbanPopulationSuburban PopulationRural PopulationHealth care categoryRuralHealth…Population CharacteristicsHealthPopulationDemographySocioeconomic FactorsUrban  HealthSuburban HealthRural Health…UrbanPopulationSuburban PopulationRural PopulationHealth care category……0.40.20.00.20.40.30.20.10.00.10.20.30.4Rural HealthSuburban HealthUrban HealthRural PopulationSuburban PopulationUrban PopulationHealthPopulation0.40.20.00.20.40.30.20.10.00.10.20.3Rural HealthSuburban HealthUrban HealthRural PopulationSuburban PopulationUrban PopulationHealthPopulation0.60.40.20.00.20.40.30.20.10.00.10.20.3Urban HealthUrban PopulationSuburban HealthSuburban PopulationRural HealthRural PopulationAccepted as a workshop contribution at ICLR 2015  Read, Jesse, Pfahringer, Bernhard, Holmes, Geoff, and Frank, Eibe. Classiﬁer chains for multi-label  classiﬁcation. Machine learning, 85(3):333–359, 2011.  Rousu, Juho, Saunders, Craig, Szedm´ak, S´andor, and Shawe-Taylor, John. Kernel-based learning of hierarchical multilabel classiﬁcation models. Journal of Machine Learning Research, 7:1601– 1626, 2006.  Silla Jr., Carlos Nascimento and Freitas, Alex Alves. A survey of hierarchical classiﬁcation across  different application domains. Data Mining and Knowledge Discovery, 22(1-2):31–72, 2011.  Tai, Farbound and Lin, Hsuan-Tien. Multilabel classiﬁcation with principal label space transforma-  tion. Neural Computation, 24(9):2508–2542, 2012.  Van der Maaten, Laurens and Hinton, Geoffrey. Visualizing data using t-SNE. Journal of Machine  Learning Research, 9(2579-2605):85, 2008.  Vens, Celine, Struyf, Jan, Schietgat, Leander, Dˇzeroski, Saso, and Blockeel, Hendrik. Decision trees  for hierarchical multi-label classiﬁcation. Machine Learning, 73(2):185–214, 2008.  Weston, Jason, Bengio, Samy, and Usunier, Nicolas. Large scale image annotation: Learning to  rank with joint word-image embeddings. Machine learning, 81(1):21–35, 2010.  Yu, Hsiang-Fu, Jain, Prateek, Kar, Purushottam, and Dhillon, Inderjit. Large-scale multi-label learn- ing with missing labels. In Proceedings of The 31st International Conference on Machine Learn- ing, pp. 593–601, 2014.  Zimek, Arthur, Buchwald, Fabian, Frank, Eibe, and Kramer, Stefan. A study of hierarchical and ﬂat classiﬁcation of proteins. IEEE/ACM Transactions on Computational Biology and Bioinformat- ics, 7:563–571, 2010.  9  ",
1412.6614,2015, In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning,"['In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning', 'Behnam Neyshabur', 'Ryota Tomioka', 'and Nathan Srebro']",https://arxiv.org/pdf/1412.6614,"5 1 0 2    r p A 6 1         ]  G L . s c [      4 v 4 1 6 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  IN SEARCH OF THE REAL INDUCTIVE BIAS: ON THE ROLE OF IMPLICIT REGULARIZATION IN DEEP LEARNING  Behnam Neyshabur, Ryota Tomioka & Nathan Srebro Toyota Technological Institute at Chicago Chicago, IL 60637, USA {bneyshabur,tomioka,nati}@ttic.edu  ABSTRACT  We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multi-layer feed- forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.  1  INTRODUCTION  Central to any form of learning is an inductive bias that induces some sort of capacity control (i.e. re- stricts or encourages predictors to be “simple” in some way), which in turn allows for generalization. The success of learning then depends on how well the inductive bias captures reality (i.e. how ex- pressive is the hypothesis class of “simple” predictors) relative to the capacity induced, as well as on the computational complexity of ﬁtting a “simple” predictor to the training data.  Let us consider learning with feed-forward networks from this perspective. If we search for the weights minimizing the training error, we are essentially considering the hypothesis class of predic- tors representable with different weight vectors, typically for some ﬁxed architecture. Capacity is then controlled by the size (number of weights) of the network1. Our justiﬁcation for using such net- works is then that many interesting and realistic functions can be represented by not-too-large (and hence bounded capacity) feed-forward networks. Indeed, in many cases we can show how speciﬁc architectures can capture desired behaviors. More broadly, any O(T ) time computable function can be captured by an O(T 2) sized network, and so the expressive power of such networks is indeed great (Sipser, 2006, Theorem 9.25).  At the same time, we also know that learning even moderately sized networks is computationally intractable—not only is it NP-hard to minimize the empirical error, even with only three hidden units, but it is hard to learn small feed-forward networks using any learning method (subject to cryptographic assumptions). That is, even for binary classiﬁcation using a network with a single hidden layer and a logarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly captured by such a small network, there is likely no efﬁcient algorithm that can ensure error better than 1/2 (Sherstov, 2006; Daniely et al., 2014)—not if the algorithm tries to ﬁt such a network, not even if it tries to ﬁt a much larger network, and in fact no matter how the algorithm represents predictors (see the Appendix). And so, merely knowing that some not-too-large architecture is excellent in expressing reality does not explain why we are able to learn using it, nor using an even larger network. Why is it then that we succeed in learning using multilayer feed- forward networks? Can we identify a property that makes them possible to learn? An alternative inductive bias?  Here, we make our ﬁrst steps at shedding light on this question by going back to our understanding of network size as the capacity control at play.  1The exact correspondence depends on the activation function—for hard thresholding activation the pseudo- dimension, and hence sample complexity, scales as O(S log S), where S is the number of weights in the network. With sigmoidal activation it is between Ω(S2) and O(S4) (Anthony & Bartlett, 1999).  1  Accepted as a workshop contribution at ICLR 2015  Our main observation, based on empirical experimentation with single-hidden-layer networks of increasing size (increasing number of hidden units), is that size does not behave as a capacity control parameter, and in fact there must be some other, implicit, capacity control at play. We suggest that this hidden capacity control might be the real inductive bias when learning with deep networks.  In order to try to gain an understanding at the possible inductive bias, we draw an analogy to matrix factorization and understand dimensionality versus norm control there. Based on this analogy we suggest that implicit norm regularization might be central also for deep learning, and also there we should think of inﬁnite-sized bounded-norm models. We then also demonstrate how (implicit) ℓ2 weight decay in an inﬁnite two-layer network gives rise to a “convex neural net”, with an inﬁnite hidden layer and ℓ1 (not ℓ2) regularization in the top layer.  2 NETWORK SIZE AND GENERALIZATION  Consider training a feed-forward network by ﬁnding the weights minimizing the training error. Speciﬁcally, we will consider a network with d real-valued inputs x = (x[1], . . . , x[d]), a single hidden layer with H rectiﬁed linear units, and k outputs y[1], . . . , y[k],  y[j] =  H  Xh=1  vhj[huh, xi]+  (1)  where [z]+ := max(z, 0) is the rectiﬁed linear activation function and uh ∈ Rd, vhj ∈ R are the weights learned by minimizing a (truncated) soft-max cross entropy loss2 on n labeled training examples. The total number of weights is then H(d + k). What happens to the training and test errors when we increase the network size H? The training error will necessarily decrease. The test error might initially decrease as the approximation error is reduced and the network is better able to capture the targets. However, as the size increases further, we loose our capacity control and generalization ability, and should start overﬁtting. This is the classic approximation-estimation tradeoff behavior.  Consider, however, the results shown in Figure 1, where we trained networks of increasing size on the MNIST and CIFAR-10 datasets. Training was done using stochastic gradient descent with momentum and diminishing step sizes, on the training error and without any explicit regularization. As expected, both training and test error initially decrease. More surprising is that if we increase the size of the network past the size required to achieve zero training error, the test error continues decreasing! This behavior is not at all predicted by, and even contrary to, viewing learning as ﬁtting a hypothesis class controlled by network size. For example for MNIST, 32 units are enough to attain zero training error. When we allow more units, the network is not ﬁtting the training data any better, but the estimation error, and hence the generalization error, should increase with the increase in capacity. However, the test error goes down. In fact, as we add more and more parameters, even beyond the number of training examples, the generalization error does not go up.  We also further tested this phenomena under some artiﬁcial mutilations to the data set. First, we wanted to artiﬁcially ensure that the approximation error was indeed zero and does not decrease  2When using soft-max cross-entropy, the loss is never exactly zero for correct predictions with ﬁnite mar- gins/conﬁdences. Instead, if the data is seperable, in order to minimize the loss the weights need to be scaled up toward inﬁnity and the cross entropy loss goes to zero, and a global minimum is never attained. In order to be able to say that we are actually reaching a zero loss solution, and hence a global minimum, we use a slightly modiﬁed soft-max which does not noticeably change the results in practice. This truncated loss returns the same exact value for wrong predictions or correct prediction with conﬁdences less than a threshold but returns zero for correct predictions with large enough margins: Let {si}k i=1 be the scores for k possible labels and c be the correct labels. Then the soft-max cross-entropy loss can be written as ℓ(s, c) = ln Pi exp(si − sc) but we instead use the differentiable loss function ˆℓ(s, c) = ln Pi f (si − sc) where f (x) = exp(x) for x ≥ −11 and f (x) = exp(−11)[x + 13]2 +/4 otherwise. Therefore, we only deviate from the soft-max cross-entropy when the margin is more than 11, at which point the effect of this deviation is negligible (we always have (cid:12) ℓ(s, c) − ˆℓ(s, c)(cid:12) ≤ 0.000003k)—if there are any actual errors the behavior on them would completely domi- (cid:12) (cid:12) (cid:12) (cid:12) nate correct examples with margin over 11, and if there are no errors we are just capping the amount by which we need to scale up the weights.  2  Accepted as a workshop contribution at ICLR 2015  0.1  0.09  0.08  0.07  0.06  0.05  0.04  0.03  0.02  0.01  r o r r E  MNIST     Training Test (at convergence) Test (early stopping)  r o r r E  0.6  0.5  0.4  0.3  0.2  0.1  CIFAR-10     Training Test (at convergence) Test (early stopping)  0     4  8  16  32  64  256  512  1K  2K  4K  128 H  0     4  8  16  32  64  256  512  1K  2K  4K  128 H  Figure 1: The training error and the test error based on different stopping criteria when 2-layer NNs with dif- ferent number of hidden units are trained on MNIST and CIFAR-10. Images in both datasets are downsampled to 100 pixels. The size of the training set is 50000 for MNIST and 40000 for CIFAR-10. The early stopping is based on the error on a validation set (separate from the training and test sets) of size 10000. The training was done using stochastic gradient descent with momentum and mini-batches of size 100. The network was ini- tialized with weights generated randomly from the Gaussian distribution. The initial step size and momentum were set to 0.1 and 0.5 respectively. After each epoch, we used the update rule µ(t+1) = 0.99µ(t) for the step size and m(t+1) = min{0.9, m(t) + 0.02} for the momentum.  as we add more units. To this end, we ﬁrst trained a network with a small number H0 of hidden units (H0 = 4 on MNIST and H0 = 16 on CIFAR) on the entire dataset (train+test+validation). This network did have some disagreements with the correct labels, but we then switched all labels to agree with the network creating a “censored” data set. We can think of this censored data as representing an artiﬁcial source distribution which can be exactly captured by a network with H0 hidden units. That is, the approximation error is zero for networks with at least H0 hidden units, and so does not decrease further. Still, as can be seen in the middle row of Figure 2, the test error continues decreasing even after reaching zero training error.  Next, we tried to force overﬁtting by adding random label noise to the data. We wanted to see whether now the network will use its higher capacity to try to ﬁt the noise, thus hurting generaliza- tion. However, as can be seen in the bottom row of Figure 2, even with ﬁve percent random labels, there is no signiﬁcant overﬁtting and test error continues decreasing as network size increases past the size required for achieving zero training error.  What is happening here? A possible explanation is that the optimization is introducing some implicit regularization. That is, we are implicitly trying to ﬁnd a solution with small “complexity”, for some notion of complexity, perhaps norm. This can explain why we do not overﬁt even when the number of parameters is huge. Furthermore, increasing the number of units might allow for solutions that actually have lower “complexity”, and thus generalization better. Perhaps an ideal then would be an inﬁnite network controlled only through this hidden complexity.  We want to emphasize that we are not including any explicit regularization, neither as an explicit penalty term nor by modifying optimization through, e.g., drop-outs, weight decay, or with one-pass stochastic methods. We are using a stochastic method, but we are running it to convergence— we achieve zero surrogate loss and zero training error. In fact, we also tried training using batch conjugate gradient descent and observed almost identical behavior. But it seems that even still, we are not getting to some random global minimum—indeed for large networks the vast majority of the many global minima of the training error would horribly overﬁt. Instead, the optimization is directing us toward a “low complexity” global minimum.  Although we do not know what this hidden notion of complexity is, as a ﬁnal experiment we tried to see the effect of adding explicit regularization in the form of weight decay. The results are shown in the top row of ﬁgure 2. There is a slight improvement in generalization but we still see that increasing the network size helps generalization.  3  Accepted as a workshop contribution at ICLR 2015  Small MNIST  Small CIFAR-10     Training Test (at convergence) Test (early stopping) Test (regularization)  r o r r E  0.2  0.18  0.16  0.14  0.12  0.1  0.08  0.06  0.04  0.02  0     4  8  16  32  64  256  512  1K  2K  4K  128 H     Training Test (at convergence) Test (early stopping)  r o r r E  0.2  0.18  0.16  0.14  0.12  0.1  0.08  0.06  0.04  0.02  0     4  8  16  32  64  256  512  1K  2K  4K  128 H     Training Test (at convergence) Test (early stopping)  r o r r E  0.2  0.18  0.16  0.14  0.12  0.1  0.08  0.06  0.04  0.02     Training Test (at convergence) Test (early stopping) Test (regularization)  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0     4  8  16  32  64  256  512  1K  2K  4K  128 H     Training Test (at convergence) Test (early stopping)  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0     4  8  16  32  64  256  512  1K  2K  4K  128 H     Training Test (at convergence) Test (early stopping)  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  r o r r E  r o r r E  r o r r E  0     4  8  16  32  64  256  512  1K  2K  4K  128 H  0     4  8  16  32  64  256  512  1K  2K  4K  128 H  Figure 2: The training error and the test error based on different stopping criteria when 2-layer NNs with different number of hidden units are trained on small subsets of MNIST and CIFAR-10. Images in both datasets are downsampled to 100 pixels. The sizes of the training and validation sets are 2000 for both MNIST and CIFAR-10 and the early stopping is based on the error on the validation set. The top plots are the errors for the original datasets with and without explicit regularization.The best weight decay parameter is chosen based on the validation error. The middle plots are on the censored data set that is constructed by switching all the labels to agree with the predictions of a trained network with a small number H0 of hidden units H0 = 4 on MNIST and H0 = 16 on CIFAR-10) on the entire dataset (train+test+validation). The plots on the bottom are also for the censored data except we also add 5 percent noise to the labels by randomly changing 5 percent of the labels. The optimization method is the same as the in Figure 1. The results in this ﬁgure are the average error over 5 random repetitions.  3 A MATRIX FACTORIZATION ANALOGY  To gain some understanding at what might be going on, let us consider a slightly simpler model which we do understand much better. Instead of rectiﬁed linear activations, consider a feed-forward  4  Accepted as a workshop contribution at ICLR 2015  network with a single hidden layer, and linear activations, i.e.:  H  y[j] =  vhj huh, xi .  (2)  Xh=1  This is of course simply a matrix-factorization model, where y = W x and W = V U ⊤. Control- ling capacity by limiting the number of hidden units exactly corresponds to constraining the rank of W , i.e. biasing toward low dimensional factorizations. Such a low-rank inductive bias is indeed sensible, though computationally intractable to handle with most loss functions.  However, in the last decade we have seen much success for learning with low norm factorizations. In such models, we do not constrain the inner dimensionality H of U , V , and instead only constrain, or regularize, their norm. For example, constraining the Frobenius norm of U and V corresponds to using the trace-norm as an inductive bias (Srebro et al., 2004):  kW ktr = min  W =V U ⊤  1 2  (kU k2  F + kV k2  F ).  (3)  Other norms of the factorization lead to different regularizers.  Unlike the rank, the trace-norm (as well as other factorization norms) is convex, and leads to tractable learning problems (Fazel et al., 2001; Srebro et al., 2004). In fact, even if learning is done by a local search over the factor matrices U and V (i.e. by a local search over the weights of the network), if the dimensionality is high enough and the norm is regularized, we can ensure convergence to a global minima (Burer & Choi, 2006). This is in stark contrast to the dimensionality-constrained low-rank situation, where the limiting factor is the number of hidden units, and local minima are abundant (Srebro & Jaakkola, 2003).  Furthermore, the trace-norm and other factorization norms are well-justiﬁed as sensible inductive biases. We can ensure generalization based on having low trace-norm, and a low-trace norm model corresponds to a realistic factor model with many factors of limited overall inﬂuence. In fact, empir- ical evidence suggests that in many cases low-norm factorization are a more appropriate inductive bias compared to low-rank models.  We see, then, that in the case of linear activations (i.e. matrix factorization), the norm of the factor- ization is in a sense a better inductive bias than the number of weights: it ensures generalization, it is grounded in reality, and it explain why the models can be learned tractably.  Let us interpret the experimental results of Section 2 in this light. Perhaps learning is succeeding not because there is a good representation of the targets with a small number of units, but rather because there is a good representation with small overall norm, and the optimization is implicitly biasing us toward low-norm models. Such an inductive bias might potentially explain both the generalization ability and the computational tractability of learning, even using local search.  Under this interpretation, we really should be using inﬁnite-sized networks, with an inﬁnite number of hidden units. Fitting a ﬁnite network (with implicit regularization) can be viewed as an approxi- mation to ﬁtting the “true” inﬁnite network. This situation is also common in matrix factorization: e.g., a very successful approach for training low trace-norm models, and other inﬁnite-dimensional bounded-norm factorization models, is to approximate them using a ﬁnite dimensional representa- tion Rennie & Srebro (2005); Srebro & Salakhutdinov (2010). The ﬁnite dimensionality is then not used at all for capacity (statistical complexity) control, but purely for computational reasons. Indeed, increasing the allowed dimensionality generally improves generalization performance, as it allows us to better approximate the true inﬁnite model.  4  INFINITE SIZE, BOUNDED NORM NETWORKS  In this ﬁnal section, we consider a possible model for inﬁnite sized norm-regularized networks. Our starting point is that of global weight decay, i.e. adding a regularization term that penalizes the sum of squares of all weights in the network, as might be approximately introduced by some implicit regularization. Our result in this Section is that this global ℓ2 regularization is equivalent to a Convex Neural Network (Convex NN; Bengio et al. (2005))—an inﬁnite network with ℓ1 regularization on the top layer. Note that such models are rather different from inﬁnite networks with ℓ2 regularization  5  Accepted as a workshop contribution at ICLR 2015  on the top layer, which reduce to linear methods with some speciﬁc kernel (Cho & Saul, 2009; Bach, 2014). Note also that our aim here is to explain what neural networks are doing instead of trying to match the performance of deep models with a known shallow model as done by, e.g., Lu et al. (2014).  For simplicity, we will focus on single output networks (k = 1), i.e. networks which compute a function f : Rd → R. We ﬁrst consider ﬁnite two-layer networks (with a single hidden layer) and show that ℓ2 regularization on both layers is equivalent to and ℓ2 constraint on each unit in the hidden layer, and ℓ1 regularization on the top unit: Theorem 1. Let L be a loss function and D = (xt, yt)n  t=1 be training examples.  argmin v∈RH ,(uh)H  is the same as  h=1   n L(cid:18)yt,XH Xt=1 h=1  n Xt=1  argmin v∈RH ,(uh)H  vh[huh, xti]+(cid:19) +  λ 2  H  Xh=1(cid:16)kuhk2 + |vh|2(cid:17)! ,  h=1  L(cid:18)yt,XH  h=1  vh[huh, xti]+(cid:19) + λ  subject to kuhk ≤ 1 (h = 1, . . . , H).  |vh|!,  H  Xh=1  (4)  (5)  Proof. By the inequality between the arithmetic and geometric means, we have  1 2  H  H  Xh=1(cid:0)kuhk2 + |vh|2(cid:1) ≥  Xh=1  kuhk · |vh|.  The right-hand side can always be attained without changing the input-output mapping by the rescal-  ing ˜uh = p|vh|/kuhk · uh and ˜vh = pkuhk/|vh| · vh. The reason we can rescale the weights  without changing the input-output mapping is that the rectiﬁed linear unit is piece-wise linear and the piece a hidden unit is on is invariant to rescaling of the weights. Finally, since the right-hand side of the above inequality is invariant to rescaling, we can always choose the norm kuhk to be bounded by one.  Now we establish a connection between the ℓ1-regularized network (5), in which we learn the input to hidden connections, to convex NN (Bengio et al., 2005).  First we recall the deﬁnition of a convex NN. Let U be a ﬁxed “library” of possible weight vectors {u ∈ Rd : kuk ≤ 1} and µ+ and µ− be positive (unnormalized) measures over U representing the positive and negative part of the weights of each unit. Then a “convex neural net” is given by predictions of the form  (dµ+(u) − dµ−(u)) [hu, xi]+  (6)  with regularizer (i.e. complexity measure) µ+(U) + µ−(U). This is simply an inﬁnite generalization of network with hidden units U and ℓ1 regularization on the second layer: if U is ﬁnite, (6) is equivalent to  v(u)[hu, xi]+  (7)  y =ZU  y = Xu∈U  with v(u) := µ+(u) − µ−(u) and regularizer kvk1 =Pu∈U |v(u)|. Training a convex NN is then  given by:  argmin  µ+,µ−   n Xt=1  L(cid:16)yt,ZU  (dµ+(u) − dµ−(u)) [hu, xti]+(cid:17) + λ (µ+(U) + µ−(U))! .  (8)  Moreover, even if U is inﬁnite and even continuous, there will always be an optimum of (8) which is a discrete measure with support at most n + 1 (Rosset et al., 2007). That is, (8) can be equivalently written as:  u1,...,un+1∈U ,v∈Rn+1   n Xt=1  argmin  L(cid:16)yt,  n+1  Xh=1  vh[huh, xti]+(cid:17) + λ kvk1! ,  (9)  6  Accepted as a workshop contribution at ICLR 2015  which is the same as (5), with U =(cid:8)u ∈ Rd(cid:12)(cid:12) kuk ≤ 1(cid:9).  The difference between the network (5), and the inﬁnite network (8) is in learning versus selecting the hidden units, and in that in (8) we have no limit on the number of units used. That is, in (8) we have all possible units in U available to us, and we merely need to select which we want to use, without any constraint on the number of units used, only the over ℓ1 norm. But the equivalence of (8) and (9) establishes that as long as the number of allowed units H is large enough, the two are equivalent: Corollary 1. As long as H > n, the weight-decay regularized network (4) is equivalent to the  convex neural net (8) with U =(cid:8)u ∈ Rd(cid:12)(cid:12) kuk ≤ 1(cid:9).  In summary, learning and selecting is equivalent if we have sufﬁciently many hidden units and Theorem 1 gives an alternative justiﬁcation for employing ℓ1 regularization when the input to hidden weights are ﬁxed and normalized to have unit norm, namely, it is equivalent to ℓ2 regularization, which can be achieved by weight decay or implicit regularization via stochastic gradient descent.  The above equivalence holds also for networks with multiple output units, i.e. k > 1, where the h=1 kvhk. Indeed, for matrix factorizations (i.e. with linear activations), such a group-lasso regularized formulation is known to be equivalent to the trace norm (3) (see Argyriou et al., 2007).  ℓ1 regularization on v is replaced with the group lasso regularizerPH  REFERENCES Anthony, Martin and Bartlett, Peter L. Neural network learning: Theoretical foundations. Cam-  bridge University Press, 1999.  Argyriou, Andreas, Evgeniou, Theodoros, and Pontil, Massimiliano. Multi-task feature learning.  Advances in neural information processing systems, pp. 41–48, 2007.  Bach, Francis.  Breaking the curse of dimensionality with convex neural networks.  http://www.di.ens.fr/˜fbach/fbach_cifar_2014.pdf, 2014.  Bengio, Yoshua, Roux, Nicolas L., Vincent, Pascal, Delalleau, Olivier, and Marcotte, Patrice. Con-  vex neural networks. Advances in neural information processing systems, pp. 123–130, 2005.  Burer, Samuel and Choi, Changhui. Computational enhancements in low-rank semideﬁnite pro-  gramming. Optimization Methods and Software, 21(3):493–512, 2006.  Cho, Youngmin and Saul, Lawrence K. Kernel methods for deep learning. Advances in neural  information processing systems, pp. 342–350, 2009.  Daniely, Amit, Linial, Nati, and Shalev-Shwartz, Shai. From average case complexity to improper  learning complexity. STOC, 2014.  Fazel, Maryam, Hindi, Haitham, and Boyd, Stephen P. A rank minimization heuristic with applica- tion to minimum order system approximation. Proceedings of American Control Conference, pp. 4734–4739, 2001.  Kearns, Michael and Valiant, Leslie. Cryptographic limitations on learning boolean formulae and  ﬁnite automata. Journal of the ACM (JACM), 41(1):67–95, 1994.  Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad. On the computational efﬁciency of training  neural networks. Advances in Neural Information Processing Systems, pp. 855–863, 2014.  Lu, Zhiyun, May, Avner, Liu, Kuan, Garakani, Alireza Bagheri, Guo, Dong, Bellet, Aurlien, Fan, Linxi, Collins, Michael, Kingsbury, Brian, Picheny, Michael, and Sha, Fei. How to scale up kernel methods to be as good as deep neural nets. Technical Report, arXiv:1411.4000, 2014.  Rennie, Jasson DM and Srebro, Nathan. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pp. 713– 719. ACM, 2005.  7  Accepted as a workshop contribution at ICLR 2015  Rosset, Saharon, Swirszcz, Grzegorz, and Srebro, Nathan. ℓ1 regularization in inﬁnite dimensional  feature spaces. In COLT, pp. 544–558. Springer, 2007.  Sherstov, Adam R Klivansand Alexander A. Cryptographic hardness for learning intersections of halfspaces. In Foundations of Computer Science, 2006. FOCS’06. 47th Annual IEEE Symposium on, pp. 553–562. IEEE, 2006.  Sipser, Michael. Introduction to the Theory of Computation. Thomson Course Technology, 2006.  Srebro, Nathan and Jaakkola, Tommi S. Weighted low-rank approximations. ICML, pp. 720–727,  2003.  Srebro, Nathan and Salakhutdinov, Ruslan. Collaborative ﬁltering in a non-uniform world: Learning with the weighted trace norm. In Advances in Neural Information Processing Systems, pp. 2056– 2064, 2010.  Srebro, Nathan, Rennie, Jason, and Jaakkola, Tommi S. Maximum-margin matrix factorization.  Advances in neural information processing systems, pp. 1329–1336, 2004.  APPENDIX  For the convenience of the reader, we formalize here the hardness of learning feed-forward neural network mentioned in the Introduction. The results are presented in a way that is appropriate for feed-forward networks with RELU activations, but they are really a direct implication of recent results about learning intersections of halfspaces. For historical completeness we note that hardness of learning logarithmic depth networks was already established by Kearns & Valiant (1994), and that the more recent results we discuss here (Sherstov, 2006; Daniely et al., 2014) establish also hardness of learning depth two networks, subject to perhaps simpler cryptographic assumptions. The presentation and construction here is similar to that of Livni et al. (2014).  Question Is there a sample complexity function m(D, H) and an algorithm A that takes as input {(xi, yi)}i=1,...,M , xi ∈ {±1}D, y ∈ ±1 and returns a description of a function f : {±1}D → ±1 such that the following is true:  For any D, any H and any distribution D(x, y) over x ∈ {±1}D, y ∈ ±1, if:  (1) There exists a feed-forward neural network with RELU actications with D inputs and H hidden units implementing a function g : RD → R such that Pr(x,y)∼D[yg(x) > 0] = 1 (i.e. for (x, y) ∼ D, the label can be perfectly predicted by a network of size H).  (2) The input to A is drawn i.i.d. from D. (3) M ≥ m(D, H) (i.e. A is provided with enough training data).  then  (1) With probability at  algorithm A returns a function f such that Px,y∼D [yf (x) < 0] < 1/4 (that is, at least half the time the algorithm succeeds in learning a function with non-trivial error).  least 1/2,  (3) m(D, H) ≤ poly(D, H) for some poly(D, H) (i.e. the sample complexity required by the algorithm is polynomial in the network size—if we needed a super-polynomial number of samples, we would have no hope of learning in polynomial time).  (4) The function f that corresponds to the description returned by A can be computed in time poly(D, H) from its description (i.e. the representation used by the learner can be a feed- forward network of any size polynomial in H and D, or of any other representation that can be efﬁciently computed).  (5) A runs in time poly(D, H, M )  Theorem. Subject for the cryptographic assumptions in Daniely et al. (2014), there is no algorithm that satisﬁes the conditions in the above question.  8  Accepted as a workshop contribution at ICLR 2015  In fact, there is no algorithm satisfying the conditions even if we require that the labels can be perfectly predicted by a network with a single hidden layer with any super-constant, e.g. log(D), number of hidden units.  Proof. We show that every intersection of k = ω(1) homogeneous halfspaces over {±1}n with normals in {±1} can be realized with unit margin by a feed-fowrad neural networks with H = 2k hidden units in a single hidden layer. For each hyperplane hwi, xi > 0, where wi ∈ {±1}D, we include two units in the hidden layer: g+ i (x) = [hwi, xi − 1]+. We set all incoming weights of the output node to be 1. Therefore, this network is realizing the following function:  i (x) = [hwi, xi]+ and g−  k  f (x) =  ([hwi, xi]+ − [hwi, xi − 1]+)  Xi=1  the outputs of the ﬁrst  inputs and all weights are integer,  Since all layer will be integer, ([hwi, xi]+ − [hwi, xi − 1]+) will be zero or one, and f realizes the intersection of the k halfs- paces with unit margin. Hence, the hypothesis class of neural intersection of k/2 halfspaces is a subset of hypothesis class of feed-forward neural networks with k hidden units in a single hidden layer. We complete the proof by applying Theorem 5.4 in Daniely et al. (2014) which states that for any k = ω(1), subject for the cryptographic assumptions in Daniely et al. (2014), the hypothesis class of intersection of homogeneous halfspaces over {±1}n with normals in {±1} is not efﬁciently PAC learnable (even improperly)3.  We proved here that even for H = ω(1) no algorithm can satisfy the condition in the question. A similar result can be shown for H = poly(D) subject to weaker cryptographic assumptions in Sherstov (2006).  The Theorem tells us not only that we cannot expect to ﬁt a small network to data even if the data is generated by the network (since doing so would give us an efﬁcient learning algorithm, which contradicts the Theorem), but that we also can’t expect to learn by using a much larger network. That is, even if we know that labels can be perfectly predicted by a small network, we cannot expect to have a learning algorithm that learns a much larger (but poly sized) network that will have non-trivial error. In fact, being representable by a small network is not enough to ensure tractable learning no matter what representation the learning algorithm uses (e.g. a much larger network, a mixture of networks, a tree over networks, etc). This is a much stronger statement than just saying that ﬁtting a network to data is N P -hard. Also, precluding the possibility of tractable learning if the labels are exactly explained by some small unknown network of course also precludes the possibility of achieving low error when the labels are only approximately explained by some small unknown network (i.e. of noisy or “agnostic” learning).  3Their Theorem 5.4 talks about unrestricted halfspaces, but the construction in Section 7.2 uses only data in  {±1}D and halfspaces speciﬁed by hw, xi > 0 with w ∈ {±1}D  9  ",
1412.6452,2015," Algorithmic Robustness for Semi-Supervised (ϵ, γ, τ)-Good Metric Learning","['Algorithmic Robustness for Semi-Supervised (ϵ', 'γ', 'τ)-Good Metric Learning', 'Maria-Irina Nicolae', 'Marc Sebban', 'Amaury Habrard', 'Éric Gaussier', 'and Massih-Reza Amini']",https://arxiv.org/pdf/1412.6452,"5 1 0 2    r a     M 1 3      ]  G L . s c [      3 v 2 5 4 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  ALGORITHMIC ROBUSTNESS FOR LEARNING VIA (ǫ, γ, τ )-GOOD SIMILARITY FUNCTIONS  Maria-Irina Nicolae, Marc Sebban & Amaury Habrard Hubert Curien Laboratory Jean Monnet University, Saint-Etienne, France {Maria.Irina.Nicolae,Marc.Sebban,Amaury.Habrard}@univ-st-etienne.fr ´Eric Gaussier & Massih-Reza Amini Laboratoire d’Informatique de Grenoble Joseph Fourier University, Grenoble, France {Eric.Gaussier,Massih-Reza.Amini}@imag.fr  ABSTRACT  The notion of metric plays a key role in machine learning problems such as clas- siﬁcation, clustering or ranking. However, it is worth noting that there is a severe lack of theoretical guarantees that can be expected on the generalization capac- ity of the classiﬁer associated to a given metric. The theoretical framework of (ǫ, γ, τ )-good similarity functions (Balcan et al., 2008) has been one of the ﬁrst attempts to draw a link between the properties of a similarity function and those of a linear classiﬁer making use of it. In this paper, we extend and complete this theory by providing a new generalization bound for the associated classiﬁer based on the algorithmic robustness framework.  1  INTRODUCTION  Most of the machine learning algorithms make use of metrics for comparing objects and making decisions (e.g. SVMs, k-NN, k-means, etc.). However, it is worth noticing that the theoretical guar- antees of these algorithms are always derived independently from the peculiarities of the metric they make use of. For example, in supervised learning, the generalization bounds on the classiﬁcation er- ror do not take into account the discriminative properties of the metrics. In this context, Balcan et al. (2008) ﬁlled this gap by proposing the ﬁrst framework that allows one to relate similarities with a classiﬁcation algorithm. This general framework, that can be used with any bounded similarity function, provides generalization guarantees on a linear classiﬁer learned from the similarity. More- over, their algorithm, whose formulation is equivalent to a relaxed L1-norm SVM (Zhu et al., 2003), does not enforce the positive deﬁniteness constraint of the similarity. In this paper, we show that using Balcan et al’s setting and the algorithmic robustness framework (Xu & Mannor, 2012), we can derive generalization guarantees which consider other properties of the similarity. This leads to new consistency bounds for different kinds of similarity functions.  2 NOTATIONS AND RELATED WORK  Let us assume we are given access to labeled examples z = (x, l(x)) drawn from some unknown distribution P over X × Y, where X ⊆ Rd and Y = {−1, 1} are respectively the instance and the output spaces. A pairwise similarity function KA over X , possibly parameterized by a matrix A ∈ Rd×d, is deﬁned as KA : X × X → [−1, 1], and the hinge loss as [c]+ = max(0, 1 − c). We denote the L1 norm by || · ||1, the L2 norm by || · ||2 and the Frobenius norm by || · ||F . We assume that ||x||2 ≤ 1. Balcan et al. (2008) introduced a theory for learning with so called (ǫ, γ, τ )-good similarity func- tions. Their generalization guarantees are based on the following deﬁnition.  1  Accepted as a workshop contribution at ICLR 2015  Deﬁnition 1. (Balcan et al., 2008) KA is a (ǫ, γ, τ )-good similarity function in hinge loss for a learning problem P if there exists a random indicator function R(x) deﬁning a probabilistic set of ”reasonable points” such that the following conditions hold:  1. E(x,l(x))∼P (cid:2)[1 − l(x)g(x)/γ]+(cid:3) ≤ ǫ, where g(x) = E(x′,l(x′),R(x′)) [l(x′)KA(x, x′)|R(x′)]. 2. Prx′(R(x′)) ≥ τ.  This deﬁnition imposes a constraint on the mass of reasonable points one must consider (greater than τ). It also expresses the tolerated margin violations in an averaged way: a (1 − ǫ) proportion of examples x are on average 2γ more similar to random reasonable examples x′ of their own label than to random reasonable examples x′ of the other label. Deﬁnition 1 can then be used to learn well: Theorem 1. (Balcan et al., 2008) Let KA be an (ǫ, γ, τ )-good similarity function in hinge loss for a learning problem P. For any ǫ1 > 0 and < δ < γǫ1/4 let S = {x′ du} be a sample of (ǫ1γ)2 (cid:17) landmarks drawn from P. Consider the mapping φS : X → Rdu du = 2 deﬁned as follows: φS i), i ∈ {1, . . . , du}. With probability 1 − δ over the random sample S, the induced distribution φS (P ) in Rdu, has a separator achieving hinge loss at most ǫ + ǫ1 at margin γ.  τ (cid:16)log(2/δ) + 16 log(2/δ)  i (x) = KA(x, x′  2, . . . , x′  1, x′  In other words, if KA is (ǫ, γ, τ )-good according to Deﬁnition 1 and enough points are available, there exists a linear separator α ∈ Rdu with error arbitrarily close to ǫ in the space φS. This separator can be learned from dl labeled examples by solving the following optimization problem:  min  1 dl  dl  Xi=1  ℓ(A, α, zi)  du  s.t.  Xj=1  |αj| ≤ 1/γ j=1 αjl(xi)KA(xi, xj)i+  where ℓ(A, α, zi = (xi, l(xi))) = h1 −Pdu is the instantaneous loss estimated at point (xi, l(xi)). Therefore, this optimization problem reduces to minimizing the empirical loss ˆRℓ = 1 i=1 ℓ(A, α, zi) over the training set S. Note that this problem can be solved efﬁciently by linear programming. Also, as the problem is L1-constrained, tuning the value of γ will produce a sparse solution.  dl Pdl  (1)  3 CONSISTENCY GUARANTEES  In this section, we provide a new generalization bound for the classiﬁer learned in Problem (1) based on the recent algorithmic robustness framework proposed by Xu & Mannor (2012). To begin with, let us recall the notion of robustness of an algorithm A. Deﬁnition 2 (Algorithmic Robustness (Xu & Mannor, 2012)). Algorithm A is (M, ǫ(·))-robust, for M ∈ N and ǫ(·) : Z dl → R, if Z can be partitioned into M disjoint sets, denoted by {Ci}M i=1, such that the following holds for all S ∈ Z dl:  ∀z = (x, l(x)) ∈ S,∀z′ = (x′, l(x′)) ∈ Z,∀i ∈ [M ] : if z, z′ ∈ Ci, then |ℓ(A, α, z) − ℓ(A, α, z′)| ≤ ǫ(S).  Roughly speaking, robustness characterizes the capability of an algorithm to perform similarly on close train and test instances. The closeness of the instances is based on a partitionning of Z: two examples are close if they belong to the same region. In general, the partition is based on the notion of covering number (Kolmogorov & Tikhomirov, 1961) allowing one to cover Z by regions where the distance/norm between two elements in the same region are no more than a ﬁxed quantity ρ (see Xu & Mannor (2012) for details about how the convering is built). Now we can state the ﬁrst theoretical contribution of this paper.  2  Accepted as a workshop contribution at ICLR 2015  Theorem 2. Given a partition of Z into M subsets {Ci} such that z = (x, l(x)) and z′ = (x′, l(x′)) ∈ Ci and l(x) = l(x′), and provided that KA(x, x′) is l-lipschitz w.r.t. its ﬁrst argument, the optimization problem (1) is (M, ǫ(S))-robust with ǫ(S) = 1 γ lρ, where ρ = supx,x′∈Ci ||x− x′||. Proof.  du  du  αjl(x′)KA(x′, xj) −  |ℓ(A, α, z) − ℓ(A, α, z′)| ≤(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) Xj=1 Xj=1 |αj| · |KA(x′, xj) − KA(x, xj )| Xj=1  |αj| · l||x − x′|| ≤  Xj=1  ≤  ≤  1 γ  lρ  du  du  αjl(x)KA(x, xj )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  (2)  (3)  (4)  Setting ρ = supx,x′∈Ci ||x − x′||1, we get the Theorem. We get Inequality (2) from the 1- lipschitzness of the hinge loss; Inequality (3) comes from the classical triangle inequality; The ﬁrst inequality on line (4) is due to the l-lipschitzness of KA(x, xj ) and the result follows from the constraint of Problem (1).  1  i=1 ℓ(A, α, zi) be the empirical loss over the training set S.  We now give a PAC generalization bound on the true loss making use of the previous robustness result. Let Rℓ = Ez∼Z ℓ(A, α, z) be the true loss w.r.t. the unknown distribution Z and ˆRℓ = dl Pdl Theorem 3. Considering that problem (1) is (M, ǫ(S))-robust, and that KA is l-lipschitz w.r.t. to its ﬁrst argument, for any δ > 0 with probability at least 1 − δ, we have: lρ + Bs 2M ln 2 + 2 ln(1/δ)  |Rℓ − ˆRℓ| ≤  1 γ  dl  ,  where B = 1 + 1  γ is an upper bound of the loss ℓ.  The proof of Theorem 3 follows the one described in Xu & Mannor (2012) and makes use of a concentration inequality over multinomial random variables (van der Vaart & Wellner, 1996). Note that in robustness bounds, the cover radius ρ can be made arbitrarily small at the expense of larger values of M . As M appears in the second term, which decreases to 0 when dl tends to inﬁnity, this bound provides a standard O(1/√dl) asymptotic convergence. The previous theorem strongly depends on the l-lipschitzness of the similarity function. following, we focus on some particular similarities that can be used in this setting: K 1 derived from the Mahalanobis distance, K 2 We provide the proof of the l-lipschitzness for K 1 Similarity function 1. We deﬁne K 1 the Mahalanobis distance. K 1  A(x, x′) = 1 − (x − x′)T A(x − x′), a similarity derived from  In the A, a similarity A an exponential similarity.  A. The two others follow the same ideas.  A a bilinear similarity and K 3  A(x, x′) is 4||A||2-lipschitz w.r.t. its ﬁrst argument.  Proof. (cid:12)(cid:12)(cid:12)  K 2  A(x, x′′) − K 2  A(x′, x′′)(cid:12)(cid:12)(cid:12)  A(x − x′′)(cid:17) − 1 + (cid:16)(x′ − x′′)T ′ − x  ′′) − (x  A(x − x  ′ − x  ′′)  T  A(x′ − x′′)(cid:17)(cid:12)(cid:12)(cid:12)  ′′) + (x  ′ − x  ′′)  T  T  (x  ′′)  A(x  ′ − x  = (cid:12)(cid:12)(cid:12) 1 − (cid:16)(x − x′′)T = (cid:12)(cid:12)(cid:12) = (cid:12)(cid:12)(cid:12) ≤ (cid:12)(cid:12)(cid:12)  (x′ − x′′)T  (x′ − x′′)T  A(x′ − x) + (x′ − x)T  A(x′ − x)(cid:12)(cid:12)(cid:12)  + (cid:12)(cid:12)(cid:12)  (x′ − x)T  A(x − x′′)(cid:12)(cid:12)(cid:12) A(x − x′′)(cid:12)(cid:12)(cid:12)  A(x − x  ′′) − (x − x  ′′)  T  A(x − x  ′′)(cid:12)(cid:12)(cid:12)  ≤||x′ − x′′||2 · ||A||2 · ||x′ − x||2 + ||x′ − x||2 · ||A||2 · ||x − x′′||2  (5)  ≤||x′ − x′′||2 · ||A||2 · (||x′||2 + ||x||2) + ||x′ − x||2 · ||A||2 · (||x||2 + ||x′′||2)  ≤4 · ||A||2 · ||x − x′||.  (6) Inequality (5) comes from the Cauchy-Schwarz inequality and some classical norm properties; Inequality (6) comes from the assumption that ||x||2 ≤ 1.  3  Accepted as a workshop contribution at ICLR 2015  Similarity function 2. Let K 2 lipschitz w.r.t. its ﬁrst argument.  Similarity function 3. Let K 3 its ﬁrst argument with l = 2||A||2  A be the bilinear form K 2  A(x, x′) = xT Ax′. K 2  A(x, x′) is ||A||2-  A(x, x′) = exp(cid:16)− (x−x′)T A(x−x′) (cid:0)exp(cid:0) 1  2σ2(cid:1) − exp(cid:0) −1 2σ2(cid:1)(cid:1).  2σ2  σ2  A(x, x′) is l-lipschitz w.r.t.  (cid:17). K 3  A(x, x′) (resp. K 2  (cid:0)exp(cid:0) 1  2σ2(cid:1) − exp(cid:0) −1  A are linear w.r.t. their arguments, they have the main advantage to keep prob- A and K 2 As both K 1 A is also based on the Mahalanobis distance, but this time it is a non lin- lem (1) convex. K 3 l = ||A||2 and ear function, ressembling more a gaussian kernel. Plugging l = 4||A||2 (resp. l = 2||A||2 2σ2(cid:1)(cid:1)) in Theorem 3, we obtain consistency results for problem (1) σ2 A(x, x′)). As the gap between empirical and true loss using K 1 presented in Theorem 3 is proportional with l for the l-lipschitzness of each similarity function, we would like to keep this parameter as small as possible. We notice that the generalization bound is tighter for K 1 A depends on the additional parameter σ, that adjusts the inﬂuence of the similarity value w.r.t. the distance to the landmarks. The value of l goes to 0 as σ augments, so larger values of σ are preferable in order to obtain a tight bound for the generalization error. However, note that when σ is large, the exponential behaves almost linearly, i.e. the projection loses its non-linear power.  A. The bound for K 3  A(x, x′) and K 3  A than for K 2  4 CONCLUSION  In this paper, we extended the theoretical analysis of the (ǫ, γ, τ )-good similarity framework. Using the algorithmic robustness setting, we derived new generalization bounds for different similarity functions. It turns out that the smaller the lipschitz constant of those similarity functions, the tighter the consistency bounds. This opens the door to new lines of research in metric learning (Bellet et al., ||A||2 is as 2013; 2015) aiming at maximizing the (ǫ, γ, τ )-goodness of similarity functions s.t. small as possible (see pioneer works like Bellet et al. (2012; 2011)).  ACKNOWLEDGEMENTS  Funding for this project was provided by a grant from R´egion Rhˆone-Alpes.  REFERENCES Balcan, M.-F., Blum, A., and Srebro, N. Improved guarantees for learning via similarity functions. In Servedio,  R. A. and Zhang, T. (eds.), COLT, pp. 287–298. Omnipress, 2008.  Bellet, A., Habrard, A., and Sebban, M. Learning good edit similarities with generalization guarantees. In  ECML PKDD 2011, pp. 188–203, 2011.  Bellet, A., Habrard, A., and Sebban, M. Similarity learning for provably accurate sparse linear classiﬁcation.  In ICML 2012, pp. 1871–1878, 2012.  Bellet, A., Habrard, A., and Sebban, M. A survey on metric learning for feature vectors and structured data.  arXiv preprint arXiv:1306.6709, 2013.  Bellet, A., Habrard, A., and Sebban, M. Metric Learning. Synthesis Lectures on Artiﬁcial Intelligence and  Machine Learning. Morgan & Claypool Publishers, 2015.  Kolmogorov, A. and Tikhomirov, V. ǫ-entropy and ǫ-capacity of sets in functional spaces. American Mathe-  matical Society Translations, 2(17):277–364, 1961.  van der Vaart, A. and Wellner, J. Weak Convergence and Empirical Processes. Springer series in statistics.  Springer, 1996.  Xu, H. and Mannor, S. Robustness and generalization. Machine Learning, 86(3):391–423, 2012.  Zhu, J., Rosset, S., Hastie, T., and Tibshirani, R. 1-norm support vector machines. In NIPS, pp. 16. MIT Press,  2003.  4  ",
1504.00028,2015, Real-World Font Recognition Using Deep Network and Domain Adaptation,"['Real-World Font Recognition Using Deep Network and Domain Adaptation', 'Zhangyang Wang', 'Jianchao Yang', 'Hailin Jin', 'Eli Shechtman', 'Aseem Agarwala', 'Jon Brandt', 'and Thomas Huang']",https://arxiv.org/pdf/1504.00028,"5 1 0 2    r a     M 1 3      ]  V C . s c [      1 v 8 2 0 0 0  .  4 0 5 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  REAL-WORLD FONT RECOGNITION USING DEEP NET- WORK AND DOMAIN ADAPTATION  Zhangyang Wang & Thomas S. Huang Beckman Institute for Advanced Science and Technology University of Illinois at Urbana-Champaign Urbana, IL 61801, USA {zwang119, t-huang1}@illinois.edu  Jianchao Yang & Hailin Jin & Eli Shechtman & Aseem Agarwala & Jonathan Brandt Adobe Research San Jose, CA, 95110 {jiayang, hljin, elishe, asagarwa, jbrandt}@adobe.com  ABSTRACT  We address a challenging ﬁne-grain classiﬁcation problem: recognizing a font style from an image of text. In this task, it is very easy to generate lots of ren- dered font examples but very hard to obtain real-world labeled images. This real- to-synthetic domain gap caused poor generalization to new real data in previous methods (Chen et al. (2014)). In this paper, we refer to Convolutional Neural Net- works, and use an adaptation technique based on a Stacked Convolutional Auto- Encoder that exploits unlabeled real-world images combined with synthetic data. The proposed method achieves an accuracy of higher than 80% (top-5) on a real- world dataset.  1  INTRODUCTION  This paper studies font recognition, i.e. identifying a particular typeface given an image of a text fragment. To apply machine learning to this problem, we require realistic text images with ground truth font labels. However, such data is scarce and expensive to obtain, since it requires a high level of domain expertise which is out of reach of most people. Therefore, it is infeasible to collect a sufﬁcient set of real-world training images. One way to overcome the training data challenge is to synthesize the training set by rendering text fragments for all the necessary fonts. However, we must face the domain mismatch between synthetic and real-world text images (Chen et al. (2014)). Characters in real-world images are spaced, stretched and distorted in numerous ways. In (Chen et al. (2014)), the authors tried to overcome this difﬁculty by adding different degradations to synthetic data. In the end, introducing all possible real-world degradations into the training data is infeasible. We address this domain mismatch problem in font recognition, by further leveraging a large corpus of synthetic data to train a Convolutional Neural Network (CNN), while introducing an adaptation technique based on Stacked Convolutional Auto-Encoder (SCAE) with the help of unlabeled real- world images. The proposed method reaches an impressive performance on real-world test images.  2 MODEL  Our basic CNN architecture is similar to the popular ImageNet CNN structure in (Krizhevsky et al. (2012)), as depicted in Fig. 1. The numbers along with the network pipeline specify the dimensions of outputs of corresponding layers. When the CNN model trained fully on a synthetic dataset, it witnesses a signiﬁcant performance drop when testing on real-world data, compared to when applied to another synthetic validation set. This also happens with other models such as in Chen et al. (2014), which uses training and testing sets of similar properties to ours. This alludes to discrepancies between the distributions of synthetic and and real-world examples.  1  Accepted as a workshop contribution at ICLR 2015  Figure 1: The CNN architecture, and its decomposition marked by different colors (N=8, K=2).  Tradition approaches to handle this gap include pre-processing steps applied on the training and/or testing data (Chen et al. (2014)). The domain adaptation method in Glorot et al. (2011) extracts low- level features that represent both the synthetic and real-world data, based on a stacked auto-encoder (SAE). We extend the method in Glorot et al. (2011) to decompose the N basic CNN layers into two sub-network parts. The ﬁrst K layers accounts for extracting low-level visual features shared by both synthetic and real-world data, and will be learned in a unsupervised way, using unlabeled data from both domains. The remaining N − K layers accounts for learning higher-level discriminative features for classiﬁcation. It will be trained in a supervised way on top of the ﬁrst part, using labeled data from the synthetic domain only. To train the ﬁrst K layers, we exploit a Stacked Convolutional Auto-Encoder (SCAE) (Masci et al. (2011)). Its ﬁrst two convolutional layers have an identical topology to the ﬁrst two layers in Fig. 1. Moreover, we set its ﬁrst and second half to be mirror-symmetrical. The cost function is the mean squared error (MSE) between the input and reconstructed patches. After SCAE is learned, its Conv. Layers 1 and 2 are imported to the CNN in Fig. 1. We adopt the SCAE implementation by Paine et al. (2014). We also ﬁnd that applying label-preserving data augmentations to synthetic training data helps re- duce the domain mismatch. Chen et al. (2014) added moderate distortions and corruptions, including noise, blur, rotations and shading effects. In addition, we also vary the character spacings and aspect ratios when rendering training data. Note that these steps are not useful for the method in Chen et al. (2014) because it exploits very localized features, but they are very helpful in our case.  3 EXPERIMENTS  We implemented and evaluated the local feature embedding-based algorithm (LFE) in (Chen et al. (2014)) as a baseline, and compare it with our model. A SCAE is ﬁrst trained on a large collection of both synthetic data and unlabeled real world data, and then exports the ﬁrst K = 2 convolutional layers. The next N − K layers are trained on labeled synthetic data covering 2,383 classes. That makes our problem quite ﬁne-grain. Testing is conducted on the the VFRWild325 dataset used by (Chen et al. (2014)), in term of top-1 and top-5 classiﬁcation errors. Our model achieves 38.15% in top-1 error and 20.62% in top-5, which outperforms 6% and 10% over LFE, respectively.  REFERENCES Chen, G., Yang, J., Jin, H., Brandt, J., Shechtman, E., Agarwala, A., and Han, T. X. Large-scale  visual font recognition. In Proceedings of CVPR, pp. 3598–3605. IEEE, 2014.  Glorot, X., Bordes, A., and Bengio, Y. Domain adaptation for large-scale sentiment classiﬁcation:  A deep learning approach. In Proceedings of ICML, pp. 513–520, 2011.  Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional  neural networks. In Proceedings of NIPS, pp. 1097–1105, 2012.  Masci, J., Meier, U., Cires¸an, D., and Schmidhuber, J. Stacked convolutional auto-encoders for  hierarchical feature extraction. In Proceedings of ICANN, pp. 52–59. Springer, 2011.  Paine, T., Khorrami, P., Han, W., and Huang, T. S. An analysis of unsupervised pre-training in light  of recent advances. arXiv preprint arXiv:1412.6597, 2014.  2  ",
1412.6514,2015, Score Function Features for Discriminative Learning,"['Score Function Features for Discriminative Learning', 'Majid Janzamin', 'Hanie Sedghi', 'and Anima Anandkumar']",https://arxiv.org/pdf/1412.6514,"5 1 0 2    r p A 9 1         ]  G L . s c [      2 v 4 1 5 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  SCORE FUNCTION FEATURES FOR DISCRIMINATIVE LEARNING  Majid Janzamin Department of Electrical Engineering and Computer Science University of California Irvine, CA 92697, USA mjanzami@uci.edu  Hanie Sedghi Department of Electrical Engineering University of Southern California Los Angeles, CA 90089, USA hsedghi@usc.edu  Anima Anandkumar Department of Electrical Engineering and Computer Science University of California Irvine, CA 92697, USA a.anandkumar@uci.edu  ABSTRACT  Feature learning forms the cornerstone for tackling challenging learning problems in domains such as speech, computer vision and natural language processing. In this paper, we consider a novel class of matrix and tensor-valued features, which can be pre-trained using unlabeled samples. We present efﬁcient algorithms for extracting discriminative information, given these pre-trained features and labeled samples for any related task. Our class of features are based on higher-order score functions, which capture local variations in the probability density function of the input. We establish a theoretical framework to characterize the nature of discrim- inative information that can be extracted from score-function features, when used in conjunction with labeled samples. We employ efﬁcient spectral decomposition algorithms (on matrices and tensors) for extracting discriminative components. The advantage of employing tensor-valued features is that we can extract richer discriminative information in the form of an overcomplete representations. Thus, we present a novel framework for employing generative models of the input for discriminative learning.  Keywords: Feature learning, semi-supervised learning, self-taught learning, pre-training, score function, spectral decomposition methods, tensor methods.  1  INTRODUCTION  Having good features or representations of the input data is critical to achieving good performance in challenging machine learning tasks in domains such as speech, computer vision and natural language processing (Bengio et al., 2013). Traditionally, feature engineering relied on carefully hand-crafted features, tailored towards a speciﬁc task: a laborious and a time-consuming process. Instead, the recent trend has been to automatically learn good features through various frameworks such as deep learning (Bengio et al., 2013), sparse coding (Raina et al., 2007), independent component analysis (ICA) (Le et al., 2011), Fisher kernels (Jaakkola et al., 1999), and so on. These approaches are  A longer version of this work is available on arXiv: http://arxiv.org/abs/1412.2863.  1  Accepted as a workshop contribution at ICLR 2015  unsupervised and can thus exploit the vast amounts of unlabeled samples, typically present in these domains.  A good feature representation incorporates important prior knowledge about the input, typically through a probabilistic model. In almost every conceivable scenario, the probabilistic model needs to incorporate latent variables to ﬁt the input data. These latent factors can be important explanatory variables for classiﬁcation tasks associated with the input. Thus, incorporating generative models of the input can hugely boost the performance of discriminative tasks.  Many approaches to feature learning focus on unsupervised learning, as described above. The hy- pothesis behind employing unsupervised learning is that the input distribution is related to the as- sociative model between the input and the label of a given task, which is reasonable to expect in most scenarios. When the distribution of the unlabeled samples, employed for feature learning, is the same as the labeled ones, we have the framework of semi-supervised learning. A more gen- eral framework, is the so-called self-taught learning, where the distribution of unlabeled samples is different, but related to the labeled ones (Raina et al., 2007). Variants of these frameworks include transfer learning, domain adaptation and multi-task learning (Bengio, 2011), and involve labeled datasets for related tasks. These frameworks have been of extensive interest to the machine learning community, mainly due to the scarcity of labeled samples for many challenging tasks. For instance, in computer vision, we have a huge corpus of unlabeled images, but a more limited set of labeled ones. In natural language processing, it is extremely laborious to annotate the text with syntactic and semantic parses, but we have access to unlimited amounts of unlabeled text.  It has been postulated that humans mostly learn in an unsupervised manner (Raina et al., 2007), gath- ering “common-sense” or “general-purpose” knowledge, without worrying about any speciﬁc goals. Indeed, when faced with a speciﬁc task, humans can quickly and easily extract relevant information from the accrued general-purpose knowledge. Can we design machines with similar capabilities? Can we design algorithms which succinctly summarize information in unlabeled samples as general- purpose features? When given a speciﬁc task, can we efﬁciently extract relevant information from general-purpose features? Can we provide theoretical guarantees for such algorithms? These are indeed challenging questions, and we provide some concrete answers in this paper.  2 SUMMARY OF RESULTS  In this paper, we consider a class of matrix and tensor-valued “general-purpose” features, pre-trained using unlabeled samples. We assume that the labels are not present at the time of feature learning. When presented with labeled samples, we leverage these pre-trained features to extract discrimi- native information using efﬁcient spectral decomposition algorithms. As a main contribution, we provide theoretical guarantees on the nature of discriminative information that can be extracted with our approach.  We consider the class of features based on higher-order score functions of the input, which involve higher-order derivatives of the probability density function (pdf). These functions capture “local manifold structure” of the pdf. While the ﬁrst-order score function is a vector (assuming a vector input), the higher-order functions are matrices and tensors, and thus capture richer information about the input distribution. Having access to these matrix and tensor-valued features allows to extract better discriminative information, and we characterize its precise nature in this work.  Given score-function features and labeled samples, we extract discriminative information based on the method of moments. We construct cross-moments involving the labels and the input score fea- tures. Our main theoretical result is that these moments are equal to the expected derivatives of the label, as a function of the input or some model parameters. In other words, these moments capture variations of the label function, and are therefore informative for discriminative tasks.  We employ spectral decomposition algorithms to ﬁnd succinct representations of the moment ma- trices/tensors. These algorithms are fast and embarrassingly parallel. See (Anandkumar et al., 2014a;b;c) for details, where we have developed and analyzed efﬁcient tensor decomposition al- gorithms (along with our collaborators). The advantage of the tensor methods is that they do not suffer from spurious local optima, compared to typical non-convex problems such as expectation maximization or backpropagation in neural networks. Moreover, we can construct overcomplete representations for tensors, where the number of components in the representation can exceed the  2  Accepted as a workshop contribution at ICLR 2015  Unsupervised estimation of  score functions  Unlabeled data: {xi}  General-purpose features:  Score functions Sm(x) := (−1)m ∇(m)p(x)  ,  p(x)  where x ∼ p(·)  Using score functions to  extract discriminative features  in the supervised setting  Form cross-moments: E [y · Sm(x)]  Labeled data:  {(xi, yi)}  Our result: obtaining derivatives of label function:  E [y · Sm(x)] = Eh∇(m)G(x)i ,  when E[y|x] := G(x)  Spectral/tensor method:  ﬁnd uj’s s.t. Eh∇(m)G(x)i = X  j∈[k]  u⊗m  j  Extract discriminative features using uj’s/  do model-based prediction with uj’s as parameters  Figure 1: Overview of the proposed framework of using the general-purpose features to generate discrimina- tive features through spectral methods.  data dimensionality. It has been argued that having overcomplete representations is crucial to getting good classiﬁcation performance (Coates et al., 2011). Thus, we can leverage the latest advances in spectral methods for efﬁcient extraction of discriminative information from moment tensors.  In our framework, the label can be a scalar, a vector, a matrix or even a tensor, and it can either be continuous or discrete. We can therefore handle a variety of regression and classiﬁcation settings such as multi-task, multi-class, and structured prediction problems. Thus, we present a uniﬁed and an efﬁcient end-to-end framework for extracting discriminative information from pre-trained features. An overview of the entire framework is presented in Figure 1 which is fully explained later in Section 3.  We now provide some important observations below.  Are the expected label function derivatives informative? Our analysis characterizes the dis- criminative information we can extract from score function features. As described above, we prove that the cross-moments between the label and the score function features are equal to the expected derivative of the label as a function of the input or model parameters. But when are these expected la- bel derivatives informative? Indeed, in trivial cases, where the derivatives of the label function vanish over the support of the input distribution, these moments carry no information. However, such cases are pathological, since then, either there is no variation in the label function or the input distribution is nearly degenerate. Another possibility is that a certain derivative vanishes, when averaged over the input distribution, even though it is not zero everywhere. If this is the case, then the next derivative cannot be averaged out to zero, and will thus carry information about the variations of the label func- tion. Thus, in practical scenarios, the cross-moments contain useful discriminative information. In fact, for many discriminative models which are challenging to learn, such as multi-layer neural net- works and mixtures of classiﬁers, we establish that these moments have an intimate relationship with the parameters of the discriminative model in subsequent works (Sedghi & Anandkumar, 2014a;b).  3  Accepted as a workshop contribution at ICLR 2015  Spectral decomposition of the moments provably recovers the model parameters. These are the ﬁrst results for guaranteed learning of many challenging discriminative latent variable models.  Contrasting with previous approaches: We now contrast our approach to previous approaches for incorporating generative models in discriminative tasks. Typically, these approaches directly feed the pre-trained features to a classiﬁer. For example, in the Fisher kernel framework, the Fisher score features are fed to a kernel classiﬁer (Jaakkola et al., 1999). The reasoning behind this is that the features obtained from unsupervised learning have information about all the classes, and the task of ﬁnding class-speciﬁc differences in the learnt representation is left to the classiﬁer. However, in practice, this may not be the case, and a common complaint is that these generative features are not discriminative for the task at hand. Previous solutions have prescribed joint training discriminative features using labeled samples, in conjunction with unlabeled samples (Mairal et al., 2009; Maaten, 2011; Wang et al., 2013). However, the resulting optimization problems are complex and expen- sive to run, may not converge to good solutions, and have to be re-trained for each new task. We present an alternative approach to extract discriminative features using efﬁcient spectral decompo- sition algorithms on moment matrices and tensors. These methods are light weight and fast, and we theoretically quantify the nature of discriminative features they can extract. These discriminative features can then be fed into the classiﬁcation pipeline. Thus, the advantage of our approach is that we can quickly generate discriminative features for new classiﬁcation tasks without going through the laborious process of re-training for new features.  We now contrast our approach with previous moment-based approaches for discriminative learning, which consider moments between the label and raw input, e.g. (Karampatziakis & Mineiro, 2014). Such methods have no theoretical guarantees. In contrast, we construct cross-moments between the label and the score function features. We show that using score function features is crucial to mining discriminative information with provable guarantees.  Extension to self-taught learning: We have so far described our framework under the semi- supervised setting, where the unlabeled and labeled samples have the same input distribution. We can also handle the framework of self-taught learning, where the two distributions are related but may not be the same. We prescribe some simple pre-processing to transfer the parameters and to re-estimate the score function features for the input of the labeled data set. Such parameter transfer frameworks have been considered before, e.g. (Raina et al., 2007), except here we present a general latent-variable framework and focus on transferring parameters for computing score functions, since we require them for subsequent operations. Our framework can also be applied to scenarios where we have different input sources with different distributions, but the classiﬁcation task is the same, and thus, the associative model between the label and the input is ﬁxed. Consider for instance, crowdsourcing applications, where the same task is presented to different groups of individuals. In our approach, we can then construct different score function features for different input sources and the different cross-moments provide information about the variations in the label function, averaged over different input distributions. We can thus leverage the diversity of different input sources for improved performance on common tasks. Thus, our approach is applicable in many challenging practical scenarios.  3 OVERVIEW OF THE FRAMEWORK  In this section, we elaborate on the end-to-end framework presented in Figure 1.  Background: The problem of supervised learning consists of learning a predictor, given labeled training samples {(xi, yi)} with input xi and corresponding label yi. Classical frameworks such as SVMs are purely discriminative since they make no distributional assumptions. However, when labeled data is limited and classiﬁcation tasks are challenging, incorporating distributional informa- tion can improve performance. In an associative model-based framework, we posit a conditional distribution for the label given the input p(y|x). However, learning this model is challenging, since maximum-likelihood estimation of p(y|x) is non-convex and NP-hard to solve in general, especially if it involves hidden variables (e.g., associative mixtures, multi-layer neural networks). In addition, incorporating a generative model for input x often leads to improved discriminative performance.  4  Accepted as a workshop contribution at ICLR 2015  Label-function derivatives are discriminative: Our main focus in this work is to extract useful information about p(y|x) without attempting to learn it in its entirety. In particular, we extract information about the local variations of conditional distribution p(y|x), as the input x (or some model parameter) is changed. For the classiﬁcation setting, it sufﬁces to consider1 E[y|x] := G(x). In this paper, we present mechanisms to estimate its expected higher order derivatives2  E[∇(m)  x G(x)], m ≥ 1,  (1)  x  where ∇(m) denotes the m-th order derivative operator w.r.t. variable x. By having access to ex- pected derivatives of the label function G(x) in (1), we gain an understanding of how the label y varies as we change the input x locally, which is valuable discriminative information.  Score functions yield label-function derivatives: One of the main contributions of this paper is to obtain these expected derivatives in (1) using features denoted by Sm(x), for m ≥ 1 (learnt from unlabeled samples) and the labeled data. In particular, we form the cross-moment between the label y and the features Sm(x), and show that they yield the derivatives as3  E[y · Sm(x)] = E[∇(m)G(x)], when E[y|x] := G(x).  (2)  We establish a simple form for features Sm(x), based on the derivatives of the probability density function p(·) of the input x as  Sm(x) = (−1)m ∇(m)p(x)  p(x)  , when x ∼ p(·).  (3)  In fact, we show that the feature Sm(x) deﬁned above is a function of higher order score functions ∇(n) x log p(x) with n ≤ m, and we derive an explicit relationship between them. This is basically why we also call these features as (higher order) score functions. Note that the features Sm(x) can be learnt using unlabeled samples, and we term them as general-purpose features since they can be applied to any labeled dataset, once they are estimated. Note the features Sm(x) can be vectors, matrices or tensors, depending on m, for multi-variate x. The choice of order m depends on the particular setup: a higher m yields more information (in the form of higher order derivatives) but requires more samples to compute the empirical moments accurately.  We then extend the framework to parametric setting, where we obtain derivatives E[∇(m) θ G(x; θ)] with respect to some model parameter θ when E[y|x; θ] := G(x; θ). These are obtained using general-purpose features denoted by Sm(x; θ) which is a function of higher order Fisher score func- tions ∇(n) log p(x; θ) with n ≤ m. Note that by using the parametric framework we can now incorporate discrete input x, while this is not possible with the previous framework.  θ  Spectral decomposition of derivative matrices/tensors: Having obtained the derivatives E[∇(m)G(x)] (which are matrices or tensors), we then ﬁnd efﬁcient representations using spec- tral/tensor decomposition methods. In particular, we ﬁnd vectors uj such that  E[∇(m)G(x)] = X  j∈[k]  m times  z { uj ⊗ uj ⊗ · · · ⊗ uj ,  }|  (4)  where ⊗ refers to the tensor product notation. Note that since the higher order derivative is a sym- metric matrix/tensor, the decomposition is also symmetric. Thus, we decompose the matrix/tensor at hand into sum of rank-1 components, and in the matrix case, this reduces to computing the SVD. In the case of a tensor, the above decomposition is termed as CP decomposition (Kruskal, 1977). In a series of works (Anandkumar et al., 2014a;b;c), we have presented efﬁcient algorithms for obtaining (4), and analyzed their performance in detail.  1In the classiﬁcation setting, powers of y, e.g., y2 contain no additional information, and hence, all the information of the associative model is in E[y|x] := G(x). However, in the regression setting, we can compute additional functions, e.g., E[∇(m)H(x)], where E[y2|x] := H(x). Our approach can also compute these derivatives.  2Note that since we are computing the expected derivatives, we also assume a distribution for the input x. 3We drop subscript x in the derivative operator ∇(m)  saying ∇(m) when there is no ambiguity.  x  5  Accepted as a workshop contribution at ICLR 2015  The matrix/tensor in hand is decomposed into a sum of k rank-1 components. Unlike matrices, for tensors, the rank parameter k can be larger than the dimension. Therefore, the decomposition prob- lems falls in to two different regimes. One is the undercomplete regime: where k is less than the dimension, and the overcomplete one, where it is not. The undercomplete regime leads to dimen- sionality reduction, while the overcomplete regime results in richer representation. Once we obtain components uj, we then have several options to perform further processing. We j x), using some non-linear function σ(·), as per- can extract discriminative features such as σ(u⊤ formed in some of the earlier works, e.g., (Karampatziakis & Mineiro, 2014). Alternatively, we can perform model-based prediction and incorporate uj’s as parameters of a discriminative model. In a subsequent paper, we show that uj’s correspond to signiﬁcant parameters of many challenging discriminative models such as multi-layer feedforward neural networks and mixture of classiﬁers, under the realizable setting.  Extension to self-taught learning: The results presented so far assume the semi-supervised set- ting, where the unlabeled samples {˜xi} used to estimate the score functions are drawn from the same distributions as the input {xi} of the labeled samples {(xi, yi)}. We present simple mechanisms to extend to the self-taught setting, where the distributions of {˜xi} and {xi} are related, but not the same. We assume latent-variable models for ˜x and x, e.g., sparse coding, independent component analysis (ICA), mixture models, restricted Boltzmann machine (RBM), and so on. We assume that the conditional distributions p(˜x|˜h) and p(x|h), given the corresponding latent variables ˜h and h are the same. This is reasonable since the unlabeled samples {˜xi} are usually “rich” enough to cover all the elements. For example, in the sparse coding setting, we assume that all the dictionary elements can be learnt through {˜xi}, which is assumed in a number of previous works, e.g (Raina et al., 2007; Zhang et al., 2008). Under this assumption, estimating the score function for new samples {xi} is relatively straightforward, since we can transfer the estimated conditional distribution p(˜x|˜h) (using unlabeled samples {˜xi}) as the estimation of p(x|h), and we can re-estimate the marginal distribu- tion p(h) easily. Thus, the use of score functions allows for easy transfer of information under the self-taught framework. The rest of the steps can proceed as before.  4 CONCLUSION  We provided a general framework for proposing discriminative features by introducing higher order score functions. The framework can be applied in semi-supervised and self-taught learning settings. We ﬁrst use the unlabeled data to estimate the score function. We then show that the score function yields the expected label-function derivative by forming the cross-moment between the label and the score function. Then we use spectral methods to extract discriminative features by decomposing the higher order derivatives of label-function into rank-1 components. We apply this framework for learning several challenging models such as multi-layer neural networks and mixtures of classiﬁers in subsequent works (Sedghi & Anandkumar, 2014a;b).  ACKNOWLEDGMENTS  M. Janzamin thanks Rina Panigrahy for useful discussions. M. Janzamin is supported by NSF Award CCF-1219234. H. Sedghi is supported by ONR Award N00014-14-1-0665. A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF Career award CCF-1254106, NSF Award CCF-1219234, ARO YIP Award W911NF-13-1-0084 and ONR Award N00014-14-1-0665.  REFERENCES Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., and Telgarsky, M. Tensor Methods for Learning  Latent Variable Models. J. of Machine Learning Research, 15:2773–2832, 2014a.  Anandkumar, Anima, Ge, Rong, and Janzamin, Majid. Guaranteed Non-Orthogonal Tensor Decom-  position via Alternating Rank-1 Updates. arXiv preprint arXiv:1402.5180, Feb. 2014b.  Anandkumar, Anima, Ge, Rong, and Janzamin, Majid. Sample Complexity Analysis for Learning Overcomplete Latent Variable Models through Tensor Methods. arXiv preprint arXiv:1408.0553, Aug. 2014c.  6  Accepted as a workshop contribution at ICLR 2015  Bengio, Yoshua. Deep learning of representations for unsupervised and transfer learning. In ICML  Workshop on Unsupervised and Transfer Learning, 2011.  Bengio, Yoshua, Courville, Aaron, and Vincent, Pascal. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798– 1828, 2013.  Coates, Adam, Ng, Andrew Y, and Lee, Honglak. An analysis of single-layer networks in unsuper- vised feature learning. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 215–223, 2011.  Jaakkola, Tommi, Haussler, David, et al. Exploiting generative models in discriminative classiﬁers.  In Advances in neural information processing systems, pp. 487–493, 1999.  Karampatziakis, Nikos and Mineiro, Paul. Discriminative features via generalized eigenvectors. In  Proceedings of The 31st International Conference on Machine Learning, pp. 494–502, 2014.  Kruskal, J.B. Three-way arrays: Rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics. Linear algebra and its applications, 18(2):95–138, 1977.  Le, Q. V., Karpenko, A., Ngiam, J., and Ng, A. Y.  ICA with Reconstruction Cost for Efﬁcient  Overcomplete Feature Learning. In NIPS, pp. 1017–1025, 2011.  Maaten, Laurens. Learning discriminative ﬁsher kernels. In Proceedings of the 28th International  Conference on Machine Learning (ICML-11), pp. 217–224, 2011.  Mairal, Julien, Ponce, Jean, Sapiro, Guillermo, Zisserman, Andrew, and Bach, Francis R. Super- vised dictionary learning. In Advances in neural information processing systems, pp. 1033–1040, 2009.  Raina, Rajat, Battle, Alexis, Lee, Honglak, Packer, Benjamin, and Ng, Andrew Y. Self-taught learn- ing: transfer learning from unlabeled data. In Proceedings of the 24th international conference on Machine learning, pp. 759–766. ACM, 2007.  Sedghi, Hanie and Anandkumar, Anima. Provable methods for training neural networks with sparse  connectivity. NIPS workshop on Deep Learning and Representation Learning, Dec. 2014a.  Sedghi, Hanie and Anandkumar, Anima. Provable Tensor Methods for Learning Mixtures of Clas-  siﬁers. arXiv preprint arXiv:1412.3046, Dec. 2014b.  Wang, Hua, Nie, Feiping, and Huang, Heng. Robust and discriminative self-taught learning. In  Proceedings of The 30th International Conference on Machine Learning, pp. 298–306, 2013.  Zhang, Jian, Ghahramani, Zoubin, and Yang, Yiming. Flexible latent variable models for multi-task  learning. Machine Learning, 73(3):221–242, 2008.  7  ",
1410.7455,2015, Parallel training of DNNs with Natural Gradient and Parameter Averaging,"['Parallel training of DNNs with Natural Gradient and Parameter Averaging', 'Daniel Povey', 'Xioahui Zhang', 'and Sanjeev Khudanpur']",https://arxiv.org/pdf/1410.7455,"5 1 0 2     n u J    2 2      ] E N . s c [      8 v 5 5 4 7  .  0 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  PARALLEL TRAINING OF DNNS WITH NATURAL GRA- DIENT AND PARAMETER AVERAGING  Daniel Povey, Xiaohui Zhang & Sanjeev Khudanpur Center for Language and Speech Processing & Human Language Technology Center of Excellence, The Johns Hopkins University, Baltimore, MD 21218, USA {dpovey@gmail.com}, {xiaohui,khudanpur@jhu.edu}  ABSTRACT  We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network trafﬁc. Our method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efﬁcient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic- averaging method to work well, as well as substantially improving the conver- gence of SGD on a single machine.  1  INTRODUCTION  Parallel training of neural networks generally makes use of some combination of model parallelism and data parallelism (Dean et al., 2012), and the normal approach to data parallelism involves com- munication of model parameters for each minibatch. Here we describe our neural-net training frame- work which uses a different version of data parallelism: we have multiple SGD processes on separate machines, and only infrequently (every minute or so) average the model parameters and redistribute them to the individual machines. This is very effective for us for large-scale training of systems for speech recognition– but in our case it only works well when combined with an efﬁcient imple- mentation of natural gradient stochastic gradient descent (NG-SGD) that we have developed. We don’t attempt in this paper to develop a framework that explains why parameter averaging should work well despite non-convexity of DNNs, or why NG-SGD is so helpful. The point of this paper is to describe our methods and to establish that empirically they work well. The signiﬁcance of this work is that we show that it is possible to get a linear speedup when increasing the number of GPUs, without requiring frequent data transfer (however, this only holds up to about 4 or 8 GPUs).  In Section 2 we describe our problem setting, which is Deep Neural Networks (DNNs) applied to speech recognition– although our ideas are more general than this. In Section 3 we introduce the parallel training method. In Section 4 we describe the general ideas behind our natural gradient method, although most of the technical details have been relegated to appendices. In this paper we don’t give any proofs, but we do discuss in Section 5 what we think we can and can’t be proven about our methods. Section 6 has experiments on the convergence of SGD with and without natural gradient and parallelism. We conclude in Section 7.  There are two versions of our NG-SGD method: a “simple” version and an “online” one. Technical details for these are in Appendices A and B respectively. Appendix C has background information on our DNN implementation.  2 PROBLEM SETTING  When training DNNs for speech recognition, the immediate problem is that of classifying vectors x ∈ RD as corresponding to discrete labels y ∈ Y. The dimension D is typically several hundred,  1  Accepted as a workshop contribution at ICLR 2015  with x being derived from short-time spectral properties of the acoustic signal; and Y corresponds to clustered HMM states of context-dependent phones, |Y| ≃ 5000 being typical. Each (x, y) pair corresponds to a single frame of speech data; frames are typically extracted at the rate of 100 per second, with a duration of 25 milliseconds, and x contains spectral information from several adjacent frames spliced together (Seide et al., 2011b). We are ultimately not just interested in the top y on each frame, but in the log-probabilities log p(y|x) for all y, since we will use them as costs in a Viterbi search for a path corresponding to the most likely word sequence. The objective function for training is the sum, over all frames of training data, of the log-probability of y given  x: Pi log p(yi|xi). Since we are maximizing this, our use of the term SGD is of course a slight  misnomer; it is gradient ascent. The supervision labels y are derived from a Viterbi alignment of a Hidden Markov Model (HMM) derived from the reference word sequence of each training utterance.  3 SGD WITH PARAMETER AVERAGING  3.1 PARAMETER-AVERAGING OVERVIEW  The parameter-averaging aspect of our training is quite simple. We have N machines (e.g. N = 4) each doing SGD separately with different randomized subsets of the training data, and we allow their parameters to gradually diverge. After each machine has processed a ﬁxed number of samples K (typically K = 400 000), we average the parameters across all the jobs and re-distribute the result to the machines. (In practice we do this by spawning new jobs in GridEngine or in whatever job management system we are using). This is repeated until we have processed all the data for a speciﬁed number of epochs, e.g. 10.  We deﬁne an outer iteration of training as the time it takes for each job to process K training examples. The number of outer iterations per epoch depends on K and the quantity of training data. We ﬁnd it useful to deﬁne the effective learning rate of the parallel SGD procedure as the learning rate ηt being used by the individual jobs, divided by the number of jobs N . As we increase the number of jobs N , in order to get a linear speed up we need to increase the learning rate proportional to N so that the effective learning rate stays the same. The concept is that when we do the parameter averaging, the parameter update from any individual SGD job gets diluted N -fold. The reason why we set this up as parameter-averaging instead of summing the parameter changes from the respective jobs, is out of concern for stability. Imagine there is some direction in parameter space where the Hessian is large enough (and our learning rate large enough) that stochastic gradient descent nearly reaches equilibrium after processing K samples. If there are, say, 4 jobs, then after processing K examples and summing the parameter changes, the parameters end up not close to the equilibrium value but off in the opposite direction and 3 times farther away than at the start. It is clear that this would lead to divergence.  3.2 OTHER ASPECTS OF OUR SGD IMPLEMENTATION  At this point we provide some more details of other relevant features of our SGD implementation, namely the learning rate schedule and the way we enforce a maximum parameter change to prevent divergence.  There are some other, less directly relevant issues which we have relegated to Appendix C: namely, CPU versus GPU-based SGD (C.1); data randomization issues (C.2); generalized model averag- ing (C.4); mixture components a.k.a. sub-classes (C.5); input data normalization (C.6); parameter initialization (C.7); sequence training (C.8); and online decoding with i-vectors (C.9).  3.2.1 LEARNING RATE SCHEDULE  It was found in (Senior et al., 2013) that when training DNNs for speech recognition, an exponen- tially decreasing learning rate works well, and we independently found the same thing. We generally use a learning rate that decreases by a factor of 10 during training, on an exponential schedule. Un- less mentioned otherwise, for experiments reported here the learning rate starts at 0.01 and ends at 0.001. We specify the number of epochs in advance; it is typically a number in the range 4 to 20 (if we have more data, we train for fewer epochs).  2  Accepted as a workshop contribution at ICLR 2015  Note that while in the experiments presented here we did not separately tune the learning rate sched- ule for SGD and NG-SGD (we just used values which had worked well in the past for NG-SGD), we have done extensive experiments in the past, on smaller setups, where the learning rate schedule was tuned independently; and in all circumstances we found NG-SGD to be helpful. It was not feasible to repeat those experiments on a large-scale setup.  3.2.2 MAXIMUM PARAMETER CHANGE  A common pathology when doing SGD for deep learning is that during training, the parameters will suddenly start getting very large and the objective function will go to negative inﬁnity. This is known as parameter divergence. The normal solution is to decrease the learning rate and start the training again, but this is a very inconvenient. To avoid this pathology, we modiﬁed the SGD procedure to enforce a maximum parameter change per minibatch. This limit tends to be active only early in training, particularly for layers closer to the output. We have provided further details on this in Appendix C.3.  4 NATURAL GRADIENT FOR SGD  In this section we describe our natural-gradient modiﬁcation to SGD, in which we scale the gradients by a symmetric positive deﬁnite matrix that is an approximation to the inverse of the Fisher matrix.  Technically speaking, Natural Gradient means taking a step along the gradient of a Riemannian parameter surface, which follows a curving path in conventional parameter space and which is ex- tremely hard to compute. However, previous work (Yang & Amari, 1998; Roux et al., 2007) has used the term “Natural Gradient” to describe methods like ours which use an an approximated inverse- Fisher matrix as the learning rate matrix, so we follow their precedent in calling our method “Natural Gradient”.  4.1 WE CAN REPLACE THE SCALAR LEARNING RATE IN SGD WITH A MATRIX  In SGD, the learning rate is often assumed to be a scalar ηt, decreasing with time, and the update equation is something like  θt+1 = θt + ηtgt  where gt is the objective-function gradient sampled on time t (e.g. computed from a training sample or a minibatch). However, it is possible to replace this scalar with a symmetric positive deﬁnite matrix, and we can write instead:  θt+1 = θt + ηtEtgt  (1) with Et the matrix component of learning-rate; it is more convenient for proofs to keep ηt separate rather than absorbing it into Et. It acceptable for Et to be random: if we can bound the eigenvalues of Et above and below, by positive constants known in advance, and Et and gt are independently sampled given the parameter θ, then we can prove convergence under the same kinds of conditions as if we were using a scalar learning rate (Bottou, 1998, Sec. 4.2.2)  In general, the learning-rate matrix should not be a function of the data sample which we are cur- rently processing, or it may prevent convergence to a local optimum. As an example of this, a matrix that was systematically smaller for a particular type of training data would clearly bias the learning by downweighting that data.  4.2 THE INVERSE FISHER MATRIX IS A SUITABLE LEARNING-RATE MATRIX  There are reasons from statistical learning theory, related to the Natural Gradient idea (Amari, 1998), why we may want to set the learning-rate matrix Et to the inverse of the Fisher information matrix. See for example, (Murata & Amari, 1999) and (Roux et al., 2007). The Fisher matrix is most directly deﬁned for situations where we are learning a distribution, as opposed to classiﬁcation problems such as the current one. Suppose x, which may be discrete or continuous, is the variable whose distribution we are modeling, and f (x; θ) is the probability or likelihood of x given parameters θ, then the Fisher information matrix I(θ) is deﬁned as the expected outer product (second moment)  3  Accepted as a workshop contribution at ICLR 2015  of the derivative of the log-probability w.r.t. the parameters, i.e. of  ∂ ∂θ  log f (x; θ).  This derivative is called the “score” in information theory. Part of the justiﬁcation for this use of the Fisher matrix is that, under certain conditions, the Fisher matrix is identical to the Hessian; and it is obvious why the inverse Hessian would be a good gradient descent direction. These conditions are quite stringent, and include that the model is correct and θ is at the value corresponding to the true data distribution; but even if these conditions do not apply, the Fisher information matrix is in some sense “dimensionally” the same as the Hessian– that is, it transforms the same way under changes of parameterization– so its inverse may still be a good choice of learning-rate matrix.  It is quite easy to generalize the notion of the Fisher matrix to a prediction task p(y; x, θ). We write p(y, x; θ) = q(x)p(y; x, θ) for a data distribution q(x) that we assume to be independently known (and not a function of θ). It is not hard to see that the score equals just ∂ ∂θ log f (x; y, θ); since q(x) does not depend on θ, there is no additional term involving q(x). The expectation that we take when computing the Fisher matrix is taken over the joint distribution of x and y. This argument also appears in (Roux et al., 2007, Section 3).  Still more generally, we can compute a quantity that is analogous to Fisher matrix for any objective function, even one that does not represent a log-probability or log-likelihood; and we will still have a matrix that transforms in the same way as the Hessian under changes of variables - i.e. its inverse may still be a reasonable choice for a learning-rate matrix.  4.3 WE NEED TO APPROXIMATE THE FISHER MATRIX IN PRACTICE  For large-scale problems, such as DNNs for speech recognition with millions of parameters, even one inversion of the Fisher matrix is impractical because it would take time O(n3) in the parameter dimension. However, it may be practical to deal with factored forms of it. There has been previous literature on this. In (Roux et al., 2007), the Fisher matrix was divided into diagonal blocks and each block was approximated by a low-rank matrix. The idea of diagonal blocks was also explored in (Bastian et al., 2011), with one block per weight matrix; our approach uses the same idea. In the unpublished manuscript (Yang & Amari, 1997) (some of the material in which was later published in (Yang & Amari, 1998)), the authors attempted to show analytically that under certain quite strong assumptions, the Fisher matrix for a single-hidden-layer neural network has the form of a Kronecker product. Although we are interested in more general networks than they considered, the Kronecker product does also appear in our factorization of the Fisher matrix.  We should note that there are ways to use Natural Gradient without factorizing the Fisher information matrix, if one is willing to accept a signiﬁcantly increased time per iteration. See for example (Pas- canu & Bengio, 2013), which uses a truncated Newton method to approximate multiplication by the inverse of the Fisher matrix.  4.4 OUR FACTORIZATION OF THE FISHER MATRIX  Our factored form of the Fisher matrix is as follows: given a neural network with I weight matrices, we divide the Fisher matrix into I diagonal blocks, one for each weight matrix. Consider the i’th diagonal block of the Fisher matrix, corresponding to the parameters of a weight matrix Wi, and assume that there is no separate bias term (we can append a 1 to the inputs and include the bias term in the weight matrix). The i’th block of the Fisher matrix is a Kronecker product of two symmetric positive deﬁnite matrices: Ai, whose dimension is the output (row) dimension of Wi, and Bi, whose dimension is the input (column) dimension of Wi. We further factorize the matrices Ai and Bi as a low-rank symmetric matrix plus a multiple of the identity matrix. We write the approximated Fisher matrix in the form  F = diag (A1 ⊗ B1, A2 ⊗ B2, . . . , AI ⊗ BI )  (2) where Ai and Bi are each factorized in the form λI+XXT . The order in which Ai and Bi appear in the Kronecker product depends on the way in which we vectorize the weight matrices– row-wise or column-wise. In practice we don’t ever deal explicitly with these Kronecker products or vectorized weight matrices in the algorithm, so this choice doesn’t matter. It is not hard to show that if the Fisher matrix can be factored this way, then its inverse can be factored the same way.  4  Accepted as a workshop contribution at ICLR 2015  4.5 HOW WE ESTIMATE THE FISHER MATRIX  We have two different methods for estimating the factorized Fisher matrix:  • Simple method: We estimate the Fisher matrix from the other samples in the minibatch we are currently training on, holding out the current sample to avoid bias. This can be done surprisingly efﬁciently. Details are in Appendix A.  • Online method: We estimate the Fisher matrix from all previous minibatches, using a forgetting factor to downweight minibatches that are distant in time. Details are in Ap- pendix B.  We generally use the online method as it is signiﬁcantly faster on GPUs and usually seems to lead to faster learning, probably due to the less noisy estimate of the Fisher matrix. We describe the simple method because it is easier to understand and helps to motivate the online method.  4.6 OPERATION ON VECTORS  Although we describe our Fisher matrix as as a Kronecker product, we do not have to explicitly construct this product in our code.  Suppose that we process the training examples one at a time. The SGD update for the i’th weight matrix is:  Wti = Wt−1,i + ηtxtiyT  ti  where xti is the derivative of the objective function w.r.t. output of the i’th weight matrix computed at the current sample, and yti is the input that the weight matrix acts on. These quantities naturally occur in backpropagation.  In our natural gradient method, this is modiﬁed as follows: Wti = Wt−1,i + ηtA−1  ti xtiyT  tiB−1 ti ,  where Ati and Bti are factors of the Fisher matrix. It is easy to show that this is equivalent to mul- tiplying the parameter step by the inverse of the Fisher matrix formed from the A and B quantities as in Equation (2).  4.7 OPERATION ON MINIBATCHES  Rather than processing training examples one at a time, we process them in minibatches (e.g. 512 at a time). Instead of vector-valued derivatives xti and inputs yti, we now have matrices Xti and Yti, each row of which corresponds to one of the x or y quantities (t is now the index for the minibatch). The update is now as follows:  (3) and note that unlike some authors, we don’t divide the gradients by the minibatch size– this makes it easier to tune the minibatch size and learning rate independently. The update now has the form  Wti = Wt−1,i + ηtXT  tiYti  with the bar indicating modiﬁed X and Y quantities. In the online version of our natural gradient method, we can write these as:  Wti = Wt−1,i + ηt ¯XT  ti  ¯Yti,  (4)  ¯Xti = XtiA−1 ti ¯Yti = YtiB−1 ti ,  (5) (6)  but in the simple method, because the A and B matrices are estimated from the other elements in the minibatch, we can’t write it this way– it is a separate matrix multiplication for each row of X and Y– but it can still be computed efﬁciently; see Appendix A.  In programming terms, we can describe the interface of the core natural-gradient code as follows:  • Simple method: Given a minibatch of vectors Xti with each row being one element of the minibatch, estimate the Fisher-matrix factors by holding out each sample, do the multipli- cation by their inverses, and return the modiﬁed vectors ¯Xti.  5  Accepted as a workshop contribution at ICLR 2015  • Online method: Given a minibatch of vectors Xti and a previous Fisher-matrix factor  At−1,i, compute ¯Xti = XtiA−1  t−1,i and the updated Fisher-matrix factor Ati.  The interface of the natural gradient code works the same with the Y and B quantities, as with X and A. We call the interface above 2I times for each minibatch: twice for each weight matrix in the network.  4.8 SCALING THE FACTORS  In both natural gradient methods, we want to prevent the Fisher-matrix multiplication from affecting the overall magnitude of the update very much, compared with the step-sizes in standard SGD. There are several reasons for this:  • Early in training, the x and y quantities may be very small or zero, leading to huge or  inﬁnite inverse-Fisher matrices.  • The conventional convergence-proof techniques require that the matrix component of the learning rate matrix should have eigenvalues bounded above and below by constants known in advance, which we cannot guarantee if we use an unmodiﬁed Fisher matrix.  • Empirically, we have found that it is hard to prevent parameter divergence if we use the  real, un-scaled Fisher matrix.  Our method is to scale the ¯Xti and ¯Yti quantities so that they have the same Frobenius norm as the corresponding inputs Xti and Yti. We will introduce notation for this in the Appendices. This scaling introduces a slight problem for convergence proofs. The issue is that each sample can now affect the value of its own learning-rate matrix (via the scalar factor that we use to rescale the matrices). As we mentioned before, it is not permissible in general to use a per-sample learning rate that is a function of the sample itself. However, we don’t view this as a practical problem because we never use a minibatch size less than 100, so the resulting bias is tiny.  4.9 SMOOTHING THE FISHER MATRIX FACTORS WITH THE IDENTITY  In both versions of NG-SGD, we smooth our estimates of the factors of the Fisher matrix by adding a multiple of the identity matrix before inverting them. In the simple method this is necessary because in general the Fisher matrix estimated from the minibatch will not be full rank. In the online method it is not strictly necessary because we deal with a factorization of the Fisher matrix that already contains a multiple of the unit matrix, but we found that by adding an additional multiple of the unit matrix, as for the simple method, we can improve the convergence of the SGD training. In both cases the smoothing is of the following form. If S ∈ RD×D is a Fisher matrix factor estimated directly from data as the uncentered covariance of the x or y quantities, then instead of using S as the Fisher-matrix factor A or B, we use instead S + βI, where  β = α  D max(tr (S), ǫ)  (7)  where ǫ = 10−20 is used to stop the smoothed S from ever being exactly zero. That is, we smooth the Fisher with the identity matrix scaled by α times the average diagonal element of S. We found in tuning experiments that the relatively large value α = 4 is suitable under a wide range of circum- stances, for both the simple and online methods, and even for settings where the noise in S should not be a big problem– e.g. for large minibatch sizes. Our interpretation is that when α is fairly large, we are using a smaller than normal learning rate only in a few directions where the x or y quantities have quite high covariance, and a relatively constant learning rate in all the remaining directions.  5 COMMENTS ON THE THEORY  Although this is not a theoretical paper, we would like to say what we think is, and is not, possible to prove about our methods.  6  Accepted as a workshop contribution at ICLR 2015  5.1 OUR FACTORIZATION OF THE FISHER MATRIX  i  replacing Bi.  If we assume that the distribution of the x and y quantities is Gaussian and independent (between x and y for a single layer, and between layers), then it should not be hard to show that the Fisher matrix has the form of (2), where the Ai and Bi quantities correspond to the uncentered covariances of the x and y quantities, and that the inverse-Fisher has the same form, with the A−1 replacing Ai and B−1 Of course these conditions won’t hold in practical deep learning applications, but we do believe that it’s a reasonable factorization. One could try to show this experimentally as follows, given a task. One could make a linear change of variables to make our approximated Fisher matrix equal the unit matrix, and then try to measure the eigenvalue distribution of the full Fisher matrix in the new co-ordinates. We believe that the eigenvalue distribution of the transformed Fisher matrix would probably be much more closerly centered around 1 than before the change of variables. Since our motivation for the work published here is a practical one, so we have not allocated effort towards this type of experiment.  i  5.2 THE CONVERGENCE OF OUR NG-SGD PROCEDURE  Regarding the convergence of SGD using our factored-Fisher learning rate matrices, the most we think is easily provable is that a slightly modiﬁed form of this method would converge under similar conditions to unmodiﬁed SGD.  The smoothing with constant α > 0 can give us a bound on the ratio of the largest to smallest eigenvalues of the A and B factors; using this together with the rescaling of Section 4.8, we can bound from above and below the eigenvalues of the rescaled A and B factors. By multiplying these together, we can get lower and upper bounds on the eigenvalues of the overall inverse-Fisher matrix that we use as the learning-rate matrix Et. It is necessary for the Fisher matrix to be randomly chosen independent of the identity of the current sample. Unfortunately this is not quite true due to the rescaling being done at the minibatch level; we mentioned in Section 4.8 that this would be a problem for proofs. As mentioned, it would be easy to use the rescaling factor from the previous minibatch; this gives us back the independence, but at the cost of no longer having such easy bounds on the upper and lower eigenvalues of the rescaled A and B factors. Alternately, one could keep the algorithm as it is and try to prove instead that the parameter value we converge to will not differ very much in some sense from an optimum of the true objective function, as the minibatch size gets large.  5.3 ONLINE UPDATE OF A LOW-RANK COVARIANCE MATRIX  There might be some interesting things to say about our online natural gradient method, described in Appendix B, in which estimate the uncentered covariance matrices A and B in a factored form as λI + XXT . Our online estimation of the covariance matrices involves multiplying X by a weighted combination of (a) the observed covariance matrix from the current minibatch, and (b) the previous value of our factored approximation to it; it is like a matrix version of the power method (Del Corso, 1997).  Probably the analysis would have to be done initially in the steady state (i.e. assuming the parameter vector θ is constant). If in addition we assume inﬁnite minibatch size so that the covariance matrix equals its expected value, we are conﬁdent that we could show that the only stable ﬁxed point of our update equations gives us in some suitable sense the closest approximation to the covariance; and, with a little more effort, that our updates will converge with probability 1 to that best approximation.  The analysis for ﬁnite minibatch size would have to involve different methods. Because of the noise and the ﬁnite forgetting factor, we would never converge to the true value; but it might be possible to deﬁne some objective function that measures some kind of goodness of approximation, and then say something about the convergence of the distribution of that objective function.  7  Accepted as a workshop contribution at ICLR 2015  5.4  INTERACTION WITH OTHER METHODS  We would like to touch on the subject of some other popular modiﬁcations of SGD, to explain why we do not use them in our experiments.  One frequently used modiﬁcation to SGD is momentum (Polyak, 1964). This can be helpful in pre- venting parameter divergence, as momentum allows SGD to use a higher effective learning rate be- fore parameter divergence is encountered. The original reason why none of our experiments involve momentum is that we found it quite hard to successfully incorporate momentum into multi-threaded parameter updates, needed for the CPU version of our training method; this is likely to be the reason why Downpour (Dean et al., 2012) does not use momentum. We developed other methods to prevent instability– namely, the limit on parameter change per layer per minibatch (Appendix C.3); and the natural gradient method itself.  Another popular modiﬁcation of SGD is Adagrad (Hazan et al., 2007). This method divides the learning rate for each parameter by the standard deviation of the sum of gradients for that parameter, averaged over time (from the beginning of optimization until the present). This naturally gives the 1/t learning rate schedule that is believed from theory to be optimal (Kushner & Yin, 2003), as well as giving separate learning rates for each diagonal element. There are two reasons why we felt that Adagrad was very unlikely to be helpful for large-scale speech recognition. Firstly, a 1/t learning rate has been found empirically be inferior to an exponentially decaying learning rate (Senior et al., 2013). Secondly, because our p-norm nonlinearities (Zhang et al., 2014) are non-saturating we don’t believe that our networks are susceptible to the kind of pathologies that would make some neurons in a layer require higher learning rates than others. This is also true between different hidden layers, due to special properties of the p-norm networks that we use here1. Essentially, we have reason to believe that as far as some directions requiring higher learning rates than others is concerned, all the interesting action for our particular type of network is “off the diagonal”– that is, it cannot be captured by a diagonal matrix. That is why we have not investigated Adagrad and why we smooth our estimates of the factors of the Fisher matrix to the identity and not to a diagonal matrix2.  6 EXPERIMENTS  We show experiments on a speech recognition setup called Fisher English3, which is English- language conversational telephone speech, sampled at 8kHz, and transcribed in a quick but rela- tively low-quality way. The total amount of training data is 1600 hours (only including transcribed segments, i.e. not the silent other half of the telephone conversation). We test on a held-out subset of the data, about 3.3 hours long, that we deﬁned ourselves.  6.1 SYSTEM DETAILS AND WORD ERROR RATE PERFORMANCE  Table 1: Word Error Rates (Fisher dev set)  Model  GMM DNN1 DNN2  %WER 31.07 23.66 23.79  Our main results are convergence plots, but to give the reader some idea of the ultimate results in Word Error Rate, we show some results in Table 1. The Word Error Rates may seem on the high  1The detailed argument in involves scale invariance of the network output w.r.t. the parameters for each layer; an invariance of the learning procedure with respect to scaling up the parameters for a layer and scaling up the learning rate at the same time; and the notion that parameters in a layer will tend to grow in size due to parameter noise, if the learning rate is too high  2Actually, there is another reason for this. We have previously derived an efﬁcient online update of a factored Fisher matrix that had a low-rank plus diagonal form (work with Oriol Vinyals, not published), and the diagonal term caused the math to become very signiﬁcantly more complicated.  3Linguistic Data Consortium (LDC) catalog numbers LDC2004S13, LDC2005S13, LDC2004T19 and  LDC2005T19  8  Accepted as a workshop contribution at ICLR 2015  side, but this is mainly due to the difﬁculty of the data and the quick transcription method used on this data.  The GMM system is based on MFCC features, spliced across ±3 frames and processed with LDA+MLLT to 40-dimensional features, then adapted with feature-space MLLR (fMLLR) in both training and test time. See (Povey et al., 2011) for an explanation of these terms and the normal system build steps. All these systems used the same phonetic context decision tree with 7 880 context-dependent states; the GMM system had 300 000 Gaussians in total. The DNN1 system uses speaker adapted features from the GMM system, so it requires a ﬁrst pass of GMM decoding and adaptation. The 40-dimensional features from GMM1 are spliced across ±4 frames of context and used as input to the DNN. DNN1 is a p-norm DNN (Zhang et al., 2014) with 5 hidden layers and p-norm (input, output) dimensions of (5000, 500) respectively, i.e. the nonlinearity reduces the dimension tenfold. We use 15 000 “sub-classes” (see Section C.5 for explanation), and the number of parameters is 19.3 million. It is trained for 12 epochs with learning rate varying from 0.08 to 0.008, trained with 8 parallel jobs with online natural gradient SGD (NG-SGD). For both this and the DNN2 system, we trained with K = 400 000 samples per outer iteration for each machine. The DNN2 system is trained for our online decoding setup (see Appendix C.9), which is geared to- wards applications where reduced latency is important and audio data must be processed strictly in the order it is received. The input features are equivalent to unadapted, un-normalized 40- dimensional log-mel ﬁlterbank features, spliced for ±7 frames, plus a 100-dimensional i-vector representing speaker characteristics, extracted from only the speaker’s audio up to and including the current time. For the results shown here, we include previous utterances of the same speaker in the same conversation when computing the i-vector. Because this system is intended for real-time decoding on a single CPU, we limit the number of parameters by using only 4 hidden layers, p-norm (input, output) dimensions of (350, 3500), and 12 000 sub-classes, for a total of 10.4 million param- eters. It was trained using online NG-SGD with 6 parallel jobs for 5 epochs, with the learning rate decreasing exponentially from 0.01 to 0.001. All our experiments below are based on this setup.  Our server hardware is fairly typical: the majority of them are Dell PowerEdge R720 servers with two Intel Xeon E5-2680v2 CPUs having with 10 cores each, running at 2.8GHz; and with a single NVidia Tesla K10 GPU card, providing two GPUs– each GPU corresponds to a single machine in our notation, and it becomes incidental that they are co-located. We also have some similar machines with K20 GPU cards, and when reporting time taken, we report the slightly more optimistic ﬁgures obtained from running the same jobs on the faster K20 GPUs.  6.2 RESULTS  Our main result is in Figure 1a (best viewed in color), where we plot the objective function versus amount of training data processed, for our parallel training method with and without natural gradient, and with 1, 2, 4, 8 and 16 jobs. In order to keep the effective learning rate (Section 3.1) constant, we make the initial/ﬁnal learning rates proportional to the number of jobs, with the default learning rates of 0.01 to 0.001 corresponding to the 6-job case.  Our natural gradient method always helps– the NG-SGD curves are all above the plain-SGD curves. Also, when using online natural-gradient, the curves shown in Figure 1a are close to each other up to about 4 jobs– i.e. after processing the same amount of data with different numbers of jobs we get about the same objective function; however, the 8- and 16-job runs converge a little slower. Thus, for small N we are getting a linear speed up in the number N of machines, because the time taken per epoch is proportional to 1/N . As N gets larger than around 4 we need more epochs to get the same improvement, so the speedup becomes sub-linear. The plot also shows that the simple and online natural gradient converge about the same (only tested with one job). We show the ﬁnal Word Error Rates in Table 2; with NG-SGD, they are not very sensitive to the number of jobs.  Figure 1b shows the same plots as Figure 1a but with time as the x-axis. This is a simulated clock time, obtained by multiplying the time taken for each “outer iteration” of training, by the number of outer iterations; the actual clock time depends on queue load. The time per outer iteration was 88 seconds for plain SGD, 93 seconds for online NG-SGD, and 208 seconds for plain NG-SGD, all measured on a K20 GPU. The circles mark the end of training, after 5 epochs.  9  Accepted as a workshop contribution at ICLR 2015  −2.4  −2.6  −2.8  −3  −3.2  −3.4  −3.6  Simple NG−SGD, 1 job Online NG−SGD, 1 job Online NG−SGD, 2 jobs Online NG−SGD, 4 jobs Online NG−SGD, 8 jobs Online NG−SGD, 16 jobs Plain SGD, 1 job Plain SGD, 2 jobs Plain SGD, 4 jobs    0  1  2  3  4  Epochs  (a) Objective function vs. epochs     5     n o i t c n u f    e v  j     i t c e b o g n n a r T  i  i  n o i t c n u f    e v  j     i t c e b o g n n a r T  i  i  −2.4   −2.6   −2.8   −3     −3.2   −3.4   −3.6     0  50  Online NG−SGD, 16 jobs Online NG−SGD, 8 jobs Online NG−SGD, 4 jobs Online NG−SGD, 2 jobs Online NG−SGD, 1 job Simple NG−SGD, 1 job Plain SGD, 1 job Plain SGD, 2 jobs Plain SGD, 4 jobs  100  150  Time (hours)  200  250  (b) Objective function vs. time  Figure 1: Convergence of training objective function (log-probability)  Table 2: Performance in %WER  #jobs N  Plain SGD  1  2  4  8  16  23.63 23.93 24.87  Simple NG-SGD 23.16 Online NG-SGD 23.19 23.00 22.84 23.12 23.35  7 CONCLUSIONS  We have described an efﬁcient Natural Gradient version of SGD training (NG-SGD). We have shown experimentally that not only does the method improve the convergence versus plain SGD, it also  10  Accepted as a workshop contribution at ICLR 2015  makes it possible for us to to use a data parallelization method where we periodically average and redistribute parameters across multiple SGD runs. This enables us to train in parallel even on ma- chines that lack fast interconnections. Although we only show results from one setup, we are conﬁ- dent based on past experience that it holds true for other types of neural network (e.g. ReLU Maas et al. (2013) or sigmoid activations) and improves our ﬁnal results (Word Error Rate) as well as convergence speed.  We do not have a very good explanation why our parallel training method only works when using Natural Gradient, except to say that the statements in (Pascanu & Bengio, 2013) that NG prevents large parameter steps and is more robust to reorderings of the training set, may be relevant.  ACKNOWLEDGEMENTS  We would like to thank Karel Vesely, who wrote the original “nnet1” neural network training code upon which the work here is based; Ehsan Variani and Pegah Ghahremani for their work on CUDA kernels; Hagen Soltau, Oriol Vinyals and Steven Eliuk for fruitful discussions; and many others, too numerous to mention, who have contributed to some aspect of the neural net setup and to Kaldi more generally.  The authors were supported by DARPA BOLT contract No HR0011-12-C-0015, and IARPA BA- BEL contract No W911NF-12-C-0015. We gratefully acknowledge the support of Cisco Systems, inc. (grant #574560) and Google, Inc. (Award 2012 R2 106, “Deep Neural Networks for Speech Recognition”), funds which were used to buy computer equipment and cloud computing time that were used in the development of these methods.  The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of DARPA, IARPA, DoD/ARL or the U.S. Government.  REFERENCES Amari, Shun-Ichi. Natural gradient works efﬁciently in learning. Neural Computation, 10:251–276,  1998.  Bacchiani, Michiel. Rapid adaptation for mobile speech applications. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 7903–7907. IEEE, 2013.  Bahl, L Brown, de Souza, P, and P Mercer, R. Maximum mutual information estimation of hidden markov model parameters for speech recognition. Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP’86., 1986.  Bastian, Michael R, Gunther, Jacob H, and Moon, Todd K. A simpliﬁed natural gradient learning  algorithm. Advances in Artiﬁcial Neural Systems, 2011:3, 2011.  Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A neural probabilistic language model. JMLR,  3:1137–1155, 2003.  Bottou, L´eon. Online learning and stochastic approximations. On-line learning in neural networks,  17:9, 1998.  Davis, Steven and Mermelstein, Paul. Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. Acoustics, Speech and Signal Processing, IEEE Transactions on, 28(4):357–366, 1980.  Dean, Jeffrey, Corrado, Greg S., Monga, Rajat, Chen, Kai, Devin, Matthieu, Le, Quoc V., Mao, Mark Z., Ranzato, Marc’Aurelio, Senior, Andrew, Tucker, Paul, Yang, Ke, and Ng, Andrew Y. Large Scale Distributed Deep Networks. In Neural Information Processing Systems (NIPS), 2012.  Dehak, Najim, Kenny, Patrick, Dehak, R´eda, Dumouchel, Pierre, and Ouellet, Pierre. Front-end fac- tor analysis for speaker veriﬁcation. Audio, Speech, and Language Processing, IEEE Transactions on, 19(4):788–798, 2011.  11  Accepted as a workshop contribution at ICLR 2015  Del Corso, Gianna M. Estimating an eigenvector by the power method with a random start. SIAM  Journal on Matrix Analysis and Applications, 18(4):913–937, 1997.  Gales, M. J. F. and Woodland, P. C. Mean and Variance Adaptation Within the MLLR Framework.  Computer Speech and Language, 10:249–264, 1996.  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural networks. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 249–256, 2010.  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.  Maxout networks. arXiv preprint arXiv:1302.4389, 2013.  Hazan, Elad, Rakhlin, Alexander, and Bartlett, Peter L. Adaptive online gradient descent. In Ad-  vances in Neural Information Processing Systems (NIPS), pp. 65–72, 2007.  Hinton, Geoffrey, Osindero, Simon, and Teh, Yee-Whye. A fast learning algorithm for deep belief  nets. Neural computation, 18(7):1527–1554, 2006.  Kushner, Harold J and Yin, George. Stochastic approximation and recursive algorithms and appli-  cations, volume 35. Springer, 2003.  LeCun, Yann A, Bottou, L´eon, Orr, Genevieve B, and M¨uller, Klaus-Robert. Efﬁcient backprop. In  Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012.  M., Gibson and T., Hain. Hypothesis Spaces For Minimum Bayes Risk Training In Large Vocabulary  Speech Recognition. In Interspeech, 2006.  Maas, Andrew L, Hannun, Awni Y, and Ng, Andrew Y. Rectiﬁer nonlinearities improve neural network acoustic models. In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing (WDLASL 2013), 2013.  Mohamed, Abdel-rahman, Yu, Dong, and Deng, Li. Investigation of full-sequence training of deep  belief networks for speech recognition. In INTERSPEECH, pp. 2846–2849, 2010.  Murata, Noboru and Amari, Shun-ichi. Statistical analysis of learning dynamics. Signal Processing,  74(1):3–28, 1999.  Niu, Feng, Recht, Benjamin, R´e, Christopher, and Wright, Stephen J. Hogwild!: A lock-free ap-  proach to parallelizing stochastic gradient descent. arXiv preprint arXiv:1106.5730, 2011.  Pascanu, Razvan and Bengio, Yoshua. Natural gradient revisited. CoRR, abs/1301.3584, 2013. URL  http://arxiv.org/abs/1301.3584.  Polyak, B.T. Some methods of speeding up the convergence of iteration methods. USSR Computa-  tional Mathematics and Mathematical Physics, 4(5):1–17, 1964.  Povey, D. Discriminative Training for Large Voculabulary Speech Recognition. PhD thesis, Cam-  bridge University, 2004.  Povey., D. and Woodland, P. C. Minimum Phone Error and I-smoothing for Improved Discriminative  Training. In ICASSP, 2002.  Povey, D., Kanevsky, D., Kingsbury, B., Ramabhadran, B., Saon, G., and Visweswariah, K. Boosted  MMI for Feature and Model Space Discriminative Training. In ICASSP, 2008.  Povey, D., Ghoshal, A., et al. The Kaldi Speech Recognition Toolkit. In Proc. ASRU, 2011.  Povey, Daniel and Kingsbury, Brian. Evaluation of proposed modiﬁcations to MPE for large scale  discriminative training. In ICASSP, 2007.  Roux, Nicolas Le, Bengio, Yoshua, and antoine Manzagol, Pierre. Topmoumoute online natural  gradient algorithm. In NIPS, 2007.  Saon, George, Soltau, Hagen, Nahamoo, David, and Picheny, Michael. Speaker adaptation of neural network acoustic models using i-vectors. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pp. 55–59. IEEE, 2013.  12  Accepted as a workshop contribution at ICLR 2015  Seide, Frank, Li, Gang, Chen, Xie, and Yu, Dong. Feature engineering in context-dependent deep neural networks for conversational speech transcription. In Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on, pp. 24–29. IEEE, 2011a.  Seide, Frank, Li, Gang, and Yu, Dong. Conversational speech transcription using context-dependent  deep neural networks. In INTERSPEECH, pp. 437–440, 2011b.  Senior, Andrew and Lopez-Moreno, Ignacio. Improving dnn speaker independence with i-vector  inputs. In Proc. ICASSP, 2014.  Senior, Andrew, Heigold, Georg, Ranzato, Marc’Aurelio, and Yang, Ke. An empirical study of learning rates in deep neural networks for speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2013.  Vesel`y, Karel, Ghoshal, Arnab, Burget, Luk´aˇs, and Povey, Daniel. Sequence-discriminative training  of deep neural networks. In Proc. Interspeech, 2013.  Yang, Howard Hua and Amari, Shun-ichi. Natural gradient descent for training multi-layer percep-  trons. Unpublished (submitted to IEEE Tr. on Neural Networks), 1997.  Yang, Howard Hua and Amari, Shun-ichi. Complexity issues in natural gradient descent method for  training multilayer perceptrons. Neural Computation, 10(8):2137–2157, 1998.  Zhang, Xiaohui, Trmal, Jan, Povey, Daniel, and Khudanpur, Sanjeev. Improving deep neural net-  work acoustic models using generalized maxout networks. In Proc. ICASSP, 2014.  13  Accepted as a workshop contribution at ICLR 2015  A FURTHER DETAILS ON SIMPLE NATURAL GRADIENT METHOD  A.1 OVERVIEW OF SIMPLE NATURAL GRADIENT METHOD  In this section we describe the natural gradient method that uses the other elements of the minibatch to estimate the factors of the Fisher matrix.  As mentioned in Section 4.7, the interface can be described as follows. Given a matrix X, each row of which represents one element of the minibatch (and with a number of columns corresponding to either the row or column dimension of one of the weight matrices in the network), do the inverse- Fisher multiplication for each row xi of X and return the modiﬁed matrix ¯X. The core of the inverse-Fisher multiplication is this: let ¯xi = F−1 i xi, where Fi is the Fisher matrix estimated from the other rows of X, i.e. if N is the minibatch size, then Fi = 1 j . We extend this basic idea by adding smoothing of Fi with the identity matrix, and by scaling the output ¯X to have the same Frobenius norm as the input.  N−1Pj6=i xj xT  A.2 DETAILS OF METHOD (NOT CONSIDERING EFFICIENCY)  In this section we describe what we compute in our “simple” natural gradient method, without considering how to compute it efﬁciently. As described in Section 4.9, we smooth the Fisher matrix with the identity. Deﬁning  (8) where our normal settings are α = 4 and ǫ = 10−20, and N and D are the number of rows and columns respectively of X, we deﬁne smoothed Fisher matrices as follows, to be applied to each row xi of X:  β = α max(tr (XT X), ǫ)/(N D)  For each row i we will then deﬁne  Gi =  βI +  1  N − 1Xj6=i  xj xT  −1  j    ˆxi = G−1  i xi  and then the result of our computation will be  (11) where the rescaling factor γ, intended to make sure that ¯X has the same Frobenius norm as the input X, is deﬁned as  ¯X = γ ˆX  γ =qtr (XT X)/tr ( ˆXT ˆX).  If the denominator of the above is zero, we take γ to be one. We should note that by computing the scalars β and γ without “holding out” the current sample, we are violating the rule that the randomly sampled learning rate matrix should be independent of the current sample. However, since we always use a fairly large minibatch size (at least 100) and these are only scalar quantities, we don’t believe the small amount of “contamination” that takes place here will signiﬁcantly bias the training. In fact, it might not turn out to be very difﬁcult to modify the equations to properly hold out the current sample for these purposes, but because we don’t believe it would perceptibly affect the results, we haven’t gone to the trouble of doing this.  A.3 EFFICIENT COMPUTATION IN SIMPLE METHOD  We now describe how we efﬁciently compute what we described above. Deﬁne the smoothed Fisher matrix  G =(cid:16)β I + 1  N−1 XT X(cid:17) ,  which is like the Gi quantities but without holding out the current sample. Next, compute  (14) where Q only differs from ˆX by xi not being held out from the corresonding Gi. There are two equivalent methods to compute Q:  Q = XG−1,  14  (9)  (10)  (12)  (13)  Accepted as a workshop contribution at ICLR 2015  (i) In column space:  (ii) In row space:  Q = X(cid:16)βI + 1 Q =(cid:16)βI + 1  (N−1) XT X(cid:17)−1 (N−1) XXT(cid:17)−1  X  We derived the rather surprising row-space version of the formulation by expanding the inverted expression on the right of the column-space expression using the Morrison-Woodbury formula, and simplifying the resulting expression.  For efﬁciency, we choose method (i) above if the minibatch size is greater than the dimension (N > D), and method (ii) otherwise. Our formula below for ˆX is derived by expressing each G−1 as a rank-one correction to G−1, and computing the corresponding correction by which each row ˆxi differs from the corresponding row qi of Q. It turns out that the correction is in the same direction as qi itself, so ˆxi just becomes a scalar multiple of qi. Deﬁning, for each row-index i,  i  and deﬁning the scalar factor  ai = xT  i qi,  bi = 1 + ai/(N −1−ai),  then we can use the following efﬁcient formula: for each row ˆxi of ˆX,  ˆxi = biqi.  (15)  (16)  (17)  We then get the output ¯X by scaling ˆX by γ, as described above. When working on CPUs with small minibatch sizes (e.g. N = 128) and large hidden-layer dimen- sions (e.g. D = 1000), the computation above is very efﬁcient, and does not comprise more than about 20% of the time of the overall backprop computation. However, when using GPUs with larger minibatch sizes (e.g. N = 512) it can take the majority of the time. Even though it typically takes considerably less than half of the total ﬂoating point operations of the overall computation, it con- tains a matrix inversion, and matrix inversions are not very easy to compute on a GPU. Our “online” method which we will describe below is designed to solve this efﬁciency problem.  B FURTHER DETAILS ON ONLINE NATURAL GRADIENT METHOD  B.1 OVERVIEW OF ONLINE NATURAL GRADIENT METHOD  The interface of the online natural-gradient method is essentially the same as the simple method: the user provides a matrix X, and we return a matrix ¯X that’s been multiplied by the inverse-Fisher and then rescaled to have the same Frobenius norm as X. Again, each row of X corresponds to an element of the minibatch and the column dimension corresponds to the row or column dimension of one of the weight matrices. A difference from the simple method is that the online method is “stateful”, because we maintain a running estimate of the Fisher matrix. Each time we process a minibatch, we use the Fisher matrix estimated from the previous minibatches; and we then update that estimate using the current minibatch. For a single neural net, the number of separate copies of this “state” that we need to maintain corresponds to twice the number of trainable weight matrices in the neural net: one for each of the Ai and Bi quantities in Equation (2). Let the input be X ∈ RN×D, where N is the minibatch size (e.g. 512) and D is the row or column size of the weight matrix we’re updating (e.g. 2000). We introduce a user-speciﬁed parameter R < D which is the rank of the non-identity part of the Fisher matrix. Let the subscript t = 0, 1, . . . correspond to the minibatch. Deﬁne  Ft  def = RT  t DtRt + ρtI  (18)  where Rt ∈ RR×D, Dt ∈ RR×R and ρt > 0 will be estimated online from data; Rt has orthonormal rows and Dt is diagonal and nonnegative. We’ll estimate these quantities online from the data with the aim being that Ft should be a good estimate of the covariance of the rows of the Xt quantities.  15  Accepted as a workshop contribution at ICLR 2015  We deﬁne Gt to be a kind of “smoothed” version of Ft where we add in more of the unit matrix, controlled by the α parameter we’ve previously discussed (normally α = 4):  and then the output will be:  Gt  def = Ft +  αtr (Ft)  D  I.  ¯Xt = γtXtG−1  t  where γt is computed so as to ensure that the Frobenius norm of ¯Xt equals that of Xt:  or γt = 1 if the denominator of the above equation is 0.  γt =qtr (XtXT  t )/tr (XtG−1  t G−1  t XT  t ),  (19)  (20)  (21)  B.2 UPDATING OUR LOW-RANK APPROXIMATION TO THE VARIANCE  Next we discuss the method we use to estimate our low-rank approximation Ft of the uncentered covariance of the rows of the inputs Xt. Deﬁne  St  def = 1  N XT  t Xt  (22)  as the uncentered covariance of the rows of Xt. We introduce a user-speciﬁed “forgetting factor” 0 < η < 1 (we describe how this is set in Section B.4), and we deﬁne  Tt  def = ηSt + (1 − η)Ft.  (23)  We will try to set Ft+1 to be a good low-rank approximation to Tt. The obvious way would be to make Dt+1 correspond to the top eigenvalues of Tt and Rt+1 to the corresponding eigenvectors, but this would be too slow. Instead we use a method inspired by the power method for ﬁnding the top eigenvalue of a matrix. On each iteration we compute  Yt  def = RtTt,  (24)  with Yt ∈ RR×D. It is useful to think of Yt as containing each eigenvector scaled by its corre- sponding eigenvalue in Tt (of course, this is true in a precise sense only at convergence). Our update to ﬁnd these scaling factors (they are actually uses symmetric eigenvalue decomposition of YtYT t the square roots of the eigenvalues of YtYT t ), puts them on the diagonal of Dt+1, and puts the corresponding eigenvectors in the rows of Rt+1. We then have to work out the correct amount of the unit-matrix to add into our factorization of the covariance matrix (i.e. set ρt+1) and subtract that amount from the diagonals of Dt+1. We will give equations for this below. Observant readers might have noted that it would seem more straightforward do do a Singular Value t . We do it Decomposition (SVD) on Yt instead of a symmetric eigenvalue decomposition on YtYT this way for speed.  The details of our update are as follows:  so Zt ∈ RR×R. Then do the symmetric eigenvalue decomposition  Zt  def = YtYT t ,  Zt = UtCtUT t ,  (25)  (26)  with U orthogonal and Ct diagonal. The diagonal elements of Ct are positive; we can prove this using ρt > 0 (which makes Tt positive deﬁnite) and using the fact that Rt has full row rank. We deﬁne Rt+1 as:  (27) t+1 using (27), it is easy to see that it reduces to the identity, hence Rt+1 If we expand out Rt+1RT has orthonormal rows. In order to make sure that Ft+1 has the desired covariance in the directions corresponding to the rows of Rt+1, we will set  t UT  Rt+1  t Yt  def = C−0.5  Dt+1  def = C0.5  t − ρt+1I,  (28)  16  Accepted as a workshop contribution at ICLR 2015  but note that at this point, ρt+1 is still unknown. When we say the “desired covariance”, we are ensuring that for each dimension r corresponding to a row of Rt+1, the value of the inner product rT Ft+1r equals that of rT Ttr, but this is only precisely true at convergence. We choose ρt+1 in order to ensure that tr (Ft+1) = tr(Tt). This value can be worked out as:  We then let  ρ′t+1 = 1  D−R(cid:0)ηtr (St)+(1−η)(Dρt+tr (Dt))−tr (C0.5  t  )(cid:1)  (29)  (30) for ǫ = 10−10; this is is to ensure that if we get a sequence of zero inputs, ρt will not become exactly zero in its machine representation.  ρt+1 = max(ǫ, ρ′t+1)  B.3 EFFICIENT COMPUTATION  The previous section described what we are computing in the online natural gradient method; here we describe how to compute it efﬁciently. The essential idea here is to reduce the multiplication by G−1 to two multiplications by a “fat” matrix (of dimension R × D). Since typically R is much smaller than D, this is quite efﬁcient. We also address how to efﬁciently keep these matrices updated, at the level of optimizing the matrix expressions. This section is mostly derivation, and will likely only be of interest to someone who is considering implementing this method. In Section B.5 below, we will summarize the algorithm we derive here. We can write Gt as:  Gt  def  = Ft + αtr (Ft) D I = RT t DtRt + βtI  βt  def = ρt + α D tr (Ft) = ρt(1 + α) + α  D tr (Dt)  (31) (32)  (33) (34)  where  Deﬁne  (35) where the factor of βt is inserted arbitrarily to simplify the update equations; a scalar factor on ˆX doesn’t matter because we will later rescale it to have the same norm as X. The output of this whole process is  ,  t  ˆXt  def = βtXtG−1  ¯Xt  def = γt ˆXt, where  γt  def  = qtr (XtXT  ˆXt),  (36)  (37)  t )/tr ( ˆXT  t  where, in the expression for γt, if the denominator is zero we take γt = 1. Note: γt is not the same as in (21) because of the arbitrary factor of βt, so consider (21) to be superseded by (37). To efﬁciently compute (35), we apply the Woodbury matrix identity to (31), giving us  where  with elements  Et  1  G−1  t =  βt (cid:0)I − RT βt (cid:18)D−1  t EtRt(cid:1) I(cid:19)−1  t +  1 βt  1  def =  etii =  1  βt/dtii + 1  (38)  (39)  (40)  In order to reduce the number of matrix multiplies, it is useful to break the expression RT two equal parts, so we deﬁne  t EtRt into  Wt  def = E0.5  t Rt,  17  (41)  Accepted as a workshop contribution at ICLR 2015  and we will never store Rt; instead, we will work with Wt and the small diagonal factors Dt and Et. We can now write the following, which is where most of our computation will take place:  (42) You may recall the symmetric matrix Zt ∈ RR×R deﬁned in (25), which is involved in the update of our factorization. The following expressions are going to be useful when computing it, and the ﬁrst of them appears as a sub-expression of (42). For convenience we state the dimensions of these quantities below:  ˆXt = Xt − XtWT  t Wt  t ∈ RN×R t Xt ∈ RR×D  t Xt t ∈ RR×R(symmetric) t Ht ∈ RR×R(symmetric)  t XtWT  t  Ht  Jt  Kt  Lt  def = XtWT def = HT = WtXT def = JtJT def = HT = WtXT = JtWT t  After we have Ht, we can compute ˆXt using a single matrix multiply as:  ˆXt = Xt − HtWt.  We can expand Yt = RtTt, deﬁned in (24), into quantities that will be computed, as:  N RtXT N E−0.5 Using (52) we can expand Zt = YtYT  Yt = η = η  t  t , as:  t Xt + (1−η)(Dt + ρtI)Rt Jt + (1−η)(Dt + ρtI)E−0.5  t Wt  Zt = η2  N 2 E−0.5  t  JtJT  t E−0.5  t  + (1−η)2 (Dt + ρtI)2 + η(1−η) JtWT  N E−0.5  t  t E−0.5  t  t WtJT and we can substitute some of the sub-expressions we deﬁned above into this, to give:  + η(1−η)  t E−0.5  t  (Dt + ρtI) N (Dt + ρtI)E−0.5  (43)  (44) (45)  (46)  (47) (48) (49)  (50)  (51) (52)  (53)  Zt = η2  N 2 E−0.5  t KtE−0.5  t  + (1−η)2 (Dt + ρtI)2  + η(1−η)  N E−0.5  t  LtE−0.5  t  (Dt + ρtI) + η(1−η)  (54) Our strategy will be to compute the symmetric quantities Lt and Kt on the GPU, and transfer them to the CPU where we can then compute Zt using the expression above – this can be done in O(R2) – and then do the symmetric eigenvalue decomposition as in (26), on the CPU. We repeat the equation here for convenience:  N (Dt + ρtI)E−0.5  LtE−0.5  (55) Here, Ut will be orthogonal, and mathematically, no element of the diagonal matrix Ct can be less than (1−η)2ρ2 t , so we ﬂoor its diagonal to that value to prevent problems later if, due to roundoff, any element is smaller than that.  Zt = UtCtUT t .  t  t  Below, we’ll say how we efﬁciently compute tr (XXT ) and tr ( ˆX ˆXT ); for now, just assume those quantities have been computed. We compute ρt+1 as follows, expanding St in (29):  ρ′t+1 =  1  D−R(cid:18) η  N  tr (XXT )+  (1−η)(Dρt+tr (Dt)) − tr (C0.5  t  )(cid:19).  (56)  18  Accepted as a workshop contribution at ICLR 2015  We can now compute Dt+1 and ρt+1; we ﬂoor both to ǫ to ensure they never go to exactly zero which could cause problems for our algorithm.  Dt+1 = max(C0.5 ρt+1 = max(ǫ, ρ′t+1)  t − ρ′t+1I, ǫI)  (57) (58)  for a small constant ǫ = 10−10 (the ﬁrst max is taken per element). We can now compute the scalar βt+1 and the diagonal matrix Et+1 (we show the formula for its diagonal elements):  βt+1 = ρt+1(1+α) +  α D  tr (Dt+1)  etii =  1  βt+1/dt+1,ii + 1  (59)  (60)  We never construct Rt+1 in memory, but instead we directly compute Wt+1. We can factor it as follows:  where  Wt+1  At  Bt  t+1Rt+1 t+1C−0.5 t+1C−0.5  t UT t UT  def = E0.5 = E0.5 = E0.5 = AtBt  t Yt  t (cid:0) η  N E−0.5  t  Jt + (1−η)(Dt+ρtI)Rt(cid:1)  t UT  t E−0.5  t  N E0.5  def = η def  t+1C−0.5 = Jt + N (1−η)  η  (Dt + ρtI) Wt,  (61) (62) (63) (64)  (65)  (66)  and note that while it might seem like a factor of E−0.5 is missing from the second term in Bt, in fact we use the fact that it commutes with (Dt + ρtI) to move it to the left, into At. If we’re using a GPU, At will be computed in time O(R2) on the CPU and transferred to the GPU; we then compute Bt on the GPU efﬁciently by scaling the rows of Wt and adding Jt; then we multiply At and Bt on the GPU.  t  B.3.1 MAINTAINING ORTHOGONALITY  We have noticed that the invariance RtRT t = I can sometimes be lost due to roundoff. A proper analysis of roundoff in our algorithm is not something we have time to do, but we will describe how we detect and ﬁx this problem in practice. For speed, we only do the following operations if the diagonal matrix Ct, has condition number greater than 106, or if any elements were ﬂoored as mentioned just after (55). Note: all the computations we describe in this paper were done in single precision.  We compute the symmetric matrix  Ot  def = RtRT t = E−0.5  t  (cid:0)WtWT  t (cid:1) E−0.5  t  ,  (67) (68)  where the part in parentheses is computed on the GPU and transferred to the CPU. If no element of Ot differs by more than 10−3 from the corresponding element of the unit matrix, we consider that Rt is sufﬁciently orthogonal and we do nothing more. Otherwise, we do a Cholesky decomposition Ot = CCT , compute the reorthogonalizing factor M = E0.5 on the CPU and copy to the GPU, and do Wt+1 ← MWt+1 to reorthogonalize. Re-orthogonalization happens extremely rarely, and usually only if something bad has already happened such as parameter divergence.  t C−1E−0.5  t  B.3.2  INITIALIZATION  In our implementation we don’t bother dumping the “state’ of the computation to disk so each new process reinitializes them for the ﬁrst minibatch it processes. We initialize them so as to most closely approximate the covariance of the ﬁrst minibatch of features. This is done by taking  S0  def = 1  N XT  0 X0  19  (69)  Accepted as a workshop contribution at ICLR 2015  and ﬁnding the top R eigenvalues and eigenvectors; the rows of R0 contain the top eigenvectors. Let λi be the corresponding eigenvalues, for 1 ≤ i ≤ R, and we set  ρ0 = max  tr (S0) −PR  D − R  i=1 λi  , ǫ!  (70)  for ǫ = 10−10, and for 1 ≤ i ≤ R, we let d0ii ← max(ǫ, λi − ρ0).  B.3.3 COMPUTING MATRIX TRACES  We mentioned above that we have a fast way of computing the quantities tr (XXT ) and tr ( ˆX ˆXT ). These are needed to compute γt using (37), and to compute ρ′t+1 using (56). We compute these as a side effect of the fact that we need, for each row ¯xti of the output, its squared norm ¯xT ti ¯xti. This will be required to enforce the “maximum parameter change” per minibatch, as described in Section 3.2.2. Suppose we’ve already computed ˆXt using (50). We compute the inner products for all rows 1 ≤ i ≤ N of ˆXt as  (71) using a single GPU kernel invocation. If we are updating the parameters of the Fisher-matrix factor- ization, then we can most efﬁciently obtain our desired traces as follows:  pi = ˆxT  ti ˆxT ti,  tr ( ˆX ˆXT ) =Pipi  tr (XXT ) = tr ( ˆX ˆXT ) − tr (LtEt) + 2tr (Lt).  (72)  (73)  The expression for tr (XXT ) was obtained by expanding tr ( ˆX ˆXT ) using (50), moving tr (XXT ) to the left, and recognizing sub-expressions that we have already computed. In case we are not updating the parameters of the Fisher-matrix factorization, we have no other need for Lt so it will be more efﬁcient to compute tr (XXT ) directly; this can of course be done in O(N D) operations and does not require a matrix multiply. Once we have the scaling factor γt we can scale the pi quantities by its square, and they will equal the quantities ¯xT ti ¯xti that we’ll need for enforcing the maximum parameter change.  B.3.4 MULTITHREADING AND OTHER ISSUES  Most of what we have written above is geared towards operation using a GPU, but we also support operation with CPUs, where our SGD implementation is multithreaded. In this case, we have to consider the interaction with multithreaded code because of the “stateful” nature of the computation. We wanted to avoid a bottleneck where different threads wait to update the parameters sequentially. Our solution is that before doing the part of the computation where we update the parameters, we try to get a lock, and if this fails, we simply apply the ﬁxed inverse-Fisher matrix but don’t update the Fisher-matrix parameters Rt and so on. Since the model parameters don’t move very fast, we don’t expect that this will make any noticeable difference to the SGD convergence, and we have seen no evidence that it does.  B.4 TYPICAL CONFIGURATION  The most important user-speciﬁed parameters for our algorithm are the rank R and the constant α that controls smoothing with the unit matrix. The value α = 4 seems to work well over a wide va- riety of conditions, so we normally leave it at that value. The rank R should generally increase with the dimension of the vectors we are multiplying. Our experiments here are with “p-norm” networks where the nonlinearity is dimension reducing, like maxout (Goodfellow et al., 2013), typically re- ducing the dimension from something like 3000 to 300. So a typical parameter matrix will increase the dimension from something like 301 to 3000 (it’s 301 instead of 300 because of the bias term). Our normal rule for ranks is to use R = 20 on the input side of each matrix and R = 80 on the output side. Part of the way we originally tuned this is to look at the diagonal matrices Et. These matrices have diagonal values 0 < etii < 1, sorted on i from greatest to least, and 1 − etii can be interpreted as the amount by which the input is scaled in a certain direction in the space. A value of etii close to 1 means we are strongly scaling down the input, and a value close to 0 means we are leaving it unchanged. If the last etii has a value of, say, 0.1, then reducing R by one will be like  20  Accepted as a workshop contribution at ICLR 2015  taking a scaling factor of 0.9 applied to a gradient, and setting to 1 instead; this seems unlikely to make any difference to the SGD, as it’s like changing the learning rate in some direction from 0.9 to 1. Our ﬁnal etii values are normally in the range 0.05 to 0.2. Another conﬁgurable constant is the “forgetting factor” 0 < η < 1: the closer η is to 1, the more rapidly we track changes in the Fisher matrix due to changes in parameters, but the more noise we will have in our estimates. Because we don’t want to have to tune η when we change the minibatch size, we set it as follows. The user speciﬁes a parameter S (interpreted as an approximate number of samples to include in our estimate of the Fisher matrix), and we set  η = 1 − exp(−N/S),  (74)  where N is the minibatch size. We normally set S = 2000; we have no reason to believe that this is a very important parameter.  In order to increase the speed of the algorithm, we normally conﬁgure it so that we only actually update the parameters of the Fisher matrix every 4 minibatches, except on the ﬁrst 10 minibatches in a process, when we always update them.  B.5 SUMMARY OF THE ONLINE NATURAL GRADIENT METHOD  Here we summarize the online natural-gradient SGD method– that is, we summarize the core part of the algorithm that takes a matrix X ∈ RN×D, and outputs a matrix ¯X ∈ RN×D. To understand how this ﬁts into the bigger picture of back-propagation and SGD, see Section 4.  For this summary we will ignore issues of multithreading. Our explanation here is just for one instance of the algorithm, corresponding to the row or column dimension of one of the weight matrices; if there are I weight matrices, there are 2I separate copies of the variables we describe here.  Typical conﬁguration variables are as follows: α = 4, S = 2000 (this will determine η), rank R = 20 (or 80), ǫ = 10−10; and let’s deﬁne a variable J = 4 that dictates the period with which we update the Fisher-matrix factors. Minibatch size N is normally 128 (on CPU), or 512 (on GPU). On t = 0, before running the steps below we have to initialize the parameters as described in Section B.3.2. Note: while in Section B.3.2 we describe how to set ρ0, R0 and D0, the variables which we actually store are ρ0, D0, and W0; to compute W0 we need Equations (34), (40) and (41). We have an input X ∈ RN×D, and despite the notation, we do not require that N be the same for all t– sometimes the last minibatch we process has a smaller than normal size. If t < 10 or J divides t exactly, then we will be updating the factored Fisher matrix; otherwise we just apply it and don’t update. There are two slightly versions of the algorithm, depending whether we will be updating the Fisher matrix.  In either case, we ﬁrst compute η from N and S using (74), and then compute  Ht = XtWT t .  (75)  From here the two cases begin to differ.  Without updating the Fisher matrix. The input is Xt. We ﬁrst compute tr (XT  If we won’t be updating the Fisher matrix, then it’s simpler. t X). Then we compute ˆXt = Xt − HtWt,  (76) overwriting the input Xt. Next, for each 1 ≤ i ≤ N we compute the row-products pi using (71), and compute tr ( ˆXT ˆX) as the sum of pi. Now we can compute γt using (37). Next we scale ˆXt by γt to produce ¯Xt. We also output for each i the quantity γ2 ti ¯xti, which is needed to enforce the “maximum parameter change per minibatch” constraint.  t pi = ¯xT  With updating the Fisher matrix. four steps, there are some more operations to do. First we compute  If we’re updating the Fisher matrix, which we usually do every  Jt = HT  t Xt ∈ RR×D.  (77)  21  Accepted as a workshop contribution at ICLR 2015  Next we want to compute Lt and Kt. We actually have two separate strategies for this. If N > D (the minibatch size exceeds the vector dimension), we do:  Lt = WtJT Kt = JtJT  t ∈ RR×R t ∈ RR×R  (78) (79)  and in our implementation we combine these into one matrix operation by placing L and K, and W and J, next to each other in memory. Otherwise, we compute Kt as above but Lt using:  t  Lt = HT  and E−0.5  t Ht ∈ RR×R.  (80) At this point, if we’re using a GPU, we transfer the symmetric matrices Kt and Lt to the CPU. We now compute some small derived quantities on the CPU: βt using (34) and Et using (40), as well as ; Et is diagonal so this is not hard. At this point we compute the symmetric R × R E0.5 matrix Zt using (54); the expression looks scary but it can be computed in O(R2) time. We do the symmetric eigenvalue decomposition as in (55), on the CPU, to get the orthogonal matrix Ut and the diagonal matrix Ct, and we ﬂoor the diagonal elements of Ct to (1−η)2ρ2 t . Next we compute  t  ˆXt = Xt − HtWt,  (81)  t  then compute the row-products pi using (71), compute tr ( ˆXT t X) using (73). We can now obtain the scaling factor γt using (37), and use it to compute the main output ¯Xt = γt ˆXt and the per-row inner products of the output which equal γ2 t pi (although in our implementation, to save time we actually output γt and let the user do the scaling later on). We next compute ρ′t+1 using (56), Dt+1 using (57) and ρt+1 using (58). Wt+1 is computed using a matrix multiply on the GPU as in (64), after working out the factors At and Bt. At this point, if we had ﬂoored any diagonal elements of Ct above or if its condition number af- ter ﬂooring exceeds 106, we do the orthogonality check and possible reorthogonalization that we described in Section B.3.1 above.  ˆX) =Pi pi, and compute tr (XT  C OTHER ASPECTS OF OUR DNN IMPLEMENTATION  Here we describe some aspects of our neural net training implementation that are of less direct relevance to our parallel training and natural gradient methods, so were not included in the main text of the paper.  In Section C.1 we discuss the CPU-based and GPU-based versions of our SGD implementation and how they differ; in Section C.2 we discuss how we randomize the training examples and store them on disk. In Section C.3 we explain how we enforce a maximum parameter-change per mini- batch; in C.4 we explain our generalized model-averaging procedure; in C.5 we explain how we use “mixture components” (a.k.a. sub-classes) for DNNs; in C.6 we introduce our method of input data normalization; in C.7 we give details on how we initialize the DNN parameters; in C.8 we give an overview of how we implemented sequence training for DNNs; and in C.9 we discuss online (real-time) decoding using i-vectors for speaker adaptation.  C.1 CPU VERSUS GPU-BASED SGD  Each machine in our parallel computation implements SGD. We have two versions of this, one for GPU and one for CPU. The GPU-based computation is standard minibatch-based SGD, typically with 512 examples per minibatch.  In the CPU-based computation, each job uses typically 16 threads in order to take advantage of multi-core processors. The threads share parameters without any locks; this is known as Hog- wild! (Niu et al., 2011) and was referred to in (Bengio et al., 2003) as asynchronous. In order to prevent divergence, each thread processes relatively small minibatches - typically, of size 128.  We should mention at this point that in our formulation, we sum the gradients over the elements of the minibatch, rather than averaging: this ensures that we make the same amount of progress per sample, regardless of minibatch size, and so gives more consistent results when changing the  22  Accepted as a workshop contribution at ICLR 2015  minibatch size. The need to limit the minibatch size in the multithreaded case can be understood as follows: think of the effective minibatch size as being the minibatch size times the number of threads. The product of the learning rate η with the effective minibatch size is relevant for stability of the SGD update: if it becomes too large, there is increased danger of divergence.  We normally use the GPU-based method, because in our experience a GPU can process data many times faster than a CPU with multiple threads. Aside from speed, the two methods give very similar results.  C.2 DATA RANDOMIZATION AND SEQUENTIAL DATA ACCESS  On spinning hard disks, sequential data access can be orders of magnitude more efﬁcient than ran- dom data access or access to small ﬁles. In the Kaldi toolkit (Povey et al., 2011), we try very hard to ensure that any high-volume data access takes the form of sequential reads or writes on large ﬁles.  For neural network training, we keep data access sequential by dumping pre-randomized “training examples” to disk. Each training example corresponds to a class label together with the correspond- ing input features, including left and right temporal context as needed by the network. The random- ization is done just once for the entire data, and the data is accessed in the same order on each epoch. This is probably not ideal from the point of view of the convergence of SGD, but our expectation is that for large amounts of data the same-order access will not affect the results noticeably.  We break up the training data into N by M rougly equal-sized blocks, where N is the number of parallel jobs, speciﬁed by the user (typically 4 ≤ N ≤ 8), and M ≥ 1 is the number of “outer iterations per epoch”, which is chosen to ensure that the number of samples processed per iteration is close to a user-speciﬁed value K (e.g. K = 400 000). The process of randomly distributing the data into N by M blocks, and ensuring that the order is randomized within each block, is done in parallel; we won’t give further details here, because the problem is straightforward and there is nothing particularly special about our method. To reduce disk or network access we compress the features on disk to 1 byte per ﬂoat, using a lossy compression method.  C.3 ENFORCING THE MAXIMUM PARAMETER CHANGE PER MINIBATCH  As mentioned in Section 3.2.2, in order to prevent instability and parameter divergence we enforce a maximum parameter-change per minibatch, which is applied for each layer of the network sepa- rately. Here we explain how this is done. We don’t claim that this is an exceptionally good method for preventing excessive parameter changes, but we describe it here anyway for the sake of com- pleteness.  Suppose the update for a single weight matrix is formulated as follows (and to keep things simple, we don’t include an index for the layer of the network):  Wt+1 = Wt + ∆t,  (82)  where ∆t is the change that standard SGD would give us, equal to the derivative of the objective function for this minibatch multiplied by the learning rate ηt. To enforce the maximum parameter chanbge, we scale the change by a scalar αt:  Wt+1 = Wt + αt∆t,  (83)  where we would like to choose αt ≤ 1 to ensure that ||αt∆t||F does not exceed a speciﬁed limit, || · ||F being the Frobenius norm. However, we don’t implement this scheme exactly as described above because it would involve creating a temporary matrix to store the product of matrices ∆t just in order to compute its norm, and we don’t want to incur this penalty. Instead we enforce it in a way that involves a sum over elements of the minibatch. If ∆t = ηXT Y, then ∆t can be written as a sum over an index i that ranges over the rows of X and Y. By properties of norms, the 2-norm of ∆t cannot exceed the sum of the 2-norms of the terms in this sum: if the rows of X and Y are written as xi and yi, then  ||∆t||F ≤Xi  η||xi||2||yi||2  23  (84)  Accepted as a workshop contribution at ICLR 2015  It does not take excessive time or memory to compute the vector norms ||xi||2 and ||yi||2, so we compute the right hand side of 84 and use it as a stand-in for ||∆t||F , giving us  αt = min(cid:18)1,  max-change-per-minibatch  Pi η||xi||2||yi||2  (cid:19)  (85)  where max-change-per-minibatch is a user-speciﬁed maximum parameter-change per minibatch. Empirically we have found that it tends to be necessary to increase max-change-per-minibatch when using a larger minibatch size, so to simplify the conﬁguration process we deﬁne  max-change-per-minibatch = N max-change-per-sample  (86)  where N is the minibatch size. We always set max-change-per-sample to 0.075 for experiments reported here. To clarify how this method interacts with the natural gradient methods described in Section 4: the natural gradient is implemented as a modiﬁcation to the X and Y matrices, so we simply apply this maximum-change logic on top of the modiﬁed X and Y quantitities.  What we’ve found that this maximum-parameter-change limit is active only early in training for layers closer to the output.  C.4 GENERALIZED MODEL AVERAGING  The convergence theory of Stochastic Gradient Descent (Kushner & Yin, 2003) suggests that, for convex problems, if we take not the last iteration’s model parameters but the average over all itera- tions, it can improve the convergence rate, particularly in ‘poorly-conditioned’ problems (i.e. where the condition number of the Hessian is very large). This is not applicable in non-convex problems such as ours, but it does suggest a related method. As mentioned above, we deﬁne an outer iter- ation as the length of time it takes for all jobs to process K samples (e.g. K = 400 000), and on each outer iteration each job dumps its ﬁnal model to disk and we average these to produce a single model. We store the models (averaged over all jobs) for each outer iteration. At the very end of training, instead of choosing the model from the ﬁnal outer iteration, we take the models from the last P outer iterations (e.g. P = 20), and search for a generalized weighted combination of these models that optimizes the objective function on a subset of training data– we tried using validation data here, but for our task we found it worked best to use training data. By generalized weighted combination, what we mean is that the parameters are a weighted combination of the parameters of the input models, but each layer can have different weighting factors. Thus, if there are P models and L layers, the number of parameters we learn on the data subdset is LP . A few more details:  • The optimization method is L-BFGS. • To improve the convergence speed of L-BFGS, we optimize in a transformed (precondi-  tioned) space where the preconditioner is related to the Fisher matrix.  • The starting point for the optimization is the best of P + 1 choices, corresponding to each  of the P ﬁnal iterations, and the average of all of them.  • We add a very tiny regularizer (like 10−10 times the square of the vector of weights) to stop the weights going to inﬁnity in cases (like p-norm networks) where the objective function is invariant to the parameter scale.  • We generally aim to optimize over the last P = 20 models (assuming they share the same  parameter structure, e.g. we haven’t added layers).  • In cases where P iterations would amount to less than one epoch, we optimize over P models where the individual models are simple averages of model parameters for a duration of about 1/P of the entire epoch.  We have generally found that this model combination slightly improves results, but it is not a focus of the current paper so we don’t provide experimental results for this here.  C.5 MIXTURE COMPONENTS (SUB-CLASSES)  When using Gaussians for speech recognition, the usual approach is to use a Gaussian mixture model (GMM) rather than a single Gaussian, to model each speech state. We have generalized  24  Accepted as a workshop contribution at ICLR 2015  this idea to neural networks, by allowing the posterior of each speech state to be written as a sum over the posterior of “sub-classes” that are analogous to the Gaussians in a GMM. About halfway through training, we “mix up” the model by increasing the dimension of the softmax layer to a user-speciﬁed number that is greater than the number of classes (usually about double the number of classes). After the softmax layer we introduce a “sum-group” layer which sums its input over ﬁxed groups of indexes to produce a posterior for each class that is a sum over the posteriors of the hidden “sub-classes”. We also tried sharing the sub-classes across classes in groups, but did not ﬁnd this helpful.  Rather than distributing the “sub-classes” evenly, we allocate more sub-classes to the more common classes. We allocate them proportional to the 1/3 power of the count of that class in the training data; this is based on the rule we use to allocate Gaussians in our GMMs.  When initializing the parameters of the “mixed-up” ﬁnal weight matrix, we make it correspond quite closely with the original weight matrix. Each row of the new weight matrix corresponds to a row of the old weight matrix, plus a small noise term to allow the values of the rows to diverge; and we modify the bias term to normalize for the fact that some classes have more sub-classes than others.  We have generally found that this slightly improves results, but again, this is not a focus of the current paper and we won’t be showing experimental results about this. More recent experimental results4 show that the mixture components may not have diverged sufﬁciently that we can regard them as truly distinct; the method may only be helping because it has the same effect as decreasing the learning rate for the ﬁnal-layer parameters corresponding to the higher-frequency classes. We describe it only for completeness.  This note is being added after publication as an ICLR workshop paper. Further experiments con- ducted by Minhua Wu discovered that the mixture components were not diverging sufﬁciently to be regarded as distinct mixtures, and the observed small improvement was likely due to a stabilizing effect on the update of parameters for higher-count classes, by splitting the gradient into multiple pieces. We were able to improve our results slightly by removing the mixture-component and adding instead a ﬁxed scaling as a separate component/layer after the ﬁnal weight matrix and before the softmax, with scales proportional to (data-count)−0.25, renormalized so that the average was 1.  C.6  INPUT DATA NORMALIZATION  As mentioned in (LeCun et al., 2012, Section 4.3), when training neural networks it is helpful to nor- malize the input data so that it is zero mean and so that more important dimensions of input data have a larger variance. We wanted a generic way to achieve this that would be invariant to arbitrary afﬁne transforms of the input. The technique we developed requires as statistics a within-class covariance W and a between-class covariance B, accumulated from the class-labeled data as if in preparation for multi-class Linear Discriminant Analysis (LDA). Assume in what follows that we have already normalized the data so that it is zero-mean. For the technique we are about to describe to make sense, the number of classes should not be much smaller than the feature dimension; fortunately, in our case it is much larger– 5000 > 300, to give typical numbers. Suppose we were to do multi-class LDA but not actually reduce the dimension. We would transform into a space where W was unit and B was diagonalized. Suppose the B in this space has diagonal elements bi. Then the total covariance in each dimension i is bi + 1. This has the desirable property that the data covariance is higher in “more important” directions, but it doesn’t drop as fast as we’d like for unimportant directions– it never goes below 1. In our method, we do the LDA-type  transform as mentioned above, then scale each row of the transform by p(bi + 0.001)/(bi + 1).  After this scaling, the total covariance becomes bi + 0.001, where bi is the ratio of between-class to within-class covariance. This seems to work well.  After creating the transform matrix as described above, we do a singular value decomposition on it, ﬂoor the singular values (normally to 5), and reconstruct again. The motivation here is to avoid a rarely encountered pathology that occurs when the training data covariance was close to singular, which leads to a transform with very large elements, that might produce very large transformed data values on mismatched test data or due to roundoff. This step rarely ﬂoors more than a handful of singular values so has little effect on the transform.  4Thanks to Minhua Wu  25  Accepted as a workshop contribution at ICLR 2015  C.7 PARAMETER INITIALIZATION  We decided not to implement generative pre-training as in (Hinton et al., 2006), because while it is well established that it improves results for small datasets, our understanding is that as the amount of training data gets larger, it eventually gives no improvement compared to a suitable random initialization or discriminative layer-wise backpropagation as in (Seide et al., 2011a). We could not ﬁnd a published reference for this; it is something we have been told verbally. We refer here speciﬁcally to speech recognition tasks; this does not apply to tasks like computer vision where much larger networks are used. In fact, the alternative “nnet1” implmentation of DNNs in Kaldi does support pre-training, and for small datasets (say, 50 hours or less), it generally gives slightly better results than the “nnet2” implementation which we speak of here. For larger datasets, the “nnet1” implementation eventually becomes impractical to run because it takes too long, and a detailed comparison is way beyond the scope of this paper.  Instead of pre-training, we use what is described in (Seide et al., 2011a) as layer-wise back- propagation (BP). What this means is, we initialize a network with one hidden layer, train with BP for a short time (two “outer iterations” for our experiments reported here), then remove the ﬁnal softmax layer and add a new, randomly initialized hidden layer on top of the existing hidden layer; train for a short time again; and repeat the process until we have the desired number of hidden layers. Similar to (Glorot & Bengio, 2010), we use a standard deviation of 1√i for the weights, where i is the fan-in to the weight matrix; but we initialize the parameters of softmax layers to zero. Note: we found it essential to discard the parameters of the ﬁnal softmax layer when adding each new hidden layer, as prescribed in (Seide et al., 2011a).  For smaller datasets we can improve results versus layer-wise BP by initializing all but the last layer of the network from a network trained on another large dataset, possibly from another language. When initializing this way we typically ﬁnd it best to use a larger network than we otherwise would have used.  Because we noticed that sometimes on an outer iteration immediately following the random initial- ization of parameters (including the ﬁrst outer iteration), the parameter averaging can degrade rather than improve the objective function, we modiﬁed our parallel training method so that on these iter- ations, instead of averaging the parameters we choose the one that had the best objective function computed on the subset of data that it was trained on (this approach avoids any extra computation).  C.8 SEQUENCE TRAINING  Sequence training (Mohamed et al., 2010) is a term that has the the same meaning for DNNs that “discriminative training” (Povey, 2004) has in the speech recognition community for GMMs. It is a collective term for various objective functions used for training DNNs for sequence tasks, that only make sense at the whole-sequence level. This contrasts with the cross-entropy objective function which, given a ﬁxed Viterbi alignment of the HMM states, easily decomposes over the frames of training data. In GMM-based speech recognition, the term “discriminative training” contrasts with Maximum Likelihood estimation; in DNN-based speech recognition it contrasts with cross-entropy training. There are two popular classes of sequence/discriminative objective functions:  • Maximum Mutual Information (MMI)-like objective functions (Bahl et al., 1986; Povey, 2004), more properly called conditional maximum likelihood: these have the form of sum over all utterances of the log-posterior of the correct word sequence for each utterance, given the model and the data. These include its popular ’boosted’ variant (Povey et al., 2008) which is inspired by margin-based objective functions.  • Minimum Bayes Risk (MBR)-like objective functions: popular variants include Minimum Phone Error (MPE) (Povey. & Woodland, 2002; Povey, 2004) and state-level Minimum Bayes Risk (M. & T., 2006; Povey & Kingsbury, 2007). These have the form of an ex- pectation, given the data and the model, of an edit-distance type of error. We can compute its derivative w.r.t. the model parameters, because the posteriors of the different sequences vary with the model parameters.  This paper is mainly about our parallel approach to standard cross-entropy training. However, we also apply the same ideas (model averaging, NG-SGD) to sequence training. We generally use  26  Accepted as a workshop contribution at ICLR 2015  state-level Minimum Bayes Risk (sMBR) (M. & T., 2006; Povey & Kingsbury, 2007) although we have also implemented Minimum Phone Error (MPE) (Povey. & Woodland, 2002) and Boosted MMI (Povey et al., 2008). The high-level details of our lattice-based training procedure are similar to (Vesel`y et al., 2013), but note that in that paper we describe an alternative implementation of deep neural nets (the “nnet1” setup) that exists within Kaldi; this paper is about the alternative “nnet2” setup. Some items in common with the sequence training described in that paper include the following:  • We use a low, ﬁxed learning rate (e.g. 0.00002). • We generally train for about 4 epochs.  Some differences include the following:  • We do parallel SGD on multiple machines, with periodic model averaging. • Rather than randomizing at the utterance level, we split the lattice into as small pieces as possible given the lattice topology, and excise parts of the lattice that would not contribute nonzero derivatives; and we randomize the order of the remaining pieces.  • To ensure that all layers of the network are trained about the same amount, we modify the learning rates in order to ensure that the relative change in parameters on each “outer iteration” is the same for each layer; their geometric average is constrained to equal the user-speciﬁed ﬁxed learning rate (e.g. 0.00002) which we mentioned above.  • In our recipe, we generate the lattices only once. • The minibatches actually consist of several small chunks of lattice (split as described  above), from many different utterances, spliced together.  Something that we should note in connection with the learning rates is that for p-norm networks, since the network output is invariant to (nonzero) scaling of the parameters of the p-norm layers5, and since the generalized weighted combination of Section C.4 may output arbitrarily scaled weights, it is hard to specify in advance a suitable learning rate. To solve this problem, we ﬁrst scale the parameters of p-norm layers so so that the expected square of a randomly chosen matrix element is one.  For sequence training, because the frames in a minibatch are not drawn independently from the training data but consist of sequential frames from one or a few utterances, our “simple” NG-SGD method is not applicable, and we only apply the online method.  C.9 ONLINE DECODING AND I-VECTOR INPUTS  In speech recognition applications it is sometimes necessary to process data continuously as it ar- rives, so that there will be no latency in response. This makes it necessary that the algorithms used should not have any dependencies that are “backwards” in time. Backwards-in-time dependencies in our conventional neural net recipes, e.g. as reported in (Zhang et al., 2014), include cepstral mean normalization (CMN), in which we subtract the mean of the input features; and fMLLR adaptation, also known as constrained MLLR adaptation (Gales & Woodland, 1996), in which we use a baseline GMM system to compute a likelihood-maximizing linear transform of the features. Although we use “online” versions of both of these things for online GMM-based decoding, it makes the system very complex and is not ideal for combination with DNNs.  In order to have a system that is easier to turn into an online algorithm, we use i-vectors (Dehak et al., 2011) as an additional input to the neural network, in addition to the spliced cepstral features. This has been done before, e.g. Saon et al. (2013); Bacchiani (2013). An i-vector is a vector normally of dimension in the range of several hundred, that represents speaker characteristics in a form suitable for speaker identiﬁcation, and which is extracted in a Maximum Likelihood way in conjunction with a single mixture-of-Gaussians model (their means are regressed on the i-vector). The parameters of the factor analysis model that extracts the i-vectors are trained without any supervision, just on a large number of audio recordings. In our case we extract i-vectors of dimension 100. Once the i-vector extractor is trained, we switch to “online” extraction of i-vectors for both training and  5This is thanks to the “renormalization layers” that follow each p-norm layer (Zhang et al., 2014)  27  Accepted as a workshop contribution at ICLR 2015  decoding, in which only frames preceding the current frame are taken as inputs to the i-vector estimation process. At the beginning of the utterance, the i-vector will be zero due to the prior term.  The actual inputs to the DNN in this setup normally consist of the i-vector, plus ±7 frames of Mel frequency cepstral coefﬁcients (MFCCs) (Davis & Mermelstein, 1980), without cepstral mean normalization. Some other authors (Senior & Lopez-Moreno, 2014) use log Mel ﬁlterbank energies; the MFCC features we use here are equivalent to log Mel ﬁlterbank energies because MFCCs are a linear transform of them (we use the same number of coefﬁcients as ﬁlterbanks, 40 for these experiments) and our input data normalization (Section C.6) is invariant to such transforms; we only use MFCCs because they are more easily compressible and our “training example” data structure (Appendix C.2) compresses the input features.  In order to train models that are well matched both to per-speaker decoding, where statistics from previous utterances of the same speaker are included in the i-vector estimation, and per-utterance decoding, where we make a fresh start each time, we generally train after splitting the speakers into “fake” speakers that each have no more than two utterances.  In experiments on a number of datasets, we have generally found that this method gives us about the same performance as our previous recipe where we trained a DNN on top of ±4 frames of the standard 40-dimensional features consisting of mean-normalized MFCC features processed with LDA and MLLT, and speaker adapted with fMLLR (a.k.a. constrained MLLR (Gales & Woodland, 1996)). We prefer it due to its convenience for applications and its convenience for cross-system transfer learning.  28  ",
1504.04054,2015, A Generative Model for Deep Convolutional Learning,"['A Generative Model for Deep Convolutional Learning', 'Yunchen Pu', 'Xin Yuan', 'and Lawrence Carin']",https://arxiv.org/pdf/1504.04054,"5 1 0 2    r p A 5 1         ] L M  . t a t s [      1 v 4 5 0 4 0  .  4 0 5 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  A GENERATIVE MODEL FOR DEEP CONVOLUTIONAL LEARNING  Yunchen Pu, Xin Yuan and Lawrence Carin Department of Electrical and Computer Engineering, Duke University, Durham, NC, 27708, USA {yunchen.pu,xin.yuan,lcarin}@duke.edu  ABSTRACT  A generative model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efﬁcient bottom-up (pretraining) and top-down (reﬁnement) probabilistic learning. Experimental results demonstrate powerful capabilities of the model to learn multi-layer features from images, and excellent classiﬁcation results are obtained on the MNIST and Caltech 101 datasets.  1  INTRODUCTION  We develop a deep generative statistical model, which starts at the highest-level features, and maps these through a sequence of layers, until ultimately mapping to the data plane (e.g., an image). The feature at a given layer is mapped via a multinomial distribution to one feature in a block of features at the layer below (and all other features in the block at the next layer are set to zero). This is analogous to the method in Lee et al. (2009), in the sense of imposing that there is at most one non-zero activation within a pooling block. We use bottom-up pretraining, in which initially we sequentially learn parameters of each layer one at a time, from bottom to top, based on the features at the layer below. However, in the reﬁnement phase, all model parameters are learned jointly, top- down. Each consecutive layer in the model is locally conjugate in a statistical sense, so learning model parameters may be readily performed using sampling or variational methods. 2 MODELING FRAMEWORK Assume N gray-scale images {X(n)}n=1,N , with X(n) ∈ RNx×Ny; the images are analyzed jointly to learn the convolutional dictionary {D(k)}k=1,K. Speciﬁcally consider the model  k=1  D(k) ∗ (Z(n,k) (cid:12) W(n,k)) + E(n),  X(n) =  (1) where ∗ is the convolution operator, (cid:12) denotes the Hadamard (element-wise) product, the elements of Z(n,k) are in {0, 1}, the elements of W(n,k) are real, and E(n) represents the residual. Z(n,k) indicates which shifted version of D(k) is used to represent X(n). Assume an L-layer model, with layer L the top layer, and layer 1 at the bottom, closest to the data. In the pretraining stage, the output of layer l is the input to layer l + 1, after pooling. Layer l ∈ {1, . . . , L} has Kl dictionary elements, and we have:  X(n,l+1) = (cid:80)Kl+1 X(n,l) = (cid:80)Kl  kl+1=1 D(kl+1,l+1) ∗(cid:0)Z(n,kl+1,l+1) (cid:12) W(n,kl+1,l+1)(cid:1) + E(n,l+1) kl=1 D(kl,l) ∗(cid:16) (cid:124)  Z(n,kl,l) (cid:12) W(n,kl,l)(cid:17) (cid:125)  +E(n,l)  (cid:123)(cid:122)  (2)  (3)  K(cid:88)  =S(n,kl ,l)  The expression X(n,l+1) may be viewed as a 3D entity, with its kl-th plane deﬁned by a “pooled” version of S(n,kl,l). The 2D activation map S(n,kl,l) is partitioned into nx × ny dimensional contiguous blocks (pooling blocks with respect to layer l + 1 of the model); see the left part of Figure 1. Associated with each  1  Accepted as a workshop contribution at ICLR 2015  Table 1: Classiﬁcation Error of MNIST data  Figure 1: Schematic of the proposed generative process. Left: bottom-up pretraining, right: top-down reﬁne- ment. (Zoom-in for best visulization and a larger version can be found in the Supplementary Material.) block of pixels in S(n,kl,l) is one pixel at layer kl of X(n,l+1); the relative locations of the pixels in X(n,l+1) are the same as the relative locations of the blocks in S(n,kl,l). Within each block of S(n,kl,l), either all nxny pixels are zero, or only one pixel is non-zero, with the position of that pixel selected stochastically via a multinomial distribution. Each pixel at layer kl of X(n,l+1) equals the largest-amplitude element in the associated block of S(n,kl,l) (i.e., max pooling). The learning performed with the top-down generative model (right part of Fig. 1) constitutes a reﬁnement of the parameters learned during pretraining, and the excellent initialization constituted by the parameters learned during pretraining is key to the subsequent model performance. In the reﬁnement phase, we now proceed top down, from (2) to (3). The generative process consti- tutes D(kl+1,l+1) and Z(n,kl+1,l+1) (cid:12) W(n,kl+1,l+1), and after convolution X(n,l+1) is manifested; the E(n,l) is now absent at all layers, except layer l = 1, at which the ﬁt to the data is performed. Each element of X(n,l+1) has an associated pooling block in S(n,kl,l). 3 EXPERIMENTAL RESULTS We here apply our model to the MNIST and Caltech 101 datasets. MNIST Dataset Table 1 summaries the clas- siﬁcation results of our model compared with some related results, on the MNIST data. The second (top) layer features corresponding to the reﬁned dictionary are sent to a nonlinear support vector machine (SVM) (Chang & Lin, 2011) with Gaussian kernel, in a one-vs-all multi-class classiﬁer, with classiﬁer parameters tuned via 5-fold cross-validation (no tuning on the deep feature learning). Caltech 101 Dataset We next consider the Caltech 101 dataset.For Caltech 101 classi- ﬁcation, we follow the setup in Yang et al. (2009), selecting 15 and 30 images per cat- egory for training, and testing on the rest. The features of testing images are inferred based on the top-layer dictionaries and sent to a multi-class SVM; we again use a Gaus- sian kernel non-linear SVM with parameters tuned via cross-validation. Ours and related results are summarized in Table 2. 4 CONCLUSIONS A deep generative convolutional dictionary-learning model has been developed within a Bayesian setting. The proposed framework enjoys efﬁcient bottom-up and top-down probabilistic inference. A probabilistic pooling module has been integrated into the model, a key component to developing a principled top-down generative model, with efﬁcient learning and inference. Extensive experimental results demonstrate the efﬁcacy of the model to learn multi-layered features from images.  DN Zeiler et al. (2010) CBDN Lee et al. (2009) HBP Chen et al. (2013) ScSPM Yang et al. (2009) P-FV Seidenari et al. (2014)  MCDNN Ciresan et al. (2012) SPCNN Zeiler & Fergus (2013) HBP Chen et al. (2013), 2-layer cFA + 2-layer features  6-layer Conv. Net + 2-layer Classiﬁer + elastic distortions Ciresan et al. (2011)  Ours, 2-layer model + 1-layer features Ours, 3-layer model + 1-layer features  Table 2: Classiﬁcation Accuracy Rate of Caltech-101.  Ours, 2-layer model + 1-layer features  Convnet Zeiler & Fergus (2014)  # Training Images per Category  R-KSVD Li et al. (2013)  Methods  15  66.9% 58.6 % 65.4% 57.7 % 65.7% 58% 73.2% 67 % 71.47% 80.13% 83% 79 % 86.5% 83.8 % 70.02% 80.31% 75.24% 82.78%  0.35% 0.23% 0.47% 0.89% 0.42%  Test error  30  2  0.30.10.90.800.30000.1000.80000.9000X(n,1)S(n,k1,1)Z(n,k1,1)X(n,2)D(k2,2)D(k1,1)S(n,k2,2)01000100100010000.300.90.800.30000000.80000.9000X(n,1)S(n,k1,1)X(n,2)D(k2,2)D(k1,1)S(n,k2,2)Accepted as a workshop contribution at ICLR 2015  REFERENCES Chang, C.-C. and Lin, C.-J. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent  Systems and Technology, 2011.  Chen, B., Polatkan, G., Sapiro, G., Blei, D., Dunson, D., and Carin, L. Deep learning with hierarchical  convolutional factor analysis. IEEE T-PAMI, 2013.  Ciresan, D., Meier, U., and Schmidhuber, J. Multi-column deep neural networks for image classiﬁcation. In  CVPR, 2012.  Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Flexible, J. Schmidhuber. high performance  convolutional neural networks for image classiﬁcation. IJCAI, 2011.  Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. Convolutional deep belief networks for scalable unsupervised  learning of hierarchical representations. ICML, 2009.  Li, Q., Zhang, H., Guo, J., Bhanu, B., and An, L. Reference-based scheme combined with K-svd for scene  image categorization. IEEE Signal Processing Letters, 2013.  Seidenari, L., Serra, G., Bagdanov, A., and Del Bimbo, A. Local pyramidal descriptors for image recognition.  IEEE T-PAMI, 2014.  Yang, J., Yu, K., Gong, Y., and Huang, T. Linear spatial pyramid matching using sparse coding for image  classiﬁcation. In CVPR, 2009.  Zeiler, M. and Fergus, R. Stochastic pooling for regularization of deep convolutional neural networks. ICLR,  2013.  Zeiler, M. and Fergus, R. Visualizing and understanding convolutional networks. ECCV, 2014.  Zeiler, M., Kirshnan, D., Taylor, G., and Fergus, R. Deconvolutional networks. CVPR, 2010.  3  ",
1412.5083,2015, Random Forests Can Hash,"['Random Forests Can Hash', 'Qiang Qiu', 'Guillermo Sapiro', 'and Alex Bronstein']",https://arxiv.org/pdf/1412.5083,"5 1 0 2    r p A 7 1         ]  V C . s c [      3 v 3 8 0 5  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  RANDOM FORESTS CAN HASH  Qiang Qiu, Guillermo Sapiro, and Alex Bronstein Duke University and Tel Aviv University {qiang.qiu, guillermo.sapiro}@duke.edu; bron@eng.tau.ac.il  ABSTRACT  Hash codes are a very efﬁcient data representation needed to be able to cope with the ever growing amounts of data. We introduce a random forest semantic hash- ing scheme with information-theoretic code aggregation, showing for the ﬁrst time how random forest, a technique that together with deep learning have shown spec- tacular results in classiﬁcation, can also be extended to large-scale retrieval. Tradi- tional random forest fails to enforce the consistency of hashes generated from each tree for the same class data, i.e., to preserve the underlying similarity, and it also lacks a principled way for code aggregation across trees. We start with a simple hashing scheme, where independently trained random trees in a forest are acting as hashing functions. We the propose a subspace model as the splitting function, and show that it enforces the hash consistency in a tree for data from the same class. We also introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. Experiments on large-scale public datasets are presented, show- ing that the proposed approach signiﬁcantly outperforms state-of-the-art hashing methods for retrieval tasks.  1  INTRODUCTION  In view of the recent huge interest in image classiﬁcation and object recognition problems and the spectacular success of deep learning and random forests in these tasks, it seems astonishing that much less efforts are being invested into related, and often more difﬁcult, problems of image content- based retrieval, and, more generally, similarity assessment in large-scale databases. These problems, arising as primitives in many computer vision tasks, are becoming increasingly important in the era of exponentially increasing information. Semantic and similarity-preserving hashing methods have recently received considerable attentions to address such a need, in part due to their memory and computational advantage over other representations. These methods learn to embed data points into a space of binary strings; thus producing compact representations with constant or sub-linear search time. Such an embedding can be considered as a hashing function on the data, which translates the underlying similarity into the collision probability of the hash or, more generally, into the similarity of the codes under the Hamming metric. Examples of recent similarity-preserving hashing methods include Gionis et al. (1999), Kulis & Grauman (2009), Weiss et al. (2009), Masci et al. (2014a), Liu et al. (2012), Liu et al. (2011), Li et al. (2011), and Zhang et al. (2010). Due to the conceptual similarity between the problems of semantic hashing and that of binary clas- siﬁcation, numerous classiﬁcation techniques have been adapted to the former task. For example, state-of-the-art supervised hashing techniques like Masci et al. (2014a), Masci et al. (2014b), and Norouzi et al. (2012) are based on deep learning methodologies. Random forest (Breiman, 2001; Criminisi & Shotton, 2013) is another popular classiﬁcation techniques. Random forests have not been so far used to construct semantic hashing schemes. This is mainly because acting as a hashing function, a random forest fails to preserve the underlying similarity due to the inconsistency of hash codes generated in each tree for the same class data; it also lacks a principled way of aggregating hash codes produced by individual trees into a single longer code. This work is the ﬁrst construction of a semantic hashing scheme based on a random forest. We ﬁrst introduce a transformation learner model for random forest enforcing the hash consistency in a tree, thereby preserving similarity. Then, we propose an information-theoretic approach for aggregating hash codes in a forest, encouraging a unique code for each class. Using challenging large-scale examples, we demonstrate signiﬁcantly more consistent and unique hashes for data from the same semantic class, when compared to other state-of-the-art hashing schemes.  1  Accepted as a workshop contribution at ICLR 2015  2 FOREST HASHING  Random forest (Breiman, 2001; Criminisi & Shotton, 2013) is an ensemble of binary decision trees. Following the random forest literature, in this paper, we specify a maximum tree depth d and also avoid post-training operations such as tree pruning. Thus, a tree of depth d consists of 2d − 2 tree nodes, excluding the root node, indexed in the breadth-ﬁrst order. During the training, we introduce randomness into the forest through a combination of random set sampling and randomized node optimization, thereby avoiding duplicate trees. As discussed in Breiman (2001) and Criminisi & Shotton (2013), training each tree with a different randomly selected set decreases the risk of over- ﬁtting, improves the generalization of classiﬁcation forests, and signiﬁcantly reduces the training time. When given more than two classes, we randomly partition the classes arriving at each binary split node into two categories for node randomness. A pedagogic hashing scheme is constructed as follows: Each data point is pushed through a tree until reaching the corresponding leaf node. We simply set ‘1’ for nodes visited, and ‘0’ for the rest. By ordering those bits in a predeﬁned node order, e.g., the breadth-ﬁrst order, we obtain a (2d − 2)- bit sparse hash code, always containing exactly d − 1 ones. In a random forest consisting of M trees of the depth d, each point is simultaneously pushed through all trees to obtain M (2d − 2)-bit hash codes. Both the training and the hashing processes can be done in parallel to achieve high computational efﬁciency on modern parallel CPU or GPU hardware. In classiﬁcation, for which the forest was originally designed, an ensemble posterior is obtained by averaging from a large number of trees, thus boosting the classiﬁcation accuracy (Breiman, 2001), and no conﬁdent class posteriors are required for individual trees. However, due to the lack of con- ﬁdent class posteriors for individual trees, we obtain highly inconsistent hashes from an individual tree for the same class data. It is also not obvious how to combine hashes from different trees given a target code length. The inconsistency of the hash codes prevents standard random forest from being directly adopted for hashing, being such codes critical for large-scale retrieval. To address these problems we ﬁrst propose a transformation as the learner model for the random forest (Qiu & Sapiro, 2014a;b), where each tree enforces consistent codes for similar points. Though a class may not be assigned a unique code in each tree due to limited leaf availability, each class shares code with different classes in different trees due to the underlying node randomness models. We further propose an information-theoretic approach to aggregate hashes across trees into a unique code for each data class. Consider a random forest consisting of M trees of depth d; the hash codes obtained for N training samples are denoted as B = {Bi}M i=1, with the Bi ∈ {0, 1}(2d−2)×N being the codes generated from the i-th tree, henceforth denoted as code blocks. Given the target hash code length L, our objective is to select k code blocks B∗, k ≤ L/(2d − 2), maximizing the mu- tual information between the selected and the remaining codes, B∗ = arg maxB:|B|=k I(B;B\B). When the class labels C are available for a subset of training samples, semi-supervised aggregation is performed as B∗ = arg maxB:|B|=k I(B;B\B) + λI(B; C). The two terms here can be evalu- ated using different samples to exploit all labeled and unlabeled data. Note that the code aggregation step is only learned once during training, no cost at testing.  3 EXPERIMENTAL EVALUATION  We present an experimental evaluation of ForestHash on retrieval tasks using standard public bench- marks. Hashing methods compared include supervised methods FastHash (Lin et al., 2014), TSH (Lin et al., 2013), HDML (Norouzi et al., 2012), KSH (Liu et al., 2012), LDAHash (Strecha et al., 2012), and unsupervised methods SH (Weiss et al., 2009), KLSH (Kulis & Grauman, 2009), AGH (Liu et al., 2011). All software was provided by the authors. We adopt the same setup as in Norouzi et al. (2012) for the image retrieval experiments on MNIST. We trained a forest of 64 trees of depth 3. Table 1 summarizes the retrieval performance of various methods at Hamming radius 0. Here HDML is a deep learning based hashing method, and FastHash is a booted trees based method. We denote the proposed method as ForestHash. Due to our subspace- based leaner models, which are known to be robust for small training samples Bengio et al. (2013), and our semi-supervised code aggregation that exploits both labeled and unlabeled data, ForestHash signiﬁcantly outperforms state-of-the-art methods for reduced training cases.  2  Accepted as a workshop contribution at ICLR 2015  (a) Six different image queries in CIFAR-10.  (b) Six different face queries in Pubﬁg.  Figure 1: Ten top matches retrieved using the proposed technique.  Table 1: 36-bit retrieval performance (%) on MNIST (rejection hamming radius 0) using different training set sizes. Test time is the average binary encoding time in microseconds (µs).  TSH HDML FastHash ForestHash  Test time (µs) 411 10 115 17  6,000 samples per class Precision Recall 86.30 92.94 84.70 88.81  3.17 60.44 76.60 68.54  100 samples per class Precision Recall 74.00 62.52 73.32 86.86  5.19 2.19 33.04 65.72  30 samples per class Precision Recall 56.86 24.28 57.08 79.19  3.94 0.21 11.77 57.93  We adopt a challenging setup as in Liu et al. (2012); Masci et al. (2014a) for the image retrieval experiments on CIFAR10 (Krizhevsky, 2009). Table 2 summarizes the retrieval performance of various methods for Hamming radius 0 and 2. ForestHash-base is the pedagogic random forest hashing scheme in Section 2, where the decision stump learner model is used and a random sub- set of trained trees are selected. It is surprising that this simple pedagogic scheme outperforms all compared supervised methods at radius 0 with orders of magnitude speedup, and the recall is signif- icantly improved with the proposed code aggregation (aggr.). ForestHash using the transformation learner dramatically improves the precision over the pedagogic scheme, signiﬁcantly outperforms all compared methods at radius 0, and reports comparable precision and signiﬁcantly higher recall at radius 2. Figure 1a presents image query examples in the CIFAR-10 dataset.  Table 2: 36-bit retrieval performance (%) on CIFAR10.  AGH1 AGH2 KSH FastHash TSH ForestHash-base ForestHash-base (aggr.) ForestHash  Test time (µs) 14 20 16 94 206 0.6 0.6 14  radius = 0  Precision Recall 29.59 29.45 13.88 14.86 15.80 17.24 15.95 32.47  0.25 0.32 0.11 0.53 0.23 4.37 12.54 5.90  radius ≤ 2  Precision Recall 29.79 27.70 33.18 36.13 36.99 14.86 14.37 31.06  0.71 1.07 1.10 2.25 2.47 9.92 21.44 11.28  Results reported in Table 3 refer to an experiment on the Pubﬁg face dataset (Kumar et al., 2009) , in which we construct the hashing forest using 30 training faces per subject (5,992 faces from 200 subjects ), and search among their 37,007 unseen faces. As subspace methods are robust for small training samples problems (Bengio et al., 2013) and extraordinarily effective in representing faces (Wright et al., 2009), ForestHash shows signiﬁcantly higher precision and recall compared to all state-of-the-art methods. Figure 1b presents several examples of face queries.  4 CONCLUSION  Considering the importance of compact and computationally efﬁcient codes, we introduced a ran- dom forest semantic hashing scheme, which, to the best of our knowledge, is the ﬁrst instance of us- ing random forests for hashing, extending the use of it beyond classiﬁcation for large-scale retrieval.  3  QueryTop-10 answersQuery Top-10 matches Accepted as a workshop contribution at ICLR 2015  Table 3: 36-bit retrieval performance (%) on the Pubﬁg face dataset (rejection radius 0), 5,992 queries (200 known subjects) over 37,007 unseen faces of query subjects. Precision Recall 9.23 22.09 33.37 25.85 30.05 58.95 17.41 97.72  Test time (µs) 8 15 10 16 2 97 693 28  SH KLSH AGH1 AGH2 LDAHash FastHash TSH ForestHash  0.21 4.05 54.17 58.10 1.28 3.47 0.22 85.12  The proposed scheme consists of a forest with transformation learners, and an information-theoretic code aggregation scheme. The proposed framework combines in a fundamental fashion feature learning, random forests, and similarity-preserving hashing, and can be straightforwardly extended to retrieval of incommensurable multi-modal data. Our method shows exceptional effectiveness in preserving similarity in hashes, and outperforms state-of-the-art hashing methods in large-scale retrieval tasks.  REFERENCES Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives.  IEEE Trans. on Patt. Anal. and Mach. Intell., 35(8):1798–1828, 2013.  Breiman, L. Random forests. Machine Learning, 45(1):5–32, 2001.  Criminisi, A. and Shotton, J. Decision Forests for Computer Vision and Medical Image Analysis.  Springer, 2013.  Gionis, A., Indyk, P., and Motwani, R. Similarity search in high dimensions via hashing. In Proc.  of International Conference on Very Large Data Bases, 1999.  Krizhevsky, Alex. Learning multiple layers of features from tiny images. Technical report, 2009.  Kulis, B. and Grauman, K. Kernelized locality-sensitive hashing for scalable image search. In Proc.  International Conference on Computer vision, 2009.  Kumar, N., Berg, A. C., Belhumeur, P. N., and Nayar, S. K. Attribute and simile classiﬁers for face  veriﬁcation. In Proc. International Conference on Computer vision, Oct 2009.  Li, Z., Ning, H., Cao, L., Zhang, T., Gong, Y., and Huang, T. S. Learning to search efﬁciently in  high dimensions. In Advances in Neural Information Processing Systems. 2011.  Lin, G., Shen, C., Suter, D., and van den Hengel, A. A general two-step approach to learning-based  hashing. In Proc. International Conference on Computer vision, 2013.  Lin, G., Shen, C., Shi, Q., van den Hengel, A., and Suter, D. Fast supervised hashing with decision trees for high-dimensional data. In Proc. IEEE Computer Society Conf. on Computer Vision and Patt. Recn., 2014.  Liu, W., Wang, J., and Chang, S. Hashing with graphs. In International Conference on Machine  Learning, 2011.  Liu, W., Wang, J., Ji, R., Jiang, Y., and Chang, S. Supervised hashing with kernels. In Proc. IEEE  Computer Society Conf. on Computer Vision and Patt. Recn., June 2012.  Masci, J., Bronstein, A. M., Bronstein, M. M., Sprechmann, P., and Sapiro, G. Sparse similarity- preserving hashing. In International Conference on Learning Representations, Banff, Canada, 2014a.  Masci, J., Bronstein, M. M., Bronstein, A. M., and Schmidhuber, J. Multimodal similarity-  preserving hashing. IEEE Trans. on Patt. Anal. and Mach. Intell., 36(4):824–830, 2014b.  4  Accepted as a workshop contribution at ICLR 2015  Norouzi, M., Fleet, D. J., and Salakhutdinov, R. Hamming distance metric learning. In Advances in  Neural Information Processing Systems, 2012.  Qiu, Q. and Sapiro, G. Learning transformations for classiﬁcation forests. In International Confer-  ence on Learning Representations, Banff, Canada, 2014a.  Qiu, Q. and Sapiro, G. Learning transformations for clustering and classiﬁcation. JMLR (to appear),  2014b.  Strecha, C., Bronstein, A.M., Bronstein, M.M., and Fua, P. Ldahash: Improved matching with  smaller descriptors. IEEE Trans. on Patt. Anal. and Mach. Intell., 34(1):66–78, Jan 2012.  Weiss, Y., Torralba, A., and Fergus, R. Spectral hashing.  Processing Systems, 2009.  In Advances in Neural Information  Wright, J., Yang, M., Ganesh, A., Sastry, S., and Ma, Y. Robust face recognition via sparse repre-  sentation. IEEE Trans. on Patt. Anal. and Mach. Intell., 31(2):210–227, 2009.  Zhang, D., Wang, J., Cai, D., and Lu, J. Self-taught hashing for fast similarity search. In Proc. of  International Conference on Research and Development in Information Retrieval, 2010.  5  ",
1412.2693,2015, Provable Methods for Training Neural Networks with Sparse Connectivity,"['Provable Methods for Training Neural Networks with Sparse Connectivity', 'Hanie Sedghi', 'and Anima Anandkumar']",https://arxiv.org/pdf/1412.2693,"5 1 0 2    r p A 8 2         ]  G L . s c [      4 v 3 9 6 2  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  PROVABLE METHODS FOR TRAINING NEURAL NET- WORKS WITH SPARSE CONNECTIVITY  Hanie Sedghi University of Southern California Los Angeles, CA 90089 hsedghi@usc.edu  Anima Anandkumar University of California Irvine, CA 92697 a.anandkumar@uci.edu  ABSTRACT  We provide novel guaranteed approaches for training feedforward neural networks with sparse connectivity. We leverage on the techniques developed previously for learning linear networks and show that they can also be effectively adopted to learn non-linear networks. We operate on the moments involving label and the score function of the input, and show that their factorization provably yields the weight matrix of the ﬁrst layer of a deep network under mild conditions. In practice, the output of our method can be employed as effective initializers for gradient descent.  Keywords: Deep feedforward networks, sparse connectivity, ℓ1-optimization, Stein’s lemma.  1  INTRODUCTION  The paradigm of deep learning has revolutionized our ability to perform challenging classiﬁcation tasks in a variety of domains such as computer vision and speech. However, so far, a complete theo- retical understanding of deep learning is lacking. Training deep-nets is a highly non-convex problem involving millions of variables, and an exponential number of ﬁxed points. Viewed naively, proving any guarantees appears to be intractable. In this paper, on the contrary, we show that guaranteed learning of a subset of parameters is possible under mild conditions.  We propose a novel learning algorithm based on the method-of-moments. The notion of using mo- ments for learning distributions dates back to Pearson (Pearson, 1894). This paradigm has seen a recent revival in machine learning and has been applied for unsupervised learning of a vari- ety of latent variable models (see (Anandkumar et al., 2014) for a survey). The basic idea is to develop efﬁcient algorithms for factorizing moment matrices and tensors. When the underlying factors are sparse, ℓ1-based convex optimization techniques have been proposed before, and been employed for learning dictionaries (Spielman et al., 2012), topic models, and linear latent Bayesian networks (Anandkumar et al., 2012). In this paper, we employ the ℓ1-based optimization method to learn deep-nets with sparse connec- tivity. However, so far, this method has theoretical guarantees only for linear models. We develop novel techniques to prove the correctness even for non-linear models. A key technique we use is the Stein’s lemma from statistics (Stein, 1986). Taken together, we show how to effectively leverage algorithms based on method-of-moments to train deep non-linear networks.  1.1 SUMMARY OF RESULTS  We present a theoretical framework for analyzing when neural networks can be learnt efﬁciently. We demonstrate how the method-of-moments can yield useful information about the weights in a neural  1  Accepted as a workshop contribution at ICLR 2015  network, and also in some cases, even recover them exactly. In practice, the output of our method can be used for dimensionality reduction for back propagation, resulting in reduced computation.  We show that in a feedforward neural network, the relevant moment matrix to consider is the cross- moment matrix between the label and the score function of the input data (i.e. the derivative of the log of the density function). The classical Stein’s result (Stein, 1986) states that this matrix yields the expected derivative of the label (as a function of the input). The Stein’s result is essentially obtained through integration by parts (Nourdin et al., 2013).  By employing the Stein’s lemma, we show that the row span of the moment matrix between the label and the input score function corresponds to the span of the weight vectors in the ﬁrst layer, under natural non-degeneracy conditions. Thus, the singular value decomposition of this moment e matrix can be used as low rank approximation of the ﬁrst layer weight matrix during back propagation, when the number of neurons is less than the input dimensionality. Note that since the ﬁrst layer typically has the most number of parameters (if a convolutional structure is not assumed), having a low rank approximation results in signiﬁcant improvement in performance and computational requirements.  We then show that we can exactly recover the weight matrix of the ﬁrst layer from the moment matrix, when the weights are sparse. It has been argued that sparse connectivity is a natural con- straint which can lead to improved performance in practice (Thom and Palm, 2013). We show that the weights can be correctly recovered using an efﬁcient ℓ1 optimization approach. Such approaches have been earlier employed for linear models such as dictionary learning (Spielman et al., 2012) and topic modeling (Anandkumar et al., 2012). Here, we establish that the method is also successful in learning non-linear networks, by alluding to Stein’s lemma.  Thus, we show that the cross-moment matrix between the label and the score function of the input contains useful information for training neural networks. This result has an intriguing connection with (Alain and Bengio, 2012), where it is shown a denoising auto-encoder approximately learns the score function of the input. Our analysis here provides a theoretical explanation of why pre- training can lead to improved performance during back propagation: the interaction between the score function (learnt during pre-training) and the label during back propagation results in correctly identifying the span of the weight vectors, and thus, it leads to improved performance.  The use of score functions for improved classiﬁcation performance is popular under the framework of Fisher kernels (Jaakkola et al., 1999). However, in (Jaakkola et al., 1999), Fisher kernel is deﬁned as the derivative with respect to some model parameter, while here we consider the derivation with respect to the input and refer to it as score function. Note that if the Fisher kernel is with respect to a location parameter, these two notions are equivalent. Here, we show that considering the moment between the label and the score function of the input can lead to guaranteed learning and improved classiﬁcation.  Note that there are various efﬁcient methods for computing the score function (in addition to the auto-encoder). For instance, Sasaki et al. (2014) point out that the score function can be estimated efﬁciently through non-parametric methods without the need to estimate the density function. In fact, the solution is closed form, and the hyper-parameters (such as the kernel bandwidth and the regularization parameter) can be tuned easily through cross validation. There are a number of score matching algorithms, where the goal is to ﬁnd a good ﬁt in terms of the score function, e.g (Hyv¨arinen, 2005; Swersky et al., 2011). We can employ them to obtain accurate estimations of the score functions.  Since we employ a method-of-moments approach, we assume that the label is generated by a feed- forward neural network, to which the input data is fed. In addition, we make mild non-degeneracy assumptions on the weights and the derivatives of the activation functions. Such assumptions make the learning problem tractable, whereas the general learning problem is NP-hard. We expect that the output of our moment-based approach can provide effective initializers for the back propagation procedure.  1.2 RELATED WORK  In this paper, we show that the method-of-moments can yield low rank approximations for weights in the ﬁrst layer. Empirically, low rank approximations of the weight matrices have been employed  2  Accepted as a workshop contribution at ICLR 2015  x  x1  x2  x3  ···  xnx  σ1  σ1  .  .  .  .  .  .  A1  h1  σ1  σ1  .  .  .  .  .  .  hd−1  Ad  ···  ···  ···  σd  y  y1  y2  ···  yny  Figure 1: Graphical representation of Feedforward model E[h|x] = σ1(A1x), E[y|h] = σ2(A2h).  successfully to improve the performance and for reducing computations (Davis and Arel, 2013). Moreover, the notion of using moment matrices for dimension reduction is popular in statistics, and the dimension reducing subspace is termed as a central subspace (Cook, 1998). We present a ℓ1 based convex optimization technique to learn the weights in the ﬁrst layer, assuming they are sparse. Note that this is different from other convex approaches for learning feedforward neural network. For instance, Bengio et al. (2005) show via a boosting approach that learning neural networks is a convex optimization problem as long as the number of hidden units can be selected by the algorithm. However, typically, the neural network architecture is ﬁxed, and in that case, the optimization is non-convex.  Our work is the ﬁrst to show guaranteed learning of a feedforward neural network incorporating both the label and the input. Arora et al. (2013) considered the auto-encoder setting, where learning is unsupervised, and showed how the weights can be learnt correctly under a set of conditions. They assume that the hidden layer can be decoded correctly using a “Hebbian” style rule, and they all have only binary states. We present a different approach for learning by using the moments between the label and the score function of the input.  2 MOMENTS OF A NEURAL NETWORK  2.1 FEEDFORWARD NETWORK WITH ONE HIDDEN LAYER  We ﬁrst consider a feedforward network with one hidden layer. Subsequently, we discuss how much this can be extended. Let y be the label vector generated from the neural network and x be the feature vector. We assume x has a well-behaved continuous probability distribution p(x) such that the score function ∇x log p(x) exists. The network is depicted in Figure 1. Let  E[y|h] = σ2(A2h), E[h|x] = σ1(A1x).  (1)  This setup is applicable to both multiclass and multilabel settings. For multiclass classiﬁcation σ2 is the softmax function and for multilabel classiﬁcation σ2 is a elementwise sigmoid function. Recall that multilabel classiﬁcation refers to the case where each instance can have more than one (binary) label (Bishop et al., 2006; Tsoumakas and Katakis, 2007).  3  Accepted as a workshop contribution at ICLR 2015  2.2 METHOD-OF-MOMENTS: LABEL-SCORE FUNCTION CORRELATION MATRIX  We hope to get information about the weight matrix using moments of the label and the input. The question is when this is possible and with what guarantees. To study the moments let us start from a simple problem. For a linear network and whitened Gaussian input x ∼ N (0, I), we have ylinear = Ax. In order to learn A, we can form the label-score function correlation matrix as  E[ylinear x⊤] = AE[xx⊤] = A.  Therefore, if A is low dimensional, we can project x into that span and perform classiﬁcation in this lower dimension. Stein’s lemma for a Gaussian random vector x (Stein, 1986) states that for a function g(·) satisfying some mild regularity conditions we have  E[g(x)x⊤] = Ex[∇xg(x)].  A more difﬁcult problem is generalized linear model (GLM) of a (whitened) Gaussian x ∈ Rnx. In this case, y = σ(Ax) for any nonlinear activation function σ(·) that satisﬁes some mild regularity conditions. Using Stein’s lemma we have  E[σ(Ax)x⊤] = Ex′ [∇x′ σ(x′))]A,  where x′ ∼ N (0, AA⊤). Therefore, assuming Ex[∇xσ(Ax))] has full column rank, we obtain the row span of A. For Gaussian (and elliptical) random vector x, PA⊤ x provides the sufﬁcient statistic with no information loss. Thus, we can project the input into this span and obtain dimensionality reduction.  The Gaussian distribution assumption is a restrictive assumption. The more challenging problem is when random vector x has a general probability distribution and the network has hidden layers. How can we deal with such an instance? Below we provide the method to learn such problems.  2.2.1 RESULTS  Let x be a random vector with probability density function p(x) and let y be the output label cor- responding to the network described in Equation (1). For a general probability distribution, we use score function of the random vector x which provides us with sufﬁcient statistics for x.  Deﬁnition: Score function The score of x with probability density function p(x) is the random vector ∇x log p(x). Let  M := E[y (∇x log p(x))⊤],  which can be calculated in a supervised setting. Note that ∇x log p(x) represents the score function for random vector x. Theorem 1. In a nonlinear neural network with feature vector x and output label y, we have  where ˜x2 = A2σ1(A1x) and ˜x1 = A1x.  M = −Ex[σ′2(˜x2)A2 Diag(σ′1(˜x1))]A1,  Proof. Our method builds upon Stein’s lemma (Stein, 1986). We use Proposition 1.  M = Ex,y[y (∇x log p(x))⊤] = ExhEy hy (∇x log p(x))⊤ |xii = Ex[σ2(A2(σ1(A1x) (∇x log p(x))⊤] = −Ex[σ′2(˜x2)A2 Diag(σ′1(˜x1))A1]  The second equality is a result of law of total expectation. The third equality follows from Stein’s lemma as in Proposition 1 below. The last equality results from Chain rule. (cid:4)  4  Accepted as a workshop contribution at ICLR 2015  Algorithm 1 Learning the weight matrix for the ﬁrst layer of a Neural Network input Labeled samples {(xi, yi)}, i ∈ [n]. 1: Estimate Score function ∇x log p(x) using auto-encoder or score matching. 2: Compute cM = 1 3: ˆA1=Sparse Dictionary Learning(cM) output ˆA1  n Pi∈[n] yi (∇x log p(x)|x=xi  )⊤  Proposition 1 (Stein’s lemma (Stein et al., 2004)). Let x ∈ Rnx be a random vector with joint density function p(x). Suppose the score function ∇x log p(x) exists. Consider any continuously differentiable function g(x) : Rnx → Rny such that all the entries of g(x)p(x)⊤ go to zero on the boundaries of support of p(x). Then, we have  Note that it is also assumed that the above expectations exist (in the sense that the corresponding integrals exist).  E[g(x) (∇x log p(x))⊤] = −E[∇xg(x)],  The proof follows integration by parts; the result for the scalar x and scalar-output functions g(x) is provided in (Stein et al., 2004). Remark 1 (Connection with pre-training). The above theorem provides us with a nice closed- If B = Ex[σ′2(˜x2)A2 Diag(σ′1(˜x1))] has full column rank, we obtain the row space of form. A1. In deep networks auto-encoder is shown to approximately learn the score function of the in- put (Alain and Bengio, 2012). It has been shown that pre-training results in better performance. Here, we are using the correlation matrix between labels and score function to obtain the span of weights. Auto-encoder appears to be doing the same by estimating the score function. Therefore, our method provides a theoretical explanation of why pre-training is helpful. Remark 2. For whitened Gaussian (and elliptical) random vector, projecting the input onto rows- pace of M is a sufﬁcient statistic. Empirically, even for non-Gaussian distribution, this has lead to improvements (Sun et al., 2013; Li, 1992). The moment method presented in this paper presents a low-rank approximation to train the neural networks.  So far, we showed that we can recover the span of A1. How can we retrieve the matrix A1? Without further assumptions this problem is not identiﬁable. A reasonable assumption is that A1 is sparse. In this case, we can pose this problem as learning A1 given its row span. This problem arises in a number of settings such as learning a sparse dictionary or topic modeling. Next, using the idea presented in (Spielman et al., 2012), we discuss how this can be done.  3 LEARNING THE WEIGHT MATRIX  In this Section, we explain how we learn the weight matrix A1 given the moment M . The complete framework is shown in Algorithm 1. Assuming sparsity we use Spielman et al. (2012) method.  Identiﬁablity The ﬁrst natural identiﬁability requirement on A1 is that it has full row rank. Spielman et al. (2012) show that for Bernoulli-Gaussian entries under relative scaling of parame- ters, we can impose that the sparsest vectors in the row-span of M are the rows of A1. Any vector in this space is generated by a linear combination w⊤M of rows of M . The intuition is random sparsity, where a combination of different sparse rows cannot make a sparse row. Under this identiﬁability condition, we need to solve the optimization problem  minimize kw⊤Mk0 subject to w 6= 0.  ℓ1 optimization In order to come up with a tractable update, Spielman et al. (2012) use the convex relaxation of ℓ0 norm and relax the nonzero constraint on w by constraining it to lie in an afﬁne hy- perplane {r⊤w = 1}. Therefore, the algorithm includes solving the following linear programming problem  minimize kw⊤Mk1 subject to r⊤w = 1.  5  Accepted as a workshop contribution at ICLR 2015  Algorithm 2 Sparse Dictionary Learning (Spielman et al., 2012).  input cM  for each j = 1, . . . , nx do  Solve minw kw⊤cMk1 subject to (cM ej)⊤w = 1, and set sj = w⊤cM . end for S = {s1, . . . , snx} for each i = 1, . . . , k do  repeat  l ← arg minsl∈S kslk0, breaking ties arbitrarily. vi = sl. S = S \ {sl}.  until Rank([v1, . . . , vi]) = i  end for  output Set ˆA1 = [ v1 kv1k  , . . . , vk kvkk  ]⊤.  It is proved that under some additional conditions, when r is chosen as a column or sum of two columns of M , the linear program is likely to produce rows of A1 with high probabil- ity (Spielman et al., 2012). We explain these conditions in our context in Section 3.1. By normalizing the rows of the output, we obtain a row-normalized version of A1. The algorithm is shown in Algorithm 2. Note that ej refers to the j-th basis vector. We ﬁnally note that there exist more sophisticated analysis and algorithms for the problem of ﬁnding the sparsest vectors in a subspace. Anandkumar et al. (2012) provide the deterministic sparsity version of the result. Barak et al. (2012) require more computation and even quasi-polynomial time but they can solve the problem in denser settings.  3.1 GUARANTEES FOR LEARNING FIRST LAYER WEIGHTS  We have the following assumptions to ensure that the weight matrix A1 ∈ Rk×nx is learnt correctly. Assumptions  A.1 Elementwise ﬁrst layer: σ1 is a elementwise function. A.2 Nondegeneracy: Ex[σ′2(A2σ1(A1x))A2 Diag(σ′1(A1x))] has full column rank1. A.3 Score function: The score function ∇x log p(x) exists. A.4 Sufﬁcient input dimension: We have nx > c1k log4 k for some positive constant c1. A.5 Sparse connectivity: The weight matrix A1 is Bernoulli(θ)-Gaussian. For some positive  constant α, we have 2  k ≤ θ ≤ α√k  .  A.6 Normalized weight matrix: The weight matrix A1 is row-normalized.  Assumption A.1 is common in deep network literature since there are only elementwise activation in the intermediate layers. Assumption A.2 is satisﬁed where A2 is full-rank and σ′2(A2σ1(A1x)), Diag(σ′1(Ax)) are non- degenerate. This is the case when the number of classes is large, i.e. ny ≥ k as in imagenets. In future, we plan to consider the setting with a small number of classes using other methods like tensor methods. For non-degeneracy assumption of σ′2(·), the reason is that we assume the functions are at least linear, i.e. their ﬁrst order derivatives are nonzero. This is true for the activation function models in deep networks such as sigmoid function, piecewise linear rectiﬁer and softmax function at the last layer.  Note that Assumption A.4 uses an improvement over Spielman’s initial result (Luh and Vu, 2015). In a deep network k is usually a few thousand while nx is in the millions. Hence, Assumption A.4 is  1Throughout this Section, we use the notation σ ′  1(A1x) to denote σ ′  1(˜x)| ˜x=Ax.  6  Accepted as a workshop contribution at ICLR 2015  satisﬁed. Note that Luh and Vu (2015) have provided an algorithm for very sparse weight matrices, which only needs nx > c1k log k. Assumption A.5 requires the weight matrix to be sparse and the expected number of nonzero ele-  ments in each column of A1 be at most O(√k) (Luh and Vu, 2015). In other words, each input is connected to at most O(√k) neurons. This is a meaningful assumption in the deep-nets literature  as it has been argued that sparse connectivity is a natural constraint which can lead to improved performance in practice (Thom and Palm, 2013).  If Assumption A.6 does not hold, we will have to learn the scaling and the bias through back propa- gation. Nevertheless, since the row-normalized ˆA1 provides the directions, the number of parameters in back propagation is reduced signiﬁcantly. Therefore, instead of learning a dense matrix we will only need to ﬁnd the scaling in a sparse matrix. This results in signiﬁcant shrinkage in the number of parameters the back propagation needs to learn.  Finally we provide the results on learning the ﬁrst layer weight matrix in a feedforward network with one hidden layer. Theorem 2. Let Assumptions A.1−A.5 hold for the nonlinear neural network (1), then Algorithm 2 uniquely recovers a row-normalized version of A1 with exponentially small probability of failure.  For proof, see (Spielman et al., 2012). Remark 3 (Efﬁcient implementation). The ℓ1 optimization is an efﬁcient algorithm to implement. The algorithm involves solving k optimization problems. Traditionally, the ℓ1 minimization can be formulated as a linear programming problem. In particular, each of these ℓ1 minimization problems can be written as a LP with 2(k − 1) inequality constraints and one equality constraint. Since the computational complexity of such a method is often too high for large scale problems, one can use approximate methods such as gradient projection (Figueiredo et al., 2007; Kim et al., 2007), iterative-shrinkage thresholding (Daubechies et al., 2004) and proximal gradient (Nesterov, 1983; Nesterov et al., 2007) that are noticeably faster (Anandkumar et al., 2012). Remark 4 (Learning ˆA2). After learning A1, we can encode the ﬁrst layer as h = σ1(A1x) and perform softmax regression to learn A2. Remark 5 (Extension to deterministic sparsity). The results in this work are proposed in the ran- dom setting where the i.i.d. Bernoulli-Gaussian entries for matrix A1 are assumed. In general, the results can be presented in terms of deterministic conditions as in (Anandkumar et al., 2012). Anandkumar et al. (2012) show that the model M = BA1 is identiﬁable when B has full column rank and the following expansion condition holds (Anandkumar et al., 2012).  |NB(S)| ≥ |S| + dmax(B),  ∀S ⊆ Columns of B, |S| ≥ 2.  Here, NB(S) := {i ∈ [k] : Bij 6= 0 for some j ∈ S} denotes the set of neighbors of columns of B in set S. They also show that under additional conditions, the ℓ1 relaxation can recover the model parameters. See (Anandkumar et al., 2012) for the details.  3.2 EXTENSION TO DEEP NETWORKS  So far, we have considered a network with one hidden layer. Now, consider a deep k-node neural network with depth d. Let y be the label vector and x be the feature vector. We have  E[y|x] = σd(Adσd−1(Ad−1σd−2(··· A2σ1(A1x)))),  (2)  where σ1 is elementwise function (linear or nonlinear). This set up is applicable to both multiclass and mutlilabel settings. For multiclass classiﬁcation, σd is the softmax function and for multilabel classiﬁcation σd is a elementwise sigmoid function. In this network, we can learn the ﬁrst layer using the idea presented earlier in this Section to learn the ﬁrst layer. From Stein’s lemma, we have  M = E[y (∇x log p(x))⊤] = −Ex[∇xy]  = E[σ′d(˜xd)Adσ′d−1(˜xd−1)Ad−1σ′d−2(˜xd−2)Ad−2 ··· σ′2(˜x2)A2 Diag(σ′1(˜x1))]A1.  Assumption B.2 Nondegeneracy:  7  Accepted as a workshop contribution at ICLR 2015  The matrix B = E[σ′d(˜xd)Adσ′d−1(˜xd−1)Ad−1σ′d−2(˜xd−2)Ad−2 ··· σ′2(˜x2)A2 Diag(σ′1(˜x1))] has full column rank. In Assumption B.2, ˜xi = Aihi, i ∈ [d] where hi denotes the input and the i-th layer. Theorem 3. Let Assumptions A.1, B.2, A.3 − A.6 hold for the nonlinear deep neural network (2). Then, Algorithm 2 uniquely recovers a row-normalized version of A1 with exponentially small prob- ability of failure.  The proof follows Stein’s lemma, use of Chain rule and (Spielman et al., 2012).  In a deep network, the ﬁrst layer includes most of the parameters (if a structure such as convolutional networks is not assumed) and other layers consist of a small number of parameters since there are small number of neurons. Therefore, the above result is a prominent progress in learning deep neural networks. Remark 6. This is the ﬁrst result to learn a subset of deep networks for general nonlinear case in supervised manner. The idea presented in (Arora et al., 2013) is for the auto-encoder setting, whereas we consider supervised setting. Also, Arora et al. (2013) assume that the hidden layer can be decoded correctly using a “Hebbian” style rule, and they all have only binary states. In addition,  they can handle sparsity level up to kγ , 0 < γ ≤ 0.2 while we can go up to √k, i.e. γ = 0.5.  Remark 7 (Challenges in learning the higher layers). In order for B to have full column rank, intermediate layers should have square weight matrices. However, if we want to learn the middle layers, A.4 requires that the number of rows of the weight matrices be smaller than the number of columns in a speciﬁc manner and therefore B cannot have full column rank. In future, we hope to investigate new methods to help in overcoming this challenge.  4 CONCLUSION  We introduced a new paradigm for learning neural networks using method-of-moments. In the literature, this method has been restricted to unsupervised setting. Here, we bridged the gap and employed it for discriminative learning. This opens up a lot of interesting research directions for future investigation. First, note that we only considered the input to have continuous distribution for which the score function exists. The question is whether learning the parameters in a neural network is possible for the discrete data. Although Stein’s lemma has a form for discrete variables (in terms of ﬁnite differences) (Wei et al., 2010), it is not clear how that can be leveraged to learn the network parameters. Next, it is worth analyzing how we can go beyond ℓ1 relaxation and provide guarantees in such cases. Another interesting problem arises in case of small number of classes. Note that for non-degeneracy condition, we require the number of classes to be bigger than the number of neurons in the hidden layers. Therefore, our method does not work for the cases where ny < k. In addition, in order to learn the weight matrices for intermediate layers, we need the number of rows to be smaller than the number of columns to have sufﬁcient input dimension. On the other hand, non-degeneracy assumption requires these weight matrices to be square matrices. Hence, learning the weights in the intermediate layers of deep networks is a challenging problem. It seems tensor methods, which have been highly successful in learning a wide range of hidden models such as topic modeling, mixture of Gaussian and community detection problem (Anandkumar et al., 2014), may provide a way to overcome the last two challenges.  ACKNOWLEDGMENT  A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF Career award CCF- 1254106, NSF Award CCF-1219234, ARO YIP Award W911NF-13-1-0084 and ONR Award N00014 − 14 − 1 − 0665. H. Sedghi is supported by ONR Award N00014 − 14 − 1 − 0665. REFERENCES Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data generating  distribution. arXiv preprint arXiv:1211.4246, 2012.  A. Anandkumar, D. Hsu, and A. Javanmard S. M. Kakade. Learning Topic Models and Latent  Bayesian Networks Under Expansion Constraints. Preprint. ArXiv:1209.5350, Sept. 2012.  8  Accepted as a workshop contribution at ICLR 2015  A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for  learning latent variable models. J. of Machine Learning Research, 15:2773–2832, 2014.  Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some  deep representations. arXiv preprint arXiv:1310.6343, 2013.  Boaz Barak, Fernando GSL Brandao, Aram W Harrow, Jonathan Kelner, David Steurer, and Yuan Zhou. Hypercontractivity, sum-of-squares proofs, and their applications. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing, pages 307–326. ACM, 2012.  Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex  neural networks. In Advances in neural information processing systems, pages 123–130, 2005.  Christopher M Bishop et al. Pattern recognition and machine learning, volume 1. springer New  York, 2006.  R Dennis Cook. Principal hessian directions revisited. Journal of the American Statistical Associa-  tion, 93(441):84–94, 1998.  Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Communications on pure and applied mathematics, 57(11):1413–1457, 2004.  Andrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation.  arXiv preprint arXiv:1312.4461, 2013.  M´ario AT Figueiredo, Robert D Nowak, and Stephen J Wright. Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems. Selected Topics in Signal Processing, IEEE Journal of, 1(4):586–597, 2007.  Aapo Hyv¨arinen. Estimation of non-normalized statistical models by score matching. In Journal of  Machine Learning Research, pages 695–709, 2005.  Tommi Jaakkola, David Haussler, et al. Exploiting generative models in discriminative classiﬁers.  In Advances in neural information processing systems, pages 487–493, 1999.  Seung-Jean Kim, Kwangmoo Koh, Michael Lustig, Stephen Boyd, and Dimitry Gorinevsky. An interior-point method for large-scale l 1-regularized least squares. Selected Topics in Signal Pro- cessing, IEEE Journal of, 1(4):606–617, 2007.  Ker-Chau Li. On principal hessian directions for data visualization and dimension reduction: another application of stein’s lemma. Journal of the American Statistical Association, 87(420):1025– 1039, 1992.  Kyle Luh and Van Vu. Dictionary learning with few samples and matrix concentration. arXiv  preprint arXiv:1503.08854, 2015.  Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2).  In Soviet Mathematics Doklady, volume 27, pages 372–376, 1983.  Yurii Nesterov et al. Gradient methods for minimizing composite objective function, 2007.  Ivan Nourdin, Giovanni Peccati, and Yvik Swan. Integration by parts and representation of infor-  mation functionals. arXiv preprint arXiv:1312.5276, 2013.  K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of  the Royal Society, London, A., page 71, 1894.  Hiroaki Sasaki, Aapo Hyv¨arinen, and Masashi Sugiyama. Clustering via mode seeking by direct  estimation of the gradient of a log-density. arXiv preprint arXiv:1404.5028, 2014.  Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In  Conference on Learning Theory, 2012.  9  Accepted as a workshop contribution at ICLR 2015  Charles Stein. Approximate computation of expectations. Lecture Notes-Monograph Series, 7:  i–164, 1986.  Charles Stein, Persi Diaconis, Susan Holmes, Gesine Reinert, et al. Use of exchangeable pairs in the analysis of simulations. In Stein’s Method, pages 1–25. Institute of Mathematical Statistics, 2004.  Yuekai Sun, Stratis Ioannidis, and Andrea Montanari. Learning mixtures of linear classiﬁers. arXiv  preprint arXiv:1311.2547, 2013.  Kevin Swersky, David Buchman, Nando D Freitas, Benjamin M Marlin, et al. On autoencoders and score matching for energy based models. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1201–1208, 2011.  Markus Thom and G¨unther Palm. Sparse activity and sparse connectivity in supervised learning.  The Journal of Machine Learning Research, 14(1):1091–1143, 2013.  Grigorios Tsoumakas and Ioannis Katakis. Multi-label classiﬁcation: An overview. International  Journal of Data Warehousing and Mining (IJDWM), 3(3):1–13, 2007.  Zhengyuan Wei, Xinsheng Zhang, and Taifu Li. On stein identity, chernoff inequality, and orthogo-  nal polynomials. Communications in StatisticsTheory and Methods, 39(14):2573–2593, 2010.  10  ",
1411.7676,2015," Visual Scene Representations: sufficiency, minimality, invariance and approximation with deep convolutional networks","['Visual Scene Representations: sufficiency', 'minimality', 'invariance and approximation with deep convolutional networks', 'Stefano Soatto and Alessandro Chiuso                   |']",https://arxiv.org/pdf/1411.7676,"6 1 0 2     b e F 9 2         ]  V C . s c [      9 v 6 7 6 7  .  1 1 4 1 : v i X r a  Published as a conference paper at ICLR 2016 VISUAL REPRESENTATIONS: DEFINING PROPERTIES AND DEEP APPROXIMATIONS  Stefano Soatto Department of Computer Science University of California, Los Angeles Los Angeles, CA 90095, USA soatto@ucla.edu  Alessandro Chiuso Dipartimento di Ingegneria dell’Informazione Universit`a di Padova Via Gradenigo 6/b, 35131 Padova, Italy alessandro.chiuso@unipd.it  ABSTRACT  Visual representations are deﬁned in terms of minimal sufﬁcient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufﬁciency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guar- antees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and ap- proximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization.  1  INTRODUCTION  A visual representation is a function of visual data (images) that is “useful” to accomplish visual tasks. Visual tasks are decision or control actions concerning the surrounding environment, or scene, and its properties. Such properties can be geometric (shape, pose), photometric (reﬂectance), dy- namic (motion) and semantic (identities, relations of “objects” within). In addition to such prop- erties, the data also depends on a variety of nuisance variables that are irrelevant to the task. De- pending on the task, they may include unknown characteristics of the sensor (intrinsic calibration), its inter-play with the scene (viewpoint, partial occlusion), and properties of the scene that are not directly of interest (e.g., illumination). We are interested in modeling and analyzing visual representations: How can we measure how “useful” one is? What guidelines or principles should inform its design? Is there such a thing as an optimal representation? If so, can it be computed? Approximated? Learned? We abstract (classes of) visual tasks to questions about the scene. They could be countable semantic queries (e.g., concerning “objects” in the scene and their relations) or continuous control actions (e.g., “in which direction to move next”). In Sect. 2.1 we formalize these questions using well-known concepts, and in Sect. 2.3 we derive an equivalent characterization that will be the starting point for designing, analyzing and learning representations.  1.1 RELATED WORK AND CONTRIBUTIONS  Much of Computer Vision is about computing functions of images that are “useful” for visual tasks. When restricted to subsets of images, local descriptors typically involve statistics of image gradi- ents, computed at various scales and locations, pooled over spatial regions, variously normalized and  1  Published as a conference paper at ICLR 2016  quantized. This process is repeated hierarchically in a convolutional neural network (CNN), with weights inferred from data, leading to representation learning Ranzato et al. (2007); LeCun (2012); Simonyan et al. (2014); Serre et al. (2007); Bouvrie et al. (2009); Susskind et al. (2011); Bengio (2009). There are more methods than we can review here, and empirical comparisons (e.g., Mikola- jczyk et al. (2004)) have recently expanded to include CNNs. Unfortunately, many implementation details and parameters make it hard to draw general conclusions Chatﬁeld et al. (2011). We take a different approach, and derive a formal expression for optimal representations from established principles of sufﬁciency, minimality, invariance. We show how existing descriptors are related to such representations, highlighting the (often tacit) underlying assumptions. Our work relates most closely to Bruna & Mallat (2011); Anselmi et al. (2015) in its aim to construct and analyze representations for classiﬁcation tasks. However, we wish to represent the scene, rather than the image, so occlusion and locality play a key role, as we discuss in Sect. 5. Also Morel & Yu (2011) characterize the invariants to certain nuisance transformations: Our models are more general, involving both the range and the domain of the data, although more restrictive than Tishby et al. (2000) and speciﬁc to visual data. We present an alternate interpretation of pooling, in the context of classical sampling theory, that differs from other analyses Gong et al. (2014); Boureau et al. (2010). Our contributions are to (i) deﬁne minimal sufﬁcient invariant representations and characterize them explicitly (Claim 1); (ii) show that local descriptors currently in use in Computer Vision can approx- imate such representations under very restrictive conditions (Claim 2 and Sec. 3.3); (iii) compute in closed form the minimal sufﬁcient contrast invariant (14) and show how local descriptors relate to it (Rem. 2); show that such local descriptors can be implemented via linear convolutions and rectiﬁed linear units (Sect. 4.3); (iv) explain the practice of “joint normalization” (Rem. 5) and “clamping” (Sect. 3.3.1) as procedures to approximate the sufﬁcient invariant; these practices are seldom ex- plained and yet they have a signiﬁcant impact on performance in empirical tests Kirchner (2015); (v) explain “spatial pooling” in terms of anti-aliasing, or local marginalization with respect to a small-dimensional nuisance group, in convolutional architectures (Sec. 4.1-4.2). In the Appendix we show that an ideal representation, if generatively trained, maximizes the information content of the representation (App. A).  2 CHARACTERIZATION AND PROPERTIES OF REPRESENTATIONS  Because of uncertainty in the mechanisms that generate images, we treat them as realizations of random vectors x (past/training set) and y (future/test set), of high but ﬁnite dimension. The scene they portray is inﬁnitely more complex.1 Even if we restrict it to be a static “model” or “parameter” θ, it is in general inﬁnite-dimensional. Nevertheless, we can ask questions about it. The number of questions (“classes”) K is large but ﬁnite, corresponding to a partition of the space of scenes, represented by samples {θ1, . . . , θK}. A simple model of image formation including scaling and occlusion Dong & Soatto (2014) is known as the Lambert-Ambient, or LA, model.  2.1 DESIDERATA  Among (measurable) functions of past data (statistics), we seek those useful for a class of tasks. Abstracting the task to questions about the scene, “useful” can be measured by uncertainty reduction on the answers, captured by the mutual information between the data x and the object of interest θ. While a representation can be no more informative than the data,2 ideally it should be no less, i.e., a sufﬁcient statistic. It should also be “simpler” than the data itself, ideally minimal. It should also discount the effects of nuisance variables g ∈ G, and ideally be invariant to them. We use a = {x1, . . . , xt}. superscript to denote a collection of t data points (the history up to t, if ordered), xt .  Thus, a representation is any function φ constructed using past data xt that is useful to answer questions about the scene θ given future data y it generates, regardless of nuisance factors g.  1Scenes are made of surfaces supporting reﬂectance functions that interact with illumination, etc. No matter  how many images we already have, even a single static scene can generate inﬁnitely many different ones.  2Data Processing Inequality, page 88 of Shao (1998).  2  Published as a conference paper at ICLR 2016  An optimal representation is a minimal sufﬁcient statistic for a task that is invariant to nuisance factors. In Sec. 2.3 we introduce the SA Likelihood, an optimal representation, and in subsequent sections show how it can be approximated.  2.2 BACKGROUND  The data X is a random variable with samples x, y; the model θ is unknown in the experiment E = {x, θ, pθ(x)} where pθ(x) is the probability density function of X, that depends on the parameter θ, evaluated at the sample x; a statistic T is a function of the sample; it is sufﬁcient (of x for θ) if X | T = τ does not depend on θ;3 it is minimal if it is a function of all other sufﬁcient statistics4. If θ is treated as a random variable and a prior is available, φ is Bayesian sufﬁcient5 if p(θ|φ(xt)) = p(θ|xt). If T is minimal, any smaller6 U entails “information loss.” If θ was a discrete random variable, the information content of T could be measured by uncertainty reduction: H(θ) − H(θ|T (X)), which is the mutual information7 between θ and T and H denotes entropy Cover & Thomas (1991); fur- thermore, T (X) ∈ arg inf φ H(θ|φ(X)), where the inﬁmum is with respect to measurable functions and is in general not unique. Consider a set G of transformations g of the data x, g(x), which we denote simply as gx. A function φG (x) is G-invariant if φG(gx) = φG (x) for all g ∈ G. The sensitivity of φ to G is S = (cid:107) ∂φ(gx) ∂g (cid:107) where φ is assumed to be differentiable. By deﬁnition, an invariant has zero sensitivity to G. . The Likelihood function is L(θ; x) = pθ(x), understood as a function of θ for a ﬁxed sample x, sometimes written as p(x|θ) even though θ is not a random variable. Theorem 3.2 of Pawitan (2001) can be extended to an inﬁnite-dimensional parameter θ (Theorem 6.1 of Bahadur (1954)): Theorem 1 (The likelihood function as a statistic). The likelihood function L(θ; x) is a minimal sufﬁcient statistic of x for θ.  2.3 NUISANCE MANAGEMENT: PROFILE, MARGINAL, AND SAL LIKELIHOODS A nuisance g ∈ G is an unknown “factor” (a random variable, parameter, or transformation) that is not of interest and yet it affects the data. Given pθ(·), when g is treated as a parameter that . = pθ(gy); when it is treated as a random variable, transforms the data via g(y) pθ(y|g)  . = pθ(gy). The proﬁle likelihood  . = gy, then pθ,g(y)  where the nuisance has been “maxed-out” is G-invariant. The marginal likelihood  pθ,G(y)  pθ,g(y)  . = sup g∈G  (cid:90)  pθ(y|G)  . =  pθ(y|g)dP (g)  (1)  (2)  G  is invariant only if dP (g) = dµ(g) is the constant8 measure on G. Both are sufﬁcient invariants, in the sense that they are invariant to G and are minimal sufﬁcient. This counters the common belief that “invariance trades off selectivity.” In Rem. 1 we argue that both can be achieved, at the price of complexity. Computing the proﬁle likelihood in practice requires reducing G to a countable set {g1, . . . , gN} of samples,9 usually at a loss. The tradeoff is a subject of sampling theory, where samples can  Blackwell & Ramamoorthi (1982).  3Deﬁnition 3.1 of Pawitan (2001) or Sec. 6.7 of DeGroot (1989), page 356 4If U is sufﬁcient, then the sigma algebra σ(T ) ⊂ σ(U ), DeGroot (1989), page 368. 5The two are equivalent for discrete random variables, but pathological cases can arise in inﬁnite dimensions 6In the sense of inclusion of sigma algebras, σ(U ) ⊂ σ(T ). 7See Cover & Thomas (1991) eq. 2.28, page 18. 8Base, or Haar measure if G is a group. It can be improper if G is not compact. 9Note that N can be inﬁnite if G is not compact.  3  (cid:90)  Published as a conference paper at ICLR 2016  be generated regularly, independent of the signal being sampled, or adaptively.10 In either case, the occurrence of spurious extrema (“aliases”) can be mitigated by retaining not the value of the function at the samples, pθ,gi(y), but an anti-aliased version consisting of a weighted average around the samples:  (cid:90)  ˆpθ,gi(y)  . =  pθ,gi(gy)w(g)dµ(g)  (3)  for suitable weights w.11 When the prior dP (g) = w(g)dµ(g) is positive and normalized, the previous equation (anti-aliasing) can be interpreted as local marginalization, and is often referred to as mean-pooling. The approximation of the proﬁle likelihood obtained by sampling and anti- aliasing, is called the SA (sampled anti-aliased) likelihood, or SAL:  ˆpθ,G(y) = max  i  ˆpθ,gi(y) = max  i  pθ,gi(gy)dP (g).  (4)  The maximization over the samples in the above equation is often referred to as max-pooling. Claim 1 (The SAL is an optimal representation). Let the joint likelihood pθ,g be smooth with respect to the base measure on G. For any approximation error (cid:15), there exists an integer N = N ((cid:15)) number of samples {gi}N i=1 and a suitable (regular or adaptive) sampling mechanism so that the SAL maxi ˆpθ,gi approximates to within (cid:15) the proﬁle likelihood supg∈G pθ,g, after normalization, in the sense of distributions.  For the case of (conditionally) Gaussian models under the action of the translation group, the claim follows from classical sampling arguments. More generally, an optimal representation is difﬁcult to compute. In the next section, we show a ﬁrst example when this can be done. Remark 1 (Invariance, sensitivity and “selectivity”). It is commonly believed that invariance comes at the cost of discriminative power. This is partly due to the use of the term invariance (or “ap- proximate invariance” or “stability”) to denote sensitivity, and of the term “selectivity” to denote maximal invariance. A function is insensitive to g if small changes in g produce small changes in its value. It is invariant to g if it is constant as a function of g. It is a maximal invariant if equal value implies equivalence up to G (Sect. 4.2 of Shao (1998), page 213). It is common to decrease sensitiv- ity with respect to a transformation g by averaging the function with respect to g, a lossy operation in general. For instance, if the function is the image itself, it can be made insensitive to rotation about its center by averaging rotated versions of it. The result is an image consisting of concentric circles, with much of the informative content of the original image gone, in the sense that it is not possible to reconstruct the original image. Nevertheless, while invertibility is relevant for reconstruction tasks, it is not necessarily relevant for classiﬁcation, so it is possible for the averaging operation to yield a sufﬁcient invariant, albeit not a maximal one.  Thus, one can have invariance while retaining sufﬁciency, albeit generally not maximality: The proﬁle likelihood, or the marginal likelihood with respect to the uniform measure, are sufﬁcient statistics and are (strictly) invariant. The price to have both is complexity, as both are inﬁnite- dimensional in general. However, they can be approximated, which is done by sampling in the SA likelihood.  2.4 A FIRST EXAMPLE: LOCAL REPRESENTATIONS/DESCRIPTORS  Let the task be to decide whether a (future) image y is of a scene θ given a single training image x of it, which we can then assume to be the scene itself: x = θ. Nuisances affecting y are limited  10{gi}N  i=1 are generated by a (deterministic or stochastic) mechanism ψ that depends on the data and respects the structure of G. If G is a group, this is known as a co-variant detector: It is a function ψ that (is Morse i=1 = {g | ∇Gψ(y, g) = 0} that equivary: ∇Gψ(˜gy, gi) = 0 ⇒ in g, i.e., it) has isolated extrema {gi(y)}N ∇Gψ(y, ˜ggi) = 0 for all i and ˜g ∈ G. The samples {gi}N i=1 deﬁne a reference frame in which an invariant can (y)y | ∇Gψ(y, gi) = 0}. be easily constructed in a process known as canonization Soatto (2009): φ(y) 11For regular sampling of stationary signals on the real line, optimal weights for reconstruction can be derived explicitly in terms of spectral characteristics of the signal, as done in classical (Shannon) sampling theory. More in general, the computation of optimal weights for function-valued signals deﬁned on a general group G for tasks other than reconstruction, is an open problem. Recent results Chen & Edelsbrunner (2011) show that diffusion on the location-scale group typically reduce the incidence of topological features such as aliases in the ﬁltered signal. Thus, low-pass ﬁltering such as (generalized) Gaussian smoothing as done here, can have anti-aliasing effects.  = {g .  −1 i  4  Published as a conference paper at ICLR 2016  to translation parallel to the image plane, scaling, and changes in pixel values that do not alter relative order.12 Under these (admittedly restrictive) conditions, the SAL can be easily computed and corresponds to known “descriptors.” Note that, by deﬁnition, x and y must be generated by the same scene θ for them to “correspond.” SIFT Lowe (2004) performs canonization10 of local similarity transformations via adaptive sam- pling of the planar translation-scale group (extrema of the difference-of-Gaussians operator in space and scale), and planar rotation (extrema of the magnitude of the oriented gradient). Alternatively, locations, scales and rotation can be sampled regularly, as in “dense SIFT.” Regardless, on the do- main determined by each sample, gi, SIFT computes a weighted histogram of gradient orientations. Spatial regularization anti-aliases translation; histogram regularization anti-aliases orientation; scale anti-aliasing, however, is not performed, an omission corrected in DSP-SIFT Dong & Soatto (2015). In formulas, if α(y) = ∠∇y ∈ S1 is a direction, θ = xi is the image restricted to a region determined by the reference frame gi, centered at (ui, vi) ∈ R2 axis-aligned and with size si > 0, we have13  φxi(y) =  κσ(ui − ˜u, vi − ˜v)κ(cid:15)(∠∇x(˜u, ˜v), α(y))(cid:107)∇x(˜u, ˜v)(cid:107)Esi(σ)d˜ud˜vdσ  (5) and φsift(α) = [φx11(y), . . . , φx44(y)] is a sampling on a 4×4 grid, with each sample assumed inde- pendent and α = ∠∇y quantized into 8 levels. Variants of SIFT such as HOG differ in the number and location of the samples, the regions where the histograms are accumulated and normalized. Here κσ and κ(cid:15) are Parzen kernels (bilinear in SIFT; Gaussian in DSP-SIFT) with parameter σ, (cid:15) > 0 and Es is an exponential prior on scales. Additional properties of SIFT and its variants are discussed in Sect. 3.3, as a consequence of which the above approximates the SAL for translation-scale and contrast transformation groups. Claim 2 (DSP-SIFT). The continuous extension of DSP-SIFT Dong & Soatto (2015) (5) is an anti- aliased sample of the proﬁle likelihood (4) for G = SE(2)×R+×H the group of planar similarities transformations and local contrast transformations, when the underlying scene θ = xi has locally stationary and ergodic radiance, and the noise is assumed Gaussian IID with variance proportional to the norm of the gradient.  (cid:90)  The proof follows from a characterization of the maximal invariant to contrast transformations de- scribed in Sect. 3.3. Out-of-plane rotations induce a scene-shape-dependent deformation of the domain of the image that cannot be determined from a single training image, as we discuss in Sect. 3. When interpreting local descriptors as samples of the SAL, they are usually assumed independent, an assumption lifted in Sect. 4 in the context of convolutional architectures.  3 A MORE REALISTIC INSTANTIATION  Relative motion between a non-planar scene and the viewer generally triggers occlusion phenomena. These call for the representation to be local. Intrinsic variability of a non-static scene must also be taken into account in the representation. In this section we describe the approximation of the SAL under more realistic assumptions than those implied by local, single-view descriptors such as SIFT.  3.1 OCCLUSION, CLUTTER AND “RECEPTIVE FIELDS” The data y has many components, y = {y1, . . . , yMy }, only an unknown subset of which from the scene or object of interest θ (the “visible” set V ⊂ D = {1, . . . , My}). The rest come from clutter, 12The planar translation-scale group can be taken as a very crude approximation of the transformation in- duced on the image plane by spatial translation. Contrast transformations (monotonic continuous transforma- tions of the range of the image) can be interpreted as crude approximations of changes of illumination. 13Here gy(ui, vi) = y(ui + u, vi + v) for the translation group and gy(ui, vi) = y (σui + u, σvi + v)) for translation-scale. In general, gy − x (cid:54)= y − g−1x, unless gy − x = 0. If px(y(u)) = q(y(u)− x(u)) for some . q is a density function for the random variable y(u), in general q(gy(u)− x(u)) (cid:54)= q(y(u)− g−1x(u)), unless the process y is G-stationary independent and identically distributed (IID), in which case px(gy) = pg−1x(y). Note that the marginal density of the gradient of natural images is to a reasonable approximation invariant to the translation-scale group Huang & Mumford (1999).  5  Published as a conference paper at ICLR 2016  V = (cid:83)M  occlusion and other phenomena unrelated to θ, although they may be informative as “context.” We indicate the restriction of y to V as y|V = {yj, j ∈ V }. Since the visible set is not known, proﬁling pθ,G(y) = maxi,V ∈P(D) pθ,gi(y|V ) requires searching over the power set P(D). To make computation tractable, we can restrict the visible set V to be the union of “receptive ﬁelds” Vj, that can be obtained by transforming14 a “base region” B0 (“unit ball,” for instance a square patch of pixels with an arbitrary “base size,” say 10 × 10) with group elements gj ∈ G, Vj = gjB0: . j=1 gjB0 where the number of receptive ﬁelds M (cid:28) My. Thus V is determined by the j=1 of receptive ﬁelds that are “active,” which are unknown  reference frames (group elements) {gj}M a-priori. Alternatively, we can marginalize V by computing, for every class (represented by a hidden variable θk as discussed next) and every receptive ﬁeld (determined by gj as above), conditional densities pθ(y|θk, gj) that can be averaged with respect to weights wjk, trained to select (if sparse along j) and combine (by averaging along k) local templates via  pθ(y|θk, gj)wjk  (6)  (cid:88)  j,k  where the weights or “ﬁlters” wjk, if positive and normalized,15 are interpreted as probabilities wjk = pθ(θk, gj). To make this marginalization tractable, we write the ﬁrst term as |θk, gj)  |θk, gj)pθ(y|V c  ) ∝ p(y|Vj  pθ(y|Vj  , y|V c  (7)  j  where we have assumed that the second factor is constant, thus ignoring photometric context beyond the receptive ﬁelds, e.g., due to mutual illumination. Under these assumptions, we have  j  |θk, gj) = p(y|Vj (cid:88)  pθ,gi(y) =  p(y|Vj  |θk, gigj)pθ,gi(gjθk)  (8)  j,k  where the ﬁrst term in the sum is known as a “feature map” and we have assumed that both gi, gj ∈ G for simplicity, with gij = gigj. The order of operations (deformation by gi and selection by gj) is arbitrary, so we assume that the selection by gj is applied ﬁrst, and then the nuisance-induced deformation gi, so pθ,gi(gjy) ∝ pθ,e(gijy), where e is the identity of the group G.  3.2  INTRA-CLASS VARIABILITY AND DEFORMABLE TEMPLATES  For category-level recognition, the parameter space can be divided into K classes, allowing vari- ability of θ within each class. Endowing the parameter space with a distribution p(θ|k) requires deﬁning a probability density in the space of shapes, reﬂectance functions etc. Alternatively one can capture the variability θ induces on the data. For any scene θk from class k, one can consider a single image generated by it xk ∼ pθk (x) as a “template” from which any other datum from the same class can be obtained by the (transitive) action of a group gk ∈ G. Thus if y ∼ pθk (y) with θk ∼ p(θ|k), which we indicate with y ∼ p(y|k), then we assume that there exists a gk such that y = g−1  k xk, so  p(y|k) =  =  p(y|xk, gk)dP (xk, gk|θk)dP (θk|k) p(gky|θk)dP (gk|θk)  (9)  (cid:90) (cid:90)  where we have used the fact that p(y|xk, gk, k) = δ(y − g−1 k xk) and that only one sample of θk is given, so all variability is represented by gk and xk conditionally on θk.16 For this approach to work, gk has to be sufﬁciently complex to allow xk to “reach” every datum y generated by an element17 of  14The action of a group g on a set B ⊂ D is deﬁned as gB ⊂ D such that g(y|B ) = y|gB. 15Note that current convolutional architectures rectify and normalize the feature maps, not the ﬁlters. How-  ever, learned biases, as well as rectiﬁcation and normalization at each layer, may partly compensate for it.  16In the last expression we assumed that xk and gk are conditionally independent given θk, i.e., that the  image formation process (noise) and deformation are independent once the scene θk is given.  17The group has to act transitively on xk. For instance, in Grenander (1993) gk was chosen to belong to the  entire (inﬁnite-dimensional) group of domain diffeomorphisms.  6  Published as a conference paper at ICLR 2016  j,k  (cid:88) (cid:88) (cid:88) (cid:124)  j,k  j,k  (cid:90) (cid:90)  the class. Fortunately, the density on a complex group can be reduced to a joint density on GM , the mutual conﬁguration of the receptive ﬁelds, as we will show. The restriction of gk to the domain of the receptive ﬁeld Vj = gjB0 is indicated by {gkj}M j=1, deﬁned by gkj x = gkx ∀ x ∈ Vj. Then, we can consider the global group nuisance gi, the selector of receptive ﬁelds gj and the local restriction of the intra-class group gk, assumed (d(gk, e)) small, as all belonging to the same group G, for instance afﬁne or similarity transformations of the domain and range space of the data. Starting from (8), neglecting gi for the moment, we have18 |θk, gj)pθ(θk, gj)  pθ(y) =  (10)  p(y|Vj  (11)  (12)  =  GM  |θk, gkj )dP ({gkj}|θk)pθ(θk, gj)  p(y|Vj  and bringing back the global nuisance gi,  pθ,G(y) = max  i  GM  p(gikj y|Vj  |θk)dPG({gkj}|θk)pθ(θk, gj) (cid:123)(cid:122) (cid:125)  pθ(giy)  where the measure in the last equation is made invariant to gi ∈ G. The feature maps p(gigkj y|Vj |θk) represent the photometric component of the likelihood. The geometric component is the relative conﬁguration of receptive ﬁelds {gkj}, which is class-dependent but G-invariant. The inner inte- gral corresponds to “mean pooling” and the maximization to “max pooling.” The sum over j, k marginalizes the local classes θk, or “parts” and selects them to compose the hypothesis θ. To summarize, gi are the samples of the nuisance group in (4); gj are the local reference frames that deﬁne each receptive ﬁeld in (8); gk are the global deformations that deﬁne the variability induced by a class k on a template in (9). The latter are in general far more complex than the former, but their restriction to each receptive ﬁeld, gkj , can be approximated by an afﬁne or similarity transformation and hence composed with gi and gj. Note that (11) can be interpreted as a model of a three-layer neural network: The visible layer, where |θk, gkj ) live, and an output layer that, after y lives, a hidden layer, where the feature maps p(y|Vj rectiﬁcation and normalization, yields an approximation of the likelihood pθ(y). Invariance to G can be obtained via a fourth layer outputting pθ,G(y) by max-pooling third-layer outputs pθ(giy) for different gi in (12).  3.3 CONTRAST INVARIANCE  Contrast is a monotonic continuous transformation of the (range space of the) data, which can be used to model changes due to illumination. It is well-known that the curvature of the level sets of the image is a maximal invariant Alvarez et al. (1993). Since it is everywhere orthogonal to the level sets, the gradient orientation is also a maximal contrast invariant. Here we compute a contrast invariant by marginalizing the norm of the gradient of the test image (thus retaining its orientation) in the likelihood function of a training image. Since the action of contrast transformations is spatially independent, in the absence of other nuisances we assume that the gradient of the test image y can be thought of as a noisy version of the gradient of the training image x, i.e.,  (13) and compute the density of y given x marginalized with respect to contrast transformations H of y. Theorem 2 (Contrast-invariant sufﬁcient statistic). The likelihood of a training image x at a given pixel, given a test image y, marginalized with respect to contrast transformations of the latter, is  ∇y ∼ N (∇x, (cid:15)2)  18Here we condition on the restrictions gkj of gk on the receptive ﬁelds Vj so that, by deﬁnition,  p(y|Vj  |θk, gj, gk) = p(y|Vj  |θk, gkj ).  7  Published as a conference paper at ICLR 2016  (cid:18)  (cid:19)  M  (14)  given by  px(y|H)  where, if we call Ψ(a) then  . = 1√ 2π  = p(∠∇y|∇x) = .  exp  1√ 2π(cid:15)2  (cid:82) a −∞ e− 1 2 τ 2  M =  − (m)2 2(cid:15)2√ (cid:15)e 2π  2(cid:15)2 sin2(∠∇y − ∠∇x)(cid:107)∇x(cid:107)2 − 1 (cid:16)− m  (cid:17)  .  + m − mΨ  (cid:15)  dτ for any a ∈ R, and m  = cos(∠∇y − ∠∇x)(cid:107)∇x(cid:107), .  (15)  The expression in (14) is, therefore, a minimal sufﬁcient statistic of y that is invariant to contrast transformations.  Figure 1: SIFT integrand (18) (red) vs. marginalized likelihood (14) (blue) computed for a random patch on α ∈ [−π, π] (left), and on a regular sub-sampling of 8 orientations (right). Several random tests are shown as mean and error-bars corresponding to three standard deviations across trials.  Remark 2 (Relation to SIFT). Compared to (14), SIFT (i) neglects the normalization factor M√ (ii) replaces the kernel  2π(cid:15)  ,  ˜κ(cid:15)(α)  . = exp  with a bilinear one κ(cid:15) deﬁned by  κ(cid:15)(α)  (cid:19)  (cid:18) 1  (cid:19)  2(cid:15)2 sin2(α)  (cid:39) exp  2(cid:15)2 α2  (cid:18) 1 (cid:26) α+(cid:15)  . =  (cid:15)2 (cid:15)−α (cid:15)2  α ∈ [−(cid:15), 0] α ∈ [0, −(cid:15)]  (cid:18) 1  (cid:19)  and, ﬁnally, (iii) multiplies the result by the norm of the gradient, obtaining the sift integrand  φsift(∠∇y|∇x) = κ(cid:15)(∠∇y − ∠∇x)(cid:107)∇x(cid:107) To see this, calling α = ∠∇y − ∠∇x and β = (cid:107)∇x(cid:107) > 0, notice that  κ(cid:15)(α)β = κ(cid:15)β(αβ) (cid:39) exp  2(cid:15)2β2 α2β2  (cid:39) ˜κ(cid:15)(α)  where the left-hand side is (18) and the right-hand side is (14) or (23). We make no claim that this approximation is good, it just happens to be the choice made by SIFT, and the above just highlights the relation to the contrast invariant (14). (cid:15) (cid:39) 0 holds uniformly (in α) pro- Remark 3 (Uninformative training images). It is possible that m (cid:15) (cid:28) 1, i.e., , if the modulus of the gradient ∇x is very small as compared to the standard vided γ deviation (cid:15). Under such circumstances, the training image x is essentially constant (“ﬂat”), and the conditional density p(α|∇x) becomes uniform  p(α|∇x) (cid:39) =  2(cid:15)2 γ2(1−(cid:104)∇y,∇x(cid:105)2) (cid:15)√ − 1 1√ 2π(cid:15)2 e (cid:15)√ 1√ 2π(cid:15)2 2π 2(cid:15)2 γ2(1−(cid:104)∇y,∇x(cid:105)2) (cid:39) 1 when γ − 1  = 1 2π  2π  where the approximation holds given that e SIFT (18), that becomes zero when the norm of the gradient goes to zero.  (cid:15) (cid:28) 1. This is unlike  8  (16)  (17)  (18)  (19)  (20)  −3−2−10123012345678910x 10−3−3−2−1012300.10.20.30.40.50.60.70.80.91Published as a conference paper at ICLR 2016  Note that, other than for the gradient, the computations in (14) can be performed point-wise, so for an image or patch with pixel values yi, if αi(y) p(α|∇x) =  = ∠∇yi, we can write .  (cid:89)  p(αi|∇xi).  (21)  i  We often omit reference to contrast transformations H in px(y|H), when the argument α makes it clear we are referring to a contrast invariant. The width of the kernel (cid:15) is a design (regularization) parameter. Remark 4 (Invariance for x). Note that (21) is invariant to contrast transformations of y, but not of x. This should not be surprising, since high-contrast training patches should yield tests with high conﬁdence, unlike low-contrast patches. However, when the training set contains instances that are subject to contrast changes, such variability must be managed. To eliminate the dependency on (cid:107)∇x(cid:107), consider a model where the noise is proportional to the norm of the gradient:  (22)  (cid:19)  where ˜(cid:15)((cid:107)∇x(cid:107)) = (cid:15)(cid:107)∇x(cid:107). Under this noise model, the sufﬁcient contrast invariant (14) becomes  M  exp  1√ 2π(cid:15)2  px(y|H)  = p(∠∇y|∇x) = .  2(cid:15)2 sin2(∠∇y − ∠∇x) − 1  (23) and M has the same expression (15) but with m = cos(∠∇y − ∠∇x). Thus a simple approach to managing contrast variability of x in addition to y is to use the above expression in lieu of (14). Remark 5 (Joint normalization). If we consider only afﬁne contrast transformations ax + b where a, b are assumed to be constant on a patch V which contains all the cells Ci where the descriptors are computed19 it is clear that to recapture invariance w.r.t. the scale factor a it is necessary and sufﬁcient that p(∠∇y|∇x(vi)) = p(∠∇y|a∇x(vi)), ∀vi ∈ V . We shall now illustrate how this invariance can be achieved. Assume that data generating model (13) is replaced by the distribution-dependent model (cid:107)∇x(cid:107)2px(∇x)d∇x  (cid:15)2(px) = σ2Ex(cid:107)∇x(cid:107)2 = σ2  ∇y ∼ N (∇x, (cid:15)2(px))  (cid:90)  (24)  ∇y ∼ N(cid:0)∇x, ˜(cid:15)2(cid:1) (cid:18)  where the noise variance (cid:15)2 depends linearly on the average squared gradient norm (w.r.t. the distribution px(∇x)); σ2 is ﬁxed constant. The resulting marginal distribution for the gradient orientation becomes  (cid:19)  (cid:107)∇x(cid:107)2 Ex(cid:107)∇x(cid:107)2  exp  (cid:18) − 1 2σ2 sin2(∠∇y − ∠∇x) √Ex(cid:107)∇x(cid:107)2 (cid:107)∇x(cid:107) − ( ¯m)2 2σ2√ σe 2π  (cid:16)− ¯m  + ¯m − ¯mΨ  (cid:17)  σ  ,  .  ¯M =  ¯M  (25)  (26)  ¯px(y|H)  = ¯p(∠∇y|∇x) = .  1√ 2πσ2 = cos(∠∇y − ∠∇x) .  where, deﬁning ¯m  Equation (25) is clearly invariant to afﬁne transformations of the image values x(v) → ax(v) + b, ∀v ∈ V .20 It is a trivial calculation to show that using ¯p(∠∇y|ρ) in lieu of p(∠∇y|ρ), the result is invariant w.r.t afﬁne transformations. To obtain a sampled version of this normalization the expected squared gradient norm can be re- placed with the sample average on the training patch  Npix(cid:88)  i=1  ˆρ2 . =  1  Npix  (cid:107)∇x(vi)(cid:107)2  19SIFT divides each patch into a 4 × 4 grid of cells Ci, i = 1, .., 16 20Note that an afﬁne transformation on the image values x(v) → ax(v) + b, ∀v ∈ V , induces a scale a px(ρ/a) and therefore the average squared  transformation on the distribution px(∇x) so that pax+b(ρ) = 1 gradient is scaled by a2, i.e. Eax+b(cid:107)ρ(cid:107)2 = a2Ex(cid:107)ρ(cid:107)2.  9  Published as a conference paper at ICLR 2016  so that (24) becomes,  Npix(cid:88)  ∇y ∼ N (∇x, (cid:15)2(V ))  (27) where vi ∈ V , i = 1, .., Npix are the pixel locations in the training patch V . This procedure is known as “joint normalization”, and is simply equivalent to normalizing the patch in pre-processing by dividing by the average gradient norm.  (cid:15)2(V ) = σ2 ˆρ2 = σ2 1 Npix  (cid:107)∇x(vi)(cid:107)2  i=1  3.3.1 CLAMPING GRADIENT ORIENTATION HISTOGRAMS  (cid:82) min{φsift(α|x), τ} S1 min{φsift(α|x), τ}dα  Local descriptors such as SIFT commonly apply a “clamping” procedure to modify a (discretized, spatially-pooled, un-normalized) density φsift of the form (18), by clipping it at a certain threshold τ, and re-normalizing:  φclamp(α|x) =  (28) where α = ∠∇y ∈ S1 is typically discretized into 8 bins and τ is chosen as a percentage of the maximum, for instance τ = 0.2 ∗ maxα φsift(α|x). Although clamping has a dramatic effect on performance, with high sensitivity to the choice of threshold, it is seldom explained, other than as a procedure to “reduce the inﬂuence of large gradient magnitudes” Lowe (2004). Here, we show empirically that (18) becomes closer to (14) after clamping for certain choices of threshold τ and sufﬁciently coarse binning. Fig. 2 shows that, without clamping, (18) is considerably more peaked than (14) and has thinner tails. After clamping, however, the approximation improves, and for coarse binning and threshold between around 20% and 30% the two are very similar.  Figure 2: Clamping effects: For α ∈ [−π, π] (abscissa), the top shows the marginalized likelihood p(α|∇x) (14) (blue), the SIFT integrand (18) (solid red), and its clamped version (28) (dashed red) for thresholds ranging from 50% to 10% of its maximum. The bottom shows the same discretized to 8 orientation bins. The clamping approximation is sensible only for coarse binning, and heavily inﬂuenced by the choice of threshold. For an 8-bin histogram, the best approximation is typically achieved with clamping threshold between 10% and 30% of the maximum; note that Lowe (2004) empirically chose 20%.  3.4 ROTATION INVARIANCE  Canonization Soatto (2009) is particularly well suited to deal with planar rotation, since it is possible to design co-variant detectors with few isolated extrema. An example is the local maximum of the norm of the gradient along the direction α = ˆαl(x). Invariance to G = SO(2) can be achieved by retaining the samples {pθ(α|ˆαl)}L l=1. When no consistent (sometimes referred to as “stable”) reference ˆαl can be found, it means that there is no co-variant functional with isolated extrema with respect to the rotation group, which means that the data is already invariant to rotation. Note that, again, planar rotations can affect both the training image x and the test image y. In some cases, a consistent reference (canonical element) is available. For instance, for geo-referenced scenes L = 1, and the projection of the gravity vector onto the image plane , ˆα, provides a canonical reference unless the two are orthogonal:  pθ(α|G) = pθ(α|ˆα).  (29)  4 DEEP CONVOLUTIONAL ARCHITECTURES  In this section we study the approximation of (12) implemented by convolutional architectures. Starting from (12) for a particular class and a ﬁnite number of receptive ﬁelds we notice that, since  10  −3−2−1012300.0010.0020.0030.0040.0050.0060.0070.0080.0090.01  ExactSIFTSIFT clamped at 50%−3−2−1012300.0010.0020.0030.0040.0050.0060.0070.0080.0090.01  ExactSIFTSIFT clamped at 40%−3−2−1012300.0010.0020.0030.0040.0050.0060.0070.0080.0090.01  ExactSIFTSIFT clamped at 30%−3−2−1012300.0010.0020.0030.0040.0050.0060.0070.0080.0090.01  ExactSIFTSIFT clamped at 20%−3−2−1012300.0010.0020.0030.0040.0050.0060.0070.0080.0090.01  ExactSIFTSIFT clamped at 10%−3−2−1012300.10.20.30.40.50.60.70.8  ExactSIFTSIFT clamped at 50%−3−2−1012300.10.20.30.40.50.60.70.8  ExactSIFTSIFT clamped at 40%−3−2−1012300.10.20.30.40.50.60.70.8  ExactSIFTSIFT clamped at 30%−3−2−1012300.10.20.30.40.50.60.70.8  ExactSIFTSIFT clamped at 20%−3−2−1012300.10.20.30.40.50.60.70.8  ExactSIFTSIFT clamped at 10%Published as a conference paper at ICLR 2016  the “true scene” θ and the nuisances g are unknown, we cannot factor the likelihood pθ,g(y) into a product of pθ,gj , which would correspond to a “bag-of-words” discarding the dependencies in dPG({gj}|θ). Convolutional architectures (CNNs) promise to capture such dependencies by hier- archical decomposition into progressively larger receptive ﬁelds. Each “layer” is a collection of separator (hidden) variables (nodes) that make lower layers (approximately) conditionally indepen- dent.  4.1 STACKING SIMPLIFIES NUISANCE MARGINALIZATION  We show this in several steps. We ﬁrst argue that managing the group of diffeomorphisms can be accomplished by independently managing small diffeomorphisms in each layer. We use marginal- ization, but a similar argument can be constructed for max-out or the SA likelihood. Then, we leverage on the local restrictions induced by receptive ﬁelds, to deal with occlusion, and argue that such small diffeomorphisms can be reduced locally to a simpler group (afﬁne, similarity, location- scale or even translations, the most common choice in convolutional architectures). Then global marginalization of diffeomorphisms can be accomplished by local marginalization of the reduced group in each layer. The following lemma establishes that global diffeomorphisms can be approxi- mated by the composition of small diffeomorphisms. Lemma 1. Let g ∈ G, g : D → D be an orientation-preserving diffeomorphism of a compact subset D of the real plane, e ∈ G the identity, and d(e, g) the “size” of g. Then for any (cid:15) > 0 there exists an N < ∞ and g1, . . . , gN such that g = g1 ◦ g2 ··· ◦ gN and d(e, gi) < (cid:15) ∀ i = 1, . . . , N. Now for two layers, let g = g1 ◦ g2, with g1, g2 ∼ p(g) drawn independently from a prior on G. Then p(g|g1, g2) = δ(g − g1 ◦ g2) (or a non-degenerate distribution if gi are approximated by elements of the reduced group). Then let θ1 = g2θ, or more generally let θ1 be deﬁned such that θ1 ⊥ g1 | θ, g2. We then have  pG (y|θ) =  p(y|θ, g)dP (g) =  p(y|θ1, g1, g)dP (θ1, g1, g2|θ, g)dP (g) =  (30)  (cid:90)  =  p(y|θ1, g1)dP (g1|θ)p(θ1|θ, g2)dP (g2|θ)dθ1  (31) where we have also used the fact that y ⊥ θ | θ1. Once the separator variable θ1 is reduced to a number K1 of ﬁlters, we have  (cid:90) (cid:90)  p(y|θ1  k, g1)dP (g1|θ)  p(θ1  pG(y|θ1  k)pG(θ1  k|θ)  (32)  k=1  k=1  in either case, by extending this construction to L = N layers, we can see that marginalization of each layer can be performed with respect to (conditionally) independent diffeomorphisms that can be chosen to be small per the Lemma above. Claim 3. Marginalization of the likelihood with respect to an arbitrary diffeomorphism g ∈ G can be achieved by introducing layers of hidden variables θl l = 1, . . . , L and independently marginal- izing small diffeomorphisms gl ∈ G at each layer.  The next step is to restrict the marginalization to each receptive ﬁeld, at which point it can be approximated by a reduced subgroup, or the (linear) generators.  (cid:90)  k|θ, g2)dP (g2|θ) (cid:39) K1(cid:88)  pG (y|θ) (cid:39) K1(cid:88)  (cid:90)  4.2 HIERARCHICAL DECOMPOSITION OF THE LIKELIHOOD  Let  . =  p(y|θ, g)dP (g)  (33)  be the marginal likelihood with respect to some prior on G and introduce a layer of “separator variables” θ1 and group actions g, deﬁned such that y ⊥ θ | (θ1, g1). This can always be done by choosing θ1 = y; we will address non-trivial choices later. In either case, forgoing the subscript G,  (cid:90)  G  pG (y|θ) (cid:90)  p(y|θ) =  p(y|θ1, g1)dP (θ1, g1|θ).  (34)  11  Published as a conference paper at ICLR 2016  K1  1, . . . , θ1 K1  If θ1 and g1 take a ﬁnite number K1 and L1 of values {θ1 }, then the above reduces to a sum over k = 1, . . . , K1 and (cid:96)1 = 1, . . . L1; the conditional likelihoods j ), . . . , p(y|θ1 j )} are the feature maps. If y has dimensions N × M and the group {p(y|θ1 1, g1 , g1 j are taken to be pixel wise translations across the image plane, so that L1 = N × M, actions g1 the feature maps p(y|θ1, g1) can be represented as a tensor with dimensions N × M × K1. One can repeat the procedure for new separator variables that take K2 possible values, and group actions g2 that take L2 = N1 × M1 values; the ﬁlters θ2 must be supported on the entire feature maps p(y|θ1, g1) (i.e., take values in N1 × M1 × K1) for the sum over k = 1, . . . , K1 to implement the marginalization above  } (ﬁlters) and {g1  1, . . . , g1 L1  p(y|θ) =  p(y|θ1  k, g1 (cid:96)1  )p(θ1  k, g1 (cid:96)1  |θ2 j , g2 (cid:96)2  )  p(θ2  j , g2 (cid:96)2  |θ).  (35)  (cid:96)2=1  j=1  (cid:96)1=1  k=1  The sum is implemented in convolutional networks by the use of translation invariant ﬁlters:  1. At the ﬁrst layer the support of θ1 is a small fraction of N × M and g1 acts on y so that21  L2(cid:88)  K2(cid:88)  (cid:34) L1(cid:88)  K1(cid:88)  (cid:35)  k, g1 (cid:96)1  y|θ1  p(y|θ1  ) = p(g1 (cid:96)1  k) = p(y|Vj|θ1 k). 2. At the second layer the ﬁlter p(θ1 k, g1 (cid:96)1 |θ2 j )  ber of group actions g1 (cid:96)1 k, g2 ) = p(θ1 p(θ1 (cid:96)2  |θ2 j , g2 (cid:96)2  k, g1 (cid:96)1  g1 (cid:96)1  ) is nonzero for a ﬁnite (and small) num- and also satisﬁes the shift invariant (convolutional) property  |θ2 j , g2 (cid:96)2  The third dimension of the ﬁlters is the number of feature maps in the previous layer.  4.3 APPROXIMATION OF THE FIRST LAYER  Each node in the ﬁrst layer computes a local representation (5) using parent node(s) as a “scene.” This relates to existing convolutional architectures where nodes compute the response of a rectiﬁed linear unit (ReLu) to a ﬁlter bank. For simplicity we restrict G to the translation group, thus reducing (5) to SIFT, but the arguments apply to similarities. A ReLu response at (u, v) to an oriented ﬁlter bank G with scale σ and orientation α is given by R+(α, u, v, σ) = max(0,G(u, v; σ, α) ∗ x(u, v)). Let N (u, v; σ) be a Gaussian, centered in (u, v) with isotropic variance σI, ∇N (u, v; σ) = [ ∂N ∂v (u, v; σ)], r(α) = [cos α sin α]T . Then G(u, v; σ, α) = ∇N (u, v; σ)r(α) is a directional ﬁlter with principal orientation α and scale . σ. Omitting rectiﬁcation for now, the response of an image to a ﬁlter bank obtained by varying α ∈ [−π, π], at each location (u, v) and for all scales σ is obtained as  ∂u (u, v; σ) ∂N  R(α, u, v, σ) = G(u, v; σ, α) ∗ x(u, v) = N (u, v; σ) ∗ ∇x(u, v)r(α)  = N (u, v; σ) ∗(cid:10) ∇x(u, v) (cid:90)  (cid:107)∇x(u, v)(cid:107) , r(α)(cid:11)(cid:107)∇x(u, v)(cid:107)  N (u − ˜u, v − ˜v; σ)κ(∠∇x(˜u, ˜v), α)(cid:107)∇x(˜u, ˜v)(cid:107)d˜ud˜v  =  (36)  (37)  (38)  where κ, the cosine function, has to be rectiﬁed for the above to approximate a histogram, κ+(α) = max(0, cos α) which yields SIFT. Unfortunately, in general the latter does not equal max(0,G ∗ x) for one cannot simply move the maximum inside the integral. However, under conditions on x, which are typically satisﬁed by natural images, this is the case. Claim 4. Let G be positive, smooth and have a small effective support σ < ∞. I.e., ∀ (cid:15)1, (cid:15)2 ∃ σ | vol(G(˜u, ˜v; σ, α) ≥ (cid:15)1) < (cid:15)2. Let x have a sparse and continuous gradient ﬁeld, so that for every α the domain of x can be partitioned in three (multiply-connected) regions D+(α), D−(α) and the remainder (the complement of their union), where the projection of the gradient in the di- rection α is, respectively, positive, negative, and negligible, and d(α) > 0 the minimum distance  21There is a non-trivial approximation here, namely that context is neglected when assuming that the likeli- k) depends only on the restriction of y to the receptive ﬁeld Vj; see also Section 3.1 and equation  (cid:96)1 y|θ1  hood p(g1 (7).  12  Published as a conference paper at ICLR 2016  between the regions D+ and D−. Then, provided that σ ≤ minα d(α), we have that  max(0,G(u, v; σ, α) ∗ x(u, v))  (cid:39)  N (u − ˜u, v − ˜v; σ)κ+(∠∇x(˜u, ˜v), α)(cid:107)∇x(˜u, ˜v)(cid:107)d˜ud˜v  (cid:124)  (cid:123)(cid:122)  ReLu  (cid:123)(cid:122)  sift  (cid:90) (cid:124)  (cid:125)  (cid:125)  (39)  4.4 STACKING INFORMATION A local hierarchical architecture allows approximating the SA likelihood pθ,G(·) by reducing nui- sance management to local marginalization and max-out of simple group transformations. The SA likelihood pθ,G(y) is an optimal representation for any query on θ given data y. For instance, for classiﬁcation, the representation pθ,G(y) is itself the classiﬁer (discriminant). Thus, if we could compute an optimal classiﬁer, the representation would be the identity; vice-versa, if we could com- pute the optimal representation, the classiﬁer would be a threshold. In practice, one restricts the family of classiﬁers – for instance to soft-max, or SVM, or linear – leaving the job of feeding the most informative statistic to the classiﬁer. In a hierarchical architecture, this is the feature maps in the last layer. This is equivalent to neglecting the global dependency pθ,G(y|θL) on θ at the last layer. The information loss inherent in this choice is the loss of assuming that θL are independent (whereas they are only independent conditioned on θ). An optimal representation with restricted complexity L < ∞, therefore, maximizes the indepen- dence of the components of θL, or equivalently the independence of the components of y given θL. Using those results, one can show that the information content of a representation ((47) in App. A) grows with the number of layers L.  5 DISCUSSION  For the likelihood interpretation of a CNN put forward here to make sense, training should be per- formed generatively, so ﬁxing the class θk one could sample (hallucinate) future images y from the class. However neither the architecture not the training of current CNN incorporate mechanisms to enforce the statistics of natural images. In this paper we emphasize the role of the task in the representation: If nothing is known about the task, nothing interesting can be said about the representation, and the only optimal one is the trivial one that stores all the data. This is because the task could end up being a query about the value of a particular pixel in a particular image. Nevertheless, there may be many different tasks that share the same representation by virtue of the fact that they share the same nuisances. In fact, the task affects what are nuisance factors, and the nuisance factors affect the design and learning of the representation. For some complex tasks, writing the likelihood function may be prohibitively complex, but some classes of nuisances such as changes of illumination or occlusions, are common to many tasks. Note that, by deﬁnition, a nuisance is not informative. Certain transformations that are nuisances for some tasks may be informative for others (and therefore would not be called nuisances). For instance, viewpoint is a nuisance in object detection tasks, as we want to detect objects regardless of where they are. It is, of course, not a nuisance for navigation, as we must control our pose relative to the surrounding environment. In some cases, nuisance and intrinsic variability can be entangled, as for the case of intra-class deformations and viewpoint-induced deformations in object categorization. Nevertheless, the deformation would be informative if it was known or measured, but since it is not, it must be marginalized. Our framework does not require nuisances to have the structure of a group. For instance, occlusions do not. Invariance and sensitivity are still deﬁned, as a statistic is invariant if it is constant with respect to variations of the nuisance. What is not deﬁned is the notion of maximal invariance, that requires the orbit structure. However, in our theory maximal invariance is not the focus. Instead, sufﬁcient invariance is. The literature on the topic of representation is vast and growing. We focus on visual representations, where several have been active. Anselmi et al. (2015) have developed a theory of representation aim- ing at approximating maximal invariants, which restricts nuisances to (locally) compact groups and  13  Published as a conference paper at ICLR 2016  therefore do not explicitly handle occlusions. Both frameworks achieve invariance at the expense of discriminative power, whereas in our framework both can be attained at the cost of complexity. Patel et al. (2015), that appeared after earlier drafts of this manuscript were made public, instead of of starting from principles and showing that they lead to a particular kind of computational architec- ture, instead assume a particular architecture and interpret it probabilistically, similarly to Ver Steeg & Galstyan (2015) that uses total correlation as a proxy of information, which is related to our App. A. However, there the representation is deﬁned absent a task, so the analysis does not account for the role of nuisance factors. In particular, Anselmi et al. (2015) deﬁne a G-invariant representation µ of a (single) image I as being selective if µ(I) = µ(I(cid:48)) ⇒ I ∼ I(cid:48) for all I, I(cid:48), i.e., if it is a maximal invariant. But while equivalence to the data up to the action of the nuisance group is critical for reconstruction, it is not necessary for other tasks. Furthermore, for non-group nuisances, such as occlusions, a maximal invariant cannot be constructed. Instead, given a task, we replace maximality with sufﬁciency for the task, and deﬁne at the outset an optimal representation to be a minimal sufﬁcient invariant statistic, or “sufﬁcient invariant,” approximated by the SAL Likelihood. The construction in Anselmi et al. (2015) guarantees maximality for compact groups. Similarly, Sundaramoorthi et al. (2009) have shown that maximal invariants can be constructed even for diffeomorphisms, which are inﬁnite- dimensional and non-compact. In practice, however, occlusions and scaling/quantization break the group structure, and therefore a different approach is needed that relies on sufﬁciency, not maximal- ity, as we proposed here. To relate our approach to Anselmi et al. (2015), we note that the orbit probability of Def. 4.2 is given by  (40) and is used to induce a probability on I, via P (I)[A] = P ({g | gI ∈ A}). On the other hand, we deﬁne the minimal sufﬁcient invariant statistic as the marginalized likelihood  ρI (A) = P ({g | gI ∈ A})  pθ,G(y)  . =  pθ,g(y)dP (g)  (41)  where y is a (future) image, and θ is the scene. If we consider the scene to be comprised of a set of images A = θ, and the future image y = I, then we see that the OP is a marginalized likelihood where dP (g) = dµ(g) is the Haar measure, and pθ,g(y) = δ(gy ∩ θ). Thus, substitutions G ← G, θ ← A, y ← I yield  (42) for the particular choice of Haar measure and impulsive density pθ,g. The TP representation can also be understood as a marginalized likelihood, as Ψ(I)[A] is the G-marginalized likelihood of I given A when using the uniform prior and an impulsive conditional pA,g(I):  P (I)[A] = pθ,G(y)  Ψ(I)[A] =  p(gI|A)dµ(g).  (43)  Finally, our treatment of representations is not biologically motivated, in the sense that we sim- ply deﬁne optimal representations from ﬁrst principles, without regards for whether they are im- plementable with biological hardware. However, we have established connections with both local descriptors and deep neural networks, that were derived using biological inspiration.  ACKNOWLEDGMENTS  Work supported by FA9550-15-1-0229, ARO W911NF-15-1-0564, ONR N00014-15-1-2261, and MIUR RBFR12M3AC “Learning meets time.” We are appreciative of discussions with Tomaso Poggio, Lorenzo Rosasco, Stephane Mallat, and Andrea Censi.  REFERENCES Alvarez, L., Guichard, F., Lions, P. L., and Morel, J. M. Axioms and fundamental equations of  image processing. Arch. Rational Mechanics, 123, 1993. 7  Anselmi, F., Rosasco, L., and Poggio, T. On invariance and selectivity in representation learning.  arXiv preprint arXiv:1503.05938, 2015. 2, 13, 14  14  (cid:90)  (cid:90)  G  Published as a conference paper at ICLR 2016  Bahadur, R. R. Sufﬁciency and statistical decision functions. Annals of Mathematical Statistics, 25  (3):423–462, 1954. 3  Bengio, Y. Learning deep architectures for ai. Foundations and trends R(cid:13) in Machine Learning, 2  (1):1–127, 2009. 2  Blackwell, D. and Ramamoorthi, R.V. A bayes but not classically sufﬁcient statistic. The Annals of  Statistics, 10(3):1025–1026, 1982. 3  Boureau, Y.-L., Ponce, J., and LeCun, Y. A theoretical analysis of feature pooling in visual recogni-  tion. In Proc. of Intl Conf. on Mach. Learning, pp. 111–118, 2010. 2  Bouvrie, J. V., Rosasco, L., and Poggio, T. On invariance in hierarchical models.  162–170, 2009. 2  In NIPS, pp.  Bruna, J. and Mallat, S. Classiﬁcation with scattering operators. In Proc. IEEE Conf. on Comp.  Vision and Pattern Recogn., 2011. 2  Chatﬁeld, K., Lempitsky, V., Vedaldi, A., and Zisserman, A. The devil is in the details: an evaluation  of recent feature encoding methods. 2011. 2  Chen, C. and Edelsbrunner, H. Diffusion runs low on persistence fast. In ICCV, pp. 423–430, 2011.  4  Cover, T. M. and Thomas, J. A. Elements of Information Theory. Wiley, 1991. 3  Creutzig, F., Globerson, A., and Tishby, N. Past-future information bottleneck in dynamical systems.  Phys. Rev. Lett. E, 79(4):19251–19255, 2009. 17  DeGroot, M. H. Probability and statistics. Addison-Wesley, 1989. 3  Dong, J. and Soatto, S. Machine Learning for Computer Vision, chapter Visual Correspondence, the Lambert-Ambient Shape Space and the Systematic Design of Feature Descriptors. R. Cipolla, S. Battiato, G.-M. Farinella (Eds), Springer Verlag, 2014. 2  Dong, J. and Soatto, S. Domain size pooling in local descriptors: Dsp-sift. In Proc. IEEE Conf. on  Comp. Vision and Pattern Recogn., 2015. 5  Fraser, D. and Naderi, A. Minimal sufﬁcient statistics emerge from the observed likelihood function.  International Journal of Statistical Science, 6:55–61, 2007. 17  Gong, Y., Wang, L., Guo, R., and Lazebnik, S. Multi-scale orderless pooling of deep convolutional  activation features. arXiv preprint arXiv:1403.1840, 2014. 2  Grenander, U. General Pattern Theory. Oxford University Press, 1993. 6  Hinkley, D. Predictive likelihood. The Annals of Statistics, pp. 718–728, 1979. 19  Huang, J. and Mumford, D. Statistics of natural images and models. In Proc. CVPR, pp. 541–547,  1999. 5  Kirchner, M. R. Automatic thresholding of SIFT descriptors. Technical Report, 2015. 2  LeCun, Y. Learning invariant feature hierarchies. In ECCV, pp. 496–505, 2012. 2  Lowe, D. G. Distinctive image features from scale-invariant keypoints. IJCV, 2(60):91–110, 2004.  5, 10  Mikolajczyk, K., Tuytelaars, T., Schmid, C., Zisserman, A., Matas, J., Schaffalitzky, F., Kadir, T.,  and Gool, L. Van. A comparison of afﬁne region detectors. IJCV, 1(60):63–86, 2004. 2  Morel, J. M. and Yu, G. Is sift scale invariant? Inverse Problems and Imaging, 5(1):115–136, 2011.  2  Patel, A., Nguyen, T., and Baraniuk, R. A probabilistic theory of deep learning. arXiv preprint  arXiv:1504.00641, 2015. 14  15  Published as a conference paper at ICLR 2016  Pawitan, Y. In all likelihood: Statistical modeling and inference using likelihood. Oxford, 2001. 3,  19  Ranzato, M., Huang, F. J., Boureau, Y.-L., and LeCun, Y. Unsupervised learning of invariant feature  hierarchies with applications to object recognition. In CVPR, pp. 1–8, 2007. 2  Serre, T., Oliva, A., and Poggio, T. A feedforward architecture accounts for rapid categorization.  Proceedings of the National Academy of Sciences, 104(15):6424–6429, 2007. 2  Shao, J. Mathematical Statistics. Springer Verlag, 1998. 2, 4  Simonyan, K., Vedaldi, A., and Zisserman, A. Learning local feature descriptors using convex  optimisation. IEEE Trans. Pattern Anal. Mach. Intell., 2(4), 2014. 2  Soatto, S. Actionable information in vision. In Proc. of the Intl. Conf. on Comp. Vision, October  2009. 4, 10, 17, 18  Sundaramoorthi, G., Petersen, P., Varadarajan, V. S., and Soatto, S. On the set of images modulo viewpoint and contrast changes. In Proc. IEEE Conf. on Comp. Vision and Pattern Recogn., June 2009. 14  Susskind, J., Memisevic, R., Hinton, G. E., and Pollefeys, M. Modeling the joint density of two In Proc. IEEE Conf. on Comp. Vision and Pattern  images under a variety of transformations. Recogn., pp. 2793–2800, 2011. 2  Tishby, N., Pereira, F. C., and Bialek, W. The information bottleneck method.  Allerton Conf., 2000. 2  In Proc. of the  Ver Steeg, G. and Galstyan, A. Maximally informative hierarchical representations of high-  dimensional data. in depth, 13:14, 2015. 14  16  Published as a conference paper at ICLR 2016  A QUANTIFYING THE INFORMATION CONTENT OF A REPRESENTATION In general we do not know the likelihood, but we have a collection of samples xt ∼ pθ,gt(x), each generated with some nuisance gt, which we can use to infer, or “learn” an approximation of the SAL likelihood Fraser & Naderi (2007).  = pθ(·) (cid:39) ˆp φθ(·) . φθ,G(·) = pθ,G(·) (cid:39) ˆp . = φθ,G(y)φθ(xt) (cid:39) ˆp .  (·) (y)ˆp  Xt,G  Xt,G  Xt (·), X t ∼ pθ(·)  Xt (xt) ∝ ˆp  xt,G  (empirical likelihood)  (44) (45) (proﬁle likelihood) (y) (learned representation)(46)  φθ,G(y, xt)  Depending on modeling choices made, including the number of samples N, the sampling mecha- nism, and the priors for local marginalization, the resulting representation will be “lossy” compared to the data. Next we quantify such a loss.  A.1  INFORMATIVE CONTENT OF A REPRESENTATION  . = H(φθ,G(xt)) where φθ,G is a  The scene θ is in general inﬁnite-dimensional. Thus, the informative content of the data cannot be directly quantiﬁed by mutual information I(θ; xt). In fact, H(θ) = ∞ and therefore, no matter how many (ﬁnite) data xt we have, I(θ; xt) = H(θ) − H(θ|xt) = ∞ − ∞ is not deﬁned. Similarly, I(θ; φ(xt)) is undeﬁned and therefore mutual information cannot be used directly to measure the informative content of a representation, or to infer the most informative statistic. The notion of Actionable Information Soatto (2009) as H(xt) maximal G-invariant, can be used to bypass the computation of H(θ): Writing formally I(θ; φθ,G(y)) = H(φθ,G(y)) − H(φθ,G(y)|θ) = H(xt) − H(φθ,G(y)|θ)  (47) we see that, if we were given the scene θ, we could generate the data y, under the action of some nuisance g, up to the residual modeling uncertainty, which is assumed white and zero-mean (lest the mean and correlations of the residual can be included in the model). Similarly, we can generate a maximal invariant φθ,G(y) up to a residual with constant entropy; therefore, the statistic which maximizes I(θ; φθ,G(y)) is the one that maximizes the ﬁrst term in (47), H(φθ,G(y)). This formal argument allows deﬁning the most informative statistic as the one that maximizes Actionable In- = H(xt), bypassing the computation of the entropy . formation, ˆφθ,G = arg maxφθ,G of the “scene” θ. Note that the task still inﬂuences the information content of a representation, by deﬁning what are the nuisances G, which in turn affect the computation of actionable information. An alternative approach is to measure the informative content of a representation bypassing consid- eration of the scene is described next. Deﬁnition 1 (Informative Content of a Representation). The information a statistic φ of xt conveys on θ is the information it conveys on a task T (e.g., a question on the scene θ), regardless of nuisances g ∈ G:  H(φθ,G(xt))  I(gT ; φ(xt)) = H(gT ) − H(gT|φ(xt)) ∀ g ∈ G  (48)  If the task is reconstruction (or prediction) T = y, where past data xt and future data y are gen- erated by the same scene θ, then the deﬁnition above relates to the past-future mutual information Creutzig et al. (2009) except for the role of the nuisance g. The following claim shows that an ideal representation, as previously deﬁned in terms of minimal sufﬁcient invariant statistic, maximizes information. Claim 5. Let past data xt and future data y, used to accomplish a task T , be generated by the same scene θ. Then the representation φθ,G maximizes the informative content of a representation. The next claim relates a representation to Actionable Information. Claim 6. If φ maximizes Actionable Information, it also maximizes the informative content of a representation.  Note that maximizing Actionable Information is a stronger condition than maximizing the infor- mation content of a representation. Since Actionable Information concerns maximal invariance,  17  Published as a conference paper at ICLR 2016  sufﬁciency is automatically implied, and the only role the task plays is the deﬁnition of the nuisance group G. This is limiting, since we want to handle nuisances that do not have the structure of a group, and therefore Deﬁnition 1 affords more ﬂexibility than Soatto (2009). The next two claims characterize the maximal properties of the proﬁle likelihood. We ﬁrst recall that the marginalized likelihood is invariant only if marginalization is done with respect to the base (Haar) measure, and in general it is not a maximal invariant, as one can show easily with a counter-example (e.g., a uniform density). On the other hand, the proﬁle likelihood is by construction invariant regardless of the distribution on G, but is also – in general – not maximal. However, it is maximal under general conditions on the likelihood. To see this, consider pθ(·) to be given; for a free variable . x, it can be written as a map q: pθ(x) = q(θ, x). For a ﬁxed x, q is a function of θ. If q is constant along x (the level curves are straight lines), then in general q(θ, y) = q(θ, x) for all θ does not imply y = x. Indeed, y can be arbitrarily different from x. Even if q is non-degenerate (non-constant along certain directions), but presents symmetries, it is possible for different y (cid:54)= x to yield the same q(θ, y) = q(θ, x) for all θ. However, under generic conditions q(θ, y) = q(θ, x) for all θ implies y = x. Now consider pθ,G(·) to be given; for a free variable x the map can be written using a function q such that pθ,G(x) = ming q(θ, gx). Note that pθ,G is, by construction, invariant to G. Also note that, following the same argument as above, the invariant is not maximal, for it is possible for pθ,G(x) = pθ,G(y) for all θ and yet x and y are not equivalent (equal up to a constant g: y = gx). However, if the function q is generic, then the invariant is maximal. In fact, let ˆg(θ, x) = arg min q(θ, gx), so that pθ,G(x) = q(θ, ˆg(θ, x)x). If we now have q(θ, ˆg(θ, y)y) = q(θ, ˆg(θ, x)x) for all θ, then we can conclude, based on the argument above, that ˆg(θ, x)x = ˆg(θ, y)y. Since x and y are ﬁxed, and the equality is for all θ, we can conclude that ˆg(θ, y)−1ˆg(θ, x) is independent of θ. That allows us to conclude the following. Claim 7 (Maximality of the proﬁle likelihood). If the density pθ(x) is generic with respect to x, then pθ,G(·) is a maximal G-invariant. Since we do not have control on the function pθ,G(·), which is instead in general constructed from data, it is legitimate to ask what happens when the generic condition above is not satisﬁed. For- tunately, distributions that yield non-maximal invariants can be ruled out as uninformative at the outset: Claim 8 (Non-maximality and non-informativeness). If q is such that, for any x (cid:54)= y we have q(θ, ˆg(θ, x)x) = q(θ, ˆg(θ, y)y) for all θ, then qθ,G(·) is uninformative. This follows from the deﬁnition of information, for any statistic T . As we have pointed out before, what matters is not that the invariant be maximal, but that it be sufﬁcient. As anticipated in Rem. 1, we can achieve invariance with no sacriﬁce of discriminative power, albeit at the cost of complexity.  B PROOFS  Theorem 1  Proof. Pick any θ0, deﬁne T (x) lemma with h(x) = L(θ0; x).  Theorem 2  = L(·;x) .  L(θ0;x) and f (T (x), θ)  . = L(θ;x)  L(θ0;x) and apply the factorization  ∇y(cid:107)∇y(cid:107) the normalized gradient of y, and similarly for x; Φ maps it to  Proof. We denote with ∇y polar coordinates (α, ρ) = Φ(∇y) and (β, γ) = Φ(∇x), where = ∠∇x γ .  = (cid:107)∇y(cid:107) β .  = ∠∇y .  . =  α  ρ  The conditional density of ∇y given ∇x takes the polar form  = (cid:107)∇x(cid:107). .  p(ρ, α|∇x) = p(∇y|∇x)∇y=Φ−1(ρ,α)ρ − 1 2(cid:15)2 (cid:107)∇y−∇x(cid:107)2  p(∇y|∇x) = 1  2π(cid:15)2 e  .  (49)  18  Published as a conference paper at ICLR 2016  Deﬁning (∇x)i to be the i-th component of ∇x, (49) can be expanded as  p(ρ, α|∇x) = ρ  1 2π(cid:15)2 e  − 1 2(cid:15)2 [(ρ cos(α)−(∇x)1)2+(ρ sin(α)−(∇x)2)2]  (50)  and the exponent is (ρ cos(α) − (∇x)1)2 + (ρ sin(α) − (∇x)2)2 = ρ2 − 2ρ((∇x)1 cos α + (∇x)2 sin α) + (cid:107)∇x(cid:107)2  = (cid:0)ρ − γ(cid:104)∇y,∇x(cid:105)(cid:1)2 (cid:90) ∞  + γ2(cid:0)1 − (cid:104)∇y,∇x(cid:105)2(cid:1)  (51)  We are now interested in the marginal of (50) with respect to ρ, i.e., p(ρ, α|∇x) dρ.  p(α|∇x) =  where we can isolate the factor that does not depend on ρ,  p(α|∇x) =  1√ 2π(cid:15)2  e  − 1 2(cid:15)2 γ2(1−(cid:104)∇y,∇x(cid:105)2)  0  (cid:90) ∞ (cid:124)  0  1√ 2π(cid:15)2  e  − 1 2(cid:15)2 (ρ−γ(cid:104)∇y,∇x(cid:105))2  (cid:123)(cid:122)  M  (52)  (53)  .  ρdρ  (cid:125)  The bracketed term M is the integral on the interval [0,∞) of a Gaussian density with mean m . = γ(cid:104)∇y,∇x(cid:105) = cos(∠∇y − ∠∇x)(cid:107)∇x(cid:107) and variance (cid:15)2; it can be rewritten, using the change of variable ξ ((cid:15)ξ + m) dξ which can be integrated by parts to yield  = (ρ − m)/(cid:15), as(cid:82) ∞  2 ξ2 e 1  .  −m/(cid:15)  1√ 2π  M =  2  m2  − 1 (cid:15)2√ (cid:15)e 2π  + m  (cid:16) (cid:18)  1 − Ψ  (cid:16)− m −(cid:107)∇x(cid:107)2 − m2  (cid:17)(cid:17) (cid:19)  (cid:15)  2(cid:15)2  and therefore  p(α|∇x) =  1√ 2π(cid:15)2  exp  M  (54)  which, once written explicitly in terms of x and y, yields (14)-(15).  Claim 5  Proof. Since pθ,G(y, xt) is sufﬁcient for θ, and it factorizes into φθ,G(y)φθ(xt), then φθ(xt) is sufﬁcient of xt for θ. By the factorization theorem (Theorem 3.1 of Pawitan (2001)), there exist functions fθ and ψ such that φθ(xt) ∝ fθ(ψ(xt)), i.e., the likelihood depends on the data only through the function ψ(·). This latter function is what is more commonly known as the sufﬁcient statistic, which in particular has the property that p(θ|xt) = p(θ|ψ(xt)). However, if φθ is sufﬁcient for θ, it is also sufﬁcient for future data generated from θ. Formally,  pG(y|xt) =  pG(y|xt, θ)p(θ|xt)dP (θ) =  (55) which shows that ψ minimizes the uncertainty of y for any g ∈ G since the right-hand-side is G-invariant. The right-hand side above is the predictive likelihood Hinkley (1979), which must therefore be proportional to ˜fy(ψ(xt)) for some ˜f and the same ψ, also by the factorization theorem.  pG(y|θ)p(θ|ψ(xt))dP (θ) = pG(y|ψ(xt))  (cid:90)  (cid:90)  Claim 4  Proof. (Sketch) The integral in (38) can be split into 3 components, one of which omitted, leaving the positive component integrated on D+, the negative component on D−. If the distance between these two is greater than σ, however, the components are disjoint, so for each (u, v) and α, only the the positive or the negative component are non-zero, and since N and (cid:107)∇x(cid:107) are both positive, and the sign is constant, rectiﬁcation inside or outside the integral is equivalent. When σ > d there is an error in the approximation, that can be bounded as a function of σ, the minimum distance and the maximum gradient component.  19  Published as a conference paper at ICLR 2016  Figure 3: D+(0) (green) and D−(0), the positive and negative responses to a gradient ﬁlter in the horizontal direction. The black region is their complement, which separates them.  Figure 4: Rectiﬁed cosine (blue) and its powers, compared to a Gaussian kernel (red). While the two are distinctly different for (cid:15) = 1, as the power/dispersion decreases, the latter approximates the former. The plot shows (cid:15) = 1, 1/5, 1/9 for the cosine, and 1/5, 1/9, 1/13 for the Gaussian.  A more general kernel could be considered, with a parameter (cid:15) that controls the decay, or width, of the kernel, κ(cid:15)(α). For instance, κ(cid:15)(α) = κ(α) 1 (cid:15) , with the default value being (cid:15) = 1. An alternative is to deﬁne κ to be an angular Gaussian with dispersion parameter (cid:15), which is constrained to be positive and therefore does not need rectiﬁcation. Although the angular Gaussian is quite different from the cosine kernel for (cid:15) = 1, it approximates it as (cid:15) decreases (Fig. 4). A corollary of the above is that the visible layer of a CNN computes the SAL Likelihood of the ﬁrst hidden layer. The interpretation of SIFT as a likelihood function given the test image y can be confusing, as ordinarily it is interpreted as a “feature vector” associated to the training image x, and compared with other feature vectors using the Euclidean distance. In a likelihood interpretation, x is used to compute the likelihood function, and y is used to evaluate it. So, there is no descriptor built for y. The same interpretational difference applies to convolutional architectures. If interpreted as a likelihood, which would require generative learning, one would compute the likelihood of different hypotheses given the test data. Instead, currently the test data is fed to the network just as training data were, thus generating features maps, that are then compared (discriminatively) by a classiﬁer.  20  −4−3−2−10123400.0020.0040.0060.0080.010.0120.014",
1412.6651,2015, Deep learning with Elastic Averaging SGD,"['Deep learning with Elastic Averaging SGD', 'Sixin Zhang', 'Anna Choromanska', 'and Yann LeCun']",https://arxiv.org/pdf/1412.6651,"5 1 0 2    t c O 5 2         ]  G L . s c [      8 v 1 5 6 6  .  2 1 4 1 : v i X r a  Deep learning with Elastic Averaging SGD  Sixin Zhang  Courant Institute, NYU zsx@cims.nyu.edu  Anna Choromanska Courant Institute, NYU  achoroma@cims.nyu.edu  Yann LeCun  Center for Data Science, NYU & Facebook AI Research  yann@cims.nyu.edu  Abstract  We study the problem of stochastic optimization for deep learning in the paral- lel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more explo- ration, i.e. the algorithm allows the local variables to ﬂuctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous vari- ant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisﬁed, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classiﬁcation on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efﬁcient.  1  Introduction  One of the most challenging problems in large-scale machine learning is how to parallelize the training of large models that use a form of stochastic gradient descent (SGD) [1]. There have been attempts to parallelize SGD-based training for large-scale deep learning models on large number of CPUs, including the Google’s Distbelief system [2]. But practical image recognition systems consist of large-scale convolutional neural networks trained on few GPU cards sitting in a single computer [3, 4]. The main challenge is to devise parallel SGD algorithms to train large-scale deep learning models that yield a signiﬁcant speedup when run on multiple GPU cards. In this paper we introduce the Elastic Averaging SGD method (EASGD) and its variants. EASGD is motivated by quadratic penalty method [5], but is re-interpreted as a parallelized extension of the averaging SGD algorithm [6]. The basic idea is to let each worker maintain its own local parameter, and the communication and coordination of work among the local workers is based on an elastic force which links the parameters they compute with a center variable stored by the master. The center variable is updated as a moving average where the average is taken in time and also in space over the parameters computed by local workers. The main contribution of this paper is a new algorithm that provides fast convergent minimization while outperforming DOWNPOUR method [2] and other  1  baseline approaches in practice. Simultaneously it reduces the communication overhead between the master and the local workers while at the same time it maintains high-quality performance measured by the test error. The new algorithm applies to deep learning settings such as parallelized training of convolutional neural networks. The article is organized as follows. Section 2 explains the problem setting, Section 3 presents the synchronous EASGD algorithm and its asynchronous and momentum-based variants, Section 4 provides stability analysis of EASGD and ADMM in the round-robin scheme, Section 5 shows ex- perimental results and Section 6 concludes. The Supplement contains additional material including additional theoretical analysis.  2 Problem setting Consider minimizing a function F (x) in a parallel computing environment [7] with p ∈ N workers and a master. In this paper we focus on the stochastic optimization problem of the following form  F (x) := E[f (x, ξ)],  (1)  where x is the model parameter to be estimated and ξ is a random variable that follows the probabil- Ω f (x, ξ)P(dξ). The optimization problem in Equation 1  min  ity distribution P over Ω such that F (x) =(cid:82) p(cid:88)  can be reformulated as follows  x  min  x1,...,xp,˜x  i=1  E[f (xi, ξi)] +  (cid:107)xi − ˜x(cid:107)2,  ρ 2  (2)  where each ξi follows the same distribution P (thus we assume each worker can sample the entire dataset). In the paper we refer to xi’s as local variables and we refer to ˜x as a center variable. The problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem [8, 9]. The quadratic penalty term ρ in Equation 2 is expected to ensure that local workers will not fall into different attractors that are far away from the center variable. This paper focuses on the problem of reducing the parameter com- munication overhead between the master and local workers [10, 2, 11, 12, 13]. The problem of data communication when the data is distributed among the workers [7, 14] is a more general problem and is not addressed in this work. We however emphasize that our problem setting is still highly non-trivial under the communication constraints due to the existence of many local optima [15].  3 EASGD update rule  The EASGD updates captured in resp. Equation 3 and 4 are obtained by taking the gradient descent step on the objective in Equation 2 with respect to resp. variable xi and ˜x,  xi t+1 = xi  ˜xt+1 = ˜xt + η  t(xi  p(cid:88) t − η(gi  t) + ρ(xi t − ˜xt),  ρ(xi  t − ˜xt))  i=1  (3)  (4)  t(xi  t) denotes the stochastic gradient of F with respect to xi evaluated at iteration t, xi  where gi ˜xt denote respectively the value of variables xi and ˜x at iteration t, and η is the learning rate. The update rule for the center variable ˜x takes the form of moving average where the average is taken over both space and time. Denote α = ηρ and β = pα, then Equation 3 and 4 become  t and  t − ηgi  xi t+1 = xi ˜xt+1 = (1 − β)˜xt + β  t(xi  (cid:33) (cid:32) p(cid:88) t − ˜xt) t) − α(xi 1 p  xi t  i=1  .  (5)  (6)  Note that choosing β = pα leads to an elastic symmetry in the update rule, i.e. there exists an t − ˜xt) between the update of each xi and ˜x. It has a crucial inﬂu- symmetric force equal to α(xi ence on the algorithm’s stability as will be explained in Section 4. Also in order to minimize the t − ˜xt between the center and the local variable, the update for the staleness [16] of the difference xi master in Equation 4 involves xi t instead of xi  t+1.  2  Note also that α = ηρ, where the magnitude of ρ represents the amount of exploration we allow in the model. In particular, small ρ allows for more exploration as it allows xi’s to ﬂuctuate further from the center ˜x. The distinctive idea of EASGD is to allow the local workers to perform more exploration (small ρ) and the master to perform exploitation. This approach differs from other settings explored in the literature [2, 17, 18, 19, 20, 21, 22, 23], and focus on how fast the center variable converges. In this paper we show the merits of our approach in the deep learning setting.  3.1 Asynchronous EASGD  We discussed the synchronous update of EASGD algorithm in the previous section. In this section we propose its asynchronous variant. The local workers are still responsible for updating the local variables xi’s, whereas the master is updating the center variable ˜x. Each worker maintains its own clock ti, which starts from 0 and is incremented by 1 after each stochastic gradient update of xi as shown in Algorithm 1. The master performs an update whenever the local workers ﬁnished τ steps of their gradient updates, where we refer to τ as the communication period. As can be seen in Algorithm 1, whenever τ divides the local clock of the ith worker, the ith worker communicates with the master and requests the current value of the center variable ˜x. The worker then waits until the master sends back the requested parameter value, and computes the elastic difference α(x − ˜x) (this entire procedure is captured in step a) in Algorithm 1). The elastic difference is then sent back to the master (step b) in Algorithm 1) who then updates ˜x. The communication period τ controls the frequency of the communication between every local worker and the master, and thus the trade-off between exploration and exploitation.  Algorithm 1: Asynchronous EASGD: Processing by worker i and the master Input: learning rate η, moving rate α, communication period τ ∈ N  Initialize: ˜x is initialized randomly, xi = ˜x,  ti = 0  Repeat  x ← xi if (τ divides ti) then  a) xi ← xi − α(x − ˜x) b) ˜x ← ˜x + α(x − ˜x)  end xi ← xi − ηgi ti ← ti + 1 Until forever  ti(x)  3.2 Momentum EASGD  Algorithm 2: Asynchronous EAMSGD: Processing by worker i and the master Input: learning rate η, moving rate α, communication period τ ∈ N, momentum term δ  Initialize: ˜x is initialized randomly, xi = ˜x,  vi = 0, ti = 0  Repeat  x ← xi if (τ divides ti) then  a) xi ← xi − α(x − ˜x) b) ˜x ← ˜x + α(x − ˜x)  end vi ← δvi − ηgi xi ← xi + vi ti ← ti + 1 Until forever  ti(x + δvi)  The momentum EASGD (EAMSGD) is a variant of our Algorithm 1 and is captured in Algorithm 2. It is based on the Nesterov’s momentum scheme [24, 25, 26], where the update of the local worker of the form captured in Equation 3 is replaced by the following update  vi t+1 = δvi xi t+1 = xi  t − ηgi t + vi  t(xi t + δvi t) t − ˜xt), t+1 − ηρ(xi  (7)  where δ is the momentum term. Note that when δ = 0 we recover the original EASGD algorithm. As we are interested in reducing the communication overhead in the parallel computing environ- ment where the parameter vector is very large, we will be exploring in the experimental section the asynchronous EASGD algorithm and its momentum-based variant in the relatively large τ regime (less frequent communication).  4 Stability analysis of EASGD and ADMM in the round-robin scheme  In this section we study the stability of the asynchronous EASGD and ADMM methods in the round- robin scheme [20]. We ﬁrst state the updates of both algorithms in this setting, and then we study  3  their stability. We will show that in the one-dimensional quadratic case, ADMM algorithm can exhibit chaotic behavior, leading to exponential divergence. The analytic condition for the ADMM algorithm to be stable is still unknown, while for the EASGD algorithm it is very simple1. The analysis of the synchronous EASGD algorithm, including its convergence rate, and its averaging property, in the quadratic and strongly convex case, is deferred to the Supplement. In our setting, the ADMM method [9, 27, 28] involves solving the following minimax problem2,  max λ1,...,λp  min  x1,...,xp,˜x  F (xi) − λi(xi − ˜x) +  (cid:107)xi − ˜x(cid:107)2,  ρ 2  (8)  where λi’s are the Lagrangian multipliers. The resulting updates of the ADMM algorithm in the round-robin scheme are given next. Let t ≥ 0 be a global clock. At each t, we linearize the function F (xi) with F (xi  (cid:13)(cid:13)2 as in [28]. The updates become  (cid:13)(cid:13)xi − xi  t  p(cid:88) (cid:11) + 1  i=1  2η  t − ˜xt)  λi t+1 =  t  t) +(cid:10)∇F (xi (cid:26) λi (cid:40) xi p(cid:88)  t), xi − xi t − (xi λi t t−η∇F (xi xi t  xi t+1 =  ˜xt+1 =  1 p  i=1  t+1 − λi (xi  t+1).  if mod (t, p) = i − 1; if mod (t, p) (cid:54)= i − 1.  t)+ηρ(λi 1+ηρ  t+1+˜xt)  if mod (t, p) = i − 1; if mod (t, p) (cid:54)= i − 1.  (9)  (10)  (11)  (cid:26) xi  Each local variable xi is periodically updated (with period p). First, the Lagrangian multiplier λi is updated with the dual ascent update as in Equation 9. It is followed by the gradient descent update of the local variable as given in Equation 10. Then the center variable ˜x is updated with the most recent values of all the local variables and Lagrangian multipliers as in Equation 11. Note that since the step size for the dual ascent update is chosen to be ρ by convention [9, 27, 28], we have re-parametrized the Lagrangian multiplier to be λi The EASGD algorithm in the round-robin scheme is deﬁned similarly and is given below  t/ρ in the above updates.  t ← λi  xi t+1 =  ˜xt+1 = ˜xt +  t − η∇F (xi (cid:88) xi t  t) − α(xi  t − ˜xt)  α(xi  t − ˜xt).  if mod (t, p) = i − 1; if mod (t, p) (cid:54)= i − 1.  (12)  (13)  i: mod (t,p)=i−1  At time t, only the i-th local worker (whose index i−1 equals t modulo p) is activated, and performs the update in Equations 12 which is followed by the master update given in Equation 13. We will now focus on the one-dimensional quadratic case without noise, i.e. F (x) = x2 For the ADMM algorithm, (λ1 composed of three linear maps which can be written as st+1 = (F i we will only write them out below for the case when i = 1 and p = 2:  time t be st = t , ˜xt) ∈ R2p+1. The local worker i’s updates in Equations 9, 10, and 11 are 1)(st). For simplicity,  the state of the (dynamical) system at 2 ◦ F i  2 , x ∈ R.  t , . . . , λp  3 ◦ F i  t , xp  t , x1  let   1 −1  0 0 0 0  1 0 0 0  F 1  1 =  0 0 1 0 0  0 0 0 1 0  1 0 0 0 1  , F 1  2 =    , F 1  3 =    1 ηρ  1+ηρ  0 0 0  0 1−η 1+ηρ  0 0 0  0 0 1 0 0  0 0 0 1 0  0 ηρ  1+ηρ  0 0 1  1 0 0 0 − 1  p  0 1 0 0 1  p − 1  p  0 0 1 0  0 0 0 1 1 p  0 0 0 0 0  .  For each of the p linear maps, it’s possible to ﬁnd a simple condition such that each map, where the ith map has the form F i 1, is stable (the absolute value of the eigenvalues of the map are 1This condition resembles the stability condition for the synchronous EASGD algorithm (Condition 17 for  3 ◦ F i  2 ◦ F i  p = 1) in the analysis in the Supplement.  2The convergence analysis in [27] is based on the assumption that “At any master iteration, updates from the workers have the same probability of arriving at the master.”, which is not satisﬁed in the round-robin scheme.  4  2 ◦ F p  3 ◦ F p  1 ◦ . . .◦ F 1  smaller or equal to one). However, when these non-symmetric maps are composed one after another 1 , the resulting map F can become unstable! (more as follows F = F p precisely, some eigenvalues of the map can sit outside the unit circle in the complex plane). We now present the numerical conditions for which the ADMM algorithm becomes unstable in the round-robin scheme for p = 3 and p = 8, by computing the largest absolute eigenvalue of the map F. Figure 1 summarizes the obtained result.  3 ◦ F 1  2 ◦ F 1  3 ◦ F p  Figure 1: The largest absolute eigenvalue of the linear map F = F p 2 ◦ F 1 as a function of η ∈ (0, 10−2) and ρ ∈ (0, 10) when p = 3 and p = 8. To simulate the chaotic behavior of the ADMM algorithm, one may pick η = 0.001 and ρ = 2.5 and initialize the state s0 either randomly or with λi On the other hand, the EASGD algorithm involves composing only symmetric linear maps due to t , ˜xt) ∈ Rp+1. the elasticity. Let the state of the (dynamical) system at time t be st = (x1 (cid:33) The activated local worker i’s update in Equation 12 and the master update in Equation 13 can be written as st+1 = F i(st). In case of p = 2, the map F 1 and F 2 are deﬁned as follows  0 = ˜x0 = 1000,∀i. Figure should be read in color.  (cid:32) 1 − η − α 0  1 ◦ . . . ◦ F 1  t , . . . , xp  2 ◦ F p  3 ◦ F 1  0 = 0, xi  (cid:33)  0  1  F 1=  0 α  α 0 1 − α  1 0  , F 2=  0 α 1 − α  For the composite map F p ◦ . . . ◦ F 1 to be stable, the condition that needs to be satisﬁed is actually the same for each i, and is furthermore independent of p (since each linear map F i is symmetric). It essentially involves the stability of the 2 × 2 matrix , whose two (real) eigenvalues λ satisfy (1 − η − α − λ)(1 − α − λ) = α2. The resulting stability condition (|λ| ≤ 1) is simple and given as 0 ≤ η ≤ 2, 0 ≤ α ≤ 4−2η 4−η .  1 − α  α  α  (cid:17)  1 − η − α  (cid:32) 1 (cid:16) 1 − η − α  0 0  α  5 Experiments  In this section we compare the performance of EASGD and EAMSGD with the parallel method DOWNPOUR and the sequential method SGD, as well as their averaging and momentum variants. All the parallel comparator methods are listed below3:  paper is enclosed in the Supplement.  • DOWNPOUR [2], the pseudo-code of the implementation of DOWNPOUR used in this • Momentum DOWNPOUR (MDOWNPOUR), where the Nesterov’s momentum scheme is applied to the master’s update (note it is unclear how to apply it to the local workers or for the case when τ > 1). The pseudo-code is in the Supplement. • A method that we call ADOWNPOUR, where we compute the average over time of the center variable ˜x as follows: zt+1 = (1 − αt+1)zt + αt+1 ˜xt, and αt+1 = 1 t+1 is a moving rate, and z0 = ˜x0. t denotes the master clock, which is initialized to 0 and incremented every time the center variable ˜x is updated. • A method that we call MVADOWNPOUR, where we compute the moving average of the center variable ˜x as follows: zt+1 = (1 − α)zt + α˜xt, and the moving rate α was chosen to be constant, and z0 = ˜x0. t denotes the master clock and is deﬁned in the same way as for the ADOWNPOUR method.  3We have compared asynchronous ADMM [27] with EASGD in our setting as well, the performance is  nearly the same. However, ADMM’s momentum variant is not as stable for large communication periods.  5  η (eta)ρ (rho)p=3  123456789123456789x 10−30.9910.9920.9930.9940.9950.9960.9970.9980.99911.001η (eta)ρ (rho)p=8  123456789123456789x 10−30.9920.9940.9960.99811.002All the sequential comparator methods (p = 1) are listed below:  • SGD [1] with constant learning rate η. • Momentum SGD (MSGD) [26] with constant momentum δ. • ASGD [6] with moving rate αt+1 = 1 t+1. • MVASGD [6] with moving rate α set to a constant.  We perform experiments in a deep learning setting on two benchmark datasets: CIFAR-10 (we refer to it as CIFAR) 4 and ImageNet ILSVRC 2013 (we refer to it as ImageNet) 5. We focus on the image classiﬁcation task with deep convolutional neural networks. We next explain the experimental setup. The details of the data preprocessing and prefetching are deferred to the Supplement.  5.1 Experimental setup  For all our experiments we use a GPU-cluster interconnected with InﬁniBand. Each node has 4 Titan GPU processors where each local worker corresponds to one GPU processor. The center variable of the master is stored and updated on the centralized parameter server [2]6. To describe the architecture of the convolutional neural network, we will ﬁrst introduce a nota- tion. Let (c, y) denotes the size of the input image to each layer, where c is the number of color channels and y is both the horizontal and the vertical dimension of the input. Let C denotes the fully-connected convolutional operator and let P denotes the max pooling operator, D de- notes the linear operator with dropout rate equal to 0.5 and S denotes the linear operator with softmax output non-linearity. We use the cross-entropy loss and all inner layers use rectiﬁed linear units. For the ImageNet experiment we use the similar approach to [4] with the follow- ing 11-layer convolutional neural network (3,221)C(96,108)P(96,36)C(256,32)P(256,16)C(384,14) the CIFAR experiment we C(384,13)C(256,12)P(256,6)D(4096,1)D(4096,1)S(1000,1). use the similar approach to [29] with the following 7-layer convolutional neural network (3,28)C(64,24)P(64,12)C(128,8)P(128,4)C(64,2)D(256,1)S(10,1). In our experiments all the methods we run use the same initial parameter chosen randomly, except that we set all the biases to zero for CIFAR case and to 0.1 for ImageNet case. This parameter is 2 (cid:107)x(cid:107)2 to the loss used to initialize the master and all the local workers7. We add l2-regularization λ function F (x). For ImageNet we use λ = 10−5 and for CIFAR we use λ = 10−4. We also compute the stochastic gradient using mini-batches of sample size 128.  For  5.2 Experimental results  For all experiments in this section we use EASGD with β = 0.98 , for all momentum-based methods we set the momentum term δ = 0.99 and ﬁnally for MVADOWNPOUR we set the moving rate to α = 0.001. We start with the experiment on CIFAR dataset with p = 4 local workers running on a single computing node. For all the methods, we examined the communication periods from the following set τ = {1, 4, 16, 64}. For comparison we also report the performance of MSGD which outperformed SGD, ASGD and MVASGD as shown in Figure 6 in the Supplement. For each method we examined a wide range of learning rates (the learning rates explored in all experiments are sum- marized in Table 1, 2, 3 in the Supplement). The CIFAR experiment was run 3 times independently from the same initialization and for each method we report its best performance measured by the smallest achievable test error. From the results in Figure 2, we conclude that all DOWNPOUR- based methods achieve their best performance (test error) for small τ (τ ∈ {1, 4}), and become highly unstable for τ ∈ {16, 64}. While EAMSGD signiﬁcantly outperforms comparator methods for all values of τ by having faster convergence. It also ﬁnds better-quality solution measured by the test error and this advantage becomes more signiﬁcant for τ ∈ {16, 64}. Note that the tendency to achieve better test performance with larger τ is also characteristic for the EASGD algorithm.  4Downloaded from http://www.cs.toronto.edu/˜kriz/cifar.html. 5Downloaded from http://image-net.org/challenges/LSVRC/2013. 6Our implementation is available at https://github.com/sixin-zh/mpiT. 7On the contrary, initializing the local workers and the master with different random seeds ’traps’ the algo-  rithm in the symmetry breaking phase.  8Intuitively the ’effective β’ is β/τ = pα = pηρ (thus ρ = β  τ pη ) in the asynchronous setting.  6  Figure 2: Training and test loss and the test error for the center variable versus a wallclock time for different communication periods τ on CIFAR dataset with the 7-layer convolutional neural network. We next explore different number of local workers p from the set p = {4, 8, 16} for the CIFAR experiment, and p = {4, 8} for the ImageNet experiment9. For the ImageNet experiment we report the results of one run with the best setting we have found. EASGD and EAMSGD were run with τ = 10 whereas DOWNPOUR and MDOWNPOUR were run with τ = 1. The results are in Figure 3 and 4. For the CIFAR experiment, it’s noticeable that the lowest achievable test error by either EASGD or EAMSGD decreases with larger p. This can potentially be explained by the fact that larger p allows for more exploration of the parameter space. In the Supplement, we discuss further the trade-off between exploration and exploitation as a function of the learning rate (section 9.5) and the communication period (section 9.6). Finally, the results obtained for the ImageNet experiment also shows the advantage of EAMSGD over the competitor methods.  6 Conclusion  In this paper we describe a new algorithm called EASGD and its variants for training deep neu- ral networks in the stochastic setting when the computations are parallelized over multiple GPUs. Experiments demonstrate that this new algorithm quickly achieves improvement in test error com- pared to more common baseline approaches such as DOWNPOUR and its variants. We show that our approach is very stable and plausible under communication constraints. We provide the stability analysis of the asynchronous EASGD in the round-robin scheme, and show the theoretical advantage of the method over ADMM. The different behavior of the EASGD algorithm from its momentum- based variant EAMSGD is intriguing and will be studied in future works.  9For the ImageNet experiment, the training loss is measured on a subset of the training data of size 50,000.  7  501001500.511.52wallclock time (min)training loss (nll)τ=1  MSGDDOWNPOURADOWNPOURMVADOWNPOURMDOWNPOUREASGDEAMSGD5010015011.52wallclock time (min)test loss (nll)τ=15010015016182022242628wallclock time (min)test error (%)τ=1501001500.511.52wallclock time (min)training loss (nll)τ=45010015011.52wallclock time (min)test loss (nll)τ=45010015016182022242628wallclock time (min)test error (%)τ=4501001500.511.52wallclock time (min)training loss (nll)τ=165010015011.52wallclock time (min)test loss (nll)τ=165010015016182022242628wallclock time (min)test error (%)τ=16501001500.511.52wallclock time (min)training loss (nll)τ=645010015011.52wallclock time (min)test loss (nll)τ=645010015016182022242628wallclock time (min)test error (%)τ=64Figure 3: Training and test loss and the test error for the center variable versus a wallclock time for different number of local workers p for parallel methods (MSGD uses p = 1) on CIFAR with the 7-layer convolutional neural network. EAMSGD achieves signiﬁcant accelerations compared to other methods, e.g. the relative speed-up for p = 16 (the best comparator method is then MSGD) to achieve the test error 21% equals 11.1.  Figure 4: Training and test loss and the test error for the center variable versus a wallclock time for different number of local workers p (MSGD uses p = 1) on ImageNet with the 11-layer convolu- tional neural network. Initial learning rate is decreased twice, by a factor of 5 and then 2, when we observe that the online predictive loss [30] stagnates. EAMSGD achieves signiﬁcant accelerations compared to other methods, e.g. the relative speed-up for p = 8 (the best comparator method is then DOWNPOUR) to achieve the test error 49% equals 1.8, and simultaneously it reduces the commu- nication overhead (DOWNPOUR uses communication period τ = 1 and EAMSGD uses τ = 10).  Acknowledgments  The authors thank R. Power, J. Li for implementation guidance, J. Bruna, O. Henaff, C. Farabet, A. Szlam, Y. Bakhtin for helpful discussion, P. L. Combettes, S. Bengio and the referees for valuable feedback.  8  501001500.511.52wallclock time (min)training loss (nll)p=4  MSGDDOWNPOURMDOWNPOUREASGDEAMSGD5010015011.52wallclock time (min)test loss (nll)p=45010015016182022242628wallclock time (min)test error (%)p=4501001500.511.52wallclock time (min)training loss (nll)p=85010015011.52wallclock time (min)test loss (nll)p=85010015016182022242628wallclock time (min)test error (%)p=8501001500.511.52wallclock time (min)training loss (nll)p=165010015011.52wallclock time (min)test loss (nll)p=165010015016182022242628wallclock time (min)test error (%)p=16050100150123456wallclock time (hour)training loss (nll)p=4  MSGDDOWNPOUREASGDEAMSGD05010015023456wallclock time (hour)test loss (nll)p=405010015042444648505254wallclock time (hour)test error (%)p=4050100150123456wallclock time (hour)training loss (nll)p=805010015023456wallclock time (hour)test loss (nll)p=805010015042444648505254wallclock time (hour)test error (%)p=8References [1] Bottou, L. Online algorithms and stochastic approximations. In Online Learning and Neural Networks.  Cambridge University Press, 1998.  [2] Dean, J, Corrado, G, Monga, R, Chen, K, Devin, M, Le, Q, Mao, M, Ranzato, M, Senior, A, Tucker, P,  Yang, K, and Ng, A. Large scale distributed deep networks. In NIPS. 2012.  [3] Krizhevsky, A, Sutskever, I, and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural  networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.  [4] Sermanet, P, Eigen, D, Zhang, X, Mathieu, M, Fergus, R, and LeCun, Y. OverFeat: Integrated Recogni-  tion, Localization and Detection using Convolutional Networks. ArXiv, 2013.  [5] Nocedal, J and Wright, S. Numerical Optimization, Second Edition. Springer New York, 2006. [6] Polyak, B. T and Juditsky, A. B. Acceleration of stochastic approximation by averaging. SIAM Journal  on Control and Optimization, 30(4):838–855, 1992.  [7] Bertsekas, D. P and Tsitsiklis, J. N. Parallel and Distributed Computation. Prentice Hall, 1989. [8] Hestenes, M. R. Optimization theory: the ﬁnite dimensional case. Wiley, 1975. [9] Boyd, S, Parikh, N, Chu, E, Peleato, B, and Eckstein, J. Distributed optimization and statistical learning  via the alternating direction method of multipliers. Found. Trends Mach. Learn., 3(1):1–122, 2011.  [10] Shamir, O. Fundamental limits of online and distributed algorithms for statistical learning and estimation.  In NIPS. 2014.  [11] Yadan, O, Adams, K, Taigman, Y, and Ranzato, M. Multi-gpu training of convnets. In Arxiv. 2013. [12] Paine, T, Jin, H, Yang, J, Lin, Z, and Huang, T. Gpu asynchronous stochastic gradient descent to speed  up neural network training. In Arxiv. 2013.  [13] Seide, F, Fu, H, Droppo, J, Li, G, and Yu, D. 1-bit stochastic gradient descent and application to data-  parallel distributed training of speech dnns. In Interspeech 2014, September 2014.  [14] Bekkerman, R, Bilenko, M, and Langford, J. Scaling up machine learning: Parallel and distributed  approaches. Camridge Universityy Press, 2011.  [15] Choromanska, A, Henaff, M. B, Mathieu, M, Arous, G. B, and LeCun, Y. The loss surfaces of multilayer  networks. In AISTATS, 2015.  [16] Ho, Q, Cipar, J, Cui, H, Lee, S, Kim, J. K, Gibbons, P. B, Gibson, G. A, Ganger, G, and Xing, E. P. More  effective distributed ml via a stale synchronous parallel parameter server. In NIPS. 2013.  [17] Azadi, S and Sra, S. Towards an optimal stochastic alternating direction method of multipliers. In ICML,  2014.  [18] Borkar, V. Asynchronous stochastic approximations. SIAM Journal on Control and Optimization,  36(3):840–851, 1998.  [19] Nedi´c, A, Bertsekas, D, and Borkar, V. Distributed asynchronous incremental subgradient methods. In Inherently Parallel Algorithms in Feasibility and Optimization and their Applications, volume 8 of Studies in Computational Mathematics, pages 381 – 407. 2001.  [20] Langford, J, Smola, A, and Zinkevich, M. Slow learners are fast. In NIPS, 2009. [21] Agarwal, A and Duchi, J. Distributed delayed stochastic optimization. In NIPS. 2011. [22] Recht, B, Re, C, Wright, S. J, and Niu, F. Hogwild: A Lock-Free Approach to Parallelizing Stochastic  Gradient Descent. In NIPS, 2011.  [23] Zinkevich, M, Weimer, M, Smola, A, and Li, L. Parallelized stochastic gradient descent. In NIPS, 2010. [24] Nesterov, Y. Smooth minimization of non-smooth functions. Math. Program., 103(1):127–152, 2005. [25] Lan, G. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1-  2):365–397, 2012.  [26] Sutskever, I, Martens, J, Dahl, G, and Hinton, G. On the importance of initialization and momentum in  deep learning. In ICML, 2013.  [27] Zhang, R and Kwok, J. Asynchronous distributed admm for consensus optimization. In ICML, 2014. [28] Ouyang, H, He, N, Tran, L, and Gray, A. Stochastic alternating direction method of multipliers.  Proceedings of the 30th International Conference on Machine Learning, pages 80–88, 2013.  In  [29] Wan, L, Zeiler, M. D, Zhang, S, LeCun, Y, and Fergus, R. Regularization of neural networks using  dropconnect. In ICML, 2013.  [30] Cesa-Bianchi, N, Conconi, A, and Gentile, C. On the generalization ability of on-line learning algorithms.  IEEE Transactions on Information Theory, 50(9):2050–2057, 2004.  [31] Nesterov, Y. Media, 2004.  Introductory lectures on convex optimization, volume 87. Springer Science & Business  9  Deep learning with Elastic Averaging SGD  (Supplementary Material)  7 Additional theoretical results and proofs  7.1 Quadratic case  We provide here the convergence analysis of the synchronous EASGD algorithm with constant learn- ing rate. The analysis is focused on the convergence of the center variable to the local optimum. We discuss one-dimensional quadratic case ﬁrst, then the generalization to multi-dimensional setting (Lemma 7.3) and ﬁnally to the strongly convex case (Theorem 7.1). Our analysis in the quadratic case extends the analysis of ASGD in [6]. Assume each of the p local workers xi  t ∈ Rn observes a noisy gradient at time t ≥ 0 of the linear form given in Equation 14.  t − b − ξi t,  i ∈ {1, . . . , p},  gi t(xi  t) = Axi  (14) t}’s are i.i.d. where the matrix A is positive-deﬁnite (each eigenvalue is strictly positive) and {ξi random variables, with zero mean and positive-deﬁnite covariance Σ. Let x∗ denote the optimum solution, where x∗ = A−1b ∈ Rn. In this section we analyze the behavior of the mean squared error (MSE) of the center variable ˜xt, where this error is denoted as E[(cid:107)˜xt − x∗(cid:107)2], as a function of t, p, η, α and β, where β = pα. Note that the MSE error can be decomposed as (squared) bias and variance10: E[(cid:107)˜xt − x∗(cid:107)2] = (cid:107)E[˜xt − x∗](cid:107)2 + V[˜xt − x∗]. For one-dimensional case (n = 1), we assume A = h > 0 and Σ = σ2 > 0. Lemma 7.1. Let ˜x0 and {xi  (cid:18) γ2 − γ2t  0}i=1,...,p be arbitrary constants, then γt − φt E[˜xt − x∗] = γt(˜x0 − x∗) + γ − φ φ2 − φ2t p2α2η2 1 − φ2 − 2 (γ − φ)2 1−pα−φ (˜x0−x∗)), a = ηh+(p+1)α, c2 = ηhpα, γ = 1− a−√  αu0, γφ − (γφ)t 1 − γφ  (cid:19) σ2  1 − γ2 +  p  ,  (15)  (16)  a2−4c2 2  ,  V[˜xt − x∗] =  where u0 =(cid:80)p  0−x∗− α  i=1(xi √ and φ = 1 − a+ It follows from Lemma 7.1 that for the center variable to be stable the following has to hold  a2−4c2 2  .  (17) It can be veriﬁed that φ and γ are the two zero-roots of the polynomial in λ: λ2 − (2 − a)λ + (1 − a + c2). Recall that φ and λ are the functions of η and α. Thus (see proof in Section 7.1.2)  −1 < φ < γ < 1.  • γ < 1 iff c2 > 0 (i.e. η > 0 and α > 0). • φ > −1 iff (2 − ηh)(2 − pα) > 2α and (2 − ηh) + (2 − pα) > α. • φ = γ iff a2 = 4c2 (i.e. ηh = α = 0).  The proof the above Lemma is based on the diagonalization of the linear gradient map (this map is symmetric due to the relation β = pα). The stability analysis of the asynchronous EASGD algorithm in the round-robin scheme is similar due to this elastic symmetry.  Proof. Substituting the gradient from Equation 14 into the update rule used by each local worker in the synchronous EASGD algorithm (Equation 5 and 6) we obtain t) − α(xi  t − ˜xt),  xi t+1 = xi  (18)  p(cid:88) t − η(Axi  t − b − ξi t − ˜xt),  α(xi  ˜xt+1 = ˜xt +  (19)  10In our notation, V denotes the variance.  i=1  10  where η is the learning rate, and α is the moving rate. Recall that α = ηρ and A = h. For the ease of notation we redeﬁne ˜xt and xi  t as follows:  ˜xt (cid:44) ˜xt − x∗ and xi  t  (cid:44) xi  t − x∗.  We prove the lemma by explicitly solving the linear equations 18 and 19. (x1  t , ˜xt)T . We rewrite the recursive relation captured in Equation 18 and 19 as simply  t , . . . , xp  Let xt =  M =  where the drift matrix M is deﬁned as  1 − α − ηh this eigenvector. Thus ut =(cid:80)p  0 ... 0 α  1 − α − ηh  xt+1 = M xt + bt,  0  0 ... 0  α α ... α  ... 0 ... 0 ...   , 1−pα−φ ˜xt). Let furthermore ξt =(cid:80)p  1 − α − ηh  1 − pα  α  0 ... α t , . . . , ηξp t , 0)T .  and the (diffusion) vector bt = (ηξ1 Note that one of the eigenvalues of matrix M, that we call φ, satisﬁes (1−α−ηh−φ)(1−pα−φ) = pα2. The corresponding eigenvector is (1, 1, . . . , 1,− pα 1−pα−φ )T . Let ut be the projection of xt onto t. Therefore we  t −  i=1(xi  i=1 ξi  α  have  ut+1 = φut + ηξt.  (20)  By combining Equation 19 and 20 as follows  p(cid:88)  ˜xt+1 = ˜xt +  α(xi  t − ˜xt) = (1 − pα)˜xt + α(ut +  pα  1 − pα − φ  ˜xt)  i=1  = (1 − pα +  pα2  1 − pα − φ  )˜xt + αut = γ ˜xt + αut,  where the last step results from the following relations: 1 − α − ηh + 1 − pα. Thus we obtained  pα2  1−pα−φ = 1 − α − ηh − φ and φ + γ =  ˜xt+1 = γ ˜xt + αut.  Based on Equation 20 and 21, we can then expand ut and ˜xt recursively, ut+1 = φt+1u0 + φt(ηξ0) + . . . + φ0(ηξt), ˜xt+1 = γt+1 ˜x0 + γt(αu0) + . . . + γ0(αut).  Substituting u0, u1, . . . , ut, each given through Equation 22, into Equation 23 we obtain  ˜xt = γt ˜x0 +  γt − φt γ − φ  αu0 + αη  γt−l − φt−l  γ − φ  ξl−1.  t−1(cid:88)  l=1  (21)  (22) (23)  (24)  To be more speciﬁc, the Equation 24 is obtained by integrating by parts,  i=0  t(cid:88) t(cid:88) t(cid:88)  i=0  γt−i(αui)  γt−i(α(φiu0 +  γt−iφi(αu0) +  i=0  γt+1 − φt+1  γ − φ  ˜xt+1 = γt+1 ˜x0 +  = γt+1 ˜x0 +  = γt+1 ˜x0 +  = γt+1 ˜x0 +  φi−1−lηξl))  l=0  i−1(cid:88) t−1(cid:88) t(cid:88) t−1(cid:88)  l=0  i=l+1  γt−iφi−1−l(αηξl)  γt−l − φt−l  γ − φ  (αηξl).  (αu0) +  l=0  11  (cid:18) γt−l − φt−l  (cid:19)2  γ − φ  t−1(cid:88)  l=0  t−1(cid:88)  Since the random variables ξl are i.i.d, we may sum the variance term by term as follows  γ2(t−l) − 2γt−lφt−l + φ2(t−l)  =  =  1  l=0  (γ − φ)2  (cid:18) γ2 − γ2(t+1) t] = 0 and V[ξt] =(cid:80)p  (γ − φ)2  1 − γ2  E[ξi  − 2  γφ − (γφ)t+1  1 − γφ  φ2 − φ2(t+1)  1 − φ2  +  (cid:19)  .  (25)  t] = pσ2. These two facts, the equality in Equation 24 and Equation 25 can then be used to compute E[˜xt] and V[˜xt] as given in Equation 15 and 16 in Lemma 7.1.  i=1  i=1  V[ξi  Note that E[ξt] =(cid:80)p  7.1.1 Visualizing Lemma 7.1  Figure 5: Theoretical mean squared error (MSE) of the center ˜x in the quadratic case, with various choices of the learning rate η (horizontal within each block), and the moving rate β = pα (vertical within each block), the number of processors p = {1, 10, 100, 1000, 10000} (vertical across blocks), and the time steps t = {1, 2, 10, 100,∞} (horizontal across blocks). The MSE is plotted in log scale, ranging from 10−3 to 103 (from deep blue to red). The dark red (i.e. on the upper-right corners) indicates divergence.  In Figure 5, we illustrate the dependence of MSE on β, η and the number of processors p over time t. We consider the large-noise setting where ˜x0 = xi 0 = 1, h = 1 and σ = 10. The MSE error is color-coded such that the deep blue color corresponds to the MSE equal to 10−3, the green color corresponds to the MSE equal to 1, the red color corresponds to MSE equal to 103 and the dark red color corresponds to the divergence of algorithm EASGD (condition in Equation 17 is then violated). The plot shows that we can achieve signiﬁcant variance reduction by increasing the number of local workers p. This effect is less sensitive to the choice of β and η for large p.  12  etabetat=1,p=1012012etabetat=1,p=10012012etabetat=1,p=100012012etabetat=1,p=1000012012etabetat=1,p=10000012012etabetat=2,p=1012012etabetat=2,p=10012012etabetat=2,p=100012012etabetat=2,p=1000012012etabetat=2,p=10000012012etabetat=10,p=1012012etabetat=10,p=10012012etabetat=10,p=100012012etabetat=10,p=1000012012etabetat=10,p=10000012012etabetat=100,p=1012012etabetat=100,p=10012012etabetat=100,p=100012012etabetat=100,p=1000012012etabetat=100,p=10000012012etabetat=inf,p=1012012etabetat=inf,p=10012012etabetat=inf,p=100012012etabetat=inf,p=1000012012etabetat=inf,p=100000120127.1.2 Condition in Equation 17  We are going to show that  • γ < 1 iff c2 > 0 (i.e. η > 0 and β > 0). • φ > −1 iff (2 − ηh)(2 − β) > 2β/p and (2 − ηh) + (2 − β) > β/p. • φ = γ iff a2 = 4c2 (i.e. ηh = β = 0).  Recall that a = ηh + (p + 1)α, c2 = ηhpα, γ = 1− a−√ We have  a2−4c2 2  √ , φ = 1− a+  a2−4c2 2  , and β = pα.  √  • γ < 1 ⇔ a−√ a2−4c2 2 √ • φ > −1 ⇔ 2 > a+ 4 − a > 0, 4 − 2a + c2 > 0 ⇔ 4 > ηh + β + α, 4 − 2(ηh + β + α) + ηhβ > 0. • φ = γ ⇔ √  a2 − 4c2 ⇔ a2 > a2 − 4c2 ⇔ c2 > 0.  > 0 ⇔ a > a2−4c2 2  a2 − 4c2 = 0 ⇔ a2 = 4c2.  a2 − 4c2 ⇔ 4 − a > 0, (4 − a)2 > a2 − 4c2 ⇔  ⇔ 4 − a >  √  The next corollary is a consequence of Lemma 7.1. As the number of workers p grows, the averaging property of the EASGD can be characterized as follows Corollary 7.1. Let the Elastic Averaging relation β = pα and the condition 17 hold, then  t→∞ pE[(˜xt − x∗)2] =  lim p→∞ lim  βηh  (2 − β)(2 − ηh)  · 2 − β − ηh + βηh β + ηh − βηh  · σ2 h2 .  Proof. Note that when β is ﬁxed, limp→∞ a = ηh + β and c2 = ηhβ. Then limp→∞ φ = min(1 − β, 1 − ηh) and limp→∞ γ = max(1 − β, 1 − ηh). Also note that using Lemma 7.1 we obtain  φ2  1 − γ2 +  (cid:18) γ2 (cid:18) γ2(1 − φ2)(1 − φγ) + φ2(1 − γ2)(1 − φγ) − 2γφ(1 − γ2)(1 − φ2) (cid:18)  (1 − γ2)(1 − φ2)(1 − γφ)  1 − φ2 − 2γφ 1 − γφ  (cid:19) σ2 (cid:19) σ2  (γ − φ)2(1 + γφ)  p  (cid:19) σ2  p  (1 − γ2)(1 − φ2)(1 − γφ)  p  E[(˜xt − x∗)2] =  lim t→∞  =  =  =  β2η2 (γ − φ)2 β2η2 (γ − φ)2 β2η2 (γ − φ)2  β2η2  (1 − γ2)(1 − φ2)  · 1 + γφ 1 − γφ  · σ2 p  .  Corollary 7.1 is obtained by plugining in the limiting values of φ and γ. The crucial point of Corollary 7.1 is that the MSE in the limit t → ∞ is in the order of 1/p which implies that as the number of processors p grows, the MSE will decrease for the EASGD algorithm. Also note that the smaller the β is (recall that β = pα = pηρ), the more exploration is allowed (small ρ) and simultaneously the smaller the MSE is.  7.2 Generalization to multidimensional case  The next lemma (Lemma 7.2) shows that EASGD algorithm achieves the highest possible rate of convergence when we consider the double averaging sequence (similarly to [6]) {z1, z2, . . .} deﬁned as below  ˜xk.  (26)  t(cid:88)  k=0  zt+1 =  1  t + 1  Lemma 7.2 (Weak convergence). If the condition in Equation 17 holds, then the normalized double averaging sequence deﬁned in Equation 26 converges weakly to the normal distribution with zero mean and variance σ2/ph2,  √  t(zt − x∗) (cid:42) N (0,  σ2 ph2 ),  t → ∞.  (27)  13  t(cid:88)  k=0  k−1(cid:88)  Proof. As in the proof of Lemma 7.1, for the ease of notation we redeﬁne ˜xt and xi  t as follows:  ˜xt (cid:44) ˜xt − x∗ and xi  t  (cid:44) xi  t − x∗.  Also recall that {ξi t}’s are i.i.d. random variables (noise) with zero mean and the same covariance Σ (cid:31) 0. We are interested in the asymptotic behavior of the double averaging sequence {z1, z2, . . .} deﬁned as  zt+1 =  1  t + 1  ˜xk.  (28)  Recall the Equation 24 from the proof of Lemma 7.1 (for the convenience it is provided below):  ˜xk = γk ˜x0 + αu0  γk − φk γ − φ  + αη  γk−l − φk−l  γ − φ  ξl−1,  where ξt =(cid:80)p t(cid:88)  ˜xk =  i=1 ξi 1 − γt+1 1 − γ  k=0  t. Therefore  ˜x0 + αu0  γ − µ  = O(1) + αη  1  γ − φ  γ  t−1(cid:88)  l=1  (cid:18) 1 − γt+1  1 − γ 1 − γt−l 1 − γ  l=1  (cid:19) − 1 − φt+1 (cid:19) 1 − φ 1 − φt−l 1 − φ  1  (cid:18)  √ Note that the only non-vanishing term (in weak convergence) of 1/  t−1(cid:88)  t(cid:88)  l=1  k=l+1  + αη  γk−l − φk−l  γ − φ  ξl−1  t(cid:80)t  k=0 ˜xk as t → ∞ is  ξl−1  (cid:19)  1  γ − φ  1 − γ  − φ 1 − φ  ξl−1.  (29)  − φ  (cid:18) γ (cid:19)  t−1(cid:88)  l=1  αη  1√ t  (cid:18) γ  Also recall that V[ξl−1] = pσ2 and  1  γ − φ  1 − γ  − φ 1 − φ  1  =  (1 − γ)(1 − φ)  =  1  ηhpα  .  Therefore the expression in Equation 29 is asymptotically normal with zero mean and variance σ2/ph2.  The asymptotic variance in the Lemma 7.2 is optimal with any ﬁxed η and β for which Equation 17 holds. The next lemma (Lemma 7.3) extends the result in Lemma 7.2 to the multi-dimensional setting. Lemma 7.3 (Weak convergence). Let h denotes the largest eigenvalue of A. If (2 − ηh)(2 − β) > 2β/p, (2 − ηh) + (2 − β) > β/p, η > 0 and β > 0, then the normalized double averaging sequence converges weakly to the normal distribution with zero mean and the covariance matrix V = A−1Σ(A−1)T ,  √  tp(zt − x∗) (cid:42) N (0, V ),  t → ∞.  Proof. Since A is symmetric, one can use the proof technique of Lemma 7.2 to prove Lemma 7.3 by diagonalizing the matrix A. This diagonalization essentially generalizes Lemma 7.1 to the mul- tidimensional case. We will not go into the details of this proof as we will provide a simpler way to look at the system. As in the proof of Lemma 7.1 and Lemma 7.2, for the ease of notation we redeﬁne ˜xt and xi  t as follows:  Let the spatial average of the local parameters at time t be denoted as yt where yt = 1 p and let the average noise be denoted as ξt, where ξt = 1 p reduced to the following  t, i=1 xi t. Equations 18 and 19 can then be  i=1 ξi  ˜xt (cid:44) ˜xt − x∗ and xi  t  (cid:44) xi  t − x∗. (cid:80)p  (cid:80)p  (30)  (31) (32)  yt+1 = yt − η(Ayt − ξt) + α(˜xt − yt), ˜xt+1 = ˜xt + β(yt − ˜xt).  14  We focus on the case where the learning rate η and the moving rate α are kept constant over time11. Recall β = pα and α = ηρ. Let’s introduce the block notation Ut = (yt, ˜xt), Ξt = (ηξt, 0), M = I − ηL and  (cid:18) A + α  η I − α η I − β β η I η I  (cid:19)  .  L =  From Equations 31 and 32 it follows that Ut+1 = M Ut + Ξt. Note that this linear system has a degenerate noise Ξt which prevents us from directly applying results of [6]. Expanding this recursive relation and summing by parts, we have  t(cid:88)  Uk = M 0U0 +  k=0  M 1U0 + M 0Ξ0 + M 2U0 + M 1Ξ0 + M 0Ξ1 + ... M tU0 + M t−1Ξ0 + ··· + M 0Ξt−1.  By Lemma 7.4, (cid:107)M(cid:107)2 < 1 and thus  M 0 + M 1 + ··· + M t + ··· = (I − M )−1 = η−1L−1.  thus  (ηL)−1(cid:80)t  that  Note 1√ t  Since A is invertible, we get  (cid:18) A−1  A−1  L−1 =  (cid:19)  ,  α  β A−1 β + α  β A−1  η  t(cid:88)  k=0  1√ t  Uk =  1√ t  U0 +  ηL−1  1√ t  t(cid:88)  k=1  Ξk−1 − 1√ t  t(cid:88)  k=1  M k+1Ξk−1.  the only non-vanishing term (in weak convergence) of k=1 Ξk−1 thus we have  t(cid:88)  (cid:18)(cid:18) 0  0  (cid:19)  (cid:18) V  V  ,  (cid:19)(cid:19)  ,  V V  (ηL)−1  1√ t where V = A−1Σ(A−1)T . Lemma 7.4. If the following conditions hold:  k=1  Ξk−1 (cid:42) N  1√ t  (cid:80)t  k=0 Uk  is  (33)  (2 − ηh)(2 − pα) > 2α (2 − ηh) + (2 − pα) > α η > 0 α > 0  (cid:19)  (cid:18) y  z  (cid:19)  (cid:18) y  z  then (cid:107)M(cid:107)2 < 1.  Proof. The eigenvalue λ of M and the (non-zero) eigenvector (y, z) of M satisfy  M  = λ  .  (34)  11As a side note, notice that the center parameter ˜xt is tracking the spatial average yt of the local parameters with a non-symmetric spring in Equation 31 and 32. To be more precise note that the update on yt+1 contains (˜xt − yt) scaled by α, whereas the update on ˜xt+1 contains −(˜xt − yt) scaled by β. Since α = β/p the impact of the center ˜xt+1 on the spatial local average yt+1 becomes more negligible as p grows.  15  Recall that  From the Equations 34 and 35 we obtain  M = I − ηL =  (cid:18) I − ηA − αI (cid:26) y − ηAy − αy + αz = λy  βy + (1 − β)z = λz  βI  (cid:19)  .  αI I − βI  .  (35)  (36)  Since (y, z) is assumed to be non-zero, we can write z = βy/(λ + β − 1). Then the Equation 36 can be reduced to  ηAy = (1 − α − λ)y +  αβ  λ + β − 1  y.  (37)  Thus y is the eigenvector of A. Let λA be the eigenvalue of matrix A such that Ay = λAy. Thus based on Equation 37 it follows that  ηλA = (1 − α − λ) +  αβ  λ + β − 1  .  (38)  Equation 38 is equivalent to  λ2 − (2 − a)λ + (1 − a + c2) = 0,  (39) where a = ηλA + (p + 1)α, c2 = ηλApα. It follows from the condition in Equation 17 that −1 < λ < 1 iff η > 0, β > 0, (2 − ηλA)(2 − β) > 2β/p and (2 − ηλA) + (2 − β) > β/p. Let h denote the maximum eigenvalue of A and note that 2 − ηλA ≥ 2 − ηh. This implies that the condition of our lemma is sufﬁcient.  As in Lemma 7.2, the asymptotic covariance in the Lemma 7.3 is optimal, i.e. meets the Fisher information lower-bound. The fact that this asymptotic covariance matrix V does not contain any term involving ρ is quite remarkable, since the penalty term ρ does have an impact on the condition number of the Hessian in Equation 2.  7.3 Strongly convex case  We now extend the above proof ideas to analyze the strongly convex case, in which the noisy gradient t(x) = ∇F (x) − ξi t has the regularity that there exists some 0 < µ ≤ L, for which µ(cid:107)x − y(cid:107)2 ≤ gi (cid:104)∇F (x) − ∇F (y), x − y(cid:105) ≤ L(cid:107)x − y(cid:107)2 holds uniformly for any x ∈ Rd, y ∈ Rd. The noise {ξi  t}’s is assumed to be i.i.d. with zero mean and bounded variance E[(cid:13)(cid:13)ξi (cid:13)(cid:13)2 Theorem 7.1. Let at = E(cid:13)(cid:13)(cid:13) 1 t − x∗(cid:13)(cid:13)2, ct = E(cid:107)˜xt − x∗(cid:107)2, E(cid:13)(cid:13)xi (cid:33) (cid:33)(cid:32)at  (cid:80)p , bt = 1 i=1 xi p √ (cid:32)at+1 µ+L and γ2 = 2ηL(1 − 2 µ+L ). If 0 ≤ η ≤ 2 µL  (cid:80)p µ+L (1 − α), 0 ≤ α < 1 and 0 ≤ β ≤ 1 then  t − x∗(cid:13)(cid:13)(cid:13)2 (cid:32)1 − γ1 − γ2 − α  γ1 = 2η µL  ] ≤ σ2.  (cid:33)  γ2  i=1  p  t  ≤  bt+1 ct+1  0 β  1 − γ1 − α  0  α α 1 − β  +  bt ct  η2 σ2  p η2σ2   .  0  (cid:80)p Proof. The idea of the proof is based on the point of view in Lemma 7.3, i.e. how close the center t. To further simplify the variable ˜xt is to the spatial average of the local variables yt = 1 i=1 xi p notation, let the noisy gradient be ∇f i t) = ∇F (xi t) − ξi t, and ∇f i t) be its deterministic part. Then EASGD updates can be rewritten as follows, t − ˜xt),  t = ∇F (xi  t,ξ − α(xi  t,ξ = gi  t(xi  xi t+1 = xi ˜xt+1 = ˜xt + β(yt − ˜xt).  We have thus the update for the spatial average,  yt+1 = yt − η  ∇f i  t,ξ − α(yt − ˜xt).  (40) (41)  (42)  t − η∇f i p(cid:88)  1 p  i=1  16  (cid:80)p  (cid:13)(cid:13)xi t − x∗(cid:13)(cid:13)2. W start from the following estimate for the strongly convex function [31],  to bound the distance (cid:107)˜xt − x∗(cid:107)2  through (cid:107)yt − x∗(cid:107)2 and  the proof  The idea of 1 p  is  i  (cid:104)∇F (x) − ∇F (y), x − y(cid:105) ≥ µL µ + L  (cid:107)x − y(cid:107)2 +  1  (cid:107)∇F (x) − ∇F (y)(cid:107)2 .  µ + L  t  t  t  .  1  t,ξ  +  (46)  (45)  (44)  (43)  t , xi  µ + L  µ + L  t − ξi  t − ˜xt  t − ˜xt  t − ˜xt  t − ˜xt  t − ˜xt  − 2η  t − ˜xt, xi  t − ˜xt, xi  2(cid:10)xi  t,ξ, xi t,ξ, xi  From Equation 40 the following relation holds,  Combining the above estimates in Equations 43, 44, 45, 46, we obtain  Since ∇f (x∗) = 0, we have t , xi  (cid:10)∇f i t+1 − x∗(cid:13)(cid:13)2 (cid:13)(cid:13)xi  By the Cauchy-Schwarz inequality, we have t − ˜xt  By the cosine rule (2(cid:104)a − b, c − d(cid:105) = (cid:107)a − d(cid:107)2 − (cid:107)a − c(cid:107)2 + (cid:107)c − b(cid:107)2 − (cid:107)d − b(cid:107)2), we have  t − x∗(cid:13)(cid:13)2 (cid:13)(cid:13)2 (cid:13)(cid:13)xi (cid:13)(cid:13)∇f i t − x∗(cid:11) ≥ µL (cid:13)(cid:13)2 + α2(cid:13)(cid:13)xi (cid:13)(cid:13)2 + η2(cid:13)(cid:13)∇f i t − x∗(cid:13)(cid:13)2 = (cid:13)(cid:13)xi t − x∗(cid:11) t − x∗(cid:11) − 2α(cid:10)xi − 2η(cid:10)∇f i (cid:11) . + 2ηα(cid:10)∇f i (cid:13)(cid:13)2 − (cid:107)˜xt − x∗(cid:107)2 . +(cid:13)(cid:13)xi t − x∗(cid:13)(cid:13)2 t − x∗(cid:11) =(cid:13)(cid:13)xi (cid:13)(cid:13) . (cid:11) ≤(cid:13)(cid:13)∇f i (cid:13)(cid:13)(cid:13)(cid:13)xi (cid:10)∇f i (cid:13)(cid:13)2 + α2(cid:13)(cid:13)xi (cid:13)(cid:13)2 + η2(cid:13)(cid:13)∇f i t − x∗(cid:13)(cid:13)2 t+1 − x∗(cid:13)(cid:13)2 ≤ (cid:13)(cid:13)xi (cid:13)(cid:13)xi (cid:18) µL (cid:13)(cid:13)2(cid:19) t − x∗(cid:13)(cid:13)2 (cid:13)(cid:13)∇f i (cid:13)(cid:13)xi + 2η(cid:10)ξi (cid:13)(cid:13)2 − (cid:107)˜xt − x∗(cid:107)2(cid:1) +(cid:13)(cid:13)xi t − x∗(cid:13)(cid:13)2 − α(cid:0)(cid:13)(cid:13)xi + 2ηα(cid:13)(cid:13)∇f i (cid:13)(cid:13)(cid:13)(cid:13)xi (cid:13)(cid:13) − 2ηα(cid:10)ξi (cid:11) . (cid:13)(cid:13)2 (cid:13)(cid:13)2 − α(cid:13)(cid:13)xi Choosing 0 ≤ α < 1, we can have this upper-bound for the terms α2(cid:13)(cid:13)xi + 2ηα(cid:13)(cid:13)∇f i (cid:13)(cid:13)2 (cid:13)(cid:13) = −α(1 − α)(cid:13)(cid:13)xi (cid:13)(cid:13) ≤ η2α (cid:13)(cid:13)(cid:13)(cid:13)xi 2ηα(cid:13)(cid:13)∇f i (cid:13)(cid:13)(cid:13)(cid:13)xi (cid:13)(cid:13)2 by (cid:13)(cid:13)∇f i (cid:13)(cid:13). Thus we can further bound Equation 47 with 4a with x =(cid:13)(cid:13)xi t+1 − x∗(cid:13)(cid:13)2 ≤ (1 − 2η (cid:13)(cid:13)xi t − x∗(cid:13)(cid:13)2 − α)(cid:13)(cid:13)xi )(cid:13)(cid:13)∇f i (cid:13)(cid:13)2 − 2η2(cid:10)∇f i (cid:11) (cid:11) + 2η(cid:10)ξi t − x∗(cid:11) − 2ηα(cid:10)ξi + η2(cid:13)(cid:13)ξi (cid:13)(cid:13)2 (cid:13)(cid:13)2 ≤ σ2), if η is chosen small enough such that η2 + η2α bounded (E(cid:13)(cid:13)ξi E(cid:13)(cid:13)xi − α)E(cid:13)(cid:13)xi t+1 − x∗(cid:13)(cid:13)2 ≤ (1 − 2η t − x∗(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 p(cid:88)  Now we apply similar idea to estimate (cid:107)yt − x∗(cid:107)2. From Equation 42 the following relation holds,  µ+L ≤ 0, then + η2σ2 + αE(cid:107)˜xt − x∗(cid:107)2 .  µ + L t , ξi t, xi t + α(cid:107)˜xt − x∗(cid:107)2  (cid:107)yt+1 − x∗(cid:107)2 = (cid:107)yt − x∗(cid:107)2 + η2  t = 0) and the variance of the noise ξi  As in Equation 48 and 49, the noise ξi  applying −ax2 + bx ≤ b2  − 2η µ + L t − ˜xt  t is zero mean (Eξi  t − x∗(cid:11)  + α2 (cid:107)yt − ˜xt(cid:107)2  t − ˜xt t − ˜xt  η2α 1 − α t, xi  + t − ˜xt  1−α − 2η  t − ˜xt  t − ˜xt  t − ˜xt  t − ˜xt  t − ˜xt  t − ˜xt  + (η2 +  ∇f i  µ + L  µ + L  µ + L  t, xi  t, xi  (48)  (49)  (50)  (47)  1−α  t is  µL  µL  +  t,ξ  p  1  t  t  t  t  t  t  t  t  i=1  − 2η  ∇f i  t,ξ, yt − x∗  − 2α(cid:104)yt − ˜xt, yt − x∗(cid:105)  + 2ηα  ∇f i  t,ξ, yt − ˜xt  .  (51)  (cid:43) (cid:43)  (cid:42) (cid:42)  1 p  p(cid:88) p(cid:88)  i=1  1 p  i=1  17  (cid:68) 1 (cid:42)  p  By  (cid:69) (cid:43)  (cid:80)p t , yt − x∗  j=1 bj  (cid:80)p p(cid:88)  1 p  i=1  i=1 ai, 1 p  ∇f i  = 1 p  =  1 p  By the cosine rule, we have  (cid:80)p i=1 (cid:104)ai, bi(cid:105) − 1 p(cid:88) (cid:10)∇f i  (cid:80) t − x∗(cid:11) − 1  t , xi  p2  i=1  i>j (cid:104)ai − aj, bi − bj(cid:105), we have (cid:69) (cid:88)  (cid:68)∇f i  t − ∇f j  t − xj  t , xi  t  p2  i>j  2(cid:104)yt − ˜xt, yt − x∗(cid:105) = (cid:107)yt − x∗(cid:107)2 + (cid:107)yt − ˜xt(cid:107)2 − (cid:107)˜xt − x∗(cid:107)2 .  .  (52)  (53)  (cid:80)p  Denote ξt = 1 p  i=1 ξi  t, we can rewrite Equation 51 as  (cid:107)yt+1 − x∗(cid:107)2 = (cid:107)yt − x∗(cid:107)2 + η2  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2  + α2 (cid:107)yt − ˜xt(cid:107)2  − 2η  ∇f i  t − ξt, yt − x∗  − 2α(cid:104)yt − ˜xt, yt − x∗(cid:105)  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1  p  p(cid:88)  i=1  .  ∇f i  t − ξt (cid:43) (cid:43) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 t − ξt (cid:88)  i>j  ∇f i  ∇f i  t − ξt, yt − ˜xt (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 p(cid:88) t − x∗(cid:11) − 1 (cid:43)  p2  i=1  p  t , xi  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2  1  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 p(cid:88) (cid:13)(cid:13)xi t − x∗(cid:13)(cid:13)2  ∇f i  i=1  p  t − ∇f j  t , xi  +  t − ξt (cid:69) t − xj (cid:43)  t  (cid:42) (cid:42)  1 p  p(cid:88) p(cid:88)  i=1  i=1  1 p  + 2ηα  − 2η  (cid:18) 1 (cid:42)  p  p(cid:88) (cid:10)∇f i p(cid:88)  i=1  1 p  i=1  − 2η  1 p  + 2η  1 p2  µ + L  i=1  (cid:18) µL p(cid:88) (cid:68)∇f i (cid:88) (cid:42) p(cid:88)  i>j  1 p  i=1  By combining the above Equations 52, 53 with 54, we obtain  (cid:107)yt+1 − x∗(cid:107)2 = (cid:107)yt − x∗(cid:107)2 + η2  + α2 (cid:107)yt − ˜xt(cid:107)2  (cid:68)∇f i  t − ∇f j  t , xi  t − xj  t  (cid:69)(cid:19)  + 2η (cid:104)ξt, yt − x∗(cid:105) − α((cid:107)yt − x∗(cid:107)2 + (cid:107)yt − ˜xt(cid:107)2 − (cid:107)˜xt − x∗(cid:107)2)  + 2ηα  ∇f i  t − ξt, yt − ˜xt  .  Thus it follows from Equation 43 and 56 that  (cid:107)yt+1 − x∗(cid:107)2 ≤ (cid:107)yt − x∗(cid:107)2 + η2  + α2 (cid:107)yt − ˜xt(cid:107)2  (cid:13)(cid:13)∇f i  t  (cid:13)(cid:13)2(cid:19)  µ + L  + 2η (cid:104)ξt, yt − x∗(cid:105) − α((cid:107)yt − x∗(cid:107)2 + (cid:107)yt − ˜xt(cid:107)2 − (cid:107)˜xt − x∗(cid:107)2)  + 2ηα  ∇f i  t − ξt, yt − ˜xt  .  (57)  (cid:80)p p(cid:88) (cid:13)(cid:13)xi t − x∗(cid:13)(cid:13)2 p(cid:88) (cid:13)(cid:13)2 (cid:13)(cid:13)∇f i  i=1  t  1 p  i=1  1 p  Recall yt = 1 p  i=1 xi  t, we have the following bias-variance relation,  (cid:13)(cid:13)2  t − yt  p(cid:88) (cid:13)(cid:13)xi (cid:13)(cid:13)(cid:13)∇f i (cid:88)  i=1  i>j  1 p  1 p2  + (cid:107)yt − x∗(cid:107)2 =  1 p2  ∇f i  t  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1  p  p(cid:88)  i=1  t − ∇f j  t  +  (cid:88) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2  i>j  .  =  =  (cid:13)(cid:13)(cid:13)xi  t − xj  t  (cid:13)(cid:13)(cid:13)2  + (cid:107)yt − x∗(cid:107)2 ,  (58)  (cid:13)(cid:13)(cid:13)2  18  (54)  (55)  (56)  p  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1 (cid:19) 1  √  µL µ + L  (cid:13)(cid:13)(cid:13)2 ≥ 2 p(cid:88)  i=1  (cid:68)∇f i (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1  p(cid:88)  i=1  1  µ + L  (cid:68)∇f i p t − ∇f j (cid:43)  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2(cid:19) (cid:69)  ∇f i  t  t − xj  t  t , xi  By the Cauchy-Schwarz inequality, we have  (cid:13)(cid:13)(cid:13)xi  (cid:13)(cid:13)(cid:13)2  (cid:13)(cid:13)(cid:13)∇f i  µL  µ + L  t − xj  t  +  1  µ + L  t − ∇f j  t  (cid:69)  .  (59)  t − ∇f j  t , xi  t − xj  t  Combining the above estimates in Equations 57, 58, 59, we obtain  (cid:107)yt+1 − x∗(cid:107)2 ≤ (cid:107)yt − x∗(cid:107)2 + η2  ∇f i  t − ξt  + α2 (cid:107)yt − ˜xt(cid:107)2  (cid:18) µL (cid:18)  − 2η  + 2η  (cid:107)yt − x∗(cid:107)2 + µ + L √ 1 − 2  (cid:88)  µL µ + L  p2  i>j  + 2ηα  (cid:42) p(cid:88) (cid:13)(cid:13)(cid:13) 1 (cid:13)(cid:13)(cid:13) (cid:107)yt − ˜xt(cid:107) ≤ η2α  1 p  i=1  ∇f i  t − ξt, yt − ˜xt (cid:80)p i=1 ∇f i  (cid:13)(cid:13)(cid:13)2  t  + 2η (cid:104)ξt, yt − x∗(cid:105) − α((cid:107)yt − x∗(cid:107)2 + (cid:107)yt − ˜xt(cid:107)2 − (cid:107)˜xt − x∗(cid:107)2)  .  (60)  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1  p  p(cid:88)  i=1  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2  ∇f i  t  (cid:13)(cid:13)(cid:13) 1  (cid:80)p i=1 ∇f i  t  Similarly if 0 ≤ α < 1, we can have this upper-bound for the terms α2 (cid:107)yt − ˜xt(cid:107)2− α(cid:107)yt − ˜xt(cid:107)2 + 4a with x = 2ηα (cid:107)yt − ˜xt(cid:107). Thus we have the following bound for the Equation 60  by applying −ax2 + bx ≤ b2  1−α  p  p  (cid:107)yt+1 − x∗(cid:107)2 ≤ (1 − 2η  − α)(cid:107)yt − x∗(cid:107)2 + (η2 +  η2α 1 − α  − 2η µ + L  )  µL  µ + L  p(cid:88)  (cid:42) (cid:18)  − 2η2  + 2η  1 p √ 1 − 2  i=1  ∇f i  t , ξt  (cid:19) 1  µL µ + L  (cid:43) (cid:88)  p2  i>j  (cid:68)∇f i  + η2 (cid:107)ξt(cid:107)2 + α(cid:107)˜xt − x∗(cid:107)2 .  + 2η (cid:104)ξt, yt − x∗(cid:105) − 2ηα(cid:104)ξt, yt − ˜xt(cid:105)  (cid:69)  t − ∇f j  t , xi  t − xj  t  Since 2  √ µ+L ≤ 1, we need also bound the non-linear term  µL  Recall the bias-variance relation 1 p if 1 p  The key observation is that  (cid:13)(cid:13)(cid:13)xi  t − xj  t  i>j  (cid:13)(cid:13)(cid:13)2  (cid:80)  Again choose η small enough such that η2 + η2α  E(cid:107)yt+1 − x∗(cid:107)2 ≤ (1 − 2η  (cid:68)∇f i  (61)  (cid:13)(cid:13)(cid:13)2  (cid:13)(cid:13)(cid:13)xi  (cid:69) ≤ L (cid:13)(cid:13)(cid:13)2  t  t  i>j  i=1  i=1  t , xi  = 1 p2  t − xj  (cid:80)p (cid:80)p  t − xj t − xj  (cid:80) t − ∇f j  (cid:13)(cid:13)(cid:13)xi t − x∗(cid:13)(cid:13)2 (cid:13)(cid:13)xi (cid:13)(cid:13)xi t − x∗(cid:13)(cid:13)2 remains bounded,  . + (cid:107)yt − x∗(cid:107)2. then larger variance implies smaller bias (cid:107)yt − x∗(cid:107)2. Thus this non-linear term can be compensated. 1−α − 2η µ+L ≤ 0 and take expectation in Equation 61, − α)E(cid:107)yt − x∗(cid:107)2 p(cid:88)  E(cid:13)(cid:13)xi t − x∗(cid:13)(cid:13)2 − E(cid:107)yt − x∗(cid:107)2  µL µ + L √ 1 − 2  (cid:19)(cid:18) 1  + 2ηL  (cid:18)  (cid:19)  t  µL µ + L  p  i=1  + η2 σ2 p  + αE(cid:107)˜xt − x∗(cid:107)2 .  (62)  As for the center variable in Equation 41, we apply simply the convexity of the norm (cid:107)·(cid:107)2 to obtain (63)  (cid:107)˜xt+1 − x∗(cid:107)2 ≤ (1 − β)(cid:107)˜xt − x∗(cid:107)2 + β (cid:107)yt − x∗(cid:107)2 .  19  Combing the estimates from Equations 50, 62, 63, and denote at = E(cid:107)yt − x∗(cid:107)2, bt =  (cid:80)p  1 p  i=1  E(cid:13)(cid:13)xi t − x∗(cid:13)(cid:13)2, ct = E(cid:107)˜xt − x∗(cid:107)2, γ1 = 2η µL (cid:32)at+1 (cid:33)  (cid:32)1 − γ1 − γ2 − α  γ2  ≤  bt+1 ct+1  0 β  √ µ+L, γ2 = 2ηL(1 − 2 (cid:33)(cid:32)at (cid:33) µ+L ), then  µL  η2 σ2  p η2σ2   ,  +  α α 1 − β  bt ct  0  1 − γ1 − α  0  as long as 0 ≤ β ≤ 1, 0 ≤ α < 1 and η2 + η2α  1−α − 2η  µ+L ≤ 0, i.e. 0 ≤ η ≤ 2  µ+L (1 − α).  8 Additional pseudo-codes of the algorithms  8.1 DOWNPOUR pseudo-code  Algorithm 3 captures the pseudo-code of the implementation of the DOWNPOUR used in this paper.  Algorithm 3: DOWNPOUR: Processing by worker i and the master Input: learning rate η, communication period τ ∈ N Initialize: ˜x is initialized randomly, xi = ˜x, vi = 0, ti = 0 Repeat  if (τ divides ti) then  ˜x ← ˜x + vi xi ← ˜x vi ← 0  end xi ← xi − ηgi vi ← vi − ηgi ti ← ti + 1 Until forever  ti(xi) ti(xi)  8.2 MDOWNPOUR pseudo-code  Algorithms 4 and 5 capture the pseudo-codes of the implementation of momentum DOWNPOUR (MDOWNPOUR) used in this paper. Algorithm 4 shows the behavior of each local worker and Algorithm 5 shows the behavior of the master.  Algorithm 4: MDOWNPOUR: Processing by worker i Initialize: xi = ˜x Repeat  Receive ˜x from the master: xi ← ˜x Compute gradient gi = gi(xi) Send gi to the master  Until forever  Algorithm 5: MDOWNPOUR: Processing by the master Input: learning rate η, momentum term δ Initialize: ˜x is initialized randomly, vi = 0, Repeat  Receive gi v ← δv − ηgi ˜x ← ˜x + δv Until forever  20  9 Experiments - additional material  9.1 Data preprocessing  For the ImageNet experiment, we re-size each RGB image so that the smallest dimension is 256 pixels. We also re-scale each pixel value to the interval [0, 1]. We then extract random crops (and their horizontal ﬂips) of size 3 × 221 × 221 pixels and present these to the network in mini-batches of size 128. For the CIFAR experiment, we use the original RGB image of size 3 × 32 × 32. As before, we re-scale each pixel value to the interval [0, 1]. We then extract random crops (and their horizontal ﬂips) of size 3 × 28 × 28 pixels and present these to the network in mini-batches of size 128. The training and test loss and the test error are only computed from the center patch (3 × 28 × 28) for the CIFAR experiment and the center patch (3 × 221 × 221) for the ImageNet experiment.  9.2 Data prefetching (Sampling the dataset by the local workers)  We will now explain precisely how the dataset is sampled by each local worker as uniformly and efﬁciently as possible. The general parallel data loading scheme on a single machine is as fol- lows: we use k CPUs, where k = 8, to load the data in parallel. Each data loader reads from the memory-mapped (mmap) ﬁle a chunk of c raw images (preprocessing was described in the previous subsection) and their labels (for CIFAR c = 512 and for ImageNet c = 64). For the CIFAR, the mmap ﬁle of each data loader contains the entire dataset whereas for ImageNet, each mmap ﬁle of each data loader contains different 1/k fractions of the entire dataset. A chunk of data is always sent by one of the data loaders to the ﬁrst worker who requests the data. The next worker request- ing the data from the same data loader will get the next chunk. Each worker requests in total k data chunks from k different data loaders and then process them before asking for new data chunks. Notice that each data loader cycles through the data in the mmap ﬁle, sending consecutive chunks to the workers in order in which it receives requests from them. When the data loader reaches the end of the mmap ﬁle, it selects the address in memory uniformly at random from the interval [0, s], where s = (number of images in the mmap ﬁle modulo mini-batch size), and uses this address to start cycling again through the data in the mmap ﬁle. After the local worker receives the k data chunks from the data loaders, it shufﬂes them and divides it into mini-batches of size 128.  9.3 Learning rates  In Table 1 we summarize the learning rates η (we used constant learning rates) explored for each method shown in Figure 2. For all values of τ the same set of learning rates was explored for each method.  Table 1: Learning rates explored for each method shown in Figure 2 (CIFAR experiment).  EASGD EAMSGD  DOWNPOUR ADOWNPOUR  MVADOWNPOUR  MDOWNPOUR  SGD, ASGD, MVASGD  MSGD  η  {0.05, 0.01, 0.005} {0.01, 0.005, 0.001} {0.005, 0.001, 0.0005}  {0.00005, 0.00001, 0.000005}  {0.05, 0.01, 0.005}  {0.001, 0.0005, 0.0001}  In Table 2 we summarize the learning rates η (we used constant learning rates) explored for each method shown in Figure 3. For all values of p the same set of learning rates was explored for each method. In Table 3 we summarize the initial learning rates η we use for each method shown in Figure 4. For all values of p the same set of learning rates was explored for each method. We also used the rule of the thumb to decrease the initial learning rate twice, ﬁrst time we divided it by 5 and the second time by 2, when we observed that the decrease of the online predictive (training) loss saturates.  21  Table 2: Learning rates explored for each method shown in Figure 3 (CIFAR experiment).  EASGD EAMSGD  DOWNPOUR MDOWNPOUR  SGD, ASGD, MVASGD  MSGD  η  {0.05, 0.01, 0.005} {0.01, 0.005, 0.001} {0.005, 0.001, 0.0005} {0.05, 0.01, 0.005}  {0.001, 0.0005, 0.0001}  {0.00005, 0.00001, 0.000005}  Table 3: Learning rates explored for each method shown in Figure 4 (ImageNet experiment).  EASGD EAMSGD  DOWNPOUR  η 0.1 0.001  for p = 4: 0.02 for p = 8: 0.01  SGD, ASGD, MVASGD  MSGD  0.05 0.0005  9.4 Comparison of SGD, ASGD, MVASGD and MSGD  Figure 6: Convergence of the training and test loss (negative log-likelihood) and the test error (orig- inal and zoomed) computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the CIFAR experiment.  Figure 7: Convergence of the training and test loss (negative log-likelihood) and the test error (orig- inal and zoomed) computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the ImageNet experiment.  Figure 6 shows the convergence of the training and test loss (negative log-likelihood) and the test error computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the CIFAR experiment. For all CIFAR experiments we always start the averaging for the ADOW N P OU R and ASGD methods from the very beginning of each experiment. For all ImageNet experiments we start the averaging for the ASGD at the same time when we ﬁrst reduce the initial learning rate. Figure 7 shows the convergence of the training and test loss (negative log-likelihood) and the test error computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the ImageNet experiment.  22  501001500.511.52wallclock time (min)training loss (nll)  SGDASGDMVASGDMSGD5010015011.52wallclock time (min)test loss (nll)501001502030405060708090wallclock time (min)test error (%)50100150171819202122wallclock time (min)test error (%)05010015023456wallclock time (hour)training loss (nll)  SGDASGDMVASGDMSGD0501001503456wallclock time (hour)test loss (nll)0501001505060708090wallclock time (hour)test error (%)05010015042444648505254wallclock time (hour)test error (%)9.5 Dependence of the learning rate  This section discusses the dependence of the trade-off between exploration and exploitation on the learning rate. We compare the performance of respectively EAMSGD and EASGD for different learning rates η when p = 16 and τ = 10 on the CIFAR experiment. We observe in Figure 8 that higher learning rates η lead to better test performance for the EAMSGD algorithm which potentially can be justiﬁed by the fact that they sustain higher ﬂuctuations of the local workers. We conjecture that higher ﬂuctuations lead to more exploration and simultaneously they also impose higher reg- ularization. This picture however seems to be opposite for the EASGD algorithm for which larger learning rates hurt the performance of the method and lead to overﬁtting. Interestingly in this ex- periment for both EASGD and EAMSGD algorithm, the learning rate for which the best training performance was achieved simultaneously led to the worst test performance.  Figure 8: Convergence of the training loss (negative log-likelihood, original) and the test error (zoomed) computed for the center variable as a function of wallclock time for EAMSGD and EASGD run with different values of η on the CIFAR experiment. p = 16, τ = 10.  9.6 Dependence of the communication period  This section discusses the dependence of the trade-off between exploration and exploitation on the communication period. We have observed from the CIFAR experiment that EASGD algorithm ex- hibits very similar convergence behavior when τ = 1 up to even τ = 1000, whereas EAMSGD can get trapped at worse energy (loss) level for τ = 100. This behavior of EAMSGD is most likely due to the non-convexity of the objective function. Luckily, it can be avoided by gradually decreasing the learning rate, i.e. increasing the penalty term ρ (recall α = ηρ), as shown in Figure 9. In contrast, the EASGD algorithm does not seem to get trapped at all along its trajectory. The performance of EASGD is less sensitive to increasing the communication period compared to EAMSGD, whereas for the EAMSGD the careful choice of the learning rate for large communication periods seems crucial. Compared to all earlier results, the experiment in this section is re-run three times with a new ran- dom12 seed and with faster cuDNN13 package14. All our methods are implemented in Torch15. The Message Passing Interface implementation MVAPICH216 is used for the GPU-CPU communication.  Figure 9: Convergence of the training loss (negative log-likelihood, original) and the test error (zoomed) computed for the center variable as a function of wallclock time for EASGD and EAMSGD (p = 16, η = 0.01, β = 0.9, δ = 0.99) on the CIFAR experiment with various communication period τ and learning rate decay γ. The learning rate is decreased gradually over time based each local worker’s own clock t with ηt = η/(1 + γt)0.5.  12To clarify, the random initialization we use is by default in Torch’s implementation. 13https://developer.nvidia.com/cuDNN 14https://github.com/soumith/cudnn.torch 15http://torch.ch 16http://mvapich.cse.ohio-state.edu  23  501001500.511.52wallclock time (min)training loss (nll)EAMSGD  0.010.0050.0015010015016182022242628wallclock time (min)test error (%)EAMSGD501001500.511.52wallclock time (min)training loss (nll)EASGD  0.050.010.0055010015016182022242628wallclock time (min)test error (%)EASGD501001502002503000.511.52wallclock time (min)training loss (nll)EASGD  τ=1,γ=0τ=10,γ=0τ=100,γ=0τ=1000,γ=05010015020025030016182022242628wallclock time (min)test error (%)EASGD501001500.511.522.5wallclock time (min)training loss (nll)EAMSGD  τ=10,γ=0τ=100,γ=0τ=100,γ=1e−45010015016182022242628wallclock time (min)test error (%)EAMSGD9.7 Breakdown of the wallclock time  In addition, we report in Table 4 the breakdown of the total running time for EASGD when τ = 10 (the time breakdown for EAMSGD is almost identical) and DOWNPOUR when τ = 1 into computation time, data loading time and parameter communication time. For the CIFAR experiment the reported time corresponds to processing 400 × 128 data samples whereas for the ImageNet experiment it corresponds to processing 1024 × 128 data samples. For τ = 1 and p ∈ {8, 16} we observe that the communication time accounts for signiﬁcant portion of the total running time whereas for τ = 10 the communication time becomes negligible compared to the total running time (recall that based on previous results EASGD and EAMSGD achieve best performance with larger τ which is ideal in the setting when communication is time-consuming).  p = 4  p = 1  p = 8 p = 16 τ = 1 12/1/0 11/2/3 11/2/5 11/2/9 τ = 10 NA 11/2/1 11/2/1 12/2/1  p = 1  p = 4  p = 8  τ = 1 1248/20/0 1323/24/173 1239/61/284 τ = 10 1266/84/11  1254/58/7  NA  Table 4: Approximate computation time, data loading time and parameter communication time [sec] for DOWNPOUR (top line for τ = 1) and EASGD (the time breakdown for EAMSGD is almost identical) (bottom line for τ = 10). Left time corresponds to CIFAR experiment and right table corresponds to ImageNet experiment.  9.8 Time speed-up  In Figure 10 and 11, we summarize the wall clock time needed to achieve the same level of the test error for all the methods in the CIFAR and ImageNet experiment as a function of the number of local workers p. For the CIFAR (Figure 10) we examined the following levels: {21%, 20%, 19%, 18%} and for the ImageNet (Figure 11) we examined: {49%, 47%, 45%, 43%}. If some method does not appear on the ﬁgure for a given test error level, it indicates that this method never achieved this level. For the CIFAR experiment we observe that from among EASGD, DOWNPOUR and MDOWNPOUR methods, the EASGD method needs less time to achieve a particular level of test error. We observe that with higher p each of these methods does not necessarily need less time to achieve the same level of test error. This seems counter intuitive though recall that the learning rate for the methods is selected based on the smallest achievable test error. For larger p smaller learning rates were selected than for smaller p which explains our results. Meanwhile, the EAMSGD method achieves signiﬁcant speed-up over other methods for all the test error levels. For the ImageNet experiment we observe that all methods outperform MSGD and furthermore with p = 4 or p = 8 each of these methods requires less time to achieve the same level of test error. The EAMSGD consistently needs less time than any other method, in particular DOWNPOUR, to achieve any of the test error levels.  Figure 10: The wall clock time needed to achieve the same level of the test error thr as a function of the number of local workers p on the CIFAR dataset. From left to right: thr = {21%, 20%, 19%, 18%}. Missing bars denote that the method never achieved speciﬁed level of test error.  .  Figure 11: The wall clock time needed to achieve the same level of the test error thr as a func- tion of the number of local workers p on the ImageNet dataset. From left to right: thr = {49%, 47%, 45%, 43%}. Missing bars denote that the method never achieved speciﬁed level of test error.  .  24   1 4 816050100150pwallclock time (min)level 21%  MSGDEAMSGDEASGDDOWNPOURMDOWNPOUR 1 4 816050100150level 20%pwallclock time (min) 1 4 816050100150level 19%pwallclock time (min) 1 4 816050100150level 18%pwallclock time (min)148050100150pwallclock time (hour)level 49%  MSGDEAMSGDEASGDDOWNPOUR148050100150level 47%pwallclock time (hour)148050100150level 45%pwallclock time (hour)148050100150level 43%pwallclock time (hour)",
1412.6177,2015, Example Selection For Dictionary Learning,"['Example Selection For Dictionary Learning', 'Tomoki Tsuchida and Garrison Cottrell']",https://arxiv.org/pdf/1412.6177,"5 1 0 2    r a     M 1 3      ]  G L . s c [      3 v 7 7 1 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  EXAMPLE SELECTION FOR DICTIONARY LEARNING  Tomoki Tsuchida & Garrison W. Cottrell Department of Computer Science and Engineering University of California, San Diego 9500 Gilman Drive, Mail Code 0404 La Jolla, CA 92093-0404, USA {ttsuchida,gary}@ucsd.edu  ABSTRACT  In unsupervised learning, an unbiased uniform sampling strategy is typically used, in order that the learned features faithfully encode the statistical structure of the training data. In this work, we explore whether active example selection strategies — algorithms that select which examples to use, based on the current estimate of the features — can accelerate learning. Speciﬁcally, we investigate effects of heuristic and saliency-inspired selection algorithms on the dictionary learning task with sparse activations. We show that some selection algorithms do improve the speed of learning, and we speculate on why they might work.  1  INTRODUCTION  The efﬁcient coding hypothesis, proposed by Barlow (1961), posits that the goal of perceptual sys- tem is to encode the sensory signal in such a way that it is efﬁciently represented. Based on this hypothesis, the past two decades have seen successful computational modeling of low-level percep- tual features based on dictionary learning with sparse codes. The idea is to learn a set of dictionary elements that encode “naturalistic” signals efﬁciently; the learned dictionary might then model the features of early sensory processing. Starting with Olshausen and Field (1996), the dictionary learn- ing task has thus been used extensively to explain early perceptual features. Because the objective of such a learning task is to capture the statistical structure of the observed signals faithfully and efﬁciently, it is an instance of unsupervised learning. As such, the dictionary learning is usually performed using unbiased sampling: the set of data to be used for learning are sampled uniformly from the training dataset. At the same time, the world contains an overabundance of sensory information, requiring organ- isms with limited processing resources to select and process only information relevant for survival Tsotsos (1990). This selection process can be expressed as perceptual action or attentional ﬁlter- ing mechanisms. This might at ﬁrst appear at odds with the goal of the dictionary learning task, since the selection process necessarily biases the set of observed data for the organism. However, the converse is also true: as better (or different) features are learned over the course of learning, the mechanisms for selecting what is relevant may change, even if the selection objective stays the same. If a dictionary learning task is to serve as a realistic algorithmic model of the feature learning process in organisms capable of attentional ﬁltering, this mutual dependency between the dictionary learning and attentional sample selection bias must be taken into consideration. In this work, we examine the effect of such sampling bias on the dictionary learning task. In particu- lar, we explore interactions between learned dictionary elements and example selection algorithms. We investigate whether any selection algorithm can approach, or even improve upon, learning with unbiased sampling strategy. Some of the heuristics we examine also have close relationships to models of attention, suggesting that they can be plausibly implemented by organisms evolving to effectively encode stimuli from their environment.  1  Accepted as a workshop contribution at ICLR 2015  2 DICTIONARY LEARNING Assume that a training set consisting of N P -dimensional signals XN (cid:44) {x(i)}N from a K-element “ground-truth” dictionary set A∗ = [a1a2 ··· aK] under the following model:  i=1 is generated  {s(i)  j  : s(i)  x(i) = A∗s(i) + (cid:15)(i), iid,  j > 0} ∼ Exp(λ) (cid:15)(i) ∼ N (0, Iσ2 (cid:15) )  iid.  (1)  Each signal column vector x(i) is restricted to having exactly k positive activations: s(i) ∈ Cs (cid:44) {s ∈ IRP≥0 : (cid:107)s(cid:107)0 = k}, and each dictionary element is constrained to the unit-norm: A∗ ∈ CA (cid:44) {A : (cid:107)(A)j(cid:107)2 = 1 ∀j}. The goal of dictionary learning is to recover A∗ from XN , assuming λ and (cid:15) are known. To that end, we wish to calculate the maximum a posteriori estimate of A∗, σ2  arg min A∈CA  1 N  min s(i)∈Cs  2σ2 (cid:15)  (cid:107)x(i) − As(i)(cid:107)2  2 + λ(cid:107)s(i)(cid:107)1  .  (2)  This is difﬁcult to calculate, because A and {s(i)}N scheme is to ﬁx one variable and alternately optimize the other, leading to subproblems  i=1 are simultaneously optimized. One practical  (cid:18) 1  N(cid:88)  i=1  (cid:34)  (cid:18) 1  (cid:19)  (cid:19)(cid:35)N  (cid:107)x(i) − ˆAs(i)(cid:107)2  2 + λ(cid:107)s(i)(cid:107)1  ˆS =  arg min s(i)∈Cs  ˆA = arg min A∈CA  1 2N  2σ2 (cid:15) (cid:107)XN − AˆS(cid:107)2 F .  i=1  ,  (3)  (4)  As in the Method of Optimal Directions (MOD) Engan et al. (1999), this alternate optimization scheme is guaranteed to converge to a locally optimal solution for ˆAMAP estimation problem (2). This scheme is also attractive as an algorithmic model of low-level feature learning, since each optimization process can be related to the “analysis” and “synthesis” phases of an autoencoder network Olshausen and Field (1997). In this paper, we henceforth refer to problems (3) and (4) as encoding and updating stages, and their corresponding optimizers as fenc and fupd.  2.1 ENCODING ALGORITHMS  The L0-constrained encoding problem (3) is NP-Hard Elad (2010), and various approximation meth- ods have been extensively studied in the sparse coding literature. One approach is to ignore the L0 constraint and solve the remaining nonnegative L1-regularized least squares problem  LARS : ˆs(i) = arg min  s≥0  (cid:107)x(i) − ˆAs(cid:107)2  2 + λ(cid:48)(cid:107)s(cid:107)1  ,  (5)  (cid:18) 1  2σ2 (cid:15)  (cid:19)  with a larger sparsity penalty λ(cid:48) (cid:44) λP/k to compensate for the lack of the L0 constraint. This (whose mean is 1/λ(cid:48)) is well approximated by works well in practice, since the distribution of s(i) j Exp(λ(cid:48)). For our simulations, we use the Least Angle Regression (LARS) algorithm Duchi et al. (2008) implemented by the SPAMS package Mairal et al. (2010) to solve this. Another approach is to greedily seek nonzero activations to minimize reconstruction errors. The matching pursuit family of algorithms operate on this idea, and they effectively approximate the encoding model  2  Accepted as a workshop contribution at ICLR 2015  (cid:18) 1  2σ2 (cid:15)  OMP :  ˆs(i) = arg min  s≥0  (cid:19)  (cid:107)x(i) − ˆAs(cid:107)2 s.t. (cid:107)s(cid:107)0 ≤ k.  2  (6)  This approximation ignores the L1 penalty, but because nonzero activations are exponentially dis- tributed and mostly small, this approximation is also effective. We use the Orthogonal Matching Pursuit (OMP) algorithm Mallat and Zhang (1993), also implemented by the SPAMS package, for this problem. An even simpler variant of the pursuit-type algorithm is the thresholding Elad (2010) or the k-Sparse algorithm Makhzani and Frey (2013). This algorithm takes the k largest values of ˆA x(i) and sets every other component to zero:  (cid:124)  k-Sparse :  ˆs(i) = supp  { ˆA  (cid:124)  x(i)}  k  (7)  This algorithm is plausibly implemented in a feedforward phase of an autoencoder with a hidden layer that competes horizontally and picks k “winners”. The simplicity of this algorithm is important for our purposes, because we allow the training examples to be selected after the encoding stage, and the encoding algorithm must operate on a much larger number of examples than the updating algorithm. This view also motivated the nonnegative constraint on s(i), because the activations of the hidden layers are likely to be conveyed by nonnegative ﬁring rates.  2.2 DICTIONARY UPDATE ALGORITHM  For the updating stage, we only consider the stochastic gradient update, another simple algorithm F , the gradient is ∇Lrec = for learning. For the reconstruction loss Lrec(A) (cid:44) 1 2(AˆS − X)ˆS (cid:124)  2N (cid:107)XN − AˆS(cid:107)2  /N, yielding the update rule  (cid:124) ˆA ← ˆA − ηt( ˆAˆS − XN )ˆS  (8) Here, ηt is a learning rate that decays inversely with the update epoch t: ηt ∈ Θ(1/t + c). After each update, ˆA is projected back to CA by normalizing each column. Given a set of training examples, this encoding and updating procedure is repeated a small number of times (10 times in our simulations).  /N.  2.3 ACTIVITY EQUALIZATION  One practical issue with this task is that a small number of dictionary elements tend to be assigned to a large number of activations. This produces “the rich get richer” effect: regularly used elements are more often used, and unused elements are left at their initial stages. To avoid this, an activity normalization procedure takes place after the encoding stage. The idea is to modulate all activities, so that the mean activity for each element is closer to the across-element mean of the mean activities; this is done at the cost of increasing the reconstruction error. The equalization is modulated by γ, with γ = 0 corresponding to no equalization and γ = 1 to fully egalitarian equalization (i.e. all elements would have equal mean activities). We use γ = 0.2 for our simulations, which we found empirically to provide a good balance between equalization and reconstruction.  3 EXAMPLE SELECTION ALGORITHMS  To examine the effect of the example selection process on the learning, we extend the alternate optimization scheme in equations (3, 4) to include an example selection stage. In this stage, a selection algorithm picks n (cid:28) N examples to use for the dictionary update (Figure 1). Ideally, the examples are to be chosen in such a way as to make learned dictionary ˆA closer to the ground-truth  3  Accepted as a workshop contribution at ICLR 2015  XN  All signals  encoding  SN  All activations  Example selection  ˆA  Dictionary estimate  updating  Xn, Sn  Selected examples,  activations  Figure 1: The interaction among encoding, selection and updating algorithms.  A∗ compared to the uniform sampling. In the following, we describe a number of heuristic selection algorithms that were inspired by models of attention. We characterize example selection algorithms in two parts. First, there is a choice of goodness measure gj, which is a function that maps (s(i), x(i)) to a number reﬂecting the “goodness” of the instance i for the dictionary element j. Applying gj to {s(i)}N i=1 yields goodness values GN for all k dictionary elements and all N examples. Second, there is a choice of selector function fsel. This function dictates the way a subset of XN is chosen using GN values.  3.1 GOODNESS MEASURES  Of the various goodness measures, we ﬁrst consider  Err :  gj(s(i), x(i)) = (cid:107) ˆAs(i) − x(i)(cid:107)1.  (9)  Err is motivated by the idea of “critical examples” in Zhang (1994), and it favors examples with large reconstruction errors. In our paradigm, the criticality measured by Err may not correspond to ground-truth errors, since it is calculated using current estimate ˆA rather than ground-truth A∗. Another related idea is to select examples that would produce large gradients in the dictionary update equation (8), without regard to their directions. This results in  Grad :  gj(s(i), x(i)) = (cid:107) ˆAs(i) − x(i)(cid:107)1 · s(i) j .  (10)  We note that Grad extends Err by multiplying the reconstruction errors by the activations s(i) therefore prefers examples that are both critical and produce large activations. One observation is that the level of noise puts a fundamental limit on the recovery of true dictionary: better approximation bound is obtained when observation noise is low. It follows that, if we can somehow collect examples that happen to have low noise, learning from those examples might be beneﬁcial. This motivated us to consider  j . It  SNR :  gj(s(i), x(i)) =  (cid:107)x(i)(cid:107)2  2  (cid:107) ˆAs(i) − x(i)(cid:107)2  2  · s(i) j .  (11)  This measure prefers examples with large estimated signal-to-noise ratio (SNR). Another idea focuses on the statistical property of activations s(i), inspired by a model of visual saliency proposed by Zhang et al. (2008). Their saliency model, called the SUN model, asserts that signals that result in rare feature activations are more salient. Speciﬁcally, the model deﬁnes the saliency of a particular visual location to be proportional the self-information of the feature activation, − log P (F = f ). Because we assume nonzero activations are exponentially distributed, this corresponds to  4  Accepted as a workshop contribution at ICLR 2015  SUN :  gj(s(i), x(i)) = s(i) j  (cid:16)∝ − log P (s(i)  j )  (cid:17)  .  (12)  We note that this model is not only simple, but also does not depend on x(i) directly. This makes SUN attractive as a neurally implementable goodness measure. Another saliency-based goodness measure is inspired by the visual saliency map model of Itti et al. (2002):  SalMap :  gj(s(i), x(i)) = SaliencyM ap(x(i)).  (13)  In contrast to the SUN measure, SalMap depends only on x(i). Consequently, SalMap is imper- vious to changes in ˆA. Since the signals in our simulations are small monochrome patches, the “saliency map” we use only has a single-scale intensity channel and an orientation channel with four directions.  3.2 SELECTOR FUNCTIONS  We consider two selector functions. The ﬁrst function chooses top n examples with high goodness values across dictionary elements:  K(cid:88)  BySum :  fsel(GN ) = top n elements of  G(i) j .  (14)  The second selector function,selects examples that are separately “good” for each dictionary ele- ment:  j=1  ByElement : {top n/K elements of G(i)  j  fsel(GN ) = | j ∈ 1...K}.  (15)  This is done by ﬁrst sorting G(i) for each j and then picking top examples in a round-robin fashion, j until N examples are selected. Barring duplicates, this yields a set consisting of top n/k elements of G(i) for each element j. Algorithm 1 describes how these operations take place within each learning j epoch. In our simulations, we consider all possible combinations of the goodness measures and selec- tor functions for the example selection algorithm, except for Err and SalMap. Since these two goodness measures do not produce different values for different dictionary element activations s(i) j , BySum and ByElement functions select equivalent example sets.  4 SIMULATIONS  In order to evaluate example selection algorithms, we present simulations across a variety of dictio- naries and encoding algorithms. Speciﬁcally, we compare results using all three possible encoding models (L0, L1, and k-Sparse) with all eight selection algorithms. Because we generate the training examples from a known ground-truth dictionary A∗, we quantify the integrity of learned dictionary ˆAt at each learning epoch t using the minimal mean square distance  D∗( ˆA, A∗) (cid:44) min  Pπ  1 KP  (cid:107) ˆAtPπ − A∗(cid:107)2 F ,  (16)  5  Accepted as a workshop contribution at ICLR 2015  (a) Gabor dictionary  (b) Alphanumeric dictionary  Figure 2: Ground-truth dictionaries and generated examples XN . Each element / generated example is a 8x8 patch, displayed as a tiled image for the ease of visualization. White is positive and black is negative.  2  K(K−1)  with Pπ spanning all possible permutations. We also investigate the effect of A∗ on the learning. One way to characterize a dictionary set A (cid:124) (cid:80) is its mutual coherence µ(A) (cid:44) maxi(cid:54)=j |a i aj| Elad (2010). This measure is useful in theoretical analysis of recovery bounds Donoho et al. (2006). A more practical characterization is the average (cid:124) i(cid:54)=j |a i aj|. Regardless, exact recovery of the dictionary is more coherence ¯µ(A) (cid:44) challenging when the coherence is high. The ﬁrst dictionary set comprises 100 8x8 Gabor patches (Figure 2a). This dictionary set is inspired by the fact that dictionary learning of natural images leads to such a dictionary Olshausen and Field (1996), and they correspond to simple receptive ﬁelds in mammalian visual cortices Jones and Palmer (1987). With µ(A∗) = 0.97 but ¯µ(A∗) = 0.13, this dictionary set is relatively incoherent, and so the learning problem should be easier. The second dictionary set is composed of 64 8x8 alphanumeric letters with alternating rotations and signs (Figure 2b). This artiﬁcial dictionary set has µ(A∗) = 0.95 with ¯µ(A∗) = 0.341. Within each epoch, 50,000 examples are generated with 5 nonzero activations per example (k = 5), (cid:15) is set so that examples have SNR of ≈ 6 dB. whose magnitudes are sampled from Exp(1). σ2 Each selection algorithm then picks 1% (n = 500) of the training set for the learning. For each experiment, ˆA is initialized with random examples from the training set.  1Both dictionaries violate the recovery bound described in Donoho et al. (2006). Amiri and Haykin (2014) notes that this bound is prone to be violated in practice; as such, we explicitly chose “realistic” parameters that violate the bounds in our simulations.  Algorithm 1 Learning with example selection Initialize random ˆA0 ∈ CA from training examples For t = 1 to max. epochs:  1. Obtain training set XN = {x(i)}N 2. Encode XN : SN = {fenc(x(i); ˆA)}N 3. Select n “good” examples  i=1  i=1  4. Loop 10 times:  i=1  – Calculate GN = {[gj(s(i), x(i))]j=1...k}N – Select n indices: Γ = fsel(GN ) – Sn = {s(i)}i∈Γ, Xn = {x(i)}i∈Γ (a) Encode Xn: Sn ← {fenc(x(i); ˆA)}n (b) Equalize Sn: ∀s(i) ∈ Sn,  i=1  · ( 1  j ← s(i) s(i)  j  i=1 s(i) (c) Update ˆA: ˆA ← ˆA − ηt( ˆASn − Xn)S (cid:124) n/n (d) Normalize columns of ˆA.  i=1 s(i)  j=1  K  j /(cid:80)n  (cid:80)K  (cid:80)n  j )γ  6  Accepted as a workshop contribution at ICLR 2015  4.1 RESULTS Figure 3 shows the average distance of ˆA from A∗ for each learning epoch. We observe that ByElement selection policies generally work well, especially in conjunction with Grad and SUN goodness measures. This trend is especially noticeable for the alphanumeric dictionary case, where most of the BySum-selectors perform worse than the baseline selector that chooses examples ran- domly (Uniform). The ranking of the selector algorithms is roughly consistent across the learning epochs (Figure 3, left column), and it is also robust with the choice of the encoding algorithms (Figure 3, right column). In particular, good selector algorithms are beneﬁcial even at the relatively early stages of learning (< 100 epochs, for instance), in contrast to the simulation in Amiri and Haykin (2014). This is surprising, because at early stages of learning, poor ˆA estimates result in bad activation estimates as well. Nevertheless, good selector algorithms soon establish a positive feedback loop for both dictionary and activation estimates. One interesting exception is the SalMap selector. It works relatively well for Gabor dictionary (and closely tracks the SUNBySum selector), but not for the alphanumeric dictionary. This is presumably due to the design of the SalMap model: because the model uses oriented Gabor ﬁlters as one of its feature maps, the overall effect is similar to the SUNBySum algorithm when the signals are generated from Gabor dictionaries.  0.015  0.01  0.005  )  ∗  A  ,  ˆA (  ∗  D  ErrBySum  Uniform SNRBySum GradBySum  SalMap  SUNBySum  GradByElement SNRByElement SUNByElement  0  0  200  400  600  800  1000  LARS  OMP k-Sparse  Epochs  (a) Gabor dictionary  0.015  0.01  0.005  )  ∗  A  ,  ˆA (  ∗  D  SalMap  ErrBySum  Uniform SNRBySum GradBySum SUNBySum  SNRByElement SUNByElement GradByElement  0  0  200  400  600  800  1000  LARS  OMP k-Sparse  Epochs  (b) Alphanumeric dictionary  Figure 3: Distance from true dictionaries. Graphs on the left column show the time course of the learning using the LARS encoding. The legends are ordered from worst to best at the end of the simulation (1000 epochs). Graphs on the right column compares the performance of different encoding models. The ordinate is the distance at the end, in the same scale as the left graphs.  4.2 ROBUSTNESS  In order to assess the robustness of the example selection algorithms, we repeated the Gabor dictio- nary simulation across a range of parameter values. Speciﬁcally, we experimented with modifying the following parameters one at a time, starting from the original parameter values:  7  Accepted as a workshop contribution at ICLR 2015  • The signal-to-noise ratio (10 log10(2λ2/σ2 • The number of nonzero elements in the generated examples (k) • The ratio of selected examples to the original training set (n/N) • The number of dictionary elements (K)  (cid:15) ) [dB])  Figure 4 shows the result of these simulations. These results show that good selector algorithms improve learning across a wide range of parameter values. Of note is the number of dictionary elements K, whose results suggest that the improvement is greatest for the “complete” dictio- nary learning cases; the advantage of selection appears to diminish for extremely over-complete (or under-complete) dictionary learning tasks.  0.015  )  0.015  )  ∗  A ˆA  ,  (  ∗  D  0.01  0.005  0 −18  ∗  A ˆA  ,  (  ∗  D  0.01  0.005  −16 −14 SNR [dB]  −12  0  2  4  8  10  6  k  ErrBySum  Uniform SNRBySum GradBySum  SalMap  GradByElement  SUNBySum  SNRByElement SUNByElement  0.015  )  0.015  )  ∗  A ˆA  ,  (  ∗  D  0.01  0.005  0 0.01  ∗  A ˆA  ,  (  ∗  D  1  0.1  n/N  0.01  0.005  0  8  16  32  64  128 256  K  Figure 4: Distances from the true dictionaries for different model parameters, using the LARS en- coding.  5 DISCUSSION  In this work, we examined the effect of selection algorithms on the dictionary learning based on stochastic gradient descent. Simulations using training examples generated from known dictionaries revealed that some selection algorithms do indeed improve learning, in the sense that the learned dictionaries are closer to the known dictionaries throughout the learning epochs. Of special note is the success of SUN selectors; since these selectors are very simple, they hold promise for more general learning applications. Few studies have so far investigated example selection strategies for the dictionary learning task, although some learning algorithms contain such procedures implicitly. For instance, K-SVD Aharon et al. (2006) relies upon identifying a group of examples that use a particular dictionary element during its update stage. The algorithm in Arora et al. (2013) also makes use of a sophisticated example grouping procedure to provably recover dictionaries. In both cases, though, the focus is on breaking the inter-dependency between ˆA and ˆS, instead of characterizing how some algorithms – notably those of the perceptual systems – might improve learning despite this inter-dependency. One recent paper that does consider example selection on its own is (Amiri and Haykin, 2014), whose cognit algorithm is explicitly related to perceptual attention. The point that differentiates this work lies in the generative assumption: cognit relies on having additional information avail- able to the learner, in their case the temporal contiguity of the generative process. With a spatially and temporally independent generation process, the generative model we considered here is simpler but more difﬁcult to solve. Why do selection algorithms improve learning at all? At ﬁrst glance, one may assume that any non-uniform sampling would skew the apparent distribution D(Xn) from the true distribution of  8  Accepted as a workshop contribution at ICLR 2015  (a) Gabor dictionary  (b) Alphanumeric dictionary  Figure 5: Characterization of Xn. Left columns: SNR of Xn (higher is better). Right columns: D(D(Xn)||D(XN )) (lower is better).  the training set D(XN ), and thus lead to learning of an incorrect dictionary. However, as we have empirically shown, this is not the case. One intuitive reason – one that also underlies the design of the SNR selectors – is that “good” selection algorithms picks samples with high information content. For instance, samples with close to zero activation content provide little information about the dictionary elements that compose them, even though such samples abound under our generative model with exponentially-distributed activations. It follows that such samples provide little beneﬁt to the inference of the statistical structure of the training set, and the learner would be well-advised to discard them. To validate this, we calculated the (true) SNR of Xn at the last epoch of the learning for each selection algorithm (Figure 5, left columns). This shows that all selection algorithms picked Xn with much higher SNR than Uniform. However, the correlation between the overall performance ranking and SNR is weak, suggesting that this is not the only factor driving good example selection. Another factor that contributes to good learning is the spread of examples within Xn. Casual obser- vation revealed that the BySum selector is prone to picking similar examples, whereas ByElement selects a larger variety of examples and thus retains the distribution of XN more faithfully. To quantify this, we measured the distance of the distribution of selected examples, D(Xn), from that of all training examples, D(XN ), using the histogram intersection distanceRubner et al. (2000). The right columns of Figure 5 shows that this distance, D(D(Xn)||D(XN )), tends to be lower for ByElement selectors (solid lines) than BySum selectors (dashed lines). Like the SNR measure, however, this quantity itself is only weakly predictive of the overall performance, suggesting that it is important to pick a large variety of high-SNR examples for the dictionary learning task. There are several directions to which we plan to extend this work. One is the theoretical analysis of the selection algorithms. For instance, we did not explore under what conditions learning with example selection leads to the same solutions as an unbiased learning, although empirically we ob- served that to be the case. As in the curriculum learning paradigm Bengio et al. (2009), it is also possible that different selection algorithms are better suited at different stages of learning. Another is to apply the active example selection processes to hierarchical architectures such as stacked au- toencoders and Restricted Boltzmann Machines. In these cases, an interesting question arises as to how information from each layer should be combined to make the selection decision. We intend to explore some of these questions in the future using learning tasks similar to this work.  REFERENCES  Michal Aharon, Michael Elad, and Alfred Bruckstein. K-SVD: An Algorithm for Designing Over- complete Dictionaries for Sparse Representation. Signal Processing, IEEE Transactions on, 54 (11):4311–4322, 2006.  Ashkan Amiri and Simon Haykin.  Improved Sparse Coding Under the Inﬂuence of Perceptual  Attention. Neural Computation, 26(2):377–420, February 2014.  9  SUNByElementSNRByElementGradByElementSUNBySumSalMapGradBySumSNRBySumUniformErrBySum0.02.04.06.08.010.012.014.016.0SNR of Xn [dB]SUNByElementSNRByElementGradByElementSUNBySumSalMapGradBySumSNRBySumUniformErrBySum0.00.10.20.30.40.50.6D(D(Xn)||D(XN))GradByElementSUNByElementSNRByElementSUNBySumGradBySumSNRBySumUniformErrBySumSalMap0.02.04.06.08.010.012.014.016.0SNR of Xn [dB]GradByElementSUNByElementSNRByElementSUNBySumGradBySumSNRBySumUniformErrBySumSalMap0.00.10.20.30.40.50.60.7D(D(Xn)||D(XN))Accepted as a workshop contribution at ICLR 2015  Sanjeev Arora, Rong Ge, and Ankur Moitra. New Algorithms for Learning Incoherent and Over-  complete Dictionaries. arXiv.org, August 2013.  Horace B Barlow. Possible Principles Underlying the Transformations of Sensory Messages. Sen-  sory Communication, pages 217–234, 1961.  Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41–48. ACM, 2009.  David L Donoho, Michael Elad, and Vladimir Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, 52(1):6–18, 2006.  John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efﬁcient projections onto the l1-ball for learning in high dimensions. In Proceedings of the International Conference on Machine Learning (ICML), 2008.  Michael Elad. Sparse and Redundant Representations: From Theory to Applications in Signal and  Image Processing. Springer, 2010.  Kjersti Engan, Sven Ole Aase, and John Hakon Husoy. Method of optimal directions for frame In 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing,  design. pages 2443–2446, 1999.  Laurent Itti, Christof Koch, and Eiebur Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(11):1254– 1259, 2002.  Judson P Jones and Larry A Palmer. An evaluation of the two-dimensional Gabor ﬁlter model of simple receptive ﬁelds in cat striate cortex. Journal of Neurophysiology, 58(6):1233–1258, 1987.  Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factor-  ization and sparse coding. The Journal of Machine Learning Research, 11:19–60, 2010.  Alireza Makhzani and Brendan Frey. k-Sparse Autoencoders. arXiv.org, December 2013.  Stephane G Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries. IEEE  Transactions on Signal Processing, 41(12):3397–3415, 1993.  Bruno A Olshausen and David J Field. Emergence of simple-cell receptive ﬁeld properties by  learning a sparse code for natural images. Nature, 381(6583):607–609, 1996.  Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy  employed by V1? Vision Research, 37(23):3311–3325, 1997.  Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric for  image retrieval. International Journal of Computer Vision, 40(2):99–121, 2000.  John K Tsotsos. Analyzing vision at the complexity level. Behav Brain Sci, 13(3):423–469, 1990.  Byoung-Tak Zhang. Accelerated learning by active example selection.  Neural Systems, 5(1):67–76, 1994.  International Journal of  Lingyun Zhang, Matthew H. Tong, Tim K Marks, Honghao Shan, and Garrison W Cottrell. SUN:  A Bayesian framework for saliency using natural statistics. Journal of Vision, 8(7), 2008.  10  ",
1412.6618,2015, Permutohedral Lattice CNNs,"['Permutohedral Lattice CNNs', 'Martin Kiefel', 'Varun Jampani', 'and Peter Gehler']",https://arxiv.org/pdf/1412.6618,"5 1 0 2     y a M 3         ]  V C . s c [      3 v 8 1 6 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  PERMUTOHEDRAL LATTICE CNNS  Martin Kiefel, Varun Jampani and Peter V. Gehler Max Planck Institute for Intelligent Systems T¨ubingen, 72076, Germany {martin.kiefel, varun.jampani, peter.gehler}@tuebingen.mpg.de  ABSTRACT  This paper presents a convolutional layer that is able to process sparse input fea- tures. As an example, for image recognition problems this allows an efﬁcient ﬁltering of signals that do not lie on a dense grid (like pixel position), but of more general features (such as color values). The presented algorithm makes use of the permutohedral lattice data structure. The permutohedral lattice was introduced to efﬁciently implement a bilateral ﬁlter, a commonly used image processing opera- tion. Its use allows for a generalization of the convolution type found in current (spatial) convolutional network architectures.  1  INTRODUCTION  In the domain of image recognition, the convolutional layer of a CNN today is almost exclusively associated with a spatial convolution in the image domain. In this work we will take a more signal theoretic viewpoint of the convolutional operation and present an algorithm that allows to process also sparse input data. This work is inspired by the use of special data structures (Adams et al., 2010) for bilateral ﬁlters (Aurich & Weule, 1995; Smith & Brady, 1997; Tomasi & Roberto, 1998) and generalizes it for the use of convolutional architectures.  Although the approach presented here is more general, the following two scenarios are instructive. Consider that at training time we have access to full resolution images to train a classiﬁer. At test time only a random number of pixels from the test image is available. In other words, we sample the signal differently during training and test time. For a traditional CNN this would require a pre- processing step, for example to map from subsets of pixels to a dense grid that is the image. In our view there is no change, it is not required that we have a dense grid and access to all pixels of the image. That is the integration domain does not change. This is one example of sparsity, here we deal with a set of pixels, whose values are RGB and features are position. Similarly, color information can be used to deﬁne the ﬁltering operation as well. One can devise a convolution with a domain respecting color and location information (or color alone). One view from the image processing community is that of an edge-aware ﬁlter, the ﬁlter will be adaptive to the color and/or gradient of the image. RGB values do not lie on a regular dense grid, therefore a direct expansion of the spatial convolution is not applicable.  This approach falls into line with the view on encoding invariants (Mallat, 2012). It is possible to encode our knowledge invariants that we have about the data with the new way of looking at the data. Encoded in a spatial convolution is the prior knowledge about translation invariance. How to encode roation invariance, how similarity in color space? In the view we take here these are simply convolutions over different domains. A grid based convolution cannot easily be used to work with the sparse data (an interpolation might be needed) but the permutohedral lattice provides the right space and allows efﬁcient implementations. Therefore the runtime is comparable to the ones of spatial convolutions, depending on the size of the invariants to include and can simply be used as a replacement of the traditional layers.  2 PERMUTOHEDRAL LATTICE CONVOLUTION  We propose a convolution operation of a d-dimensional input space that entirely works on a lattice. Input data is a tuple (fi, vi) of feature locations fi ∈ Rd and corresponding signal values vi ∈  1  Accepted as a workshop contribution at ICLR 2015  Splat  Convolve  Slice  Figure 1: The permutohedral convolution consists of three steps: ﬁrst the samples are splatted onto the lattice, then a convolution operates on the lattice considering a margin of s = 2 neighbors of a node, and ﬁnally the result of the convolution is transformed back to the output samples.  R. Importantly, this does not assume the feature locations fi to be sampled on a regular grid, for example fi can be position and RGB value. We then map the input signal to a regular structure, the so-called permutohedral lattice. A convolution then operates on the constructed lattice and the result is mapped back to the output space. Hence, the entire operation consists of three stages (see ﬁg. 1): splat (the mapping to the lattice space), convolution and slice (the mapping back from the lattice). This strategy has already been used to implement fast Gaussian ﬁltering (Paris & Durand, 2009; Adams et al., 2010; 2009). Here we generalize it to arbitrary convolutions.  The permutohedral lattice is the result of the projection of the set Zd+1 onto a plane deﬁned by its orthogonal vector 1 ∈ Rd+1. This d dimensional plane is embedded into Rd+1. The lattice points tessellate the subspace with regular cells. Given a point from the embedding space, it is efﬁcient to ﬁnd the enclosing simplex of the projection onto the plane. We will represent a sparse set of points from Rd by a sparse set of simplex corners in the lattice. Importantly, the number of corners does not grow exponentially with the dimension d as it would for an axis-align simplex representation. We continue to describe the different parts of the permutohedral convolution.  The splat and slice operations take the role of an interpolation between the different signal repre- sentations. All input samples that belong to a cell adjacent to lattice point j are summed up and weighted with the barycentric coordinates to calculate the value lj = Pi∈C(j) bi,jvi. This is the splatting operation. The barycentric coordinates bi,j depend on both the corner point j and the feature location fi. The reverse operation slice ﬁnds an output value v′ k by using its barycentric coordinates inside the lattice simplex and sums over the corner points v′  j. k = Pj∈C(k) bk,jl′  The convolution is then performed on the permutohedral lattice. It uses a convolution kernel wn to j ′ = P(n,j)∈N (j ′) wnlj. The convolution kernel wn is problem speciﬁc and its domain compute l′ is restricted to the set of neighboring lattice points N (j). For bilateral ﬁlters, this is set to be a Gaussian ﬁlter, here we learn the kernel values using back-propagation.  The size of the neighborhood takes a similar role as the ﬁlter size (spatial extent) of the grid-based CNN. A transitional convolutional kernel which considers s sampled points to either side has (2s + 1)d ∈ O(sd) parameters. A comparable ﬁlter on the permutohedral lattice with a s neighborhood has (s + 1)d+1 − sd+1 ∈ O(sd) elements.  3 SPARSE CNNS AND ENCODING INVARIANTS  The permutohedral convolution can be used as a new building block in a CNN architecture. We will omit the derivation of the gradients for the ﬁlter elements with respect to the output of such a new layer due to space constraints. We will discuss two possible application scenarios.  First, as mentioned before we are free to change the sampling of the input signal of a lattice-based convolution. The choice of the sampling is problem speciﬁc. Missing measurements or domain speciﬁc sampling techniques that gather more information in highly discriminant areas are only two possible scenarios. Furthermore, as we will show in our experiments the method is robust in cases where train-time sampling and test-time sampling do not coincide.  Second, the proposed method provides a tool to encode additional data invariances in a principled way. A common technique to include domain knowledge is to artiﬁcially augment the training set  2  Accepted as a workshop contribution at ICLR 2015  Test Subsampling  Original Method 0.9919 LeNet PCNN 0.9903 LeNet 100% 0.9856 PCNN 100% 0.9900 0.9848 LeNet 60% 0.9885 PCNN 60% 0.9763 LeNet 20% PCNN 20% 0.9728  100% 0.9660 0.9844 0.9809 0.9863 0.9821 0.9864 0.9754 0.9735  60% 0.9348 0.9534 0.9678 0.9699 0.9740 0.9771 0.9695 0.9701  20% 0.6434 0.5767 0.7386 0.6910 0.8151 0.8214 0.8928 0.9042  Method Noisy Input CNN PCNN Gauss PCNN Trained PCNN + CNN Trained  PSNR  20.17  26.27  26.51  26.58  26.65  (b) Denoising  (a) Random Sampling  Table 1: (a) Classiﬁcation accuracy on MNIST. We compare the LeNet (LeCun et al., 1998) im- plementation that is part of Caffe (Jia et al., 2014) to the network with the ﬁrst layer replaced by a permutohedral convolution layer (PCNN). Both are trained on the original image resolution (ﬁrst two rows). Three more PCNN and CNN models are trained with randomly subsampled images (100%, 60% and 20% of the pixels). An additional bilinear interpolation layer samples the input signal on a spatial grid for the CNN model. (b) PSNR results of a denoising task using the BSDS500 dataset (Arbelez et al., 2011).  with deformations that leave the output signal invariant, such as translations, rotations, or nosiy versions. A feature mapping Φ is invariant with respect to a transformation L and a signal v if Φ(v) ≈ Φ(vL). In the case where L belongs to a set of translations a possible invariant feature is the convolution with a window function w (given its support has the right size) Φ(v, s) = R w(t)v(s − t)dt. The same idea can be applied to the more general case and again calculating a mean with the help of a window function: Φ(v, L) = R w(M )v(M −1L)dM . We can use the permutohedral convolution to encode invariances like rotation and translation. Ap- proximating the above integral by a ﬁnite sum and using lattice points as integration samples we arrive at Φ(v, L) ≈ Plattice: M wM v(M −1L). We further approximate v(M −1L) with look-up at a lattice point location.  Consider the case of rotation and translation invariance. More intuitively, we stack rotated versions of the input images onto each other in a 3 dimensional space – 2 dimensions for the location of a sample and 1 dimension for the rotation of the image. A grid-based convolution would not work here because the rotated image points might not coincide with a grid anymore. Filtering in the permutohedral space naturally respects the augmented feature space.  4 EXPERIMENTS  We investigate the performance and ﬂexibility of the proposed method on two sets of experiments. The ﬁrst setup compares the permutohedral convolution with a spatial convolution that has been combined with a bilinear interpolation. The second part adds a denoising experiment to show the modelling strength of the permutohedral convolution.  It is natural to ask, how a spatial convolution combined with an interpolation compares to a permu- tohedral convolutional neural network (PCNN). The proposed convolution is particularly advanta- geous in cases where samples are addressed in a higher dimensional space. Nevertheless, a bilinear interpolation prior to a spatial convolution can be used for dense 2-dimensional positional features.  We take a reference implementation of LeNet (LeCun et al., 1998) that is part of the caffe project (Jia et al., 2014) on the MNIST dataset as a starting point for the following experiments. The permutohedral convolutional layer is also implemented in this framework.  We ﬁrst compare the LeNet in terms of test-time accuracy when substituting only the ﬁrst of the convolutional layers with a (position only) permutohedral layer and leave the rest identical. Table 1a shows that a similar performance is achieved, so it seems model ﬂexibility is not lost. The network is  3  Accepted as a workshop contribution at ICLR 2015  trained according to the training parameters from the reference implementation. Next, we randomly sample continuous points in the input image, use their interpolated values as signal and continuous positions as features. Interestingly, we can train models with a different amount of sub-sampling than at test time. The permutohedral representation is robust with respect to this sparse input signal. Table 1a shows experiments with different signal degradation levels. All the sampling strategies have in common that the original input space of 28 by 28 pixels is densely covered. Hence, a bilinear interpolation prior to the ﬁrst convolution allows us to compare against the original LeNet architecture. This baseline model performs similar to a PCNN.  One of the strengths of the proposed method is that it does not depend on a regular grid sampling as the tranditional convolution operators. We highlight this feature with the following denoising exper- iment and change the sampling space to be both sparse and 3-dimensional. The higher dimensional space renders a bilinear interpolation and spatial convolution more and more in-feasible due to the high number of corners of the hyper-cubical tessellation of the space. We compare the proposed per- mutohedral convolution in an illustrative denoising experiment to a spatial convolution. For bilateral ﬁltering, which is one of the algorithmic use-cases of the permutohedral lattice, the input space fea- tures contain both the coordinates of a data sample and the color information of the image; hence a 5-dimensional vector for color images and a 3-dimensional vector for gray-scale images. In contrast to a direct application of a bilateral convolution to the noisy input the ﬁlter for a bilateral layer of a PCNN can now be trained. All experiments compare the performance of a PCNN to a common CNN with images from the BSDS500 dataset (Arbelez et al., 2011). Each image is transformed into gray-scale by taking the mean across channels and noise is artiﬁcially added to it with samples from a Gaussian distribution N (µ, σ2), µ = 0, σ = 25 255 . The baseline network uses a spatial convolution (“CNN” in Table 1b) with a kernel size of 5 and predicts the scalar gray-scale value at each pixel (25 ﬁlter weights). The layer is trained with a ﬁxed learning rate of 10−3, momentum weight of 0.9 and a weight decay of 5 · 10−4 on the “train” set. In the second architecture the convolution is performed on the permutohedral lattice (“PCNN Gauss” and “PCNN Trained” in Table 1b). We include the pixel’s gray value as an additional feature for the generalized operation and set the neighborhood size to 2 (65 ﬁlter weights). The ﬁlter weights are initialized with a Gaussian blur and are either applied directly to the noisy input (“PCNN Gauss”) or trained on the “train” set to minimize the Euclidean distance to the clean image with a learning rate of 0.1. We cross-validate the scaling of the input space on the “val” image set and reuse this setting for all experiments that operate on the permutohedral lattice. A third architecture that combines both spatial and permutohedral convolutions by summation (“CNN + PCNN”) is similarly trained and tested.  We evaluate the PSNR utility averaged over the images from the “test” set and see a slightly better performance of the bilateral network (“PCNN trained”) with trained ﬁlters in comparison to a bilat- eral ﬁlter (“PCNN Gauss”) and linear ﬁlter (“CNN”), see Table 1b. Both convolutional operations combined further improve the performance and suggest that they have complementary strengths. Admittedly this setup is rather simple, but it validates that the generalized ﬁltering has an advantage.  In the future we plan to investigate the use of the PCNN architecture for other computer vision prob- lems, e.g. semantic segmentation, and modeling domain knowledge like rotation or scale invariance.  5 CONCLUSION  This paper presents a generalization of the convolutional operation to sparse input signals. We envision many consequences of this work. Consider signals that are naturally represented as mea- surements instead of images, like MRT scan readings. The permutohedral lattice ﬁltering avoids the pre-processing assembling operation into a dense image, it is possible to work on the measured sparse signal directly. Another promising use of this ﬁlter is to encode scale invariance, typically this is encoded by presenting multiple scaled versions of an image to several branches of a network. The convolution presented here can be deﬁned on the continuous range of image scales without a ﬁnite subselection. In summary, this technique allows to encode prior knowledge about the observed signal to deﬁne the domain of the convolution. The typical spatial ﬁlter of CNNs is a particular type of prior knowledge, we generalize this to sparse signals.  4  Accepted as a workshop contribution at ICLR 2015  REFERENCES Adams, Andrew, Gelfand, Natasha, Dolson, Jennifer, and Levoy, Marc. Gaussian kd-trees for fast high-dimensional ﬁltering. In ACM SIGGRAPH 2009 Papers, SIGGRAPH ’09, pp. 21:1–21:12, New York, NY, USA, 2009.  Adams, Andrew, Baek, Jongmin, and Davis, Myers Abraham. Fast high-dimensional ﬁltering using  the permutohedral lattice. Comput. Graph. Forum, 29(2):753–762, 2010.  Arbelez, Pablo, Maire, Michael, Fowlkes, Charless, and Malik, Jitendra. Contour detection and hierarchical image segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 33(5):898–916, May 2011.  Aurich, Volker and Weule, J¨org. Non-linear Gaussian ﬁlters performing edge preserving diffusion. In Mustererkennung 1995, 17. DAGM-Symposium, Bielefeld, 13.-15. September 1995, Proceed- ings, pp. 538–545, 1995.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998.  Mallat, St´ephane. Group invariant scattering. Communications in Pure and Applied Mathematics,  65(10):1331–1398, 2012.  Paris, Sylvain and Durand, Fr´edo. A fast approximation of the bilateral ﬁlter using a signal process-  ing approach. International Journal of Compututer Vision, 81(1):24–52, January 2009.  Smith, Stephen M. and Brady, J. Michael. SUSAN – a new approach to low level image processing.  Int. J. Comput. Vision, 23(1):45–78, May 1997. ISSN 0920-5691.  Tomasi, Carlo and Roberto, Manduchi. Bilateral ﬁltering for gray and color images. In Proceedings of the Sixth International Conference on Computer Vision, ICCV ’98, pp. 839–846, Washington, DC, USA, 1998. IEEE Computer Society.  5  ",
1412.4385,2015, Unsupervised Domain Adaptation with Feature Embeddings,"['Unsupervised Domain Adaptation with Feature Embeddings', 'Yi Yang and Jacob Eisenstein']",https://arxiv.org/pdf/1412.4385,"5 1 0 2    r p A 6 1         ] L C . s c [      3 v 5 8 3 4  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  UNSUPERVISED DOMAIN ADAPTATION WITH FEA- TURE EMBEDDINGS  Yi Yang & Jacob Eisenstein School of Interactive Computing Georgia Institute of Technology {yiyang, jacobe}@gatech.edu  ABSTRACT  Representation learning is the dominant technique for unsupervised domain adap- tation, but existing approaches often require the speciﬁcation of “pivot features” that generalize across domains, which are selected by task-speciﬁc heuristics. We show that a novel but simple feature embedding approach provides better perfor- mance, by exploiting the feature template structure common in NLP problems.  1  INTRODUCTION  Domain adaptation is crucial if natural language processing is to be successfully employed in high- impact application areas such as social media, patient medical records, and historical texts. Un- supervised domain adaptation is particularly appealing, since it requires no labeled data in the target domain. Some of the most successful approaches to unsupervised domain adaptation are based on representation learning: transforming sparse high-dimensional surface features into dense vector representations, which are often more robust to domain shift (Blitzer et al., 2006; Glorot et al., 2011). However, these methods are computationally expensive, and often require special task-speciﬁc heuristics to select good “pivot features”. We present FEMA (Feature EMbeddings for domain Adaptation), a novel representation learning approach for domain adaptation in structured feature spaces. Like prior work in representation learning, FEMA learns dense features that are more robust to domain shift. However, FEMA diverges from previous approaches based on reconstructing pivot features; instead, it uses techniques from neural language models to directly obtain low-dimensional embeddings. FEMA outperforms prior work on adapting POS tagging from the Penn Treebank to web text.  2 LEARNING FEATURE EMBEDDINGS  Feature co-occurrence statistics are the primary source of information driving many unsupervised methods for domain adaptation. For example, both Structural Correspondence Learning (SCL; Blitzer et al., 2006) and Denoising Autoencoders (Chen et al., 2012) learn to reconstruct a sub- set of “pivot features”, as shown in Figure 1(a). The reconstruction function is then employed to project each instance into a dense representation, which will hopefully be better suited to cross- domain generalization. The pivot features are chosen to be both predictive of the label and general across domains. Meeting these two criteria requires task-speciﬁc heuristics. Furthermore, the pivot features correspond to a small subspace of the feature co-occurrence matrix. We face a tradeoff between the amount of feature co-occurrence information that we can use, and the computational complexity for representation learning and downstream training. We avoid this tradeoff by inducing low dimensional feature embeddings directly. We exploit the tendency of many NLP tasks to divide features into templates, with exactly one active feature per template (Smith, 2011); this is shown in the center of Figure 1. Rather than treating each instance as an undifferentiated bag-of-features, we exploit this template structure to induce feature embeddings: dense representations of individual features. Each embedding is selected to help predict the features that ﬁll out the other templates; see Figure 1(b). The embeddings for each active feature are then concatenated together across templates, giving a dense representation for the entire instance.  1  Accepted as a workshop contribution at ICLR 2015  Figure 1: Representation learning techniques in structured feature spaces  (cid:105)  T(cid:88)  T(cid:88)  (cid:104)  t=1  t(cid:48)(cid:54)=t  Our feature embeddings are based on the skip-gram model, trained with negative sampling (SGNS; Mikolov et al., 2013), which is a simple yet efﬁcient method for learning word embeddings. The training objective is to ﬁnd feature embeddings that are useful for predicting other active features in the instance. For the instance n ∈ {1 . . . N} and feature template t ∈ {1 . . . T}, we denote fn(t) as the index of the active feature; for example, in the instance shown in Figure 1, fn(t) = ‘new’ when t indicates the previous-word template. The skip-gram approach induces distinct “input” and “output” embeddings for each feature, written ufn(t) and vfn(t), respectively. The role of these embeddings can be seen in the negative sampling objective,  (cid:96)n =  1 T  log σ(u(cid:62)  fn(t)vfn(t(cid:48))) + kE  log σ(−u(cid:62)  fn(t)vi)  i∼P (n) t(cid:48)  (1)  where t and t(cid:48) are feature templates, k is the number of negative samples, P (n) t(cid:48) for template t(cid:48), and σ is the sigmoid function. Feature embeddings can be applied to domain adaptation by learning embeddings of all features on the union of the source and target data sets. The dense feature vector for each instance is obtained by concatenating the feature embeddings for each template. Finally, since it has been shown that non- linearity is important for generating robust representations (Bengio et al., 2013), we follow (Chen et al., 2012) and apply the hyperbolic tangent function to the embeddings. The augmented rep- resentation x(aug) of instance n is the concatenation of the original feature vector and the feature embeddings: x(aug)  n = xn ⊕ tanh[ufn(1) ⊕ ··· ⊕ ufn(T )], where ⊕ is vector concatenation.  is a noise distribution  n  3 EXPERIMENTS  We evaluate FEMA on part-of-speech (POS) tagging: adaptation of English POS tagging from news text to web text, as in the SANCL shared task (Petrov & McDonald, 2012).  3.1 EXPERIMENT SETUP  Datasets We use data from the SANCL shared task (Petrov & McDonald, 2012), which contains several web-related corpora (newsgroups, reviews, weblogs, answers, emails) as well as the WSJ portion of OntoNotes corpus (Hovy et al., 2006). Following Schnabel & Sch¨utze (2014), we use sections 02-21 of WSJ for training and section 22 for development, and use 100,000 unlabeled WSJ sentences from 1988 for learning representations. On the web text side, each of the ﬁve target domains has an unlabeled training set of 100,000 sentences, along with development and test sets of about 1000 labeled sentences each.  SVM tagger While POS tagging is classically treated as a structured prediction problem, we fol- low Schnabel & Sch¨utze (2014) by taking a classiﬁcation-based approach. Speciﬁcally, we apply a support vector machine (SVM) classiﬁer, adding dense features from FEMA (and the alternative representation learning techniques) to the set of basic features. We apply sixteen basic feature tem- plates introduced by Ratnaparkhi et al. (1996). Feature embeddings are learned for all lexical and afﬁx features, yielding a total of thirteen embeddings per instance.  2  Accepted as a workshop contribution at ICLR 2015  Target  NEWSGROUPS REVIEWS WEBLOGS ANSWERS EMAILS  baseline MEMM SCL 89.33 88.56 91.53 91.02 94.28 93.67 89.05 89.56 88.42 88.12  89.11 91.43 94.15 88.92 88.68  mDA word2vec 89.87 91.96 94.18 90.06 88.71  89.70 91.70 94.17 89.83 88.51  FLORS 90.86 92.95 94.71 90.30 89.44  FEMA 91.26 92.82 94.95 90.69 89.72  Table 1: Accuracy results for adaptation from WSJ to Web Text on SANCL dev set.  Target  NEWSGROUPS REVIEWS WEBLOGS ANSWERS EMAILS  baseline MEMM SCL 91.51 91.02 90.29 89.79 91.85 92.32 90.04 89.52 87.45 88.04  91.25 90.30 92.32 89.74 87.77  mDA word2vec 91.83 90.95 92.39 90.61 88.11  91.35 90.87 92.42 90.48 88.28  FLORS 92.41 92.25 93.14 91.17 88.67  FEMA 92.60 92.15 93.43 91.35 89.02  Table 2: Accuracy results for adaptation from WSJ to Web Text on SANCL test set.  Competitive systems We consider two competitive unsupervised domain adaptation methods that require pivot features: Structural Correspondence Learning (SCL; Blitzer et al., 2006) and marginal- ized Denoising Autoencoders (mDA; Chen et al, 2012). We use structured dropout noise for mDA (Yang & Eisenstein, 2014). We also directly compare with WORD2VEC1 word embeddings, and with a baseline approach in which we simply train on the source domain data using the surface features, and then test on the target domain. Aside from our own implemented methods, we compare against published results of FLORS (Schnabel & Sch¨utze, 2014), which uses distributional features for domain adaptation. We also republish the results of Schnabel and Schutze using the Stanford POS Tagger, a maximum entropy Markov model (MEMM) tagger.  Parameter tuning All the hyper-parameters are tuned on development data. Following Blitzer et al. (2006), we consider 6918 pivot features that appear more than 50 times in all the domains for SCL and mDA. The best parameters for SCL are dimensionality K = 50 and rescale factor α = 5. For both FEMA and word2vec, the best embedding size is 100 and the best number of negative samples is 5. The noise distribution P (n) is simply the unigram probability of each feature in the template t. Mikolov et al. (2013b) argue for exponentiating the unigram distribution, but we ﬁnd it makes little difference here. The window size of word embeddings is set as 5.  t  3.2 RESULTS  As shown in Table 1 and 2, FEMA outperforms competitive systems on all target domains except REVIEW, where FLORS performs slightly better. FLORS uses more basic features than FEMA; these features could in principle be combined with feature embeddings for better performance. Compared with the other representation learning approaches, FEMA is roughly 1% better on average, corre- sponding to an error reduction of 10%. Its training time is approximately 70 minutes on a 24-core machine, using an implementation based on gensim.2 This is slightly faster than SCL, although slower than mDA with structured dropout noise.  4 RELATED WORK  Representation learning approaches to domain adaptation seek for cross-domain representations, which were ﬁrst induced via auxiliary prediction problems (Ando & Zhang, 2005), such as the prediction of pivot features (Blitzer et al., 2006). In these approaches, as well as in later work on denoising autoencoders (Chen et al., 2012), the key mechanism is to learn a function to predict a subset of features for each instance, based on other features of the instance.  1https://code.google.com/p/word2vec/ 2http://radimrehurek.com/gensim/  3  Accepted as a workshop contribution at ICLR 2015  Word embeddings can be viewed as special case of representation learning, where the goal is to learn representations for each word, and then to supply these representations in place of lexical features (Turian et al., 2010).  5 CONCLUSION  Feature embeddings can be used for domain adaptation in any problem involving feature templates. They offer strong performance, avoid practical drawbacks of alternative representation learning ap- proaches, and are easy to learn using existing word embedding methods.  REFERENCES Ando, Rie Kubota and Zhang, Tong. A framework for learning predictive structures from multiple  tasks and unlabeled data. The Journal of Machine Learning Research, 6:1817–1853, 2005.  Bengio, Yoshua, Courville, Aaron, and Vincent, Pascal. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828, 2013. ISSN 0162-8828. doi: http://doi.ieeecomputersociety.org/10.1109/TPAMI.2013.50.  Blitzer, John, McDonald, Ryan, and Pereira, Fernando. Domain adaptation with structural cor- respondence learning. In Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pp. 120–128, 2006.  Chen, Minmin, Xu, Z., Weinberger, Killian, and Sha, Fei. Marginalized denoising autoencoders In Proceedings of the International Conference on Machine Learning  for domain adaptation. (ICML), 2012.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Domain adaptation for large-scale sentiment In Proceedings of the International Conference on  classiﬁcation: A deep learning approach. Machine Learning (ICML), Seattle, WA, 2011.  Hovy, Eduard, Marcus, Mitchell, Palmer, Martha, Ramshaw, Lance, and Weischedel, Ralph. In Proceedings of the North American Chapter of the Associa-  Ontonotes: the 90% solution. tion for Computational Linguistics (NAACL), pp. 57–60, New York, NY, 2006.  Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efﬁcient estimation of word repre- sentations in vector space. In Proceedings of International Conference on Learning Representa- tions, 2013a.  Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed repre- sentations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp. 3111–3119, 2013b.  Petrov, Slav and McDonald, Ryan. Overview of the 2012 shared task on parsing the web. In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59, 2012.  Ratnaparkhi, Adwait et al. A maximum entropy model for part-of-speech tagging. In Proceedings  of Empirical Methods for Natural Language Processing (EMNLP), pp. 133–142, 1996.  Schnabel, Tobias and Sch¨utze, Hinrich. Flors: Fast and simple domain adaptation for part-of-speech  tagging. Transactions of the Association of Computational Linguistics, 2:51–62, 2014.  Smith, Noah A. Linguistic structure prediction. Synthesis Lectures on Human Language Technolo-  gies, 4(2):1–274, 2011.  Turian, Joseph, Ratinov, Lev, and Bengio, Yoshua. Word Representation: A Simple and General In Proceedings of the Association for Computational  Method for Semi-Supervised Learning. Linguistics (ACL), pp. 384–394, Uppsala, Sweden, 2010.  Yang, Yi and Eisenstein, Jacob. Fast easy unsupervised domain adaptation with marginalized struc- tured dropout. In Proceedings of the Association for Computational Linguistics (ACL), Baltimore, MD, 2014.  4  ",
1412.6645,2015, Weakly Supervised Multi-embeddings Learning of Acoustic Models,"['Weakly Supervised Multi-embeddings Learning of Acoustic Models', 'Gabriel Synnaeve and Emmanuel Dupoux']",https://arxiv.org/pdf/1412.6645,"Accepted as a workshop contribution at ICLR 2015  WEAKLY SUPERVISED MULTI-EMBEDDINGS LEARNING OF ACOUSTIC MODELS  Gabriel Synnaeve & Emmanuel Dupoux LSCP ENS/EHESS/CNRS 29 rue d’Ulm 75005, Paris, France gabriel.synnaeve@gmail.com emmanuel.dupoux@gmail.com  ABSTRACT  We trained a Siamese network with multi-task same/different information on a speech dataset, and found that it was possible to share a network for both tasks without a loss in performance. The ﬁrst task was to discriminate between two same or different words, and the second was to discriminate between two same or different talkers.  1  INTRODUCTION  Theoretically, algorithms performing unsupervised or weakly supervised discovery of linguistic structure represent plausible models of language acquisition in the human infant (Vallabha et al., 2007). Practically, they can be put to use for low resource languages (Park & Glass, 2008). Building on the fact that infant can recognize some words (Bergelson & Swingley, 2012) and dis- criminate between speakers (Johnson et al., 2011) before they have constructed adult-like phoneme representations, we propose to test a neural network architecture where word and talker identity are used as side information to help learning an acoustic model (phone embedding). Previous work has shown that same-different side information can be used for metric learning (Xing et al., 2003), and Synnaeve et al. (2014) demonstrated that it can be used with Deep Neural Network (DNN) architec- ture for learning phone embeddings. Here, we extend this work using multi-task (word and talker identity) side information. As this paper is a feasibility study, we used gold same-different labels, and leave it to further work to derive them in an unsupervised fashion using spoken term discovery (Jansen et al., 2010) and talker diarization (Anguera Miro et al., 2012).  2 MODEL  We used the architure of a Siamese network (Bromley et al., 1993), as shown in Fig. 3.1. It is a duplicated feedforward neural network taking two inputs in parallel. Each of the inputs consists in 11 stacked frames of 40 coefﬁcients log-compressed Mel-ﬁlterbanks. Each network contains 3 hidden layers of 500 units with sigmoid activations, and two output embeddings each of 100 dimensions. One of the embeddings is the one in which we compute the similarity between the two inputs according to the same/different “word type” indication, while the other looks at the same/different “speaker” indication. More formally:  xA and xB ∈ R11×40 ; yA,W , yA,S, yB,W and yB,S ∈ R100  5 1 0 2    r p A 0 2         ]  D S . s c [      3 v 5 4 6 6  .  2 1 4 1 : v i X r a  The loss function that we use (for two inputs xA and xB) is a simple sum of the COSCOS2 losses in each of the embeddings (see Synnaeve et al. (2014) for a comparison with other loss functions):  L(A, B) = LW (A, B) + LS(A, B)  with W ∈ {0, 1} (different or same word) and S ∈ {0, 1} (different or same speaker), both losses are similar (here for speakers):  LS(A, B) = S × (1 − cos(yA,S, yB,S)) + (1 − S) × (cos2(yA,S, yB,S))  1  Accepted as a workshop contribution at ICLR 2015  3 EXPERIMENTS  3.1 DATASET  We used about 1/3rd (12 speakers) of the Buckeye corpus1, on which we performed a dynamic time- warping (DTW) alignment of pairs of same words, in the features space (ﬁlterbanks), exactly as in (Synnaeve et al., 2014). This consisted in doing 76407 pairs of long “same” word (1057 types in total), said 1/4th of the time by the same speakers (we subsampled “same word and different speakers” pairs). During training, we also sample pairs of tokens coming from different words (often called negative sampling), with a ratio of pairs of same/different words of 1:1. This yields about 5M frames for pairs of same words and 4.3M frames for sampled pairs of different words.  Figure 1: Left: Architecture of our multi-embeddings learning Siamese network. We used NF=11, NH=500, and NE=100. Right: Evolution of cosine similarities for pairs of same/different words/speakers during training for the train set (saturated) and validation set (pastel).  3.2 RESULTS  We trained the model with Adadelta (Zeiler, 2012), a variant of stochastic gradient descent with an adaptive learning rate method correcting the magnitude of the updates using an accumulation of past gradients (on a sliding window) and a local approximation of the Hessian. We used ρ = 0.95 (hyper-parameter on the momentum) and (cid:15) = 10−6 (precision of the updates), we performed early stopping on a small (10%) held-out development set. We compared three network setups. In the multi-task setup, we use the combined loss function incorporating both the word and the speaker losses. In the single-loss setup, we only use one of the losses (word or speaker), even though the topology of the network remains the same. This means that the weights of only one of the two embeddings is updated, the other remaining in their initial state, thereby implementing a random projection from the last hidden layer. As a control, we also trained a fully supervised DNN using dropout (Srivastava et al., 2014). It has 11 stacked ﬁlterbank frames as input, 4 hidden layers of 2400 units, and 46 phones as outputs of the logistic regression (with a 37.9% classiﬁcation accuracy). Figure 3.1 shows the evolution of the cosine similarities for the different conditions, and shows that the training of the speaker task takes more time than the training of the phoneme task, even though the cosine similarities start off less favorably for the former than the latter. In both cases, the difference between the train and the dev sets shows that the network is not really overﬁtting. Unsupervised systems do not necessarily discover phoneme-like units. Therefore, evaluating them with a phone error rate may not be appropriate. Similarly, using them as front end for a word rec- ognizer may not be straightforward using standard HMMs. Here, we follow the lead of Carlin et al. (2011) and Schatz et al. (2013) who propose to use instead a discrimination task, which makes no assumption about the shape of the coded categories (phone-like, Gaussian, linearly separable, etc.)  1http://buckeyecorpus.osu.edu  2  Accepted as a workshop contribution at ICLR 2015  and does not depend on the training of a classiﬁer or a language model. The ABX discrimination task consists in computing two pairwise distances, between the token pair X and A, and between X and B and deciding which of them is larger. When A and B are tokens of different linguistic cate- gories, and X belongs to the category of either A or B, this metric measures the degree of separation of the two categories A and B in the embedding. Here, we use as categories, minimal pairs of tri- phones of the shape /a/-/t/-/i/ vs /a/-/p/-/i/, where the left and right context phones are kept identical, and the center phone varies. As distance metric, we use the cosine distance along the DTW path. We setup two tasks, on which we will test our two embeddings:  • phone.talker is a phoneme discrimination task across speakers. For instance, A=/a/-/p/-/i/, B=/a/-/t/-/i/, both being said by the same speaker, and X is phonetically identical to A or B, but is uttered by a different speaker.  • talker.phone is a speaker discrimination task, across phonemes. For instance, both A and B share the same triphone (eg., /a/-/p/-/i/) but are said by different speakers; X is uttered by either the speaker of A or the speaker of B, but has different phonemes (eg., /a/-/t/-/i/).  The two tasks are mirror image of one another regarding the discrimination of the phonemes or of the talkers. In both cases, the context (left and right) phonemes are kept identical. To run this task, we select the set of all eligible ABX triplets of triphones in the dataset, and compute the aggregate ABX score by averaging across context, phoneme and speaker pairs.  Figure 2: ABX scores for speech features (11 frames of stacked ﬁlterbanks), for three Siamese networks: one trained with a same/different word loss function (“word only”), one with a multi-task loss (“both”), and one same/different speakers loss functions (“spkr only”). The three networks have the same topology and the ABX tasks (phone or speaker) are run, respectively, on the phone-based and the speaker-based embeddings. A control shows a supervised DNN trained on phones.  3.3 DISCUSSION  The ABX scores for the phoneme and speaker tasks are shown in Fig. 3.2. Globally, speaker discrim- ination seems easier to optimize than phoneme discrimination (even though it starts the other way around when evaluated from the ﬁlterbanks). This is probably due to the small number of speaker classes (N=12) compared to the number of phoneme classes (N=48). In addition, the multi-task net- work gives the best results across the two tasks, compared to single-task networks. Therefore, learn- ing to do two tasks at once using the same network does not incur a decrease in performance, but on the contrary is slightly beneﬁciary (especially for the talker task). Interestingly the single-task net- works behave asymmetrically with respect to the untrained task. Indeed, the performance on phone discrimination is worse for the network that was trained only on the speaker loss, compared to the ﬁlterbank performance. This makes sense: if you are trained to ignore phoneme identity, phoneme encoding should be progressively removed from the hidden layers of the network. But vice-versa  3  Accepted as a workshop contribution at ICLR 2015  the performance on speaker discrimination is better for the phoneme-loss network compared to the ﬁlterbank base performance. This means that in order to determine speaker identity, it is actually useful for the network to code some information about the phonemes. This last result meshes well with the fact that speaker identiﬁcation depends not so much on raw acoustic features, but on small deformations relative to a background pronunciation distribution (as encoded in i-vectors, Dehak et al. (2011)). Specialization on the task is even more extreme for the fully supervised DNN trained on phone labels: it gives us a higher bound on the phone accross talkers task (81.9% correct), and shows degraded talker accross phones score (54.8% correct) compared to the ﬁlterbank.  Figure 3: Coding speciﬁcity of the input, hidden and embedding layers of the AB net, computed using the ratio of between- to within-category variance (F-test). Left: 11 stacked ﬁlterbanks coding of speakers (shades of blue) and phones (shades of red). The x-axis represent the 11 stacked frames, the y-axis represents the 40 ﬁlterbanks coefﬁcients. Right: Cumulative barplots representing the number of units in the layers coding speciﬁcally for speakers (blue), phones (red), both phones and speakers (purple), or none (black). A unit is deemed code-speciﬁc if the between/within variance ratio for that category is more than the network-wide median.  In order to understand the nature of information encoding in a multi-layer network, it can be useful to inspect the hidden layers in details (Mohamed et al., 2012). Here, we inspected each hidden unit by computing the ratio of between-class to within-class variance in unit activation over the entire corpus (F-test). To compute the phoneme variance ratio, we took the variance of the activation value of the unit across all (between) the phone categories versus within each phone category. We did a similar computation for the speakers categories. Intuitively, a unit with a large phoneme variance ratio is strongly encoding phoneme information; a unit with a small ratio is not very sensitive to that information. Similarly for speaker information. If we split the ratio distribution using the median, this gives rise to a typology of 4 kinds of units according to whether they respond strongly or not to either phone or speaker information. In Figure 3, we can see three phenomena regarding the coding of units in the three hidden layers. First, phone-coding units are predominant in the ﬁrst layers, and progressively, more and more speaker-coding units appear. Second, the number of doubly-coding units diminishes. Third, the sparsity of the code increases (ie., the number of units not coding anything). Inspection of the task-speciﬁc embeddings is interesting, as it reveals an almost pure (and very sparse) coding of speaker identity in the speaker embedding. This is consistent with the high performance of speaker discrimination in this layer. In contrast, inspection of the phone embedding reveals a much less sparse coding and a predominance of doubly-used units. This is consistent with the rather low performance of phoneme discrimination in this layer, and suggests that further layers or more (speaker variability in) training examples would be necessary to “purge” this layer from speaker-speciﬁc effects. Finally, inspection of the ﬁlterbanks (here coded in shade of red and blue) shows that most of the lower frequency ﬁlterbanks are sensitive to phone infrmation (relatively localized in time to the center frames, as we compute it on phonetic annotation), whereas the higher frequency ﬁlterbanks are sensitive to speaker information (relatively not localized in time).  4 CONCLUSION  We have demonstrated that a Siamese network can perform both phoneme and speaker discrim- ination using only a moderate amount of side information (indication of same/different word or  4  Accepted as a workshop contribution at ICLR 2015  speaker for only ≈1000 word types and 12 speakers). Further work is needed to study the effect of the amount of information, and whether the obtained speaker embeddings could replace or com- plement i-vectors. Finally, the phone embedding should be evaluated as a ﬁrst step in a subsequent word recognizer or language model adapted for this kind of representation.  ACKNOWLEDGMENTS  This project is funded in part by the European Research Council (ERC-2011-AdG-295810 BOOT- PHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX- 0001-02 PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, and the Region Ile de France (DIM cerveau et pense).  REFERENCES Anguera Miro, Xavier, Bozonnet, Simon, Evans, Nicholas, Fredouille, Corinne, Friedland, Ger- ald, and Vinyals, Oriol. Speaker diarization: A review of recent research. Audio, Speech, and Language Processing, IEEE Transactions on, 20(2):356–370, 2012.  Bergelson, Elika and Swingley, Daniel. At 6–9 months, human infants know the meanings of many  common nouns. Proceedings of the National Academy of Sciences, 109(9):3253–3258, 2012.  Bromley, Jane, Bentz, James W, Bottou, L´eon, Guyon, Isabelle, LeCun, Yann, Moore, Cliff, S¨ackinger, Eduard, and Shah, Roopak. Signature veriﬁcation using a siamese time delay neu- ral network. Internat. Journ. of Pattern Recog. and Artiﬁc. Intell., 7(04):669–688, 1993.  Carlin, Michael A, Thomas, Samuel, Jansen, Aren, and Hermansky, Hynek. Rapid evaluation of  speech representations for spoken term discovery. In INTERSPEECH, pp. 821–824, 2011.  Dehak, Najim, Kenny, Patrick, Dehak, R´eda, Dumouchel, Pierre, and Ouellet, Pierre. Front-end fac- tor analysis for speaker veriﬁcation. Audio, Speech, and Language Processing, IEEE Transactions on, 19(4):788–798, 2011.  Jansen, Aren, Church, Kenneth, and Hermansky, Hynek. Towards spoken term discovery at scale  with zero resources. In INTERSPEECH, pp. 1676–1679, 2010.  Johnson, Elizabeth K, Westrek, Ellen, Nazzi, Thierry, and Cutler, Anne. Infant ability to tell voices  apart rests on language experience. Developmental Science, 14(5):1002–1011, 2011.  Mohamed, Abdel-rahman, Hinton, Geoffrey, and Penn, Gerald. Understanding how deep belief In Acoustics, Speech and Signal Processing (ICASSP),  networks perform acoustic modelling. 2012 IEEE International Conference on, pp. 4273–4276. IEEE, 2012.  Park, Alex S. and Glass, James R. Unsupervised pattern discovery in speech. IEEE Transactions on Audio, Speech, and Language Processing, 16(1):186–197, January 2008. ISSN 1558-7916. doi: 10.1109/TASL.2007.909282.  Schatz, Thomas, Peddinti, Vijayaditya, Bach, Francis, Jansen, Aren, Hynek, Hermansky, and Dupoux, Emmanuel. Evaluating speech features with the minimal-pair abx task: Analysis of the classical mfc/plp pipeline. In INTERSPEECH-2013, pp. 1781–1785, 2013.  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15(1):1929–1958, 2014.  Synnaeve, Gabriel, Schatz, Thomas, and Dupoux, Emmanuel. Phonetics embedding learning with  side information. In IEEE SLT, 2014.  Vallabha, Gautam K, McClelland, James L, Pons, Ferran, Werker, Janet F, and Amano, Shigeaki. Unsupervised learning of vowel categories from infant-directed speech. Proceedings of the Na- tional Academy of Sciences, 104(33):13273–13278, 2007.  5  Accepted as a workshop contribution at ICLR 2015  Xing, Eric P, Ng, Andrew Y, Jordan, Michael I, and Russell, Stuart. Distance metric learning with application to clustering with side-information. Advances in neural information processing sys- tems, pp. 521–528, 2003.  Zeiler, Matthew D. Adadelta: An adaptive learning rate method. arXiv preprint:1212.5701, 2012.  6  ",
1412.6830,2015, Learning Activation Functions to Improve Deep Neural Networks,"['Learning Activation Functions to Improve Deep Neural Networks', 'Forest Agostinelli', 'Matthew Hoffman', 'Peter Sadowski', 'and Pierre Baldi                                  |']",https://arxiv.org/pdf/1412.6830,"5 1 0 2    r p A 1 2         ] E N . s c [      3 v 0 3 8 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  LEARNING ACTIVATION FUNCTIONS TO IMPROVE DEEP NEURAL NETWORKS  Forest Agostinelli Department of Computer Science University of California - Irvine Irvine, CA 92697, USA {fagostin}@uci.edu  Matthew Hoffman Adobe Research San Francisco, CA 94103, USA {mathoffm}@adobe.com  Peter Sadowski, Pierre Baldi Department of Computer Science University of California - Irvine Irvine, CA 92697, USA {peter.j.sadowski,pfbaldi}@uci.edu  ABSTRACT  Artiﬁcial neural networks typically have a ﬁxed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation func- tion that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural net- work architectures composed of static rectiﬁed linear units, achieving state-of-the- art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.  1  INTRODUCTION  Deep learning with artiﬁcial neural networks has enabled rapid progress on applications in engi- neering (e.g., Krizhevsky et al., 2012; Hannun et al., 2014) and basic science (e.g., Di Lena et al., 2012; Lusci et al., 2013; Baldi et al., 2014). Usually, the parameters in the linear components are learned to ﬁt the data, while the nonlinearities are pre-speciﬁed to be a logistic, tanh, rectiﬁed linear, or max-pooling function. A sufﬁciently large neural network using any of these common nonlinear functions can approximate arbitrarily complex functions (Hornik et al., 1989; Cho & Saul, 2010), but in ﬁnite networks the choice of nonlinearity affects both the learning dynamics (especially in deep networks) and the network’s expressive power. Designing activation functions that enable fast training of accurate deep neural networks is an active area of research. The rectiﬁed linear activation function (Jarrett et al., 2009; Glorot et al., 2011), which does not saturate like sigmoidal functions, has made it easier to quickly train deep neural net- works by alleviating the difﬁculties of weight-initialization and vanishing gradients. Another recent innovation is the “maxout” activation function, which has achieved state-of-the-art performance on multiple machine learning benchmarks (Goodfellow et al., 2013). The maxout activation function computes the maximum of a set of linear functions, and has the property that it can approximate any convex function of the input. Springenberg & Riedmiller (2013) replaced the max function with a probabilistic max function and Gulcehre et al. (2014) explored an activation function that replaces the max function with an LP norm. However, while the type of activation function can have a signiﬁcant impact on learning, the space of possible functions has hardly been explored.  1  Accepted as a workshop contribution at ICLR 2015  One way to explore this space is to learn the activation function during training. Previous efforts to do this have largely focused on genetic and evolutionary algorithms (Yao, 1999), which attempt to select an activation function for each neuron from a pre-deﬁned set. Recently, Turner & Miller (2014) combined this strategy with a single scaling parameter that is learned during training. In this paper, we propose a more powerful adaptive activation function. This parametrized, piecewise linear activation function is learned independently for each neuron using gradient descent, and can represent both convex and non-convex functions of the input. Experiments demonstrate that like other piecewise linear activation functions, this works well for training deep neural networks, and we obtain state-of-the-art performance on multiple benchmark deep learning tasks.  2 ADAPTIVE PIECEWISE LINEAR UNITS  S(cid:88)  Here we deﬁne the adaptive piecewise linear (APL) activation unit. Our method formulates the activation function hi(x) of an APL unit i as a sum of hinge-shaped functions,  hi(x) = max(0, x) +  i max(0,−x + bs as i )  (1)  s=1  i , bs  i variables control the slopes of the linear segments, while the bs  The result is a piecewise linear activation function. The number of hinges, S, is a hyperparameter i for i ∈ 1, ..., S are learned using standard gradient descent set in advance, while the variables as during training. The as i variables determine the locations of the hinges. The number of additional parameters that must be learned when using these APL units is 2SM, where M is the total number of hidden units in the network. This number is small compared to the total number of weights in typical networks. Figure 1 shows example APL functions for S = 1. Note that unlike maxout, the class of func- tions that can be learned by a single unit in- cludes non-convex functions. In fact, for large enough S, hi(x) can approximate arbitrarily complex continuous functions, subject to two conditions:  Theorem 1 Any continuous piecewise-linear function g(x) can be expressed by Equation 1 for some S, and ai, bi, i ∈ 1, ..., S, assuming that:  1. There is a scalar u such that g(x) = x  for all x ≥ u.  2. There are two scalars v and α such  that ∇xg(x) = α for all x < v.  This theorem implies that we can reconstruct any piecewise-linear function g(x) over any subset of the real line, and the two conditions on g(x) constrain the behavior of g(x) to be linear as x gets very large or small. The ﬁrst condi- tion is less restrictive than it may seem. In neu- ral networks, g(x) is generally only of interest as an input to a linear function wg(x) + z; this linear function effectively restores the two degrees of freedom that are eliminated by constraining the rightmost segment of g(x) to have unit slope and bias 0.  Figure 1: Sample activation functions obtained from changing the parameters. Notice that ﬁgure b shows that the activation function can also be non-convex. Asymptotically, the activation func- tions tend to g(x) = x as x → ∞ and g(x) = αx − c as x ← −∞ for some α and c. S = 1 for all plots.  Proof Let g(x) be piecewise linear with K + 2 linear regions separated by ordered boundary points b0, b1, ...,bK, and let ak be the slope of the k-th region. Assume also that g(x) = x for all x ≥ bK.  2  Accepted as a workshop contribution at ICLR 2015  We show that g(x) can be expressed by the following special case of Equation 1:  h(x) ≡ −a0 max(0,−x + b0)  +(cid:80)K  k=1 ak(max(0,−x + bk−1) − max(0,−x + bk))  − max(0,−x) + max(0, x) + max(0,−x + bK),  (2)  The ﬁrst term has slope a0 in the range (−∞, b0) and 0 elsewhere. Each element in the summation term of Equation 2 has slope ak over the range (bk−1, bk) and 0 elsewhere. The last three terms together have slope 1 when x ∈ (bK,∞) and 0 elsewhere. Now, g(x) and h(x) are continuous, their slopes match almost everywhere, and it is easily veriﬁed that h(x) = g(x) = x for x ≥ bK. Thus, we conclude that h(x) = g(x) for all x. (cid:3)  2.1 COMPARISON WITH OTHER ACTIVATION FUNCTIONS  In this section we compare the proposed approach to learning activation functions with two other nonlinear activation functions: maxout (Goodfellow et al., 2013), and network-in-network (Lin et al., 2013). We observe that both maxout units and network-in-network can learn any nonlinear activation func- tion that APL units can, but require many more parameters to do so. This difference allows APL units to be applied in very different ways from maxout and network-in-network nonlinearities: the small number of parameters needed to tune an APL unit makes it practical to train convolutional networks that apply different nonlinearities at each point in each feature map, which would be com- pletely impractical in either maxout networks or network-in-network approaches.  Maxout. Maxout units differ from traditional neural network nonlinearities in that they take as input the output of multiple linear functions, and return the largest: k∈{1,...,K} wk · x + bk.  hmaxout(x) = max  (3)  Incorporating multiple linear functions increases the expressive power of maxout units, allowing them to approximate arbitrary convex functions, and allowing the difference of a pair of maxout units to approximate arbitrary functions. Networks of maxout units with a particular weight-tying scheme can reproduce the output of an APL unit. The sum of terms in Equation 1 with positive coefﬁcients (including the initial max(0, x) term) is a convex function, and the sum of terms with negative coefﬁcients is a concave function. One could approximate the convex part with one maxout unit, and the concave part with another maxout unit:  hconvex(x) = max  k  cconvex k  w · x + dconvex  k  ;  hconcave(x) = max  k  where c and d are chosen so that  hconvex(x) − hconcave(x) = max(0, w · x + u) +(cid:80)  cconcave k  w · x + dconcave  k  ,  s as max(0, w · x + u).  (4)  (5)  In a standard maxout network, however, the w vectors are not tied. So implementing APL units (Equation 1) using a maxout network would require learning O(SK) times as many parameters, where K is the size of the maxout layer’s input vector. Whenever the expressive power of an APL unit is sufﬁcient, using the more complex maxout units is therefore a waste of computational and modeling power.  Network-in-Network. Lin et al. (2013) proposed replacing the simple rectiﬁed linear activation in convolutional networks with a fully connected network whose parameters are learned from data. This “MLPConv” layer couples the outputs of all ﬁlters applied to a patch, and permits arbitrarily complex transformations of the inputs. A depth-M MLPConv layer produces an output vector f M ij from an input patch xij via the series of transformations  f 1 ijk = max(0, w1  k · xij + b1  k), . . . , f M  ijk = max(0, wM  k · f M−1  ij  + bM  k ).  (6)  3  Accepted as a workshop contribution at ICLR 2015  As with maxout networks, there is a weight-tying scheme that allows an MLPConv layer to repro- duce the behavior of an APL unit:  ijk = max(0, ckwκ(k) · xij + b1 f 1  k), f 2  (cid:96)|κ((cid:96))=k akf 1  ij(cid:96),  (7)  ijk =(cid:80)  where the function κ(k) maps from hinge output indices k to ﬁlter indices κ, and the coefﬁcient ck ∈ {−1, 1}. This is a very aggressive weight-tying scheme that dramatically reduces the number of parameters used by the MLPConv layer. Again we see that it is a waste of computational and modeling power to use network-in-network wherever an APL unit would sufﬁce. However, network-in-network can do things that APL units cannot—in particular, it efﬁciently cou- ples and summarizes the outputs of multiple ﬁlters. One can get the beneﬁts of both architectures by replacing the rectiﬁed linear units in the MLPconv layer with APL units.  3 EXPERIMENTS  Experiments were performed using the software package CAFFE (Jia et al., 2014). The hyperpa- rameter, S, that controls the complexity of the activation function was determined using a validation set for each dataset. The as i parameters were regularized with an L2 penalty, scaled by 0.001. Without this penalty, the optimizer is free to choose very large values of as i balanced by very small weights, which would lead to numerical instability. We found that adding this penalty improved re- sults. The model ﬁles and solver ﬁles are available at https://github.com/ForestAgostinelli/Learned- Activation-Functions-Source/tree/master.  i and bs  3.1 CIFAR  The CIFAR-10 and CIFAR-100 datasets (Krizhevsky & Hinton, 2009) are 32x32 color images that have 10 and 100 classes, respectively. They both have 50,000 training images and 10,000 test im- ages. The images were preprocessed by subtracting the mean values of each pixel of the training set from each image. Our network for CIFAR-10 was loosely based on the network used in (Srivastava et al., 2014). It had 3 convolutional layers with 96, 128, and 256 ﬁlters, respectively. Each kernel size was 5x5 and was padded by 2 pixels on each side. The convolutional layers were followed by a max-pooling, average-pooling, and average-pooling layer, respectively; all with a kernel size of 3 and a stride of 2. The two fully connected layers had 2048 units each. We applied dropout (Hinton et al., 2012) to the network as well. We found that applying dropout both before and after a pooling layer increased classiﬁcation accuracy. The probability of a unit being dropped before a pooling layer was 0.25 for all pooling layers. The probability for them being dropped after each pooling layers was 0.25, 0.25, and 0.5, respectively. The probability of a unit being dropped for the fully connected layers was 0.5 for both layers. The ﬁnal layer was a softmax classiﬁcation layer. For CIFAR-100, the only difference was the second pooling layer was max-pooling instead of average-pooling. The baseline used rectiﬁed linear activation functions. When using the APL units, for CIFAR-10, we set S = 5. For CIFAR-100 we set S = 2. Table 1 shows that adding the APL units improved the baseline by over 1% in the case of CIFAR-10 and by almost 3% in the case of CIFAR-100. In terms of relative difference, this is a 9.4% and a 7.5% decrease in error rate, respectively. We also try the network-in-network architecture for CIFAR-10 (Lin et al., 2013). We have S = 2 for CIFAR-10 and S = 1 for CIFAR-100. We see that it improves performance for both datasets. We also try our method with the augmented version of CIFAR-10 and CIFAR-100. We pad the image all around with a four pixel border of zeros. For training, we take random 32 x 32 crops of the image and randomly do horizontal ﬂips. For testing we just take the center 32 x 32 image. To the best of our knowledge, the results we report for data augmentation using the network-in-network architecture are the best results reported for CIFAR-10 and CIFAR-100 for any method. In section 3.4, one can observe that the learned activations can look similar to leaky rectiﬁed linear units (Leaky ReLU) (Maas et al., 2013). This activation function is slightly different than the ReLU  4  Accepted as a workshop contribution at ICLR 2015  because it has a small slope k when the input x < 0.  (cid:26)x,  h(x) =  if x > 0  kx, otherwise  In (Maas et al., 2013), k is equal to 0.01. To compare Leaky ReLUs to our method, we try different values for k and pick the best value one. The possible values are positive and negative 0.01, 0.05, 0.1, and 0.2. For the standard convolutional neural network architecture k = 0.05 for CIFAR-10 and k = −0.05 for CIFAR-100. For the network-in-network architecture k = 0.05 for CIFAR-10 and k = 0.2 for CIFAR-100. APL units consistently outperform leaky ReLU units, showing the value of tuning the nonlinearity (see also section 3.3).  Table 1: Error rates on CIFAR-10 and CIFAR-100 with and without data augmentation. This in- cludes standard convolutional neural networks (CNNs) and the network-in-network (NIN) architec- ture (Lin et al., 2013). The networks were trained 5 times using different random initializations — we report the mean followed by the standard deviation in parenthesis. The best results are in bold.  Method  Without Data Augmentation  CNN + ReLU (Srivastava et al., 2014) CNN + Channel-Out (Wang & JaJa, 2013) CNN + Maxout (Goodfellow et al., 2013) CNN + Probout (Springenberg & Riedmiller, 2013) CNN (Ours) + ReLU CNN (Ours) + Leaky ReLU CNN (Ours) + APL units NIN + ReLU (Lin et al., 2013) NIN + ReLU + Deep Supervision (Lee et al., 2014) NIN (Ours) + ReLU NIN (Ours) + Leaky ReLU NIN (Ours) + APL units  With Data Augmentation  CNN + Maxout (Goodfellow et al., 2013) CNN + Probout (Springenberg & Riedmiller, 2013) CNN + Maxout (Stollenga et al., 2014) CNN + Maxout + Selective Attention (Stollenga et al., 2014) CNN (Ours) + ReLU CNN (Ours) + APL units NIN + ReLU (Lin et al., 2013) NIN + ReLU + Deep Supervision (Lee et al., 2014) NIN (Ours) + ReLU NIN (Ours) + APL units  CIFAR-10  CIFAR-100  12.61% 13.2% 11.68% 11.35% 12.56 (0.26)% 11.86 (0.04)% 11.38 (0.09)% 10.41% 9.78% 9.67 (0.11)% 9.75 (0.22)% 9.59 (0.24)%  9.38% 9.39% 9.61% 9.22% 9.99 (0.09)% 9.89 (0.19)% 8.81% 8.22% 7.73 (0.13)% 7.51 (0.14)%  37.20% 36.59% 38.57% 38.14% 37.34 (0.28)% 35.82 (0.34)% 34.54 (0.19)% 35.68% 34.57% 35.96 (0.13)% 36.00 (0.36)% 34.40 (0.16)%  - - 34.54% 33.78% 34.50 (0.12)% 33.88 (0.45)% - - 32.75 (0.13)% 30.83 (0.24)%  3.2 HIGGS BOSON DECAY The Higgs-to-τ +τ− decay dataset comes from the ﬁeld of high-energy physics and the analysis of data generated by the Large Hadron Collider (Baldi et al., 2015). The dataset contains 80 million collision events, characterized by 25 real-valued features describing the 3D momenta and energies of the collision products. The supervised learning task is to distinguish between two types of physical processes: one in which a Higgs boson decays into τ +τ− leptons and a background process that produces a similar measurement distribution. Performance is measured in terms of the area under the receiver operating characteristic curve (AUC) on a test set of 10 million examples, and in terms of discovery signiﬁcance (Cowan et al., 2011) in units of Gaussian σ, using 100 signal events and 5000 background events with a 5% relative uncertainty. Our baseline for this experiment is the 8 layer neural network architecture from (Baldi et al., 2015) whose architecture and training hyperparameters were optimized using the Spearmint al- gorithm (Snoek et al., 2012). We used the same architecture and training parameters except that  5  Accepted as a workshop contribution at ICLR 2015  dropout was used in the top two hidden layers to reduce overﬁtting. For the APL units we used S = 2. Table 2 shows that a single network with APL units achieves state-of-the-art performance, increasing performance over the dropout-trained baseline and the ensemble of 5 neural networks from (Baldi et al., 2015).  Table 2: Performance on the Higgs boson decay dataset in terms of both AUC and expected dis- covery signiﬁcance. The networks were trained 4 times using different random initializations — we report the mean followed by the standard deviation in parenthesis. The best results are in bold.  Method  DNN + ReLU (Baldi et al., 2015) DNN + ReLU + Ensemble(Baldi et al., 2015) DNN (Ours) + ReLU DNN (Ours) + APL units  AUC  0.802 0.803 0.803 (0.0001) 0.804 (0.0002)  Discovery Signiﬁcance 3.37σ 3.39σ 3.38 (0.008) σ 3.41 (0.006) σ  3.3 EFFECTS OF APL UNIT HYPERPARAMETERS  Table 3 shows the effect of varying S on the CIFAR-10 benchmark. We also tested whether learning the activation function was important (as opposed to having complicated, ﬁxed activation functions). For S = 1, we tried freezing the activation functions at their random initialized positions, and not allowing them to learn. The results show that learning activations, as opposed to keeping them ﬁxed, results in better performance.  Table 3: Classiﬁcation accuracy on CIFAR-10 for varying values of S. Shown are the mean and standard deviation over 5 trials.  Values of S  baseline S = 1 (activation not learned) S = 1 S = 2 S = 5 S = 10  Error Rate 12.56 (0.26)% 12.55 (0.11)% 11.59 (0.16)% 11.73 (0.23)% 11.38 (0.09)% 11.60 (0.16)%  3.4 VISUALIZATION AND ANALYSIS OF ADAPTIVE PIECEWISE LINEAR FUNCTIONS  The diversity of adaptive piecewise linear functions was visualized by plotting hi(x) for sample neurons. Figures 2 and 3 show adaptive piecewise linear functions for the CIFAR-100 and Higgs→ τ +τ− experiments, along with the random initialization of that function. In ﬁgure 4, for each layer, 1000 activation functions (or the maximum number of activation functions for that layer, whichever is smaller) are plotted. One can see that there is greater variance in the learned activations for CIFAR-100 than there is for CIFAR-10. There is greater variance in the learned activations for Higgs→ τ +τ− than there is for CIFAR-100. For the case of Higgs→ τ +τ−, a trend that can be seen is that the variance decreases in the higher layers.  4 CONCLUSION  We have introduced a novel neural network activation function in which each neuron computes an independent, piecewise linear function. The parameters of each neuron-speciﬁc activation function are learned via gradient descent along with the network’s weight parameters. Our experiments demonstrate that learning the activation functions in this way can lead to signiﬁcant performance improvements in deep neural networks without signiﬁcantly increasing the number of parameters. Furthermore, the networks learn a diverse set of activation functions, suggesting that the standard one-activation-function-ﬁts-all approach may be suboptimal.  6  Accepted as a workshop contribution at ICLR 2015  Figure 2: CIFAR-100 Sample Activation Functions. Initialization (dashed line) and the ﬁnal learned function (solid line).  Figure 3: Higgs→ τ +τ− Sample Activation Functions. Initialization (dashed line) and the ﬁnal learned function (solid line).  ACKNOWLEDGMENTS  F. Agostinelli was supported by the GEM Fellowship. This work was done during an internship at Adobe. We also wish to acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research, NSF grant IIS-0513376, and a Google Faculty Research award to P. Baldi, and thanks to Yuzo Kanomata for computing support.  REFERENCES Baldi, P, Sadowski, P, and Whiteson, D. Searching for exotic particles in high-energy physics with  deep learning. Nature Communications, 5, 2014.  Baldi, Pierre, Sadowski, Peter, and Whiteson, Daniel. Enhanced higgs to τ +τ− searches with deep  learning. Physics Review Letters, 2015. In press.  Cho, Youngmin and Saul, Lawrence. Large margin classiﬁcation in inﬁnite neural networks. Neural  Computation, 22(10), 2010.  7  Accepted as a workshop contribution at ICLR 2015  (a) CIFAR-10 Activation Functions.  (b) CIFAR-100 Activation Functions.  (c) Higgs→ τ +τ− Activation Functions.  Figure 4: Visualization of the range of the values for the learned activation functions for the deep neural network for the CIFAR datasets and Higgs→ τ +τ− dataset.  Cowan, Glen, Cranmer, Kyle, Gross, Eilam, and Vitells, Ofer. Asymptotic formulae for likelihood- based tests of new physics. Eur.Phys.J., C71:1554, 2011. doi: 10.1140/epjc/s10052-011-1554-0.  Di Lena, P., Nagata, K., and Baldi, P. Deep architectures for protein contact map prediction. Bioin- formatics, 28:2449–2457, 2012. doi: 10.1093/bioinformatics/bts475. First published online: July 30, 2012.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectiﬁer networks. In Proceed- ings of the 14th International Conference on Artiﬁcial Intelligence and Statistics. JMLR W&CP Volume, volume 15, pp. 315–323, 2011.  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.  Maxout networks. arXiv preprint arXiv:1302.4389, 2013.  Gulcehre, Caglar, Cho, Kyunghyun, Pascanu, Razvan, and Bengio, Yoshua. Learned-norm pool- ing for deep feedforward and recurrent neural networks. In Machine Learning and Knowledge Discovery in Databases, pp. 530–546. Springer, 2014.  Hannun, Awni Y., Case, Carl, Casper, Jared, Catanzaro, Bryan C., Diamos, Greg, Elsen, Erich, Prenger, Ryan, Satheesh, Sanjeev, Sengupta, Shubho, Coates, Adam, and Ng, Andrew Y. Deep  8  Accepted as a workshop contribution at ICLR 2015  speech: Scaling up end-to-end speech recognition. CoRR, abs/1412.5567, 2014. URL http: //arxiv.org/abs/1412.5567.  Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Rus- lan R. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.  Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert. Multilayer feedforward networks are  universal approximators. Neural networks, 2(5):359–366, 1989.  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, M, and LeCun, Yann. What is the best multi-stage ar- chitecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pp. 2146–2153. IEEE, 2009.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images.  Computer Science Department, University of Toronto, Tech. Rep, 2009.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang, Zhengyou, and Tu, Zhuowen. Deeply-  supervised nets. arXiv preprint arXiv:1409.5185, 2014.  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. arXiv preprint arXiv:1312.4400,  2013.  Lusci, Alessandro, Pollastri, Gianluca, and Baldi, Pierre. Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules. Journal of chemical information and modeling, 53(7):1563–1575, 2013.  Maas, Andrew L, Hannun, Awni Y, and Ng, Andrew Y. Rectiﬁer nonlinearities improve neural  network acoustic models. In Proc. ICML, volume 30, 2013.  Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. Practical bayesian optimization of machine In Advances in Neural Information Processing Systems, pp. 2951–2959,  learning algorithms. 2012.  Springenberg, Jost Tobias and Riedmiller, Martin. Improving deep neural networks with probabilis-  tic maxout units. arXiv preprint arXiv:1312.6116, 2013.  Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15(1):1929–1958, 2014.  Stollenga, Marijn F, Masci, Jonathan, Gomez, Faustino, and Schmidhuber, J¨urgen. Deep networks with internal selective attention through feedback connections. In Advances in Neural Information Processing Systems, pp. 3545–3553, 2014.  Turner, Andrew James and Miller, Julian Francis. Neuroevolution: Evolving heterogeneous artiﬁcial  neural networks. Evolutionary Intelligence, pp. 1–20, 2014.  Wang, Qi and JaJa, Joseph. From maxout to channel-out: Encoding information on sparse pathways.  arXiv preprint arXiv:1312.1909, 2013.  Yao, Xin. Evolving artiﬁcial neural networks. Proceedings of the IEEE, 87(9):1423–1447, 1999.  9  ",
1406.3407,2015, Restricted Boltzmann Machine for Classification with Hierarchical Correlated Prior,"['Restricted Boltzmann Machine for Classification with Hierarchical Correlated Prior', 'Gang Chen and Sargur Srihari']",https://arxiv.org/pdf/1406.3407,"5 1 0 2    r p A 0 2         ]  G L . s c [      2 v 7 0 4 3  .  6 0 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  RESTRICTED BOLTZMANN MACHINE FOR CLASSIFI- CATION WITH HIERARCHICAL CORRELATED PRIOR  Gang Chen & Sargur N. Srihari Department of Computer Science and Engineering State University of New York at Buffalo Buffalo, NY 14260, USA gangchen@buffalo.edu, srihari@cedar.buffalo.edu  ABSTRACT  Restricted Boltzmann machines (RBM) and its variants have been widely used on classiﬁcation problems. In a sense, its success of RBM should be attributed to its strong representation power with hidden variables. Often, classiﬁcation RBM ig- nores the interclass relationship or prior knowledge of sharing information among classes. In this paper, we propose a RBM with hierarchical prior for classiﬁcation problem, by generalizing the classiﬁcation RBM with sharing information among different classes. Basically, we assume the hierarchical prior over classes, where parameters for nearby nodes are correlated in the hierarchical tree, and further the parameters at each node of the tree to be orthogonal to those at its ancestors. Through the hierarchical prior, our model improves the information sharing be- tween different classes and reduce the redundancy for robust classiﬁcation. We test our method on several datasets, and show promising results compared to com- petitive baselines.  1  INTRODUCTION  Restricted Boltzmann machines (RBM) Hinton (2002) are a speciﬁc neural network with no hidden- hidden and visible-visible connections. They have attracted signiﬁcant attention recently on many machine learning problems, such as dimension reduction Hinton & Salakhutdinov (2006), text cat- egorization Larochelle et al. (2012), collaborative ﬁltering Salakhutdinov et al. (2007) and object recognition Krizhevsky et al. (2012). A recent survey Bengio et al. (2012) shows how to improve classiﬁcation accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classiﬁcation accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efﬁcient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more speciﬁc characteristics. In other words, the semantic relationship among classes is constructed from gen- eralization to speciﬁcation as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively. Unfortunately, traditional RBM Larochelle & Bengio (2008); Larochelle et al. (2012) treats the category structure as ﬂat and little work has been done to explore the interclass relationship. In this paper, we generalize RBM with hierarchical prior for classiﬁcation problems. Basically, we divide the classiﬁcation RBM into traditional RBM for representation learning and multinomial logit model for classiﬁcation, see Fig. 1(a) for intuitive understanding. For the traditional RBM (red in Fig. 1(a)), we can extend it into deep belief network (DBN), while for the multinomial logit model (green in Fig. 1(a)), we can incorporate the interclass relationship to it. In this work, we focus on the hierarchical prior over the classiﬁcation RBM, and we take a similar strategy as corrMNL, that means we use sums of parameters along paths from root to a speciﬁc leaf in the tree as model parameters for hierarchical classiﬁcation. However, we consider it in a rather different way from the previous work. We can think our method is a kind of mixture of corrMNL Shahbaba & Neal (2007) and the orthogonal SVM model Xiao et al. (2011). However, our model inherits the advantage of  1  Accepted as a workshop contribution at ICLR 2015  Figure 1: (a) It is the classiﬁcation restricted Boltzmann machine, which integrates restricted Boltz- mann machine and logistic regression model; the left red dash area is restricted Boltzmann machine for representation learning, while the green region shows the logistic regression model for multi- class problems. (b) A hierarchical example for explanation, in which all internal nodes are depicted with white background, while leafs/classes are shown in gray in the hierarchy. The parameters for each classes are presented as a sum of parameters along its ancestors at different level of hierarchy. For example, the coefﬁcient parameter of class 1 is A12 + A21.  RBM, which can learn the hidden representation for better classiﬁcation Hinton & Salakhutdinov (2006); Larochelle et al. (2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al. (2004); Xiao et al. (2011). Moreover, we only have a single RBM in our model, while there are multiple SVMs in the orthogonal hierarchical SVM Xiao et al. (2011). Our contributions are: (1) we introduce the hierarchical semantic prior over labels into restricted Boltzmann machine; (2) we add orthogonal constraints over adjacent layers in the hierarchy, which makes our model more robust for classiﬁcation problems. We test our method in the experiments, and show comparative results over competitive baselines.  2 CLASSIFICATION RESTRICTED BOLTZMANN MACHINE WITH  HIERARCHICAL CORRELATED PRIOR  We will revisit the classiﬁcation RBM, then we will introduce our model. Throughout the paper, matrix variables are denoted with bold uppercases, and vector quantities are written in bold lower- case. For matrix W, we indicate its i-th row and j-th column element as Wij, its i-th row vector Wi. and j-th column vector W.j. For different matrixes, we use different subscripts to discern them. For example, A12 and A21 are different matrixes, which are indicated by different subscripts.  2.1 CLASSIFICATION RESTRICTED BOLTZMANN MACHINE Denote X ∈ Rd be an instance domain and Y be a set of labels. Assume that we have a training set D = {(xi, yi)}, comprising for the i-th pair: an input vector xi ∈ X and a target class yi ∈ Y, where xi ∈ Rd and yi ∈ {1, ..., K}. An RBM with n hidden units is a parametric model of the joint distribution between a layer of hidden variables h = (h1, ..., hn) and the observations x = (x1, ..., xd) and y. The classiﬁcation RBM was ﬁrst proposed in Hinton (2007) and was further developed in Larochelle & Bengio (2008); Larochelle et al. (2012) with discriminative training model. The joint likelihood of the classiﬁcation RBM takes the following form:  p(y, x, h) ∝ e−E(y,x,h)  (1)  where the energy function is  E(y, x, h) = −hT Wx − bT x − cT h − dT y − hT Uy  with parameters Θ = {W, b, c, d, U} and y = (1y=i)K and U ∈ Rn×K. For classiﬁcation problem, we need to compute the conditional probability for p(y|x). As shown in Salakhutdinov et al. (2007), this conditional distribution has explicit formula and can be calculated  (2) i=1 for K classes, where matrix W ∈ Rn×d,  2  A12 h U W x y 0	  0	  1	  (a)	  	  (b)	  	  1 2 3 Class 1 Class 2 Class 3 Class 4 Class 5 A13 A21 A22 A31 A32 A33 Accepted as a workshop contribution at ICLR 2015  exactly, by writing it as follows:  p(y|x) =  edy(cid:81)n y∗ edy∗(cid:81)n (cid:80)  j=1  i Wij xi(cid:1) (cid:0)1 + ecj +Ujy+(cid:80) (cid:0)1 + ecj +Ujy∗ +(cid:80) i Wij xi(cid:1)  j=1  (3)  To learn RBM parameters, we need to optimize the joint likelihood p(y, x) on training data D. Note that it is intractable to compute p(y, x), because it needs to model p(x). Fortunately, Hinton proposed an efﬁcient stochastic descent method, namely contrastive divergence (CD) Hinton (2002) to maximize the joint likelihood. Thus, we get the following stochastic gradient updates for W and U from CD respectively  ∂logp(x, y)  ∂Wij  = (cid:104)vihj(cid:105)data − (cid:104)vihj(cid:105)model  ∂logp(x, y)  ∂Ujk  = (cid:104)hjyk(cid:105)data − (cid:104)hjyk(cid:105)model  (4)  And update Θ until convergence with gradient descent  Θ = Θ + η  ∂logp(x, y)  ∂Θ  (5)  where η is the learning rate for the classiﬁcation RBM.  2.2 RESTRICTED BOLTZMANN MACHINE WITH HIERARCHICAL PRIOR  Our model introduces hierarchical prior over label sets for logistic regression classiﬁer in the clas- siﬁcation RBM. Note that we divide the classiﬁcation RBM into two parts: RBM (feature learning) and multinomial logit model (classiﬁer), corresponding to red and green regions shown in Fig. 1(a) respectively. Our model introduces the hierarchical prior over multinomial logit regression classiﬁer, which is vital for classiﬁcation problems under RBM framework. Deﬁne the hierarchical tree T = (V,E), the number of node N = |V| and the number of edge M = |E|. Furthermore, we assume all parameters along edges are A = {A1, .., Am}, where {Aj}m j=1 describes the parameter for each edge in the hierarchy respectively and Aj has the same size as U in the above subsection 2.1. For any node ν in the tree, we denote A(ν) as its direct parent (vertex adjacent to v), and A(i)(ν) to be its i-th ancestor of ν. As in Dekel et al. (2004), we also deﬁne the path for each node ν ∈ T , deﬁne P (ν) to be the set of nodes along the path from root to v,  P (ν) = {µ ∈ T : ∃i µ = A(i)(ν)} Now we can deﬁne the coefﬁcient parameters for each leaf node ν as  (6)  Aµ  (7)  (cid:88)  µ∈P (ν)  A(ν) =  where the classiﬁcation coefﬁcient for each class in Eq. (7) is decomposed into contributions along paths from root to the leaf associated to that class. For our model, each leaf node is associated to one class, which takes the same methodology as in Salakhutdinov et al. (2007). Fig. 1(b) is an example with total ﬁve classes, where the sums of parameters along the path to the leaf node are coefﬁcient parameters used for classiﬁcation. In Fig. 1(b), A12 and A13 are parameters along branches in the ﬁrst level, and A21, A22, A31, A32 and A33 are parameters in the second level. For example, the coefﬁcient parameter of class 1 is A12 + A21 according to Eq. (7); similarly, for class 4, its coefﬁcient parameter is A13 + A32. For example, we can see class 1 and class 2 sharing the common term A12, which can be thought as the prior correlation between the parameters of nearby classes in the hierarchy. For K classes, we have U ∈ Rn×K and Aj ∈ Rn×K for j = {1, .., m}. Thus we can factorize  (8) where A = {A1, .., Am} ∈ Rmn×K is the concatenation of parameters {Aj}m j=1 of all edges in the hierarchy, while V ∈ Rn×mn implies the hierarchical prior over labels, refer Eq. (7) for construction of the correlated matrix V. Note that V (just) encodes the given hierarchical structures with 0 or 1 and is ﬁxed during training the models. In addition, we introduce orthogonal restrictions  U = VA  3  Accepted as a workshop contribution at ICLR 2015  just as in Xiao et al. (2011) to reduce redundancy between adjacent layers. Given a training set D = {(xi, yi)}, we propose the following objective function:  L(D; Θ) = −  logp(yi, xi) + C  trace(AT  µ Aν)  (9)  |D|(cid:88)  i=1  (cid:88)  ν,µ∈P (ν)  where C is the weight to balance the two terms. The ﬁrst term is from the negative log likelihood as in RBM and the second term forces parameters at children to be orthogonal to those at its ancestor as much as possible. The differences between our model and RBM lie: (1) hierarchical prior over labels, which can induce correlation between the parameters of nearby nodes in the tree; (2) we have orthogonal regularization which can make our model more robust, and also reduce redundancy in model parameters. For parameters updating, we have the same equations as in the classiﬁcation RBM, except for U which introduces hierarchical prior and orthogonal restrictions among children-parent pairs. According to chain rule, we can differenciate L(D; Θ) r.w.t Aν and get the following derivative  i=1 logp(yi, xi)  ∂U  · ∂U ∂Aν  + C  Aµ  (10)  (cid:88)  µ∈P (ν)  = − ∂(cid:80)|D|  ∂L(D; Θ)  Note that the derivative of(cid:80)|D|  ∂Aν  i=1 logp(yi, xi) w.r.t. U can be computed via Eq. (4). Thus, we can use Eq. (10) to calculate derivative w.r.t. Aν, and then update Aν with stochastic gradient descent. Given Aν, we can use Eq. (8) to update U.  2.3 ALGORITHM  Note that our model incorporates the hierarchical prior and orthogonal constraints through U. In other words, we can update all parameters with CD, except U. Because U is the function of A, we can compute the derivative of U w.r.t. A and update A with gradient descent. After we get A, we can calculate U, which can be used in the next iteration. We list the pseudo code below in Alg. 1.  for each batch do  Algorithm 1 Learning RBM with hierarchical correlated prior Input: training data D = {(xi, yi)}, the number of hidden nodes n, learning rate η, C and maximum epoch T Output: Θ = {W, b, c, d, U} 1: Initialize parameters W, b, c, d, U; 2: Divide the training data into batches; 3: for t = 1 to T do 4: 5: 6: 7: 8: 9: 10: 11: end for 12: Output W, b, c, d, U; 13: End  Use 1-step Gibbs sampling to update the gradient according to Eq. (4); Update all other parameters except U with CD; Compute gradient w.r.t. Aν according to Eq. (10); Update A with gradient descent with Eq. (5); Update U according to Eq. (8);  end for  3 EXPERIMENTAL RESULTS  We analyze our model with experiments on two classiﬁcation problems: character recognition and document classiﬁcation, and compare our results to those from competitive baselines below. RBM or RBM for classiﬁcation was ﬁrst proposed in Hinton & Salakhutdinov (2006) and later was further developed in Larochelle et al. (2012). Its mathematical formula is shown in Eq. (2). Hierarchical classiﬁcation RBM with soft assignment (HRBMs) is a nested hierarchical classiﬁer in a top-down way, shown in Fig. 2(b). In the training stage, for each internal node (including root  4  Accepted as a workshop contribution at ICLR 2015  (a)  (b)  Figure 2: (a) The hierarchical structure prior over label sets from MNIST digital dataset; we use this prior over labels with the purpose to capture similar structure information between different characters. For example, ‘3’ and ‘8’ share some parts, and similar structure information can be found in pairs ‘4’ and ‘7’, as well as ‘1’ and ‘9’. (b) The hierarchical classiﬁcation RBM (HRBM), which is constructed according the hierarchical prior Fig. 2(a). In order to learn HRBM classiﬁer, we learn a RBM classiﬁer for each node and recursively to the leafs in a top-down manner.  Figure 3: The hierarchical structure from 20 news group dataset. The root (or the ﬁrst level) cover documents from all categories, while the leaf level indicates labels where documents attached to.  node) in the current level, HRBM will split training data according to its children and learn a clas- siﬁcation RBM for multiple classes (decided by the number of its children). In the inference stage, the likelihood for certain classes in the current layer depends both on the output probability of this layer classiﬁer and also the conditional likelihood on the upper levels. For example, the probability to assign label 2 to a given instance in Fig. 2(b) depends on the output probabilities from RBM1, RBM21 and RBM31. For each data instance, its probability belongs to each class is the probability production along path from root to the leaf of that class, and ﬁnally we assign the data instance to the label with largest probability. Hierarchical classiﬁcation RBM with hard assignment (HRBMh) has the similar hierarchical struc- ture as HRBMs in Fig. 2(b). The difference between HRBMs and HRBMh is that HRBMs assign classiﬁcation probability to each node, while HRBMh assign labels. Hidden hierarchical classiﬁcation RBM (HHRBM) is similar as the hierarchical classiﬁcation RBM (HRBM) in a top-down manner. For any current node, HHRBM learns a classiﬁcation RBM and projects the training data into hidden space for its children (Note that RBM can map any input instance into its hidden space). Then, all its children recursively learn classiﬁcation RBMs with projected hidden states as input from its parent node until to leaf level. In a sense, HHRBM works similar to the deep believe network (DBN) in Hinton (2007). Hence, the only difference between HHRBM and HRBM is that HRBM computes the classiﬁcation probability with the visual data as input for all levels, while HHRBM calculates the classiﬁcation probability with hidden states as in- put in a top-down manner. Multinomial logit model (MNL), a.k.a multiclass logistic regression, has no class correlated hier- archical structure. Correlated Multinomial logit regression (corrMNL) 1 extends MNL with hierarchical prior over classes, refer to Shahbaba & Neal (2007) for more details. In all the above baselines, HRBMs, HRBMh, HHRBM and corrMNL leverage the hierarchical prior over label sets for classiﬁcation, while RBM and MNL have no such prior information available. As for the difference in the number of RBMs used, (H)HRBMs belong to the tow-down classiﬁcation approaches where multiple RBMs are constructed and each of which is trained to classify training  1http://www.ics.uci.edu/˜babaks/Site/Codes.html  5  2	  5	  0	  3	  8	  6	  4	  7	  1	  9	  	  2	  5	  0	  3	  8	  6	  4	  7	  1	  9	  	  2	  5	  0	  3	  8	  6	  2	  5	  0	  6	  3	  	  8	  4	  7	  	  1	  	  4	  	  7	  	  1	  9	  	  9	  	  RBM1	  (2	  classes)	  	  RBM21(2	  classes)	  RBM22(2	  classes)	  	  RBM31	  (2	  classes)	  RBM32	  (4	  classes)	  2	  5	  0	  6	  3	  	  8	  RBM33	  (2	  classes)	  1	  	  4	  	  7	  	  RBM34	  (2	  classes)	  	  9	  	  comp.	  talk.	  rec.	  med.	  misc.	  forsale	  alt.	  Atheism.	  soc.religon.chris5an	  	  graphics.	  os.	  sys.	  window.	  ms-­‐windows.misc	  ibm.pc.	  hardware	  mac.	  hardware	  poli5cs.	  misc.	  guns	  mideast.	  religon.misc	  autos.	  motorcycles.	  sport.	  baseball.	  hockey.	  crypt.	  electronics.	  sci.	  space.	  comp/talk/rec/sci/alt/mis/soc	  Accepted as a workshop contribution at ICLR 2015  Error Rate (%)  Datasets MNIST  SVM MNL corrMNL HRBMh HRBMs HHRBM RBM Ours 7.91 10.8  11.10  10.6  8.97  12.1  7.95  8.22  Model  Table 1: The experimental comparison on a subset of MNIST dataset, with total 5000 training exam- ples and 1000 testing samples. We compare the performances between our method and the baselines. It demonstrates that our method with hierarchical prior over labels can improve recognition accuracy.  (a)  (b)  Figure 4: (a) How the error rate changes with the balanced training samples; (b) the average error rate on the rare classes, which sample a few examples for each rare class, while keeping other classes each with 500 samples respectively. It shows that our method works better than RBM with few training cases, and yield higher accuracy on the rare classes.  examples into one of its children in a hierarchical tree while our approach maintains only a single RBM. Character Recognition MNIST digits2 consists of 28 × 28-size images of handwriting digits from 0 through 9, and has been widely used to test character recognition methods. In the experiment, we use Fig. 2(a) as our hierarchical prior over label sets. To test our method and other baselines, we sample 5000 images from the training sets as our training examples and 1000 examples from the testing sets as our testing data. The reason that we use a subset of MNIST is to answer whether the correlation between different classes is valuable for classiﬁcation problem when the number of training examples for individual classes may be relatively small. In order to make our method com- parable to other baselines, we have the same parameter setting for RBM related methods (including RBM, HRBMs, HRBMh and our method). We set the number of hidden states n = 100 and the learning rate η = 0.1 for RBM related methods, and the extra parameter in our model C = 0.1. Both HRBMs and HRBMh learn a RBM for each node and recursively to leafs, shown in Fig. 3. For the HHRBM with 4 layers decided by the hierarchical prior in Fig. 2(a), we set its number of hidden states 100, 50, 25 and 20 for each layer respectively. The comparison between our method and the baselines is shown in Table (1). Our method incorpo- rates the hierarchical prior structure over labels, and the experimental results show that our method outperform other RBM related methods, and also demonstrates that the hierarchical prior in our method is helpful to improve the recognition accuracy. To further indicate whether our method is helpful or not with few training samples and how it per- forms on rare classes, we tested our model on the balanced and unbalanced cases. For the balanced case (each class was sampled equally), we random sampled from 1000 to 5000 examples respec- tively and tested on the 1000 samples from testing set. The results in Fig. 4(a) demonstrates that our model works better than RBM. We also tested our method on the rare classes. Basically, we sampled a few examples for each rare class, while keep the other classes with 500 samples respec- tively. For example, we sample the 10 examples for the class ‘0’, while the rest 9 classes have 500  2http://yann.lecun.com/exdb/mnist/  6  50010001500200025003000350040004500500055006789101112the number of training samples (balanced)error rate (%)Accuracy changes with the number of training samples  RBMOur method020406080100120140160180200220102030405060708090100the number of training samples for the rare class (unbalanced)error rate (%)Accuracy changes with the number of training samples  RBMOur methodAccepted as a workshop contribution at ICLR 2015  Model RBM DRBM Larochelle et al. (2012) RBM + NNet Larochelle et al. (2012) HDRBM Larochelle et al. (2012) HRBMh (η = 0.1, n = 2000) HRBMs (η = 0.1, n = 2000) HHRBM (η = 0.1, n = 1000, 500, 200 and 200) Ours (η = 0.01, n = 1500 and C = 0) Ours (η = 0.01, n = 1500 and C = 0.1) MNL corrMNL Shahbaba & Neal (2007) SVM Larochelle et al. (2012) NNet Larochelle et al. (2012)  Error rate (%) 24.9 27.6 26.8 23.8 30.6 63.7 32.0 30.1 23.6 30.8 79.3 32.8 28.2  Table 2: The experimental comparison on 20 news group dataset. We compare the performances between our method and other RBM models. It demonstrates that our method with hierarchical prior over labels can improve recognition accuracy.  training examples respectively. Then, we training our model with these samples, and test it on the test cases. Similarly, we did the same testing on classes ‘1’ to ‘9’ respectively. We tested our method for each rare class on the testing set, and show the over average error rate in Fig. 4(b), which clearly demonstrates that our method is much better than RBM on rare classes. Document Classiﬁcation We also evaluated our model on 20 news group dataset for document classiﬁcation. The 20 news group dataset3 has 18,846 articles with with 61188 vocabularies, which has been widely used in text categorization and document classiﬁcation. In the experiment, we tested our model on the version of the 20 news group dataset4, in order to make our results comparable to the current state of the art results. In the experiment, we used the hierarchical prior structure over label shown in Fig. 3 for HHRBM, HRBMh, HRBMs and our model. As for parameter setting, we use CD-1, and set the number of hidden states n = 2000, learning rate η = 0.1 and the maximum epoch equals to 100 for RBM related methods. For HHRBM, we set the number of hidden states to be 1000, 500, 200 and 200 respectively for each layer. As for our method, we set n = 1500, η = 0.01, C = 0.1 and maximum epoch 200. The results of different methods are shown in Table (2). Once again, our model outperforms the other RBM models, also get better results than SVM and neural network classiﬁers. HRBMs and corrMNL have bad performance in this dataset. The reason we guess is that HRBMs calculates the classiﬁcation probability for each class by multiplying the output probabilities along the path from root to the leaf associated to that class. Thus, HRBMs will prefer the high level class for unbalanced hierarchical structure. Note that the hierarchical tree in Fig. 3 is unbalanced structure. For HRBMs, ‘alt.Atheism’, ‘misc.forsale’ and ‘soc.religon.christian’ will have higher probability to be labeled compared to leafs (or classes) in the level 4. corrMNL may have the same problem as HRBMs. Another reason for the low performance is that corrMNL does not consider the parameter redundancy problem between adjacent layers as in our model. We also evaluate how the regularization term inﬂuences the performance. We set C = 0 to remove the orthogonal restriction, and get accuracy 30.1% in Table (2), which is signiﬁcant lower than the result with orthogonal restriction. Hence, it demonstrates that it is useful to introduce orthogonal restriction to the correlated hierarchical prior.  4 RELATED WORK  The hierarchical structure is organized according to the similarity of classes. Two classes are con- sidered similar if it is difﬁcult to distinguish one from the other on the basis of their representation. The similarity of classes increases as we descend the hierarchy. Thus, the hierarchical prior over  3http://people.csail.mit.edu/jrennie/20Newsgroups/ 4http://www.cs.toronto.edu/˜larocheh/public/datasets/20newsgroups/  20newsgroups_{train,valid,test}_binary_5000_voc.txt  7  Accepted as a workshop contribution at ICLR 2015  categories provides semantic meaning and valuable information among different classes; and thus to some extent it can assist classiﬁcation problems in hand Shahbaba & Neal (2007); Xiao et al. (2011); Rohit et al. (2013). Much work has extensively been done in the past years to exploit hierar- chical prior over labels for classiﬁcation problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007). Two most popular approaches to lever- age hierarchical prior can be categorized below. The ﬁrst approach classiﬁes each node recursively, by choosing the label of which the associated vector has the largest output score among its siblings till to a leaf node. An variant way is to compute the conditional probability for each class at each level, and then multiply these probabilities along every branch to compute the ﬁnal assignment prob- ability for each class. Xiao et al. introduced a hierarchical classiﬁcation method with orthogonal transfer Xiao et al. (2011), which requires the parameters of children nodes are orthogonal to those of its parents as much as possible. Another example is the nested multinomial logit model Shahbaba & Neal (2007), in which the nested classiﬁcation model for each node is statistically independent, conditioned on its parent in the upper levels. One weakness of this strategy for hierarchical classiﬁ- cation is that errors will propagate from parents to children, if any misclassiﬁcation happened in the top level. The other methodology for hierarchical classiﬁcation prefers to use the sum of parameters along the tree for classifying cases ended at leaf nodes. Cai and Hoffmann Cai & Hofmann (2004) proposed a hierarchical larger margin multi-class SVM with tree-induced loss functions. Similarly, Dekel et al. in Dekel et al. (2004) improved Cai & Hofmann (2004) into an online version for hi- erarchical classiﬁcation. Recently, Shahbaba et al. proposed a correlated multinomial logit model (corrMNL) Shahbaba & Neal (2007), whose regression coefﬁcients for each leaf node are repre- sented by the sum of parameters on all the branches leading to that class. Apart from the two approaches mentioned above, there are also other methods proposed in the past. Dumais and Chen trained different classiﬁers kind of layer by layer by exploring the hierarchical structure Dumais & Chen (2000). Cesa-Bianchi et al. combined Bayesian inference with the prob- abilities output from SVM classiﬁers in Cesa-Bianchi et al. (2006) for hierarchical classiﬁcation. Similarly, Gopal et al. Gopal et al. (2012) used Bayesian approach (with variational inference) with hierarchical prior for classiﬁcation problems.  5 CONCLUSION  We consider restricted Boltzmann machines (RBM) for classiﬁcation problems, with prior knowl- edge of sharing information among classes in a hierarchy. Basically, our model decompose clas- siﬁcation RBM into traditional RBM for representation learning and multi-class logistic model for classiﬁcation, and then introduce hierarchical prior over multi-class logistic model. In order to re- duce the redundancy between node parameters, we also introduce orthogonal restrictions in our objective function. To the best of our knowledge, this is the ﬁrst paper that incorporates hierarchical prior over RBM framework for classiﬁcation. We test our method on challenge datasets, and show promising results compared to benchmarks.  REFERENCES Bengio, Yoshua, Courville, Aaron, and Vincent, Pascal. Representation learning: A review and new  perspectives. TPAMI, 2012.  Cai, Lijuan and Hofmann, Thomas. Hierarchical document categorization with support vector ma- In Proceedings of the Thirteenth ACM International Conference on Information and chines. Knowledge Management, CIKM ’04, pp. 78–87, New York, NY, USA, 2004. ACM. ISBN 1- 58113-874-1.  Cesa-Bianchi, Nicol`o, Gentile, Claudio, and Zaniboni, Luca. Hierarchical classiﬁcation: Combining In Proceedings of the 23rd International Conference on Machine Learning, bayes with svm. ICML ’06, pp. 177–184, New York, NY, USA, 2006. ACM. ISBN 1-59593-383-2. doi: 10.1145/ 1143844.1143867. URL http://doi.acm.org/10.1145/1143844.1143867.  Dekel, Ofer, Keshet, Joseph, and Singer, Yoram. Large margin hierarchical classiﬁcation. In Pro- ceedings of the Twenty-ﬁrst International Conference on Machine Learning, ICML ’04, pp. 27– 34, New York, NY, USA, 2004. ACM.  8  Accepted as a workshop contribution at ICLR 2015  Deng, Jia, Dong, Wei, Socher, Richard, jia Li, Li, Li, Kai, and Fei-fei, Li. Imagenet: A large-scale  hierarchical image database. In CVPR, 2009.  Dumais, Susan and Chen, Hao. Hierarchical classiﬁcation of web content. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’00, pp. 256–263, New York, NY, USA, 2000. ACM. ISBN 1-58113-226-3. doi: 10.1145/345508.345593. URL http://doi.acm.org/10.1145/345508.345593.  Fellbaum, Christiane (ed.). WordNet An Electronic Lexical Database. The MIT Press, May 1998.  Goodman, Joshua. Classes for fast maximum entropy training. ICASSP, 2001.  Gopal, Siddharth, Yang, Yiming, Bai, Bing, and Niculescu-Mizil, Alexandru. Bayesian models for large-scale hierarchical classiﬁcation. In Bartlett, Peter L., Pereira, Fernando C. N., Burges, Christopher J. C., Bottou, Lon, and Weinberger, Kilian Q. (eds.), NIPS, pp. 2420–2428, 2012.  Hinton, G E and Salakhutdinov, R R. Reducing the dimensionality of data with neural networks.  Science, 313(5786):504–507, July 2006.  Hinton, Geoffrey E. Training products of experts by minimizing contrastive divergence. Neural  Comput., 14(8):1771–1800, August 2002. ISSN 0899-7667.  Hinton, Geoffrey E. Learning multiple layers of representation. Trends in Cognitive Sciences, 11:  428–434, 2007.  Koller, Daphne and Sahami, Mehran. Hierarchically classifying documents using very few words. In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97, pp. 170–178, San Francisco, CA, USA, 1997. Morgan Kaufmann Publishers Inc. ISBN 1-55860- 486-3.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In Advances in Neural Information Processing Systems, 2012.  Larochelle, Hugo and Bengio, Yoshua. Classiﬁcation using discriminative restricted boltzmann machines. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pp. 536–543, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-205-4.  Larochelle, Hugo, Mandel, Michael, Pascanu, Razvan, and Bengio, Yoshua. Learning algorithms for the classiﬁcation restricted boltzmann machine. J. Mach. Learn. Res., 13(1):643–669, March 2012. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=2503308. 2188407.  Marszalek, Marcin and Schmid, Cordelia. Semantic hierarchies for visual object recognition. In  CVPR. IEEE Computer Society, 2007.  McCallum, Andrew, Rosenfeld, Ronald, Mitchell, Tom M., and Ng, Andrew Y.  Improving text In Proceedings of the Fifteenth Interna- classiﬁcation by shrinkage in a hierarchy of classes. tional Conference on Machine Learning, pp. 359–367, San Francisco, CA, USA, 1998. Morgan Kaufmann Publishers Inc.  Rohit, Babbar, Ioannis, Partalas, Eric, Gaussier, and Massih-Reza, Amini. On ﬂat versus hierarchical classiﬁcation in large-scale taxonomies. In Neural Information Processsing Systems (NIPS), 2013.  Salakhutdinov, Ruslan, Mnih, Andriy, and Hinton, Geoffrey. Restricted boltzmann machines for collaborative ﬁltering. In Proceedings of the 24th International Conference on Machine Learning, ICML ’07, pp. 791–798, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-793-3.  Shahbaba, Babak and Neal, Radford M. Improving classiﬁcation when a class hierarchy is available  using a hierarchy-based prior. Bayesian Analysis, 2(1):221–237, 2007.  Weigend, Andreas S., Wiener, Erik D., and Pedersen, Jan O. Exploiting hierarchy in text catego-  rization. Inf. Retr., 1(3):193–216, 1999.  Xiao, Lin, Zhou, Dengyong, and Wu, Mingrui. Hierarchical classiﬁcation via orthogonal transfer.  In ICML, pp. 801–808, 2011.  9  ",
1407.2538,2015, Learning Deep Structured Models,"['Learning Deep Structured Models', 'Liang-Chieh Chen', 'Alexander Schwing', 'Alan Yuille', 'and Raquel Urtasun']",https://arxiv.org/pdf/1407.2538,"5 1 0 2    r p A 7 2         ]  G L . s c [      3 v 8 3 5 2  .  7 0 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  LEARNING DEEP STRUCTURED MODELS  Liang-Chieh Chen ∗ Department of Computer Science, UCLA, lcchen@cs.ucla.edu  Alexander G. Schwing ∗ Department of Computer Science, University of Toronto, aschwing@cs.toronto.edu  Alan L. Yuille Department of Statistics, UCLA, yuille@stat.ucla.edu  Raquel Urtasun Department of Computer Science, University of Toronto, urtasun@cs.toronto.edu  ABSTRACT  Many problems in real-world applications involve predicting several random vari- ables which are statistically related. Markov random ﬁelds (MRFs) are a great mathematical tool to encode such relationships. The goal of this paper is to com- bine MRFs with deep learning algorithms to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efﬁcient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as multi-class classiﬁcation of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in signiﬁcant performance gains.  1  INTRODUCTION  Deep learning algorithms attempt to model high-level abstractions of the data using architec- tures composed of multiple non-linear transformations. A multiplicity of variants have been pro- posed (Hinton et al., 1984; LeCun et al., 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural lan- guage processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014). Recently, state-of-the-art results have been achieved in many computer vision tasks, outperforming competitive methods by a large margin (Krizhevsky et al., 2013; Girshick et al., 2014). Deep neural networks can, however, be even more powerful when combined with graphical models in order to capture the statistical dependencies between the variables of interest. For example, Deng et al. (2014) exploit mutual exclusion, overlapping and subsumption properties of class labels in or- der to better predict in large scale classiﬁcation tasks. In pose estimation, more accurate predictions can be obtained when encoding the spatial relationships between joint locations (Tompson et al., 2014). It is, however, an open problem how to develop scalable deep learning algorithms that can learn higher-order knowledge taking into account the output variable’s dependencies. Existing approaches often rely on a two-step process (Nowozin et al., 2011; Xu et al., 2014) where a non-linear classiﬁer that employs deep features is trained ﬁrst, and its output is used to generate potentials for the struc- tured predictor. This piece-wise training is, however, suboptimal as the deep features are learned  ∗The ﬁrst two authors contributed equally to this work.  1  Accepted as a workshop contribution at ICLR 2015  while ignoring the dependencies between the variables of interest, e.g., independently learned seg- mentation and detection features (Hariharan et al., 2014) might be focusing on predicting the same examples correctly. But when learned jointly they can improve their predictive power by exploiting complementary information to ﬁx additional mistakes. In this paper we extend deep learning algorithms to learn complex representations taking into ac- count the dependencies between the output random variables. Towards this goal, we propose a learning algorithm that is able to learn structured models with arbitrary graphs jointly with deep features that form Markov random ﬁeld (MRF) potentials. Our approach is efﬁcient as it blends learning and inference resulting in a single loop algorithm which makes use of GPU acceleration. We demonstrate the effectiveness of our method in the tasks of predicting words from noisy images, and multi-class classiﬁcation of Flickr photographs. We show that joint learning of deep features and MRF parameters results in big performance gains.  2 LEARNING DEEP STRUCTURED MODELS  a search over a space of size(cid:81)N  be a product space, i.e., Y = (cid:81)N  In this section we investigate how to learn ‘deep features’ that take into account the dependen- cies between the output variables. Let y ∈ Y be the set of random variables y = (y1, . . . , yN ) that we are interested in predicting. In this work we assume the space of valid conﬁgurations to i=1 Yi, and the domain of each individual variable yi to be dis- crete, i.e., Yi = {1, . . . ,|Yi|}. Given input data x ∈ X and parameters w ∈ RA of the function F (x, y; w) : X × Y × RA → R, inference amounts to ﬁnding the highest scoring conﬁguration y∗ = arg maxy F (x, y; w). Note that if F is a deep network and there are no connections between the output variables to be predicted, inference corresponds to a forward pass to evaluate the function, followed by independently ﬁnding the largest response for each variable. This can be interpreted as inference in a graphical model with only unary potentials. However, for arbitrary graphical models it is NP-hard to ﬁnd the maximizing conﬁguration y∗ since the inference program generally requires i=1 |Yi|. Note also that log-linear models are a special case of this program, with F (x, y; w) = w(cid:62)φ(x, y) and φ(x, y) denoting a feature vector computed using the input-output pair (x, y). In this work, we consider the general setting where F (x, y; w) is an arbitrary scalar-valued func- tion of w and (x, y). In our experiments F is a function composition of non-linear base mappings like convolutions, rectiﬁcations, pooling etc. We let the probability of an arbitrary conﬁguration ˆy be given by the annealed soft-max p(x,y)(ˆy|w, (cid:15)) = Z(cid:15)(x,w) exp(F (x, ˆy; w))1/(cid:15). Hereby Z(cid:15)(x, w) refers to the partition function, normalizing the distribution p(x,y) to lie within the probability sim- ˆy∈Y exp(F (x, ˆy; w))1/(cid:15). The annealing/temperature parameter (cid:15) ≥ 0 is used to adjust the uniformity of the distribution. We consider general graphical models where the computation of Z(cid:15)(x, w) is #P-hard.  plex ∆ via Z(x, w) = (cid:80)  1  2.1 LEARNING VIA GRADIENT DESCENT During learning, given a training set D of input-output pairs (x, y) ∈ D, we are interested in ﬁnding the parameters w of the model. We do so by maximizing the data likelihood, i.e., minimizing the  negative log-likelihood − ln(cid:81)  (cid:88) (x,y)∈D p(x,y)(y|w, (cid:15)) which yields  ((cid:15) ln Z(cid:15)(x, w) − F (x, y; w)) .  min  w  (x,y)∈D  (1)  this  (cid:80)  is equivalent  Note that to maximizing the cross-entropy between a target distri- bution p(x,y),tg(ˆy) = δ(ˆy = y) placing all its mass on the groundtruth label, and the model distribution p(x,y)(ˆy|w, (cid:15)). is equivalently obtained by (x,y),ˆy∈Y p(x,y),tg(ˆy) ln p(x,y)(ˆy|w, (cid:15)). It is easily possible to incorporate more general tar- maxw get distributions into Eq. (1). Note also that (cid:15) = 0 recovers the structured hinge loss objective. (x,y)∈D − ln p(x,y)(y|w, (cid:15)), Minimizing Eq. (1) w.r.t. w requires computation of the gradient ∂ ∂w which is given by a transformed difference between the distributions of the model p(x,y)(ˆy|w, (cid:15)) and  Hence Eq.  (cid:80)  (1)  2  Accepted as a workshop contribution at ICLR 2015  Repeat until stopping criteria 1. Forward pass to compute F (x, ˆy; w) 2. Compute p(x,y)(ˆy|w, (cid:15)) 3. Backward pass via chain rule to obtain gradient 4. Update parameters w  Figure 1: Gradient descent algorithm for learning deep structured models.  the target p(x,y),tg(ˆy):(cid:88)  (cid:88)  (x,y)∈D  ˆy∈Y  F (x, ˆy; w)(cid:0)p(x,y)(ˆy|w, (cid:15)) − p(x,y),tg(ˆy)(cid:1) .  ∂ ∂w  (2)  A gradient descent algorithm for minimizing Eq. (1) will iterate between the following steps: (i) For a given w evaluate the function F , (ii) compute the model distribution p(x,y)(ˆy|w, (cid:15)), (iii) propagate the difference between the model and target distribution using a backward pass (resembling the chain rule for composite functions) and (iv) update the parameters w. This is summarized in Fig. 1.  2.2 APPROXIMATE LEARNING Note that for general graphical models the exact computation of p(x,y)(ˆy|w, (cid:15)) is not possible. As a consequence it is intractable to compute the exact gradient of the cost-function given in Eq. (2) and one has to resort to approximate solutions. Inspired by approximations used for log-linear models, we make use of the following identity (Wain- wright & Jordan, 2008; Koller & Friedman, 2009):  (cid:15) ln Z(cid:15)(x, w) = max  p(x,y)(ˆy)∈∆  E[F (x, ˆy; w)] + (cid:15)H(p(x,y)),  (3)  subset of variables yr, i.e., F (x, y; w) =(cid:80)  where E denotes an expectation over p(x,y)(ˆy) and H refers to the entropy. For most applications, F (x, y; w) decomposes into a sum of functions, each depending on a local r∈R fr(x, yr; w). Hereby r is a restriction of the variable tuple y = (y1, . . . , yN ) to the subset r ⊆ {1, . . . , N}, i.e., yr = (yi)i∈r. All subsets r required to compute the model function F are summarized in the set R. Plugging this decomposition into Eq. (cid:15) ln Z(cid:15)(x, w) via maxp(x,y)(ˆy)∈∆  (3), we equivalently get the log-partition function p(x,y),r(ˆyr)fr(x, ˆyr; w) + (cid:15)H(p(x,y)), where we use  (cid:80)  marginals p(x,y),r(ˆyr) =(cid:80)  r,ˆyr p(x,y)(y).  y\yr  Despite the assumed locality of the scoring function, the learning task remains computationally challenging since the entropy H(p(x,y)) can only be computed exactly for a very small set of models, e.g., models for which the joint distribution p(x,y)(y) is equivalently described by low tree-width models. In addition, the marginalization constraints are exponential in size. To deal with both issues a common solution in log-linear models is to approximate the true marginals p(x,y),r with local beliefs b(x,y),r that are not required to fulﬁll marginalization constraints globally, but only locally (Wainwright & Jordan, 2008). That means marginals b(x,y),r are not required to arise from a common joint distribution p(x,y). In addition, we approximate the entropy via the fractional r crH(b(x,y),r). Counting numbers cr are employed to weight the marginal entropies. Putting all this together, we obtain the following approximation for (cid:15) ln Z(cid:15)(x, w):  entropy (Wiegerinck & Heskes, 2003), i.e., H(p(x,y)) ≈ (cid:80)  b(x,y),r(ˆyr)fr(x, ˆyr; w) +  (cid:15)crH(b(x,y),r).  (4)  (cid:88) (cid:40) ∀r ∀r, ˆyr, p ∈ P (r) (cid:80)  r,ˆyr  max  b(x,y)∈C(x,y)  C(x,y) =  (cid:88)  r  Hereby beliefs are constrained to the local polytope  b(x,y),p(ˆyp) = b(x,y),r(ˆyr),  b(x,y),r ∈ ∆ ˆyp\ˆyr  3  Accepted as a workshop contribution at ICLR 2015  Let ˜cr,p = cp/(cr +(cid:80)  p(cid:48)∈P (r) cp(cid:48)). Repeat until stopping criteria  1. Forward pass to compute fr(x, ˆyr; w) ∀(x, y), r, yr 2. Get beliefs b(x,y),r ∝ exp  ˆfr(x,ˆyr;w,λ)  µ(x,y),p→r(ˆyr) = (cid:15)cp ln  by iterating over r: ∀(x, y), p ∈ P (r), ˆyr  λ(x,y),p→p(cid:48)(ˆyp) + (cid:80)  r(cid:48)∈C(p)\r  fp(x, ˆyp; w) − (cid:80) (cid:88)  (cid:15)cr  (cid:88) fr(x, ˆyr; w) +  ˆyp\ˆyr  exp  p(cid:48)∈P (p)  λ(x,y),c→r(ˆyc) +  µ(x,y),p→r(ˆyr)  (cid:15)cp  (cid:88)  λ(x,y),r(cid:48)→p(ˆyr(cid:48))  − µ(x,y),p→r(ˆyr)  λ(x,y),r→p(ˆyr) ∝ ˜cr,p  3. Backward pass via chain-rule for gradient g =(cid:80)  c∈C(r)  4. Update parameters w using stepsize η via w ← w − ηg  (x,y),r,ˆyr  p∈P (r) b(x,y),r(ˆyr)∇wfr(x, ˆyr; w)−∇wF (w)  Figure 2: Efﬁcient learning algorithm that blends learning and inference.  with P (r) the set of parents of region r, i.e., P (r) ⊆ {p ∈ R : r ⊂ p}, which subsumes those regions for which we want the marginalization constraint to hold. Conversely, we deﬁne the set of children as C(r) = {c ∈ R : r ∈ P (c)}. We can thus rewrite the learning problem by plugging the approximations derived in Eq. (4) into Eq. (1). This gives rise to the new approximated learning program  (cid:88)  r   − F (x, y; w)  . (5)  (cid:88)   max  b(x,y)∈C(x,y)  (cid:88)  r,ˆyr  min  w  (x,y)∈D  b(x,y),r(ˆyr)fr(x, ˆyr; w) +  (cid:15)crH(b(x,y),r)  To iteratively update the parameters for the non-smooth approximated cost function given in Eq. (5) we require a sub-gradient w.r.t. w, which in turn requires to solve the maximization w.r.t. the beliefs b exactly. This is a non-trivial task in itself as inference in general graphical models is NP-hard. Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed. Importantly, note that combining the procedure outlined in Fig. 1 with iterative message passing to approximate p(x,y)(ˆy|w, (cid:15)) results in a double-loop algorithm which would be slow for many graphical models of interest.  2.3 EFFICIENT APPROXIMATE LEARNING BY BLENDING LEARNING AND INFERENCE  In this section we propose a more efﬁcient algorithm that is based on the principle of blending learning (i.e., parameter updates) and inference. Thus we are interested in only performing a single message passing iteration before updating the parameters w. Note that simply reducing the number of iterations is generally not an option as the obtained beliefs b(x,y),r are by no means accurate. However, assuming all counting numbers cr to be positive, we can derive an algorithm that is able to interleave minimization w.r.t. w and maximization of the beliefs b. Such a procedure is more efﬁcient as we are able to update the parameters w much more frequently. To interleave both programs we convert maximization of the beliefs into a minimization by employ- ing the dual program as detailed in the following claim. This is possible since the maximization problem is concave in b(x,y) if ∀r, (cid:15)cr ≥ 0.  Claim 1 Assume (cid:15)cr ≥ 0 ∀r and let F (w) = (cid:80) constraint(cid:80)  (x,y)∈D F (x, y; w) denote the sum of empirical function observations. Let λ(x,y),r→p(ˆyr) be the Lagrange multipliers for each marginalization b(x,y),p(ˆyp) = b(x,y),r(ˆyr) within the polytope C(x,y). Then the approximated  ˆyp\ˆyr  general structured prediction task shown in Eq. (5) is equivalent to  (cid:88)  (cid:88)  min w,λ  (cid:15)cr ln  (x,y),r  ˆyr  exp  ˆfr(x, ˆyr; w, λ)  (cid:15)cr  − F (w),  (6)  4  Accepted as a workshop contribution at ICLR 2015  banal  drein  julep  yojan  resty  mothy  Figure 3: Samples from the Word50 dataset. High degree of rotation, scaling and translation.  where we employed the re-parameterization score  ˆfr(x, ˆyr; w, λ) = fr(x, ˆyr; w) +  c∈C(r)  (cid:80) (cid:80)  λ(x,y),c→r(ˆyc) − (cid:80) b(x,y),r(ˆyr) ˆfr(x, ˆyr; w, λ) +(cid:80)  p∈P (r)  λ(x,y),r→p(ˆyr).  (cid:15)cr  r,ˆyr  ˆfr(x,ˆyr;w,λ)  Proof: To obtain the dual of the maximization w.r.t. b(x,y) we utilize its Lagrangian L(x,y) = r(cid:15)crH(b(x,y),r). Maximization of the Lagrangian w.r.t. the pri- mal variables b is possible by employing the relationship stated in Eq. (3) locally ∀r. We then obtain the dual function being the ﬁrst term in Eq. (6). For strict convexity, i.e., (cid:15)cr > 0, we recon- struct the beliefs to be proportional to the exponentiated, loss-augmented re-parameterization score b(x,y),r ∝ exp . For (cid:15)cr = 0 the beliefs correspond to a uniform distribution over the set (cid:4) of maximizers of the loss-augmented re-parameterization score ˆfr(x, ˆyr; w, λ). It is important to note that by applying duality we managed to convert the min-max task in Eq. (5) into a single minimization as shown in Eq. (6). Performing block coordinate descent updates to minimize Eq. (6), we are therefore able to interleave both, updating the weights (i.e., learning) and the messages (i.e., inference). This results in a more efﬁcient algorithm, as inference does not have to be run until convergence. Even a single update of the messages sufﬁces. We note that this is possible only if (cid:15)cr ≥ 0 ∀r. Strictly speaking, we require concavity only within the set of feasible beliefs C(x,y). However, for simplicity we neglect this extension in the following. Fig. 2 summarizes our efﬁcient deep structured prediction algorithm which iterates between the following steps. Given parameters w we perform a standard forward pass to compute fr(x, ˆyr; w) for all regions. We then iterate through all regions r and use block-coordinate descent to ﬁnd the globally optimal value of Eq. (6) w.r.t. λ(x,y),r→p(ˆyr) ∀(x, y), ˆyr, p ∈ P (r). This can be done in closed form and therefore is computed very efﬁciently. We refer the reader to Schwing (2013) for a derivation in the log-linear setting. We then compute the gradient using a standard backward pass before we update the parameters by performing a step of size η along the negative gradient.  2.4  IMPLEMENTATION DETAILS  We implemented the general algorithm presented in Fig. 2 in C++ as a library for Linux, Windows and OS X platforms. It supports usage of the GPU for the forward and backward pass using both, standard linear algebra packages and manually tuned GPU-kernels. In addition to standard gradi- ent descent, we allow speciﬁcation of both mini-batches, moments and different regularizers like 2-norm and ∞-norm. Between iterations the step-size can be reduced based on either the nega- tive log-likelihood or validation set performance. Contrasting available deep learning packages, our function F is speciﬁed using a general computation tree. Hence we support an arbitrarily nested function structure composed of data, parameters and function prototypes (convolution, afﬁne func- tion aka fully connected, dropout, local response normalization, pooling, rectiﬁed linear, sigmoid and softmax units). The aforementioned library is accompanied by a program performing learning, inference and gradient checks. To accommodate for large datasets it reads data from HDF5 storage while a second thread simultaneously performs the computation. Google protocol buffers are em- ployed to effectively specify the function F without the need to modify any source code. We will release this library upon publication. We believe that it will be useful for many researchers.  5  Accepted as a workshop contribution at ICLR 2015  Graph  MLP  1st order Markov One Layer  2nd order Markov One Layer  1st order Markov Two Layer  2nd order Markov Two Layer  Method  H1 = 128 H1 = 256 H1 = 512 H1 = 768 H1 = 1024 Unary only 8.60 / 61.32 10.80 / 64.41 12.50 / 65.69 12.95 / 66.66 13.40 / 67.02 JointTrain 16.80 / 65.28 25.20 / 70.75 31.80 / 74.90 33.05 / 76.42 34.30 / 77.02 PwTrain 12.70 / 64.35 18.00 / 68.27 22.80 / 71.29 23.25 / 72.62 26.30 / 73.96 PreTrainJoint 20.65 / 67.42 25.70 / 71.65 31.70 / 75.56 34.50 / 77.14 35.85 / 78.05 JointTrain 25.50 / 67.13 34.60 / 73.19 45.55 / 79.60 51.55 / 82.37 54.05 / 83.57 10.05 / 58.90 14.10 / 63.44 18.10 / 67.31 20.40 / 70.14 22.20 / 71.25 PwTrain PreTrainJoint 28.15 / 69.07 36.85 / 75.21 45.75 / 80.09 50.10 / 82.30 52.25 / 83.39 H1 = 512 H2 = 64 H2 = 128 H2 = 256 H2 = 512 Unary only 15.25 / 69.04 18.15 / 70.66 19.00 / 71.43 19.20 / 72.06 20.40 / 72.51 JointTrain 35.95 / 76.92 43.80 / 81.64 44.75 / 82.22 46.00 / 82.96 47.70 / 83.64 34.85 / 79.11 38.95 / 80.93 42.75 / 82.38 45.10 / 83.67 45.75 / 83.88 PwTrain PreTrainJoint 42.25 / 81.10 44.85 / 82.96 46.85 / 83.50 47.95 / 84.21 47.05 / 84.08 JointTrain 54.65 / 83.98 61.80 / 87.30 66.15 / 89.09 64.85 / 88.93 68.00 / 89.96 PwTrain 39.95 / 81.14 48.25 / 84.45 52.65 / 86.24 57.10 / 87.61 62.90 / 89.49 PreTrainJoint 62.60 / 88.03 65.80 / 89.32 68.75 / 90.47 68.60 / 90.42 69.35 / 90.75  H2 = 32  Table 1: Word / Character accuracy. Performance improves as (1) joint-training is employed, (2) the model is more structured, and (3) deeper unary classiﬁers are utilized. The number of hidden units for the ﬁrst and second layer are denoted as H1 and H2 respectively.  Unary weights  distance-1 edges  distance-2 edges Neg. Log-Likelihood  Figure 4: darker, the larger the weight. (right) Negative log-likelihood for different learning approaches.  (left) Subset of the learned unary weights. Pairwise weights (middle two panels), the  3 EXPERIMENTAL EVALUATION  We demonstrate the performance of our model on two tasks: word recognition and image classiﬁ- cation. We investigate four strategies to learn the model parameters. ‘Unary only’ denotes training only unary classiﬁers while ignoring the structure of the graphical model, i.e., pairwise weights are equal to 0. ‘JointTrain’ initializes all weights at random and trains them jointly. ‘PwTrain’ uses piecewise training by ﬁrst training the unary potentials and then keeping them ﬁxed when learn- ing the pairwise potentials. ‘PreTrainJoint’ pre-trains the unaries but jointly optimizes pairwise weights as well as unary weights in a second step.  3.1 WORD RECOGNITION: WORD50  Our ﬁrst task consists of word recognition from noisy images. Towards this goal, we created a challenging dataset by randomly selecting 50 words, each consisting of ﬁve characters. We then generated writing variations of each word as follows: we took the lower case characters from the Chars74K dataset (de Campos et al., 2009), and inserted them in random background image patches (similar to Larochelle et al. (2007)) by alpha matting, i.e., characters have transparency. To increase the difﬁculty, we perturbed each character image of size 28× 28 by scaling, rotation and translation. As shown in Fig. 3 the task is very challenging, some characters are fairly difﬁcult to recognize even for humans. We denote the resulting dataset ‘Word50.’ The training, validation and test sets have 10, 000, 2, 000 and 2, 000 variations of words respectively. We experimented with graphical models composed of unary and pairwise regions deﬁned over ﬁve random variables, one per character. We encode unary potentials fr(x, yi; wu) using multi-layer per- ceptrons (MLPs) with rectiﬁed linear units (ReLU). Unless otherwise stated, we deﬁne all pairwise  6  abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz00.511.522.533.544.55x 1043004005006007008009001000Neg. Log−LikelihoodIteration  JointTrainPwTrainPreTrainJointAccepted as a workshop contribution at ICLR 2015  One-Layer MLP Chain  Two-Layer MLP Chain  Figure 5: Learning non-linear pairwise functions: Word recognition as a function of the number of hidden units for the unary potential. Colors represent different number of hidden units for the pairwise potentials. The y-axis shows the word accuracy of using Linear function, or 16 (PairH16), 32 (PairH32), and 64 (PairH64) hidden units for the pairwise function.  female/indoor/portrait female/indoor/portrait  sky/plant life/tree water/animals/sea sky/plant life/tree water/animals/sky  animals/dog/indoor  animals/dog  indoor/ﬂower/plant life  ∅  Figure 6: Flickr test set images and some assigned tags as well as our predictions (bottom row).  (cid:88)  interactions via  Wmn · δ(yi = m, yj = n),  mn  fr(x, yi, yj; wp) =  (7) where r = {i, j}, wp = {W}, Wmn is the element of matrix W , and δ refers to the indicator function. For all experiments, we share all unary weights across the nodes of the graphical model as well as all pairwise weights for all edges. Note that due to the use of ReLU units, the negative log-likelihood is non-smooth, non-linear and non-convex w.r.t. w. Because of the non-smoothness of F , we utilize momentum based sub-gradient descent methods to estimate the weights. In particular, we use a mini-batch size of 100, a step size of 0.01 and a momentum of 0.95. If the unary potential is pre- trained, the initial step size is reduced to 0.001. All the unary classiﬁers are trained with 100, 000 iterations over mini-batches. For all experiments, the validation set is only used to decrease the step size, i.e., if the accuracy on the validation set decreases, we reduce the step size by 0.5. We use (cid:15) = 1, set cr = 1 for all regions r, and perform 10 message passing iterations to compute the marginal beliefs b(x,y),r at step 2 in Fig. 2 when dealing with loopy models. We experiment with two graphical models, Markov models of ﬁrst (i.e., there are links only between yi and yi+1) and second order (i.e., there are links between yi and yi+1, yi+2) as well as two types of unary potentials with varying degree of structure. We report two metrics, the average character and word accuracy, which correspond to Hamming loss and zero-one loss respectively. Table 1 depicts the results for the different models, learning strategies and number of hidden units. We observe the following trends. Joint training helps: Joint training with pre-trained unary classiﬁers (PreTrainJoint) outperforms all the other approaches in almost all cases. The piecewise training method (PwTrain), unable to adapt the non-linearities while learning pairwise weights, often leads to performance worse than joint training. Structure helps: Adding structure to the model is key to capture complex dependencies. As shown in Table 1, more structured models (i.e., second order Markov model) consistently improves perfor- mance.  7  H1=128H1=256H1=512H1=768H1=1024161820222426283032343638  LinearPairH16PairH32PairH64H2=32H2=64H2=128H2=256H2=512343638404244464850  LinearPairH16PairH32PairH64Accepted as a workshop contribution at ICLR 2015  (a)  (b)  Figure 7: (a) Correlation matrix (i.e., pairwise potentials) learned on the Flickr dataset. (b) Blending learning and inference speeds-up training signiﬁcantly.  Deep helps: We tested our models using one layer and two-layer perceptrons with both short- range and long-range connections in the MRF. For the two-layer MLP, the number of hidden units in the ﬁrst layer is ﬁxed to H1 = 512, and we varied the number of hidden units H2 in the second layer. As shown in Table 1 we observe that the deeper and the more structured the model, the better the performance we achieve. As expected, performance also grows with the number of hidden units. Efﬁciency: Using GPUs, it takes on average 0.064s per iteration for the 1st order Markov model and 0.104s for the 2nd order Markov model. The time employed for training the one layer vs. the multi-layer models is approximately the same. Note that our approach is very efﬁcient, as this is the time per iteration to train 831,166 weights. Learned parameters: As shown in the left column of Fig. 4, the learned unary weights resemble character strokes. The middle two panels show the learned pairwise weights for distance-1 edges (i.e., edges with only neighboring connections) and distance-2 edges (i.e., edges connecting every other variable). For example, it shows that ‘q’ is likely to be followed by ‘u,’ and ‘e’ is likely to be distance-2 away from ‘q’ in this dataset. On the right-most panel, we also show the negative log-likelihood as a function of the number of joint training iterations. PreTrainJoint can achieve the lowest cost value, while PwTrain has the highest value. Non-linear pairwise functions: To further demonstrate the generality of our approach, we replaced the linear pairwise function in Eq. (7) by a one-layer MLP, while keeping the other settings identical. For this experiment we utilize a 1st order Markov model. As shown in Fig. 5, our model attains best performance when using a non-linear pairwise function. We found 16 to 64 hidden units for the non-linear pairwise function to be sufﬁcient for modeling the bi-gram combinations in this dataset. In this case the largest model has 974,846 weights and training takes on average 0.068s per iteration.  3.2  IMAGE CLASSIFICATION: FLICKR  We next evaluate the importance of blending learning and inference. Towards this goal, we make use of the Flickr dataset, which consists of 10, 000 training and 10, 000 test images from Flickr. The task is to predict which of 38 possible tags should be assigned to each image. Fig. 6 shows some examples. The graphical model has 38 binary random variables, each denoting the presence/absence of a particular tag. We deﬁne the non-linear unaries fr(x, yi; wu) using the 8-layer deep-net archi- tecture from Krizhevsky et al. (2013) followed by a 76-dimensional top layer. Hence the function is composed out of two subsequent stacks of convolution, rectiﬁed linear (ReLU), pooling and local response normalization units. Those are followed by three convolution–ReLU function pairs. After- wards pooling is applied before two fully-connected–ReLU–dropout combinations are employed to yield the input into a fully connected layer which ﬁnally computes the unary potentials. We employ pairwise potentials similar to Eq. (7) which now fully model the correlations between any pair of  8  0.000.680.040.060.020.240.03−0.00−0.010.010.04−0.00−0.05−0.010.07−0.01−0.00−0.120.040.010.010.020.040.020.680.000.060.06−0.000.360.03−0.08−0.05−0.030.02−0.06−0.12−0.050.74−0.04−0.03−0.210.01−0.03−0.03−0.030.05−0.030.040.060.000.05−0.060.07−0.12−0.07−0.35−0.03−0.46−0.02−0.340.110.02−0.15−0.14−0.01−0.07−0.210.03−0.080.06−0.030.060.060.050.000.100.110.070.090.030.100.010.100.020.090.060.080.070.070.080.060.090.090.080.100.02−0.00−0.060.100.000.040.080.050.160.17−0.020.09−0.020.060.030.140.360.060.050.010.080.140.060.100.240.360.070.110.040.000.010.03−0.020.05−0.020.04−0.010.030.120.020.01−0.070.050.050.030.040.070.050.030.03−0.120.070.080.010.000.020.140.070.140.040.050.030.060.080.07−0.030.360.100.040.050.040.07−0.00−0.08−0.070.090.050.030.020.000.020.07−0.030.070.340.04−0.040.040.040.020.050.060.060.060.020.07−0.01−0.05−0.350.030.16−0.020.140.020.000.120.220.040.24−0.02−0.000.440.12−0.040.100.300.010.230.050.110.01−0.03−0.030.100.170.050.070.070.120.00−0.000.090.090.070.010.120.260.060.060.100.070.120.070.180.040.02−0.460.01−0.02−0.020.14−0.030.22−0.000.000.010.04−0.050.060.08−0.04−0.060.140.09−0.000.060.030.02−0.00−0.06−0.020.100.090.040.040.070.040.090.010.000.040.07−0.010.060.090.260.060.050.070.090.050.09−0.05−0.12−0.340.02−0.02−0.010.050.340.240.090.040.040.00−0.03−0.070.090.010.010.080.680.020.05−0.070.10−0.01−0.050.110.090.060.030.030.04−0.020.07−0.050.07−0.030.00−0.010.030.030.030.050.010.060.060.040.070.070.740.020.060.030.120.06−0.04−0.000.010.06−0.01−0.07−0.010.000.00−0.01−0.100.04−0.020.010.000.060.01−0.01−0.04−0.150.080.140.020.080.040.440.120.080.060.090.030.000.000.09−0.000.070.110.050.22−0.010.10−0.00−0.03−0.140.070.360.010.070.040.120.26−0.040.090.010.03−0.010.090.000.050.020.030.050.100.030.27−0.12−0.21−0.010.070.06−0.07−0.030.02−0.040.06−0.060.260.010.03−0.10−0.000.050.000.020.000.220.03−0.010.050.040.01−0.070.080.050.050.360.050.100.060.140.060.080.050.040.070.020.020.000.110.060.080.070.060.01−0.03−0.210.060.010.050.100.060.300.100.090.050.680.01−0.020.110.030.000.110.000.040.09−0.000.120.01−0.030.030.090.080.030.040.060.010.07−0.000.070.020.060.010.050.050.220.060.040.000.060.050.070.02−0.03−0.080.090.140.040.050.060.230.120.060.090.050.060.000.220.100.030.080.090.060.000.060.100.040.050.060.080.060.070.040.020.050.070.030.05−0.070.040.06−0.010.03−0.010.07−0.000.050.060.000.070.02−0.03−0.030.100.100.050.070.070.110.180.020.090.100.070.010.100.270.050.060.120.070.100.070.00femalepeopleindoorbabyseaportraittransportflowerskylakestructuresbirdplant lifefoodmalecloudswateranimalscartreedogsunsetnightriverfemalepeopleindoorbabyseaportraittransportflowerskylakestructuresbirdplant lifefoodmalecloudswateranimalscartreedogsunsetnightriver05000100000246810x 105Time [s]Neg. Log−Likelihood  w/o blendw blend05000100000200040006000800010000Time [s]Training error  w/o blendw blendAccepted as a workshop contribution at ICLR 2015  output variables. This amounts to a total of 57, 182, 408 parameters arising from the convolutional units, fully connected units and corresponding biases as well as the pairwise weights. We use a momentum based sub-gradient method for training with a mini-batch size of 300, a step size of 0.0001, a momentum of 0.95 and set (cid:15) = 1 and cr = 1 ∀r. We initialized the deep-net parameters using a model pre-trained on ImageNet (Deng et al., 2009). Our error metric is the classiﬁcation error (i.e., Hamming loss). Joint training helps: The mean error for unary only potentials (‘Unary only’), piecewise training (‘PwTrain’) and joint pretraining (‘PreTrainJoint’) is 9.36%, 7.70% and 7.25% respectively. Similar to the Word50 dataset we observe that joint training is beneﬁcial. We provide examples for perfect (two left-most images), roughly accurate and failing predictions (right image) in Fig. 6. Learned pairwise weights: In Fig. 7(a) we illustrate the learned correlations for a subset of the 38 classes. We observe that the class ‘people’ correlates highly with ‘female,’ ‘male,’ and ‘portrait.’ The ‘indoor’ tag does not co-occur with ‘sky,’ ‘structures,’ ‘plant life’ and ‘tree.’ ‘Sea’ appears typically with ‘water,’ ‘clouds,’ ‘lake’ and ‘sky.’ Efﬁciency of Blending: To illustrate that blending is indeed beneﬁcial we compare the negative log-likelihood and the training error as a function of run-time in Fig. 7(b). The standard approach is limited to 20 iterations of message passing to avoid time-consuming, repeated computation of a stopping criterion involving both the approximated log-partition function and its dual. As show in Fig. 7(b) blending learning and inference speeds up parameter estimation signiﬁcantly. For larger graphical models, we expect the differences to be even more signiﬁcant.  4 DISCUSSION & CONCLUSION  Joint training of neural networks and graphical models: Neural Networks have been incorpo- rated as unary potentials in graphical models. One of the earliest works by Bridle (1990) jointly opti- mizes a system consisting of multilayer perceptrons and hidden Markov models for speech recogni- tion. For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition. Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities. However, they assume that exact inference can be performed either via a forward-backward pass within the graphical model or dynamic programming. In con- trast, in this paper we present learning algorithms for general graphical models, where inference is hard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin loss during training which is incorporated into our framework by choosing (cid:15) = 0. More recently, Li & Zemel (2014) use a hinge loss to learn the unary term deﬁned as a neural net, but keep the pairwise potentials ﬁxed (i.e., no joint training). Domke (2013) considers non-linear structured prediction and decomposes the learning problem into a subset of logistic regressors, which require the parameter updates to be run till convergence before updating the messages. Tompson et al. (2014) also jointly train convolutional neural networks and a graphical model for pose estimation. However, the MRF inference procedure is approximated by their Spatial-Model which ignores the partition function. Blending learning and inference: In this paper we deﬁned learning to be a min-max task. The blending strategy, which was previously employed for learning log-linear models by (Meshi et al., 2010; Hazan & Urtasun, 2010), amounts to converting the maximization task into a minimization problem using its dual. Subsequently we make use of block-coordinate descent strategies to obtain a more efﬁcient algorithm. Importantly any order of block-updates is possible. It remains an open problem to ﬁnd the optimal tradeoff. We have proposed an efﬁcient algorithm to learn deep models enriched to capture the dependencies between the output variables. Our experiments on word prediction from noisy images and multi- class image classiﬁcation showed that the deeper and the more structured the model, the better the performance we achieve. Furthermore, joint learning of all weights outperforms all other strate- gies. In the future we plan to learn deeper models in applications such as holistic semantic scene understanding. We will also extend our approach to deal with hidden variables.  9  Accepted as a workshop contribution at ICLR 2015  ACKNOWLEDGMENTS  We thank NVIDIA Corporation for the donation of GPUs used in this research. This work was partially funded by ONR-N00014-14-1-0232.  REFERENCES Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. Greedy Layer-Wise Training of Deep Networks. In  Proc. NIPS, 2007.  Bottou, L., Bengio, Y., and LeCun, Y. Global training of document processing systems using graph transformer  networks. In Proc. CVPR, 1997.  Bridle, J. S. Training stochastic model recognition algorithms as networks can lead to maximum mutual infor-  mation estimation of parameters. In Proc. NIPS, 1990.  Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing  (almost) from scratch. JMLR, 2011.  de Campos, T. E., Babu, B. R., and Varma, M. Character recognition in natural images. In Proc. VISAPP, 2009.  Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image  database. In Proc. CVPR, 2009.  Deng, J., Ding, N., Jia, Y., Frome, A., Murphy, K., Bengio, S., Li, Y., Neven, H., and Adam, H. Large-Scale  Object Classiﬁcation using Label Relation Graphs. In Proc. ECCV, 2014.  Do, T.-M.-T. and Artieres, T. Neural conditional random ﬁelds. In Proc. AISTATS, 2010.  Domke, J. Structured Learning via Logistic Regression. In Proc. NIPS, 2013.  Eigen, D., Rolfe, J., Fergus, R., and LeCun, Y. Understanding Deep Architectures using a Recursive Convolu-  tional Network. In Proc. ICLR, 2014.  Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and  semantic segmentation. In Proc. CVPR, 2014.  Hariharan, B., Arbel´aez, P., Girshick, R., and Malik, J. Simultaneous detection and segmentation. In Proc.  ECCV, 2014.  Hazan, T. and Urtasun, R. A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Struc-  tured Prediction. In Proc. NIPS, 2010.  Hinton, G. E. and Salakhutdinov, R. R. Reducing the dimensionality of data with neural networks. Science,  2006.  Hinton, G. E., Sejnowski, T. J., and Ackley, D. H. Boltzmann Machines: Constraint Satisfaction Networks that  Learn. Technical report, University of Toronto, 1984.  Jia, Y. Caffe: An Open Source Convolutional Architecture for Fast Feature Embedding. http://caffe.  berkeleyvision.org/, 2013.  Koller, D. and Friedman, N. Probabilistic Graphical Models: Principles and Techniques. MIT Press, 2009.  Krizhevsky, A., Sutskever, I., and Hinton, G. E.  Networks. In Proc. NIPS, 2013.  ImageNet Classiﬁcation with Deep Convolutional Neural  Larochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio, Y. An empirical evaluation of deep architec-  tures on problems with many factors of variation. In Proc. ICML, 2007.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 1998.  Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. Convolutional deep belief networks for scalable unsupervised  learning of hierarchical representations. In Proc. ICML, 2009.  Li, Y. and Zemel, R. High Order Regularization for Semi-Supervised Learning of Structured Output Problems.  In Proc. ICML, 2014.  10  Accepted as a workshop contribution at ICLR 2015  Ma, J., Peng, J., Wang, S., and Xu, J. A conditional neural ﬁelds model for protein threading. Bioinformatics,  2012.  Meltzer, T., Globerson, A., and Weiss, Y. Convergent Message Passing Algorithms: a unifying view. In Proc.  UAI, 2009.  Meshi, O., Sontag, D., Jaakkola, T., and Globerson, A. Learning Efﬁciently with Approximate inference via  Dual Losses. In Proc. ICML, 2010.  Morris, J. and Fosler-Lussier, E. Conditional random ﬁelds for integrating local discriminative classiﬁers. IEEE  Trans. Audio, Speech, and Language Processing, 2008.  Nowozin, S., Rother, C., Bagon, S., Sharp, T., Yao, B., and Kohli, P. Decision tree ﬁelds. In Proc. ICCV, 2011.  Pearl, J. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann,  1988.  Peng, J., Bo, L., and Xu, J. Conditional Neural Fields. In Proc. NIPS, 2009.  Prabhavalkar, R. and Fosler-Lussier, E. Backpropagation training for multilayer conditional random ﬁeld based  phone recognition. In Proc. ICASSP, 2010.  Salakhutdinov, R. R. and Hinton, G. E. An Efﬁcient Learning Procedure for Deep Boltzmann Machines. Neural  Computation, 2012.  Schwing, A. G. Inference and Learning Algorithms with Applications to 3D Indoor Scene Understanding. PhD  thesis, ETH Zurich, 2013.  Socher, R., Huval, B., Bhat, B., Manning, C. D., and Ng, A. Y. Convolutional-Recursive Deep Learning for 3D  Object Classiﬁcation. In Proc. NIPS, 2012.  Tompson, J., Jain, A., LeCun, Y., and Bregler, C. Joint Training of a Convolutional Network and a Graphical  Model for Human Pose Estimation. In Proc. NIPS, 2014.  Wainwright, M. J. and Jordan, M. I. Graphical Models, Exponential Families and Variational Inference. Foun-  dations and Trends in Machine Learning, 2008.  Wainwright, M. J., Jaakkola, T., and Willsky, A. S. A new class of upper bounds on the log partition function.  Trans. Information Theory, 2005.  Weiss, Y., Yanover, C., and Meltzer, T. MAP Estimation, Linear Programming and Belief Propagation with  Convex Free Energies. In Proc. UAI, 2007.  Wiegerinck, W. and Heskes, T. Fractional belief propagation. In Proc. NIPS, 2003.  Xu, J., Schwing, A. G., and Urtasun, R. Tell me what you see and I will show you where it is. In Proc. CVPR,  2014.  Yedidia, J. S., Freeman, W. T., and Weiss, Y. Constructing free-energy approximations and generalized belief  propagation algorithms. Trans. Information Theory, 2005.  Zeiler, M. D. and Fergus, R. Visualizing and Understanding Convolutional Networks. In Proc. ECCV, 2014.  11  ",
1412.6277,2015, N-gram-Based Low-Dimensional Representation for Document Classification,"['N-gram-Based Low-Dimensional Representation for Document Classification', 'Rémi Lebret and Ronan Collobert']",https://arxiv.org/pdf/1412.6277,"5 1 0 2    r p A 0 1         ] L C . s c [      2 v 7 7 2 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  N-GRAM-BASED LOW-DIMENSIONAL REPRESENTA- TION FOR DOCUMENT CLASSIFICATION  R´emi Lebret Idiap Research Institute, Martigny, Switzerland Ecole Polytechnique F´ed´erale de Lausanne (EPFL), Lausanne, Switzerland remi@lebret.ch  Ronan Collobert∗ Facebook AI Research, Menlo Park, CA, USA Idiap Research Institute, Martigny, Switzerland ronan@collobert.com  ABSTRACT  The bag-of-words (BOW) model is the common approach for classifying docu- ments, where words are used as feature for training a classiﬁer. This generally involves a huge number of features. Some techniques, such as Latent Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been designed to sum- marize documents in a lower dimension with the least semantic information loss. Some semantic information is nevertheless always lost, since only words are con- sidered. Instead, we aim at using information coming from n-grams to overcome this limitation, while remaining in a low-dimension space. Many approaches, such as the Skip-gram model, provide good word vector representations very quickly. We propose to average these representations to obtain representations of n-grams. All n-grams are thus embedded in a same semantic space. A K-means clustering can then group them into semantic concepts. The number of features is there- fore dramatically reduced and documents are then represented as bag of semantic concepts. We show that this model outperforms LSA and LDA on a sentiment classiﬁcation task, and yields similar results than a traditional BOW-model with far less features.  1  INTRODUCTION  Text document classiﬁcation aims at assigning a text document to one or more classes. Successful methods are traditionally based on bag-of-words (BOW). Finding discriminative keywords is, in general, good enough for text classiﬁcation. Given a dictionary of words D to consider, documents are represented by a |D|-dimensional vector (the bag of its words). Each dimension is either a binary value (present or not in the document) or a word occurrence frequency. Some term weightings (e.g. the popular td-idf) have also been deﬁned to reﬂect how discriminative a word is for a document. These are considered as features for training a classiﬁer. Naive Bayes (NB) and Support Vector Machine (SVM) models are often the ﬁrst choices. One limitation of the bag-of-words model is that the discriminative words are usually not the most frequent ones. A large dictionary of words needs to be deﬁned to obtain a robust model. Classiﬁers then have to deal with a huge number of features, and thus become time-consuming and memory-hungry.  Some techniques have been proposed to reduce the dimensionality and represent documents in a low-dimensional semantic space. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) uses the term-document matrix and a singular value decomposition (SVD) to represent terms and doc- uments in a new low-dimensional space. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a generative probabilistic model of a corpus. Each document is represented as a mixture of la- tent topics, where each topic is characterized by a distribution over words. By deﬁning K topics,  ∗All research was conducted at the Idiap Research Institute, before Ronan Collobert joined Facebook AI  Research.  1  Accepted as a workshop contribution at ICLR 2015  documents can then be represented as K-dimensional vectors. Pessiot et al. (2010) also proposed probabilistic models for unsupervised dimensionality reduction in the context of document cluster- ing. They make the hypothesis that words occuring with the same frequencies in the same document are semantically related. Based on this assumption, words are partioned into word topics. Docu- ment are then represented by a vector where each feature corresponds to a word-topic representing the number of occurrences of words from that word-topic in the document. Other techniques have tried to improve text document clustering by taking into account relationships between important terms. Some have enriched document representations by integrating core ontologies as background knowledge (Staab & Hotho, 2003), or with Wikipedia concepts and category information (Hu et al., 2009). Part-of-speech tags have also been used to disambiguate words (Sedding & Kazakov, 2004).  All these techniques are based on words alone, which raises another limitation. A collection of words cannot capture phrases or multi-word expressions, while n-grams have shown to be helpful features in several natural language processing tasks (Tan et al., 2002; Lin & Wu, 2009; Wang & Manning, 2012). N -gram features are not commonly used in text classiﬁcation, probably because the dictio- nary Dn tends to grow exponentially with n. Phrase structure extraction can be used to identify only n-grams which are phrase patterns, and thus limit the dictionary size. However, this adds another step to the model, making it more complex. To overcome these barriers, we propose that documents be represented as a bag of semantic concepts, where n-grams are considered instead of only words. Good word vector representations are obtained very quickly with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Lebret & Collobert, 2014; Pennington et al., 2014). Mikolov et al. (2013a) also showed that simple vector addition can often produce meaning- ful results, such as king - man + woman ≈ queen. By leveraging the ability of these word vector representations to compose, representations for n-grams are easily computed with an element-wise addition. Using a clustering algorithm such as K-means, those representations are grouped into K clusters which can be viewed as semantic concepts. Text documents are now represented as bag of semantic concepts, with each feature corresponding to the presence or not of n-grams from the resulting clusters. Therefore, more information is captured while remaining in a low-dimensional space. As Mikolov et al’s Skip-gram model and K-means are highly parallelizable, this model is much faster to compute than LSA or LDA. The same classiﬁers as with BOW-based models are then applied on these bag of semantic concepts. We show that such model is a good alternative to LSA or LDA to represent documents and yields even better results on movie review tasks.  2 A BAG OF SEMANTIC CONCEPTS MODEL  The model is divided into three steps: (1) vector representations of n-grams are obtained by aver- aging pre-trained representations of its individual words; (2) n-grams are gouped into K semantic concepts by performing K-means clustering on all n-gram representations; (3) documents are rep- resented by a bag of K semantic concepts, where each entry depends on the presence of n-grams from the concepts deﬁned in the previous step.  2.1 N -GRAM REPRESENTATION  The ﬁrst step of the model is to generate continuous vector representations xw for each word w within the dictionary D. Leveraging recent models, such as the Skip-gram (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) models, they are trained over a large corpus of unlabeled data in an efﬁcient manner. These models are indeed highly parallelizable, which helps to obtain these representations very quickly. Word representations are then summed to generate n-gram represen- tations:  1 n  n  Xi=1  xwi .  (1)  These representations are vectors which keep the semantic information of n-grams with different n in the same dimensionality. Distances between them are thus computable. It allows the use of a K-means clustering for grouping all n-grams into K classes.  2  Accepted as a workshop contribution at ICLR 2015  2.2 K-MEANS CLUSTERING  K-means is an unsupervised learning algorithm commonly used to automatically partition a data set into K clusters. Considering a set of n-gram representations xi ∈ Rm, the algorithm will determine a set of K centroids γk ∈ Rm, so as to minimize the average distance from each representation to its nearest centroid:  Xi  ||xi − γσi||2 , where σi = argmin  ||xi − γk||2 .  (2)  k  The limitation due to the size of the dictionary is therefore overcomed. By setting K to a low value, documents can also be represented by more compact vectors than with a bag-of-words model, while keeping all the meaningful information.  2.3 DOCUMENT REPRESENTATION  Denoting D = (d1, d2, . . . , dL) a set of text documents, where each document di contains a set of n-grams. First, each n-gram is embedded into a common vector space by averaging its word vector representations. The resulting n-grams representations are assigned to clusters using the centroids γk deﬁned by the K-means clustering. Documents di are then represented by a vector of i usually corresponds to the frequency of n-grams from the kth K features, fi ∈ RK. Each entry f k cluster within the document di. The set of text documents is then deﬁned as ˆD = {(fi, yi)| fi ∈ RK, yi ∈ {−1, 1}}L  i=1.  With NB features. For certain type of document, such as movie reviews, the use of Naive Bayes features can improve the general performance (Wang & Manning, 2012). Success in sentiment anal- ysis relies mostly on the capability of the models to detect negative and positive n-grams in a doc- ument. A proper normalization is then calculated to determine how important each n-gram is for a given class y. We denote ngm = (ngm1, . . . , ngmN ) a set of count vectors for all n-grams con- t represents the number of occurence of the n-gram t in the training tained in D, ngmi ∈ RL. ngmi document di. Deﬁning count vectors as p = 1 + Pi:yi=1 ngmi and q = 1 + Pi:yi=−1 ngmi, a  log-count ratio is calculated to determine how important n-grams are for the classes y:  r = log(cid:18) p/||p||1  q/||q||1(cid:19) , with r ∈ RN .  (3)  Because n-grams are in clusters, we extract the maximum absolute log-count ratio for every cluster:  ˜f k i = argmax  rt  |rt| , ∀t ∈ k, ngmi  t > 0  (4)  These document representations can then be used for several NLP tasks such as classiﬁcation or information retrieval. As for BOW-based models, this model is particulary suitable for linear SVM.  3 EXPERIMENTS WITH SENTIMENT ANALYSIS  Sentiments can have a completely different meaning if n-grams are considered instead of words. A classiﬁer might leverage a bigram such as “not good” to classify a document as negative, while this would probably fail if only unigrams (words) were considered. We thus benchmark the bag of semantic concepts model on sentiment analysis.  3.1  IMDB MOVIE REVIEWS DATASETS  Datasets from IMDB have the nice property of containing long documents. It is thus valuable to considerer n-grams in such a framework. We did experiments with small and large collections of reviews. We can thus analyse how well our model compares against classical models, for different dataset sizes.  3  Accepted as a workshop contribution at ICLR 2015  3.1.1 PANG & LEE (2004)  The collection consists of 1,000 positive and 1,000 negative processed reviews1. So a random guess yields 50% accuracy. The authors selected only reviews where rating was expressed either with stars or some numerical value. To avoid domination of the corpus by a small number of proliﬁc reviewers, they imposed a limit of fewer than 20 reviews per author per sentiment category. As there is no test set, we used 10-fold cross-validation.  3.1.2 MAAS ET AL. (2011)  The collection consists of 100,000 reviews2. It has been divided into three datasets: training and test sets (25,000 labeled reviews each), and 50,000 unlabeled training reviews. It allows no more than 30 reviews per movie. It contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. Only highly polarized reviews have been considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10.  3.2 EXPERIMENTAL SETUP  We ﬁrst learn word vector representations over a large corpus of unlabeled text. This step could however be skipped by taking existing pre-trained word representations3 instead of learning them from scratch. By following the three steps described in Section 2, movie reviews are then represented as bags of semantic concepts. These representations are ﬁnally used for training a linear SVM to classify sentiment.  3.2.1 LEARNING WORD REPRESENTATION OVER LARGE CORPORA  Our English corpus is composed of the entire English Wikipedia4, the Reuters corpus and the Wall Street Journal (WSJ) corpus. We consider lower case words and replace digits with a special token. The resulting text is tokenized using the Stanford tokenizer. The ﬁnal data set contains about 2 billion words. Our dictionary D consists of all the words appearing at least one hundred times. This results in a 202,255 words dictionary. We then train a Skip-gram model to get word representation in a 100-dimensional vector. This dimension is intentionally quite low to speed up the clustering afterwards. As other hyperparameters, we use a ﬁxed learning rate of 0.01, a context size of 5 phrases, Negative Sampling with 5 negative samples for each positive sample, and a subsampling approach with a threshold of 10−5  3.2.2 BAG OF SEMANTIC CONCEPTS FOR MOVIE REVIEWS  Computing n-gram representations. We consider n-grams up to n = 3. Only n-grams with words from our dictionary are considered for both datasets.5 This results in a set of 34,360 1- gram representations, 419,918 2-gram representations, and 921,837 3-gram representations for the Pang and Lee’s dataset. And 67,847 1-gram representations, 1,842,461 2-gram representations, and 5,724,871 3-gram representations for the Maas et al.’s dataset. Because n-gram representations are computed by averaging representations of its word, all n-grams are also represented in a 100- dimensional vector.  Partitioning n-grams into semantic concepts. Because n-grams are represented in a common vector space, similarities between n-grams of different length can be computed. To evaluate the beneﬁt of adding n-grams for sentiment analysis, we deﬁne semantic concepts with different com- binations of n-grams: (1) only 1-grams (i.e. clusters of words), (2) only 2-grams, (3) only 3-grams, (4) with 1-grams and 2-grams, and (5) with 1-grams, 2-grams and 3-grams. Each of these ﬁve sets  1Available at http://www.cs.cornell.edu/people/pabo/movie-review-data/. 2Available at http://www.andrew-maas.net/data/sentiment. 3Different  pre-trained  vector  word  representations  available http://stanford.edu/˜jpennin/  are  at or  https://code.google.com/p/word2vec/, http://lebret.ch/words/.  4We took the January 2014 version. 5Our English corpus is not large enough to cover all the words present in the IMDB datasets. We thus use  the same 1-gram dictionary with the other methods.  4  Accepted as a workshop contribution at ICLR 2015  of n-gram representations are then partitioned in K = {100, 200, 300} clusters with the K-means clustering. The centroids γk ∈ R100 are obtained after 10 iterations of the algorithm.  Movie review representations. Movie reviews are then represented as bags of semantic concepts with naive bayes features as described in Section 2.3. The log-count ratio for each n-gram is calcu- lated on the training set for both datasets.  3.2.3 COMPARISON WITH OTHER METHODS  We compare our models with two classical techniques for representing text documents in a low- dimensional vector space: LSA and LDA. Both methods use the same 1-gram dictionaries than with the bag of semantic concepts model with K = {100, 200, 300}. In the framework of Maas et al.’s dataset, LSA and LDA beneﬁt from the large set of unlabeled reviews.  Latent Sentiment Analysis (LSA) (Deerwester et al., 1990). Let X ∈ R|D|×L be a matrix where each element Xi,j describes the log count ratio of words i in document j, with L the number of training documents and D the dictionary of words (i.e. 34,360 for Pang and Lee’s dataset, 67,847 for Maas et al’s dataset). By applying truncated SVD to the log-count ratio matrix X, we thus obtain semantic representations in a K-dimensional space for movie reviews.  Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We train the K-topics LDA model using the code released by Blei et al. (2003)6. We leave the LDA hyperparameters at their default values. Like our model, LDA extracts K topics (i.e. semantic concepts) and assigns words to these topics. Considering only the words in documents, we thus apply the method described in Section 2.3 to get document representations. A movie review di is then represented in a K-dimensional vector, where each feature ˜f k  i is the maximum absolute log-count ratio for the kth topic.  3.2.4 CLASSIFICATION USING SVM  Having representations of movie reviews in a K-dimensional vector, a classiﬁer is trained to de- termine whether a given review is positive or negative. Given the set of training documents ˜D = {(˜fi, yi)| ˜fi ∈ RK, yi ∈ {−1, 1}}L i=1, we picked a linear SVM as a classiﬁer, trained using the LIBLINEAR library (Fan et al., 2008):  min  w  1 2  wT w + CXi  max(0, 1 − yiwT ˜fi)2 ,  (5)  with w the weight vector, and C a penalty parameter.  3.3 RESULTS  The overall results summarized in Table 1 show that the bag of semantic concepts approach out- performs the traditionnal LDA and LSA approaches to represent documents in a low-dimensional space. Good performance is achieved even with only 100 clusters, where LSA needs more clus- ters to improve. We also denote that our approach performs well on a small dataset, where LDA fails. A signiﬁcant increase is observed when using 2-grams instead of 1-grams. However, using only 3-grams hurts the performance. The best results are obtained using a combination of n-grams, which conﬁrms the beneﬁt of the method. That also means that word vector representations can be combined while keeping relevant semantic information. This is illustrated in Table 3 where semanti- cally close n-grams are in the same cluster. We can see that the model is furthermore able to clearly separate antonyms, which is a good asset for sentiment classiﬁcation. The results are also very competitive with a traditional BOW-model. Using the same 1-gram dictionary and a linear SVM classiﬁer with the naive bayes features, BOW-model achieves 83% accuracy for Pang and Lee’s dataset, and 88.58% for Maas et al’s dataset. Our model therefore performs better with about 344 times less features for the ﬁrst dataset, and yields similar result with about 678 times less features for the second one.  6Available at http://www.cs.princeton.edu/˜blei/lda-c/.  5  Accepted as a workshop contribution at ICLR 2015  Pang and Lee, 2004  Maas et al., 2011  K =  100  200  300  100  200  300  LDA LSA 1-grams 2-grams 3-grams 1+2-grams 1+2+3-grams  76.20 81.60 81.60 82.30 73.85 83.85 82.45  77.10 82.55 82.60 82.25 73.05 84.00 83.05  76.80 83.75 82.70 83.15 72.65 84.00 83.05  85.43 85.82 84.51 88.02 87.41 88.10 88.39  85.45 86.63 84.76 88.06 87.46 88.19 88.46  84.40 86.88 85.54 87.87 87.22 88.18 88.55  Table 1: Classiﬁcation accuracy on both movie review tasks with K = {100, 200, 300} number of features.  3.4 COMPUTATION TIME  The bag of semantic concepts model can leverage information coming from n-grams to improve sentiment classiﬁcation of documents. This model has also the nice property to build document rep- resentations in an efﬁcient and timely manner. The most time-consuming and costly process step in the model is the K-means clustering, especially when dealing with millions of n-gram representa- tions. However, this step can be done very quickly with low memory by using mini-batch K-means method. Computation times for generating 300-dimensional representations are reported in Table 2. All experiments have been run on single CPU core Intel i7 2600K 3.4 GHz. Despite the fact that single CPU has been used for this benchmark, the three steps of the model are highly parallelizable. The recorded times could thus be divided by the number of CPUs available. We see that represen- tations can be computed in less than one minute with only 1-gram dictionary. About 10 minutes are necessary when adding 2-grams, and about 40 minutes by adding 3-grams. In comparison, LDA needs six hours for extracting 100 topics and three days for 300 topics. Our model is also very com- petitive with LSA which takes 540 seconds to generate 300-dimensional document representations. However, adding 2-grams and 3-grams to perform a LSA would be extremely time-consuming and memory-hungry while our model can handle it.  1-grams  2-grams  3-grams  1+2-grams  1+2+3-grams  N -gram Representations K-means Document Representations  Total  0 14.18 36.45  50.63  43.00 291.62 173.48  164.34 747.90 494.06  508.10  1406.30  43.00 302.34 343.29  688.63  207.34 1203.99 949.01  2360.34  Table 2: Computation time for building movie review representations with K = 300 semantic concepts. Time is reported in seconds.  3.5  INFERRING SEMANTIC CONCEPTS FOR UNSEEN n-GRAMS  Another drawback of classical models is that they cannot deal with unseen words. Only words present in the training documents are used to infer representation for a new text document. Unlike these models, our model can easily assign semantic concepts for new n-grams. Because n-gram representations are based on its word vector representations, a new n-gram vector representation can be calculated if a representation is available for each of its words. This new representations is then assigned to the nearest centroid γk, which determines its semantic concept. With a small training set, this is a valuable asset when compared to other models.  6  Accepted as a workshop contribution at ICLR 2015  good  k=269  not good  k=297  enjoy  k=160  did n’t enjoy  k=108  nice one liked here is pretty nice the greatest thing  sufﬁciently bad not liked is far worse not that greatest  entertain adored them enjoying watched and enjoy  sceptics did n’t like n’t enjoy any valueless  Table 3: Selected pairs of antonyms and their cluster number. Here, n-grams from Maas et al’s dataset have been partitioned into 300 clusters. Each n-gram is accompagnied with a selection of others from its cluster.  4 CONCLUSION  Word vector representations can be quickly obtained with recent techniques such as the Skip-gram model. N -grams with different length n can then be embedded in a same dimensional vector space with a simple element-wise addition. This makes it possible to compute distances between n-grams, which can have many applications in natural language processing. We therefore proposed a bag of semantic concepts model to represent documents in a low-dimensional space. These semantic concepts are obtained by performing a K-means clustering which partition all n-grams into K clus- ters. This model has several advantages over classical approaches for representing documents in a low-dimensional space: it leverages semantic information coming from n-grams; it builds document representations with low resource consumption (time and memory); it can infer semantic concepts for unseen n-grams. Furthermore, we have shown that such model is suitable for document classi- ﬁcation. Competitive performance has been reached on binary sentiment classiﬁcation tasks, where this model outperforms traditional approaches. It also attained similar results to traditional bag-of- words with considerably less features.  ACKNOWLEDGMENTS  This work was supported by the HASLER foundation through the grant “Information and Commu- nication Technology for a Better World 2020” (SmartWorld).  REFERENCES  Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent Dirichlet Allocation. Journal of Machine Learning  Research, 2003.  Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. Indexing by latent  semantic analysis. Journal of the American Society for Information Science, 1990.  Fan, R., Chang, K., Hsieh, C., Wang, X., and Lin, C. LIBLINEAR: A Library for Large Linear  Classiﬁcation. Journal of Machine Learning Research, 2008.  Hu, X., Zhang, X., Lu, C., Park, E. K., and Zhou, X. Exploiting Wikipedia As External Knowledge for Document Clustering. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09, pp. 389–396, New York, NY, USA, 2009. ACM.  Lebret, R. and Collobert, R. Word Embeddings through Hellinger PCA.  EACL, 2014.  In Proceedings of the  Lin, D. and Wu, X. Phrase clustering for discriminative learning. In Proceedings of ACL, 2009.  7  Accepted as a workshop contribution at ICLR 2015  Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning Word Vectors  for Sentiment Analysis. In Proceedings of ACL, 2011.  Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁcient Estimation of Word Representations in  Vector Space. ICLR Workshp, 2013a.  Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. Distributed Representations of Words  and Phrases and their Compositionality. In NIPS. 2013b.  Mnih, A. and Kavukcuoglu, K. Learning word embeddings efﬁciently with noise-contrastive esti-  mation. In NIPS. 2013.  Pang, B. and Lee, L. A sentimental education: Sentiment analysis using subjectivity. In Proceedings  of ACL, 2004.  Pennington, J., Socher, R., and Manning, C. D. GloVe: Global Vectors for Word Representation. In  Proceedings of EMNLP, 2014.  Pessiot, J.-F., Kim, Y.-M., Amini, M.-R., and Gallinari, P. Improving Document Clustering in a  Learned Concept Space. Information Processing & Management, 46(2):180–192, 2010.  Sedding, J. and Kazakov, D. WordNet-based Text Document Clustering. In Proceedings of the 3rd Workshop on RObust Methods in Analysis of Natural Language Data, pp. 104–113, Stroudsburg, PA, USA, 2004. Association for Computational Linguistics.  Staab, S. and Hotho, A. Ontology-based Text Document Clustering.  In Intelligent Information Processing and Web Mining, Proceedings of the International IIS: IIPWM’03 Conference held in Zakopane, pp. 451–452, 2003.  Tan, C., Wang, Y., and Lee, C. The use of bigrams to enhance text categorization. Journal of  Information Processing and Management, 2002.  Wang, S. I. and Manning, C. D. Baselines and Bigrams: Simple, Good Sentiment and Topic Classi-  ﬁcation. In Proceedings of ACL, 2012.  8  ",
1412.7024,2015, Low precision arithmetic for deep learning,"['Low precision arithmetic for deep learning', 'Matthieu Courbariaux', 'Yoshua Bengio', 'and Jean-Pierre David']",https://arxiv.org/pdf/1412.7024,"Accepted as a workshop contribution at ICLR 2015  TRAINING DEEP NEURAL NETWORKS WITH LOW PRECISION MULTIPLICATIONS  Matthieu Courbariaux & Jean-Pierre David ´Ecole Polytechnique de Montr´eal {matthieu.courbariaux,jean-pierre.david}@polymtl.ca  Yoshua Bengio Universit´e de Montr´eal, CIFAR Senior Fellow yoshua.bengio@gmail.com  ABSTRACT  Multipliers are the most space and power-hungry arithmetic operators of the digi- tal implementation of deep neural networks. We train a set of state-of-the-art neu- ral networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: ﬂoating point, ﬁxed point and dynamic ﬁxed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the ﬁnal error after training. We ﬁnd that very low precision is sufﬁcient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.  1  INTRODUCTION  The training of deep neural networks is very often limited by hardware. Lots of previous works address the best exploitation of general-purpose hardware, typically CPU clusters (Dean et al., 2012) and GPUs (Coates et al., 2009; Krizhevsky et al., 2012a). Faster implementations usually lead to state of the art results (Dean et al., 2012; Krizhevsky et al., 2012a). Actually, such approaches always consist in adapting the algorithm to best exploit state of the art general-purpose hardware. Nevertheless, some dedicated deep learning hardware is appearing as well. FPGA and ASIC implementations claim a better power efﬁciency than general-purpose hard- ware (Kim et al., 2009; Farabet et al., 2011; Pham et al., 2012; Chen et al., 2014a;b). In contrast with general-purpose hardware, dedicated hardware such as ASIC and FPGA enables to build the hardware from the algorithm. Hardware is mainly made out of memories and arithmetic operators. Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. The objective of this article is to assess the possibility to reduce the precision of the multipliers for deep learning:  • We train deep neural networks with low precision multipliers and high precision accumu- • We carry out experiments with three distinct formats:  lators (Section 2).  5 1 0 2     p e S 3 2         ]  G L . s c [      5 v 4 2 0 7  .  2 1 4 1 : v i X r a  points (Section 5)  1. Floating point (Section 3) 2. Fixed point (Section 4) 3. Dynamic ﬁxed point, which we think is a good compromise between ﬂoating and ﬁxed • We use a higher precision for the parameters during the updates than during the forward • Maxout networks (Goodfellow et al., 2013a) are a set of state-of-the-art neural networks (Section 7). We train Maxout networks with slightly less capacity than Goodfellow et al. (2013a) on three benchmark datasets: MNIST, CIFAR-10 and SVHN (Section 8).  and backward propagations (Section 6).  1  Accepted as a workshop contribution at ICLR 2015  • For each of the three datasets and for each of the three formats, we assess the impact of the precision of the multiplications on the ﬁnal error of the training. We ﬁnd that very low precision multiplications are sufﬁcient not just for running trained networks but also for training them (Section 9). We made our code available 1.  2 MULTIPLIER-ACCUMULATORS  Multiplier (bits) Accumulator (bits) Adaptive Logic Modules (ALMs)  32 16 16  32 32 16  504 138 128  Table 1: Cost of a ﬁxed point multiplier-accumulator on a Stratix V Altera FPGA.  Algorithm 1 Forward propagation with low precision multipliers.  for all layers do  Reduce the precision of the parameters and the inputs Apply convolution or dot product (with high precision accumulations) Reduce the precision of the weighted sums Apply activation functions  end for Reduce the precision of the outputs  Applying a deep neural network (DNN) mainly consists in convolutions and matrix multiplications. The key arithmetic operation of DNNs is thus the multiply-accumulate operation. Artiﬁcial neurons are basically multiplier-accumulators computing weighted sums of their inputs. The cost of a ﬁxed point multiplier varies as the square of the precision (of its operands) for small widths while the cost of adders and accumulators varies as a linear function of the precision (David et al., 2007). As a result, the cost of a ﬁxed point multiplier-accumulator mainly depends on the precision of the multiplier, as shown in table 1. In modern FPGAs, the multiplications can also be implemented with dedicated DSP blocks/slices. One DSP block/slice can implement a single 27 × 27 multiplier, a double 18 × 18 multiplier or a triple 9 × 9 multiplier. Reducing the precision can thus lead to a gain of 3 in the number of available multipliers inside a modern FPGA. In this article, we train deep neural networks with low precision multipliers and high precision accumulators, as illustrated in Algorithm 1.  3 FLOATING POINT  Format Double precision ﬂoating point Single precision ﬂoating point Half precision ﬂoating point  Total bit-width Exponent bit-width Mantissa bit-width 64 32 16  11 8 5  52 23 10  Table 2: Deﬁnitions of double, single and half precision ﬂoating point formats.  Floating point formats are often used to represent real values. They consist in a sign, an exponent, and a mantissa, as illustrated in ﬁgure 1. The exponent gives the ﬂoating point formats a wide range, and the mantissa gives them a good precision. One can compute the value of a single ﬂoating point number using the following formula:  (cid:18)  (cid:19)  value = (−1)sign ×  1 +  mantissa  223  × 2(exponent−127)  1 https://github.com/MatthieuCourbariaux/deep-learning-multipliers  2  Accepted as a workshop contribution at ICLR 2015  Figure 1: Comparison of the ﬂoating point and ﬁxed point formats.  Table 2 shows the exponent and mantissa widths associated with each ﬂoating point format. In our experiments, we use single precision ﬂoating point format as our reference because it is the most widely used format in deep learning, especially for GPU computation. We show that the use of half precision ﬂoating point format has little to no impact on the training of neural networks. At the time of writing this article, no standard exists below the half precision ﬂoating point format.  4 FIXED POINT  Fixed point formats consist in a signed mantissa and a global scaling factor shared between all ﬁxed point variables. The scaling factor can be seen as the position of the radix point. It is usually ﬁxed, hence the name ”ﬁxed point”. Reducing the scaling factor reduces the range and augments the precision of the format. The scaling factor is typically a power of two for computational efﬁciency (the scaling multiplications are replaced with shifts). As a result, ﬁxed point format can also be seen as a ﬂoating point format with a unique shared ﬁxed exponent , as illustrated in ﬁgure 1. Fixed point format is commonly found on embedded systems with no FPU (Floating Point Unit). It relies on integer operations. It is hardware-wise cheaper than its ﬂoating point counterpart, as the exponent is shared and ﬁxed.  5 DYNAMIC FIXED POINT  Algorithm 2 Policy to update a scaling factor. Require: a matrix M, a scaling factor st, and a maximum overﬂow rate rmax. Ensure: an updated scaling factor st+1. if the overﬂow rate of M > rmax then else if the overﬂow rate of 2 × M ≤ rmax then else  st+1 ← 2 × st st+1 ← st/2 st+1 ← st  end if  When training deep neural networks,  1. activations, gradients and parameters have very different ranges. 2. gradients ranges slowly diminish during the training.  As a result, the ﬁxed point format, with its unique shared ﬁxed exponent, is ill-suited to deep learn- ing.  3  Accepted as a workshop contribution at ICLR 2015  The dynamic ﬁxed point format (Williamson, 1991) is a variant of the ﬁxed point format in which there are several scaling factors instead of a single global one. Those scaling factors are not ﬁxed. As such, it can be seen as a compromise between ﬂoating point format - where each scalar variable owns its scaling factor which is updated during each operations - and ﬁxed point format - where there is only one global scaling factor which is never updated. With dynamic ﬁxed point, a few grouped variables share a scaling factor which is updated from time to time to reﬂect the statistics of values in the group. In practice, we associate each layer’s weights, bias, weighted sum, outputs (post-nonlinearity) and the respective gradients vectors and matrices with a different scaling factor. Those scaling factors are initialized with a global value. The initial values can also be found during the training with a higher precision format. During the training, we update those scaling factors at a given frequency, following the policy described in Algorithm 2.  6 UPDATES VS. PROPAGATIONS  We use a higher precision for the parameters during the updates than during the forward and back- ward propagations, respectively called fprop and bprop. The idea behind this is to be able to accu- mulate small changes in the parameters (which requires more precision) and while on the other hand sparing a few bits of memory bandwidth during fprop. This can be done because of the implicit averaging performed via stochastic gradient descent during training:  θt+1 = θt − (cid:15)  ∂Ct(θt)  ∂θt  where Ct(θt) is the cost to minimize over the minibatch visited at iteration t using θt as parameters and (cid:15) is the learning rate. We see that the resulting parameter is the sum  T−1(cid:88)  t=1  θT = θ0 − (cid:15)  ∂Ct(θt)  ∂θt  .  The terms of this sum are not statistically independent (because the value of θt depends on the value of θt−1) but the dominant variations come from the random sample of examples in the minibatch (θ moves slowly) so that a strong averaging effect takes place, and each contribution in the sum is relatively small, hence the demand for sufﬁcient precision (when adding a small number with a large number).  7 MAXOUT NETWORKS  A Maxout network is a multi-layer neural network that uses maxout units in its hidden layers. A maxout unit outputs the maximum of a set of k dot products between k weight vectors and the input vector of the unit (e.g., the output of the previous layer):  hl i =  k  max j=1  (bl  i,j + wl  i,j · hl−1)  where hl is the vector of activations at layer l and weight vectors wl eters of the j-th ﬁlter of unit i on layer l. A maxout unit can be seen as a generalization of the rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b)  i,j are the param-  i,j and biases bl  hl i = max(0, bl  i + wl  i · hl−1)  which corresponds to a maxout unit when k = 2 and one of the ﬁlters is forced at 0 (Goodfellow et al., 2013a). Combined with dropout, a very effective regularization method (Hinton et al., 2012), maxout networks achieved state-of-the-art results on a number of benchmarks (Goodfellow et al., 2013a), both as part of fully connected feedforward deep nets and as part of deep convolutional nets. The dropout technique provides a good approximation of model averaging with shared parameters across an exponentially large number of networks that are formed by subsets of the units of the original noise-free deep network.  4  Accepted as a workshop contribution at ICLR 2015  8 BASELINE RESULTS  We train Maxout networks with slightly less capacity than Goodfellow et al. (2013a) on three bench- mark datasets: MNIST, CIFAR-10 and SVHN. In Section 9, we use the same hyperparameters as in this section to train Maxout networks with low precision multiplications.  Dataset MNIST CIFAR-10 SVHN  Dimension 784 (28 × 28 grayscale) 3072 (32 × 32 color) 3072 (32 × 32 color)  Labels Training set Test set 10 10 10  60K 50K 604K  10K 10K 26K  Table 3: Overview of the datasets used in this paper.  Format Goodfellow et al. (2013a) Single precision ﬂoating point Half precision ﬂoating point Fixed point Dynamic ﬁxed point  Prop. Up. 32 32 32 32 16 16 20 20 12 10  PI MNIST MNIST CIFAR-10 0.94% 1.05% 1.10% 1.39% 1.28%  11.68% 14.05% 14.14% 15.98% 14.82%  0.45% 0.51% 0.51% 0.57% 0.59%  SVHN 2.47% 2.71% 3.02% 2.97% 4.95%  Table 4: Test set error rates of single and half ﬂoating point formats, ﬁxed and dynamic ﬁxed point formats on the permutation invariant (PI) MNIST, MNIST (with convolutions, no distortions), CIFAR-10 and SVHN datasets. Prop. is the bit- width of the parameters updates. The single precision ﬂoating point line refers to the results of our experiments. It serves as a baseline to evaluate the degradation brought by lower precision.  is the bit-width of the propagations and Up.  8.1 MNIST  The MNIST (LeCun et al., 1998) dataset is described in Table 3. We do not use any data- augmentation (e.g. distortions) nor any unsupervised pre-training. We simply use minibatch stochas- tic gradient descent (SGD) with momentum. We use a linearly decaying learning rate and a linearly saturating momentum. We regularize the model with dropout and a constraint on the norm of each weight vector, as in (Srebro and Shraibman, 2005). We train two different models on MNIST. The ﬁrst is a permutation invariant (PI) model which is unaware of the structure of the data. It consists in two fully connected maxout layers followed by a softmax layer. The second model consists in three convolutional maxout hidden layers (with spatial max pooling on top of the maxout layers) followed by a densely connected softmax layer. This is the same procedure as in Goodfellow et al. (2013a), except that we do not train our model on the validation examples. As a consequence, our test error is slightly larger than the one reported in Goodfellow et al. (2013a). The ﬁnal test error is in Table 4.  8.2 CIFAR-10  Some comparative characteristics of the CIFAR-10 (Krizhevsky and Hinton, 2009) dataset are given in Table 3. We preprocess the data using global contrast normalization and ZCA whitening. The model consists in three convolutional maxout layers, a fully connected maxout layer, and a fully connected softmax layer. We follow a similar procedure as with the MNIST dataset. This is the same procedure as in Goodfellow et al. (2013a), except that we reduced the number of hidden units and that we do not train our model on the validation examples. As a consequence, our test error is slightly larger than the one reported in Goodfellow et al. (2013a). The ﬁnal test error is in Table 4.  5  Accepted as a workshop contribution at ICLR 2015  8.3 STREET VIEW HOUSE NUMBERS  The SVHN (Netzer et al., 2011) dataset is described in Table 3. We applied local contrast nor- malization preprocessing the same way as Zeiler and Fergus (2013). The model consists in three convolutional maxout layers, a fully connected maxout layer, and a fully connected softmax layer. Otherwise, we followed the same approach as on the MNIST dataset. This is the same procedure as in Goodfellow et al. (2013a), except that we reduced the length of the training. As a consequence, our test error is bigger than the one reported in Goodfellow et al. (2013a). The ﬁnal test error is in Table 4.  9 LOW PRECISION RESULTS  Figure 2: Final test error depending on the radix point position (5 means after the 5th most sig- niﬁcant bit) and the dataset (permutation invariant MNIST and CIFAR-10). The ﬁnal test errors are normalized, that is to say divided by the dataset single ﬂoat test error. The propagations and parameter updates bit-widths are both set to 31 bits (32 with the sign).  Figure 3: Final test error depending on the propagations bit-width, the format (dynamic ﬁxed or ﬁxed point) and the dataset (permutation invariant MNIST, MNIST and CIFAR-10). The ﬁnal test errors are normalized, which means that they are divided by the dataset single ﬂoat test error. For both formats, the parameter updates bit-width is set to 31 bits (32 with the sign). For ﬁxed point format, the radix point is set after the ﬁfth bit. For dynamic ﬁxed point format, the maximum overﬂow rate is set to 0.01%.  6  Accepted as a workshop contribution at ICLR 2015  Figure 4: Final test error depending on the parameter updates bit-width, the format (dynamic ﬁxed or ﬁxed point) and the dataset (permutation invariant MNIST, MNIST and CIFAR-10). The ﬁnal test errors are normalized, which means that they are divided by the dataset single ﬂoat test error. For both formats, the propagations bit-width is set to 31 bits (32 with the sign). For ﬁxed point format, the radix point is set after the ﬁfth bit. For dynamic ﬁxed point format, the maximum overﬂow rate is set to 0.01%.  Figure 5: Final test error depending on the maximum overﬂow rate and the propagations bit-width. The ﬁnal test errors are normalized, which means that they are divided by the dataset single ﬂoat test error. The parameter updates bit-width is set to 31 bits (32 with the sign).  9.1 FLOATING POINT  Half precision ﬂoating point format has little to no impact on the test set error rate, as shown in Table 4. We conjecture that a high-precision ﬁne-tuning could recover the small degradation of the error rate.  9.2 FIXED POINT  The optimal radix point position in ﬁxed point is after the ﬁfth (or arguably the sixth) most important bit, as illustrated in Figure 2. The corresponding range is approximately [-32,32]. The corresponding scaling factor depends on the bit-width we are using. The minimum bit-width for propagations in ﬁxed point is 19 (20 with the sign). Below this bit-width, the test set error rate rises very sharply, as illustrated in Figure 3. The minimum bit-width for parameter updates in ﬁxed point is 19 (20 with the sign). Below this bit-width, the test set error rate rises very sharply, as illustrated in Figure 4. Doubling the number of hidden units does not allow any further reduction of the bit-widths on the  7  Accepted as a workshop contribution at ICLR 2015  permutation invariant MNIST. In the end, using 19 (20 with the sign) bits for both the propagations and the parameter updates has little impact on the ﬁnal test error, as shown in Table 4.  9.3 DYNAMIC FIXED POINT  We ﬁnd the initial scaling factors by training with a higher precision format. Once those scaling factors are found, we reinitialize the model parameters. We update the scaling factors once every 10000 examples. Augmenting the maximum overﬂow rate allows us to reduce the propagations bit-width but it also signiﬁcantly augments the ﬁnal test error rate, as illustrated in Figure 5. As a consequence, we use a low maximum overﬂow rate of 0.01% for the rest of the experiments. The minimum bit-width for the propagations in dynamic ﬁxed point is 9 (10 with the sign). Below this bit-width, the test set error rate rises very sharply, as illustrated in Figure 3. The minimum bit-width for the parameter updates in dynamic ﬁxed point is 11 (12 with the sign). Below this bit-width, the test set error rate rises very sharply, as illustrated in Figure 4. Doubling the number of hidden units does not allow any further reduction of the bit-widths on the permutation invariant MNIST. In the end, using 9 (10 with the sign) bits for the propagations and 11 (12 with the sign) bits for the parameter updates has little impact on the ﬁnal test error, with the exception of the SVHN dataset, as shown in Table 4. This is signiﬁcantly better than ﬁxed point format, which is consistent with our predictions of Section 5.  10 RELATED WORKS  Vanhoucke et al. (2011) use 8 bits linear quantization to store activations and weights. Weights are scaled by taking their maximum magnitude in each layer and normalizing them to fall in the [-128, 127] range. The total memory footprint of the network is reduced by between 3× and 4×. This is very similar to the dynamic ﬁxed point format we use (Section 5). However, Vanhoucke et al. (2011) only apply already trained neural networks while we actually train them. Training neural networks with low precision arithmetic has already been done in previous works (Holt and Baker, 1991; Presley and Haggard, 1994; Simard and Graf, 1994; Wawrzynek et al., 1996; Savich et al., 2007) 2. Our work is nevertheless original in several regards:  • We are the ﬁrst to train deep neural networks with the dynamic ﬁxed point format. • We use a higher precision for the weights during the updates. • We train some of the latest models on some of the latest benchmarks.  11 CONCLUSION AND FUTURE WORKS  We have shown that:  • Very low precision multipliers are sufﬁcient for training deep neural networks. • Dynamic ﬁxed point seems well suited for training deep neural networks. • Using a higher precision for the parameters during the updates helps.  Our work can be exploited to:  • Optimize memory usage on general-purpose hardware (Gray et al., 2015). • Design very power-efﬁcient hardware dedicated to deep learning.  There is plenty of room for extending our work: • Other tasks than image classiﬁcation.  2 A very recent work (Gupta et al., 2015) also trains neural networks with low precision. The authors propose to replace round-to-nearest with stochastic rounding, which allows to reduce the numerical precision to 16 bits while using the ﬁxed point format. It would be very interesting to combine dynamic ﬁxed point and stochastic rounding.  8  Accepted as a workshop contribution at ICLR 2015  • Other models than Maxout networks. • Other formats than ﬂoating point, ﬁxed point and dynamic ﬁxed point.  12 ACKNOWLEDGEMENT  We thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library which allowed us to easily develop a fast and optimized code for GPU. We also thank the developers of Pylearn2 (Goodfellow et al., 2013b), a Python library built on the top of Theano which allowed us to easily interface the datasets with our Theano code. We are also grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR.  REFERENCES Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- In  Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Chen, T., Du, Z., Sun, N., Wang, J., Wu, C., Chen, Y., and Temam, O. (2014a). Diannao: A small- In Proceedings of the footprint high-throughput accelerator for ubiquitous machine-learning. 19th international conference on Architectural support for programming languages and operating systems, pages 269–284. ACM.  Chen, Y., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L., Chen, T., Xu, Z., Sun, N., et al. (2014b). Dadiannao: A machine-learning supercomputer. In Microarchitecture (MICRO), 2014 47th Annual IEEE/ACM International Symposium on, pages 609–622. IEEE.  Coates, A., Baumstarck, P., Le, Q., and Ng, A. Y. (2009). Scalable learning for object detection with gpu hardware. In Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International Conference on, pages 4287–4293. IEEE.  David, J., Kalach, K., and Tittley, N. (2007). Hardware complexity of modular multiplication and  exponentiation. Computers, IEEE Transactions on, 56(10), 1308–1319.  Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., and Ng, A. Y. (2012). Large scale distributed deep networks. In NIPS’2012.  Farabet, C., Martini, B., Corda, B., Akselrod, P., Culurciello, E., and LeCun, Y. (2011). Neuﬂow: A runtime reconﬁgurable dataﬂow processor for vision. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on, pages 109–116. IEEE.  Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep sparse rectiﬁer neural networks.  TATS’2011.  In AIS-  Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013a). Maxout  networks. Technical Report Arxiv report 1302.4389, Universit´e de Montr´eal.  Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., and Bengio, Y. (2013b). Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214.  Gray, S., Leishman, S., and Koster, U. (2015). Nervanagpu library. Accessed: 2015-06-30.  Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. (2015). Deep learning with limited  numerical precision. CoRR, abs/1502.02551.  Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012).  Im- proving neural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.  9  Accepted as a workshop contribution at ICLR 2015  Holt, J. L. and Baker, T. E. (1991). Back propagation simulations using limited precision calcula- tions. In Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, volume 2, pages 121–126. IEEE.  Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage archi- tecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pages 2146–2153. IEEE.  Kim, S. K., McAfee, L. C., McMahon, P. L., and Olukotun, K. (2009). A highly scalable restricted In Field Programmable Logic and Applications,  Boltzmann machine FPGA implementation. 2009. FPL 2009. International Conference on, pages 367–372. IEEE.  Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images. Tech-  nical report, University of Toronto.  Krizhevsky, A., Sutskever, I., and Hinton, G. (2012a). ImageNet classiﬁcation with deep convolu- tional neural networks. In Advances in Neural Information Processing Systems 25 (NIPS’2012).  Krizhevsky, A., Sutskever, I., and Hinton, G. (2012b). ImageNet classiﬁcation with deep convolu-  tional neural networks. In NIPS’2012.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to  document recognition. Proceedings of the IEEE, 86(11), 2278–2324.  Nair, V. and Hinton, G. (2010). Rectiﬁed linear units improve restricted Boltzmann machines. In  ICML’2010.  Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading digits in natural images with unsupervised feature learning. Deep Learning and Unsupervised Feature Learning Workshop, NIPS.  Pham, P.-H., Jelaca, D., Farabet, C., Martini, B., LeCun, Y., and Culurciello, E. (2012). Neuﬂow: dataﬂow vision processing system-on-a-chip. In Circuits and Systems (MWSCAS), 2012 IEEE 55th International Midwest Symposium on, pages 1044–1047. IEEE.  Presley, R. K. and Haggard, R. L. (1994). A ﬁxed point implementation of the backpropagation learning algorithm. In Southeastcon’94. Creative Technology Transfer-A Global Affair., Proceed- ings of the 1994 IEEE, pages 136–138. IEEE.  Savich, A. W., Moussa, M., and Areibi, S. (2007). The impact of arithmetic representation on implementing mlp-bp on fpgas: A study. Neural Networks, IEEE Transactions on, 18(1), 240– 252.  Simard, P. and Graf, H. P. (1994). Backpropagation without multiplication. In Advances in Neural  Information Processing Systems, pages 232–239.  Srebro, N. and Shraibman, A. (2005). Rank, trace-norm and max-norm. In Proceedings of the 18th  Annual Conference on Learning Theory, pages 545–560. Springer-Verlag.  Vanhoucke, V., Senior, A., and Mao, M. Z. (2011). Improving the speed of neural networks on cpus.  In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop.  Wawrzynek, J., Asanovic, K., Kingsbury, B., Johnson, D., Beck, J., and Morgan, N. (1996). Spert-ii:  A vector microprocessor system. Computer, 29(3), 79–86.  Williamson, D. (1991//). Dynamically scaled ﬁxed point arithmetic.  pages 315 – 18, New York, NY, USA. dynamic scaling;iteration stages;digital ﬁlters;overﬂow probability;ﬁxed point arithmetic;ﬁxed-point ﬁlter;.  Zeiler, M. D. and Fergus, R. (2013). Stochastic pooling for regularization of deep convolutional  neural networks. In International Conference on Learning Representations.  10  ",
1412.2302,2015, Theano-based Large-Scale Visual Recognition with Multiple GPUs,"['Theano-based Large-Scale Visual Recognition with Multiple GPUs', 'Weiguang Ding', 'Ruoyan Wang', 'Fei Mao', 'and Graham Taylor']",https://arxiv.org/pdf/1412.2302,"5 1 0 2    r p A 6         ]  G L . s c [      4 v 2 0 3 2  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  THEANO-BASED LARGE-SCALE VISUAL RECOGNI- TION WITH MULTIPLE GPUS  Weiguang Ding & Ruoyan Wang School of Engineering, University of Guelph {wding, ruoyanry}@uoguelph.ca  Fei Mao Sharcnet, Compute Canada feimao@sharcnet.ca  Graham Taylor School of Engineering, University of Guelph gwtaylor@uoguelph.ca  ABSTRACT  In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) im- plementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the ﬁrst open-source Python-based AlexNet implementation to-date.  1  INTRODUCTION  Deep neural networks have greatly impacted many application areas. In particular, AlexNet (Krizhevsky et al., 2012), a type of convolutional neural network (LeCun et al., 1998) (ConvNet), has signiﬁcantly improved the performance of image classiﬁcation by winning the 2012 ImageNet Large Scale Visual Recognition Challenge (Russakovsky et al., 2014) (ILSVRC 2012). With the increasing popularity of deep learning, many open-source frameworks have emerged with the capa- bility to train deep ConvNets on datasets with over 1M examples. These include Caffe (Jia et al., 2014), Torch7 (Collobert et al., 2011) and cuda-convnet (Krizhevsky et al., 2012). However, the con- venience of using them are limited to building “standard” architectures. To experiment with brand new architectures, researchers have to derive and implement the corresponding gradient functions in order to do backpropagation or other types of gradient descent optimizations. Theano (Bergstra et al., 2010; Bastien et al., 2012), on the other hand, provides the automatic differ- entiation feature, which saves researchers from tedious derivations and can help in avoiding errors in such calculations. The other advantage of Theano is that it has a huge existing user and developer base which leverages the comprehensive scientiﬁc Python stack (102 contributors at the time of writ- ing). However, there is no previously reported work of using Theano to do large scale experiments, such as the above mentioned ILSVRC 2012. Here, we report a Theano-based AlexNet trained on ImageNet data1. We also introduce a naive data parallelism implementation on multiple GPUs, to further accelerate training.  2 METHODS  “AlexNet” is a now a standard architecture known in the deep learning community and often used for benchmarking. It contains 5 convolutional layers, 3 of which are followed by max pooling layers,  1The code is open sourced at https://github.com/uoguelph-mlrg/theano_alexnet. In ad- dition, a toy example is provided at https://github.com/uoguelph-mlrg/theano_multi_gpu.  1  Accepted as a workshop contribution at ICLR 2015  Figure 1: Illustration of parallelized training and loading (1 or 2 GPUs)  Figure 2: Illustration of exchanging and averaging weights (2 GPUs)  2 fully connected layers, and 1 softmax layer (Krizhevsky et al., 2012). In our AlexNet implemen- tation, we used 2 types of convolution and max pooling operators. The 1st is from the Pylearn2 (Goodfellow et al., 2013) wrapper of cuda-convnet, the original implementation of AlexNet. The 2nd is the recently developed Theano wrapper of cuDNN (Chetlur et al., 2014). We also use func- tions in the PyCUDA library (Kl¨ockner et al., 2012) to transfer Theano shared variables between different python processes for two tasks: 1. loading image mini-batches into GPUs during training; and 2. exchanging weights between models trained on multiple-GPUs.  2.1 PARALLEL DATA LOADING  Figure 1 illustrates the process of parallelized training and data loading. Two processes run at the same time, one is for training, and the other one is for loading image mini-batches. While the training process is working on the current minibatch, the loading process is copying the next minibatch from disk to host memory, preprocessing2 it and copying it from host memory to GPU memory. After training on the current minibatch ﬁnishes, the data batch will be moved “instantly” from the loading process to the training process, as they access the same GPU.  2.2 DATA PARALLELISM  In this implementation, 2 AlexNets are trained on 2 GPUs. They are initialized identically. At each step, they are updated on different minibatches respectively, and then their parameters (weights, biases) as well as momentum are exchanged and averaged.  2Preprocessing includes subtracting the mean image, randomly cropping and ﬂipping images (Krizhevsky  et al., 2012).  2  batch(i)batch(i+1)batchesbatch(i+1)batch(i+1)batcheslocked for weight updatingpreprocessing,copying from disk to GPUcopying within the same GPUmemory in the training processmemory in the loading processsingle GPUbatch(i)batch(i)batchesdisksteps of updating on batch(i)batch(i+1)Timew1iw2i-1w1i-1w2iw1iw2iw1iw2iw iw2iw1iw iupdatingexchanging weightsGPU1GPU2w i-1w2i-1w1i-1w i-1weight from the other gpuweight updated on this gpuweight updated on this gpuweight from the other gpuaveraging weightssteps of updating on batch(i)batch(i+1)TimeAccepted as a workshop contribution at ICLR 2015  Table 1: Training time per 20 iterations (sec)  Parallel loading  Yes No  cuda-convnet  cuDNN-R1  cuDNN-R2  2-GPU 1-GPU 2-GPU 1-GPU 2-GPU 1-GPU 32.76 23.39 43.52 28.92  19.72 26.23  39.72 49.11  20.58 27.31  34.71 45.45  Caffe  26.26  Caffe with cuDNN 20.25  Figure 2 illustrates the steps involved in training on one minibatch. For each weight3 matrix in the model, there are 2 shared variables allocated: one for updating, and one for storing weights copied from the other GPU. The shared variables for updating on 2 GPUs start the same. In the 1st step, they are updated separately on different data batches. In the 2nd step, weights are exchanged between GPUs. In the 3rd step, these weights (no longer the same) are averaged on both GPUs. At this point, 2 AlexNets sharing the same parameters are ready for training on the next mini-batch.  3 RESULTS  Our experimental system contains 2 Intel Xeon E5-2620 CPUs (6-core each and 2.10GHz), and 3 Nvidia Titan Black GPUs. 2 of the GPUs are under the same PCI-E switch and are used for the 2-GPU implementation. We did not use the third GPU. For the cuDNN library, we performed experiments on both the version of R1 and R2. For the experiments on a single GPU, we used batch size 256. Equivalently, we used batch size 128 for experiments on 2 GPUs. We recorded the time to train 20 batches (5,120 images) under different settings and compared them with Caffe4 in Table 1. We can see that both parallel loading and data parallelism on 2 GPUs bring signiﬁcant speed ups. The 2-GPU & parallel loading implementation (cuDNN-R2) is on par with the “Caffe with cuDNN” implementation. After 65 epochs of training, the top-1 class validation error rate is 42.6%, and the top-5 error rate is 19.9%, without the intensity and illumination data augmentation 5. This is within 0.5% of the results reported in the similar Caffe implementation6.  4 DISCUSSION  4.1 NATIVE THEANO MULTI-GPU SUPPORT  Native Theano multi-GPU support is under development7. Our present implementation is a tempo- rary work-around before its release, and might also provide helpful communication components on top of it.  4.2 RELATED WORK  Many multi-GPU frameworks has been proposed and implemented (Yadan et al., 2013; Zou et al., 2014; Paine et al., 2013; Krizhevsky, 2014), usually adopting a mixed data and model parallelism. This report only implements the data parallelism framework, but it could potentially, with a non- trivial amount of effort, be extended to incorporate model parallelism.  3The same operation is performed for biases and momentum. 4Performance of Caffe is according to http://caffe.berkeleyvision.org/performance_ hardware.html, where timing information for CaffeNet is provided. As CaffeNet has similar structures, we consider this as a rough reference.  5The  pretrained  parameters  are  available  for  downloading  at https://github.com/  uoguelph-mlrg/theano_alexnet/tree/master/pretrained/alexnet  6https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_  caffenet  7https://groups.google.com/d/msg/theano-users/vtR_L0QltpE/Kp5hK1nFLtsJ  3  Accepted as a workshop contribution at ICLR 2015  4.3 CHALLENGES IN PYTHON-BASED PARALLELIZATION  The Global Interpreter Lock 8 (GIL) makes parallelization difﬁcult in CPython, by disabling concur- rent threads within one process. Therefore, to parallelize, it is necessary to launch multiple processes and communicate between these processes. Straightforward inter-process communication, using the “multiprocessing” module, is very slow for 2 reasons: 1) it serializes Numpy arrays before passing between processes; 2) communication is done through host memory. These problems lead us to GPUDirect peer-to-peer memory copy, which also has many pitfalls under the multi-process set- ting. For instance, there is no host-side synchronization performed with device-to-device memory copy even when the sync API is called 9. This problem is dealt with by CUDA context syncing and additional message communications between processes, however, this and similar issues are not straightforward.  4.4 LIMITATIONS  To use the fast peer-to-peer GPU memory copy, GPUs have to be under the same PCI-E switch. Otherwise, communication has to go through the host memory which results in longer latency. Sit- uations involved with more GPUs are discussed in Krizhevsky (2014). Due to our current hardware limitation, we have only proposed and experimented with a 2-GPU implementation. This report and the code will be updated once experiments on more GPUs are performed.  ACKNOWLEDGMENTS  We acknowledge Lev Givon for giving helpful suggestions on how to use the PyCUDA library. We also acknowledge NVIDIA for an Academic Hardware Grant.  REFERENCES Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian, Bergeron, Arnaud, Bouchard, Nicolas, Warde-Farley, David, and Bengio, Yoshua. Theano: new features and speed improvements. arXiv preprint arXiv:1211.5590, 2012.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a cpu and gpu math expression compiler. In Proceedings of the Python for scientiﬁc computing confer- ence (SciPy), volume 4, pp. 3, 2010.  Chetlur, Sharan, Woolley, Cliff, Vandermersch, Philippe, Cohen, Jonathan, Tran, John, Catanzaro, Bryan, and Shelhamer, Evan. cudnn: Efﬁcient primitives for deep learning. arXiv preprint arXiv:1410.0759, 2014.  Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement. Torch7: A matlab-like environment  for machine learning. In BigLearn, NIPS Workshop, number EPFL-CONF-192376, 2011.  Goodfellow, Ian J, Warde-Farley, David, Lamblin, Pascal, Dumoulin, Vincent, Mirza, Mehdi, Pas- canu, Razvan, Bergstra, James, Bastien, Fr´ed´eric, and Bengio, Yoshua. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- In Proceedings of the ACM International Conference on Multimedia, pp. 675–678. bedding. ACM, 2014.  Kl¨ockner, Andreas, Pinto, Nicolas, Lee, Yunsup, Catanzaro, Bryan, Ivanov, Paul, and Fasih, Ahmed. Pycuda and pyopencl: A scripting-based approach to gpu run-time code generation. Parallel Computing, 38(3):157–174, 2012.  8https://wiki.python.org/moin/GlobalInterpreterLock 9http://docs.nvidia.com/cuda/cuda-driver-api/api-sync-behavior.html  4  Accepted as a workshop contribution at ICLR 2015  Krizhevsky, Alex. One weird trick for parallelizing convolutional neural networks. arXiv preprint  arXiv:1404.5997, 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  Paine, Thomas, Jin, Hailin, Yang, Jianchao, Lin, Zhe, and Huang, Thomas. Gpu asynchronous stochastic gradient descent to speed up neural network training. arXiv preprint arXiv:1312.6186, 2013.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, et al. Imagenet large scale visual recognition challenge. arXiv preprint arXiv:1409.0575, 2014.  Yadan, Omry, Adams, Keith, Taigman, Yaniv, and Ranzato, MarcAurelio. Multi-gpu training of  convnets. arXiv preprint arXiv:1312.5853, 2013.  Zou, Yongqiang, Jin, Xing, Li, Yi, Guo, Zhimao, Wang, Eryu, and Xiao, Bin. Mariana: Tencent deep learning platform and its applications. Proceedings of the VLDB Endowment, 7(13), 2014.  5  ",
1412.6568,2015, Improving zero-shot learning by mitigating the hubness problem,"['Improving zero-shot learning by mitigating the hubness problem', 'Georgiana Dinu and Marco Baroni']",https://arxiv.org/pdf/1412.6568,"5 1 0 2    r p A 5 1         ] L C . s c [      3 v 8 6 5 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  IMPROVING ZERO-SHOT LEARNING BY MITIGATING THE HUBNESS PROBLEM  Georgiana Dinu, Angeliki Lazaridou, Marco Baroni Center for Mind/Brain Sciences University of Trento (Italy) georgiana.dinu|angeliki.lazaridou|marco.baroni@unitn.it  ABSTRACT  The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains.  1  INTRODUCTION  Extensive research in computational linguistics and neural language modeling has shown that con- textual co-occurrence patterns of words in corpora can be effectively exploited to learn high-quality vector-based representations of their meaning in an unsupervised manner (Collobert et al., 2011; Clark, 2015; Turney & Pantel, 2010). This has in turn led to the development of the so-called zero- shot learning paradigm as a way to address the manual annotation bottleneck in domains where other vector-based representations (e.g., images or brain signals) must be associated to word labels (Palatucci et al., 2009). The idea is to use the limited training data available to learn a general map- ping function from vectors in the domain of interest to word vectors, and then apply the induced function to map vectors representing new entities (that were not seen in training) onto word space, retrieving the nearest neighbour words as their labels. This approach has originally been tested in neural decoding (Mitchell et al., 2008; Palatucci et al., 2009), where the task consists in learning a regression function from fMRI activation vectors to word representations, and then applying it to the brain signal of a concept outside the training set, in order to “read the mind” of subjects. In computer vision, zero-shot mapping of image vectors onto word space has been applied to the task of retriev- ing words to label images of objects outside the training inventory (Frome et al., 2013; Socher et al., 2013), as well as using the inverse language-to-vision mapping for image retrieval (Lazaridou et al., 2014a). Finally, the same approach has been applied in a multilingual context, using translation pair vectors to learn a cross-language mapping, that is then exploited to translate new words (Mikolov et al., 2013b). Zero-shot learning is a very promising and general technique to reduce manual supervision. How- ever, while all experiments above report very encouraging results, performance is generally quite low in absolute terms. For example, the system of Frome et al. (2013) returns the correct image label as top hit in less than 1% of cases in all zero-shot experiments (see their Table 2). Performance is always above chance, but clearly not of practical utility. In this paper, we study one speciﬁc problem affecting the quality of zero-shot labeling, following up on an observation that we made, qualitatively, in our experiments: The neighbourhoods surrounding mapped vectors contain many items that are “universal” neighbours, that is, they are neighbours of a large number of different mapped vectors. The presence of such vectors, known as hubs, is an intrinsic problem of high-dimensional spaces (Radovanovi´c et al., 2010b). Hubness has already  1  Accepted as a workshop contribution at ICLR 2015  been shown to be an issue for word-based vectors (Radovanovi´c et al., 2010a).1 However, as we show in Section 2, the problem is much more severe for neighbourhoods of vectors that are mapped onto a high-dimensional space from elsewhere through a regression algorithm. We leave a theoret- ical understanding of why hubness affects regression-based mappings to further work. Our current contributions are to demonstrate the hubness problem in the zero-shot setup, to present a simple and efﬁcient method to get rid of it by adjusting the similarity matrix after mapping, and to show how this brings consistent performance improvements across different tasks. While one could address the problem by directly designing hubness-repellent mapping functions, we ﬁnd our post-processing so- lution more attractive as it allows us to use very simple and general least-squares regression methods to train and perform the mapping. We use use the term pivots to stand for a set of vectors we retrieve neighbours for (these comprise at least, in our setting, the zero-shot-mapped vectors) and targets for the subspace of vectors we retrieve the neighbours from (often, corresponding to the whole space of interest). Then, we can phrase our proposal as follows. Standard nearest neighbour queries rank the targets independently for each pivot. A single target is allowed to be the nearest neighbour, or among the top k nearest neighbours, of a large proportion of pivots: and this is exactly what happens empirically (the hubness problem). We can greatly mitigate the problem by taking the global distribution of targets across pivots into account. In particular, we use the very straightforward and effective strategy of inverting the query: we convert the similarity scores of a target with all pivots to the corresponding ranks, and then retrieve the nearest neighbours of a pivot based on such ranks, instead of the original similarity scores. We will empirically show that with this method high-hubness targets are down-ranked for many pivots, and will kept as neighbours only when semantically appropriate.  2 HUBNESS IN ZERO-SHOT MAPPING  The Zero-shot setup In zero-shot learning, training data consist of vector representations in the source domain (e.g., source language for translation, image vectors for image annotation) paired with language labels (the target domain): Dtr = {(xi, yi)}m i=1, where xi ∈ Ru and yi ∈ Ttr, a vocabulary containing training labels. At test time, the task is to label vectors which have a novel label: Dts = {(xi, yi)}n i=1, yi ∈ Tts, with Tts ∩ Ttr = ∅. This is possible because labels y have vector representations y ∈ Rv.2 Training is cast as a multivariate regression problem, learning a function which maps the source domain vectors to their corresponding target (linguistic-space) vectors. A straightforward and performant choice (Lazaridou et al., 2014a; Mikolov et al., 2013b) is to assume the mapping function is a linear map W, and use a l2-regularized least-squares error objective:  ˆW = arg min W∈Rv×u  ||XW − Y||F + λ||W||  (1)  where X and Y are matrices obtained through the concatenation of train source vectors and the target vectors of the corresponding labels. Once the linear function has been estimated, any source vector x ∈ Ru can be mapped into the target domain through xT W. Target space label retrieval Given a source element x ∈ S and its vector x, the standard way to retrieve a target space label (T ) is by returning the nearest neighbour (according to some similarity measure) of mapped x from the set of vector representations of T . Following common practice, we use the cosine as our similarity measure. We denote by Rankx,T (y) the rank of an element y ∈ T w.r.t. its similarity to x and assuming a query space T . More precisely, this is the position of y in the (decreasingly) sorted list of similarities: [cos(x, yi)|yi ∈ T ]. This is an integer from 1 to |T| (assuming distinct cosine values). Under this notation, the standard nearest neighbour of x is given by:  NN1(x, T ) = arg min  y∈T  Rankx,T (y)  (2)  1Radovanovi´c et al. (2010a) propose a supervised hubness-reducing method for document vectors that is  not extensible to the zero-shot scenario, as it assumes a binary relevance classiﬁcation setup.  2We use x and x to stand for a label and its corresponding vector.  2  Accepted as a workshop contribution at ICLR 2015  (a) Original  (b) Mapped, with regularization  (c) Mapped, no regularization  Figure 1: Distribution of N20 values of target space elements (N20,T s(y) for y ∈ T ). Test elements (pivots) in the target space (original) vs. corresponding vectors obtained by mapping from the source space (mapped). Signiﬁcantly larger N20 values are observed in the mapped spaces (maxima at 57 and 40 vs. 11 in original space.)  We will use NNk(x, T ) to stand for the set of k-nearest neighbours in T , omitting the T argument for brevity. Hubness We can measure how hubby an item y ∈ T is with respect to a set of pivot vectors P (where T is the search space) by counting the number of times it occurs in the k-nearest neighbour lists of elements in P :  Nk,P (y) = |{y ∈ NNk(x, T )|x ∈ P}|  (3) An item with a large Nk value (we will omit the set subscript when it is clear from the context) occurs in the NNk set of many elements and is therefore a hub. Hubness has been shown to be an intrinsic problem of high-dimensional spaces: as we increase the dimensionality of the space, a number of elements, which are, by all means, not similar to all other items, become hubs. As a results nearest neighbour queries return the hubs at top 1, harming It is known that the problem of hubness is related to concentration, the tendency of accuracy. pairwise similarities between elements in a set to converge to a constant as the dimensionality of the space increases (Radovanovi´c et al., 2010b). Radovanovi´c et al. (2010a) show that this also holds for cosine similarity (which is used almost exclusively in linguistic applications): the expectation of pairwise similarities becomes constant and the standard deviation converges to 0. This, in turn, is known to cause an increase in hubness.  Original vs. mapped vectors In previous work we have (qualitatively) observed a tendency of the hubness problem to become worse when we query a target space in which some elements have been mapped from a different source space. In order to investigate this more closely, we compare the properties of mapped elements versus original ones. We consider word translation as an application and use 300-dimensional vectors of English words as source and vectors of Italian words as target. We have, in total, vocabularies of 200,000 English and Italian words, which we denote S and T . We use a set of 5,000 translation pairs as training data and learn a linear map. We then pick a random test set T s of 1,500 English words that have not been seen in training and map them to Italian using the learned training function (full details in Section 3.1 below). We compute the hubness of all elements in T using the test set items as pivots, and considering all 200, 000 items in the target space as potential neighbours (as any of them could be the right translation of a test word). In the ﬁrst setting (original), we use target space items: for the test instance car → auto, we use the true Italian auto vector. In the second and third settings (mapped) we use the mapped vectors (our predicted translation vector of car into Italian), mapped through a matrix learned without and with regularization, respectively. Figure 1 plots the distribution of the N20,T s(y) scores in these three settings.  3  Accepted as a workshop contribution at ICLR 2015  As the plots show, the hubness problem is indeed greatly exacerbated. When using the original T s elements, target space hubs reach a N20 level of at most 11, meaning they occur in the NN20 sets of 11 test elements. On the other hand, when using mapped elements the maximum N20 values are above 40 (note that the x axes are on different scales in the plots!). Moreover, regularization does not signiﬁcantly mitigate hubness, suggesting that it is not just a matter of overﬁtting, such that the mapping function projects everything near vectors it sees during training.  3 A GLOBALLY CORRECTED NEIGHBOUR RETRIEVAL METHOD  One way to correct for the increase in hubness caused by mapping is to compute hubness scores for all target space elements. Then, given a test set item, we re-rank its nearest neighbours by downplay- ing the importance of elements that have a high hubness score. Methods for this have been proposed and evaluated, for example, by Radovanovi´c et al. (2010a) and Tomasev et al. (2011a). We adopt a much simpler approach (similar in spirit to Tomasev et al., 2011b, but greatly simpliﬁed), which takes advantage of the fact that we almost always have access not to just 1 test instance, but more vectors in the source domain (these do not need to be labeled instances). We map these additional pivot elements and conjecture that we can use the topology of the subspace where the mapped pivot set lives to correct nearest neighbour retrieval. We consider ﬁrst the most straightforward way to achieve this effect. A hub is an element which appears in many NNk lists because it has high sim- ilarity with many items. A simple way to correct for this is to normalize the vector of similarities of each target item to the mapped pivots to length 1, prior to performing NN queries. This way, a vector with very high similarities to many pivots will be penalized. We denote this method NNnrm. We propose a second corrected measure, which does not re-weight the similarity scores, but ranks target elements using NN statistics for the entire mapped pivot set. Instead of the nearest neighbour retrieval method in Equation (2), we use a following globally-corrected (GC) approach, that could be straightforwardly implemented as:  GC1(x, T ) = arg min  y∈T  Ranky,P (x)  (4)  To put it simply, this method reverses the querying: instead of returning the nearest neighbour of pivot x as a solution, it returns the target element y which has x ranked highest. Intuitively, a hub may still occur in the NN lists of some elements, but only if not better alternatives are present. The formulation of GC in Equation (4) can however lead to many tied ranks: For example, we want to translate car, but both Italian auto and macchina have car as their second nearest neighbour (so both rank 2) and no Italian word has car as ﬁrst neighbour (no rank 1 value). We use the cosine scores to break ties, therefore car will be translated with auto if the latter has a higher cosine with the mapped car vector, with macchina otherwise. Note that when only one source vector is available, the GC method becomes equivalent to a standard NN query. As the cosine is smaller than 1 and ranks larger or equal to 1, the following equation implements GC with cosine-based tie breaking:  GC1(x, T ) = arg min  y∈T  (Ranky,P (x) − cos(x, y))  (5)  3.1 ENGLISH TO ITALIAN WORD TRANSLATION  We ﬁrst test our methods on bilingual lexicon induction. As the amount of parallel data is limited, there has been a lot of work on acquiring translation dictionaries by using vector-space methods on monolingual corpora, together with a small seed lexicon (Haghighi et al., 2008; Klementiev et al., 2012; Koehn & Knight, 2002; Rapp, 1999). One of the most straightforward and effective methods is to represent words as high-dimensional vectors that encode co-occurrence only with the words in the seed lexicon and are therefore comparable cross-lingually (Klementiev et al., 2012; Rapp, 1999). However, this method is limited to vector spaces that use words as context features, and does not extend to vector-based word representations relying on other kinds of dimensions, such as those neural language models that have recently been shown to greatly outperform context-word-based representations (Baroni et al., 2014). The zero-shot approach, that induces a function from one space to the other based on paired seed element vectors, and then applies it to new data, works irrespective of the choice of vector representation. This method has been shown to be effective for bilingual lexicon construction by Mikolov et al. (2013b), with Dinu & Baroni (2014) reporting overall better performance than with the seed-word-dimension method. We set up a similar evaluation on the task of ﬁnding Italian translations of English words.  4  Accepted as a workshop contribution at ICLR 2015  Word representations The cbow method introduced by Mikolov et al. (2013a) induces vector- based word representations by trying to predict a target word from the words surrounding it within a neural network architecture. We use the word2vec toolkit3 to learn 300-dimensional representations of 200,000 words with cbow. We consider a context window of 5 words to either side of the target, we set the sub-sampling option to 1e-05 and estimate the probability of a target word with the negative sampling method, drawing 10 samples from the noise distribution (Mikolov et al., 2013a). We use 2.8 billion tokens as input (ukWaC + Wikipedia + BNC) for English and the 1.6 billion itWaC tokens for Italian.4  Training and testing Both train and test translation pairs are extracted from a dictionary built from Europarl, available at http://opus.lingfil.uu.se/ (Europarl, en-it) (Tiedemann, 2012). We use 1,500 English words split into 5 frequency bins as test set (300 randomly chosen in each bin). The bins are deﬁned in terms of rank in the (frequency-sorted) lexicon: [1-5K], [5K-20K], [20K-50K], [50K-100K] and [100K-200K]. The bilingual lexicon acquisition literature generally tests on very frequent words only. Translating medium or low frequency words is however both more challenging and useful. We also sample the training translation pairs by frequency, using the top 1K, 5K, 10K and 20K most frequent translation pairs from our dictionary (by English frequency), while making sure there is no overlap with test elements. For each test element we query the entire (200,000) target space and report translation accuracies. An English word may occur with more than one Italian translation (1.2 on average in the entire data): in evaluation, an instance is considered correct if any of these is predicted. We test the standard method (regular NN querying) as well as the two corrected methods: NNnrm and GC. As previously discussed, the latter beneﬁt from more mapped data, in addition to an individual test instance, to be used as pivots. In addition to the 1,500 test elements, we report performance when mapping other 20,000 randomly chosen English words (their Italian translations are not needed). We actually observed improvements also when using solely the 1,500 mapped test elements as pivots, but increasing the size with arbitrary additional data (that can simply be sampled from the source space without any need for supervision) helps performance.  Results Results are given in Figure 2. We report results without regularization as well as with the regularization parameter λ estimated by generalized cross-validation (GCV) (Hastie et al., 2009, p. 244). Both corrected methods achieve signiﬁcant improvements over standard NN, ranging from 7% to 14%. For the standard method, the performance decreases as the training data size increases beyond 5K, probably due to the noise added by lower-frequency words. The corrected measures are robust against this effect: adding more training data does not help, but it does not harm them either. Regularization does not improve, and actually hampers the standard method, whereas it beneﬁts the corrected measures when using a small amount of training data (1K), and does not affect performance otherwise. The results by frequency bin show that most of the improvements are brought about for the all-important medium- and low-frequency words. Although not directly comparable, the absolute numbers we obtain are in the range of those reported by Mikolov et al. (2013b), whose test data correspond, in terms of frequency, to those in our ﬁrst 2 bins. Furthermore, we observe, similarly to them, that the accuracy scores underestimate the actual performance, as many translations are in fact correct but not present in our gold dictionary. The elements with the largest hub score are shown in Figure 3 (left). As can be seen, they tend to be “garbage” low-frequency words. However, in any realistic setting such low-frequency terms should not be ﬁltered out, as good translations might also have low frequency. As pointed out by Radovanovi´c et al. (2010b), hubness correlates with proximity to the test-set mean vector (the average of all test vectors). Hubness level is plotted against cosine-to-mean in Figure 3 (right). Table 1 presents some cases where wrong translation are “corrected” by the GC measure. The latter consistently pushes high-hubness elements down the neighbour lists. For example, 11/09/2002, that was originally returned as the translation of backwardness, can be found in the N20 list of 110 En- glish words. With the corrected method, the right translation, arretratezza, is obtained. 11/09/2002 is returned as the translation, this time, of only two other English pivot words: orthodoxies and ku-  3https://code.google.com/p/word2vec/ 4Corpus sources: http://wacky.sslmit.unibo.it, http://en.wikipedia.org, http://  www.natcorp.ox.ac.uk  5  Accepted as a workshop contribution at ICLR 2015  Train size  1K 5K 10K 20K  Train size  1K 5K 10K 20K  No regularization  NN NNnrm GC 20.9 14.9 37.7 30.3 37.5 30.0 37.9 25.1  20.9 33.1 33.5 32.9  GCV  NN NNnrm GC 28.7 12.4 27.7 37.5 37.3 28.2 23.7 37.8  25.5 32.9 33.1 32.0  Figure 2: Percentage accuracy scores for En→It translation. Left: No regularization and Generalized Cross-validation (GCV) regression varying the training size. Right: Results split by frequency bins, 5K training and no regularization.  Hub blockmonthoff 04.02.05 communauts limassol and ampelia 11/09/2002 cgsi 100.0 cingevano  N20 40 26 26 25 23 23 20 19 18 18  Figure 3: En→It translation. Left: Top 10 hubs and their N20,T st scores. Right: N20 plotted against cosine similarities to test-set mean vector. N20 values correlate signiﬁcantly with cosines (Spearman ρ = 0.30, p = 1.0e − 300).  N20(Hub) x|Hub = NN1(x)  Translation almighty→onnipotente NN:dio GC: onnipotente Hub: dio (god) killers→killer NN: violentatori GC: killer Hub: violentatori (rapists) backwardness→arretratezza NN: 11/09/2002 Hub: 11/09/2002 GC: arretratezza Table 1: En→It translation. Examples of GC “correcting” NN. The wrong NN translation is always a polluting hub. For these NN hubs, we also report their hubness scores before and after correction (N20, over the whole pivot set) and examples of words they are nearest neighbour of (x|Hub = NN1(x)) for NN and GC.  righteousness,almighty,jehovah,incarnate,god... god killers,anders,rapists,abusers,ragnar rapists backwardness,progressivism,orthodoxies... orthodoxies,kumaratunga  38 20 64 22 110 24  6  Accepted as a workshop contribution at ICLR 2015  Train  Chance  GCV  Vis→Lang Lang→Vis  0.02 0.02  NN NNnrm GC 1.5 1.0 0.6 2.2  0.8 1.4  Table 2: Percentage label and image retrieval accuracy.  maratunga. The hubs we correct for are not only garbage ones, such as 11/09/2002, but also more standard words such as dio (god) or violentatori (rapists), also shown in Table 1.5  3.2 ZERO-SHOT IMAGE LABELING AND RETRIEVING  In this section we test our proposed method in a cross-modal setting, mapping images to word labels and vice-versa.  Experimental setting We use the data set of Lazaridou et al. (2014b) containing 5,000 word labels, each associated to 100 ImageNet pictures (Deng et al., 2009). Word representations are extracted from Wikipedia with word2vec in skip-gram mode. Images are represented by 4096- dimensional vectors extracted using the Caffe toolkit (Jia et al., 2014) together with the pre-trained convolutional neural network of Krizhevsky et al. (2012). We use a random 4/1 train/test split. Results We consider both the usual image labeling setting (Vision→Language) and the image retrieval setting (Language→Vision). For the Vision→Language task, we use as pivot set the 100K test images (1,000 labels x 100 images/label) and an additional randomly chosen 100K images. The search space is the entire label set of 5,000 words. For Language→Vision, we use as pivot set the entire word list (5,000) and the target set is the entire set of images (500,000). The objects depicted in the images form a set of 5,000 distinct elements, therefore, for the word cat, for example, returning any of the 100 cat images is correct. Chance accuracy in both settings is thus at 1/5,000. Table 2 reports accuracy scores.6 We observe that, differently from the translation case, correcting by normalizing the cosine scores of the elements in the target domain (NNnrm) leads to poorer results than no correction. On the other hand, the GC method is consistent across domains, and it improves signiﬁcantly on the standard NN method in both settings. Note that, while there are differences between the setups, Frome et al. (2013) report accuracy results below 1% in all their zero-shot experiments, including those with chance levels comparable to ours. In order to investigate the hubness of the corrected solution, we plot similar ﬁgures as in Section 2, computing the N20 distribution of the target space elements w.r.t the pivots in the test set.7 Fig- ure 4 shows this distribution 1) for the vectors of the gold word labels in language space, 2) the corresponding Vision→Language mapped test vectors, as well as 3) N20 values computed using GC correction.8 Similarly to the translation case, the maximum hubness values increase signiﬁ- cantly from the original target space vectors to the mapped items. When adjusting the rank with the GC method, hubness decreases to a level that is now below that of the original items. We observe  corresponding prediction ˆyi = Wxi, the error is given by:(cid:80)k  5Prompted by a reviewer, we also performed preliminary experiments with a margin-based ranking objective similar to the one in WSABIE Weston et al. (2011) and DeViSE Frome et al. (2013) which is typically reported to outperform the l2 objective in Equation 1 (Socher et al. (2014)). Given a pair of training items (xi, yi) and the j=1,j(cid:54)=i max{0, γ+dist(ˆyi, yi)−dist(ˆyi, yj)}, where dist is a distance measure, which we take to be inverse cosine, and γ and k are the margin and the number of negative examples, respectively. We tune γ-the margin and k-the number of negative samples on a held-out set containing 25% of the training data. We estimate W using stochastic gradient descent where per-parameter learning rates are tuned with Adagrad Duchi et al. (2011). Results on the En→It task are at 38.4 (NN) and further improved to 40.6 (GC retrieval), conﬁrming that GC is not limited to least-squares error estimation settings.  6The non-regularized objective led to very low results in both directions and for all methods, and we omit  label (e.g., we obtain a single cat vector in image space by averaging the vectors of 100 cat pictures).  7In order to facilitate these computations, we use “aggregated” visual vectors corresponding to each word 8We abuse notation here, as N20 is deﬁned as in Equation 3 for 1) and 2) and as |{y ∈ GCk(x, T )|x ∈ P (cid:48)}|  these results.  for 3).  7  Accepted as a workshop contribution at ICLR 2015  (a) Original  (b) Mapped  (c) Corrected  Figure 4: Vision→Language. Distribution of N20 values of target space elements (N20,T s(y) for y ∈ T ). Pivot vectors of the gold word labels of test elements in target (word) space (original) vs. corresponding mapped vectors (mapped) vs. corrected mapped vectors (GC) (corrected). N20 values increase from original to mapped sets (max 35 vs. 190), but they drop when using GC cor- rection (max 20).  the same trend in the Language→Vision direction (as well as in the translation experiments in the previous section), the speciﬁcs of which we however leave out for brevity.  4 CONCLUSION  In this paper we have shown that the basic setup in zero-shot experiments (use multivariate linear regression with a regularized least-squares error objective to learn a mapping across representational vectors spaces) is negatively affected by strong hubness effects. We proposed a simple way to cor- rect for this by replacing the traditional nearest neighbour queries with globally adjusted ones. The method only requires the availability of more, unlabeled source space data, in addition to the test instances. While more advanced ways for learning the mapping could be employed (e.g., incorpo- rating hubness avoidance strategies into non-linear functions or different learning objectives), we have shown that consistent improvements can be obtained, in very different domains, already with our query-time correction of the basic learning setup, which is a popular and attractive one, given its simplicity, generality and high performance. In future work we plan to investigate whether the hubness effect carries through to other setups: For example, to what extent different kinds of word representations and other learning objectives are affected by it. This empirical work should pose a solid basis for a better theoretical understanding of the causes of hubness increase in cross-space mapping.  5 ACKNOWLEDGMENTS  This work was supported by the ERC 2011 Starting Independent Research Grant n. 283554 (COM- POSES).  REFERENCES  Baroni, Marco, Dinu, Georgiana, and Kruszewski, Germ´an. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of ACL, pp. 238–247, Baltimore, MD, 2014.  Clark, Stephen. Vector space models of lexical meaning. In Lappin, Shalom and Fox, Chris (eds.), Handbook of Contemporary Semantics, 2nd ed. Blackwell, Malden, MA, 2015. In press; http: //www.cl.cam.ac.uk/˜sc609/pubs/sem_handbook.pdf.  8  Accepted as a workshop contribution at ICLR 2015  Collobert, Ronan, Weston, Jason, Bottou, L´eon, Karlen, Michael, Kavukcuoglu, Koray, and Kuksa, Pavel. Natural language processing (almost) from scratch. Journal of Machine Learning Re- search, 12:2493–2537, 2011.  Deng, Jia, Dong, Wei, Socher, Richard, Li, Lia-Ji, and Fei-Fei, Li. Imagenet: A large-scale hierar-  chical image database. In Proceedings of CVPR, pp. 248–255, Miami Beach, FL, 2009.  Dinu, Georgiana and Baroni, Marco. How to make words with vectors: Phrase generation in distri-  butional semantics. In Proceedings of ACL, pp. 624–633, Baltimore, MD, 2014.  Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning  and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.  Frome, Andrea, Corrado, Greg, Shlens, Jon, Bengio, Samy, Dean, Jeff, Ranzato, Marc’Aurelio, and Mikolov, Tomas. DeViSE: A deep visual-semantic embedding model. In Proceedings of NIPS, pp. 2121–2129, Lake Tahoe, NV, 2013.  Haghighi, Aria, Liang, Percy, Berg-Kirkpatrick, Taylor, and Klein, Dan. Learning bilingual lexicons In Proceedings of ACL, pp. 771–779, Columbus, OH, USA, June  from monolingual corpora. 2008. URL http://www.aclweb.org/anthology/P/P08/P08-1088.  Hastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. The Elements of Statistical Learning, 2nd  edition. Springer, New York, 2009.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- In Proceedings of the ACM International Conference on Multimedia, MM ’14, pp. bedding. 675–678, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3063-3. doi: 10.1145/2647868. 2654889. URL http://doi.acm.org/10.1145/2647868.2654889.  Klementiev, Alexandre, Irvine, Ann, Callison-Burch, Chris, and Yarowsky, David. Toward statistical machine translation without parallel corpora. In Proceedings of EACL, pp. 130–140, Avignon, France, 2012. ISBN 978-1-937284-19-0. URL http://dl.acm.org/citation.cfm? id=2380816.2380835.  Koehn, Philipp and Knight, Kevin. Learning a translation lexicon from monolingual corpora. In In Proceedings of ACL Workshop on Unsupervised Lexical Acquisition, pp. 9–16, Philadelphia, PA, USA, 2002.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey. ImageNet classiﬁcation with deep convo-  lutional neural networks. In Proceedings of NIPS, pp. 1097–1105, Lake Tahoe, Nevada, 2012.  Lazaridou, Angeliki, Bruni, Elia, and Baroni, Marco. Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world. In Proceedings of ACL, pp. 1403–1414, Baltimore, MD, 2014a.  Lazaridou, Angeliki, Pham, The Nghia, and Baroni, Marco. Combining language and vision with a  multimodal skip-gram model. In NIPS workshop on Learning Semantics, 2014b.  Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efﬁcient estimation of word repre-  sentations in vector space. http://arxiv.org/abs/1301.3781/, 2013a.  Mikolov, Tomas, Le, Quoc, and Sutskever, Ilya. Exploiting similarities among languages for Ma-  chine Translation. http://arxiv.org/abs/1309.4168, 2013b.  Mitchell, Tom, Shinkareva, Svetlana, Carlson, Andrew, Chang, Kai-Min, Malave, Vincente, Mason, Robert, and Just, Marcel. Predicting human brain activity associated with the meanings of nouns. Science, 320:1191–1195, 2008.  Palatucci, Mark, Pomerleau, Dean, Hinton, Geoffrey E, and Mitchell, Tom M. Zero-shot learning  with semantic output codes. In Proceedings of NIPS, pp. 1410–1418, 2009.  Radovanovi´c, Milos, Nanopoulos, Alexandros, and Ivanovi´c, Mirjana. On the existence of obstinate  results in vector space models. In Proceedings of SIGIR, pp. 186–193, 2010a.  9  Accepted as a workshop contribution at ICLR 2015  Radovanovi´c, Miloˇs, Nanopoulos, Alexandros, and Ivanovi´c, Mirjana. Hubs in space: Popular nearest neighbors in high-dimensional data. Journal of Machine Learning Research, 11:2487– 2531, 2010b.  Rapp, Reinhard. Automatic identiﬁcation of word translations from unrelated english and german corpora. In Proceedings of the 37th annual meeting of the Association for Computational Lin- guistics on Computational Linguistics, ACL ’99, pp. 519–526. Association for Computational Linguistics, 1999.  Socher, Richard, Ganjoo, Milind, Manning, Christopher, and Ng, Andrew. Zero-shot learning  through cross-modal transfer. In Proceedings of NIPS, pp. 935–943, Lake Tahoe, NV, 2013.  Socher, Richard, Le, Quoc, Manning, Christopher, and Ng, Andrew. Grounded compositional se- mantics for ﬁnding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218, 2014.  Tiedemann, J¨org. Parallel data, tools and interfaces in opus. In Proceedings of the Eight Interna-  tional Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, 2012.  Tomasev, Nenad, Brehar, Raluca, Mladenic, Dunja, and Nedevschi, Sergiu. The inﬂuence of hubness Intelligent Computer Communication and  on nearest-neighbor methods in object recognition. Processing (ICCP), 2011 IEEE International Conference, 2011a.  Tomasev, Nenad, Radovanovic, Milos, Mladenic, Dunja, and Ivanovic, Mirjana. A probabilistic  approach to nearest-neighbor classiﬁcation: naive hubness bayesian knn. In CIKM, 2011b.  Turney, Peter and Pantel, Patrick. From frequency to meaning: Vector space models of semantics.  Journal of Artiﬁcial Intelligence Research, 37:141–188, 2010.  Weston, Jason, Bengio, Samy, and Usunier, Nicolas. Wsabie: Scaling up to large vocabulary image  annotation. In Proceedings of IJCAI, pp. 2764–2770, 2011.  10  ",
1412.5836,2015, Incorporating Both Distributional and Relational Semantics in Word Representations,"['Incorporating Both Distributional and Relational Semantics in Word Representations', 'Daniel Fried and Kevin Duh']",https://arxiv.org/pdf/1412.5836,"5 1 0 2    r a     M 1 2      ] L C . s c [      3 v 6 3 8 5  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  INCORPORATING BOTH DISTRIBUTIONAL AND RELA- TIONAL SEMANTICS IN WORD REPRESENTATIONS  Daniel Fried∗ Department of Computer Science University of Arizona Tucson, Arizona, USA dfried@email.arizona.edu  Kevin Duh Graduate School of Information Science Nara Institute of Science and Technology Ikoma, Nara, JAPAN kevinduh@is.naist.jp  ABSTRACT  We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which ﬂexibly optimizes a distribu- tional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.  INTRODUCTION  1 We are interested in algorithms for learning vector representations of words. Recent work has shown that such representations can capture the semantic and syntactic regularities of words (Mikolov et al., 2013a) and improve the performance of various Natural Language Processing systems (Turian et al., 2010; Wang & Manning, 2013; Socher et al., 2013a; Collobert et al., 2011). Although many kinds of representation learning algorithms have been proposed so far, they are all essentially based on the same premise of distributional semantics (Harris, 1954). For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations using the context window around the word. Intuitively, these algorithms learn to map words with similar context to nearby points in vector space. However, distributional semantics is by no means the only theory of word meaning. Relational semantics, exempliﬁed by WordNet (Miller, 1995), deﬁnes a graph of relations such as synonymy and hypernymy (Cruse, 1986) between words, reﬂecting our world knowledge and psychological predispositions. For example, a relation like “dog is-a mammal” describes a precise hierarchy that complements the distributional similarities observable from corpora. We believe both distributional and relational semantics are valuable for word representations, and investigate combining these approaches into a uniﬁed representation learning algorithm based on the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011). Its advantages include (a) ﬂexibility in incorporating arbitrary objectives, and (b) relative ease of implementation. We show that ADMM effectively optimizes the joint objective and present preliminary results on several tasks. 2 DISTRIBUTIONAL AND RELATIONAL OBJECTIVES Distributional Semantics Objective: We implement distributional semantics using the Neural Language Model (NLM) of Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi ∈ Rd, the word’s embedding. An n-length sequence of words (i1, i2, . . . , in) is represented as a vector x by concatenating the vector embeddings for each word, x = [wi1 ; wi2 . . . ; win ]. This vector x is then scored by feeding it through a two-layer neural net- work with h hidden nodes: SN LM (x) = u(cid:62)(f (Ax + b)), where A ∈ Rh×(nd), b ∈ Rh, u ∈ Rh  ∗Currently at the University of Cambridge.  1  Accepted as a workshop contribution at ICLR 2015  LN LM (x, xc) = max(0, 1 − SN LM (x) + SN LM (xc))  are network parameters and f is the sigmoid f (t) = 1/(1 + e−t) applied element-wise. The model is trained using noise contrastive estimation (NCE) (Mnih & Kavukcuoglu, 2013), where training text is corrupted by random replacement of random words to provide an implicit negative training example, xc. The hinge-loss function, comparing positive and negative training example scores, is: (1) The word embeddings, w, and other network parameters are optimized with backpropagation using stochastic gradient descent (SGD) over n-grams in the training corpus. Relational Semantics Objective: We investigate three different objectives, each modeling relations from WordNet. The Graph Distance loss, LGD, enforces the idea that words close together in the WordNet graph should have similar embeddings in vector space. First, for a word pair (i, j), we de- ﬁne a pairwise word similarity W ordSim(i, j) as the normalized shortest path between the words’ synonym sets in the WordNet relational graph (Leacock & Chodorow, 1998). Then, we encourage the cosine similarity between their embeddings vi and vj to match that of W ordSim(i, j):  LGD(i, j) =  − [a × W ordSim(i, j) + b]  (2)  (cid:19)2  (cid:18) vi · vj  ||vi||2||vj||2  where a and b are parameters that scale W ordSim(i, j) to be of the same range as the cosine simi- larity. Training proceeds by SGD: word pairs (i, j) are sampled from the WordNet graph, and both the word embeddings v and parameters a, b are updated by gradient descent on the loss function. A different approach directly models each WordNet relation as an operation in vector space. These models assign scalar plausibility scores to input tuples (vl, R, vr), modeling the plausibility of a relation of type R between words vl and vr. In both of the relational models we consider, each type of relationship (for example, synonymy or hypernymy) has a distinct set of parameters used to represent the relationship as a function in vector space. The TransE model of Bordes et al. (2013) represents relations as linear translations: if the relationship R holds for two words vl and vr, then their embeddings vl, vr ∈ Rd should be close after translating vl by a relation vector R ∈ Rd:  (3) Socher et al. (2013b) introduce a Neural Tensor Network (NTN) that models interaction between embeddings using tensors and a non-linearity function. The scoring function for a input tuple is:  ST ransE(vl, R, vr) = −||vl + R − vr||2  (cid:62)  (cid:62) l WRvr + VR  f  v  vr  + bR  SN T N (vl, R, vr) = U  (4) where U ∈ Rh, WR ∈ Rd×d×h, VR ∈ Rh×2d and bR ∈ Rk are parameters for relationship R. As in the NLM, parameters for these relational models are trained using NCE (producing a noisy example for each training example by randomly replacing one of the tuples’ entries) and SGD, using the hinge loss as deﬁned in Eq. 1, with SN LM replaced by the ST ransE or SN T N scoring function. Joint Objective Optimization by ADMM: We now describe an ADMM formulation for joint op- timization of the above objectives. Let w be the set of word embeddings {w1, w2, . . . wN(cid:48)} for the distributional objective, and v be the set of word embeddings {v1, v2, . . . vN(cid:48)(cid:48)} for the relational objective, where N(cid:48) and N(cid:48)(cid:48) are the vocabulary size of the corpus and WordNet, respectively. Let I be the set of N words that occur in both. Then we deﬁne a set of vectors y = {y1, y2, . . . yN}, which correspond to Lagrange multipliers, to penalize the difference (wi − vi) between sets of embeddings for each word i in the joint vocabulary I, producing a Lagrangian penalty term:  (cid:20)vl  (cid:21)  (cid:19)  (cid:18)  (cid:88)  (cid:16)  i∈I  (cid:17)  (cid:32)(cid:88)  i∈I  LP (w, v) =  i (wi − vi) (cid:62) y  +  ρ 2  (wi − vi)  (cid:62)  (wi − vi)  (5)  (cid:33)  In the ﬁrst term, y has same dimensionality as w and v, so a scalar penalty is maintained for each entry in every embedding vector. This constrains corresponding w and v vectors to be close to each other. The second residual penalty term with hyperparameter ρ is added to avoid saddle points; ρ can be viewed as a step-size during the update of y. This augmented Lagrangian term (Eq. 5) is added to the sum of the loss terms for each objective (Eq. 1 and Eq. 2). Let θ = (u, A, b) be the parameters of the distributional objective, and φ be the parameters of the relational objective. The ﬁnal loss function we optimize becomes:  L = LN LM (w, θ) + LGD(v, φ) + LP (w, v)  (6)  2  Accepted as a workshop contribution at ICLR 2015  Figure 1: Analysis of ADMM behavior by training iteration, for varying ρ. Left: Joint loss, LN LM + LGD, on the training data . Right: Normalized residual magnitude, averaged across embeddings.  Knowledge Base  Analogy Test  Parsing  NLM GD - 41  - 42 76.03  75.90  GD+NLM TransE 82.87  - 41 76.18  37  75.86  TransE+NLM NTN NTN+NLM  83.10 38  76.01  80.95  36  75.85  81.27  41  76.14  Table 1: Results summary: Accuracy on knowledge base completion, MaxDiff accuracy on Analogy Test, and Label Arc Score Accuracy on Dependency Parsing for single- and joint-objective models.  The ADMM algorithm proceeds by repeating the following three steps until convergence: (1) Perform SGD on w and θ to minimize LN LM + LP , with all other parameters ﬁxed. (2) Perform SGD on v and φ to minimize LGD + LP , with all other parameters ﬁxed. (3) For all embeddings i corresponding to words in both the n-gram and relational training sets, update the constraint vector yi := yi + ρ(wi − vi). Since LN LM and LGD share no parameters, Steps (1) and (2) can be optimized easily using the single-objective NCE and SGD procedures, with additional regularization term ρ (wi − vi). 3 PRELIMINARY EXPERIMENTS & DISCUSSIONS The distributional objective LN LM is trained using 5-grams from the Google Books English corpus1, containing over 180 million 5-gram types. The top 50k unigrams by frequency are used as the vocabulary, and each training iteration samples 100k n-grams from the corpus. For training LGD, we sample 100k words from WordNet and compute the similarity of each to 5 other words in each ADMM iteration. For training LT ransE and LN T N , we use the dataset of Socher et al. (2013b), presenting the entire training set of correct and noise-contrastive corrupted examples one instance at a time in randomized order for each iteration. We ﬁrst provide an analysis of the behavior of ADMM on the training set, to conﬁrm that it ef- fectively optimizes the joint objective. Fig. 1(left) plots the learning curve by training iteration for various values of the ρ hyperparameter. We see that ADMM attains a reasonable objective value rel- atively quickly in 100 iterations. Fig. 1(right) shows the averaged difference between the resulting sets of embeddings w and v, which decreases as desired.2 Next, we compare the embeddings learned with different objectives on three standard benchmark tasks (Table 1). First, the Knowledge Base Completion task (Socher et al., 2013b) evaluates the models’ ability to classify relationship triples from WordNet as correct. Triples are scored using the relational scoring functions (Eq.3 and 4) with the learned model parameters. The model uses a de- velopment set of data to determine a plausibility threeshold, and classiﬁes triples with a higher score than the threshold as correct, and those with lower score as incorrect. Secondly, the SemEval2012 Analogy Test is a relational word similarity task similar to SAT-style analogy questions (Jurgens et al., 2012). Given a set of four or ﬁve word pairs, the model selects the pairs that most and least represent a particular relation (deﬁned by a set of example word pairs) by comparing the cosine similarity of the vector difference between words in each pair. Finally, the Dependency Parsing task on the SANCL2012 data (Petrov & McDonald, 2012) evaluates the accuracy of parsers trained on news domain adapted for web domain. We incorporate the embeddings as additional features in  1Berkeley distribution: tomato.banatao.berkeley.edu:8080/berkeleylm_binaries/ 2The reason for the peak around iteration 50 in Fig. 1 is that the embeddings begin with similar random initializations, so initially differences are small; as ADMM starts to see more data, w and v diverge, but converge eventually as y become large.  3  6008000.000.050.100.150.200.250.300.350.400.45LLM+LGDNLMloss+WordNetlossAccepted as a workshop contribution at ICLR 2015  a standard maximum spanning tree dependency parser to see whether embeddings improve general- ization of out-of-domain words. The evaluation metric is the labeled attachment score, the accuracy of predicting both correct syntactic attachment and relation label for each word. For both Knowledge Base and Parsing tasks, we observe that joint objective generally improves over single objectives: e.g. TransE+NLM (83.10%) > TransE (82.87%) for Knowledge Base, GD+NLM (76.18%) > GD (75.90%) for Parsing. The improvements are not large, but relatively consistent. For the Analogy Test, joint objectives did not improve over the single objective NLM baseline. We provide further analysis as well as extended descriptions of methods and experiments in a longer version of the paper here: http://arxiv.org/abs/1412.4369.  ACKNOWLEDGMENTS  This work is supported by a Microsoft Research CORE Grant and JSPS KAKENHI Grant Number 26730121. D.F. was supported by the Flinn Scholarship during the course of this work. We thank Haixun Wang, Jun’ichi Tsujii, Tim Baldwin, Yuji Matsumoto, and several anonymous reviewers for helpful discussions at various stages of the project.  REFERENCES Bengio, Yoshua, Ducharme, R´ejean, Vincent, Pascal, and Jauvin, Christian. A neural probabilistic  language models. JMLR, 2003.  Bordes, Antoine, Usunier, Nicolas, Garcia-Duran, Alberto, Weston, Jason, and Yakhnenko, Oksana. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, pp. 2787–2795, 2013.  Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and Eckstein, Jonathan. Distributed opti- mization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1–122, 2011.  Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.  Cruse, Alan D. Lexical Semantics. Cambridge Univ. Press, 1986.  Harris, Zellig. Distributional structure. Word, 10(23):146–162, 1954.  Jurgens, David A, Turney, Peter D, Mohammad, Saif M, and Holyoak, Keith J. Semeval-2012 task 2: Measuring degrees of relational similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pp. 356–364. Association for Computational Linguistics, 2012.  Leacock, Claudia and Chodorow, Martin. Combining local context and WordNet similarity for word  sense identiﬁcation. WordNet: An Electronic Lexical Database, pp. 265–283, 1998.  Mikolov, Tomas, Yih, Wen-tau, and Zweig, Geoffrey. Linguistic regularities in continuous space In Proceedings of the 2013 Conference of the North American Chapter word representations. of the Association for Computational Linguistics: Human Language Technologies, pp. 746–751, Atlanta, Georgia, June 2013a. Association for Computational Linguistics. URL http://www. aclweb.org/anthology/N13-1090.  Mikolov, Tom´a˘s, Sutskever, Ilya, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Distributed repre-  sentations of words and phrases and their compositionality. In NIPS, 2013b.  Miller, George A. WordNet: A lexical database for English. Communications of the ACM, 38(11):  39–41, 1995.  Mnih, Andriy and Kavukcuoglu, Koray.  Learning word embeddings efﬁciently with noise- contrastive estimation. In Advances in Neural Information Processing Systems 26 (NIPS 2013), 2013.  4  Accepted as a workshop contribution at ICLR 2015  Petrov, Slav and McDonald, Ryan. Overview of the 2012 shared task on parsing the web. In Notes  of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), 2012.  Schwenk, Holger. Continuous space language models. Computer Speech and Language, 21(3): ISSN 0885-2308. doi: 10.1016/j.csl.2006.09.003. URL http://dx.  492–518, July 2007. doi.org/10.1016/j.csl.2006.09.003.  Socher, Richard, Bauer, John, Manning, Christopher D., and Ng, Andrew Y. Parsing with com- positional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 455–465, Soﬁa, Bulgaria, August 2013a. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ P13-1045.  Socher, Richard, Chen, Danqi, Manning, Christopher D., and Ng, Andrew Y. Reasoning with neural  tensor networks for knowledge base completion. In NIPS, 2013b.  Turian, Joseph, Ratinov, Lev-Arie, and Bengio, Yoshua. Word representations: A simple and general method for semi-supervise learning. In Proceedings of the 48th Annual Meeting of the Associ- ation for Computational Linguistics, pp. 384–394, Uppsala, Sweden, July 2010. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P10-1040.  Wang, Mengqiu and Manning, Christopher D. Effect of non-linear deep architecture in sequence labeling. In Proceedings of the Sixth International Joint Conference on Natural Language Pro- cessing, pp. 1285–1291, Nagoya, Japan, October 2013. Asian Federation of Natural Language Processing. URL http://www.aclweb.org/anthology/I13-1183.  5  ",
1412.6581,2015, Variational Recurrent Auto-Encoders,"['Variational Recurrent Auto-Encoders', 'Otto Fabius and Joost van Amersfoort']",https://arxiv.org/pdf/1412.6581,"5 1 0 2     n u J    5 1      ] L M  . t a t s [      6 v 1 8 5 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  VARIATIONAL RECURRENT AUTO-ENCODERS  Otto Fabius & Joost R. van Amersfoort Machine Learning Group University of Amsterdam {ottofabius,joost.van.amersfoort}@gmail.com  ABSTRACT  In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efﬁcient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.  1  INTRODUCTION  Recurrent Neural Networks (RNNs) exhibit dynamic temporal behaviour which makes them suitable for capturing time dependencies in temporal data. Recently, they have been succesfully applied to handwriting recognition (Graves et al., 2009) and music modelling (Boulanger-Lewandowski et al., 2012). In another more recent development, Cho et al. (2014) introduced a new model structure consisting of two RNN networks, an encoder and a decoder. The encoder encodes the input to an intermediate representation which forms the input for the decoder. The resulting model was able to obtain a state-of-the-art BLEU score. We propose a new RNN model based on Variational Bayes: the Variational Recurrent Auto Encoder (VRAE). This model is similar to an auto-encoder in the sense that it learns an encoder that learns a mapping from data to a latent representation, and a decoder from latent representation to data. However, the Variational Bayesian approach maps the data to a distribution over latent variables. This type of network can be efﬁciently trained with Stochastic Gradient Variational Bayes (SGVB), introduced last year at ICLR by Kingma & Welling (2013), and our resulting model has similarities to the Variational Auto-Encoder presented in their paper. Combining RNNs with SGVB is partly inspired by the work of Justin Bayer, the ﬁrst results of which were presented as workshop at NIPS 2014 (Bayer & Osendorfer, 2014). A VRAE allows to map time sequences to a latent representation and it enables efﬁcient, large scale unsupervised variational learning on time sequences. Also, a trained VRAE gives a sensible initialisation of weights and network state for a standard RNN. In general, the network states are initialised at zero, however Pascanu et al. (2013) have shown that the network state is a large factor in explaining the exploding gradients problem. Initializing a standard RNN with weights and a network state obtained from the VRAE will likely make training more efﬁcient and will possibly avoid the exploding gradients problem and enable better scores.  2 METHODS  2.1 SGVB  (cid:82) p(z)p(x|z)dz is intractable for these models and sampling based methods are too computationally  Stochastic Gradient Variational Bayes (SGVB) as independently developed by Kingma & Welling (2013) and Rezende et al. (2014) is a way to train models where it is assumed that the data is generated using some unobserved continuous random variable z. In general, the marginal likelihood expensive even for small datasets. SGVB solves this by approximating the true posterior p(z|x) by q(z|x) and then optimizing a lower bound on the log-likelihood. Similar to the nomenclature in Kingma’s paper, we call q(z|x) the encoder and p(x|z) the decoder.  1  Accepted as a workshop contribution at ICLR 2015  The log-likelihood of a datapoint i can be written as a sum of the lower bound and the KL divergence term between the true posterior p(z|x) and the approximation q(z|x), with θ the parameters of the model:  log p(x(i)) = DKL(q(z|x(i))||p(z|x(i))) + L(θ; x(i))  Since the KL divergence is non-negative, L(θ; x(i)) is a lower bound on the log-likelihood. This lower bound can be expressed as:  L(θ; x(i)) = −DKL(q(z|x(i))||p(z)) + E  q(z|x(i))[log pθ(x(i)|z)]  If we want to optimize this lower bound with gradient ascent, we need gradients with respect to all the parameters. Obtaining the gradients of the encoder is relatively straightforward, but obtaining the gradients of the decoder is not. In order to solve this Kingma & Welling (2013) introduced the ”reparametrization trick” in which they reparametrize the random variable z ∼ q(z|x) as a deterministic variable z = g((cid:15), x). In our model the latent variables are univariate Gaussians, so the reparametrization is z = µ + σ(cid:15) with (cid:15) ∼ N (0, 1). Modelling the latent variables in this way allows the KL divergence to be integrated analytically, resulting in the following estimator:  (1 + log((σ(i))2) − (µ(i)  j )2 − (σ(i)  j )2) +  log p(x(i)|z(i,l))  L(θ; x(i)) (cid:39) J(cid:88)  j=1  L(cid:88)  l=1  1 L  For more details refer to Kingma & Welling (2013). They also present an elaborate derivation of this estimator in their appendix.  2.2 MODEL  The encoder contains one set of recurrent connections such that the state ht+1 is calculated based on the previous state and on the data xt+1 of the corresponding time step. The distribution over Z is obtained from the last state of the RNN, hend, such that: encht + W T  ht+1 = tanh(W T  inxt+1 + benc)  µz = W T log(σz) = W T  µ hend + bµ σ hend + bσ  Where h0 is initialised as a zero vector. Using the reparametrization trick, z is sampled from this encoding and the initial state of the de- coding RNN is computed with one set of weights. Hereafter the state is once again updated as a traditional RNN:  h0 = tanh(W T ht+1 = tanh(W T xt = sigm(W T  z z + bz) decht + W T outht + bout)  x xt + bdec)  3 EXPERIMENTS  3.1 DATA AND PREPROCESSING  For our experiments we used 8 MIDI ﬁles (binary data with one dimension for each pitch) of well- known 80s and 90s video game songs1 sampled at 20Hz. Upon inspection, only 49 of the 88 dimen- sions contained a signiﬁcant amount of notes, so the other dimensions were removed. The songs are divided into short parts, where each part becomes one data point. In order to have an equal number of data points from each with song, only the ﬁrst 520 data points from each song were used.  1Tetris, Spongebob Theme Song, Super Mario, Mario Underworld, Mario Underwater, Mariokart 64 Choco  Mountain, Pokemon Center and Pokemon Surf  2  Accepted as a workshop contribution at ICLR 2015  3.2 TRAINING A MODEL  The choice of optimizer proved vital to make the VRAE learn a useful representation, especially adaptive gradients and momentum are important. In our experiments we used Adam, which is an optimizer inspired by RMSprop with included momentum and a correction factor for the zero bias, created by Kingma & Ba (2014). We trained a VRAE with a two-dimensional latent space and 500 hidden units on the dataset de- scribed in the last section. The songs were divided into non-overlapping sequences of 50 time steps each. Adam parameters used are β1 = 0.05 and β2 = 0.001. Due to instability, the learning rate was decreased gradually during learning. The initial learning rate was to 1 · 10−3 and the ﬁnal learning rate was 5 · 10−6. The resulting lower bound during training is shown in Figure 1.  Figure 1: On the left is the lower bound of the log-likelihood per datapoint per time step during training. The ﬁrst 10 epochs were cut off for scale reasons. On the right is the organisation of all data points in latent space. Each datapoint is encoded, and visualized at the location of the resulting two-dimensional mean µ of the encoding. ”Mario Underworld” (green triangles), ”Mario” (red triangles) and ”Mariokart” (blue triangles) occupy the most distinct regions.  With a model that has only two-dimensional latent space, it is possible to show the position of each data point in latent space. The data points are only a few seconds long and can therefore not capture all the characteristics of the song. Nevertheless, Figure 1 shows some clustering as certain songs occupy distinct regions in latent space. A two-dimensional latent space, however, is suboptimal for modelling the underlying distribution of the data. Therefore we also trained a model with twenty latent variables. For this model, we used sequences of 40 time steps with overlap, such that the start of each data point is halfway through the previous data point. This way the model not only learns the individual data points but also the transitions between them, which enables generating music of arbitrary length. As in training the ﬁrst model, Adam parameters used are β1 = 0.05 and β2 = 0.001. The learning rate was 2 · 10−5 and was adjusted to 1 · 10−5 after 1.6 · 104 epochs. The resulting lower bound is shown in Figure 2. Similar to 1, the organisation of the data in latent space using this model is shown in 2. In order to visualize the twenty-dimensional latent representations we used t-SNE (Van der Maaten & Hinton, 2008).  3.3 GENERATING DATA  Given a latent space vector, the decoding part of the trained models can be used for generating data. The ﬁrst model described in this chapter was trained on non-overlapping sequences of 50 time steps. Therefore, it can not be expected that generating longer sequences will yield data from the same distribution as the training data. However, since we know for each data point its latent representation in two dimensions and we can inspect their positions (see Figure 1) we use the model to interpolate between parts of different songs. The resulting music, which only lasts for a few seconds, clearly has elements of both parts. The model trained on overlapping data points was used to generate music of 1000 time steps (∼50 seconds) with various (20-dimensional) latent state vectors. It is possible to  3  Accepted as a workshop contribution at ICLR 2015  Figure 2: On the left is the lower bound of the log-likelihood per datapoint per time step during training. The ﬁrst 10 epochs were cut off for scale reasons. On the right is a visualization of the organisation of the encoded data in latent space. We calculated the 20-dimensional latent repre- sentation is calculated for each data point. The mean µ of this representation is visualized in two dimensions using t-SNE. Each color represents the data points from one song. It can be seen that for each song, the parts of that song occupy only a part of the space and the parts of some songs (e.g. ”mariounderworld”, in purple), are clearly grouped together. Of course, how much the parts of one song can be grouped together depends on the homogeneity of the song relative to the sim- ilarity between the different songs, as well as on how much spatial information is lost during the dimensionality reduction of t-SNE.  obtain latent vectors by encoding a data point, or to sample randomly from latent space. Doing this creates what one might call a ”medley” of the songs used for training. A generated sample from a randomly chosen point in latent space is available on YouTube 2.  4 DISCUSSION  We have shown that it is possible to train RNNs with SGVB for effective modeling of time se- quences. An important difference with earlier, similar approaches is that our model maps time sequences to one latent vector, as opposed to latent state sequences. A ﬁrst possible improvement over the current model is dividing each song into as many data points as possible for training (i.e. one datapoint starting at each time step) instead of data points that only have 50% overlap. Another improvement is to reverse the order of the input, such that the the ﬁrst time steps are more strongly related to the latent space than the last time steps. This will likely improve upon the length of the time dependency that can be captured, which was around 100 time steps with our current approach. Another way to train on longer time sequences is to incorporate the LSTM framework (Hochreiter & Schmidhuber, 1997). Direct applications of our approach include recognition, denoising and feature extraction. The model can be combined with other (supervised or unsupervised) models for sequential data, for example to improve on current music genre tagging methods, e.g. Sigtia et al. (2014). In addition, this method could complement current methods for supervised training of RNNs by providing initial hidden states.  2http://youtu.be/cu1_uJ9qkHA  4  Accepted as a workshop contribution at ICLR 2015  REFERENCES Bayer, Justin and Osendorfer, Christian. Learning stochastic recurrent networks.  Workshop on Advances in Variational Inference, 2014.  In NIPS 2014  Boulanger-Lewandowski, Nicolas, Bengio, Yoshua, and Vincent, Pascal. Modeling temporal de- pendencies in high-dimensional sequences: Application to polyphonic music generation and tran- scription. In The 29th International Conference on Machine Learning (ICML), 2012.  Cho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using rnn encoder-decoder for statisti- cal machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.  Graves, Alex, Liwicki, Marcus, Fern´andez, Santiago, Bertolami, Roman, Bunke, Horst, and Schmidhuber, J¨urgen. A novel connectionist system for unconstrained handwriting recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(5):855–868, 2009.  Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computation, 9(8):  1735–1780, 1997.  Kingma, Diederik P and Ba, Jimmy. Adam: A method for stochastic optimization. ArXiv preprint  arXiv:1412.6980, 2014.  Kingma, Diederik P and Welling, Max. Auto-encoding variational bayes. In The 2nd International  Conference on Learning Representations (ICLR), 2013.  Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the difﬁculty of training recurrent In Proceedings of the 30th International Conference on Machine Learning  neural networks. (ICML), 2013.  Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31th International Con- ference on Machine Learning, (ICML), 2014.  Sigtia, Siddharth, Benetos, Emmanouil, Cherla, Srikanth, Weyde, Tillman, Garcez, Artur S dAvila, and Dixon, Simon. An rnn-based music language model for improving automatic music tran- scription. In International Society for Music Information Retrieval Conference (ISMIR), 2014.  Van der Maaten, Laurens and Hinton, Geoffrey. Visualizing data using t-sne. Journal of Machine  Learning Research, 9(2579-2605):85, 2008.  5  ",
1412.7155,2015, Learning Compact Convolutional Neural Networks with Nested Dropout,"['Learning Compact Convolutional Neural Networks with Nested Dropout', 'Chelsea Finn', 'Lisa Anne Hendricks', 'and Trevor Darrell']",https://arxiv.org/pdf/1412.7155,"5 1 0 2    r p A 0 1         ]  V C . s c [      4 v 5 5 1 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  LEARNING COMPACT CONVOLUTIONAL NEURAL NETWORKS WITH NESTED DROPOUT  Chelsea Finn, Lisa Anne Hendricks & Trevor Darrell Department of Computer Science UC Berkeley Berkeley, CA 94704, USA {cbfinn, lisa anne, trevor}@eecs.berkeley.edu  ABSTRACT  Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing recon- struction cost (Rippel et al., 2014). However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.  1 MOTIVATION  Supervised convolutional neural networks (CNNs) learn representations that are effective on a wide range of tasks. A drawback of current such approaches, however, is that the selection of such architectures is largely optimized by hand, with researchers explicitly searching architecture hyper- parameters via cross-validation. Model selection for network architectures has been explored in the context of learning network connectivity dating back to Optimal Brain Damage from LeCun et al. (1989) and has been continued to be explored in the context of learning optimal sparse models. To our knowledge there is no deep visual network capable of increasing its representation capacity based on the complexity of available data or tasks. The recently proposed nested dropout method implicitly accomplishes this, by learning deep representation units in an incremental fashion. We in- vestigate whether such an approach is applicable to visual CNN models, and propose a visual CNN model which can learn to scale its capacity according to the complexity of the data presented to the network during training. In standard dropout, units in the layer are independently dropped out with probability p, namely the output of that unit is set to zero. This is traditionally applied to convolutional and fully-connected layers during training time, and has been shown to act as a regularizer, discouraging over-ﬁtting to the training data (Hinton et al., 2012), though it has also been applied to entire channels of a convolutional layer output by Tompson et al. (2014). Empirically, when training large networks, such as those trained on ImageNet, drop out is necessary to avoid over ﬁtting (Krizhevsky et al., 2012). Nested dropout, on the other hand, randomly draws unit indices from a geometric distribution and drops out all of the units that follow the number drawn, e.g. if a number k is drawn, then the units 0 through k are kept and the remaining units are dropped. When applied to a single layer semi-linear autoencoder, this technique has been proven to enforce an ordering of the units by their information capacity, while not decreasing the ﬂexibility of the representation nor the quality of the resulting solution (Rippel et al., 2014). The primary contributions of this paper are to (1) demonstrate that nested dropout can successfully be applied to convolutional layers trained by back-propagation, (2) propose nested dropout as an advantageous method to learn CNNs that adapt to task and data complexity in a deep learning set- ting, and (3) provide our implementation in Caffe (Jia et al., 2014), a widely used deep learning framework, upon publication.  1  Accepted as a workshop contribution at ICLR 2015  2 NESTED DROPOUT ON CNNS  The nested dropout algorithm for a convolutional layer with n channels is as follows: for each sample in a mini-batch, we draw a number k from a geometric distribution and drop out the latter n − k channels of the output of the layer. Nested dropout can also be applied to multiple layers in a network by applying nested dropout to each layer iteratively. First, the number of ﬁlters ni for layer i is determined through nested dropout. After ﬁxing the number of ﬁlters ni in layer i, nested dropout can then be used to determine the number of ﬁlters, ni+1, for layer i + 1. We trained our CNNs using Stochastic Gradient Descent (SGD) and mini-batches of 100 samples. Because dropped out units are determined by drawing from a geometric distribution, units with a low index are rarely dropped out and thus converge quickly, whereas latter units are frequently dropout out and thus learned very slowly. Filters are incrementally ﬁxed once they have converged, and only the remaining ﬁlters are considered when drawing numbers from the geometric distribution. Though incrementing the sweeping index could be done upon ﬁlter convergence, we achieved satisfactory results by simply incrementing the unit sweeping index after a set number of iterations. To implement this in the Caffe framework, we added a nested dropout layer that can follow any layer (e.g. convolutional, fully-connected) in the same way as the standard dropout layer. We also added customizations to the solver to support unit sweeping during training.  3 EXPERIMENTS  Figure 1: Accuracy as a function of the number of ﬁlters in conv1, with and without nested dropout. The oracle consists of 32 separate networks, whereas the nested dropout curve only requires training a single network. The “brain damaged” baseline is trained without nested dropout and only the k best ﬁlters are used during test. The nested dropout network achieves maximum accuracy with a compact representation of 23 ﬁlters while requiring considerably fewer iterations than a brute force approach.  We apply nested dropout to the ﬁrst convolutional layer of a CNN trained to classify images in the CIFAR-10 dataset, using the default Caffe architecture and training with a ﬁxed learning rate. In Figure 1, we show the test accuracy of a single network trained with nested dropout as a function  2  Accepted as a workshop contribution at ICLR 2015  of the number of conv1 ﬁlters. We report the accuracy with k ﬁlters by using the partially-trained network after the kth ﬁlter has converged, and testing it with the last n − k ﬁlters dropped out. We compare to two naive approaches to select the number of ﬁlters in conv1. The ﬁrst approach simply trains 32 separate networks which differ in the number of conv1 ﬁlters. The second approach trains a single network with 32 ﬁlters, but at test time, a varying number of conv1 ﬁlters are used with the remaining dropped out. Note that because there is no inherent ordering to the learned representation without nested dropout, removing a ﬁlter severely damages the network resulting in low accuracies. Our experiments in Figure 1 demonstrate that nested dropout efﬁciently determines the relationship between model capacity and test accuracy. The nested dropout implementation requires 90,000 iterations of training, whereas the brute force approach requires signiﬁcantly more, up to millions iterations. For example, training 32 separate networks to completion requires 2,880,000 iterations.  (a)  (b)  Figure 2: conv1 ﬁlters of networks trained with nested dropout (a) and without (b). Note that the latter ﬁlters of (a) carry very little information, yet both networks achieve similar performance, giving an indication for how many units are necessary for this model.  In Figure 2, we visualize the ﬁlters learned with and without nested dropout. Though the latter ﬁlters carry very little information, the test accuracy of the two sets of ﬁlters are comparable with 0.787 test accuracy for the network trained with nested dropout and 0.786 test accuracy for the baseline network. Applying nested dropout to the second convolutional layer yielded similar results to our experiments with conv1. After training a network with nested dropout applied to conv1 ﬁlters, we determined only 23 conv1 ﬁlters are necessary to achieve the maximum classiﬁcation accuracy for this network. We next train a network in which nested dropout follows conv2 and with a unit sweeping index of 5,000. After learning 25 conv2 ﬁlters, the training accuracy converges to 78%. Thus, by using nested dropout we can reduce the total number of parameters in the ﬁrst two layers by 25% from 64 ﬁlters to 48, while maintaining similar accuracy.  4 DISCUSSION  In summary, we have provided a simple method for determining a more compact representation of the convolutional layers. In our experiments, we learned a representation that achieved the same classiﬁcation accuracy using 23 conv1 ﬁlters and 25 conv2 ﬁlters rather than the baseline 32 each, within the same optimization framework. A main advantage of our method is that it enables the network to gradually increase network capacity during training. Additionally, we hope that, in the future, ordering parameters may provide insights into optimization of deep convolutional neural networks and how the network architecture impacts performance.  3  Accepted as a workshop contribution at ICLR 2015  ACKNOWLEDGMENTS  This work was supported in part by DARPA’s MSEE and SMISC programs, NSF awards IIS- 1427425, IIS-1212798, and IIS-1116411, Toyota, and the Berkeley Vision and Learning Center. Chelsea Finn was supported by a Berkeley EECS Fellowship and Lisa Anne Hendricks by an ND- SEG Fellowship.  REFERENCES Hinton, Geoffrey E., Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Improving neural networks by preventing co-adaptation of feature detectors. CoRR,  Ruslan. abs/1207.0580, 2012. URL http://arxiv.org/abs/1207.0580.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- In Proceedings of the ACM International Conference on Multimedia, pp. 675–678. bedding. ACM, 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  LeCun, Yann, Denker, John S, Solla, Sara A, Howard, Richard E, and Jackel, Lawrence D. Optimal  brain damage. In NIPs, volume 2, pp. 598–605, 1989.  Rippel, Oren, Gelbart, Michael A, and Adams, Ryan P. Learning ordered representations with nested  dropout. arXiv preprint arXiv:1402.0915, 2014.  Tompson, Jonathan, Goroshin, Ross, Jain, Arjun, LeCun, Yann, and Bregler, Christoph. Efﬁcient object localization using convolutional networks. CoRR, abs/1411.4280, 2014. URL http: //arxiv.org/abs/1411.4280.  4  ",
1412.3708,2015, Compact Part-Based Image Representations: Extremal Competition and Overgeneralization,"['Compact Part-Based Image Representations: Extremal Competition and Overgeneralization', 'Marc Goessling and Yali Amit']",https://arxiv.org/pdf/1412.3708,"6 1 0 2    t c O 9 2         ]  V C . s c [      4 v 8 0 7 3  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  COMPACT COMPOSITIONAL MODELS  Marc Goessling Department of Statistics University of Chicago Chicago, IL 60637, USA goessling@galton.uchicago.edu  Yali Amit Departments of Statistics and Computer Science University of Chicago Chicago, IL 60637, USA amit@galton.uchicago.edu  ABSTRACT  Learning compact and interpretable representations is a very natural task, which has not been solved satisfactorily even for simple binary datasets. In this paper, we review various ways of composing experts for binary data and argue that com- petitive forms of interaction are best suited to learn low-dimensional representa- tions. We propose a new composition rule that discourages experts from focusing on similar structures and that penalizes opposing votes strongly so that abstain- ing from voting becomes more attractive. We also introduce a novel sequential initialization procedure, which is based on a process of oversimpliﬁcation and correction. Experiments show that with our approach very intuitive models can be learned.  1  INTRODUCTION  In recent years, multi-layer network architectures have drastically improved the discriminative per- formance in several classiﬁcation tasks. These deep networks can be constructed based on desired invariances and stability properties (e.g., Bruna & Mallat, 2013) or they can be learned from usually large datasets as a cascade of nonlinear functions (e.g., Lee et al., 2009a). While being effective for classiﬁcation the used transformations are typically high-dimensional and individual features are not always semantically meaningful. Moreover, the described structures are sometimes more global than desired and often there are many features that describe similar structures. As pointed out by Bengio et al. (2013) learning to disentangle the factors of variation remains a key challenge in deep networks. Even for very simple classes, the basic task of learning a compact and interpretable representation has not yet been solved in a satisfactory manner. For example, the most natural rep- resentation of the letter T is in terms of a vertical and a horizontal bar. Consequently, this class can efﬁciently be represented by six coordinates, corresponding to the row/column location and the orientation of the bars. In this work, we learn robust representations by seeking a parsimonious set of experts corresponding to the largest stable structures in the data. Apart from being intuitively appealing, low-dimensional explicit representations are, for example, useful for obtaining coarse scene descriptions in computer vision. We emphasize that our focus is not on achieving state-of-the-art performance in terms of classiﬁcation rates or likelihoods but rather on learning simple models from few examples. In Sec- tion 2 we review various ways of composing experts for binary data and discuss their impact on the resulting representation. A new composition rule is then introduced that is particularly well suited for learning compact representations. The rule causes extremal competition among the experts over the data dimensions. In particular, the rule ensures that identical experts cannot be used to improve likelihoods and that opposing expert opinions always lead to a reduced likelihood. In Section 3 we present an appropriate sequential inference procedure for the type of compositional models that we consider. In Section 4 we describe a batch as well as online version of an EM-style learning proce-  1  Accepted as a workshop contribution at ICLR 2015  dure for the new composition rule. Moreover, we propose a sequential initialization method, which can be described as a process of oversimpliﬁcation and correction. Results for a synthetic dataset and handwritten letters are presented in Section 5.  2 COMPOSITION RULES We model binary data x ∈ {0, 1}D through a product Bernoulli distribution P(x| µ), i.e., the variables x(d) are assumed to be conditionally independent given µ. The global template µ is a composition of experts µ1, . . . , µK, where each expert is a Bernoulli template µk ∈ [0, 1]D. The way in which the experts are combined in order to create the composed template is speciﬁed through a composition rule (also known as patchwork operation, mixing function or voting scheme). Such rules are the generative counterpart of activation functions in feed-forward neural networks. For- mally, a composition rule is a function γ : [0, 1]K → [0, 1] with a varying number K of arguments. The composed template µ is obtained by applying the composition rule to the expert “opinions” for each dimension  µ(d) = γ(µ1(d), . . . , µK(d)).  Of special interest is the ability of an expert to abstain from voting. By that we mean there exists a value q ∈ [0, 1] such that  γ(q, p1, . . . , pK) = γ(p1, . . . , pK)  for all p1, . . . , pK ∈ [0, 1] and all K. In the following we consider two classes of models. The ﬁrst class is a natural choice for binary images and is referred to as write-black models (Saund, 1995). In these models the default variable state is “off” (white pixel). Underlying factors are now able to turn variables “on” (black pixel). If multiple causes for a variable are present the state will still be “on”. This type of model is appropriate, for example, for ﬁgure-ground segmentations (white corresponding to background and black corresponding to foreground) where experts describe object parts. Composition rules for write-black models encode “no vote” through µk(d) = 0 and we refer to them as asymmetric rules. Note that the value 0 is used for regions far away from the expert support. A template probability µk(d) = 1/2 on the other hand is used for variables that are close to the boundary of the support and can hence be interpreted as “not sure”. In write-black models, samples from individual experts will look like actual object parts. The second class we consider are write-white-and-black models (Saund, 1995). In such models experts are able to cast votes in favor of “on” as well as “off”. Composition rules for write-white- and-black models encode abstention through µk(d) = 1/2 and we refer to them as symmetric rules. Note that “not sure” is also encoded through µk(d) = 1/2, so this value can have either of the two meanings. Samples from single experts for image data will not look like actual object parts because about half of the pixels in the background region will be turned on.  2.1 ASYMMETRIC RULES  2.1.1 NOISY OR  A straightforward composition rule for write-black models is the disjunctive composition  γ(p1, . . . , pK) = 1 − K(cid:89)  (1 − pk).  The composed probability is simply the probability of observing at least one success when drawing independently from Bernoulli distributions with probabilities p1, . . . , pK. This rule was used by Saund (1995).  k=1  2.1.2 SUM OF ODDS  It was argued by Dayan & Zemel (1995) that the noisy-or rule offers little incentive for experts to focus on different structures in the data. A more competitive composition rule was therefore  2  Accepted as a workshop contribution at ICLR 2015  Figure 1: Top: Comparison of composition rules, plotted as functions of p2 for p1 = 0.7. Bottom: Compositions of two experts (the probabilities in the ﬁrst template are 0.5 and 0.7, the probabilities in the second template are 0.7 and 0.01) using different rules.  proposed, which is of the form  γ(p1, . . . , pK) = 1 −  (cid:32)  1 +  K(cid:88)  k=1  pk 1 − pk  (cid:33)−1  .  This rule can be motivated as the probability of observing a success when drawing independently from Bernoulli distributions with probabilities pk conditioned on observing at most one success. It is easy to see that the composed odds are just the sum of the individual odds. While in a global mixture model exactly one component is responsible for generating the whole data vector, here exactly one expert is responsible for each dimension that is turned “on”. So, in contrast to the noisy-or rule the responsibility for individual variables is not shared.  2.1.3 MAXIMUM  The most extreme form of competition is achieved through the max rule  γ(p1, . . . , pK) = max  pk.  k=1,...,K  Such a rule was used by L¨ucke & Sahani (2008). Since only the strongest template matters, experts have no incentive to represent structures that are already present, unless their opinion is the most extreme one. Consequently, experts tend to focus on different aspects of the data. In contrast to the noisy-or and the sum-of-odds rule, likelihoods cannot be improved by using the same expert multiple times. Other than for the sum-of-odds rule, with this composition rule for each variable x(d) it is known which expert is responsible. This fact makes it possible to use an analytic formula in the M-step of the EM learning procedures for such models. The left panel in Figure 1 visualizes the different asymmetric composition rules for two experts. We see that the max rule is ﬂat at 0 and hence inhibits the experts from leaving the “no vote” state.  3  Accepted as a workshop contribution at ICLR 2015  2.2 SYMMETRIC RULES  2.2.1 ARITHMETIC MEAN  An intuitive composition rule for write-white-and-black models is the simple average  γ(p1, . . . , pK) =  1 K  pk.  K(cid:88)  k=1  The composition can be interpreted as an equal mixture of the individual expert templates. This rule was used by Amit & Trouv´e (2007). Note however that with this composition rule it is impossible for an expert to abstain from voting. Consequently, the support of the experts has to be restricted manually.  2.2.2 SUM OF LOG-ODDS  An often used composition rule is the sum of log-odds  (cid:32) K(cid:88)  k=1  (cid:33)  ,  γ(p1, . . . , pK) = σ  log  pk(x) 1 − pk(x)  where σ(t) = (1 + e−t)−1 is the logistic function (the inverse of the logit function). This type of composition is used in restricted Boltzmann machines (Hinton, 2002) and sigmoid belief networks (Neal, 1992). A sum followed by the logistic link function is also used in generalized linear models for binary data. This includes logistic PCA (Schein et al., 2003), latent trait models (Bartholomew et al., 2011), exponential family sparse coding (Lee et al., 2009b) and binary matrix factorization (Meeds et al., 2006). Abstention is expressed through log-odds of 0, i.e., probabilities of 1/2. Since the ﬁrst step is to sum up the individual votes (casted in terms of log-odds), opinions of experts voting in the opposite direction can be completely canceled out. Indeed, as seen in Figure 1, even if p1 = 0.7 the composed probability approaches 0 as p2 approaches 0. As a result, there is little pressure to abstain from voting. At the same time, two very similar experts may complement each other without reducing the likelihood compared to a single strong expert.  2.2.3 NORMALIZED SUM  A more competitive composition rule was proposed by Saund (1995), which for active disagreement results in a net uncertainty. For pk ∈ {0, 1/2, 1} the rule is  (cid:32)(cid:80)K (cid:80)K k=1(pk − 1 2 ) 2| + 1 k=1 |pk − 1  (cid:33)  γ(p1, . . . , pK) =  1 2  and linear interpolation is used for values in between. However, due to the computational cost for linear interpolations in K dimensions a more tractable approximation was actually used for the experiments in that work.  2.2.4 MAXIMUM MINUS MINIMUM  We propose a new symmetric composition rule, which reduces redundancy among experts and in- centivizes abstention at the same time. Analogously to the max composition, we would like to rule out the possibility to increase the likelihood just by using the same expert multiple times. This can be achieved by using only the most extreme opinion. On the other hand, similarly to the normalized sum, opposing opinions should result in a limitation of the maximum achievable likelihood. These considerations naturally lead to the max-minus-min rule pk − q  γ(p1, . . . , pK) = q +  pk − q  (cid:18)  (cid:19)  (cid:18)  (cid:19)  max  min  −  .  k=1,...,K  +  k=1,...,K  −  The subscript +/− denotes the positive/negative part of a real number. We set q = 1/2, but other values could also be used. To our knowledge, this composition rule has not been used before.  4  Accepted as a workshop contribution at ICLR 2015  Figure 2: Log-likelihood functions for a two-expert model with sum-of-log-odds composition (left) and max-minus-min composition (right).  A comparison of the symmetric composition rules is shown in the right panel of Figure 1. To illustrate the difference between the sum of log-odds composition and the max-minus-min compo- sition we compute the corresponding log-likelihood functions in a simple example. Consider the ground-truth model, which creates completely white images with probability 1/4, completely black images with probability 1/4 and random images (in which each pixel is drawn independently from a Bernoulli-1/2 distribution) with probability 1/2. We attempt to learn this model by training two experts, which can be combined using the sum-of-log-odds or the max-minus-min composition rule, respectively. For simplicity we reduce the expert templates to a single parameter µk(x) = pk, k = 1, 2, i.e., each pixel has the same chance of being turned on. We show the resulting (expected) log-likelihood functions in Figure 2 in the limit of a large image resolution. There are two equivalent global maxima, one at (1, 0) and one at (0, 1), each of them corresponds to one expert for black im- ages and one expert for white images. If the initial parameters in the sum-of-log-odds model are on the same side of 1/2 (top-right or bottom-left quadrant) gradient descent will change both parameters in the same direction (i.e., it will increase both values or it will decrease both values). This partially explains why restricted Boltzmann machines (Hinton, 2002) with randomly initialized weights have a tendency to yield multiple similar experts. The situation is very different for the max-minus-min composition. If both parameters are on the same side of 1/2 then moving along the gradient direc- tion would only change the more extreme value while the other expert would remain close to the “no vote” state (and would hence be “available” in later stages of the learning process). Note however that we are not using gradient descent to train max-minus-min models but rather employ the EM algorithm (see Section 4). The example also suggests that it could be beneﬁcial to use a sequential initialization procedure in which additional experts take care of structures that cannot be explained by the existing ones.  3 SEQUENTIAL INFERENCE  Given experts µk the inference task is to determine the posterior distribution on expert conﬁgura- tions given the observation x, or at least ﬁnding the expert conﬁguration that is most likely to have generated the data. For our purposes it is important that the inference procedure yields compact representations. We mention a few common approaches before we present a sequential procedure that we call likelihood matching pursuit. In restricted Boltzmann machines (Hinton, 2002) experts are evaluated independently. This is computationally efﬁcient but activates all experts that match the data sufﬁciently well, rather than providing the sparsest possible activation that can explain the data. In other works, the indicator variables for expert presence are relaxed to real values in [0, 1]. For example, Dayan & Zemel (1995) and Vincent et al. (2008) use simple mean-ﬁeld approximations. Saund (1995) uses gradient descent starting from a point with all coordinates equal to 1/2. This it-  5  Accepted as a workshop contribution at ICLR 2015  Figure 3: 1st panel: Scene to be analyzed. 2nd panel: Noisy version. 3rd panel: Resolved scene using robustiﬁed templates (the digits are detected in the order red, green, blue, yellow, magenta). 4th panel: First detected digit using the original templates.  erative procedure becomes inefﬁcient however if the number of possible experts is much larger than the typical number of active experts. L¨ucke & Sahani (2008) use a truncated search, which evaluates all expert conﬁgurations with a small number of active components. We propose a simple sequential inference procedure for max and max-minus-min compositions. The data is ﬁrst explained by the expert µ = µk1 that yields the highest likelihood for the observation x. Additional experts µk are then evaluated by forming the composition (according to the chosen rule) of the current global template µ with the candidate experts and computing the new likelihood. The best expert is added and the global template is updated accordingly. The procedure ends when the likelihood cannot be improved anymore. This yields a sparse activation in a single sequential pass because experts are only added if they are able to explain structures that have not been explained before. Note that this procedure is quite similar to matching pursuit (Mallat & Zhang, 1993) for sparse coding. Instead of minimizing the squared error, we maximize the Bernoulli likelihood. This sequential ﬁtting works well for write-white-and-black models (using the max-minus-min rule). However, since write-black models (using the max rule) encode abstention through a probability of 0 rather than 1/2, the value of P (x| µk) will heavily depend on which structures other than the one described by µk are present in the observation x. Consequently, it is easy to run into situations where  P (x| µk1 ) > P (x| µk(cid:48)  )  1  but  Hence, the choice of the ﬁrst expert may lead to a poor local maximum. The problem is that the ﬁrst expert tries to explain the entire data vector. As a simple ﬁx we propose to use truncated templates  P (x| γ(µk1 , µk2 , . . . , µkJ )) (cid:28) P (x| γ(µk(cid:48) (cid:19)  1  (cid:18) 1  ˜µk(d) = max  , µk(d)  2  , µk2 , . . . , µkJ )).  instead. This eliminates the impact of the data in the background region of the expert. Indeed, P (x| ˜µk) only depends on variables x(d) for those d that satisfy µk(d) > 1/2. That means the likelihood of the truncated template only depends on the data in the support of expert k. A similar idea to robustify templates was used by Williams & Titsias (2004). They mix the original templates with a uniform distribution. In the case of binary data this amounts to αµ(d) + (1 − α)/2 for some α ∈ (0, 1), i.e., a convex combination between µ and 1/2 is formed. For a write-black model this is a less effective transformation than our truncation. We illustrate the effectiveness of the proposed modiﬁcation through a simple scene analysis example. Five digits are placed at random locations in the image according to a write-black model and the task is to recover the identity and locations of the digits in a noisy version of this scene. Using the robustiﬁed templates the scene can be resolved perfectly, as shown in Figure 3. However, with the original templates the ﬁrst digit, which is placed down, corresponds to the largest structure in the image that can be explained by a single template through a moderately well ﬁt. Note that the suggested ﬁx is not the only possible solution. If we allow for an iterative procedure then the scene can also be resolved with the original templates. The modiﬁcation is simply meant as a robustiﬁcation of the greedy inference procedure.  6  Accepted as a workshop contribution at ICLR 2015  4 EM LEARNING  We now describe an approximate EM procedure (Dempster et al., 1977) for learning max-minus- min models. When working with image data, we explicitly model transformations like shifts and rotations. Each template µk then provides multiple transformed versions µkt = Φt(µk) where t denotes the transformation. This allows us to share parameters among all transformed versions. Since our templates will describe rather large structures, we assume that only one of the trans- formed versions is present in each image. In the (hard) E-step we use the likelihood matching pursuit procedure from the previous section, which yields for each observation x a representation (k1, t1), . . . , (kJ , tJ ). We deﬁne  k(cid:63)(d) = argmax j=1,...,J  µkj tj (d),  (cid:96)(cid:63)(d) = argmin j=1,...,J  µkj tj (d)  provided that the maximum is larger than q and the minimum is smaller than q, respectively (other- wise we leave k(cid:63)(d) or (cid:96)(cid:63)(d) undeﬁned). In words, k(cid:63)(d) and l(cid:63)(d) are the active experts with the most extreme opinions for variable x(d). In the M-step we then simply compute  1{k=k(cid:63)  (cid:80)N  n=1  n(d)} Φ−1  tnk  n(d) or k=(cid:96)(cid:63) 1{k=k(cid:63)  n(d) or k=(cid:96)(cid:63)  n(d)} + 2(cid:15)  (xn)(d) + (cid:15)  (1)  (cid:80)N  µk(d) =  n=1  n, l(cid:63)  where k(cid:63) n are the maximizers respectively minimizers and tnk is the transformation correspond- ing to k-th expert in the representation of the n-th training example xn. The pseudocount (cid:15) > 0 is added for regularization purposes and can be interpreted as a Beta((cid:15), (cid:15)) prior on the expert probabili- ties. In all our experiments we use (cid:15) = 1, which corresponds to a uniform prior. The update formula (1) is not the exact maximizer for the expert templates but a very good heuristic. The reason why we can use such a simple analytic expression is that with extremal composition rules the responsibility for individual variables x(d) is not split among many experts. With other composition rules gradi- ent descent methods are often required in order to perform the M-step. Also note that for q = 0 the max-minus-min model reduces to the max model. In that special case the update formula (1) only involves k(cid:63) but not (cid:96)(cid:63).  4.1 ONLINE LEARNING AND SEQUENTIAL INITIALIZATION  It is straightforward to provide an online version of the learning procedure. The M-step updates simply are  µk(d) ← Nk(d)µk(d) + 1{k=k(cid:63)  Nk(d) + 1{k=k(cid:63)  n(d) or k=(cid:96)(cid:63)  n(d) or k=(cid:96)(cid:63)  (xn)(d)  ,  Nk(d) ← Nk(d) + 1{k=k(cid:63)  n(d) or k=(cid:96)(cid:63)  tnk  n(d)} Φ−1 n(d)} n(d)}.  The variable Nk(d) counts how many training examples have been used to compute the current estimate for the d-th dimension of expert k. The online version is attractive because it allows us to use a sequential initialization scheme. Since the learning problem is non-convex, it is crucial to have a good initialization. In accordance with our attempt to learn a parsimonious representation we start off with a single global template derived from the ﬁrst training example and add more experts later on. The idea is to use “oversimpliﬁed” models in the sense that they try to explain new examples through the experts learned so far. The models are then “corrected” by appending additional templates to the collection of experts. Deﬁne ˜µK+1(d) = (xn(d) + (cid:15))/(1 + 2(cid:15)). When using the max-minus-min composition the additional template can be initialized as  µK+1(d) =  2 ˜µK+1(d) otherwise  if P(xn(d)| µ(d)) ≥ P(xn(d)| ˜µK+1(d))  (cid:26) 1  where µ is the composed template using the existing experts. This means that the new expert abstains from voting for dimensions that are already well explained by the other experts. For max compositions we suggest to use  µK+1(d) =  2 ˜µK+1(d) otherwise  if P(xn(d)| µ(d)) ≥ 1  2  .  (cid:26) 1  7  Accepted as a workshop contribution at ICLR 2015  This makes sure that in the background region µK+1 is close to 0 (rather than equal to 1/2). Our successive reﬁnement is in stark contrast to the typical bottom-up grouping of local structures in part-based compositional models. For example, Fidler & Leonardis (2007) and Zhu et al. (2008) start with elementary edge features and combine simple structures into more complex ones through a hierarchical clustering procedure.  4.2 GEOMETRIC COMPONENT  For image data, the spatial arrangement of the experts can be modeled through a joint Gaussian distribution for shifts and rotations. This only requires us to compute the mean and covariance of the training conﬁgurations provided by the inference procedure. As emphasized by Bruna & Mallat (2013), a very desirable property of a representation is that intraclass deformations are linearized. As the experiment in Section 5.2 conﬁrms, our representation transforms the complex deformation orbit (in the original space) into a linear space in which a Gaussian distribution satisfactorily describes the deformations.  5 EXPERIMENTS  5.1 A SYNTHETIC WRITE-WHITE-AND-BLACK MODEL  We compare the max-minus-min model to denoising autoencoders (Vincent et al., 2008) and re- stricted Boltzmann machines (Hinton, 2002) on a synthetic model for binary images of size 6 × 6 pixels. The image grid is divided into a regular grid of four quadrants. Each quadrant is inde- pendently activated with probability 1/2. An activated quadrant is either entirely black or entirely white, each with probability 1/2. For non-activated quadrants the pixels are drawn independently from a Bernoulli-1/2 distribution (samples from this model can be found in Figure 4). The task is to recover the 8 underlying experts corresponding to the four quadrants with two polarities. The learning algorithm for the max-minus-min model typically converged after a few iterations. In the left panel of Figure 5 we visualize the obtained templates after 1, 2 and 5 iterations when using 100 training examples. As we see, our model is able to almost perfectly recover the original experts. The denoising autoencoder and the restricted Boltzmann machine were run for 100 epochs and the learning rates were tuned. For the denoising autoencoder we also tuned the corruption level. Fig- ure 6 shows a comparison with the max-minus-min model in terms of the cross-entropy (i.e., the negative log-likelihood of the composed templates) on a test set of 1,000 samples. The reconstruc- tion error for the max-minus-min model is much lower. This is because with the sum-of-log-odds composition multiple ground-truth experts are mixed up. The right panel of Figure 5 shows that the templates are indeed combinations of multiple ground-truth experts, i.e., the factors of varia- tion have not been disentangled successfully. In order to make a fair comparison we have left out the sequential initialization procedure (as well as the transformation modeling) and initialized all models with completely random templates. That means the performance gain is in fact due to the more competitive expert interaction. As a further comparison we learned a sparse coding dictio- nary (Mairal et al., 2009). Note that the data is then treated as real-valued and the task is to ﬁnd basis vectors whose linear combinations allow for good reconstructions in a squared error sense. The basis vectors learned from 100 samples are visualized in the right panel of Figure 5. Again, the ground-truth experts are not recovered. However, the basis vectors look more structured than the experts learned by denoising autoencoders or restricted Boltzmann machines. In terms of L2 reconstruction error this dictionary is actually even better than the ground-truth generative model. Figure 4 visualizes the reconstructions obtained via sparse coding and via the learned max-minus- min model. The sparse coding reconstruction is visually closer to the data but much more noisy than the reconstruction of our model. Indeed, the max-minus-min reconstruction is almost identical to the ground-truth templates. The reason for this different behavior is that the squared error is more forgiving for small deviations compared to cross-entropy and penalizes harder than cross-entropy if the deviation is large (around 1/2).  5.2 HANDWRITTEN LETTERS  We now train experts on the letter classes from the TiCC handwritten characters dataset (van der Maaten, 2009) while modeling shifts and rotations. The images are of size 56 × 56 pixels and each  8  Accepted as a workshop contribution at ICLR 2015  Figure 4: a) Sparse coding reconstruction. b) 100 samples from the synthetic model. c) Ground-truth templates for the samples above. d) Reconstruction of the max-minus-min model.  Figure 5: Left: Initialization (1st row) and learned experts after 1, 2 and 5 EM iterations (2nd - 4th row) for the max-minus-min model trained on 100 examples from the synthetic model. Blue, gray and yellow color corresponds to probabilities of 0, 1/2 and 1, respectively. Right: Experts obtained from a denoising autoencoder (top), a restricted Boltzmann machine (center) and dictionary learning for sparse coding (bottom) using 100 samples.  9  Accepted as a workshop contribution at ICLR 2015  Figure 6: Cross-entropy reconstruction error for different models and training sizes (lower is better). The dashed black line is the cross-entropy of the ground-truth model.  Figure 7: Online learning for the letter T. Left panel: The 10 samples used for training (1st row) and the two expert templates (2nd & 3rd row) at step i = 1, . . . , 10. Right panel: 9 sampled expert conﬁgurations based on a multivariate Gaussian distribution of the spatial expert arrangement using the ﬁnal templates.  sample provides a label for the writer of the corresponding character. Since “on” pixels in this dataset correspond to black ink it is natural to use the max composition rule. The purpose of this experiment is to illustrate the effectiveness of our sequential initialization procedure from Section 4.1. The left panel in Figure 7 visualizes the online learning process for the letter T, using the ﬁrst sample of the ﬁrst 10 writers. The ﬁrst expert is initialized by the ﬁrst training example. The second expert is initialized by the characteristics of the second example that cannot be explained through the ﬁrst expert. Every additional image then updates both experts. After 10 examples the learning process has converged to a vertical and a horizontal bar. Samples (Figure 7, right panel) from the learned spatial distribution look realistic and cover the principal deformations of the class. We also learned up to four experts for the other1 letters, using the ﬁrst sample of the ﬁrst 20 writers. The results are shown in Figure 8. Most experts correspond to natural elements of the character class. Note that in contrast to Lake et al. (2013) no motion information was necessary to learn these character primitives.  6 CONCLUSION AND FUTURE WORK  We considered various composition rules and discussed their adequacy for learning compact repre- sentations. Inference and learning procedures for models with extremal composition rules were then proposed and their performance was tested in experiments. An alternative to competitive interaction rules would be to use a prior on the part parameters (e.g., an L1-penalty on the log-odds). However, this would create a bias, which affects all experts. Our approach on the other hand allows us to use maximum-likelihood estimation. The focus of this paper was on binary data. A natural next step is to study compositions of experts for real-valued data. This includes considering composition rules for variances (in addition to means) and achieving continuity when transitioning from one expert to another.  1The letter X is missing from the dataset.  10  Accepted as a workshop contribution at ICLR 2015  Figure 8: Experts learned from 20 examples per letter class. Each expert is plotted at its mean location with mean orientation. For each pixel the color (red, green, blue, magenta) indicates the maximum expert and the intensity visualizes the template value (white corresponding to 0, color corresponding to 1).  REFERENCES Amit, Yali and Trouv´e, Alain. Pop: Patchwork of parts models for object recognition. International  Journal of Computer Vision, 75(2):267–282, 2007.  Bartholomew, David J., Knott, Martin, and Moustaki, Irini. Latent Variable Models and Factor  Analysis. Wiley, 2011.  Bengio, Yoshua, Courville, Aaron, and Vincent, Pascal. Representation learning: A review and new  perspectives. Pattern Analysis and Machine Intelligence, 35(8):1798–1828, 2013.  Bruna, Joan and Mallat, St´ephane. Invariant scattering convolution networks. Pattern Analysis and  Machine Intelligence, 35(8):1872–1886, 2013.  Dayan, Peter and Zemel, Richard S. Competition and multiple cause models. Neural Computation,  7(3):565–579, 1995.  Dempster, Arthur P, Laird, Nan M, and Rubin, Donald B. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (methodological), pp. 1–38, 1977.  Fidler, Sanja and Leonardis, Ales. Towards scalable representations of object categories: Learning  a hierarchy of parts. In Computer Vision and Pattern Recognition, pp. 1–8, 2007.  Hinton, Geoffrey E. Training products of experts by minimizing contrastive divergence. Neural  computation, 14(8):1771–1800, 2002.  Lake, Brenden M., Salakhutdinov, Ruslan, and Tenenbaum, Josh. One-shot learning by inverting a compositional causal process. In Advances in Neural Information Processing Systems, pp. 2526– 2534, 2013.  Lee, Honglak, Grosse, Roger, Ranganath, Rajesh, and Ng, Andrew Y. Convolutional deep be- lief networks for scalable unsupervised learning of hierarchical representations. In International Conference on Machine Learning, pp. 609–616, 2009a.  Lee, Honglak, Raina, Rajat, Teichman, Alex, and Ng, Andrew Y. Exponential family sparse coding  with application to self-taught learning. In IJCAI, volume 9, pp. 1113–1119, 2009b.  11  Accepted as a workshop contribution at ICLR 2015  L¨ucke, J¨org and Sahani, Maneesh. Maximal causes for non-linear component extraction. The  Journal of Machine Learning Research, 9:1227–1267, 2008.  Mairal, Julien, Bach, Francis, Ponce, Jean, and Sapiro, Guillermo. Online dictionary learning for  sparse coding. In International Conference on Machine Learning, pp. 689–696, 2009.  Mallat, St´ephane G. and Zhang, Zhifeng. Matching pursuits with time-frequency dictionaries. Signal  Processing, 41(12):3397–3415, 1993.  Meeds, Edward, Ghahramani, Zoubin, Neal, Radford M., and Roweis, Sam T. Modeling dyadic data with binary latent factors. In Advances in Neural Information Processing Systems, pp. 977–984, 2006.  Neal, Radford M. Connectionist learning of belief networks. Artiﬁcial intelligence, 56(1):71–113,  1992.  Saund, Eric. A multiple cause mixture model for unsupervised learning. Neural Computation, 7(1):  51–71, 1995.  Schein, Andrew I., Saul, Lawrence K., and Ungar, Lyle H. A generalized linear model for princi- pal component analysis of binary data. In International Workshop on Artiﬁcial Intelligence and Statistics, volume 38, pp. 46, 2003.  van der Maaten, Laurens. A new benchmark dataset for handwritten character recognition. Technical  report, TiCC TR 2009-002, Tilburg University, 2009.  Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Extracting and composing robust features with denoising autoencoders. In International Conference on Machine Learning, pp. 1096–1103, 2008.  Williams, Christopher K.I. and Titsias, Michalis K. Greedy learning of multiple objects in images  using robust statistics and factorial learning. Neural Computation, 16(5):1039–1062, 2004.  Zhu, Long L., Lin, Chenxi, Huang, Haoda, Chen, Yuanhao, and Yuille, Alan. Unsupervised structure learning: Hierarchical recursive composition, suspicious coincidence and competitive exclusion. In Computer Vision–ECCV, pp. 759–773. 2008.  12  ",
1504.02518,2015, Unsupervised Feature Learning from Temporal Data,"['Unsupervised Feature Learning from Temporal Data', 'Ross Goroshin', 'Joan Bruna', 'Jonathan Tompson', 'David Eigen', 'and Yann LeCun']",https://arxiv.org/pdf/1504.02518,"5 1 0 2    r p A 5 1         ]  V C . s c [      2 v 8 1 5 2 0  .  4 0 5 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  UNSUPERVISED FEATURE LEARNING FROM TEMPO- RAL DATA  Ross Goroshin1, Joan Bruna1,2, Jonathan Tompson1, David Eigen1, Yann LeCun1,2 1Courant Institute of Mathematical Science 719 Broadway 12th Floor, New York, NY 10003 2Facebook AI Research, 770 Broadway, New York, NY 10003 {goroshin,bruna,tompson,deigen,yann}@cims.nyu.edu  Current state-of-the-art classiﬁcation and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pool- ing auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to deﬁne a more temporally and semantically coherent metric. Our main assumption is that data samples that are temporal neighbors are also likely to be neighbors in the latent space. For example, adjacent frames in a video sequence are more likely to be seman- tically similar than non-adjacent frames. This assumption naturally leads to the slowness prior on features which was introduced in SFA (Wiskott & Sejnowski (2002)). Temporal coherence can be exploited by assuming a prior on the features extracted from the temporal data sequence. One such prior is that the features should vary slowly with respect to time. In the discrete time setting this prior corresponds to minimizing an Lp norm of the difference of feature (cid:80)T vectors for temporally adjacent inputs. Consider a video sequence with T frames, if zt represents the feature vector extracted from the frame at time t then the slowness prior corresponds to minimizing t=1 (cid:107)zt − zt−1(cid:107)p. To avoid the degenerate solution zt = z0 for t = 1...T , a second term is introduced which encourages data samples that are not temporal neighbors to be separated by at least a distance of m-units in feature space, where m is known as the margin. In the temporal setting this corresponds to minimizing max(0, m − (cid:107)zt − zt(cid:48)(cid:107)p), where |t − t(cid:48)| > 1. Together the two terms form the loss function introduced in Hadsell et al. (2006) as a dimension reduction and data visualization algorithm known as DrLIM. Assume that there is a differentiable mapping from input space to feature space which operates on individual temporal samples. Denote this mapping by G and assume it is parametrized by a set of trainable coefﬁcients denoted by W . That is, zt = GW (xt). The per-sample loss function can be written as:  (cid:26) (cid:107)GW (xt) − GW (xt(cid:48))(cid:107)p,  L(xt, xt(cid:48), W ) =  max(0, m − (cid:107)GW (xt) − GW (xt(cid:48))(cid:107)p)  if |t − t(cid:48)| = 1 if |t − t(cid:48)| > 1  (1)  The second contrastive term in Equation 1 only acts to avoid the degenerate solution in which GW is a constant mapping, it does not guarantee that the resulting feature space is informative with respect to the input. This discriminative criteria only depends on pairwise distances in the representation space which is a geometrically weak notion in high dimensions. We propose to replace this con- trastive term with a term that penalizes the reconstruction error of both data samples. Introducing a reconstruction terms not only prevents the constant solution but also acts to explicitly preserve infor- mation about the input. This is a useful property of features which are obtained using unsupervised learning; since the task to which these features will be applied is not known a priori, we would like to preserve as much information about the input as possible. What is the optimal architecture of GW for extracting slow features? Slow features are invariant to temporal changes by deﬁnition. In natural video and on small spatial scales these changes mainly correspond to local translations and deformations. Invariances to such changes can be achieved us- ing appropriate pooling operators Bruna & Mallat (2013); LeCun et al. (1998). Such operators are at the heart of deep convolutional networks (ConvNets), currently the most successful supervised feature learning architectures Krizhevsky et al. (2012). Inspired by these observations, let GWe be a two stage encoder comprised of a learned, generally over-complete, linear map (We) and rectifying nonlinearity f (·), followed by a local pooling. Let the N hidden activations, h = f (Wex), be sub-  1  Accepted as a workshop contribution at ICLR 2015  (a)  (b)  Figure 1: Pooled decoder dictionaries learned without (a) and with (b) the L1 penalty using (2).  Figure 2: Block diagram of the Siamese convolutional model trained on pairs of frames.  (cid:17) 1  (cid:16)(cid:80)  We  hp tj  j∈Pi  p =  (t) = (cid:107)ht(cid:107)Pi  divided into K potentially overlapping neighborhoods denoted by Pi. Note that biases are absorbed by expressing the input x in homogeneous coordinates. Feature zi produced by the encoder for the p . Training through a local input at time t can be expressed as Gi pooling operator enforces a local topology on the hidden activations, inducing units that are pooled together to learn complimentary features. In the following experiments we will use p = 2. Although it has recently been shown that it is possible to recover the input when We is sufﬁciently redundant, reconstructing from these coefﬁcients corresponds to solving a phase recovery problem Bruna et al. (2014) which is not possible with a simple inverse mapping, such as a linear map Wd. Instead of reconstructing from z we reconstruct from the hidden representation h. This is the same approach taken when training group-sparse auto-encoders Kavukcuoglu et al. (2009). In order to promote sparse activations in the case of over-complete bases we additionally add a sparsifying L1 penalty on the hidden activations. Including the rectifying nonlinearity becomes critical for learning sparse inference in a hugely redundant dictionary, e.g. convolutional dictionaries Gregor & LeCun (2010). The complete loss functional is:  (cid:88)  (cid:0)(cid:107)Wdhτ − xτ(cid:107)2 + α|hτ|(cid:1) + β  (cid:12)(cid:12)(cid:107)ht(cid:107)Pi − (cid:107)ht(cid:48)(cid:107)Pi(cid:12)(cid:12)  K(cid:88)  i=1  (2)  L(xt, xt(cid:48), W ) =  τ ={t,t(cid:48)}  Figure 2 shows a convolutional version of the proposed architecture and loss. By replacing all linear operators in our model with convolutional ﬁlter banks and including spatial pooling, translation invariance need not be learned LeCun et al. (1998). In all other respects the convolutional model is conceptually identical to the fully connected model described in the previous section.  REFERENCES Bengio, Yoshua, Courville, Aaron C., and Vincent, Pascal. Representation learning: A review and new per-  spectives. Technical report, University of Montreal, 2012.  Bromley, Jane, Bentz, James W, Bottou, L´eon, Guyon, Isabelle, LeCun, Yann, Moore, Cliff, S¨ackinger, Eduard, and Shah, Roopak. Signature veriﬁcation using a siamese time delay neural network. International Journal of Pattern Recognition and Artiﬁcial Intelligence, 7(04):669–688, 1993.  2   L₁ L₁ReconstructionReconstructionL₂-PoolingL₂-PoolingDecoder  FiltersEncoder  FiltersXtXt₊₁htht₊₁Decoder  FiltersEncoder  Filters SlownessAccepted as a workshop contribution at ICLR 2015  Bruna, Joan and Mallat, St´ephane. Invariant scattering convolution networks. Pattern Analysis and Machine  Intelligence, IEEE Transactions on, 35(8):1872–1886, 2013.  Bruna, Joan, Szlam, Arthur, and LeCun, Yann. Signal recovery from pooling representations. In ICML, 2014.  Cadieu, Charles F. and Olshausen, Bruno A. Learning intermediate-level representations of form and motion  from natural movies. Neural Computation, 2012.  Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout net-  works. In ICML, 2013.  Goroshin, Rostislav and LeCun, Yann. Saturating auto-encoders. In ICLR, 2013.  Gregor, Karol and LeCun, Yann. Learning fast approximations of sparse coding. In ICML’2010, 2010.  Hadsell, Raia, Chopra, Soumit, and LeCun, Yann. Dimensionality reduction by learning an invariant mapping.  In CVPR, 2006.  Hyv¨arinen, Aapo, Karhunen, Juha, Oja, and Erkki. Independent component analysis, volume 46. John Wiley  & Sons, 2004.  Hyv¨arinen, Aapo, Hurri, Jarmo, and V¨ayrynen, Jaakko. Bubbles: a unifying framework for low-level statistical  properties of natural image sequences. JOSA A, 20(7):1237–1252, 2003.  Kavukcuoglu, Koray, Ranzato, MarcAurelio, Fergus, Rob, and LeCun, Yann. Learning invariant features  through topographic ﬁlter maps. In CVPR, 2009.  Kayser, Christoph, Einhauser, Wolfgang, Dummer, Olaf, Konig, Peter, and Kding, Konrad. Extracting slow  subspaces from natural videos leads to complex cells. In ICANN’2001, 2001.  Krizhevsky, Alex. Learning multiple layers of features from tiny images. Master’s thesis, University of Toronto,  April 2009.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convolutional  neural networks. In NIPS, volume 1, pp. 4, 2012.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.  Proc. IEEE, 86(11):2278–2324, 1998.  Lies, Jorn-Philipp, Hafner, Ralf M, and Bethge, Matthias. Slowness and sparseness have diverging effects on  complex cell learning. 10, 2014.  Mobahi, Hossein, Collobert, Ronan, and Weston, Jason. Deep learning from temporal coherence in video. In  ICML, 2009.  Rifai, Salah, Vincent, Pascal, Muller, Xavier, Galrot, Xavier, and Bengio, Yoshua. Contractive auto-encoders:  Explicit invariance during feature extraction. In ICML, 2011.  Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Extracting and composing  robust features with denoising autoencoders. Technical report, University of Montreal, 2008.  Wiskott, Laurenz and Sejnowski, Terrence J. Slow feature analysis: Unsupervised learning of invariances.  Neural Computation, 2002.  Zou, Will, Zhu, Shenghuo, Yu, Kai, and Ng, Andrew Y. Deep learning of invariant features via simulated  ﬁxations in video. In Advances in Neural Information Processing Systems, pp. 3212–3220, 2012.  3  ",
1412.6567,2015, Classifier with Hierarchical Topographical Maps as Internal Representation,"['Classifier with Hierarchical Topographical Maps as Internal Representation', 'Pitoyo Hartono', 'Paul Hollensen', 'and Thomas Trappenberg']",https://arxiv.org/pdf/1412.6567,"5 1 0 2    r p A 3         ] E N . s c [      4 v 7 6 5 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  CLASSIFIER WITH HIERARCHICAL TOPOGRAPHICAL MAPS AS INTERNAL REPRESENTATION  Thomas Trappenberg & Paul Hollensen Faculty of Computer Science Dalhousie University Halifax, Canada {tt,hollensen}@cs.dal.edu  Pitoyo Hartono School of Engineering Chukyo University Nagoya, Japan hartono@ieee.org  ABSTRACT  In this study we want to connect our previously proposed context-relevant topo- graphical maps with the deep learning community. Our architecture is a classi- ﬁer with hidden layers that are hierarchical two-dimensional topographical maps. These maps differ from the conventional self-organizing maps in that their orga- nizations are inﬂuenced by the context of the data labels in a top-down manner. In this way bottom-up and top-down learning are combined in a biologically rel- evant representational learning setting. Compared to our previous work, we are here speciﬁcally elaborating the model in a more challenging setting compared to our previous experiments and to advance more hidden representation layers to bring our discussions into the context of deep representational learning.  1  INTRODUCTION  Early cortical representations of sensory information in the mammalian brain have spatial organi- zations (Hubel & Wiesel, 1962). For example, simple cells in the primary visual cortex respond to edges at speciﬁc locations, while adjacent cells tend to respond best to edges with only small variations of orientations and locations. Hubel and Wiesel identiﬁed such hypercolumns in which edge orientations are topographically represented. Topographic representations are also present in other sensory modalities such as in the primary auditory cortex (Romani et al., 1982) and the so- matosensory cortex (Wall et al., 1983). Models to describe self-organizations of such sensory maps have long been studied (Willshaw & Von Der Malsburg, 1976; Kohonen, 1982). In the context of representational learning, such algorithms must be considered as unsupervised learning algorithms that are purely data driven, and we characterize such learning also as bottom-up learning.  In contrast, recent developments in deep learning have overwhelmingly shown the abilities of super- vised learning in which representations are mainly guided by back-propagating errors from labelled data (Krizhevsky et al., 2012). While early deep learning approaches have emphasized the advan- tage of unsupervised pre-training (Hinton et al., 2006), most current successes do not rely on this form of unsupervised learning, which in their basic form do not include topographic constraints. It has hence become a major puzzle why biological system have this form of representational organi- zation. In terms of learning theories, such organizations represent representational constraints that on a ﬁrst glimpse can restrict the classiﬁcation performance of deep learning classiﬁers.  In this paper we study the interaction between bottom-up self-organization and top-down error cor- rection learning in a simple model that combines a Kohonen-type self-organizing map (SOM) with backprogagation learning in some classiﬁcation tasks. We have introduced such a model with a single hidden layer in Hartono et al. (2014) and tested this on simple machine learning benchmarks.  1  Accepted as a workshop contribution at ICLR 2015  Here we start to apply this model to more challenging tasks and to generalize our model to ver- sions with multiple hidden layers, as outlined in Fig. 1, in order to ultimately scale the model to much larger problems. The Kohonen SOM uses nodes that have Gaussian activation functions, and backpropagating errors through such a network corresponds to a radial basis function (RBF) net- work. However, the RBF network is restricted by the bottom-up constraints of the SOM, and thus we called this network a restricted RBF (rRBF). Interestingly, by combining the Kohonen SOM learning with backpropagation, we found that the top down error has the effect of enforcing the topographic neighbourhood of items with the same label while ”repelling” representations of items with different labels.  teacher signal  contextual error  output  context  regulated self-organization  supervised learning  hidden  N  hidden 2  hidden 1  Figure 1: Multilayerd Restricted Radial Basis Function Network  input  The evaluation of the rRBF with one hidden SOM layer on basic machine learning examples has shown that there is a much stronger clustering of classes in the hidden layer compared to the standard SOM and that the classiﬁcation performance is comparable to a basic mulitlayer perceptron. Hence, while we can argue that our approach aids in the visualization of the representations, our approach does not lead to better classiﬁcations that would excite machine learning practitioners. Including label information in SOM has also been done in various ways in the past (Ritter & Kohonen, 1989). However, as mentioned before, our main aim here is to understand the consequences of biological facts – here, topographic representations – in terms of deep learning representations. More specif- ically, our initial tests have been simple examples where a shallow 2-dimensional representation is mostly sufﬁcient. Here we ask if more complicated relationships can be represented with deeper version of this architecture, and how such a version would compare to traditional deep learning approaches.  2 THE MULTILAYERED RESTRICTED RADIAL BASIS FUNCTION NETWORK  The multilayered restricted radial basis function network (M-rRBF) contains an input layer, an out- put layer and a hierarchical stack of N hidden layers between them as shown in Fig. 1.  The activation I M that of a standard radial basis function network,  k and corresponding OM  k of the k-th neuron in the M -th layer of the M-rRBF are  1 2  I M k (t) = k (t) = e−IM  OM  k (t)σ(k∗M , k, t),  kW M  k (t) − OM −1(t)k2 for (1 ≤ M ≤ N )  (1)  where W M k (t) is the reference vector associated with the k-th neuron in the M -th layer. Here, O0(t) = X(t) where X(t) is the input vector at time t. In this equation, k∗M is the best matching unit (BMU) in the M -th hidden layer,  wM = arg min  k  I M k (t).  2  (2)  Accepted as a workshop contribution at ICLR 2015  The neighbourhood function, σ, becomes narrower over the course of learning according to  σ(k∗M , k, t) = e−  kk∗M −kk2  s(t)  The output layer is a sigmoidal perceptron,  s(t) = s0(  send s0  t  tend  )  Il(t) = X  vkl(t)ON  k (t) − θl(t)  yl(t) =  k  1  1 + e−Il(t)  ,  (3)  (4)  (5)  where vkl(t) is the weight connecting the k-th neuron in the last hidden layer, and θl(t) is the bias of that neuron at time t.  We train the network on a quadratic error function at time t,  E(t) =  1 2 X  l  (yl(t) − Tl(t))2,  (6)  where Tl(t) is the l-th component of the teacher signal at time t. The connection weights from the N -th hidden layer to the output layer and corresponding biases of the output neurons are minimized with gradient descent  vkl(t + 1) = vkl(t) − η1  θl(t + 1) = θl(t) − η1  ∂E(t) ∂vkl(t) ∂E(t) ∂θl(t)  .  (7)  (8)  The gradients of the output weights is given by ∂E(t) ∂yl(t)  ∂E(t) ∂vkl(t)  =  ∂Il(t) ∂vkl(t)  ∂yl(t) ∂Il(t) k (t)  (9) (10) and similarly for the biases. The gradient descent of the reference vectors associated with the neu- rons in the N -th hidden layer is  δl(t) = (yl(t) − Tl(t))yl(t)(1 − yl(t)),  = δl(t)ON  W N  jk (t + 1) = W N  jk (t) − ηhid  ∂E(t) ∂W N jk  where ηhid is the learning rate for the hidden layers. This leads to the learning rule (t) − W N  k (t)σ(k∗N , k, t)(ON −1  jk (t + 1) = W N  jk (t) + ηhidδN  W N  j  jk (t))  k (t) = −(X δN  δl(t)vkl(t))e−I N  k (t).  l  (11)  (12)  (13)  If ON −1 is considered to be the input vector to the N -th hidden layer, the modiﬁcation in Eq. 12 is similar to the modiﬁcation rule of the conventional SOM. The only difference is that in SOM, the reference vector is always pulled toward the input vector, while in Eq. 12, it is not necessarily so. The direction of the reference vector’s modiﬁcation is decided by the sign of δN k (t), in which a k (t) modiﬁes the reference vector as in SOM, while a negative one repels the reference positive δN k (t) is a regulatory signal of error feedback from the supervised vector from the input vector. This δN layer, and thus reﬂects the label context to the otherwise purely self-organizing process in this layer.  The modiﬁcation of the reference vectors in the (N − L)-th layer with (1 ≤ L ≤ N − 1) is correspondingly given by  W N −L  (t + 1) = W N −L  ab  ab ∆W N −L  ab δN −L b  (t) = δN −L (t) = (−1)l(X  b  ab  (t)  (t) + ηhid∆W N −L (t)σ(k∗N −L, b, t)(ON −L−1 (t))e−I N −L  ∆W N −L+1  a  b  ab  (t)  (t) − W N −L  ab  (t))  (14) (15)  (16)  The equations show that also in this deeper version of our architecture the organizing process in the (N − L)-th layer is regulated by contextual error, δN −L  , that is passed from the higher layer.  b  a  3  Accepted as a workshop contribution at ICLR 2015  3 EXPERIMENTS  3.1 DIFFERENCE BETWEEN MLP, SOM AND CRSOM  It is often criticized that a MLP is a black box in which the operation at each level is not readily understood in terms of simple rules. A frequent motivating goal of SOMs is therefore the embed- dings of a feature space in a low-dimensional, visualizable form. The primary difference between CRSOM and SOM is that SOM preserves the topological structures of high-dimensional data into a low dimensional map based on their similarities of the data alone, as often measured by Euclidean distance, while CRSOM also incorporates their context as provided by the class labels. It should be noted that similarities in the features of data do not always translate into the similarities of their con- texts. For example the physical features of lion and zebra, such as their sizes, number of legs, place to live and running speed are very similar, so if they are characterized with those features they should be mapped closely in a SOM. However, if they are viewed in the context of carnivore-herbivore, they should be mapped far from each other.  To demonstrate the difference between SOM and CRSOM we ran preliminary experiments using the animal data set proposed in Ritter & Kohonen (1989). The data set in this experiment contains 16 animals, each one characterized by 16 binary features. Figure 2 shows the mapping from the 16-dimensional data to a 2-dimensional map learned by conventional SOM and three CRSOMs provided different contexts. For example, the SOM maps ”duck” and ”hen”, which share similar features, into a single point which is far from the point where ”horse” is mapped to.  To illustrate the context-relevance of CRSOM, different contexts were infused into the data. The ﬁrst CRSOM mapping in Figure 2 (middle left) shows the CRSOM produced by training a single- layered rRBF in which the original animal data were labeled either as ”carnivore” or ”herbivore”. In this CRSOM, neurons that were chosen as winners for herbivores were drawn as (cid:3), and neurons for carnivores were drawn as (cid:13), while their sizes reﬂects their winning frequencies. In this CR- SOM only ﬁves neurons were chosen as winners: two in the upper half of the map are occupied by herbivores while three neurons in the lower-half are occupied by carnivores. It is interesting to observe that the infusion of contexts changed the appearance of the low-dimensional representation of the data. For example in the original SOM ”duck” and ”zebra” were diagonally distanced from each other, but in the carnivore/herbivore-contexted CRSOM they are positioned close to each other due to their common context of ”herbivore”. It should be noted that CRSOM also preserves the topological characteristics of the data, for example ”duck”, ”dove”, ”hen” and ”goose” that share similar features are mapped into a single point.  Figure 2: Mappings learned from the animals dataset Ritter & Kohonen (1989). From left to right: SOM (no context), CRSOM with ”carnivore” context, ”speed” context, and ”avian” context.  In order to see how a shifting context would inﬂuence the hidden representations we ran two more experiments with altered class structure. In the second experiment each data point was labeled ac- cording to the animals’ moving velocity, either as ”fast”, ”medium” or ”slow”. These are illustrated as (cid:13), ♦ and (cid:3), respectively, in Fig. 2 (middle right). In the third experiment an rRBF was trained to classify the data points into either of avian or non-avian, and the resulting CRSOM is shown in Fig. 2 (far right), where (cid:3) illustrate a neuron for avians and (cid:13) is a neuron for non-avians. The three experiments with different class context show that the CRSOM preserves the topographical characteristics of high dimensional data in relevance to their contexts.  4  Accepted as a workshop contribution at ICLR 2015  3.2 CRSOM ON MNIST DATA  To start comparing our biological motivated model with more typical deep learning benchmarks, we trained a single-layered rRBF with part of the MNIST database of handwritten digits. We only used the digits from 0 to 4 in the following illustrations mainly for time reasons as we did not have the time to experiment with larger networks. However, this is not a principle restiction and larger models are now under investigation. Some of the training examples are shown in Fig. 3(a).  (a)  (b)  (c)  Figure 3: MNIST database: 784 features, 5 classes, 1269 instances  The conventional SOM for part of the MNIST handwriting data is shown in Fig. 3(b) and the CRSOM produced by the single-layered rRBF in this experiment is shown in Fig. 3(c). In both ﬁgures, red dots indicate digit 4, blue is for 3, green is for 2, blue is for 1 and magenta is for 0. From these ﬁgures, we can observe that the representation of rRBF for this problem is different from the purely topographical characteristics of the data. The generated CRSOM is also sparser and provides more information than the conventional SOM. For example from Fig. 3 we can intuitively learn that the red dots (digit 4) are more varied than for example digit 0. To show that the CRSOM is a better representation than SOM, we also tested the classiﬁcation performance of the rRBF with CRSOM as its internal layer, and a classiﬁer with SOM as the internal layer. In this test, the network with CRSOM as its internal layer produced 0.24% classiﬁcation error, versus 1.5% for the SOM-based network.  3.3 FIRST RESULTS ON DEEPER REPRESENTATIONS  In order to test how a second layer will inﬂuence the learned representations in our architecture, we repeated experiments on well known UCI datasets. While these experiments might not be exciting in terms of improving practical machine learning applications, our aim here is to understand how representational learning would look in a biological motivated network.  Figure 4 shows the mappings learned by SOM and by CRSOM with 1 and 2 hidden layers on the Iris, Heart, E.Coli and Lung cancer datasets. For the SOM, the contexts (classes of the data) did not have any inﬂuence to the organization of the map, but for visualization clarity they were drawn with different shapes in the map, in which their sizes show their winning frequencies, and ×s show the reference vectors that were selected as winners for inputs belonging to different classes. Taking Iris as an example, the single layer CRSOM, as well as the ﬁrst layer of the M-CRSOM, nicely illustrates the characteristics of this problem, where one class is easily separable, while the remaining two are not easily separable from each other. The second layer of the CRSOM beneﬁts from the depth of the preceeding representation in order to fully separate the classes. The SOM mapping, on the other hand, does not offer any obvious information about the class structure. Across the datasets, the CRSOM representation is consistently much sparser, and this sparsity increases with layer depth. Margins between class clusters also consistently increase with layer depth, and some datasets which were non-separable in the ﬁrst layer become so in the second.  Figure 5 shows the average error during the learning process of Iris problem over 10 trials, with the typical gradual map formation process during the learning process.  5  Accepted as a workshop contribution at ICLR 2015  Figure 4: Mappings learned by SOM, CRSOM, and M-CRSOM on the UCI dataset.  Figure 5: Learning Curve: average of learning error of Iris problem over 10 trials, shown with CRSOM formation during the learning process  6  Accepted as a workshop contribution at ICLR 2015  3.4 GENERALIZATION OF RRBF  The generalization performance of rRBF is tested with a 10-fold cross validation for some classiﬁ- cation problems as shown in Fig. 6. The performance of the classiﬁers are statistically equivalent for all the methods, given the standard deviations plotted as error bars. This is an encouraging re- sult as our previous tests with single hidden layer networks did not consistently perform as well as comparators.  While at this point there is no statistically signiﬁcant difference in the classiﬁcation performance, average errors do decrease with increasing depth of representation. It is also interesting to observe that the correlation between the generalization performance and the visual appearance of the map. The appearance of the CRSOM is correlated with the generalization performance of the rRBF. For example, in the Iris problems, where well- separated clusters were formed, the rRBF usually pro- duced high generalization performance. Such clustered representations should also produce more robustness to representational noise.  )  % ( e t a R    r o r r  E   n o i t a z  i l  a r e n e G  70  60  50  40  30  20  10  0  MLP rRBF-1 rRBF-2  ecoli  fertility  heart  iris  Problems  lung  thyroid  wine  Figure 6: Generalization Comparison  4 DISCUSSION AND CONCLUSIONS  This paper is intended to connect our previous work on context relevant maps and known biological representations with the deep learning community. We showed that a generalization of the formu- las to architectures with many hidden layers is straight forward and still preserves the ﬁnding that context from class labels can bias the topographic maps in the manner found previously.  The ﬁrst experiments with the deeper network are very encouraging. While this is still a simple problem, the results show that a much better clustering of the IRIS data are achieved relative to the single hidden layer case. It is clear that the inﬂuence of different contexts for data will make this a much more difﬁcult clustering problem compared to the situation where only bottom-up self- organization is considered. It should hence not be expected that such representations, as well as representations of much more complex data, are achieved with simple 2-dimensional maps. The hierarchical architecture offers here a solution in that early representations could still reﬂect higher dimensional relations while higher representations could work more efﬁciently on the simpliﬁed and compressed previous representations.  While our preliminary tests are encouraging us to look further into applying this model to more complicated large scale benchmarks, it is still not clear what the advantage of topographic organiza- tion in biological systems is. It is clear that visualization can not be an argument for our brain, and in terms of machine learning it seems that topography is a hindering constraint. However, given that nature had time to optimize brain architectures, it is very possible that the topographic constraints allow better generalizations in problem domains that are speciﬁcally addressed by the brain.  7  Accepted as a workshop contribution at ICLR 2015  REFERENCES Hartono, Pitoyo, Hollensen, Paul, and Trappenberg, Thomas. Learning-regulated context relevant  topographic maps. IEEE Trans. on Neural Networks and Intelligent Systems, (in press), 2014.  Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye. A fast learning algorithm for deep belief  nets. Neural Computation, 18:1527–1554, 2006.  Hubel, David H and Wiesel, Torsten N. Receptive ﬁelds, binocular interaction and functional archi-  tecture in the cat’s visual cortex. The Journal of physiology, 160(1):106–154, 1962.  Kohonen, Teuvo. Self-organized formation of topologically correct feature maps. Biological Cyber-  netics, 43:59–69, 1982.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In Advances in neural information processing systems, 2012.  Ritter, Helge and Kohonen, Teuvo. A self-organizing semantic maps. Biological Cybernetics, 61:  241–254, 1989.  Romani, Gian Luca, Williamson, Samuel J, and Kaufman, Lloyd. Tonotopic organization of the  human auditory cortex. Science, 216(4552):1339–1340, 1982.  Wall, John T, Felleman, Daniel J, and Kaas, Jon H. Recovery of normal topography in the so- matosensory cortex of monkeys after nerve crush and regeneration. Science, 221(4612):771–773, 1983.  Willshaw, David J and Von Der Malsburg, Christoph. How patterned neural connections can be set up by self-organization. Proceedings of the Royal Society of London B: Biological Sciences, 194 (1117):431–445, 1976.  A APPENDIX  The modiﬁcation of the reference vectors in the (N − 1)-th layer can be calculated as follows.  ∂E(t) ∂W N −1  ij  (t)  =  ∂E(t) ∂yl(t)  ∂yl(t) ∂W N −1  ij  (t)  = X  l  (yl(t) − Tl(t))yl(t)(1 − yl(t))  ∂Il(t) ∂W N −1  ij  (t)  = − X  (X  k  l  δl(t)vkl(t))ON  k (t)(ON −1  j  (t) − W N  jk (t))  j  ∂ON −1 ∂W N −1  ij  (t)  (t)  = (X  k  ∆W N  jk (t))e−I N −1  j  (t)σ(k∗N −1, j, t)(ON −2  i  (t) − W N −1  ij  (t))  (17)  ∆W N  jk (t) = δN  k (t)σ(k∗N , k, t)(ON −1  (t) − W N  jk (t))  W N −1  ij  (t + 1) = W N −1  ij  (t)σ(k∗N −1, j, t)(ON −2  i  (t) − W N −1  ij  (t))  j (t) + ηhidδN −1  j  δN −1 j  (t) = −(X  k  ∆W N  jk (t))e−I N −1  j  (t)  (18)  (19)  (20)  Following this chain rule, the modiﬁcation of the reference vectors in the (N − L)-th layer (1 ≤ L ≤ N − 1) can be written as follows.  W N −L  (t + 1) = W N −L  ab  (t) + ηhid∆W N −L  ab  (t)  ab ∆W N −L  ab δN −L b  (t) = δN −L (t) = (−1)l(X  (t)σ(k∗N −L, b, t)(ON −L−1 (t))e−I N −L  ∆W N −L+1  ab  a  b  b  (t)  (t) − W N −L  ab  a  8  (t))  (21) (22)  (23)  ",
1412.5673,2015, Entity-Augmented Distributional Semantics for Discourse Relations,"['Entity-Augmented Distributional Semantics for Discourse Relations', 'Yangfeng Ji and Jacob Eisenstein']",https://arxiv.org/pdf/1412.5673,"Accepted as a workshop contribution at ICLR 2015  ENTITY-AUGMENTED DISTRIBUTIONAL SEMANTICS FOR DISCOURSE RELATIONS  Yangfeng Ji & Jacob Eisenstein School of Interactive Computing Georgia Institute of Technology Atlanta, GA 30332, USA {jiyfeng,jacobe}@gatech.edu  ABSTRACT  Discourse relations bind smaller linguistic elements into coherent texts. However, automatically identifying discourse relations is difﬁcult, because it requires un- derstanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.  1  INTRODUCTION  The high-level organization of text can be characterized in terms of discourse relations between adjacent spans of text (Knott, 1996; Mann, 1984; Webber et al., 1999). Identifying these relations has been shown to be relevant to tasks such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al., 2011). While the Penn Discourse Treebank (PDTB) now provides a large dataset annotated for discourse relations (Prasad et al., 2008), the automatic identiﬁcation of implicit discourse relations is a difﬁcult task, with state-of-the-art performance at roughly 40% (Lin et al., 2009).  One reason for this poor performance is that predicting implicit discourse relations is a fundamen- tally semantic task, and the relevant semantics may be difﬁcult to recover from surface level features. For example, consider the discourse relation between the following two sentences in Example (1), where a discourse connector like “because” seems appropriate to indicate the relationship. How- ever, without discourse connector, there is little surface information to signal the relationship. We address this issue by applying a discriminatively-trained model of compositional distributional se- mantics to discourse relation classiﬁcation (Socher et al., 2013; Baroni et al., 2014). The meaning of each sentence is represented as a vector (Turney et al., 2010), which is computed through a series of compositional operations over the parse tree.  5 1 0 2    r p A 8 2         ] L C . s c [      3 v 3 7 6 5  .  2 1 4 1 : v i X r a  Example (1) : Bob gave Tina the burger. Example (2) : Bob gave Tina the burger.  She was hungry.  He was hungry.  We further argue that purely vector-based representations on sentences are insufﬁciently expressive to capture discourse relations. To see why, consider what happens in Example (2), where a tiny change is made based on Example (1). After changing the subject of the second sentence to Bob, the original discourse relation seems no longer holding in Example (2). But despite the radical difference in meaning, the distributional representation of the second sentence will be almost un- changed: the syntactic structure remains identical, and the words “he” and “she” have very similar word representations. We address this issue by computing vector representations not only for each sentence, but also for each coreferent entity mention within the sentences. These representations are  1  Accepted as a workshop contribution at ICLR 2015  meant to capture the role played by the entity in the text. We compute entity-role representations using a novel feed-forward compositional model, which combines upward and downward passes through the syntactic structure. Representations for these coreferent mentions are then combined into a classiﬁcation model, and help to predict the implicit discourse relation. In combination, our approach achieves a 3% improvement in accuracy over the best previous work (Lin et al., 2009) on the second-level discourse relation identiﬁcation in the PDTB.1. Our model requires a syntactic parse tree, which is produced automatically from the Stanford CoreNLP parser Klein & Manning (2003). A reviewer asked whether it might be better to employ a left-to-right recurrent neural network, which would obviate the need for this language-speciﬁc resource. While it would clearly be preferable to avoid the use of language-speciﬁc resources when- ever possible, we think this approach is unlikely to succeed in this case. A key difference between language and other types of data is that language has inherent recursive structure. A rich litera- ture in both linguistics and natural language processing elaborates on the close relationship between (recursively-structured) syntax and semantics. Therefore, we see strong theoretical evidence — as well as practical evidence from the history of natural language processing — that syntactic parse structures are central to capturing the meaning in text.  Regarding the multilingual question, there are now accurate parsers and annotated treebanks for dozens of languages,2 and training accurate parsers for “low resource” languages is a hot research topic, with substantial interest from both industry and academia. Languages differ substantially in the importance of word ordering, with English emphasizing word order more than most other languages (Bender, 2013). To our knowledge, it is an open question as to whether left-to-right recurrent neural networks will successfully extract meaning in languages where word order is more free.  2 ENTITY AUGMENTED DISTRIBUTIONAL SEMANTICS FOR RELATION  IDENTIFICATION  We brieﬂy describe our approach to entity-augmented distributional semantics and to discourse rela- tion identiﬁcation. Our relation identiﬁcation model is named as DISCO2, since it is a distributional compositional approach to discourse relations.  2.1 ENTITY AUGMENTED DISTRIBUTIONAL SEMANTICS  The entity-augmented distributional semantics includes two passes in composition procedure: the upward pass for distributional representation of sentence, while the downward pass for distributional representation of entities shared between sentences.  Upward pass Distributional representations for sentences are computed in a feed-forward upward pass: each non-terminal in the binarized syntactic parse tree has a K-dimensional distributional rep- resentation that is computed from the distributional representations of its children, bottoming out in representations of individual words. We follow the Recursive Neural Network (RNN) model pro- posed by Socher et al. (2011). Speciﬁcally, for a given parent node i, we denote the left child as ℓ(i), and the right child as r(i). We compose their representations to obtain, ui = tanh (cid:0)U[uℓ(i); ur(i)](cid:1), where tanh (·) is the element-wise hyperbolic tangent function, and U ∈ RK×2K is the upward com- position matrix. We apply this compositional procedure from the bottom up, ultimately obtaining the sentence-level representation u0.  Downward pass As seen in the contrast between Examples (1) and (2), a model that uses a single vector representation for each sentence would ﬁnd little to distinguish between “she was hungry” and “he was hungry”. It would therefore almost certainly fail to identify the correct discourse relation for at least one of these cases, which requires tracking the roles played by the entities that are coreferent in each pair of sentences. To address this issue, we augment the representation of each sentence with additional vectors, representing the semantics of the role played by each coreferent entity in each  1For more details, please refer to the long version of this paper (Ji & Eisenstein, 2015) 2http://universaldependencies.github.io/docs/#language-other  2  Accepted as a workshop contribution at ICLR 2015  Model  +Entity semantics  +Surface features K Accuracy(%)  Yes  Prior work 1. Lin et al. (2009) Our work 2. Surface feature model 3. DISCO2 4. DISCO2 5. DISCO2 6. DISCO2 ∗ signﬁcantly better than Lin et al. (2009) with p < 0.05  Yes No No Yes Yes  No Yes No Yes  40.2  39.69 36.98 37.63 42.53 43.56∗  50 50 50 50  Table 1: Experimental results on multiclass classiﬁcation of second-level discourse relations. The results of Lin et al. (2009) are shown in line 1; the results for our reimplementation of this system are shown in line 2.  sentence. Rather than represent this information in a logical form — which would require robust parsing to a logical representation — we represent it through additional distributional vectors. The role of a constituent i can be viewed as a combination of information from two neighboring nodes in the parse tree: its parent ρ(i), and its sibling s(i). We can make a downward pass, computing the downward vector di from the downward vector of the parent dρ(i), and the upward vector of the sibling us(i): di = tanh (cid:0)V[dρ(i); us(i)](cid:1), where V ∈ RK×2K is the downward composition matrix. The base case of this recursive procedure occurs at the root of the parse tree, which is set equal to the upward representation, d0 , u0.  2.2 RELATION IDENTIFICATION MODEL  To predict the discourse relation between an sentence pair (m, n), the decision function is a sum of bilinear products,  ψ(y) = (u(m)  0  )⊤Ayu(n)  0 + X  (d(m)  i  i,j∈A(m,n)  )⊤Byd(n)  j + β⊤  y φ(m,n) + by,  (1)  where the predicted relation is given by ˆy = arg maxy∈Y ψ(y), and Ay, By ∈ RK×K are the classiﬁcation parameters for relation y. A scalar by is used as the bias term for relation y, and A(m, n) is the set of coreferent entity mentions shared among the sentence pair (m, n). For the cases where there are no coreferent entity mentions between two sentences, A(m, n) = ∅, the classiﬁcation model considers only the upward vectors at the root. We also use the surface features vector φ(m,n) in the decision function, as we ﬁnd that, this approach outperforms prior work on the classiﬁcation of implicit discourse relations in the PDTB, when combined with a small number of surface features.  3 EXPERIMENTS  We evaluate our approach on the implicit discourse relation identiﬁcation in the Penn Discourse Treebank (PDTB). PDTB relations may be explicit, meaning that they are signaled by discourse connectives (e.g., because); alternatively, they may be implicit, meaning that the connective is ab- sent. We focus on the more challenging problem of classifying implicit discourse relations. Aiming to build a discourse parser in future, we follow the same experimental setting proposed by Lin et al. (2009), and evaluate our relation identiﬁcation model on the second-level relation types.  We run the Stanford parser (Klein & Manning, 2003) and the Berkeley coreference system (Durrett & Klein, 2013) to obtain syntactic trees and coreference results respectively. In the PDTB, each discourse relation is annotated between two argument spans. For non-sentence argument span, we identify the syntactic subtrees with the span, and construct a right-branching superstructure to unify them into a tree.  Table 1 presents results for multiclass identiﬁcation of second-level PDTB relations. As shown in lines 5 and 6, DISCO2 outperforms the prior state-of-the-art (line 1). The strongest performance is  3  Accepted as a workshop contribution at ICLR 2015  obtained by including the entity distributional semantics, with a 3.4% improvement over the accu- racy reported by Lin et al. (2009) (p < .05). The improvement over our reimplementation of this work (line 2) is even greater, which shows how the distributional representation provides additional value over the surface features. The contribution of entity semantics is shown in Table 1 by the accuracy differences between lines 3 and 4, and between lines 5 and 6.  4 CONCLUSION  Discourse relations are determined by the meaning of their arguments, and progress on discourse parsing therefore requires computing representations of the argument semantics. We present a com- positional method for inducing distributed representations not only of discourse arguments, but also of the entities that thread through the discourse. By jointly learning the relation classiﬁcation weights and the compositional operators, this approach outperforms prior work based on hand-engineered surface features. More discussion and experimental results can be found in a forthcoming journal paper (Ji & Eisenstein, 2015).  REFERENCES Baroni, Marco, Bernardi, Raffaella, and Zamparelli, Roberto. Frege in space: A program for com-  positional distributional semantics. Linguistic Issues in Language Technologies, 2014.  Bender, Emily M. Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax, volume 6 of Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers, June 2013. doi: 10.2200/s00493ed1v01y201303hlt020. URL http://dx.doi.org/10.2200/s00493ed1v01y201303hlt020.  Durrett, Greg and Klein, Dan. Easy Victories and Uphill Battles in Coreference Resolution.  In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Seattle, Washington, October 2013. Association for Computational Linguistics.  Ji, Yangfeng and Eisenstein, Jacob. One vector is not enough: Entity-augmented distributional semantics for discourse relations. Conditionally accepted to Transactions of the Association for Computational Linguistics (TACL), 2015.  Klein, Dan and Manning, Christopher D. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pp. 423–430. Associa- tion for Computational Linguistics, 2003.  Knott, Alistair. A data-driven methodology for motivating a set of coherence relations. PhD thesis,  The University of Edinburgh, 1996.  Lin, Ziheng, Kan, Min-Yen, and Ng, Hwee Tou. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pp. 343–351. Association for Computational Linguistics, 2009.  Lin, Ziheng, Ng, Hwee Tou, and Kan, Min-Yen. Automatically Evaluating Text Coherence Using Discourse Relations. In Proceedings of ACL, pp. 997–1006, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.  Louis, Annie, Joshi, Aravind, and Nenkova, Ani. Discourse indicators for content selection in In Proceedings of the 11th Annual Meeting of the Special Interest Group on  summarization. Discourse and Dialogue, pp. 147–156. Association for Computational Linguistics, 2010.  Mann, William. Discourse structures for text generation. In Proceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Compu- tational Linguistics, pp. 367–375. Association for Computational Linguistics, 1984.  Prasad, Rashmi, Dinesh, Nikhil, Lee, Alan, Miltsakaki, Eleni, Robaldo, Livio, Joshi, Aravind, and  Webber, Bonnie. The Penn Discourse Treebank 2.0. In LREC, 2008.  4  Accepted as a workshop contribution at ICLR 2015  Socher, Richard, Lin, Cliff C, Manning, Chris, and Ng, Andrew Y. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning, pp. 129–136, 2011.  Socher, Richard, Perelygin, Alex, Wu, Jean Y, Chuang, Jason, Manning, Christopher D, Ng, An- drew Y, and Potts, Christopher. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of Empirical Methods for Natural Language Processing (EMNLP), 2013.  Somasundaran, Swapna, Namata, Galileo, Wiebe, Janyce, and Getoor, Lise. Supervised and unsu- pervised methods in employing discourse relations for improving opinion polarity classiﬁcation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 170–179. Association for Computational Linguistics, 2009.  Turney, Peter D, Pantel, Patrick, et al. From frequency to meaning: Vector space models of seman-  tics. Journal of artiﬁcial intelligence research, 37(1):141–188, 2010.  Webber, Bonnie, Knott, Alistair, Stone, Matthew, and Joshi, Aravind. Discourse relations: A struc- tural and presuppositional account using lexicalised tag. In Proceedings of the Association for Computational Linguistics (ACL), pp. 41–48, 1999.  Yoshida, Yasuhisa, Suzuki, Jun, Hirao, Tsutomu, and Nagata, Masaaki. Dependency-based Dis-  course Parser for Single-Document Summarization. In EMNLP, 2014.  5  ",
1412.5474,2015, Flattened Convolutional Neural Networks for Feedforward Acceleration,"['Flattened Convolutional Neural Networks for Feedforward Acceleration', 'Jonghoon Jin', 'Aysegul Dundar', 'and Eugenio Culurciello']",https://arxiv.org/pdf/1412.5474,"5 1 0 2     v o N 0 2         ] E N . s c [      4 v 4 7 4 5  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  FLATTENED CONVOLUTIONAL NEURAL NETWORKS FOR FEEDFORWARD ACCELERATION  Jonghoon Jin, Aysegul Dundar & Eugenio Culurciello Purdue University, West Lafayette, IN 47907, USA {jhjin, adundar, euge}@purdue.edu  ABSTRACT  We present ﬂattened convolutional neural networks that are designed for fast feed- forward execution. The redundancy of the parameters, especially weights of the convolutional ﬁlters in convolutional neural networks has been extensively stud- ied and different heuristics have been proposed to construct a low rank basis of the ﬁlters after training. In this work, we train ﬂattened networks that consist of consecutive sequence of one-dimensional ﬁlters across all directions in 3D space to obtain comparable performance as conventional convolutional networks. We tested ﬂattened model on different datasets and found that the ﬂattened layer can effectively substitute for the 3D ﬁlters without loss of accuracy. The ﬂattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the signiﬁcant reduction of learning param- eters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained.  1  INTRODUCTION  Recent success on fast implementation of convolutional neural networks (CNNs), and new tech- niques such as dropout enable researchers to train large networks that were not possible before. These large CNNs show great promise in visual and audio understanding which make them useful for applications in autonomous robots, security systems, mobile phones, automobiles and wearable supports. These applications require networks with high degree of accuracies, but also networks that can be executed in real-time. However, CNNs are computationally very expensive and require high performance servers or graphics processing units (GPUs). To accelerate forward and backward passes of CNNs, there has been extensive work for efﬁcient implementation of CNNs on GPUs (Krizhevsky et al., 2012; Chetlur et al., 2014) and CPUs (Van- houcke et al., 2011), including linear quantization of network weights and inputs. For mobile plat- forms, like smartphones, computation of these big networks is still demanding and takes place on off-site servers because of their limited computing power and battery life. However, that requires a necessity to a reliable connectivity between the mobile device and off-site servers. Because this is not the case always, custom architectures have been explored for power and speed efﬁcient imple- mentation of CNNs (Jin et al., 2014; Merolla et al., 2014). Another approach to speed up evaluation of CNNs is to reduce the number of parameters in the network representation. The work by Denil et al. (2013) is a good example to show that these networks have high redundancy in them. Considering that state of the art CNNs require hundreds of ﬁlters each layer and consist of three to ﬁve convolutional layers in general, ﬁnding essential representation with smaller parameters brings signiﬁcant performance boost in terms of time and memory. Jaderberg et al. (2014); Denton et al. (2014) exploit the redundancy within convolutional layer after training and could obtain speedup by keeping the accuracy within 1% of the original models. In this work, we take a similar approach to decrease the redundancy of the ﬁlters in convolutional neural networks. We achieve that in the training phase by separating the conventional 3D convo- lution ﬁlters into three consecutive 1D ﬁlters: convolution across channels (lateral), vertical and  1  Accepted as a workshop contribution at ICLR 2015  horizontal direction. We report similar or better accuracies on well-known datasets with the baseline network which has about ten times more parameters.  2 RELATED WORK  Convolutional Neural Networks (CNNs) exhibit high redundancy in the representation expressed as weight and bias parameters. The ﬁlters, visual interpretation of weights, in the network often have similar patterns and some of them have noise rather than distinct features. Having redundancy in the parameters not only degrades learning capacity of networks but accompanies unnecessary computa- tions during feedforward pass as well as backpropagation. Many approaches have been proposed to ﬁnd compact representation of CNNs by applying constraints on cost function or structure. Sparsity in ﬁlters often help accelerate computations of ﬁltering operation by simply skipping com- putations over non-zero values. Sparse feature learning aligned with ﬁndings in V1 neurons is pro- posed by Lee et al. (2007). By iterating L1 and L2 regularizer, this work successfully ﬁnds sparse and essential basis ﬁlters in over-complete system. However, size of the non-zero values in sparse ﬁlters is irregular and these non-zero values are located in arbitrary positions. Due to the arbitrary locations and shapes of sparse ﬁlters, in practice it is difﬁcult to take advantage of sparsity with highly parallelized processing pipelines. A classic but powerful method to accelerate ﬁltering operations is to condition separability on object function (Rigamonti et al., 2013) so as to force the network to learn separable ﬁlters. In the literature, difﬁculties of optimization problem with L1 penalty are relaxed. The separable 2D ﬁlter has a rank of one and makes the operation equivalent to two consecutive 1D convolutions, which signiﬁcantly shortens the evaluation time of CNNs by an order of magnitude. Recent works from Jaderberg et al. (2014); Denton et al. (2014) speed up CNNs evaluation time with low rank ﬁlter approximation. They compress convolutional layer of pre-trained networks by ﬁnding an appropriate low-rank approximation. Denton et al. (2014) extends the method to a large- scale task. Pre-trained 3D ﬁlters are approximated to low rank ﬁlters and the error is minimized by using clustering and post training to tune the accuracy. Their method demonstrates speedup of convolutional layers by a factor of two, while keeping the accuracy within 1% of the original model. Another approach to reduce the parameters in CNNs is to explore the connections between layers. In the structure of state of the art CNNs, all convolutional planes are fully connected. Such con- nection scheme can handle and generalize all possible cases in training but in practice it is hard to learn sparse connectivity in output prediction. The importance of sparse connections in CNNs has been mentioned in the recent work (Szegedy et al., 2014). Also the connectivity previously was investigated by Culurciello et al. (2013), though many issues remain open for further research. We apply structural constraints to conventional CNNs in order to learn 1D separated ﬁlters for feed- forward acceleration. Our method does not alter training procedure of CNNs; backpropagating the error from output to the input along constrained paths. The approach bypasses difﬁculties in op- timization problem witnessed in Rigamonti et al. (2013), but successfully learns 1D convolution ﬁlters. The proposed method does not require any manual tuning or changes in the structure once trained (Jaderberg et al., 2014; Denton et al., 2014), which simplify overall method. The concept of 1D convolution across channels is equivalent to the operation denoted as mlpconv layers in Net- work in Network (Lin et al., 2013). We also use 1 × 1 ﬁlters across channels to increase model discriminability for local patches within the receptive ﬁeld. This layer also determines the number of ﬁlters in the subsequent layers because we add vertical and horizontal 1D ﬁlters which only per- forms convolution per channel. The difference between their and our work is that all of our ﬁlters are one-dimensional which provides signiﬁcant reduction in parameters.  3 FLATTENING CONVOLUTION FILTERS  Similar to the notation used in Denton et al. (2014), weights in CNNs can be described as 4- dimensional ﬁlters: W ∈ RC×X×Y ×F , C is the number of input channels, X and Y are the spatial dimensions of the ﬁlter, and F is the number of ﬁlters or the number of output channels. Convolution  2  Accepted as a workshop contribution at ICLR 2015  (a) 3D convolution  (b) 1D convolutions over different directions  Figure 1: The concept of 3D ﬁlter separation under rank-one assumption in the context of CNNs. Summation over all planes convolved with 2D ﬁlters produces a single output plane, which can be considered as 3D convolution. Three consecutive 1D ﬁltering is an equivalent representation of 3D ﬁlter if its rank is one. C is the number of planes and its value is 3 in the diagram. Y and X denote ﬁlter height and width respectively. Bias is not considered here for simplicity.  C(cid:88)  X(cid:88)  Y(cid:88)  c=1  x(cid:48)=1  y(cid:48)=1  for each channel output requires a ﬁlter W ∈ RC×X×Y and is described as  Ff (x, y) = I ∗ Wf =  I(c, x − x(cid:48), y − y(cid:48))Wf (c, x(cid:48), y(cid:48))  (1)  assuming a stride of one where f is an index of output channel, I ∈ RC×N×M×F is the input map, N and M are the spatial dimensions of the input. A rule of thumb to accelerate multi-dimensional convolution is to apply ﬁlter separation. Under rank-one assumption of the ﬁlter Wf , the unit rank ﬁlter ˆWf can be separated into cross-products of three one-dimensional ﬁlters as follows.  ˆWf = αf × βf × γf  (2) We denote 1D convolution vectors as a lateral ﬁlter αf : convolving features across channels; vertical ﬁlter βf : across Y dimension; horizontal ﬁlter γf : across X dimension. However, separability of ﬁlters is a strong condition and the intrinsic rank of ﬁlter Wf is higher than one in practice. As the difﬁculty of classiﬁcation problem increases, the more number of leading components is required to solve the problem (Montavon et al., 2011). Learned ﬁlters in deep networks have distributed eigenvalues and applying the separation directly to the ﬁlters results in signiﬁcant information loss. Alternatively, we could restrict connections in receptive ﬁelds so that the model can learn 1D sep- arated ﬁlters upon training. When applied to the equation 1, a single layer of convolutional neural networks is modiﬁed to  X(cid:88)   Y(cid:88)  (cid:32) C(cid:88)  x(cid:48)=1  y(cid:48)=1  c=1  (cid:33)   γf (y(cid:48))  ˆFf (x, y) = I ∗ ˆWf =  I(c, x − x(cid:48), y − y(cid:48))αf (c)  βf (x(cid:48))  (3)  With this modiﬁcation number of parameters to calculate each feature map decreases from XY C to X + Y + C, and number of the operations needed decreases from M N CXY to M N (C + X + Y ). Here we deﬁne ﬂattened convolutional networks as CNNs whose one or more convolutional layer is converted to a sequence of 1D convolutions. We did not add the bias terms in the equations above to keep the equations clean. However; bias term is important for the training, and removing the bias terms in some of the 1D ﬁlters results in very slow learning. In our tests, we have separate bias terms for each three 1D ﬁlters.  4 EXPERIMENTAL RESULTS  We tested the performance of the proposed model in alignment with a baseline model of CNNs on different classiﬁcation tasks. In experiments, we used the Torch7 environment (Collobert et al., 2011) to demonstrate model performance as well as to handle customized gradient updates.  3  CxYxXCx1x11xYx11x1xXAccepted as a workshop contribution at ICLR 2015  Figure 2: A single layer structure of ﬂattened convolutional networks. Flattened layer includes l sets of 1D-separated convolutions over channels (lateral, L), vertical (V ) and horizontal (H) direction. In this work, two stages of LV H combinations (l = 2) were chosen by cross-validation and it reported the same accuracy as measured in baseline model. V and H convolutions are operated in full mode to preserve the same output dimension as the baseline model. Bias is added after each of 1D convolution, but skipped in this illustration. No non-linear operator is applied within the ﬂattened layer.  4.1 TRAINING BASELINE MODEL  We choose a CNN model architecture same as the baseline model used in Srivastava & Salakhutdi- nov (2013) with a smaller multilayer perceptron. We keep the structure of CNNs to be generic so as to minimize unwanted interruption from hidden variables and make comparison to ﬂattened model transparent. The model consists of 3 convolutional layers with 5 × 5 ﬁlters and double stage mul- tilayer perceptron. The number convolutional ﬁlters in each layer are 96, 128 and 256 respectively, each layer includes a rectiﬁer linear unit and a max-pooling with sizes of 2 and strides of 3. The two fully-connected multilayer perceptrons have 256 units each. The model is regularized with ﬁve dropout layers with the probability of 0.1, 0.25, 0.25, 0.5 and 0.5 respectively from lower to higher layer in order to prevent co-adaptation of features. In our tests, we did not use data augmentation in order to concentrate learning capacity of models with respect to its structure. The training is initially set up with a learning rate of 0.1 and a momen- tum of 0.9. After the ﬁrst eight epochs, the learning rate is reduced by one tenth. With the vanilla CNN and training conﬁguration we were able to achieve performance comparable to state-of-the-art results on many datasets (see Table 2).  4.2 TRAINING FLATTENED MODEL  In ﬂattened model, we use CNNs constructed with 1D ﬁlters as described in Figure 1. First, lateral ﬁlters (L) perform convolution across channels like mlpconv layers in Network in Network (Lin et al., 2013). Then, each channel is convolved with vertical and horizontal ﬁlters (V and H) whose ﬁlter sizes are Y × 1 and 1× X in space respectively. While mlpconv applies 1D convolution across channels, this work extends 1D convolutions in space as well. Thus, the proposed method can be viewed as a generalization of training with 1D-separated ﬁlters in R3. Once the model structure is deﬁned at the training stage, no post processing or ﬁne-tuning is needed. The structural constraint forces the model to learn 1D separated ﬁlters, equivalently a rank-one 3D ﬁlter, except biases. Replacing ﬁlters with dimension of C × X × Y to ﬁlters dimensions of C, X, Y resulted in 2− 3% accuracy drops in our experiments on different datasets. 1D separated ﬁlters with dimensions of C, X and Y contains only 5% of the parameters as in 3D ﬁlter in commonly used CNNs (Krizhevsky et al., 2012; Sermanet et al., 2013). While Denil et al. (2013) demonstrates that 5% of essential parameters can predict the rest of parameters in the best case, such reduction in parameters ac- companies accuracy loss (Gong et al., 2014; Lebedev et al., 2014). In our 1D training pipeline, parameters in one set of 1D ﬁlters are not enough to distinguish discriminate features, thus, results in failure to recover the target performance. We found that removing one direction from the LV H pipeline caused a signiﬁcant accuracy loss, which implies that convolutions toward all directions are essential.  4  LateralVerticalHorizontaln-th layerinputn-th layeroutputLateralVerticalHorizontalAccepted as a workshop contribution at ICLR 2015  (a) Training accuracy  (b) Testing accuracy  Figure 3: Convergence rate of ﬂattened and baseline models both in training and testing. The struc- ture of baseline and ﬂattened model is speciﬁed in the section 4.1 and the table 1, respectively, and CIFAR-10 dataset is used in this experiment. The solid line denotes a mean and the shade around the line indicates a standard deviation of the curve. The variation of training for the ﬂattened model is too small to appear in the illustration.  the gradients update δxi =(cid:80)K  We cascaded a set of Lateral-Vertical-Horizontal (LV H) convolutional layers to compensate accu- racy loss at the cost of using more parameters. Empirical results show that two cascaded sets of LV H layers achieve the same performance as a standard convolution layer in the baseline model. The ﬂattened layer, consisting of two stages of LV H, is illustrated in Figure 2 and used throughout the experiments. The V and H convolutions within the ﬂattened layer are operated in full mode in order not to lose boundary features and to keep output dimensions the same as the baseline model. With this method, we were able to decrease the number of parameters 8 − 10× compared to that in the baseline model. Different number of LV H layers could be cascaded depending on the difﬁcul- ties of classiﬁcation tasks and the effect of parameter reduction is discussed in the section 4.3. We used the same training conﬁguration of baseline model except decreased weight decay to give more freedom to adapt features. The serialized model with 1D convolutions is more vulnerable to vanishing gradient problem than standard CNNs. Attaching many 1D convolution layer provides more ﬂexibility in ﬁlter shape there- fore helps draw delicate decision boundary. However, longer gradient path experiences more steps of parameter updates and error accumulation, which possibly cause fast decaying gradients. This trend is more persistent for V or H convolution because they get feedback from only one channel. From k=1 wkδxi+1, accumulation of gradients and attenuation by weights are balanced each other in the standard CNNs. However, the ﬂattened structure has generally few connections in H and V convolution. Vanishing gradients can be handled with smart weight initial- ization. Normalized initialization balanced with forward and backward passes (Glorot & Bengio, 2010) or constant error propagation helps deliver gradients to lower layers. In our experiments, they yielded more successful training than the heuristic (LeCun et al., 1998b) while the baseline model with the highest accuracy was trained with the heuristic initialization. With the proper weight initialization, the ﬂattened model achieves the comparable accuracy as the baseline model. Figure 3 reports the training and testing accuracy on the CIFAR-10 dataset. Consid- ering that learning rate for the baseline model is decreased in order to achieve the highest accuracy, the baseline model saturates earlier and to a lower accuracy compared to the ﬂattened model. The shaded region in the plots indicates a variation of accuracies obtained from samples with different random seeds. The learning curve of the ﬂattened model shows consistent results with less variation. The ﬂattened method alleviates the training effort by accelerating backpropagation and is presented in the section 4.6. The ﬁrst layer ﬁlters trained on CIFAR-10 within the ﬂattened structure are reconstructed to 3D ﬁlters and presented in Figure 4 though the ﬁrst layer is not converted in the main experiment (see Section 4.3). Filters are reconstructed by cross-product of 1D ﬁlters trained from two sets of LV H  5  020406080100120140Epoch5060708090100Training Accuracy (%)BaselineFlattened020406080100120140Epoch505560657075808590Testing Accuracy (%)BaselineFlattenedAccepted as a workshop contribution at ICLR 2015  (a) Filters in baseline model  (b) Reconstructed ﬁlters in ﬂattened model  Figure 4: Visualization of the ﬁrst layer ﬁlters trained on CIFAR-10. Filters are reconstructed from cross-product of 1D convolution ﬁlters and contains clear and diverse features. Filters are sorted by variance in descending order and bias is excluded during reconstruction.  convolution layers. The richness of distinct features in the ﬁlters is necessary for model discrim- inability. Surprisingly, the reconstructed ﬁlters have distinct Gabor edges and color blobs. Features have high contrast and edges are sparse as in Lee et al. (2007) without L1 penalty. This ﬁnding sup- ports the effectiveness of the method and explains the comparable performance of proposed model over the baseline model.  4.3 PARAMETER REDUCTION  Flattened model applied to CNNs generally relaxes computational demands by reducing the number of parameters. Here we analyze parameter reduction and its trade-off in practical viewpoint. Flattened convolutional layer used in this work has two stages of LV H convolutions. Corresponding ﬁlter dimensions is F (F + C + 2X + 2Y ) while the regular 3D ﬁlters in the baseline model has dimension of CXY F , where C and F denote the number of input and output channels, X and Y are the spatial dimensions of the ﬁlter. Considering the ﬁlter dimensions are relatively small and ﬁxed throughout layers, our gain for pa- rameter reduction is mostly depend on the ratio between F and C. We denote the ratio k = F/C and we want to ﬂatten a convolutional layer only if k satisﬁes the equation 4  C 2k2 + C 2k + 2C(X + Y ) < C 2XY k  (4)  where the left side denotes the computions required for a ﬂattened layer and the right side for a standard convolution layer. Figure 5 visualizes the relationship between the parameter reduction of ﬂattened network com- pare to baseline network. The ﬂattening method is guaranteed to reduce a large portion of pa- rameters as long as the number of channels in- creases smoothly. Most layers of CNNs can beneﬁt from this method since the number of channels does not decrease over layers and the ratio between channels usually resides between 1 and 3, other than the ﬁrst layer (Krizhevsky et al., 2012; Sermanet et al., 2013; Szegedy et al., 2014; Simonyan & Zisserman, 2014). First layer begins with three channels, so per- forming 96 ﬁlters gives us a ratio of 32 where the baseline model has less parameters. For the Figure 5: Filter dimensions of X = Y = 5 and purpose of parameter reduction, we selectively C = 128 are used to observe reduction efﬁciency. applied ﬂattening to the second and third layers of the baseline CNN though we successfully applied the ﬂattening to the all convolutional layers and  6  01020304050Ratio (#output channels / #input channels)104105106107108The number of parametersBaselineFlattenedAccepted as a workshop contribution at ICLR 2015  achieved the same accuracy. Table 1 summarizes details of two CNN models in terms of the number of parameters.  Table 1: The number of parameters in each layer of ﬂattened and baseline CNN model.  Baseline Model Flattened Model Reduction  Layer 1 Parameters Layer 2 Parameters Layer 3 Parameters  7, 200 307, 200 819, 200  Not applied 1 30, 912 102, 144  0.0% 89.9% 87.5%  4.4 MEMORY USAGE  In this section we compare the memory con- sumption of the baseline and ﬂattened network in training. It is an important concern since memory limit could affect the degree of par- allelism as the model scales up. The ﬂat- tened layer needs to hold all intermediate states for the backward pass whose size is as big as the size of output planes. Considering that each convolution is broken down into N pieces of 1D convolution, the ﬂattened structure pro- duces N − 1 intermediate states that needed to be stored in the memory. This is true for the na¨ıve implementation of convolution as seen in ﬁgure 6 where it uses nested for-loops and is optimized in memory usage. However, BLAS-friendly convolution routine is generally used to achieve the highest perfor- mance in time and adapted in scientiﬁc comput- ing framework (Collobert et al., 2011; Jia et al., 2014; Chetlur et al., 2014). The na¨ıve approach exploits limited parallelism due to its frequent memory access, which throttles the real-time performance of CNNs on many platforms. The BLAS implementation for the baseline model handles convolution as a matrix multiplication at the cost of additional memory copy and space. On the other hand, 1D convolution pipeline in ﬂattened layer can achieve sufﬁcient parallelism without additional resources in feedforward and with little extra memory in backpropagation as opposed to the 3D convolution. Therefore, the ﬂattened layer uses less memory than a baseline convolution layer in practice even though each convolution is broken down into 6 pieces as illustrated in ﬁgure 6.  Figure 6: Theoretical memory usage for a sin- gle ﬂattened layer. Memory usage optimized im- plementation (Na¨ıve) and performance optimized implementation (BLAS) are presented.  4.5 CLASSIFICATION ACCURACY  Despite of the reduced number of parameters, the ﬂattened model does not suffer from accuracy loss. We tested the model on CIFAR-10, CIFAR-100 and MNIST datasets. The CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) are datasets of 32 × 32 RGB images and often used as a standard measurement to evaluate classiﬁcation capability. CIFAR-10 has 10 classes while CIFAR-100 has 100 classes. Both datasets consist of with 50, 000 training and 10, 000 testing images. Before use, datasets are preprocessed with contrast normalization per image fol- lowed by ZCA whitening to the whole dataset as Goodfellow et al. (2013). Both models tend to reach the same performance but in our experiments the accuracy of ﬂattened model outperforms the baseline model slightly as can be seen from Table 2.  1The ﬂattening is not applied to the ﬁrst layer for purpose of parameter reduction based on ﬁgure 5 though  the technique in the ﬁrst layer achieved comparable accuracy as well.  7  16x1632x3248x4864x6480x80Input image sizes020406080100120140160180Theoretical Memory Usage [MByte]Baseline (Naive)Baseline (BLAS)Flattened (Naive)Flattened (BLAS)Accepted as a workshop contribution at ICLR 2015  The MNIST dataset (LeCun et al., 1998a) consists of hand written digits of 0-9. This dataset contains 60, 000 training and 10, 000 testing images. We applied contrast normalization per image as Good- fellow et al. (2013) without ZCA whitening since hand-written images have low cross-correlation values as opposed to CIFAR datasets. The classiﬁcation on MNIST is relatively easier than CIFAR datasets and the accuracies of two models are highly saturated. Whereas it is difﬁcult to compare models because of the simplicity of the dataset, baseline and ﬂattened models give almost the same accuracies.  Table 2: Classiﬁcation accuracy of baseline and ﬂattened model on different datasets  Dataset  Model Type  Test Accuracy  CIFAR-10  Baseline Model Flattened Model  CIFAR-100 Baseline Model Flattened Model  MNIST  Baseline Model Flattened Model  86.42% 87.04%  60.08% 60.92%  99.62% 99.56%  4.6 ACCELERATION  Due to the reduced parameters in 1D convolution pipelines, ﬂattened structure reduces computa- tional demands of CNNs thus accelerate both feedforward and backward computations of CNNs. Proﬁling results of ﬂattened and baseline models for feedforward and backpropagation passes are presented in Figure 7. We ran the same tests on CPU and GPU to check performance of ﬂattened structure versus different degree of parallelism. The performance was measured on Intel i7 3.3GHz CPU and NVIDIA Tesla K-40 GPU. Different image sizes from 16 × 16 to 80 × 80 with 128 batch were applied to the models. We used a single convolution layer for baseline model which has dimen- sions of 128 × 128 × 5 × 5 ﬁlters, and the corresponding 6 pieces of convolution layer for ﬂattened model which has 2 sets of ﬁlters with dimensions of 128 × 128 × 1 × 1, 128 × 128 × 5 × 1 and 128 × 128 × 1 × 5 ﬁlters. Feedforward pass is the part that mostly beneﬁts from the manipulation of 1D convolution ﬁlters. The ﬂattened layer runs about two times faster than the conventional convolutional layer during forward pass on both platforms. The acceleration tends to increase as the size of images gets larger because overhead time becomes negligible for large size of images. We implemented L, V and H convolution routines that are optimized in speed. It is evident that the ﬁlter separation reduces the absolute number of computations, which provides the acceleration. Efﬁcient memory access is another factor for feedforward acceleration. In the 1D convolution pipeline, convolution is processed along one direction only, therefore input pixels can be considered as a 1D image sequence, which minimizes effort in data index and access. The results imply that the ﬂattened structure is more promising if used in the lower layers of CNNs where image size is not small and the ratio between input/output channels increases smoothly. Using ﬂattened layer also reduces training time on CPU and GPU. The backpropagation consists of gradient propagation and parameter update. In the former case, its computation beneﬁts from the coalesced memory access of 1D convolution pipeline as in feedforward. However, the latter requires data accumulation over all pixels at each 1D convolutional layer. Therefore the serial operation with frequent global memory access make the acceleration negligible on GPU.  5 CONCLUSION  In this work, we propose a ﬂattening technique to reduce the number of parameters in convolu- tional neural networks for feedforward acceleration. We convert each of convolutional layer of 3D  8  Accepted as a workshop contribution at ICLR 2015  (a) Feedforward on CPU  (b) Feedforward on GPU  (c) Backpropagation on CPU  (d) Backpropagation on GPU  Figure 7: Speed-up comparison for feedforward and backpropagation execution time measured on CPU and GPU. Input images with 128 batch size were applied to a single ﬂattened layer, which is equivalent to 128-to-128 convolution with 5 × 5 ﬁlter in terms of classiﬁcation accuracy. Flat- tened model runs faster on both platforms and the performance gap increases as the size of image is larger. Proﬁling results were measured on BLAS-accelerated convolution routines for all exper- iments. More optimized CUDA implementation for 1D convolution would shorten execution time on GPU.  convolution pipeline into a sequence of 1D convolutions across channels, vertical and horizontal di- rections in training phase. We found that the model molded in 1D structure is able to learn 1D ﬁlters successfully and achieves about two times speed-up in evaluation compared to the baseline model. Furthermore, with ten times less parameters, the ﬂattened convolutional networks achieve similar or better accuracies on CIFAR-10, CIFAR-100, and MNIST. In addition, the proposed method does not require efforts in manual tuning or post processing once the model is trained and it bypasses the difﬁculties in solving optimization problem to learn 1D ﬁlters. The simple nature of the parameter reduction method could be applied to accelerate a very large-scale model as well and this remains as future work.  ACKNOWLEDGMENTS  This work is supported by Ofﬁce of Naval Research (ONR) grants 14PR02106-01 P00004 and MURI N000141010278. We gratefully appreciate the support of NVIDIA Corporation with the donation of GPUs used for this research.  9  16x1632x3248x4864x6480x80Input image sizes0.00.51.01.52.02.53.0Feedforward runtime [sec]Baseline (CPU)Flattened (CPU)16x1632x3248x4864x6480x80Input image sizes0.000.050.100.150.200.250.300.350.400.45Feedforward runtime [sec]Baseline (GPU)Flattened (GPU)16x1632x3248x4864x6480x80Input image sizes012345678Backpropagation runtime [sec]Baseline (CPU)Flattened (CPU)16x1632x3248x4864x6480x80Input image sizes0.00.20.40.60.81.01.21.41.61.8Backpropagation runtime [sec]Baseline (GPU)Flattened (GPU)Accepted as a workshop contribution at ICLR 2015  REFERENCES Chetlur, Sharan, Woolley, Cliff, Vandermersch, Philippe, Cohen, Jonathan, Tran, John, Catanzaro, Bryan, and Shelhamer, Evan. cudnn: Efﬁcient primitives for deep learning. arXiv preprint arXiv:1410.0759, 2014.  Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement. Torch7: A matlab-like environment  for machine learning. In BigLearn, Neural Information Processing Systems Workshop, 2011.  Culurciello, Eugenio, Jin, Jonghoon, Dundar, Aysegul, and Bates, Jordan. An analysis of the con-  nections between layers of deep neural networks. arXiv preprint arXiv:1306.0152, 2013.  Denil, Misha, Shakibi, Babak, Dinh, Laurent, de Freitas, Nando, et al. Predicting parameters in deep  learning. In Advances in Neural Information Processing Systems, pp. 2148–2156. 2013.  Denton, Emily L, Zaremba, Wojciech, Bruna, Joan, LeCun, Yann, and Fergus, Rob. Exploiting In Advances in Neural  linear structure within convolutional networks for efﬁcient evaluation. Information Processing Systems, pp. 1269–1277. 2014.  Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural networks. In Teh, Yee W. and Titterington, D. M. (eds.), Proceedings of the Thirteenth Interna- tional Conference on Artiﬁcial Intelligence and Statistics (AISTATS-10), volume 9, pp. 249–256, 2010.  Gong, Yunchao, Liu, Liu, Yang, Ming, and Bourdev, Lubomir D. Compressing deep convolutional  networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.  Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.  Maxout networks. arXiv preprint arXiv:1302.4389, 2013.  Jaderberg, Max, Vedaldi, Andrea, and Zisserman, Andrew. Speeding up convolutional neural net-  works with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Jin, Jonghoon, Gokhale, Vinayak, Dundar, Aysegul, Krishnamurthy, Bharadwaj, Martini, Berin, and Culurciello, Eugenio. An efﬁcient implementation of deep convolutional neural networks on a mobile coprocessor. In Circuits and Systems (MWSCAS), 2014 IEEE 57th International Midwest Symposium on, pp. 133–136. IEEE, 2014.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images.  Computer Science Department, University of Toronto, Tech. Rep, 2009.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Lebedev, Vadim, Ganin, Yaroslav, Rakhuba, Maksim, Oseledets, Ivan V., and Lempitsky, Victor S. Speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. arXiv preprint arXiv:1412.6553, 2014.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied  to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998a.  LeCun, Yann, Bottou, L´eon, Orr, Genevieve B., and M¨uller, Klaus-Robert. Efﬁicient backprop. In Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pp. 9–50, London, UK, UK, 1998b. Springer-Verlag. ISBN 3-540-65311-2.  Lee, Honglak, Battle, Alexis, Raina, Rajat, and Ng, Andrew Y. Efﬁcient sparse coding algorithms. In Sch¨olkopf, B., Platt, J.C., and Hoffman, T. (eds.), Advances in Neural Information Processing Systems, pp. 801–808. MIT Press, 2007.  10  Accepted as a workshop contribution at ICLR 2015  Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. arXiv preprint arXiv:1312.4400,  2013.  Merolla, P.A., Arthur, J.V., Alvarez-Icaza, R., Cassidy, A.S., Sawada, J., Akopyan, F., Jackson, B.L., Imam, N., Guo, C., Nakamura, Y., Brezzo, B., Vo, I., Esser, S.K., Appuswamy, R., Taba, B., Amir, A., Flickner, M.D., Risk, W.P., Manohar, R., and Modha, D.S. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science, pp. 668–673, August 2014.  Montavon, Gr´egoire, Braun, Mikio, and M¨uller, Klaus-Robert. Kernel analysis of deep networks.  Journal of Machine Learning Research, 12:2563–2581, 2011.  Rigamonti, R., Sironi, A., Lepetit, V., and Fua, P. Learning separable ﬁlters. In Computer Vision  and Pattern Recognition (CVPR), 2013 IEEE Conference on, pp. 2754–2761, June 2013.  Sermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Micha¨el, Fergus, Rob, and LeCun, Yann. Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229, 2013.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. arXiv preprint arXiv:1409.1556, 2014.  Srivastava, Nitish and Salakhutdinov, Ruslan. Discriminative transfer learning with tree-based pri-  ors. In Advances in Neural Information Processing Systems, pp. 2094–2102. 2013.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.  Vanhoucke, Vincent, Senior, Andrew, and Mao, Mark Z. Improving the speed of neural networks  on CPUs. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2011.  11  ",
1504.02902,2015, Gradual Training Method for Denoising Auto Encoders,"['Gradual Training Method for Denoising Auto Encoders', 'Alexander Kalmanovich and Gal Chechik']",https://arxiv.org/pdf/1504.02902,"5 1 0 2    r p A 1 1         ]  G L . s c [      1 v 2 0 9 2 0  .  4 0 5 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  GRADUAL TRAINING METHOD FOR DENOISING AUTO ENCODERS  Alexander Kalmanovich & Gal Chechik The Gonda Brain Research Center Bar Ilan University 52900 Ramat-Gan, Israel sashakal@gmail.com, gal.chechik@biu.ac.il  ABSTRACT  Stacked denoising auto encoders (DAEs) are well known to learn useful deep rep- resentations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE lay- ers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classiﬁcation error over stacked training on MNIST and CIFAR datasets.  1 GRADUAL TRAINING OF DENOISING AUTOENCODERS  We test here gradual training of deep denoising auto encoders, training the network layer-by-layer, but lower layers keep adapting throughout training. To allow lower layers to adapt continuously, noise is injected at the input level. This training procedure differs from stack-training of auto en- coders (Vincent et al., 2010) More speciﬁcally, in gradual training, the ﬁrst layer of the deep DAE is trained as in stacked training, producing a layer of weights w1. Then, when adding the second layer autoencoder, its weights w2 are tuned jointly with the already-trained weights w1. Given a training sample x, we generate a noisy version ˜x, feed it to the 2-layered DAE, and compute the activation at the subsequent layers h1 = Sigmoid(w(cid:62) 3 h2). Importantly, the loss function is computed over the input x, and is used to update all the weights including w1. Similarly, if a 3rd layer is trained, it involves tuning w1 and w2 in addition to w3 and w(cid:48) 4.  2 h1) and y = Sigmoid(w(cid:48)(cid:62)  1 x), h2 = Sigmoid(w(cid:62)  2 EXPERIMENTAL PROCEDURES  We compare the performance of gradual and stacked training in two learning setups: an unsuper- vised denoising task, and a supervised classiﬁcation task initialized using the weights learned in an unsupervised way. Evaluations were made on three benchmarks: MNIST, CIFAR-10 and CIFAR- 100, but only show here MNIST results due to space constraints. We used a test subset of 10,000 samples and several sizes of training-set all maintaining the uniform distribution over classes. Hyper parameters were selected using a second level of cross validation, including the learning rate, SGD batch size, momentum and weight decay. In the supervised experiments, training was ’early stopped’ after 35 epochs without improvement. The results reported below are averages over 3 train-validation splits. Since gradual training involves updating lower layers, every presentation of a sample involves more weight updates than in a single-layered DAE. To compare stacked and gradual training on a common ground, we limited gradual training to use the same budget of weight update steps as stacked training. For example, when training the second layer for n epochs in gradual training, we allocate 2n training epochs for stacked training (details in the full paper).  1  Accepted as a workshop contribution at ICLR 2015  Figure 1: Unsupervised and supervised training results on MNIST dataset. Er- ror bars are over 3 train- validation splits. Network has 2 hidden layers with 1000 units each (a) Recon- struction error of unsuper- vised training methods mea- sured by cross-entropy loss. The shown cross-entropy er- ror is relative to the mini- mum possible  error, computed as the cross-entropy error of the original uncorrupted test set with itself. All com- pared methods used the same budget of update operations. Images were corrupted with 15% mask- ing noise. The 1st hidden layer is trained for 50 epochs. Total epoch budget for the 2nd hidden layer is 80 epochs. (b) Classiﬁcation error of supervised training initialized based on DAEs. Each curve shows a different pre-training type. Text labels show the percentage of error improvement of Stacked-vs-Gradual 0 pretraining compared to Stacked-vs-Gradual 1 pretraining.  3 RESULTS  We evaluate gradual and stacked training in unsupervised task of image denoising, and then evalu- ated the quality of the two methods for initializing a network in a supervised learning task. Unsupervised learning for denoising. We ﬁrst evaluate gradual training in an unsupervised task of image denoising. Here, the network is trained to minimize a cross-entropy loss over corrupted images. In addition to stacked and gradual training, we also tested a hybrid method that spends some epochs on tuning only the second layer (as in stacked training), and then spends the rest of the training budget on both layers (as in gradual training). We deﬁne the Stacked-vs-Gradual fraction 0 ≤ f ≤ 1 as the fraction of weight updates that occur during stacked-type training. f = 1 is equivalent to pure stacked training while f = 0 is equivalent to pure gradual training. Given a budget of n training epochs, we train the 2nd hidden layer with gradual training for n(1− f ) epochs, and with stacked training for 2nf epochs. More speciﬁcally, since stacked training tunes a single layer of weights and gradual training tunes two layers of weights, we selected the number of stacked epochs s, and the number of gradual epochs g, such that s + 2g = n, and changed several values of s and g to get different ratios f = 1 − g n. Figure 1a shows the test-set cross entropy error when training 2-layered DAEs, as a function of the Stacked-vs-Gradual fraction. Pure gradual training achieved signiﬁcant lower reconstruction error than any mix of stacked and gradual training with the same budget of update steps. Gradual-training DAE for initializing a network in a supervised task. We further tested the beneﬁts of using the DAEs trained in the previous experiment for initializing a deep network in a supervised classiﬁcation task. We initialized the ﬁrst two layers of the deep network with the weights of the SDAE and added a classiﬁcation layer on top with output units matching the classes in the dataset, with randomly initialized weights. To quantify the beneﬁt of gradual unsupervised pretraining we trained these networks on subsets of the training set. Figure. 1b traces the classiﬁcation error as a function of training set size, demon- strating a consistent but small improvement when using gradual training over stacked training (text legends). This effect is mostly relevant for datasets with less than 50K samples. Similar results were obtained using CIFAR10 and CIFAR100.  REFERENCES Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11:3371–3408, 2010.  2         a) Unsupervised Training                                                   b)  Supervised training00.250.510.410.510.610.7Cross entropy% Stacked VS Gradual trainingStacked VS Gradual  0Stacked VS Gradual  0.5Stacked VS Gradual  11K2.5K5K10K20K60K0246810Classification error (%)Number of train cases 17.61% 13.35% 3.63% 6.35% 3.16% 7.80%",
1411.1045,2015, Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet,"['Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet', 'Matthias Kümmerer', 'Lucas Theis', 'and Matthias Bethge']",https://arxiv.org/pdf/1411.1045,"5 1 0 2    r p A 9         ]  V C . s c [      4 v 5 4 0 1  .  1 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  DEEP GAZE I: BOOSTING SALIENCY PREDICTION WITH FEATURE MAPS TRAINED ON IMAGENET  Matthias K¨ummerer, Lucas Theis & Matthias Bethge Werner Reichardt Centre for Integrative Neuroscience University T¨ubingen, Germany {matthias.kuemmerer,lucas,matthias}@bethgelab.org  ABSTRACT  Recent results suggest that state-of-the-art saliency models perform far from op- timal in predicting ﬁxations. This lack in performance has been attributed to an inability to model the inﬂuence of high-level image features such as objects. Re- cent seminal advances in applying deep neural networks to tasks like object recog- nition suggests that they are able to capture this kind of structure. However, the enormous amount of training data necessary to train these networks makes them difﬁcult to apply directly to saliency prediction. We present a novel way of reusing existing neural networks that have been pretrained on the task of object recognition in models of ﬁxation prediction. Using the well-known network of Krizhevsky et al. (2012), we come up with a new saliency model that signiﬁcantly outper- forms all state-of-the-art models on the MIT Saliency Benchmark. The structure of this network allows new insights in the psychophysics of ﬁxation selection and potentially their neural implementation. To train our network, we build on recent work on the modeling of saliency as point processes.  By understanding how humans choose eye ﬁxations, we can hope to understand and explain human behaviour in a number of vision-related tasks. For this reason human eye movements have been studied for more than 80 years (e. g. Buswell, 1935). During the last 20 years, many models have been developed trying to explain ﬁxations in terms of so called “saliency maps”. Recently, it has been suggested to model saliency maps probabilistically using point processes (Barthelm´e et al., 2013) and to evaluate them using log-likelihood (K¨ummerer et al., 2014). This evaluation revealed that state-of-the-art models of saliency explain only one third of the explainable information in the spatial ﬁxation structure (K¨ummerer et al., 2014). Most of the existing models use low-level cues like edge-detectors and color ﬁlters (Itti et al., 1998) or local image statistics (Zhang et al., 2008; Bruce & Tsotsos, 2009). However, human ﬁxations are largely clustered around objects (see Figure 1 for examples). This has led to some models trying to incorporate more high level features: Cerf et al. (2008) combined existing saliency map models with a face detector, while Judd et al. (2009) included detectors for faces, people, cars and horizon. Nevertheless, current saliency models mostly fail to capture these high-level inﬂuences which might be the main reason for the poor overall performance of state-of-the-art models. This analysis raises the question whether there are any computational systems capable of capturing such high-level inﬂuences. Independent of these developments, the last two years have seen the rise of deep neural networks to solve multifarious tasks like object detection, speech recognition or automatic translation. Provided with enough training data, deep neural networks show impressive results, often outperforming all competing methods. It has also been shown that deep convolutional networks that have been opti- mized for object classiﬁcation can be used to predict neuron responses in higher brain areas of the visual system (Yamins et al., 2014; Razavian et al., 2014). Deep neural networks have also proven to generalize well over tasks (Donahue et al., 2013): a network trained for some task like object detection can often be easily retrained to achieve state-of-the-art performance in some other only loosely related task like scene recognition. Motivated by these developments, we here try to use pretrained deep neural networks to model ﬁx- ation selection. The results of Yamins et al. (2014) connect neural network representations with IT  1  Accepted as a workshop contribution at ICLR 2015  h  s n o  i t  a x ﬁ  t i  w e g a m  I  e z a G p e e D  s a b  i  t  r e n e c o n  s a b  i  r e  t  n e c  h  t i  e z a G p e e D  w  Figure 1: Example saliency maps: The top row shows example images from the dataset by Judd et al. (2009). The ﬁxations of the subjects are indicated by dots. The middle row shows the log- densities produced by Deep Gaze I for these images when assuming a uniform prior distribution instead of a center bias. The bottom row shows the log-densities for the same images when using the center bias of the full dataset. Note that only the ﬁrst two images were included in the set of images used to train Deep Gaze I.  and similar neural representations. This suggests that we can hope not only to improve prediction performance, but also to improve our understanding of the internal implementation of ﬁxation selec- tion in the brain by formulating new hypotheses that lead to new experimental paradigms. Finally, results from Zeiler & Fergus (2013) show ways to interpret the ﬁlters of deeper layers in a way that would allow to formulate predictions that can be tested psychophysically. A ﬁrst attempt at modelling saliency with deep convolutional networks has been performed recently by Vig et al. (2014) (eDN), yielding state-of-the-art performance. However, training deep neural networks on ﬁxations suffers from the usually small training sets compared to the training data used in other tasks. To reach their state-of-the-art performance, neural networks trained for object or speech recognition need massive amounts of training data. Most ﬁxation datasets have at most 1000 images with usually not signiﬁcantly more than 100 ﬁxations per image. Deep neural networks can easily have millions of parameters, which would lead to massive overﬁtting on these small datasets. Therefore, eDN uses only three convolutional layers, while the Krizhevsky network uses 5 convolutional layers and the most recent networks used in the ImageNet challenge (ILSVRC2014) use around 20 layers. Here we present a new model of ﬁxation prediction that builds on these results: it uses the well known deep network from Krizhevsky et al. (2012) to generate a high-dimensional feature space, which is then used for the actual ﬁxation prediction. This deep network has been optimized for object recognition using a massive dataset consisting of more than one million images (Deng et al., 2009). Keeping the parameters of the deep network ﬁxed, we train our model on half of the MIT1003 dataset (Judd et al., 2009) and show that it outperforms state-of-the-art models by a large margin, increasing the amount of explained information by 67 %. Furthermore, we analyze how the model exploited the feature space provided by the Krizhevsky network.  1 METHODS  In Figure 2, the model architecture is visualized. After an initial downsampling, the RGB input image is fed into the Krizhevsky network. The Krizhevsky architecture consists of stacked con- volutions, each one followed by a rectiﬁying nonlinearity and optional maxpooling and response normalization. The ﬁnal three fully connected layers of the Krizhevsky network were removed as we are only interested in spatially located features. Each layer (convolution, rectiﬁer, pooling and normalization) results in a single image of response for each ﬁlter in the layer. To predict ﬁxations,  2  Accepted as a workshop contribution at ICLR 2015  Krizhevsky/caffe network  1 v n o c  1 u e r  l  l  1 o o p  1 m r o n  2 v n o c  . . .  5 u e r  l  5 m r o n  (cid:80)  k wkrk(x, y)  rk(x, y)  blur  +  softmax  1 . . .  l l  u f  Figure 2: The model structure of Deep Gaze I: The image is ﬁrst downsampled and preprocessed with the Krizhevsky network. The responses of the layers that are included in the model are then scaled up to the size of the largest network layer and normalized to have unit standard deviation. This list of maps is then linearly combined and blured with a Gaussian kernel. To compensate for the central ﬁxation bias, an estimate of the prior distribution is added. Finally, the model output is fed through a softmax rectiﬁcation, yielding a two dimensional probability distribution.  we ﬁrst select one or multiple layers from the network. We rescale all the response images that we want to include in our model to the size of the largest layer of the network, resulting in a list of up to 3712 responses for each location in an image. Each of these responses is then individually normalized to have unit standard deviation on the full dataset. After this preprocessing, the features are fed into the following model. At leach image location, our saliency model linearly combines the responses rk(x, y) using weights wk. The resulting image is then convoled with a Gaussian kernel whose width is controlled by σ, yielding the saliency map  (cid:88)  s(x, y) =  wkrk(x, y) ∗ Gσ.  It is well known that ﬁxation locations are strongly biased towards the center of an image (Tatler, 2007). To account for this center bias, the saliency prediction is linearly combined with a ﬁxed center bias prediction c(x, y):  k  o(x, y) = αc(x, y) + s(x, y)  To predict ﬁxation probabilities, this output is ﬁnally fed into a softmax, yielding a probability distribution over the image:  For generalization, (cid:96)1-regularization on the weights is used to encourage sparsity. For training ﬁxa- tions (x1, y1), . . . , (xN , yN ) this yields the cost function  p(x, y) =  exp (o(x, y)) x,y exp (o(x, y))  c(µ, α, w) = − 1 N  log p(xi, yi) + λ  (cid:107)w(cid:107)1 (cid:107)w(cid:107)2  (cid:80)  N(cid:88)  i  3  Accepted as a workshop contribution at ICLR 2015  To quantify which layers help most in predicting the ﬁxations and lead to least overﬁtting, we trained models on a variety of subsets of layers (see subsection 2.3 and Figure 5). We checked the general- ization performance of these models on the remaining 540 images from MIT1003 that have not been used in training. As performance measure we use shufﬂed area under the curve (shufﬂed AUC) here (Tatler et al., 2005). In AUC, the saliency map is treated as a classiﬁer score to separate ﬁxations from “nonﬁxations”: presented with two locations in the image, the classiﬁer chooses the location with the higher saliency value as ﬁxation. The AUC measures the classiﬁcation performance of this classifer. The standard AUC uses a uniform nonﬁxation distribution, while in the case of shufﬂed AUC, ﬁxations from other images are used as nonﬁxations. As shufﬂed AUC assumes the saliency maps not include the biases of the prior distribution (see Barthelm´e et al., 2013) we had to use a uniform center bias for this evaluation.  1.1  IMPLEMENTATION DETAILS  For training, we used roughly half of the dataset MIT1003 (Judd et al., 2009). By using only the images of the most common size of 1024 × 768 pixels (resulting in 463 images), we were able to use the nonparametric estimate of the center bias described in K¨ummerer et al. (2014) (mainly a 2d histrogram distribution ﬁtted using the ﬁxations from all other images). Our implementation of the Krizhevsky network uses the architecture and trained ﬁlters as published by Jia et al. (2014) with the following modiﬁcations: the original architecture uses a ﬁxed input size of 224 × 224. As we removed the fully connected layers, we do not need to restrict to a ﬁxed input size but can feed arbitrary images into the network. Furthermore we use convolutions of type full (i.e. zero-pad the input) instead of valid which would result in convolution outputs that are smaller than the input. This modiﬁcation is useful, because we need saliency predictions for every point in the image. Note that the caffe implementation of the Krizhevsky network differs slightly from the original architecture in Krizhevsky et al. (2012), as the pooling and the normalization layers have been switched. The subsampling factor for the inital downsampling of the images was set to 2. The sparsity parameter λ was chosen using grid search and turned out to be 0.001 in the ﬁnal model. However, even setting it to much smaller values did have very little effect on training and test performance (see subsection 6.1 for more details). All calculations of log-likelihoods, cost functions and gradients were done in theano (Bergstra et al., 2010). To minimize the cost function on the training set of ﬁxations, the mini-batch based BFGS method as described in Sohl-Dickstein et al. (2014) was used. It combines the beneﬁts of batch based methods with the advantage of second order methods, yielding high convergence rates with next to no hyperparameter tuning. To avoid overﬁtting to the subjects, leave-one-out cross-validation over the 15 subjects contained in the database was used. The code for our model including training and analysis will be published at http://www. bethgelab.org/code/deepgaze/.  2 RESULTS  2.1 PERFORMANCE RESULTS  We use an information theoretic measure to evaluate our model: log-likelihood. Log-likelihood is a principled measure for probabilistic models and has numerous advantages. See K¨ummerer et al. (2014) for an extensive discussion. Log-likelihoods are much easier to understand when expressed as difference of log-likelihood rel- ative to a baseline model. This information gain1 expresses how much more efﬁcient the model is in describing the ﬁxations than the baseline model: if a model with an information gain of 1 bit/ﬁx is used to encode ﬁxation data, it can save on average one bit per ﬁxation compared to the baseline model. The information gain is even more intuitive when compared to the explainable information gain, i.e., the information gain of the real distribution compared to the baseline model. This comparison yields a ratio of explained information gain to explainable information gain which will be called  1To be more precise, this value is an estimated expected information gain  4  Accepted as a workshop contribution at ICLR 2015  Figure 3: Performance of Deep Gaze I compared to a list of other inﬂuential models, expressed as the ratio of explained information (see text for details). All models except for Deep Gaze I have been postprocessed to account for a pointwise nonlinearity, center bias and blurring (see K¨ummerer et al. (2014) for details).  “explainable information gain explained” or just “information gain explained” in the following. See K¨ummerer et al. (2014) for a more thorough explanation of this notion. The baseline model is a non-parametric model of the image-independent prior distribution p(x, y), while the explainable information is estimated using a non-parametric model of the ﬁxation distri- bution p(x, y | I) for a given image I (which we call the gold standard model). The gold standard model is cross-validated between subjects and thus captures all the structure in the ﬁxations that is purely due to the spatial structure of the image. See K¨ummerer et al. (2014) for details on the baseline model and the gold standard model. By expressing the information gain of a model as a percentage of the possible information gain, we can asses how far we have come in describing the ﬁxations. It is important to note that this interpretation is only possible due to the fact that information gain is on a ratio scale (Michell, 1997): differences and ratios of information gains are meaningful – opposed to other measures like AUC. In Figure 3, the percentage of information gain explained is plotted for our model in comparison to a range of inﬂuential saliency models, including the state-of-the-art models. Of the possible information gain, the best existing model (eDN) is able to explain only 34 %. Deep Gaze I is able to increase this information gain to 56 %.  2.2 RESULTS ON MIT SALIENCY BENCHMARK  We submitted our model to the MIT Saliency Benchmark (Bylinskii et al.). The benchmark evaluates saliency models on a dataset of 300 images and 40 subjects. The ﬁxations are not available to make training for these ﬁxations impossible. The MIT Saliency Benchmark evaluates models on a variety of metrics, including AUC with uniform nonﬁxation distribution and shufﬂed AUC (i.e. AUC with center bias as nonﬁxation distribution). The problem with these metrics is that most of them use different deﬁnitions of saliency maps. This hold especially for the two most used performance metrics: AUC and shufﬂed AUC. While AUC expects the saliency maps to model the center bias, shufﬂed AUC explicitly does not so and penalizes models that do (see Barthelm´e et al. (2013) for details). As Deep Gaze I uses an explicit representation of the prior distribution, it is straightforward to produce the saliency maps according to both deﬁnitions of AUC: For AUC we use a nonparametric prior estimate, for shufﬂed AUC we use a uniform prior distribution. As the images of the dataset are of different size, we could not use our non-parametric center bias as is. Instead, we took all ﬁxations from the full MIT-1003 dataset and transformed their position to be relative to a image of size 100 × 100. Then we trained a Gaussian kernel density estimator on these ﬁxations. This density estimate was then rescaled and renormalized for each image. Doing so, we beat the state-of-the-art models in the MIT Saliency Benchmark by a large margin in AUC as well as shufﬂed AUC (see Figure 4): For shufﬂed AUC, we reach 71.69% compared to 67.90% for the best performing model AWS (center bias is at 50%). For AUC we reach 84.40%  5  03456100information gain explainedBaselineIttiKochKienzleCovSalHouZhangSUN, origGBVSIttiKoch2Context AwareTorralbaJuddSUN, optimRAREAIMBMSeDNDeep Gaze IGold standardAccepted as a workshop contribution at ICLR 2015  a)  b)  Figure 4: Performance results on the MIT benchmark. (a): Shufﬂed AUC performance of Deep Gaze I (green bar, 71.69%) compared with all other models in the MIT benchmark. The x-axis is at the level of the center bias model. The three top performing models after Deep Gaze I are in order of decreasing performance: AWS (67.90%, Garcia-Diaz et al. (2012)), RARE2012 (66.54%, Riche et al. (2013)), and AIM (65.64%, Bruce & Tsotsos (2009)). (b) AUC performance of Deep Gaze I (green bar, 84.40%) compared with all other models in the MIT benchmark that performed better than the center bias. The x-axis is at the level of the center bias model. The three top performing models after Deep Gaze I are in order of decreasing performance: BMS (82.57%, Zhang & Sclaroff (2013)), Mixture of Saliency Models (82.09%, Han and Satoh, 2014), and eDN (81.92%, Vig et al. (2014)). Notice that AUC and shufﬂed AUC use different deﬁnitions of saliency map: While AUC expects the saliency maps to model the center bias, shufﬂed AUC explicitly does not and penalizes models that do. Therefore, for the shufﬂed AUC performances of Deep Gaze I the saliency maps have been calculated with a uniform prior distribution, while for the AUC performances the saliency maps have been calculated with a nonparametric prior (see text for details) 2. Performances of other models from the MIT benchmark as of September 2014.  compared to 82.57% for the best performing model BMS (center bias is at 78.31%). Relative to the center bias, this is an increase of AUC performance by more than 40%.  2.3 LAYER SELECTION  The ﬁnal model used only the convolutions of the top-most layer of the Krizhevsky-architecture. This is a principled choice: the top layer can be expected to include most high-level inﬂuences and the relu, pool and norm units are often viewed mainly as the nonlinearities needed to provide a new feature space for the next level of convolutions. But this choice was also backed by a series of comparison models where more or other layers have been included in the model: In Figure 5, performance results are reported for models including layers from a given depth upwards (Figure 5a), layers up to a given depth (Figure 5b), layers of a given depth (Figure 5c) and layers of a given type (Figure 5d). It can be seen that the architecture chosen ﬁnally (layer 5 convolutions) generalizes best to the images of the test set in terms of shufﬂed AUC. It is also worth noting that models including more layers are substantially better at predicting the test subjects ﬁxations on the images used in training (Figure 5a, left plot): when using all layers, a performance of 83% information gain explained is reached for the test subjects. This suggests that the generalization problems of these models are not due to intersubject variability. They most prob- ably suffer from the fact that the variety of objects in the training images is not rich enough, leading to overﬁtting to the images (not to the subjects). Therefore we can expect improved performance from using a larger set of images in training.  2Note that the MIT Saliency Benchmark webpage reports only performances for the saliency maps with the  nonparametric prior. Therefore, there the shufﬂed AUC performance is lower.  6  55606570shuffled AUC [%]79808182838485AUC [%]Deep Gaze IBMSMoSMeDNAccepted as a workshop contribution at ICLR 2015  a)  b)  c)  d)  Figure 5: Performance of Deep Gaze I when trained on different subsets of the Krizhevsky layers: (a): Results for models that use layers from a given depth upwards. The left plot shows the percent- age of explainable information gain explained on the images used in training for training subjects and test subjects (refer to subsection 2.1 for an explanation of this measure). The dotted line indi- cates the performance of the model we used in the MIT Saliency Benchmark (which only used the output of the convolutions of layer 5). The right plot shows the shufﬂed AUC on the images used in training and on the remaining test images. Here, the models have been averaged over all test subjects and the saliency maps assume uniform center bias, as expected by shufﬂed AUC (see subsection 2.2 for details). The dotted line indicates the performance of the ﬁnal model on the test images. (b), (c), (d): Results for models that use layers up to a given depth (b), layers of a certain depth (c) and layers of a certain type (d). The plots are as in (a).  2.4 ANALYSIS OF USED FEATURES  In this section we analyze which features of the Krizhevsky architecture contributed most to the ﬁxation predictions. By getting a solid understanding of the involved features, we can hope to extract predictions from the model that can be tested psychophysically in the future. In Figure 6, we took the 10 most weighted features from the 256 convolution features in layer 5. For each of these 10 features, we plotted the 9 patches from the dataset that led to the highest response (resp. lowest response for features with negative weight). In Figure 7, the ﬁrst four patches of the ﬁrst four features are shown in more detail: The patches are shown in the context of the entire image and also the feature’s response to this image is shown. Clearly, the most important feature is sensitive to faces. The second most important feature seems to respond mainly to text. The third most important feature shows some sort of pop-out response: it seems to respond to whichever feature sticks out from an image: the sign of a bar in the ﬁrst patch, two persons in a desert in the second patch and, most notably, the target in a visual search image in the fourth patch. Note that the salient feature depends heavily on the image context, so that a simple luminance or color contrast detector would not achieve the same effect.  7  layer 1­5layer 2­5layer 3­5layer 4­5layer 5layer 5 (conv)020406080100information gain explained [%]trainingtestlayer 1­5layer 2­5layer 3­5layer 4­5layer 5layer 5 (conv)0.600.650.700.750.80sROC [%]training imagestest imageslayer 1layer 1­2layer 1­3layer 1­4layer 1­5020406080100information gain explained [%]trainingtestlayer 1layer 1­2layer 1­3layer 1­4layer 1­50.600.650.700.750.80sROC [%]training imagestest imageslayer1layer2layer3layer4layer5020406080100information gain explained [%]trainingtestlayer1layer2layer3layer4layer50.600.650.700.750.80sROC [%]training imagestest imagesconvrelupoolnorm020406080100information gain explained [%]trainingtestconvrelupoolnorm0.600.650.700.750.80sROC [%]training imagestest imagesAccepted as a workshop contribution at ICLR 2015  1.00  0.85  0.76  0.72  0.72  0.72  0.66  0.65  Figure 6: Analysis of used features I: (a) Patches of maximum response: Each square of patches shows for a speciﬁc feature of the Krizhevsky architecture the nine patches that led to highest re- sponse (resp. smallest response, if the feature has a negative weight in the model). Each patch corresponds to exactly the part of the image that contributes to the response in the location of maxi- mum response. The features used have been choosen by the absolute value of the weight that Deep Gaze I assigned to them. The numbers over the patches show |wk|/ maxk |wk|.  This shows that Deep Gaze I is not only able to capture the inﬂuence of high level objects like faces or text, but also more abstract high-level concepts (like popout).  3 DISCUSSION  Deep Gaze I was able to increase the explained information gain to 56 % compared to 34 % for state of the art models. On the MIT Saliency Benchmark we were also able to beat the state of the art models by a substantial margin. One main reason for this performance is the ability of our model to capture the inﬂuence of several high-level features like faces and text but also more abstract ones like popout (2.4). It is important to note that all reported results from Deep Gaze I are direct model performances, with- out any ﬁtting of a pointwise nonlinearity as performed in K¨ummerer et al. (2014). This indicates that the deep layers provide a sufﬁciently rich feature space to enable ﬁxation prediction via simple linear combination of the features. The convolution responses turned out to be most informative about the ﬁxations. While features trained on ImageNet have been shown to generalize to other recognition and detection tasks (e. g. Donahue et al., 2013; Razavian et al., 2014), to our knowledge this is the ﬁrst work where ImageNet features have been used to predict behaviour. Extending state-of-the-art neural networks with attention is an exciting new direction of research (Tang et al., 2014; Mnih et al., 2014). Humans use attention for efﬁcient object recognition and we showed that Krizhevsky features work well for predicting human attention. Therefore it is likely that these networks could be brought closer to human performance by extending them with Krizhevsky features. This could be an interesting ﬁeld for future research.  4 CONCLUSIONS  Our contribution in this work is twofold: First, we have shown that deep convolutional networks that have been trained on computer vision tasks like object detection boost saliency prediction.  8  Accepted as a workshop contribution at ICLR 2015  (a)  (b)  (c)  (d)  Figure 7: Analysis of used features II: Details for some of the patches from Figure 6 The four double columns (a) to (d) correspond to the ﬁrst four features shown Figure 6. In each double column, the four rows correspond to the ﬁrst four patches shown for this feature in Figure 6. The left column of each double column shows the patches in the context of the full image, while the feature’s response over the full image is shown in the right column. The position of the maximum is indicated by a dot.  Using the well-known Krizhevsky network (Krizhevsky et al., 2012), we were able to outperform state-of-the-art saliency models by a large margin, increasing the amount of explained information by 67 % compared to state-of-the art. We believe this approach will enable the creation of a new generation of saliency models with high predictive power and deep implications for psychophysics and neuroscience (Yamins et al., 2014; Zeiler & Fergus, 2013). An obvious next step suggested by this approach is to replace the Krizhevsky network by the ImageNet 2014 winning networks such as VGG (Simonyan & Zisserman, 2014) and GoogLeNet (Szegedy et al., 2014). A second conceptual contribution of this work is to optimize the saliency model by maximizing the log-likelihood of a point process (see Barthelm´e et al., 2013; K¨ummerer et al., 2014).  9  Accepted as a workshop contribution at ICLR 2015  We believe that the combination of high performance feature spaces for object recognition as ob- tained from the ImageNet benchmark with principled maximum likelihood learning opens the door for a “Deep Gaze” program towards explaining all the explainable information in the spatial image- based ﬁxation structure.  5 ACKNOWLEDGEMENTS  This work was mainly supported by the German Research Foundation (DFG; priority program 1527, Sachbeihilfe BE 3848-1) and additionally by the German Ministry of Education, Science, Research and Technology through the Bernstein Center for Computational Neuroscience (FKZ 01GQ1002) and the German Excellency Initiative through the Centre for Integrative Neuroscience T¨ubingen (EXC307).  REFERENCES Barthelm´e, Simon, Trukenbrod, Hans, Engbert, Ralf, and Wichmann, Felix. Modelling ﬁxation  locations using spatial point processes. Journal of Vision, 13(12), 2013. doi: 10.1167/13.12.1.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Bruce, Neil DB and Tsotsos, John K. Saliency, attention, and visual search: An information theoretic  approach. Journal of vision, 9(3), 2009.  Buswell, Guy Thomas. How people look at pictures. University of Chicago Press Chicago, 1935.  Bylinskii, Zoya, Judd, Tilke, Durand, Fr´edo, Oliva, Aude, and Torralba, Antonio. Mit saliency  benchmark. http://saliency.mit.edu/.  Cerf, Moran, Harel, Jonathan, Einhaeuser, Wolfgang, and Koch, Christof. Predicting human gaze using low-level saliency combined with face detection. In Platt, J.C., Koller, D., Singer, Y., and Roweis, S.T. (eds.), Advances in Neural Information Processing Systems 20, pp. 241–248. Curran Associates, Inc., 2008. URL http://papers.nips.cc/paper/ 3169-predicting-human-gaze-using-low-level-saliency-combined-with-face-detection. pdf.  Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248–255. IEEE, 2009.  Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013.  Garcia-Diaz, Ant´on, Lebor´an, V´ıctor, Fdez-Vidal, Xos´e R, and Pardo, Xos´e M. On the relationship between optical variability, visual saliency, and eye ﬁxations: A computational approach. Journal of vision, 12(6):17, 2012.  Itti, Laurent, Koch, Christof, and Niebur, Ernst. A model of saliency-based visual attention for rapid scene analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20(11): 1254–1259, 1998. doi: 10.1109/34.730558.  Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em- bedding. arXiv preprint arXiv:1408.5093, 2014.  Judd, Tilke, Ehinger, Krista, Durand, Fr´edo, and Torralba, Antonio. Learning to predict where humans look. In Computer Vision, 2009 IEEE 12th international conference on, pp. 2106–2113. IEEE, 2009.  10  Accepted as a workshop contribution at ICLR 2015  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  K¨ummerer, M., Wallis, T., and Bethge, M. How close are we to understanding image-based saliency?  arXiv preprint arXiv:1409.7686, Sep 2014. URL http://arxiv.org/abs/1409.7686.  Michell, Joel. Quantitative science and the deﬁnition of measurement in psychology. British Journal  of Psychology, 88(3):355–383, 1997.  Mnih, Volodymyr, Heess, Nicolas, Graves, Alex, et al. Recurrent models of visual attention. In  Advances in Neural Information Processing Systems, pp. 2204–2212, 2014.  Razavian, Ali Sharif, Azizpour, Hossein, Sullivan, Josephine, and Carlsson, Stefan. Cnn features off-the-shelf: an astounding baseline for recognition. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pp. 512–519. IEEE, 2014.  Riche, Nicolas, Mancas, Matei, Duvinage, Matthieu, Mibulumukini, Makiese, Gosselin, Bernard, and Dutoit, Thierry. RARE2012: A multi-scale rarity-based saliency detection with its com- parative statistical analysis. Signal Processing: Image Communication, 28(6):642–658, July 2013. ISSN 09235965. doi: 10.1016/j.image.2013.03.009. URL http://linkinghub. elsevier.com/retrieve/pii/S0923596513000489.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.  Sohl-Dickstein, Jascha, Poole, Ben, and Ganguli, Surya. An adaptive low dimensional quasi-newton  sum of functions optimizer. In International Conference on Machine Learning, 2014.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. URL http://arxiv.org/abs/1409.4842.  Tang, Yichuan, Srivastava, Nitish, and Salakhutdinov, Ruslan R. Learning generative models with  visual attention. In Advances in Neural Information Processing Systems, pp. 1808–1816, 2014.  Tatler, Benjamin W. The central ﬁxation bias in scene viewing: Selecting an optimal viewing po- sition independently of motor biases and image feature distributions. Journal of Vision, 7(14):4, 2007. doi: 10.1167/7.14.4.  Tatler, Benjamin W., Baddeley, Roland J., and Gilchrist, Iain D. Visual correlates of ﬁxation selec- tion: effects of scale and time. Vision Research, 45(5):643 – 659, 2005. ISSN 0042-6989. doi: http://dx.doi.org/10.1016/j.visres.2004.09.017. URL http://www.sciencedirect.com/ science/article/pii/S0042698904004626.  Vig, Eleonora, Dorr, Michael, and Cox, David. Large-scale optimization of hierarchical features In Computer Vision and Pattern Recognition, 2014.  for saliency prediction in natural images. CVPR’14. IEEE Conference on. IEEE, 2014.  Yamins, Daniel LK, Hong, Ha, Cadieu, Charles F, Solomon, Ethan A, Seibert, Darren, and DiCarlo, James J. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the National Academy of Sciences, pp. 201403112, 2014.  Zeiler, Matthew D and Fergus, Rob. Visualizing and understanding convolutional neural networks.  arXiv preprint arXiv:1311.2901, 2013.  Zhang, Jianming and Sclaroff, Stan. Saliency detection: a boolean map approach. In Computer  Vision (ICCV), 2013 IEEE International Conference on, pp. 153–160. IEEE, 2013.  Zhang, Lingyun, Tong, Matthew H, Marks, Tim K, Shan, Honghao, and Cottrell, Garrison W. Sun:  A bayesian framework for saliency using natural statistics. Journal of Vision, 8(7), 2008.  11  Accepted as a workshop contribution at ICLR 2015  Figure 8: Performance of Deep Gaze I when trained on the conv5-layer with different regularization parameters. The left plot shows the percentage of explainable information gain explained on the images used in training for training subjects and test subjects (refer to subsection 2.1 for an expla- nation of this measure). The dotted line indicates the performance of the model we used in the MIT Saliency Benchmark (λ = 0.001). The right plot shows the shufﬂed AUC on the images used in training and on the remaining test images. Here, the models have been averaged over all test subjects and the saliency maps assume uniform center bias, as expected by shufﬂed AUC (see subsection 2.2 for details). The dotted line indicates the performance of the ﬁnal model on the test images.  6 SUPPLEMENTARY MATERIAL  6.1 REGULARIZATION  The model uses a regularization parameter λ to encourage sparsity in the feature weights (see sec- tion 1). This parameter was choosen using grid search. In Figure 8, training and test performances are shown for different choices of λ when ﬁtting the model using only the ﬁnal convolutional layer (as done in the ﬁnal model). It can be seen that the choice of the regularization parameter had a visible but only very small effect on the test performance (especially if compared to the inﬂuences of the different layers used, see Figure 5).  12  =======020406080100information gain explained [%]trainingtest=======0.600.650.700.750.80sROC [%]training imagestest images",
1412.7525,2015, Difference Target Propagation,"['Difference Target Propagation', 'Dong-Hyun Lee', 'Saizheng Zhang', 'Asja Fischer', 'Antoine Biard', 'and Yoshua Bengio']",https://arxiv.org/pdf/1412.7525,"5 1 0 2     v o N 5 2         ]  G L . s c [      5 v 5 2 5 7  .  2 1 4 1 : v i X r a  Difference Target Propagation  Dong-Hyun Lee1, Saizheng Zhang1, Asja Fischer1, and  Yoshua Bengio1,2  1 Universit´e de Montr´eal, Quebec, Canada  2 CIFAR Senior Fellow  Abstract. Back-propagation has been the workhorse of recent successes of deep learning but it relies on inﬁnitesimal effects (partial derivatives) in order to per- form credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of non- linearity where the relation between parameters and cost is actually discrete. In- spired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradi- ents, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with dis- crete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks. 3  1  Introduction  Recently, deep neural networks have achieved great success in hard AI tasks [2, 11, 13, 18], mostly relying on back-propagation as the main way of performing credit as- signment over the different sets of parameters associated with each layer of a deep net. Back-propagation exploits the chain rule of derivatives in order to convert a loss gra- dient on the activations over layer l (or time t, for recurrent nets) into a loss gradient on the activations over layer l − 1 (respectively, time t − 1). However, as we consider deeper networks– e.g., consider the recent best ImageNet competition entrants [19] with 19 or 22 layers – longer-term dependencies, or stronger non-linearities, the com- position of many non-linear operations becomes more strongly non-linear. To make this concrete, consider the composition of many hyperbolic tangent units. In general, this  3 This paper was accepted in ECML/PKDD 2015. Please cite like following : Dong-Hyun Lee, Saizheng Zhang, Asja Fischer and Yoshua Bengio, Difference Target Propagation, in Machine Learning and Knowledge Discovery in Databases, pages 498-515, Springer International Pub- lishing, 2015  2  D-H. Lee, S. Zhang, A. Fischer and Y. Bengio  means that derivatives obtained by back-propagation are becoming either very small (most of the time) or very large (in a few places). In the extreme (very deep compu- tations), one would get discrete functions, whose derivatives are 0 almost everywhere, and inﬁnite where the function changes discretely. Clearly, back-propagation would fail in that regime. In addition, from the point of view of low-energy hardware implemen- tation, the ability to train deep networks whose units only communicate via bits would also be interesting.  This limitation of back-propagation to working with precise derivatives and smooth networks is the main machine learning motivation for this paper’s exploration into an alternative principle for credit assignment in deep networks. Another motivation arises from the lack of biological plausibility of back-propagation, for the following reasons: (1) the back-propagation computation is purely linear, whereas biological neurons in- terleave linear and non-linear operations, (2) if the feedback paths were used to prop- agate credit assignment by back-propagation, they would need precise knowledge of the derivatives of the non-linearities at the operating point used in the corresponding feedforward computation, (3) similarly, these feedback paths would have to use exact symmetric weights (with the same connectivity, transposed) of the feedforward connec- tions, (4) real neurons communicate by (possibly stochastic) binary values (spikes), (5) the computation would have to be precisely clocked to alternate between feedforward and back-propagation phases, and (6) it is not clear where the output targets would come from.  The main idea of target propagation is to associate with each feedforward unit’s activation value a target value rather than a loss gradient. The target value is meant to be close to the activation value while being likely to have provided a smaller loss (if that value had been obtained in the feedforward phase). In the limit where the target is very close to the feedforward value, target propagation should behave like back-propagation. This link was nicely made in [15, 16], which introduced the idea of target propagation and connected it to back-propagation via a Lagrange multipliers formulation (where the constraints require the output of one layer to equal the input of the next layer). A similar idea was recently proposed where the constraints are relaxed into penalties, yielding a different (iterative) way to optimize deep networks [9]. Once a good target is computed, a layer-local training criterion can be deﬁned to update each layer separately, e.g., via the delta-rule (gradient descent update with respect to the cross-entropy loss).  By its nature, target propagation can in principle handle stronger (and even discrete) non-linearities, and it deals with biological plausibility issues (1), (2), (3) and (4) de- scribed above. Extensions of the precise scheme proposed here could handle (5) and (6) as well, but this is left for future work.  In this paper, we describe how the general idea of target propagation by using auto-encoders to assign targets to each layer (as introduced in an earlier technical re- port [4]) can be employed for supervised training of deep neural networks (section 2.1 and 2.2). We continue by introducing a linear correction for the imperfectness of the auto-encoders (2.3) leading to robust training in practice. Furthermore, we show how the same principles can be applied to replace back-propagation in the training of auto-encoders (section 2.4). In section 3 we provide several experimental results on rather deep neural networks as well as discrete and stochastic networks and auto-  Difference Target Propagation  3  encoders. The results show that the proposed form of target propagation is comparable to back-propagation with RMSprop [21] - a very popular setting to train deep networks nowadays- and achieves state of the art for training stochastic neural nets on MNIST.  2 Target Propagation  Although many variants of the general principle of target propagation can be devised, this paper focuses on a speciﬁc approach, which is based on the ideas presented in an earlier technical report [4] and is described in the following.  2.1 Formulating Targets  Let us consider an ordinary (supervised) deep network learning process, where the train- ing data is drawn from an unknown data distribution p(x, y). The network structure is deﬁned by  hi = fi(hi−1) = si(Wihi−1), i = 1, . . . , M  (1) where hi is the state of the i-th hidden layer (where hM corresponds to the output of the network and h0 = x) and fi is the i-th layer feed-forward mapping, deﬁned by a non-linear activation function si (e.g. the hyperbolic tangents or the sigmoid function) and the weights Wi of the i-th layer. Here, for simplicity of notation, the bias term of the i-th layer is included in Wi. We refer to the subset of network parameters deﬁning the mapping between the i-th and the j-th layer (0 ≤ i < j ≤ M) as θi,j W = {Wk, k = i + 1, . . . , j}. Using this notion, we can write hj as a function of hi depending on parameters θi,j  W , that is we can write hj = hj(hi; θi,j W ).  Given a sample (x, y), let L(hM (x; θ0,M  W ), y) be an arbitrary global loss function measuring the appropriateness of the network output hM (x; θ0,M W ) for the target y, e.g. the MSE or cross-entropy for binomial random variables. Then, the training ob- jective corresponds to adapting the network parameters θ0,M W so as to minimize the expected global loss Ep{L(hM (x; θ0,M W )y)} under the data distribution p(x, y). For i = 1, . . . , M − 1 we can write L(hM (x; θ0,M  W ), y) = L(hM (hi(x; θ0,i  W ); θi,M  W ), y)  (2)  to emphasize the dependency of the loss on the state of the i-th layer.  Training a network with back-propagation corresponds to propagating error signals through the network to calculate the derivatives of the global loss with respect to the parameters of each layer. Thus, the error signals indicate how the parameters of the net- work should be updated to decrease the expected loss. However, in very deep networks with strong non-linearities, error propagation could become useless in lower layers due to exploding or vanishing gradients, as explained above.  To avoid this problems, the basic idea of target propagation is to assign to each W ) a nearby value ˆhi which (hopefully) leads to a lower global loss, that is  hi(x; θ0,i which has the objective to fulﬁll  L(hM (ˆhi; θi,M  W ), y) < L(hM (hi(x; θ0,i  W ); θi,M  W ), y) .  (3)  4  D-H. Lee, S. Zhang, A. Fischer and Y. Bengio  Such a ˆhi is called a target for the i-th layer.  Given a target ˆhi we now would like to change the network parameters to make hi move a small step towards ˆhi, since – if the path leading from hi to ˆhi is smooth enough – we would expect to yield a decrease of the global loss. To obtain an update direction for Wi based on ˆhi we can deﬁne a layer-local target loss Li, for example by using the MSE  Li(ˆhi, hi) = ||ˆhi − hi(x; θ0,i  (4) Then, Wi can be updated locally within its layer via stochastic gradient descent, where ˆhi is considered as a constant with respect to Wi. That is  W )||2 2 .  W (t+1)  i  = W (t)  i − ηfi  ∂Li(ˆhi, hi)  ∂Wi  = W (t)  i − ηfi  ∂Li(ˆhi, hi)  ∂hi(x; θ0,i W )  ∂hi  ∂Wi  ,  (5)  where ηfi is a layer-speciﬁc learning rate.  Note, that in this context, derivatives can be used without difﬁculty, because they correspond to computations performed inside a single layer. Whereas, the problems with the severe non-linearities observed for back-propagation arise when the chain rule is applied through many layers. This motivates target propagation methods to serve as alternative credit assignment in the context of a composition of many non-linearities.  However, it is not directly clear how to compute a target that guarantees a decrease of the global loss (that is how to compute a ˆhi for which equation (3) holds) or that at least leads to a decrease of the local loss Li+1 of the next layer, that is  Li(ˆhi+1, fi(ˆhi)) < Li(ˆhi+1, fi(hi)) .  (6)  Proposing and validating answers to this question is the subject of the rest of this paper.  2.2 How to assign a proper target to each layer  Clearly, in a supervised learning setting, the top layer target should be directly driven from the gradient of the global loss  ˆhM = hM − ˆη  ∂L(hM , y)  ∂hM  ,  (7)  where ˆη is usually a small step size. Note, that if we use the MSE as global loss and ˆη = 0.5 we get ˆhM = y.  But how can we deﬁne targets for the intermediate layers? In the previous technical report [4], it was suggested to take advantage of an “approximate inverse”. To formalize this idea, suppose that for each fi we have a function gi such that gi(fi(hi−1)) ≈ hi−1 .  fi(gi(hi)) ≈ hi  (8)  or  Then, choosing  (9) would have the consequence that (under some smoothness assumptions on f and g) minimizing the distance between hi−1 and ˆhi−1 should also minimize the loss Li of  ˆhi−1 = gi(ˆhi)  Difference Target Propagation  5  the i-th layer. This idea is illustrated in the left of Figure 1. Indeed, if the feed-back mappings were the perfect inverses of the feed-forward mappings (gi = f−1 ), one gets  i  Li(ˆhi, fi(ˆhi−1)) = Li(ˆhi, fi(gi(ˆhi))) = Li(ˆhi, ˆhi) = 0 .  (10)  But choosing g to be the perfect inverse of f may need heavy computation and insta- bility, since there is no guarantee that f−1 applied to a target would yield a value that is in the domain of fi−1. An alternative approach is to learn an approximate inverse gi, making the fi / gi pair look like an auto-encoder. This suggests parametrizing gi as follows:  i  gi(hi) = ¯si(Vihi),  (11) where ¯si is a non-linearity associated with the decoder and Vi the matrix of feed-back weights of the i-th layer. With such a parametrization, it is unlikely that the auto-encoder will achieve zero reconstruction error. The decoder could be trained via an additional auto-encoder-like loss at each layer  i = 1, ..., M  i = ||gi(fi(hi−1)) − hi−1||2 Linv 2 . Changing Vi based on this loss, makes g closer to f−1 . By doing so, it also makes fi(ˆhi−1) = fi(gi(ˆhi)) closer to ˆhi, and is thus also contributing to the decrease of Li(ˆhi, fi(ˆhi−1)). But we do not want to estimate an inverse mapping only for the con- crete values we see in training but for a region around the these values to facilitate the computation of gi( ˆhi) for ˆhi which have never been seen before. For this reason, the loss is modiﬁed by noise injection  (12)  i  i = ||gi(fi(hi−1 + (cid:15))) − (hi−1 + (cid:15))||2 Linv 2,  (cid:15) ∼ N (0, σ) ,  (13)  which makes fi and gi approximate inverses not just at hi−1 but also in its neighbor- hood.  As mentioned above, a required property of target propagation is, that the layer- wise parameter updates, each improving a layer-wise loss, also lead to an improvement of the global loss. The following theorem shows that, for the case that gi is a perfect inverse of fi and fi having a certain structure, the update direction of target propagation does not deviate more then 90 degrees from the gradient direction (estimated by back- propagation), which always leads to a decrease of the global loss. Theorem 1. 4 Assume that gi = f−1 , i = 1, ..., M, and fi satisﬁes hi = fi(hi−1) = Wisi(hi−1) 5 where si can be any differentiable monotonically increasing element- wise function. Let δW tp be the target propagation update and the back- propagation update in i-th layer, respectively. If ˆη in Equation (7) is sufﬁciently small, then the angle α between δW tp  i and δW bp  is bounded by  i  i  i and δW bp  i  0 <  1 + ∆1(ˆη)  λmax λmin  + ∆2(ˆη)  ≤ cos(α) ≤ 1  (14)  4 See the proof in the Appendix. 5 This is another way to obtain a non-linear deep network structure.  6  D-H. Lee, S. Zhang, A. Fischer and Y. Bengio  Fig. 1. (left) How to compute a target in the lower layer via difference target propagation. fi(ˆhi−1) should be closer to ˆhi than fi(hi−1). (right) Diagram of the back-propagation-free auto-encoder via difference target propagation.  Here λmax and λmin are the largest and smallest singular values of (JfM . . . Jfi+1)T , where Jfk is the Jacobian matrix of fk and ∆1(ˆη) and ∆2(ˆη) are close to 0 if ˆη is sufﬁciently small.  2.3 Difference target propagation  From our experience, the imperfection of the inverse function leads to severe optimiza- tion problems when assigning targets based on equation (9). This brought us to propose the following linearly corrected formula for target propagation which we refer to as “difference target propagation”  ˆhi−1 = hi−1 + gi(ˆhi) − gi(hi) .  (15)  Note, that if gi is the inverse of fi, difference target propagation becomes equivalent to vanilla target propagation as deﬁned in equation (9). The resulting complete training procedure for optimization by difference target propagation is given in Algorithm 1.  In the following, we explain why this linear corrected formula stabilizes the opti- mization process. In order to achieve stable optimization by target propagation, hi−1 should approach ˆhi−1 as hi approaches ˆhi. Otherwise, the parameters in lower layers continue to be updated even when an optimum of the global loss is reached already by the upper layers, which then could lead the global loss to increase again. Thus, the condition  hi = ˆhi ⇒ hi−1 = ˆhi−1  (16)  greatly improves the stability of the optimization. This holds for vanilla target propaga- tion if gi = f−1  , because  i  hi−1 = f−1  i  (hi) = gi(ˆhi) = ˆhi−1 .  (17)  Algorithm 1 Training deep neural networks via difference target propagation  Difference Target Propagation  7  Compute unit values for all layers: for i = 1 to M do hi ← fi(hi−1)  end for Making the ﬁrst target: ˆhM−1 ← hM−1 − ˆη Compute targets for lower layers: for i = M − 1 to 2 do  ˆhi−1 ← hi−1 − gi(hi) + gi(ˆhi)  end for Training feedback (inverse) mapping: for i = M − 1 to 2 do  ∂L  ∂hM−1  ,  (L is the global loss)  Update parameters for gi using SGD with following a layer-local loss Linv i = ||gi(fi(hi−1 + (cid:15))) − (hi−1 + (cid:15))||2 Linv end for Training feedforward mapping: for i = 1 to M do  2, (cid:15) ∼ N (0, σ)  i  Update parameters for fi using SGD with following a layer-local loss Li Li = ||fi(hi−1) − ˆhi||2 if i = M.  if i < M, Li = L (the global loss)  2  end for  Although the condition is not guaranteed to hold for vanilla target propagation if gi (cid:54)= f−1  , for difference target propagation it holds by construction, since  i  ˆhi−1 − hi−1 = gi(ˆhi) − gi(hi) .  (18)  Furthermore, under weak conditions on f and g and if the difference between hi and ˆhi is small, we can show for difference target propagation that if the input of the i-th layer becomes ˆhi−1 (i.e. the i − 1-th layer reaches its target) the output of the i-th layer also gets closer to ˆhi. This means that the requirement on targets speciﬁed by equation (6) is met for difference target propagation, as shown in the following theorem  Theorem 2. 6 Let the target for layer i − 1 be given by Equation (15), i.e. ˆhi−1 = hi−1 + gi(ˆhi)− gi(hi). If ˆhi − hi is sufﬁciently small, fi and gi are differentiable, and the corresponding Jacobian matrices Jfi and Jgi satisfy that the largest eigenvalue of (I − JfiJgi )T (I − JfiJgi) is less than 1, then we have  ||ˆhi − fi(ˆhi−1)||2  2 < ||ˆhi − hi||2 2 .  (19)  The third condition in the above theorem is easily satisﬁed in practice, because gi is learned to be the inverse of fi and makes gi ◦ fi close to the identity mapping, so that (I − JfiJgi) becomes close to the zero matrix which means that the largest eigenvalue of (I − JfiJgi)T (I − JfiJgi) is also close to 0.  6 See the proof in Appendix.  8  D-H. Lee, S. Zhang, A. Fischer and Y. Bengio  2.4 Training an auto-encoder with difference target propagation  Auto-encoders are interesting for learning representations and serve as building blocks for deep neural networks [10]. In addition, as we have seen, training auto-encoders is part of the target propagation approach presented here, where they model the feedback paths used to propagate the targets.  In the following, we show how a regularized auto-encoder can be trained using dif- ference target propagation instead of back-propagation. Like in the work on denoising auto-encoders [22] and generative stochastic networks [6], we consider the denoising auto-encoder like a stochastic network with noise injected in input and hidden units, trained to minimize a reconstruction loss. This is, the hidden units are given by the encoder as  h = f (x) = sig(W x + b) ,  (20)  where sig is the element-wise sigmoid function, W the weight matrix and b the bias vector of the input units. The reconstruction is given by the decoder z = g(h) = sig(W T (h + (cid:15)) + c), (cid:15) ∼ N (0, σ) ,  (21)  with c being the bias vector of the hidden units. And the reconstruction loss is  L = ||z − x||2  2 + ||f (x + (cid:15)) − h||2  2, (cid:15) ∼ N (0, σ) ,  (22)  where a regularization term can be added to obtain a contractive mapping. In order to train this network without back-propagation (that is, without using the chain rule), we can use difference target propagation as follows (see Figure 1 (right) for an illustration): at ﬁrst, the target of z is just x, so we can train the reconstruction mapping g based on the loss Lg = ||g(h) − x||2 2 in which h is considered as a constant. Then, we compute the target ˆh of the hidden units following difference target propagation where we make use of the fact that f is an approximate inverse of g. That is,  ˆh = h + f (ˆz) − f (z) = 2h − f (z) ,  (23)  where the last equality follows from f (ˆz) = f (x) = h. As a target loss for the hidden layer, we can use Lf = ||f (x + (cid:15)) − ˆh||2 2, where ˆh is considered as a constant and which can be also augmented by a regularization term to yield a contractive mapping.  3 Experiments  In a set of experiments we investigated target propagation for training deep feedforward deterministic neural networks, networks with discrete transmissions between units, stochastic neural networks, and auto-encoders. For training supervised neural networks, we chose the target of the top hidden layer (number M−1) such that it also depends directly on the global loss instead of an inverse mapping. That is, we set ˆhM−1 = hM−1 − ˜η ∂L(hM ,y) , where L is the global loss (here the multiclass cross entropy). This may be helpful when the number of units in the  ∂hM−1  Difference Target Propagation  9  output layer is much smaller than the number of units in the top hidden layer, which would make the inverse mapping difﬁcult to learn, but future work should validate that. For discrete stochastic networks in which some form of noise (here Gaussian) is injected, we used a decaying noise level for learning the inverse mapping, in order to stabilize learning, i.e. the standard deviation of the Gaussian is set to σ(e) = σ0/(1 + e/e0) where σ0 is the initial value, e is the epoch number and e0 is the half-life of this decay. This seems to help to ﬁne-tune the feedback weights at the end of training.  In all experiments, the weights were initialized with orthogonal random matrices and the bias parameters were initially set to zero. All experiments were repeated 10 times with different random initializations. We put the code of these experiments online (https://github.com/donghyunlee/dtp).  3.1 Deterministic feedforward deep networks  As a primary objective, we investigated training of ordinary deep supervised networks with continuous and deterministic units on the MNIST dataset. We used a held-out val- idation set of 10000 samples for choosing hyper-parameters. We trained networks with 7 hidden layers each consisting of 240 units (using the hyperbolic tangent as activation function) with difference target propagation and back-propagation.  Training was based on RMSprop [21] where hyper-parameters for the best valida- tion error were found using random search [7]. RMSprop is an adaptive learning rate algorithm known to lead to good results for back-propagation. Furthermore, it is suitable for updating the parameters of each layer based on the layer-wise targets obtained by target propagation. Our experiments suggested that when using a hand-selected learning rate per layer rather than the automatically set one (by RMSprop), the selected learning rates were different for each layer, which is why we decided to use an adaptive method like RMSprop.  Fig. 2. Mean training cost (left) and train/test classiﬁcation error (right) with target propagation and back-propagation using continuous deep networks (tanh) on MNIST. Error bars indicate the standard deviation.  The results are shown in Figure 2. We obtained a test error of 1.94% with target propagation and 1.86% with back propagation. The ﬁnal negative log-likelihood on the training set was 4.584 × 10−5 with target propagation and 1.797 × 10−5 with back  10  D-H. Lee, S. Zhang, A. Fischer and Y. Bengio  propagation. We also trained the same network with rectiﬁer linear units and got a test error of 3.15% whereas 1.62% was obtained with back-propagation. It is well known that this nonlinearity is advantageous for back-propagation, while it seemed to be less appropriate for this implementation of target propagation.  In a second experiment we investigated training on CIFAR-10. The experimental setting was the same as for MNIST (using the hyperbolic tangent as activation func- tion) except that the network architecture was 3072-1000-1000-1000-10. We did not use any preprocessing, except for scaling the input values to lay in [0,1], and we tuned the hyper-parameters of RMSprop using a held-out validation set of 1000 samples. We obtained mean test accuracies of 50.71% and 53.72% for target propagation and back- propagation, respectively. It was reported in [14], that a network with 1 hidden layer of 1000 units achieved 49.78% accuracy with back-propagation, and increasing the num- ber of units to 10000 led to 51.53% accuracy. As the current state-of-the-art perfor- mance on the permutation invariant CIFAR-10 recognition task, [12] reported 64.1% but when using PCA without whitening as preprocessing and zero-biased auto-encoders for unsupervised pre-training.  3.2 Networks with discretized transmission between units  To explore target propagation for an extremely non-linear neural network, we inves- tigated training of discrete networks on the MNIST dataset. The network architecture was 784-500-500-10, where only the 1st hidden layer was discretized. Inspired by bi- ological considerations and the objective of reducing the communication cost between neurons, instead of just using the step activation function, we used ordinary neural net layers but with signals being discretized when transported between the ﬁrst and second layer. The network structure is depicted in the right plot of Figure 3 and the activations of the hidden layers are given by  h1 = f1(x) = tanh(W1x)  (24) where sign(x) = 1 if x > 0, and sign(x) = 0 if x ≤ 0. The network output is given by  and h2 = f2(h1) = tanh(W2sign(h1))  p(y|x) = f3(h2) = sof tmax(W3h2) .  (25)  The inverse mapping of the second layer and the associated loss are given by  g2(h2) = tanh(V2sign(h2)) ,  (26)  (cid:15) ∼ N (0, σ) .  2 = ||g2(f2(h1 + (cid:15))) − (h1 + (cid:15))||2 Linv 2,  (27) If feed-forward mapping is discrete, back-propagated gradients become 0 and useless when they cross the discretization step. So we compare target propagation to two base- lines. As a ﬁrst baseline, we train the network with back-propagation and the straight- through estimator [5], which is biased but was found to work well, and simply ignores the derivative of the step function (which is 0 or inﬁnite) in the back-propagation phase. As a second baseline, we train only the upper layers by back-propagation, while not changing the weight W1 which are affected by the discretization, i.e., the lower layers do not learn.  Difference Target Propagation  11  The results on the training and test sets are shown in Figure 3. The training error for the ﬁrst baseline (straight-through estimator) does not converge to zero (which can be explained by the biased gradient) but generalization performance is fairly good. The second baseline (ﬁxed lower layer) surprisingly reached zero training error, but did not perform well on the test set. This can be explained by the fact that it cannot learn any meaningful representation at the ﬁrst layer. Target propagation however did not suffer from this drawback and can be used to train discrete networks directly (training signals can pass the discrete region successfully). Though the training convergence was slower, the training error did approach zero. In addition, difference target propagation also achieved good results on the test set.  Fig. 3. Mean training cost (top left), mean training error (top right) and mean test error (bottom left) while training discrete networks with difference target propagation and the two baseline versions of back-propagation. Error bars indicate standard deviations over the 10 runs. Diagram of the discrete network (bottom right). The output of h1 is discretized because signals must be communicated from h1 to h2 through a long cable, so binary representations are preferred in order to conserve energy. With target propagation, training signals are also discretized through this cable (since feedback paths are computed by bona-ﬁde neurons).  3.3 Stochastic networks  Another interesting model class which vanilla back-propagation cannot deal with are stochastic networks with discrete units. Recently, stochastic networks have attracted attention [3, 20, 5] because they are able to learn a multi-modal conditional distribution  D-H. Lee, S. Zhang, A. Fischer and Y. Bengio  12 P (Y |X), which is important for structured output predictions. Training networks of stochastic binary units is also biologically motivated, since they resemble networks of spiking neurons. Here, we investigate whether one can train networks of stochastic binary units on MNIST for classiﬁcation using target propagation. Following [17], the network architecture was 784-200-200-10 and the hidden units were stochastic binary units with the probability of turning on given by a sigmoid activation:  i = P (Hi = 1|hi−1) = σ(Wihi−1), hi ∼ P (Hi|hi−1) , hp  (28)  that is, hi is one with probability hp i .  As a baseline, we considered training based on the straight-through biased gradient estimator [5] in which the derivative through the discrete sampling step is ignored (this method showed the best performance in [17].) That is  δhp  i−1 = δhp  i  ∂hp i ∂hp i−1  ≈ σ(cid:48)(Wihi−1)W T  i δhp  i  .  (29)  With difference target propagation the stochastic network can be trained directly, setting the targets to  where gi(hp  and  ˆhp 2 = hp  ˆhp 1 = hp  ∂L ∂h2 i ) is trained by the loss  2 − η i ) = tanh(Vihp i = ||gi(fi(hi−1 + (cid:15))) − (hi−1 + (cid:15))||2 Linv 2, i − hp  and layer-local target losses are deﬁned as Li = ||ˆhp  (cid:15) ∼ N (0, σ) , i ||2 2.  1 + g2(ˆhp  2) − g2(hp 2)  (30)  (31)  Method  Test Error(%)  Difference Target-Propagation, M=1  Straight-through gradient estimator [5] + backprop, M=1  as reported in Raiko et al. [17]  as reported in Tang and Salakhutdinov [20], M=20  as reported in Raiko et al. [17], M=20  1.54%  1.71% 3.99% 1.63%  Table 1. Mean test Error on MNIST for stochastoc networks. The ﬁrst row shows the results of our experiments averaged over 10 trials. The second row shows the results reported in [17]. M corresponds to the number of samples used for computing output probabilities. We used M=1 during training and M=100 for the test set.  For evaluation, we averaged the output probabilities for a given input over 100 sam- ples, and classiﬁed the example accordingly, following [17]. Results are given in Table 1. We obtained a test error of 1.71% using the baseline method and 1.54% using tar- get propagation, which is – to our knowledge – the best result for stochastic nets on MNIST reported so far. This suggests that target propagation is highly promising for training networks of binary stochastic units.  Difference Target Propagation  13  3.4 Auto-encoder We trained a denoising auto-encoder with 1000 hidden units with difference target propagation as described in Section 2.4 on MNIST. As shown in Figure 4 stroke-like ﬁlters can be obtained by target propagation. After supervised ﬁne-tuning (using back- propagation), we got a test error of 1.35%. Thus, by training an auto-encoder with target propagation one can learn a good initial representation, which is as good as the one ob- tained by regularized auto-encoders trained by back-propagation on the reconstruction error.  Fig. 4. Filters learned by the back-propagation-free auto-encoder. Each ﬁlter corresponds to the hidden weights of one of 100 randomly chosen hidden units. We obtain stroke ﬁlters, similar to those usually obtained by regularized auto-encoders.  4 Conclusion  We introduced a novel optimization method for neural networks, called target propa- gation, which was designed to overcome drawbacks of back-propagation and is bio- logically more plausible. Target propagation replaces training signals based on partial derivatives by targets which are propagated based on an auto-encoding feedback loop. Difference target propagation is a linear correction for this imperfect inverse mapping which is effective to make target propagation actually work. Our experiments show that target propagation performs comparable to back-propagation on ordinary deep net- works and denoising auto-encoders. Moreover, target propagation can be directly used on networks with discretized transmission between units and reaches state of the art performance for stochastic neural networks on MNIST.  Acknowledgments We would like to thank Junyoung Chung for providing RMSprop code, Caglar Gulcehre and Antoine Biard for general discussion and feedback, Jyri Kivinen for discus- sion of backprop-free auto-encoder, Mathias Berglund for explanation of his stochastic networks.  14  D-H. Lee, S. Zhang, A. Fischer and Y. Bengio  We thank the developers of Theano [8, 1], a Python library which allowed us to easily develop a fast and optimized code for GPU. We are also grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR.  References  [1] Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I.J., Bergeron, A., Bouchard, N., Bengio, Y.: Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop (2012)  [2] Bengio, Y.: Learning deep architectures for AI. Now Publishers (2009) [3] Bengio, Y.: Estimating or propagating gradients through stochastic neurons. Tech.  Rep. arXiv:1305.2982, Universite de Montreal (2013)  [4] Bengio, Y.: How auto-encoders could provide credit assignment in deep networks  via target propagation. Tech. rep., arXiv:1407.7906 (2014)  [5] Bengio, Y., L´eonard, N., Courville, A.: Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv:1308.3432 (2013) [6] Bengio, Y., Thibodeau-Laufer, E., Yosinski, J.: Deep generative stochastic net-  works trainable by backprop. In: ICML’2014 (2014)  [7] Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization. J. Ma-  chine Learning Res. 13, 281–305 (2012)  [8] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., Bengio, Y.: Theano: a CPU and GPU math expression compiler. In: Proceedings of the Python for Scientiﬁc Computing Conference (SciPy) (Jun 2010), oral Presentation  [9] Carreira-Perpinan, M., Wang, W.: Distributed optimization of deeply nested sys-  tems. In: AISTATS’2014, JMLR W&CP. vol. 33, pp. 10–19 (2014)  [10] Erhan, D., Courville, A., Bengio, Y., Vincent, P.: Why does unsupervised pre- training help deep learning? In: JMLR W&CP: Proc. AISTATS’2010. vol. 9, pp. 201–208 (2010)  [11] Hinton, G., Deng, L., Dahl, G.E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., Kingsbury, B.: Deep neural networks for acoustic mod- eling in speech recognition. IEEE Signal Processing Magazine 29(6), 82–97 (Nov 2012)  [12] Konda, K., Memisevic, R., Krueger, D.: Zero-bias autoencoders and the beneﬁts of co-adapting features. Under review on International Conference on Learning Representations (2015)  [13] Krizhevsky, A., Sutskever, I., Hinton, G.: ImageNet classiﬁcation with deep con-  volutional neural networks. In: NIPS’2012 (2012)  [14] Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images.  Master’s thesis, University of Toronto (2009)  [15] LeCun, Y.: Learning processes in an asymmetric threshold network. In: Fogelman- Souli´e, F., Bienenstock, E., Weisbuch, G. (eds.) Disordered Systems and Biologi- cal Organization, pp. 233–240. Springer-Verlag, Les Houches, France (1986)  [16] LeCun, Y.: Mod`eles connexionistes de l’apprentissage. Ph.D. thesis, Universit´e  de Paris VI (1987)  Difference Target Propagation  15  [17] Raiko, T., Berglund, M., Alain, G., Dinh, L.: Techniques for learning binary stochastic feedforward neural networks. NIPS Deep Learning Workshop 2014 (2014)  [18] Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural  networks. Tech. rep., arXiv:1409.3215 (2014)  [19] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. Tech. rep., arXiv:1409.4842 (2014)  [20] Tang, Y., Salakhutdinov, R.: A new learning algorithm for stochastic feedforward ICML’2013 Workshop on Challenges in Representation Learning  neural nets. (2013)  [21] Tieleman, T., Hinton, G.: Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning 4 (2012)  [22] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.A.: Stacked denois- ing autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Machine Learning Res. 11 (2010)  Appendix A Proof of Theorem 1  Proof. Given a training example (x, y) the back-propagation update is given by  δW bp  i = − ∂L(x, y; θ0,M W ) = Wi · S(cid:48)  ∂Wi  = −J T  fi+1  . . . J T fM  ∂L ∂hM  (si(hi−1))T ,  i(hk−1), k = i + 1, . . . , M. Here S(cid:48)  i(hk−1) is a where Jfk = ∂hk ∂hk−1 diagonal matrix with each diagonal element being element-wise derivatives and Jfk is the Jacobian of fk(hk−1). In target propagation the target for hM is given by ˆhM = hM − ˆη ∂L . If all hk’s are allocated in smooth areas and ˆη is sufﬁciently small, we can apply a Taylor expansion to get ˆhi = gi+1(. . . gM (ˆhM ) . . . ) = gi+1(. . . gM (hM ) . . . )− ˆηJgi+1 . . . JgM  +o(ˆη) ,  ∂hM  ∂L ∂hM  where o(ˆη) is the remainder satisfying limˆη→0 o(ˆη)/ˆη = 0. Now, for δW tp  i we have  δW tp  2  ∂Wi  i = − ∂||hi(hi−1; Wi) − ˆhi||2 = −(hi − (hi − ˆηJ−1 . . . J−1 = −ˆηJ−1 ∂L ∂hM  . . . J−1  fi+1  fi+1  fM  fM  ∂L ∂hM  + o(ˆη)))(si(hi−1))T  (si(hi−1))T + o(ˆη)(si(hi−1))T .  16  D-H. Lee, S. Zhang, A. Fischer and Y. Bengio  We write ∂L ∂hM production of vector forms of δW bp i  and δW tp i  is  as l , si(hi−1) as v and JfM . . . Jfi+1 as J for short. Then the inner  (cid:104)vec(δW bp  i ), vec(δW tp  i )(cid:105) = tr((J T lvT )T (ˆηJ−1lvT + o(ˆη)vT ))  = ˆηtr(vlT JJ−1lvT ) − tr(vlT Jo(ˆη)vT ) = ˆη||v||2  2||l||2  2 − (cid:104)J T l, o(ˆη)(cid:105)||v||2 2 .  (cid:113)  i )||2 and ||vec(δW tp i )||2 =  For ||vec(δW bp ||vec(δW bp and similarly  i )||2 we have  tr((−J T lvT )T (−J T lvT )) = ||v||2||J T l||2 ≤ ||v||2||J T||2||l||2  ||vec(δW tp  i )||2 ≤ ˆη||v||2||J−1||2||l||2 + ||o(ˆη)||2||v||2 ,  where ||J T||2 and ||J−1||2 are matrix Euclidean norms, i.e. the largest singular value of (JfM . . . Jfi+1)T , λmax, and the largest singular value of (JfM . . . Jfi+1)−1, λmin (λmin is the smallest singular value of (JfM . . . Jfi+1 )T , because fk is invertable, so all the smallest singular values of Jacobians are larger than 0). Finally, if ˆη is sufﬁciently small, the angle α between vec(δW bp (cid:104)vec(δW bp  i ) satisﬁes:  1  i ) and vec(δW tp i )(cid:105) i ), vec(δW tp i )||2 · ||vec(δW tp i )||2 ˆη||v||2  2||l||2  2 − (cid:104)J T l, o(ˆη)(cid:105)||v||2  2  cos(α) =  ||vec(δW bp  ≥  =  (||v||2λmax||l||2)(ˆη||v||2(  )||l||2 + ||o(ˆη)||2||v||2)  1  λmin  ˆη||l||2  −(cid:104)J T l,o(ˆη)(cid:105) + λmax||o(ˆη)||2  ˆη||l||2  2  1 +  λmax λmin  =  1 + ∆1(ˆη)  λmax λmin  + ∆2(ˆη)  where the last expression is positive if ˆη is sufﬁciently small and cos(α) ≤ 1 is trivial.  B Proof of Theorem 2 Proof. Let e = ˆhi − hi. Applying Taylor’s theorem twice, we get ˆhi − fi(ˆhi−1) = ˆhi − fi(hi−1 + gi(ˆhi) − gi(hi)) = ˆhi − fi(hi−1 + Jgi e + o(||e||2))  = ˆhi − fi(hi−1) − Jfi(Jgi e + o(||e||2)) − o(||Jgie + o(||e||2)||2) = ˆhi − hi − JfiJgie − o(||e||2) = (I − JfiJgi)e − o(||e||2)  where the vector o(||e||2) represents the remainder satisfying lime→0 o(||e||2)/||e||2 = 0. Then for ||ˆhi − fi(ˆhi−1)||2  2 we have  ||ˆhi − fi(ˆhi−1)||2  2 = ((I − JfiJgi)e − o(||e||2))T ((I − Jfi Jgi)e − o(||e||2)) = eT (I − Jfi Jgi)T (I − JfiJgi)e − o(||e||2)T (I − JfiJgi)e  −eT (I − JfiJgi)T o(||e||2) + o(||e||2)T o(||e||2))  = eT (I − Jfi Jgi)T (I − JfiJgi)e + o(||e||2 2) ≤ λ||e||2  2 + |o(||e||2 2)|  (A-1)  17 2) is the scalar value resulting from all terms depending on o(||e||2) and where o(||e||2 λ is the largest eigenvalue of (I − JfiJgi)T (I − Jfi Jgi). If e is sufﬁciently small to guarantee |o(||e||2 2, then the left of Equation (A-1) is less than ||e||2 which is just ||ˆhi − hi||2 2.  2)| < (1 − λ)||e||2  Difference Target Propagation  2  ",
1411.3815,2015," Predictive encoding of contextual relationships for perceptual inference, interpolation and prediction","['Predictive encoding of contextual relationships for perceptual inference', 'interpolation and prediction', 'Mingmin Zhao', 'Chengxu Zhuang', 'Yizhou Wang', 'and Tai Sing Lee']",https://arxiv.org/pdf/1411.3815,"5 1 0 2    r p A 6 1         ]  G L . s c [      6 v 5 1 8 3  .  1 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  PREDICTIVE ENCODING OF CONTEXTUAL RELATION- SHIPS FOR PERCEPTUAL INFERENCE, INTERPOLATION AND PREDICTION  Mingmin Zhao Computer Science Department Peking University zhaomingmin@pku.edu.cn  Chengxu Zhuang Electrical Engineering Department Tsinghua University zcx11@mails.tsinghua.edu.cn  Yizhou Wang Computer Science Department Peking University Yizhou.Wang@pku.edu.cn  Tai Sing Lee Center for the Neural Basis of Cognition and Computer Science Department Carnegie Mellon University tai@cnbc.cmu.edu  ABSTRACT  We propose a new neurally-inspired model that can learn to encode the global relationship context of visual events across time and space and to use the contextual information to modulate the analysis by synthe- sis process in a predictive coding framework. The model learns latent contextual representations by maximizing the predictability of visual events based on local and global contextual information through both top-down and bottom-up processes. In contrast to standard predictive coding models, the prediction error in this model is used to update the contextual representation but does not alter the feedforward input for the next layer, and is thus more consistent with neurophysiological ob- servations. We establish the computational feasibility of this model by demonstrating its ability in several aspects. We show that our model can outperform state-of-art performances of gated Boltzmann machines (GBM) in estimation of contextual information. Our model can also interpolate missing events or predict future events in image sequences while simultaneously estimating contextual information. We show it achieves state-of-art performances in terms of prediction accuracy in a variety of tasks and possesses the ability to interpolate missing frames, a function that is lacking in GBM.  1  INTRODUCTION  In theoretical neuroscience, it has been proposed that in order to rapidly process the constant inﬂux of sensory inputs, which are complex, noisy and full of ambiguity, the brain needs to learn inter- nal models of the world, and use them to generate expectations and predictions based on memory and context to speed up and facilitate inference. Comprehension is achieved when the synthesized prediction or expectation, mediated by recurrent feedback from the higher visual areas to the early visual areas, explains the incoming signals (Mumford, 1992). This framework was recently popu- larized by Rao & Ballard (1999) in psychology and neuroscience as the predictive coding theory, and can be understood more generally in the framework of hierarchical Bayesian inference (Lee & Mumford, 2003). The predictive coding idea has been generalized to non-visual systems (Bar, 2007; Todorovic et al., 2011), and even a “uniﬁed theory” of the brain (Friston, 2010). However, the computational utility and power of these conceptual models remains to be elucidated. In this work, we propose a new framework that can learn internal models of contextual relationships between visual events in space and time. These internal models of context allow the system to interpolate missing frames in image sequences or to predict future frames. The internal models,  1  Accepted as a workshop contribution at ICLR 2015  learned as latent variables, help accomplish these tasks by rescaling the weights of the basis functions represented by the neurons during the synthesis process in the framework of predictive coding. This model is inspired and related to Memisevic and Hinton’s (Memisevic, 2013; Susskind et al., 2011; Memisevic, 2011) gated Boltzmann machines (GBM) and gated autoencoder (GAE) which also model spatiotemporal transformations in image sequences. Their gated machines, modeling 3-way multiplicative interaction, make strong assumption on the role of neural synchrony utilized in learning and inference. Generalizing that model to N-way interaction is problematic because it involves N + 1 way multiplicative interaction. As a result, the GBM machines has primarily been used to learn transformation between two frames. Prediction in GBM is accomplished by applying the transformation estimated based on the ﬁrst two frames to the second frame to generate/predict a third frame. It cannot interpolate a missing frame in the middle if given a frame before and a frame after the missing frame. Our model explicitly deﬁnes a cost function based on mutual predictability. It is more ﬂexible and can propagate information in both directions to predict future frames and interpolate missing frames in a uniﬁed framework. In our formulation, synchrony is not required. Evidence from multiple frames are weighed and then summed together in a way similar to spatiotemporal ﬁltering of the input signals by visual cortical neurons in the primary visual cortex. The inference of the latent context variables is nonlinear, ac- complished by minimizing the prediction error of the synthesized image sequence and the observed sequence. This is a crucial difference from GBM, which, as is the case with most deep learning net- works (Hinton et al., 2006), relies on one-pass feedforward computation for inference. Our model, by exploiting top-down and bottom-up processes to minimize the same predictive coding cost func- tion during both learning and inference, is able to estimate more meaningful and accurate contextual information. Our framework of predictive coding under contextual modulation allows the model to accomplish the similar functions as GBM, but also makes it more ﬂexible and achieve more functions such as integrating more than 2 frames, and performing interpolation. Our model is also more biologically plausible than the standard predictive coding model (Rao & Ballard, 1999) in that the prediction error signals are used to update the contextual representation only, and do not replace the feedforward input to the next layer. This model also provides a framework for understanding how contextual modulation can inﬂuence the certain constructive and generative aspects of visual perception.  2 DESCRIPTION OF THE MODEL  The proposed model seeks to learn relationships between visual events in a spatial or temporal neigh- borhood to provide contextual modulation for image reconstruction, interpolation and prediction. It can be conceptualized as an autoencoder with contextual modulation, or a context-dependent pre- dictive coding model. A predictive coding model states that the brain continually generates models of the world based on context and memory to predict sensory input. It synthesizes a predicted image and feeds back to match the input image represented in the lower sensory areas. The mismatch be- tween the prediction and the input produces a residue signal that can be used to update the top-down models to generate predictions to explain away the inputs (Mumford, 1992; Rao & Ballard, 1999). Our model extends this “standard” predictive coding model, using the residue signals to update the contextual representation, which in turn modulates the image synthesis process by rescaling the ba- sis functions of the neurons adaptively so that the synthesized images for all the frames in a temporal neighborhood maximally predict one another. The problem can be formulated as the following energy function,  L(x1...N , z; θ) =  (cid:107)xt − ˆxt(x1...N , z; θ)(cid:107)2  2 + λ(cid:107)z(cid:107)1,  (1)  (cid:88)  t  where xt is the input signal, ˆxt is the predicted signal, z is the contextual latent variables, θ = {W 1...N , W z} is a collection of parameters of the model to be learned, including the feedforward connections or receptive ﬁelds of the neurons in the y hidden layer, and the feedback connections, W z from the z to modulate the generation of ˆx by y. The second term in the function is a L1 regularization term that makes the contextual latent variables sparse. λ > 0 serves to balance the importance of the prediction error term and the regularization term. Note that this cost function can be considered as a generalization of the classical energy functional or Bayesian formulation in computer vision (Horn 1986, Blake and Zisserman 1987, Geman and Geman 1984) with the ﬁrst ”data term” term replaced by a generative model and the second ”smoothness prior” term replaced by a set of contextual relationship priors. The objective of the model is to learn a set of relationship contexts that can modulate the predictive synthesis process to maximize the mutual predictability of the synthesized images across space and/or time. We will now describe how the prediction can be generated in this model. The model’s information ﬂow can be depicted as a circuit diagram shown in Figure 1 (left panel). It consists of an input (visible) layer xt, a hidden layer yt that performs a spatiotemporal ﬁlter operation on  2  Accepted as a workshop contribution at ICLR 2015  the input layer, a prediction layer ˆxt that represents the prediction generated by yt with contextual modulation from the latent contextual representation layer z. Prediction error signals are propagated up to drive the updating of the contextual representation z.  Figure 1: Left: Computational circuit of the Predictive Encoder. Right: Graphical model represen- tation of Predictive Encoder and its expansion for sequence of arbitrary length T . Units in the visible layer, xt ∈ RD, t = 1, ..., N, represents a sequence of images, with xt rep- resenting the image frame (with D number of pixels) at t and x1...N indicates a sequence of video image frames. Note that in the visible layer it is not necessary for all the visual events to be present, since the model possesses the ability to predict the missing events from available partial observations based on the principle of mutual predictability. Units in the hidden layer, yt ∈ RB, t = 1, ..., N, are deﬁned as: |N (t)| W τ xτ ,  (cid:88)  yt =  (2)  1  τ∈N (t)  where B is the number of yt units for each t, N (t) deﬁnes the index set of xt’s neighbors that provide the local temporal support to yt, and | · | returns the size of a set. W t ∈ RB×D, 1 ≤ t ≤ N is weight matrix to be learned as parameters. Each row of W t can be viewed as a feature ﬁlter for a particular visual event in an image frame t. It can be considered as feedforward weight or t neuron or that neuron’s spatial receptive ﬁeld at a particular time frame. The ﬁlter for a hidden yi t is the spatiotemporal ﬁlter corresponding rows of a particular sequence of related Wt for unit yi t as the response of its spatiotemporal ﬁlter at frame receptive ﬁeld of that neuron whose activity yi t to a particular sequence of image frames in N (t) . Our deﬁnition of the neighborhood is ﬂexible. The temporal neighborhood can be made causal, including up to frame t, or t − 1 and the model would still work. Here, we use this non-causal symmetrical neighborhood to underscore the fact that our model can be used to model spatial context (which is symmetrical), and can go back and forth in time to modify our interpretation of the past events based on current evidence and recent history. An additional crucial hidden layer, with a set of latent variables z, is used to model contextual information. z is computed by minimizing the residue errors between a sequence of N reconstructed image frames and the N input frames. z is ﬁltered by a weight matrix W z (the dot product with a row of this matrix) to provide feedback to rescale the contribution of each latent variable activity yi t for generating the prediction signal ˆxt. In Section 4.2, we will study the contextual representation z in greater details. The prediction ˆxt in the prediction layer is given by  T(yt (cid:12) W zz),  ˆxt = W t  (3) where W z is a set of weights or basis functions that ﬁlter the contextual representation z to generate a modulating signal for each yi(t), (cid:12) is an element-wise product, and thus the contribution of each t neuron to the predicted ˆxt is its activity due to feedforward input rescaled by context modulation yi W zz to produce a weight for its spatial synthesis basis function W t. Modulator W zz can be viewed as a high-dimensional distributed representation of context, the structure of which is modeled by a low-dimensional contextual representation z which is made sparse by the sparsity term. Combining all the equations together, the prediction generated by the context-dependent predictive coding model is given by  ˆxt(x1...N , z; θ) = W t  T((  1  |N (t)| W τ xτ ) (cid:12) W zz)  (4)  (cid:88)  τ∈N (t)  3  .........x1x2x3xNy1y2y3yNxˆ1xˆ2xˆ3xˆNz............x1x2x3x4xNxN+1xTz(1:N)z(2:N+1)z(T−N+1:T)Accepted as a workshop contribution at ICLR 2015  Computationally, the update of the contextual latent variables z is driven by the residue signals xt − ˆxt. The model can also be considered as a factor graph as shown in Figure 1 (right panel) together with its expansion for sequence of arbitrary length T . Each factor node (represented by the solid squares) corresponds to the mutual predictability deﬁned by Equation 1 between N consec- utive frames xt modulated by the contextual representation z(t:t+N−1). Contextual representation z will evolve over time, thus priors such as smoothness constraint can be imposed on the temporal evolution of z, as shown in the graphical model, though no priors are imposed in our current im- plementation. At the abstract graphical model level, our model is very similar to autoencoder, as well as to Memisevic and Hinton’s gated Boltzmann machines (Susskind et al., 2011; Memisevic, 2011). But there are difference in the concrete formulation of our model, as well as the learning and inference algorithms, with GBM, as discussed in the introduction.  3 DESCRIPTION OF THE ALGORITHMS  In this section, we describe the learning and inference algorithms developed for our model. Bottom- up and top-down estimations are involved during both inference and learning.  3.1 UNSUPERVISED PARAMETERS LEARNING 1...N} and we assume each of The training dataset is composed of m image sequences {x(1) them to be an i.i.d sample from an unknown distribution. The objective is to optimize the following problem:  1...N , ..., x(m)  minimize  θ,z  L(x(i)  1...N , z(i); θ),  (5)  m(cid:88)  i=1  (cid:88)  s∈S(k)  We adopt an EM-like algorithm that updates parameters and imputes hidden variable z alternatively while keeping the other one ﬁxed. Update θ We use Stochastic Gradient Descent (SGD) to update θ based on the following update rule:  θ(k+1) = θ(k) + ∆θ(k)  (6)  ∆θ(k) = η  ∂  ∂θ(k)  (  L(x(s)  1...N , z(s); θ(k))) + ν∆θ(k−1)  (7)  where the free parameter η ∈ R+ is the learning rate, S(k) deﬁnes the mini-batch used for training at time k and ∆θ(k−1) is the momentum term weighted by a free parameter ν ∈ R+. The momentum term helps to avoid oscillations during the iterative update procedure and to speed up the learning process. All the free parameters in the experiments are chosen under the guidance of Hinton (2010). The algorithm is implemented using Theano (Bergstra et al., 2010) which provides highly optimized symbolic differentiation for efﬁcient and automatic gradient calculation with respect to the objective function. The idea of denoising (Vincent et al., 2008) is also used to learn more robust ﬁlters. Estimate z Given ﬁxed θ, we estimate the contextual representation z(i) for each sequence by solving the following optimization problem independently and in parallel: 2 + λ(cid:107)z(i)(cid:107)1  t − ˆxt(x1...N , z; θ)(cid:107)2  (cid:88)  minimize  (cid:107)x(i)  (8)  z(i)  t  where ˆx(i) is computed by Eqn.(4). To better exploit the quadratic structure of the objective function, t we solve this convex optimization problem using a more efﬁcient quasi-Newton method Limited memory BFGS (L-BFGS) algorithm instead of gradient descent (Ngiam et al., 2011). During the training with each batch of data, we ﬁrst update the parameter θ using one step stochastic gradient descent, then iterate at most ﬁve steps of L-BFGS to estimate the hidden variable z.  3.2  INFERENCE WITH PARTIAL OBSERVATION  Inference with partial observation refers to prediction or reconstruction of a missing image frame by the trained model given observed neighboring frames in the sequence. This problem is posed as an optimization problem that simultaneously estimates the latent variables z for the contextual representation and the missing event/frame xu, 1 ≤ u ≤ N:  minimize  xu,z  L(x1...N , z; θ)  (9)  4  Accepted as a workshop contribution at ICLR 2015  This optimization problem can be solved efﬁciently and iteratively by an alternating top-down and bottom-up estimation procedure. The top-down estimation “hallucinates” a missing event based on the neighboring events and the higher-level contextual representation. The bottom-up procedure uses the prediction residue error to update the contextual representation. Speciﬁcally, minimizing Eqn.(9) is realized by alternately estimating xu and z iteratively. Estimate z Given learned θ and current estimation of xu, we use the same method as Eqn.(8). Estimate xu Given learned θ and current estimation of z, we estimate a missing event/frame by solving the following optimization problem:  minimize  xu  t∈N (u)∪{u}  (cid:107)xt − ˆxt(x1...N , z; θ)(cid:107)2  2  (10)  (cid:88)  While Eqn.(4) considers only the prediction of xu, this optimization problem factors the role of xu in predicting/constructing its neighbors. Notice that this objective function is a standard quadratic function, which has a closed form solution in one step. For a video sequence, predicting a future frame and interpolating a missing frame are formulated and accomplished in a uniﬁed framework.  4 EXPERIMENTAL RESULTS  4.1 RECEPTIVE FIELD LEARNING  In the ﬁrst experiment, we trained our model using movies synthesized from natural images. Each movie sequence exhibited either translation, rotation or scaling transformation. We trained models for each type of transformation movies independently, as well as a mixture of the three. We will show results of the feedforward ﬁlters W t of models trained with three frames (N = 3). The algorithm, however, is not limited to three frames and we will also show results of the model trained with relatively longer sequences such as six frames (N = 6). The images used to generate the training movie sequences were random samples from the whitened natural images used in Olshausen & Field (1997). For translation or rotation, the patch size was 13 × 13 pixels. Translation steps were sampled from the interval [-3, 3] (in pixels) uniformly, and rotation angles were uniformly sampled from [−21◦, 21◦] at 3◦ intervals. For scaling and the mixture of motion cases, the patch size was 21 × 21 pixels. The scaling ratio was uniformly sampled from [0.6, 1.8]. For mixture of motion, the training set was simply a combination of all three types of single-transformation movies, each with a constant transformation parameter. For models trained by a single type of motion, we used 25 contextual representation units in z and 20000 training sequences and the size of W z is 100 × 25. For the models trained with all three types of motions, we used 75 contextual representation units and 30000 training sequences and the size of W z is 300 × 75. We used unsupervised parameters learning algorithm described earlier with a learning rate (η) of 0.05 and momentum (ν) 0.5. Every model was trained for 500 epochs.  (a)  (c)  (b)  (d)  Figure 2: Filters learned from: (a) Translation, (b) Rotation, (c) Scaling, (d) Mixture of Transforma- tions, (e) 6-frame Rotation (N = 6).  (e)  5  Accepted as a workshop contribution at ICLR 2015  Figure 2(a) shows that the feedforward ﬁlters (or receptive ﬁelds) learned from translation resemble Fourier basis with a quadrature phase difference between frames. Figure 2(b) shows that the ﬁlters learned from rotation are Fourier basis in polar coordinates, also with a quadrature phase in polar angle between frames. The ﬁlters learned from scaling shown in Figure 2(c) depicts ﬁlters trained by scaling. They resemble rays emanating from the center or circles contracting to the center, reﬂecting the trajectories of points during scaling. Figure 2(d) shows the ﬁlters trained with a motion mixture, which appear to encode the transformations in a distributed manner using localized Gabor ﬁlters, similar to the receptive ﬁelds of the simple neurons in the primary visual cortex. Figure 2(e) shows the ﬁlters trained with 6 frames rotation sequences. This demonstrates the model can be used to learn longer sequence ﬁlters. We found training time scales linearly with N.  4.2 UNDERSTANDING THE CONTEXTUAL REPRESENTATION  To understand the information encoded in the contextual relationship latent variables z, we used t-SNE method (Van der Maaten & Hinton, 2008) to see how pattern content and transformation content are clustered in a low-dimensional space. We applied the model pre-trained by the motion mixture to a combination of test data of 6,000 synthetic movies generated by randomly translating, rotating, or scaling image patches. The image patches were randomly sampled from 3 different datasets (MNIST, natural whiten images, and bouncing balls). Translation steps were no less than 1 pixel per frame, the rotation angles were no less than 6◦, and scaling ratio sampled from [0.6, 0.9] or [1.1, 1.8] to keep the different transformations distinct. We visualize the activities in the z-layer using Hinton’s t-sne algorithm in Figure 3(a), 3(b) in re- sponse to sequences from the three databases. It can be observed that the content data from the three databases (natural images, balls and MNIST) are all mixed together, indicating that the latent vari- ables z cannot discriminate the image patterns. On the other hand, the transformations are relatively well clustered and segregated, suggesting that these transformations are distinctly encoded in, and can be decoded from, z.  (a)  (b)  (c)  Figure 3: a) t-SNE visualization for latent variables labeled by datasets; b) Visualization by trans- formations; c)CDFs of relative regression error  To investigate how transformations are distinctly represented in z, we trained 3 SVM based on z to decode each of the three transformations (rotation, translation and scaling) from z. For each test sequence, we inferred the context representation z, and then computed the probability of the three SVMs and chose the classiﬁcation with the highest probability. All the SVMs were trained using the dot product as kernel function only. The confusion matrix as shown in Table 2 suggests that the contextual representations encode the content-invariant transformation information.  Accuracy  Time  Our model  0.9387 30 min  GBM 0.8664 2 hours  GAE 0.7464 15 min  Table 1: Prediction accuracy and training time (for 100 epoches).  Rotation Translation  Rotation Translation  Scaling  9369 427 204  144 9773 83  Scaling  392 330 9278  Table 2: Confusion Matrix: ﬁrst column is predicted labels  We also compared the representational power of the inferred z in our model, computed using bottom-up and top-down processing, with the transformation latent variables z in the GBM and GAE, computed using one-pass feedforward computation. We compared a 2-frame version of our model with 3-way (2-frame) GBM and GAE 1 and trained 3 SVMs for each model to decode the type of transformation. As shown in Table 1, our model is comparable, and in fact outperforms those two models while the time needed for training is comparable to GAE and faster than GBM. In addition, we trained a linear regression model using contextual representation z as the regressor to predict/estimate the transformations, namely, translation velocity, angular rotation velocity and scal-  1We used the CPU implementation of GBM on http://www.cs.toronto.edu/˜rfm/factored/ and used theano implementation of GAE on http://www.iro.umontreal.ca/˜memisevr/code. html and ran it on CPU.  6  00.511.522.500.20.40.60.81Relative error of regressionCDF  TranslationRotationScalingAccepted as a workshop contribution at ICLR 2015  ing factor. ρ denotes the three transformation parameters. The relative regression error is deﬁned as |ρgroundtruth−ρpredicted| . The cumulative distribution functions (CDFs) of relative regression error of the estimates in Figure 3(c) shows that the contextual representation contains sufﬁcient information about the transformation parameters.  |ρgroundtruth|  4.3 PREDICTION AND INTERPOLATION  A crucial feature of our model is its ability to predict and interpolate. Note that the GBM can perform prediction but not interpolation. We ﬁrst tested the model’s ability in interpolation and prediction using training sequences generated from three datasets: face images (Gourier et al., 2004), MNIST handwritten digits and natural images (Olshausen & Field, 1997) and then we evaluated its performance in predicting and interpolating 3D rotation on the NORB dataset (LeCun et al., 2004). In our test, we drew an image from one of the three databases, applied one of the transformation to generate another transformed image. This pair of images were feed into nodes x1 and x2 of our model, which was trained using the mixture transformation sequences of natural images in the previous section. The bottom-up top-down process simultaneously infer the latent contextual variables z and the subsequent frame x3 as the prediction of the model. In GBM, z would be ﬁrst inferred, and then applied to the second frame to generate the third frame. In contrast, the prediction of the next frame in our model is not limited to the second frame, but could be based on as many previous frames as stipulated by the model.  Figure 4: Prediction results (third row) generated by the model based on the ﬁrst two rows of frames. Figure 4 shows the results of predictions (third row) given the ﬁrst and second frames. They demon- strate our model’s ability to accomplish prediction using the top-down bottom-up algorithm. These ﬁndings show the contextual representation z encoded sufﬁcient content-invariant transformation information for providing contextual modulation to generate predictions. We used a similar parameter setting as that in Michalski et al. (2014). Each chirp sequence contained 160 frames in one second, partitioned into 16 non-overlapping 10-frame intervals, yielding 10- dimensional input vectors. The frequency of chirp signals varies between 15Hz and 20Hz. The task is to predict the subsequent intervals given the ﬁrst 5 intervals. The RMSE results per interval for each of the subsequent intervals being predicted is shown in Table 4.  Ours GBM GAE  Prediction  0.2194 0.2552 0.2281  Interpolation  0.3510  - -  Predict-ahead Interval  Ours RNN CRBM  1  0.78 0.71 1.00  2  0.87 0.85 1.03  3  0.93 0.95 1.01  4  1.03 1.04 1.02  5  1.02 1.02 1.04  6  1.04 1.03 1.06  Table 3: RMSE of prediction and inter- polation (“-” means not applicable). Interpolation can be accomplished in the same way. When x1 and x3 are provided to the model, z and x2 are simultaneously inferred. The second row of Figure 5 shows the interpolation results; GBM cannot perform such computation.  Table 4: RMSE of prediction on chirp signals  Figure 5: Interpolation results (second row) generated based on the other two rows of frames.  Next, we tested our model with a more challenging the NORB dataset that contains images of objects in 3D rotation, under different views and lighting conditions. There were 5 categories of objects (animals, human, cars, trucks, planes) with 10 objects in each category taken with a camera in 18 directions, 9 camera elevations and under 6 illuminations. We trained a model for each of the ﬁve object categories. Within each category, the data were divided into a training set and a test set based on their elevations. The test set for each model included all the images taken at two particular elevations (4th and 6th), and the training set included image sequences taken at the 7 other elevations. At each elevation, the camera was ﬁxed and the object was rotating in 3D across frames. To train the model for each category, we took sequences of three successive frames (each representing a view  7  Accepted as a workshop contribution at ICLR 2015  from a particular azimuth) of each object in under a particular condition to learn a 3-input model. We tested the model with two input images in a sequence taken from one of the untrained elevations. The prediction results of the NORB dataset were obtained in a similar manner for the face and digit cases by presenting the image frames to x1 and x2 to infer z and x3 simultaneously. The results of the prediction are shown in the third column of each object instance in Figure 6. These results are comparable to the reported in GBM, actually with slightly better performance (see Table 3). For all the prediction and interpolation results, we normalized the output images by matching the pixel histogram of the output image with that of the input images using histogram matching techniques.  Figure 6: Left: Prediction results (third row) of NORB. Right: Interpolation results (second row) of NORB. The receptive ﬁelds of the model trained with three or more consecutive frames in this database exhibited a quadrature phase relationship between adjacent frames. That means the ﬁlters for y1 and y3 have a phase shift of 180 degrees. With only the responses of these two ﬁlters to x1 and x3, but missing x2, the direction of motion is underdetermined – the movement could go in either direction. The model fails to interpolate in this case. We improved the temporal resolution of the model by training the ﬁlters for y1 and y3 with the sequences, which develop a quadrature phase relationship, then we ﬁxed the ﬁlters for y1 and y3 to train y3 with more sequences. This allowed the adjacent ﬁlters in the model to have ﬁner phase difference, and yielded reasonable interpolation results, as shown in Figure 6. A more elegant solution to this problem requires further investigation. We reported the performance (root mean square error) of our model on prediction and interpolation quantitatively in Table 3. We also used 3-way GBM and GAE in the prediction test as described in Section 4.2. All the models were trained using the mixture of motion sequences and tested on other image sequences. The result suggests that our model is comparable to, and in fact slightly outperforms, the gated machines in prediction. Additionally, our model can perform interpolation, which is not possible for GBM.  5 DISCUSSION  In this paper, we have presented a new predictive coding framework that can learn to encode contex- tual relationships to modulate analysis by synthesis during perception. As discussed in the Introduc- tion, this model is distinct from the standard predictive coding model (Rao & Ballard, 1999) and in addition to being conceptually novel, might be more biologically plausible. Our model shares some similarities with the autoencoder model but differs in that (1) our model uses contextual representa- tion to gate the prediction synthesis process while the autoencoder does not utilize context, (2) the autoencoder relies solely on fast feed-forward computation while our model utilizes a fast top-down and bottom-up procedure to update the contextual representations which modulate image synthesis during inference. Such contextual variables can be considered as a generalized form of the smooth- ness constraint in early vision, and can be implemented locally by interneurons in each particular visual area. A key contribution of this work is in demonstrating for the ﬁrst time the usefulness of local contextual modulation in the predictive coding or the auto-encoder framework. Recurrent neural networks still provide state-of-the-art performance in sequence modeling. But RNN requires a lot of data to train. Thus, despite its power in modeling short and long sequences, particularly when trained with large dataset, it falters in a more limited dataset like the NORB database. The predictive encoder proposed here learns contextual latent variables that can provide information about transformation explicitly while RNNs latent variables are used to represent the content of images only and the transformations are encoded in connections. The transforms encoded in the latent variables in the predictive encoder are directly related to the perceptual variables such as motion velocity, optical ﬂow and binocular disparity. Our model shares similar goals with the gated Boltzmann machine in learning transformational re- lationships but our model utilizes a different mechanism, with a uniﬁed framework for inference, interpolation and prediction. We consider the GBM as a state-of-art method for learning transform with limited amount of data and thus we have mostly focused our quantitative evaluation of our pre- dictive encoder model against the performance of GBM. We found that our model is comparable or superior in performance relative to the gated Boltzmann machine in terms of inference and prediction while being additionally able to perform interpolation. Our model relies on standard spatiotemporal  8  Accepted as a workshop contribution at ICLR 2015  ﬁltering in the feedforward path, without the need for the N-way multiplicative interaction or neural synchrony as required in the GBM. It is thus simpler in conceptualization and maybe more biologi- cally plausible. It is important to recognize that our model currently just is a module that uses latent variables to encode spatiotemporal local context and transformation. Such a module can be used to build recurrent neural networks to model the temporal evolution of the contextual variables, or be stacked up to form deep networks to learn hierarchical features, each layer with their own local spatiotemporal contextual representations.  6 ACKNOWLEDGMENTS This research was supported by research grants 973-2015CB351800, NSFC-61272027 (to YZ Wang), and NSF 1320651 and NIH R01 EY022247 (to TS Lee). Both Wang and Lee’s labs acknowl- edge the support of NVIDIA Corporation for the donation of GPUs for this research. Mingmin Zhao and Chengxu Zhuang were supported by Peking University and Tsinghua University undergraduate scholarships respectively when they visited Carnegie Mellon to carry out this research.  REFERENCES Bar, Moshe. The proactive brain: using analogies and associations to generate predictions. Trends  in cognitive sciences, 2007.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a cpu and gpu math expression compiler. In Proceedings of the Python for scientiﬁc computing confer- ence, 2010.  Friston, Karl. The free-energy principle: a uniﬁed brain theory? Nature Reviews Neuroscience,  2010.  Gourier, Nicolas, Hall, Daniela, and Crowley, James L. Estimating face orientation from robust de- tection of salient facial structures. In FG Net Workshop on Visual Observation of Deictic Gestures. FGnet (IST–2000–26434) Cambridge, UK, 2004.  Hinton, Geoffrey. A practical guide to training restricted boltzmann machines. Momentum, 2010. Hinton, Geoffrey, Osindero, Simon, and Teh, Yee-Whye. A fast learning algorithm for deep belief  nets. Neural computation, 2006.  LeCun, Yann, Huang, Fu Jie, and Bottou, Leon. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition, CVPR 2004. Proceedings of the IEEE Computer Society Conference on, 2004.  Lee, Tai Sing and Mumford, David. Hierarchical bayesian inference in the visual cortex. JOSA A,  2003.  Memisevic, R. Learning to relate images. Pattern Analysis and Machine Intelligence, IEEE Trans-  actions on, 35(8):1829–1846, Aug 2013. ISSN 0162-8828. doi: 10.1109/TPAMI.2013.53.  Memisevic, Roland. Gradient-based learning of higher-order image features. In ICCV. IEEE, 2011. Michalski, Vincent, Memisevic, Roland, and Konda, Kishore. Modeling deep temporal dependen-  cies with recurrent “grammar cells”. In NIPS, pp. 1925–1933, 2014.  Mumford, David. On the computational architecture of the neocortex. Biological cybernetics, 1992. Ngiam, Jiquan, Coates, Adam, Lahiri, Ahbik, Prochnow, Bobby, Le, Quoc V, and Ng, Andrew Y.  On optimization methods for deep learning. In ICML, 2011.  Olshausen, Bruno A and Field, David J. Sparse coding with an overcomplete basis set: A strategy  employed by v1? Vision research, 1997.  Rao, Rajesh PN and Ballard, Dana H. Predictive coding in the visual cortex: a functional interpre-  tation of some extra-classical receptive-ﬁeld effects. Nature neuroscience, 1999.  Susskind, Joshua, Memisevic, Roland, Hinton, Geoffrey, and Pollefeys, Marc. Modeling the joint  density of two images under a variety of transformations. In CVPR. IEEE, 2011.  Todorovic, Ana, van Ede, Freek, Maris, Eric, and de Lange, Floris P. Prior expectation mediates neural adaptation to repeated sounds in the auditory cortex: an meg study. The Journal of neuro- science, 2011.  Van der Maaten, Laurens and Hinton, Geoffrey. Visualizing data using t-sne. Journal of Machine  Learning Research, 9(2579-2605):85, 2008.  Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Extracting and  composing robust features with denoising autoencoders. In ICML, 2008.  9  ",
1412.6249,2015, Purine: A Bi-Graph based deep learning framework,"['Purine: A Bi-Graph based deep learning framework', 'Min Lin', 'Shuo Li', 'Xuan Luo', 'and Shuicheng Yan']",https://arxiv.org/pdf/1412.6249,"5 1 0 2    r p A 6 1         ] E N . s c [      5 v 9 4 2 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  PURINE: A BI-GRAPH BASED DEEP LEARNING FRAME- WORK  Min Lin1,2, Shuo Li3, Xuan Luo3 & Shuicheng Yan2 1. Graduate School of integrated Sciences and Engineering 2. Department of Electrical and Computer Engineering National University of Singapore 3. Zhiyuan College, Shanghai Jiao Tong University {linmin, eleyans}@nus.edu.sg li3shuo1@gmail.com roxanneluo@sjtu.edu.cn  ABSTRACT  In this paper, we introduce a novel deep learning framework, termed Purine. In Purine, a deep network is expressed as a bipartite graph (bi-graph), which is com- posed of interconnected operators and data tensors. With the bi-graph abstraction, networks are easily solvable with event-driven task dispatcher. We then demon- strate that different parallelism schemes over GPUs and/or CPUs on single or multiple PCs can be universally implemented by graph composition. This eases researchers from coding for various parallelization schemes, and the same dis- patcher can be used for solving variant graphs. Scheduled by the task dispatcher, memory transfers are fully overlapped with other computations, which greatly reduces the communication overhead and helps us achieve approximate linear ac- celeration.  1  INTRODUCTION  The need for training deep neural networks on large-scale datasets has motivated serveral research works that aim to accelerate the training process by parallelising the training on multiple CPUs or GPUs. There are two different ways to parallelize the training. (1) Model parallelism: the model is distributed to different computing nodes (Sutskever et al., 2014) (2) Data parallelism: different nodes train on different samples for the same model (Seide et al., 2014; Chilimbi et al., 2014). Some of the works even use a hybrid of them (Krizhevsky, 2014; Dean et al., 2012; Le, 2013). For data parallelism, there are also two schemes regarding communication between the peers. (1) the allreduce approach where all updates from the peers are aggregated at the synchronization point and the averaged update is broadcasted back to the peers (Seide et al., 2014; Krizhevsky, 2014). (2) the parameter server approach handles the reads and writes of the parameters asynchronously (Dean et al., 2012; Le, 2013; Chilimbi et al., 2014). Efﬁcient implementations of the various parallelization schemes described by previous works are non-trivial. To facilitate the implementation of various parallelization schemes, we built a bigraph-based deep learning framework called “Purine”. It is named “Purine”, which is an analog of caffeine in molec- ular structure, because we beneﬁted a lot from the open source Caffe framework (Jia et al., 2014) in our research and the math functions used in Purine are ported from Caffe.  2 BI-GRAPH ABSTRACTION  Purine abstracts the processing procedure of deep neural networks into directed bipartite graphs (Bi-Graphs). The Bi-Graph contains two types of vertices, tensors and operators. Directed edges are only between tensors and operators and there is no interconnection within tensors or operators. Figure 1 illustrates the Bi-Graph for the convolution layer deﬁned in Caffe. All feed-forward neural nets can be represented by a directed acyclic bipartite graph, which can be solved by a universal task dispatcher. There are several works that use similar abstractions. For  1  Accepted as a workshop contribution at ICLR 2015  Figure 1: (a) shows the convolution layer deﬁned in Caffe together with its inputs and outputs. (b) is the corresponding bipartite graph that describes the underlying computation inside the convolution layer. There are two types of vertices in the Bi-Graph. Boxes represent data tensors and the circles represent operators. Operators are functions of the incoming tensors and the results of the functions are put in the outgoing tensors.  example, the dataﬂow graph in Dryad (Isard et al., 2007) and Pig Latin (Olston et al., 2008) are the same as the Bi-Graph abstraction introduced in this paper. Graphlab (Low et al., 2010) proposed a more general abstraction which is applicable to iterative algorithms. However, these systems are designed for general problems and do not support GPU. Theano (Bergstra et al., 2010) compiles math expressions and their symbolic differentiations into graphs for evalutation. Though it supports GPU and is widely used for deep learning, the ability to parallelize over multiple GPUs and over GPUs on different machines is not complete.  2.1 TASK DISPATCHER  Purine solves the Bi-Graph by scheduling the operators within the Bi-Graph with an event-driven task dispatcher. Execution of an operator is triggered when all the incoming tensors are ready. A tensor is ready when all its incoming operators have completed computation. The computation of the Bi-Graph starts from the sources of the graph and stops when all the sinks are reached. This process is scheduled by the task dispatcher.  2.2  ITERATIONS  Although it has been argued in (Low et al., 2010) that the directed acyclic graph could not effectively express iterative algorithms as the graph structure would depend on the number of iterations. We overcome this by iteration of the graphs. Because the task dispatcher waits until all the sinks of the graph are reached, it acts as a synchronization point. Thus parallelizable operations can be put in a single graph, while sequential tasks (iterations) are implemented by iteration of graphs. A concrete example is shown in Figure 2.  3 PARALLELIZATION  Parallelization of the Bi-Graph on a cluster of CPUs or GPUs or mixed can be easily implemented by introducing a “location” property for the tensors and operators. The “location” property uniquely identiﬁes the computation resource (CPUs/GPUs) on which a tensor/operator should be allocated. The “location” property comprises two ﬁelds: hostname and device id. In a multi-machine cluster, hostname identiﬁes the machine that the vertice resides on. Device id speciﬁes whether the ten- sor/operator should be allocated on CPU or GPU and the ordinal of the GPU if there are multiple GPUs installed on a single machine. Besides the “location” property, another property “thread” is assigned to operators because both CPU and GPU support multithreading. Operators with the same  2  ConvWeightBottomConv w.r.t bottomΔBottomConv w.r.t weightΔWeightAdd biasBiasΔBiasBias gradientTopΔTopBottomTopConvolution LayerΔTopΔBottom(a) Caffe Convolution Layer(b) Bipartite GraphAccepted as a workshop contribution at ICLR 2015  Figure 2: Implementation of SGD by Graph iteration. Every iteration of SGD calculates a modiﬁ- cation of the network parameter, and updates the parameter before the next iteration. Since direct updating the network parameter would form a cyclic loop in the graph, it is dissected into two parts. (a) The DNN graph calculates the updated parameters and places them in “new params”, (b) The swap graph will swap the memory address of the “new” and “old” parameters. As a whole, SGD is implemented by iterating the two graphs as in (c).  thread id will be queued in the same thread, while those with different ids are parallelized whenever possible. It is up to the user to decide the assignment of the graph over the computation resources.  3.1 COPY OPERATOR  In the multidevice setting, data located on one device are not directly accessible by operators on another. Thus a special “Copy” operator is introduced to cross the boundary, connecting parts of the Bi-Graph on individual devices. The Copy operators, just like other operators, are scheduled by the task dispatcher. Therefore it is straightforward to overlap copy operations with other computation tasks by assigning different threads to them.  3.2 TASK DISPATCHER  In the case of single machine and multiple devices, only one dispatcher process is launched. Op- erators are associated to their threads and scheduled by the global task dispatcher. In the case of multiple machines and multiple devices, individual dispatcher processes are launched on each of the machines. Copy operators that copy data from machine A to machine B are sinks on machine A and sources on machine B. This way, each machine only needs to schedule its own subgraph and no global scheduling mechanism or communication between dispatchers is necessary.  3.3 MODEL PARALLELISM  We demonstrate how model parallelism can be implemented in Purine by taking a two-layer fully connected neural network as example. It can be extended to deeper networks easily. As is shown in Figure 3, execution of the two-layer network can be divided into three sequential steps. They are labeled as A, B, C correspondingly. To keep resources busy all the time, the network is replicated three times and executed in order.  3.4 DATA PARALLELISM  Data parallelism is a simple yet straightforward way to parallelize deep networks. In data paral- lelism, computation peers each keep a replicate of the deep network. The communication between peers can be either synchronous or asynchronous. In the synchonous case, the gradients from peers are gathered by the parameter server. The updated parameter is calculated and copied back to all the peers. A hybrid of data parallelism and model parallelism has previously been proposed by Krizhevsky (2014) in which the convolution layers use data parallelism and fully connected layers use model parallelism. This is based on the observation that the number of parameters is large and thus the communication cost is big for fully connected layers. The hybrid approach greatly reduces the communication cost. A different approach to reduce communication overhead is to overlap the data  3  DNNparamsnew paramsSwapparamsnew params(a)(b)(c)DNN, Swap, DNN, Swap, ... ... Accepted as a workshop contribution at ICLR 2015  Figure 3: Implementing model parallelism in Purine. (a) The two-layer network can be divided into three subgraphs which execute in sequence. (b) The network is replicated three times and executed in order.  transfer with computations. Double buffering is proposed by Seide et al. (2014) to break a minibatch in half and exchange the gradients of the ﬁrst half while doing computaion of the second half. With the scheduling of the task dispatcher in Purine, we propose a more straightforward way to hide the communication overhead. We show that data parallelism is feasible even for fully connected layers, especially when the network is very deep. Since the fully connected layers are usually at the top of the neural networks, exchange of the parameter gradients can be overlapped with the backward computation of the convolution layers. As is shown in Figure 4, exchange of gradients in the higher layer can be overlapped with the computation of lower layers. Gradient exchange of lower layers could be less of a problem because they usually have a much smaller number of parameters.  Figure 4: Overlapping communication with computation.  4  Inner Product LayerSoftmax LayerSoftmaxSoftmax DiffLossLabelCopy OpCopied Blob on another locationDevice 2Device 1ACBA1  A2  A3  A1  A2  A3 ......B1  B2  B3  B1  B2  B3 ......C1  C2  C3  C1  C2  C3 ......Iterations(a)(b)Gradient exchangeGreen arrows can overlap in timeForwardBackwardWeightsGradientsAccepted as a workshop contribution at ICLR 2015  4 RESULTS  We carried out experiments on the Purine framework with data parallelism on GoogLeNet (Szegedy et al., 2014). Data parallelism often results in larger batch sizes, which are unfavorable for SGD convergence demonstrated by previous studies. In this paper we ignored the possible change in convergence rate but instead studied how much more data can be processed per unit time with the parallelization. We compared the number of images processed per second for GoogLeNet with different numbers of GPUs for data parallelism. The batch size we use is 128 per GPU. There are 3 GPUs installed on each workstation, interconnected with 10 Gigabit Ethernet. As is shown in Table 1, the speed increases linearly with more GPUs added. The speed is faster than the previous version of this paper because we upgraded the CUDNN library to version 2, which is faster compared to version 1. Note that the machines are connected by 10 gigabit ethernet and thus data on GPU need to go through CPU memory to be tranferred over the ethernet. Even with this limitation, the speed up is linear thanks to the overlapping of communication with computation.  Table 1: Number of images per second with increasing number of GPUs. (GPU number smaller than or equal to 3 are tested on single machine. Performances of GPU number larger than 3 are on different machines interconnected with 10 Gigabit Ethernet.)  Number of GPUs Images per second  1  112.2  2  222.6  3  336.8  6  673.7  9  1010.5  12  1383.7  Running GoogLeNet with 4 GPUs on a single machine is proﬁled and shown in Figure 5. It can be seen that the memory copy of model parameters between CPUs and GPUs is fully overlapped with the computations in the backward pass. The only overhead is in the ﬁrst layer of the network, which results in the gap between iterations. It is favorable to have small batch size in stochastic gradient descent. However, regarding paral- lelism, it is more favorable to have larger batch size and thus higher computation to communication ratio. We searched for the minumum batch size possible to achieve linear speed up by exploring different batch sizes.  Figure 5: Proﬁling results of Purine. Memory copies (row 1 and 2) are overlapped with computation (row 3). The only overhead is the memory copy of ﬁrst convolution layer, which results in the gap between iterations.  Table 2: Number of images per second with 12 GPUs and different batch sizes. Batch size per GPU Images per second Acceleration Ratio  1299.1 11.26  1292.3 11.20  1279.7 11.09  1230.2 10.66  1099.8 9.53  1383.7  128  12  64  56  48  40  32  Table 2 shows the processing speed with different batch sizes. The acceleration ratio is not reduced much with batch size 64 as compared to 128. We can still achieve 9.53 fold acceleration with 12 GPUs when the batch size is set to 32.  5  Start of graphEnd of graphCUDA Kernel CallMemcpy GPU to CPUMemcpy CPU to GPUForwardBackwardAccepted as a workshop contribution at ICLR 2015  REFERENCES James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU In Proceedings of the Python for Scientiﬁc Computing Conference math expression compiler. (SciPy), June 2010. Oral Presentation.  Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project adam: In Proceedings of the 11th Building an efﬁcient and scalable deep learning training system. USENIX conference on Operating Systems Design and Implementation, pages 571–582. USENIX Association, 2014.  Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in Neural Information Processing Systems, pages 1223–1231, 2012.  Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly. Dryad: distributed data- parallel programs from sequential building blocks. In ACM SIGOPS Operating Systems Review, volume 41, pages 59–72. ACM, 2007.  Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Ser- gio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed- ding. In Proceedings of the ACM International Conference on Multimedia, pages 675–678. ACM, 2014.  Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint  arXiv:1404.5997, 2014.  Quoc V Le. Building high-level features using large scale unsupervised learning.  In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8595– 8598. IEEE, 2013.  Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos Guestrin, and Joseph M arXiv preprint  Hellerstein. Graphlab: A new framework for parallel machine learning. arXiv:1006.4990, 2010.  Christopher Olston, Benjamin Reed, Utkarsh Srivastava, Ravi Kumar, and Andrew Tomkins. Pig latin: a not-so-foreign language for data processing. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1099–1110. ACM, 2008.  Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.  Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural net-  works. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.  Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.  6  ",
1504.01989,2015, Pixel-wise Deep Learning for Contour Detection,"['Pixel-wise Deep Learning for Contour Detection', 'Jyh-Jing Hwang and Tyng-Luh Liu']",https://arxiv.org/pdf/1504.01989,"5 1 0 2    r p A 8         ]  V C . s c [      1 v 9 8 9 1 0  .  4 0 5 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  PIXEL-WISE DEEP LEARNING FOR CONTOUR DETEC- TION  Jyh-Jing Hwang & Tyng-Luh Liu Institute of Information Science Academia Sinica Taipei, Taiwan {jyhjinghwang,liutyng}@iis.sinica.edu.tw  ABSTRACT  We address the problem of contour detection via per-pixel classiﬁcations of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efﬁcient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classiﬁer to accomplish contour detection. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and verify their performance on BSDS500.  1  INTRODUCTION  We consider deep nets to construct a per-pixel feature learner for contour detection. As the task is essentially a classiﬁcation problem, we adopt deep convolutional neural networks (CNNs) to establish a discriminative approach. However, one subtle deviation from typical applications of CNNs should be emphasized. In our method, we intend to use the CNN architecture, e.g., AlexNet (Krizhevsky et al., 2012), to generate features for each image pixel, not just a single feature vector for the whole input image. Such a distinction would call for a different perspective of parame- ter ﬁne-tuning so that a pre-trained per-image CNN on ImageNet (Deng et al., 2009) can be adapted into a new model for per-pixel edge classiﬁcations. To investigate the property of the features from different convolutional layers, we carry out a number of experiments to evaluate their effectiveness in performing contour detection on the benchmark BSDS Segmentation dataset (Martin et al., 2001).  2 PER-PIXEL CNN FEATURES  Learning features by employing a deep architecture of neural net has been shown to be effective, but most of the existing techniques focus on yielding a feature vector for an input image (or image patch). Such a design may not be appropriate for vision applications that require investigating image characteristics in pixel level. For contour detection, the central task is to decide whether an under- lying pixel is an edge point or not. Thus, it would be convenient that the deep network could yield per-pixel features. To this end, we extract per-pixel CNN features in AlexNet (Krizhevsky et al., 2012) using DenseNet (Iandola et al., 2014), and pixel-wise concatenate them to feed into a support vector machine (SVM) classiﬁer.  Our implementation uses DenseNet for CNN feature extraction owing to its efﬁciency, ﬂexibility, and availability. DenseNet is an open source system that computes dense and multiscale features from the convolutional layers of a Caffe CNN based object classiﬁer. The process of feature ex- traction proceeds as follows. Given an input image, DenseNet computes its multiscale versions and stitches them to a large plane. After processing the whole plane by CNNs, DenseNet would unstitch the descriptor planes and then obtain multiresolution CNN descriptors.  The dimensions of convolutional features are ratios of the image size, e.g., one-fourth for Conv1, and one-eighth for Conv2. We rescale feature maps of all the convolutional layers to the image size. That is, there is a feature vector in every pixel. The dimension of the resulting feature vector is 1376 × 1, which is concatenated by Conv1 (96 × 1), Conv2 (256 × 1), Conv3 (384 × 1), Conv4 (384 × 1), and Conv5 (256 × 1).  1  Accepted as a workshop contribution at ICLR 2015  Table 1: Contour detection results of using CNN features from different layers.  Conv1 Conv2 Conv3 Conv4 Conv5 Conv1-5  ODS .627 .660 OIS AP .625  .699 .718 .712  .655 .670 .619  .654 .667 .615  .604 .620 .546  .741 .759 .757  For classiﬁcation, we use the combined per-pixel feature vectors to learn a binary linear SVM. It is worth noting that, in our multiscale setting, we train the SVM based on only the original resolution. In test time, we classify test images using both the original and the double resolutions. We average the two resulting edge maps for the ﬁnal output of contour detection.  3 EXPERIMENTAL RESULTS  We test our method on the Berkeley Segmentation Dataset and Benchmark (BSDS500) (Martin et al., 2001; Arbelaez et al., 2011). To better assess the effects of the features of different layers, we report their respective performance of contour detection. The BSDS500 dataset con- tains 200 training, 100 validation, and 200 testing images. Boundaries in each image are labeled by several workers and are averaged to form the ground truth. The accuracy of contour detection is evaluated by three measures: the best F-measure on the dataset for a ﬁxed threshold (ODS), the aggregate F-measure on the dataset for the best threshold in each image (OIS), and the average pre- cision (AP) on the full recall range (Arbelaez et al., 2011). Prior to evaluation, we apply a standard non-maximal suppression technique to edge maps to obtain thinned edges (Canny, 1986).  In Table 1, we see that features in Conv2 contribute the most, and then Conv3 and Conv4. These suggest that low- to mid-level features are most useful for contour detection, while the lowest- and higher-level features provide additional boost. Although features in Conv1 and Conv5 are less effective when employed alone, we achieve the best results by combining all ﬁve streams. It indicates that the local edge information in low-level features and the object contour information in higher- level features are both necessary for achieving high performance in contour detection tasks.  ACKNOWLEDGMENTS  This work was supported in part by NSC 102-2221-E-001-021-MY3.  REFERENCES Arbelaez, Pablo, Maire, Michael, Fowlkes, Charless, and Malik, Jitendra. Contour detection and hierarchical image segmentation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(5):898–916, 2011.  Canny, John. A computational approach to edge detection. Pattern Analysis and Machine Intelli-  gence, IEEE Transactions on, (6):679–698, 1986.  Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. ImageNet: A large-scale  hierarchical image database. In CVPR, pp. 248–255, 2009.  Iandola, Forrest, Moskewicz, Matt, Karayev, Sergey, Girshick, Ross, Darrell, Trevor, and Keutzer, Kurt. Densenet: Implementing efﬁcient convnet descriptor pyramids. arXiv preprint arXiv:1404.1869, 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Martin, David, Fowlkes, Charless, Tal, Doron, and Malik, Jitendra. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecolog- ical statistics. In Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on, volume 2, pp. 416–423. IEEE, 2001.  2  ",
1412.5335,2015, Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews,"['Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews', 'Grégoire Mesnil', 'Tomas Mikolov', ""Marc'Aurelio Ranzato"", 'and Yoshua Bengio']",https://arxiv.org/pdf/1412.5335,"5 1 0 2     y a M 7 2         ] L C . s c [      7 v 5 3 3 5  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  ENSEMBLE OF GENERATIVE AND DISCRIMINATIVE TECHNIQUES FOR SENTIMENT ANALYSIS OF MOVIE REVIEWS  Gr´egoire Mesnil University of Montr´eal University of Rouen  Tomas Mikolov & Marc’Aurelio Ranzato Facebook Artiﬁcial Intelligence Research  Yoshua Bengio University of Montr´eal  ABSTRACT  Sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document (typically a consumer review). In the simplest settings, we discriminate only between positive and negative sentiment, turning the task into a standard binary classiﬁcation problem. We compare several ma- chine learning approaches to this problem, and combine them to achieve a new state of the art. We show how to use for this task the standard generative lan- guage models, which are slightly complementary to the state of the art techniques. We achieve strong results on a well-known dataset of IMDB movie reviews. Our results are easily reproducible, as we publish also the code needed to repeat the experiments. This should simplify further advance of the state of the art, as other researchers can combine their techniques with ours with little effort.  1  INTRODUCTION  Sentiment analysis is among the most popular, simple and useful tasks in natural language process- ing. It aims at predicting the attitude of text, typically a sentence or a review. For instance, movies or restaurant are often rated with a certain number of stars, which indicate the degree to which the reviewer was satisﬁed.  This task is often considered as one of the simplest in NLP because basic machine learning tech- niques can yield strong baselines (Wang & Manning, 2012), often beating much more intricate ap- proaches (Socher et al., 2011). In the simplest settings, this task can be seen as a binary classiﬁcation between positive and negative sentiment.  However, there are several challenges towards achieving the best possible accuracy. It is not obvious how to represent variable length documents beyond simple bag of words approaches that lose word order information. One can use advanced machine learning techniques such as recurrent neural net- works and their variations (Mikolov et al., 2010; Socher et al., 2011), however it is not clear if these provide any signiﬁcant gain over simple bag-of-words and bag-of-ngram techniques (Pang & Lee, 2008; Wang & Manning, 2012).  In this work, we compared several different approaches and realized, without much surprise, that model combination performs better than any individual technique. The ensemble best beneﬁts from models that are complementary, thus having diverse set of techniques is desirable. The vast major- ity of models proposed in the literature are discriminative in nature, as their parameters are tuned for the classiﬁcation task directly. In this work, we boost the performance of the ensemble by considering a generative language model. To this end, we train two language models, one on the positive reviews and one on the negative ones, and use the likelihood ratio of these two models  1  Accepted as a workshop contribution at ICLR 2015  evaluated on the test data as an additional feature. For example, we assume that a positive re- view will have higher likelihood to be generated by a model that was trained on a large set of positive reviews, and lower likelihood given the negative model. In this paper, we constrained our work to binary classiﬁcation where we trained two generative models, positive and negative. One could consider a higher number of classes since this approach scales linearily with the number of models to be train, i.e. one for each class. The large pool of diverse models is a) simple to im- plement (in line with previous work by Wang and Manning (Wang & Manning, 2012)) and b) it yields state of the art performance on one of the largest publicly available benchmarks of movie reviews, the Stanford IMDB dataset of reviews. Code to reproduce our experiments is available at https://github.com/mesnilgr/iclr15.  2 DESCRIPTION OF THE MODELS  In this section we describe in detail the approaches we considered in our study. The novelty of this paper consists in combining both generative and discriminative models together for sentiment prediciton.  2.1 GENERATIVE MODEL  A generative model deﬁnes a distribution over the input. By training a generative model for each class, we can then use Bayes rule to predict which class a test sample belongs to. More formally, given a dataset of pairs {x(i), y(i)}i=1,...,N where x(i) is the i-th document in the training set, y(i) ∈ {−1, +1} is the corresponding label and N is the number of training samples, we train two models: p+(x|y = +1) for {x(i) subject to y(i) = +1} and p−(x|y = −1) for {x subject to y = −1}. Then, given an input x at test time we compute the ratio (derived from Bayes rule): r = p+(x|y = +1)/p−(x|y = −1) × p(y = +1)/p(y = −1). If r > 1, then x is assigned to the positive class, otherwise to the negative class.  is the k-th word in the i-th document.  We have a few different choices of distribution we can choose from. The most common one is the n-gram, a count-based non-parametric method to compute p(x(i) k−N +1), where x(i) In order to compute the likelihood of a document, we use the Markov assumption and simply multiply the n-gram probabilities over all words in the document: p(x(i)) = QK k−N +1). As mentioned before, we train one n-gram language model using the positive documents and one model using the negative ones.  k−2, . . . , x(i)  k−2, . . . , x(i)  k=1 p(x(i)  k−1, x(i)  k−1, x(i)  k |x(i)  k |x(i)  k  In our experiments, we used SRILM toolkit (Stolcke et al., 2002) to train the n-gram language mod- els using modiﬁed Kneser-Ney smoothing (Kneser & Ney, 1995). Furthermore, as both language models are trained on different datasets, there is a mismatch between vocabularies: some words can appear only in one of the training sets. This can be a problem during scoring, as the test data contain novel words that were not seen in at least one of the training datasets. To avoid this problem, it is needed to add penalty during scoring for each out of vocabulary word.  N-grams are a very simple data-driven way to build language models. However, they suffer from both data sparsity and large memory requirement. Since the number of word combinations grows exponentially with the length of the context, there is always little data to accurately estimate proba- bilities for higher order n-grams.  In contrast with N-grams languages models, Recurrent neural networks (RNNs) (Mikolov et al., 2010) are parametric models that can address these issues. The inner architecture of the RNNs gives them potentially inﬁnite context window, allowing them to perform smoother predictions. We know that in practice, the context window is limited due to exploding and vanishing gradients (Pascanu et al., 2012). Still, RNNs outperform signiﬁcantly n-grams and are the state of the art for statistical language modeling. A review of these techniques is beyond the scope of this short paper and we point the reader to (Mikolov, 2012) for a more in depth discussion on this topic.  Both when using n-grams and RNNs, we compute the probability of the test document belonging to the positive and negative class via Bayes’ rule. These scores are then averaged in the ensemble with other models, as explained in Section 2.4.  2  Accepted as a workshop contribution at ICLR 2015  Table 1: Performance of SVM with Wang & Manning (2012) rescaling for different N-grams  Input features Unigrams Unigrams+Bigrams Unigrams+Bigrams+Trigrams  Accuracy 88.61% 91.56% 91.87%  2.2 LINEAR CLASSIFICATION OF WEIGHTED N-GRAM FEATURES  Among purely discriminative methods, the most popular choice is a linear classiﬁer on top of a bag- of-word representation of the document. The input representation is usually a tf-idf weighted word counts of the document. In order to preserve local ordering of the words, a better representation would consider also the position-independent n-gram counts of the document (bag-of-n-grams).  In our ensemble, we used a supervised reweighing of the counts as in the Naive Bayes Support Vector Machine (NB-SVM) approach (Wang & Manning, 2012). This approach computes a log-ratio vector between the average word counts extracted from positive documents and the average word counts extracted from negative documents. The input to the logistic regression classiﬁer corresponds to the log-ratio vector multiplied by the binary pattern for each word in the document vector. Note that the logictic regression can be replaced by a linear SVM. Our implementation1 slightly improved the performance reported in (Wang & Manning, 2012) by adding tri-grams (improvement of +0.6%), as shown in Table 1.  2.3 SENTENCE VECTORS  Recently, (Le & Mikolov, 2014) proposed an unsupervised method to learn distributed represen- tations of words and paragraphs. The key idea is to learn a compact representation of a word or paragraph by predicting nearby words in a ﬁxed context window. This captures co-occurence statis- tics and it learns embeddings of words and paragraphs that capture rich semantics. Synonym words and similar paragraphs often are surrounded by similar context, and therefore, they will be mapped into nearby feature vectors (and vice versa).  Such embeddings can then be used to represent a new document (for instance, by averaging the representations of the paragraphs that constitute the document) via a ﬁxed size feature vector. The authors then use such a document descriptor as input to a one hidden layer neural network for sentiment discrimination.  2.4 MODEL ENSEMBLE  In this work, we combine the log probability scores of the above mentioned models via linear inter- polation. More formally, we deﬁne the overall probability score as the weighted geometric mean of baseline models: p(y = +1|x) = Q pk(y = +1|x)αk , with αk > 0. We ﬁnd the best setting of weights via brute force grid search, quantizing the coefﬁcient values in the interval [0, 1] at increments of 0.1. The search is evaluated on a validation set to avoid overﬁtting. We do not focus on a smarter way to ﬁnd the α since we consider only 3 models in our approach and we consider it out of the scope of this paper. Using more models would make the use of such method prohibitive. For a larger number of models, one might want to consider random search of the α coefﬁcients or even Bayesian approaches as these techniques will give better running time performance.  3 RESULTS  In this section we report results on one of the largest publicly available sentiment analysis datasets, the IMDB dataset of movie reviews. The dataset consists of 50, 000 movie reviews which are cat- egorized as being either positive or negative. We use 25, 000 reviews for training and the rest for  1https://github.com/mesnilgr/nbsvm  3  Accepted as a workshop contribution at ICLR 2015  Table 2: Performance of Individual Models  Accuracy Single Methods N-gram 86.5% RNN-LM 86.6% Sentence Vectors 88.73% NB-SVM Trigram 91.87%  Table 3: Performance of Different Model Combinations  Ensemble RNN-LM + NB SVM Trigram RNN-LM + Sentence Vectors Sentence Vectors + NB-SVM Trigrams All State of the art  Accuracy 92.13% 90.4% 92.39% 92.57% 91.22%  testing, using the same protocol proposed by (Maas et al., 2011). All experiments can be reproduced using the code available at https://github.com/mesnilgr/iclr15.  Table 2 reports the results of each individual model. We have found that generative models per- formed the worst, with RNNs slightly better than n-grams. The most competitive method is the method based on reweighed bag-of-words (Wang & Manning, 2012) 2. Favoring simplicity and reproducibility of our performance, all results reported in this paper were produced by a linear clas- siﬁer.  Finally, Table 3 reports the results of combining the previous models into an ensemble. When we interpolate the scores of RNN, sentence vectors and NB-SVM, we achieve a new state-of-the-art performance of 92.57%, to be compared to 91.22% reported by (Wang & Manning, 2012). Notice that our implementation of the Sentence Vectors method (Le & Mikolov, 2014) alone yielded only 88.73% (a difference of ≃ 4%). In order to measure the contribution of each model to the ﬁnal en- semble classiﬁer, we remove one model at a time from the ensemble. We observe that the removal of the generative model affects the least the ensemble performance. Overall, all three models contribute to the success of the overall ensemble, suggesting that these three models pick up complimentary features useful for discrimination. In Table 4, we show test reviews misclassiﬁed by single models but classiﬁed accurately by the ensemble.  4 CONCLUSION  We have proposed a very simple yet powerful ensemble system for sentiment analysis. We combine three rather complementary and conceptually different baseline models: one based on a generative approach (language models), one based on continuous representations of sentences and one based on a clever reweighing of tf-idf bag-of-word representation of the document. Each such model contributes to the success of the overall system, achieving the new state of the art performance on the challenging IMDB movie review dataset. Code to reproduce our experiments is available at: https://github.com/mesnilgr/iclr15. We hope researchers will take advantage of our code to include their new results into our ensemble and focus on improving the state of the art for Sentiment Analysis.  REFERENCES Kneser, Reinhard and Ney, Hermann. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on,  2In our experiments, to match the results from (Le & Mikolov, 2014), we followed the suggestion by Quoc Le to use hierarchical softmax instead of negative sampling. However, this produces the 92.6% accuracy result only when the training and test data are not shufﬂed. Thus, we consider this result to be invalid.  4  Accepted as a workshop contribution at ICLR 2015  Table 4: Reviews misclassiﬁed by Single Models but classiﬁed accurately by the Ensemble  Model  NB-SVM  RNN-LM  Sentence Vector  Sentences (positive) a really realistic , sensible movie by ramgopal verma . no stupidity like songs as in other hindi movies . class acting by nana patekar . much similarities to real ’encounters’ . (negative) leslie nielson is a very talented actor , who made a huge mistake by doing this ﬁlm . it doesn’t even come close to being funny . the best word to describe it is stupid ! (positive) this is a good ﬁlm . this is very funny . yet after this ﬁlm there were no good ernest ﬁlms ! (negative) a real hoot , unintentionally . sidney portier’s character is so sweet and lovable you want to smack him . nothing about this movie rings true . and it’s boring to boot . (positive) this movie is based on the novel island of dr . moreau by version by john frankenheimer . (negative) if it wasn’t for the terriﬁc music , i would not hesitate to give this cinematic underachievement 2/10 . but the music actually makes me like certain passages , and so i give it 5/10 .  volume 1, pp. 181–184. IEEE, 1995.  Le, Quoc V. and Mikolov, Tomas. Distributed representations of sentences and documents.  International Conference on Machine Learning, 2014.  In  Maas, Andrew L., Daly, Raymond E., Pham, Peter T., Huang, Dan, Ng, Andrew Y., and Potts, Christopher. Learning word vectors for sentiment analysis. In Proceedings of the Annual Meet- ing of the Association for Computational Linguistics. Association for Computational Linguistics, 2011.  Mikolov, Tom´aˇs. Statistical language models based on neural networks. PhD thesis, 2012.  Mikolov, Tomas, Karaﬁ´at, Martin, Burget, Lukas, Cernock`y, Jan, and Khudanpur, Sanjeev. Recur-  rent neural network based language model. In INTERSPEECH, pp. 1045–1048, 2010.  Pang, Bo and Lee, Lillian. Opinion mining and sentiment analysis. Foundations and trends in  information retrieval, 2(1-2):1–135, 2008.  Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the difﬁculty of training recurrent  neural networks. arXiv preprint arXiv:1211.5063, 2012.  Socher, Richard, Pennington, Jeffrey, Huang, Eric, Ng, Andrew, and Manning, Christopher D. Semi- supervised recursive autoencoders for predicting sentiment distributions. Conference on Empiri- cal Methods in Natural Language Processing, 2011.  Stolcke, Andreas et al. Srilm-an extensible language modeling toolkit. In INTERSPEECH, 2002.  Wang, Sida and Manning, Christopher D. Baselines and bigrams: Simple, good sentiment and topic classiﬁcation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pp. 90–94. Association for Computational Linguistics, 2012.  5  ",
1503.08873,2015, Fast Label Embeddings for Extremely Large Output Spaces,"['Fast Label Embeddings for Extremely Large Output Spaces', 'Paul Mineiro and Nikos Karampatziakis']",https://arxiv.org/pdf/1503.08873,"5 1 0 2    r a     M 0 3      ]  G L . s c [      1 v 3 7 8 8 0  .  3 0 5 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  FAST LABEL EMBEDDINGS FOR EXTREMELY LARGE OUTPUT SPACES  Paul Mineiro & Nikos Karampatziakis Microsoft Cloud Information Services Lab {pmineiro,nikosk}@microsoft.com  ABSTRACT  Many modern multiclass and multilabel problems are characterized by increas- ingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efﬁciency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a random- ized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Direc- tory Project, where we obtain state of the art results.  1 CONTRIBUTIONS  We provide a statistical motivation for label embedding by demonstrating that the optimal rank- constrained least squares estimator can be constructed from an optimal unconstrained estimator of an embedding of the labels. In other words, embedding can provide beneﬁcial sample complexity reduction even if computational constraints are not binding.  We identify a natural object to deﬁne label similarity: the expected outer product of the conditional label probabilities. In particular, in conjunction with a low-rank constraint, this indicates two label embeddings are similar when their conditional probabilities are linearly dependent across the dataset. This uniﬁes prior work utilizing the confusion matrix for multiclass (Bengio et al., 2010) and the empirical label covariance for multilabel (Tai & Lin, 2012).  We apply techniques from randomized linear algebra (Halko et al., 2011) to develop an efﬁcient and scalable algorithm for constructing the embeddings, essentially via a novel randomized algorithm for partial least squares (Geladi & Kowalski, 1986). Intuitively, this technique implicitly decomposes the prediction matrix of a model which would be prohibitively expensive to form explicitly.  2 DESCRIPTION  2.1 NOTATION  We denote vectors by lowercase letters x, y etc. and matrices by uppercase letters W , Z etc. The input dimension is denoted by d, the output dimension by c and the embedding dimension by k. For an m × n matrix X ∈ Rm×n we use ||X||F for its Frobenius norm, X † for the pseudoinverse, ΠX,L for the projection onto the left singular subspace of X.  2.2 PROPOSED ALGORITHM  Our proposal is Rembrandt, described in Algorithm 1. We use the top right singular space of ΠX,LY as a label embedding, or equivalently, the top principal components of Y ⊤ΠX,LY (leveraging the fact that the projection is idempotent). Using randomized techniques, we can decompose this matrix without explicitly forming it, because we can compute the product of ΠX,LY with another matrix Q via Y ⊤ΠX,LY Q = Y ⊤XZ ∗ where Z ∗ = arg minZ∈Rd×(k+p) kY Q − XZk2 F . Algorithm 1  1  Accepted as a workshop contribution at ICLR 2015  Algorithm 1 Rembrandt: Response EMBedding via RANDomized Techniques  (p, q) ← (20, 1) Q ← randn(c, k + p) for i ∈ {1, . . . , q} do  Z ← arg min kY Q − XZk2 F Q ← orthogonalize(Y ⊤XZ)  1: function REMBRANDT(k, X ∈ Rn×d, Y ∈ Rn×c) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: end function  end for F ← (Y ⊤XQ)⊤(Y ⊤XQ) (V, Σ2) ← eig(F, k) V ← QV return (V, Σ)  ⊲ These hyperparameters rarely need adjustment.  ⊲ Randomized range ﬁnder for Y ⊤ΠX,LY  ⊲ NB: total of (q + 1) data passes, including next line ⊲ F ∈ R(k+p)×(k+p) is “small”  ⊲ V ∈ Rc×k is the embedding  is a specialization of randomized PCA to this particular form of the matrix multiplication opera- tor. Fortunately, because X †(ΠX,LY )k is the optimal rank-constrained least squares weight matrix, Rembrandt is a randomized algorithm for partial least squares (Geladi & Kowalski, 1986).  Algorithm 1 is inexpensive to compute. The matrix vector product Y Q is a sparse matrix-vector product so complexity O(nsk) depends only on the average (label) sparsity per example s and the embedding dimension k, and is independent of the number of classes c. The ﬁt is done in the embedding space and therefore is independent of the number of classes c, and the outer product with the predicted embedding is again a sparse product with complexity O(nsk). The orthogonalization step is O(ck2), but this is amortized over the data set and essentially irrelevant as long as n > c. Furthermore random projection theory suggests k should grow only logarithmically with c.  2.3 EXPERIMENTS  Table 1: Data sets used for experimentation and times to compute an embedding. Timings are for a Matlab implementation on a standard desktop (dual 3.2Ghz Xeon E5-1650 CPU and 48Gb of RAM).  Dataset  Type  Modality Examples  Features Classes  ODP  Multiclass LSHTC Multilabel  Text Text  k ∼ 1.5M ∼ 0.5M ∼ 100K 300 ∼ 2.4M ∼ 1.6M ∼ 325K 500  Rembrandt  Time (sec)  6,530 8,006  Table 2: ODP results. k = 300 for all embedding strategies. RE = Rembrandt; CS = compressed sensing; PCA = unsupervised (feature) embedding; LT = LomTree (Choromanska & Langford, 2014); “A+LR” = logistic regression on representation A.  Method Test Error  RE + LR CS + LR PCA + LR 83.15%  85.14%  90.37%  LT  93.46%  Table 3: LSHTC results. FastXML and LPSR-NB are from Prabhu & Varma (2014). “A+ILR” = independent logistic regression on representation A.  Method  Precision-at-1  RE (k = 800) + ILR RE (k = 500) + ILR FastXML LPSR-NB 27.91%  53.39%  52.84%  49.78%  REFERENCES Bengio, Samy, Weston, Jason, and Grangier, David. Label embedding trees for large multi-class  tasks. In Advances in Neural Information Processing Systems, pp. 163–171, 2010.  Choromanska, Anna and Langford, John. Logarithmic time online multiclass prediction. arXiv  preprint arXiv:1406.1822, 2014.  2  Accepted as a workshop contribution at ICLR 2015  Geladi, Paul and Kowalski, Bruce R. Partial least-squares regression: a tutorial. Analytica chimica  acta, 185:1–17, 1986.  Halko, Nathan, Martinsson, Per-Gunnar, and Tropp, Joel A. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53 (2):217–288, 2011.  Prabhu, Yashoteja and Varma, Manik. Fastxml: a fast, accurate and stable tree-classiﬁer for ex- treme multi-label learning. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 263–272. ACM, 2014.  Tai, Farbound and Lin, Hsuan-Tien. Multilabel classiﬁcation with principal label space transforma-  tion. Neural Computation, 24(9):2508–2542, 2012.  3  ",
1412.6597,2015, An Analysis of Unsupervised Pre-training in Light of Recent Advances,"['An Analysis of Unsupervised Pre-training in Light of Recent Advances', 'Tom Paine', 'Pooya Khorrami', 'Wei Han', 'and Thomas Huang']",https://arxiv.org/pdf/1412.6597,"5 1 0 2    r p A 0 1         ]  V C . s c [      4 v 7 9 5 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  AN ANALYSIS OF UNSUPERVISED PRE-TRAINING IN LIGHT OF RECENT ADVANCES  Tom Le Paine*, Pooya Khorrami*, Wei Han, Thomas S. Huang ∗Beckman Institute for Advanced Science and Technology University of Illinois at Urbana-Champaign Urbana, IL 61801 paine1,pkhorra2,weihan3,t-huang1@illinois.edu  ABSTRACT  Convolutional neural networks perform well on object recognition because of a number of recent advances: rectiﬁed linear units (ReLUs), data augmentation, dropout, and large labelled datasets. Unsupervised data has been proposed as an- other way to improve performance. Unfortunately, unsupervised pre-training is not used by state-of-the-art methods leading to the following question: Is unsu- pervised pre-training still useful given recent advances? If so, when? We answer this in three parts: we 1) develop an unsupervised method that incorporates ReLUs and recent unsupervised regularization techniques, 2) analyze the beneﬁts of un- supervised pre-training compared to data augmentation and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised samples, 3) verify our ﬁnd- ings on STL-10. We discover unsupervised pre-training, as expected, helps when the ratio of unsupervised to supervised samples is high, and surprisingly, hurts when the ratio is low. We also use unsupervised pre-training with additional color augmentation to achieve near state-of-the-art performance on STL-10.  1  INTRODUCTION  We analyze the beneﬁts of unsupervised pre-training in the context of recent deep learning inno- vations including: rectiﬁed linear units, data augmentation, and dropout. Recent work shows that convolutional neural networks (CNNs) can achieve state-of-the-art performance for object classiﬁ- cation (Krizhevsky et al. (2012)) and object detection (Girshick et al. (2013)), when there is enough training data. However, in many cases there is a dearth of labeled data. In these cases regularization is necessary for good results. The most common types of regularization are data augmentations (Krizhevsky et al. (2012); Dosovitskiy et al. (2014)) and dropout (Hinton et al. (2012)). Another form of regularization, unsupervised pre-training (Hinton et al. (2006); Bengio et al. (2007); Erhan et al. (2010)), has recently fallen out of favor. While there has been signiﬁcant work in unsupervised learning, most of these works came before rectiﬁed linear units, which signiﬁcantly help training deep supervised neural networks, and before simpler regularization schemes for unsupervised learning, such as zero-bias with linear encoding for auto-encoders (Memisevic et al. (2014)). We train an unsupervised method that takes advantage of these improvements we call Zero-bias Convolutional Auto-encoders (CAEs). Previous work showed that pre-trained tanh CAEs achieved an increase in performance over randomly initialized tanh CNNs. We conduct this experiment with our zero-bias CAE and observe a larger boost in performance. We analyze the effectiveness of our technique when combined with the popular regularization tech- niques used during supervised training on CIFAR-10 while varying the ratio of unsupervised to supervised samples. We do this comparing against randomly initialized CNNs without any addi- tional regularization. We ﬁnd that, when ratio is large, unsupervised pre-training provides useful regularization, increasing test set performance. When the ratio is small, we ﬁnd that unsupervised pre-training hurts performance.  ∗- Authors contributed equally to this work.  1  Accepted as a workshop contribution at ICLR 2015  We verify our ﬁnding that unsupervised pre-training can boost performance when the ratio of unsu- pervised to supervised samples is high by running our algorithm on the STL-10 dataset, which has a ratio of 100:1. As expected, we observe an improvement (3.87%). When combined with additional color augmentation, we achieve near state-of-the-art results. Our unsupervised regularization still yields an improvement of (1.69%). We will begin by reviewing related work on fully-connected and convolutional auto-encoders. In Section 3, we will present our method and how it is trained both during unsupervised pre-training and supervised ﬁne-tuning. We present our results on the CIFAR-10 and STL-10 datasets in Section 4, and in Section 5 we conclude the paper.  2 RELATED WORK  Many methods have used unsupervised learning to learn parameters, which are subsequently used to initialize a neural network to be trained on supervised data. These are called unsupervised pre- training, and supervised ﬁne-tuning respectively. We will highlight some of the unsupervised learn- ing methods related to our work.  2.1 AUTO-ENCODERS  One of the most widely-used models for unsupervised learning, an auto-encoder is a model that learns a function that minimizes the squared error between the input x ∈ Rn and its reconstruction r(x):  L = (cid:107)x − r(x)(cid:107)2  2  r(x) = W T  d f (Wex + b) + c  (1) (2) In the above equation, We represents the weight matrix that transforms the input, x into some hid- den representation, b is vector of biases for each hidden unit and f (·) is some nonlinear function. Commonly chosen examples for f (·) include the sigmoid and hyperbolic tangent functions. Mean- while, Wd is the weight matrix that maps back from the hidden representation to the input space and c is a vector of biases for each input (visible) unit. These parameters are commonly learned by minimizing the loss function over the training data via stochastic gradient descent. When no other constraints are imposed on the loss function, the auto-encoder weights tend to learn the identity function. To combat this, some form of regularization must imposed upon the model so that the model can uncover the underlying structure in the data. Some forms of regularization include adding noise to the input units (Vincent et al. (2010)) and requiring the hidden unit activations be sparse (Coates et al. (2011)) or have small derivatives (Rifai et al. (2011)). These models are known as de-noising, sparse, and contractive auto-encoders respectively. A more recent work by Memisevic et al. (2014) showed that training an auto-encoder with rectiﬁed linear units (ReLU) caused the activations to form tight clusters due to having negative bias values. They showed that using thresholded linear (TLin) or thresholded rectiﬁer (TRec) activations with no bias can allow one to train an auto-encoder without the need for additional regularization.  2.2 CONVOLUTIONAL AUTO-ENCODERS  While the aforementioned fully-connected techniques have shown impressive results, they do not di- rectly address the structure of images. Convolutional neural networks (CNNs) (LeCun et al. (1998); Lee et al. (2009)) present a way to reduce the number of connections by having each hidden unit only be responsible for a small local neighborhood of visible units. Such schemes allow for dense feature extraction followed by pooling layers which when stacked could allow the network to learn over larger and larger receptive ﬁelds. Convolutional auto-encoders (CAEs) combined aspects from both auto-encoders and convolutional neural nets making it possible to extract highly localized patch- based information in an unsupervised fashion. There have been several works in this area including Jarrett et al. (2009) and Zeiler et al. (2010). Both rely on sparse coding to force their unsupervised learning to learn non-trival solutions. Zeiler et al. (2011) extended this work by introducing pool- ing/unpooling and visualizing how individual feature maps at different layers inﬂuenced speciﬁc portions of the reconstruction. These sparse coding approaches had limitations because they used  2  Accepted as a workshop contribution at ICLR 2015  an iterative procedure for inference. A later work by Masci et al. (2011) trained deep feed forward convolutional auto-encoders, using only max-pooling and saturating tanh non-linearities as a form of regularization, while still showing a modest improvement over randomly initialized CNNs. While tanh was a natural choice at the time, Krizhevsky et al. (2012) showed that ReLUs are more suitable for learning given their non-saturating behavior.  3 OUR APPROACH  Our method’s training framework can be broken up into two phases: (i) unsupervised pre-training and (ii) supervised ﬁne-tuning. We describe those in more detail below.  3.1 UNSUPERVISED PRE-TRAINING  Our method incorporates aspects of previous unsupervised learning methods in order to learn salient features, yet be efﬁcient to train. Our model is similar to the deconvolutional network in Zeiler et al. (2011) where the cost we minimize at each layer is the mean square error on the original image. However, unlike the network in Zeiler et al. (2011), our method does not use any form of sparse coding. Our model also is similar to that of Masci et al. (2011), however we improve upon it by introducing regularization in the convolutional layers through the use of zero-biases and ReLUs as discussed in Memisevic et al. (2014). We now describe the model architecture in detail. Like the previous work described above, our model involves several encoding modules followed by several decoding modules. A single encoding module El(·) consists of a convolution layer Fl, a nonlinearity f (·), followed by a pooling layer Psl with switches sl.  El(x) = Psl f (Flx)  (3)  Each encoding module has an associated decoding module Dl, which unpools using El pooling switches sl and deconvolves with El’s ﬁlters, (i.e. F T  l ).  A two layer network can be written as:  Dl(x) = F T  l Usl x  r(x) = D1(D2(E2(E1(x))))  (4)  (5)  We train each encoder/decoder pair in a greedy fashion (i.e. ﬁrst a 1 layer CAE, then a 2 layer CAE, etc.) while keeping the parameters of previous layers ﬁxed. Like Zeiler et al. (2011), we compute the cost by taking the mean squared error between the original image and the network’s reconstruction of the input. Thus, the costs for a one layer network (C1(x)) and two layer network (C2(x)) would be expressed in the following manner:  C1(x) = (cid:107)x − D1(E1(x))(cid:107)2  2  C2(x) = (cid:107)x − D1(D2(E2(E1(x))))(cid:107)2  2  (6) (7)  We regularize our learned representation by ﬁxing the biases of our convolutional and deconvolu- tional layers at zero and using ReLUs as our activation function during encoding. We use linear activations for our decoders. Unlike the work by Memisevic et al. (2014) which analyzes fully- connected auto-encoders, our work is the ﬁrst, to our knowledge, that trains zero-bias CAEs for unsupervised learning.  3.1.1 UNSUPERVISED WEIGHT INITIALIZATION  Weight initialization is often a key component of successful neural network training. For ReLU’s it is important to ensure the input to the ReLU is greater than 0. This can be achieved by setting the bias appropriately. This cannot be done for zero-bias auto-encoders. Instead we use two methods  3  Accepted as a workshop contribution at ICLR 2015  for initializing the weights to achieve this 1) in the ﬁrst layer, we initialize each of the ﬁlters to be a randomly drawn patch from the dataset, 2) on the later layers, we sample weights from a Gaussian distribution and ﬁnd the nearest orthogonal matrix by taking the singular value decomposition (SVD) of the weight matrix and setting all of the singular values to one. For CNNs we must take into account the additive effect of overlapping patches thus we weight each ﬁlter by a 2D hamming window to prevent intensity build-up.  3.2 SUPERVISED FINE-TUNING  After the weights of the CAE have been trained, we remove all of the decoder modules and leave just the encoding modules. We add an additional fully-connected layer and a softmax layer to the pre- trained encoding modules. The weights of these layers are drawn from a Gaussian distribution with zero mean and standard deviation of k/  NF AN IN , where k is drawn uniformly from [0.2, 1.2].  √  3.3 TRAINING  For both unsupervised and supervised training we use stochastic gradient descent with a constant momentum of 0.9, and a weight decay parameter of 1e-5. We select the highest learning rate that doesn’t explode for the duration of training. For these experiments we do not anneal the learning rate. The only pre-processing we do to each patch is centering (i.e. mean subtraction) and scaling to unit variance.  4 EXPERIMENTS AND ANALYSIS  4.1 DATASETS  We run experiments on two natural image datasets, CIFAR-10 (Krizhevsky and Hinton (2009)) and STL-10 (Coates et al. (2011)). CIFAR-10 is a common benchmark for object recognition. Many unsupervised and supervised neural network approaches have been tested on it. It consists of 32x32 pixel color images drawn from 10 object categories. It has 50,000 training images, and 10,000 testing images. STL-10 is also an object recognition benchmark, but was designed to test unsupervised learning algorithms, so it has a relatively small labeled training set of 500 images per class, and an additional unsupervised set which contains 100,000 unlabeled images. The test set contains 800 labeled images per class. All examples are 96x96 pixel color images.  4.2 CIFAR-10  On CIFAR-10, we train a network with structure similar to Masci et al. (2011), so that we can directly show the beneﬁts of our modiﬁcations. The network consists of three convolutional layers with 96, 144, and 192 ﬁlters respectively. The ﬁlters in the ﬁrst two layers are of size 5x5 while the ﬁlters in the third layer are of size 3x3. We also add 2x2 max pooling layers after the ﬁrst two convolutional layers. There is also a full-connected layer with 300 hidden units followed by a softmax layer with 10 output units. All of our nets were trained using our own open source neural network library 1. As stated in the methods section, we ﬁrst train our unsupervised model on 100% of the training images, do supervised ﬁne-tuning, and report overall accuracy on the test set. We 1) present quali- tative results of unsupervised learning, 2) show our zero-bias convolutional auto-encoder performs well compared to previous convolutional auto-encoder work by Masci et al. (2011) developed before the popularization of rectiﬁed linear units, and zero-bias auto-encoders, 3) we show our analysis of various regularization techniques, and vary the ratio of unsupervised to supervised data, 4) for com- pleteness we report our best results when training on the full CIFAR-10 dataset, however this is not the main point of this work.  4.2.1 QUALITATIVE RESULTS  One way in which we ensure the quality of our learned representation is by inspecting the ﬁrst layer ﬁlters. We visualize the ﬁlters learned by our model in Figure 1. So that we can directly compare  1https://github.com/ifp-uiuc/anna  4  Accepted as a workshop contribution at ICLR 2015  with the ﬁlters presented in Masci et al. (2011), we trained an additional zero-bias convolutional auto-encoder with ﬁlters of size 7x7x3 (instead of 5x5x3) in the ﬁrst layer. From Figure 1, we can see that, indeed, our model is able to capture interpretable patterns such as Gabor-like oriented edges (both color and intensity) and center-surrounds.  4.2.2 UNSUPERVISED PRE-TRAINING FOR TANH CAES AND ZERO-BIAS CAES  For our quantitative experiments, we ﬁrst com- pare the performance of the tanh CAE proposed by Masci et al. (2011) with our zero-bias CAE. In their paper, Masci et al. (2011) trained a tanh CNN from a random initialization and com- pared it with one pre-trained using a tanh CAE. They also added 5% translations as a form of data augmentation. We re-conduct this experi- ment using a zero-bias CNN trained from a ran- dom initialization, and compare it to one pre- trained using our zero-bias CAE. In Table 1 we compare the improvements of our model with that of Masci et al. (2011)’s, on var- ious subsets of CIFAR-10. As expected, the zero-bias CNN (a ReLU CNN without bias parameters) performs signiﬁcantly better than the tanh CNN (2.53%, 8.53%, 5.23%). More interestingly, notice that on each subset, compared to Masci et al. (2011) our pre-trained model shows similar or better performance over the randomly initialized CNN. When the ratio of unsupervised to supervised data is high, we experience an 8.44% increase in accuracy as opposed to Masci et al. (2011)’s 3.22% increase.  Figure 1: First layer ﬁlters learned by our zero- bias convolutional auto-encoder. Each ﬁlter has dimension 7x7x3. (Best viewed in color.) For di- rect comparison with tanh CAE please see Masci et al. (2011) Figure 2c.  4.2.3 ANALYSIS OF REGULARIZATION METHODS  Next, we analyze how different supervised regularization techniques affect our model’s perfor- mance. Speciﬁcally, we consider the effects of dropout, data augmentation (via translations and horizontal ﬂips), unsupervised pre-training (with our zero-bias CAE) and their combinations. We compare each regularization technique to a zero-bias CNN trained from random initialization with- out any regularization (labeled CNN in Figure 2). Figure 2 shows the classiﬁcation accuracy im- provement over CNN for each type of regularization both individually and together. We perform this analysis for subsets of CIFAR-10 with different unsupervised to supervised sample ratios ranging from 50:1 to 1:1, by ﬁxing the unsupervised data size, and varying the number of supervised examples. It is important to note that as this ratio approaches 1:1, the experimental setup favors data augmentation and dropout because the number of virtual supervised samples is larger than number of unsupervised samples. In Figure 2a, where the ratio of unsupervised to supervised samples is 50:1, there are three notable effects: (i) unsupervised pre-training alone yields a larger improvement (4.09%) than data augmen- tation (2.67%) or dropout (0.59%), (ii) when unsupervised pre-training is combined with either data augmentation or dropout, the improvement is greater than the sum of the individual contributions, (iii) we experience the largest gains (15.86%) when we combine all three forms of regularization.  Table 1: Comparison between Tanh CAE (Masci et al. (2011)) and our model on various subsets of CIFAR-10.  Unsupervised to supervised ratio (Samples per Class) Tanh CNN - Masci et al. (2011) Tanh CAE - Masci et al. (2011) Zero-bias CNN Zero-bias CAE  1:1  5:1  10:1 (500) — —  50:1 (1000) (100) 64.77 % 77.50 % 44.48 % 47.70 % 65.65 % 78.20 % 47.01 % 64.76 % 73.30 % 82.73 % 55.45 % 68.42 % 74.06 % 83.64 %  (5000)  5  Accepted as a workshop contribution at ICLR 2015  (a) 50:1 unsupervised to supervised sample ratio (100 samples per class), baseline CNN: 44.3%  (b) 10:1 unsupervised to supervised sample ratio (500 samples per class), baseline CNN: 62.0%  (c) 5:1 unsupervised to supervised sample ratio (1000 samples per class), baseline CNN: 67.8%  (d) 1:1 unsupervised to supervised sample ratio (5000 samples per class), baseline CNN: 80.2%  Figure 2: Analysis of the effects of different types of regularization (A: data augmentation, D: dropout, U: unsupervised learning), individually and jointly, on different subsets of CIFAR-10.  We see that effect (ii) is also observed in the case where the ratio of unsupervised to supervised samples is 10:1 (Figure 2b), and to a lesser extent when the ratio is 5:1 (Figure 2c). Unfortunately, effects (i) and (iii) are not observed when the ratio of unsupervised to supervised samples decreases. We will elaborate on effect (i) below. In Figure 3, we observe that the improvement in performance from unsupervised learning decreases rapidly as the ratio of unsupervised to supervised samples decreases. Surprisingly, when the ratio is 1:1, we see that unsupervised learning actually hurts performance (-0.67%).  4.2.4 COMPARISON WITH EXISTING METHODS  We also compare the performance of our algorithm on the full CIFAR-10 dataset with other tech- niques in Table 2, though we show above our method performs worse when the ratio of unsupervised to supervised samples is 1:1. We outperform all methods that use unsupervised pre-training (Masci et al. (2011), Coates et al. (2011), Dosovitskiy et al. (2014), Lin and Kung (2014)), however we are not competitive with supervised state-of-the-art. We include some representative supervised methods in Table 2.  4.3 STL-10  Next, we assess the effects of unsupervised pre-training on STL-10. From the CIFAR-10 exper- iments, it is clear unsupervised pre-training can be beneﬁcial if the unsupervised dataset is much larger than the supervised dataset. STL-10 was designed with this in mind, and has a ratio of unsu- pervised to supervised data of 100:1. So we experimentally show this beneﬁt.  6    +A+D+U+AD+AU+DU+ADUAdditional regularizationClassiﬁcationaccuracyimprovementover CNN(absolute %)1614121086420CNN  +A+D+U+AD+AU+DU+ADUAdditional regularizationClassiﬁcationaccuracyimprovementover CNN(absolute %)1614121086420CNN  +A+D+U+AD+AU+DU+ADUAdditional regularizationClassiﬁcationaccuracyimprovementover CNN(absolute %)1614121086420CNN  +A+D+U+AD+AU+DU+ADUAdditional regularizationClassiﬁcationaccuracyimprovementover CNN(absolute %)161412108642-20CNNAccepted as a workshop contribution at ICLR 2015  We design our network to have structure similar to Dosovitskiy et al. (2014), to ease comparison. The network used consists 3 convolutional layers with 64, 128, and 256 ﬁlters in each layer, a fully-connected layer with 512 units, and a softmax layer with 10 output units. We also apply max- pooling layers of size 2x2 after the ﬁrst two convolutional layers and quadrant pooling after the third convolutional layer. We train the zero-bias CAE on 100,000 unlabeled images. We then ﬁne-tune the network on each of the 10 provided splits of training set, each consisting of 1000 samples (100 samples per class), and eval- uate all of them on the test set. The accu- racies are subsequently averaged to obtain the ﬁnal recognition accuracy. Similar to our CIFAR-10 experiments, we also train a zero-bias CNN with the same structure as our zero-bias CAE on each of the splits to further highlight the beneﬁts of unsu- pervised learning. Table 3 presents our results on the STL- 10 dataset and compares them with other methods. As expected, unsupervised pre- training gives a 3.87% increase over the randomly initialized CNN.  Figure 3: The beneﬁts of unsupervised learning vs. un- supervised to supervised sample ratio. When the ratio is 50:1, we see a 4.09% increase in performance. But the beneﬁt shrinks as the ratio decreases. When the ratio is 1:1, there is a penalty for using unsupervised pre-training.  4.3.1 ADDITIONAL DATA AUGMENTATION: COLOR AND CONTRAST  The current best result on STL-10 (Dosovitskiy et al. (2014)) makes extensive use of additional data augmentation including: scaling, rotation, color and two forms of contrast. They do not perform these augmentations during supervised training, but during a discriminative unsupervised feature learning period. We test the regularizing effects of these additional augmentations when applied directly to supervised training, and test how these regularization effects hold up when combined with with unsupervised pre-training. To do this, we use some of these additional data-augmentations during our supervised training: color augmentation and contrast augmentation. Color augmentation: The images are represented in HSV color space (h, s, v). Here we generate a single random number for each image and add it to the hue value for each pixel like so:  a ∼ U nif orm(−0.1, 0.1) h = h + a  (8) (9)  Table 2: Quantitative comparison with other methods on CIFAR-10 (A: Data Augmentation, D: Dropout, U: Unsupervised Learning).  Algorithm Convolutional Auto-encoders - Masci et al. (2011) Single layer K-means - Coates et al. (2011) Convolutional K-means Networks - Coates and Ng (2011) Exemplar CNN - Dosovitskiy et al. (2014) Convolutional Kernel Networks - Mairal et al. (2014) NOMP - Lin and Kung (2014) Max-Out Networks - Goodfellow et al. (2013b) Network In Network - Lin et al. (2013) Deeply-Supervised Nets - Lee et al. (2014) Zero-bias CNN +ADU Zero-bias CNN +AD  Accuracy 79.20 % 79.60 % 82.00 % 82.00 % 82.18 % 82.90 % 90.65 % 91.20 % 91.78 % 86.44 % 86.70 %  7  Unsupervised to supervised sample ratio(Supervised samples per class)50:1(100)10:1(500)5:1(1000)1:1(5000)Classiﬁcationaccuracyimprovementover CNN(absolute %)543210-1Accepted as a workshop contribution at ICLR 2015  Table 3: Quantitative comparison with other methods on STL-10 (A: Data Augmentation, D: Dropout, C: Color Augmentation, U: Unsupervised Learning).  Algorithm Convolutional K-means Networks - Coates and Ng (2011) Convolutional Kernel Networks - Mairal et al. (2014) Hierarchical Matching Pursuit (HMP) - Bo et al. (2013) NOMP - Lin and Kung (2014) Multi-task Bayesian Optimization - Swersky et al. (2013) Exemplar CNN - Dosovitskiy et al. (2014) Zero-bias CNN +AD Zero-bias CNN +ADU Zero-bias CNN +ADC Zero-bias CNN +ADCU  Accuracy  62.32 %  60.1 % ± 1.0 % 64.5 % ± 1.0 % 67.9 % ± 0.6 % 70.1 % ± 0.6 % 72.8 % ± 0.4 % 62.01 % ± 1.9 % 65.88 % ± 0.9 % 68.51 % ± 0.8 % 70.20 % ± 0.7 %  Contrast augmentation: Here we generate six random numbers for each image, with the following distributions:  a, d ∼ U nif orm(0.7, 1.4) b, e ∼ U nif orm(0.25, 4) c, f ∼ U nif orm(−0.1, 0.1)  And use them to modify the saturation and value for every pixel in the image, like so:  s = asb + c v = dse + f  (10) (11) (12)  (13) (14)  We ﬁnd that a) additional data-augmentation is incredibly helpful, increasing accuracy by 6.5%, b) unsupervised pre-training still maintains an advantage (1.69%).  5 CONCLUSIONS  We present a new type of convolutional auto-encoder that has zero-bias and ReLU activations and achieves superior performance to previous methods. We conduct thorough experiments on CIFAR- 10 to analyze the effects of unsupervised pre-training as a form of regularization when used in isolation and in combination with supervised forms of regularization such as data augmentation and dropout. We observe that, indeed, unsupervised pre-training can provide a large gain in performance when the ratio of unsupervised to supervised samples is large. Finally, we verify our ﬁndings by applying our model to STL-10, a dataset with far more unlabeled samples than labeled samples (100:1). We ﬁnd that with additional regularization, via color augmentation, our method is able to achieve nearly state-of-the-art results.  CODE  All experiments were run using our own open source library Anna, which can be found at: https: //github.com/ifp-uiuc/anna Code to reproduce the experiments can be found at: https://github.com/ifp-uiuc/ an-analysis-of-unsupervised-pre-training-iclr-2015  ACKNOWLEDGMENTS  This material is based upon work supported by the National Science Foundation under Grant No. 392 NSF IIS13-18971. The two Tesla K40 GPUs used for this research were donated by the NVIDIA Corporation. We would like to acknowledge Theano (Bergstra et al. (2010)) and Pylearn2 (Good- fellow et al. (2013a)), on which our code is based. Also, we would like to thank Shiyu Chang for many helpful discussions and suggestions.  8  Accepted as a workshop contribution at ICLR 2015  REFERENCES Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, et al. Greedy layer-wise training  of deep networks. Advances in neural information processing systems, 19:153, 2007.  James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU In Proceedings of the Python for Scientiﬁc Computing Conference math expression compiler. (SciPy), June 2010. Oral Presentation.  Liefeng Bo, Xiaofeng Ren, and Dieter Fox. Unsupervised feature learning for rgb-d based object  recognition. In Experimental Robotics, pages 387–402. Springer, 2013.  Adam Coates and Andrew Y Ng. Selecting receptive ﬁelds in deep networks. In Advances in Neural  Information Processing Systems, pages 2528–2536, 2011.  Adam Coates, Andrew Y Ng, and Honglak Lee. An analysis of single-layer networks in unsuper- vised feature learning. In International Conference on Artiﬁcial Intelligence and Statistics, pages 215–223, 2011.  Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Pro- cessing Systems 27, pages 766–774. Curran Associates, Inc., 2014.  Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660, 2010.  Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu-  rate object detection and semantic segmentation. arXiv preprint arXiv:1311.2524, 2013.  Ian J Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Fr´ed´eric Bastien, and Yoshua Bengio. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013a.  Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout  networks. arXiv preprint arXiv:1302.4389, 2013b.  Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief  nets. Neural computation, 18(7):1527–1554, 2006.  Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut- Improving neural networks by preventing co-adaptation of feature detectors. CoRR,  dinov. abs/1207.0580, 2012.  Kevin Jarrett, Koray Kavukcuoglu, M Ranzato, and Yann LeCun. What is the best multi-stage ar- chitecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146–2153. IEEE, 2009.  Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Com-  puter Science Department, University of Toronto, Tech. Rep, 2009.  Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu- tional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.  Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to  document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-  supervised nets. arXiv preprint arXiv:1409.5185, 2014.  9  Accepted as a workshop contribution at ICLR 2015  Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.  Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,  2013.  Tsung-Han Lin and H. T. Kung. Stable and efﬁcient representation learning with nonnegativity constraints. In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Con- ference on Machine Learning (ICML-14), pages 1323–1331. JMLR Workshop and Conference Proceedings, 2014.  Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel net- works. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2627–2635. Curran Associates, Inc., 2014.  Jonathan Masci, Ueli Meier, Dan Cires¸an, and J¨urgen Schmidhuber. Stacked convolutional auto- encoders for hierarchical feature extraction. In Artiﬁcial Neural Networks and Machine Learning– ICANN 2011, pages 52–59. Springer, 2011.  Roland Memisevic, Kishore Konda, and David Krueger. Zero-bias autoencoders and the beneﬁts of  co-adapting features. arXiv preprint arXiv:1402.3337, 2014.  Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto- encoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 833–840, 2011.  Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization. In Advances  in Neural Information Processing Systems, pages 2004–2012, 2013.  Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11:3371–3408, 2010.  Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Robert Fergus. Deconvolutional net- works. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2528–2535. IEEE, 2010.  Matthew D Zeiler, Graham W Taylor, and Rob Fergus. Adaptive deconvolutional networks for mid and high level feature learning. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2018–2025. IEEE, 2011.  10  ",
1412.7144,2015, Fully Convolutional Multi-Class Multiple Instance Learning,"['Fully Convolutional Multi-Class Multiple Instance Learning', 'Deepak Pathak', 'Evan Shelhamer', 'Jonathan Long', 'and Trevor Darrell']",https://arxiv.org/pdf/1412.7144,"5 1 0 2    r p A 5 1         ]  V C . s c [      4 v 4 4 1 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  FULLY CONVOLUTIONAL MULTI-CLASS MULTIPLE INSTANCE LEARNING  Deepak Pathak, Evan Shelhamer, Jonathan Long & Trevor Darrell UC Berkeley {pathak,shelhamer,jonlong,trevor}@cs.berkeley.edu  ABSTRACT  Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learn- ing by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end- to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.  1  INTRODUCTION  Convolutional networks (convnets) are achieving state-of-the-art performance on many computer vision tasks but require costly supervision. Following the ILSVRC12-winning image classiﬁer of Krizhevsky et al. (2012), progress on detection (Girshick et al., 2014) and segmentation (Long et al., 2014) demonstrates that convnets can likewise address local tasks with structured output. Most deep learning methods for these tasks rely on strongly annotated data that is highly time- consuming to collect. Learning from weak supervision, though hard, would sidestep the annotation cost to scale up learning to available image-level labels. In this work, we propose a novel framework for multiple instance learning (MIL) with a fully con- volutional network (FCN). The task is to learn pixel-level semantic segmentation from weak image- level labels that only signal the presence or absence of an object. Images that are not centered on the labeled object or contain multiple objects make the problem more difﬁcult. The insight of this work is to drive the joint learning of the convnet representation and pixel classiﬁer by multiple instance learning. Fully convolutional training learns the model end-to-end at each pixel. To learn the seg- mentation model from image labels, we cast each image as a bag of pixel-level-instances and deﬁne a pixelwise, multi-class adaptation of MIL for the loss. MIL can reduce the need for bounding box annotations (Cinbis et al., 2014; Song et al., 2014), but it is rarely attempted for segmentation. Oquab et al. (2014) improve image classiﬁcation by inferring latent object location, but do not evaluate the localization. Hoffman et al. (2014) train by MIL ﬁne-tuning but rely on bounding box supervision and proposals for representation learning. Most MIL problems are framed as max-margin learning (Andrews et al., 2002; Felzenszwalb et al., 2010), while other approaches use boosting (Ali & Saenko, 2014) or Noisy-OR models (Heckerman, 2013). These approaches are limited by (1) ﬁxed representations and (2) sensitivity to initial hypotheses of the latent instance-level labels. We aim to counter both shortcomings by simultaneously learning the representation to maximize the most conﬁdent inferred instances. We incorporate multi-class annotations by making multi-class inferences for each image. When an image / bag contains multiple classes the competition of pixelwise models help to better infer the latent instance-level classes. We investigate the following ideas and carry out preliminary experiments to these ends:  1  Accepted as a workshop contribution at ICLR 2015  • We perform MIL jointly with end-to-end representation learning in a fully convolutional network. This eliminates the need to instantiate instance-label hypotheses. FCN learning and inference can process images of different sizes without warping or object proposal pre-processing. This makes training simple and fast. • We propose a multi-class pixel-level loss inspired by the binary MIL scenario. This tries to maximize the classiﬁcation score based on each pixel-instance, while simultaneously taking advantage of inter-class competition in narrowing down the instance hypotheses. • We target the under-studied problem of weakly supervised image segmentation. Our belief is that pixel-level consistency cues are helpful in disambiguating object presence. In this way weak segmentation can incorporate more image structure than bounding boxes.  2 FULLY CONVOLUTIONAL MIL  A fully convolutional network (FCN) is a model designed for spatial prediction problems. Every layer in an FCN computes a local operation on relative spatial coordinates. In this way, an FCN can take an input of any size and produce an output of corresponding dimensions. For weakly supervised MIL learning, the FCN allows for the efﬁcient selection of training instances. The FCN predicts an output map for all pixels, and has a corresponding loss map for all pixels. This loss map can be masked, re-weighted, or otherwise manipulated to choose and select instances for computing the loss and back-propagation for learning. We use the VGG 16-layer net (Simonyan & Zisserman, 2014) and cast it into fully convolutional form as suggested in Long et al. (2014) for semantic segmentation by replacing fully connected layers with corresponding convolutions. The network is ﬁne-tuned from the pre-trained ILSVRC classiﬁer weights i.e. pre-trained to predict image-level labels. We then experiment with and without initializing the last layer weights i.e. the classiﬁer layer. These initializations, without MIL ﬁne- tuning, act as the baselines (row 1 and 2 in Table). If there is no image-level pretraining, the model quickly converges to all background. Semantic segmentation requires a background class but the classiﬁcation task has none; we simply zero initialize the background classiﬁer weights.  3 MULTI-CLASS MIL LOSS  We deﬁne a multi-class MIL loss as the multi-class logistic loss computed at maximum predictions. This selection is enabled by the output map produced by FCN i.e. for an image of any size, the FCN outputs a heat-map for each class (including background) of corresponding size. We identify the max scoring pixel in the coarse heat-maps of classes present in image and background. The loss is then only computed on these coarse points, and is back propagated through the network. The alternating optimization in the binary MIL problem inspires this ignoring of the loss at non- maximally scoring points. The background class is analogous to the negative instances by competing against the positive object classes. Let the input image be I, its label set be LI (including background label) and ˆpl(x, y) be the output heat-map for the lth label at location (x, y). The loss is deﬁned as:  (xl, yl) = arg max∀(x,y)  ˆpl(x, y)  ∀l ∈ LI  =⇒ MIL LOSS =  log ˆpl(xl, yl)  (cid:88)  l∈LI  −1 |LI|  Ignoring the loss at all non-maximally scoring points is key to avoid biasing the learning of the FCN to the background. Simultaneous training exploits multi-label images through inter-class confusion to help reﬁne the intra-class pixel accuracy. At inference time, the MIL-FCN takes the top class prediction at every point in the coarse prediction and bilinearly interpolates to image resolution to obtain a pixelwise segmentation.  4 EXPERIMENTS  All results are on the PASCAL VOC segmentation challenge. We train and validate on the VOC 2011 train augmented by Hariharan et al. (2011) and val sets then evaluate on the completely held-  2  Accepted as a workshop contribution at ICLR 2015  out VOC 2012 test set. The evaluation metric is intersection over union (IU), and is deﬁned per class as the percentage of pixels in the intersection of ground truth segmentation mask, and the predicted mask out of the number of pixels in their union. The MIL-FCN model is initialized from the 16-layer VGG ILSVRC14 classiﬁer (Simonyan & Zisserman, 2014) then ﬁne-tuned by the MIL loss. Long et al. (2014) ﬁne-tune from all but the output layer, as they have access to complete supervision. In our setting however, transferring the output layer parameters for the classes common to both PASCAL and ILSVRC improves results. Including these classiﬁer parameters helps prevent degenerate solutions of predicting all background. We train our model with a learning rate 0.0001 , momentum 0.9 and weight decay 0.0005. The training is quick and the network converges in less than 10,000 iterations.  Table 1: Results on PASCAL VOC 2011 segmentation validation and 2012 test data. Fine-tuning with the MIL loss achieves 96% relative improvement over the baseline.  Approach  mean IU (VOC2011 val) mean IU (VOC2012 test)  Baseline (no classiﬁer) Baseline (with classiﬁer) MIL-FCN Oracle (supervised)  3.52% 13.11% 25.05% 59.43%  -  13.09% 25.66% 63.80%  Figure 1: Sample images from PASCAL VOC 2011 val-segmentation data. Each row shows input (left), ground truth (center) and MIL-FCN output (right).  3  Accepted as a workshop contribution at ICLR 2015  Table 1 shows quantitative intersection-over-union (IU) scores while example outputs from MIL- FCN are shown in Figure 1. MIL-FCN achieves 96% relative improvement over the baseline results when the classiﬁer is ﬁne-tuned from the common classes. These are preliminary but encour- aging results.  5 DISCUSSION  We propose a novel model of joint multiple instance and representation learning with a multi-class pixelwise loss inspired by binary MIL. This model is learned end-to-end as a fully convolutional network for the task of weakly supervised semantic segmentation. It precludes the need for any kind of proposal or instance hypothesis mechanisms. Inference is fast (≈ 1/5 sec). These results are encouraging, and can be improved further. Currently, the coarse output is merely interpolated; conditional random ﬁeld regularization or super-pixel (Achanta et al., 2012) projection could reﬁne the predictions. These grouping methods could likewise drive learning by selecting whole segments instead of single points for MIL training. Moreover, controlling convnet learning by manipulating the loss map could have further uses such as encouraging consistency across images for co-segmentation or hard negative mining.  REFERENCES Achanta, Radhakrishna, Shaji, Appu, Smith, Kevin, Lucchi, Aurelien, Fua, Pascal, and Susstrunk, Sabine. Slic superpixels compared to state-of-the-art superpixel methods. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2012.  Ali, K. and Saenko, K. Conﬁdence-rated multiple instance boosting for object detection. In IEEE  Conference on Computer Vision and Pattern Recognition, 2014.  Andrews, Stuart, Tsochantaridis, Ioannis, and Hofmann, Thomas. Support vector machines for  multiple-instance learning. In Proc. NIPS, pp. 561–568, 2002.  Cinbis, Ramazan Gokberk, Verbeek, Jakob, and Schmid, Cordelia. Multi-fold mil training for  weakly supervised object localization. In CVPR, 2014.  Felzenszwalb, Pedro F, Girshick, Ross B, McAllester, David, and Ramanan, Deva. Object detection  with discriminatively trained part-based models. IEEE Tran. PAMI, 32(9):1627–1645, 2010.  Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object  detection and semantic segmentation. In In Proc. CVPR, 2014.  Hariharan, Bharath, Arbel´aez, Pablo, Bourdev, Lubomir, Maji, Subhransu, and Malik, Jitendra. Semantic contours from inverse detectors. In Computer Vision (ICCV), 2011 IEEE International Conference on, pp. 991–998. IEEE, 2011.  Heckerman, David. A tractable inference algorithm for diagnosing multiple diseases. arXiv preprint  arXiv:1304.1511, 2013.  Hoffman, Judy, Pathak, Deepak, Darrell, Trevor, and Saenko, Kate. Detector discovery in the wild:  Joint multiple instance and representation learning. arXiv preprint arXiv:1412.1135, 2014.  Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classiﬁcation with deep convolutional  neural networks. In Proc. NIPS, 2012.  Long, Jonathan, Shelhamer, Evan, and Darrell, Trevor. Fully convolutional networks for semantic  segmentation. arXiv preprint arXiv:1411.4038, 2014.  Oquab, Maxime, Bottou, L´eon, Laptev, Ivan, Sivic, Josef, et al. Weakly supervised object recogni-  tion with convolutional neural networks. In Proc. NIPS, 2014.  Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image  recognition. arXiv preprint arXiv:1409.1556, 2014.  Song, Hyun Oh, Lee, Yong Jae, Jegelka, Stefanie, and Darrell, Trevor. Weakly-supervised discovery  of visual pattern conﬁgurations. In Proc. NIPS, 2014.  4  ",
1504.02485,2015, What Do Deep CNNs Learn About Objects?,"['What Do Deep CNNs Learn About Objects?', 'Xingchao Peng', 'Baochen Sun', 'Karim Ali', 'and Kate Saenko']",https://arxiv.org/pdf/1504.02485,"5 1 0 2    r p A 9         ]  V C . s c [      1 v 5 8 4 2 0  .  4 0 5 1 : v i X r a  Accepted as workshop contribution at ICLR 2015  WHAT DO DEEP CNNS LEARN ABOUT OBJECTS?  Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko∗ Department of Computer Science University of Masschusetts Lowell One University Avenue, Lowell, MA, USA {xpeng,bsun,karim,saenko}@cs.uml.edu  1  INTRODUCTION  Deep convolutional neural networks learn extremely powerful image representations, yet most of that power is hidden in the millions of deep-layer parameters. What exactly do these parameters represent? Recent work has started to analyse CNN representations, ﬁnding that, e.g., they are invariant to some 2D transformations Fischer et al. (2014), but are confused by particular types of image noise Nguyen et al. (2014). In this work, we delve deeper and ask: how invariant are CNNs to object-class variations caused by 3D shape, pose, and photorealism? To analyse deep representations, we treat the activations of a hidden layer as input features to a linear classiﬁer, and test how well the classiﬁer generalizes across intra-class variations due to the above factors. The hypothesis is that, if the representation is invariant to a certain factor, then similar neurons will activate whether or not that factor is present in the input image. For example, if the network is invariant to “cat” texture, then it will have similar activations on cats with and without texture, i.e. it will “hallucinate” the right texture when given a texureless cat shape. Then the classiﬁer will learn equally well from both sets of training data. If, on the other hand, the network is not invariant to texture, then the feature distributions will differ. As a consequence, the classiﬁer trained on textureless cat data will perform worse. Because factors such as object texture and background scene are difﬁcult to isolate using 2D image data, we rely on computer graphics to generate synthetic images from 3D object models.  2 EXPLORING THE INVARIANCES OF CNN FEATURES  We design a series of experiments to probe CNN invariance in the context of object detection. For each experiment, we follow these steps: 1) select image rendering parameters, 2) generate a batch of synthetic 2D images with those parameters, 3) sample positive and negative patches for each object class, 4) extract hidden CNN layer activations from the patches as features, 5) train a classiﬁer for each object category, 6) test the classiﬁers on real images. CNN Model, Training, and Synthetic Data Generation. We adopt the detection method of Gir- shick et al. (2013), which uses the eight-layer “AlexNet” architecture with over 60 million param- eters Krizhevsky et al. (2012). Our hypothesis is that the network will learn different invariances, depending on how it is trained. Therefore, we evaluate two different variants of the network: one trained on the ImageNet ILSVRC 1000-way classiﬁcation task, which we call IMGNET, and the same network also ﬁne-tuned for the PASCAL detection task, which we call PASC-FT. For both net- works, we extract the last hidden layer (fc7) as the feature representation. We choose to focus on the last hidden layer as it is the most high-level representation and has learned the most invariance. We choose a subset of factors that can easily be modeled using simple computer graphics techniques, namely, object texture and color, context/background appearance and color, 3D pose and 3D shape. We study the invariance of the CNN representation to these parameters using synthetic data. We also study the invariance to 3D rigid rotations using real data. Object Color, Texture and Context. We begin by investigating various combination of object colors and textures placed against a variety of background scene colors and textures. Examples of our texture and background generation settings are shown in Table 1.  ∗www.cs.uml.edu/˜{xpeng,bsun,karim,saenko}  1  Accepted as workshop contribution at ICLR 2015  BG TX  RR-RR Real RGB Real RGB  W-RR White  Real RGB  W-UG White  Unif. Gray  RR-UG Real RGB Unif. Gray  RG-UG Real Gray Unif. Gray  RG-RR Real Gray Real RGB  Table 1: Different conﬁguration of background, color, texture  Net  PASC-FT PASC-FT PASC-FT PASC-FT  PASC-FT(-front)  all  Views  aero bike bird bus car 64.2 69.7 50 62.6 71  cow dog hrs mbikshp trn mAP 58.5 56.1 60.6 66.8 52.8 57.9 64.7 61.2 -random 62.1 70.3 49.7 61.1 70.2 54.7 55.4 61.7 67.4 55.7 57.9 64.2 60.9 -front 61.7 67.3 45.1 58.6 70.9 56.1 55.1 59.0 66.1 54.2 53.3 61.6 59.1 -side 62.0 70.2 48.9 61.2 70.8 57.0 53.6 59.9 65.7 53.7 58.1 64.2 60.4 -front 59.7 63.1 42.7 55.3 64.9 54.4 54.0 56.1 64.2 55.1 47.4 60.1 56.4  tv  Table 2: Results of training on different real image views. ’-’ represent removing a certain view.  We trained a series of detectors with each of the above background and object texture conﬁgura- tions and tested them on the PASCAL VOC test set, reporting the average precision (AP) across categories. The somewhat unexpected result is that the generation settings RR-RR(28.9%), W- RR(31.2%), W-UG(30.1%), RG-RR(31.2%) with PASC-FT all achieve comparable performance, despite the fact that W-UG has no texture and no context. Results with real texture but no color in the background (RG-RR, W-RR) are the best. This indicates that the network has learned to be invariant to the color and texture of the object and its background. Image Pose. We also test view invariance on real images. We are interested here in objects whose frontal view presentation differs signiﬁcantly (ex: the side-view of a horse vs a frontal view). To this end, we selected 12 categories from the PASCAL VOC training set which match this criteria. Held out categories included rotationally invariant objects such as bottles or tables. Next, we split the training data for these 12 categories to prominent side-view and front-view, as shown in Table 2. We train classiﬁers exclusively by removing one view (say front-view) and test the resulting detector on the PASCAL VOC test set containing both side and front-views.We also compare with random view sampling. Results, shown in Table 2, point to important and surprising conclusions regard- ing the representational power of the CNN features. Note that mAP drops by less than 2% when detectors exclusively trained by removing either view are tested on the PASCAL VOC test set. 3D Shape. Finally, we experiment with reducing intra-class shape variation by using fewer CAD models per category. We otherwise use the same settings as in the RR-RR condition with PASC-FT. From our experiments, we ﬁnd that the mAP decreases by about 5.5 points from 28.9% to 23.53% when using only a half of the 3D models. This shows a signiﬁcant boost from adding more shape variation to the training data, indicating less invariance to this factor.  3 CONCLUSION  We investigated the sensitivity of convnets to various factors in the training data: 3D pose, fore- ground texture and color, background image and color. To simulate these factors we used synthetic data generated from 3D CAD models and a few real images. Our results demonstrate that the popular deep convnet of Krizhevsky et al. (2012) ﬁne-tuned for detection on real images for a set of categories is indeed invariant to these factors. For more details and results, we refer the reader to the following paper Peng et al. (2014).  2  Accepted as workshop contribution at ICLR 2015  REFERENCES Fischer, Philipp, Dosovitskiy, Alexey, and Brox, Thomas. Descriptor matching with convolutional neural  networks: a comparison to sift. arXiv preprint arXiv:1405.5769, 2014.  Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik, Jitendra. Rich feature hierarchies for accurate object  detection and semantic segmentation. arXiv preprint arXiv:1311.2524, 2013.  Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classiﬁcation with deep convolutional neural net-  works. In NIPS, 2012.  Nguyen, Anh, Yosinski, Jason, and Clune, Jeff. Deep neural networks are easily fooled: High conﬁdence  predictions for unrecognizable images. arXiv preprint arXiv:1412.1897, 2014.  Peng, Xingchao, Sun, Baochen, Ali, Karim, and Saenko, Kate. Exploring invariances in deep convolutional neural networks using synthetic images. CoRR, abs/1412.7122, 2014. URL http://arxiv.org/abs/ 1412.7122.  3  ",
1412.6134,2015, Representation using the Weyl Transform,"['Representation using the Weyl Transform', 'Qiang Qiu', 'Andrew Thompson', 'Robert Calderbank', 'and Guillermo Sapiro']",https://arxiv.org/pdf/1412.6134,"Data Representation using the Weyl Transform  Qiang Qiu∗, Andrew Thompson†, Robert Calderbank∗, and Guillermo Sapiro∗  1  5 1 0 2    l u J    1 2      ]  V C . s c [      5 v 4 3 1 6  .  2 1 4 1 : v i X r a  Abstract—The Weyl transform is introduced as a rich frame- work for data representation. Transform coefﬁcients are con- nected to the Walsh-Hadamard transform of multiscale au- tocorrelations, and different forms of dyadic periodicity in a signal are shown to appear as different features in its Weyl coefﬁcients. The Weyl transform has a high degree of symmetry with respect to a large group of multiscale transformations, which allows compact yet discriminative representations to be obtained by pooling coefﬁcients. The effectiveness of the Weyl transform is demonstrated through the example of textured image classiﬁcation.  Index Terms—Weyl transform, invariant representations, au- tocorrelation, Walsh-Hadamard transform, texture classiﬁcation.  I. INTRODUCTION  Many signal processing tasks, such as detection, clustering, and classiﬁcation, rely on representations which are invariant to a given group of transformations. Additional invariance to transformations which permute feature coefﬁcients can often be achieved by pooling coefﬁcients, for example Gabor wavelets [1] and scattering transforms [2].  Our focus here is not on new bases for representing signals, rather it is on the type of measurement that is fundamental to widely used algorithms for signal processing tasks. Thus, our focus is autocorrelation, and we show that autocorrelations can be calculated from trace inner products of covariance matrices with signed permutation matrices from the discrete Heisenberg-Weyl group. When a signal is transformed by an element of the (much larger) discrete symplectic group, these autocorrelations are ﬁxed up to permutation and sign change. The symplectic group can be viewed as a discrete approximation to the full unitary group, and because it is so large (the order is 2m2 · (22m − 1)(22m−2 − 1) . . . (22 − 1) for a signal of length 2m), it allows for great versatility in the design of pooling strategies. Our approach makes use of the power of autocorrelation for describing multiscale periodicity in signals, and we demonstrate through the example of texture classiﬁcation that this framework is useful for signal representation.  The mapping between a signal and its autocorrelation coefﬁ- cients based on the Heisenberg-Weyl group is called the Weyl transform. This instance of the Weyl transform is a special case of a general framework for representation of operators in harmonic analysis. In radar the larger (continuous) framework is fundamental to the study of the radar ambiguity function [3],  ∗Q. Qiu, R. Calderbank and G. Shapiro are with the Department of Electrical Engineering, Duke University, USA; email: {qiang.qiu, robert.calderbank, guillermo.sapiro}@duke.edu; †A. Thompson is with the Mathematical Institute, University of Oxford,  UK; email: thompson@maths.ox.ac.uk  Q. Qiu and A. Thompson have contributed equally to this work.  [4], [5]. The binary Heisenberg-Weyl group studied here plays an important role in coding theory [6]. In this paper we make a new connection to signal analysis by describing the Weyl transform in terms of the Walsh-Hadamard transform [7] of binary autocorrelations.  Autocorrelation is known to be a powerful tool for detecting periodicity while imposing shift invariance, and has been used in a variety of signal processing tasks, including but not restricted to speech coding [8], pitch estimation [9] and noise removal [10]. Applications in image processing include character recognition, face detection, texture classiﬁcation, and pattern recognition [11], [12], [13], [14]. Autocorrelation can be cyclic, or dyadic [15], the latter being especially well suited to representing multiscale texture. The Walsh- Hadamard Transform (WHT) is also well suited to periodic signals since it captures binary harmonics of data, being the binary counterpart of the Discrete Fourier Transform (DFT). The WHT can be combined with the Wiener-Khintchine convolution theorem to provide a fast method for calculating dyadic autocorrelation [16], and the windowed WHT has been used for pattern matching of images [17]. While all of the aforementioned previous work involving autocorrelation and the WHT is to some extent heuristic, we develop for the ﬁrst time a rigorous mathematical theory of invariance which combines the two notions.  Fig. 1 illustrates how effectively the Weyl transform is able to encode differences in texture. It displays ﬁve randomly sampled 16x16 patches from each of two different textures. Each patch is broken down into smaller 4 × 4 sub-patches, their Weyl transforms calculated, and their absolute values aggregated over all sub-patches. Fig. 1(g) displays a similarity matrix where darker entries indicate greater Euclidean dis- tance between Weyl transforms of different patches. Adjacent patches form clusters that clearly separate the two textures.  Section II introduces the Weyl transform and its invari- ance property, namely that the absolute values of Weyl co- efﬁcients are invariant to certain symmetries of the signal (Theorem II.1). We also establish a connection between the Weyl transform and the WHT of dyadic autocorrelations (Theorem II.3). In Section III, we illustrate the theory with examples of real-world textures. We demonstrate the versa- tility of the Weyl transform by pooling coefﬁcients to obtain features with additional invariance to 90◦ rotation and cyclic translations. We also describe a supervised learning example in which training data is used to select the Weyl coefﬁcients that are most signiﬁcant in distinguishing the classes. For both examples, we compare our approach with three state-of- the-art image representations: Gabor wavelets [1], HOG [18] and LBP [19]. On the examples tested, the Weyl transform regularly outperforms these other methods while using a signiﬁcantly shorter feature vector. Full proofs of all results  2  A. The discrete Weyl transform  Given a vectorized signal y ∈ R2m, we deﬁne its Weyl  coefﬁcients ωa,b(y) to be  (a) Jeans  (b) 5 Jeans patches  (c) Cotton  (d) 5 Cotton patches  (e) Weyl transforms of (b)  (f) Weyl transforms of (d)  (g) Similarity  Fig. 1: The Weyl transform examples. (b) and (d) are ﬁve 16 × 16 patches randomly sampled from two textures (a) and (c), respectively. (e) and (f) are the Weyl transforms of (b) and (d); see Section II for details. (g) shows the similarity matrix for the Weyl transforms of patches from Jeans followed by Cotton, where darker colors indicate lower similarity. As shown in (g), patches sampled from the same texture share similar Weyl transforms, even though they exhibit obvious dissimilarity, e.g., due to translation.  can be found in Section IV.  II. WEYL TRANSFORM THEORY  We now introduce some new fundamental theory about the Weyl transform. We ﬁrst give a summary of the theory in Section II-A, in which we deﬁne the Weyl transform and give a crucial result about its invariance to certain multiscale transformations. The Weyl transform consists of inner products with matrices from the binary Heisenberg-Weyl group, and we describe this group of matrices more fully in Section II-B. In Section II-C, we make a connection between the Weyl transform and the Walsh-Hadamard Transform. Note that [6], [3], [4] show material related to the discrete Weyl transform, but presented in a different context, less suited to the data and applications described here.  ωa,b(y) :=  Tr[yyT · D(a, b)],  1  2m/2  where a = (am−1 . . . a0)T and b = (bm−1 . . . b0)T are binary m-tuples, and where the {D(a, b)} are multiscale signed permutation matrices from the binary Heisenberg-Weyl group (see Section II-B for more details). We will denote the set of binary m-tuples by Zm 2 . Figure 2 gives examples of matrices D(a, b) for m = 4.  Fig. 2: Examples of the matrices D(a, b) for m = 4: -1 (black), 0 (grey), 1 (white).  The mapping from covariance matrices yyT ∈ R2m×2m to vectors of Weyl coefﬁcients in R22m is an isometry, which we will refer to as the Weyl transform. A main result of this paper is that, whenever a D(a, b) matrix is applied to a signal vector y, the magnitudes of its Weyl coefﬁcients are unchanged. The Weyl transform is therefore invariant to a large class of multiscale signed permutations, a desirable property for a representation designed to detect periodicity.  Theorem II.1 (Weyl transform invariance property). Let {ωa,b(y)} be the Weyl coefﬁcients of y ∈ R2m. If y(cid:48) = D(a(cid:48), b(cid:48))y for some (a(cid:48), b(cid:48)) ∈ Zm 2 , then, for all (a, b) ∈ Zm such that aT b = 01, |ωa,b(y(cid:48))| = |ωa,b(y)|.  2  Proof: See Section IV-A. As an illustration of the above result, Figure 3 displays a simple texture pattern, along with four patches taken from it at different positions and under different orientations. In the context of texture classiﬁcation, it is desirable to have a repre- sentation which identiﬁes all of these patches as coming from the same texture. In fact, all four patches can be obtained from each other by applying D(a, b) matrices, and they therefore have the same Weyl transform coefﬁcients in absolute value. In Section I, we presented an example demonstrating the ability of the Weyl transform to effectively classify patches from two different textures (Fig. 1). The Weyl transform dis- tinguishes the two textures by capturing multiscale symmetries which are important in describing texture. Moreover, invari- ance to multiscale transformations (Theorem II.1) ensures that the Weyl transforms of patches from the same texture exhibit similarity. Each D(a, b) matrix is a product of a permutation matrix and a diagonal ‘sign-change’ matrix with ±1 entries. The sign- change patterns turn out to be columns of the Walsh-Hadamard transform matrix, also known as Walsh functions [7]. We will show in Section II-C that the Weyl transform can equally be viewed as a combination of autocorrelations and the Walsh- Hadamard transform, two familiar tools in signal processing (see Section I). This connection also provides a method for  1The result in fact holds for all (a, b), but we make the assumption aT b = 0  to simplify some of the analysis.  05010015000.050.10.1505010015000.050.10.1505010015000.050.10.1505010015000.050.10.1505010015000.050.10.1505010015000.10.205010015000.10.205010015000.10.205010015000.10.205010015000.10.212345678910123456789103  The matrices D(0, b) for m = 4 are displayed in the second row of Fig. 4. We will show in Section II-C that the sign patterns are the columns of the Walsh-Hadamard transform matrix, also known as Walsh functions [7]. Deﬁne D(a, b), for a, b ∈ Zm  2 , by  D(a, b) := D(a, 0)D(0, b),  (2) to obtain a collection of signed permutation matrices. The third row of Fig. 4 gives some examples of matrices D(a, b) for m = 4.  Fig. 4: The signed permutation matrices D(a, b) for m = 4: -1 (black), 0 (grey), 1 (white). First row: Permutation matrices D(a, 0). Second row: Sign change matrices D(0, b). Third row: Examples of matrices D(a, b).  (cid:9) .  2 , λ ∈ Z4  Deﬁne  HW 2m :=(cid:8)iλD(a, b) : a, b ∈ Zm  (3) The set HW 2m forms the binary Heisenberg-Weyl group  under matrix multiplication [4]. The set of all real symmetric matrices is a vector space, which we denote by V . Given real symmetric matrices R, S ∈ V , associate with V the inner product (R, S) := Tr(RT S), which induces the Frobenius norm (cid:107)R(cid:107)F := [Tr(RT R)] 1 2 on V . We now show that matrices D(a, b) with aT b = 0 form a basis for V . Deﬁne  2 :=(cid:8)(a, b) : a, b,∈ Zm  Ym  2 , aT b = 0(cid:9) , (cid:27)  and  B2m :=  D(a, b) : a, b ∈ Ym  2  .  (cid:26) 1  2m/2  Lemma II.2. B2m is an orthonormal basis for V .  Proof: If aT b = 1 then D(a, b) squares to −I, and hence the inner product of D(a, b) with any real symmetric matrix is zero. For more details see Section IV-A. Lemma II.2 implies that any real symmetric matrix R ∈ R2m×2m can be expanded as  (cid:27) 1  Tr[R · D(a, b)]  D(a, b).  2m/2  In particular, given a vectorized signal y ∈ R2m, its correlation matrix yyT can be expanded as  (cid:88)  (cid:26) 1  R =  (a,b)∈Ym  2  2m/2  (cid:26) 1  (cid:88) (cid:88)  yyT =  =  Tr[yyT · D(a, b)]  (a,b)∈Ym  2  2m/2  ωa,b(y)  1  2m/2  D(a, b).  (a,b)∈Ym  2  (cid:27) 1  2m/2  D(a, b)  (4)  The coefﬁcients ωa,b(y) are the Weyl coefﬁcients of y as intro- duced in Section II-A. When we connect the Weyl transform to autocorrelation in Section II-C, it will be convenient to pad  (a)  (b)  (c)  Fig. 3: (a) A simple texture pattern; (b) Four subpatches taken at different positions and orientations; (c) The Weyl transforms of each subpatch.  the fast computation of the Weyl transform by means of the fast (fully binary) Walsh-Hadamard transform (FWHT).  B. The binary Heisenberg-Weyl group  The Weyl transform consists of inner products with signed permutation matrices from the binary Heisenberg-Weyl group, which we next describe in more detail. The Heisenberg-Weyl group can be thought of as a multiscale extension of the Dihedral group D8, namely the symmetries of the square: four rotations through multiples of 90◦, and reﬂections in its four axes of symmetry. These transformations can equally be viewed as matrix multiplications. Setting  (cid:18) 1 the matrix group(cid:8)±X aZ b : a, b ∈ Z2 (cid:9) provides a represen-  (cid:18) 0 1  0 0 −1  and Z =  (cid:19)  (cid:19)  1 0  X =  (1)  ,  tation of D8, where Z2 := {0, 1}.  Next we use binary m-tuples to label the entries of a vector of length 2m for some positive integer m. The v-th coordinate is labeled by the binary expansion of v, and coordinates are ordered from right to left, so that v = vm−12m−1 + . . . + v0 is represented by (vm−1 . . . v1 v0)T ∈ Zm (where, for 2 = {(0 0)T , (0 1)T , (1 0)T , (1 1)T}). Given a = example, Z2 (am−1 . . . a0)T ∈ Zm 2 , deﬁne D(a, 0) to be the permutation matrix given by the Kronecker product  2  D(a, 0) := (X am−1 ) ⊗ . . . ⊗ (X a0 ).  Note that the D(a, 0) are dyadic multiscale permutations, with the leftmost terms giving coarse-scale permutations and the rightmost terms giving ﬁne-scale permutations. The ﬁrst row of Fig. 4 displays the matrices D(a, 0) in the case of m = 4. Similarly, given b = (bm−1 . . . b0)T ∈ Zm 2 , deﬁne D(0, b) to be the sign change matrix given by the Kronecker product  D(0, b) := (Z bm−1) ⊗ . . . ⊗ (Z b0 ).  with zeros by deﬁning ωa,b := 0 when aT b = 1. Since we have expanded in an orthonormal basis, the mapping from yyT to {ωa,b} is an isometry, namely the Weyl transform as introduced in Section II-A. The Weyl transform also has a striking geometrical inter- pretation. Each D(a, b) matrix (except for the identity) has ±1 eigenspaces of multiplicity m/2 respectively, so that  D(a, b) = (cid:2)Pa,b Qa,b  (cid:3)(cid:20)I  (cid:21)(cid:20)P T  (cid:21)  a,b QT a,b  0 0 −I a,b − Qa,bQT a,b,  = Pa,bP T  where Pa,b and Qa,b are orthonormal bases for the +1 and −1 eigenspaces respectively. The Weyl coefﬁcient ωa,b can therefore be expressed as  ωa,b = =  1  2m/2 Tr[yyT (Pa,bP T  a,by(cid:107)2  F − (cid:107)QT  1  2m/2  (cid:110)(cid:107)P T  (cid:111) a,b − Qa,bQT a,b)] ,  a,by(cid:107)2  F  which reveals that ωa,b gives information about the relative distance of the covariance matrix yyT from two half-spaces. A large positive value means that the image is in the +1 eigenspace; a large negative value means that the image is in the −1 eigenspace. A large absolute value of either sign indicates that the image exhibits periodic symmetry.  It is shown in [20] that the collection of half-spaces induced by the Weyl transform is in fact the optimal half-space packing originally given in [21]2. The Weyl transform can therefore be viewed as a principled matched ﬁlter for covariance matrices.  C. The Weyl transform and autocorrelation: a connection  We next establish the bridge between Weyl coefﬁcients and autocorrelation, showing that the Weyl transform can equally be viewed in terms of the Walsh-Hadamard transform [7] of binary autocorrelations. Consider a vectorized signal y ∈ R2m indexed by a binary m-tuple v = (vm−1 . . . v0)T . Then the correlation matrix yyT may be divided into 2m autocorrelation bands za ∈ R2m, where  (za)v := yvyv+a.  (5)  Each autocorrelation band za gives information on the invari- ance of a signal to a particular binary translation v → v + a. Given a binary m-tuple a, deﬁne ωa ∈ R2m to be the subset of Weyl coefﬁcients indexed by a, that is (ωa)b := ωa,b. Deﬁne the 2m × 2m Walsh-Hadamard transform matrix H2m by  (H2m)v,w :=  (−1)vT w,  1  2m/2  where v and w are binary m-tuples [7]. The following result characterizes the Weyl transform as a combination of autocor- relations and the Walsh-Hadamard transform.  Theorem II.3 (Weyl transform in terms of autocorrelation). Let the autocorrelation bands of y, {za}, be deﬁned as in (5). Then ωa(y) = H2m za, where {ωa}b = 0 if aT b = 1.  Proof: See Section IV-B.  2It is also shown in [21] that this subspace packing can be constructed  recursively.  D  4  III. ILLUSTRATIVE EXAMPLES: THE INVARIANCE AND  VERSATILITY OF THE WEYL TRANSFORM  We next further illustrate fundamental properties of the Weyl transform using real-world texture examples. We suggest here two ways to exploit the Weyl transform for effective signal rep- resentation: equivalence class histograms and supervised coef- ﬁcient selection. We ﬁrst exploit the underlying group structure of the transform to build invariance to particular geometrically- signiﬁcant transformations; and then we describe how training data can be used to select the Weyl coefﬁcients that are most signiﬁcant in distinguishing the classes.  A. Equivalence class histograms  While the absolute values of the Weyl coefﬁcients are al- ready invariant under Heisenberg-Weyl transformations, more can be said if we allow the coefﬁcients to be permuted (but unchanged in absolute value). Given an arbitrary linear transformation y → φy, the cyclic property of the trace implies that  ωa,b(φy) = =  1  2m/2 Tr[φyyT φT D(a, b)] 2m/2 Tr[yyT φT D(a, b)φ],  1  which in turn implies that φ permutes the Weyl coefﬁcient absolute values if it is an outer automorphism of HW 2m. The outer automorphism group of the Heisenberg-Weyl group is known: it is the binary symplectic group Sp(2m, Z2) [4]. Based on these observations, we may conceive of a method of pooling Weyl coefﬁcients which builds invariance to any desired symplectic transformation, which we next describe. While often, e.g., in deep learning, pooling is heuristically designed [22], here we provide a principled approach to it [23], [2]. Let G be some (sub)group of symplectic transformations. Then G acts on HW 2m by conjugation, and the action partitions HW 2m into equivalence classes. This means that the group G can only permute the elements of HW 2m within each equivalence class. It follows that the average of the absolute values of the Weyl coefﬁcients within a given equivalence class is invariant under G. Such equivalence class averaging has the further appeal of reducing the number of Weyl coefﬁcients, and it is also extremely versatile since any group of geometrically- signiﬁcant symplectic transformations can be considered. As an example, we illustrate the proposed approach in the context of a vectorized N × N image y, taking G to be the group of transformations generated by 90◦ rotation and cyclic horizontal and vertical translation by any multiple of N/4. We ﬁrst show that these are each indeed symplectic transformations. We assume that our original image Y has dimensions N × N, N = 2r, and that is vectorized columnwise to give y, a vector of length 2m where m = 2r. Let a, b ∈ Z2r 2 , and let us write a = (a2r−1 . . . a0)T and b = (b2r−1 . . . b0)T . We write 1r for the vector of r ones.  it  Proposition III.1 (Symplectic permutations). (i) Rotation of Y by 90◦ clockwise corresponds to the mapping  (cid:18)(cid:20) a1  a2  (cid:21)  (cid:20) b1  b2  ,  (cid:21)(cid:19)  −→ (−1)1T b1 D  (cid:18)(cid:20) a2  a1  (cid:21)  (cid:20) b2  b1  ,  (cid:21)(cid:19)  , (6)  5  (a) Jeans  (b) CottonB  (c) CottonG  (d) FabricG  (e) Fabric  (f) TextileG  (g) TextileB  Fig. 5: Seven fabric textures.  where a1, a2, b1, b2 ∈ Zr 2. (ii) Cyclic translation of Y by N/4 vertically corresponds to the mapping  (cid:32)(cid:20) a1  (cid:21)  (cid:34) b1  (cid:35)(cid:33)  (cid:18)(cid:20) a1  (cid:21)  (cid:20) b1  (cid:21)(cid:19)  D  ,  j k a2  where a1, b1 ∈ Zr  l m b2  −→ (−1)mD 2, a2, b2 ∈ Zr−2  2  j+k  k a2  ,  l  l+m  b2  and j, k, l, m ∈ Z2.  ,  (7)  Proof: See Section IV-C. An analogous result for cyclic translation by N/4 horizon- tally can be obtained by permuting the vertical and horizontal coordinates. Note that a cyclic translation by N/2 ﬁxes the Weyl coefﬁcients’ absolute values, revealing the transforma- tion to be an element of the Heisenberg-Weyl group. For illustration, in the following experiments, we consider the case of a 4 × 4 image patch, vectorized to give y ∈ R16, so that its Weyl coefﬁcients ωa,b are indexed by a pair of 4- tuples. Let G be the group of transformations generated by 90◦ rotation and cyclic translation by N/4 (in this case, all possible cyclic translations) either vertically or horizontally. G partitions the a coordinates into the 6 equivalence classes  (cid:8)(0 0 0 0)(cid:9), (cid:8)(1 0 1 0)(cid:9), (cid:8)(0 0 1 0), (1 0 0 0)(cid:9), (cid:8)(0 0 0 1), (0 0 1 1), (0 1 0 0), (1 1 0 0)(cid:9), (cid:8)(0 1 0 1), (0 1 1 1), (1 1 0 1), (1 1 1 1)(cid:9), (cid:8)(0 1 1 0), (1 0 0 1), (1 0 1 1), (1 1 1 0)(cid:9), (cid:8)(0 0 0 0)(cid:9), (cid:8)(0 1 0 1)(cid:9), (cid:8)(0 0 0 1), (0 1 0 0)(cid:9), (cid:8)(0 0 1 0), (0 0 1 1), (1 0 0 0), (1 1 0 0)(cid:9) (cid:8)(1 0 1 0), (1 0 1 1), (1 1 1 0), (1 1 1 1)(cid:9), (cid:8)(1 0 0 1), (0 1 1 0), (0 1 1 1), (1 1 0 1)(cid:9).  while, independently, G also partitions the b coordinates into the 6 equivalence classes  (8)  (9)  We thus partition the Weyl coefﬁcients into 36 equivalence classes.  Six of the classes contain Weyl coefﬁcients which are all zero, since aT b = 1 for all members of the class, and so we discard these classes. In addition, the six classes for which a = 0 give variance information rather than autocorrelation information, so we also discard these classes. We are left with 24 classes, within each of which we average the Weyl coefﬁcient absolute values. Since G permutes the members of each class, these averages are invariant to all transformations in G. We have thereby reduced the number of Weyl coefﬁcients from 136 to 24, creating a representation that is invariant under not only the Heisenberg-Weyl group, but also the group G. In particular, compactness means that different ways of orienting and translating the same texture produce the same descriptor. Note that this equivalent class histogram approach is versatile: any geometrically-signiﬁcant symplectic transformations could  be quotiented out in this way depending on the expected image symmetries to be encountered.  Fabric texture examples. Seven real-world fabric textures are shown in Fig. 5. In this set of experiments, we randomly sample 500 16×16-sized gray-scale patches from each texture, and mix them up. The obtained 3500 texture patches are represented using 5 different descriptors: Weyl, intensity (the gray-scale value), Gabor [1], LBP [19], and HOG [18]. With the equivalence classes deﬁned above, 24 Weyl coefﬁcients (absolute values) are required to represent each 4×4 patch. For a 16× 16 patch, we further average across the 16 obtained 24- sized vectors to form a 24-sized Weyl descriptor. The Gabor features used here have 5 scales and 8 orientations, down- sampled by a factor of 4, and each feature vector is of length 640 . We use the basic LBP histogram of length 256. For HOG features, we use a block size of 4 × 4 and 36 histogram bins, which gives a 576-sized feature vector. The proposed Weyl representation is therefore the most compact one.  In various texture analysis tasks, e.g., texture classiﬁcation, segmentation, and retrieval, it is critical to have a consistent representation for patches of the same texture type; and it is also desirable to have for each patch a small size descriptor to enable efﬁcient computation. Fig. 6 shows the obtained 3500 texture patches represented in different descriptors. Different textures are shown in different colors. For visualization, data are plotted with the dimension reduced to 2 using PCA. A good texture descriptor encourages a compact and isolated cluster for the same color points. To enable a quantitative assessment, we perform k-means clustering on texture patches represented in different descriptors (before dimension reduc- tion is applied). The proposed Weyl representation is the most compact and discriminative: a 24-sized descriptor gives 86.49% clustering accuracy. Note that the clustering accuracy here only approximately assesses the discriminability of a representation, since different clustering schemes can lead to different clustering results. The popular HOG and LBP provide comparable discriminability, but require an order of magnitude longer descriptors than the Weyl representation. We notice that the Gabor descriptor fails to effectively represent patches from Fabric and Textile Blue. After excluding these two textures, we obtain 74.96% clustering accuracy using Gabor features still below our proposed representation.  We next obtain multiple variants of each texture in Fig. 5 using reﬂections in horizontal and vertical axes and rotations through multiples of 90◦ (dihedral transformations). We repeat the above experiments by sampling 500 patches from multiple variants of each texture. The sampled patches are represented using different descriptors, and plotted in Fig. 7 with different  6  (a) Weyl (86.49%)  (b) Intensity (50.82%)  (c) HOG (78.28%)  (d) Gabor (48.48%)  (e) LBP (78.71%)  Fig. 6: Fabric texture patches represented with different descriptors. Data are visualized with the dimension reduced to 2 using PCA. Different textures are shown in different colors: Jeans (blue), CottonB (green), CottonG (red), FabricG (cyan), Fabric (magenta), TextileG (yellow), TextileB (black). The k-means clustering accuracies in parentheses approximately assess the discriminability (and class compactness) of each descriptor. The respective size of each descriptor is Weyl (24), Intensity (256), HOG (576), Gabor (640) and LBP (256). The proposed Weyl descriptor is both the most compact and discriminative.  (a) Weyl (84.62%)  (b) Intensity (33.65%)  (c) HOG (30.00%)  (d) Gabor (32.65%)  (e) LBP (53.40%)  Fig. 7: Fabric texture patches under the dihedral transformations represented with different descriptors (see Fig. 6 for notation description). Comparing to Fig. 6, we observe that the Weyl representation is invariant under dihedral group transformations. For other descriptors, we observe new sub-clusters, indicating that signiﬁcantly different descriptors are assigned to the same texture under dihedral transformations.  (a) Sand  (b) Rock  (c) Sea  (d) Sky  (e) Wood  Fig. 8: Five nature textures.  colors for different texture types. By comparing Fig. 7 to Fig. 6, we observe that the Weyl representation is invariant under dihedral transformations. For other descriptors, new sub- clusters emerge among the same color points; in other words, those descriptors assign signiﬁcantly different representations to the same texture under different dihedral transformations, causing inferior performance for many texture analysis tasks. Textures from Nature. We conduct the same set of ex- periments on textures of signiﬁcantly less regularity, i.e., the ﬁve nature textures in Fig. 8. As shown in Fig. 9, the Weyl representation is still the most compact and discriminative. In Fig. 10, we again observe the invariance of the Weyl represen- tation under dihedral transformations, enabling signiﬁcantly more consistent representation than other descriptors.  B. Supervised coefﬁcient selection  We suggest here another effective way to adopt the Weyl representation through supervised coefﬁcient selection. Con- sider two-class data points (multi-class cases can be supported using the standard one-vs-all strategy), where the class labels  are known beforehand for training purposes. Let Y+ and Y− denote the set of points in each of the two classes respectively, where points are arranged as columns of the corresponding matrix. We assume here that the same number of points are selected for each class for computational efﬁciency.  We now rank all Weyl coordinates based on the discrim- inability, i.e., how signiﬁcant they are in distinguishing the classes, and only use the best K Weyl coordinates for classiﬁ- cation. The ability of the Weyl coordinate (a, b) in separating two classes can be assessed by the Weyl coefﬁcient magnitude |ωa,b(M )| of the matrix M = (Y+ − Y−)(Y+ + Y−)(cid:48): We pick one point from each class, y+ ∈ Y+ and y− ∈ Y−, with their respective Weyl coefﬁcient difference at the coordinate (a, b) given by,  (cid:12)(cid:12)Tr[y+yT +D(a, b)] − Tr[y−yT−D(a, b)](cid:12)(cid:12) +D(a, b)y+] − Tr[yT−D(a, b)y−](cid:12)(cid:12) = (cid:12)(cid:12)Tr[yT = (cid:12)(cid:12)yT +D(a, b)y+ − yT−D(a, b)y−(cid:12)(cid:12) = (cid:12)(cid:12)(y+ − y−)T D(a, b)(y+ + y−)(cid:12)(cid:12) = (cid:12)(cid:12)Tr[(y+ − y−)(y+ + y−)T D(a, b)](cid:12)(cid:12) .  (10)  −0.15−0.1−0.0500.050.1−0.06−0.04−0.0200.020.040.060.08  JeansCottonBCottonGFabricGFabricTextileGTextileB−0.8−0.6−0.4−0.200.20.40.60.8−0.8−0.6−0.4−0.200.20.40.60.8  JeansCottonBCottonGFabricGFabricTextileGTextileB−0.8−0.6−0.4−0.200.20.40.6−0.8−0.6−0.4−0.200.20.40.6  JeansCottonBCottonGFabricGFabricTextileGTextileB−15−10−505101520−15−10−5051015  JeansCottonBCottonGFabricGFabricTextileGTextileB−40−30−20−100102030−20−15−10−50510152025  JeansCottonBCottonGFabricGFabricTextileGTextileB−0.15−0.1−0.0500.050.1−0.06−0.04−0.0200.020.040.06  JeansCottonBCottonGFabricGFabricTextileGTextileB−0.8−0.6−0.4−0.200.20.40.60.8−0.8−0.6−0.4−0.200.20.40.60.8  JeansCottonBCottonGFabricGFabricTextileGTextileB−0.8−0.6−0.4−0.200.20.40.6−0.8−0.6−0.4−0.200.20.40.6  JeansCottonBCottonGFabricGFabricTextileGTextileB−15−10−505101520−20−15−10−5051015  JeansCottonBCottonGFabricGFabricTextileGTextileB−40−30−20−100102030−25−20−15−10−505101520  JeansCottonBCottonGFabricGFabricTextileGTextileB7  (a) Weyl (70.78%)  (b) Intensity (40.16%)  (c) HOG (61.11%)  (d) Gabor (26.48%)  (e) LBP (68.90%)  Fig. 9: Nature textures represented with different descriptors. For visualization, data are plotted with the dimension reduced to 2 using PCA. Different textures are shown in different colors: Sand (blue), Rock (green), Sea (red), Sky (cyan), Wood (magenta). The k-means clustering accuracies in the parentheses approximately assess the discriminability of each descriptor. The proposed Weyl descriptor is the most compact and discriminative.  (a) Weyl (74.97%)  (b) Intensity (31.95%)  (c) HOG (44.29%)  (d) Gabor (26.71%)  (e) LBP (51.18%)  Fig. 10: Same as Fig. 7 for the textures in Fig. 8.  Thus, we can calculate the total Weyl coefﬁcient difference between Y+ and Y− at the coordinate (a, b) as |Tr[(Y+ − Y−)(Y+ + Y−)T D(a, b)]|. We rank all Weyl coordinates (a, b) in descending order of |ωa,b(M )|, and only use the best K coordinates for classiﬁcation. Recalling the geometrical interpretation of Weyl coefﬁcients in terms of relative distance from two half-spaces in Section II-B, the aim here is to identity the K subspace pairs whose relative distances discriminate the most between the two classes.  We pick a pair of texture images from Fig. 5 and 8, and randomly sample 500 16 × 16-sized gray-scale patches from each texture. We randomly choose 20 patches for each texture as training, and the remaining 960 patches as testing. Patches are represented using different descriptors, and Nearest Neigh- bor classiﬁers are used to assign each testing patch to a texture type. Note that, for testing samples, only coefﬁcients at the K selected Weyl coordinates need to be computed. Table 11 reports the results. The best Weyl coefﬁcient already provides accuracies comparable to a 256 long intensity feature descriptor. When 16 Weyl coefﬁcients are used, we obtain accuracies comparable to all other popular descriptors, but with an order of magnitude shorter descriptor. Fig. 12 shows the classiﬁcation accuracies when different numbers of best Weyl coefﬁcients are used; note the fast convergence.  IV. PROOFS  size  256 576 640 256 1 3 16  Intensity HOG Gabor LBP Weyl Weyl Weyl  Sand  Jeans vs. TextileB vs. Sea 86.45 97.60 91.14 100 99.89 100 100  74.06 99.79 71.25 100 99.68 99.68 99.79  Sand vs. Rock 66.56 83.54 64.79 85.41 66.56 76.25 84.06  Fig. 11: Accuracy (%) of classifying texture patches repre- sented using different descriptors.  Fig. 12: Classiﬁcation accuracies with different number of best Weyl coefﬁcients.  A. Proof of the Weyl transform invariance property  Recalling the deﬁnitions of the D(a, b) matrices given in  (2) and (1), we also introduce the notation  (cid:19)  (cid:18) 1  0  (cid:18) 0  1  (cid:19)  ,  This section contains proofs of all results given in Sec-  tions II and III.  e0 :=  ,  e1 :=  −0.15−0.1−0.0500.050.1−0.06−0.04−0.0200.020.040.06  SandRockSeaSkyWood−1−0.8−0.6−0.4−0.200.20.40.60.81−1−0.8−0.6−0.4−0.200.20.40.60.81  SandRockSeaSkyWood−0.6−0.4−0.200.20.40.60.8−0.8−0.6−0.4−0.200.20.40.6  SandRockSeaSkyWood−20−15−10−50510152025−20−15−10−505101520  SandRockSeaSkyWood−40−30−20−100102030−60−50−40−30−20−10010203040  SandRockSeaSkyWood−0.15−0.1−0.0500.050.1−0.06−0.04−0.0200.020.040.060.08  SandRockSeaSkyWood−1−0.8−0.6−0.4−0.200.20.40.60.81−1−0.8−0.6−0.4−0.200.20.40.60.81  SandRockSeaSkyWood−0.8−0.6−0.4−0.200.20.40.60.8−0.4−0.3−0.2−0.100.10.20.30.40.50.6  SandRockSeaSkyWood−25−20−15−10−5051015−20−15−10−505101520  SandRockSeaSkyWood−40−30−20−10010203040−30−20−10010203040  SandRockSeaSkyWood1020304050600.60.70.80.91Number of CoefficientsClassification Accuracy  Sand vs. RockJeans vs. TextileBSand vs. Seafor the canonical basis elements in R2. In order to prove Lemma 2.1 and Theorem 2.2 we need the following lemma, which gives various properties of the D(a, b) matrices.  8  (cid:26) 1  It remains to show that the elements of B2m are orthonor-  mal. It follows from (12) that, for any (a, b) ∈ Ym 2 ,  Tr  [D(a, b)]T  1  2m/2  2m/2  [D(a, b)]  =  1  2m Tr[(−1)aT bI] = 1,  (cid:27)  (cid:0)X aZ b(cid:1)  Lemma IV.1. Let X, Z, and D(a, b) be deﬁned in (2) and (1). Then, for all v, a, b, a(cid:48), b(cid:48) ∈ Zm 2 , (i) D(a, b)ev = (−1)bT vev+a; (ii) [D(a, b)]T = [D(a, b)]−1 = (−1)aT bD(a, b); (iii) D(a, b)D(a(cid:48), b(cid:48)) = (−1)a(cid:48)T bD(a + a(cid:48), b + b(cid:48)). Proof: It follows from (1) that, given v, a, b ∈ Z2  (11)  (12)  (13)  Given v = (vm−1 . . . v0)T ∈ Zm  v = (−1)bvev+a. 2 , we have  e(vm−1...v0) = evm−1 ⊗ . . . ⊗ ev0,  (15) which combines with (2) and a standard property of the Kronecker product to give  D(a, b)ev  Combining (14) and (16) then gives  (cid:3) ⊗ . . . ⊗(cid:2)(−1)b0v0e(v0+a0)  (cid:2)(−1)bm−1vm−1 e(vm−1+am−1)  = (X am−1Z bm−1) ⊗ . . . ⊗ (X a0Z b0 )(evm−1 ⊗ . . . ⊗ ev0) = (X am−1Z bm−1evm−1) ⊗ . . . ⊗ (X a0Z b0 ev0). (16) (cid:3)  D(a, b)ev = = (−1)bT v(e(vm−1+am−1) ⊗ . . . ⊗ e(v0+a0)), and (11) now follows by (15). It follows from (11) that [D(a, b)]2 = I for all a, b ∈ Zm 2 , from which it follows [D(a, b)]T = [D(a, b)]−1. Inverting (11), and letting that w = v + a, we obtain [D(a, b)]−1ew = (−1)bT (w+a)ew+a = (−1)aT bD(a, b)ew for all w, a, b ∈ Zm also have, by (11), that  2 , which completes the proof of (12). We  D(a + a(cid:48), b + b(cid:48))ev = (−1)(b+b(cid:48))T vev+a+a(cid:48)  (17) 2 . But two applications of (11) give  for all v, a, b, a(cid:48), b(cid:48) ∈ Zm  D(a, b)D(a(cid:48), b(cid:48))ev = D(a, b)(−1)b(cid:48)T vev+a(cid:48)  = (−1)bT (v+a(cid:48))(−1)b(cid:48)T vev+a+a(cid:48) = (−1)a(cid:48)T b(−1)(b+b(cid:48))T vev+a+a(cid:48).  Since (17) and (18) hold for all v ∈ Zm  2 , (13) follows.  (18) (cid:3)  Proof of Lemma 2.1: A symmetric matrix is determined by the entries on and above the diagonal, which gives  dim(V ) = 2m +  · 2m(2m − 1) =  1 2  1 2  · 2m(2m + 1).  (19)  On the other hand, writing a = (am−1 . . . a0)T , b = (bm−1 . . . b0)T , conditioning on the pair (am−1, bm−1) leads to the recurrence |Ym 2 | + 22(m−1), from which it is easy to deduce that |Ym 2 · 2m(2m + 1), which combines with (19) to give |Ym  2 | = 2|Ym 2 | = 1 2 | = dim(V ).  where in the last step we use the assumption that aT b = 0. Similarly, it follows from (12) and (13) that, for any a, b, a(cid:48), b(cid:48) ∈ Zm 2 ,  (cid:26) 1 (cid:104)  2m/2  Tr  [D(a, b)]T  1  2m/2  [D(a(cid:48), b(cid:48))]  (cid:27)  (cid:105)  =  1 2m Tr  (−1)aT b(−1)a(cid:48)T bD(a + a(cid:48), b + b(cid:48))  . (20) If a (cid:54)= a(cid:48), D(a + a(cid:48), b + b(cid:48)) is zero on its diagonal, whereas if a = 0 and b (cid:54)= 0, the diagonal terms are nonzero but sum to zero. This observation combines with (20) to show that the elements of B2m are orthogonal, which completes the proof. (cid:3)  (14)  Proof of Theorem 2.2 By (12) and (13), we have, for all (a, b) ∈ Ym 2 , [D(a(cid:48), b(cid:48))]T D(a, b)D(a(cid:48), b(cid:48))  = (−1)a(cid:48)T b[D(a(cid:48), b(cid:48))]T D(a + a(cid:48), b + b(cid:48)) = (−1)a(cid:48)T b(cid:48) = (−1)a(cid:48)T (b+b(cid:48))(−1)(a+a(cid:48))T b(cid:48) = (−1)aT b(cid:48)+a(cid:48)T bD(a, b),  (−1)a(cid:48)T bD(a(cid:48), b(cid:48))D(a + a(cid:48), b + b(cid:48))  D(a, b)  which proves the result.  (cid:3)  B. Proof of Theorem 2.3  We may use (11) to deduce  (cid:88)  D(a, b)y =  which gives  D(a, b)yvev =  v  v  (cid:88)  (−1)bT vyvev+a,  {D(a, b)y}v = (−1)bT (v+a)yv+a. Meanwhile, the cyclic property of the trace gives  (21)  Tr[yyT D(a, b)] = Tr[yT D(a, b)y] = yT D(a, b)y.  (22)  Combining (21) and (22), we have  {ωa(y)}b = ωa,b =  =  =  =  yT D(a, b)y  (cid:88) yv {D(a, b)y}v (cid:88) (cid:88)  v  (−1)bT vyvyv+a  (−1)bT wywyw+a, (23)  1  2m/2  1  v  2m/2 (−1)aT b 2m/2 1  2m/2  w  where in the last step we made the substitution w = v + a. Now (23) implies that {ωa(y)}b = 0 whenever aT b = 1, and the result now follows from the deﬁnitions of H2m and za.(cid:3)  C. Proof of Proposition 3.1  Rotation by 90◦ clockwise can be achieved by an inter- change of vertical and horizontal coordinates followed by a reversal of the horizontal coordinates, which is the mapping (24) where y is indexed by the binary 2r-tuple v = (v1 v2)T , v1, v2 ∈ Zr 2, and ev is a canonical basis vector. Writing a = (a1 a2)T , b = (b1 b2)T , we have by (24) and (11) that  θ : e(v1,v2) −→ e(v2+1r,v1),  (cid:18)(cid:20)a1 (cid:18)(cid:20)a1  a2  θ−1D  = θ−1D = (−1)1T = (−1)1T  a2 r b1+bT  r b1+bT  ,  ,  b2  (cid:21) (cid:21)  θe(v1,v2)  (cid:20)b1 (cid:20)b1  (cid:21)(cid:19) (cid:21)(cid:19) b2 2 v1θ−1e(v2+a1+1r,v1+a2) 1 v2+bT 2 v1e(v1+a2,v2+a1). (cid:21)  e(v2+1r,v1)  (cid:21)(cid:19)  (cid:18)(cid:20)a2  (cid:20)b2  1 v2+bT  (−1)1T = (−1)1T  r b1 D  a1 1 v2+bT  r b1+bT  ,  b1  ev1,v2  2 v1e(v1+a2,v2+a1),  (25)  Meanwhile, (11) also implies that  which combines with (25) to give the result for rotation. Turning to the translation, consider the permutation  where v1 ∈ Zr permutes the ﬁrst two vertical binary coordinates as  t : e(v1,p,q,v2) −→ e(v1,p+q,q+1,v2), 2, v2 ∈ Zr−2 (cid:21)  (26) and p, q ∈ Z2. The mapping t  (cid:20) 0 0 1 1  (cid:20) 0  (cid:21)  2  −→  1 1 0 0 1 0  1  ,  0 1 0 1  which gives a vertical cyclic translation through N/4. Writing a = (a1 j k a2)T , b = (b1 l m b2)T , we have by (26) and (11) that t−1D  te(v1,p,q,v2) =  ,  (cid:18)(cid:20) a1 (cid:18)(cid:20) a1  j k a2  (cid:21) (cid:21)  (cid:20) b1 (cid:20) b1  l m b2  (cid:21)(cid:19) (cid:21)(cid:19)  j k a2 1 v1+bT  t−1D (−1)bT = (−1)bT  l m b2  e(v1,p+q,q+1,v2) =  , 2 v2+l(p+q)+m(q+1)t−1e(v1+a1,p+q+j,q+1+k,v2+a2) 2 v2+lp+lq+mq+me(v1+a1,p+j+k+1,q+k,v2+a2). (27)  Meanwhile, (11) also implies that (−1)mD k a2 = (−1)bT 1 v1+bT 2 v2+lp+lq+mq+me(v1+a1,p+j+k+1,q+k,v2+a2), which combines with (27) to give the result for translation.(cid:3)  ev1,p,q,v2  l+m  j+k  b2  ,  l  1 v1+bT  (cid:32)(cid:20) a1  (cid:34) b1  (cid:35)(cid:33)  (cid:21)  V. CONCLUDING REMARKS  We have described how to represent measurement using the binary Weyl transform and developed new theory, supported by texture classiﬁcation experiments, that connects mathematical properties of the transform with a broad class of signal processing objectives. In particular, we used invariance of the transform to a large class of multi-scale dihedral transfor- mations to pool Weyl coefﬁcients, thereby developing very concise histograms with high discriminative power. We also  9  described a supervised method that learns the transform coef- ﬁcient with the greatest discriminative value through training. Illustrative examples using real world texture examples were provided for both approaches.  REFERENCES  [1] S. Mallat, A Wavelet Tour of Signal Processing: The Sparse Way, 3rd ed.  Academic Press, 2009.  [2] L. Sifre and S. Mallat, “Rotation, scaling and deformation invariant scattering for texture discrimination,” in IEEE Conference on Computer Vision and Pattern Recognition, 2013.  [3] S. Howard, A. Calderbank, and W. Moran, “Finite Heisenberg-Weyl groups and Golay complementary sequences,” in International Waveform Diversity and Design Conference, 2006.  [4] ——, “The ﬁnite Heisenberg-Weyl groups in radar and communication,”  in EURASIP Journal on Applied Signal Processing, 2006, pp. 1–12.  [5] W. Miller Jr, Topics in harmonic analysis with applications to radar and sonar, ser. IMA Volumes in Mathematics and its Applications. Springer- Verlag, 1991, pp. 66–168.  [6] A. Calderbank, P. Cameron, W. Kantor, and J. Seidel, “Z4-kerdock codes, orthogonal spreads and Euclidean line-sets,” Proceedings of the London Mathematical Society, vol. 75, no. 3, pp. 436–480, 1997.  [7] S. Agaian, H. Sarukhanyan, K. Egiazarian, and J. Astola, Hadamard  Transforms. SPIE Press, 2011.  [8] T. Ogunfunmi and M. Narasimha, Principles of Speech Coding. CRC  Press, 2010.  [9] P. McLeod and G. Wyvill, “A smarter way to ﬁnd pitch,” in Proceedings  of the International Computer Music Conference, 2005.  [10] H. Seck, Y. Zhang, and Y. Soh, “Autocorrelation noise removal for optical coherence tomography by sparse ﬁlter design,” Journal of Biomedical Optics, vol. 17, no. 7, 2012.  [11] D. Chetverikov and Z. Foldvari, “Afﬁne-invariant texture classiﬁcation using regularity features,” in Texture Analysis in Machine Vision, ser. Machine Perception and Artiﬁcial Intelligence, M. Pietikainen, Ed. World Scientiﬁc, 2000.  [12] R. Goudail, E. Lange, T. Iwamoto, K. Kyuma, and N. Otsu, “Face recognition system using local autocorrelations and multiscale integra- tion,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 18, no. 10, pp. 1024–1028, 1996.  [13] J. McLaughlin and J. Raviv, “Nth-order autocorrelations in pattern recognition,” Journal of Information Control, vol. 12, pp. 121–142, 1968.  [14] V. Popovici and J. Thiran, “Pattern recognition using higher-order local autocorrelation coefﬁcients,” Pattern Recognition Letters, vol. 25, no. 10, pp. 1107–1113, 2004.  [15] J. Xiaoguang and M. Nixon, “Analysing front view face proﬁles for face recognition via the Walsh transform,” Pattern Recognition Letters, vol. 15, no. 6, pp. 551–558, 1994.  [16] R. Stankovic and M. Karpovsky, “Remarks on calculation of autocor- relation on ﬁnite dyadic groups by local transformations of decision diagrams,” in EUROCAST Conference on Computer Aided Systems Theory, 2005, pp. 301–310.  [17] Y. Hel-Or and H. Hel-Or, “Real-time pattern matching using projection kernels,” IEEE Transactions on Pattern Analysis and Machine Intelli- gence, vol. 27, no. 9, pp. 1430–1435, 2005.  [18] N. Dalal and B. Triggs, “Histograms of oriented gradients for human  detection,” in Computer Vision and Pattern Recognition, June 2005.  [19] M. Heikkila and M. Pietikainen, “A texture-based method for modeling the background and detecting moving objects,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 4, pp. 657–662, 2006.  [20] R. Calderbank, R. Hardin, E. Rains, P. Shor, and N. Sloane, “A group- theoretic framework for the construction of packings in grassmannian spaces,” Journal of Algebraic Combinatorics, vol. 9, no. 2, pp. 129–140, 1999.  [21] P. Shor and N. Sloane, “A family of optimal packings in grassmannian spaces,” Journal of Algebraic Combinatorics, vol. 7, no. 2, pp. 157–163, 1998.  [22] Y.-L. Boureau, N. Le Roux, F. Bach, J. Ponce, and Y. LeCun, “Ask the locals: Multi-way local pooling for image recognition,” in IEEE International Conference on Computer Vision, 2011.  [23] J. Bruna and S. Mallat, “Invariant scattering convolution networks,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1872–1886, 2013.  ",
1412.7210,2015, Denoising autoencoder with modulated lateral connections learns invariant representations of natural images,"['Denoising autoencoder with modulated lateral connections learns invariant representations of natural images', 'Antti Rasmus', 'Harri Valpola', 'and Tapani Raiko']",https://arxiv.org/pdf/1412.7210,"5 1 0 2    r a     M 1 3      ] E N . s c [      4 v 0 1 2 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  DENOISING AUTOENCODER WITH MODULATED LATERAL CONNECTIONS LEARNS INVARIANT REPRESENTATIONS OF NATURAL IMAGES  Antti Rasmus and Tapani Raiko Aalto University Finland {antti.rasmus,tapani.raiko}@aalto.fi  Harri Valpola ZenRobotics Ltd. Vilhonkatu 5 A, 00100 Helsinki, Finland harri@zenrobotics.com  ABSTRACT  Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant represen- tations. In regular autoencoders, detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verify that adding modulated lateral connec- tions to the model 1) improves the accuracy of the probability model for inputs, as measured by denoising performance; 2) results in representations whose degree of invariance grows faster towards the higher layers; and 3) supports the formation of diverse invariant poolings.  1  INTRODUCTION  Denoising autoencoders (dAE; Vincent et al., 2008) provide an easily accessible method for unsu- pervised learning of representations since training can be based on simple back-propagation and a quadratic error function. An autoencoder is built from two mappings: an encoder that maps cor- rupted input data to features, and a decoder that maps the features back to denoised data as output. Thus, in its basic form, autoencoders need to store all the details about the input in its representation. Deep learning has used unsupervised pretraining (see Bengio, 2013, for a review) but recently purely supervised learning has become the dominant approach at least in cases where a large number of labeled data is available (e.g., Ciresan et al., 2010; Krizhevsky et al., 2012). One difﬁculty with combining autoencoders with supervised learning is that autoencoders try to retain all the information whereas supervised learning typically loses some. For instance, in clas- siﬁcation of images, spatial pooling of activations throws away some of the location details while retaining identity details. In that sense, the unsupervised and supervised training are pulling the model in very different directions. From a theoretical perspective, it is clear that unsupervised learning must be helpful at least in a semi-supervised setting. Indeed, Kingma et al. (2014) obtained very promising results with varia- tional autoencoders. This raises hopes that the same could be achieved with simpler dAEs. Recently, Valpola (2015) proposed a variant of the denoising autoencoder that can lose informa- tion. The novelty is in lateral connections that allow higher levels of an autoencoder to focus on invariant abstract features and in layer-wise cost function terms that allow the network to learn deep  1  Accepted as a workshop contribution at ICLR 2015  (a) Basic autoencoder  (b) Lateral connections  (c) Layer size ratio  Figure 1: Examples of two-hidden-layer models: (a) denoising autoencoder and (b) Ladder network with lateral connections. Illustration of ratio, α, of h(2) size to h(1) size (right) with hidden layers of sizes 2 and 8. Ratio of α = 1.0 corresponds to hidden layers of equal size (a and b).  hierarchies efﬁciently. Valpola (2015) hypothesized that modulated lateral connections support the development of invariant features and provided initial results with artiﬁcial data to back up the idea. As seen in Figure 1, information can ﬂow from the input to the output through alternative routes, and the details no longer need to be stored in the abstract representation. This is a step closer to being compatible with supervised learning which can select which types of invariances and abstractions are relevant for the task at hand. In this paper, we focus on investigating the effects of lateral connections. We extend earlier results with experiments using natural image data and make comparisons with regular denoising autoen- coders without lateral connections in Section 2. We show the following:  of layers from balanced to bottom heavy.  • The proposed structure attains a better model of the data as measured by ability to de- noise. There are good reasons to believe that this indicates that the network has captured a more accurate probabilistic model of the data since denoising is one way of representing distributions (Bengio et al., 2013). • Including the modulated lateral connections changes the optimal balance between the sizes • The degree of invariance of the representations grows towards the higher levels in all tested models but much faster with modulated lateral connections. In particular, the higher levels of the model seem to focus entirely on invariant representations whereas the higher levels of the regular autoencoder have a few invariant features mixed with a large number of details. • Modulated lateral connections guide the layer above them to learn various types of pool- ings. The pooled neurons participate in several qualitatively different poolings that are each selective and invariant to different aspects of the input.  1.1  INVARIANT FEATURES THROUGH POOLING  There are typically many sources of variation which are irrelevant for classiﬁcation. For example, in object recognition from images, these sources could include position, orientation and scale of the recognized object and illumination conditions. In order to correctly classify new samples, these sources of variation are disregarded while retaining information needed for discriminating between different classes. In other words, classiﬁcation needs to be invariant to the irrelevant transformations. A simple but naive way to achieve such invariance would be to list all the possible realizations of objects under various transformations, but this is not usually practical due to the vast amounts of possible realizations. Also, this does not offer any generalization to new objects. A useful solution is to split the generation of invariance into more manageable parts by representing the inputs in terms of features which are each invariant to some types of transformations. Since different kinds of objects can be represented by the same set of features, this approach makes it possible to generalize the invariances to new, unseen objects. So rather than listing all the possible realizations of individual objects, we can list possible realiza- tions of their constituent features. Invariance is achieved by retaining only the information about  2  f(1)g(0)g(1)f(2)h(1)ˆh(1)h(2)ˆx˜xf(1)g(0)g(1)f(2)g(2)h(1)ˆh(1)h(2)˜xˆxˆh(2)↵=0.25↵=4.0Accepted as a workshop contribution at ICLR 2015  whether any of the possible realizations is present and discarding information about which one ex- actly. Such an operation is known as pooling and there are various ways of implementing it. In the case of binary inputs, OR operation is a natural choice. There are many ways to generalize this to continuous variables, including maximum operation (Riesenhuber & Poggio, 1999) or summation followed by a concave function (Fukushima, 1979). While pooling achieves invariance to irrelevant transformations, the features also need to be discrim- inative along relevant dimensions. This selectivity is often achieved by coincidence detection, i.e. by detecting the simultaneous presence of multiple input features. In the binary case, it can be imple- mented by an AND operation. In the continuous case, possibilities include extensions of the AND operation, such as product or summation followed by a convex function, but also lateral inhibition (i.e., competition) among a set of feature detectors. All of these operations typically produce sparse coding where the output features are sensitive to speciﬁc combinations of input features. Since this type of coding tends to increase the number of features, it is also known as feature expansion. The idea of alternating pooling and feature expansion dates at least back to Hubel & Wiesel (1962) who found that the early stages of visual processing in the cat cerebral cortex have alternating steps of feature expansion, implemented by lateral competition among so called simple cells, and invariance-generating poolings by so called complex cells. In such hierarchies of alternating steps, the degree of invariance grows towards the higher levels. Cortical processing also includes various normalizations, a feature which has also been included in some models (e.g., Fukushima, 1979). There are various ways of ﬁnding poolings that generate invariances (from specialized to general):  1. Invariance by design. For instance, invariance to translation and small deformations is achieved by pooling shifted versions of a feature (Fukushima, 1979). Similar pooling op- erations are now popular in convolutional neural networks (see, e.g., Schmidhuber, 2015). 2. Invariance to hand-crafted transformations. The transformations are applied to input sam- ples (e.g., image patches can be shifted, rotated, scaled or skewed, and colors or contrast can be modiﬁed) and pooling is then learned by requiring the output to stay constant over the transformation. This category includes supervised learning from inputs deformed by various transformations.  3. Invariance to transformations over time. Relies on nature to provide transformations as sequences during which the underlying feature (e.g., identity of object) changes slower than the transformation (e.g., F¨oldi´ak, 1991).  4. Invariance by exploiting higher-order correlations within individual samples. This is how supervised learning can ﬁnd poolings: target labels correlate nonlinearly with inputs. There are also unsupervised methods that can do the same. For example, subspace ICA can ﬁnd complex-cell like poolings for natural images (Hyv¨arinen & Hoyer, 2000).  We focus on the last type: exploiting higher-order correlations. Very few assumptions are made so the method is very general but it is also possible to combine this approach with supervised learning or any of the more specialized ways of generating invariances.  1.2 DENOISING AUTOENCODERS  Autoencoder networks have a natural propensity to conserve information and are therefore well suited for feature expansion. Autoencoder networks consist of two parameterized functions, encod- ing f and decoding g. Function f maps from input space x to feature space h, and g in turn maps back to input space producing a reconstruction, ˆx, of the original input, when the training criterion is to minimize the reconstruction error. This enables learning of features in an unsupervised manner. Denoising autoencoder (Vincent et al., 2008) is a variant of the traditional autoencoder, where the input x is corrupted with noise and the objective of the network is to reconstruct the original un- corrupted input x from the corrupted ˜x. Bengio et al. (2013) show that denoising autoencoders implicitly estimate the data distribution as the asymptotic distribution of the Markov chain that alternates between corruption and denoising. This interpretation provides a solid probabilistic foun- dation for them. Consequently, the denoising teaching criterion enables learning of over-complete representations, a property which is crucial for adding lateral connections to an autoencoder.  3  Accepted as a workshop contribution at ICLR 2015  As with normal feedforward networks, there are various options for choosing the cost function but, in the case of continuous input variables, a simple choice is  C = (cid:107)ˆx − x(cid:107)2 = (cid:107)g(f (˜x)) − x(cid:107)2 .  (1)  Denoising autoencoders can be used to build deep architectures either by stacking several on top of each other and training them in a greedy layer-wise manner (e.g., Bengio et al., 2007) or by chaining several encoding and decoding functions and training all layers simultaneously. For L layers and encoding functions f (l), the encoding path would compose as f = f (L) ◦ f (L−1) ◦ . . . f (1). We denote the intermediate feature vectors by h(l) and corresponding decoded, denoised, vectors by ˆh(l). Figure 1a depicts such a structure for L = 2. Encoding functions are of the form  starting from h(0) = ˜x. The corresponding decoding functions are  h(l) = f (l)(h(l−1)) = φ(cid:0)W(l) ˆh(l) = g(l)(ˆh(l+1)) = φ(cid:0)W(l)  ˆh(L) = h(L)  f h(l−1) + b(l)  f  ˆh(l+1) + b(l) g  g  (cid:1),  (cid:1),  1 ≤ l ≤ L,  1 ≤ l ≤ L − 1,  (2)  (3) (4) (5)  ˆx = g(0)(ˆh(1)) = W(0)  g  ˆh(1) + b(0) g .  Function φ is the activation function and typically left out from the lowest layer.  2 EXPERIMENTS  The tendency of regular autoencoders to preserve information seems to be at odds with the de- velopment of invariant features which relies on poolings that selectively discard some types of information. Our goal here is to investigate the hypothesis that suitable lateral connections allow autoencoders to discard details from higher layers and only retain abstract invariant features because the decoding functions g(l) can recover the discarded details from the encoder. We compare three different denoising autoencoder structures: basic denoising autoencoder and two variants with lateral connections. We experimented with various model deﬁnitions prior to deciding the ones deﬁned in Section 2.1 because there are multiple ways to merge lateral connections.  2.1 MODELS WITH LATERAL CONNECTIONS  We add lateral connections from h(l) to ˆh(l) as seen in Figure 1b. Autoencoders trained without noise would short-circuit the input and output with an identity mapping. Now that input contains noise, there is pressure to ﬁnd meaningful higher-level representations that capture regularities and allow for denoising. Note that the encoding function f has the same form as before in Eq. (2).  2.1.1 ADDITIVE LATERAL CONNECTIONS  As the ﬁrst version, we replace the decoding functions in Eq. (3–4) with  ˆh(L) = g(L)(h(L)) = (h(L) + b(L) ˆh(l) = g(l)(h(l), ˆh(l+1)) = (h(l) + b(l)  a ) (cid:12) σ(cid:0)a(L) (cid:12) h(L) + b(L) (cid:1) , a ) (cid:12) σ(cid:0)a(l) (cid:12) h(l) + b(l)  b  b  (cid:1) + φ(cid:0)W(l)  g  (cid:1) ,  (6) (7)  ˆh(l+1) + b(l) g  where (cid:12) is the element-wise (i.e., Hadamard) product, a(l), b(l) b are learnable parameter vectors along with weights and biases, and σ is a sigmoid function to ensure that the modulation stays within reasonable bounds. Function g(0) stays afﬁne as in Eq. (5). The functional form of Eq. (6), with element-wise decoding, is motivated by element-wise denoising functions that are used in denoising source separation (S¨arel¨a & Valpola, 2005) and corresponds to assuming the elements of h independent a priori.  a , and b(l)  4  Accepted as a workshop contribution at ICLR 2015  2.1.2 MODULATED LATERAL CONNECTIONS  Our hypothesis is that an autoencoder can learn invariances efﬁciently only if its decoder can make good use of them. Valpola (2015) proposed connecting the top-down mapping inside the sigmoid term of Eq. (7), a choice motivated by optimal denoising in hierarchical variance models. Our ﬁnal proposed model includes the encoding functions in Eq. (2), the top connection g(L) in Eq. (6), bottom decoding function g(0) as in Eq. (5), but the middle decoding functions are deﬁned as  a ) (cid:12) σ(cid:0)a(l) (cid:12) h(l) + W(l)  g  (cid:1),  ˆh(l+1) + b(l) b  1 ≤ l ≤ L − 1.  (8)  g(l)(h(l), ˆh(l+1)) = (h(l) + b(l)  g has dropped because it is redundant with the bias term b(l) b  In contrast to additive lateral connection in Eq. (7), the signal from the abstract layer ˆh(l+1) is used to modulate the lateral connection so that the top-down connection has moved from φ(·) to σ(·), and the bias b(l) Modulated (also known as gated or three-way) connections have been used in autoencoders before but in a rather different context. Memisevic (2011) uses a weight tensor to connect two inputs to a feature. We connect the ith component of h(l) only to the ith component of ˆh(l), keeping the number of additional parameters small.  in σ(·).  2.2 TRAINING PROCEDURE  f  T  g = W(l+1)  In order to compare the models, we optimized each model structure constraining to 1 million pa- rameters and 1 million mini-batch updates to ﬁnd the best denoising performance, that is, the lowest reconstruction cost C in Eq. (1). The better the denoising performs the better the implicit probabilis- tic model is. The tasks is the same for all models, so the comparison is fair. With two-layer models (L = 2) the focus was to ﬁnd the optimal size for layers, especially the ratio of h(2) size to h(1) size (see Fig. 1c for the deﬁnition of the ratio α). All the models use rectiﬁed linear unit as the activation function φ(·) and the noise was Gaussian with zero mean and standard deviation scaled to be 50% of that of the data. In order to ﬁnd the best possible baseline for comparison, we evaluated weight tying for autoencoder without lateral connections , that is, W(l) , and noticed that tying weights resulted in faster convergence and thus better denoising performance than without weight tying. However, when combining both so that weights are tied in the beginning and untied for the latter half of the training, denoising performance improved slightly (by 1%), but did not affect the relative difference of various models. Since weight tying had only negligible impact on the results, but it complicates training and reproducibility, we designed the experiments so that all models use tied weights counting tied weights as separate parameters. Results of the denoising performance are described in Section 2.3. We performed the analysis on natural images because the invariances and learned features are easy to visualize, we know beforehand that such invariances do exist, and because computer vision is an important application ﬁeld in its own right. We used 16 × 16 patches from two image datasets: CIFAR-10 (Krizhevsky & Hinton, 2009) and natural images used by Olshausen & Field (1996)1. We refer to this dataset as O&F. The training length was limited to 1 million mini-batch updates with mini-batch of size 50 and learn- ing rate was adapted with ADADELTA (Zeiler, 2012). The best variants of each model were then trained longer, for 4 million updates, and further analyzed to determine invariance of the learned rep- resentations. This is described and reported in Section 2.4. Supplementary material provides more details about data preprocessing, division between training and validation sets, training procedure, hyperparameters used with ADADELTA, and how weights were initialized. We also tried stacked layer-wise training by training each layer for 500,000 updates followed by ﬁne- tuning phase of another 500,000 updates such that the total number of updates for each parameter equals to 1 million. We also tried a local cost function and noise source on each layer or using the global cost, but we did not ﬁnd any such stacked training beneﬁcial compared to the direct simultaneous training of all layers, which is what we report in this paper.  1Available at https://redwood.berkeley.edu/bruno/sparsenet/  5  Accepted as a workshop contribution at ICLR 2015  Figure 2: The best validation cost per element as a function of α, the ratio of h(2) size to h(1) size, for CIFAR-10 (left) and O&F (right) datasets. Dotted line is the result of linear denoising (model Linear in Table 1), two dashed lines represent the denoising performance of one-layer models (L = 1) with and without lateral connections according to their colors. Note that Add and Mod models are identical when L = 1. The scale of the horizontal axis is linear until 0.5 and logarithmic after that.  Figure 3: Translation invariance measure of neurons on h(2) as a function of signiﬁcance. Color indicates the average sign of the connected weights from negative (blue) to positive (red). Best viewed in color. See Section 2.4 for more details.  2.3 DENOISING PERFORMANCE  The results of denoising performance for models with one and two layers is presented in the Figure 2 which shows the lowest reconstruction cost on a validation dataset for both datasets. Each conﬁgu- ration was trained 5 times with different random initialization for conﬁdence bounds calculated as corrected sample standard deviation of the lowest reconstruction cost. The best performing No-lat model (autoencoder without lateral connections) is a one-layer model and is shown in dashed line. The best two-layer No-lat models in CIFAR-10 and O&F have ratios of αmin = 1.0 and αmin = 1.4, respectively. Since No-lat autoencoders need to push all the information through each layer, it is intuitive that very narrow bottlenecks are bad for the model, that is, large and small ratios perform poorly. A further study of why the optimal ratio is larger for O&F revealed that the lower layer can be smaller because the effective dimensionality in terms of principal components is lower for O&F compared to CIFAR-10. Second layer is not beneﬁcial with No-lat model given the parameter number constraint. Mod (modulated lateral connections) model beneﬁts from the second layer and works best when the ratio is small, namely αmin = 0.03 and αmin = 0.12 for CIFAR-10 and O&F, respectively. The second layer does not hurt or beneﬁt Add (additive lateral connection) model signiﬁcantly and its performance is between No-lat and Mod models. The results are also presented as numbers in Table 1 in supplementary material.  6  0.00.10.20.30.40.51.02.03.0α0.160.180.20CostModNo-latAdd0.00.10.20.30.40.51.02.03.0α0.1150.1200.125CostModNo-latAdd10−210−11000.00.20.40.60.81.0γ(2)iMod256-1622-5010−210−1100Add256-839-33610−210−1100No-lat256-590-589Accepted as a workshop contribution at ICLR 2015  2.4 RESULTING INVARIANT FEATURES  Practically no prior information about poolings was incorporated in either the model structure or treatment of training data. This means that any invariances learned by the model must have been present in the higher-order correlations of the inputs, as explained in Section 1.1. It is well known that such invariances can be developed from natural images (e.g., Hyv¨arinen & Hoyer, 2000) but the question is, how well are the different model structures able to represent and learn these features. To test this, we generated sets of rotated, scaled and translated images and measured how invariant the activations h(l) are for each type of transformation separately. As an example for translation, each set contained 16 translated images (from a 4× 4 grid). For each set s in a given transformation type, we calculated the mean activation (cid:104)h(l)(cid:105)s and compared their variances with the variance var{h(l)} over all samples:  i = var{(cid:104)h(l) γ(l)  i (cid:105)s}/var{h(l)  i } .  (9) i ≤ 1. If the feature is completely invariant with respect From the deﬁnition it follows that 0 ≤ γ(l) to the transformation, γ(l) equals one2. As the overall conclusions are similar to all tested transfor- i mations, the main text concentrates on the translation invariance. Results for other invariances are reported in supplementary material, Section 4.4. The average layer-wise invariance, γ(l) = (cid:104)γ(l) i (cid:105)i, grows towards the higher layers in all models but much faster in the best Mod models than in others3, i.e. for CIFAR-10 the best Mod model has γ(0)...(2) = (0.20, 0.31, 0.84), whereas for the best No-lat γ(0)...(2) = (0.20, 0.23, 0.30). All γ(l) values are reported in Table 1 in supplementary material.  To further illustrate this, we plotted in Figure 3 the invariance measures γ(2) for the best variant of each model. In each plot, dots correspond to hidden neurons h(2) and the color reﬂects the average sign of the encoder connection to that neuron (blue for negative, red for positive). The horizontal axis is the signiﬁcance of a neuron, a measure of how much the model uses that hidden neuron, which is deﬁned and analyzed in supplementary material, Section 4.3.  i  i  i  There are two notable observations. First, all h(2) neurons of Mod model are highly invariant, whereas the other models have only few invariant neurons and the vast majority of neurons have very low invariance. For No-lat model, invariance seems to be even smaller for those neurons that the model uses more. Moreover, we tested that the second layer of Mod model stays highly invariant even if the layer size is increased (shown in Figure 7c in supplementary material). Second, invariant neurons tend to have far more and stronger negative than positive weights, especially so with Mod and No-lat models. Since the nonlinearity φ(x) on each layer was the rectiﬁed linear unit, a convex function which saturates on the negative side, negative tied weights mean that the network ﬂipped these functions into −φ(−x), that is, concave functions that saturate on the positive side resembling OR operation. This interpretation is further discussed in supplementary material, Section 4.3.2.  2.5 LEARNED POOLINGS  The modulated Mod model used practically all of the second layer neurons for pooling. When study- ing the poolings, we found that typically every Layer 1 neuron participates in several qualitatively different poolings. As can be seen from the Figure 4, each Layer 1 neuron (shown on the left-most column) participates in different kinds of poolings each of which is sensitive to a particular set of features and invariant to other types. For example, Layer 1 neuron (c) is selective to orientation, frequency and color but it participates in three different Layer 2 poolings . The ﬁrst one is selective to color but invariant to orientation. The second one is selective to orientation but invariant to color. The third one only responds to high frequency and orientation. More details of the analysis are available in supplementary material.  2Various measures of invariance have been proposed (e.g., Goodfellow et al., 2009). Our invariance measure γ is closely related to autocorrelation which has been used to measure invariance, e.g., by Dosovitskiy et al. (2014). The set of translated patches includes pairs with various translations and therefore our measure is a weighted average of the autocorrelation function. The wider the autocorrelation, the larger the measure γ.  3Our measure γ can be fooled by copying the same invariant feature to many hidden neurons but we veriﬁed that this is not happening here: slow feature analysis is robust against such redundancy but yields qualitatively the same results for the tested networks.  7  Accepted as a workshop contribution at ICLR 2015  (a)  (b)  (c)  (d)  a . . . h(1)  Figure 4: Various pooling groups a neuron. Each set (a)-(d) represents three relevant pooling groups the selected neuron h(1) d , depicted in the ﬁrst column, belongs to. Each row represents an h(2) pooling neuron by showing the 20 most relevant h(1) neurons associated with it. Poolings were found by selecting a few Layer 1 neurons and following their strongest links to Layer 2 to identify the poolings in which they participate. Consecutively, the Layer 1 neurons corresponding to each pooling group were identiﬁed by walking back the strongest links from each Layer 2 neuron. Best viewed in color. The procedure of choosing features in the plot is also depicted in Figure 8 in supplementary material.  3 DISCUSSION  The experiments showed that invariance in denoising autoencoders increased towards the higher lay- ers but signiﬁcantly so only if the decoder had a suitable structure where details could be combined with invariant features via multiplicative interactions. Lateral connections from encoder to decoder allowed the models to discard details from the higher levels but only if the details and invariant features were combined suitably. The best models with modulated lateral connections were able to learn a large number of poolings in an unsupervised manner. We tested their invariance but nothing in the model biased learning in that direction and we observed invariance and selective discrimination of several different dimensions such as color, orientation and frequency. In summary, these ﬁndings are fully in line with the earlier proposition that the unsupervised denois- ing autoencoder with modulated lateral connections can work in tandem with supervised learning because, as we have shown here for the ﬁrst time, the higher layers of the model have the ability to focus on abstract representations and, unlike regular autoencoders, should therefore be able to discard details if supervised learning deems them irrelevant. It now becomes possible to combine autoencoders with the popular supervised learning pipelines which include in-built pooling opera- tions (see, e.g., Schmidhuber, 2015). There are multiple ways to extend the work, including 1) explicit bias towards invariances; 2) sparse structure such as convolutional networks, making much larger scale models and deeper hierarchies feasible; 3) dynamic models; and 4) semi-supervised learning.  REFERENCES Bengio, Y, Lamblin, P, Popovici, D, and Larochelle, H. Greedy layer-wise training of deep networks.  In Advances in Neural Information Processing Systems (NIPS 2006), 2007.  8  Accepted as a workshop contribution at ICLR 2015  Bengio, Y. Deep learning of representations: Looking forward. In Statistical Language and Speech  Processing, pp. 1–37. Springer, 2013.  Bengio, Y, Yao, L, Alain, G, and Vincent, P. Generalized denoising auto-encoders as generative  models. In Advances in Neural Information Processing Systems, pp. 899–907, 2013.  Ciresan, D. C, Meier, U, Gambardella, L. M, and Schmidhuber, J. Deep big simple neural nets for  handwritten digit recogntion. Neural Computation, 22(12):3207–3220, 2010.  Dosovitskiy, A, Springenberg, J. T, Riedmiller, M, and Brox, T. Discriminative unsupervised feature learning with convolutional neural networks. In Ghahramani, Z, Welling, M, Cortes, C, Lawrence, N, and Weinberger, K (eds.), Advances in Neural Information Processing Systems (NIPS 2014), pp. 766–774. 2014.  F¨oldi´ak, P. Learning invariance from transformation sequences. Neural Computation, 3:194–200,  1991.  Fukushima, K. Neural network model for a mechanism of pattern recognition unaffected by shift in  position - Neocognitron. Trans. IECE, J62-A(10):658–665, 1979.  Goodfellow, I, Lee, H, Le, Q. V, Saxe, A, and Ng, A. Y. Measuring invariances in deep networks.  In Advances in Neural Information Processing Systems (NIPS), pp. 646–654, 2009.  Hubel, D. H and Wiesel, T. N. Receptive ﬁelds, binocular interaction and functional architecture in  the cat’s visual cortex. The Journal of physiology, 160(1):106, 1962.  Hyv¨arinen, A and Hoyer, P. Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces. Neural computation, 12(7):1705–1720, 2000. Kingma, D. P, Mohamed, S, Rezende, D. J, and Welling, M. Semi-supervised learning with deep generative models. In Ghahramani, Z, Welling, M, Cortes, C, Lawrence, N, and Weinberger, K (eds.), Advances in Neural Information Processing Systems (NIPS 2014), pp. 3581–3589. Curran Associates, Inc., 2014.  Krizhevsky, A and Hinton, G. Learning multiple layers of features from tiny images. Technical  report, University of Toronto, 2009.  Krizhevsky, A, Sutskever, I, and Hinton, G. E.  ImageNet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS 2012), pp. 1106– 1114, 2012.  Memisevic, R. Gradient-based learning of higher-order image features. In 2011 IEEE International  Conference on Computer Vision (ICCV), pp. 1591–1598, November 2011.  Olshausen, B. A and Field, D. J. Emergence of simple-cell receptive ﬁeld properties by learning a  sparse code for natural images. Nature, 381:607–609, 1996.  Raiko, T, Valpola, H, and LeCun, Y. Deep learning made easier by linear transformations in percep- trons. In Lawrence, N. D and Girolami, M (eds.), AISTATS, volume 22 of JMLR Proceedings, pp. 924–932. JMLR.org, 2012.  Riesenhuber, M and Poggio, T. Hierarchical models of object recognition in cortex. Nature Neuro-  science, 2(11):1019–1025, 1999.  S¨arel¨a, J and Valpola, H. Denoising source separation. Journal of Machine Learning Research, 6:  233–272, 2005.  Schmidhuber, J. Deep learning in neural networks: an overview. Neural Networks, 61:85–117,  2015.  Valpola, H. From neural PCA to deep unsupervised learning. In Advances in Independent Compo-  nent Analysis and Learning Machines. Elsevier, 2015. Preprint available as arXiv:1411.7783.  Vincent, P, Larochelle, H, Bengio, Y, and Manzagol, P.-A. Extracting and composing robust features  with denoising autoencoders. Technical Report 1316, Universit´e de Montr´eal, dept. IRO, 2008. Zeiler, M. D. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.  9  Accepted as a workshop contribution at ICLR 2015  Table 1: Denoising performance and translation invariance measure of selected models. All models have exactly the same input layer and data so the average invariance is the same for all models, e.g. for CIFAR-10, γ(0) = 0.20.  Dataset Model CIFAR-10 Linear CIFAR-10 No-lat CIFAR-10 Add/Mod CIFAR-10 No-lat CIFAR-10 Add CIFAR-10 Mod O&F O&F O&F O&F O&F O&F  Linear No-lat Add/Mod No-lat Add Mod  Layer sizes 256 256-1948 256-1937 256-590-589 256-839-336 256-1622-50 256 256-1948 256-1937 256-512-718 256-1061-212 256-1395-100  4 SUPPLEMENTARY MATERIAL  0.00 0.00 1.00 0.40 0.03  0.00 0.00 1.40 0.20 0.07  Ratio α Min cost ± std  0.20316 ± 0.00011 NA 0.16144 ± 0.00007 0.15793 ± 0.00007 0.17595 ± 0.00034 0.15734 ± 0.00015 0.15086 ± 0.00022 0.14686 ± 0.00010 NA 0.12290 ± 0.00008 0.11891 ± 0.00006 0.12446 ± 0.00015 0.11723 ± 0.00009 0.11234 ± 0.00005  Invariance, γ(l)  0.20, 0.29 0.20, 0.29 0.20, 0.23, 0.30 0.20, 0.27, 0.33 0.20, 0.31, 0.84  0.16, 0.24 0.16, 0.25 0.16, 0.19, 0.27 0.16, 0.25, 0.41 0.16, 0.24, 0.95  4.1 PREPROCESSING OF DATA Patches of size 16 × 16 were sampled randomly and continuously during training. Separate test images were put aside for testing generalization performance: the last 10,000 samples for CIFAR- 10 and the sixth image of O&F dataset. Continuous sampling allows generation of millions of data samples alleviating overﬁtting problems. O&F dataset was already (partially) whitened so no further preprocessing was applied to it. RGB color patches of CIFAR-10 were whitened and dimensionality was reduced to 256 with PCA to match the dimensionality of grayscale images of O&F. Despite dimensionality reduction, 99% of the variance was retained.  4.2 DETAILS OF THE TRAINING PROCEDURE  White additive Gaussian noise of σN = 0.5 was used for corrupting the inputs which were scaled to have standard deviation of σ = 1.0. During training, ADADELTA was used to adapt the learning rate with its momentum set to 0.99, and (cid:15) = 10−8. All weight vectors were initialized from normal distribution to have a norm of 1.0, and orthogonalized. In order to improve the convergence speed of all models, we centered the hidden unit activations following Raiko et al. (2012): there is an auxiliary bias term β (Raiko et al., 2012, Eq. (2)), applied immediately after the nonlinearity, that centers the output to zero mean.4  4.3 ANALYSIS OF MAPPINGS  We analyzed the mappings learned by different types of models in several ways and present here some of the most interesting ﬁndings. First, it turned out that different models had very different proportions of invariant neurons on h(2) (invariance measure γ is deﬁned in Eq. (9)) and we wanted to understand better what was going on. Some key questions were how important roles different types of features had and how the invariances were formed. Second level invariances could be low-frequency features which are invariant already on the ﬁrst layer (and thus not particularly interesting) or formed through pooling Layer 1 neurons. The ﬁrst question can be answered by looking at where the connections are coming from. The connections W(l) g are visualized in Figures 5a–5c. The neurons on each layer have been ordered with respect to invariance which increases from left to right. The connecting edges have been colored according to the sign of the connecting weight and the strength of each edge reﬂects the signiﬁcance of the connection. Signiﬁcance is deﬁned as the proportion of variance that the higher-level neuron  4We did not use the other transformation α since it would have required shortcut connections.  10  Accepted as a workshop contribution at ICLR 2015  generates on the lower-level neurons. We initially tried visualizing simply the magnitudes of the weights but the problem is that when an input neuron has a low variance or the output neuron is saturated, a large weight magnitude does not indicate that the connection is important; it would not make much difference if such a connection were removed.  i  4.3.1 SIGNIFICANCE MEASURE When visualizing W(l) g , we ﬁrst took the squares and scaled them by the input neuron’s variance }. Assuming input neurons are independent, this quantity reﬂects the input variance each var{h(l+1) neuron h(l) receives. Depending on the saturation of the neuron, a smaller or greater proportion j j }. We therefore scaled all the of this variance is transmitted to the actual output variance var{h(l) j }. We incoming variances for each h(l) j named this quantity the signiﬁcance of the connection and it approximately measures where the output variance of each layer originates from. This signiﬁcance is also depicted in Figure 3 where the x coordinate is the sum of output signiﬁcances for each h(2)  such that their sum matches the output variance var{h(l)  .  i  4.3.2 ROLE OF NEGATIVE WEIGHTS  It turned out that the invariant neurons tend to have far more and stronger negative than positive weights. We visualized this with color in Figure 5: blue signiﬁes negative and red positive weights. In the images, the connections are translucent which means that equal number (and signiﬁcance) of positive and negative weights results in purple color. A striking feature of these plots is that the most invariant features tend to have all negative weights. Since the nonlinearity φ(x) on each layer was the rectiﬁed linear unit, a convex function which saturates on the negative side, negative tied weights mean that the network ﬂipped these functions into −φ(−x), that is, concave functions that saturate on the positive side. It therefore looks like the network learned to add a layer of concave OR-like invariance-generating poolings on top of convex AND-like coincidence detectors. Forming convex AND using positive weights and concave OR using negative weights is demonstrated respectively as truth tables in Table 2 and Table 3, and the geometric form is illustrated in Figure 6a and Figure 6b.  4.4  INVARIANCE  We also studied invariance of the second layer neurons for scaling and rotation transformations using the same invariance measure as deﬁned in Section 2.4 for translation. We formed sets of 16 samples by scaling CIFAR-10 images with a zoom factors 0.6, 0.65, . . . , 1.35 and rotating images −32◦,−28◦, . . . , 28◦ for scaling and rotation invariance experiments, respectively. The results are shown in Figure 7a and Figure 7b and are similar to the translation invariance in Figure 3. Figure 7c illustrates the impact of increasing α, i.e. increasing the size of the second layer. Notably all second layer neurons stay highly invariant even when the layer size is increased.  11  Accepted as a workshop contribution at ICLR 2015  (a) Connections of Mod model 256-1622-50  (b) Connections of Add model 256-839-336.  (c) Connections of No-lat model 256-590-589.  Figure 5: Neurons are ordered according to increasing translation invariance from left to right. Blue denotes negative and red positive weights in W(l) g . Strength of the connections depends on the signiﬁcance of the connection (see text for details). Layer 2 neurons of this model are also visualized in Figure 3. Best viewed in color.  12  Accepted as a workshop contribution at ICLR 2015  Table 2: Truth table for logical AND operation given three-element binary input vector x and a single perceptron with ReLU activation function φ(·).  φ(z) = φ(xT w + b) = φ(x0w0 + x1w1 + x2w2 + b)  φ(z) AND  x0 0 0 0 0 1 1 1 1  x1 0 0 1 1 0 0 1 1  x2 wi 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1  b -2 -2 -2 -2 -2 -2 -2 -2  z -2 -1 -1 0 -1 0 0 1  0 0 0 0 0 0 0 1  0 0 0 0 0 0 0 1  Table 3: Truth table for logical OR operation given three-element binary input vector x and a single perceptron with ReLU activation function φ(·).  φ(z)  φ(z) = φ(xT w + b) = φ(x0w0 + x1w1 + x2w2 + b) x1 0 0 1 1 0 0 1 1  1 − φ(z) NOR OR 0 1 1 1 1 1 1 1  x2 wi 0 -1 -1 1 -1 0 -1 1 -1 0 -1 1 -1 0 1 -1  z 1 0 0 -1 0 -1 -1 -2  b 1 1 1 1 1 1 1 1  1 0 0 0 0 0 0 0  0 1 1 1 1 1 1 1  1 0 0 0 0 0 0 0  x0 0 0 0 0 1 1 1 1  (a) Function φ(xT w + b), wi = 1, b = −2  (b) Function 1 − φ(xT w + b), wi = −1, b = 1  Figure 6: Conceptual illustration of linear rectiﬁer unit, φ(·), performing logical AND and OR operations for three-element binary input x, xi ∈ {0, 1} after the afﬁne transform z = xT w + b = x0w0 + x1w1 + x2w2 + b. AND operations has positive weights and convex form, whereas OR operation has negative weights and concave functional form.  13  0123Numberofactiveinputs012ConvexANDfunction0123Numberofactiveinputs012ConcaveORfunctionAccepted as a workshop contribution at ICLR 2015  (a) Rotation invariance  (b) Scaling invariance  (c) Translation invariance for Mod models with larger α  Figure 7: Various invariance measures for neurons of h(2) as a function of signiﬁcance for CIFAR- 10. Color indicates the average sign of the connected weights from negative (blue) to positive (red). Best viewed in color. See Section 2.4 for more details.  14  10−210−11000.00.20.40.60.81.0γ(2)iMod256-1622-5010−210−1100Add256-839-33610−210−1100No-lat256-590-58910−210−11000.00.20.40.60.81.0γ(2)iMod256-1622-5010−210−1100Add256-839-33610−210−1100No-lat256-590-58910−310−210−11000.00.20.40.60.81.0γ(2)iMod256-1622-5010−310−210−1100Mod256-1500-7510−310−210−1100Mod256-1395-100Accepted as a workshop contribution at ICLR 2015  Figure 8: Method for selecting pooling groups including a given h(1) neuron contains two phases. First, follow the strongest links up to the second layer to identify 3 pooling groups the ﬁrst-layer neuron belongs to (left). Second, visualize each pooling group by identifying the Layer 1 neurons that have the strongest links back from each Layer 2 neuron (right). In this example, the ﬁrst pooling group contains neurons marked with green, the second pooling group with red, and the third consist of purple neurons. Colors correspond to rows in Figure 4 and both phases were performed for each set (a)-(d).  15  ",
1412.5068,2015, Towards Deep Neural Network Architectures Robust to Adversarial Examples,"['Towards Deep Neural Network Architectures Robust to Adversarial Examples', 'Shixiang Gu and Luca Rigazio']",https://arxiv.org/pdf/1412.5068,"5 1 0 2    r p A 9         ]  G L . s c [      4 v 8 6 0 5  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  TOWARDS DEEP NEURAL NETWORK ARCHITECTURES ROBUST TO ADVERSARIAL EXAMPLES  Shixiang Gu Panasonic Silicon Valley Laboratory Panasonic R&D Company of America shane.gu@us.panasonic.com  Luca Rigazio Panasonic Silicon Valley Laboratory Panasonic R&D Company of America luca.rigazio@us.panasonic.com  ABSTRACT  Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100% mis-classiﬁcation for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We ﬁnd that DAEs can remove substantial amounts of the adversarial noise. How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial exam- ples, without a signiﬁcant performance penalty.  1  INTRODUCTION  Deep neural networks have recently led to signiﬁcant improvement in countless areas of machine learning, from speech recognition to computer vision Krizhevsky et al. (2012); Dahl et al. (2012); Taigman et al. (2013); Zhang et al. (2013). DNNs achieve high performance because deep cascades of nonlinear units allow to generalize non-locally, in data-speciﬁc manifolds Bengio (2009). While this ability to automatically learn non-local generalization priors from data is a strength of DNNs, it also creates counter-intuitive properties. In particular Szegedy et al. (2014b) showed in their sem- inal paper that one can engineer small perturbations to the input data, called adversarial examples, that make an otherwise high-performing DNN misclassify every example. For image datasets, such perturbations are often imperceptible to the human eye, thus creating potential vulnerabilities when deploying neural networks in real environments. As an example, one could envision situations where an attacker having knowledge of the DNN parameters could use adversarial examples to attack the system and make it fail consistently. Even worse, due to the cross-model, cross-dataset general- ization properties of the adversarial examples Szegedy et al. (2014b) , the attacker might generate adversarial examples from independent models without full knowledge of the system and still be able to conduct a highly successful attack. This indicates there is still a signiﬁcant robustness gap between machine and human perception, despite recent results showing machine vision performance closing in on human performance Taigman et al. (2013). More formally, the challenge is: can we design and train a deep network that not only generalizes in abstract manifold space to achieve good recognition accuracy, but also retains local generalization in the input space?  1  Accepted as a workshop contribution at ICLR 2015  A main result from Szegedy et al. (2014b) is that the smoothness assumption that underlies many kernel methods such as Support Vector Machines (SVMs) does not hold for deep neural networks trained through backpropagation. This points to a possible inherent instability in all deterministic, feed-forward neural network architectures. In practice, SVMs can be used to replace the ﬁnal soft- max layer in classiﬁer neural networks leading to better generalization Tang (2013), but applying SVM in the manifold space does not guarantee local generalization in the input space. Recently, Duvenand et al. (2014) categorize distributions of deep neural networks through deep Gaussian Pro- cess (GP) and show that in stacked architectures, the capacity of the network captures fewer degrees of freedom as the layers increase. They propose to circumvent this by connecting inputs to every layer of the network. Without this trick, the input locality is hardly preserved in higher layers due to the complexity of nonlinear mapping cascades.  A framework leveraging both approaches is Random Recursive SVM (R2SVM) Vinyals et al. (2012), which recursively solves a SVM whose input combines input data and outputs from the previous SVM layer, randomly projected to the same dimension as the input data. R2SVM avoids solving nonconvex optimization by recursively solving a SVM and demonstrates generalization on small datasets. However, performance is suboptimal compared to state-of-the-art DNNs, possibly due to lack of end-to-end training Vinyals et al. (2012); Tang (2013). Another work inspired by the recursive nature of the human perceptual system is Deep Attention Selective Network (das- Net) Stollenga et al. (2014), which dynamically ﬁne-tunes the weight of each convolutional ﬁlter at recognition time. We speculate that the robustness of human perception is due to complex hierar- chies and recursions in the wirings of the human brain Felleman & Van Essen (1991); Douglas et al. (1995), since recursions provide multiple paths to input data and could retain locality information at multiple levels of representation. Such an intuition is also partially supported by the recent state-of- the-art models for object classiﬁcation and detection involving multi-scale processing Szegedy et al. (2014a). Since modeling such recursions in DNNs is notoriously hard and often relies on additional techniques such as reinforcement learning Stollenga et al. (2014); Mnih et al. (2014), we will at ﬁrst investigate explicit inclusion of input generalization as an additional objective for the standard DNN training process.  It is important to note that the adversarial examples are universal and unavoidable by their deﬁnition: one could always engineer an additive noise at input to make the model misclassify an example, and it is also a problem in shallow models such as logistic regression Szegedy et al. (2014b). The question is how much noise is needed to make the model misclassify an otherwise correct example. Thus, solving the adversarial examples problem is equivalent to increasing the noticeability of the smallest adversarial noise for each example.  In this paper we investigate new training procedures such that the adversarial examples gener- ated based on Szegedy et al. (2014b) have higher distortion, where distortion is measured by 1 i − xi)2 where x′, x ∈ Rn are the adversarial data and original data respectively. First, n P(x′ we investigate the structure of the adversarial examples, and show that contrary to their small dis- tortion it is difﬁcult to recover classiﬁcation performance through additional perturbations, such as Gaussian additive noises and Gaussian blur. This suggests the size of “blind-spots” are in fact relatively large, in input space volume, and locally continuous. We also show that adversarial ex- amples are quite similar Szegedy et al. (2014b), and an autoencoder (AE) trained to denoise adver- sarial examples from one network generalizes well to denoise adversarials generated from different architectures. However, we also found that the AE and the classiﬁer DNN can be stacked and the resulting network can again be attacked by creating new, adversarial examples of even smaller distortion. Because of this, we conclude that ideal architectures should be trained end-to-end and incorporate input invariance with respect to the ﬁnal network output. We ﬁnd that ideas from denois- ing autoencoder (DAE), contractive autoencoder (CAE), and most recently marginalized denoising autoencoder (mDAE) provide strong framework for training neural networks that are robust against adversarial noises Rifai et al. (2011b); Alain & Bengio (2012); Chen et al. (2014). We propose Deep Contractive Networks (DCNs), which incorporate a layer-wise contractive penalty, and show that ad- versarials generated from such networks have signiﬁcantly higher distortion. We believe our initial results could serve as the basis for training more robust neural networks that can only be misdirected by a substantial noise, in a way that is more attuned to how human perception performs.  2  Accepted as a workshop contribution at ICLR 2015  Table 1: Model architectures, datasets, and baseline error-rates  Model Name Dataset Description Train err. Test err. Adv. err. Av. distortion N100-100-10 MNIST ReLU net N200-200-10 MNIST ReLU net AE400-10 ConvNet  MNIST AE net MNIST Conv. net  1.77% 1.65% 2.0% 0.90%  99.9% 99.9% 99.6% 100%  0.084 0.087 0.098 0.095  0% 0% 1.0% 0%  2 FRAMEWORK  2.1 GENERATING ADVERSARIAL EXAMPLES  We follow the procedure outlined in Szegedy et al. (2014b) for generating adversarial examples from the classiﬁer neural network, with a slight modiﬁcation for computational efﬁciency. Using the same notation, we denote the classiﬁer by f : Rm −→ {1...k} and its associated continuous loss function by L : Rm × {1...k} −→ R+. Then, for a given image x ∈ Rm, whose m pixels normalized to [0, 1], and target label l ∈ {1...k}, the minimal adversarial noise r is approximated by optimizing the following problem, given c > 0 and subject to x + r ∈ [0, 1]m:  minr c|r|2 + L(x + r, l)  (1) Instead of ﬁnding the minimum c through line-search per example, we use constant c during eval- uation of a given dataset and model architecture. We ﬁnd c such that for a sufﬁciently large subset of data of size n, Pn−1 i=0 k ri k2 is minimized, subject to the constraint that the mean prediction error rate of f (xi + ri) is greater than e. e is chosen as 99% throughout the experiments. Since we are interested in macro-scale evaluation of adversarial examples per dataset and model, we ﬁnd this setting sufﬁciently simpliﬁes and speeds up the procedure while allowing quantitative analysis of the results.  2.2 DATASETS AND MODEL ARCHITECTURES  We perform our experiements on the MNIST dataset, using a number of architectures LeCun & Cortes (1998); Krizhevsky & Hinton (2009). Table 1 summarizes experimental settings, baseline error rate, adversarial examples’ error rate and average adversarial distortion. L2 weight decay is applied with λ = 10−3, except in convolutional layers. For MNIST ConvNet has two convolutional layers, one fully-connected layer, and one softmax layer.  3 RECOVERING FROM ADVERSARIAL EXAMPLES  In order to gain insight into the properties of adversarial noises, we explore three pre-processing methods aiming at recovering from adversarial noise, as presented in the following sections.  3.1 NOISE INJECTION  Given the tiny nature of adversarial noise, we investigate a recovery strategy based on additional corruptions, in the hope that we can move the input outside the network “blind-spots”, which we initially assumed to be small and localized. We experiment with additive Gaussian noise and Gaus- sian blurring. For Gaussian noise we averaged predictions over 20 feed-forward runs to reduce the prediction variance. Results for Gaussian additive noises are summarized in Figure 1. It shows the and the trade-off between the adversarial examples recovered and the clean examples misclassiﬁed as one varies the amount of additive noises, which are added to only input layer or the input plus all the hidden layers. Results for Gaussian blurring are summarized in Table 2. For Gaussian additive noises, “L1” refers to the noise applied at input layer; “L*” refers to the noise applied at input layer plus all hidden layers. Gaussian blurring is only applied at input layer.  The results show that convolution seems to help with recovering from the adversarial examples. For example, for ConvNet model, applying Gaussian blur kernel of size 11 to all input data can recover more than 50% of adversarial examples, at the expense of 3% increase in the test error on  3  Accepted as a workshop contribution at ICLR 2015  Figure 1: Noise Injection (Gaussian additive noise): Test error on clean data (Left) and on adversarial data (Right) vs. standard deviation of Gaussian additive noise  Table 2: Noise Injection (Gaussian blurring): Test error on clean data (Left) and on adversarial data (Right) vs. blur kernel size  Blur Kernel Size N100-100-10 N200-200-10 AE400-10 ConvNet  - 1.8 1.6 2.0 0.9  5 2.6 2.5 3.2 1.2  11 11.3 14.8 16.6 4.0  - 99.9 99.9 99.6 100  5 43.5 47.0 68.3 53.8  11 62.8 65.5 78.8 43.8  clean data (Table 2). In addition, for ConvNet model, adding Gaussian noise of σ = 0.1 at input layer plus hidden layers allow the model to recover more than 35% at similar small loss in model performance on clean data (Figure 1). However, neither Gaussian additive noises or blurring is effective in removing enough noise such that its error on adversarial examples could match that of the error on clean data.  3.2 AUTOENCODER  To assess the structure of the adversarial noise, we trained a three-hidden-layer autoencoder (784- 256-128-256-784 neurons) on mapping adversarial examples back to the original data samples. An important detail is that we also train the model to map original training data back to itself, so that the autoencoder preserves the original data if the non-adversarial data samples are fed in; this allows us to stack several autoencoders. We train the autoencoder using adversarial examples from the training set only, and test generalization capabilities on adversarial examples from the test set across different model topologies. Table 3 shows generalization performance of autoencoders trained on adversarial examples from different models. Columns indicate whose adversarial data the autoencoder is trained on, rows indicate whose adversarial test data the autoencoder is used to denoise. Entries correspond to error rates when the outputs from the autoencoder is fed into the model identiﬁed by the row labels. We observe that autoencoders generalize very well on adversarial examples from different models. All autoencoders are able to recover at least 90% of adversarial errors, regardless of the model from which it originates.  While this is a very successful experiment, we found one drawback: the autoencoder and its cor- responding classiﬁer can be stacked to form a new feed-forward neural network, then adversarial examples can again generated from this stacked network. The last row in Table 3 and Figure 2 show such stacked network adversarial examples to have a signiﬁcantly smaller distortion than adversar- ial examples from the original classiﬁer network, suggesting that while the autoencoder effectively recovers the from the weaknesses of the original classiﬁer network, the stacked network is then even more susceptible to adversarial noises. One possible explanation is that since the autoencoder is trained without the knowledge of the classiﬁcation objective, it has more “blind-spots” with re-  4  Accepted as a workshop contribution at ICLR 2015  Table 3: Cross-model autoencoder generalization test. Error-rates on adversarial test data. Last two rows shows the error-rates on clean test data and the average minimal distortion of adversarial examples generated from stacked network, respectively. The error-rates on the adversarial test data without the autoencoder preprocessing is approximately 100% as shown in Table 1.  N-100-100-10 N-200-200-10 AE400-10 ConvNet Test error (clean) Avg adv distortion  N-100-100-10 N200-200-10 AE-400-10 ConvNet 2.3% 2.3% 3.6% 7.7% 2.1% 0.049  5.2% 5.4% 9.2% 2.6% 1.1% 0.038  2.4% 2.2% 3.5% 7.6% 1.9% 0.051  2.3% 2.2% 2.7% 8.3% 2.1% 0.043  Figure 2: 1st column is the original data. 2nd and 3rd are the adversarial examples and their noises for the original model. 4th and 5th are for the stacked model of AE + the original net.  spect to that ﬁnal objective. This again conﬁrms the necessity of end-to-end training in deep neural networks.  3.3 DENOISING AUTOENCODER  In this section, a standard denoising autoencoder (DAE) is trained, without the knowledge of the adversarial noise distribution. A DAE maps corrupted input data to clean input data. At each train- ing batch, each pixel in the input data is corrupted by adding independent Gaussian noise with 0 mean and σ standard deviation. Table 4 summarizes the results indicating that a standard denoising autoencoder can still recover a signiﬁcant portion of the adversarial noises. In particular, a denois- ing auto-encoder with σ = 0.1 Gaussian noise could denoise adversarial examples almost as well as an autoencoder trained on actual adversarials noises, as shown in Table 3. However, this model also suffers the same deﬁciency as in Section 3.2, that a stacked network is more susceptible to adversarials. In this case, this deﬁciency likely arises due to imperfect training of DAE itself.  3.4 DISCUSSION  Our experiments have shown that the adversarial noise is fairly robust against local perturbations such as additive Gaussian noise, suggesting that the size of “blind-spots” is relatively large. In the image data case, the effect of adversarial examples can be signiﬁcantly reduced by low-pass ﬁlter- ing, such as Gaussian blurring, suggesting that adversarial noise mostly resides in high-frequency domain. Moreover, the success of the autoencoder and the denoising autoencoder experiment shows adversarial noise to have simple structure that is easily exploitable.  A key observation about the adversarial examples is that they are unavoidable and intrinsic property of any feed-forward architecture. For any pre-processing, it is always possible to backpropagate the error signal through the additional functions and ﬁnd new adversarial examples, not only for deterministic pre-processing steps such as Gaussian blurring and autoencoders. Surprisingly, our experiments show that the distortion of adversarials from stacked network is even lower than the  5  Accepted as a workshop contribution at ICLR 2015  Table 4: Denoising autoencoder test. Error on the adversarial test data.  DAE, σ = 0.1 DAE, σ = 0.5  N-100-100-10 N200-200-10 AE-400-10 ConvNetM 5.0% 10.0%  9.1% 15.3%  11.5% 16.3%  4.9% 10.6%  distortion of adversarials from the original classiﬁer network. Furthermore, Table 4 also hints that even a simple Gaussian additive noise, often used in data augmentation, effectively creates ﬂat, invariant regions around the input data points. Based on these results and observations, we thus postulate that solving the adversarial problem should correspond to ﬁnding new training procedures and objective functions so as increase the distortion of the smallest adversarial examples. Therefore, in Section 4, we formulate a new model that could propagate the input invariance toward the ﬁnal network outputs and be trained in end-to-end.  4 DEEP CONTRACTIVE NETWORK  In this section, we formulate Deep Contractive Network, which imposes a layer-wise contractive penalty in a feed-forward neural network. The layer-wise penalty approximately minimizes the network outputs variance with respect to perturbations in the inputs, enabling the trained model to achieve “ﬂatness” around the training data points.  4.1 CONTRACTIVE AUTOENCODER  Contractive autoencoder (CAE) is a variant of an autoencoder (AE) with an additional penalty for minimizing the squared norm of the Jacobian of the hidden representation with respect to input data Rifai et al. (2011b). A standard AE consists of an encoder and a decoder. The encoder maps input data to the hidden representation, and the decoder attempts to reconstruct the input from the hidden representation. Formally, given input x ∈ Rdx and hidden representation h ∈ Rdh, the encoder parametrized by dh × dx matrix We and bias vector bh ∈ Rdh and the decoder parametrized by dx × dh matrix Wd and bias vector by ∈ Rdh, the output y ∈ Rdx is given by:  y = σd(Wdh + by) = σd(Wdσe(Wex + bh) + by)  (2) where σe and σd are non-linear activation functions for the encoder and decoder respectively. Given m training data points, the AE is trained by ﬁnding the model parameters θ = {We, Wd, bh, by} that minimize the following objective function:  JAE(θ) =  m  X  i=1  L(x(i), y(i))  For CAE, the objective function has an additional term:  JCAE(θ) =  m  X  i=1  (L(x(i), y(i)) + λ k  ∂h(i) ∂x(i) k2)  (3)  (4)  λ is a scaling factor that trades off reconstruction objective with contractive objective. k ∂h(i) the Frobenius norm of the Jacobian matrix of h(i) with respect to x(i).  ∂x(i) k2 is  4.2 DEEP CONTRACTIVE NETWORKS  A Deep Contractive Network (DCN) is a generalization of the contractive autoencoder (CAE) to a feed-forward neural network that outputs y ∈ Rdy with a target t ∈ Rdy . For a network with H hidden layers, let fi denote the function for computing hidden representation hi ∈ Rdhi at hidden layer i: hi = fi(hi−1), i = 1...H + 1, h0 = x and hH+1 = y. Ideally, the model should penalize the following objective:  JDCN (θ) =  m  X  i=1  (L(t(i), y(i)) + λ k  ∂y(i) ∂x(i) k2)  (5)  6  Accepted as a workshop contribution at ICLR 2015  Table 5: The error-rates on clean test data and the average distortion of adversarial examples gen- erated from the original model (orig) and the same model with contractive penalty (DCN). The error-rates on the adversarial examples are 100%.  Model Name DCN error DCN adv. distortion N100-100-10 N200-200-10 AE400-10 ConvNet  0.107 0.102 0.106 0.106  2.3% 2.0% 2.0% 1.2%  orig. error 1.8% 1.6% 2.0% 0.9%  orig. adv. distortion 0.084 0.087 0.098 0.095  Table 6: The error-rates on clean test data and the average distortion on adversarial examples from N200-200-10 models with different training conditions. “GN” refers to Gaussian additive noise during training.  Training Condition DCN GN,L1,σ = 0.1 GN,L*,σ = 0.1 DCN+GN,L1,σ = 0.1  Test error Av. distortion 2.0% 1.8% 2.0% 2.2%  0.102 0.095 0.099 0.108  However, such a penalty is computationally expensive for calculating partial derivatives at each layer in the standard back-propagation framework. Therefore, a simpliﬁcation is made by approximating the objective with the following:  JDCN (θ) =  m  X  i=1  (L(t(i), y(i)) +  H+1  X  j=1  λj k  ∂h(i) j ∂h(i) j−1  k2)  (6)  This layer-wise contractive penalty enables partial derivatives to be computed in the same way as in a contractive autoencoder, and is easily incorporated into the backpropagation procedure. This objective does not guarantee global optimality for the solution to Eq. 5, and also limits the capacity of the neural network. However, it is a computationally efﬁcient way to greedily propagate input invariance through a deep network.  Note that for a neural network with additive Gaussian noise N (0, σ2I) added to input during the training, if we let σ → 0, the training objective is equivalent to Eq. 5 Alain & Bengio (2012). However, such stochastic penalties require many passes of data to train the model effectively. For efﬁciency, we decided to employ a deterministic penalty instead Rifai et al. (2011b); Alain & Bengio (2012); Chen et al. (2014).  4.3 EXPERIMENTS AND RESULTS  The experiments involve applying a contractive penalty to the models in Table 1. The models were trained until they achieved nearly the same accuracy as the original models which lacked a contrac- tive penalty. The adversarial examples are generated following the method deﬁned in Section 2.1.  Table 5 shows that the contractive penalty successfully increases the minimum distortion of the ad- versarial noises. Table 6 shows the comparison of the Deep Contractive Network penalty against stochastic noise addition. Deep Contractive Networks are more robust than a standard neural net- work trained with Gaussian input noise, and can be easily augmented by adding Gaussian input noise to further increase the minimum distortion of adversarial noises.  4.4 DISCUSSIONS AND FUTURE WORK  Results show that Deep Contractive Networks can successfully be trained to propagate contractivity around the input data through the deep architecture, without signiﬁcant loss in ﬁnal accuracies. The model can be improved by augmenting the layer-wise contractive penalty based on Higher-Order Contractive autoencoders Rifai et al. (2011a), and marginalized Denoising autoencoders Chen et al.  7  Accepted as a workshop contribution at ICLR 2015  Figure 3: 1st column is the original data. 2nd and 3rd are the adversarials and noises for the original model. 4th and 5th are the adversarials and noises for the model trained with contractive penalty.  (2014). While in this paper, Deep Contractive Networks are used as a framework for alleviating effects due to the adversarial examples, they also provide a suitable framework for probing the invariance properties learnt by deep neural networks. A further study should be conducted to eval- uate the performance loss due to layer-wise penalties as opposed to global contractive objectives as deﬁned in Eq. 5. In addition, exploring non-Euclidean adversarial examples, e.g. small afﬁne transformation on images, and varying contractivity at higher layers of the network could lead to insights into semantic attributes of features learned at high levels of representation. For example, explicitly learning instantiation parameters as previously attempted by models such as Transforming Autoencoder Hinton et al. (2011).  Our work also bridges supervised learning with unsupervised representation learning, by introducing the penalty from DAE and CAE to standard DNN. Such penalty not only acts as practical regular- izers, but also is a highly efﬁcient way to learn obvious information from the training data, such as local generalization in the input space. Recent progress in deep neural networks is driven by both end-to-end supervised training and various modes of unsupervised feature learning Krizhevsky et al. (2012); Bengio (2009), and thus we believe the merge of the two could likely enable new milestones in the ﬁeld.  5 CONCLUSIONS  We tested several denoising architectures to reduce the effects of the adversarial examples, and conclude that while the simple and stable structure of adversarial examples makes them easy to remove with autoencoders, the resulting stacked network is even more sensitive to new adversarial examples. We conclude that neural network’s sensitivity to adversarial examples is more related to intrinsic deﬁciencies in the training procedure and objective function than to model topology. The crux of the problem is then to come up with an appropriate training procedure and objective function that can efﬁciently make the network learn ﬂat, invariant regions around the training data. We propose Deep Contractive Networks to explicitly learn invariant features at each layer and show some positive initial results.  ACKNOWLEDGMENTS  The authors thank Nitish Srivastava for releasing his DeepNet Library, and Volodymyr Mnih for his CUDAMat Library Mnih (2009).  REFERENCES  Alain, Guillaume and Bengio, Yoshua. What regularized auto-encoders learn from the data gener-  ating distribution. arXiv preprint arXiv:1211.4246, 2012.  Bengio, Yoshua. Learning deep architectures for ai. Foundations and trends R(cid:13) in Machine Learning,  2(1):1–127, 2009.  8  Accepted as a workshop contribution at ICLR 2015  Chen, Minmin, Weinberger, Kilian, Sha, Fei, and Bengio, Yoshua. Marginalized denoising auto-  encoders for nonlinear representations. JMLR, 32(1):1476–1484, 2014.  Dahl, George E, Yu, Dong, Deng, Li, and Acero, Alex. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42, 2012.  Douglas, Rodney J, Koch, Christof, Mahowald, Misha, Martin, KA, and Suarez, Humbert H. Re-  current excitation in neocortical circuits. Science, 269(5226):981–985, 1995.  Duvenand, David, Rippel, Oren, Adams, Ryan P., and Ghahramani, Zoubin. Avoiding pathologies  in very deep networks. arXiv preprint arXiv:1402.5836, 2014.  Felleman, Daniel J and Van Essen, David C. Distributed hierarchical processing in the primate  cerebral cortex. Cerebral cortex, 1(1):1–47, 1991.  Hinton, Geoffrey E, Krizhevsky, Alex, and Wang, Sida D. Transforming auto-encoders. In Artiﬁcial  Neural Networks and Machine Learning–ICANN 2011, pp. 44–51. Springer, 2011.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images.  Computer Science Department, University of Toronto, Tech. Rep, 2009.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, volume 1, pp. 4, 2012.  LeCun, Yann and Cortes, Corinna. The mnist database of handwritten digits, 1998.  Mnih, Volodymyr. Cudamat: a cuda-based matrix class for python. Department of Computer  Science, University of Toronto, Tech. Rep. UTML TR, 4, 2009.  Mnih, Volodymyr, Heess, Nicolas, Graves, Alex, and Kavukcuoglu, Koray. Recurrent models of visual attention. CoRR, abs/1406.6247, 2014. URL http://arxiv.org/abs/1406.6247.  Rifai, Salah, Mesnil, Gr´egoire, Vincent, Pascal, Muller, Xavier, Bengio, Yoshua, Dauphin, Yann, and Glorot, Xavier. Higher order contractive auto-encoder. In Machine Learning and Knowledge Discovery in Databases, pp. 645–660. Springer, 2011a.  Rifai, Salah, Vincent, Pascal, Muller, Xavier, Glorot, Xavier, and Bengio, Yoshua. Contractive auto- encoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 833–840, 2011b.  Stollenga, Marijn, Masci, Jonathan, Gomez, Faustino, and Schmidhuber, Juergen. Deep networks with internal selective attention through feedback connections. arXiv preprint arXiv:1407.3068, 2014.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014a.  Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow,  Ian, and Fergus, Rob. Intriguing properties of neural networks. ICLR, 2014b.  Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf, Lior. Deepface: Closing the gap In Proceedings of the IEEE Conference on  to human-level performance in face veriﬁcation. Computer Vision and Pattern Recognition, pp. 1701–1708, 2013.  Tang, Yichuan. Deep learning using support vector machines. arXiv preprint arXiv:1306.0239,  2013.  Vinyals, Oriol, Jia, Yangqing, Deng, Li, and Darrell, Trevor. Learning with recursive perceptual  representations. In Advances in Neural Information Processing Systems, pp. 2825–2833, 2012.  Zhang, Ning, Paluri, Manohar, Ranzato, Marc’Aurelio, Darrell, Trevor, and Bourdev, Lubomir. Panda: Pose aligned networks for deep attribute modeling. arXiv preprint arXiv:1311.5591, 2013.  9  ",
1412.6615,2015, Explorations on high dimensional landscapes,"['Explorations on high dimensional landscapes', 'Levent Sagun', 'Ugur Guney', 'and Yann LeCun']",https://arxiv.org/pdf/1412.6615,"5 1 0 2    r p A 6         ] L M  . t a t s [      4 v 5 1 6 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  EXPLORATIONS ON HIGH DIMENSIONAL LANDSCAPES  Levent Sagun1, V. U˘gur G¨uney2, G´erard Ben Arous1, & Yann LeCun1,3 1Courant Institute of Mathematical Sciences, New York, NY 10012 2Department of Physics, The City University of New York, New York, NY 10016 3Facebook AI Research, 770 Broadway, New York, NY 10003 sagun@cims.nyu.edu, uguney@gc.cuny.edu benarous@cims.nyu.edu, yann@cs.nyu.edu  ABSTRACT  Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. We provide evidence that some such func- tions that are deﬁned on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to inﬁnity. Furthermore our ex- periments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We ﬁnally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.  1  INTRODUCTION  Many problems of practical interest can be rephrased as forms of an optimization problem in which one needs to locate a point in a given domain that has some optimal energy or cost value. Given a dynamics on such a surface the question of where it will stop on the landscape on which it is moving is a challenging one, especially if the terrain is rugged. Complex1 systems can be described by func- tions that are highly non-convex, that lives on high dimensional spaces and they have exponentially many critical points. One such problem arises in the context of learning: Given a set of input-output pairs, one seeks to ﬁnd a function whose values roughly match the ones in the set and which also performs well on new inputs. That is, it learns the task described by the set it began with. To this end, a loss function is formed which is optimized over the parameters of the desired function. Performance of this loss minimization is tested against another set of input-output pairs. The second problem we consider comes from statistical physics: In magnetic materials, the spins are aligned according to interactions with their neighbors. Over time equilibrium is reached when particles settle down to a conﬁguration that has a reasonably low energy, if not the lowest possible. Stability of this state is tested against small ﬂuctuations. Some machine learning problems, such as deep networks, hint at similarity with spin systems of statistical mechanics. Finding connections between the two became an attractive research area. For a nice survey of theoretical results between Hopﬁeld networks and Boltzmann machines, see Agliari et al. (2014), and Barra et al. (2012). In a slightly different approach Dauphin et al. (2014) and Choromanska et al. (2014) exhibit that stochastic gradient descent might end up in critical points other than local minima. The energy landscapes of spin glasses are of interest in themselves. One of the main questions concerns the number of critical points of a given index at a given level. For a detailed analysis of this in spherical spin glasses, see Aufﬁnger et al. (2013), Aufﬁnger & Ben Arous (2013) and  1Even though we use ‘complex’ to refer to systems that has exponentially many critical points, we would like to point out that to the best of our knowledge there is no agreed upon deﬁnition of what complex a system is across disciplines.  1  Accepted as a workshop contribution at ICLR 2015  Aufﬁnger & Chen (2014). The ﬁrst of this sequence of papers will be used in this present paper in the corresponding experimental section. It establishes the existence of an energy level, which we named ﬂoor2, in which bulk of the low index critical points lie in the absence of an external ﬁeld. It is proved that this level contains exponentially many local minima. For an algorithm to pass this level the search time should be exponentially long. Yet the ﬂoor lies just above the ground state, therefore from an optimization point of view it does not matter whether the algorithm reaches to the global minimum or local minimum at the ﬂoor. For a similar analysis in a slightly more general context using random matrices, see Fyodorov (2013), and Fyodorov & Le Doussal (2013). For a review of random matrices and extreme value statistics, see Dean & Majumdar (2008). The added external ﬁeld as a tunable parameter allows changing the topology of the landscape. Inspired by this work, Mehta et al. (a) gives a detailed experimental analysis of the case in which there are polynomially many critical points, and Mehta et al. (b) counts critical points for third order interactions in which there are exponentially many critical points. In this work, we focus on the case that has exponentially many critical points. In such high dimen- sional non-convex surfaces, a moving object is likely to get stuck in a local minimum point or a plateau of ﬂat areas that is close to degenerate. A reasonable algorithm should have enough noise to make the particle jump out of those critical points that have high energy. However, if the land- scape has exponentially many critical points that lie around the same energy level, then jumping out of this well will lead to another point of similar energy, clearly not resulting in any improvement. The existence of such a ﬂoor will increase the search time for points that have lower energy values regardless of the method of choice. Alternatively, one can attempt to change the system so that ﬂoor is at a different, more favorable value, namely closer to the global minimum. One will of course be curious about the performance of such a point in the learning context, or its stability in the spin glass context. So we ask two questions: 1) Where does the particle end up and how good is that point for the task at hand? 2) What do various descent algorithms do differently? We perform experiments in spin glasses and in deep networks to address those questions above. The differences in the graph connectivities of spins and weights indicate that the two systems are not mathematically analogous. Rather we propose that they are special cases of a more general phenomenon which is not yet discovered. The authors hope to extract further knowledge from such complex systems that will shed light to a thorough theoretical understanding in the future.  2 SETTING FOR EXPERIMENTS AND PREVIOUS THEORETICAL WORK  2.1 MEAN FIELD SPHERICAL SPIN GLASS MODEL  (cid:80)  The simplest model of a magnetic material is the one in which atoms have spins with two states: +1 for spin up, or -1 for spin down. Considering pairwise interactions (in other words 2-body inter- ij wiwj where w represents actions) between neighbors gives the total energy of the system: − spin sign of the particle located at position i. Mean ﬁeld assumption ignores the lattice geometry of particles and assumes all interactions have equal strength. Introducing a weak external ﬁeld leads all particles to favor alignment, which incidentally corresponds to the minimum energy conﬁguration which will be achieved at the point where all of the particles have spin up (or down for that matter). This is a rather simple landscape whose global minimum is easy to achieve. This assumes no frus- tration, that is, all particles favor alignment. This model is known as the Currie-Weiss model. If we have an alloy in which some particle pairs favor alignment and some favor dis-alignment, the picture changes drastically. This introduces a whole new set of constraints which may not be simultaneously attained, leading to glassy states. One way to model this is to introduce random interaction coefﬁ- ij xijwiwj this model, which has a rather complex energy landscape, is known as the Sherrington-Kirkpatrick model. See Agliari et al. (2014) and Sherrington (2014) for a survey on spin glasses and their connection to complex systems.  cients between particles:(cid:80)  2Throughout this paper the energy levels that an algorithm can not pass in a reasonable amount of time will be referred to as ﬂoor. For example in the spin glass context of the following section it corresponds to the smallest energy of the highest value of the function Θ(u) in equation 1, which is −E∞.  2  Accepted as a workshop contribution at ICLR 2015  We can also disregard the discrete states and put the points on the sphere and consider 3-body  Hamiltonian with standard normal interactions is given by(cid:80) interactions of N particles. Taking a point w ∈ SN−1(√N ) ⊂ RN and x(·) ∼ Gaussian(0, 1), the  ijk xijkwiwjwk. We have arrived at the model of interest for the purposes of this paper. While this model has been studied previously in great detail, our particular focus is on the location of critical points. The main results we will use in this section and further details on the model can be found in Aufﬁnger et al. (2013) and references therein. Aufﬁnger et al. (2013) considers a more general version that involves any combinations of interac- tions not only 2 or 3. The main reason we consider triplets is because it is the smallest system that has exponentially many critical points. Before moving any further we would like to remark on why i = N and  there are only polynomially many critical points in the binary interaction case. For(cid:80) w2  xij ∼ Gaussian(0,1) the Hamiltonian of the system is given by  H(w) =  wiwjxij = (M x, x)  (cid:88)  i∼j  which is a quadratic form where Mij = xij +xji is a random N × N Gaussian Orthogonal Ensemble matrix. This system has 2N critical points at eigenvectors and their corresponding energy values are eigenvalues scaled by N.  2  2.1.1 FLOOR OF THE HAMILTONIAN  For w ∈ SN−1(√N ) ⊂ RN and x(·) ∼ Gaussian(0, 1) consider the function  xijkwiwjwk  (1)  N(cid:88)  i,j,k  HN (w) =  1 N  Once the random coefﬁcients are chosen, the landscape that will be explored is ﬁxed. Note that as a continuous function on a compact set HN attains its minimum and maximum. Also, since the landscape is symmetric through origin, its local or global maximum points have similar energy values with the opposite sign. We only focus on its minimum. Let NN,k(u) denote the total number of index-k critical points of HN that lies below the level N u. In other words NN,k(u) is the random variable that counts critical points in the set {w : HN (w) ≤ N u}. Aufﬁnger et al. (2013) ﬁnds asymptotic expected value of this quantity in logarithmic scale: (2)  E[NN,k(u)] (cid:16) eN Θk(u)  Figure 1: Plot of Θ0(u) (upper curve for local minima) and Θ1(u) (lower curve for index 1). Their values agree at the point −E∞, which is the asymptotic value of the ﬂoor, and remain constant. Also note that the energy values on the horizontal axis are scaled by N.  In other words Θ(u) is a measure of the cumulative number of critical points from the ground state to the level N u. An exact analytic description of the function Θ(u) can be found in Aufﬁnger et al. (2013). Figure 1 shows that analytic expression for k = 0, local minima, and k = 1, saddles of  3  −1.66−1.65−1.64−1.63−1.62u−0.0050.0000.0050.0100.015Θ(u)E0E∞k=0k=1Accepted as a workshop contribution at ICLR 2015  index 1. It shows, as expected, that Θ in equation (2) is non-decreasing as it counts the number of critical points cumulatively. Θ becomes positive3, and keeps increasing until it becomes constant at a value denoted as −E∞, which is the lowest u for which Θ is at its maximum value. Hence the number of critical points of low index do not increase at high energy values. Clearly the function in (1) is a Gaussian random variable as it is formed by a sum of independent Gaussians. The 1/N normalization factor is chosen for the model so that Var(H) = N. This implies that we have a random ﬁeld with mean zero and variance N at each point. Another implication of the size of variance of the Hamiltonian is that its extensive quantities scale with N, for which Aufﬁnger et al. (2013) gives,  (cid:0) min  w  (cid:1)  lim inf N→∞  HN (w)  N  ≥ −E0  (3)  where −E0 is the zero of Θ function. In words, the global minimum of the Hamiltonian on the sphere is bounded from below by −N E0. This lower bound gives a measure on how far a given point is away from the ground state. For our experiments the energy at the ground state and ﬂoor is calculated to be −N 1.657 and −N 1.633, respectively. For practical purposes the ﬂoor level is close to the ground state. At the level of −N 1.633 (the vertical line in Figure 1) the landscape is expected to exponentially many points that are local minimum or saddles of low index, and the exponent is at its maximum. Therefore probability of ﬁnding a local minimum that lies below the ﬂoor level is exponentially small. This leads us to conjecture that this ﬂoor is the ﬁrst place to get stuck when using a descent method which is conﬁrmed in our simulations. Diving deeper would require a long search time. Therefore trying to ﬁnd points that has lower energy levels is not necessary and even realizable.  2.2 SETTING FOR MNIST Consider a probability measure µ on Rn, centralized, that represents data, and a function G from Rn to R that correctly labels data. True loss then measures how well a given function, say G(w), approximates G:  (cid:90)  d(w) = d(G, G(w))  =  Rn  d(G(x), G(w)(x))dµ(x)  For an integer P consider i.i.d. training sample, xp for p = 1, ..., P , of the measure µ and deﬁne the empirical training loss:  P(cid:88)  p=1  LTrain(w) =  1 P  L(xp, w)  where L is the loss per sample which can be the mean square loss, the hinge loss or cross-entropy. Also by the law of large numbers:  The limit holds pointwise so that the sampled loss approximates well the true loss as the number of samples increases. Moreover, by the central limit theorem the following holds pointwise:  LTrain(w) → Eµ[L(x, w)] = d(w) µ-a.s. as P → ∞.  √P (LTrain(w) − d(w))  d.  −→ N (0, σ2(w))  where σ2(w) = Eµ[L2(x, w)] − d2(w)  So that the ﬂuctuations of this approximation is pointwise Gaussian. In real life problems true loss is not accessible, instead we have LTrain(w) which approximates d(w). From the above convergence properties we expect both the test and training loss to converge with good accuracy to the true loss. Therefore we expect LTrain(w) ∼ LTest(w) as well if there is enough data for both functions. A natural question is how close are they to each other? If we sample points from the ﬂoor of training surface, do they necessarily correspond to the ﬂoor of the test surface? How does the smaller scale ﬂuctuations effect learning? We address these questions in the rest of the paper.  3Note that in the binary interaction case described above, the exponent in (2) never crosses beyond zero otherwise it would imply exponentially many critical points. So that the binary interaction case, or the two- body case is an example of a simple system.  4  Accepted as a workshop contribution at ICLR 2015  3 SIMULATIONS AND EXPERIMENTS  Our simulations in the ﬁrst part below show existence of a critical dimension before which the land- scape is hectic and does not really show any sign of a well-deﬁned value that algorithms converge. This implies that in low dimensions the surface is not trivial, and that there are traps at high energy levels and those traps are located at somewhat arbitrary values as seen in ﬁgure 2. On the other hand high dimensional picture is drastically different. The landscape is trivial in the following sense: Starting from a random point, and following the gradient descent algorithm almost always leads to a very narrow band of values. Moreover in the second part we show that this descent is irrespective of which algorithm is being used.  3.1 FLOOR FOR HIGH DIMENSIONAL SYSTEMS  3.1.1 SPIN GLASSES  Simulations of the spin glass model shows a clear qualitative difference between low dimensional and high dimensional surfaces. Namely, low dimensional surfaces do not exhibit a well deﬁned ﬂoor however high dimensional surfaces does. Another important feature is that the ﬂoor level is very close to the global minimum, therefore ﬂoor level is enough for practical purposes of optimization, even when we set aside the problem of reaching global minimum. The gradient descent algorithm is used on spin glass landscape. Procedure starts with ﬁxing random couplings. Given the dimension N, sample N 3 many i.i.d. standard normal random variables: 1. Pick a random element w of the sphere SN−1(√N ) as a starting point for each trial. 2. Iteratate4 wt+1 = wt − γt∇wH(wt), and normalize, √N wt+1 3. Stop when the gradient size is below the order of 10−5.  ||wt+1|| ← wt+1.  Figure 2: Vertical black lines indicate the theoretical value of the ﬂoor in the large N limit. When N is small the system gets trapped at bad energy values (upper histogram). When N is ﬁnite but large, ﬂoor is a narrow band (lower histogram).  For each N the experiment is repeated 5000 times. And the stopping criteria is the norm of the gradient vector. The theoretical results of the previous section holds true asymptotically. At this point, to the best of the knowledge of authors, there is no proof of how fast the energy of critical  4Note here the constraint that the gradient is tangential to the sphere.  5  1.651.601.551.501.451.401.35H(w)/N0.000.020.040.060.080.100.120.140.16Histograms of normalized energy501502501.651.601.551.501.451.401.35H(w)/N0.000.020.040.060.080.100.12Histograms of normalized energy350450550Accepted as a workshop contribution at ICLR 2015  points concentrate around the limiting ﬂoor value. We hope this simulation will lead to a prediction in this direction. Nevertheless it is observed that the points found with this procedure lies within a narrow band of values that is just above the asymptotic ﬂoor level that is given in the previous section.  3.1.2 TRI-PARTITE SPIN GLASS: A TOY MODEL FOR MULTILAYER RESTRICTED BOLTZMANN  MACHINES  Figure 3: Graph connectivity: p-spin spin glass vs tri-partite spin glass.  The graph structure of a fully connected spin glass above is highly coupled, in this section we modify the function so the terms of the polynomial have a layered structure. This is achieved by simulating the uncoupled version of the above spin glass model in a way that mimics the layered structure of a multilayer Restricted Boltzmann Machine. From an optimization point of view a In this new setting, take three similar consideration can be found in Frieze & Kannan (2008).  Figure 4: Tri-partite spins low dimension vs high dimension comparison.  (cid:80) xijkw1  j w3  i w2  different points on the sphere, or equivalently, take one point on the product of three spheres to form the tri-partite Hamiltonian. Instead of equation 1 the uncoupled function at hand becomes ˜HN (w) = 1 k. Then perform gradient descent over all parameters and normalize N the resulting 3N dimensional vector on the product of spheres. Since there are fewer constraints, the values obtained by this procedure should be lower, which is indeed the case. Figure 4 shows a similar ﬂoor structure of the landscape. However, in this case we do not have any theoretical result that tells us the value of its ground state; therefore, even though we ﬁnd evidence of ﬂoor in high dimensions, it is hard tell how far away that ﬂoor value lies from the actual ground state. Since the order of growth of the function is still N, the ratio of the ground state to the value of the ﬂoor should also be the same as in the previous section, however we do not, at this point, have a proof for this.  6  2.802.752.702.652.602.552.50H(w)/N0.000.050.100.150.200.25Histograms of normalized energy2550751502.802.752.702.652.602.552.50H(w)/N0.000.020.040.060.080.100.120.140.16Histograms of normalized energy250350450550Accepted as a workshop contribution at ICLR 2015  3.1.3 TEACHER-STUDENT NETWORK: CASE OF A KNOWN GROUND STATE AT ZERO  In this section we design a system for which we know where its global minimum lies. The idea for this experiment is inspired by the teacher-student network comparisons of Saad et al. (1996) and West et al. (1997). It goes as follows: Split the MNIST training set into two parts. Using only the ﬁrst half of the training data, train a network of two hidden layers, 784-500-300-10, with ReLU’s at ﬁrst two layers and soft-max at the last output layer. The teacher makes 211 mistakes in the test set of size 10,000. Use cross entropy loss and train the system with SGD. This gives the teacher network. Using the teacher network create new labels on the second half of the training set by replacing actual tags with the probabilities assigned by the outputs of the teacher network.  Table 1: Student networks trained with SGD Training cost Test cost Test error  St.Dev.(test error)  784-50-50-10 student 784-250-150-10 student 784-500-300-10 student 784-1200-1200-10 student  1.83e-02 1.72e-02 1.70e-02 1.68e-02  1.69e-01 1.31e-01 1.25e-01 1.18e-01  326 276 265 257  11.3 7.6 6.6 5.2  If size of the student network is at least as big as the teacher network, it is guaranteed that zero cost value points exist. Moreover there are exponentially many of them: an exact copy of the network can be found, and appealing to the symmetries of the network one can permute all the weight connections without changing the cost. All student networks are trained with SGD, and it does not reach zero cost. The algorithm either gets stuck at a critical point or extremely slows down at a very ﬂat area in the surface whose value is away from zero. We conjecture that this level is the ﬂoor for the student training surface. Also notice the different behavior of low dimensional networks that have bad critical points at which the ﬂoor is not well deﬁned, which is again in contrast with the high dimensional networks.  Figure 5: Training cost values for different student network sizes. High dimensional surfaces exhibit a similar approach to the asymptotic ﬂoor value. The student networks that are at least of the same size as the teacher there is exponentially many possible zero values on the training surface, yet SGD can not locate them!  Data that students are trained on are not exact, they are partial in the sense that many labels are vague. For some characters, the teacher does not know the answer for sure. Figure 6 captures this vague response of the teacher which is learned by the student. But this ambiguity does not stop the teacher from teaching; instead, the teacher passes information with the ambiguity, which is some information by itself (this reminds us the Dark Knowledge at Hinton et al.). Recall that the teacher did not see the second half of the data during its training, but used it to teach its students. We look at its students to see how well they learned the things the teacher taught. It turns out, perhaps not surprisingly, that the ambiguity propagates. Many mistakes that the teacher makes in the second half of the data are also made by its students. There are cases that a student might judge more correctly. See Figure 6 for an example digit 0 that the teacher mistakenly thought was 6. Luckily, as an honest teacher, it did not train its student as if it were a ﬁrm 6. With the help of this extra information, the student was able to classify correctly the digit by a tiny margin.  7  0.0150.0160.0170.0180.0190.0200.0210.0220.000.050.100.150.200.25training cost histogram of studentsbigsamemediumsmallAccepted as a workshop contribution at ICLR 2015  Figure 6: One example of a medium sized student that correctly classiﬁes a mistake of the teacher. The digit is seen on the right side. Histograms are probability outputs of the teacher (left) and the student (middle).  3.2 FLOOR BY GRADIENT DESCENT VS. STOCHASTIC GRADIENT DESCENT  This section compares the two algorithms in terms of where they reach on their training surface.  3.2.1 SGD FOR SPIN GLASS  A spin glass ﬁeld is created by a sum of smaller such ﬁelds:  N(cid:88)  i,j,k  1 N  HN (w) =  xijkwiwjwk =  =  P(cid:88) (cid:0) 1 N(cid:88)  p=1  N  N(cid:88) (cid:0) P(cid:88)  i,j,k  1 N  i,j,k  p=1  xp ijkwiwjwk  (cid:1) (cid:1)wiwjwk  xp ijk  (4)  ijk ∼ Gaussian(0, 1  Here xp P ) so that the resulting ﬁeld of the sum is distributed as the one in equation 1. Then at the pth step, the point on the sphere is updated in the negative direction of the gradient of the pth summand. This procedure is then the stochastic gradient descent with a minibatch of size 1. It is important to note that all summands are independent from each other, unlike the MNIST case. SGD still goes to the ﬂoor. The tiny difference in the values of SGD comes from the fact that the SGD slows down very fast and stops at the walls of a well. Once SGD slows down, one could restart GD from that point and reach the bottom of that well.  Figure 7: Both methods reach the ﬂoor. Different P ’s give eventually the same energy when mea- sured at the same step-size-times-number-of-steps. P = 1 corresponds to the gradient descent.  8  02468100.00.20.40.60.81.002468100.00.20.40.60.81.00510152025051015202501020304050step size / P5004003002001000100H(w)GD and SGD for spin glassP=1P=20P=30Accepted as a workshop contribution at ICLR 2015  Table 2: Averages of results for Gradient Descent and Stochastic Gradient Descent for MNIST  Training cost Test cost Test error  St.Dev.test error  50-50 GD 50-50 SGD 500-300 GD 500-300 SGD  4.52e-04 3.86e-04 1.55e-04 1.38e-04  2.02e-01 1.57e-01 1.02e-01 8.47e-02  299 256 194 174  15.5 11.6 6.9 5.3  3.2.2 MNIST  This experiment compares GD with SGD over full MNIST in a two layer network. One property that is attributed to SGD is that due to its noisy nature it is capable of escaping local minima at higher cost values. This would imply that GD would get stuck before SGD slows down. However it does not seem like this is the case at all. Within the same stepsize SGD and GD perfoms very similar on the training surface. Table 2 shows mean cost values and the difference in test errors.  Figure 8: Mean costs of GD and SGD experiments are given by the thin lines, and shady areas around the curve is the standard deviation around the mean. Vertical axis is in log scale. Decay of the function is not noticable in linear scale. Progress is very slow even in the log-scale. And the two curves follow each other tight.  9  0100002000030000400005000010-410-310-210-1100101Cost vs. step no for 50-50 networkSGD trainSGD testGD trainGD test0100002000030000400005000010-410-310-210-1100101Cost vs. step no for 500-300 networkSGD trainSGD testGD trainGD testAccepted as a workshop contribution at ICLR 2015  4 CONCLUSION  High dimensional systems are typically considered as systems that come with their curses, however they also exhibit lots of symmetries which can be a blessing as observed in the ﬁrst part of the simulations. One goal of the paper is to trigger theoretical research on non-convex surfaces on high dimensional domains. It is crucial to repeat here that we do not suggest a direct equivalency between spin glasses and deep networks; rather, we hint at a more general phenomenon that governs the two different cases. Finding similar properties in a variety of other problems might help us identify and quantify the properties of such complex systems. We hope further investigation in other optimization problems will lead to supporting conclusion that is in line with this research.  ACKNOWLEDGEMENTS  We thank David Belius for valuable discussions, Taylan Cemgil and Atilla Yılmaz for valuable feedback, and reviewers for valuable suggestions. We thank the developers of Torch7 (Collobert et al. (2011)) which we used for the spin glass simulations and Theano (Bastien et al. (2012) and Bergstra et al. (2010)) which we used for MNIST experiments. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for part of this research.  REFERENCES Agliari, Elena, Barra, Adriano, Galluzzi, Andrea, Tantari, Daniele, and Tavani, Flavia. A walk in  the statistical mechanical formulation of neural networks. 1948:12, July 2014.  Aufﬁnger, Antonio and Ben Arous, G´erard. Complexity of random smooth functions on the high- dimensional sphere. The Annals of Probability, 41(6):4214–4247, November 2013. ISSN 0091- 1798. doi: 10.1214/13-AOP862.  Aufﬁnger, Antonio and Chen, Wei-kuo. Free Energy and Complexity of Spherical Bipartite doi:  Journal of Statistical Physics, 157(1):40–59, July 2014.  ISSN 0022-4715.  Models. 10.1007/s10955-014-1073-0.  Aufﬁnger, Antonio, Ben Arous, G´erard, and ˇCern´y, Ji˘r´ı. Random Matrices and Complexity of Spin Glasses. Communications on Pure and Applied Mathematics, 66(2):165–201, February 2013. ISSN 00103640. doi: 10.1002/cpa.21422.  Barra, Adriano, Genovese, Giuseppe, Guerra, Francesco, and Tantari, Daniele. How glassy are neural networks? Journal of Statistical Mechanics: Theory and Experiment, 2012(07):P07009, 2012.  Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des- jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con- ference (SciPy), June 2010. Oral Presentation.  Choromanska, Anna, Henaff, Mikael, Mathieu, Michael, Ben Arous, G´erard, and LeCun, Yann. The  Loss Surface of Multilayer Networks. November 2014.  Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement. Torch7: A matlab-like environment  for machine learning. In BigLearn, NIPS Workshop, 2011.  Dauphin, Yann N, Pascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, Ganguli, Surya, and Ben- gio, Yoshua. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Ghahramani, Z, Welling, M, Cortes, C, Lawrence, N D, and Weinberger, K Q (eds.), Advances in Neural Information Processing Systems 27, pp. 2933–2941. Curran Asso- ciates, Inc., June 2014.  10  Accepted as a workshop contribution at ICLR 2015  Dean, David S and Majumdar, Satya N. Extreme value statistics of eigenvalues of Gaussian doi:  random matrices. Physical Review E, 77(4):041108, April 2008. 10.1103/PhysRevE.77.041108.  ISSN 1539-3755.  Frieze, Alan and Kannan, Ravi. A new approach to the planted clique problem.  In Hariharan, Ramesh, Mukund, Madhavan, and Vinay, V (eds.), IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science, volume 2 of Leibniz International Proceedings in Informatics (LIPIcs), pp. 187–198, Dagstuhl, Germany, 2008. Schloss Dagstuhl– Leibniz-Zentrum fuer Informatik. ISBN 978-3-939897-08-8. doi: http://dx.doi.org/10.4230/ LIPIcs.FSTTCS.2008.1752.  Fyodorov, Yan V. High-Dimensional Random Fields and Random Matrix Theory. pp. 40, July  2013.  Fyodorov, Yan V and Le Doussal, Pierre. Topology Trivialization and Large Deviations for the Minimum in the Simplest Random Optimization. Journal of Statistical Physics, 154(1-2):466– 490, September 2013. ISSN 0022-4715. doi: 10.1007/s10955-013-0838-1.  Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Dark Knowledge. http://www.iro.  umontreal.ca/˜bengioy/cifar/NCAP2014-summerschool/slides/geoff_ hinton_dark14.pdf  Mehta, Dhagash, Hauenstein, Jonathan D, Niemerg, Matthew, Simm, Nicholas J, and Stariolo, Daniel A. Energy Landscape of the Finite-Size Mean-ﬁeld 2-Spin Spherical Model and Topology Trivialization. pp. 1–9, a.  Mehta, Dhagash, Stariolo, Daniel A, and Kastner, Michael. Energy Landscape of the Finite-Size  Mean-ﬁeld 3-Spin Spherical Model. pp. 1–10, b.  Saad, David, Birmingham, B, and Solla, Sara A. Dynamics of On-Line Gradient Descent Learning In Touretzky, D S, Mozer, M C, and Hasselmo, M E (eds.),  for Multilayer Neural Networks. Advances in Neural Information Processing Systems 8, pp. 302–308. MIT Press, 1996.  Sherrington, David. Physics and Complexity: An Introduction. In Delitala, Marcello and Ajmone Marsan, Giulia (eds.), Managing Complexity, Reducing Perplexity, volume 67 of Springer Pro- ceedings in Mathematics & Statistics, pp. 119–129. Springer International Publishing, Cham, 2014. ISBN 978-3-319-03758-5. doi: 10.1007/978-3-319-03759-2.  West, Ansgar H L, Saad, David, and Nabney, Ian T. The Learning Dynamcis of a Universal Ap- proximator. In Mozer, M C, Jordan, M I, and Petsche, T (eds.), Advances in Neural Information Processing Systems 9, pp. 288–294. MIT Press, 1997.  11  ",
1412.7009,2015, Generative Class-conditional Autoencoders,"['Generative Class-conditional Autoencoders', 'Jan Rudy and Graham Taylor']",https://arxiv.org/pdf/1412.7009,"5 1 0 2    r p A 9         ] E N . s c [      3 v 9 0 0 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  GENERATIVE CLASS-CONDITIONAL DENOISING AU- TOENCODERS  Jan Rudy & Graham Taylor School of Engineering University of Guelph Guelph, Ontario, Canada {jrudy,gwtaylor}@uoguelph.ca  ABSTRACT  Recent work by Bengio et al. (2013) proposes a sampling procedure for denoising autoencoders which involves learning the transition operator of a Markov chain. The transition operator is typically unimodal, which limits its capacity to model complex data. In order to perform efﬁcient sampling from conditional distribu- tions, we extend this work, both theoretically and algorithmically, to gated au- toencoders (Memisevic, 2013), The proposed model is able to generate convincing class-conditional samples when trained on both the MNIST and TFD datasets.  1  INTRODUCTION  In the ﬁeld of deep neural networks, purely supervised models trained on massive labeled datasets have garnered much attention over the last few years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014). However, recent work has rekin- dled interest in generative models of data (Bengio et al., 2013; Bengio & Thibodeau-Laufer, 2013; Kingma & Welling, 2014; Goodfellow et al., 2014a). The recently proposed sampling procedure for denoising autoencoders (Bengio et al., 2013) and their generalization to Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) presents a novel training procedure which, instead of attempting to maximize the likelihood of the data under the model, amounts to learning the transition operator of a Markov chain (Bengio et al., 2013). Although these models have shown both theoretically and empirically to have the capacity to model the underlying data generating distribution, the unimodal transition operator learned in (Bengio et al., 2013) and (Bengio & Thibodeau-Laufer, 2013) limits the types of distributions that can be modeled successfully. One way to address this issue is by adopting an alternative generative model such as the Neural Autoregressive Density Estimator (NADE) as the output distribution of the tran- sition operator (Ozair et al., 2013). In this paper, we propose a alternate approach. Our main motivation is that when labeled data is available, we can use the label information in order to carve up the landscape of the data distribution. Although our model is generative, this work shares architectural similarities with discriminative models such as the Hybrid Discriminative Restricted Boltzmann Machine (Larochelle & Bengio, 2008), the gated softmax (Memisevic et al., 2010), and discriminative ﬁne-tuning of class-speciﬁc autoencoders (Kamyshanska & Memisevic, 2013). This work begins by presenting an overview of related work, including a treatment of autoencoders, denoising autoencoders, autoencoders as generative models and gated autoencoders. Next, we pro- pose a class-conditional gated autoencoder along with training and sampling procedures based on the work of Bengio et al. (2013). Finally, we present experimental results of our model on two image-based datasets.  1  Accepted as a workshop contribution at ICLR 2015  2 RELATED WORK  2.1 AUTOENCODERS  An autoencoder is a feed-forward neural network which aims to minimize the reconstruction error of an input data vector via a latent representation. This can be interpreted as the composition of two learned functions, the encoder function f and the decoder function g. The encoder function f is a mapping from input space onto the representation space. Formally, given input vector x ∈ RnX , input weights W ∈ RnH×nX and hidden biases bh ∈ RnH , f (x) = sH (Wx + b)  (1)  where nH is the dimension of the hidden representation and sH is an activation function. The activation is a non-linear function, often from the sigmoidal family (e.g. the logistic or hyperbolic tangent functions) or a piecewise linear function (e.g. rectiﬁed linear). The decoder then projects this representation h = f (x) back onto the input space,  ˆx = g(h) = sO(W(cid:48)h + b(cid:48))  (2) where ˆx is the reconstruction of the input vector, sO is the output activation function, W(cid:48) ∈ RnX×nH are the output weights and b(cid:48) ∈ RnX are the output biases. In order to restrict the num- ber of free parameters of the model, the input and output weight matrices are often ‘tied’ such that W(cid:48) = WT . The model parameters (i.e. weights and biases) are updated via a gradient-based optimization in order to minimize a loss function L(x) based on reconstruction error. The choice of loss function depends on the data domain. When dimensions of x are real-valued, a typical choice of L(x) is the i=1(xi − ˆxi)2. When x is binary, a more appropriate loss function is  squared error, i.e. L(x) =(cid:80)nX the cross-entropy loss, i.e. L(x) =(cid:80)nX  i=1 xi log ˆxi + (1 − xi) log(1 − ˆxi).  2.2 DENOISING AUTOENCODERS  When the dimension of the hidden representation nH is smaller than the dimension of the data space nX, the learning procedure encourages the model to learn the underlying structure of the data. The data representation can exploit structure in order to compress the data to fewer dimensions than the original space. As such, each dimension of the representation space is interpretable as a useful feature of the data. However, when nH ≥ nX, the autoencoder can achieve perfect reconstruction without learning useful features in the data by simply learning the identity function. In this so-called “overcomplete” setup, regularization is essential. Among the various kinds of regularized autoencoders, the de- noising autoencoder (DAE) (Vincent et al., 2008) is among the most popular and well-understood. Instead of reconstructing the data from the actual input vector x, the DAE attempts to reconstruct the original input from an encoding of a corrupted version, f (˜x). This effectively prohibits the model from learning a trivial solution while learning robust features of the data. The corruption procedure is deﬁned as a sample from the conditional distribution ˜x ∼ C(˜x|x). Here, ˜x is a noisy version of x where the type of noise is deﬁned by the distribution C(˜x|x). Typical choices of noise are  1. Gaussian noise - adding (cid:15)i ∼ N (0, σ) to each dimension. 2. Masking noise - setting xi = 0 with probability ρ, 3. Salt-and-pepper noise - similar to masking noise, but corrupting xi with probability ρ and  each corrupted dimension is set to xi = 0 or xi = 1 with probability 0.5.  Apart from preventing the model to learn a trivial representation of the input by acting as a form or regularization, the DAE can be interpreted as a means of learning the manifold of the underlying data generating distribution (Vincent et al., 2008). Under the assumption that the data lies along a low dimensional manifold in the input space, the corruption procedure is a means of moving the training  2  Accepted as a workshop contribution at ICLR 2015  data away from this manifold. In order to successfully denoise the corrupted input, the model must learn to project the corrupted input back onto the manifold. Under this interpretation the hidden representation can be interpreted as a coordinate system of manifolds (Vincent et al., 2008).  2.3 DENOISING AUTOENCODERS AS GENERATIVE MODELS  Although DAEs are useful as a means of pre-training discriminative models, especially when stacked to form deep models (Vincent et al., 2010), recent work by Bengio et al. (2013) has shown that DAEs and their variants locally characterize the data generating density. This provides an important link between DAEs and probabilistic generative models. We deﬁne observed data x such that x ∼ P(x) where P(x) is the true data distribution and we deﬁne C(˜x|x) as the conditional distribution of the corruption process. When such models are trained using a loss function that can be interpreted as log-likelihood, by predicting x given ˜x, the model is learning the conditional distribution Pθ(x|˜x) (where θ represents the parameters of the model). In order to generate samples from the model, one simply forms a Markov chain which alternately samples from learned distribution and the corruption distribution (Bengio et al., 2013). Where xt is the state of the Markov chain at time t, then xt ∼ Pθ(x|˜xt−1) and ˜xt ∼ C(˜x|xt). In other words, samples can be generated by alternating between the corruption process and the denoising function learned by the autoencoder. Notice that this is not a true Gibbs sampling procedure, as (xt, ˜xt−1) may not share the same asymp- totic distribution as (xt, ˜xt). Regardless, theoretical results indicate that under some conditions elaborated on in Sec. 3, the asymptotic distribution of the generated samples converges to the data- generating distribution (Bengio et al., 2013). Although the above procedure does produce convincing samples, it will also generate spurious ex- amples that lie far from any of the training data. This is because the training procedure does not sufﬁciently explore the input space. Under the manifold interpretation described above, the corrup- tion procedure deﬁnes a region around each example that is explored during training, the size of which is determined by the amount of the corruption (e.g. σ in the case of Gaussian noise and ρ for masking or salt and pepper noise). This can leave much of the input space unexplored, allowing the model to place appreciable amounts of probability mass (i.e. spurious modes) in regions of the space that lie far from any training example. One solution involves using large or increasing amounts of noise during training, however this results in a naive search of the space. A more efﬁcient procedure called “walkback training” is described in (Bengio et al., 2013) and bears resemblance to contrastive divergence training of restricted Boltzman machines. Instead of deﬁning the loss as the reconstruction cost of a single step of corruption and reconstruction chain, walkback training deﬁnes a series of k reconstructions via a random walk orig- inating at the training example. Each reconstruction is corrupted and subsequently reconstructed, where the ﬁnal cost is deﬁned as the sum of the reconstruction costs of each intermediate recon- struction. Since the training procedure mimics that of the sampling procedure, walkback training is a means of seeking out these spurious modes and redistributing their probability mass back towards the manifold.  2.4 GATED AUTOENCODERS  Gated autoencoders (GAE), also called Relational autoencoders, are an extension the autoencoder framework which learn relations on input-output pairs x ∈ RnX given y ∈ RnY (Memisevic, 2013). Instead of deﬁning a ﬁxed weight matrix W ∈ RnH×nX , the GAE learns a function w(y) where the model weights are modulated by the value of the conditioning variable y. The naive implementation involves constructing a weight tensor W ∈ RnH×nX×nY and deﬁning  nY(cid:88)  wij(y) =  Wijkyk  where the subscripts indicate indexing. Under this model, the encoder in Equation 1 becomes  h = f (x, y) = sH (w(y)x + b)  k=1  3  (3)  (4)  Accepted as a workshop contribution at ICLR 2015  Figure 1: Illustration of a typical DAE (a) and GAE (b) model each with a single hidden layer. Biases have been omitted for clarity.  However, this requires storing and learning (nX × nY × nH ) model parameters. In practice, this is infeasible for all but the smallest problems as the number of weights is cubic in the number of units (assuming nX, nY and nH are roughly equal). However, we can restrict the interactions between x, y, and h by ﬁrst projecting each onto factors F X , F Y , F H ∈ RnF and allowing only element-wise interaction between the factors. Instead of a quadratic number of weights needed in the naive method above, the factored model is parameterized via three weight matrices: WX ∈ RnX×nF , WY ∈ RnY ×nF and WH ∈ RnF ×nH . The hidden representation under the factored model is deﬁned as  (cid:16)(cid:0)WH(cid:1)T(cid:0)WX x ⊗ WY y(cid:1) + bH(cid:17)  h = f (x, y) = sH  (5) where ⊗ denotes elementwise multiplication and bH ∈ RnH is the hidden bias. Notice that the encoder is a function over both x and y. Similarly, the decoder function will also be over two input variables, the choice of which being dependant on which of x and y are to be reconstructed. When learning a conditional model, one of the input variables will be held ﬁxed. For example, the reconstruction of x given y is deﬁned as  (cid:16)(cid:0)WX(cid:1)T(cid:0)WH h ⊗ WY y(cid:1) + bX(cid:17)  ˆx = g(h, y) = sO  (6) where bH ∈ RnX is the output bias and sO is the output activation function. Equations 5 and 6 describe a symmetric model where the encoder and decoder share the same set of weight matrices. However, this is not a hard requirement and various regimes of tied vs untied weights for gated models have been explored by Alain & Olivier (2013). Like the traditional autoencoder, the GAE is typically trained with a denoising criterion where the noise is applied to both x and y inputs. Yet where a traditional autoencoder learns features of the input, a GAE can learn relationships between its two inputs. For example, when trained on pairs of images where one is translated version of the other, the ﬁlters learned by the input factors resemble phase-shifted Fourier components (Memisevic, 2013). The loss function is deﬁned in much the same way as the classical autoencoder described above, using squared error on real-valued inputs and cross entropy loss where the input values are binary. As all operations are differentiable, the model can be trained via stochastic gradient descent while making use of any optimization techniques (e.g. momentum, adaptive learning rates, RMSprop, etc.) that have been developed for neural network training.  4  (a)(b)^^Accepted as a workshop contribution at ICLR 2015  3 GATED AUTOENCODERS AS CLASS-CONDITIONAL GENERATIVE MODELS  The sampling procedure proposed by Bengio et al. (2013) for classical denoising autoencoders can also be applied to a GAE. Here, we deﬁne the true data distribution as the conditional distribution P(x|y). Here x ∈ RnX is an input data point and y ∈ RnY is the associated class label in a ‘one- hot’ encoding. Although GAE training typically applies noise to both x and y, we will examine the case where the corruption procedure is applied to x only. Thus, the corruption distribution is the same as in the DAE framework, namely the noise procedure draws samples from C(˜x|x). By choosing a loss function which is interpretable as log-likelihood, the GAE learns the conditional distribution Pθ(x|˜x, y) Like the sampling procedure for DAEs, the Markov chain formed by alternating samples from xt ∼ Pθ(x|˜xt−1, y) and ˜xt ∼ C(˜x|x) will generate samples from the true distribution P(x|y). During training, we can also apply a class-conditional version of the walkback training algorithm to seek out and squash any spurious modes of our model. Bengio et al. (2013) provide a proof of the following theorem: Theorem 1. If Pθ(x|˜x) is a consistent estimator of the true conditional distribution P(x|˜x) and Tn deﬁnes an ergodic Markov chain, then as the number of examples n → ∞, the asymptotic distribution πn(x) of the generated samples converges to the data-generating distribution P(x). For the conditional case of GAE the same arguments hold, where each of the following substitutions are made: Pθ(x|˜x) with Pθ(x|˜x, y), P(x|˜x) with P(x|˜x, y) and P(x) with P(x|y). The arguments for consistency and ergoticity can also be made in the same manner as those in (Bengio et al., 2013). Geometrically, this model can be seen as learning a conditional manifold of the data. Like the DAE, the model learns to correct the noisy input by pushing it back towards the data manifold. However, the model makes use of the class labels in order to learn a separate manifold for each class label. The F Y factors, via their multiplicative interactions, scale the features learned by the F X factors depending on the class. It is akin to learning a DAE for each class, yet the gated model can make use of cross-class structure by sharing weights between each of the class-speciﬁc models. When training on images of hand written digits, for example, the ‘tail’ of a 4, 7, or 9 may all make use of the same feature learned by one of the F X factors. As such, the F Y factor for these classes would have a relatively large value for this shared feature. However, this tail feature may not be useful for reconstruction of a 3, 8 or 6. For these classes, the F Y values that correspond to the tail feature can learn to down-weight the importance of this tail feature. Under this interpretation, the F Y factors are a means of weighting each of the F X factors dependent on the conditioning class label.  4 EXPERIMENTS  We demonstrate the generative properties of the class-conditional GAE model on two datasets: bi- narized MNIST (LeCun & Cortes, 1998) and the Toronto Face Database (TFD) (Susskind et al., 2010). The MNIST database of handwritten digits consists of 60,000 training examples and 10,000 test examples. Pixel intensities of each 28 × 28 image have been scaled to [0, 1] and thresholded at 0.5, such that each pixel takes on a binary value from {0, 1}. For the MNIST dataset, a GAE was trained with 1024 factors and 512 hidden units. The hidden units used a rectiﬁed linear (ReLU) activation function, while the visible activation was the logistic (i.e. sigmoid) function. Using a cross entropy loss, the model was trained for 200 epochs via mini- batch gradient descent with a batch size of 100. The initial learning rate of 0.25 was decreased multiplicatively at each epoch by a factor of 0.995. For optimization, the model was trained using Nesterov’s accelerated gradient (Sutskever et al., 2013) with a parameter of 0.9. Salt and pepper noise was applied during training where each pixel has a 0.5 probability of corruption. Training followed the walkback training procedure, with the training loss averaged over 5 reconstructions from the Markov chain starting at the training example. Figure 2 shows 250 consecutive samples generated while conditioning on each class label. Note that the ‘samples’ depicted show the expected value of each pixel for each step of the Markov chain deﬁned by the sampling procedure. Each chain was initialized to a vector of zeros and at each step,  5  Accepted as a workshop contribution at ICLR 2015  Figure 2: Consecutive samples generated by the Markov chain learned by the class-conditional GAE model trained on binarized MNIST.  the image was corrupted with 0.5 salt-and-pepper noise. The model begins to generate convincing samples after only a few steps of the chain. Also notice that the chain exhibits mixing between modes in the data distribution. For example, the samples from the ‘2’ class contains both twos with a loop and a cusp. Finally, notice that the samples contain very few spurious examples – i.e. most samples resemble recognizable digits. The TFD consists of 4,178 expression-labeled images of human faces. Each image is 48 × 48 pixels with 256 integer value levels of gray intensity. The intensity values were linearly scaled to the range [0, 1]. Unlike the MNIST experiment, the intensity values were not binarized. A GAE with 512 factors and 1024 ReLU hidden units and sigmoid outputs was trained on squared- error loss for 500 epochs. Again, we used mini-batch gradient descent, however, a mini-batch size of 50 was used with a learning rate of 1.0 annealed by a factor of 0.995 at each epoch. Training followed 5 walkback steps with a Nesterov momentum of 0.9. The model was trained and sampled using salt-and-pepper noise with probability of 0.5.  6  Accepted as a workshop contribution at ICLR 2015  When training on MNIST, each walkback reconstruction was then sampled using the sigmoid ac- tivation as the probabilities of a factorized Bernoulli distribution. Since the TFD faces were not binarized, the sigmoid activations were used instead. Figure 3 shows 150 consecutive samples for each class label, each starting at the zero vector. Notice that there is less variation in the TFD samples than those from the MNIST model. This is likely due to the relative small size of the TFD training set (4,178 TFD training cases vs. 60,000 MNIST training cases). The TFD dataset also provides 112,234 unlabeled examples and it may be possible to pretrain the F X factor weights on this unlabeled data. However, these experiments are beyond the scope of the current analysis.  5 CONCLUSION  The class-conditional GAE can be interpreted as a generative model in much the same ways as the DAE. In fact, the GAE is akin to learning a separate DAE model for each class, but with signiﬁcant weight sharing between the models. In this light, the gating acts as a means of modulating the model’s weights depending on the class label. As such, the theoretical and practical consideration that have been applied to DAE’s as generative models can also be applied to gated models. Future work will apply these techniques to richer conditional distributions, such as the task of image tagging as explored by Mirza & Osindero (2014).  REFERENCES Alain, Droniou and Olivier, Sigaud. Gated autoencoders with tied input weights. In Proceedings of  The 30th International Conference on Machine Learning, pp. 154–162, 2013.  Bengio, Yoshua and Thibodeau-Laufer, ´Eric. Deep generative stochastic networks trainable by  backprop. arXiv preprint arXiv:1306.1091, 2013.  Bengio, Yoshua, Yao, Li, Alain, Guillaume, and Vincent, Pascal. Generalized denoising auto- encoders as generative models. In Advances in Neural Information Processing Systems, pp. 899– 907, 2013.  Dahl, George E, Marc’Aurelio Ranzato, Abdel-rahman Mohamed, Mohamed, Abdel-rahman, and Hinton, Geoffrey E. Phone recognition with the mean-covariance restricted boltzmann machine. In NIPS, pp. 469–477, 2010.  Deng, Li, Seltzer, Michael L, Yu, Dong, Acero, Alex, Mohamed, Abdel-Rahman, and Hinton, Ge- offrey E. Binary coding of speech spectrograms using a deep auto-encoder. In Interspeech, pp. 1692–1695. Citeseer, 2010.  Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sher- jil, Courville, Aaron, and Bengio, Yoshua. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672–2680, 2014a.  Goodfellow, Ian J, Bulatov, Yaroslav, Ibarz, Julian, Arnoud, Sacha, and Shet, Vinay. Multi-digit number recognition from street view imagery using deep convolutional neural networks. In ICLR, 2014b.  Kamyshanska, Hanna and Memisevic, Roland. On autoencoder scoring. In Proceedings of the 30th  International Conference on Machine Learning (ICML-13), pp. 720–728, 2013.  Kingma, Diederik P and Welling, Max. Auto-encoding variational bayes. In Proceedings of the  International Conference on Learning Representations (ICLR), 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep con-  volutional neural networks. In NIPS, volume 1, pp. 4, 2012.  Larochelle, Hugo and Bengio, Yoshua. Classiﬁcation using discriminative restricted boltzmann machines. In Proceedings of the 25th international conference on Machine learning, pp. 536– 543. ACM, 2008.  7  Accepted as a workshop contribution at ICLR 2015  Figure 3: Consecutive samples generated by the Markov chain learned by the class-conditional GAE model trained on TFD images. From left to right, top to bottom, the classes are: anger, disgust, fear, happiness, sadness, surprise, neutral.  8  Accepted as a workshop contribution at ICLR 2015  LeCun, Yann and Cortes, Corinna. The mnist database of handwritten digits, 1998.  Memisevic, Roland. Learning to relate images. Pattern Analysis and Machine Intelligence, IEEE  Transactions on, 35(8):1829–1846, 2013. ISSN 0162-8828. doi: 10.1109/TPAMI.2013.53.  Memisevic, Roland, Zach, Christopher, Pollefeys, Marc, and Hinton, Geoffrey E. Gated softmax classiﬁcation. J., Zemel, R.S., and Culotta, A. (eds.), Advances in Neural Information Processing Systems 23, pp. 1603–1611. Curran Associates, Inc., 2010. URL http://papers.nips.cc/paper/ 3895-gated-softmax-classification.pdf.  J.D., Williams, C.K.I., Shawe-Taylor,  In Lafferty,  Mirza, Mehdi and Osindero, Simon. Conditional generative adversarial nets. In NIPS 2014 Work-  shop on Deep Learning, 2014.  Ozair, Sherjil, Yao, Li, and Bengio, Yoshua. Multimodal transitions for generative stochastic net-  works. arXiv preprint arXiv:1312.5578, 2013.  Susskind, Josh M, Anderson, Adam K, and Hinton, Geoffrey E. The toronto face database. Depart-  ment of Computer Science, University of Toronto, Toronto, ON, Canada, Tech. Rep, 2010.  Sutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initial- ization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 1139–1147, 2013.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.  Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096–1103. ACM, 2008.  Vincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 9999:3371–3408, 2010.  9  ",
1412.7054,2015, Attention for Fine-Grained Categorization,"['Attention for Fine-Grained Categorization', 'Pierre Sermanet', 'Andrea Frome', 'and Esteban Real']",https://arxiv.org/pdf/1412.7054,"5 1 0 2    r p A 1 1         ]  V C . s c [      3 v 4 5 0 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  ATTENTION FOR FINE-GRAINED CATEGORIZATION  Pierre Sermanet, Andrea Frome, Esteban Real Google, Inc. {sermanet,afrome,ereal,}@google.com  ABSTRACT  This paper presents experiments extending the work of Ba et al. (2014) on recur- rent neural models for attention into less constrained visual environments, specif- ically ﬁne-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments, whereas we present results for ﬁne-grained cat- egorization better than the state-of-the-art GoogLeNet classiﬁcation model. We show that our model learns to direct high resolution attention to the most discrim- inative regions without any spatial supervision such as bounding boxes, and it is able to discriminate ﬁne-grained dog breeds moderately well even when given only an initial low-resolution context image and narrow, inexpensive glimpses at faces and fur patterns. This and similar attention models have the major advantage of being trained end-to-end, as opposed to other current detection and recognition pipelines with hand-engineered components where information is lost. While our model is state-of-the-art, further work is needed to fully leverage the sequential input.  1  INTRODUCTION  This work presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, speciﬁcally ﬁne-grained categorization on the Stanford Dogs data set. Ba et al. (2014) tackles the challenging problem of sequence prediction in simpliﬁed visual settings (MNIST and Street View House Numbers) using a recurrent model of attention similar to Mnih et al. (2014). Complementary to that work, we are addressing the simpler task of classiﬁcation but in a visual environment with signiﬁcant clutter and occlusion, variations in lighting and pose, and a more difﬁcult class discrimination task. Previous work in learned visual attention models has tackled a number of computer vision problems and demonstrated the beneﬁts of various attention mechanisms, though most of the work has been focused on toy or more constrained environments, such as detecting simple shapes (Schmidhuber & Huber, 1991), tasks based on MNIST digits (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014), the vision-control game of “catch” (Mnih et al., 2014), expression classiﬁcation for 100×100 aligned faces (Larochelle & Hinton, 2010; Zheng et al., 2014), detection of frontal faces (Tang et al., 2013), tracking of hockey players (Bazzani et al., 2011; Denil et al., 2012) and gesture recognition (Darrell & Pentland, 1996). Most recently, two papers have explored attention mechanisms for more complex visual input: Gonzalez-Garcia et al. (2014) presented an attention-like model for detection of chairs, people, cars, doors, and tables from SUN2012 images (Xiao et al., 2010); Xu et al. (2015) applied two different neural network visual attention models to the task of caption generation for MSCOCO images (Lin et al., 2014). Recent work in object detection (Szegedy et al., 2014b; Girshick et al., 2014) and ﬁne-grained cat- egorization (Zhang et al., 2014) use candidate object or part proposals as a precursor to a more expensive classiﬁcation stage. In these systems, the proposals are generated from a bottom-up seg- mentation as in Girshick et al. (2014) or from a separate neural network as in Szegedy et al. (2014b). As in our work, these pipelines are able to process candidate regions for classiﬁcation at a higher res- olution and save processing by focusing on a restricted set of regions. Drawbacks to these systems are that they consist of independent processing steps with engineered connections between them  1  Accepted as a workshop contribution at ICLR 2015  where information is lost. As an example, they either do not aggregate evidence from independent candidate image regions or do so with an ad hoc technique. They also lose rich information between the candidate proposal and classiﬁcation parts of the pipeline. In contrast, our models and others with similar structure are trained end-to-end to incorporate information across observations, which can be expected to yield a better ﬁnal result.  Figure 1: Three classes from the Dogs data set that are difﬁcult to tell apart due to high intra-class variability and high similarity across classes. The lines show the class boundaries; the classes are Eskimo Dog on the left, Malamute in the center, and Siberian Husky on the right. Of the 120 classes in Stanford Dogs, our model performs worst on Siberian Husky.  We apply the visual attention model from Ba et al. (2014) to the Stanford Dogs ﬁne-grained catego- rization task (Khosla et al., 2011), choosing to perform the task without using the provided bounding boxes for training or testing. This amounts to learning to simultaneously localize and classify ob- jects within scenes despite difﬁcult class boundaries, large variations in pose and lighting, varying and cluttered backgrounds, and occlusion (Figure 1). Fine-grained categorization is a natural prov- ing ground for attention-based models. When performing classiﬁcation at the sub-category level, e.g. German Shepherd versus Poodle, the background is often uncorrelated with class and acts as a distraction to the primary task. As a result, several hand-crafted vision pipelines use provided bounding boxes to isolate the object of interest or may perform segmentation of the object from the background, e.g. Parkhi et al. (2011); Chai et al. (2013); Angelova & Zhu (2013). Attention models could address this challenge by learning to focus processing and discriminatory power on the parts of the image that are relevant for the task without requiring expensive hand-labeled bounding boxes. In addition to ignoring the distractors in the image, a good attention model could learn to focus processing power on the speciﬁc features of the objects that help to tell them apart, for example the face, ears, and particular fur patterns for dogs. Future versions of this model could potentially also choose the scale at which to examine details.  2 MODEL DESCRIPTION  The structure of our model is nearly the same as that presented in Ba et al. (2014) with a few differences; we give an overview of their model here and describe the ways in which our model differs. We refer the reader to that work for a more in-depth description of the network choices and training procedure. Figure 2 shows the structure of the model. The system as a whole takes as input an image of any size and outputs N-way classiﬁcation scores using a softmax classiﬁer, which is a similar task to the ﬁnding digits and digit addition tasks in Ba et al. (2014). The model is a recurrent neural network, with N steps that correlate with N “glimpses” into the input image. At step n, the model receives row and column coordinates ln−1, which describe a point in the input image. The network extracts a multi-resolution patch from the input image at those coordinates, passes the pixels through fully-  2  Accepted as a workshop contribution at ICLR 2015  Figure 2: Diagram of the model. The grayed-out boxes denote resolutions not in use; in our exper- iments the context is always a low-resolution patch, while each glimpse can be any combination of the low-, medium-, and high-resolution patches.  connected layers which combine with activations from the previous glimpse step, and either outputs coordinates ˆln for the next glimpse or a ﬁnal classiﬁcation ys. The structure of the glimpse images is shown in Figure 3. Each glimpse is a multi-resolution image created by extracting patches at varying sizes, resizing them all to 96×96, and concatenating them side-by-side. The goal is to emulate a “foveal” structure with the sharpest part of the image in the center and lower resolution toward the periphery. The top row shows glimpses for a 2-resolution model and the bottom row for a 3-resolution model. The high-resolution patch is extracted from a square that is a ﬁxed size for a given image (more on scale selection below). The medium-resolution patch is from a square that is twice the length on a side of the high-resolution patch, and the low- resolution patch is twice the length of the medium-resolution patch on a side. For example, if the high resolution patch is 100×100, then the medium- and low-resolution patches are 200×200 and 400×400, respectively. Where an extraction box extends off the edge of the image, we ﬁll the pixels with random noise. Figure 3 shows composite images which are a helpful visualization to understand what pixels the network sees in aggregate, though the network is not presented with them in this form; these images are generated from the glimpse pixels by displaying at each pixel the highest-resolution pixel available in any glimpse, and any pixels not captured by the glimpses are ﬁlled with noise. The model begins processing from a “context image”, which is a square low-resolution patch from the input image that is the same size as our low-resolution glimpse patch and is also resized to 96×96. The location of the context image is chosen randomly in training, but it is centered during inference when it captures the central square of the image. The context image is used by layer r(2) to produce the ﬁrst glimpse location l0 and can inﬂuence the selection of subsequent glimpse o locations through the recurrent connections between the r(2) layers along the “top deck” of the n model. However, the double-decker structure prevents the context image from having a pathway to the classiﬁer except through the ln coordinates. See Ba et al. (2014) for more discussion about this design choice. In training, the r(2) n layers that produce the ˆln coordinates are trained with a mix of backpropagation from the connection to layer r(1) There are four major differences between our system and the classiﬁcation-type models from Ba et al. (2014). First, there is wide variation in image size across our data set, however the size of the objects scales with the image size most of the time. To be robust to input image size, our multi-resolution patches are sized relative to the input image. In our experiments, a side of the high-  n+1 and a policy gradient update.  3  rn(2)rn(1)contextIcoarsel3r3(1)r3(2)l2r2(1)r2(2)l1r1(1)r1(2)l0l0＾＾＾＾r0(2)emissionysclassification≀≀GoogLeNetGoogLeNetGoogLeNetGoogLeNetinput imageGoogLeNetl1l2ln-1foveal glimpses:Accepted as a workshop contribution at ICLR 2015  (a)  (b)  Figure 3: Visualizations of 2-resolution (a) and 3-resolution (b) glimpses on an image from our validation set, with learned ﬁxation points. For each the glimpse images are in order, from top to bottom, and the box diagram corresponds to the second glimpse. The composite image is created from all three glimpses. The context image is not shown but is always the same resolution and size as the low-resolution glimpse patches shown in (b).  resolution square patch is 1/4 the shorter dimension of the input image, making the sides of the medium- and low-resolution patches 1/2 and the full length of the image’s short side, respectively.  n and r(2)  Second, we use a “vanilla” RNN instead of an LSTM, where r(1) n at glimpse n each consist of 4,096 nodes, and rn(i) is fully-connected to rn+1(i) for i = 1, 2. Third, instead of element-wise multiplying the outputs of the glimpse visual core Gimage(xn|Wimage) and Gloc(ln|Wloc), our model linearly combines them by concatenating their outputs and passing through a fully-connected layer. Future experiments will incorporate both of these variations. The ﬁnal and largest difference is that we replace the visual glimpse network Gimage(xn|Wimage) described in Ba et al. (2014) with a more powerful visual core based on the “GoogLeNet” model (Szegedy et al., 2014a) that won the ILSVRC 2014 classiﬁcation challenge. We start from a GoogLeNet model that we pre-trained on the ImageNet 1000-way task on a subset of the ILSVRC 2012 training data (Russakovsky et al., 2014) (more on our use of this data set below). We then ﬁne-tune the visual network outside of the RNN model on ILSVRC images using random multi-scale patches as input and targeting the ImageNet 1000-way classiﬁcation labels. In this stage of training, we replicate the visual model for each input scale, yielding 3 “towers” which share parameters and join their outputs in different combinations with depth-concatenating layers (Figure 4). All towers are jointly trained by back-propagating the loss from multiple 1000-way softmax classiﬁers (called “heads”) as shown in the ﬁgure. This multi-headed training model ensures each tower remains independently relevant even if another tower is more informative. We have found that if taken independently, the lowest-resolution patch typically yields best results and learning might rely on it solely otherwise. We initially used a truncated version of GoogLeNet as the visual core of the RNN because the full model is designed for 224×224 inputs and if applied to 96×96 inputs, the subsampling and pooling layers cause the ﬁnal output to be too small for our purposes. To remedy this, we initially chopped off the last two “inception” layers, skipping ﬁve convolutional layers and an average-pooling layer. We later discovered a large gain in performance by changing the stride of the ﬁrst convolution of the network from two to one and restoring the visual network to its full depth. Stride-1 convolutions were historically used in early deep learning works, however Krizhevsky et al. (2012) later popular- ized strides of two in early layers for efﬁciency reasons. In our experiments, we changed the stride only in the ﬁne-tuning phase, starting from a pre-trained model with a stride of two. Although it is not obvious that changing the stride of a pre-trained model should work, when incorporated with the  4  Accepted as a workshop contribution at ICLR 2015  Figure 4: Pre-training of the visual core (left) and inference within the RNN (right).  RNN, the best performance of the three-resolution, three-glimpse attention model increased from 68% to 76.8%. During training of the attention model, we remove all training heads and take the output of the depth concatenation of multiple towers as glimpse input as shown in Figure 4. For this work, we hold the visual core’s parameters ﬁxed during attention model training. We pre-train with all three resolutions, but when used in the RNN, we vary across experiments which subsets of the resolutions are used, so the training regime in those experiments is slightly different than testing. In all stages of training for the attention RNN and our experimental baselines, we use a subset of the ILSVRC 2012 training set from which we removed the Stanford Dogs test images as well as the Dogs images that we use for validation. We refer to this in our experimental section as the “de-duped” ILSVRC data set. Pre-training with de-duped data instead of the original set does make a small difference in performance: we saw a drop of 3% accuracy in the full GoogLeNet baseline model when trained with de-duped data relative to one trained with the full ILSVRC 2012 training set. Finally, it is worth noting that when ﬁne-tuning the visual core, we did not use the Stanford Dogs training set, and since the parameters of the visual core are held ﬁxed while training the RNN on Dogs, this means the powerful visual processing component in the RNN is not trained on the ﬁnal task. We performed an experiment with a visual core ﬁne-tuned on the Stanford Dogs training data, and we did not see an increase in performance, demonstrating again that the ﬁnal RNN model is fairly robust to the pre-training and ﬁne-tuning procedure.  3 EXPERIMENTAL RESULTS  We trained and evaluated our model on the Stanford Dogs ﬁne-grained categorization data set (Khosla et al., 2011). The task is to categorize each of 8,580 test images as containing one of 120 dog breeds. The training set consists of 100 images per class, and the test images are un- evenly distributed across classes, averaging about 71 test images per class. The training and test sets both include bounding boxes that provide a tight crop around the target dog, and while the best re- sults we know of in the literature use the bounding boxes both in training and testing, we use neither the training nor testing boxes. We follow the practice common in the literature of augmenting the training set by reﬂecting the images along the vertical axis. Our model starts from the full images, without cropping, reshaping, or scaling. We performed experiments and chose hyperparameters us- ing an 80/20 training/validation split of the Stanford Dogs training set. We selected hyperparameters (e.g. learning rate, sample variance) using the 20% validation set then trained on the full training set, and we only performed ﬁnal evaluation on the Dogs test set on our selected hyperparameters. The background in the images is not highly correlated with the class label, so any method not using the bounding boxes needs to localize the object of interest in order to classify it. This is a nice task to explore for our attention model in a couple ways: (1) the model can use the context image in order to focus its glimpses on the object of interest, and (2) we can intuit which parts of the image the model  5  Accepted as a workshop contribution at ICLR 2015  Table 1: Results on Stanford Dogs for (a) our RNN model and (b) our GoogLeNet baselines and previous state-of-the-art results, measured by mean accuracy percentage (mA) as described in Chai et al. (2013). The GoogLeNet baseline models were pre-trained on the de-duped ILSVRC 2012 training set and ﬁne-tuned with the Stanford Dogs training set. Results marked with a star indicate use of tight ground truth bounding boxes around the dogs in training and testing.  # glimpses high res only medium res only low res only high+medium res 3-resolution  1  43.5 70.1 70.3 70.7 76.3  (a)  2  48.3 72.3 70.1 72.6 76.5  3  49.6 72.8 70.7 72.7 76.8  Yang et al. (2012)* Chai et al. (2013)* Gavves et al. (2013)* GoogLeNet 96×96 GoogLeNet 224×224  (b)  38.0 45.6 50.1 58.8 75.5  should observe to make a prediction. With many other natural image object classiﬁcation data sets, such as ImageNet, the signal from the surrounding context is mixed with the object for classiﬁcation (e.g. boats are expected to be surrounded by water). The size of the data set is also more suitable to a deep learning method than most other available ﬁne-grained data sets, though Caltech-UCSD Birds 2011 (Wah et al., 2011) is similar in size with 12,000 training images for 200 categories.1 Lastly, it remains a difﬁcult data set, with a large amount of intra-class variation, similarity across classes, and large variation in pose, lighting, and background (Figure 1). Table 1(a) shows the mean accuracy (mA) for different combinations of resolutions and number of glimpses. We experimented with high, medium, and low resolutions individually, medium and high combined, and all three resolutions and with one, two, and three glimpses. The table also shows previously published results on the data set2. Versions of our model that use medium and low resolution patches outperform state-of-the-art results. When using only two small high-resolution patches from the image, our model matches the best published result. All previously published results shown use ground truth bounding boxes for training and testing while our model does not. While the high-resolution single-glimpse model has the lowest performance of the set, visualizations of the selected glimpse locations show that it is learning to take a good ﬁrst action (Figure 5). These are fairly representative of the behavior of the model, which most frequently chooses a patch on or near the dog’s face. While it may make an informative ﬁrst glimpse, it is often not able to correctly classify the dog from that single sample. It is important to note that the model automatically learned to focus on the most discriminative features such as faces and fur without ever receiving spatial clues such as bounding boxes. This is pretty remarkable in that bounding boxes are usually required for good performance on this task, and obtaining bounding boxes at scale is difﬁcult and expensive. It also raises the possibility of attention models providing a signal for detection without labeled bounding boxes. The ﬁgure shows two images where it classiﬁed the dog correctly in green, and one in red where it assigned an incorrect label. One pathological pattern we noticed is that if there are two dogs in the image, it often chooses a patch that is halfway between the two, which is likely due to the regression-style output of the glimpse coordinates which may encourage the model to output the average of two predicted targets. A set of randomly chosen examples is shown in the appendix with Figure 6. Comparisons to results not using deep learning do not give a good sense of the strength of the model, however. In the last couple years deep nets have been winning the ILSVRC classiﬁcation challenge by a signiﬁcant margin, so it may be expected that a deep neural net would outperform the existing results. To address this we also evaluated GoogLeNet on the full image without the attention  1We have not yet experimented with Caltech-UCSD Birds 2011, but the approach here should apply nicely. 2Missing from the results are entries to the FGComp 2013 ﬁne-grained competition. There are high- performing entries from deep learning models in the dogs category, though to our knowledge these models have not been published. CognitiveVision and CafeNet scored 61% and 57% on the challenge, respectively, us- ing bounding boxes both in training and testing. The challenge training set is from Stanford Dogs, but the test set is independent, the class labels have not been made public, and the evaluation server is no longer running. As such, we cannot compare directly to these numbers, but we have been told anecdotally that scores on the FGComp 2013 challenge tend to be about 10% absolute lower than on the Stanford Dogs test set.  6  Accepted as a workshop contribution at ICLR 2015  (a)  (b)  (c)  Figure 5: Selected examples of the high-resolution, one-glimpse model run on the validation set. The system takes (a) the original image, subsamples it to create (b) the context image, and uses the context to select (c) a single high resolution glimpse. The outline of the high-resolution glimpse is also shown on the full image in (a) for context. Rows surrounded by green were correctly classiﬁed, while red indicates a classiﬁcation error. Note that while the bottom example is misclassiﬁed, the model still learned to look at the dog face despite clutter, occlusion, and an uncommon pose. See Figure 6 for a random selection of results.  7  Accepted as a workshop contribution at ICLR 2015  RNN. We experimented with two baseline versions of GoogLeNet: “full” and “low-resolution”. The full GoogLeNet uses the same architecture as in Szegedy et al. (2014a) and is trained and tested on 224×224 padded versions of the full Dog images. This is the strongest of the two baselines. The low-resolution GoogLeNet has the same architecture as our RNN visual core and also takes 96×96 inputs and uses a stride of one for the ﬁrst convolution. It does not have three input scales like the RNN visual core but instead takes the full image with padding that centers it. The low- resolution GoogLeNet input is close in resolution to the low-resolution foveal input to our attention model. Both versions were pre-trained using the de-duped ILSVRC 2012 data set, then the top fully- connected layer and softmax layers were resized and reinitialized, and the full model was trained to convergence on the Dogs training set. In addition to the mirroring applied for RNN training, brightness and color transformations were also applied to the training images for the baselines. Unlike Szegedy et al. (2014a), for this comparison we did not average across patches or different GoogLeNet models. Our three-resolution, one-glimpse attention model reached 76.3% mA compared to 75.5% for the full GoogLeNet model (1(b)). This version of the attention model gets three 96×96 inputs it can use for classiﬁcation and the 96×96 context image for choosing the position of the one glimpse. Compared to the 224×224 input to GoogLeNet, we perform better with 73% of the pixels. This doesn’t reduce the amount of computation, however, because the attention model uses a stride of one in the ﬁrst convolutional layer compared to two in the full GoogLeNet. With three glimpses, the performance increases slightly to 76.8%, though this almost triples the number of pixels input3. With only medium and high resolution inputs (72.7%, three glimpses), the attention model is nearing the performance of full GoogLeNet with 55% of the pixels. A comparison between the low-resolution GoogLeNet (58.8%) and our low resolution attention model (70.3%, one glimpse) shows that the increase in convolution stride does not account for the strong performance because both models share the same visual network and similar resolution inputs (depending on ratio of the long and short sides of the image). This indicates the large difference in performance is due to the network choosing an informative part of the image to use as input for classiﬁcation. Lastly, it is interesting to compare one-, two-, and three- glimpse results for different resolution in- puts. Using three resolutions, the performance only increases slightly from 76.3% for one glimpse to 76.8% for three. One likely cause is that the three-resolution glimpse contains enough information from the full image that the information gained from additional glimpses is minimal. An additional piece of evidence is that the performance order of low- and medium-only resolution models swap when going from one to two glimpses. We ran the high resolution-only glimpse experiment to test this; the results for one, two, and three glimpses are 43.5%, 48.3%, and 49.6%, respectively, demonstrating that when the amount of information in each glimpse is restricted, the model ben- eﬁts more from several glimpses. However, the improvement with increased number of glimpses ﬂattens quickly, indicating that the model has limited capacity to make use of more than two or three glimpses. One hypothesis is the RNN is not passing enough of the information from the early glimpses along to the classiﬁcation layer. It is future work to explore using LSTM cells and increas- ing the recurrent capacity of the network.  REFERENCES Angelova, Anelia and Zhu, Shenghuo. Efﬁcient object detection and segmentation for ﬁne-grained recognition. In CVPR’13: IEEE Conference on Computer Vision and Pattern Recognition, June 23-28, 2013. doi: 10.1109/CVPR.2013.110.  Ba, Jimmy, Mnih, Volodymyr, and Kavukcuoglu, Koray. Multiple Object Recognition with Visual  Attention. CoRR, TBD, 2014. URL http://arxiv.org/abs/1412.7755.  Bazzani, Loris, de Freitas, Nando, Larochelle, Hugo, Murino, Vittorio, and Ting, Jo-Anne. Learning attentional policies for object tracking and recognition in video with deep networks. In ICML’11: Proceedings of the 28th International Conference on Machine Learning, pp. 937–944. ACM, 2011.  3For a fairer comparison between multiple-glimpse attention and GoogLeNet, we would ideally take an  equal number of random crops as input to GoogLeNet and average the outputs.  8  Accepted as a workshop contribution at ICLR 2015  Chai, Yuning, Lempitsky, Victor, and Zisserman, Andrew. Symbiotic Segmentation and Part Local- ization for Fine-Grained Categorization. In ICCV’13: IEEE International Conference on Com- puter Vision, 2013.  Darrell, T. and Pentland, A. Active gesture recognition using learned visual attention. In Advances  in Neural Information Processing Systems 8. MIT Press, Cambridge MA, 1996.  Denil, Misha, Bazzani, Loris, Larochelle, Hugo, and de Freitas, Nando. Learning where to attend ISSN  with deep architectures for image tracking. Neural Comput., 24(8):2151–2184, 2012. 0899-7667.  Gavves, Efstratios, Fernando, Basura, Snoek, Cees, Smeulders, Arnold, and Tuytelaars, Tinne. In ICCV’13: IEEE International Conference on  Fine-Grained Categorization by Alignments. Computer Vision, 2013.  Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik, Jitendra. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.  Gonzalez-Garcia, Abel, Vezhnevets, Alexander, and Ferrari, Vittorio. An active search strategy for efﬁcient object detection. CoRR, abs/1412.3709, 2014. URL http://arxiv.org/abs/ 1412.3709.  Khosla, Aditya, Jayadevaprakash, Nityananda, Yao, Bangpeng, and Fei-Fei, Li. Novel Dataset for Fine-Grained Image Categorization. In First Workshop on Fine-Grained Visual Categorization, CVPR’11: IEEE Conference on Computer Vision and Pattern Recognition, June 2011.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Larochelle, Hugo and Hinton, Geoffrey E. Learning to combine foveal glimpses with a third-order boltzmann machine. In NIPS’10: Advances in Neural Information Processing Systems 23, pp. 1243–1251. Curran Associates, Inc., 2010.  Lin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Doll´ar, Piotr, and Zitnick, C. Lawrence. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.0312.  Mnih, Volodymyr, Heess, Nicolas, Graves, Alex, and kavukcuoglu, koray. Recurrent models of In Advances in Neural Information Processing Systems 27, pp. 2204–2212.  visual attention. Curran Associates, Inc., 2014.  Parkhi, Omkar M., Vedaldi, Andrea, Jawahar, C. V., and Zisserman, Andrew. The Truth About Cats  and Dogs. In ICCV’11: IEEE International Conference on Computer Vision, 2011.  Ranzato, Marc’Aurelio. On Learning Where To Look. CoRR, abs/1405.5488, 2014. URL http:  //arxiv.org/abs/1405.5488.  Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge, 2014.  Schmidhuber, Jrgen and Huber, Rudolf. Learning to generate artiﬁcial fovea trajectories for target  detection. International Journal of Neural Systems, pp. 135–141, 1991.  Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. CoRR, abs/1409.4842, 2014a. URL http://arxiv.org/abs/1409.4842.  Szegedy, Christian, Reed, Scott, Erhan, Dumitru, and Anguelov, Dragomir. Scalable, high-quality object detection. CoRR, abs/1412.1441, 2014b. URL http://arxiv.org/abs/1412. 1441.  9  Accepted as a workshop contribution at ICLR 2015  Tang, Yichuan, Srivastava, Nitish, and Salakhutdinov, Ruslan. Learning generative models with visual attention. CoRR, abs/1312.6110, 2013. URL http://arxiv.org/abs/1312.6110.  Wah, Catherine, Branson, Steve, Welinder, Peter, Perona, Pietro, and Belongie, Serge. The Caltech- UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.  Xiao, Jianxiong, Hays, James, Ehinger, Krista A., Oliva, Aude, and Torralba, Antonio. SUN In Computer Vision and Pat- database: Large-scale scene recognition from abbey to zoo. tern Recognition (CVPR), 2010. URL http://dblp.uni-trier.de/db/conf/cvpr/ cvpr2010.html#XiaoHEOT10.  Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun, Courville, Aaron, Salakhutdinov, Ruslan, Zemel, Richard, and Bengio, Yoshua. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. CoRR, arXiv:1502.03044, 2015. URL http://arxiv.org/abs/ 1502.03044.  Yang, Shulin, Bo, Liefeng, Wang, Jue, and Shapiro, Linda G. Unsupervised Template Learning for Fine-Grained Object Recognition. In NIPS’12: Advances in Neural Information Processing Systems, December 2012.  Zhang, Ning, Donahue, Jeff, Girshick, Ross B., and Darrell, Trevor. Part-based R-CNNs for Fine- grained Category Detection. CoRR, abs/1407.3867, 2014. URL http://arxiv.org/abs/ 1407.3867.  Zheng, Yin, Zemel, Richard S., Zhang, Yu-Jin, and Larochelle, Hugo. A Neural Autoregressive Approach to Attention-based Recognition. IJCV’14: International Journal of Computer Vision, pp. 1–13, 2014.  A RANDOM VALIDATION SAMPLES  Figure 6 shows ten randomly-selected validation examples from four different versions of the at- tention model: three-resolution (low, medium, high) with three glimpses, three-resolution with one glimpse, high-resolution only with three glimpses, and high-resolution with one glimpse. For each model and input, on the left it shows the original image with dots at the centers of the glimpses, and on the right it shows a composite image. Green borders indicate a correct classiﬁcation, and red borders indicate an error. The leftmost column is the most accurate system, however it is interesting to see that while the rightmost column is least accurate, the model correctly directs its attention to the most informative areas (dog faces) but lacks enough information to classify correctly. It is also interesting to note in the last sample, the rightmost model correctly classiﬁes a breed given a non-face feature, showing that the system has learned to identify a variety of useful parts instead of relying solely on facial features.  10  Accepted as a workshop contribution at ICLR 2015  3 resolutions, 3 glimpses  3 resolutions,  1 glimpse  high-res only, 3 glimpses  high-res only,  1 glimpse  original composite  original composite  original composite  original composite  Figure 6: Ten randomly-selected validation samples from four different model variations. See text for details.  11  ",
1412.6574,2015, A Baseline for Visual Instance Retrieval with Deep Convolutional Networks,"['A Baseline for Visual Instance Retrieval with Deep Convolutional Networks', 'Ali Sharif Razavian', 'Josephine Sullivan', 'Atsuto Maki', 'and Stefan Carlsson']",https://arxiv.org/pdf/1412.6574,"6 1 0 2     y a M 9         ]  V C . s c [      4 v 4 7 5 6  .  2 1 4 1 : v i X r a  Paper Visual Instance Retrieval with Deep Convolutional Networks  Ali S. Razavian †, Josephine Sullivan †, Stefan Carlsson †,  Atsuto Maki †  Abstract This paper provides an extensive study on the availability of image representations based on convolutional networks (ConvNets) for the task of visual instance retrieval. Besides the choice of convolutional layers, we present an efﬁcient pipeline exploiting multi-scale schemes to extract local features, in particular, by taking geometric invariance into explicit account, i.e. positions, scales and spatial consistency. In our experiments using ﬁve standard image retrieval datasets, we demonstrate that generic ConvNet image representations can outperform other state-of-the-art methods if they are extracted appropriately.  Key words: Convolutional network, Visual instance retrieval, Multi-resolution search, Learning representation  1.  Introduction  Visual instance retrieval is amongst widely studied appli- cations of computer vision, and the research progress has been reported based on several benchmark datasets. Using a database of reference images each having a label corre- sponding to the particular object or scene and given a new unlabeled query image, the task is to ﬁnd images containing the same object or scene as in the query. Not to mention the accuracy of the retrieval results, there is also the challenge of search efﬁciency because modern image collections can con- tain millions if not billions of images. To solve the problem, image representations one has two major design choices: and similarity measures (or distance metrics). The task can be therefore viewed as encoding a large dictionary of image appearances where each visual instance needs a compact but descriptive representation.  Image representations based on convolutional networks (ConvNets) are increasingly permeating in many established application domains of computer vision. Much recent work has illuminated the ﬁeld on how to design and train Con- vNets to maximize performance1) 5) and also how to exploit learned ConvNet representations to solve visual recognition tasks7) 9)11) in the sense of transfer learning. Based on these ﬁndings, although less explored, recent works also suggest the potential of ConvNet representations for the task of im- age retrieval11)13)14), which we concern ourselves in this paper. Possible alternatives for such a representation include holistic representations where the whole image is mapped to a single vector13)14) and multi-scale schemes where local  Received  ; Revised  ; Accepted  † Royal Institute of Technology (KTH) (Stockholm, Sweden)  Fig. 1  In visual instance retrieval, items of interest often ap- pear in different viewpoints, lightings, scales and lo- cations. The primary objective of the paper is to intro- duce a generic pipeline that can well handle all those variations.  features are extracted at multiple scale levels11)16). An es- sential requirement is that a proper representation should be ideally robust against variations such as scale and position of items in the image. That is, representations for the same instance in different viewpoints, or in various sizes, should be as close to each other as possible. Considering the ge- ometric invariance as our priority, in this paper, we study the multi-scale scheme and design our pipeline including the similarity measure accordingly.  Another practical aspect to consider for instance retrieval is the dimensionality and memory requirements of the image representation. Usually, two separate categories are consid- ered; the small footprint representations are encoding each image with less than 1kbytes and the medium footprint rep- resentations which have dimensionality between 10k and 100k. The former is required when the number of images is huge and memory is a bottleneck, while the latter is more useful when the number of images is less than 50k. To the best of our knowledge,this work is the ﬁrst to show that the ConvNet representations are well suited for the task of both  ITE Transactions on Media Technology and Applications Vol. xx, No. xx, pp. 1–8 (20xx)  ( 1 ) 1  small and medium footprint retrieval while our main focus will be on the medium footprint representations.  The overall objective of this work is to highlight the po- tential of ConvNets image representations for the task of vi- sual instance retrieval while taking into account above men- tioned factors, i.e. geometric invariance, the choice of lay- ers, and search efﬁciency. In sum, the contributions of the paper are three-fold:  • We demonstrate that a ConvNet image representation can outperform state-of-the-art methods on all the ﬁve standard retrieval datasets if they are extracted suitably. These datasets cover the whole spectrum of patterns from texture-less to repeated ones.  • We formulate a powerful pipeline for retrieval tasks by extracting local features based on ConvNet representa- tions.  • We suggest and show that spatial pooling is well suited to the task of instance retrieval regarding dimensional- ity reduction and preserving spatial consistency.  It should also be noted that our pipeline does not rely on the bias of the dataset as it heavily relies on the generic ConvNet train on ImageNet (except to estimate the distribution of the data for the representation). Therefore, it can be highly par- allelized. For a fair comparison, the benchmarking of our methods will be with pipelines that do not include any search reﬁnement procedures such as query expansion17) and spatial re-ranking18)19).  2. Related work  It was not until Krizhevsky and Hinton20) evaluated their binary codes that representations produced by deep learn- ing were considered for image retrieval. Since the recent success of deep convolutional networks on image classiﬁca- tion21), trained with large scale datasets22), the learned rep- resentations of ConvNet attracted increasing attention also for other applications of visual recognition. See for exam- ple4)5)7)8). Among those a few papers have recently shown promising early results for image retrieval tasks11)13)14)23) us- ing ConvNet representations although the subject is yet to be explored.  The key factor for the task of visual instance retrieval is to ﬁnd an efﬁcient image representation. Popular approaches for this goal include vector aggregation methods such as the bag-of-words (BOW) representation24), the Fisher Vector (FV)25), and the VLAD representation26). The performance of27) and28) have been among the state-of-the-art in this do- main.  Another line of work in retrieval deals with the compro-  2 ( 2 )  mise between performance and the representation size. The challenge of this task is to encode the most distinctive repre- sentation on small-footprint representations. Depending on the task, the size of representation can vary from 1k dimen- sion to less than 16 bytes 26)32).  The current paper is most related to11)13)14)16) concerning the usage of ConvNet representations. The work in13)14) for in- stance examined ConvNet representations taken from differ- ent layers of the network while focusing on holistic features. The representation in the ﬁrst fully connected layer was the choice in11), and it was also concluded13) to perform the best but still not better than other state-of-the-art holistic features (e.g. Fisher vectors30)). It is a natural next challenge to im- prove further the performance to the extent comparable to or better than the state-of-the-art approaches by taking differ- ent aspects of representations and similarity measures into consideration.  3. The ConvNet representation of an image  We ﬁrst review our design choices as regards to the Con- vNet representation we obtain from a deep convolutional network, preceding the description of our general pipeline which is provided in the next section. The basic idea under- lying our pipeline is that we have a way to extract a good feature representation of the input, I, whether it is an image or its sub-patch. Such a representation can be deﬁned by exploiting generic ConvNets trained on ImageNet.  3. 1 The choice of layer: the ﬁnal convolution layer In the following, we employ the standard architecture for the convolutional network21) consisting of multiple convolu- tional layers, Layerj(I)(j = 1, ..., c), and three fully con- nected layers.  Now, let us visit possible interpretation on the function that each layer has4): units in the ﬁrst convolutional layer respond to simple patterns like edges or textures. Second layer’s units aggregate those responses into more complex patterns. As the process continues further (deeper), units are expected to respond to more and more complex patterns in their corresponding receptive ﬁelds4)31). The ﬁrst fully con- nected layer has pooled all the patterns together and the next two fully connected layers learn a non-linear classiﬁer and estimate the probability of each class. Nevertheless, the abil- ity to recognize speciﬁc patterns should be an integral com- ponent of an image retrieval system.  Given this, it makes more sense to use the responses of the last convolutional layer (Layerc(I)) each response in this layer corresponds to a speciﬁc appearance pattern (up to the deformations allowed by the max-pooling and activation function applied after each convolutional layer) occurring in  ITE Transactions on Media Technology and Applications Vol. xx, No. xx (20xx)  Fig. 2  Schematic of spatial pooling. The sketches show how the dimensionality of the representation of a convolu- tional layer can be reduced by max-pooling (left:1×1 pooling, right: 2×2 pooling). The width of the vol- umes corresponds to the number of ﬁlters used for con- volutions. A 1×1 pooling results in one number per feature map while 2×2 pooling results in four num- bers. See section 3. 2 for the details.  a speciﬁc large sub-patch of the original image. In fact, ex- perimental evidence is provided in the study for the factors of transferability5) that the last convolutional layer is the best alternative for instance retrieval. Our intuition is that, at this level, relevant information for instance retrieval that is not suited for classiﬁcation is still preserved.  Another beneﬁt of using Layerc(I) is that it enables us to extract local features for rectangular patches which is an important advantage for retrieval. This is in contrast to the case of using a fully connected layer where one would need to either crop a part of the image; this will cause some loss of information or break the aspect ratio, which is harmful to the task of visual retrieval. With these motivations, we use the response maps from Layerc(I), the last convolutional layer in this paper. See Figure 3 for the empirical justiﬁcations.  3. 2 Spatial max-pooling As we use a deep and wide network, the dimensional- ity of this representation is also very large (approximately 40×40×512). It is neither feasible nor practical to compute distances and similarities with vectors of this size. Therefore some form of dimensionality reduction is necessary.  We reduce the dimension of our feature vector by exploit- ing the usual max-pooling applied in the convolutional lay- ers of a ConvNet. The difference is that we have a ﬁxed number of cells in our spatial grid as opposed to a spatial grid of cells of a ﬁxed size. This type of pooling is not new and has been employed before. Because max-pooling as such allows one to input images of variable size into the Con- vNet and still extract a ﬁxed sized response before the ﬁrst fully connected layer. Our experiments conﬁrm that such a low-dimensional representation is more distinctive than the hand-crafted representation of the same memory footprint. Refer to section 5 for details.  If we apply spatial pooling with a grid of size 2×2 to  Paper  Fig. 3 The effect of rescaling the input image and the choice of layer on retrieval task. Distinctive pat- terns for building landmarks are usually small details and by increasing the size of image, the performance on Oxford5k dataset increases while the most distinc- tive patterns for sculptures are the general shape of them and when the shape of sculpture becomes bigger than the size of the receptive ﬁelds, the performance starts to degrade. In our experiments, given the bias of datasets, we found that 600×600 is a reasonable size for our target dataset. See section 5.1 for detailed discussion. Also, Convolutional layers preserve more spatial information which is crucial to the task of re- trieval. This observation solidiﬁes the reported experi- ments of5). Please refer to section 3.1 for more details.  the original (w×h×512) volume of response maps, we get a 2×2×512 (=2048) dimensional feature vector. Likewise, with a grid of size 1×1, we get a 1×1×512 (= 512) dimen- sional feature vector. See Figure 2. Max-pooling with a grid of size 2×2 preserves more spatial consistency than 1×1, which is useful for the task of retrieval. But max-pooling over a grid with too many cells generally tends to reduce performance. In this paper, we also exploit the max-pooling to allow us to input larger sized images to the ConvNet than were used while training the original network. Spatial max- pooling is most useful when spatial consistency is the most distinctive feature.  3. 3 Image scale As discussed earlier, the units in convolutional layers re- spond to patterns in their receptive ﬁelds. Therefore, re- scaling the size of input image naturally results in a change in the activation of units in the response map. Figure 3 il- lustrates scaling effect on the performance followed by the explanation in section 5. 1.  ( 3 ) 3  2003004005006007008000.10.20.30.40.5InputimagesidelengthPerformanceOxford5K(conv)Sculpture6k(conv)Oxford5k(fc)Sculpture6k(fc)Fig. 5  Schematic of distance matrix between two images: The distance between two images is computed by pooling the distances between different patches of the query and the reference image according to equations (3-4).  (cid:18)  cations. In fact, we extract patches of L different sizes. The different lengths of the side of the patches are  s = max  wl =  2w l + 1  , hl =  2h l + 1  ,  for l = 1, . . . , L  (1)  (cid:19)  (cid:19)  (cid:18) wl  For each square sub-patch of size s, we extract l2 sub- patches centered at the locations  + (i − 1) bw,  hl 2  2 w − wl l − 1  ,  (2)  + (j − 1) bh h − hl l − 1  , (l |= 1)  where bw =  , and bh =  30(=(cid:80)4  for i = 1, . . . , l and j = 1, . . . , l. If the boundary of the square sub-patch exceeds the boundary of the image, we crop the rectangular sub-patch that is within the valid por- tion of the image.  Finally, we resize all the sub-patches to have the same area and then extract its ConvNet representation. We should mention that we want all the sub-patches to have the same bias and in the ideal case, all the patches should have the same aspect ratio. To achieve this, we try to extract square sub-patches wherever possible and only extract rectangular sub-patches when there is no other option.  In our experiments, we set L = 4 and therefore extract  l=1 l2) sub-patches from each image.  Multi-resolution search is most helpful when the item of interest has appeared on a smaller scale at an arbitrary po- sition in the reference image. For the justiﬁcation on why extracting local patches from each image works for the task of instance retrieval, please refer to the recent work of33).  Jittering It is known that jittering increases the perfor- mance of ConvNet representations21). That is, to have a more robust pipeline, instead of employing one patch from the query, we extract multiple patches in the same manner that we extracted sub-patches from reference images.  Jittering is a powerful technique when the item of interest has appeared partially or in a bigger scale in the reference image.  ITE Transactions on Media Technology and Applications Vol. xx, No. xx (20xx)  Fig. 4 The effect of Multi-resolution search and Jittering on patch similarities: a) Multi-resolution search is helpful when the item of interest has appeared on a smaller scale in the reference image than the query image. b) Jittering provides more robust results and is helpful in particular when the item of interest has appeared on a bigger scale in the reference image than the query. c & d) Both Multi-resolution search and Jit- tering are helpful when the overlap between the query image and the reference image decreases. Please refer to 4 for discussions.  It should be noted that max-pooling operation needs be consistent across all the patches to provide meaningful com- parisons and therefore all the images should be rescaled to have the same area.  4. Pipeline for measuring similarities of images  In this section, we describe our general pipeline exploit- ing multi-resolution search and approach to compute the dis- tance (similarity) between a query and a reference image.  The items of interest (objects, buildings, etc.) in the refer- ence and query images can occur at any location and scale. Then, different scales and other variations of input images may affect the behavior of convolutional layers as images pass through. Facing with this potential issues, we choose to perform a form of multi-scale spatial search when we com- pute the distance (similarity) between a query image and a reference image. Our search has two distinct sets of param- eters and deﬁnitions: the ﬁrst set describes the number, size and location of the sub-patches we extract from each query and reference images I. The second set deﬁnes how we use the sub-patches extracted from both the query image and the reference image to compute the distance between the two images. We now describe the two steps in more detail.  4. 1 Extracting local features Multi-resolution search From each reference image, we extract multiple sub-patches of different sizes at different lo-  4 ( 4 )  10−0.3100100.3100.6100.90.40.60.81ReferencetoqueryscaleRep.Similaritya)Multi-resolutionSearchNone2×23×34×410−0.3100100.3100.6100.90.40.60.81ReferencetoqueryscaleRep.Similarityb)JitteringNone2×23×300.20.40.60.810.40.60.81OverlapratioRep.Similarityc)Multi-resolutionSearchNone2×23×34×400.20.40.60.810.40.60.81OverlapratioRep.Similarityd)JitteringNone2×23×3Method  #dim Oxford5k Paris6k Holidays Sculp6k UKB  Method  #dim Oxford5k Paris6k Sculp6k Holidays UKB Ox105k  32k  512  46.2  95.1  46.5  74.6  67.4  90.6  89.6  87.9  59.3  84.3  8k 8k 8k  15k 15k  78.5 82.2  76.3 77.8  71.6 72.4  88.0 92.7  40.1 38.9  84.2 93.8 95.8  2.5k 7k 15k  70.7 70.2 69.1  58.0 63.4 65.5  85.9 77.5 65.8  40.3 39.8 39.5  68.0 68.5 68.5  73.0 80.7 82.4  71.0 82.3 86.0  74.7 85.6 89.5  36.1 43.2 46.3  Baseline MR2×2 MR3×3 MR4×4 MR4×4 + Jtr2×2 MR4×4 + Jtr3×3 MR4×4 + PCAw MR4×4 + Jtr2×2 + PCAw MR4×4 + Jtr3×3 + PCAw MR4×4 + Jtr3×3 + SP2×2 + PCAw BoB 35) CVLAD 38) PR-proj 27) - ASMK+MA 28) - Table 1 The effect of individual components in our pipeline. Baseline refers to computing a single rep- resentation with 1 × 1 spatial pooling. MR refers to Multi-resolution search on the reference image where MR2×2 stands for the settings of L = 2, and likewise Jtr refers to Jittering on the query image. PCAw refers to PCA whitening and SP refers to spa- tial pooling.  51.4 82.5 83.8  81.0 80.5  45.4  82.7  90.5  88.0  - - -  - -  -  -  -  -  Evaluation To study the effectiveness of Multi-resolution search and Jittering, we evaluated these two methods on the similarity of 100 random patches extracted from Afﬁne Co- variant Regions Dataset10). We isolated scaling from transla- tion. See Figure 4 for the evaluation results.  4. 2 The distance between query and reference Next, we describe how we measure the distance (similar- ity) between a query image I (q) and a reference image I (r). The reference image I (r) generates sets of feature vectors i,j,l}. We deﬁne the distance between a sub-patch, I (q) {f (r) ∗ , extracted from I (q) and the image I (r) as:  (cid:16)  (cid:17)  d∗(I (q)  ∗ , I (r)) = min 1<=m<=L, 1<=r,s<=m  d  f (q) ∗ , f (r)  r,s,m  (3)  where d(·,·) corresponds to a distance measure between two vectors. Typically d(·,·) used in equation (3) is the L2 nor- malized distance.∗  The ﬁnal distance between I (q) and I (r) is deﬁned by the sum of each query sub-patch’s minimum distance to the ref- erence image:  L(cid:88)  l(cid:88)  l(cid:88)  D(I (q), I (r)) =  d∗(I (q)  i,j,l, I (r))  (4)  l=1  i=1  j=1  Where d∗ is deﬁned in equation 3. See Figure 5 for a schematic of computing the distance.  4. 3 Post-processing - PCA whitening There are some further optional, but standard post- processing steps that can be applied after the max-pooling ∗ When d(·, ·) represents a similarity measure, we replace the minimum in equa-  tion (3) with a maximum.  Paper  Max pooling + l1 dist. Max pooling(quantized) +l2 dist. VLAD+CSurf15) mVLAD+Surf15) T-embedding30) T-embedding30)  Sum pooling + PCAw29) Ng et al,23)  256 32B  128 128 128 256  256 128  53.3 43.6  29.3 38.7 43.3 47.2  58.9 59.3  67.0 54.9  37.7 26.1  - - - -  -  59.0  - - - -  - -  71.6 57.8  73.8 71.8 61.7 65.7  80.2 83.6  84.2 69.3  83.0 87.5 85.0 86.3  91.3  -  48.9 38.0  - -  35.3 40.8  57.8  -  Table 2  Performance of small memory footprint regimes. The top two rows show our results. ConvNet based methods constantly outperform previous s.o.a. for small memory footprint representations. Two bottom results refer to the recent advancement in ConvNet based instance retrieval methods. For this experi- ment we trained our network similar to OxfordNet but with 256 kernels in the last convolutional layer. The result reported on Sculpture dataset was com- puted on 227 × 227 images.  step and prior to computing the distance between image- patches. First the representation is L2 normalized. Then given a reference set of training images (X) from the dataset, a covariance matrix C is calculated (C = (X−µX )T (X−µX ) where n is the size of the reference set) to allow PCA whitening of the data such that a dimensionality reduction of the representation is enabled. We chose to reduce the di- mensionality by half.  n  5. Results  We report results on ﬁve standard image retrieval datasets: Oxford5k buildings18), Paris6k buildings34), Sculptures6k 35), Holidays36) and UKbench37).  5. 1 Medium footprint representation & choice of pa-  rameters  To evaluate our model we used the publicly available net- work of39). This network contains 16 convolutional layers followed by three fully connected layers. We should note that we extended the bounding box of query images by half the size of the receptive ﬁelds so that the center of receptive ﬁelds for all the units remains inside the bounding box.  We have earlier displayed in Figure 3 the effect of resiz- ing the input image size on the ﬁnal result on two datasets. The most interesting jump in performance due to increasing the size of the input image is seen for the Oxford5k building dataset. An explanation for this is as follows. The distinc- tive elements of the images in the Oxford5k building dataset correspond to relatively small sub-patches in the original im- age. A response in the last convolutional layer, Lc(I), corre- sponds to a whether a particular pattern is present in a partic- ular sub-patch of the input image. Increasing the size of the input image means that the area of this sub-patch is a smaller  ( 5 ) 5  Fig. 6  Visualization of relevant images with lowest score. For query image (leftmost), four relevant images with the lowest similarity score have been retrieved. As ex- pected, some portion of these images contain the item of interest with the most radical viewpoint or lighting changes.  Fig. 7  Visualization of irrelevant images with highest score. For query image (leftmost), four irrelevant im- ages with the highest similarity score have been re- trieved. In some cases, it is even hard for humans to tell the difference apart.  proportion of the area of the input image, i.e. the represen- tation is better at describing local structures. In contrast, in- creasing the resizing factor on the input image decreases the performance on the Sculpture6k dataset because for these images better recognition seems to be achieved by describ- ing the global structure. For all the experiments, we resized the images to have an area of 600×600 pixels as it turns out to be a good dimension given these datasets.  In Table 1 we report results for our medium sized repre- sentations and compare their performance to that of other s.o.a. image representations. From examining the numbers one can see that our image retrieval pipeline based on Con- vNet representations outperform hand-crafted image repre- sentations, such as VLAD and IFV, which mostly require ad- ditional iterations of learning from specialized training data. Table 1 shows the results and the effect of each step in the pipeline. Baseline refers to computing a single representa- tion with 1 × 1 spatial pooling. It is evident that reported performance is relatively good already at this stage. Multi- resolution search (MR) helps the most in cases when the query item has appeared on a smaller scale in the reference image. The improvement is most noticeable in Oxford5k and Paris6k datasets where each query contains a full size landmark while in the reference set they have appeared in an arbitrary position and scale. On the other hand, employing MR alone degrades the performance on UKB dataset where items have appeared more or less at a uniform scale. Jit- tering (Jtr) and PCA whitening (PCAw) are almost always  6 ( 6 )  useful. Spatial pooling (SP) preserves spatial consistency, and it is always helpful as anticipated when the spatial posi- tion of patterns are the most distinctive feature. For exam- ple, it boosts the performance for Sculpture dataset where the boundary of the sculpture is the primary source of in- formation. As a whole, the ConvNet representation with the proposed pipeline outperforms all the state-of-the-art on all datasets. None of the numbers reported in this table con- siders the effect of post-processing techniques such as query expansion or re-ranking which can be commonly applied to different techniques including ours.  Figure 6 shows the relevant images in reference set with lowest score for every query. Figure 7 displays irrelevant images with highest score to each query. 5. 2 Small footprint representations In Table 2 we report results for our small memory rep- resentations and compare them to those by state-of-the-art methods with 128 or more dimensions. For the ConvNet methods, we can aggressively quantize the encoding of the numbers in each dimension of the representation to reduce the memory use.  Preceded by the initial  introduction of our proposed pipeline in 20146), a few authors have advanced the perfor- mance of the small footprint retrieval pipelines with Con- vNet. Ng et al.23) replaced max pooling with traditional VLAD over the response map. Babenko and Lempitsky29) suggested that sum-pooling can perform better than max- pooling and Arandjelovic et al.12) included learning in the pipeline as opposed to the off-the-shelf usage. Note that  ITE Transactions on Media Technology and Applications Vol. xx, No. xx (20xx)  those have been built on the basis of our pipeline which is formulated in the current paper.  5. 3 Memory and computational overhead The memory footprint and computational complexity of our pipeline is in the order of O(L3), and therefore, it is im- portant to keep the L small. In our experiments, we only extended L to 4 and the memory footprint corresponding to the last row of Table 1 is 32k for reference images and 16k for query images. The sub-patch distance matrix has around 120M elements for the task of Oxford5k and 350M elements for Holidays and the whole pipeline can ﬁt in the RAM of an average computer.  Computing the distance matrix from sub-patch distance matrix according to equations (3-4) takes 30 − 40 seconds on a single CPU core and 50 − 60 milliseconds on a single K40 GPU in our experiment for the task of Oxford5k. This yields to a reasonable time complexity. All the modules in our pipeline can signiﬁcantly beneﬁt from parallelism. To accelerate the feature extraction process, instead of extract- ing 30 representations for 30 sub-patches of an image, we feed the image in four different scales and extract the repre- sentation from response maps of the last Convolutional layer corresponding to the sub-patches as mentioned earlier.  6. Conclusion  We have presented an efﬁcient pipeline for visual instance retrieval based on ConvNets image representations. Our approach beneﬁts from multi-scale scheme to extract lo- cal features that take geometric invariance into explicit ac- count. For the representation, we chose to use last convo- lutional layer while adapting max-pooling, which not only made the layer available in terms of its dimensionality but helped to boost the performance. Throughout the experi- ments with ﬁve standard image retrieval datasets, we demon- strated that generic ConvNet image representations can out- perform other state-of-the-art methods in all the tested cases if they are extracted appropriately. Our pipeline does not rely on the bias of the dataset, and the only recourse we make to specialized training data is in the PCA whitening.  For our future work, we plan to investigate domain adap- tation to improve further the performance. For example, ﬁne-tuning the ConvNet with landmark dataset13) increases the performance on Oxford5k up to 85.3. We would like to highlight that our result should still be viewed as a baseline. Simple additions such as concatenating multi-scale, multi- layer and different architecture representations give a boost in performance (e.g. 87.2 for Oxford5k). From our measure of similarity, a rough bound can also be estimated within our framework, which may be useful for a novel search reﬁne-  Paper  ment techniques.  References  1) Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman, “Deep inside convo- lutional networks: Visualising image classiﬁcation models and saliency maps.,” arXiv:1312.6034 [cs.CV], 2013.  2) Ross B. Girshick, Forrest N. Iandola, Trevor Darrell, and Jitendra Malik, “De- arXiv:1409.5403  formable part models are convolutional neural networks,” [cs.CV], 2014.  3) Ken Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman, “Return of the devil in the details: Delving deep into convolutional nets,” arXiv:1405.3531 [cs.CV], 2014.  4) Matthew D. Zeiler and Rob Fergus, “Visualizing and understanding convolu-  tional networks,” in ECCV, 2014, pp. 818–833.  5) Hossein Azizpour, Ali S. Razavian, Josephine Sullivan, Atsuto Maki, and Stefan Carlsson, “Factors of transferability for a generic ConvNet representation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016.  6) Ali S. Razavian, Josephine Sullivan, Atsuto Maki, and Stefan Carlsson, “Visual instance retrieval with deep convolutional networks,” arxiv:1412.6574 [cs.CV], 2014.  7) Maxime Oquab, L´eon Bottou, Ivan Laptev, and Josef Sivic, “Learning and trans- ferring mid-level image representations using convolutional neural networks,” in CVPR, 2014.  8) Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell, “Decaf: A deep convolutional activation feature for generic visual recognition,” in ICML, 2014.  9) Philipp Fischer, Alexey Dosovitskiy, and Thomas Brox, “Descriptor matching with convolutional neural networks: a comparison to SIFT,” arXiv:1405.5769 [cs.CV], 2014.  10) Krystian Mikolajczyk and Cordelia Schmid, “A Performance Evaluation of Local Descriptors,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005.  11) Ali S. Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson, “CNN features off-the-shelf: an astounding baseline for recognition,” CVPRW, 2014.  12) Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic, “NetVLAD: CNN architecture for weakly supervised place recognition,” arxiv:1511.07247 [cs.CV], 2015.  13) Artem Babenko, Anton Slesarev, Alexander Chigorin, and Victor S. Lempitsky,  “Neural codes for image retrieval,” in ECCV, 2014.  14) Ji Wan, Dayong Wang, Steven Chu Hong Hoi, Pengcheng Wu, Jianke Zhu, Yong- dong Zhang, and Jintao Li, “Deep learning for content-based image retrieval: A comprehensive study,” in ACM Multimedia, 2014, pp. 157–166.  15) Eleftherios Spyromitros-Xiouﬁs, Symeon Papadopoulos, Ioannis (Yiannis) Kom- patsiaris, Grigorios Tsoumakas, and Ioannis Vlahavas, “A Comprehensive Study Over VLAD and Product Quantization in Large-Scale Image Retrieval,” in IEEE Transactions on Multimedia, 2014, pp. 1713–1728.  16) Yunchao Gong, Liwei Wang, Ruiqi Guo, and Svetlana Lazebnik, “Multi-scale orderless pooling of deep convolutional activation features,” in ECCV, 2014, pp. 392–407.  17) Ondrej Chum, James Philbin, Josef Sivic, Micheal Isard, and Andrew Zisserman, “Total recall: Automatic query expansion with a generative feature model for object retrieval,” in ICCV, 2007.  18) James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman, “Object retrieval with large vocabularies and fast spatial matching,” in CVPR, 2007.  19) Michal Perdoch, Ondrej Chum, and Jiri Matas, “Efﬁcient representation of local  geometry for large scale object retrieval,” in CVPR, 2009.  20) Alex Krizhevsky and Geoffrey E. Hinton, “Using very deep autoencoders for content-based image retrieval,” in Proceedings of the European Symposium of Artiﬁcal Neural Networks (ESANN), 2011.  21) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, “ImageNet classiﬁca-  tion with deep convolutional neural networks,” in NIPS, 2012.  22) J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A  Large-Scale Hierarchical Image Database,” in CVPR, 2009.  23) Joe Yue-Hei Ng, Fan Yang, and Larry S. Davis, “Exploiting local features from  deep networks for image retrieval,” CVPRW, 2015.  24) Josef Sivic and Andrew Zisserman, “Video google: A text retrieval approach to  object matching in videos,” in ICCV, 2003.  25) Florent Perronnin and Christopher R. Dance, “Fisher kernels on visual vocabu-  laries for image categorization,” in CVPR, 2007.  26) Herv´e J´egou, Florent Perronnin, Matthijs Douze, Jorge S´anchez, Patrick P´erez, and Cordelia Schmid, “Aggregating local image descriptors into compact codes,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1704–1716, 2012.  27) Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman, “Learning local fea- ture descriptors using convex optimisation,” IEEE Transactions on Pattern Anal-  ( 7 ) 7  ysis and Machine Intelligence, vol. 36, no. 8, pp. 1573–1585, 2014.  28) Giorgos Tolias, Yannis S. Avrithis, and Herv´e J´egou, “To aggregate or not to  aggregate: Selective match kernels for image search,” in ICCV, 2013.  29) Artem Babenko, and Victor Lempitsky, “Aggregating Local Deep Features for  Image Retrieval” in ICCV, 2015.  30) Herv´e J´egou and Andrew Zisserman, “Triangulation embedding and democratic  aggregation for image search,” in CVPR, 2014.  31) Bolei Zhou, Aditya Khosla, `Agata Lapedriza, Aude Oliva, and Antonio Torralba,  “Object detectors emerge in deep scene cnns,” in ICLR, 2015.  32) Antonio Torralba, Rob Fergus, and Yair Weiss, “Small codes and large image  databases for recognition” in CVPR, 2008.  33) Ran Tao, Efstratios Gavves, Cees G. M. Snoek, and Arnold W. M. Smeulders, “Locality in generic instance search from one example,” in CVPR, 2014, pp. 2099–2106.  34) James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman, “Lost in quantization: Improving particular object retrieval in large scale image databases,” in CVPR, 2008.  35) Relja Arandjelovi´c and Andrew Zisserman, “Smooth object retrieval using a bag  of boundaries,” in ICCV, 2011.  36) Herv´e J´egou, Matthijs Douze, and Cordelia Schmid, “Hamming embedding and  weak geometric consistency for large scale image search,” in ECCV, 2008.  37) David Nist´er and Henrik Stew´enius, “Scalable recognition with a vocabulary  tree,” in CVPR, 2006.  38) Wan-Lei Zhao, Herv´e J´egou, Guillaume Gravier, et al., “Oriented pooling for  dense and non-dense rotation-invariant features,” in BMVC, 2013.  39) Karen Simonyan and Andrew Zisserman, “Very deep convolutional networks for  large-scale image recognition,” in ICLR, 2015.  8 ( 8 )  ITE Transactions on Media Technology and Applications Vol. xx, No. xx (20xx)  ",
1412.6607,2015, Visual Scene Representation: Scaling and Occlusion,"['Visual Scene Representation: Scaling and Occlusion', 'Stefano Soatto', 'Jingming Dong', 'and Nikolaos Karianakis']",https://arxiv.org/pdf/1412.6607,"Accepted as a workshop contribution at ICLR 2015  VISUAL SCENE REPRESENTATIONS: SCALING AND OCCLUSION IN CONVOLUTIONAL ARCHITECTURES∗  Stefano Soatto, Jingming Dong & Nikolaos Karianakis UCLA Vision Lab University of California, Los Angeles Los Angeles, CA 90095, USA {soatto,dong}@cs.ucla.edu, nikarianakis@ucla.edu  ABSTRACT  We study the structure of representations, deﬁned as approximations of minimal sufﬁcient statistics that are maximal invariants to nuisance factors, for visual data subject to scaling and occlusion of line-of-sight. We derive analytical expressions for such representations and show that, under certain restrictive assumptions, they are related to features commonly in use in the computer vision community. This link highlights the conditions tacitly assumed by these descriptors, and also sug- gests ways to improve and generalize them.  1  INTRODUCTION  Soatto & Chiuso (2014) deﬁne an optimal representation as a minimal sufﬁcient statistic (of past data for the scene) and a maximal invariant (of future data to nuisance factors), and propose a measure of how “useful” (informative) a representation is, via the uncertainty of the prediction density. What is a nuisance depends on the task, that includes decision and control actions about the surrounding environment, or scene, and its geometry (shape, pose), photometry (reﬂectance), dynamics (motion) and semantics (identities, relations of “objects” within). We show that optimal management of nuisance variability due to occlusion is generally intractable, but can be approximated leading to a composite (correspondence) hypothesis test, which provides grounding for the use of “patches” or “receptive ﬁelds,” ubiquitous in practice. The analysis reveals that the size of the domain of the ﬁlters should be decoupled from spectral characteristics of the image, unlike traditionally taught in scale-space theory, an unintuitive consequence of the analysis. This idea has been exploited by Dong & Soatto (2015) to approximate the optimal descriptor of a single image, under an explicit model of image formation (the Lambert-Ambient, or LA, model) and nuisance variability, leading to DSP-SIFT. Extensions to multiple training images, leading to MV-HoG and R-HoG, have been championed by Dong et al. (2013). Here, we apply domain-size pooling to the scattering transform Bruna & Mallat (2011) leading to DSP-SC, to a convolutional neural network, leading to DSP-CNN, and to deformable part models Felzenszwalb et al. (2008), leading to DSP-DPM, in Sect. 2.2, 2.3 and 2.4 respectively. We treat images as random vectors x, y and the scene θ as an (inﬁnite-dimensional) parameter. An = {x1, . . . , xt} that maximally reduces . optimal representation is a function φ of past images xt uncertainty on questions about the scene Geman et al. (2015) given images from it and regardless of nuisance variables g ∈ G. In Soatto & Chiuso (2014) the sampled orbit anti-aliased (SOA) likelihood is introduced as:  5 1 0 2    r p A 7 1         ]  V C . s c [      5 v 7 0 6 6  .  2 1 4 1 : v i X r a  ˆLG,(cid:15)(θ; x) = max  i  ˆL(θ, gi; x), i = 1, . . . , N ((cid:15))  where  ˆL(θ, gi; x)  . =  L(θ, gig; x)dP (g)  (cid:90)  (1)  (2)  . and L(θ, g; x) = pθ,g(x) is the joint likelihood, understood as a function of the parameter θ and nuisance g for ﬁxed data x, with dP (g) = w(g−1)dµ(g) an anti-aliasing measure with positive  G  ∗Also UCLA Technical Report CSD140024, November 12, 2014  1  Accepted as a workshop contribution at ICLR 2015  weights w. The SOA likelihood is an optimal representation in the sense that, for any (cid:15), it is possible to choose N and a ﬁnite number of samples {gi}N . = ˆLG,(cid:15)(θ; xt) approximates to within (cid:15) a minimal sufﬁcient statistic (of xt for θ) that is maximally invariant to group transforma- tions in G. This result is valid under the assumptions of the Lambert-Ambient (LA) model Dong & Soatto (2014), which is the simplest known to capture the phenomenology of image formation including scaling, occlusion, and rudimentary illumination.  i=1 so that φθ(xt)  2 CONSTRUCTING VISUAL REPRESENTATIONS  Theorem 1 (Contrast invariant). Given a training image x and a test image y, assuming that the latter is affected by noise that is independent in the gradient direction and magnitude, then the maximal invariant of y to the group G of contrast transformations is given by  (3)  (5)  (6)  px,G (y) = p(∠∇y|x) (cid:107)∇x(cid:107). (cid:89)  i  Note that, other than for the gradient, the computations above can be performed point-wise under the assumption of LA model, so we could write (3) at each pixel yi: if α NS1 (αi − ∠∇xi; (cid:15)α)(cid:107)∇xi(cid:107)  = ∠∇yi, .  φx(α) =  (4)  Note that (4) is invariant to contrast transformations of y, but not of x. Invariance to contrast trans- formations in the (single) training image can be performed by normalizing the likelihood, which in turn can be done by simply dividing by the integral over α, which is the (cid:96)1 norm of the histogram across the entire image/patch  φx(α) (cid:107)φx(α)(cid:107)(cid:96)1  =  p(α|x)(cid:107)∇x(cid:107)  (cid:82) p(α|x)dα(cid:107)∇x(cid:107) = p(α|x)  that should be used instead of the customary (cid:96)2 Lowe (2004). Once invariance to contrast trans- formations is achieved, which can be done on a single image x, we are left with nuisances G that include general viewpoint changes, including the occlusions they induce. This can be handled by computing the SOA likelihood with respect to G of SE(3) (Sect. 2.1) from a training sample xt, leading to  ˆL(θ, gi; xt) =  φxt(α|gi ◦ g)dP (g)  (cid:111)N  i=1  (cid:110)(cid:90)  G  Occlusion, or visibility, is arguably the single most critical aspect of visual representations. It en- forces locality, as dealing with occlusion nuisances entails searching through, or marginalizing, all possible (multiply-connected) subsets of the test image. This power set is clearly intractable even for very small images. Missed detections (treating a co-visible pixel as occluded) and false alarms (treating an occluded pixel as visible) have different costs: Omitting a co-visible pixel from Ω de- creases the likelihood by a factor corresponding to multiplication by a Gaussian for samples drawn from the same distribution; vice-versa, including a pixel from Ωc (false alarm) decreases the log- likelihood by a factor equal to multiplying by a Gaussian evaluated at points drawn from another distribution, such as uniform. So, testing for correspondence on subsets of the co-visible regions, assuming the region is sufﬁciently large, reduces the power, but not the validity, of the test. This observation can be used to ﬁx the shape of the regions, leaving only their size to be marginalized, or searched over. This reasoning justiﬁes the use of “patches” or “receptive ﬁelds” to seed image matching, but emphasizes that a search over different sizes Dong & Soatto (2015) is needed. Together with the SOA likelihood, this also justiﬁes the local marginalization of domain sizes, along with translation, as recently championed in Dong & Soatto (2015). Corollary 1 (DSP-SIFT). The DSP-SIFT descriptor Dong & Soatto (2015) approximates an optimal representation (6) for G the group of planar similarities and local contrast transformations, when the scene is a single training image, and the test image is restricted to a subset of its domain.  The assumptions underlying all local representations built using a single image break down when the scene is not ﬂat and not moving parallel to the image plane. In this case, multiple views are necessary to manage nuisance due to general viewpoint changes.  2  Accepted as a workshop contribution at ICLR 2015  2.1 GENERAL VIEWPOINT CHANGES  If a co-variant translation-scale and size sampling/anti-aliasing mechanism is employed, then around each sample the only residual variability to viewpoint SE(3) = R3 × SO(3) is reduced. In some cases, a consistent reference (canonical element) for both training and test images is avail- able when scenes or objects are geo-referenced: The projection of the gravity vector onto the image plane Jones & Soatto (2011). In this case, ˆα is the angle of the projection of gravity onto the im- age plane (well deﬁned unless they are orthogonal). Alternatively, multiple (principal) orientation references can be selected based on the norm of the directional derivative Lowe (2004):  pθ(α|G) = pθ(α|ˆα).  (7)  This leaves out-of-plane rotations to be managed. Dong et al. (2013) have proposed extensions of local descriptors to multiple views, based on a sampling approximation of the likelihood function, ˆpθ, or on a point estimate of the scene pˆθ, MV-HoG and R-HoG respectively. The estimated scene has a geometric component (shape) ˆS and a photometric component (radiance) ˆρ, inferred from the LA model as described in Dong & Soatto (2014). Once the effects of occlusions are considered (which force the representation to be local), and the effects of general viewpoint changes are ac- counted for (which creates the necessity for multiple training images of the same scene), a maximal contrast/viewpoint/occlusion invariant can be approximated:  the SOA likelihood (6) becomes: (xj); (cid:15)α)κσ(i− j)dµ(j)dP (σ)dPSO(3)(g))  (cid:111)N  ˆLSE(3),(cid:15)(N )(αi) = max  k  NS1 (αi − ˆρ◦ gkg ◦ π  −1 ˆS  SO(3)  k=1 (8) in addition to domain-size pooling. The assumption that all existing multiple-view extensions of SIFT do not overcome is the conditional independence of the intensity of different pixels. This is discussed in Soatto & Chiuso (2014) for the case of convolutional deep architectures, and in the next section for Scattering Networks. Capturing the joint statistics of different components of the SOA likelihood is key to modeling intra-class variability of object or scene categories.  (cid:110)(cid:90)  2.2 DSP-SCATTERING NETWORKS  The scattering transform Bruna & Mallat (2011) convolves an image (or patch) with a Gabor ﬁlter bank at different rotations and dilations, takes the modulus of the responses, and applies an averaging operator to yield the scattering coefﬁcients. This is repeated to produce coefﬁcients at different layers in a scattering network. The ﬁrst layer is equivalent to SIFT Bruna & Mallat (2011), in the sense that (3) can be implemented via convolution with a Gabor element with orientation α then taking the modulus of the response. One could conjecture that domain-size pooling (DSP) applied to a scattering network would improve performance in tasks that involve changes of scale and visibility. We call the resulting method DSP Scattering Transform (DSP-SC). Indeed, this is the case, as we show in the Appendix of Soatto et al. (2014), where we compare DSP-SC to the single-scale scattering transform (SC) to the datasets of Mikolajczyk & Schmid (2003) (Oxford) and Fischer et al. (2014).  2.3 DSP-CNN  Deep convolutional architectures can be understood as implementing successive approximations of an optimal representation by stacking layers of (conditionally) independent local representations of the form (8), which have been shown by Soatto & Chiuso (2014) to increasingly achieve invariance to large deformations, despite locally marginalizing only afﬁne (or similarity) transformations. As Dong & Soatto (2015) did for SIFT, and as we did for the Scattering Transform above, we conjec- tured that pooling over domain size would improve the performance of a convolutional network. In the Appendix of Soatto et al. (2014), we report experiments to test the conjecture using a pre-trained network which is ﬁne-tuned with domain-size pooling on benchmark datasets.  2.4 DSP-DPM  We have also developed domain-size pooling extensions of deformable part models (DPMs) Felzen- szwalb et al. (2008), small trees of local HOG descriptors (“parts”), whereby local photometry is  3  Accepted as a workshop contribution at ICLR 2015  encoded in the latter (nodes), and geometry is encoded in their position on the image relative to the root node (edges). Intra-class shape variability is captured by the posterior density of edge values, learned from samples. Photometry is captured by a “HOG pyramid” where the size of each part is pre-determined and ﬁxed relative to the root. One could therefore conjecture that performing anti-aliasing with respect to the size of the parts would improve performance. Experimental results, reported in the Appendix of Soatto et al. (2014), validate the conjecture.  ACKNOWLEDGMENTS  We acknowledge discussions with Alessandro Chiuso, Joshua Hernandez, Arash Amini, Ying-Nian Wu, Taco Cohen, Virginia Estellers, Jonathan Balzer. Research supported by ONR N000141110863, NSF RI-1422669, and FA8650-11-1-7154.  REFERENCES Bruna, J. and Mallat, S. Classiﬁcation with scattering operators. In Proc. IEEE Conf. on Comp. Vision and  Pattern Recogn., 2011.  Dong, J. and Soatto, S. The Lambert-Ambient Shape Space and the Systematic Design of Feature Descriptors.  R. Cipolla, S. Battiato, G.-M. Farinella (Eds), Springer Verlag, 2014.  Dong, J. and Soatto, S. Domain-size pooling in local descriptors: DSP-SIFT. In Proc. IEEE Conf. on Comp.  Vision and Pattern Recogn., 2015.  Dong, J., Karianakis, N., Davis, D., Hernandez, J., Balzer, J., and Soatto, S. Multi-view feature engineering and learning. In Proc. IEEE Conf. on Comp. Vision and Pattern Recogn., 2015. (also ArXiv: 1311.6048, 2013).  Felzenszwalb, P., McAllester, D., and Ramanan, D. A discriminatively trained, multiscale, deformable part  model. In CVPR, pp. 1–8, 2008.  Fischer, P., Dosovitskiy, A., and Brox, T. Descriptor matching with convolutional neural networks: a compari-  son to sift. ArXiv:1405.5769, 2014.  Geman, D., Geman, S., Hallonquist, N., and Younes, L. Visual turing test for computer vision systems. Pro-  ceedings of the National Academy of Sciences, 112(12):3618–3623, 2015.  Jones, E. and Soatto, S. Visual-inertial navigation, localization and mapping: A scalable real-time large-scale  approach. Intl. J. of Robotics Res., 2011.  Lowe, D. G. Distinctive image features from scale-invariant keypoints. IJCV, 2(60):91–110, 2004.  Mikolajczyk, K. and Schmid, C. A performance evaluation of local descriptors. 2003.  Soatto, S. and Chiuso, A. Visual scene representations: sufﬁciency, minimality, invariance and deep approxi-  mation. Proc. of the ICLR Workshop, 2015 (also ArXiv: 1411.7676, 2014).  Soatto, S., Dong, J., and Karianakis, N. Visual scene representation: scaling and occlusion in convolutional  architectures. (Extended version of this manuscript) Technical report UCLA CSD140024, 2014.  4  ",
1412.7479,2015, Deep networks with large output spaces,"['Deep networks with large output spaces', 'Sudheendra Vijayanarasimhan', 'Jon Shlens', 'Jay Yagnik', 'and Rajat Monga']",https://arxiv.org/pdf/1412.7479,"5 1 0 2    r p A 0 1         ] E N . s c [      4 v 9 7 4 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  DEEP NETWORKS WITH LARGE OUTPUT SPACES  Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga & Jay Yagnik Google Research Mountain View, CA 94043, USA {svnaras,shlens,rajatmonga,jyagnik}@google.com  ABSTRACT  Deep neural networks have been extremely successful at various image, speech, video recognition tasks because of their ability to model deep structures within the data. However, they are still prohibitively expensive to train and apply for prob- lems containing millions of classes in the output layer. Based on the observation that the key computation common to most neural network layers is a vector/matrix product, we propose a fast locality-sensitive hashing technique to approximate the actual dot product enabling us to scale up the training and inference to millions of output classes. We evaluate our technique on three diverse large-scale recognition tasks and show that our approach can train large-scale models at a faster rate (in terms of steps/total time) compared to baseline methods.  1  INTRODUCTION  Deep neural networks have proven highly successful at various image, speech, language and video recognition tasks (Krizhevsky et al., 2012; Mikolov et al., 2013; Karpathy et al., 2014). These net- works typically have several layers of units connected in feedforward fashion between the input and output spaces. Each layer performs a speciﬁc function such as convolution, pooling, normalization, or plain matrix products in the case of fully connected layers followed by some form of non-linear activation such as sigmoid or rectiﬁed linear units.  Despite their attractive qualities, and the relative efﬁciency of their local architecture, these networks are still prohibitively expensive to train and apply for large-scale problems containing millions of classes or nodes. There are several such problems proposed in the literature. The Imagenet dataset which is one of the largest datasets for image classiﬁcation contains around 21000 classes. Wordnet, which is a superset of Imagenet, consists of 117, 00 synsets. Freebase, which is a community-curated database of well-known people, places, and things contains close to 20 million entities. Image mod- els of text queries have ranged from 100000 queries in academic benchmarks Weston et al. (2011b) to several million in commercial search engines such as Google, Bing, Yahoo, etc. Duplicate video content identiﬁcation (Shang et al., 2010; Song et al., 2011; Zhao et al., 2007) and video recommen- dations are also large-scale problems with millions of classes.  We note that the key computation common to softmax/logistic regression layers is a matrix product between the activations from a layer, x, and the weights of the connections to the next layer, W . As the number of classes increases this computation becomes the main bottleneck of the entire network. Based on this observation, we exploit a fast locality-sensitive hashing technique (Yagnik et al., 2011) in order to approximate the actual dot product in the ﬁnal output layer which enables us to scale up the training and inference to millions of output classes.  Our main idea is to approximate the dot product between the output layer’s parameter vector and the input activations using hashing. We ﬁrst compute binary hash codes for the parameter vectors, W , of a layer’s output nodes and store the indices of the nodes in locations corresponding to the hash codes within hash tables. During inference, given an input activation vector, x, we compute the hash codes of the vector and retrieve the set of output nodes Ok that are closest to the input vector in the hash space. Following this we compute the actual dot product between x and the parameter vectors of Ok and set all other values to zero. By avoiding the expensive dot product operation between the input activation vector and all output nodes we show that our approach can easily scale up to millions of output classes during inference.  1  Accepted as a workshop contribution at ICLR 2015  Furthermore, using the same technique when training the models, we show that our approach can train large-scale models at a faster rate both in terms of number of steps and the total time compared to both the standard softmax layers and the more computationally efﬁcient hierarchical softmax layer of (Mikolov et al., 2013).  2 RELATED WORK  Several methods have been proposed for performing classiﬁcation in deep networks over large vo- cabularies. Traditional methods such as logistic regression and softmax (multinomial regression) are known to have poor scaling properties with the number of classes (Dean et al., 2013) as the number of dot products grows with the number of classes C that must be considered.  One method for contending with this is hierarchical softmax whereby a tree is constructed of depth log2C in which the leaves are the individual classes which must be classiﬁed (Morin & Bengio, 2005; Mikolov et al., 2013). A beneﬁt of this approach is that each step merely requires computa- tions associated with the tree traversal to an individual leaf.  A second direction is to instead train a dense embedding space representation and perform classi- ﬁcation by employing k-nearest-neighbors in this embedding space on unseen examples. Typical methods for training such embedding representations employ a hinge rank loss with a clever selec- tion of negative examples, e.g. (Weston et al., 2011a).  Locality sensitive hashing (LSH) (Gionis et al., 1999) provides a third alternative by providing meth- ods to perform approximate nearest neighbor search in sub-linear time for various similarity met- rics. An LSH scheme based on ordinal similarity is proposed in (Yagnik et al., 2011) which is used in (Dean et al., 2013) to speed-up ﬁlter based object detection. We expand on these techniques to enable learning large-scale deep network models.  3 APPROACH  The goal of this work is to enable approximate computation of the matrix product of the parameters of a layer and its input activations, xT W , in a deep network so that the number of output dimensions can be increased by several orders of magnitude. In the following sections, we demonstrate that a locality sensitive hashing based approximation can provide such a solution without too much degra- dation in overall accuracy. As a ﬁrst step we employ this technique to scale up the ﬁnal classiﬁcation layer since the beneﬁts of hashing are easily seen when the cardinality is quite large.  3.1 SOFTMAX/LOGISTIC CLASSIFICATION  Softmax and logistic regression functions are two popular choices for the ﬁnal layer of a deep net- work for multi-class and binary classiﬁcation problems respectively. Formally, the two functions are deﬁned as  Psof tmax(y = j|x) =  Plogistic(y = j|x) =  exT wj k=1 exT wk  PN  1  1 + e−(xT wj +βj)  (1)  (2)  (3)  where P (y = j|x) is the probability of the jth class given the input vector x and {wj, j = 1...N } are distinct linear functions for the N classes. When the number of classes is large, not all classes are relevant to a given input example. Therefore, in many situations we are only interested in the K classes with the highest probabilities. We could obtain the top K classes by equivalently determining the K vectors, WK, that have the largest dot products with the input vector x and computing the probabilities for only these K classes, setting all others to zero.  2  Accepted as a workshop contribution at ICLR 2015  softmax  WTA softmax  softmax activation  O(N)  softmax activation  O(K)  add bias  O(N)  add bias  O(K)  matrix multiply  O(dN)  matrix multiply  O(dK)  ﬁnd top K collisions ! in hash table lookup  O(log N)  X  X  Figure 1: A diagram comparing a typical classiﬁcation network trained with softmax with the proposed WTA softmax. The left column shows the operations of softmax = f (W X + b). X are the input network activations, f (·) is the softmax activation function, b are biases for each of N classes and W is an N × d matrix where each row is the weight vector associated with an individual class. The matrix product W X is the most expensive operation for the entire network when the number of classes N is extremely large. The right column diagrams the WTA softmax operation. The hashing operation identiﬁes the K << N most likely labels for a given X. The remainder of the WTA softmax operations are largely identical although they only operate on the K << N likely labels.  We note that this is equivalent to the problem of ﬁnding the approximate nearest neighbors of a vector based on cosine (dot product) similarity which has a rich literature beginning with the seminal work of (Gionis et al., 1999). It has been shown that approximate nearest neighbors can be obtained in time that is sub-linear in the number of database vectors with certain guarantees which is the key motivation for our approach. In our case the database vectors are the parameter vectors of the output layer, wj, and the query vector is the input activation from the previous layer x. In this work, we employ the subfamily of hash functions, winner-take-all (WTA) hashing introduced in (Yagnik et al., 2011), since it has been successfully applied for the similar task of scaling up ﬁlter-based object detection in (Dean et al., 2013).  3.2 WINNER-TAKE-ALL HASHING (WTA)  Given a vector x or w in Rd, its WTA hash is deﬁned by permuting its elements using P distinct permutations and recording the index of the maximum value of the ﬁrst k elements (Yagnik et al., 2011). Each index can be compactly represented using log2k bits resulting in P ∗ log2k bits for the entire hash. The WTA hash has several desirable properties; since the only operation involved in computing the hash is comparison, it can be completely implemented using integer arithmetic and the algorithm can be efﬁciently implemented without accruing branch prediction penalties.  Furthermore, each WTA hash function deﬁnes an ordinal embedding and it has been shown in (Yagnik et al., 2011) that as P → d!, the dot product between two WTA hashes tends to the rank correlation between the underlying vectors. Therefore, WTA is well suited as a basis for locality- sensitive hashing as ordinal similarity can be used as a more robust proxy for dot product similarity.  Given binary hash codes, u of a vector, w, there are several schemes that can be employed in or- der to perform approximate nearest neighbor search. In this work, we employ the scheme used in (Dean et al., 2013) due to its simplicity and limited overhead.  3  Accepted as a workshop contribution at ICLR 2015  Figure 2: A schematic describing the use of the hash table during inference and training. The learned parameter vectors W are stored in hash tables using WTA hashing and the hash codes of the input vector x are used to retrieve the top K classes with the largest dot products with the input vector. The actual dot product and the corresponding probabilites are computed for only these retrieved classes. Similarly, during the backward pass the gradients are computed based on the top K retrieved nodes and the parameter vectors are updated.  In this scheme, we ﬁrst divide the compact hash code, u, containing P elements of log2k bits into M bands, um, each containing P/M elements. We create a hash table for each band {Tm, m = 1...M } and store the index of the vector in the hash bins corresponding to um in each hash table Tm. During retrieval, we similarly compute the hash codes of x and divide it into M bands and retrieve the set of all IDs in the corresponding hash bins along with their counts. The counts provide a lower bound for the dot product between the two hash vectors which is related to the ordinal similarity between the two vectors. Therefore, the top K IDs from this list approximate the K nearest neighbors to the input vector based on dot product similarity. The actual dot product can now be computed for these vectors with the input vector x to obtain their probabilities. The complexity of the scheme proposed above depends on the dimensionality of the vectors for computing the hash codes, the number of bands or hash tables that are used during retrieval, M , and the number of IDs for which the actual dot product is computed, K. Since all three quantities are independent of the number of classes in the output layer our approach can accomodate any number of classes in the output layer. As shown in Figure 1, the naive softmax has a complexity of O(d∗N +N ) whereas our WTA based approximation has a complexity of O(d ∗ K + K + constant). The overall speed-up we obtain is of the order of N K assuming the cost of computing the hash function and lookup are much smaller than d ∗ N . Of course, since both M and K relate to the accuracy of the approximation they provide a trade-off between the time complexity and the accuracy of the network.  3.3  INFERENCE  We can apply our proposed approximation during both model inference and training. For infer- ence, given a learned model, we ﬁrst compute the hash codes of the parameter vectors of the soft- max/logistic regression layer and store the IDs of the corresponding classes in the hash table as described in Section 3.2. This is a one time operation that is performed before running inference on any input examples.  Given an input example, we pass it through all layers leading up to the classiﬁcation layer as before and compute the hash codes of the input activations to the classiﬁcation layer. We then query the hash table to retrieve the top K classes and compute probabilties using Equation 3 for only these classes. Figure 1 shows a rough schematic of this procedure.  3.4 TRAINING  We train the models using downpour SGD, an asynchronous stocastic gradient descent procedure supporting a large number of model replicas, proposed in (Dean et al., 2012). During backpropaga- tion we only propagate gradients based on the top K classes that were retrieved during the forward  4  Accepted as a workshop contribution at ICLR 2015  pass of the model and update the parameter vectors of only these retrieved classes using the error vector. Additionally, we add all the positive labels for an input example to the list of non-zero classes in order to always provide a positive gradient. In Section 4 we show empirical results of performing only these top K updates. These sparse gradients are much more computationally efﬁcient and ad- ditionally perform the function of hard negative mining since only the closest classes to a particular example are updated.  While inference using WTA hashing is straightforward to implement, there are several challenges that need to be solved to make training efﬁcient using such a scheme. Firstly, unlike during inference, the parameter vectors are constantly changing as new examples are seen. It would be infeasible to request updated parameters for all classes and update the hash table after every step.  However, we found that gradients based on a small set of examples do not perturb the parameter vector signiﬁcantly and WTA hashing is only sensitive to changes in the ordering of the various dimensions and is more robust to small changes in the absolute values of the different dimensions. Based on these observations we implemented a scheme where the hash table locations of classes are updated in batches in a round-robin fashion such that all classes are updated over a course of several hundred or thousand steps which turned out to be quite effective.  Therefore, we only request updated parameters for the set of retrieved classes, the positive training classes and the classes selected in the round-robin scheme. Figure 2 shows a schematic of these interactions with the parameter server and the hash tables.  4 EXPERIMENTS  We empirically evaluate our proposed technique on several large-scale datasets with the aim of in- vestigating the trade-off between accuracy and time complexity of the WTA based softmax classiﬁer in comparison to the baseline approaches of softmax (exhaustive) and hierarchical softmax.  4.1  IMAGENET 21K  The 2011 Imagenet 21K dataset consists of 21,900 classes and 14 million images. We split the set into equal partitions of training and testing sets as done in (Le et al., 2012). We selected values of 16, 1000, 3000 for the k, M , P parameters of the WTA approach for all experiments based on results on a small set of images which agreed with the parameters mentioned in (Dean et al., 2013). We varied the value of K, which is the number of retrieved classes for which the actual dot product is computed, since it directly affects both the accuracy and the time complexity of the approach.  We used the artichecture proposed in (Krizhevsky et al., 2012) (AlexNet) for all experiments replac- ing only the classiﬁcation layer with the proposed approach. All methods were optimized using downpour SGD with a starting learning rate of 0.001 with exponential decay in conjunction with a momentum of 0.9. We used a cluster of about 100 machines containing multi-core CPUs with 20GB of RAM running at 2.4Ghz to perform training and inference. Figure 3 ﬁrst reports the time taken during inference by the WTA Softmax and Softmax layers alone (ignoring the rest of the model) as both the batch size and the top K is varied for this problem. We note that WTA Softmax provides signiﬁcant speed-up over Softmax for both small batch sizes and small values of K. For large batch sizes Softmax is very efﬁcient due to optimizations over dense matrices. For large values of K the dot product with the retrieved vectors begins to dominate the time complexity.  Figure 4 report the accuracies obtained when using WTA during inference on a learned model as compared to the baseline accuracy of the softmax model. We ﬁnd that even with as few as 30 re- trieved classes our approach is able to reach up to 83% of the baseline accuracy and almost matches the baseline accuracy with 3000 retrieved classes. Note that the ceiling on this problem is the accu- racy of the base network since we are approximating an already trained network using WTA. This vindicates our claim that only a small percentage of classes are relevant to any input example and WTA hashing provides an efﬁcient technique for obtaining the top K most relevant classes for a given input example. Based on these ﬁgures we conclude that the proposed approach is advanta- geous when either N is very large or for small batch sizes.  5  Accepted as a workshop contribution at ICLR 2015  Batch Size  K  1 1 1 8 8 8 32 32 32 64 64 64  30 300 3000 30 300 3000 30 300 3000 30 300 3000  Time (ms)  128.2  WTA Softmax 1.8 3.0 9.4 2.3 4.2 23.0 6.7 13.2 77.9 13.3 25.2 173.1  158.2  210.7  277.2  Speedup  71.2x 42.7x 13.6x 68.8x 37.7x 6.9x 31.3x 16.0x 2.7x 20.8x 11x 1.6x  Figure 3: Time taken by the WTA softmax layer and regular softmax layer alone for various values of batch size and top K for a prediction space of 21K classes during inference. WTA provides sig- niﬁcant speed-upds over softmax for small batch sizes and small values of K. Note that due to the sublinear nature of hash retreival, the speedups will be larger for bigger problems.  Figure 4: Accuracies obtained by the WTA model as the number of retrieved classes, K, is varied from 30 to 3000. Even with as few as 30 classes the WTA model is able to reach 83% of the accuracy of the baseline model and almost reaches the baseline accuracy for K = 3000. Note that this result uses WTA to just approximate an al- ready trained network and hence the ceiling is the accuracy of the base network.  Figure 5: Trade-off between the speed-up achieved over the baseline softmax model at a ﬁxed batch size and the percentage of the base- line accuracy reached. WTA achieves a speed-up of 10x at 90% of the baseline accuracy.  Figure 6: Speed-up achieved by WTA at 95% of the baseline accuracy at various batch sizes.  Figure 5 reports the trade-off between the speedup achieved over baseline softmax at a ﬁxed batch size and the percentage of the baseline accuracy reached by the WTA model. We ﬁnd that the WTA model achieves a speedup of 10x over the baseline model at 90% accuracy. Figure 6 reports the speedup achieved at 95% of the baseline accuracy for various batch sizes. As noted previously we ﬁnd that the WTA model achieves higher speedups for smaller batch sizes.  4.2 SKIPGRAM DATASET  One popular application of deep networks has been building and training models of language and semantics. Recent work from (Mikolov et al., 2013; Q.V. Le, 2014) has demonstrated that a shallow, simple architecture can be trained efﬁciently by across language corpora. The resulting embedding vector representation for language exhibits rich structure about semantics and syntactics that can be exploited for many other purposes. For all of these models, a crucial aspect of training is to predict surrounding and nearby words in a sequence. The prediction task is typically quite large, i.e. the cardinality is the size of the vocabulary, O(1M-10M) words.  6  Accepted as a workshop contribution at ICLR 2015  H-Softmax WTA-Softmax  precision@1 precision@3 precision@5 precision@10 precision@20 precision@50 precision@100 average precision  1.15% 2.36% 3.14% 4.52% 6.28% 9.63% 13.2% 2.31%  1.93% 5.18% 7.48% 10.2% 13.4% 16.5% 18.5% 4.53%  Table 1: The precison@k values of WTA softmax and hierarchical softmax models on the prediction problem of the skipgram dataset. WTA softmax achieves higher predictive accuracy even though it processes much fewer training examples in the alloted time.  A key insight of recent work has been to exploit novel and efﬁcient methods for performing discrete classiﬁcation over large cardinalities. In particular, said work employs a hierarchical softmax to fast inference and evaluation.  As a test of the predictive performance our hashing techniques, we compare the performance of WTA hashing on the language modeling task. We note that this is an extremely difﬁcult task the perplexity of language (or just cooccurrence statistics of words in a sentence) is quite high. Thus, any method attempts to predict nearby words will at best report low predictive performances.  In our experiments, we download and parse Wikipedia consisting of several billion sentences. We tokenize this text corpora with the 1M most popular words. The task of the network is to perform a 1M-way prediction nearby words based on neighboring words.  We performed our experiments with three loss functions, traditional softmax, hierarchical softmax and WTA-based softmax. We found measure the precision@K for the top K predictions from each softmax model.  We compare all networks with three loss functions after 100 hours of training time across similar CPU time. We ﬁnd that all networks have converged within this time frame although the hierarhical softmax has processed 100 billion examples while the WTA softmax has processed 100 million examples.  As seen in Table 1, we ﬁnd that WTA softmax achieves superior predictive performance than the hierarchical softmax even though hierarchical softmax has processed O(100) times more examples. In particular, we ﬁnd that WTA softmax achieves roughly two-fold better predictive performance.  However, the WTA softmax produces underlying embedding vector representations that do not per- form as well on analogy tasks as highlighted by (Mikolov et al., 2013). For instance, the hierarchical softmax achieves 50% accuracy on analogy tasks where as WTA softmax produces 5% accuracy on similar tasks. This is partly due to the fewer number of examples processed by WTA in the same time frame as hierarchical softmax is signiﬁcantly faster than WTA because it performs just log(N ) dot products.  4.3 VIDEO IDENTIFICATION  While the 21K problem is one of the largest tested for the baseline softmax model, the beneﬁts of hashing are best seen for problems of much larger cardinality. In order to illustrate this we next consider a large-scale classiﬁcation task for video identiﬁcation. This task is modeled on Youtube’s content ID classiﬁcation problem which has also been addressed in several recent work under various settings Shang et al. (2010); Song et al. (2011); Zhao et al. (2007).  The task we propose is to predict the ID of a video based on its frames. We use the Sports 1M action recognition dataset introduced in (Karpathy et al., 2014) for this problem. The Sports 1M dataset consists of roughly 1.2 million Youtube sports videos annotated with 487 classes. We divide the ﬁrst ﬁve minutes of each video into two parts where the ﬁrst 50% of the video’s frames are used for training and the remaining 50% are used for evaluating the models. The prediction space of the problem spans 1.2 million classes and each class has roughly 150 frames for training and evaluation.  7  Accepted as a workshop contribution at ICLR 2015  y c a r u c c A  0.7  0.6  0.5  0.4  0.3  0.2  0.1     0 0     WTA Softmax Regular Softmax Hierarchical Softmax  0.7  0.6  0.5  0.4  0.3  0.2  0.1  y c a r u c c A     WTA Softmax Regular Softmax Hierarchical Softmax  0.5  1  1.5  2  Steps  2.5 x 106  0   0  100  200  300  400  500  600  Time (hours)  Figure 7: Accuracy of the var- ious models on the Sports 1M evaluation set as a function of the number of training steps.  Figure 8: Accuracy of the var- ious models on the Sports 1M evaluation set as a function of the total training time.  s e s s a C  l    f o   e g a t n e c r e P  0.25  0.2  0.15  0.1  0.05     0 0     Sports 1M Imagenet  500  Variance  1000  1500  Figure 9: The in-class variance of the examples in the Imagenet 21k and the Sports 1M datasets. The examples within a class are more spread out in the Imagenet 21k dataset than the Sports 1M dataset.  We trained three models for this problem with the AlexNet architecture where the top layer uses one of softmax, WTA softmax and hierarchical softmax each. We used learning rates of {0.1, 0.05, 0.001, 0.005} and report the best results for each of the models. For WTA we used a value of 3000 for the K parameter based on the results in the previous section and a batch size of 32 for all models.  Figure 7 reports the accuracy on the evalution set against the number of steps trained for each model and Figure 8 reports the accuracy against the actual time taken to complete these steps. We ﬁnd that on both counts the WTA based model learns faster than both softmax and hierarchical softmax.  The step time of the WTA model is about 4 times lower than the softmax model but about 4 higher than hierarchical softmax. This is because hierarchical softmax is much more efﬁcient as it only computes log(N ) dot products compared to K for WTA. However, even though hierarchical softmax processes signiﬁcantly more number of examples the WTA models is able to achieve much higher accuracies.  In order to better understand the signiﬁcant difference between WTA and the baselines on this task as opposed to the Imagenet 21K problem we computed the in-class variance of all the classes in the two datasets based on the 4000-dim feature from the penultimate layer of the AlexNet model. Figure 9 reports a histogram of the in-class variance of the examples belonging to a class on the two datasets. We ﬁnd that in the Imagenet task the examples within a class are much more spread out than the Sports 1M task which is expected given that frames within a video would have similar context and more correlation. This could explain the relative efﬁciency of the top K gradient updates used by the WTA model on the Sports 1M task.  5 CONCLUSIONS  We proposed a locality sensitive hashing approach for approximating the computation of xT W in the classiﬁcation layer of deep network models which enables us to scale up the training and inference of these models to millions of classes. Empirical evaluations of the proposed model on various large-scale datasets shows that the proposed approach provides signiﬁcant speed-ups over baseline softmax models and can train such large-scale models at a faster rate than alternatives such as hierarchical softmax. Our approach is advantageous whenever the number of classes considered is large or where batching is not possible.  In the future we would like to extend this technique to intermediate layers also as the proposed method explicitly places sparsity constraints which is desirable in hierarchical learning. Given the scaling properties of hashing, our approach could, for instance, be used to increase the number of ﬁlters used in the convolutional layers from hundreds to tens of thousands with a few hundred being active at any time.  8  Accepted as a workshop contribution at ICLR 2015  REFERENCES Dean, Jeffrey, Corrado, Greg S., Monga, Rajat, Chen, Kai, Devin, Matthieu, Le, Quoc V., Mao, Mark Z., Senior, Andrew, Tucker, Paul, Yang, Ke, and Ng, Andrew Y. Large scale distributed deep networks. In Advances in Neural Information Processing Systems. 2012.  Dean, Thomas, Ruzon, Mark A., Segal, Mark, Shlens, Jonathon, Vijayanarasimhan, Sudheendra, and Yagnik, Jay. Fast, accurate detection of 100,000 object classes on a single machine. In Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, CVPR ’13, pp. 1814–1821. IEEE Computer Society, Washington, DC, USA, 2013. ISBN 978-0-7695-4989-7. doi: 10.1109/CVPR.2013.237. URL http://dx.doi.org/10.1109/CVPR.2013.237.  Gionis, Aristides, Indyk, Piotr, and Motwani, Rajeev. Similarity search in high dimensions via hashing. In Atkinson, Malcolm P., Orlowska, Maria E., Valduriez, Patrick, Zdonik, Stanley B., and Brodie, Michael L. (eds.), Proceedings of the 25th International Conference on Very Large Data Bases, pp. 518–529. Morgan Kaufmann, 1999.  Karpathy, Andrej, Toderici, George, Shetty, Sanketh, Leung, Thomas, Sukthankar, Rahul, and Fei-Fei, Li. Large-scale video classiﬁcation with convolutional neural networks. In Proc. CVPR, pp. 1725–1732. Colum- bus, Ohio, USA, 2014.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. ImageNet classiﬁcation with deep convolutional  neural networks. In Proc. NIPS, pp. 1097–1105. Lake Tahoe, Nevada, USA, 2012.  Le, Quoc V., Monga, Rajat, Devin, Matthieu, Chen, Kai, Corrado, Greg S., Dean, Jeff, and Ng, Andrew Y. In In International Conference on  Building high-level features using large scale unsupervised learning. Machine Learning. 2012.  Mikolov, Tomas, Sutskever,  Ilya, Chen, Kai, Corrado, Greg S., and Dean,  Representations of Words and Phrases and their Compositionality. tou, L., Welling, M., Ghahramani, Z., and Weinberger, K. Q. Information Processing Systems 26, pp. 3111–3119. Curran Associates, http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-  Jeff.  Distributed In Burges, C. J. C., Bot- (eds.), Advances in Neural URL  Inc., 2013.  Morin, Frederic and Bengio, Yoshua. Hierarchical probabilistic neural network language model.  TATS’05, pp. 246–252. 2005.  In AIS-  Q.V. Le, T. Mikolov. Distributed representations of sentences and documents. In ICML. 2014.  Shang, Lifeng, Yang, Linjun, Wang, Fei, Chan, Kwok-Ping, and Hua, Xian-Sheng. Real-time large scale near- duplicate web video retrieval. In Proceedings of the International Conference on Multimedia, MM ’10, pp. 531–540, New York, NY, USA, 2010. ACM. ISBN 978-1-60558-933-6. doi: 10.1145/1873951.1874021. URL http://doi.acm.org/10.1145/1873951.1874021.  Song, Jingkuan, Yang, Yi, Huang, Zi, Shen, Heng Tao, and Hong, Richang. Multiple feature hashing for real- time large scale near-duplicate video retrieval. In Proceedings of the 19th ACM International Conference on Multimedia, MM ’11, pp. 423–432, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0616-4. doi: 10.1145/2072298.2072354. URL http://doi.acm.org/10.1145/2072298.2072354.  Weston,  Jason, Bengio, Samy,  ulary image annotation. ence on Artiﬁcial 2011a. http://dx.doi.org/10.5591/978-1-57735-516-8/IJCAI11-460.  Intelligence - Volume Volume Three,  ISBN 978-1-57735-515-1.  In Proceedings of  Scaling up to large vocab- the Twenty-Second International Joint Confer- IJCAI’11, pp. 2764–2770. AAAI Press, URL  10.5591/978-1-57735-516-8/IJCAI11-460.  and Usunier, Nicolas.  Wsabie:  doi:  Weston,  Jason, Bengio, Samy,  ulary image annotation. ence on Artiﬁcial 2011b. http://dx.doi.org/10.5591/978-1-57735-516-8/IJCAI11-460.  Intelligence - Volume Volume Three,  ISBN 978-1-57735-515-1.  In Proceedings of  Scaling up to large vocab- the Twenty-Second International Joint Confer- IJCAI’11, pp. 2764–2770. AAAI Press, URL  10.5591/978-1-57735-516-8/IJCAI11-460.  and Usunier, Nicolas.  Wsabie:  doi:  Yagnik, Jay, Strelow, Dennis, Ross, David A., and Lin, Ruei-sung. The power of comparative reasoning. In  IEEE International Conference on Computer Vision. IEEE, 2011.  Zhao, Wan-Lei, Ngo, Chong-Wah, Tan, Hung-Khoon, and Wu, Xiao. learning. 10.1109/TMM.2007.898928.  identiﬁcation with interest 1037–1048, August 2007. http://dx.doi.org/10.1109/TMM.2007.898928.  Near-duplicate keyframe 9(5): URL  ISSN 1520-9210.  point matching  Trans. Multi.,  pattern doi:  and  9  ",
1412.7091,2015, Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets,"['Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets', 'Pascal Vincent']",https://arxiv.org/pdf/1412.7091,"5 1 0 2    l u J    4 1      ] E N . s c [      3 v 1 9 0 7  .  2 1 4 1 : v i X r a  Efﬁcient Exact Gradient Update for training Deep  Networks with Very Large Sparse Targets  [ Technical report ]  Département d’Informatique et de Recherche Opérationnelle  Pascal Vincent  Université de Montréal  Montréal, Québec, CANADA  and CIFAR  vincentp@iro.umontreal.ca  Département d’Informatique et de Recherche Opérationnelle  Alexandre de Brébisson  Université de Montréal  Montréal, Québec, CANADA  alexandre.de.brebisson@umontreal.ca  Xavier Bouthillier  Département d’Informatique et de Recherche Opérationnelle  Université de Montréal  Montréal, Québec, CANADA  xavier.bouthillier@iumontreal.ca  Abstract  An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Com- puting the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D × d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efﬁcient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approxi- mate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradi- ent for backpropagation, all in O(d2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields a speedup of D 4d, i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.  1  1  Introduction  Many modern applications of neural networks have to deal with data represented, or representable, as very large sparse vectors. Such representations arise in natural language related tasks, where the dimension D of that vector is typically (a multiple of) the size of the vocabulary, but also in the sparse user-item matrices of collaborative-ﬁltering applications. It is trivial to handle very large sparse inputs to a neural network in a computationally efﬁcient manner: the forward propagation and update to the input weight matrix after backpropagation are correspondingly sparse. By contrast, training with very large sparse prediction targets is problematic: even if the target is sparse, the computation of the equally large network output and the corresponding gradient update to the huge output weight matrix are not sparse and thus computationally prohibitive. This has been a practical problem ever since Bengio et al. [1] ﬁrst proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications [2]. Several approaches have been proposed to attempt to address this difﬁculty essentially by sidestepping it. They fall in two categories:  • Sampling or selection based approximations consider and compute only a tiny fraction of the output’s dimensions sampled at random or heuristically chosen. The reconstruction sampling of Dauphin et al. [3], the efﬁcient use of biased importance sampling in Jean et al. [4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al. [7] all fall under this category. As does the more recent use of approximate Maximum Inner Product Search based on Locality Sensitive Hashing techniques[8, 9] to select a good candidate subset.  the computation of the normalized probability of the target class.  • Hierarchical softmax [10, 7] imposes a heuristically deﬁned hierarchical tree structure for Compared to the initial problem of considering all D output dimensions, both kinds of approaches are crude approximations. In the present work, we will instead investigate a way to actually perform the exact gradient update that corresponds to considering all D outputs, but do so implicitly, in a computationally efﬁcient manner, without actually computing the D outputs. This approach works for a relatively restricted class of loss functions, the simplest of which is linear output with squared error (a natural choice for sparse real-valued regression targets). The most common choice for mul- ticlass classiﬁcation, the softmax loss is not part of that class, but we may use an alternative spherical softmax, which will also yield normalized class probabilities. For simplicity, our presentation will focus on squared error and on an online setting, and we only later brieﬂy mention its extension to minibatches and to a more general class of loss functions.  2 The problem  2.1 Problem deﬁnition and setup  We are concerned with gradient-descent based training of a deep feed-forward neural network with target vectors of very high dimension D (e.g. D = 200 000) but that are sparse, i.e. a comparatively small number, at most K (cid:28) D, of the elements of the target vector are non-zero. Such a K- sparse vector will typically be stored and represented compactly as 2K numbers corresponding to pairs (index, value). A network to be trained with such targets will naturally have an equally large output layer of dimension D. We can also optionally allow the input to the network to be a similarly high dimensional sparse vector of dimension Din. Between the large sparse target, output, and (optionally large sparse) input, we suppose the network’s intermediate hidden layers to be of smaller, more typically manageable, dimension d (cid:28) D (e.g. d = 500)1. Mathematical notation: Vectors are denoted using lower-case letters, e.g. h, and are considered column-vectors; corresponding row vectors are denoted with a transpose, e.g. hT . Matrices are denoted using upper-case letters, e.g. W , with W T the transpose of W . The ith column of W is  denoted Wi , and its ith row W:i (both viewed as a column vector). U−T =(cid:0)U−1(cid:1)T denotes the transpose of the inverse of a square matrix. Id is the d × d identity matrix.  1Our approach does not impose any restriction on the architecture nor size of the hidden layers, as long as  they are amenable to usual gradient backpropagation.  2  Figure 1: The computational problem posed by very large sparse targets. Dealing with sparse in- put efﬁciently is trivial, with both the forward and backward propagation phases easily achieved in O(Kd). However this is not the case with large sparse targets. They incur a prohibitive compu- tational cost of O(Dd) at the output layer as forward propagation, gradient backpropagation and weight update each require accessing all D × d elements of the large output weight matrix.  Network architecture: We consider a standard feed forward neural network architecture as de- picted in Figure 1. An input vector x ∈ RDin is linearly transformed into a linear activation a(1) = W (1)T x + b(1) through a Din × d input weight matrix W (1) (and an optional bias vector b(1) ∈ Rd). This is typically followed by a non-linear transformation s to yield the representation of the ﬁrst hidden layer h(1) = s(a(1)). This ﬁrst hidden layer representation is then similarly trans- formed through a number of subsequent non-linear layers (that can be of any usual kind amenable to backpropagation) e.g. h(k) = s(a(k)) with a(k) = W (k)T h(k−1) + b(k) until we obtain last hidden layer representation h = h(m). We then obtain the ﬁnal D-dimensional network output as o = W h where W is a D × d output weight matrix, which will be our main focus in this work. Finally, the network’s D-dimensional output o is compared to the D-dimensional target vector y associated with input x using squared error, yielding loss L = (cid:107)o − y(cid:107)2. Training procedure: This architecture is a typical (possibly deep) multi-layer feed forward neural network architecture with a linear output layer and squared error loss. Its parameters (weight matri- ces and bias vectors) will be trained by gradient descent, using gradient backpropagation Rumelhart et al. [11], LeCun [12, 13] to efﬁciently compute the gradients. The procedure is shown in Figure 1. Given an example from the training set as an (input,target) pair (x, y), a pass of forward propagation proceeds as outlined above, computing the hidden representation of each hidden layer in turn based on the previous one, and ﬁnally the network’s predicted output o and associated loss L. A pass of gradient backpropagation then works in the opposite direction, starting from ∇o = ∂L ∂o = 2(o − y) and propagating back the gradients ∇h(k) = ∂L ∂a(k) upstream through the net- work. The corresponding gradient contributions on parameters (weights and biases), collected along the way, are straightforward once we have the associated ∇a(k). Speciﬁcally they are ∇b(k) = ∇a(k) and ∇W (k) = h(k−1)(∇a(k))T . Similarly for the input layer ∇W (1) = x(∇a(1))T , and for the output layer ∇W = (o − y)hT . Parameters are then updated through a gradient descent step W (k) ← W (k) − η∇W (k) and b(k) ← b(k) − η∇b(k), where η is a positive learning-rate. Similarly for the output layer which will be our main focus here: W ← W − η∇W .  ∂h(k) and ∇a(k) = ∂L  3  Efﬁcient Exact Gradient Update for Training Deep Networks with Very Large Sparse TargetsPascal Vincent *          Alexandre de Brébisson        Xavier BouthillierAbstractAn important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 500 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D ! d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efﬁcient handling of large sparse network inputs is trivial, this case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach that, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. Training time is thus independent of output-layer size (or number of classes). Compared to naive backprop, the proposed algorithm is expected to yield an actual speedup of at least D/4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computation that often dominates the training time in this kind of network architecture.The Problem‣Training deep neural networks with very large sparse targets is an important problem‣Arises e.g. in Neural Language Models [1] with large vocabulary size (e.g. D = 500 000 one-hot target).‣Efﬁcient handling of large sparse inputs is trivial.‣But backprop training with large sparse targets is prohibitively expensive.‣Focus on output layer: maps last hidden representation h of reasonable dimension d (e.g. 500)to very large output o of dimension D (e.g. 500 000) with a Dxd parameter matrix W:Experimental validationTiming of output layer computations, for  CPU implementation on 2 GHz Intel Core i7. Minibatch size m =10.Both naive backprop version and the proposed factorised parameter version learn the same actual W. Detailed algorithm, beneﬁts and limitationsAcceptedasaworkshopcontributionatICLR20153.5PUTTINGITALLTOGETHER:ALGORITHMFORCOMPUTINGTHECOSTL,GRADIENTONh,ANDUPDATINGUANDVEfﬁcientcomputationofcostL,gradientwithrespecttoh(tobelaterbackpropagatedfurther)aswellasupdatingUandVandperformingthebookkeepingforU−TandQ.Thefollowingtabledescribesthealgorithmicstepsthatweputtogetherfromtheequationsderivedabove.Step#OperationComputationalcomplexityNumberofmultiply-adds1:ˆh=QhO(d2)d22:ˆy=UT(VTy)O(Kd+d2)Kd+d23:ˆz=ˆh−ˆyO(d)d4:∇h=2ˆzO(d)d5:L=hTˆh−2hTˆy+yTyO(2d+K)2d+K+16:Unew=U−2η(Uh)hTO(d2)2d2+d7:U−Tnew=U−T+2η1−2η￿h￿2(U−Th)hTO(d2)2d2+2d+38:Vnew=V+2ηy(U−Tnewh)TO(d2+Kd)d2+K+Kd9:Qnew=Q−2η￿hˆzT+ˆzhT￿+(4η2L)hhTO(d2)4+2d+3d24DISCUSSION:EXPECTEDBENEFITS,EXTENSIONSANDLIMITATIONSHavingK￿d￿DweseethattheproposedalgorithmrequiresO(d2)operationswhereasthestandardapproachrequiredO(Dd)operations.IfwetakeK≈d,wemaystatemorepreciselythattheproposedalgorithm,forcomputingthelossandthegradientupdateswillrequiresroughly12d2operationswhereasthestandardapproachrequiredroughly3Ddoperations.SooveralltheproposedalgorithmchangecorrespondstoacomputationalspeedupbyafactorofD4d.ForD=200000andd=500theexpectedspeedupisthus100.Notethattheadvantageisnotonlyincomputationalcomplexity,butalsoinmemoryaccess.Foreachexample,thestandardapproachneedstoaccessandchangeallD×delementsofmatrixW,whereastheproposedapproachonlyaccessesthemuchsmallernumberK×delementofVaswellasthethreed×dmatricesU,U−T,andQ.Sooverallwehaveamuchfasteralgorithm,whichwhiledoingsoimplicitly,willhoweverperformtheexactsamegradientupdateasthestandardapproach.Wewanttoemphasizeherethatwhatwearedoingisnotatallthesameassimplychaining2linearlayersUandVandperformingordinarygradientdescentupdatesonthese:thiswouldresultinthesameprohibitivecomputationalcomplexityasthestandardapproach,andsuchordinaryseparategradientupdatestoUandVwouldnotbeequivalenttotheordinarygradientupdatetoW=VU.Ouralgorithmcanbestraightforwardlyextendedtotheminibatchcase,andisexpectedtoyieldthesamespeedupfactorcomparedtothestandardapproach.ButoneneedstobecarefulinordertokeepthecomputationofU−Threasonablyefﬁcient.Indeed,dependingonthesizeoftheminibatchm,itmaybemoreefﬁcienttoresolvethecorrepsondinglinearequationforeachminibatchfromscratchratherthanupdatingU−TwiththeWoodburyequation(whichgeneralizestheSheman-Morrisonformulaform>1).Thisapproachthatwedetailedforlinearoutputandsquarederrorcaneasilybeextendedtoslightlymoreexoticlossfunctions:basicallyanylossfunctionthatcanbeexpressedusingonlytheocassociatedtonon-zeroycand￿o￿2=￿jo2jthesquarednormofthewholeoutputvector,whichwecancomputecheaply.Thisfamilyoflossfunctionsdoesnotincludethestandardsoftmax,butincludestheso-calledsphericalsoftmax:logo2c￿jo2j(wherecisthecorrectclasslabel).Itremainstobeseeninpracticehowthisapproachperformscomputationally,andwhetherwelosesomethingduetousingthismorelimitedfamilyoflossfunctions.7...￿￿￿￿...￿￿￿￿(large D, but K-sparse) (large D, but K-sparse) ...(small d)...large D, not sparse LossInput xTarget yOutput olast hidden hL=￿o−y￿2hidden 2(small d)hidden 1(small d)O(Kd)  O(d2)  O(d2)  O(Dd)  Prohibitivley expensive!Ex: D = 500 000, K=5Ex: d = 500O(D)  O(d2)  O(d2)  O(d2)  Forward propagationBackpropagation(dxd)W(2)O(D)  !o = 2(o-y)  O(Dd)  !h = W T !o  O(Dd)  W ""W- ! !o hT  O(Kd)  W(1) ""W (1)- ! x !aT  cheap!W(1)(Dxd)Prohibitivley expensive!Altogether: O( Dd )  3Problem: expensive computationwe suppose K << d << D o = Wh* and CIFARProposed approachWe can do much better than O( Dd ). We can compute!loss L!gradient w.r.t. last hidden layer !h !exact same gradient update to Wall in O(d2) without ever computing full output o=Wh !First trick: L and !h can be computed efﬁciently if we keep an up-to-date d x d matrix Q = WTW Second trick: represent W implicitly as factorization     and update U and V instead5.1ComputingthesquarederrorlosseﬃcientlySupposewehave,foranetworkinputexamplex,computedlasthiddenrepre-sentationh∈Rdthroughforwardpropagation.Thenetwork’sDdimensionaloutputo=Whistheninprinciplecomparedtohighdimensionaltargety∈RD.ThecorrespondingsquarederrorlossisL=￿Wh−y￿2.AswehaveseeninSection3.3,computingitinthedirectnaivewaywouldhaveaprohibitivecom-putationalcomplexityofO(Dd+D)=O(Dd)becausecomputingoutputWhwithafullD×dmatrixWandatypicallynon-sparsehisO(Dd).Notehoweverthatwecanrewritethisas:L=￿Wh−y￿2=(Wh−y)T(Wh−y)=hTWTWh−yTWh−hTWTy+yTy=hTQh−2hT(WTy)+yTy=hTQh−2hTUTVTy+yTy=hT(Qh)−2hT(UT(VTy))+yTy=hT(Qh￿￿￿￿ˆh−2(UT(VTy)￿￿￿￿ˆy)+yTySHORTIDEAFORMULATIONFORSLIDES:L=￿O(Dd)￿￿￿￿Wh−y￿2=(Wh−y)T(Wh−y)=hTWTWh−2hT(WTy)+yTy=hT(Qh￿￿￿￿O(d2)−2(WTy)￿￿￿￿O(Kd))+yTy￿￿￿￿O(K)withQ=WTWSupposingwehavemaintainedanup-to-dateQ=WTW,whichisacompactd×dmatrix(wewillseehowweupdateQcheaplyinsection??????),computingˆh=QhhasacomplexityofO(d2).ThankstotheK−sparsityandsparserepresentationofy,computingVTyisO(Kd)andresultsinad−dimensionalvector,sothatcomputingˆy=UT(VTy)isO(Kd+d2).ThelasttermisO(K).SotheoverallcomputationalcomplexityforcomputingLinthiswayisO(Kd+d2)=O((K+d)d).WithK￿Dandd￿DthiscanbeseveralordersofmagnitudecheaperthantheprohibitiveO(Dd)ofthedirectapproach.Ifwedeﬁneintermediatevectorsˆh=Qhandˆy=WTy=UT(VTy)thecomputationofLcanberewrittenalittlemorecompactlyasL=hT(ˆh−2ˆy)+￿y￿25this is O(Kd +d2 +K) = O(d2)Computing loss L5.2ComputingthegradientonheﬃcientlyTobackpropagatethegradientthroughthenetwork,weneedtocomputethegradientoflossLwithrespecttolasthiddenlayerrepresentationh.Thisis∇h=∂L∂h=∂￿Wh−y￿2∂h=2WT(Wh−y).Again,ifweweretocomputeitdirectlyinthismannerthecomputationalcomplexitywouldbeaprohibitiveO(Dd).Butwecaninsteadrewriteitas∇h=∂L∂h=∂￿Wh−y￿2∂h=2WT(Wh−y)=2￿WTWh−WTy￿=2￿Qh−UTVTy￿=2￿Qh−UT(VTy)￿=2(ˆh−ˆy)Again,supposingwehavemaintainedanup-to-dateQ(wewillseehowweupdateQcheaplyinsection?????)computing∂L∂hthiswayisO(Kd+d2)=O((K+d)d),muchcheaperthantheO(Dd)ofthedirectapproach.SHORTIDEAFORMULATIONFORSLIDES:∇h=∂L∂h=∂￿Wh−y￿2∂h=2WT(Wh−y)=2(Qh￿￿￿￿O(d2)−WTy￿￿￿￿O(Kd))5.3EﬃcientgradientupdateofWThegradientofthesquarederrorlosswithrespecttooutputlayerweightmatrixWis∂L∂W=∂￿Wh−y￿2∂W=2(Wh−y)hT.AndthecorrespondinggradientdescentupdatetoWwouldbeWnew←W−2η(Wh−y)hTwhereηisapositivelearningrate.Again,computedinthismanner,thisinducesaprohibitiveO(Dd)computationalcomplexity,bothtocomputeoutputandresidueWh−y,andthentoupdatealltheDdelementsofW(sincegenerallyneitherWh−ynorhwillbesparse).ToovercomethisdiﬃcultyletusﬁrstrewritetheupdateasWnew=W−2η(Wh−y)hT=W−2ηWhhT+2ηyhTNotethatwecandecomposethisupdateintotwoconsecutiveupdatesteps:6this is O(Kd +d2) = O(d2)Provided we maintain an up-to-date Q = WTW (achievable cheaply) Computing gradient !h  w.r.t. last hidden layerW￿￿￿￿D×d=V￿￿￿￿D×dU￿￿￿￿d×d5.2ComputingthegradientonheﬃcientlyTobackpropagatethegradientthroughthenetwork,weneedtocomputethegradientoflossLwithrespecttolasthiddenlayerrepresentationh.Thisis∇h=∂L∂h=∂￿Wh−y￿2∂h=2WT(Wh−y).Again,ifweweretocomputeitdirectlyinthismannerthecomputationalcomplexitywouldbeaprohibitiveO(Dd).Butwecaninsteadrewriteitas∇h=∂L∂h=∂￿Wh−y￿2∂h=2WT(Wh−y)=2￿WTWh−WTy￿=2￿Qh−UTVTy￿=2￿Qh−UT(VTy)￿=2(ˆh−ˆy)Again,supposingwehavemaintainedanup-to-dateQ(wewillseehowweupdateQcheaplyinsection?????)computing∂L∂hthiswayisO(Kd+d2)=O((K+d)d),muchcheaperthantheO(Dd)ofthedirectapproach.SHORTIDEAFORMULATIONFORSLIDES:∇h=∂L∂h=∂￿Wh−y￿2∂h=2WT(Wh−y)=2(Qh￿￿￿￿O(d2)−WTy￿￿￿￿O(Kd))5.3EﬃcientgradientupdateofWThegradientofthesquarederrorlosswithrespecttooutputlayerweightmatrixWis∂L∂W=∂￿Wh−y￿2∂W=2(Wh−y)hT.AndthecorrespondinggradientdescentupdatetoWwouldbeWnew←W−2η(Wh−y)hTwhereηisapositivelearningrate.Again,computedinthismanner,thisinducesaprohibitiveO(Dd)computationalcomplexity,bothtocomputeoutputandresidueWh−y,andthentoupdatealltheDdelementsofW(sincegenerallyneitherWh−ynorhwillbesparse).ToovercomethisdiﬃcultyletusﬁrstrewritetheupdateasWnew=W−2η(Wh−y)hT=W−2ηWhhT+2ηyhTNotethatwecandecomposethisupdateintotwoconsecutiveupdatesteps:6Naive gadient update is a rank-one update to W (all Dd elements of W modiﬁed!)Equivalently decomposed in 2 sequential steps:O( Dd )  a)W←W−2ηWhhTb)W←W+2ηyhTWewillnowseehowwecanperformeachoftheseupdatesimplicitlybyupdatingonlyUandVrespectively,aswellashowwemaintaincorrespondinglyup-to-dateversionsofQ=VTV(neededtoeﬃcientlycomputecostLandgradientonhinEquations????and????above)andU−T=(U−1)T(thatwillbeneededforupdateb)).Solution:a)Unew=U−2η(Uh)hTb)Vnew=V+2ηy(U−Tnewh)TProof:VnewUnew=(V+2ηy(U−Tnewh)T)Unew=VUnew+2ηy(U−Tnewh)TUnew=VUnew+2ηyhTU−1newUnew=V(U−2η(Uh)hT)+2ηyhT(U−1newUnew)=VU−2ηVUhhT+2ηyhT=VU−2η(VUh−y)hT=W−2η(Wh−y)ThT=Wnewa)FirstupdateoftheformW←W−2ηWhhTThiscanbeachievedimplicitlybyupdatingonlyUasfollows:Unew=U−2η(Uh)hTProof:Wnew=VUnew=V(U−2η(Uh)hT)=VU−2ηVUhhT=W−2ηWhhTChangingUdoesn’tchangeQ=VTV.Butwewillneedanup-to-dateU−Tinthesecondupdateb).ProvidedwealreadyhaveU−TthiscanbeachievedcheaplybyusingtheSherman-Morissonformulafortherank-oneupdatetotheinverseofU:(U+uvT)−1=U−1−11+vTU−1uU−1uvTU−17a)W←W−2ηWhhTb)W←W+2ηyhTWewillnowseehowwecanperformeachoftheseupdatesimplicitlybyupdatingonlyUandVrespectively,aswellashowwemaintaincorrespondinglyup-to-dateversionsofQ=VTV(neededtoeﬃcientlycomputecostLandgradientonhinEquations????and????above)andU−T=(U−1)T(thatwillbeneededforupdateb)).Solution:a)Unew=U−2η(Uh)hTb)Vnew=V+2ηy(U−Tnewh)TProof:VnewUnew=(V+2ηy(U−Tnewh)T)Unew=VUnew+2ηy(U−Tnewh)TUnew=VUnew+2ηyhTU−1newUnew=V(U−2η(Uh)hT)+2ηyhT(U−1newUnew)=VU−2ηVUhhT+2ηyhT=VU−2η(VUh−y)hT=W−2η(Wh−y)ThT=Wnewa)FirstupdateoftheformW←W−2ηWhhTThiscanbeachievedimplicitlybyupdatingonlyUasfollows:Unew=U−2η(Uh)hTProof:Wnew=VUnew=V(U−2η(Uh)hT)=VU−2ηVUhhT=W−2ηWhhTChangingUdoesn’tchangeQ=VTV.Butwewillneedanup-to-dateU−Tinthesecondupdateb).ProvidedwealreadyhaveU−TthiscanbeachievedcheaplybyusingtheSherman-Morissonformulafortherank-oneupdatetotheinverseofU:(U+uvT)−1=U−1−11+vTU−1uU−1uvTU−17That can be performed implicity through U and V:rank-1 update to U: O(d2)O(Kd)O(d2)provided we updated U-1 cheaplyusing Sherman-MorrisonSparse update: only K rows of V instead of all D rows of W !O( Dd )  Proof:AcceptedasaworkshopcontributionatICLR2015a)W←W−2ηWhhTb)W←W+2ηyhTNoticethatwecanperformeachoftheseupdatesimplicitlybyupdatingonlyUandVrespectively.:a)Unew=U−2η(Uh)hT(4)b)Vnew=V+2ηy(U−Tnewh)T(5)ThisresultsinimplicitlyupdatingWaswedidexplicitlyinthenaiveapproachofEq.3.Proof:VnewUnew=(V+2ηy(U−Tnewh)T)Unew=VUnew+2ηy(U−Tnewh)TUnew=VUnew+2ηyhTU−1newUnew=V(U−2η(Uh)hT)+2ηyhT(U−1newUnew)=VU−2ηVUhhT+2ηyhT=VU−2η(VUh−y)hT=W−2η(Wh−y)ThT=WnewWeseethattheupdateofUinEq.4isasimpleO(d2)operation.Followingthissimplerank-oneupdatetoU,wecanusetheSherman-Morrisonformulatoderivethecorrespondingrank-oneupdatetoU−TwhichwillalsobeO(d2):U−Tnew=U−T+2η1−2η￿h￿2(U−Th)hT(6)ItistheneasytocomputetheU−Tnewh,anO(d2)operationneededinEq.5,andtheensuingrank-oneupdateofV,thankstotheK-sparsityofyisonlyO(Kd).ThankstotheK−sparsityandsparserepresentationofy,computingˆy=VTyisO(Kd)and￿t￿2isO(K).Computationofˆh=U−ThisO(d2).Giventhese,theupdateofQisO(d2)andtherank-oneupdateofV,thankstotheK-sparsityofyisO(Kd).SotheseoperationstogetherhavecomputationalcomplexityofO(Kd+d2)=O((K+d)d),whichismuchcheaperthantheprohibitiveO(Dd)ofthedirectapproach.3.4BOOKKEEPING:KEEPINGANUP-TO-DATEQANDU−TWehavealreadyseen,inEq.6,howwecancheaplymaintainanup-to-dateU−TfollowingourupdateofU.Similarly,followingourupdatestoUandV,weneedtokeepanup-to-dateQ=WTWwhichisneededtoefﬁcientlycomputethelossL(Eq.1)andgradient∇h(Eq.2).TheupdatestoUandVinEquations4and5areequivalenttoimplicitlyupdatingWasinEq.3,andthistranslatesintothefollowingupdatetoQ=WTW:ˆz=Qh−UT(VTy)Qnew=Q−2η￿hˆzT+ˆzhT￿+(4η2L)hhT(7)Proofisstraightforwardbutnotprovidedhereduetospaceconstraints.6Bookkeeping operations as we update U and V:!Using factored representation of W=VU does not change the complexity of the computation of L and  !h .!Need to maintain an up-to-date U-1 following rank-1 update to U.  "" achieved in O(d2) through Sherman-Morrison formula. !Need to maintain an up-to-date Q = WTW following updates to U and V. "" achieved in O(d2) as follows:a)W←W−2ηWhhTb)W←W+2ηyhTWewillnowseehowwecanperformeachoftheseupdatesimplicitlybyupdatingonlyUandVrespectively,aswellashowwemaintaincorrespondinglyup-to-dateversionsofQ=VTV(neededtoeﬃcientlycomputecostLandgradientonhinEquations????and????above)andU−T=(U−1)T(thatwillbeneededforupdateb)).Solution:a)Unew=U−2η(Uh)hTb)Vnew=V+2ηy(U−Tnewh)TProof:VnewUnew=(V+2ηy(U−Tnewh)T)Unew=VUnew+2ηy(U−Tnewh)TUnew=VUnew+2ηyhTU−1newUnew=V(U−2η(Uh)hT)+2ηyhT(U−1newUnew)=VU−2ηVUhhT+2ηyhT=VU−2η(VUh−y)hT=W−2η(Wh−y)ThT=WnewSHORTFORMULATIONFORSLIDESOFUPDATEOFQINON-LINECASE:ˆz=Qh−UT(VTy)Qnew=Q−2η￿hˆzT+ˆzhT￿+(4η2L)hhTa)FirstupdateoftheformW←W−2ηWhhTThiscanbeachievedimplicitlybyupdatingonlyUasfollows:Unew=U−2η(Uh)hTProof:Wnew=VUnew=V(U−2η(Uh)hT)=VU−2ηVUhhT=W−2ηWhhT7Note: this is NOT the same as a ordinary backprop update on two consecutive layers U and V which would still be O( Dd ).Altogether: O( d2 )  we suppose K << d << D we suppose K << d << D Current workarounds are approximations:‣Sampling based approximations compute only a tiny fraction of the output’s dimensions sampled at random. Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.‣Hierarchical softmax [6, 4] imposes a heuristically deﬁned hierarchical tree structure for the computation of the normalized probability of the target class.[1] Bengio, Y., Ducharme, R., and Vincent, P. (2001). A neural probabilistic language model. NIPS 2000.[2] Dauphin, Y., Glorot, X., and Bengio, Y. (2011). Large-scale learning of embeddings with reconstruction sampling. ICML 2011.[5] Mnih, A. and Kavukcuoglu, K. (2013). Learning word embeddings efﬁciently with noise-contrastive estimation. NIPS 2013.[6] Morin, F. and Bengio, Y. (2005). Hierarchical probabilistic neural network language model. AISTATS 2005.[3] Gutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. AISTATS 2010.[4] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁcient estimation of word representations in vector space. ICLR 2013 workshop track.we suppose K << d << D Full algorithm (online version):!Computation:  O(12 d2) v.s. O(3 Dd)     "" speedup of D/4d        for typical sizes: between 50 and 300!Memory access: for each example access only Kd elements of  V and d2 elements of  U, U-1 and Q v.s. Dd elements of  W.Anticipated beneﬁts:!Approach limited to loss functions expressible using ||o||2 and the oc associated to non-zero yc only:✓ linear output + squared error# not regular log softmax✓ linear+spherical softmax: !Step 6 can lead over time to ill conditioning "" must periodically apply numerical stabilization strategy.LimitationsExtension for minibatch of size m:!Straightforward except for step 7:!Update of U-T no longer with simple Sherman-Morrison. !Several possibilities: Woodbury identity (must invert m x m matrix), or iterated Sherman-Morrison, or solving UTx = h each time. Best choice will depends on m.!"" complexity remains O(d2) per example.AcceptedasaworkshopcontributionatICLR20153.5PUTTINGITALLTOGETHER:ALGORITHMFORCOMPUTINGTHECOSTL,GRADIENTONh,ANDUPDATINGUANDVEfﬁcientcomputationofcostL,gradientwithrespecttoh(tobelaterbackpropagatedfurther)aswellasupdatingUandVandperformingthebookkeepingforU−TandQ.Thefollowingtabledescribesthealgorithmicstepsthatweputtogetherfromtheequationsderivedabove.Step#OperationComputationalcomplexityNumberofmultiply-adds1:ˆh=QhO(d2)d22:ˆy=UT(VTy)O(Kd+d2)Kd+d23:ˆz=ˆh−ˆyO(d)d4:∇h=2ˆzO(d)d5:L=hTˆh−2hTˆy+yTyO(2d+K)2d+K+16:Unew=U−2η(Uh)hTO(d2)2d2+d7:U−Tnew=U−T+2η1−2η￿h￿2(U−Th)hTO(d2)2d2+2d+38:Vnew=V+2ηy(U−Tnewh)TO(d2+Kd)d2+K+Kd9:Qnew=Q−2η￿hˆzT+ˆzhT￿+(4η2L)hhTO(d2)4+2d+3d24DISCUSSION:EXPECTEDBENEFITS,EXTENSIONSANDLIMITATIONSHavingK￿d￿DweseethattheproposedalgorithmrequiresO(d2)operationswhereasthestandardapproachrequiredO(Dd)operations.IfwetakeK≈d,wemaystatemorepreciselythattheproposedalgorithm,forcomputingthelossandthegradientupdateswillrequiresroughly12d2operationswhereasthestandardapproachrequiredroughly3Ddoperations.SooveralltheproposedalgorithmchangecorrespondstoacomputationalspeedupbyafactorofD4d.ForD=200000andd=500theexpectedspeedupisthus100.Notethattheadvantageisnotonlyincomputationalcomplexity,butalsoinmemoryaccess.Foreachexample,thestandardapproachneedstoaccessandchangeallD×delementsofmatrixW,whereastheproposedapproachonlyaccessesthemuchsmallernumberK×delementofVaswellasthethreed×dmatricesU,U−T,andQ.Sooverallwehaveamuchfasteralgorithm,whichwhiledoingsoimplicitly,willhoweverperformtheexactsamegradientupdateasthestandardapproach.Wewanttoemphasizeherethatwhatwearedoingisnotatallthesameassimplychaining2linearlayersUandVandperformingordinarygradientdescentupdatesonthese:thiswouldresultinthesameprohibitivecomputationalcomplexityasthestandardapproach,andsuchordinaryseparategradientupdatestoUandVwouldnotbeequivalenttotheordinarygradientupdatetoW=VU.Ouralgorithmcanbestraightforwardlyextendedtotheminibatchcase,andisexpectedtoyieldthesamespeedupfactorcomparedtothestandardapproach.ButoneneedstobecarefulinordertokeepthecomputationofU−Threasonablyefﬁcient.Indeed,dependingonthesizeoftheminibatchm,itmaybemoreefﬁcienttoresolvethecorrepsondinglinearequationforeachminibatchfromscratchratherthanupdatingU−TwiththeWoodburyequation(whichgeneralizestheSheman-Morrisonformulaform>1).Thisapproachthatwedetailedforlinearoutputandsquarederrorcaneasilybeextendedtoslightlymoreexoticlossfunctions:basicallyanylossfunctionthatcanbeexpressedusingonlytheocassociatedtonon-zeroycand￿o￿2=￿jo2jthesquarednormofthewholeoutputvector,whichwecancomputecheaply.Thisfamilyoflossfunctionsdoesnotincludethestandardsoftmax,butincludestheso-calledsphericalsoftmax:logo2c￿jo2j(wherecisthecorrectclasslabel).Itremainstobeseeninpracticehowthisapproachperformscomputationally,andwhetherwelosesomethingduetousingthismorelimitedfamilyoflossfunctions.7Prohibitive!Time taken by naive backprop (dotted lines) and the proposed factorised parameter version (full lines).Speedup of factorised parameter version v.s. naive backprop (theoretical and experimentally measured).Conclusion and future work‣We developed an original algorithm that yields a huge speedup for performing a full exact gradient update in networks with very large sparse targets: remarkably time is independent of output size (number of classes).‣Gain is from a fundamental algorithmic computational complexity improvement, not from low-level hardware-speciﬁc tricks or tuning.  ‣Future: GPU implementation; spherical softmax cost; compare quality of word embeddings learned with these costs to standard softmax.  References:2.2 The easy part: input layer forward propagation and weight update  It is easy and straightforward to efﬁciently compute the forward propagation, and the backpropa- gation and weight update part for the input layer when we have a very large Din-dimensional but K−sparse input vector x with appropriate sparse representation. Speciﬁcally we suppose that x is represented as a pair of vectors u, v of length (at most) K, where u contains integer indexes and v the associated real values of the elements of x such that xi = 0 if i /∈ u, and xuk = vk.  k=1 vkW (1)  :uk where each W (1)  O(Kd) operation rather than O(Dd).  • Forward propagation through the input layer: The sparse representation of x as the positions of K elements together with their value makes it cheap to compute W (1)T x. Even though W (1) may be a huge full Din × d matrix, only K of its rows (those cor- responding to the non-zero entries of x) need to be visited and summed to compute W (1)T x. Precisely, with our (u, v) sparse representation of x this operation can be written asW (1)T x = (cid:80)K :uk is a d-dimensional vector, making this an • Gradient and update through input layer: Let us for now suppose that we were able to get gradients (through backpropagation) up to the ﬁrst hidden layer activations a(1) ∈ Rd in the form of gradient vector ∇a(1) = ∂L ∂a(1) . The corresponding gradient-based update to input layer weights W (1) is simply W (1) ← W (1) − ηx(∇a(1))T . This is a rank-one update to W (1). Here again, we see that only the K rows of W (1) associated to the (at most) K non-zero entries of x need to be modiﬁed. Precisely this operation can be written as:W (1) :uk ← W (1) :uk − ηvk∇a(1) ∀k ∈ {1, . . . , K} making this again a O(Kd) operation rather than O(Dd).  2.3 The hard part: output layer propagation and weight update  Given some network input x we suppose we can compute without difﬁculty through forward propa- gation the associated last hidden layer representation h ∈ Rd. From then on:  y) which is another O(Dd) matrix-vector product.  • Computing the ﬁnal output o = W h incurs a prohibitive computational cost of O(Dd) since W is a full D×d matrix. Note that there is a-priori no reason for representation h to be sparse (e.g. with a sigmoid non-linearity) but even if it was, this would not fundamentally change the problem since it is D that is extremely large, and we supposed d reasonably sized already. Computing the residual (o − t) and associated squared error loss (cid:107)o − t(cid:107)2 incurs an additional O(D) cost. • The gradient on h that we need to backpropagate to lower layers is ∇h = ∂L ∂h = 2W T (o − • Finally, when performing the corresponding output weight update W ← W − η(o − y)hT we see that it is a rank-one update that updates all D×d elements of W , which again incurs a prohibitive O(Dd) computational cost. For very large D, all these three O(Dd) operations are prohibitive, and the fact that y is sparse, seen from this perspective, doesn’t help, since neither o nor o − y will be sparse. 3 A computationally efﬁcient algorithm for performing the exact online  gradient update  Previously proposed workarounds are approximate or use stochastic sampling. We propose a differ- ent approach that results in the exact same, yet efﬁcient gradient update, remarkably without ever having to compute large output o.  3.1 Computing the squared error loss L and the gradient with respect to h efﬁciently  Suppose that, we have, for a network input example x, computed the last hidden representation h ∈ Rd through forward propagation. The network’s D dimensional output o = W h is then in principle compared to the high dimensional target y ∈ RD. The corresponding squared error loss is L = (cid:107)W h − y(cid:107)2. As we saw in Section 2.3, computing it in the direct naive way would have a prohibitive computational complexity of O(Dd + D) = O(Dd) because computing output W h  4  with a full D × d matrix W and a typically non-sparse h is O(Dd). Similarly, to backpropagate the gradient through the network, we need to compute the gradient of loss L with respect to last hidden layer representation h. This is ∇h = ∂L = 2W T (W h − y). So again, if we were to compute it directly in this manner, the computational complexity would be a prohibitive O(Dd). Provided we have maintained an up-to-date matrix Q = W T W , which is of reasonable size d × d and can be cheaply maintained as we will see in Section 3.3, we can rewrite these two operations so as to perform them in O(d2):  ∂h = ∂(cid:107)W h−y(cid:107)2  ∂h  Loss computation:  Gradient on h:  L = (cid:107)  O(Dd)  (cid:122)(cid:125)(cid:124)(cid:123)W h −y(cid:107)2  = (W h − y)T (W h − y) = hT W T W h − yT W h − hT W T y + yT y = hT Qh − 2hT (W T y) + yT y = hT ( Qh ) + yT y (cid:124)(cid:123)(cid:122)(cid:125)O(K)  −2 W T y (cid:124)(cid:123)(cid:122)(cid:125)  (cid:124)(cid:123)(cid:122)(cid:125)O(d2)  O(Kd)  (1)  ∇h =  ∂L ∂h  ∂h  ∂(cid:107)W h − y(cid:107)2 = = 2W T (W h − y) = 2(cid:0)W T W h − W T y(cid:1)  = 2( Qh  )  − W T y (cid:124)(cid:123)(cid:122)(cid:125)  O(Kd)  (cid:124)(cid:123)(cid:122)(cid:125)O(d2)  (2)  The terms in O(Kd) and O(K) are due to leveraging the K-sparse representation of target vector y. With K (cid:28) D and d (cid:28) D, we get altogether a computational cost of O(d2) which can be several orders of magnitude cheaper than the prohibitive O(Dd) of the direct approach.  3.2 Efﬁcient gradient update of W  ∂W  The gradient of the squared error loss with respect to output layer weight matrix W is ∂L ∂W = ∂(cid:107)W h−y(cid:107)2 = 2(W h − y)hT . And the corresponding gradient descent update to W would be Wnew ← W − 2η(W h− y)hT , where η is a positive learning rate. Again, computed in this manner, this induces a prohibitive O(Dd) computational complexity, both to compute output and residual W h − y, and then to update all the Dd elements of W (since generally neither W h − y nor h will be sparse). All D× d elements of W must be accessed during this update. On the surface this seems hopeless. But we will now see how we can achieve the exact same update on W in O(d2). The trick  and update U and V instead:  is to represent W implicitly as the factorization W(cid:124)(cid:123)(cid:122)(cid:125)D×d  = V(cid:124)(cid:123)(cid:122)(cid:125)D×d  U(cid:124)(cid:123)(cid:122)(cid:125)d×d  a) Unew = U − 2η(U h)hT b) Vnew = V + 2ηy(U−T  newh)T `u  (3) (4)  This results in implicitly updating W as we did explicitly in the naive approach as we now prove:  VnewUnew = (V + 2ηy(U−T  newh)T ) Unew  newUnew  newh)T Unew  = V Unew + 2ηy(U−T = V Unew + 2ηyhT U−1 = V (U − 2η(U h)hT ) + 2ηyhT (U−1 = V U − 2ηV U hhT + 2ηyhT = V U − 2η(V U h − y)hT = W − 2η(W h − y)T hT = Wnew  newUnew)  We see that the update of U in Eq. 3 is a simple O(d2) operation. Following this simple rank-one update to U, we can use the Sherman-Morrison formula to derive the corresponding rank-one update to U−T which will also be O(d2):  5  U−T new = U−T +  2η  1 − 2η (cid:107)h(cid:107)2 (U−T h)hT  newh, an O(d2) operation needed in Eq. 4. The ensuing rank-one It is then easy to compute the U−T update of V in Eq 4, thanks to the K-sparsity of y is only O(Kd): only theK rows V associated to non-zero elements in y are accessed and updated, sited of all D rows of W we had to modify in the naive update! Note that with the factored representation of W as V U, we only have W implicitly, so the W T y terms that entered in the computation of L and ∇h in the previous paragraph need to be adapted slightly as ˆy = W T y = U T (V T y), which becomes O(d2 + Kd) rather than O(Kd) in computa- tional complexity. But this doesn’t change the overall O(d2) complexity of these computations.  3.3 Bookkeeping: keeping an up-to-date Q and U−T  We have already seen, in Eq. 5, how we can cheaply maintain an up-to-date U−T following our update of U. Similarly, following our updates to U and V , we need to keep an up-to-date Q = W T W which is needed to efﬁciently compute the loss L (Eq. 1) and gradient ∇h (Eq. 2). We have shown that updates to U and V in equations 3 and 4 are equivalent to implicitly updating W as Wnew ← W − 2η(W h − y)hT , and this translates into the following update to Q = W T W :  ˆz = Qh − U T (V T y) Qnew = Q − 2η(cid:0)hˆzT + ˆzhT(cid:1) + (4η2L)hhT  The proof is straightforward but due to space constraints we put it in supplementary material. One can see that this last bookkeeping operation also has a O(d2) computational complexity.  (5)  (6)  3.4 Putting it all together: detailed algorithm and expected beneﬁts  We have seen that we can efﬁciently compute cost L, gradient with respect to h (to be later back- propagated further) as well as updating U and V and performing the bookkeeping for U−T and Q. Algorithm 1 describes the detailed algorithmic steps that we put together from the equations derived above. Having K (cid:28) d (cid:28) D we see that the proposed algorithm requires O(d2) operations, whereas the standard approach required O(Dd) operations. If we take K ≈ d , we may state more precisely that the proposed algorithm, for computing the loss and the gradient updates will require roughly 12d2 operations whereas the standard approach required roughly 3Dd operations. So over- all the proposed algorithm change corresponds to a computational speedup by a factor of D 4d. For D = 200 000 and d = 500 the expected speedup is thus 100. Note that the advantage is not only in computational complexity, but also in memory access. For each example, the standard approach needs to access and change all D × d elements of matrix W , whereas the proposed approach only accesses the much smaller number K × d elements of V as well as the three d× d matrices U, U−T , and Q. So overall we have a substantially faster algorithm, which, while doing so implicitly, will nevertheless perform the exact same gradient update as the standard approach. We want to empha- size here that our approach is completely different from simply chaining 2 linear layers U and V and performing ordinary gradient descent updates on these: this would result in the same prohibitive computational complexity as the standard approach, and such ordinary separate gradient updates to Uand V would not be equivalent to the ordinary gradient update to W = V U.  3.5 Controlling numerical stability and extension to the minibatch case  The update of U in Equation 3 may over time lead U to become ill-conditioned. To prevent this, we regularly (every 100 updates) monitor its conditioning number2. If either the smallest or largest singular value moves outside an acceptable range, we bring it back to 1 by doing an appropriate  2Largest and smallest singular value can be computed with an SVD or using the power iteration method.  6  Algorithm 1 Efﬁcient computation of cost L, gradient h, and update to parameters U and V  Step # 1: 2: 3: 4: 5: 6: 7:  8: 9:  Operation  ˆh = Qh ˆy = U T (V T y) ˆz = ˆh − ˆy ∇h = 2ˆz L = hT ˆh − 2hT ˆy + yT y Unew = U − 2η(U h)hT U−T new = U−T + Vnew = V + 2ηy(U−T Qnew =  1−2η(cid:107)h(cid:107)2 (U−T h)hT newh)T Q − 2η(cid:0)hˆzT + ˆzhT(cid:1) +  (4η2L)hhT Altogether:  2η  Computational complexity  Number of  multiply-adds  O(d2)  d2  O(Kd + d2)  Kd + d2  O(d) O(d)  O(2d + K)  O(d2) O(d2)  d d  2d + K + 1  2d2 + d  2d2 + 2d + 3  O(d2 + Kd)  O(d2)  d2 + K + Kd 4 + 2d + 3d2  O(d2) provided K < d (cid:28) D  ≈ 12d2 elementary operations  rank-1 update to V (which costs Dd operations, but is only done rarely). Our algorithm can also be straightforwardly extended to the minibatch case (the derivations are given in the supplemen- tary material section) and yields the same theoretical speedup factor with respect to the standard naive approach. But one needs to be careful in order to keep the computation of U−T h reasonably efﬁcient: depending on the size of the minibatch m, it may be more efﬁcient to solve the correspond- ing linear equation for each minibatch from scratch rather than updating U−T with the Woodbury equation (which generalizes the Sheman-Morrison formula for m > 1).  3.6 Generalization to a broader class of loss functions  associated to non-zero yc and (cid:107)o(cid:107)2 = (cid:80)j o2  The approach that we detailed for linear output and squared error can easily be extended to slightly more exotic loss functions: basically any loss function that can be expressed using only the oc j the squared norm of the whole output vector, which we can compute cheaply. This family of loss functions does not include the standard log of softmax, but includes the so-called spherical softmax: log (oc+(cid:15))2 remains to be seen in practice how this approach performs computationally, and whether we lose something due to using this more limited family of loss functions.  (cid:80)j (oj +(cid:15))2 (where c is the correct class label). It  4 Experimental validation  We implemented both a CPU version using blas and a parallel GPU (Cuda) version using cublas of the proposed algorithm3. We evaluated the GPU and CPU implementations by training word embeddings with simple neural language models, in which a probability map of the next word given its preceding n-gram is learned by a neural network. We used a Nvidia Titan Black GPU and a i7-4820K @ 3.70GHz CPU and ran experiments on the one billion word dataset[14], which is composed of 0.8 billions words belonging to a vocabulary of 0.8 millions words. We evaluated the resulting word embeddings with the recently introduced Simlex-999 score [15], which measures the similarity between words. We also compared our approach to unfactorised versions and to a two-layer hierarchical softmax. Figure 2 and 3 (left) illustrate the practical speedup of our approach for the output layer only. Figure 3(right) shows that the LST (Large Sparse Target) models are much faster to train than the softmax models and converge to only slightly lower Simlex-999 scores. Table 1 summarizes the speedups for the different output layers we tried, both on CPU and GPU. We also emprically veriﬁed that our proposed factored algorithm learns the exact same model weights (V U )  3Open source code will be released upon ofﬁcial publication of this research.  7  Table 1: Speedups with respect to the baseline naive model on CPU, for a minibatch of 128 and the whole vocabulary of D = 793471 words. This is a two hidden layer model with 300 neurons on all its layers (so d = 300).  Model  output layer only speedup whole model speedup  cpu unfactorised (naive) gpu unfactorised (naive) gpu hierarchical softmax  cpu factorised gpu factorised  1 6.8 125.2 763.3 3257.3  1 4.7 178.1 501  1852.3  Figure 2: Timing of different algorithms. Time taken by forward and backward propagations in the output layer, including weight update, on a minibatch of size 128 for different sizes of vocabulary D on both CPU and GPU. The input size d is ﬁxed to 300. The Timing of a 2 layer hierarchical softmax efﬁcient GPU implementation (h_softmax) is also provided for comparison. Right plot is in log-log scale. As expected, the timings of factorized versions are independent of the size of the vocabulary.  as the corresponding naive unfactored algorithm’s W , as it theoretically should (up to negligible numerical precision differences), and followed the exact same learning curves (as a function of number of iterations, not time!).  5 Conclusion and future work  We introduced a new algorithmic approach to efﬁciently compute the exact gradient updates for training deep networks with very large sparse targets. Remarkably the complexity of the algorithm is independent of the target size, which allows tackling very large problems. Our CPU and GPU implementation yield similar speedups to the theoretical one and can thus be used in practical ap- plications, which could be explored in further work. In particular, neural language models seem good candidates. But it remains unclear how using a loss function other than log-softmax may affect the quality of the resulting word embeddingsm and further research should be carried out in this direction. Extensions of the approach to other possible losses than the simple squared error should also be empirically investigated in this light, in particular log-spherical-softmax.  Acknowledgements  We would like to thank the developers of Theano [16, 17] and Blocks [18]. This research is supported by NSERC and Ubisoft.  8  0200040006000800010000Size of the vocabulary D0.0000.0020.0040.0060.0080.010Timing (sec) of a minibatch of size 128un-factorised CPUun-factorised GPUfactorised GPUfactorised CPUh_softmax GPU101102103104105106Size of the vocabulary D10-310-210-1100101Timing (sec) of a minibatch of size 128un-factorised CPUun-factorised GPUfactorised GPUfactorised CPUh_softmax GPUFigure 3: Left: Practical and theoretical speedups for different sizes of vocabulary D and ﬁxed input size d=300. The practical unfact / fact speedup is similar to the theoretical one. Right: Evolution of the Simlex-999 score obtained with different models as a function of training time (CPU softmax times were extrapolated from fewer iterations). Softmax models are zero hidden-layer models, while our large sparse target (LST) models have two hidden layers. These were the best architectures retained in both cases (surprisingly the softmax models with hidden layers performed no better on this task). The extra non-linear layers in LST may help compensate for the lack of a softmax. LST models converge to slightly lower scores at similar speed as the hierarchical softmax model but signiﬁcantly faster than softmax models.  References [1] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model.  NIPS’00, pages 932–938. MIT Press, 2001.  In  [2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language process-  ing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.  [3] Y. Dauphin, X. Glorot, and Y. Bengio. Large-scale learning of embeddings with reconstruction sampling.  In Proceedings of the 28th International Conference on Machine learning, ICML ’11, 2011.  [4] Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target  vocabulary for neural machine translation. In ACL-IJCNLP’2015, 2015. arXiv:1412.2007.  [5] M. Gutmann and A. Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormal- ized statistical models. In Proceedings of The Thirteenth International Conference on Artiﬁcial Intelli- gence and Statistics (AISTATS’10), 2010.  [6] Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efﬁciently with noise-contrastive estimation. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2265–2273. Curran Associates, Inc., 2013.  [7] T. Mikolov, I. Sutskever, K. Chen, G.S. Corrado, and J. Dean. Distributed representations of words and  phrases and their compositionality. In NIPS’2013, pages 3111–3119. 2013.  [8] Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2321–2329. Curran Associates, Inc., 2014.  [9] Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, and Jay Yagnik. Deep networks with large  output spaces. arxiv:1412.7479, 2014.  [10] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model.  In Robert G. Cowell and Zoubin Ghahramani, editors, Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics, pages 246–252. Society for Artiﬁcial Intelligence and Statistics, 2005.  9  0100200300400500600700800Size of the vocabulary D (in thousands)0200400600800100012001400Speedupcpu_unfact / cpu_fact, experimentalgpu_unfact / gpu_fact, experimentalunfact / fact, theoreticalcpu_unfact / gpu_fact, experimentalcpu_unfact / gpu_unfact, experimental10-1100101102103Training time (hours)0.100.050.000.050.100.150.200.25SimLex-999LST CPULST GPUSoftmax CPUSoftmax GPUH-Softmax GPU[11] D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning representations by back-propagating errors.  Nature, 323:533–536, 1986.  [12] Yann LeCun. Une procédure d’apprentissage pour Réseau à seuil assymétrique.  In Cognitiva 85: A la Frontière de l’Intelligence Artiﬁcielle, des Sciences de la Connaissance et des Neurosciences, pages 599–604, Paris 1985, 1985. CESTA, Paris.  [13] Yann LeCun. Learning processes in an asymmetric threshold network. In E. Bienenstock, F. Fogelman- Soulié, and G. Weisbuch, editors, Disordered Systems and Biological Organization, pages 233–240. Springer-Verlag, Berlin, Les Houches 1985, 1986.  [14] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH 2014, 15th Annual Conference of the International Speech Communication Association, Singapore, September 14-18, 2014, pages 2635–2639, 2014.  [15] Felix Hill, Roi Reichart, and Anna Korhonen. Simlex-999: Evaluating semantic models with (genuine)  similarity estimation. CoRR, abs/1408.3456, 2014.  [16] James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Des- jardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expres- sion compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy), 2010. Oral Presentation.  [17] Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  [18] B. van Merriënboer, D. Bahdanau, V. Dumoulin, D. Serdyuk, D. Warde-Farley, J. Chorowski, and Y. Ben-  gio. Blocks and Fuel: Frameworks for deep learning. ArXiv e-prints, June 2015.  10  Appendix  A Minibatch version of the algorithm  The algorithm we derived for online gradient is relatively straightforward to extend to the case of minibatches containing m examples, and will still yield the same theoretical speedup factor with respect to the standard naive approach. One may want to be careful in order to keep the computation of U−T h (or ore precisely U−T H in the minibatch case) reasonably efﬁcient. In the minibatch version presented below, we update U−T based on the Woodbury equation (which generalizes the Sheman-Morrison formula for m > 1 and involves inverting an m × m matrix). But depending on the size of the minibatch m, it may become more efﬁcient to solve the corresponding linear equations for each minibatch from scratch every time, rather than inverting that m × m matrix. In which case we won’t need to maintain an U−T at all.  Algorithm 2 Minibatch version of the update algorithm Initialization  • we can initialize D × d matrix V randomly as we would have initialized W so that we initially have V = W . Alternatively we can initialize V to 0 (there won’t be symmetry breaking issues with having W initially be 0 provided the other layers are initialized randomly, since varying inputs and targets will naturally break symmetry for the output layer)  • initialize Q ← V T V (or more cheaply initialize Q ← 0 if we have initialized V to 0). • we initialize U to the identity: U ← Id so that, trivially, we initially have V U = W . • initialize U−T ← Id  Update We suppose we receive m target vectors in the m columns of sparse matrix Y , and corresponding m hidden representations in the m columns of matrix H.  Computation complexity  O(md2) O(mKd +  md2) O(md) O(md) O(m2d +  m2K)  Computational complexity with the multiplicative factor left in. O(md2) O(mKd + md2)  O(md) O(md) O(2m2d + m2K)  O(m) O(md2) O(m2d + m3 + md2)  O(md2 +  mKd)  O(md2 +  dm2)  O(m) O(2md2) O(2m2d + m3 + 2md2)  O(md2 + mKd)  O(3md2 + m2d)  Operation  Step #  1: 2:  3: 4: 5:  6: 7: 8:  9:  10:  ˆH = QH ˆY = U T (V T Y )  ˆZ = ˆH − ˆY ∇H = 2 ˆZ M = H T ˆZ − ˆY T H + Y T Y or alternatively M = H T ˆH − ( ˆY T H + H T ˆY ) + Y T Y L = Tr(M ) Unew = U − 2η(U H)H T new = U−T − U−T 2η Im)−1H T(cid:17) (U−T H)(cid:16)(H T H − 1 Qnew = Q − 2η(cid:16)H ˆZ T + ˆZH T(cid:17) +  Vnew = V + 2ηY (U−T  4η2(HM )H T  newH)T  11  B Detailed proof for computation of update of Q  Update to Q corresponds to Wnew ← W − 2η(W H − Y )H T We will use the following precomputed quantities: Q = W T W , ˆH = QH and ˆY = W T Y = U T (V T Y ) and ˆZ = ˆH − ˆY .  Qnew = W T  newWnew  = (cid:0)W − 2η(W H − Y )H T(cid:1)T(cid:0)W − 2η(W H − Y )H T(cid:1) = W T W − 2ηH(W H − Y )T W − 2ηW T (W H − Y )H T +4η2H(W H − Y )T (W H − Y )H T = Q − 2η(cid:0)HH T W T W − HY T W(cid:1) − 2η(cid:0)W T W HH T − W T Y H T(cid:1) +4η2H(H T W T W H − H T W T Y − Y T W H + Y T Y )H T = Q − 2η(cid:0)HH T Q − H(W T Y )T(cid:1) − 2η(cid:0)QHH T − (W T Y )H T(cid:1) +4η2H(H T QH − H T (W T Y ) − (W T Y )T H + Y T Y )H T = Q − 2η(cid:16)H ˆH T − H ˆY T + ˆHH T − ˆY H T(cid:17) +4η2H(H T ˆH − H T ˆY − ˆY T H + Y T Y )H T = Q − 2η(cid:16)H( ˆH − ˆY )T + ( ˆH − ˆY )H T(cid:17) + 4η2H(H T ( ˆH − ˆY ) − ˆY T H + Y T Y )H T = Q − 2η(cid:16)H ˆZ T + ˆZH T(cid:17) + 4η2H(cid:16)H T ˆZ − ˆY T H + Y T Y(cid:17) (cid:125)  (cid:123)(cid:122)  (cid:124)  H T  M  This is what is listed as step 10 of the above minibatch algorithm. In the online case, this becomes:  Qnew = Q − 2η(cid:0)hˆzT + ˆzhT(cid:1) + 4η2(cid:0)hT ˆz − ˆyT h + yT y(cid:1) hhT = Q − 2η(cid:0)hˆzT + ˆzhT(cid:1) + 4η2(cid:16)hT ˆh − hT ˆy − ˆyT h + yT y(cid:17) hhT = Q − 2η(cid:0)hˆzT + ˆzhT(cid:1) + 4η2(cid:16)hT ˆh − 2hT ˆy + yT y(cid:17) hhT = Q − 2η(cid:0)hˆzT + ˆzhT(cid:1) + (4η2L)hhT  which is the update listed as step 9 in the online algorithm (Algorithm 1 on page 7).  12  C Details regarding controlling numerical stability  The update of U (step 6 of the online algorithm, step 7 in the minibatch version) may over time lead to U becoming ill-conditioned. Simultaneously, as we update U and U−T (using Sherman-Morrison or Woodbury) our updated U−T may numerically start to diverge from the true U−T due to numerical precision. It is thus important to prevent both of these form happening, i.e. make sure U stays well conditioned, to ensure the numerical stability of the algorithm. We present here progressively reﬁned strategies for achieving this.  Restoring the system in a pristine stable state  One simple way to ensure numerical stability is to once in a while restore the system in its pristine state where V = W and U = Id = U−T . This is easily achieved as follows:  V ← V U U ← Id U−T ← Id.  This operation doesn’t affects the product V U, so the implicit matrix W remains unchanged, nor does it affect Q = W T W . And it does restore U to a perfectly well conditioned identity matrix. But computing V U is an extremely costly O(Dd2) operation, so if possible we want to avoid it (except maybe once at the very end of training, if we want to compute the actual W ). In the next paragraphs we develop a more efﬁcient strategy.  Stabilizing only problematic singular values  U becoming ill-conditioned is due to its singular values over time becoming too large and/or too small. Let use deﬁne σ1, . . . , σd as the singular values of U ordered in decreasing order. The conditioning number of U is deﬁned as σ1 and it can become overly large when σ1 becomes too σd large and/or when σd becomes too small. Restoring the system in its pristine state, as shown in the previous paragraph, in effect brings back all singular values of U back to 1 (since it brings back U to being the identity). It is instead possible, and computationally far less costly, to correct when needed only for the singular values of U that fall outside a safe range. Most often we will only need to occasionally correct for one singular value (usually the smallest, and only when it becomes too small). Once we have determined the offending singular value and its corresponding singular vectors, correcting for that singular value, i.e. effectively bringing it back to 1, will be a O(Dd) operation. The point is to apply corrective steps only on the problematic singular values and only when needed, rather than blindly, needlessly and inefﬁciently correcting for all of them through the basic O(Dd2) full restoration explained in the previous paragraph.  13  Here is the detailed algorithm that achieves this:  Algorithm 3 Numerical stabilization procedure for problematic singular values  • The chosen safe range for singular values is [σlow, σhigh] (ex: [0.001, 100] ) • The procedures given below act on output layer parameters U, U−T and V . • For concision, we do not enlist these parameters explicitly in their parameter list. • Procedure SINGULAR-STABILIZE gets called after every ncheck gradient updates (ex:  ncheck = 100).  procedure SINGULAR-STABILIZE( )  ¯U, σ, ¯V = SVD(U) (cid:46) Computes singular value decomposition of U as U = ¯U diag(σ) ¯VT for all k ∈ {1, . . . , d} do  if σk < σlow OR σk > σhigh then  FIX-SINGULAR-VALUE(σk, ¯Uk, 1)  end if  end for  end procedure  The following procedure will change singular value σ of U associated to singular vector u to become target singular value σ∗ (typically 1). It doesn’t change U’s singular vectors, only that one singular value. It also changes V symetrically (with a rank-one update) in such a way that W = V U remains unchanged.  procedure FIX-SINGULAR-VALUE(σ, u, σ∗)  1+α  ∗ −σ α = σ σ β = − α U ← U + αu(U T u)T V ← V + β(V u)uT U−T ← U−T + βu(U−1u)T  (cid:46) Where U−1 is obtained as the transpose of U−T . But we may instead of this prefer to recompute U−T from scratch by inverting U to ensure it doesn’t stray too much due to numerical imprecisions. end procedure  The proof that the FIX-SINGULAR-VALUE procedure achieves what it is supposed to is relatively straightforward, and left to the reader.  Avoiding the cost of a full singular-value decomposition Computing the SVD of d× d matrix U as required above, costs roughly 25d3 elementary operations (use the so-called R-SVD algorithm). But since the offending singular values will typically be only the smallest or the largest, it is wasteful to compute all d singular values every time. A possibly cheaper alternative is to use the power iteration method with U to ﬁnd its largest singular value and associated singular vector, and similarly with U−1to obtain the smallest singular value of U (which corresponds to the inverse of the largest singular value of U−1). Each iteration of the power iteration method requires only O(d2) operations, and a few iterations may sufﬁce. In our experiments we ﬁxed it to 100 power iterations. Also it is probably not critical if the power iteration method is not run fully to convergence, as correcting along an approximate offending singular vector direction can be sufﬁcient for the purpose of ensuring numerical stability. With this reﬁnement, we loop over ﬁnding the smallest singular value with the power iteration method, correcting for it to be 1 by calling FIX-SINGULAR-VALUE if it is too small, and we repeat this until we ﬁnd the now smallest singular value to be inside the acceptable range. Similarly for the largest singular values. Note that while in principle we may not need to ever invert U from scratch (as we provided update formulas of U−T with every change we make to U), it nevertheless proved to be necessary to do so regularly to ensure U−T doesn’t stray too much from the correct value due to numerical impreci- sions. Inverting U using Gaussian-elimination costs roughly d3 operations, so it is very reasonable  14  and won’t affect the computational complexity if we do it no more often than every d training ex- amples (which will typically correspond to less than 10 minibatches of size 128). In practice, we recompute U−T from scratch every time before we run this check for singular value stabilization.  15  ",
1412.6563,2015, Self-informed neural network structure learning,"['Self-informed neural network structure learning', 'David Warde-Farley', 'Andrew Rabinovich', 'and Dragomir Anguelov']",https://arxiv.org/pdf/1412.6563,"5 1 0 2    r p A 3 1         ] L M  . t a t s [      2 v 3 6 5 6  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  SELF-INFORMED NEURAL NETWORK STRUCTURE LEARNING  David Warde-Farley ∗ D´epartement d’Informatique et de Recherche Op´erationelle Universit´e de Montr´eal Montreal, Quebec, Canada wardefar@iro.umontreal.ca  Andrew Rabinovich & Dragomir Anguelov Google, Inc. Mountain View, CA 94043, USA {amrabino,dragomir}@google.com  ABSTRACT  We study the problem of large scale, multi-label visual recognition with a large number of possible classes. We propose a method for augmenting a trained neural network classiﬁer with auxiliary capacity in a manner designed to signiﬁcantly improve upon an already well-performing model, while minimally impacting its computational footprint. Using the predictions of the network itself as a descrip- tor for assessing visual similarity, we deﬁne a partitioning of the label space into groups of visually similar entities. We then augment the network with auxilliary hidden layer pathways with connectivity only to these groups of label units. We report a signiﬁcant improvement in mean average precision on a large-scale ob- ject recognition task with the augmented model, while increasing the number of multiply-adds by less than 3%.  1  INTRODUCTION  In the context of large scale visual recognition, it is not uncommon for state-of-the-art convolutional networks to be trained for days or weeks before convergence (Krizhevsky et al., 2012; Sermanet et al., 2014; Szegedy et al., 2014). Performing exhaustive architecture search is quite challenging and computationally expensive. Furthermore, once a satisfactory architecture has been discovered, it can be extremely difﬁcult to improve upon; small changes to the architecture more often decrease performance than improve it. In architectures containing fully-connected layers, naively increasing the dimensionality of such layers increases the number of parameters between them quadratically, increasing both the computational workload and the tendency towards overﬁtting. In settings where the domain of interest comprises thousands of classes, improving performance on speciﬁc subdomains can prove challenging, as the jointly learned features that succeed on the overall task on average may not be sufﬁcient for correctly identifying the “long tail” of classes, or for making ﬁne-grained distinctions between very similar entities. Side information in the form of metadata – for example, from Freebase (Bollacker et al., 2008) – often only roughly corresponds to the kind of similarity that would make correct classiﬁcation challenging. In the context of object classiﬁcation, visually similar entities may belong to vastly different high-level categories (e.g. a sporting activity and the equipment used to perform it), whereas two entities in the same high-level semantic category may bear little resemblance to one another visually. A traditional approach to building increasingly accurate classiﬁers is to average the predictions of a large ensemble. In the case of neural networks, a common approach is to add more layers or making existing layers signiﬁcantly larger, possibly with additional regularization. These strategies present a signiﬁcant problem in runtime-sensitive production environments, where a classiﬁer must be rapidly  ∗Work done while DWF was an intern at Google.  1  Accepted as a workshop contribution at ICLR 2015  Figure 1: A schematic of the augmentation process. Left: the original network. Right: the network after augmentation.  evaluated in a matter of milliseconds to comply with service-level agreements. It is therefore often desirable to increase a classiﬁer’s capacity in a way that signiﬁcantly improves performance while minimally impacting the computational resources required to evaluate the classiﬁer; however, it is not immediately obvious how to satisfy these two competing objectives. We present a method for judiciously adding capacity to a trained neural network using the network’s own predictions on held-out data to inform the augmentation of the network’s structure. We demon- strate the efﬁcacy of this method by using it to signiﬁcantly improve upon the performance of a state-of-the-art industrial object recognition pipeline based on Szegedy et al. (2014) with less than 3% extra computational overhead.  2 METHODS  Given a trained network, we evaluate the network on a held out dataset in order to compute a con- fusion matrix. We then apply spectral clustering (Chung, 1997) to generate a partitioning of the possible labels. We augment the trained network’s structure by adding additional stacks of fully connected layers, connected in parallel with the pre-existing stack of fully-connected layers. The output of each “aux- iliary head” is connected by a weight matrix only to a subset of the output units, corresponding to the label clusters discovered by spectral clustering. We train the augmented network by initializing the pre-existing portions of the network (minus the classiﬁer layer’s weights and biases) to the parameters of the original network, and by randomly ini- tializing the remaining portions. We train holding the pre-existing weights and biases ﬁxed, learning only the hidden layer weights for the new portions and retraining the classiﬁer layer’s weights. This allows for training to focus on making good use of the auxiliary capacity rather than adapting the pre-initialized weights to compensate for the presence of the new hidden units. Note that it is also possible to ﬁne-tune the whole network after training the augmented section, though we did not perform such ﬁne-tuning in the experiments described below.  3 RELATED WORK  Our method can be seen as similar in spirit to the mixture of experts approach of Jacobs et al. (1991). Rather than jointly learning a gating function as well as experts to be gated, we employ as a starting point a strong generalist network, whose outputs then inform decisions about which specialist networks to deploy for different subsets of classes. Our specialists also do not train with the original data as input but rather a higher-level feature representation output by the original network’s convolutional layers.  2  Low-level featuresGeneralistLow-level featuresGeneralistSpecialistSpecialistSpecialistSpecialistAccepted as a workshop contribution at ICLR 2015  Recent work on distillation (Hinton et al., 2014), building on earlier work termed model compres- sion (Bucilu et al., 2006), emphasizes the idea that a great deal of valuable information can be gleaned from the non-maximal predictions of neural network classiﬁers. Distillation makes use of the averaged overall predictions of several expensive-to-evaluate neural networks as “soft targets” in order to train a single network to both predict the correct label and mimic the overall predictions of the ensemble as closely as possible. As in Hinton et al. (2014), we use the predictions of the model itself, however we use this knowledge in the pursuit of carefully adding capacity to a single, already trained network, rather than mimicking the performance of many networks with one. Our approach is arguably complementary, and could conceivably be applied after distilling an ensemble into a single mimic network in order to further improve ﬁne-grained performance.  4 EXPERIMENTS  Our base model consists of Inception architecture employed in GoogLeNet (Szegedy et al., 2014), plus two fully connected hidden layers of 4,096 rectiﬁed lin- ear (ReLU) units each. Our output layer consists of logistic units, one per class. We evaluated the trained network on 9 million images not used during training. Let  the same convolutional  if example x has ground truth annotation for class j  (cid:26) 1, (cid:26) 1,  gj(x) =  Mi,K(x) =  0, otherwise  0, otherwise  if model M’s top K predicted labels on example x includes class i  (1)  (2)  We compute the following matrix on the hold-out set S:  A = [aij]; aij = Ex∈S [Mi,K(x) · gj(x)]  (3) using K = 100. We use the seemingly large value of K = 100 in order to recover annotations for a large fraction of possible classes on at least one example in the hold-out set. We term the detection of class i in the context of ground truth class j a confusion of i with j; the (i, j)th entry of this matrix thus encodes the fraction of the time class i is “confused” with class j on the hold-out set. We also experimented with the matrix  A = [aij]; aij = Ex∈S [Mi,K(x) · Mj,K(x)]  (4)  wherein we eschew the use of ground truth and only look at co-detections, again with K = 100. We symmetrize either matrix as B = ATA, and apply spectral clustering using B as our similarity matrix, following the formulation of Ng et al. (2002). In all of our experiments, our specialist sub- networks consisted of two layers of 512 ReLUs each. We evaluate our method on an expanded version of the JFT dataset described in Hinton et al. (2014), an internal Google dataset with a training set of approximately 100 million images spanning 17,000 classes.  5 RESULTS  5.1 LABEL CLUSTERS RECOVERED  In Table 1, we observe that spectral clustering on the matrix B was able to successfully recover clusters consisting of visually similar entities.  5.2 TEST SET PERFORMANCE IMPROVEMENTS  We evaluate on a balanced test set with the same number of classes per image. For each of the confusion and co-detection cases, we compare against a network with identical capacity and topol- ogy (i.e. same number of labels per cluster) with labels randomly permuted, in order to assess the importance of the particular partitioning discovered while carefully controlling for the number of parameters being learned.  3  Accepted as a workshop contribution at ICLR 2015  Runway, Handshake, Douglas dc-3, Tarmac, Boeing, Air show, Interceptor, Hospital ship, Coast guard, Republic p-47 thunderbolt,  Sikorsky sh-3 sea king, Boeing 737, Mcdonnell douglas dc-10, Air force, Boeing 757, Boeing 717, Hovercraft, Lockheed ac-130, McDonnell Douglas, Travel,  Aircraft engine, Flight, Yawl, Lockheed c-5 galaxy, Cockpit, Bomber, Lockheed p-3 orion, Avro lancaster, Jet aircraft. . .  Pickled food, Grilled food, North african cuisine, Vinegret, Woku, Lasagne, Lard, Meringue, Peanut butter and jelly sandwich, Sparkling wine, Salting,  Raclette, Mussel, Galliformes, Chemical compound, Succotash, Cucurbita, Alcoholic beverage, Bento, Osechi, Okonomiyaki, Nabemono, Miso soup, Dango,  Onigiri, Tempura, Mochi, Soba, Shiitake, Indian cuisine, Andhra food, Foie gras, Krill, Sour cream, Saumagen, Compote. . .  Lingonberry, Rooibos, Persimmon, Rutabaga, Banana family, Ensete, Apple, Viola, Shamrock, Walnut, Beech, Poppy, Kimjongilia, Chicory, Bay leaf, Melon, Grain, Juniper, Spruce, Fir, Birch family, Hawthorn, Guava, Gooseberry, Tick, Pouchong, Bonsai, Caraway, Fennel, Sea anemone, Maple sugar,  Boysenberry, Mustard and cabbage family, Pond, Moss, Daikon, Wild ginger, Groundcover, Holly, Viburnum lentago, Ivy family, Mustard seed. . .  Table 1: Examples of partial sets of labels grouped together by performing spectral clustering on the base network’s confusions, based on the 100 top scoring predictions. The ﬁrst row appears aviation-related, the second focusing on mainly food, and the third broadly concerned with plant-related entities.  Description Base network Base + 6 heads, confusions Base + 6 heads, randomized Base + 13 heads, co-detections Base + 13 heads, randomized  mAP @ top 50  36.80% 39.41% 32.97% 38.07% 32.13%  1.52B 1.56B  1.60B  ”  ”  1.000× 1.026× 1.053×  ”  ”  # Multiply-Adds Extra Computation  While both methods improve upon the base network, the use of ground truth appears to provide a signiﬁcant edge. Our best performing network, with 6 specialist heads, increases the number of multiply-adds required for evaluation from 1.52 billion to 1.56 billion, a modest increase of 2.6%. We also provide, in Figure 2, an evaluation of our best performing JFT network against the ImageNet 1,000-class test set, on the subset of JFT classes that can be mapped to classes from the ImageNet task (approximately 660 classes). These results are thus not directly comparable to results obtained on the ImageNet training set; a more direct comparison is left to follow-up work.  Figure 2: A preliminary evaluation of our trained network on the subset of classes in JFT that are mappable to the 1,000-class ImageNet classiﬁcation task.  6 CONCLUSIONS & FUTURE WORK  We have presented a simple and general method for improving upon trained neural network classi- ﬁers by carefully adding capacity to groups of output classes that the trained model itself considers similar. While we demonstrate results on a computer vision task, this is not an assumption underly- ing the approach, and we plan to extend it to other domains in follow-up work. In these experiments we have allocated a ﬁxed extra capacity to each label group, regardless of the number of labels in that group. Further investigation is needed into strategies for the allocation of  4  Accepted as a workshop contribution at ICLR 2015  capacity to each label group. Seemingly relevant factors include both the cardinality of each group and the amount of training data available for the labels contained therein; however, the difﬁculty of the discrimination task does not necessarily scale with either of these. In the case of the particular convolutional network we have described, it is not obvious that the best place to connect these auxiliary stacks of hidden layers is following the last convolutional layer. Most of the capacity, and therefore arguably most of the discriminative knowledge in the network, is contained in the fully connected layers, and appealing to this part of the network for augmentation purposes seems natural. Nonetheless, it is possible that one or more layers of group-speciﬁc convo- lutional feature maps could be beneﬁcial as well. Note that the augmentation procedure could also theoretically be applied more than once, and not necessarily in the same location. Each subsequent clustering and retraining step could potentially identify a complementary division of the label space, capturing new information. Finally, this can be seen as a small step towards the “conditional computation” envisioned by Ben- gio (2013), wherein relevant pathways of a large network are conditionally activated based on task relevance. Here we have focused on the relatively large gains to be had with computationally inex- pensive, targeted augmentations. Similar strategies could pave the way towards networks with much higher capacity specialists that are only evaluated when necessary.  REFERENCES Bengio, Yoshua. Deep learning of representations: Looking forward. In Statistical Language and  Speech Processing, pp. 1–37. Springer, 2013.  Bollacker, Kurt, Evans, Colin, Paritosh, Praveen, Sturge, Tim, and Taylor, Jamie. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pp. 1247–1250. ACM, 2008.  Bucilu, Cristian, Caruana, Rich, and Niculescu-Mizil, Alexandru. Model compression.  In Pro- ceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535–541. ACM, 2006.  Chung, Fan RK. Spectral graph theory, volume 92. American Mathematical Soc., 1997.  Hinton, Geoffrey E., Vinyals, Oriol, and Dean, Jeff. Distilling the knowledge in a neural network.  In NIPS 2014 Deep Learning Workshop, 2014.  Jacobs, Robert A, Jordan, Michael I, Nowlan, Steven J, and Hinton, Geoffrey E. Adaptive mixtures  of local experts. Neural computation, 3(1):79–87, 1991.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.  Ng, Andrew Y, Jordan, Michael I, Weiss, Yair, et al. On spectral clustering: Analysis and an algo-  rithm. Advances in neural information processing systems, 2:849–856, 2002.  Sermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Michael, Fergus, Rob, and LeCun, Yann. In  Overfeat: Integrated recognition, localization and detection using convolutional networks. International Conference on Learning Representations (ICLR2014). CBLS, April 2014.  Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and  Rabinovich, A. Going Deeper with Convolutions. ArXiv e-prints, September 2014.  5  ",
