unique_id,year,title,authors,pdf_url,paper_text,abstract
1301.4168,2013,Herded Gibbs Sampling  ,"['Luke Bornn', 'Yutian Chen', 'Nando de Freitas', 'Maya Baya', 'Jing Fang', 'Max Welling']",https://arxiv.org/pdf/1301.4168.pdf,"3 1 0 2    r a     M 6 1      ]  G L . s c [      2 v 8 6 1 4  .  1 0 3 1 : v i X r a  Herded Gibbs Sampling  Luke Bornn  Harvard University  Yutian Chen  UC Irvine  Nando de Freitas  UBC  bornn@stat.harvard.edu  yutian.chen@uci.edu  nando@cs.ubc.ca  Mareija Eskelin  UBC  Jing Fang Facebook  mareija@cs.ubc.ca  jingf@cs.ubc.ca  Max Welling  University of Amsterdam welling@ics.uci.edu  Abstract  The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an O(1/T ) convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.  1  Introduction  Over the last 60 years, we have witnessed great progress in the design of randomized sampling algorithms; see for example [16, 9, 3, 22] and the references therein. In contrast, the design of deter- ministic algorithms for “sampling” from distributions is still in its inception [8, 13, 7, 20]. There are, however, many important reasons for pursuing this line of attack on the problem. From a theoreti- cal perspective, this is a well deﬁned mathematical challenge whose solution might have important consequences. It also brings us closer to reconciling the fact that we typically use pseudo-random number generators to run Monte Carlo algorithms on classical, Von Neumann architecture, comput- ers. Moreover, the theory for some of the recently proposed deterministic sampling algorithms has √ taught us that they can achieve O(1/T ) convergence rates [8, 13], which are much faster than the standard Monte Carlo rates of O(1/ T ) for computing ergodic averages. From a practical perspec- tive, the design of deterministic sampling algorithms creates an opportunity for researchers to apply a great body of knowledge on optimization to the problem of sampling; see for example [4] for an early example of this. The domain of application of currently existing deterministic sampling algorithms is still very nar- row. Importantly, there do not exist deterministic tools for sampling from unnormalized multivariate probability distributions. This is very limiting because the problem of sampling from unnormalized distributions is at the heart of the ﬁeld of Bayesian inference and the probabilistic programming approach to artiﬁcial intelligence [17, 6, 18, 11]. At the same time, despite great progress in Monte Carlo simulation, the celebrated Gibbs sampler continues to be one of the most widely-used algo- rithms. For, example it is the inference engine behind popular statistics packages [17], several tools for text analysis [21], and Boltzmann machines [2, 12]. The popularity of Gibbs stems from its simplicity of implementation and the fact that it is a very generic algorithm. Without any doubt, it would be remarkable if we could design generic deterministic Gibbs sam- plers with fast (theoretical and empirical) rates of convergence. In this paper, we take steps toward  Authors are listed in alphabetical order.  1  achieving this goal by capitalizing on a recent idea for deterministic simulation known as herding. Herding [24, 23, 10] is a deterministic procedure for generating samples x ∈ X ⊆ Rn, such that the empirical moments µ of the data are matched. The herding procedure, at iteration t, is as follows:  (cid:104)w(t−1), φ(x)(cid:105) x(t) = arg max w(t) = w(t−1) + µ − φ(x(t)),  x∈X  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)µ − 1  T  T(cid:88)  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2  (1) where φ : X → H is a feature map (statistic) from X to a Hilbert space H with inner product (cid:104)·,·(cid:105), w ∈ H is the vector of parameters, and µ ∈ H is the moment vector (expected value of φ over the data) that we want to match. If we choose normalized features by making (cid:107)φ(x)(cid:107) constant for all x, then the update to generate samples x(t) for t = 1, 2, . . . , T in Equation 1 is equivalent to minimizing the objective  i  ,  t=1  (2)  φ(x(t))  J(x1, . . . , xT ) =  where T may have no prior known value and (cid:107) · (cid:107) = (cid:112)(cid:104)·,·(cid:105) is the naturally deﬁned norm based upon the inner product of the space H [8, 4]. (cid:80)n Herding can be used to produce samples from normalized probability distributions. This is done as follows. Let µ denote a discrete, normalized probability distribution, with µi ∈ [0, 1] and φ(x) = (0, 1, 0, 0, 0)T . Hence,(cid:98)µ = T −1(cid:80)T i=1 µi = 1. A natural feature in this case is the vector φ(x) that has all entries equal to zero, except for the entry at the position indicated by x. For instance, if x = 2 and n = 5, we have t=1 φ(x(t)) is an empirical estimate of the distribution. In this case, one step of the herding algorithm involves ﬁnding the largest component of the weight vector (i(cid:63) = arg maxi∈{1,2,...,n} w(t−1) ), setting x(t) = i(cid:63), ﬁxing the i(cid:63)-entry of φ(x(t)) to one output is a set of samples {x(1), . . . , x(T )} for which the empirical estimate (cid:98)µ converges on the and all other entries to zero, and updating the weight vector: w(t) = w(t−1) + (µ − φ(x(t))). The  target distribution µ as O(1/T ). The herding method, as described thus far, only applies to normalized distributions or to problems where the objective is not to guarantee that the samples come from the right target, but to ensure that some moments are matched. An interpretation of herding in terms of Bayesian quadrature has been put forward recently by [14]. In this paper, we will show that it is possible to use herding to generate samples from more complex unnormalized probability distributions. In particular, we introduce a deterministic variant of the popular Gibbs sampling algorithm, which we refer to as herded Gibbs. While Gibbs relies on draw- ing samples from the full-conditionals at random, herded Gibbs generates the samples by matching the full-conditionals. That is, one simply applies herding to all the full-conditional distributions. The experiments will demonstrate that the new algorithm outperforms Gibbs sampling and mean ﬁeld methods in the domain of sampling from sparsely connected probabilistic graphical models, such as grid-lattice Markov random ﬁelds (MRFs) for image denoising and conditional random ﬁelds (CRFs) for natural language processing. We advance the theory by proving that the deterministic Gibbs algorithm converges for distributions of independent variables and fully-connected probabilistic graphical models. However, a proof es- tablishing suitable conditions that ensure convergence of herded Gibbs sampling for sparsely con- nected probabilistic graphical models is still unavailable.  i=1, Xi ∈ X , let π denote the target distribution deﬁned on G.  2 Herded Gibbs Sampling For a graph of discrete nodes G = (V, E), where the set of nodes are the random variables V = {Xi}N Gibbs sampling is one of the most popular methods to draw samples from π. Gibbs alternates (either systematically or randomly) the sampling of each variable Xi given XN (i) = xN (i), where i is the index of the node, and N (i) denotes the neighbors of node i. That is, Gibbs generates each sample from its full-conditional distribution p(Xi|xN (i)).  2  Algorithm 1 Herded Gibbs Sampling.  Input: T . Step 1: Set t = 0. 1, π(Xi = 1|xN (i))). for t = 1 → T do  Initialize X(0) in the support of π and w(0)  i,xN (i)  in (π(Xi = 1|xN (i)) −  Step 2: Pick a node i according to some policy. Denote w = w(t−1) i,x(t−1) N (i) Step 3: If w > 0, set x(t) Step 4: Update weight w(t)  i = 0. + π(Xi = 1|x(t−1)N (i) ) − x(t)  i = 1, otherwise set x(t)  .  i  = w(t−1) i,x(t−1) N (i)  i,x(t)  N (i)  Step 5: Keep the values of all the other nodes x(t) w(t) end for Output: x(1), . . . , x(T )  ,∀j (cid:54)= i or xN (j) (cid:54)= x(t−1)N (i) .  = w(t−1)  j,xN (j)  j,xN (j)  j = x(t−1)  j  ,∀j (cid:54)= i and all the other weights  Herded Gibbs replaces the sampling from full-conditionals with herding at the level of the full- conditionals. That is, it alternates a process of matching the full-conditional distributions p(Xi = xi|XN (i)). To do this, herded Gibbs deﬁnes a set of auxiliary weights {wi,xN (i)} for any value of Xi = xi and XN (i) = xN (i). For ease of presentation, we assume the domain of Xi is binary, X = {0, 1}, and we use one weight for every i and assignment to the neighbors xN (i). Herded Gibbs can be trivially generalized to the multivariate setting by employing weight vectors in R|X| instead of scalars. If the binary variable Xi has four binary neighbors XN (i), we must maintain 24 = 16 weight vectors. Only the weight vector corresponding to the current instantiation of the neighbors is updated, as illustrated in Algorithm 1. The memory complexity of herded Gibbs is exponential in the maximum node degree. Note the algorithm is a deterministic Markov process with state (X, W).  The initialization in step 1 guarantees that X(t) always remains in the support of π. For a deter- ministic scan policy in step 2, we take the value of variables x(tN ), t ∈ N as a sample sequence. Throughout the paper all experiments employ a ﬁxed variable traversal for sample generation. We call one such traversal of the variables a sweep.  3 Analysis  As herded Gibbs sampling is a deterministic algorithm, there is no stationary probability distribution of states. Instead, we examine the average of the sample states over time and hypothesize that it converges to the joint distribution, our target distribution, π. To make the treatment precise, we need the following deﬁnition: Deﬁnition 1. For a graph of discrete nodes G = (V, E), where the set of nodes V = {Xi}N i=1, Xi ∈ X , P (τ ) is the empirical estimate of the joint distribution obtained by averaging over T samples acquired from G. P (τ ) is derived from T samples, collected at the end of every sweep over N variables, starting from the τ th sweep:  T  T  P (τ )  T (X = x) =  1 T  I(X(kN ) = x)  (3)  τ +T−1(cid:88)  k=τ  Our goal is to prove that the limiting average sample distribution over time converges to the target distribution π. Speciﬁcally, we want to show the following:  T→∞ P (τ ) lim  T  (x) = π(x),∀τ ≥ 0  (4)  If this holds, we also want to know what the convergence rate is.  3  We begin the theoretical analysis with a graph of one binary variable. For this graph, there is only one weight w. Denote π(X = 1) as π for notational simplicity. The sequence of X is determined by the dynamics of w (shown in Figure 1):  w(t) = w(t−1) + π − I(w(t−1) > 0), X (t) =  (5) Lemma 3 in the appendix shows that (π − 1, π] is the invariant interval of the dynamics, and the state X = 1 is visited at a frequency close to π with an error: |P (τ ) T (X = 1) − π| ≤ 1 T  (6)  0  if w(t−1) > 0 otherwise  (cid:26) 1  This is known as the fast moment matching property in [24, 23, 10]. We will show in the next two theorems that the fast moment matching property also holds for two special types of graphs, with proofs provided in the appendix.  Figure 1: Herding dynamics for a single variable.  In an empty graph, all the variables are independent of each other and herded Gibbs reduces to running N one-variable chains in parallel. Denote the marginal distribution πi := π(Xi = 1). Examples of failing convergence in the presence of rational ratios between the πis were observed in [4]. There the need for further theoretical research on this matter was pointed out. The following theorem provides formal conditions for convergence in the restricted domain of empty graphs. Theorem 1. For an empty graph, when herded Gibbs has a ﬁxed scanning order, and {1, π1, . . . , πN} are rationally independent, the empirical distribution P (τ ) converges to the tar- get distribution π as T → ∞ for any τ ≥ 0.  T  numbers, a1, a2, . . . , an, we have(cid:80)n  A set of n real numbers, x1, x2, . . . , xn, is said to be rationally independent if for any set of rational i=1 aixi = 0 ⇔ ai = 0,∀1 ≤ i ≤ n. The proof of Theorem 1 consists of ﬁrst formulating the dynamics of the weight vector as a constant translation mapping in a circular unit cube, and then proving that the weights are uniformly distributed by making use of Kronecker-Weyl’s theorem [25]. For fully-connected (complete) graphs, convergence is guaranteed even with rational ratios. In fact, herded Gibbs converges to the target joint distribution at a rate of O(1/T ) with a O(log(T )) burn-in period. This statement is formalized in Theorem 2. Theorem 2. For a fully-connected graph, when herded Gibbs has a ﬁxed scanning order and a Dobrushin coefﬁcient of the corresponding Gibbs sampler η < 1, there exist constants l > 0, and B > 0 such that  where λ = 2N (1+η)  l(1−η) , T ∗ = 2B  l  dv(P (τ )  T − π) ≤ λ T , τ∗(T ) = log 2  ,∀T ≥ T ∗, τ > τ∗(T )  (cid:16) (1−η)lT  (cid:17)  1+η  4N  , and dv(δπ) := 1  2||δπ||1.  (7)  The constants l and B are deﬁned in Equation 31 for Proposition 4 in the appendix. If we ignore the burn-in period and start collecting samples simply from the beginning, we achieve a convergence rate of O( log(T ) ) as stated in Corollary 10 in the appendix. The constant l in the convergence rate has an exponential term, with N in the exponent. An exponentially large constant seems to be unavoidable for any sampling algorithm when considering the convergence to a joint distribution with 2N states. As for the marginal distributions, it is obvious that the convergence rate of herded Gibbs is also  T  4  T . After one iteration, this discrepancy is bounded above by O(1/T ).  O(1/T ) because marginal probabilities are linear functions of the joint distribution. However, in practice, we observe very rapid convergence results for the marginals, so stronger theoretical results about the convergence of the marginal distributions seem plausible. The proof proceeds by ﬁrst bounding the discrepancy between the chain of empirical estimates of the joint obtained by averaging over T herded Gibbs samples, {P (s) T }, s ≥ τ, and a Gibbs chain initialized at P (τ ) The Gibbs chain has geometric convergence to π and the distance between the Gibbs and herded Gibbs chains is bounded by O(1/T ). The geometric convergence rate to π dominates the discrep- ancy of herded Gibbs and thus we infer that P (τ ) converges to π geometrically. To round-off the proof, we must ﬁnd a limiting value for τ. The proof concludes with an O(log(T )) burn-in for τ. However, for a generic graph we have no mathematical guarantees on the convergence rate of herded Gibbs. In fact, one can easily construct synthetic examples for which herded Gibbs does not seem to converge to the true marginals and joint distribution. For the examples covered by our theorems and for examples with real data, herded Gibbs demonstrates good behaviour. The exact conditions under which herded Gibbs converges for sparsely connected graphs are still unknown.  T  4 Experiments  4.1 Simple Complete Graph  We begin with an illustration of how herded Gibbs substantially outperforms Gibbs on a simple complete graph. In particular, we consider a fully-connected model of two variables, X1 and X2, as shown in Figure 2; the joint distribution of these variables is shown in Table 1. Figure 3 shows the marginal distribution P (X1 = 1) approximated by both Gibbs and herded Gibbs for different (cid:15). As (cid:15) decreases, both approaches require more iterations to converge, but herded Gibbs clearly outperforms Gibbs. The ﬁgure also shows that Herding does indeed exhibit a linear convergence rate.  X1  X2  X2 = 0 X2 = 1 P(X1)  X1 = 0 X1 = 1 P(X2) 1/4 − (cid:15)  (cid:15)  (cid:15)  1/4  3/4 − (cid:15) 3/4  1/4 3/4 1  Figure 2: Two-variable model.  Table 1: Joint distribution of the two-variable model.  4.2 MRF for Image Denoising  Next, we consider the standard setting of a grid-lattice MRF for image denoising. Let us assume that we have a binary image corrupted by noise, and that we want to infer the original clean image. Let Xi ∈ {−1, +1} denote the unknown true value of pixel i, and yi the observed, noise-corrupted value of this pixel. We take advantage of the fact that neighboring pixels are likely to have the same label by deﬁning an MRF with an Ising prior. That is, we specify a rectangular 2D lattice with the following pair-wise clique potentials:  (cid:18) eJij  e−Jij  e−Jij eJij  ψij(xi, xj) =  (cid:19)  1  2   ,  (cid:88)  i∼j  Jijxixj  (8)  (9)  and joint distribution:  p(x|J) =  1  Z(J)  (cid:89)  i∼j  ψij(xi, xj) =  1  Z(J)  exp  where i ∼ j is used to indicate that nodes i and j are connected. The known parameters Jij establish the coupling strength between nodes i and j. Note that the matrix J is symmetric. If all the Jij > 0, then neighboring pixels are likely to be in the same state.  5  (a) Approximate marginals obtained via Gibbs (blue) and herded Gibbs (red).  (b) Log-log plot of marginal approximation errors obtained via Gibbs (blue) and herded Gibbs (red).  (c) Inverse of marginal approximation errors obtained via Gibbs (blue) and herded Gibbs (red).  Figure 3: (a) Approximating a marginal distribution with Gibbs (blue) and herded Gibbs (red) for an MRF of two variables, constructed so as to make the move from state (0, 0) to (1, 1) progressively more difﬁcult as (cid:15) decreases. The four columns, from left to right, are for (cid:15) = 0.1, (cid:15) = 0.01, (cid:15) = 0.001 and (cid:15) = 0.0001. Table 1 provides the joint distribution for these variables. The error bars for Gibbs correspond to one standard deviation. Rows (b) and (c) illustrate that the empirical convergence rate of herded Gibbs matches the expected theoretical rate. In the plots of rows (b) and (c), the upper-bound in the error of herded Gibbs was used to remove the oscillations so as to illustrate the behaviour of the algorithm more clearly.  The MRF model combines the Ising prior with a likelihood model as follows:  p(x, y) = p(x)p(y|x) =  ψij(xi, xj)  p(yi|xi)  (10)  The potentials ψij encourage label smoothness. The likelihood terms p(yi|xi) are conditionally independent (e.g. Gaussians with known variance σ2 and mean µ centered at each value of xi, denoted µxi). In more precise terms,  p(x, y|J, µ, σ) =  1  Z(J, µ, σ)  exp  Jijxixj − 1 2σ2  (yi − µxi)2  (11)  (cid:34)(cid:89)   .  i  (cid:88)  i  (cid:35)   .   1  Z  (cid:89)  i∼j   1  2  (cid:88)  i∼j  6  Figure 4: Original image (left) and its corrupted version (right), with noise parameter σ = 4.  Figure 5: Reconstruction errors for the image denoising task. The results are averaged across 10 corrupted images with Gaussian noise N (0, 16). The error bars correspond to one standard deviation. Mean ﬁeld requires the speciﬁcation of the damping factor D.  Hence, different neighbor conﬁgurations result  When the coupling parameters Jij are identical, say Jij = J, we have (cid:80) J(cid:80) J(cid:80)  ij Jijf (xi, xj) = ij f (xi, xj). in the same value of ij f (xi, xj). If we store the conditionals for conﬁgurations with the same sum together, we only need to store as many conditionals as different possible values that the sum could take. This enables us to develop a shared version of herded Gibbs that is more memory efﬁcient where we only maintain and update weights for distinct states of the Markov blanket of each variable. In this exemplary image denoising experiment, noisy versions of the binary image, seen in Figure 4 (left), were created through the addition of Gaussian noise, with varying σ. Figure 4 (right) shows a corrupted image with σ = 4. The L2 reconstruction errors as a function of the number of iterations, for this example, are shown in Figure 5. The plot compares the herded Gibbs method against Gibbs and two versions of mean ﬁeld with different damping factors [19]. The results demonstrate that the herded Gibbs techiques are among the best methods for solving this task. A comparison for different values σ is presented in Table 2. As expected mean ﬁeld does well in the low-noise scenario, but the performance of the shared version of herded Gibbs as the noise increases is signiﬁcantly better.  7  Table 2: Errors of image denoising example after 30 iterations (all measurements have been scaled by ×10−3). We use an Ising prior with Jij = 1 and four Gaussian noise models with different σ’s. For each σ, we generated 10 corrupted images by adding Gaussian noise. The ﬁnal results shown here are averages and standard deviations (in parentheses) across the 10 corrupted images. D denotes the damping factor in mean ﬁeld.  (cid:80)(cid:80)(cid:80)(cid:80)(cid:80)(cid:80)  Method  σ  2  4  6  8  Herded Gibbs Herded Gibbs - shared Gibbs Mean ﬁeld (D=0.5) Mean ﬁeld (D=1)  21.58(0.26) 22.24(0.29) 21.63(0.28) 15.52(0.30) 17.67(0.40)  32.07(0.98) 31.40(0.59) 37.20(1.23) 41.76(0.71) 32.04(0.76)  47.52(1.64) 42.62(1.98) 63.78(2.41) 76.24(1.65) 51.19(1.44)  67.93(2.78) 58.49(2.86) 90.27(3.48) 104.08(1.93) 74.74(2.21)  Person  Other  Other  Other  Other  Other  Person  Other  Other  Other  Other  Zed  ’  s  dead  baby  .  Zed  ’  s  dead  .  Figure 6: Typical skip-chain CRF model for named entity recognition.  4.3 CRF for Named Entity Recognition  Named Entity Recognition (NER) involves the identiﬁcation of entities, such as people and loca- tions, within a text sample. A conditional random ﬁed (CRF) for NER models the relationship between entity labels and sentences with a conditional probability distribution: P (Y |X, θ), where X is a sentence, Y is a labeling, and θ is a vector of coupling parameters. The parameters, θ, are fea- ture weights and model relationships between variables Yi and Xj or Yi and Yj. A chain CRF only employs relationships between adjacent variables, whereas a skip-chain CRF can employ relation- ships between variables where subscripts i and j differ dramatically. Skip-chain CRFs are important in language tasks, such as NER and semantic role labeling, because they allow us to model long dependencies in a stream of words, see Figure 6. Once the parameters have been learned, the CRF can be used for inference; a labeling for some sentence X is found by maximizing the above probability. Inference for CRF models in the NER domain is typically carried out with the Viterbi algorithm. However, if we want to accommodate long term dependencies, thus resulting in the so called skip-chain CRFs, Viterbi becomes prohibitively expensive. To surmount this problem, the Stanford named entity recognizer [15] makes use of annealed Gibbs sampling. To demonstrate herded Gibbs on a practical application of great interest in text mining, we modify the standard inference procedure of the Stanford named entity recognizer by replacing the annealed Gibbs sampler with the herded Gibbs sampler. The herded Gibbs sampler in not annealed. To ﬁnd the maximum a posteriori sequence Y , we simply choose the sample with highest joint discrete probability. In order to be able to compare against Viterbi, we have purposely chosen to use single- chain CRFs. We remind the reader, however, that the herded Gibbs algorithm could be used in cases where Viterbi inference is not possible. We used the pre-trained 3-class CRF model in the Stanford NER package [15]. This model is a linear chain CRF with pre-deﬁned features and pre-trained feature weights, θ. For the test set, we used the corpus for the NIST 1999 IE-ER Evaluation. Performance is measured in per-entity F1 F1 = 2 · precision·recall . For all the methods, except Viterbi, we show F1 scores after 100, 400 and 800 iterations in Table 3. For Gibbs, the results shown are the averages and standard deviations over 5 random runs. We used a linear annealing schedule for Gibbs. As the results illustrate,  precision+recall  (cid:16)  (cid:17)  8  ”Pumpkin” (Tim Roth) and ”Honey Bunny” (Amanda Plummer) are having breakfast in a diner. They decide to rob it after realizing they could make money off the customers as well as the business, as they did during their previous heist. Moments after they initiate the hold-up, the scene breaks off and the title credits roll. As Jules Winnﬁeld (Samuel L. Jackson) drives, Vincent Vega (John Travolta) talks about his experiences in Europe, from where he has just returned: the hash bars in Amsterdam, the French McDonald’s and its ”Royale with Cheese”.  Figure 7: Results for the application of the NER CRF to a random Wikipedia sample [1]. Entities are automatically classiﬁed as Person, Location and Organization.  herded Gibbs attains the same accuracy as Viterbi and it is faster than annealed Gibbs. Unlike Viterbi, herded Gibbs can be easily applied to skip-chain CRFs. After only 400 iterations (90.5 seconds), herded Gibbs already achieves an F1 score of 84.75, while Gibbs, even after 800 iterations (115.9 seconds) only achieves an F1 score of 84.61. The experiment thus clearly demonstrates that (i) herded Gibbs does no worse than the optimal solution, Viterbi, and (ii) herded Gibbs yields more accurate results for the same amount of computation. Figure 7 provides a representative NER example of the performance of Gibbs, herded Gibbs and Viterbi (all methods produced the same annotation for this short example).  Table 3: Gibbs, herded Gibbs and Viterbi for the NER task. The average computational time each approach took to do inference for the entire test set is listed (in square brackets). After only 400 iterations (90.48 seconds), herded Gibbs already achieves an F1 score of 84.75, while Gibbs, even after 800 iterations (115.92 seconds) only achieves an F1 score of 84.61. For the same computation, herded Gibbs is more accurate than Gibbs.  (cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)  Iterations  Method  100  400  800  Annealed Gibbs Herded Gibbs Viterbi  84.36(0.16) [55.73s] 84.70 [59.08s]  84.51(0.10) [83.49s] 84.75 [90.48s]  84.61(0.05) [115.92s] 84.81 [132.00s] 84.81[46.74s]  5 Conclusions and Future Work  In this paper, we introduced herded Gibbs, a deterministic variant of the popular Gibbs sampling al- gorithm. While Gibbs relies on drawing samples from the full-conditionals at random, herded Gibbs generates the samples by matching the full-conditionals. Importantly, the herded Gibbs algorithm is very close to the Gibbs algorithm and hence retains its simplicity of implementation. The synthetic, denoising and named entity recognition experiments provided evidence that herded Gibbs outperforms Gibbs sampling. However, as discussed, herded Gibbs requires storage of the conditional distributions for all instantiations of the neighbors in the worst case. This storage re- quirement indicates that it is more suitable for sparse probabilistic graphical models, such as the CRFs used in information extraction. At the other extreme, the paper advanced the theory of de- terministic sampling by showing that herded Gibbs converges with rate O(1/T ) for models with independent variables and fully-connected models. Thus, there is gap between theory and practice that needs to be narrowed. We do not anticipate that this will be an easy task, but it is certainly a key direction for future work. We should mention that it is also possible to design parallel versions of herded Gibbs in a Jacobi fashion. We have indeed studied this and found that these are less efﬁcient than the Gauss-Seidel version of herded Gibbs discussed in this paper. However, if many cores are available, we strongly recommend the Jacobi (asynchronous) implementation as it will likely outperform the Gauss-Seidel (synchronous) implementation.  9  The design of efﬁcient herding algorithms for densely connected probabilistic graphical models remains an important area for future research. Such algorithms, in conjunction with Rao Black- wellization, would enable us to attack many statistical inference tasks, including Bayesian variable selection and Dirichlet processes. There are also interesting connections with other algorithms to explore. If, for a fully connected graphical model, we build a new graph where every state is a node and directed connections exist between nodes that can be reached with a single herded Gibbs update, then herded Gibbs becomes equivalent to the Rotor-Router model of Alex Holroyd and Jim Propp1 [13]. This deterministic ana- logue of a random walk has provably superior concentration rates for quantities such as normalized hitting frequencies, hitting times and occupation frequencies. In line with our own convergence √ results, it is shown that discrepancies in these quantities decrease as O(1/T ) instead of the usual T ). We expect that many of the results from this literature apply to herded Gibbs as well. O(1/ The connection with the work of Art Owen and colleagues, see for example [7], also needs to be explored further. Their work uses completely uniformly distributed (CUD) sequences to drive Markov chain Monte Carlo schemes. It is not clear, following discussions with Art Owen, that CUD sequences can be constructed in a greedy way as in herding.  References [1] Pulp ﬁction - wikipedia, the free encyclopedia @ONLINE, June 2012. [2] D. H. Ackley, G. Hinton, and T.. Sejnowski. A learning algorithm for Boltzmann machines.  Cognitive Science, 9:147–169, 1985.  [3] C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan. An Introduction to MCMC for Machine  Learning. Machine Learning, 50(1):5–43, 2003.  [4] F. Bach, S. Lacoste-Julien, and G. Obozinski. On the equivalence between herding and condi-  tional gradient algorithms. In International Conference on Machine Learning, 2012.  [5] P. Br´emaud. Markov chains: Gibbs ﬁelds, Monte Carlo simulation, and queues, volume 31.  Springer, 1999.  [6] P. Carbonetto, J. Kisynski, N. de Freitas, and D. Poole. Nonparametric Bayesian logic. In  Uncertainty in Artiﬁcial Intelligence, pages 85–93, 2005.  [7] S. Chen, J. Dick, and A. B. Owen. Consistency of Markov chain quasi-Monte Carlo on con-  tinuous state spaces. Annals of Statistics, 39(2):673–701, 2011.  [8] Y. Chen, M. Welling, and A.J. Smola. Supersamples from kernel-herding. In Uncertainty in  Artiﬁcial Intelligence, pages 109–116, 2010.  [9] A. Doucet, N. de Freitas, and N. Gordon. Sequential Monte Carlo Methods in Practice. Statis-  tics for Engineering and Information Science. Springer, 2001.  [10] A. Gelfand, Y. Chen, L. van der Maaten, and M. Welling. On herding and the perceptron In Advances in Neural Information Processing Systems, pages 694–702,  cycling theorem. 2010.  [11] N. D. Goodman, V. K. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum. Church:  a language for generative models. Uncertainty in Artiﬁcial Intelligence, 2008.  [12] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural net-  works. Science, 313(5786):504–507, 2006.  [13] Alexander E Holroyd and James Propp. Rotor walks and Markov chains. Algorithmic Proba-  bility and Combinatorics, 520:105–126, 2010.  [14] F. Husz´ar and D. Duvenaud. Optimally-weighted herding is Bayesian quadrature. Arxiv  preprint arXiv:1204.1664, 2012.  [15] T. Grenager J. R. Finkel and C. Manning. Incorporating non-local information into informa- tion extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 363–370, Stroudsburg, PA, USA, 2005. Association for Computational Linguistics.  1We thank Art Owen for pointing out this connection.  10  [16] J. S. Liu. Monte Carlo strategies in scientiﬁc computing. Springer, 2001. [17] D. J. Lunn, A. Thomas, N. Best, and D. Spiegelhalter. WinBUGS a Bayesian modelling frame- work: Concepts, structure, and extensibility. Statistics and Computing, 10(4):325–337, 2000. In  [18] B. Milch and S. Russell. General-purpose MCMC inference over relational structures.  Uncertainty in Artiﬁcial Intelligence, pages 349–358, 2006.  [19] K. P. Murphy. Machine Learning: a Probabilistic Perspective. MIT Press, 2012. [20] I. Murray and L. T. Elliott. Driving Markov chain Monte Carlo with a dependent random  stream. Technical Report arXiv:1204.3187, 2012.  [21] I. Porteous, D. Newman, A. Ihler, A. Asuncion, P. Smyth, and M. Welling. Fast collapsed Gibbs sampling for latent Dirichlet allocation. In ACM SIGKDD international conference on Knowledge discovery and data mining, pages 569–577, 2008.  [22] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, 2nd edition, 2004. [23] M. Welling. Herding dynamic weights for partially observed random ﬁeld models. In Uncer-  tainty in Artiﬁcial Intelligence, pages 599–606, 2009.  [24] M. Welling. Herding dynamical weights to learn. In International Conference on Machine  Learning, pages 1121–1128, 2009.  [25] Hermann Weyl.  ¨Uber die gleichverteilung von zahlen mod. eins. Mathematische Annalen,  77:313–352, 1916.  11  A Proof of Theorem 1  We ﬁrst show that the weight dynamics of a one-variable herding algorithm are restricted to an invariant interval of length 1. Lemma 3. If w is the weight of the herding dynamics of a single binary variable X with probability P (X = 1) = π, and w(s) ∈ (π−1, π] at some step s ≥ 0, then w(t) ∈ (π−1, π],∀t ≥ s. Moreover, for T ∈ N, we have:  s+T(cid:88) s+T(cid:88)  t=s+1  t=s+1  I[X (t) = 1] ∈ [T π − 1, T π + 1]  I[X (t) = 0] ∈ [T (1 − π) − 1, T (1 − π) + 1].  (12)  (13)  Proof. We ﬁrst show that w ∈ (π − 1, π],∀t ≥ s. This is easy to observe by induction as w(s) ∈ (π − 1, π] and if w(t) ∈ (π − 1, π] for some t ≥ s, then, following Equation 5, we have:  w(t+1) =  w(t) + π ∈ (2π − 1, π] ⊆ (π − 1, π]  (cid:26) w(t) + π − 1 ∈ (π − 1, 2π − 1] ⊆ (π − 1, π] T π − s+T(cid:88)  I[X (t) = 1] = w(s+T ) − w(s) ∈ [−1, 1].  if w(t) > 0 otherwise.  (14)  (15)  Summing up both sides of Equation 5 over t immediately gives us the result of Equation 12 since:  In addition, Equation 13 follows by observing that I[X (t) = 0] = 1 − I[X (t) = 1].  t=s+1  When w is outside the invariant interval, it is easy to observe that w will move into it monotonically at a linear speed in a transient period. So we will always consider an initialization of w ∈ (π − 1, π] from now on. Equivalently, we can take a one-to-one mapping w ← w mod 1 (we deﬁne 1 mod 1 = 1) and think of w as updated by a constant translation vector in a circular unit interval (0, 1] as shown in Figure 8. That is,  w(t) = (w(t−1) + π) mod 1,  x(t) =  if w(t−1) < π otherwise  (16)  (cid:26) 1  0  Figure 8: Equivalent weight dynamics for a single variable.  Figure 9: Dynamics of herding with two indepen- dent variables.  We are now ready to give the proof of Theorem 1.  12  Proof of Theorem 1. For an empty graph of N independent vertices, the dynamics of the weight vector w are equivalent to a constant translation mapping in an N-dimensional circular unit space (0, 1], as shown in Figure 9:  w(t) = (w(t−1) + π) mod 1  = (w(0) + tπ) mod 1,  x(t) i =  (cid:26)  if w(t−1) otherwise  i  < πi  1 0  ,∀1 ≤ i ≤ N  (17)  The Kronecker-Weyl theorem [25] states that the sequence ˜w(t) = tπ mod 1, t ∈ Z+ is equidis- tributed (or uniformly distributed) on (0, 1] if and only if (1, π1, . . . , πN ) is rationally indepen- dent. Since we can deﬁne a one-to-one volume preserving transformation between ˜w(t) and w(t) as ( ˜w(t) + w(0)) mod 1 = w(t), the sequence of weights {w(t)} is also uniformly distributed in (0, 1]N . Deﬁne the mapping from a state value xi to an interval of wi as if x = 1 if x = 0  Ai(x) =  (18)  and let |Ai| be its measure. We obtain the limiting distribution of the joint state as  T→∞ P (τ ) lim  T (X = x) = lim T→∞  (cid:34)  I  w(t−1) ∈ N(cid:89)  t=1  i=1  (cid:35)  Ai(xi)  (cid:26) (0, πi] T(cid:88)  (πi, 1]  1 T  N(cid:89) N(cid:89)  i=1  =  =  |Ai(xi)|  π(Xi = xi)  i=1  = π(X = x)  (19)  13  B Proof of Theorem 2  In this appendix, we give an upper bound for the convergence rate of the sampling distribution in fully connected graphs. As herded Gibbs sampling is deterministic, the distribution of a variable’s state at every iteration degenerates to a single state. As such, we study here the empirical distribution of a collection of samples. The structure of the proof is as follows (with notation deﬁned in the next subsection): We study the distribution distance between the invariant distribution π and the empirical distribution of T samples T . We show that the distance decreases as τ ⇒ τ + 1 with the collected starting from sweep τ, P (τ ) help of an auxiliary regular Gibbs sampling Markov chain initialized at π(0) = P (τ ) T , as shown in Figure 10. On the one hand, the distance between the regular Gibbs chain after one iteration, π(1), and π decreases according to the geometric convergence property of MCMC algorithms on compact state spaces. On the other hand, we show that in one step the distance between P (τ +1) and π(1) increases by at most O(1/T ). Since the O(1/T ) distance term dominates the exponentially small distance term, the distance between P (τ +1) and π is bounded by O(1/T ). Moreover, after a short burn-in period, L = O(log(T )), the empirical distribution P (τ +L) will have an approximation error in the order of O(1/T ).  T  T  T  Figure 10: Transition kernels and relevant distances for the proof of Theorem 2.  B.1 Notation  Assume without loss of generality that in the systematic scanning policy, the variables are sampled in the order 1, 2,··· , N.  B.1.1 State Distribution  probability.  • Denote by X+ the support of the distribution π, that is, the set of states with positive  • We use τ to denote the time in terms of sweeps over all of the N variables, and t to denote the time in terms of steps where one step constitutes the updating of one variable. For example, at the end of τ sweeps, we have t = τ N.  • Recall the sample/empirical distribution, P (τ ) vides a visual interpretation of the deﬁnition.  T , presented in Deﬁnition 1. Figure 11 pro-  • Denote the sample/empirical distribution at the ith step within a sweep as P (τ )  T,i , τ ≥ 0, T >  0, 0 ≤ i ≤ N, as shown in Figure 12:  τ +T−1(cid:88)  k=τ  I(X(kN +i) = x).  P (τ )  T,i (X = x) =  1 T  14  Figure 11: Distribution over time at the end of every sweep.  This is the distribution of T samples collected at the ith step of every sweep, starting from the τ th sweep. Clearly, P (τ )  T,0 = P (τ−1) T,N .  T = P (τ )  Figure 12: Distribution over time within a sweep.  • Denote the distribution of a regular Gibbs sampling Markov chain after L sweeps of updates  over the N variables with π(L), L ≥ 0. For a given time τ, we construct a Gibbs Markov chain with initial distribution π0 = P (τ ) and the same scanning order of herded Gibbs, as shown in Figure 10.  T  B.1.2 Transition Kernel  • Denote the transition kernel of regular Gibbs for the step of updating a variable Xi with Ti, and for a whole sweep with T . By deﬁnition, π0T = π1. The transition kernel for a single step can be represented as a 2N × 2N matrix: Ti(x, y) =  , 1 ≤ i ≤ N, x, y ∈ {0, 1}N  if x−i (cid:54)= y−i  (cid:26)  (20)  π(Xi = yi|x−i) otherwise  0  where x is the current state vector of N variables, y is the state of the next step, and x−i denotes all the components of x excluding the ith component. If π(x−i) = 0, the condi- tional probability is undeﬁned and we set it with an arbitrary distribution. Consequently, T can also be represented as:  T = T1T2 ···TN .  • Denote the Dobrushin ergodic coefﬁcient [5] of the regular Gibbs kernel with η ∈ [0, 1].  When η < 1, the regular Gibbs sampler has a geometric rate of convergence of  dv(π(1) − π) = dv(T π(0) − π) ≤ ηdv(π(0) − π),∀π(0). A common sufﬁcient condition for η < 1 is that π(X) is strictly positive.  (21)  • Consider the sequence of sample distributions P (τ )  T , τ = 0, 1,··· in Figures 11 and 12. We deﬁne the transition kernel of herded Gibbs for the step of updating variable Xi with ˜T (τ ) T,i , and for a whole sweep with ˜T (τ ) T . Unlike regular Gibbs, the transition kernel is not homogeneous. It depends on both the time τ and the sample size T . Nevertheless, we can still represent the single step transition  15  kernel as a matrix:  (cid:26)  ˜T (τ ) T,i (x, y) =  0 T,i (Xi = yi|x−i) P (τ ) T,i (Xi = yi|x−i) is deﬁned as:  where P (τ )  T,i (Xi = yi|x−i) = P (τ )  Nnum Nden  Nnum = T P (τ )  T,i (X−i = x−i, Xi = yi) =  τ +T−1(cid:88)  if x−i (cid:54)= y−i if x−i = y−i  , 1 ≤ i ≤ N, x, y ∈ {0, 1}N ,  (22)  τ +T−1(cid:88)  k=τ  I(X(kN +i)  −i  = x−i, X (kN +i)  i  = yi)  Nden = T P (τ )  T,i−1(X−i = x−i) =  I(X(kN +i−1)  −i  = x−i),  (23)  k=τ  where Nnum is the number of occurrences of a joint state, and Nden is the number of oc- currences of a conditioning state in the previous step. When π(x−i) = 0, we know that T,i = Ti for Nden = 0 with a proper initialization of herded Gibbs, and we simply set ˜T (τ ) these entries. It is not hard to verify the following identity by expanding every term with its deﬁnition  P (τ ) T,i = P (τ )  T,i−1  ˜T (τ )  T,i  and consequently,  with  B.2 Linear Visiting Rate  ˜T (τ )  T  P (τ +1) ˜T (τ ) T = ˜T (τ )  = P (τ ) ˜T (τ ) T,2 ··· ˜T (τ ) T,N .  T,1  T  T  We prove in this section that every joint state in the support of the target distribution is visited, at least, at a linear rate. This result will be used to measure the distance between the Gibbs and herded Gibbs transition kernels. Proposition 4. If a graph is fully connected, herded Gibbs sampling scans variables in a ﬁxed order, and the corresponding Gibbs sampling Markov chain is irreducible, then for any state x ∈ X+ and any index i ∈ [1, N ], the state is visited at least at a linear rate. Speciﬁcally, ∃l > 0, B > 0, s.t.,∀i ∈ [1, N ], x ∈ X+, T ∈ N, s ∈ N  s+T−1(cid:88)  I(cid:104)  (cid:105) ≥ lT − B  X(t=N k+i) = x  (24)  k=s  Denote the minimum nonzero conditional probability as  πmin =  min  1≤i≤N,π(xi|x−i)>0  π(xi|x−i).  The following lemma, which is needed to prove Proposition 4, gives an inequality between the number of visits of two sets of states in consecutive steps. Lemma 5. For any integer i ∈ [1, N ] and two sets of states X, Y ⊆ X+ with a mapping F : X → Y that satisﬁes the following condition:  (25) we have that, for any s ≥ 0 and T > 0, the number of times Y is visited in the set of steps Ci = {t = kN + i : s ≤ k ≤ k + T − 1} is lower bounded by a function of the number of times X is visited in the previous steps Ci−1 = {t = kN + i − 1 : s ≤ k ≤ k + T − 1} as:  ∀x ∈ X, F(x)−i = x−i, ∪x∈XF (x) = Y, (cid:88)  I(cid:104) X(t) ∈ X(cid:105) − |Y|  I(cid:104) X(t) ∈ Y(cid:105) ≥ πmin  (cid:88)  (26)  t∈Ci  t∈Ci−1  16  (27)  (28)  Proof. As a complement to Condition 25, we can deﬁne F −1 as the inverse mapping from Y to subsets of X so that for any y ∈ Y, x ∈ F −1(y), we have x−i = y−i, and ∪y∈YF −1(y) = X. Consider any state y ∈ Y, when y is visited in Ci, the weight wi,y−i is active. Let us denote the set of all the steps in [sN + 1, s(N + T ) + N ] when wi,y−i is active by Ci(y−i), that is, Ci(y−i) = {t : t ∈ Ci, X(t)−i = y−i}. Applying Lemma 3 we get  (cid:105) ≥ π(yi|y−i)|Ci(y−i)| − 1 ≥ πmin|Ci(y−i)| − 1.  X(t) = y  I(cid:104)  (cid:88)  t∈Ci  Since the variables X−i are not changed at steps in Ci, we have  |Ci(y−i)| =  X(t)−i = y−i  X(t) ∈ F −1(y)  (cid:88)  I(cid:104)  t∈Ci−1  (cid:105) ≥ (cid:88)  I(cid:104)  t∈Ci−1  (cid:105)  .  Combining the fact that ∪y∈YF −1(y) = X and summing up both sides of Equation 27 over Y proves the lemma:  I(cid:104)  X(t) ∈ Y(cid:105) ≥(cid:88)  πmin  (cid:88)  I(cid:104)  y∈Y  t∈Ci−1  (cid:88)  t∈Ci  (cid:105) − 1   ≥ πmin  X(t) ∈ F −1(y)  (cid:88)  I(cid:104)  X(t) ∈ X(cid:105)−|Y|.  t∈Ci−1  (29)  Remark 6. A fully connected graph is a necessary condition for the application of Lemma 3 in the proof. If a graph is not fully connected (N (i) (cid:54)= −i), a weight wi,yN (i) may be shared by multiple full conditioning states. In this case Ci(y−i) is no longer a consecutive sequence of times when the weight is updated, and Lemma 3 does not apply here.  Now let us prove Proposition 4 by iteratively applying Lemma 5.  Proof of Proposition 4. Because the corresponding Gibbs sampler is irreducible and any Gibbs sam- pler is aperiodic, there exists a constant t∗ > 0 such that for any state y ∈ X+, and any step in a sweep, i, we can ﬁnd a path of length t∗ for any state x ∈ X+ with a positive transition probability, P ath(x) = (x = x(0), x(1), . . . , x(t∗) = y), to connect from x to y, where each step of the path follows the Gibbs updating scheme. For a strictly positive distribution, the minimum value of t∗ is N. Denote τ∗ = (cid:100)t∗/N(cid:101) and the jth element of the path P ath(x) as P ath(x, j). We can deﬁne t∗ + 1 subsets Sj ⊆ X+, 0 ≤ j ≤ t∗ as the union of all the jth states in the path from any state in X+:  Sj = ∪x∈X+P ath(x, j)  By deﬁnition of these paths, we know S0 = X+ and St∗ = {y}, and there exits an integer i(j) and a mapping Fj : Sj−1 → Sj,∀j that satisfy the condition in Lemma 5 (i(j) is the index of the variable to be updated, and the mapping is deﬁned by the transition path). Also notice that any state in Sj can be different from y by at most min{N, t∗ − j} variables, and therefore |Sj| ≤ 2min{N,t∗−j}.  17  Let us apply Lemma 5 recursively from j = t∗ to 1 as  s+T−1(cid:88)  I(cid:104)  k=s  X(t=N k+i) = y  (cid:105) ≥ s+T−1(cid:88) s+T−1(cid:88)  k=s+τ∗  =  k=s+τ∗  (cid:105)  (cid:105)  X(t=N k+i−1) ∈ St∗−1  k=s+τ∗  X(t=N k+i) = y  I(cid:104) I(cid:104)  X(t=N k+i) ∈ St∗  I(cid:104) I(cid:104) s+T−1(cid:88) s+T−1(cid:88) min(T − τ∗) − t∗−1(cid:88) min2min{N,j}. πj t∗−1(cid:88)  k=s+τ∗  j=0  min  ≥ πmin ≥ ···  ≥ πt∗  ≥ πt∗  X(t=N k+i−t∗) ∈ S0 = X+  l = πt∗  min, B = τ∗πt∗  min +  min2min{N,j}. πj  (cid:105) − |St∗| (cid:105) − t∗−1(cid:88)  j=0  min|St∗−j| πj  (30)  (31)  The proof is concluded by choosing the constants  j=0  is an Approximation to T  B.3 Herded Gibbs’s Transition Kernel ˜T (τ ) The following proposition shows that ˜T (τ ) sition kernel T with an error of O(1/T ). Proposition 7. For a fully connected graph, if the herded Gibbs has a ﬁxed scanning order and the corresponding Gibbs sampling Markov chain is irreducible, then for any τ ≥ 0, T ≥ T ∗ := 2B where l and B are the constants in Proposition 4, the following inequality holds:  is an approximation to the regular Gibbs sampler’s tran-  T  T  l  (cid:107) ˜T (τ )  T − T (cid:107)∞ ≤ 4N lT  (32)  Proof. When x (cid:54)∈ X+, we have the equality ˜T (τ ) but y (cid:54)∈ X+, then Nden = 0 (see the notation of ˜T (τ ) visited and thus ˜T (τ ) with x, y ∈ X+ in the following. Because X−i is not updated at ith step of every sweep, we can replace i− 1 in the deﬁnition of Nden by i and get  T,i (x, y) = Ti(x, y) by deﬁnition. When x ∈ X+ for deﬁnition of Nden) as y will never be T,i (x, y)  T,i (x, y) = 0 = Ti(x, y) also holds. Let us consider the entries in ˜T (τ )  T  τ +T−1(cid:88)  Nden =  I(X(kN +i)  −i  = x−i).  Notice that the set of times {t = kN + i : τ ≤ k ≤ τ + T − 1, Xt−i = x−i)}, whose size is Nden, is a consecutive set of times when wi,x−i is updated. By Lemma 3, we obtain a bound for the numerator  k=τ  Nnum ∈ [Ndenπ(Xi = yi|x−i) − 1, Ndenπ(Xi = yi|x−i) + 1] ⇔ |P (τ ) T,i (Xi = yi|x−i) − π(Xi = yi|x−i)| = | Nnum Nden  (33) Also by Proposition 4, we know every state in X+ is visited at a linear rate, there hence exist constants l > 0 and B > 0, such that the number of occurrence of any conditioning state x−i, Nden,  − π(Xi = yi|x−i)| ≤ 1 Nden  .  18  is bounded by  Nden ≥ τ +T−1(cid:88)  k=τ  I(X(kN +i) = x) ≥ lT − B ≥ l 2  T,  ∀T ≥ 2B l  .  Combining equations (33) and (34), we obtain  |P (τ ) T,i (Xi = yi|x−i) − π(Xi = yi|x−i)| ≤ 2 lT  ,  ∀T ≥ 2B l  .  (34)  (35)  Since the matrix ˜T (τ ) induced norm of the transposed matrix of their difference by  T,i and Ti differ only at those elements where x−i = y−i, we can bound the L1  (cid:107)( ˜T (τ )  T,i − Ti)T(cid:107)1 = max  x  = max  x  T,i (x, y) − Ti(x, y)| | ˜T (τ ) |P (τ ) T,i (Xi = yi|x−i) − π(Xi = yi|x−i)|  (cid:88) (cid:88)  y  yi  ≤ 4 lT  ,  ∀T ≥ 2B l  (36)  Observing that both ˜T (τ ) transition matrices, ˜T (τ )  T  and T are multiplications of N component transition matrices, and the and Ti, have a unit L1 induced norm as:  T  (cid:107)( ˜T (τ )  T,i )T(cid:107)1 = max (cid:107)(Ti)T(cid:107)1 = max  x  x  (cid:88) (cid:88)  y  y  (cid:88) T,i (x, y)| = max | ˜T (τ ) (cid:88) |Ti(x, y)| = max  x  y  x  y  T,i (Xi = yi|x−i) = 1 P (τ )  P (Xi = yi|x−i) = 1  (37)  (38)  T − T )T . Let P ∈ RN be any vector with we can further bound the L1 norm of the difference, ( ˜T (τ ) nonzero norm. Using the triangular inequality, the difference of the resulting vectors after applying ˜T (τ )  and T is bounded by  T  (cid:107)P ( ˜T (τ )  T,N − PT . . .TN(cid:107)1 T,N − PT1 ˜T (τ )  T,1 . . . ˜T (τ )  T − T )(cid:107)1 =(cid:107)P ˜T (τ ) ≤(cid:107)P ˜T (τ ) (cid:107)PT1 ˜T (τ ) . . . (cid:107)PT1 . . .TN−1 ˜T (τ )  T,2 . . . ˜T (τ ) ˜T (τ ) T,3 . . . ˜T (τ ) ˜T (τ )  T,1  T,2  T,N − PT1 . . .TN−1TN(cid:107)1  T,2 . . . ˜T (τ )  T,N(cid:107)1+ T,3 . . . ˜T (τ )  T,N(cid:107)1+  T,N − PT1T2 ˜T (τ )  where the i’th term is (cid:107)PT1 . . .Ti−1( ˜T (τ )  T,i − Ti) ˜T (τ )  T,i+1 . . . ˜T (τ )  T,i − Ti)(cid:107)1 T,N(cid:107)1 ≤ (cid:107)PT1 . . .Ti−1( ˜T (τ ) 4 lT  ≤ (cid:107)PT1 . . .Ti−1(cid:107)1 ≤ (cid:107)P(cid:107)1  4 lT  Consequently, we get the L1 induced norm of ( ˜T (τ ) (cid:107)P ( ˜T (τ )  (cid:107)( ˜T (τ )  T − T )T(cid:107) = max  T − T )T as T − T )(cid:107)1 (cid:107)P(cid:107)1  P  ≤ 4N lT  19  (39)  (Unit L1 norm, Eqn. 37)  (Eqn. 36)  (Unit L1 norm, Eqn. 38)  (40)  (41)  ,  ∀T ≥ 2B l  ,  B.4 Proof of Theorem 2  When we initialize the herded Gibbs and regular Gibbs with the same distribution (see Figure 10), since the transition kernel of herded Gibbs is an approximation to regular Gibbs and the distribution of regular Gibbs converges to the invariant distribution, we expect that herded Gibbs also approaches the invariant distribution.  Proof of Theorem 2. Construct an auxiliary regular Gibbs sampling Markov chain initialized with π(0)(X) = P (τ ) T (X) and the same scanning order as herded Gibbs. As η < 1, the Gibbs Markov chain has uniform geometric convergence rate as shown in Equation (21). Also, the Gibbs Markov chain must be irreducible due to η < 1 and therefore Proposition 7 applies here. We can bound the distance between the distributions of herded Gibbs after one sweep of all variables, P (τ +1)  , and the distribution after one sweep of regular Gibbs sampling, π(1) by  T  T  dv(P (τ +1) ≤ 2N lT  (cid:107)π(0)(cid:107)1 =  2N lT  ,  − π(1)) = dv(π(0)( ˜T (τ )  T − T )) = ∀T ≥ T ∗, τ ≥ 0.  (cid:107)π(0)( ˜T (τ )  T − T )(cid:107)1  1 2  Now we study the change of discrepancy between P (τ ) Applying the triangle inequality of dv:  T  and π as a function as τ.  T  dv(P (τ +1) ≤ 2N lT  − π) = dv(P (τ +1)  T  + ηdv(P (τ )  T − π),  − π(1) + π(1) − π) ≤ dv(P (τ +1) ∀T ≥ T ∗, τ ≥ 0.  T  − π(1)) + dv(π(1) − π)  (42)  (43)  The last inequality follows Equations (21) and (42). When the sample distribution is outside a neighborhood of π, B(cid:15)1 (π), with (cid:15)1 = 4N  (1−η)lT , i.e. T − π) ≥  dv(P (τ )  4N  (1 − η)lT  ,  (44)  − π) ≤ 1 − η  we get a geometric convergence rate toward the invariant distribution by combining the two equa- tions above:  T  dv(P (τ +1)  (45) So starting from τ = 0, we have a burn-in period for herded Gibbs to enter B(cid:15)1(π) in a ﬁnite number of rounds. Denote the ﬁrst time it enters the neighborhood by τ(cid:48). According to the geometric convergence rate in Equations 45 and dv(P (0)  T − π) ≤ 1  T − π) + ηdv(P (τ )  T − π) =  T − π).  dv(P (τ )  dv(P (τ )  1 + η  2  2  τ(cid:48) ≤  log 1+η 2  (46) After that burn-in period, the herded Gibbs sampler will stay within a smaller neighborhood, B(cid:15)2(π), with (cid:15)2 = 1+η 1−η  lT , i.e.  log 1+η 2  ((cid:15)1)  2N  (  )  (cid:15)1 T − π) dv(P (0)  = (cid:100)τ∗(T )(cid:101).  dv(P (τ )  T − π) ≤ 1 + η 1 − η  2N lT  ,  ∀τ > τ(cid:48).  (47)  (cid:39)  ≤(cid:108)  (cid:109)  (cid:38)  This is proved by induction:  we get  1. Equation (47) holds at τ = τ(cid:48) + 1. This is because P (τ(cid:48))  ∈ B(cid:15)1(π) and following Eqn. 43  T  dv(P (τ(cid:48)+1) 2. For any τ ≥ τ(cid:48) + 2, assume P (τ−1)  T  T  + η(cid:15)1 = (cid:15)2  − π) ≤ 2N lT ∈ B(cid:15)2(π). Since (cid:15)2 < (cid:15)1, P (τ−1)  T  B(cid:15)1(π). We can apply the same computation as when τ = τ(cid:48) + 1 to prove dv(P (τ ) (cid:15)2. So inequality (47) is always satisﬁed by induction.  is also in the ball T − π) ≤  (48)  20  Consequently, Theorem 2 is proved when combining (47) with the inequality τ(cid:48) ≤ (cid:100)τ∗(T )(cid:101) in Equation( 46). Remark 8. Similarly to the regular Gibbs sampler, the herded Gibbs sampler also has a burn-in period with geometric convergence rate. After that, the distribution discrepancy is in the order of O(1/T ), which is faster than the regular Gibbs sampler. Notice that the length of the burn-in period depends on T , speciﬁcally as a function of log(T ). Remark 9. Irrationality is not required to prove the convergence on a fully-connected graph. Corollary 10. When the conditions of Theorem 2 hold, and we start collecting samples at the end of every sweep from the beginning, the error of the sample distribution is bounded by: ∀T ≥ T ∗ + τ∗(T ∗)  − π) ≤ λ + τ∗(T )  log(T )  = O(  dv(P (τ =0)  (49)  ),  T  T  T  Proof. Since τ∗(T ) is a monotonically increasing function of T , for any T ≥ T ∗ + τ∗(T ∗), we can ﬁnd a number t so that  T = t + τ∗(t), t ≥ T ∗.  Partition the sample sequence S0,T = {X(kN ) : 0 ≤ k < T} into two parts: the burn-in period S0,τ∗(t) and the stable period Sτ∗(t),T . The discrepancy in the burn-in period is bounded by 1 and according to Theorem 2, the discrepancy in the stable period is bounded by  dv( ˜P (St,T ) − π) ≤ λ t Hence, the discrepancy of the whole set S0,T is bounded by  .  (cid:18) τ∗(t) (cid:19)  dv( ˜P (S0,T ) − π) = dv  (cid:18) τ∗(t) ≤ dv ≤ τ∗(t) ≤ τ∗(t)  T ( ˜P (S0,τ∗(t)) − π T dv( ˜P (S0,τ∗(t)) − π) + ≤ τ∗(T ) + λ · 1 +  T  t T  t T  λ t  T  T  (cid:19)  ˜P (S0,τ∗(t)) +  ˜P (Sτ∗(t),T ) − π  t T  + dv  ( ˜P (Sτ∗(t),T ) − π  (cid:19)  (cid:18) t  T  dv( ˜P (Sτ∗(t),T ) − π)  .  (50)  21  ","The Gibbs sampler is one of the most popular algorithms for inference instatistical models. In this paper, we introduce a herding variant of thisalgorithm, called herded Gibbs, that is entirely deterministic. We prove thatherded Gibbs has an $O(1/T)$ convergence rate for models with independentvariables and for fully connected probabilistic graphical models. Herded Gibbsis shown to outperform Gibbs in the tasks of image denoising with MRFs andnamed entity recognition with CRFs. However, the convergence for herded Gibbsfor sparsely connected probabilistic graphical models is still an open problem."
1301.4083,2013,Knowledge Matters: Importance of Prior Information for Optimization  ,"['Çağlar Gülçehre', 'Yoshua Bengio']",https://arxiv.org/pdf/1301.4083.pdf,"3 1 0 2    l u J    3 1      ]  G L . s c [      6 v 3 8 0 4  .  1 0 3 1 : v i X r a  Knowledge Matters: Importance of Prior Information for  Optimization  gulcehrc@iro.umontreal.ca  bengioy@iro.umontreal.ca  C¸ a˘glar G¨ul¸cehre D´epartement d’informatique et de recherche op´erationnelle Universit´e de Montr´eal, Montr´eal, QC, Canada  Yoshua Bengio D´epartement d’informatique et de recherche op´erationnelle Universit´e de Montr´eal, Montr´eal, QC, Canada  Editor: Not Assigned  Abstract  We explore the eﬀect of introducing prior information into the intermediate level of deep supervised neural networks for a learning task on which all the black-box state-of-the-art ma- chine learning algorithms tested have failed to learn. We motivate our work from the hypothesis that there is an optimization obstacle involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset for which each image input contains three sprites, and the binary target class is 1 if all three have the same shape. Black-box machine learning algorithms only got chance on this task. Standard deep supervised neural networks also failed. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the pres- ence of each object) allows to nail the task. Much better than chance but imperfect results are also obtained by exploring architecture and optimization variants, pointing towards a diﬃcult optimization task. We hypothesize that the learning diﬃculty is due to the composition of two highly non-linear tasks. Our ﬁndings are also consistent with hypotheses on cultural learning inspired by the observations of eﬀective local minima (possibly due to ill-conditioning and the training procedure not being able to escape what appears like a local minimum).  Keywords: Deep Learning, Neural Networks, Optimization, Evolution of Culture, Curricu- lum Learning, Training with Hints  1. Introduction  There is a recent emerging interest in diﬀerent ﬁelds of science for cultural learning (Henrich and McElreath, 2003) and how groups of individuals exchanging information can learn in ways superior to individual learning. This is also witnessed by the emergence of new research ﬁelds such as ”Social Neuroscience”. Learning from other agents in an environment by the means of cultural transmission of knowledge with a peer-to-peer communication is an eﬃcient and natural way of acquiring or propagating common knowledge. The most popular belief on how the information is transmitted between individuals is that bits of information are transmitted by small units, called memes, which share some characteristics of genes, such as self-replication, mutation and response to selective pressures (Dawkins, 1976).  1  This paper is based on the hypothesis (which is further elaborated in Bengio (2013a)) that human culture and the evolution of ideas have been crucial to counter an optimization issue: this diﬃculty would otherwise make it diﬃcult for human brains to capture high level knowl- edge of the world without the help of other educated humans. In this paper machine learning experiments are used to investigate some elements of this hypothesis by seeking answers for the following questions: are there machine learning tasks which are intrinsically hard for a lone learning agent but that may become very easy when intermediate concepts are provided by another agent as additional intermediate learning cues, in the spirit of Curriculum Learn- ing (Bengio et al., 2009b)? What makes such learning tasks more diﬃcult? Can speciﬁc initial values of the neural network parameters yield success when random initialization yield com- plete failure? Is it possible to verify that the problem being faced is an optimization problem or with a regularization problem? These are the questions discussed (if not completely addressed) here, which relate to the following broader question: how can humans (and potentially one day, machines) learn complex concepts?  In this paper, results of diﬀerent machine learning algorithms on an artiﬁcial learning task involving binary 64×64 images are presented. In that task, each image in the dataset contains 3 Pentomino tetris sprites (simple shapes). The task is to ﬁgure out if all the sprites in the image are the same or if there are diﬀerent sprite shapes in the image. Several state-of-the-art machine learning algorithms have been tested and none of them could perform better than a random predictor on the test set. Nevertheless by providing hints about the intermediate concepts (the presence and location of particular sprite classes), the problem can easily be solved where the same-architecture neural network without the intermediate concepts guidance fails. Surprisingly, our attempts at solving this problem with unsupervised pre-training algorithms failed solve this problem. However, with speciﬁc variations in the network architecture or training procedure, it is found that one can make a big dent in the problem. For showing the impact of intermediate level guidance, we experimented with a two-tiered neural network, with supervised pre-training of the ﬁrst part to recognize the category of sprites independently of their orientation and scale, at diﬀerent locations, while the second part learns from the output of the ﬁrst part and predicts the binary task of interest.  The objective of this paper is not to propose a novel learning algorithm or architecture, but rather to reﬁne our understanding of the learning diﬃculties involved with composed tasks (here a logical formula composed with the detection of object classes), in particular the training diﬃculties involved for deep neural networks. The results also bring empirical evidence in favor of some of the hypotheses from Bengio (2013a), discussed below, as well as introducing a particular form of curriculum learning (Bengio et al., 2009b).  Building diﬃcult AI problems has a long history in computer science. Speciﬁcally hard AI problems have been studied to create CAPTCHA’s that are easy to solve for humans, but hard to solve for machines (Von Ahn et al., 2003). In this paper we are investigating a diﬃcult problem for the oﬀ-the-shelf black-box machine learning algorithms.1  1.1 Curriculum Learning and Cultural Evolution Against Eﬀective Local Minima  What Bengio (2013a) calls an eﬀective local minimum is a point where iterative training stalls, either because of an actual local minimum or because the optimization algorithm is  1. You can access the source code of some experiments presented in that paper and their hyperparameters from  here: https://github.com/caglar/kmatters  2  unable (in reasonable time) to ﬁnd a descent path (e.g., because of serious ill-conditioning). In this paper, it is hypothesized that some more abstract learning tasks such as those obtained by composing simpler tasks are more likely to yield eﬀective local minima for neural networks, and are generally hard for general-purpose machine learning algorithms.  The idea that learning can be enhanced by guiding the learner through intermediate easier tasks is old, starting with animal training by shaping (Skinner, 1958; Peterson, 2004; Krueger and Dayan, 2009). Bengio et al. (2009b) introduce a computational hypothesis related to a presumed issue with eﬀective local minima when directly learning the target task: the good solutions correspond to hard-to-ﬁnd-by-chance eﬀective local minima, and intermediate tasks prepare the learner’s internal conﬁguration (parameters) in a way similar to continuation meth- ods in global optimization (which go through a sequence of intermediate optimization problems, starting with a convex one where local minima are no issue, and gradually morphing into the target task of interest).  In a related vein, Bengio (2013a) makes the following inferences based on experimental  observations of deep learning and neural network learning:  Point 1: Training deep architectures is easier when some hints are given about the function that the intermediate levels should compute (Hinton et al., 2006; Weston et al., 2008; Salakhutdinov and Hinton, 2009; Bengio, 2009). The experiments performed here expand in particular on this point.  Point 2: It is much easier to train a neural network with supervision (where examples ar provided to it of when a concept is present and when it is not present in a variety of examples) than to expect unsupervised learning to discover the concept (which may also happen but usually leads to poorer renditions of the concept). The poor results obtained with unsupervised pre-training reinforce that hypothesis.  Point 3: Directly training all the layers of a deep network together not only makes it diﬃcult to exploit all the extra modeling power of a deeper architecture but in many cases it actually yields worse results as the number of required layers is increased (Larochelle et al., 2009; Erhan et al., 2010). The experiments performed here also reinforce that hypothesis.  Point 4: Erhan et al. (2010) observed that no two training trajectories ended up in the same eﬀective local minimum, out of hundreds of runs, even when comparing solutions as functions from input to output, rather than in parameter space (thus eliminating from the picture the presence of symmetries and multiple local minima due to relabeling and other reparametrizations). This suggests that the number of diﬀerent eﬀective local minima (even when considering them only in function space) must be huge.  Point 5: Unsupervised pre-training, which changes the initial conditions of the descent pro- cedure, sometimes allows to reach substantially better eﬀective local minima (in terms of generalization error!), and these better local minima do not appear to be reachable by chance alone (Erhan et al., 2010). The experiments performed here provide another piece of evidence in favor of the hypothesis that where random initialization can yield rather poor results, speciﬁcally targeted initialization can have a drastic impact, i.e., that  3  eﬀective local minima are not just numerous but that some small subset of them are much better and hard to reach by chance.2  Based on the above points, Bengio (2013a) then proposed the following hypotheses regarding  learning of high-level abstractions.  • Optimization Hypothesis: When it learns, a biological agent performs an approximate  optimization with respect to some implicit objective function.  • Deep Abstractions Hypothesis: Higher level abstractions represented in brains re-  quire deeper computations (involving the composition of more non-linearities).  • Local Descent Hypothesis: The brain of a biological agent relies on approximate local  descent and gradually improves itself while learning.  • Eﬀective Local Minima Hypothesis: The learning process of a single human learner  (not helped by others) is limited by eﬀective local minima.  • Deeper Harder Hypothesis: Eﬀective local minima are more likely to hamper learning  as the required depth of the architecture increases.  • Abstractions Harder Hypothesis: High-level abstractions are unlikely to be discov- ered by a single human learner by chance, because these abstractions are represented by a deep subnetwork of the brain, which learns by local descent.  • Guided Learning Hypothesis: A human brain can learn high level abstractions if guided by the signals produced by other agents that act as hints or indirect supervision for these high-level abstractions.  • Memes Divide-and-Conquer Hypothesis: Linguistic exchange, individual learning and the recombination of memes constitute an eﬃcient evolutionary recombination oper- ator in the meme-space. This helps human learners to collectively build better internal representations of their environment, including fairly high-level abstractions.  This paper is focused on “Point 1 ” and testing the “Guided Learning Hypothesis”, using machine learning algorithms to provide experimental evidence. The experiments performed also provide evidence in favor of the “Deeper Harder Hypothesis” and associated “Abstractions Harder Hypothesis”. Machine Learning is still far beyond the current capabilities of humans, and it is important to tackle the remaining obstacles to approach AI. For this purpose, the question to be answered is why tasks that humans learn eﬀortlessly from very few examples, while machine learning algorithms fail miserably?  2. Recent work showed that rather deep feedforward networks can be very successfully trained when large quantities of labeled data are available (Ciresan et al., 2010; Glorot et al., 2011a; Krizhevsky et al., 2012). Nonetheless, the experiments reported here suggest that it all depends on the task being considered, since even with very large quantities of labeled examples, the deep networks trained here were unsuccessful.  4  2. Culture and Optimization Diﬃculty  As hypothesized in the “Local Descent Hypothesis”, human brains would rely on a local ap- proximate descent, just like a Multi-Layer Perceptron trained by a gradient-based iterative op- timization. The main argument in favor of this hypothesis relies on the biologically-grounded assumption that although ﬁring patterns in the brain change rapidly, synaptic strengths un- derlying these neural activities change only gradually, making sure that behaviors are generally consistent across time. If a learning algorithm is based on a form of local (e.g. gradient-based) descent, it can be sensitive to eﬀective local minima (Bengio, 2013a).  When one trains a neural network, at some point in the training phase the evaluation of error seems to saturate, even if new examples are introduced. In particular Erhan et al. (2010) ﬁnd that early examples have a much larger weight in the ﬁnal solution. It looks like the learner is stuck in or near a local minimum. But since it is diﬃcult to verify if this is near a true local minimum or simply an eﬀect of strong ill-conditioning, we call such a “stuck” conﬁguration an eﬀective local minimum, whose deﬁnition depends not just on the optimization objective but also on the limitations of the optimization algorithm.  Erhan et al. (2010) highlighted both the issue of eﬀective local minima and a regulariza- tion eﬀect when initializing a deep network with unsupervised pre-training. Interestingly, as the network gets deeper the diﬃculty due to eﬀective local minima seems to be get more pro- nounced. That might be because of the number of eﬀective local minima increases (more like an actual local minima issue), or maybe because the good ones are harder to reach (more like an ill-conditioning issue) and more work will be needed to clarify this question.  As a result of Point 4 we hypothesize that it is very diﬃcult for an individual’s brain to discover some higher level abstractions by chance only. As mentioned in the “Guided Learning Hypothesis” humans get hints from other humans and learn high-level concepts with the guid- ance of other humans3. Curriculum learning (Bengio et al., 2009a) and incremental learning (Solomonoﬀ, 1989), are examples of this. This is done by properly choosing the sequence of examples seen by the learner, where simpler examples are introduced ﬁrst and more complex examples shown when the learner is ready for them. One of the hypothesis on why curriculum works states that curriculum learning acts as a continuation method that allows one to discover a good minimum, by ﬁrst ﬁnding a good minimum of a smoother error function. Recent ex- periments on human subjects also indicates that humans teach by using a curriculum strategy (Khan et al., 2011).  Some parts of the human brain are known to have a hierarchical organization (i.e. visual cortex) consistent with the deep architecture studied in machine learning papers. As we go from the sensory level to higher levels of the visual cortex, we ﬁnd higher level areas corresponding to more abstract concepts. This is consistent with the Deep Abstractions Hypothesis.  Training neural networks and machine learning algorithms by decomposing the learning task into sub-tasks and exploiting prior information about the task is well-established and in fact constitutes the main approach to solving industrial problems with machine learning. The contribution of this paper is rather on rendering explicit the eﬀective local minima issue and providing evidence on the type of problems for which this diﬃculty arises. This prior information and hints given to the learner can be viewed as inductive bias for a particular task, an important ingredient to obtain a good generalization error (Mitchell, 1980). An interesting  3. But some high-level concepts may also be hardwired in the brain, as assumed in the universal grammar  hypothesis (Montague, 1970), or in nature vs nurture discussions in cognitive science.  5  earlier ﬁnding in that line of research was done with Explanation Based Neural Networks (EBNN) in which a neural network transfers knowledge across multiple learning tasks. An EBNN uses previously learned domain knowledge as an initialization or search bias (i.e. to constrain the learner in the parameter space) (O’Sullivan, 1996; Mitchell and Thrun, 1993).  Another related work in machine learning is mainly focused on reinforcement learning al- gorithms, based on incorporating prior knowledge in terms of logical rules to the learning algorithm as a prior knowledge to speed up and bias learning (Kunapuli et al., 2010; Towell and Shavlik, 1994).  As discussed in “Memes Divide and Conquer Hypothesis“ societies can be viewed as a distributed computational processing systems. In civilized societies knowledge is distributed across diﬀerent individuals, this yields a space eﬃciency. Moreover computation, i.e. each individual can specialize on a particular task/topic, is also divided across the individuals in the society and hence this will yield a computational eﬃciency. Considering the limitations of the human brain, the whole processing can not be done just by a single agent in an eﬃcient manner. A recent study in paleoantropology states that there is a substantial decline in endocranial volume of the brain in the last 30000 years Henneberg (1988). The volume of the brain shrunk to 1241 ml from 1502 ml (Henneberg and Steyn, 1993). One of the hypothesis on the reduction of the volume of skull claims that, decline in the volume of the brain might be related to the functional changes in brain that arose as a result of cultural development and emergence of societies given that this time period overlaps with the transition from hunter-gatherer lifestyle to agricultural societies.  3. Experimental Setup  Some tasks, which seem reasonably easy for humans to learn4, are nonetheless appearing almost impossible to learn for current generic state-of-art machine learning algorithms.  Here we study more closely such a task, which becomes learnable if one provides hints to the learner about appropriate intermediate concepts. Interestingly, the task we used in our experiments is not only hard for deep neural networks but also for non-parametric machine learning algorithms such as SVM’s, boosting and decision trees.  The result of the experiments for varying size of dataset with several oﬀ-the-shelf black box machine learning algorithms and some popular deep learning algorithms are provided in Table 1. The detailed explanations about the algorithms and the hyperparameters used for those algorithms are given in the Appendix Section 5.2. We also provide some explanations about the methodologies conducted for the experiments at Section 3.2.  3.1 Pentomino Dataset In order to test our hypothesis, an artiﬁcial dataset for object recognition using 64×64 binary images is designed5. If the task is two tiered (i.e., with guidance provided), the task in the ﬁrst part is to recognize and locate each Pentomino object class6 in the image. The second  4. keeping in mind that humans can exploit prior knowledge, either from previous learning or innate knowledge. 5. The source code for the script that generates the artiﬁcial Pentomino datasets (Arcade-Universe) is available at: https://github.com/caglar/Arcade-Universe. This implementation is based on Olivier Breuleux’s bugland dataset generator.  6. A human learner does not seem to need to be taught the shape categories of each Pentomino sprite in order to solve the task. On the other hand, humans have lots of previously learned knowledge about the notion of shape and how central it is in deﬁning categories.  6  (a) sprites, not all same type  (b) sprites, all of same type  Figure 1: Left (a): An example image from the dataset which has a diﬀerent sprite type in it. Right (b): An example image from the dataset that has only one type of Pentomino object in it, but with diﬀerent orientations and scales.  part/ﬁnal binary classiﬁcation task is to ﬁgure out if all the Pentominos in the image are of the same shape class or not. If a neural network learned to detect the categories of each object at each location in an image, the remaining task becomes an XOR-like operation between the detected object categories. The types of Pentomino objects that is used for generating the dataset are as follows:  Pentomino sprites N, P, F, Y, J, and Q, along with the Pentomino N2 sprite (mirror of “Pentomino N” sprite), the Pentomino F2 sprite (mirror of “Pentomino F” sprite), and the Pentomino Y2 sprite (mirror of “Pentomino Y” sprite).  Figure 2: Diﬀerent classes of Pentomino shapes used in our dataset.  As shown in Figures 1(a) and 1(b), the synthesized images are fairly simple and do not have any texture. Foreground pixels are “1” and background pixels are “0”. Images of the training and test sets are generated iid. For notational convenience, assume that the domain of raw input images is X, the set of sprites is S, the set of intermediate object categories is Y for each possible location in the image and the set of ﬁnal binary task outcomes is Z = {0, 1}. Two diﬀerent types of rigid body transformation is performed: sprite rotation rot(X, γ) where Γ = {γ : (γ = 90 × φ) ∧ [(φ ∈ N), (0 ≤ φ ≤ 3)]} and scaling scale(X, α) where α ∈ {1, 2} is the scaling factor. The data generating procedure is summarized below. Sprite transformations: Before placing the sprites in an empty image, for each image x ∈ X, a value for z ∈ Z is randomly sampled which is to have (or not) the same three sprite shapes in the image. Conditioned on the constraint given by z, three sprites are randomly  7  selected sij from S without replacement. Using a uniform probability distribution over all possible scales, a scale is chosen and accordingly each sprite image is scaled. Then rotate each sprite is randomly rotated by a multiple of 90 degrees.  Sprite placement: Upon completion of sprite transformations, a 64×64 uniform grid is gener- ated which is divided into 8×8 blocks, each block being of size 8×8 pixels, and randomly select three diﬀerent blocks from the 64=8×8 on the grid and place the transformed objects into diﬀerent blocks (so they cannot overlap, by construction).  Each sprite is centered in the block in which it is located. Thus there is no object translation inside the blocks. The only translation invariance is due to the location of the block inside the image.  A Pentomino sprite is guaranteed to not overﬂow the block in which it is located, and there are no collisions or overlaps between sprites, making the task simpler. The largest possible Pentomino sprite can be ﬁt into an 8×4 mask.  3.2 Learning Algorithms Evaluated  Initially the models are cross-validated by using 5-fold cross-validation. With 40,000 examples, this gives 32,000 examples for training and 8,000 examples for testing. For neural network algorithms, stochastic gradient descent (SGD) is used for training. The following standard learning algorithms were ﬁrst evaluated: decision trees, SVMs with Gaussian kernel, ordinary fully-connected Multi-Layer Perceptrons, Random Forests, k-Nearest Neighbors, Convolutional Neural Networks, and Stacked Denoising Auto-Encoders with supervised ﬁne-tuning. More details of the conﬁgurations and hyper-parameters for each of them are given in Appendix Section 5.2. The only better than chance results were obtained with variations of the Structured Multi-Layer Perceptron described below.  3.2.1 Structured Multi-Layer Perceptron (SMLP)  The neural network architecture that is used to solve this task is called the SMLP (Structured Multi-Layer Perceptron), a deep neural network with two parts as illustrated in Figure 5 and 7:  The lower part, P1NN (Part 1 Neural Network, as it is called in the rest of the paper), has shared weights and local connectivity, with one identical MLP instance of the P1NN for each patch of the image, and typically an 11-element output vector per patch (unless otherwise noted). The idea is that these 11 outputs per patch could represent the detection of the sprite shape category (or the absence of sprite in the patch). The upper part, P2NN (Part 2 Neural Network) is a fully connected one hidden layer MLP that takes the concatenation of the outputs of all patch-wise P1NNs as input. Note that the ﬁrst layer of P1NN is similar to a convolutional layer but where the stride equals the kernel size, so that windows do not overlap, i.e., P1NN can be decomposed into separate networks sharing the same parameters but applied on diﬀerent patches of the input image, so that each network can actually be trained patch-wise in the case where a target is provided for the P1NN outputs. The P1NN output for patch pi which is extracted from the image x is computed as follows:  fθ(pi) = g2(V g1(U pi + b) + c)  (1)  8  where pi ∈ Rd is the input patch/receptive ﬁeld extracted from location i of a single image. U ∈ Rdh×d is the weight matrix for the ﬁrst layer of P1NN and b ∈ Rd h is the vector of biases for the ﬁrst layer of P1NN. g1(·) is the activation function of the ﬁrst layer and g2(·) is the activation function of the second layer. In many of the experiments, best results were obtained with g1(·) a rectifying non-linearity (a.k.a. as RELU), which is max(0, X) (Jarrett et al., 2009b; Nair and Hinton, 2010; Glorot et al., 2011a; Krizhevsky et al., 2012). V ∈ Rdh×do is the second layer’s weights matrix, such that and c ∈ Rdo are the biases of the second layer of the P1NN, with do expected to be smaller than dh.  In this way, g1(U pi + b) is an overcomplete representation of the input patch that can potentially represent all the possible Pentomino shapes for all factors of variations in the patch (rotation, scaling and Pentomino shape type). On the other hand, when trained with hints, fθ(pi) is expected to be the lower dimensional representation of a Pentomino shape category invariant to scaling and rotation in the given patch. In the experiments with SMLP trained with hints (targets at the output of P1NN), the P1NN is expected to perform classiﬁcation of each 8×8 non-overlapping patches of the original 64×64 input image without having any prior knowledge of whether that speciﬁc patch contains a Pentomino shape or not. P1NN in SMLP without hints just outputs the local activations for each patch, and gradients on fθ(pi) are backpropagated from the upper layers. In both cases P1NN produces the input representation for the Part 2 Neural Net (P2NN). Thus the input representation of P2NN is the concatenated output of P1NN across all the 64 patch locations: ho = [fθ(p0), ..., fθ(pi), ..., fθ(pN))] where N is the number of patches and the ho ∈ Rdi, di = do × N . ho is the concatenated output of the P1NN at each patch.  There is a standardization layer on top of the output of P1NN that centers the activations and performs divisive normalization by dividing by the standard deviation over a minibatch of the activations of that layer. We denote the standardization function z(·). Standardization makes use of the mean and standard deviation computed for each hidden unit such that each hidden unit of ho will have 0 activation and unit standard deviation on average over the minibatch. X is the set of pentomino images in the minibatch, where X ∈ Rdin×N is a matrix with N images. h(i) o (xj) is the vector of activations of the i-th hidden unit of hidden layer ho(xj) for the j-th example, with xj ∈ X.  (cid:88)  1 N  h(i) o (xj)  µ  =  h(i) o  (cid:115)(cid:80)N  xj∈X o (xj) − µ  j (h(i)  )2  h(i) o  σ  h(i) o  =  z(h(i)  o (xj)) =  N o (xj) − µ h(i) h(i) o max(σ , (cid:15))  h(i) o  + (cid:15)  (2)  (3)  (4)  where (cid:15) is a very small constant, that is used to prevent numerical underﬂows in the standard  deviation. P1NN is trained on each 8×8 patches extracted from the image.  ho is standardized for each training and test sample separately. Diﬀerent values of (cid:15) were  used for SMLP-hints and SMLP-nohints.  The concatenated output of P1NN is fed as an input to the P2NN. P2NN is a feedforward MLP with a sigmoid output layer using a single RELU hidden layer. The task of P2NN is to perform a nonlinear logical operation on the representation provided at the output of P1NN.  9  3.2.2 Structured Multi Layer Perceptron Trained with Hints (SMLP-hints)  The SMLP-hints architecture exploits a hint about the presence and category of Pentomino objects, specifying a semantics for the P1NN outputs. P1NN is trained with the intermediate target Y , specifying the type of Pentomino sprite shape present (if any) at each of the 64 patches (8×8 non-overlapping blocks) of the image. Because a possible answer at a given location can be “none of the object types” i.e., an empty patch, yp (for patch p) can take one of the 11 possible values, 1 for rejection and the rest is for the Pentomino shape classes, illustrated in Figure 2:  (cid:40)  yp =  if patch p is empty  0 s ∈ S if the patch p contains a Pentomino sprite .  A similar task has been studied by Fleuret et al. (2011) (at SI appendix Problem 17), who  compared the performance of humans vs computers.  The SMLP-hints architecture takes advantage of dividing the task into two subtasks during training with prior information about intermediate-level relevant factors. Because the sum of the training losses decomposes into the loss on each patch, the P1NN can be pre-trained patch- wise. Each patch-speciﬁc component of the P1NN is a fully connected MLP with 8×8 inputs and 11 outputs with a softmax output layer. SMLP-hints uses the the standardization given in Equation 3 but with (cid:15) = 0.  The standardization is a crucial step for training the SMLP on the Pentomino dataset, and yields much sparser outputs, as seen on Figures 3 and 4. If the standardization is not used, even SMLP-hints could not solve the Pentomino task. In general, the standardization step dampens the small activations and augments larger ones(reducing the noise). Centering the activations of each feature detector in a neural network has been studied in (Raiko et al., 2012) and (Vatanen et al., 2013). They proposed that transforming the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average makes ﬁrst order optimization methods closer to the second order techniques.  By default, the SMLP uses rectiﬁer hidden units as activation function, we found a sig- niﬁcant boost by using rectiﬁcation compared to hyperbolic tangent and sigmoid activation functions. The P1NN has a highly overcomplete architecture with 1024 hidden units per patch, and L1 and L2 weight decay regularization coeﬃcients on the weights (not the biases) are re- spectively 1e-6 and 1e-5. The learning rate for the P1NN is 0.75. 1 training epoch was enough for the P1NN to learn the features of Pentomino shapes perfectly on the 40000 training ex- amples. The P2NN has 2048 hidden units. L1 and L2 penalty coeﬃcients for the P2NN are 1e-6, and the learning rate is 0.1. These were selected by trial and error based on validation set error. Both P1NN (for each patch) and P2NN are fully-connected neural networks, even though P1NN globally is a special kind of convolutional neural network.  Filters of the ﬁrst layer of SMLP are shown in Figure 6. These are the examples of the ﬁlters obtained with the SLMP-hints trained with 40k examples, whose results are given in Table 1. Those ﬁlters look very noisy but they work perfectly on the Pentomino task.  3.2.3 Deep and Structured Supervised MLP without Hints (SMLP-nohints)  SMLP-nohints uses the same connectivity pattern (and deep architecture) that is also used in the SMLP-hints architecture, but without using the intermediate targets (Y ). It directly predicts the ﬁnal outcome of the task (Z), using the same number of hidden units, the same  10  Figure 3: Bar chart of concatenated softmax output activations ho of P1NN (11×64=704 out- puts) in SMLP-hints before standardization, for a selected example. There are very large spikes at each location for one of the possible 11 outcome (1 of K representa- tion).  11  Figure 4: Softmax output activations ho of P1NN at SMLP-hints before standardization. There are positive spiked outputs at the locations where there is a Pentomino shape. Posi- tive and negative spikes arise because most of the outputs are near an average value. Activations are higher at the locations where there is a pentomino shape.  12  Figure 5: Structured MLP architecture, used with hints (trained in two phases, ﬁrst P1NN, bottom two layers, then P2NN, top two layers). In SMLP-hints, P1NN is trained on each 8x8 patch extracted from the image and the softmax output probabilities of all 64 patches are concatenated into a 64×11 vector that forms the input of P2NN. Only U and V are learned in the P1NN and its output on each patch is fed into P2NN. The ﬁrst level and the second level neural networks are trained separately, not jointly.  Figure 6: Filters of Structured MLP architecture, trained with hints on 40k examples.  13  Structured MLP Architecture with HintsFinal Binary task labelsIntermediate level targets.Second Level Neural NetworkFirst Level Neural Networkconnectivity and the same activation function for the hidden units as SMLP-hints. 120 hy- perparameter values have been evaluated by randomly selecting the number of hidden units from [64, 128, 256, 512, 1024, 1200, 2048] and randomly sampling 20 learning rates uniformly in the log-domain within the interval of [0.008, 0.8]. Two fully connected hidden layers with 1024 hidden units (same as P1NN) per patch is used and 2048 (same as P2NN) for the last hid- den layer, with twenty training epochs. For this network the best results are obtained with a learning rate of 0.05.7  Figure 7: Structured MLP architecture, used without hints (SMLP-nohints). It is the same architecture as SMLP-hints (Figure 5) but with both parts (P1NN and P2NN) trained jointly with respect to the ﬁnal binary classiﬁcation task.  We chose to experiment with various SMLP-nohint architectures and optimization proce- dures, trying unsuccessfully to achieve as good results with SMLP-nohint as with SMLP-hints.  Rectiﬁer Non-Linearity A rectiﬁer nonlinearity is used for the activations of MLP hidden layers. We observed that using piecewise linear nonlinearity activation function such as the rectiﬁer can make the optimization more tractable.  7. The source code of the structured MLP is available at the github repository: https://github.com/caglar/  structured_mlp  14  Structured MLP Architecture without HintsFinal Binary task labelsSecond Level Neural NetworkFirst Level Neural NetworkFigure 8: First layer ﬁlters learned by the Structured MLP architecture, trained without us- ing hints on 447600 examples with online SGD and a sigmoid intermediate layer activation.  Intermediate Layer The output of the P1NN is considered as an intermediate layer of the SMLP. For the SMLP-hints, only softmax output activations have been tried at the intermediate layer, and that suﬃced to learn the task. Since things did not work nearly as well with the SMLP-nohints, several diﬀerent activation functions have been tried: softmax(·), tanh(·), sigmoid(·) and linear activation functions.  Standardization Layer Normalization at the last layer of the convolutional neural networks has been used occasionaly to encourage the competition between the hidden units. (Jarrett et al., 2009a) used a local contrast normalization layer in their architecture which performs subtractive and divisive normalization. A local contrast normalization layer enforces a local competition between adjacent features in the feature map and between features at the same spatial location in diﬀerent feature maps. Similarly (Krizhevsky et al., 2012) observed that using a local response layer that enjoys the beneﬁt of using local normalization scheme aids generalization.  Standardization has been observed to be crucial for both SMLP trained with or with- out hints. In both SMLP-hints and SMLP-nohints experiments, the neural network was not able to generalize or even learn the training set without using standardization in the SMLP intermediate layer, doing just chance performance. More speciﬁcally, in the SMLP-nohints ar- chitecture, standardization is part of the computational graph, hence the gradients are being backpropagated through it. The mean and the standard deviation is computed for each hidden unit separately at the intermediate layer as in Equation 4. But in order to prevent numerical underﬂows or overﬂows during the backpropagation we have used (cid:15) = 1e − 8 (Equation 3).  The beneﬁt of having sparse activations may be speciﬁcally important for the ill-conditioned problems, for the following reasons. When a hidden unit is “oﬀ”, its gradient (the derivative of the loss with respect to its output) is usually close to 0 as well, as seen here. That means that all oﬀ-diagonal second derivatives involving that hidden unit (e.g. its input weights) are also near 0. This is basically like removing some columns and rows from the Hessian matrix associated with a particular example. It has been observed that the condition number of the Hessian matrix (speciﬁcally, its largest eigenvalue) increases as the size of the network increases (Dauphin and Bengio, 2013), making training considerably slower and ineﬃcient (Dauphin and Bengio, 2013). Hence one would expect that as sparsity of the gradients (obtained because of sparsity of the activations) increases, training would become more eﬃcient, as if we were training a smaller sub-network for each example, with shared weights across examples, as in dropouts (Hinton et al., 2012).  In Figure 9, the activation of each hidden unit in a bar chart is shown: the eﬀect of  standardization is signiﬁcant, making the activations sparser.  15  (a) Before standardization.  (b) After standardization.  Figure 9: Activations of the intermediate-level hidden units of an SLMP-nohints for a particular examples (x-axis: hidden unit number, y-axis: activation value). Left (a): before standardization. Right (b): after standardization.  In Figure 10, one can see the activation histogram of the SMLP-nohints intermediate layer, showing the distribution of activation values, before and after standardization. Again the sparsifying eﬀect of standardization is very apparent.  (a) Before standardization.  (b) After standardization.  Figure 10: Distribution histogram of activation values of SMLP-nohints intermediate layer.  Left (a): before standardization. Right (b): after standardization.  In Figures 10 and 9, the intermediate level activations of SMLP-nohints are shown before and after standardization. These are for the same SMLP-nohints architecture whose results are presented on Table 1. For that same SMLP, the Adadelta (Zeiler, 2012) adaptive learning  16  rate scheme has been used, with 512 hidden units for the hidden layer of P1NN and rectiﬁer activation function. For the output of the P1NN, 11 sigmoidal units have been used while P2NN had 1200 hidden units with rectiﬁer activation function. The output nonlinearity of the P2NN is a sigmoid and the training objective is the binary crossentropy.  Adaptive Learning Rates We have experimented with several diﬀerent adaptive learning rate algorithms. We tried rmsprop 8, Adadelta (Zeiler, 2012), Adagrad (Duchi et al., 2010) and a linearly (1/t) decaying learning rate (Bengio, 2013b). For the SMLP-nohints with sigmoid activation function we have found Adadelta(Zeiler, 2012) converging faster to an eﬀective local minima and usually yielding better generalization error compared to the others.  3.2.4 Deep and Structured MLP with Unsupervised Pre-Training  Several experiments have been conducted using an architecture similar to the SMLP-nohints, but by using unsupervised pre-training of P1NN, with Denoising Auto-Encoder (DAE) and/or Contractive Auto-Encoders (CAE). Supervised ﬁne-tuning proceeds as in the deep and struc- tured MLP without hints. Because an unsupervised learner may not focus the representation just on the shapes, a larger number of intermediate-level units at the output of P1NN has been explored: previous work on unsupervised pre-training generally found that larger hidden layers were optimal when using unsupervised pre-training, because not all unsupervised features will be relevant to the task at hand. Instead of limiting to 11 units per patch, we experimented with networks with up to 20 hidden (i.e., code) units per patch in the second-layer patch-wise auto-encoder.  In Appendix 5.1 we also provided the result of some experiments with binary-binary RBMs  trained on 8×8 patches from the 40k training dataset.  In unsupervised pretraining experiments in this paper, both contractive auto-encoder (CAE)  with sigmoid nonlinearity and binary cross entropy cost function and denoising auto-encoder (DAE) have been used. In the second layer, experiments were performed with a DAE with rectiﬁer hidden units utilizing L1 sparsity and weight decay on the weights of the auto-encoder. Greedy layerwise unsupervised training procedure is used to train the deep auto-encoder architecture (Bengio et al., 2007). In unsupervised pretraining experiments, tied weights have been used. Diﬀerent combinations of CAE and DAE for unsupervised pretraining have been tested, but none of the conﬁgurations tested managed to learn the Pentomino task, as shown in Table 1.  3.3 Experiments with 1 of K representation  To explore the eﬀect of changing the complexity of the input representation on the diﬃculty of the task, a set of experiments have been designed with symbolic representations of the information in each patch. In all cases an empty patch is represented with a 0 vector. These representation can be seen as an alternative input for a P2NN-like network, i.e., they were fed as input to an MLP or another black-box classiﬁer.  The following four experiments have been conducted, each one using one using a diﬀerent  input representation for each patch:  8. This is learning rate scaling method that is discussed by G. Hinton in his Video Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  17  Algorithm  20k dataset  40k dataset  80k dataset  Training  Error  SVM RBF K Nearest Neighbors Decision Tree Randomized Trees MLP Convnet/Lenet5 Maxout Convnet 2 layer sDA Struct. Supervised MLP w/o hints Struct. MLP+CAE Supervised Finetuning Struct. MLP+CAE+DAE, Supervised Finetuning Struct. MLP+DAE+DAE, Supervised Finetuning  26.2 24.7 5.8 3.2 26.5 50.6 14.5 49.4 0.0 50.5 49.1 49.5  Test Error 50.2 50.0 48.6 49.8 49.3 49.8 49.5 50.3 48.6 49.7 49.7 50.3  Training  Error  28.2 25.3 6.3 3.4 33.2 49.4 0.0 50.2 0.0 49.8 49.4 49.7  Struct. MLP with Hints  0.21  30.7  0  Test Error 50.2 49.5 49.4 50.5 49.9 49.8 50.1 50.3 36.0 49.7 49.7 49.8  3.1  Training  Error  30.2 25.6 6.9 3.5 27.2 50.2 0.0 49.7 0.0 50.3 50.1 50.3  0  Test Error 49.6 49.0 49.9 49.1 50.1 49.8 44.6 50.3 12.4 49.7 49.7 49.7  0.01  Table 1: The error percentages with diﬀerent learning algorithms on Pentomino dataset with  diﬀerent number of training examples.  Experiment 1-Onehot representation without transformations: In this experiment sev-  eral trials have been done with a 10-input one-hot vector per patch. Each input corre- sponds to an object category given in clear, i.e., the ideal input for P2NN if a supervised P1NN perfectly did its job.  Experiment 2-Disentangled representations: In this experiment, we did trials with 16 binary inputs per patch, 10 one-hot bits for representing each object category, 4 for rotations and 2 for scaling, i.e., the whole information about the input is given, but it is perfectly disentangled. This would be the ideal input for P2NN if an unsupervised P1NN perfectly did its job.  Experiment 3-Onehot representation with transformations: For each of the ten object types there are 8 = 4×2 possible transformations. Two objects in two diﬀerent patches are the considered “the same” (for the ﬁnal task) if their category is the same regardless of the transformations. The one-hot representation of a patch corresponds to the cross- product between the 10 object shape classes and the 4×2 transformations, i.e., one out of 80=10×4 × 2 possibilities represented in an 80-bit one-hot vector. This also contains all the information about the input image patch, but spread out in a kind of non-parametric and non-informative (not disentangled) way, like a perfect memory-based unsupervised learner (like clustering) could produce. Nevertheless, the shape class would be easier to read out from this representation than from the image representation (it would be an OR over 8 of the bits).  Experiment 4-Onehot representation with 80 choices: This representation has the same 1 of 80 one-hot representation per patch but the target task is deﬁned diﬀerently. Two ob- jects in two diﬀerent patches are considered the same iﬀ they have exactly the same 80-bit onehot representation (i.e., are of the same object category with the same transformation applied).  The ﬁrst experiment is a sanity check. It was conducted with single hidden-layered MLP’s with rectiﬁer and tanh nonlinearity, and the task was learned perfectly (0 error on both training and test dataset) with very few training epochs.  18  (a) Training and Test Errors for Experiment 4  (b) Training and Test Errors for Experiment 3  Figure 11: Tanh MLP training curves. Left (a): The training and test errors of Experiment 3 over 800 training epochs with 100k training examples using Tanh MLP. Right (b):The training and test errors of Experiment 4 over 700 training epochs with 100k training examples using Tanh MLP.  The results of Experiment 2 are given in Table 2. To improve results, we experimented with the Maxout non-linearity in a feedforward MLP (Goodfellow et al., 2013) with two hidden layers. Unlike the typical Maxout network mentioned in the original paper, regularizers have been deliberately avoided in order to focus on the optimization issue, i.e: no weight decay, norm constraint on the weights, or dropout. Although learning from a disentangled representation is more diﬃcult than learning from perfect object detectors, it is feasible with some architectures such as the Maxout network. Note that this representation is the kind of representation that one could hope an unsupervised learning algorithm could discover, at best, as argued in Bengio et al. (2012).  The only results obtained on the validation set for Experiment 3 and Experiment 4 are shown respectively in Table 3 and Table 4. In these experiments a tanh MLP with two hidden layers have been tested with the same hyperparameters. In experiment 3 the complexity of the problem comes from the transformations (8=4×2) and the number of object types.  But in experiment 4, the only source of complexity of the task comes from the number of diﬀerent object types. These results are in between the complete failure and complete success observed with other experiments, suggesting that the task could become solvable with better training or more training examples. Figure 11 illustrates the progress of training a tanh MLP, on both the training and test error, for Experiments 3 and 4. Clearly, something has been learned, but the task is not nailed yet. On experiment 3 for both maxout and tanh the maxout there was a long plateau where the training error and objective stays almost same. Maxout did just chance on the experiment for about 120 iterations on the training and the test set. But after 120th iteration the training and test error started decline and eventually it was able to solve the task. Moreover as seen from the curves in Figure 11(a) and 11(b), the training and test error curves are almost the same for both tasks. This implies that for onehot inputs, whether you increase the number of possible transformations for each object or the number of  19  01002003004005006007008009000.00.10.20.30.40.50.6Training Error RateTest Error Rate01002003004005006007008000.00.10.20.30.40.50.6Training Error RateTest Error RateLearning Algorithm Training Error Test Error SVM Random Forests Tanh MLP Maxout MLP  35.6 40.475 0.0 0.0  0.0 1.29 0.0 0.0  Table 2: Performance of diﬀerent learning algorithms on disentangled representation in Exper-  iment 2.  Learning Algorithm Training Error Test Error SVM Random Forests Tanh MLP Maxout MLP  11.212 24.839 0.0 0.0  32.37 48.915 22.475 0.0  Table 3: Performance of diﬀerent learning algorithms using a dataset with onehot vector and  80 inputs as discussed for Experiment 3.  object categories, as soon as the number of possible conﬁgurations is same, the complexity of the problem is almost the same for the MLP.  3.4 Does the Eﬀect Persist with Larger Training Set Sizes?  The results shown in this section indicate that the problem in the Pentomino task clearly is not just a regularization problem, but rather basically hinges on an optimization problem. Otherwise, we would expect test error to decrease as the number of training examples increases. This is shown ﬁrst by studying the online case and then by studying the ordinary training case with a ﬁxed size training set but considering increasing training set sizes. In the online minibatch setting, parameter updates are performed as follows:  θt+1 = θt − ∆θt (cid:80)N i ∇θtL(xt, θt)  (5)  ∆θt = (cid:15)  is the learning rate.  (6) where L(xt, θt) is the loss incurred on example xt with parameters θt , where t ∈ Z + and (cid:15) Ordinary batch algorithms converge linearly to the optimum θ∗, however the noisy gradient estimates in the online SGD will cause parameter θ to ﬂuctuate near the local optima. However, online SGD directly optimizes the expected risk, because the examples are drawn iid from the ground-truth distribution (Bottou, 2010). Thus:  N  L∞ = E[L(x, θ)] =  L(x, θ)p(x)dx  (7)  (cid:90)  x  20  Learning Algorithm Training Error Test Error SVM Random Forests Tanh MLP  4.346 23.456 0  40.545 47.345 25.8  Table 4: Performance of diﬀerent algorithms using a dataset with onehot vector and 80 binary  inputs as discussed in Experiment 4.  where L∞ is the generalization error. Therefore online SGD is trying to minimize the  expected risk with noisy updates. Those noisy updates have the eﬀect of regularizer:  (cid:80)N i ∇θtL(xt, θt)  N  ∆θt = (cid:15)  = (cid:15)∇θtL(x, θt) + (cid:15)ξt  (8)  where ∇θtL(x, θt) is the true gradient and ξt is the zero-mean stochastic gradient “noise”  due to computing the gradient over a ﬁnite-size minibatch sample.  We would like to know if the problem with the Pentomino dataset is more a regularization or an optimization problem. An SMLP-nohints model was trained by online SGD with the ran- domly generated online Pentomino stream. The learning rate was adaptive, with the Adadelta procedure (Zeiler, 2012) on minibatches of 100 examples. In the online SGD experiments, two SMLP-nohints that is trained with and without standardization at the intermediate layer with exactly the same hyperparameters are tested. The SMLP-nohints P1NN patch-wise submodel has 2048 hidden units and the SMLP intermediate layer has 1152 = 64 × 18 hidden units. The nonlinearity that is used for the intermediate layer is the sigmoid. P2NN has 2048 hidden units.  SMLP-nohints has been trained either with or without standardization on top of the output units of the P1NN. The experiments illustrated in Figures 12 and 13 are with the same SMLP without hints architecture for which results are given in Table 1. In those graphs only the results for the training on the randomly generated 545400 Pentomino samples have been presented. As shown in the plots SMLP-nohints was not able to generalize without standardization. Although without standardization the training loss seems to decrease initially, it eventually gets stuck in a plateau where training loss doesn’t change much.  Training of SMLP-nohints online minibatch SGD is performed using standardization in the intermediate layer and Adadelta learning rate adaptation, on 1046000 training examples from the randomly generated Pentomino stream. At the end of the training, test error is down to 27.5%, which is much better than chance but from from the score obtained with SMLP-hints of near 0 error.  In another SMLP-nohints experiment without standardization the model is trained with the 1580000 Pentomino examples using online minibatch SGD. P1NN has 2048 hidden units and 16 sigmoidal outputs per patch. for the P1NN hidden layer. P2NN has 1024 hidden units for the hidden layer. Adadelta is used to adapt the learning rate. At the end of training this SMLP, the test error remained stuck, at 50.1%.  21  Figure 12: Test errors of SMLP-nohints with and without standardization in the intermediate layer. Sigmoid as an intermediate layer activation has been used. Each tick (batch no) in the x-axis represents 400 examples.  22  0100200300400500600700800Batch no0.200.250.300.350.400.450.500.55Test ErrorSMLP with standardizationSMLP without standardizationFigure 13: Training errors of SMLP-nohints with and without standardization in the interme- diate layer. Sigmoid nonlinearity has been used as an intermediate layer activation function. The x-axis is in units of blocks of 400 examples in the training set.  23  0100200300400500600700Batch no01234567Training LossSMLP with standardizationSMLP without standardization3.4.1 Experiments with Increased Training Set Size  Here we consider the eﬀect of training diﬀerent learners with diﬀerent numbers of training examples. For the experimental results shown in Table 1, 3 training set sizes (20k, 40k and 80k examples) had been used. Each dataset was generated with diﬀerent random seeds (so they do not overlap). Figure 14 also shows the error bars for an ordinary MLP with three hidden layers, for a larger range of training set sizes, between 40k and 320k examples. The number of training epochs is 8 (more did not help), and there are three hidden layers with 2048 feature detectors. The learning rate we used in our experiments is 0.01. The activation function of the MLP is a tanh nonlinearity, while the L1, L2 penalty coeﬃcients are both 1e-6.  Table 1 shows that, without guiding hints, none of the state-of-art learning algorithms could perform noticeably better than a random predictor on the test set. This shows the importance of intermediate hints introduced in the SMLP. The decision trees and SVMs can overﬁt the training set but they could not generalize on the test set. Note that the numbers reported in the table are for hyper-parameters selected based on validation set error, hence lower training errors are possible if avoiding all regularization and taking large enough models. On the training set, the MLP with two large hidden layers (several thousands) could reach nearly 0% training error, but still did not manage to achieve good test error.  In the experiment results shown in Figure 14, we evaluate the impact of adding more training data for the fully-connected MLP. As mentioned before for these experiments we have used a MLP with three hidden layers where each layer has 2048 hidden units. The tanh(·) activation function is used with 0.05 learning rate and minibatches of size 200.  As can be seen from the ﬁgure, adding more training examples did not help either training or test error (both are near 50%, with training error slightly lower and test error slightly higher), reinforcing the hypothesis that the diﬃcult encountered is one of optimization, not of regularization.  Figure 14: Training and test error bar charts for a regular MLP with 3 hidden layers. There is no signiﬁcant improvement on the generalization error of the MLP as the new training examples are introduced.  24  3.5 Experiments on Eﬀect of Initializing with Hints  Initialization of the parameters in a neural network can have a big impact on the learning and generalization (Glorot and Bengio, 2010). Previously Erhan et al. (2010) showed that initializing the parameters of a neural network with unsupervised pretraining guides the learn- ing towards basins of attraction of local minima that provides better generalization from the training dataset. In this section we analyze the eﬀect of initializing the SMLP with hints and then continuing without hints at the rest of the training. For experimental analysis of hints based initialization, SMLP is trained for 1 training epoch using the hints and for 60 epochs it is trained without hints on the 40k examples training set. We also compared the same archi- tecture with the same hyperparameters, against to SMLP-nohints trained for 61 iterations on the same dataset. After one iteration of hint-based training SMLP obtained 9% training error and 39% test error. Following the hint based training, SMLP is trained without hints for 60 epochs, but at epoch 18, it already got 0% training and 0% test error. The hyperparameters for this experiment and the experiment that the results shown for the SMLP-hints in Table 1 are the same. The test results for initialization with and without hints are shown on Figure 15. This ﬁgure suggests that initializing with hints can give the same generalization performance but training takes longer.  Figure 15: Plots showing the test error of SMLP with random initialization vs initializing with  hint based training.  3.5.1 Further Experiments on Optimization for Pentomino Dataset  With extensive hyperparameter optimization and using standardization in the intermediate level of the SMLP with softmax nonlinearity, SMLP-nohints was able to get 5.3% training and  25  6.7% test error on the 80k Pentomino training dataset. We used the 2050 hidden units for the hidden layer of P1NN and 11 softmax output per patch. For the P2NN, we used 1024 hidden units with sigmoid and learning rate 0.1 without using any adaptive learning rate method. This SMLP uses a rectiﬁer nonlinearity for hidden layers of both P1NN and P2NN. Considering that architecture uses softmax as the intermediate activation function of SMLP-nohints. It is very likely that P1NN is trying to learn the presence of speciﬁc Pentomino shape in a given patch. This architecture has a very large capacity in the P1NN, that probably provides it enough capacity to learn the presence of Pentomino shapes at each patch eﬀortlessly.  An MLP with 2 hidden layers, each 1024 rectiﬁer units, was trained using LBFGS (the implementation from the scipy.optimize library) on 40k training examples, with gradients com- puted on batches of 10000 examples at each iteration. However, after convergence of training, the MLP was still doing chance on the test dataset.  We also observed that using linear units for the intermediate layer yields better general- ization error without standardization compared to using activation functions such as sigmoid, tanh and RELU for the intermediate layer. SMLP-nohints was able to get 25% generaliza- tion error with linear units without standardization whereas all the other activation functions that has been tested failed to generalize with the same number of training iterations without standardization and hints. This suggests that using non-linear intermediate-level activation functions without standardization introduces an optimization diﬃculty for the SMLP-nohints, maybe because the intermediate level acts like a bottleneck in this architecture.  4. Conclusion and Discussion  In this paper we have shown an example of task which seems almost impossible to solve by standard black-box machine learning algorithms, but can be almost perfectly solved when one encourages a semantics for the intermediate-level representation that is guided by prior knowledge. The task has the particularity that it is deﬁned by the composition of two non- linear sub-tasks (object detection on one hand, and a non-linear logical operation similar to XOR on the other hand).  What is interesting is that in the case of the neural network, we can compare two networks with exactly the same architecture but a diﬀerent pre-training, one of which uses the known intermediate concepts to teach an intermediate representation to the network. With enough capacity and training time they can overﬁt but did not not capture the essence of the task, as seen by test set performance.  We know that a structured deep network can learn the task, if it is initialized in the right place, and do it from very few training examples. Furthermore we have shown that if one pre-trains SMLP with hints for only one epoch, it can nail the task. But the exactly same architecture which started training from random initialization, failed to generalize.  Consider the fact that even SMLP-nohints with standardization after being trained using online SGD on 1046000 generated examples and still gets 27.5% test error. This is an indication that the problem is not a regularization problem but possibly an inability to ﬁnd a good eﬀective local minima of generalization error.  What we hypothesize is that for most initializations and architectures (in particular the fully-connected ones), although it is possible to ﬁnd a good eﬀective local minimum of training error when enough capacity is provided, it is diﬃcult (without the proper initialization) to ﬁnd a good local minimum of generalization error. On the other hand, when the network architecture  26  is constrained enough but still allows it to represent a good solution (such as the structured MLP of our experiments), it seems that the optimization problem can still be diﬃcult and even training error remains stuck high if the standardization isn’t used. Standardization obviously makes the training objective of the SMLP easier to optimize and helps it to ﬁnd at least a better eﬀective local minimum of training error. This ﬁnding suggests that by using speciﬁc architectural constraints and sometimes domain speciﬁc knowledge about the problem, one can alleviate the optimization diﬃculty that generic neural network architectures face.  It could be that the combination of the network architecture and training procedure pro- duces a training dynamics that tends to yield into these minima that are poor from the point of view of generalization error, even when they manage to nail training error by providing enough capacity. Of course, as the number of examples increases, we would expect this discrepancy to decrease, but then the optimization problem could still make the task unfeasible in practice. Note however that our preliminary experiments with increasing the training set size (8-fold) for MLPs did not reveal signs of potential improvements in test error yet, as shown in Figure 14. Even using online training on 545400 Pentomino examples, the SMLP-nohints architecture was still doing far from perfect in terms of generalization error (Figure 12).  These ﬁndings bring supporting evidence to the “Guided Learning Hypothesis” and “Deeper Harder Hypothesis” from Bengio (2013a): higher level abstractions, which are expressed by composing simpler concepts, are more diﬃcult to learn (with the learner often getting in an eﬀective local minimum ), but that diﬃculty can be overcome if another agent provides hints of the importance of learning other, intermediate-level abstractions which are relevant to the task.  Many interesting questions remain open. Would a network without any guiding hint even- tually ﬁnd the solution with a enough training time and/or with alternate parametrizations? To what extent is ill-conditioning a core issue? The results with LBFGS were disappointing but changes in the architectures (such as standardization of the intermediate level) seem to make training much easier. Clearly, one can reach good solutions from an appropriate initialization, pointing in the direction of an issue with local minima, but it may be that good solutions are also reachable from other initializations, albeit going through a tortuous ill-conditioned path in parameter space. Why did our attempts at learning the intermediate concepts in an unsu- pervised way fail? Are these results speciﬁc to the task we are testing or a limitation of the unsupervised feature learning algorithm tested? Trying with many more unsupervised vari- ants and exploring explanatory hypotheses for the observed failures could help us answer that. Finally, and most ambitious, can we solve these kinds of problems if we allow a community of learners to collaborate and collectively discover and combine partial solutions in order to obtain solutions to more abstract tasks like the one presented here? Indeed, we would like to discover learning algorithms that can solve such tasks without the use of prior knowledge as speciﬁc and strong as the one used in the SMLP here. These experiments could be inspired by and inform us about potential mechanisms for collective learning through cultural evolutions in human societies.  Acknowledgments  We would like to thank to the ICLR 2013 reviewers for their insightful comments, and NSERC, CIFAR, Compute Canada and Canada Research Chairs for funding.  27  References  A. Ben-Hur and J. Weston. A user’s guide to support vector machines. Methods in Molecular  Biology, 609:223–239, 2010.  Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep  networks. In NIPS’2006, 2007.  Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine  Learning, 2(1):1–127, 2009. Also published as a book. Now Publishers, 2009.  Yoshua Bengio. Evolving culture vs local minima. In Growing Adaptive Machines: Integrating Development and Learning in Artiﬁcial Neural Networks, number also as ArXiv 1203.2990v1, pages T. Kowaliw, N. Bredeche & R. Doursat, eds. Springer-Verlag, March 2013a. URL http://arxiv.org/abs/1203.2990.  Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In K.-R. M¨uller, G. Montavon, and G. B. Orr, editors, Neural Networks: Tricks of the Trade. Springer, 2013b.  Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In L´eon Bottou and Michael Littman, editors, Proceedings of the Twenty-sixth International Conference on Machine Learning (ICML’09). ACM, 2009a.  Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.  In ICML’09, 2009b.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Unsupervised feature learning and deep learning: A review and new perspectives. Technical Report arXiv:1206.5538, U. Montreal, 2012. URL http://arxiv.org/abs/1206.5538.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Unsupervised feature learning and deep learning: A review and new perspectives. IEEE Trans. Pattern Analysis and Machine Intel- ligence (PAMI), 2013.  James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guil- laume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy), 2010.  L´eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of  COMPSTAT’2010, pages 177–186. Springer, 2010.  Leo Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.  D. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber. Deep big simple neural nets  for handwritten digit recognition. Neural Computation, 22:1–14, 2010.  Yann Dauphin and Yoshua Bengio. Big neural networks waste capacity. Technical Report  arXiv:1301.3583, Universite de Montreal, 2013.  Richard Dawkins. The Selﬁsh Gene. Oxford University Press, 1976.  28  J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2010.  Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, Journal of  and Samy Bengio. Why does unsupervised pre-training help deep learning? Machine Learning Research, 11:625–660, February 2010.  Fran¸cois Fleuret, Ting Li, Charles Dubout, Emma K Wampler, Steven Yantis, and Donald Geman. Comparing machines and humans on a visual categorization test. Proceedings of the National Academy of Sciences, 108(43):17621–17625, 2011.  X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer neural networks.  In AISTATS,  2011a.  Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training deep feedforward neural networks. In JMLR W&CP: Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2010), volume 9, pages 249–256, May 2010.  Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In JMLR W&CP: Proceedings of the Fourteenth International Conference on Artiﬁcial Intelli- gence and Statistics (AISTATS 2011), April 2011b.  Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.  Maxout networks. In ICML, 2013.  Maciej Henneberg. Decrease of human skull size in the holocene. Human biology, pages 395–405,  1988.  Maciej Henneberg and Maryna Steyn. Trends in cranial capacity and cranial index in subsa-  haran africa during the holocene. American journal of human biology, 5(4):473–479, 1993.  J. Henrich and R. McElreath. The evolution of cultural evolution. Evolutionary Anthropology:  Issues, News, and Reviews, 12(3):123–135, 2003.  Geoﬀrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep  belief nets. Neural Computation, 18:1527–1554, 2006.  Geoﬀrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Improving neural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580, 2012.  C.W. Hsu, C.C. Chang, C.J. Lin, et al. A practical guide to support vector classiﬁcation, 2003.  Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pages 2146–2153. IEEE, 2009a.  Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. What is the best  multi-stage architecture for object recognition? In ICCV’09, 2009b.  29  Faisal Khan, Xiaojin Zhu, and Bilge Mutlu. How do humans teach: On curriculum learning and teaching dimension. In Advances in Neural Information Processing Systems 24 (NIPS’11), pages 1449–1457, 2011.  Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey Hinton.  ImageNet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25 (NIPS’2012). 2012.  Kai A. Krueger and Peter Dayan. Flexible shaping: how learning in small steps helps. Cognition,  110:380–394, 2009.  G. Kunapuli, K.P. Bennett, R. Maclin, and J.W. Shavlik. The adviceptron: Giving advice to the perceptron. Proceedings of the Conference on Artiﬁcial Neural Networks In Engineering (ANNIE 2010), 2010.  Hugo Larochelle, Yoshua Bengio, Jerome Louradour, and Pascal Lamblin. Exploring strategies  for training deep neural networks. Journal of Machine Learning Research, 10:1–40, 2009.  Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document  recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  T.M. Mitchell. The need for biases in learning generalizations. Department of Computer  Science, Laboratory for Computer Science Research, Rutgers Univ., 1980.  T.M. Mitchell and S.B. Thrun. Explanation-based neural network learning for robot control.  Advances in Neural information processing systems, pages 287–287, 1993.  R. Montague. Universal grammar. Theoria, 36(3):373–398, 1970.  V. Nair and G. E Hinton. Rectiﬁed linear units improve restricted Boltzmann machines. In  ICML’10, 2010.  L.B.J.H.F.R.A. Olshen and C.J. Stone. Classiﬁcation and regression trees. Belmont, Calif.:  Wadsworth, 1984.  Joseph O’Sullivan. Integrating initialization bias and search bias in neural network learning,  1996.  F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. The Journal of Machine Learning Research, 12:2825–2830, 2011.  Gail B. Peterson. A day of great illumination: B. F. Skinner’s discovery of shaping. Journal of  the Experimental Analysis of Behavior, 82(3):317–328, 2004.  Tapani Raiko, Harri Valpola, and Yann LeCun. Deep learning made easier by linear transfor- mations in perceptrons. In International Conference on Artiﬁcial Intelligence and Statistics, pages 924–932, 2012.  Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive  auto-encoders: Explicit invariance during feature extraction. In ICML’2011, 2011.  30  Salah Rifai, Yoshua Bengio, Yann Dauphin, and Pascal Vincent. A generative process for sam- pling contractive auto-encoders. In Proceedings of the Twenty-nine International Conference on Machine Learning (ICML’12). ACM, 2012. URL http://icml.cc/discuss/2012/590. html.  R. Salakhutdinov and G.E. Hinton. Deep Boltzmann machines. In Proceedings of the Twelfth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2009), volume 8, 2009.  Burrhus F. Skinner. Reinforcement today. American Psychologist, 13:94–99, 1958.  R.J. Solomonoﬀ. A system for incremental learning based on algorithmic probability. In Proceed- ings of the Sixth Israeli Conference on Artiﬁcial Intelligence, Computer Vision and Pattern Recognition, pages 515–527. Citeseer, 1989.  G.G. Towell and J.W. Shavlik. Knowledge-based artiﬁcial neural networks. Artiﬁcial intelli-  gence, 70(1):119–165, 1994.  Tommi Vatanen, Tapani Raiko, Harri Valpola, and Yann LeCun. Pushing stochastic gradient towards second-order methods–backpropagation learning with transformations in nonlinear- ities. arXiv preprint arXiv:1301.3476, 2013.  Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11:3371–3408, December 2010.  Luis Von Ahn, Manuel Blum, Nicholas J Hopper, and John Langford. Captcha: Using hard In Advances in CryptologyEUROCRYPT 2003, pages 294–311.  ai problems for security. Springer, 2003.  Jason Weston, Fr´ed´eric Ratle, and Ronan Collobert. Deep learning via semi-supervised em- bedding. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, Pro- ceedings of the Twenty-ﬁfth International Conference on Machine Learning (ICML’08), pages 1168–1175, New York, NY, USA, 2008. ACM. doi: 10.1145/1390156.1390303.  ISBN 978-1-60558-205-4.  Matthew D Zeiler.  Adadelta: An adaptive learning rate method.  arXiv preprint  arXiv:1212.5701, 2012.  5. Appendix  5.1 Binary-Binary RBMs on Pentomino Dataset We trained binary-binary RBMs (both visible and hidden are binary) on 8×8 patches extracted from the Pentomino Dataset using PCD (stochastic maximum likelihood), a weight decay of .0001 and a sparsity penalty9. We used 256 hidden units and trained by SGD with a batch size of 32 and a annealing learning rate (Bengio, 2013b) starting from 1e-3 with annealing rate  9. implemented as TorontoSparsity in pylearn2, see the yaml ﬁle in the repository for more details  31  1.000015. The RBM is trained with momentum starting from 0.5. The biases are initialized to -2 in order to get a sparse representation. The RBM is trained for 120 epochs (approximately 50 million updates).  After pretraining the RBM, its parameters are used to initialize the ﬁrst layer of an SMLP- nohints network. As in the usual architecture of the SMLP-nohints on top of P1NN, there is an intermediate layer. Both P1NN and the intermediate layer have a sigmoid nonlinearity, and the intermediate layer has 11 units per location. This SMLP-nohints is trained with Adadelta and standardization at the intermediate layer 10.  Figure 16: Training and test errors of an SMLP-nohints network whose ﬁrst layer is pre-trained as an RBM. Training error reduces to 0% at epoch 42, but test error is still chance.  5.2 Experimental Setup and Hyper-parameters  5.2.1 Decision Trees  We used the decision tree implementation in the scikit-learn (Pedregosa et al., 2011) python package which is an implementation of the CART (Regression Trees) algorithm. The CART algorithm constructs the decision tree recursively and partitions the input space such that the samples belonging to the same category are grouped together (Olshen and Stone, 1984). We used The Gini index as the impurity criteria. We evaluated the hyper-parameter conﬁgura- tions with a grid-search. We cross-validated the maximum depth (max depth) of the tree (for preventing the algorithm to severely overﬁt the training set) and minimum number of samples  10. In our auto-encoder experiments we directly fed features to P2NN without standardization and Adadelta.  32  051015202530354045Epoch0.00.10.20.30.40.5Error percentageRBM Test and Training ErrorsTraining ErrorTest ErrorFigure 17: Filters learned by the binary-binary RBM after training on the 40k examples. The  RBM did learn the edge structure of Pentomino shapes.  Figure 18: 100 samples generated from trained RBM. All the generated samples are valid Pen-  tomino shapes.  33  required to create a split (min split). 20 diﬀerent conﬁgurations of hyper-parameter values were evaluated. We obtained the best validation error with max depth = 300 and min split = 8.  5.2.2 Support Vector Machines  We used the “Support Vector Classiﬁer (SVC)” implementation from the scikit-learn package which in turn uses the libsvm’s Support Vector Machine (SVM) implementation. Kernel- based SVMs are non-parametric models that map the data into a high dimensional space and separate diﬀerent classes with hyperplane(s) such that the support vectors for each category will be separated by a large margin. We cross-validated three hyper-parameters of the model using grid-search: C, γ and the type of kernel(kernel type). C is the penalty term (weight decay) for the SVM and γ is a hyper-parameter that controls the width of the Gaussian for the RBF kernel. For the polynomial kernel, γ controls the ﬂexibility of the classiﬁer (degree of the polynomial) as the number of parameters increases (Hsu et al., 2003; Ben-Hur and Weston, 2010). We evaluated forty-two hyper-parameter conﬁgurations. That includes, two kernel types: {RBF, P olynomial}; three gammas: {1e− 2, 1e− 3, 1e− 4} for the RBF kernel, {1, 2, 5} for the polynomial kernel, and seven C values among: {0.1, 1, 2, 4, 8, 10, 16}. As a result of the grid search and cross-validation, we have obtained the best test error by using the RBF kernel, with C = 2 and γ = 1.  5.2.3 Multi Layer Perceptron  We have our own implementation of Multi Layer Perceptron based on the Theano (Bergstra et al., 2010) machine learning libraries. We have selected 2 hidden layers, the rectiﬁer activation function, and 2048 hidden units per layer. We cross-validated three hyper-parameters of the model using random-search, sampling the learning rates (cid:15) in log-domain, and selecting L1 and L2 regularization penalty coeﬃcients in sets of ﬁxed values, evaluating 64 hyperparameter values. The range of the hyperparameter values are (cid:15) ∈ [0.0001, 1], L1 ∈ {0., 1e−6, 1e−5, 1e−4} and L2 ∈ {0, 1e − 6, 1e − 5}. As a result, the following were selected: L1 = 1e − 6, L2 = 1e − 5 and (cid:15) = 0.05.  5.2.4 Random Forests  We used scikit-learn’s implementation of “Random Forests” decision tree learning. The Ran- dom Forests algorithm creates an ensemble of decision trees by randomly selecting for each tree a subset of features and applying bagging to combine the individual decision trees (Breiman, 2001). We have used grid-search and cross-validated the max depth, min split, and number of trees (n estimators). We have done the grid-search on the following hyperparameter val- ues, n estimators ∈ {5, 10, 15, 25, 50}, max depth ∈ {100, 300, 600, 900}, and min splits ∈ {1, 4, 16}. We obtained the best validation error with max depth = 300, min split = 4 and n estimators = 10.  5.2.5 k-Nearest Neighbors  We used scikit-learn’s implementation of k-Nearest Neighbors (k-NN). k-NN is an instance- based, lazy learning algorithm that selects the training examples closest in Euclidean distance to the input query. It assigns a class label to the test example based on the categories of the k closest neighbors. The hyper-parameters we have evaluated in the cross-validation are the number of neighbors (k) and weights. The weights hyper-parameter can be either “uniform” or  34  “distance”. With “uniform”, the value assigned to the query point is computed by the majority vote of the nearest neighbors. With “distance”, each value assigned to the query point is computed by weighted majority votes where the weights are computed with the inverse distance between the query point and the neighbors. We have used n neighbours ∈ {1, 2, 4, 6, 8, 12} and weights ∈ {”unif orm”, ”distance”} for hyper-parameter search. As a result of cross-validation and grid search, we obtained the best validation error with k = 2 and weights=“uniform”.  5.2.6 Convolutional Neural Nets  We used a Theano (Bergstra et al., 2010) implementation of Convolutional Neural Networks (CNN) from the deep learning tutorial at deeplearning.net, which is based on a vanilla version of a CNN LeCun et al. (1998). Our CNN has two convolutional layers. Following each convolutional layer, we have a max-pooling layer. On top of the convolution-pooling- convolution-pooling layers there is an MLP with one hidden layer. In the cross-validation we have sampled 36 learning rates in log-domain in the range [0.0001, 1] and the number of ﬁlters from the range [10, 20, 30, 40, 50, 60] uniformly. For the ﬁrst convolutional layer we used 9×9 receptive ﬁelds in order to guarantee that each object ﬁts inside the receptive ﬁeld. As a result of random hyperparameter search and doing manual hyperparameter search on the validation dataset, the following values were selected:  • The number of features used for the ﬁrst layer is 30 and the second layer is 60. • For the second convolutional layer, 7×7 receptive ﬁelds. The stride for both convolutional  layers is 1.  • Convolved images are downsampled by a factor of 2×2 at each pooling operation. • The learning rate for CNN is 0.01 and it was trained for 8 epochs.  5.2.7 Maxout Convolutional Neural Nets  We used the pylearn2 (https://github.com/lisa-lab/pylearn2) implementation of maxout convolutional networks (Goodfellow et al., 2013). There are two convolutional layers in the selected architecture, without any pooling. In the last convolutional layer, there is a maxout non-linearity. The following were selected by cross-validation: learning rate, number of channels for the both convolution layers, number of kernels for the second layer and number of units and pieces per maxout unit in the last layer, a linearly decaying learning rate, momentum starting from 0.5 and saturating to 0.8 at the 200’th epoch. Random search for the hyperparameters was used to evaluate 48 diﬀerent hyperparameter conﬁgurations on the validation dataset. For the ﬁrst convolutional layer, 8×8 kernels were selected to make sure that each Pentomino shape ﬁts into the kernel. Early stopping was used and test error on the model that has the best validation error is reported. Using norm constraint on the fan-in of the ﬁnal softmax units yields slightly better result on the validation dataset.  As a result of cross-validation and manually tuning the hyperparameters we used the fol-  lowing hyperparameters:  • 16 channels per convolutional layer. 600 hidden units for the maxout layer. • 6x6 kernels for the second convolutional layer.  35  • 5 pieces for the convolution layers and 4 pieces for the maxout layer per maxout units. • We decayed the learning rate by the factor of 0.001 and the initial learning rate is 0.026367. But we scaled the learning rate of the second convolutional layer by a constant factor of 0.6.  • The norm constraint (on the incoming weights of each unit) is 1.9365.  Figure 19 shows the ﬁrst layer ﬁlters of the maxout convolutional net, after being trained  on the 80k training set for 85 epochs.  Figure 19: Maxout convolutional net ﬁrst layer ﬁlters. Most of the ﬁlters were able to learn  the basic edge structure of the Pentomino shapes.  5.2.8 Stacked Denoising Auto-Encoders  Denoising Auto-Encoders (DAE) are a form of regularized auto-encoder (Bengio et al., 2013). The DAE forces the hidden layer to discover more robust features and prevents it from simply learning the identity by reconstructing the input from a corrupted version of it (Vincent et al., 2010). Two DAEs were stacked, resulting in an unsupervised transformation with two hidden layers of 1024 units each. Parameters of all layers are then ﬁne-tuned with supervised ﬁne- tuning using logistic regression as the classiﬁer and SGD as the gradient-based optimization algorithm. The stochastic corruption process is binomial (0 or 1 replacing each input value, with probability 0.2). The selected learning rate is (cid:15)0 = 0.01 for the DAe and (cid:15)1 = 0.1 for supervised ﬁne-tuning. Both L1 and L2 penalty for the DAEs and for the logistic regression layer are set to 1e-6.  CAE+MLP with Supervised Finetuning: A regularized auto-encoder which sometimes outperforms the DAE is the Contractive Auto-Encoder (CAE), (Rifai et al., 2012), which penalizes the Frobenius norm of the Jacobian matrix of derivatives of the hidden units with respect to the CAE’s inputs. The CAE serves as pre-training for an MLP, and in the supervised ﬁne-tuning state, the Adagrad method was used to automatically tune the learning rate (Duchi et al., 2010).  After training a CAE with 100 sigmoidal units patch-wise, the features extracted on each patch are concatenated and fed as input to an MLP. The selected Jacobian penalty coeﬃcient is 2, the learning rate for pre-training is 0.082 with batch size of 200 and 200 epochs of un- supervised learning are performed on the training set. For supervised ﬁnetuning, the learning rate is 0.12 over 100 epochs, L1 and L2 regularization penalty terms respectively are 1e-4 and 1e-6, and the top-level MLP has 6400 hidden units.  36  Greedy Layerwise CAE+DAE Supervised Finetuning: For this experiment we stack a CAE with sigmoid non-linearities and then a DAE with rectiﬁer non-linearities during the pre- training phase. As recommended by Glorot et al. (2011b) we have used a softplus nonlinearity for reconstruction, sof tplus(x) = log(1 + ex). We used an L1 penalty on the rectiﬁer outputs to obtain a sparser representation with rectiﬁer non-linearity and L2 regularization to keep the non-zero weights small.  The main diﬀerence between the DAE and CAE is that the DAE yields more robust recon-  struction whereas the CAE obtains more robust features (Rifai et al., 2011).  As seen on Figure 7 the weights U and V are shared on each patch and we concatenate the outputs of the last auto-encoder on each patch to feed it as an input to an MLP with a large hidden layer.  We used 400 hidden units for the CAE and 100 hidden units for DAE. The learning rate used for the CAE is 0.82 and for DAE it is 9*1e-3. The corruption level for the DAE (binomial noise) is 0.25 and the contraction level for the CAE is 2.0. The L1 regularization penalty for the DAE is 2.25*1e-4 and the L2 penalty is 9.5*1e-5. For the supervised ﬁnetuning phase the learning rate used is 4*1e-4 with L1 and L2 penalties respectively 1e-5 and 1e-6. The top-level MLP has 6400 hidden units. The auto-encoders are each trained for 150 epochs while the whole MLP is ﬁne-tuned for 50 epochs.  Greedy Layerwise DAE+DAE Supervised Finetuning: For this architecture, we have trained two layers of denoising auto-encoders greedily and performed supervised ﬁnetuning after unsupervised pre-training. The motivation for using two denoising auto-encoders is the fact that rectiﬁer nonlinearities work well with the deep networks but it is diﬃcult to train CAEs with the rectiﬁer non-linearity. We have used the same type of denoising auto-encoder that is used for the greedy layerwise CAE+DAE supervised ﬁnetuning experiment.  In this experiment we have used 400 hidden units for the ﬁrst layer DAE and 100 hidden units for the second layer DAE. The other hyperparameters for DAE and supervised ﬁnetuning are the same as with the CAE+DAE MLP Supervised Finetuning experiment.  37  ","We explore the effect of introducing prior information into the intermediatelevel of neural networks for a learning task on which all the state-of-the-artmachine learning algorithms tested failed to learn. We motivate our work fromthe hypothesis that humans learn such intermediate concepts from otherindividuals via a form of supervision or guidance using a curriculum. Theexperiments we have conducted provide positive evidence in favor of thishypothesis. In our experiments, a two-tiered MLP architecture is trained on adataset with 64x64 binary inputs images, each image with three sprites. Thefinal task is to decide whether all the sprites are the same or one of them isdifferent. Sprites are pentomino tetris shapes and they are placed in an imagewith different locations using scaling and rotation transformations. The firstpart of the two-tiered MLP is pre-trained with intermediate-level targets beingthe presence of sprites at each location, while the second part takes theoutput of the first part as input and predicts the final task's target binaryevent. The two-tiered MLP architecture, with a few tens of thousand examples,was able to learn the task perfectly, whereas all other algorithms (includeunsupervised pre-training, but also traditional algorithms like SVMs, decisiontrees and boosting) all perform no better than chance. We hypothesize that theoptimization difficulty involved when the intermediate pre-training is notperformed is due to the {\em composition} of two highly non-linear tasks. Ourfindings are also consistent with hypotheses on cultural learning inspired bythe observations of optimization problems with deep learning, presumablybecause of effective local minima."
1301.3530,2013,The Neural Representation Benchmark and its Evaluation on Brain and Machine  ,"['Charles Cadieu', 'Ha Hong', 'Dan Yamins', 'Nicolas Pinto', 'Najib J. Majaj', 'James J. DiCarlo']",https://arxiv.org/pdf/1301.3530.pdf,"3 1 0 2     n a J    5 2      ] E N . s c [      2 v 0 3 5 3  .  1 0 3 1 : v i X r a  The Neural Representation Benchmark and its  Evaluation on Brain and Machine  Charles F. Cadieu, Ha Hong, Dan Yamins, Nicolas Pinto, Najib J. Majaj, James J. DiCarlo  McGovern Institute for Brain Research and Department of Brain and Cognitive Sciences  Massachusetts Institute of Technology  Cambridge, MA 02139 cadieu@mit.edu  Abstract  A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of in- spiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representa- tion in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classiﬁcation loss on the ordered eigendecom- position of a kernel matrix [Montavon et al., 2011]. In our analysis we ﬁnd that the neural representation in visual area IT is superior to visual area V4, indicating an increase in representational performance in higher levels of the cortical visual hierarchy. In our analysis of representational learning algorithms, we ﬁnd that three-layer models approach the representational performance of V4 and the algo- rithm in [Le et al., 2012] surpasses the performance of V4. Impressively, we ﬁnd that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance comparable to that of IT for an intermediate level of image variation difﬁculty, and surpasses IT at a higher difﬁculty level. We believe this result represents a major milestone: it is the ﬁrst learning algorithm we have found that exceeds our current estimate of IT representation performance. To enable researchers to utilize this benchmark, we make available image datasets, analysis tools, and neural mea- surements of V4 and IT. We hope that this benchmark will assist the community in matching the representational performance of visual cortex and will serve as an initial rallying point for further correspondence between representations derived in brains and machines.  1  Introduction  One of the primary goals of representational learning is to produce algorithms that learn transforma- tions from unstructured data and produce representational spaces that are well suited to problems of interest, such as visual object recognition or auditory speech recognition. In the pursuit of this goal, the brain and the representations that it produces has been used as a source of inspiration and even suggested as a benchmark for success in the ﬁeld. In this work, we attempt to provide a new bench- mark to measure progress in representational learning with deﬁned measures of success relative to high-level visual cortex.  1  The machine learning and signal processing communities have achieved many successes by incor- porating insights from neural processing, even when a complete understanding of the neural systems was lacking. The initial formulations of neural networks took explicit inspiration for how neurons might transform their inputs [Rosenblatt, 1958]. David Lowe, in his original formulation of the SIFT algorithm cites inspiration from complex cells in primary visual cortex [Lowe, 2004] and IT cortex [Lowe, 2000]. The concepts of hierarchical processing and intermediate features also have a history of cross pollination between computer vision and neuroscience [Fukushima, 1980; Riesen- huber and Poggio, 1999; Stringer and Rolls, 2002; Serre et al., 2007; Pinto et al., 2009]. This cross pollination has also had great inﬂuence on the ﬁeld of neuroscience and has suggested ways to in- vestigate how the brain works, suggesting speciﬁc hypotheses about its computational principles. For the work presented here, the architectures and algorithms devised for hierarchical (deep) neural networks may serve as concrete hypotheses for the computational mechanisms used by the visual cortex to achieve fast and robust object recognition. We believe that the neuroscience ﬁeld needs more concrete hypotheses and we hope that the latest representational learning algorithms will ﬁll that void. How do we measure representational efﬁcacy? Any quantitative evaluation of progress made in rep- resentational learning must address this question. Here we advocate for the use of “kernel analysis,” formulated in the works of [Braun, 2006; Braun et al., 2008; Montavon et al., 2011]. We believe that kernel analysis has two main advantages. First, it measures the accuracy of a representation as a function of the complexity of the task decision boundary. This allows us to identify representations that achieve high accuracy for a given complexity. This also avoids a measurement confound that arises when using cross-validated accuracy: the decision boundary’s complexity and/or constraints are dependent on the size and choice of the training dataset, factors that can strongly affect accuracy scores. By measuring how the accuracy is affected by the complexity of the decision boundary, kernel analysis allows us to explicitly take this dependency into account. Second, kernel analysis is particularly advantageous for comparisons between models and the brain because it is robust to the number of samples used in the measurement. While our ability to measure neural activity in the brain has increased exponentially (see [Stevenson and Kording, 2011] for an analysis of simul- taneous recording growth rate, which is related to the number stimuli that can be measured), we are still orders of magnitude away from the dataset sizes achieved in the machine learning commu- nity. For this reason, measures that are useful in this low-sample regime are particularly important when evaluating the performance of neural representations. Kernel analysis exhibits this property as it converges quickly as a function of the number of samples (in our case images) used in the analysis. Therefore, while other measures of representational efﬁcacy may be related to kernel anal- ysis (such as cross-validated classiﬁcation accuracies, or counting the number of support vectors) we here utilize kernel analysis for its convergence properties and explicit measurement of accuracy versus complexity. In general, there are a number of methodologies we might consider when comparing algorithms to neural responses. One approach is to model neural variation directly [Wu et al., 2006]. This approach is valid scientiﬁcally in the pursuit of understanding neural mechanisms, but it lacks a representational aspect. For example, some details of neural activity may have no representational value, insofar as their variation does not relate to any variable we are interested in representing outside the neural mechanism. Therefore, we seek a measure that blends the neural measurement with the representational tasks of interest. This approach does have its downsides; most troubling of which is that we must choose a speciﬁc aspect of the world that is represented in the neural system. We can hope that our chosen task is one that the neural system effectively represents – ideally, one that the neural system has been optimized to represent. A major, unaccomplished, goal of computational neuroscience is to determine the representation formed in the brain by ﬁnding the mapping between external factors and neural response. In the methodology we propose, we do not claim to have solved the problem of choosing the aspects of the world that the brain has been optimized to represent, but we do believe we have chosen a reasonable task or aspect of the visual environment: category-level object recognition.1  1In relation to the scientiﬁc goal of ﬁnding those aspects of the world that the brain is representing, kernel analysis may be a way to measure which aspects of the world the brain has been optimized to represent: the attributes of the environment that the neural representation is found to perform well on, may be those aspects that the brain has been optimized to represent. However, such an examination is beyond the scope of this paper.  2  This work builds on a series of previous efforts to measure the representational efﬁcacy of mod- els against that of the brain. The work of Nikolaus Kriegeskorte and colleagues, see for exam- ple [Kriegeskorte et al., 2008b], examined the variation present in neural populations to visual stim- uli presentations and compared this variation to the variation produced in model feature spaces to the same stimuli. This work has inﬂuenced us in the pursuit of ﬁnding such mappings, but it has a major downside for our purposes: it does not measure the variations in the neural or model spaces that are relevant for a particular task, such as class-level object classiﬁcation2. There exist a number of pub- lished accounts of neural datasets that might be useful for the type of comparison we seek [Wallis and Rolls, 1997; Hung et al., 2005; Kiani et al., 2007; Rust and DiCarlo, 2010; Zhang et al., 2011], but these measurements have not been released, are often made on only a handful of images, and the measures given – typically cross-validated performance – are not as robust to low image counts as the kernel analysis metric we use here. In comparing algorithms to the brain, it is important to choose carefully the neural system to mea- sure and the type of neural measurement to make. In this work we analyze the ventral stream of macaque monkey, a non-human primate species. Using macaque visual cortex allows us to leverage an extensive literature that includes behavioral measurements [Fabre-Thorpe et al., 1998], neural anatomy [Felleman and Van Essen, 1991], extensive physiological measurements in numerous cor- tical visual areas, and measurements using a variety of techniques, from single cell measurements, to fMRI (for a review of high-level processing see [Orban, 2008]). These experiments indicate that macaque has visual abilities that are close to those of humans, that the ventral cortical process- ing stream (spanning V1, V2, V4, and IT) is relevant for object recognition, and that multi-unit recordings in high-level visual areas exhibit responses that are increasingly robust to object identity preserving variations (for a review see [DiCarlo et al., 2012]). With these considerations in mind, we describe a neural representation benchmark that may be used to judge the representational efﬁcacy of representational learning algorithms. Importantly, we present a measurement of visual areas V4 and IT in macaque cortex on this benchmark. These measurements allow researchers to test their algorithms against a known, high-performing repre- sentation. They may also provide an evaluation and thus facilitate a long sought goal of artiﬁcial intelligence: to achieve representations as effective as those found in the brain. Our preliminary evaluation of machine representations indicates that we may be coming close to this goal. The paper is organized as follows. In the Methods section we describe the images and task we use, the neural measurements, the use of kernel analysis, and our suggested protocol for measuring al- gorithms. In the Results section we provide the kernel analysis measurement on V4 and IT, on a number of control models, and on some recently published high-performing neural network mod- els. We conclude with a discussion of additional aspects of the neural system that will need to be investigated to ultimately conclude that representational learning algorithms are as effective as the brain.  2 Methods  The proposed benchmark utilizes an image dataset composed of seven object classes and is broken down into three levels of variation, which present increasing levels of difﬁculty. We measure the representational efﬁcacy of a feature space using kernel analysis, which measures the classiﬁcation loss under an eigendecomposition of the representation’s kernel matrix (kernel PCA).  2.1  Image dataset generation  For the representational task, we have chosen class-level object recognition under the effect of image variations due to object exemplar, geometric transformations (position, scale, and rotation/pose), and background. The task is deﬁned through an image generation process. An image is constructed by ﬁrst choosing one of seven categories, then one of seven object exemplars from that category, then a randomly chosen background image, and ﬁnally the variation parameters drawn from one of three distributions. The three different variation parameter distributions systematically increase the degree  2However, see [Kriegeskorte et al., 2008a] and [Mur et al., 2012], for discussion of methodologies to account for dissimilarity matrices by class-distance matrices. Such a methodology will produce a single summary number, and not the accuracy-complexity curves we achieve with kernel analysis.  3  of variation that is sampled, from Low Variation, which presents objects at a ﬁxed position, scale, and pose, to Medium Variation, to High Variation, which presents objects at positions spanning the image, under multi-octave scale dilation, and from a wide range of poses. Example images for each variation level are shown in Figure 1.  Figure 1: Example testing images for each variation level. For each variation level, Low, Medium and High Variation, we show two example images from the Car class and two example images from the Animal class. The car images shown all contain the same object instance, thus showing the image variability due only to the variation parameters and backgrounds. The animal images contain either a cow object instance or an elephant object instance, thus showing variability due to exemplar, variation parameters, and background.  The resulting image set has several advantages and disadvantages. Advantageously, this procedure eliminates dependencies between objects and backgrounds that may be found in real-world im- ages [Oliva and Torralba, 2007], and introduces a controlled amount of variability or difﬁculty in the task, which has been used to produce image datasets that are known to be difﬁcult for current algorithms [Pinto et al., 2008, 2010, 2011]. While the resulting images may have an artiﬁcial quality to them, having such control allows us to scientiﬁcally investigate neural coding in relation to these parameters. The disadvantages of using this image set are that it does not expose contextual effects that are present in the real world and may be used by both neural and machine systems, and we do not (currently) include other relevant variations, e.g. lighting, texture, natural deformations, or oc- clusion. We view these disadvantages as opportunities for future datasets and neural measurements.  2.2 Kernel analysis methodology  In measuring the efﬁcacy of a representation we seek a measure that will favor representations that allow for a simple task solution to be learned. For this measure, we turn to the work presented in [Montavon et al., 2011], which is based on theory presented in [Braun, 2006], and [Braun et al., 2008]. We provide a brief description of this measure and refer the reader to those references for additional details and justiﬁcation. The measurement procedure, which we refer to here as kernel analysis, utilizes kernel principal component analysis to determine how much of the task in question can be solved by the leading kernel principal components. Kernel principal components analysis will decompose the variation in the representational space due to the stimuli in question. A good representation will have high variability in relation to the task in question. Therefore, if the leading kernel principal components are effective at modeling the task, the representational space is effective for that task. In contrast, an ineffective representational space will have very little variation relevant for the task in question and variation relevant for the task is only contained in the eigenvectors corresponding to the smallest eigenvalues of the kernel principal component analysis. Intuitively, a good representation is one that learns a simple boundary from a small number of randomly-chosen examples, while a poor representation makes a more complicated boundary, requiring many examples to do so. Following [Montavon et al., 2011], kernel analysis consists of estimating the d ﬁrst components of the kernel feature space and ﬁtting a linear model on this low-rank representation to minimize the loss function for the task. The subspaces formed by the d ﬁrst components controls the complex-  4  High VariationMedium VariationLow Variation""Car""""Animal""ity of the model and the accuracy is measured by the loss in that subspace e(d). We refer to the dimensionality of the subspace d as the complexity and 1 − e(d) as the accuracy. Thus, the curve 1 − e(d) provides us with a measurement of the accuracy as a function of the model complexity for the given representational space. The curves produced by different representational spaces will in- form us about the simplicity of the task in that representational space, with higher curves indicating that the problem is simpler for the representation. One of the advantages of kernel analysis is that the kernel PCA method converges favorably from a limited number of samples. Braun et al. [2008] show that the kernel PCA projections obtained with a ﬁnite and typically small number of samples n (images) are close with multiplicative errors to those that would be obtained in the asymptotic case where n (cid:55)→ ∞. This result is especially important in our setting as the number of images we can reasonably obtain from the neural measurements is comparatively low. Therefore, kernel analysis provides us with a methodology for assessing representational effectiveness that has favorable properties in the low image sample regime, here thousands of images. We next present the speciﬁc computational procedure for computing kernel analysis. Given the learning problem p(x, y) and a set of n data points {(x1, y1), ..., (xn, yn)} drawn independently from p(x, y) we evaluate a representation deﬁned as a mapping x (cid:55)→ φ(x). For our case, the inputs x are images, the y are category labels, and the φ denotes a feature extraction process. As suggested by [Montavon et al., 2011], we utilize the Gaussian kernel because this kernel implies a smoothness of the task of interest in the input space [Smola et al., 1998]. We compute the kernel matrix Kσ associated to the data set as  kσ(φ(x1), φ(x1))  ...  kσ(φ(xn), φ(x1))  Kσ =   ,  ...  kσ(φ(x1), φ(xn))  ...  ... kσ(φ(xn), φ(xn))  where the standard Gaussian kernel is deﬁned as kσ(x, x(cid:48)) = exp(−||x − x(cid:48)||2/2σ2). We perform an eigendecomposition of Kσ where the eigenvectors u1, ..., un are sorted in decreasing magnitude of their corresponding eigenvalues λ1, ..., λn:  Kσ = (u1|...|un)· diag(λ1, ..., λn)· (u1|...|un)T .  (2) Let ˆUd = (u1|...|ud) and ˆΛd = diag(λ1, ..., λd) be the d-dimensional approximation of the eigen- decomposition. Note that we have dropped, for the moment, the dependency on σ. We then solve the learning problem using a linear model in the corresponding subspace. For our problem we ﬁnd the least squares solution to the multi-way regression problem denoted as Θ∗  d and deﬁned as  d = argminΘ || ˆUdΘ − Y ||2 Θ∗  F = ˆU T  d Y.  The resulting model prediction is then ˆYd = ˆUdΘ∗ 1 n  e(d, σ) =  d. The resulting loss, with dependence on σ is || ˆYd − Y ||2 F .  To remove the dependence of the kernel on σ we ﬁnd the value that minimizes the loss at that dimensionality d: e(d) = argminσ e(d, σ). Finally, for convenience we plot accuracy (1 − e(d)) against normalized complexity (d/D), where D is total dimensionality. Note that we have chosen to use a squared error loss function for our multi-way classiﬁcation prob- lem. While it might be more appropriate to evaluate a multi-way logistic loss function, we have chosen to use the least-squares loss for its computational simplicity, because it provides a stronger requirement on the representational space to reduce variance within class and to increase variance between classes, and it allows us to distinguish representations that may be identical in terms of separability for a certain dimensionality d but still have differences in their feature mappings. The kernel analysis of deep Boltzmann machines in [Montavon and M¨uller, 2012] also uses a mean squared loss function in the classiﬁcation problem setting. In the discussion above, Y = (y1, . . . , yn) represents the vector of task labels for the images (x1, . . . , xn). In our speciﬁc case, the yi are category identity values, and are assumed to be discrete  5  (1)  (3)  (4)  (cid:28) 1  n  (cid:29)  binary values in equations 3 and 4 above. To generalize to the case of multiway categorization, we use a version of the common one-versus-all strategy. Assuming k distinct categories, we form the label matrix  (cid:26)1 if image xi is in category j  (5)  Y = (yij) =  0 otherwise  where j ∈ [1, . . . , k]. Then for each category j, we compute the per-class prediction Y j d by replacing Y in equations 3 and 4 with Yj, the j-th column of Y. The overall error is then the average over classes of the per-class error, e.g.  e(d, σ) = (cid:104)ej(d, σ)(cid:105)j =  || ˆY j  d − Yj||2  F  .  j  (6)  Minimization over σ then proceeds as in the binary case.  2.3 Suggested protocol  To evaluate both neural representations and machine representations we measure the kernel analysis curves and area under the curves (KA-AUC) for each variation. The testing image dataset consists of seven object classes with seven instances per object class, broken down into three levels of variation, with 490 images in Low Variation, 1960 in Medium Variation, and 1960 in High Variation. The classes are Animals, Cars, Chairs, Faces, Fruits, Planes and Tables. To measure statistical variation due to subsampling of image variation parameters we evaluate 10 pre-deﬁned subsets of images, each taking 80% of the data from each variation level. Within each subset we equalize the number of images from each class. For each representation, we maximize over the values of the Gaussian kernel σ parameter chosen at 10%, 50%, and 90% quantiles in the distance distribution. For each variation level and representation, this procedure produces a kernel analysis curve and AUC for each of the subsets, and we compute the mean and standard deviation of the AUC values. We also provide a training dataset that may be used for model selection. This dataset allows both unsupervised and supervised training of representational learning algorithms.  2.4 Provided data and tools  To allow researchers to utilize this dataset we provide the following tools and downloadable data: • Testing images: a set of images containing seven object classes with seven instances per object class, broken down into three levels of variation, with 490 images in Low Variation, 1960 in Medium Variation, and 1960 in High Variation. The classes are Animals, Cars, Chairs, Faces, Fruits, Planes and Tables. Computing features on this set of images is sufﬁcient to evaluate an algorithm. Each image is grayscale and 256 by 256 pixels. To prevent over-ﬁtting, candidate algorithms should not be trained on this dataset, and any parameter estimation involved in model selection should be estimated independently of these testing images. • Training images: a set of 128,000 images consisting of 16 object classes with 16 object instances per object class, these images are produced from a similar rendering procedure as the testing image set. The training set contains no speciﬁc constituent objects or background images in common with the testing set, but it does have new objects in each of the original seven categories, in addition to 9 new categories. This image set can therefore be used for independent model selection and learning, using either supervised for unsupervised methods, see Appendix A. Use of this training set is, however, optional.  • Testing set kernel analysis curves and KA-AUC values for V4 and IT. • Tools to evaluate kernel analysis from features produced by a model to be tested. These tools and datasets can be found at: http://dicarlolab.mit.edu/neuralbenchmark.  2.5 Neural data collection  We collected 168 multi-unit sites from IT cortex and 128 multi-unit sites from V4. To form the neural feature vectors for IT and V4 we normalized responses by background ﬁring rate and by variance within a presentation of all images within a variation. See Appendix B for details. This  6  post-processing procedure has been shown to account for human performance [Majaj et al., 2012] (also see [Hung et al., 2005; Rust and DiCarlo, 2010] for results utilizing a similar procedure).  2.6 Machine representations  We evaluate a number of machine representations from the literature, including several recent best of breed representational learning algorithms and visual representation models, as well as a feed- forward three layer hierarchical model optimized on the training set. V1-like We evaluate the V1-like representation from Pinto et al.’s V1S+ [Pinto et al., 2008]. This model attempts to capture a ﬁrst-order account of primary visual cortex (V1). It computes a collec- tion of locally-normalized, thresholded Gabor wavelet functions spanning orientation and frequency. This model is a simple, baseline biologically-plausible representation, against which more sophisti- cated representations can be compared. High-throughput L3 model class (HT-L3) We evaluate the same three layer hierarchical convolu- tional neural net model class described in [Pinto et al., 2009] and [Pinto and Cox, 2011], the “L3 model class”. Each model in this class, is a three layer model in which each layer sequentially performs local ﬁltering, thresholding, saturation, pooling, and normalization. To choose a high performing model from this class, we performed a high-throughput search of the parameter space, using kernel-analysis performance on the provided training image set as the optimization criterion. The top performing model on the training set is then evaluated on the testing set (Top HT-L3). See Appendix A for further details. Coates et al. NIPS 2012 We evaluate the unsupervised feature learning model in [Coates et al., 2012], which learns 150,000 features from millions of unlabeled images collected from the Internet. We evaluate the second layer “complex cells,” a 10,000 dimensional feature space, by rescaling the input images to 96 by 96 pixels and computing the model’s output on a 3 by 3 grid of non- overlapping 32 by 32 pixel windows. The resulting output is 90,000 dimensional. Le et al. ICML 2012 We evaluate the model in [Le et al., 2012], which is a hierarchical locally connected sparse auto encoder with pooling and local contrast normalization and is trained unsu- pervised from a dataset of 10 million images downloaded from the Internet and ﬁne-tuned with ImageNet images and labels. We use the penultimate layer outputs (69696 features) of the network for the feature representation (the layer before class-label prediction). Images are resized to the model’s input dimensions, here 200 by 200 pixels. Krizhevsky et al. NIPS 2012 (SuperVision) We evaluate the deep convolutional neural network model ‘SuperVision’ described in [Krizhevsky et al., 2012], which is trained by supervised learning on the ImageNet 2011 Fall release (∼15M images, 22K classes) with additional training on the LSVRC-2012 dataset (1000 classes). The authors computed the features of the penultimate layer of their model (4096 features) on the testing images by cropping out the center 224 by 224 pixels (this is the input size to their model). This mimics the procedure described in [Krizhevsky et al., 2012], in which this feature is fed into logistic regression to predict class labels.  3 Results  Evaluation of neural representations  In Figure 2 we present kernel analysis curves obtained from the measured V4 and IT neural popula- tions for each variation level. KA-AUC values for V4 are 0.88, 0.66, and 0.56, and for IT are 0.90, 0.86, and 0.72, for Low, Medium, and High Variation, respectively. For each variation level, our bootstrap analysis indicates that the KA-AUC measurements between IT and V4 are signiﬁcantly different (see Table 1). At Low Variation there is not a large difference between V4 and IT. This might be expected, as this variation level does not test for variability due to scale, position, or pose, which are variations that the neural responses in IT are more tolerant to than in V4. The higher variation sets, Medium and High Variation, show increased separation between V4 and IT, and reduced performance for both representations, indicating the increased difﬁculty of the task under these representations. However, the IT representation maintains high accuracy at low complexity even in the High Variation condi-  7  Figure 2: Kernel analysis curves of V4 and IT. Each panel shows the kernel analysis curves for each variation level. Accuracy, one minus loss (1− e(d)), is plotted against complexity, the normal- ized dimensionality of the eigendecomposition (d/D). Shaded regions indicate the maximum and minimum accuracy obtained over testing subsets, which are often smaller than the line thickness.  tion. The IT representation under Medium and High Variation shows a sharp increase in accuracy at low complexity, indicating that the IT representation is able to accurately capture the class-level object recognition task with a simple decision boundary. Note that these kernel analysis measure-  8  High VariationMedium VariationLow VariationIT (est.)V4 (est.)IT (est.)V4 (est.)IT (est.)V4 (est.)Table 1: Kernel analysis results. For each representation we measure the KA-AUC at each variation level for each testing subset. The means over testing subsets are given in the table, with standard deviations in parentheses. Top performing models are highlighted. Note that our measurements of IT and V4 Cortex are our current best estimates (est.) and are subject to experimental limitations.  IT Cortex (est.) V4 Cortex (est.) V1-like Top HT-L3 Coates et al. NIPS 2012 Le et al. ICML 2012 Krizhevsky et al. NIPS 2012  Low Variation Medium Variation High Variation 0.72 (2.2e-03) 0.90 (2.4e-03) 0.56 (3.2e-03) 0.88 (2.0e-03) 0.84 (2.0e-03) 0.52 (2.0e-03) 0.92 (1.4e-03) 0.53 (1.7e-03) 0.52 (2.9e-03) 0.83 (1.5e-03) 0.57 (3.0e-03) 0.90 (2.4e-03) 0.75 (3.0e-03) 0.88 (2.6e-03)  0.86 (1.2e-03) 0.66 (3.2e-03) 0.57 (2.9e-03) 0.62 (1.8e-03) 0.54 (3.0e-03) 0.69 (2.5e-03) 0.85 (2.0e-03)  ments are only our current estimate of representation in V4 and IT. We discuss the limitations of these estimates in the discussion section and provide an extrapolation in Appendix C.  Evaluation of machine representations  In Figure 3 we present the kernel analysis evaluation for the machine representations we have evalu- ated along with the neural representations for comparison. The corresponding KA-AUC numbers are presented in Table 1. The V1-like model shows high accuracy at low complexity on Low Variation but performs quite poorly on Medium Variation and High Variation, indicating that these tasks are interesting tests of the object recognition problem. The Top HT-L3 model is the highest performing representation at Low Variation and achieves performance that approaches V4 on Medium Variation and High Variation. The model presented in [Coates et al., 2012] performs similarity to the V1-like model on all variation levels. This low performance may be due to the large variety of images this model was trained on, its relatively shallow architecture, and/or the mismatch in our testing image size and the 32 by 32 pixel patches of the base model. The model presented in [Le et al., 2012] performs comparably to IT on Low Variation, and surpasses V4 at Medium and Variations. The model in [Krizhevsky et al., 2012] performs comparably to V4 at Low Variation, nearly matches the performance of IT at Medium Variation, and surpasses IT representation on High Variation. In- terestingly, this model matches IT performance at Medium Variation across the entire complexity range and exceeds it across the complexity range at High Variation. We view this result as highly signiﬁcant as it is the ﬁrst model we have measured that matches our current estimate of IT repre- sentation performance at Medium Variation and surpasses it at High Variation.  4 Discussion  There are a number of issues related to our measurement of macaque visual cortex, including view- ing time, behavioral paradigm, neural subsampling, and mapping the neural recording to a neural feature, that will be necessary to address in determining the ultimate representational measurement of macaque visual cortex. The presentation time of the images shown to the animals was intention- ally brief (100 ms), but is close to typical ﬁxation time (∼200 ms). Therefore, it will be interesting to measure how the neural representational space changes with increased viewing time, especially considering that natural viewing conditions typically allow for longer ﬁxation times and multiple ﬁxations. Another aspect to consider is that animals are engaged in passive viewing during the ex- perimental procedure. Does actively performing a task inﬂuence the neural representation? This question may be related to what are commonly referred to as attentional phenomena [e.g. biased competition]. Current experimental techniques only allow us to measure a small portion of the neu- rons in a cortical area. While our analysis in Appendix C suggests that we are reaching saturation in our estimate of KA-AUC with our current neural sample, our sample is biased spatially on the cortical sheet because of our use of electrode grids. This bias likely leads to an underestimate of KA-AUC. Finally, the neural code is a topic of heated debate in the neuroscience community and the mapping from multi-unit recordings to the neural feature vector we have used for our analysis is only one possible mapping. Importantly, this mapping has been shown to account for human  9  Figure 3: Kernel analysis curves of brain and machine. “V4 (est.)” and “IT (est.)” are brain representations and all others are machine representations. Each panel shows the kernel analysis curves for each variation level. Accuracy, one minus loss (1−e(d)), is plotted against complexity, the normalized dimensionality of the eigendecomposition (d/D). Shaded regions indicate the maximum and minimum accuracy obtained over testing subsets, which are often smaller than the line thickness.  behavioral performance [Majaj et al., 2012]. However, as we gain more knowledge about cortical processing (such as intrinsic dynamics [Canolty et al., 2010]) our best guess at the neural code may evolve and update our neural representation benchmark accordingly.  10  High VariationMedium VariationLow VariationIT (est.)V4 (est.)IT (est.)V4 (est.)IT (est.)V4 (est.)Another aspect that our measurement does not address is the direct impact of visual experience on the representations observed in IT cortex. Interestingly, the macaques involved in these studies have had little or no real-world experience with a number of the object categories used in our evaluation, though they do beneﬁt from millions of years of evolution and years of postnatal experience. How- ever, learning effects in adult IT cortex are well observed [Kobatake et al., 1998; Baker et al., 2002; Sigala and Logothetis, 2002], even with passive viewing [Li and DiCarlo, 2010]. Remaining unan- swered questions are: how has the exposure during the experimental protocol affected the neural representation, and could the neural representation be further enhanced with increased exposure? A related question: is the training set we have provided sufﬁcient to achieve IT level performance on the testing set? We do not have a positive example of such transfer, and we expect that algorithms leveraging massive amounts of visual data may produce the best results on the testing set. Such algorithms, and their data dependence, will be informative. Furthermore, to what extent do we need to build additional structure into our representations and representational learning algorithms to achieve representations equivalent to those found in the brain? Could human neural representation, if measured, be better than what we observe in macaque IT cor- tex? If the volume of cortical tissue is related to representational efﬁcacy, it is likely that the human ventral stream would achieve even better performance. While determining human homologues of macaque visual cortex is under active investigation, it is known that primary visual cortex in humans is twice as large as in macaque [Van Essen, 2003]. While this is suggestive that human visual repre- sentation may be even better under our metric, the scaling of human visual cortex over macaque may be optimizing representational aspects that we are not measuring here. In summary, we suspect that the estimates for representational performance in macaque we have presented here provide a lower bound in performance of the human visual system. One way to address human visual representation may be through the use of fMRI or inference of the human representational space from behavioral measurements. We, and others in the neuroscience ﬁeld, are actively pursuing these directions.  Where are we today?  Under our analysis, we believe that the ﬁeld has made signiﬁcant advances with recent algorithms. On the intermediate level variation task (Medium Variation) these advances are quite evident: the recent representational learning algorithm in [Le et al., 2012] surpasses the representation in V4 and, surprisingly, the supervised algorithm of [Krizhevsky et al., 2012] matches the representation in IT. These advances are also evident on the high level variation task (High Variation): the [Le et al., 2012] algorithm is narrowly better than V4 and the [Krizhevsky et al., 2012] algorithm beats IT by an ample margin. It will be informative to measure the elements of these models that lead to this performance and it will be interesting to see if purely unsupervised algorithms can achieve similar performance.  A vision for the future  The methodology we have proposed here can be extended to other sensory domains where repre- sentation is critical and neural representations are thought to be effective. For example, it should be possible to deﬁne similar task protocols for auditory stimuli and measure the neural responses in auditory cortex. Such measurements would not only have implications for discovering effective au- ditory representations, but may also provide the data necessary to validate representational learning algorithms that are effective in multiple contexts. Representational learning algorithms that prove effective across these domains may serve as hypotheses for a canonical cortical algorithm, a ‘holy grail’ for artiﬁcial intelligence research.  Acknowledgments  This work was supported by the U.S. National Eye Institute (NIH NEI: 5R01EY014970-09), the National Science Foundation (NSF: 0964269), and the Defense Advanced Research Projects Agency (DARPA: HR0011- 10-C-0032). C.F.C was supported by the U.S. National Eye Institute (NIH: F32 EY022845-01). We thank Adam Coates, Quoc Le, and Alex Krizhevsky for their help in evaluating their models and comments on the paper.  11  References CI Baker, M Behrmann, and CR Olson. Impact of learning on representation of parts and wholes in monkey  inferotemporal cortex. Nature Neuroscience, 2002.  ML Braun. Accurate Error Bounds for the Eigenvalues of the Kernel Matrix. JMLR, 2006. ML Braun, JM Buhmann, and KR M¨uller. On relevant dimensions in kernel feature spaces. JMLR, 2008. RT Canolty, K Ganguly, SW Kennerley, CF Cadieu, K Koepsell, JD Wallis, and JM Carmena. Oscillatory  phase coupling coordinates anatomically dispersed functional cell assemblies. PNAS, 2010.  MM Churchland, JP Cunningham, MT Kaufman, JD Foster, P Nuyujukian, SI Ryu, and KV Shenoy. Neural  population dynamics during reaching. Nature, 2012.  A Coates, A Karpathy, and A Ng. Emergence of Object-Selective Features in Unsupervised Feature Learning.  NIPS, 2012.  JJ DiCarlo, D Zoccolan, and NC Rust. How Does the Brain Solve Visual Object Recognition? Neuron, 2012. M Fabre-Thorpe, G Richard, and SJ Thorpe. Rapid categorization of natural images by rhesus monkeys.  Neuroreport, 1998.  DJ Felleman and DC Van Essen. Distributed hierarchical processing in the primate cerebral cortex. Cerebral  cortex, 1991.  K Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition  unaffected by shift in position. Biological cybernetics, 1980.  CP Hung, G Kreiman, T Poggio, and JJ DiCarlo. Fast readout of object identity from macaque inferior temporal  cortex. Science, 2005.  R Kiani, H Esteky, K Mirpour, and K Tanaka. Object category structure in response patterns of neuronal  population in monkey inferior temporal cortex. Journal of Neurophysiology, 2007.  E Kobatake, G Wang, and K Tanaka. Effects of shape-discrimination training on the selectivity of inferotem-  poral cells in adult monkeys. Journal of Neurophysiology, 1998.  N Kriegeskorte, Mur M, and Bandettini P. Representational Similarity Analysis – Connecting the Branches of  Systems Neuroscience. Frontiers in Systems Neuroscience, 2008a.  N Kriegeskorte, M Mur, DA Ruff, R Kiani, J Bodurka, H Esteky, K Tanaka, and PA Bandettini. Matching  categorical object representations in inferior temporal cortex of man and monkey. Neuron, 2008b.  A Krizhevsky, I Sutskever, and G Hinton. ImageNet classiﬁcation with deep convolutional neural networks.  NIPS, 2012.  QV Le, R Monga, M Devin, K Chen, GS Corrado, J Dean, and AY Ng. Building high-level features using large  scale unsupervised learning. ICML, 2012.  N Li and JJ DiCarlo. Unsupervised Natural Visual Experience Rapidly Reshapes Size-Invariant Object Repre-  sentation in Inferior Temporal Cortex. Neuron, 2010.  DG Lowe. Towards a computational model for object recognition in IT cortex. In BMVC, 2000. DG Lowe. Distinctive Image Features from Scale-Invariant Keypoints.  International Journal of Computer  Vision, 2004.  N Majaj, H Hong, E Solomon, and JJ DiCarlo. A uniﬁed neuronal population code fully explains human object  recognition. In COSYNE, 2012.  G Montavon and KR M¨uller. Deep Boltzmann Machines and the Centering Trick. Neural Networks: Tricks of  the Trade, 2012.  G Montavon, ML Braun, and KR M¨uller. Kernel Analysis of Deep Networks. JMLR, 2011. M Mur, DA Ruff, J Bodurka, P De Weerd, PA Bandettini, and N Kriegeskorte. Categorical, Yet Graded – Single- Image Activation Proﬁles of Human Category-Selective Cortical Regions. The Journal of Neuroscience, 2012.  A Oliva and A Torralba. The role of context in object recognition. Trends in Cognitive Sciences, 2007. GA Orban. Higher order visual processing in macaque extrastriate cortex. Physiological Reviews, 2008. N Pinto and D Cox. Beyond simple features: A large-scale feature search approach to unconstrained face  recognition. FG, 2011.  N Pinto, David D Cox, and JJ DiCarlo. Why is Real-World Visual Object Recognition Hard? PLoS Computa-  tional Biology, 2008.  N Pinto, D Doukhan, JJ DiCarlo, and DD Cox. A High-Throughput Screening Approach to Discovering Good  Forms of Biologically Inspired Visual Representation. PLoS Computational Biology, 2009.  12  N Pinto, N Majaj, Y Barhomi, E Solomon, and JJ DiCarlo. Human versus machine: comparing visual object  recognition systems on a level playing ﬁeld. In COSYNE, 2010.  N Pinto, Y Barhomi, DD Cox, and JJ DiCarlo. Comparing state-of-the-art visual features on invariant object  recognition tasks. WACV, 2011.  M Riesenhuber and T Poggio. Hierarchical models of object recognition in cortex. Nature Neuroscience, 1999. F Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain.  Psychological review, 1958.  NC Rust and JJ DiCarlo. Selectivity and tolerance (“invariance”) both increase as visual information propagates  from cortical area V4 to IT. Journal of Neuroscience, 2010.  T Serre, L Wolf, S Bileschi, M Riesenhuber, and T Poggio. Robust Object Recognition with Cortex-Like  Mechanisms. PAMI, 2007.  N Sigala and NK Logothetis. Visual categorization shapes feature selectivity in the primate temporal cortex.  Nature, 2002.  AJ Smola, B Sch¨olkopf, and KR M¨uller. The connection between regularization operators and support vector  kernels. Neural Networks, 1998.  IH Stevenson and KP Kording. How advances in neural recording affect data analysis. Nature Neuroscience,  2011.  IH Stevenson, BM London, ER Oby, and NA Sachs. Functional Connectivity and Tuning Curves in Populations  of Simultaneously Recorded Neurons. PLOS Computational Biology, 2012.  SM Stringer and ET Rolls. Invariant Object Recognition in the Visual System with Novel Views of 3D Objects.  Neural Computation, 2002.  DC Van Essen. Organization of Visual Areas in Macaque and Human Cerebral Cortex. In The Visual Neuro-  sciences. 2003.  G Wallis and ET Rolls. Invariant Face and Object Recognition in the Visual System. Progress in Neurobiology,  1997.  MC Wu, SV David, and JL Gallant. Complete functional characterization of sensory neurons by system iden-  tiﬁcation. Annual Review of Neuroscience, 2006.  Y Zhang, EM Meyers, NP Bichot, T Serre, T Poggio, and R Desimone. Object decoding with attention in  inferior temporal cortex. PNAS, 2011.  Appendix  Appendix A: High-throughput evaluation of the L3 model class  Using the L3 model class, we performed a high-throughput search of the parameter space by evaluating ap- proximately 500 parameter selections at random on training image sets. We ran each L3 model parameter instantiation on category-balanced subset of 12,800 randomly chosen images from the 128,000-image training set. For each instantiation, we evaluated a kernel analysis protocol similar to that used for the testing set, but with the 16 object class labels of the training set as opposed to the 7 present in the testing set. For each model instantiation, we also extracted features on the testing set images, and ran the standard kernel analysis protocol. To evaluate the transfer between the training set and testing set, we examined how well training set scores predict testing set scores by comparing how relative performance rankings on the training set transfer to the testing set. Figure 4 shows these results. Performance on the training set is strongly correlated with performance on Medium (r = 0.64) and High Variation (r = 0.58) components of testing set, and weakly correlated on the Low Variation condition. This might be expected, as the training set contains variation similar to the High Variation testing set. The single best model from training achieves a high score on the testing set relative to other models in the training set and is in the range of the top machine-learning representations. This data indicates that models that are trained using the provided training set can perform favorably on the testing set.  Appendix B: Neural data collection  We have collected neural data from V4 and IT across two adult male rhesus monkeys (Macaca mulatta, 7 and 9 kg) by using a multi-electrode array recording system (BlackRock Microsystems, Cerebus System). We chron- ically implanted three arrays per animal and have recorded the best 128 visually driven neural measurement sites (determined by separate pilot images) in one animal (58 IT, 70 V4) and 168 in another (110 IT, 58 V4). During stimulus presentation we recorded multi-unit neural responses to our images (see Section 2.1) from the V4 and IT sites. Stimuli were presented on an LCD screen (Samsung, SyncMaster 2233RZ at 120Hz) one  13  at a time. Each image was presented for 100ms with a radius of 8◦ at the center of the screen on top of the half-gray background and was followed by a 100ms half-gray “blank” period. The animal’s eye movement was monitored by a video eye tracking system (SR Research, EyeLink II), and the animal was rewarded upon the successful completion of 6–8 image presentations while maintaining good eye ﬁxation (jitter within ±2◦ was determined acceptable ﬁxation) at the center of the screen, indicated by a small (0.25◦) red dot. Presentations with large eye movements were discarded. In each experimental block, we recorded responses to all images of a certain variation level. Within one block each image was repeated three times for Low Variation and once for Medium and High Variation. This resulted in the collection of 28, 51, and 47 image repetitions for Low, Medium, and High Variation respectively. All surgical and experimental procedures are in accordance with the National Institute of Health guidelines and the Massachusetts Institute of Technology Committee on Animal Care. We convert the raw neural responses to a neural representation through the following normalization process. For each image in a block, we compute the vector of raw ﬁring rates across measurement sites by counting the number of spikes between 70ms and 170ms after the onset of the image for each site. We then subtracted the background ﬁring rate, which is the ﬁring rate during presentation of a half-gray background or “blank” image, from the evoked response. In order to minimize the effect of variable external noise, we normalize by the standard deviation of each site’s response to a block of images for Medium and High Variation. For the Low Variation stimuli, we divide the three repetitions within each block into three separate sets, each containing a complete set of images, and normalize by the standard deviation of each site’s response within its set. Finally, the neural representation is calculated by taking the mean across the repetitions for each image and for each site, producing a scalar valued matrix of neural sites by images. This post-processing procedure is only our current best-guess at a neural code, which has been shown to account for human performance [Majaj et al., 2012]. Therefore, it may be possible to develop a more effective neural decoding, for example inﬂuenced by intrinsic cortical variability [Stevenson et al., 2012], or dynamics [Churchland et al., 2012; Canolty et al., 2010].  Appendix C: KA-AUC and subsampling of neural sites  Current experimental techniques only allow us to measure a small portion of the neurons in a cortical area. We seek to estimate how our kernel analysis metric would be affected by having a larger neural sample. In Figure 5 we estimate the effect of subsampling the neural population in our measurement, showing the KA- AUC as a function of the number of neural measurement sites. To estimate the asymptotic convergence of each neural representation (V4 and IT) at each variation level, we ﬁt a curve of the form AU C(t) = a + be−ctd, where t is the number of neural sites and a, b, c, and d are parameters3. This provides us with an estimate of the KA-AUC for the entire neural population. The estimated asymptotic values for the KA-AUC’s for V4 are 0.89, 0.69, and 0.66, and for IT are 0.93, 0.91, and 0.75, for Low Variation, Medium Variation, and High Variation, respectively. Interestingly we ﬁnd that for the number of neural sites we have measured we are already approaching the asymptotic value. Therefore, for the given task speciﬁcation, preprocessing procedure, and convergence estimate, we believe we are reaching saturation in our estimate of KA-AUC for the neural population in V4 and IT.  Errata  An earlier version of this manuscript contained incorrectly computed kernel analysis curves and KA-AUC values for V4, IT, and the HT-L3 models. They have been corrected in this version.  3We found that this functional form ﬁt well a similar analysis performed on a computational representation in which we subsampled the number of features included in the analysis. This allowed us to estimate the behavior of KA-AUC in much larger feature spaces (>4000 features) than in the neural measurements.  14  Figure 4: High-throughput L3 model relationship between training and testing performance. Each panel shows a scatter plot between the measured training KA-AUC and the testing KA-AUC for each variation level. Red lines indicate best linear ﬁt. Red dots are the best and worst performing models on the training set and best performing model on each testing set (standard deviations are shown as error bars and are small in the testing axis). Note that there is only one value for the training KA-AUC for each model. The linear relationships we observe indicate that the provided training set is informative for the testing set.  15  Figure 5: Effect of sampling in neural areas. We estimate the effect of sampling the neural sites on the testing set KA-AUC. Each panel shows the effect for each variation level. Best ﬁt curves are shown as solid lines with measured samples indicated by ﬁlled circles. Estimated asymptotes are indicated by dashed horizontal lines. See text for more details.  16  High VariationMedium VariationLow Variation","A key requirement for the development of effective learning representationsis their evaluation and comparison to representations we know to be effective.In natural sensory domains, the community has viewed the brain as a source ofinspiration and as an implicit benchmark for success. However, it has not beenpossible to directly test representational learning algorithms directly againstthe representations contained in neural systems. Here, we propose a newbenchmark for visual representations on which we have directly tested theneural representation in multiple visual cortical areas in macaque (utilizingdata from [Majaj et al., 2012]), and on which any computer vision algorithmthat produces a feature space can be tested. The benchmark measures theeffectiveness of the neural or machine representation by computing theclassification loss on the ordered eigendecomposition of a kernel matrix[Montavon et al., 2011]. In our analysis we find that the neural representationin visual area IT is superior to visual area V4. In our analysis ofrepresentational learning algorithms, we find that three-layer models approachthe representational performance of V4 and the algorithm in [Le et al., 2012]surpasses the performance of V4. Impressively, we find that a recent supervisedalgorithm [Krizhevsky et al., 2012] achieves performance comparable to that ofIT for an intermediate level of image variation difficulty, and surpasses IT ata higher difficulty level. We believe this result represents a major milestone:it is the first learning algorithm we have found that exceeds our currentestimate of IT representation performance. We hope that this benchmark willassist the community in matching the representational performance of visualcortex and will serve as an initial rallying point for further correspondencebetween representations derived in brains and machines."
1301.3391,2013,Feature grouping from spatially constrained multiplicative interaction  ,"['Felix Bauer', 'Roland Memisevic']",https://arxiv.org/pdf/1301.3391.pdf,"3 1 0 2    r a     M 1 1      ]  G L . s c [      3 v 1 9 3 3  .  1 0 3 1 : v i X r a  Feature grouping from spatially constrained  multiplicative interaction  Felix Bauer  Frankfurt Institute for Advanced Studies  Frankfurt, Germany  fbauer@fias.uni-frankfurt.de  Roland Memisevic University of Montreal  Montreal, Canada  memisevr@iro.umontreal.ca  Abstract  We present a feature learning model that learns to encode relationships between images. The model is deﬁned as a Gated Boltzmann Machine, which is con- strained such that hidden units that are nearby in space can gate each other’s con- nections. We show how frequency/orientation “columns” as well as topographic ﬁlter maps follow naturally from training the model on image pairs. The model also offers a simple explanation why group sparse coding and topographic feature learning yields features that tend to by grouped according to frequency, orienta- tion and position but not according to phase. Experimental results on synthetic image transformations show that spatially constrained gating is an effective way to reduce the number of parameters and thereby to regularize a transformation- learning model.  1  Introduction  Feature-learning methods have started to become a standard component in many computer-vision pipelines, because they can generate representations which are better at encoding the content of images than raw images themselves. Feature learning works by projecting local image patches onto a set of feature vectors (aka. “ﬁlters”), and using the vector of ﬁlter responses as the representation of the patch. This representation gets passed on to further processing modules like spatial pooling and classiﬁcation. Filters can be learned using a variety of criteria, including maximization of sparseness across ﬁlter responses [18], minimizing reconstruction error [24], maximizing likelihood [5], and many others. Under any of these learning criteria, Gabor features typically emerge when training on natural image patches. There has been an increasing interest recently in imposing group structure on learned ﬁlters. For learning, ﬁlters are encouraged to come in small groups, such that all members of a group share certain properties. The motivation for this is that group structure can explain several biological phe- nomena such as the presence of complex cells [6], it provides a simple way to model dependencies between features (eg., [9] and references therein), and it can make learned representations more robust to small transformations which is useful for recognition [10]. Filter grouping is referred to also as “structured sparse coding” or “group sparse coding”. Feature grouping can also be used as a way to obtain topographic feature maps [8]. To this end, features are layed out in a 2-dimensional grid, and groups are deﬁned on this grid such that each ﬁlter group shares ﬁlters with its neighboring groups. In other words, groups overlap with each other. Training feature grouping and topographic models on natural image patches typically yields Gabor ﬁlters whose frequency, orientation and po- sition is similar for all the ﬁlters within a group. Phase, in contrast, tends to vary randomly across the ﬁlters within a group (eg. [7, 10, 8]). Various approaches to performing group sparse coding and topographic feature learning have been proposed. Practically all of these are based on the same recipe: The set of ﬁlters is pre-partitioned  1  into groups before training. Then during learning, a second layer computes a weighted sum over the squares of all ﬁlter responses within a group (eg., [6, 9, 10]). The motivation for using square- pooling architectures to learn group structure is based in part on empirical evidence (it seems to work across many models and learning objectives). There have also been various attempts to explain it, based on a variety of heuristics: One explanation, for example, follows from the fact that some non- linearity must be used before pooling features in a group, because in the absence of a nonlinearity, we would wind up with a standard (linear) feature learning model. The square is a non-linearity that is simple and canonical. It can also be shown that even-symmetric functions, like the square, applied to ﬁlter responses, show strong dependencies. So they seem to capture a lot of what we are missing after performing a single layer of feature learning [8]. Another motivation is that Gabor features are local Fourier components, so computing squares is like computing spectral energies. Finally, it can be shown that in the presence of a upstream square-root non-linearity, using squared features generalizes standard feature learning, since it degenerates to a standard feature-learning model when using group size 1 [10]. For a summary of these various heuristics, see [8] (page 215). Topographic grouping of frequency, orientation and position is also a well-known feature of mammalian brains (eg., [8]). Thus, another motivation for using square-pooling in feature learning in general has been that, by means of replicating this effect, it may yield models that are biologically consistent. In this work, we show that a natural motivation for the use of squared ﬁlter responses can be de- rived from the perspective of encoding relationships between images. In particular, we show that the emergence of group structure and topography follows automatically from the computation of binocular disparity or motion, if we assume that neurons that are nearby in space exert multiplica- tive inﬂuences on each other. Our work is based on the close relationship between the well-known “energy models” of motion and binocularity [1, 17] and the equivalent “cross-correlation” models [3, 14]. It may help shed light onto the close relationship between topographic organization and motion processing as well as binocular vision. That way, it may also help shed light onto the phe- nomenon that topographic ﬁlter maps do not seem to be present in rodents [20].  2 Factored Gated Boltzmann Machine  While feature learning has been applied predominantly to single, static images in the past, there has been an increasing interest recently in learning features to encode relationships between multiple images, for example, to encode motion (eg., [11, 22]). We focus in this work on the Gated Boltzmann Machine (GBM) [15, 22] which models the relationship between two binary images x and y using the three-way energy function  E(x, y, h) =  wijkxiyjhk  (1)  (cid:88)  ijk  The energy gets exponentiated and normalized to deﬁne the probability over image pairs (we drop any bias-terms here to avoid clutter):  (cid:88)  exp(cid:0)E(x, y, h)(cid:1), Z =  (cid:88)  exp(cid:0)E(x, y, h)(cid:1)  (2)  p(x, y) =  1 Z  h  x,y,h  By adding appropriate penalty terms to the log-likelihood, one can extend the model to learn real- valued images [15]. Since there is a product involving every triplet of an input pixel, an output pixel and a mapping unit, the number of parameters is roughly cubic in the number of pixels. To reduce that number, [16] suggested factorizing the three-way parameter tensor W with entries wijk in Equation 1 into a three-way inner product:  (cid:88)  wijk =  wx  if wy  jf wh  kf  (3)  f  Here, f is a latent dimension that has to be chosen by hand or by cross-validation. This form of tensor factorization is also known as PARAFAC or “canonical decomposition” in the literature, and it can be viewed as a three-way generalization of the SVD [2]. It is illustrated in Figure 1 (left). The factorization makes use of a diagonal core tensor, that contains ones along its diagonal and that is zero elsewhere. Plugging in the factorized representation for W and using the distributive law yields  2  Figure 1: Factorizing the parameter tensor of a Gated Boltzmann Machine (left) is equivalent to gating ﬁlter responses instead of raw pixels (middle). In a group-gating model (right), ﬁlters are grouped, and products of all ﬁlters within a group are passed on to the mapping units. In the example, there are two groups, resulting in four unique products per group.  E =  (cid:88)  (cid:0)(cid:88)  (cid:1)(cid:0)(cid:88)  the factorized energy function:  (cid:1) are independent, given the data [16]. More speciﬁcally we have p(h|x, y) =(cid:81) (cid:1)(cid:1),  Inferring the transformation h, given two images, x and y, is efﬁcient, because the hidden variables k p(hk|x, y) with (5)  p(hk|x, y) = σ(cid:0)(cid:88)  (cid:1)(cid:0)(cid:88)  (cid:0)(cid:88)  (cid:1)(cid:0)(cid:88)  kf hk  wh kf  jf yj  jf yj  if xi  if xi  wy  wy  wh  wx  wx  (4)  k  f  j  i  f  i  j  1  1+exp(−z). Thus, to perform inference, images are projected where σ is the logistic sigmoid σ(z) = onto F basis functions (“ﬁlters”) and those basis functions that are in correspondence (i.e., that have the same index f) are multiplied. Finally, each hidden unit, hk, receives a weighted sum over all products as input. An illustration is shown in Figure 1 (middle, right). It is important to note that projections onto ﬁlters typically do not serve to reduce the dimensionality of inputs. Rather, it is the restricted connectivity in the projected space that leads to the reduction in the number of parameters. In fact, it is not uncommon to use a number of factors that is larger than the dimensionality of the input data (for example, [16, 19]). To train the model, one can use contrastive divergence [16], score-matching or a variety of other approximations. Recently, [21, 13] showed that one may equivalently add a decoder network, effec- tively turning the model into a “gated” version of a de-noising auto-encoder [24] and train it using back-prop. Inference is then still the same as in a Boltzmann machine. We use this approach in most of our experiments, after verifying that the performance is similar to contrastive divergence. More speciﬁcally, for a training image pair (x, y), let ph(x, y) denote the vector of inferred hidden probabilities (Eq. 5) and let Wx, Wy, Wh denote the matrices containing the input ﬁlters, output ﬁlters, and hidden ﬁlters, respectively (stacked column-wise). Training now amounts to minimizing  the average reconstruction error(cid:0)y − ˆy(x)(cid:1)2 (cid:16)(cid:16)  +(cid:0)x − ˆx(y)(cid:1)2, with (cid:16)  (cid:17)  ˆy(x) = Wy  (6) where ∗ denotes element-wise multiplication. ˆx(y) is deﬁned analogously, with all occurrences of x and y exchanged. One may add noise to the data during training (but reconstruct the original, not noisy, data to compute the cost) [24]. We observed that this helps localize ﬁlters on natural video, but on synthetic data (shifts and rotations) it is possible to train without noise.  h ph(x, y)  W T  W T  x x  ∗  ,  (cid:17)(cid:17)  2.1 Phase usage in modeling transformations  Figure 2 (left, top) shows ﬁlter-pairs that were learned from translated random-dot images using a Factored Gated Boltzmann Machine (for details on training, see Section 4 below). Training on translations turns ﬁlters into Fourier components, as was initially observed by [16]. The left-bottom plot shows histograms over the occurrence of frequencies and orientations for input- and output- image ﬁlters, respectively. These were generated by ﬁrst performing a 2D-DFT on each ﬁlter and  3  xyhxyh···Figure 2: Left, top: Filter pairs learned from translated random-dot images. (Input ﬁlters on the left, output ﬁlters on the right). Left, bottom: Corresponding histograms showing the number of ﬁlters per frequency/orientation bin. The size of each blob is proportional to the number of ﬁlters in each bin. Right: Phase differences between learned ﬁlters. Each row corresponds to one frequency/orientation bin. Colors are used to simplify visibility.  then picking the frequency and orientation of the strongest component for the ﬁlter. The histograms show that the learned ﬁlters evenly cover the space of frequencies and orientations. This is to be expected, as all frequencies and orientations contribute equally to the set of random translations (e.g., [4]). It is also well-known, however, that multiple different translations will affect each frequency and orientation differently. More speciﬁcally, any given translation induces a set of phase-shifts for every component of a given frequency and orientation. The phase-shift depends linearly on frequency [4]. Likewise, two different image translations of the same orientation will induce two different phase-shifts for each frequency/orientation. In order to represent translations, it is necessary to specify the phase at every frequency and orientation. This shows that mapping units in a GBM must have access to multiple different phase-shifts at every frequency and orientation to be able to represent translations. Also, there is no need to multiply ﬁlter responses of different frequencies and/or orientations, only of different phases. As a repercussion, for each input ﬁlter in Figure 2 (left, top) there need to be multiple phase-shifted copies present in the output-ﬁlter bank, so that ﬁlters with varying phase differences can be matched (which means the ﬁlters’ responses will be multiplied with each other). Likewise for each output ﬁlter. When the number of factors is small, the model has to ﬁnd a compromise, for example, by connecting hidden variables, hk, to multiple phase-shifted versions of ﬁlters with the correct phase- shift but with only a similar, but not the same frequency and orientation. Figure 2 (right) depicts the occurrences of phase differences between input- and output-image ﬁlters. It shows that the model learned to use a variety of phase differences at each frequency/orientation to represent the training transformations. In the model, each phase difference corresponds to exactly one ﬁlter pair. The analysis generalizes to other transformations, such as rotations or natural videos (e.g., [14]). In particular, it also generalizes to Gabor features which are localized Fourier features that emerge when training the GBM on natural video (e.g., [22]).  3 Group gating  The analysis in the previous section strongly suggests using a richer connectivity that supports the re-use of ﬁlters. In this case the model can learn to match any ﬁlter wx f with multiple phase-shifted copies wy f of itself rather than with a single one. All phase differences in Figure 2 (right) can then in principle be obtained using a much smaller number of ﬁlters.  4  One way to support the re-use of ﬁlters to represent multiple phase-shifts is by relaxing the diagonal factorization (Eq. 3) with a factorization that allows for a richer connectivity:  wijk =  Cdef wx  idwy  jewz kf  (7)  (cid:88)  def  where we Cdef are the components of a (non-diagonal) core tensor C. Note that, if C is diagonal so that Cdef = 1 iff d = e = f, we would recover the PARAFAC factorization (Eq. 3). The energy function now turns into (cf. Eq. 10):  E =  Cdef  (cid:88)  (cid:0)(cid:88) p(hk|x, y) = σ(cid:0)(cid:88)  def  i  def  wx  idxi  wy  jeyj  wh  kf hk  (cid:1)(cid:0)(cid:88) (cid:0)(cid:88)  j  (cid:1)(cid:0)(cid:88) (cid:1)(cid:0)(cid:88)  k  (cid:1) (cid:1)(cid:1)  Cdef wh kf  wx  idxi  wy  jeyj  i  j  and inference into  (8)  (9)  As the number of factors is typically large, a full matrix C would be computationally too expensive. In fact, as we discuss in Section 2 there is no other reason to project onto ﬁlters than reducing connectivity in the projected representation. Also, by the discussion in the previous section, there is very a strong inductive bias towards allowing groups of factors to interact. This suggests using a core-tensor that allows features to come in groups of a ﬁxed, small size, such that all pairs of ﬁlters within a group can provide products to mapping units. By the analysis in Section 2, training on translations or natural video is then likely to yield groups of roughly constant frequency and orientation and to differ with respect to phase. We shall refer to this model as “group- gating” model in the following. As the values Cdef may be absorbed into the factor matrices and are learned from data, it is sufﬁcient to distinguish only between non-zero and zero entries in C, and we set all non-zero entries to one in what follows. By deﬁning the ﬁlter groups Gg, g = 1, . . . , G, we can write inference in the model consequently as (10)  p(hk|x, y) = σ(cid:0)(cid:88)  (cid:1)(cid:0)(cid:88)  (cid:0)(cid:88)  (cid:88)  (cid:88)  k,d·|Gg|+e  (cid:1)(cid:1)  jeyj  idxi  wy  wh  wx  i  j  g  d∈Gg  e∈Gg  which is illustrated in Figure 1 (right). Note that each hidden unit can still pool over all pair-wise products between features. The overall number of feature products is equal to the number of groups times the group size. In practice, it makes sense to set the number of factors to be a multiple of the group size, so that all groups can have the same size. A convenient way to implement the group-gating model is by computing all required products by creating G copies of the input-factor matrix and G copies of the output-factor matrix and permuting the columns (factors) of one of the two matrices appropriately. (Note in particular that when using a large number of factors, masking, i.e. forcing a subset of entries of C to be zero, would not be feasible.) It is possible to reduce the number of ﬁlters further by allowing for multiplicative interactions between only one ﬁlter per group from the input image and all ﬁlters from the output image (or vice versa). This leads to an asymmetric model, where the number of ﬁlters is not equal for both images.  3.1 Signiﬁcance for square-pooling models  It is interesting to note that the same analysis applies to the responses of energy-model complex cells (e.g., [7, 19]), too, if images and ﬁlters are contrast normalized [14]. In this case, the response of the energy model is the same as a factored GBM mapping unit applied to a single image, i.e. x = y (see, for example, [19, 14]). This shows that the gating interpretation of frequency/orientation groups and topographic structure applies to these models, too. In the same way, we can interpret a square-pooling model applied to a single image as an encod- ing of the relationship between, say, the rows or the columns within the patch. In natural images, the predominant transformation that takes a row to the next row is a local translation. The emer- gence of oriented Gabor features can therefore also be viewed as the result of modeling these local translations.  5  Figure 3: Classiﬁcation accuracy of a Gated Boltzmann Machine vs. a cRBM on shifts (left) and rotations (right). A total of 25 different sets of parameters were tested, with the numbers of ﬁlters and mapping units being varied between parameter sets.  4 Experiments  Cross-correlation models vs. energy models: To get some quantitative insights into the rela- tionship between cross-correlation and energy models, we compared the standard Gated Boltzmann Machine with a square-pooling GBM [e.g. 19, 13] trained on the concatenation of the images. We used the task of classifying transformations from the mapping units, using transformations for which we can control the ground truth. The models are trained on image patches of size 13 × 13 pixels, which are cropped from larger images to ensure that no border artifacts are introduced. Training-, validation- and test-data contain 100.000 cases each. We use logistic regression to classify transfor- mations from mapping units, where we determine the optimal number of mapping units and factors, as well as the learning rates for the transformation models on the validation data. Using random images ensures that single images contain no information about the transformation; in other words, a single image cannot be used to predict the transformation. We compare the models on translations and rotations. Shifts are drawn uniform-randomly in the range [−3, 3] pixels. We used four different labels corresponding to the four quadrants of motion direction. Rotations are drawn from a von Mises distribution (range [−π, π] rad), which we scaled down to a maximal angle of 36◦. We used 10 labels by equally dividing the set of rotation angles. The results are shown in Figure 3 and they demonstrate that both types of model do a reasonably good job at prediction the transformations from the image pairs. The experiment veriﬁes the approx- imate equivalence of the two types of model derived, for example, in [3, 14]. The cross-correlation model does show slightly better performance than the energy model for large data-set sizes, and the difference gets more pronounced as the training dataset size is decreased, which is also in line with the theory. Energy models have been the standard approach to feature grouping in the past [8]. Learning simple transformations: We trained the group-gating model with group size 3, 128 mapping units and 392 ﬁlters on translating random-dot images of size 13 × 13 pixels. Figure 4 shows three pairs of plots, where the left, center and right pair depict the dominant frequencies, orientations and phases of the ﬁlters, respectively. We extracted these from the ﬁlters using FFT. Each pair shows properties of input (top) and output ﬁlters (bottom). Within an image, each ﬁlter is represented by a single uni-colored square. For frequency plots, the squares are in gray-scale with the brightness corresponding to the frequency of the ﬁlter (black represents frequency zero, white is the highest frequency across all ﬁlters); in the orientation and phase plots, the squares differ in color, where the angle determines the color according to the HSV representation of RGB color space. The ﬁgures conﬁrm that ﬁlter-groups tend to be homogeneous with respect to frequency and orientation1. Contrary to that, phases differ within each group, as expected. The same can be seen in Figure 5 (left and middle), which shows subsets of ﬁlter groups learned from translations and rotations of random images, using patchsize 25× 25 and group size 5. 1In some groups, there appear to be some outliers, whose frequency or orientation does not match the other ﬁlters in the group. These are typically the result of spurious maxima in the FFT amplitude due to small irregularities in the ﬁlters.  6  Figure 4: Frequency (left), orientation (middle) and phase (right) of ﬁlters trained on translated image pairs, with ﬁlters applied to the input images in the top and ﬁlters for the output image on the bottom. A group size of 3 is used here, which means each three consecutive input ﬁlters and their corresponding output ﬁlters form a group. Notice that frequency and orientation are often identical for all ﬁlters within a group, whereas the phase varies. Best viewed in color.  Figure 5: Filter-groups learned from translations (left) and rotations (middle) using a group-gating model with group size 5. Each group corresponds to one column in the plot. Right: Filter-groups learned from natural videos, using a group-gating model with group size 4. Only output ﬁlters are shown. Frequency, orientation and position are constant, while phase varies, within each group.  Learning transformation features from natural videos: Figure 5 (right) shows a subset of ﬁlter groups learned from about one million patches of size 12 × 12 pixels that we crop from the van Hateren broadcast television database [23]. We trained the asymmetric model to predict each frame from its predecessor. All patches were PCA-whitened, retaining 95% of the variance. We trained a model with 128 mapping units and 256 ﬁlters, and we used a group size of 4. The ﬁgure shows that the model learns Gabor features, where frequency, orientation and position are nearly constant within groups, and the set of possible phases is covered relatively evenly per group. Quantitative results for group-gating: Classiﬁcation accuracies for the same model and data as described in Section 4 are reported in Table 1. The equivalent number of group-gating ﬁlters is shown in parentheses, where equivalence means that the group-gating model has the same num- ber of parameters in total as the factored GBM. This makes it possible to obtain a comparison that is fair in terms of pure parameter count by comparing performance across the columns of the tables. However, the table also shows that even along columns, the group-gating model robustly outperforms the factored GBM, except when using a very small number of factors. In this case, the parameter-equivalent number of 44 factors is probably too small to represent the transformations with a sufﬁciently high resolution.  4.1 Topographic feature maps from local gating  We also experimented with overlapping group structures by letting Gg share ﬁlters. This makes it necessary to deﬁne which ﬁlters are shared among which groups. A convenient approach is to lay out all features (“simple cells”) in a low-dimensional space and to deﬁne groups over all those units which reside within some pre-deﬁned neighborhoods, such as in a grid of size n× n units (e.g., [8]). Figure 6 shows the features learned from the van Hateren data (cf., Section 4) with patchsize 16×16 and 99.9% variance retained after whitening. We used 400 ﬁlters that we arranged in a 2-D grid (with wrap-around) using a group size of 5 × 5. We found that learning is simpliﬁed when we also  7  TRANSLATION  GROUP GATING  NUMBER OF FILTERS  81 (44) 225 (121) 441 (237) 729 (392) 900 (483)  GATING 89.69% 92.40% 92.20% 91.44% 91.29%  81.68% 92.59% 93.36% 93.46% 93.20%  GATING 90.08% 91.50% 90.76% 91.71% 89.95%  ROTATION  GROUP GATING  89.06% 94.65% 95.65% 95.51% 95.00%  Table 1: Classiﬁcation accuracy of the standard and group-gating models on image pairs showing translations and rotations.  Figure 6: Topographic ﬁlter maps learned by laying out features in two dimensions and allowing for multiplicative interactions among nearby units. Left: Input ﬁlters, Right: Output ﬁlters.  initialize the factor-to-mapping parameters so that each mapping unit has access to the ﬁlters of only one group. The ﬁgure shows how learned ﬁlters are arranged in two topographic feature maps with slowly varying frequency and orientation within each map. It also shows how low-frequency ﬁlters have the tendency of being grouped together [8]. From the view of phase analysis (see Section 2), the topographic organization is a natural result of imposing an additional constraint: Filter sets that come in similar frequency, orientation and position are now forced to share coefﬁcients with those of neighboring groups. The simplest way to satisfy this constraint is by having nearby groups be similar with respect to the properties they share. The apparent randomness of phase simply follows from the requirement that multiple phases be present within each neighborhood. It is interesting to note that using shared group structure is equivalent to letting units that are nearby in space affect each other multiplicatively. Localized gating may provide a somewhat more plausible explanation for the emergence of pinwheel-structures than squaring non-linearities, which are used, for example, in subspace models or topographic ICA (see [6, 7]).  5 Conclusions  Energy mechanisms and “square-pooling” are common approaches to modeling feature dependen- cies in sparse coding, and to learn group-structured or invariant dictionaries. In this work we re- visited group-structured sparse coding from the perspective of learning image motion and disparity from local multiplicative interactions. Our work shows that the commonly observed constancy of frequency and orientation of ﬁlter-groups in energy models can be explained as a result of rep- resenting transformations with local, multiplicative feature gating. Furthermore, topographically  8  structured representations (“pinwheels”) can emerge naturally as the result of binocular or spatio- temporal learning that utilizes spatially constrained multiplicative interactions. Our work may pro- vide some support for the claim that localized multiplicative interactions are a biologically plausible alternative to square pooling for implementing stereopsis and motion analysis [12]. It may also help explain why the development of pinwheels in V1 may be tied the presence of binocular vision and why topographic organization of features does not appear to occur, for example, in rodents [20].  References [1] E.H. Adelson and J.R. Bergen. Spatiotemporal energy models for the perception of motion. J. Opt. Soc.  Am. A, 2(2):284–299, 1985.  [2] J. Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling via  an N-way generalization of Eckart-Young decomposition. Psychometrika, 35(3), 1970.  [3] D. Fleet, H. Wagner, and D. Heeger. Neural encoding of binocular disparity: Energy models, position  shifts and phase shifts. Vision Research, 36(12):1839–1857, June 1996.  [4] Rafael C. Gonzalez and Richard E. Woods. Digital Image Processing (3rd Edition). Prentice Hall, 3  edition, August 2007.  [5] Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Compu-  tation, 14(8):1771–1800, 2002.  [6] Aapo Hyv¨arinen and Patrik Hoyer. Emergence of phase- and shift-invariant features by decomposition of  natural images into independent feature subspaces. Neural Computation, 12:1705–1720, July 2000.  [7] Aapo Hyv¨arinen, Patrik O. Hoyer, and Mika Inki. Topographic ICA as a model of natural image statistics.  In Biologically Motivated Computer Vision. Springer, 2000.  [8] Aapo Hyv¨arinen, J. Hurri, and Patrik O. Hoyer. Natural Image Statistics: A Probabilistic Approach to  Early Computational Vision. Springer Verlag, 2009.  [9] Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, and Francis Bach. Proximal methods for sparse  hierarchical dictionary learning. In ICML, 2010.  [10] Koray Kavukcuoglu, Marc’Aurelio Ranzato, Rob Fergus, and Yann LeCun. Learning invariant features  through topographic ﬁlter maps. In CVPR 2009. IEEE, 2009.  [11] Q.V. Le, W.Y. Zou, S.Y. Yeung, and A.Y. Ng. Learning hierarchical spatio-temporal features for action  recognition with independent subspace analysis. In CVPR 2011, 2011.  [12] Bartlett W. Mel, Daniel L. Ruderman, and Kevin A. Archie. Toward a single-cell account for binocular  disparity tuning: an energy model may be hiding in your dendrites. In NIPS 1997.  [13] Roland Memisevic. Gradient-based learning of higher-order image features. In ICCV 2011, 2011. [14] Roland Memisevic. On multi-view feature learning. In ICML 2012, 2012. [15] Roland Memisevic and Geoffrey Hinton. Unsupervised learning of image transformations.  In CVPR  2007, 2007.  [16] Roland Memisevic and Geoffrey E Hinton. Learning to represent spatial transformations with factored  higher-order Boltzmann machines. Neural Computation, 22(6):1473–92, 2010.  [17] Izumi Ohzawa, Gregory C. Deangelis, and Ralph D. Freeman. Stereoscopic Depth Discrimination in the  Visual Cortex: Neurons Ideally Suited as Disparity Detectors. Science, 249(4972):1037–1041, 1990.  [18] B. Olshausen and D. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code  for natural images. Nature, 381(6583):607–609, June 1996.  [19] Marc’Aurelio Ranzato and Geoffrey E. Hinton. Modeling Pixel Means and Covariances Using Factorized  Third-Order Boltzmann Machines. In CVPR 2010, 2010.  [20] CF Stevens. A universal design principle for visual system pinwheels. Brain, behavior and evolution,  77(3):132, 2011.  [21] Kevin Swersky, Marc’Aurelio Ranzato, David Buchman, Benjamin Marlin, and Nando Freitas. On au-  toencoders and score matching for energy based models. In ICML 2011, 2011.  [22] Graham, W. Taylor, Rob Fergus, Yann LeCun, and Christoph Bregler. Convolutional learning of spatio-  temporal features. In ECCV 2010, 2010.  [23] L. van Hateren and J. Ruderman.  Independent component analysis of natural image sequences yields spatio-temporal ﬁlters similar to simple cells in primary visual cortex. Proc. Biological Sciences, 265(1412):2315–2320, 1998.  [24] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and compos-  ing robust features with denoising autoencoders. In ICML 2008, 2008.  9  ","We present a feature learning model that learns to encode relationshipsbetween images. The model is defined as a Gated Boltzmann Machine, which isconstrained such that hidden units that are nearby in space can gate eachother's connections. We show how frequency/orientation ""columns"" as well astopographic filter maps follow naturally from training the model on imagepairs. The model also helps explain why square-pooling models yield featuregroups with similar grouping properties. Experimental results on syntheticimage transformations show that spatially constrained gating is an effectiveway to reduce the number of parameters and thereby to regularize atransformation-learning model."
1301.3775,2013,Discriminative Recurrent Sparse Auto-Encoders  ,"['Jason Rolfe', 'Yann LeCun']",https://arxiv.org/pdf/1301.3775.pdf,"3 1 0 2    r a     M 9 1      ]  G L . s c [      4 v 5 7 7 3  .  1 0 3 1 : v i X r a  Discriminative Recurrent Sparse Auto-Encoders  Jason Tyler Rolfe & Yann LeCun  Courant Institute of Mathematical Sciences, New York University  719 Broadway, 12th Floor  New York, NY 10003  {rolfe, yann}@cs.nyu.edu  Abstract  We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectiﬁed linear units, unrolled for a ﬁxed number of itera- tions, and connected to two linear decoders that reconstruct the input and predict its supervised classiﬁcation. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classiﬁcation. The depth implicit in the temporally-unrolled form allows the system to exhibit far more representational power, while keeping the number of trainable parameters ﬁxed. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-deﬁned class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven di- rectly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.  1  Introduction  Deep networks complement the hierarchical structure in natural data (Bengio, 2009). By breaking complex calculations into many steps, deep networks can gradually build up complicated decision boundaries or input transformations, facilitate the reuse of common substructure, and explicitly com- pare alternative interpretations of ambiguous input (Lee, Ekanadham, & Ng, 2008; Zeiler, Taylor, & Fergus, 2011). Leveraging these strengths, deep networks have facilitated signiﬁcant advances in solving sensory problems like visual classiﬁcation and speech recognition (Dahl, et al., 2012; Hinton, Osindero, & Teh, 2006; Hinton, et al., 2012). Although deep networks have traditionally used independent parameters for each layer, they are equivalent to recurrent networks in which a disjoint set of units is active on each time step. The corresponding representations are sparse, and thus invite the incorporation of powerful techniques from sparse coding (Glorot, Bordes, & Bengio, 2011; Lee, Ekanadham, & Ng, 2008; Olshausen & Field, 1996, 1997; Ranzato, et al., 2006). Recurrence opens the possibility of sharing parameters between successive layers of a deep network. This paper introduces the Discriminative Recurrent Sparse Auto-Encoder model (DrSAE), compris- ing a recurrent encoder of rectiﬁed linear units (ReLU; Coates & Ng, 2011; Glorot, Bordes, & Bengio, 2011; Jarrett, et al., 2009; Nair & Hinton, 2010; Salinas & Abbott, 1996), connected to two linear decoders that reconstruct the input and predict its supervised classiﬁcation. The recurrent en- coder is unrolled in time for a ﬁxed number of iterations, with the input projecting to each resulting layer, and trained using backpropagation-through-time (Rumelhart, et al., 1986). Training initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a  1  discriminative term on the supervised classiﬁcation. In its temporally-unrolled form, the network can be seen as a deep network, with parameters shared between the hidden layers. The temporal depth allows the system to exhibit far more representational power, while keeping the number of trainable parameters ﬁxed. Interestingly, experiments show that DrSAE does not just discover more discriminative “parts” of the form conventionally produced by sparse coding. Rather, the hidden units spontaneously dif- ferentiate into two types: a small number of categorical-units and a larger number of part-units. The categorical-units have decoder bases that look like prototypes of the input classes. They are weakly inﬂuenced by the input and activate late in the dynamics as the result of interaction with the part-units. In contrast, the part-units are strongly inﬂuenced by the input, and encode small trans- formations through which the prototypes of categorical-units can be reshaped into the current input. Categorical-units compete with each other through mutual inhibition and cooperate with relevant part-units. This can be interpreted as a representation of the data manifold in which the categorical- units are points on the manifold, and the part-units are akin to tangent vectors along the manifold.  1.1 Prior work  The encoder architecture of DrSAE is modeled after the Iterative Shrinkage and Threshold Algo- rithm (ISTA), a proximal method for sparse coding (Chambolle, et al., 1998; Daubechies, Defrise, & De Mol, 2004). Gregor & LeCun (2010) showed that the sparse representations computed by ISTA can be efﬁciently approximated by a structurally similar encoder with a less restrictive, learned pa- rameterization. Rather than learn to approximate a precomputed optimal sparse code, the LISTA au- toencoders of Sprechmann, Bronstein, & Sapiro (2012a,b) are trained to directly minimize the sparse reconstruction loss function. DrSAE extends LISTA autoencoders with a non-negativity constraint, which converts the shrink nonlinearity of LISTA into a rectiﬁed linear operator; and introduces a uni- ﬁed classiﬁcation loss, as previously used in conjunction with traditional sparse coders (Bradley & Bagnell, 2008; Mairal, et al., 2009; Mairal, Bach, & Ponce, 2012) and other autoencoders (Boureau, et al., 2010; Ranzato & Szummer, 2008). DrSAEs resemble the structure of deep sparse rectiﬁer neural networks (Glorot, Bordes, & Ben- gio, 2011), but differ in that the parameter matrices at each layer are tied (Bengio, Boulanger- Lewandowski, & Pascanu, 2012), the input projects to all layers, and the outputs are normalized. DrSAEs are also reminiscent of the recurrent neural networks investigated by Bengio & Gingras (1996), but use a different nonlinearity and a heavily regularized loss function. Finally, they are simi- lar to the recurrent networks described by Seung (1998), but have recurrent connections amongst the hidden units, rather than between the hidden units and the input units, and introduce classiﬁcation and sparsiﬁcation losses.  2 Network architecture  In the following, we use lower-case bold letters to denote vectors, upper-case bold letters to denote matrices, superscripts to indicate iterative copies of a vector, and subscripts to index the columns (or rows, if explicitly speciﬁed by the context) of a matrix or (without boldface) the elements of a vector. We consider discriminative recurrent sparse auto-encoders (DrSAEs) of rectiﬁed linear units with the architecture shown in ﬁgure 1:  zt+1 = max(cid:0)0, E · x + S · zt − b(cid:1)  for t = 1, . . . , T , where n-dimensional vector zt is the activity of the hidden units at iteration t, m- dimensional vector x is the input, and zt=0 = 0. Unlike traditional recurrent autoencoders (Bengio, Boulanger-Lewandowski, & Pascanu, 2012), the input projects to every iteration. We call the n× m parameter matrix E the encoding matrix, and the n × n parameter matrix S the explaining-away matrix. The n-element parameter vector b contains a bias term. The parameters also include the m × n decoding matrix D and the l × n classiﬁcation matrix C. We pretrain DrSAEs using stochastic gradient descent on the unsupervised loss function  (1)  (2)  ·(cid:12)(cid:12)(cid:12)(cid:12)x − D · zT(cid:12)(cid:12)(cid:12)(cid:12)2  2 + λ ·(cid:12)(cid:12)(cid:12)(cid:12)zT(cid:12)(cid:12)(cid:12)(cid:12)1 ,  LU =  1 2  2  Figure 1: The discriminative recurrent sparse auto-encoder (DrSAE) architecture. zt is the hidden representation after iteration t of T , and is initialized to z0 = 0; x is the input; and y is the supervised classiﬁcation. Overbars denote approximations produced by the network, rather than the true input. E, S, D, and b are learned parameters.  (cid:18)  (cid:19)  C · zT ||zT||  ,  (cid:33)  ezi  ,  (cid:32)(cid:88)  with the magnitude of the columns of D bounded by 1,1 and the magnitude of the rows of E bounded by 1.25  T .2 We then add in the supervised classiﬁcation loss function  LS = logisticy  (3)  where the multinomial logistic loss function is deﬁned by  logisticy(z) = zy − log  i  and y is the index of the desired class.3 Starting with the parameters learned by the unsupervised pretraining, we perform discriminative ﬁne-tune by stochastic gradient descent on LU + LS, with the magnitude of the rows of C bounded by 5.4 The learning rate of each matrix is scaled down by the number of times it is repeated in the network, and the learning rate of the classiﬁcation matrix is scaled down by a factor of 5, to keep the effective learning rate consistent amongst the parameter matrices. We train DrSAEs with T = 11 recurrent iterations (ten nontrivial passes through the explaining- away matrix S)5 and 400 hidden units on the MNIST dataset of 28× 28 grayscale handwritten digits (LeCun, et al., 1998), with each input normalized to have (cid:96)2 magnitude equal to 1. We use a training set of 50,000 elements, and a validation set of 10,000 elements to perform early-stopping. Encoding, decoding, and classiﬁcation matrices learned via this procedure are depicted in ﬁgure 2. The dynamics of equation 1 are inspired by the Learned Iterative Shrinkage and Thresholding Al- gorithm (LISTA) (Gregor & LeCun, 2010), an efﬁcient approximation to the sparse coding Iterative Shrinkage and Threshold Algorithm (ISTA) (Chambolle, et al., 1998; Daubechies, Defrise, & De  1This sets the scale of z; otherwise, the magnitude of z will shrink to zero and the magnitude of the columns  of D will explode. This and all other such constraints are enforced by a projection after each SGD step.  2The size of each ISTA step must be sufﬁciently small to guarantee convergence. As the step size grows large, the input will be over-explained by multiple aligned hidden units, leading to extreme oscillations. This bound serves the same function as (cid:96)2 weight regularization (Hinton, 2010). The particular value of the bound is heuristic, and was determined by an informal search of parameter space.  3Consistent with standard autoencoders but unlike traditional applications of backpropagation-through-time,  the loss functions LU and LS only depend directly on the ﬁnal iteration of the hidden units zT .  4As in the case of the encoder, this serves the same function as (cid:96)2 weight regularization (Hinton, 2010). The  particular value of the bound is heuristic, and was determined by an informal search of parameter space. computational expense. Experiments were conducted with T ∈ {2, 6, 11, 21}.  5The chosen number of recurrent iterations achieves a heuristic balance between representational power and  3  D̄xC̄yDecoding matrixClassification matrixxyzTL1 loss0z/∥z∥L2 lossNormalizationLogistic lossEncoding matrix−bS+()+Ex0ztRepeated T timesExplaining awayRectification(a) Enc  (b) Dec  (c) Clas  Figure 2: The hidden units differentiate into spatially localized part-units, which have well-aligned encoders and decoders; and global prototype categorical-units, which have poorly aligned encoders and decoders. A subset of the rows of encoding matrix E (a) and the columns of decoding ma- trix D (b), and all rows of the classiﬁcation matrix C (c) after training. The ﬁrst row of (a,b) shows the most categorical units; the last row contains the least categorical units; and the middle row evenly steps through the remaining units in order of categoricalness. Gray pixels denote connections with weight 0; darker pixels indicate more positive connections.  Mol, 2004). ISTA is an algorithm for minimizing the (cid:96)1-regularized reconstruction loss function LU of equation 2 with respect to zT . It is deﬁned by the iterative step  (cid:0)α · D(cid:62) · x +(cid:0)I − α · D(cid:62) · D(cid:1) · zt(cid:1) ,  zt+1 = hα·λ  where [hθ(x)]i = sign (xi) · max (0,|xi| − θ) and α is a small step-size parameter. With non- negative units, ISTA is equivalent to projected gradient descent of LU of equation 2. As the number of iterations T → ∞, a DrSAE deﬁned by equation 1 becomes a non-negative version of ISTA if it satisﬁes the restrictions: E = α · D(cid:62),  (4) where the positive scale factor α is less than the maximal eigenvalue of D(cid:62) · D, and I is the n × n identity matrix. As in LISTA, but unlike ISTA, the encoding matrix E and explaining-away matrix S in a DrSAE are independent of the decoding matrix D. Connections from the input to the hidden units, and recurrent connections between the hidden units, are all-to-all, so the network structure is agnostic to permutations of the input. DrSAEs can also be understood as deep, feedforward networks with the parameter matrices tied between the layers.  S = I − α · D(cid:62) · D ,  bi = α · λ ,  i ≥ 0 , zt  and  3 Analysis of the hidden unit representation  Discriminative ﬁne-tuning naturally induces the hidden units of a DrSAE to differentiate into a hierarchy-like continuum. On one extreme are part-units, which perform an ISTA-like sparse cod- ing computation; on the other are categorical-units, which use a sophisticated form of pooling to integrate over matching part-units, and implement winner-take-all dynamics amongst themselves. Converging lines of evidence indicate that these two groups use distinct computational mechanisms and serve different representational roles. In the ISTA algorithm, each row of the encoding matrix Ei (which we sometimes call the encoder of unit i) is proportional to the corresponding column of the decoding matrix Di (which we call the (cid:62) · D, as in equation 4. As a result, decoder of unit i), and each row (S − I)i is proportional to (Di) the angle between Ei and Di, and the angle between the rows of S − I and D(cid:62) · D, are both simple measures of the degree to which a unit’s dynamics follow the ISTA algorithm, and thus perform  4  11 enc iters, no discriminative training  2 enc iters, no discriminative training  (b)  11 enc iters, with discriminative training  2 enc iters, with discriminative training  (d)  (f)  (a)  (c)  (e)  Figure 3: The hidden units differentiate into two populations after discriminative ﬁne-tuning. The magnitude of row (S − I)i (a,b,e) and Ci (c,d), versus the angle between encoder row and decoder column, for each unit from networks using 11 (a,c,e) and 2 (b,d,f) iterations. All plots are from discriminatively ﬁne-tuned networks except (a,b), which are only subject to unsupervised pretrain- ing. We call the dense cloud in the bottom-left part-units, and the tail extending to the top-right categorical-units.  sparse coding.6 These quantities are equal to 0 in the case of perfect ISTA, and grow larger as the network diverges from ISTA. Of these two angles, the explaining-away matrix comparison is more difﬁcult to interpret, since a distortion of any one unit’s decoding column Di will affect all rows of D(cid:62) · D, whereas the angle between the encoder row Ei and decoder column Di only depends upon a single unit. For this reason, we use the angle between the encoder row and decoder column as a measure of the position of each unit on the part/categorical continuum.  6We always use S − I when plotting recurrent connection strength, since it governs the perturbation of the  otherwise stable hidden unit activations, as in projected gradient descent of LU ; i.e., ISTA.  5  Figure 3 plots, for each unit i, the magnitude of row (S − I)i and column Ci, versus the angle between row Ei and column Di. Before discriminative ﬁne-tuning, there are no categorical-units; the angle between the encoder row and decoder column is small and the incoming recurrent con- nections are weak for all units, as in ﬁgure 3(a,b). After discriminative ﬁne-tuning, there remains a dense cloud of points for which the angle between the encoder row and decoder column is very small, and the incoming recurrent and outgoing classiﬁcation connections are weak. Abutting this is an extended tail of points that have a larger angle between the encoder row and decoder column, and stronger incoming recurrent and outgoing classiﬁcation connections. We call units composing the dense cloud part-units, since they have ISTA-compatible connections, while we refer to those making up the extended tail as categorical-units, since they have strong connections to the classiﬁ- cation output.7 When trained on MNIST, part-units have localized, pen stroke-like decoders, as can be seen in the bottom rows of ﬁgure 2(a,b). Categorical-units, in contrast, tend to have whole-digit prototype-like decoders, as in the top rows of ﬁgure 2(a,b). Discriminative ﬁne-tuning induces the differentiation of categorical-units regardless of the depth of the encoder.  3.1 Part-units Examination of the relationship between the elements of S − I and D(cid:62) · D conﬁrms that part- units with an encoder-decoder angle less than 0.5 radians abide by ISTA, and so perform sparse coding on the residual input after the categorical-unit prototypes are subtracted out. The prominent diagonals with matching slopes in ﬁgure 4(a,b), which plot the value of Si,j − δi,j versus Di· Dj for connections between part-units, and from categorical-units to part-units, respectively, demonstrate that part-units receive ISTA-consistent connections from all units. The ﬁdelity of these connections to the ISTA ideal is not strongly dependent upon whether the afferent units are ISTA-compliant part- units, or ISTA-ignoring categorical-units. As a result, the part-units treat the categorical-units as if they were also participating in the reconstruction of the input, and only attempt to reconstruct the residual input not explained by the categorical-unit prototypes. As can be seen in ﬁgure 4(c), the degree to which the encoder conforms to the ISTA algorithm is strongly correlated with the degree to which the explaining-away matrix matches the ISTA al- gorithm. Figure 5 shows the decoders associated with the strongest recurrent connections to three representative part-units. As expected, the decoders of these afferent units tend to be strongly aligned or anti-aligned with their target’s decoder, and include both part-units and categorical-units.  3.2 Categorical-units  In contrast, the recurrent connections to categorical-units with an encoder-decoder angle greater than 0.7 radians are not strongly correlated with the values predicted by ISTA. Rather than analyz- ing connections to the categorical-units only based upon their destination, it is more informative to consider them organized by their source. Part-units are compatible with categorical-units of certain classes,8 and not with others, as shown by ﬁgure 6(a). Part-units generally have positive connec- tions to categorical-units with parallel prototypes, independent of offset, and negative connections to categorical-units with orthogonal prototypes, as shown in ﬁgure 7(a). This corresponds to a so- phisticated form of pooling (Jarrett, et al., 2009), with a single categorical-unit drawing excitation from a large collection of parallel but not necessarily perfectly aligned part-units, as in ﬁgure 6(c). It is also suggestive of the standard Hubel and Wiesel model of complex cells in primary visual cortex (Hubel & Wiesel, 1962). ISTA would instead predict a connection proportional to the inner product, which is zero for orthogonal prototypes and negative for anti-aligned prototypes. Part-units use sparse coding dynamics, and so are not disproportionately suppressed by categorical- units that represent any particular class. However, each part-unit is itself compatible with (i.e., has positive connections to) categorical-units of only a subset of the classes. As a result, the categorical-  7For the purpose of constructing ﬁgures characterizing the difference between part-units and categorical- units, we consider units with encoder-decoder angle less than 0.5 radians to be part-units, and units with encoder-decoder angle greater than 0.7 radians to be categorical-units. These thresholds are heuristic, and fail to reﬂect the continuum that exists between part- and categorical-units, but they facilitate analysis of the extremes.  8Categorical-units have strong, sparse classiﬁcation matrix projections, as shown in ﬁgures 2(c) and 3(e,f),  and can be identiﬁed with the output class to which they have the strongest projection.  6  (b)  (a)  (c)  Figure 4: Part-units have connections consistent with ISTA. The actual connection weights S − I versus the ISTA-predicted weights D(cid:62) · D, for connections from part-units to part-units (a) and categorical-units to part-units (b); and the angle between the rows of S − I and the ISTA-ideal D(cid:62) · D versus the angle between the encoder rows and decoder columns (c). Units are considered part-units if the angle between their encoder and decoder is less than 0.5 radians, and categorical- units if the angle between their encoder and decoder is greater than 0.7 radians.  Dest Source units  Figure 5: Part-units receive ISTA-compatible connections and thus perform sparse coding on the residual input after the contribution of the categorical-units is subtracted out. The decoders of the twenty units with the strongest explaining-away connections |Si,j − δi,j| to three typical part-units, sorted by connection magnitude. The left-most column depicts the decoder of the recipient part- unit. The bars above the decoders in the remaining columns indicate the strength of the connections. Black bars are used for positive connections, and white bars for negative connections.  units and thus the class chosen are determined by the part-unit activations. In particular, only a subset of the possible deformations implemented by part-unit decoders are freely available for each prototype, since part-units with a strong negative connection to a categorical-unit will tend to silence it, and so cannot be used to transform the prototype of that categorical-unit. Categorical-units implement winner-take-all-like dynamics amongst themselves, as shown in ﬁg- ure 6(b), with negative connections to most other categorical-units. Positive total self-connections Si,i facilitate the integration of inputs over time.  7  Src Destination units  (a)  (b)  (c)  Dest Source units  Figure 6: Categorical-units execute a sophisticated form of pooling over part-units, and have winner- take-all dynamics amongst themselves. The decoders of the categorical-units receiving the twenty strongest connections |Si,j − δi,j| from representative part-units (a) and categorical-units (b), and the decoders of the part-units sending the twenty strongest projections to representative categorical- units (c). The connections are sorted ﬁrst by the class of their destination, and then by the magnitude of the connection. The left-most column depicts the decoder of the source (a,b) or destination (c) unit. The bars above the decoders in the remaining columns indicate the strength of the connections. Black bars are used for positive connections, and white bars for negative connections.  When activated, the categorical-units make a much larger contribution to the reconstruction than any single part-unit, as can be seen in ﬁgure 7(b). Since, the projections from categorical-units to part-units are consistent with ISTA, the magnitude of the categorical-unit contribution to the reconstruction need not be tightly regulated. The part-units adjust accordingly to accommodate whatever residual is left by the categorical-units. The units form a rough hierarchy, with part-units on the bottom and categorical-units on the top. Categorical-units receive strong recurrent connections, as shown in ﬁgure 3(c,d) implying that their activity is more determined by other hidden units and less by the input (since the magnitude of the input connections is bounded), and thus they are higher in the hierarchy. As shown in ﬁgure 7(c), part-units receive most of their input from other part-units; categorical-units receive a larger fraction of their input from other categorical-units. Whereas part-units have well-structured encoders and are generally activated directly by the input on the ﬁrst iteration, categorical-units are more likely to ﬁrst achieve a non-zero activation on the second iteration, as shown in ﬁgure 7(d), suggesting that they require stimulation from part-units. The immediate response of part-units in contrast to the gradual reﬁnement of categorical-units is apparent in ﬁgure 8, which shows the optimal decoding matrix for selected units, inferred from their observed activity at each iteration.  4 Performance  The comparison of MNIST classiﬁcation performance in table 1 demonstrates the power of the hi- erarchical representation learned by DrSAEs. Rather than learn to minimize the sum of equations 2 and 3, Gregor & LeCun (2010) train the LISTA encoder to approximate the code generated by a traditional sparse coder. While they do not report classiﬁcation performance using LISTA, Gregor and LeCun do evaluate MNIST classiﬁcation error using the related learned coordinate descent al- gorithm. Sprechmann, Bronstein, & Sapiro (2012a,b) extend this approach by training a LISTA auto-encoder to reconstruct the input directly, using loss functions similar to equation 2. Although they identify the possibility of using regularization dependent upon supervised information, Sprech- mann and colleagues do not consider a parameterized classiﬁer operating on a common hidden  8  (b)  (d)  (a)  (c)  (e)  Figure 7: Statistics of connections indicate the presence of a rough hierarchy, with categorical-units on the top integrating over part-units on the bottom. Average explaining-away connection weight Sij, binned by alignment between decoders, for connections from part-units to categorical-units (a). If no units fall in a given bin, the average is set to zero. Average ﬁnal value of a unit zt=T , given that > 0, versus the angle between the encoder row Ei and decoder column Di (b). Average angle zt=T i between encoder row Ej and decoder column Dj of afferents to unit i, weighted by the strength of the connection to unit i, versus the angle between encoder row Ei and decoder column Di (c). Probability that z1 i > 0, versus the angle between the encoder row Ei and decoder column Di (d). Average value of the decoder column Di versus the angle between the encoder row Ei and the decoder column Di (e).  i = 0 and z2  i  representation. Instead, they train a separate encoder for each class, and classify each input based upon the encoder with the lowest sparse coding error. DrSAEs signiﬁcantly outperform these other techniques based upon a LISTA encoder. DrSAEs also perform well compared to other techniques using encoders related to LISTA. Deep sparse rectiﬁer neural networks (Glorot, Bordes, & Bengio, 2011) combine discriminative training with an encoder similar to LISTA, but do not tie the parameters between the layers and only allow the input to project to the ﬁrst layer. Differentiable sparse coding (Bradley & Bagnell, 2008) and  9  Enc  Optimal inferred decoders  Dec  (a)  (b)  Figure 8: Part-units (a) respond to the input quickly, while the activity of categorical-units (b) reﬁnes slowly. Columns of the optimal decoding matrices Dt minimizing the input reconstruction error ||x − Dt · zt||2 2 from the hidden representation zt for t = 1, . . . , T . The ﬁrst and last columns show the corresponding encoder and decoder for the chosen representative units. Intermediate columns represent successive iterations t.  LISTA auto-encoder, 10 ×(cid:0)289-1005(cid:1)  (Sprechmann, Bronstein, & Sapiro, 2012a) Learned coordinate descent, 784-78450-10 (Gregor & LeCun, 2010) Differentiable sparse coding, 180-256∗-10 (Bradley & Bagnell, 2008) Deep sparse rectiﬁer neural network 784-1000-1000-1000-10 (Glorot, Bordes, & Bengio, 2011) Deep belief network, 784-500-500-2000-10 (Hinton, et al., 2012) Discriminative recurrent sparse auto-encoder 784-40011-10 Supervised dictionary learning, 45 × (784-24∗) to 45 × (784-96∗) (Mairal, et al., 2009)  3.76  (5.98 with 289 hidden units)  2.29  1.30  1.20  (1.16 with tanh nonlinearity)  1.18  (0.92 with dropout)  1.08 (1.21 with 200 hidden units)  1.05  (3.56 without contrastive loss)  Table 1: MNIST classiﬁcation error rate (%) for pixel-permutation-agnostic encoders without boosting-like augmentations. The ﬁrst column indicates the size of each layer in the speciﬁed en- coder, separated by hyphens. Exponents specify the number of recurrent iterations; asterisks denote repetition to convergence. 10× (··· ) indicates that a separate encoder is trained for each input class; 45 × (··· ) indicates that a separate encoder is trained for each pairwise binary classiﬁcation prob- lem. Further performance improvements have been reported with regularization techniques such as dropout, architectures that enforce translation-invariance, and datasets augmented by deformations, as discussed in the main text.  supervised dictionary learning (Mairal, et al., 2009) also train discriminatively, but effectively use an inﬁnite-depth ISTA-like encoder, and are thus much less computationally efﬁcient than DrSAEs. Supervised dictionary learning achieves performance statistically indistinguishable from DrSAEs using a contrastive loss function. A similar technique achieves MNIST classiﬁcation error as low  10  as 0.54% when the dataset is augmented with shifted copies of the inputs (Mairal, Bach, & Ponce, 2012). Additional regularizations and boosting-like techniques can further improve performance of net- works with LISTA-like encoders. Recent examples include dropout, which trains and then averages over a large set of random subnetworks formed by removing a constant fraction of the hidden units from the original network (Goodfellow, et al., 2013; Hinton, et al., 2012). Deep belief networks and deep Boltzmann machines ﬁne-tuned with dropout are the current state-of-the-art for pixel- permutation-agnostic handwritten digit recognition (Hinton, et al., 2012), and can achieve MNIST classiﬁcation error as low as 0.79% with a carefully tuned network structure and multi-step training procedure. Deep convex networks, which iteratively reﬁne the classiﬁcation by successively training a stack of classiﬁers, with the output of the i − 1st classiﬁer provided as input to the ith classiﬁer, can achieve an MNIST error of 0.83% (Deng & Yu, 2011). Regularizing by explicit modeling of the data manifold, and then minimizing the square of the Jacobian of the output along the tangent bun- dle around the training datapoints, can reduce MNIST error to 0.81% (Rifai, et al., 2011). Further performance improvements are possible if translation invariance is built directly into the network via a convolutional architecture, and deformations of the inputs are included in the training set (LeCun, et al., 1998), yielding error as low as 0.23% (Ciresan, Meier, & Schmidhuber, 2012). These regular- izations and augmentations are potentially compatible with DrSAE, but we defer their exploration to future work. Recurrence is essential to the performance of DrSAEs. If the number of recurrent iterations is de- creased from eleven to two, MNIST classiﬁcation error in a network with 400 hidden units increases from 1.08% to 1.32%. With only 200 hidden units, MNIST classiﬁcation error increases from 1.21% to 1.49%, although the hidden units still differentiate into part-units and categorical-units, as shown in ﬁgure 3(d,f).  5 Discussion  It is widely believed that natural stimuli, such as images and sounds, fall near a low-dimensional manifold within a higher-dimensional space (the manifold hypothesis) (Bengio, Courville, & Vin- cent, 2012; Lee, Pedersen, & Mumford, 2003; Olshausen & Field, 2004). The low-dimensional data manifold provides an intuitively compelling and empirically effective basis for classiﬁcation (Rifai, et al., 2011; Simard, LeCun, & Denker, 1993; Simard, et al., 1998). The continuous deformations that deﬁne the data manifold usually preserve identity, whereas even relatively small invalid trans- formations may change the class of a stimulus. For instance, the various handwritten renditions of the digit 3 in in the last column of ﬁgure 9(c) barely overlap, and so the Euclidean distance between them in pixel space is greater than that to the nearest 8 formed by closing both loops. Neverthe- less, smooth deformations of one 3 into another correspond to relatively short trajectories along the data manifold,9 whereas the transformation of a 3 into an 8 requires a much longer path within the data manifold. A prohibitive amount of data is required to fully characterize the data manifold (Narayanan & Mitter, 2010), so it is often approximated by the set of linear submanifolds tangent to the data manifold at the observed datapoints, known as the tangent spaces (Ekanadham, Tranchina, & Simoncelli, 2011; Rifai, et al., 2011; Simard, et al., 1998). DrSAEs naturally and efﬁciently form a tangent space-like representation, consisting of a point on the data manifold indicated by the categorical-units, and a shift within the tangent space speciﬁed by the part-units. Before discriminative ﬁne-tuning, DrSAEs perform a traditional part-based decomposition, familiar from sparse coding, as shown in ﬁgure 9(a). The decoding matrix columns are class-independent, local pen strokes, and many units make a comparable, small contribution to the reconstruction. Af- ter discriminative ﬁne-tuning, the hidden units differentiate into sparse coding local part-units, and global prototype categorical-units that integrate over them. As shown in ﬁgure 9(b,c), the input is de- composed into a prototype, corresponding to a point on the data manifold; and a set of deformations from this prototype along the data manifold, corresponding to shifts within the tangent space. The same prototype can be used for very different inputs, as demonstrated in ﬁgure 9(c), since the space of deformations is rich enough to encompass diverse transformations without moving off the data  9In particular, ﬁgure 9(c) shows how each input can be produced by identity-preserving deformations from  a common prototype, using the tangent space decomposition produced by our network.  11  Progressive reconstruction  Fin Inp  (a)  (b)  (c)  Figure 9: Discriminative recurrent sparse auto-encoders decompose the input into a prototype and deformations along the data manifold. The progressive reconstruction of selected inputs by the hid- den representation before (a) or after (b,c) discriminative ﬁne-tuning. The columns from left to right depict either the components of the reconstruction (top row of each pair), or the partial reconstruc- tion induced by the ﬁrst n parts (bottom row of each pair). Parts are added to the reconstruction in order of decreasing contribution magnitude; smoother transformations are possible with an op- timized sequence. The last two columns show the ﬁnal reconstruction with all parts (Fin), and the original input (Inp). Bars above the decoding matrix columns indicate the scale factor/hidden unit activity associated with the column.  manifold. Even when the prototype is very different from the input, all steps along the reconstruction trajectories in ﬁgure 9(b,c) are recognizable as members of the same class. The prototypes learned by the categorical-units for each class are not simply the average over the elements of the class, as depicted in ﬁgure 10. Each class includes many possible input variations, so its average is blurry. The prototypes, in contrast, are sharp, and look like representative elements of the appropriate class. Many categorical-units are available for each class, as shown in ﬁgure 6. Not all categorical-units correspond to full prototypes; some capture global transformations of a prototype, such as rotations (Simard, et al., 1998). Consistent with prototypes for the non-negative MNIST inputs, the decoding matrix columns of the categorical-units are generally positive, as shown in ﬁgure 7(e). In contrast, the decoders of the part-units are approximately mean-zero and so cannot serve as prototypes themselves. Rather, they shift and transform prototypes, moving activation from one region in the image to another, as demonstrated in ﬁgure 9(b,c).  12  Avg Most class-speciﬁc units  Avg Most class-speciﬁc units  Figure 10: The prototypes learned by categorical-units resemble representative instances of the ap- propriate class, and are sharper than the average over all members of the class in the dataset. The left-most column in each group depicts the average over all elements of each of the ten MNIST digit classes. The other columns show the decoders of the associated units with the largest-magnitude columns in the classiﬁcation matrix C. Bars above the decoders indicate the angle between the en- coder and the decoder for the displayed unit. The most prototypical unit always makes the strongest contribution to the classiﬁcation, and has a large (but not necessarily the largest) angle between its encoder and decoder. Some units that make large contributions to the classiﬁcation represent global transformations, such as rotations, of a prototype (Simard, et al., 1998).  Discrepancies between the prototype and the input due to transformations along the data manifold are explained by class-consistent part-units, and only serve to further activate the categorical-units of that class, as in ﬁgure 6(a,c). Discrepancies between the prototype and the input due to deformations orthogonal to the data manifold are explained by class-incompatible part-units, and serve to suppress the categorical-units of that class, both directly and via activation of incompatible categorical-units. If the wrong prototype is turned on, the residual input will generally contain substantial unexplained components. Part-units obey ISTA-like dynamics and thus function as a sparse coder on the residual input, so part-units that match the unexplained components of the input will be activated. These part- units will have positive connections to categorical-units with compatible prototypes, and so will tend to activate categorical-units associated with the true class (so long as the unexplained components of the input are diagnostic). The spuriously activated categorical-unit will not be able to sustain its activity, since few compatible part-units will be required to capture the residual input. The classiﬁcation approach used by DrSAEs is different from one based upon a traditional sparse coding decomposition: it projects into the space of deviations from a prototype, which is not the same as the space of prototype-free parts, as is clear from ﬁgure 9(a,b). For instance, a 5 can easily be constructed using the parts of a 6, making it difﬁcult to distinguish the two. Indeed, the ﬁrst seven progressive reconstruction steps of the 6 in ﬁgure 9(a) could just as easily be used to produce a 5. However, starting from a 6 prototype, the parts required to break the bottom loop are outside the data manifold of the 6 class, and so will tend to change the active prototype. DrSAEs naturally learn a hierarchical representation within a recurrent network, thereby implement- ing a deep network with parameter sharing between the layers.  References  Bengio, Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine Learn-  ing, 2(1), 1–127.  Bengio, Y., Boulanger-Lewandowski, N., & Pascanu, R. (2012). Advances in optimizing recurrent  networks. arXiv:1212.0901v2 [cs.LG]  Bengio, Y., Courville, A., & Vincent, P. (2012). Representation learning: A review and new per-  spectives. arXiv:1206.5538 [cs.LG]  13  Bengio, Y., & Gingras, F. (1996). Recurrent neural networks for missing or asynchronous data. In D. Touretzky, M. Mozer, & M. Hasselmo (Eds.) Advances in Neural Information Processing Systems (NIPS 8) (pp. 395–401).  Bradley, D. M., & Bagnell, J. A. (2008). Differentiable sparse coding. In D. Koller, D. Schuurmans, Y. Bengio, & L. Bottou (Eds.) Advances in Neural Information Processing Systems (NIPS 21) (pp. 113–120).  Boureau, Y., Bach, F., LeCun, L., & Ponce, J. (2010). Learning mid-level features for recognition In Proceedings of the 23rd IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2010).  Chambolle, A., De Vore, R. A., Lee, N. Y., & Lucier, B. J. (1998). Nonlinear wavelet image processing: Variational problems, compression, and noise removal through wavelet shrinkage. IEEE Transactions on Image Processing, 7(3), 319–335.  Ciresan, D., Meier, U., & Schmidhuber, J. (2012). Multi-column deep neural networks for image In Proceedings of the 25th IEEE Conference on Computer Vision and Pattern  classiﬁcation. Recognition (CVPR 2012) (pp. 3642–3649).  Coates, A., & Ng, A. Y. (2011). The importance of encoding versus training with sparse coding and vector quantization L. Getoor & T. Scheffer (Eds.) Proceedings of the 28th International Conference on Machine Learning (ICML 2011) (pp. 921–928).  Dahl, G. E., Yu, D., Deng, L., & Acero, A. (2012). Context-dependent pre-trained deep neural IEEE Transactions on Audio, Speech, and  networks for large-vocabulary speech recognition. Language Processing, 20(1), 30–42.  Daubechies, I., Defrise, M., & De Mol, C. (2004). An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Communications on Pure and Applied Mathematics, 57(11), 1413–1457.  Ekanadham, C., Tranchina, D., & Simoncelli, E. P. (2011). Recovery of sparse translation-invariant signals with continuous basis pursuit. IEEE Transactions on Signal Processing, 59(10), 4735– 4744.  Glorot, X., Bordes, A., & Bengio, Y. (2011). Deep sparse rectiﬁer neural networks. In G. Gor- don, D. Dunson, & M. Dudik (Eds.) JMLR W&CP: Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011) (pp. 315–323).  Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., & Bengio, Y. (2013). Maxout net-  works arXiv:1302.4389v3 [stat.ML]  Gregor, K., & LeCun, Y. (2010). Learning fast approximations of sparse coding. In J. F¨urnkranz & T. Joachims (Eds.) Proceedings of the 27th International Conference on Machine Learning (ICML 2010) (pp. 399–406).  Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets.  Neural Computation, 18(7), 1527–1554.  Hinton, G. (2010). A practical guide to training restricted Boltzmann machines (UTML TR 2010-  003, version 1). Toronto, Canada: University of Toronto, Department of Computer Science.  Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R. (2012). Improv- ing neural networks by preventing co-adaptation of feature detectors arXiv:1207.0580v1 [cs.NE] Hubel, D. H., & Wiesel, T. N. (1962). Receptive ﬁelds, binocular interaction and functional archi-  tecture in the cat’s visual cortex. The Journal of Physiology, 160(1), 106–154.  Jarrett, K., Kavukcuoglu, K., Ranzato, M. A., & LeCun, Y. (2009). What is the best multi-stage In Proceedings of the 12th International Conference on  architecture for object recognition? Computer Vision (ICCV 2009) (pp. 2146–2153).  LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to docu-  ment recognition. Proceedings of the IEEE, 86(11), 2278–2324.  Lee, A. B., Pedersen, K. S., & Mumford, D. (2003). The nonlinear statistics of high-contrast patches  in natural images. International Journal of Computer Vision, 54(1), 83–103.  Lee, H., Ekanadham, C., & Ng, A. (2008). Sparse deep belief net model for visual area V2. In J. C. Platt, D. Koller, Y. Singer & S. Roweis (Eds.) Advances in Neural Information Processing Systems (NIPS 20), (pp. 873–880).  14  Mairal, J., Bach, F., Ponce, J., Sapiro, G., & Zisserman, A. (2009). Supervised dictionary learning. In D. Koller, D. Schuurmans, Y. Bengio, & L. Bottou (Eds.) Advances in Neural Information Processing Systems (NIPS 21) (pp. 1033–1040).  Mairal, J., Bach, F., & Ponce, J. (2012). Task-driven dictionary learning. IEEE Transactions on  Pattern Analysis and Machine Intelligence, 34(4), 791–804.  Nair, V., & Hinton, G. E. (2010). Rectiﬁed linear units improve restricted boltzmann machines. In J. F¨urnkranz & T. Joachims (Eds.) Proceedings of the 27th International Conference on Machine Learning (ICML 2010) (pp. 807-814).  Narayanan, H. & MItter, S. (2010). Sample complexity of testing the manifold hypothesis. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, & A. Culotta (Eds.) Advances in Neural Information Processing Systems (NIPS 23) (pp. 1786–1794).  Olshausen, B. A., & Field, D. J. (1996). Emergence of simple-cell receptive ﬁeld properties by  learning a sparse code for natural images. Nature, 381(6583), 607–609.  Olshausen, B. A., & Field, D. J. (1997). Sparse coding with an overcomplete basis set: A strategy  employed by VI? Vision Research, 37(23), 3311–3326.  Olshausen, B. A., & Field, D. J. (2004). Sparse coding of sensory inputs. Current opinion in  neurobiology, 14(4), 481–487.  Ranzato M., Poultney, C., Chopra, S., & LeCun, Y. (2006). Efﬁcient learning of sparse represen- tations with an energy-based model. In B. Sch¨olkopf, J. Platt, & T. Hoffman (Eds.) Advances in Neural Information Processing Systems (NIPS 19), (pp. 1137–1144).  Ranzato, M., & Szummer, M. (2008). Semi-supervised learning of compact document representa- tions with deep networks In A. McCallum & S. Roweis (Eds.), Proceedings of the 25th Annual International Conference on Machine Learning (ICML 2008) (pp. 792–799).  Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., & Muller, X. (2011). The manifold tangent classiﬁer. In J. Shawe-Taylor, R. S. Zemel, P. Bartlett, F. C. N. Pereira, & K. Q. Weinberger (Eds.) Advances in Neural Information Processing Systems (NIPS 24) (pp. 2294–2302).  Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations by error propagation. In D. E. Rumelhart, J. L. McClelland, and the PDP Research Group (Eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Vol. 1. Foundations (pp. 318–362). Cambridge, MA: MIT Press.  Salinas, E., & Abbott, L. F. (1996). A model of multiplicative neural responses in parietal cortex. Proceedings of the National Academy of Sciences of the United States of America, 93(21), 11956– 11961.  Seung, H. S. (1998). Learning continuous attractors in recurrent networks. In M. I. Jordan, M. J. Kearns, & S. A. Solla (Eds.) Advances in Neural Information Processing Systems (NIPS 10) (pp. 654–660).  Simard, P., LeCun, Y., & Denker, J. S. (1993). Efﬁcient pattern recognition using a new transforma- tion distance. In S. J. Hanson, J. D. Cowan, & C. L. Giles (Eds.) Advances in Neural Information Processing Systems (NIPS 5) (pp. 50–58).  Simard, P., LeCun, Y., Denker, J., & Victorri, B. (1998). Transformation invariance in pattern recognition: Tangent distance and tangent propagation. In G. Orr, & K. Muller (Eds.), Neural networks: Tricks of the trade. Berlin: Springer.  Sprechmann, P., Bronstein, A., & Sapiro, G. (2012). Learning efﬁcient structured sparse models. In J. Langford & J. Pineau (Eds.) Proceedings of the 29th International Conference on Machine Learning (ICML 12) (pp. 615–622).  Sprechmann, P., Bronstein, A., & Sapiro, G. (2012). Learning efﬁcient sparse and low rank models.  arXiv:1212.3631 [cs.LG]  Deng, L. & Yu, D. (2011). Deep convex net: A scalable architecture for speech pattern classiﬁca- tion. In Proceedings of the 12th Annual Conference of the International Speech Communication Association (INTERSPEECH 2011) (pp. 2285-2288).  Zeiler, M. D., Taylor, G. W., & Fergus, R. (2011). Adaptive deconvolutional networks for mid and high level feature learning. In Proceedings of the 13th International Conference on Computer Vision (ICCV 2011) (pp. 2018–2025).  15  ","We present the discriminative recurrent sparse auto-encoder model, comprisinga recurrent encoder of rectified linear units, unrolled for a fixed number ofiterations, and connected to two linear decoders that reconstruct the input andpredict its supervised classification. Training viabackpropagation-through-time initially minimizes an unsupervised sparsereconstruction error; the loss function is then augmented with a discriminativeterm on the supervised classification. The depth implicit in thetemporally-unrolled form allows the system to exhibit all the power of deepnetworks, while substantially reducing the number of trainable parameters.From an initially unstructured network the hidden units differentiate intocategorical-units, each of which represents an input prototype with awell-defined class; and part-units representing deformations of theseprototypes. The learned organization of the recurrent encoder is hierarchical:part-units are driven directly by the input, whereas the activity ofcategorical-units builds up over time through interactions with the part-units.Even using a small number of hidden units per layer, discriminative recurrentsparse auto-encoders achieve excellent performance on MNIST."
1301.3529,2013,Discrete Restricted Boltzmann Machines  ,"['Guido F. Montufar', 'Jason Morton']",https://arxiv.org/pdf/1301.3529.pdf,"4 1 0 2    r p A 2 2         ] L M  . t a t s [      4 v 9 2 5 3  .  1 0 3 1 : v i X r a  Discrete Restricted Boltzmann Machines  Guido Mont´ufar Jason Morton  Department of Mathematics Pennsylvania State University University Park, PA 16802, USA  GFM10@PSU.EDU MORTON@MATH.PSU.EDU  Abstract  We describe discrete restricted Boltzmann machines: probabilistic graphical models with bipar- tite interactions between visible and hidden discrete variables. Examples are binary restricted Boltzmann machines and discrete na¨ıve Bayes models. We detail the inference functions and distributed representations arising in these models in terms of conﬁgurations of projected prod- ucts of simplices and normal fans of products of simplices. We bound the number of hidden variables, depending on the cardinalities of their state spaces, for which these models can ap- proximate any probability distribution on their visible states to any given accuracy. In addition, we use algebraic methods and coding theory to compute their dimension. Keywords: Restricted Boltzmann Machine, Na¨ıve Bayes Model, Representational Power, Dis- tributed Representation, Expected Dimension  1 Introduction  A restricted Boltzmann machine (RBM) is a probabilistic graphical model with bipartite interactions between an observed set and a hidden set of units [see Smolensky, 1986, Freund and Haussler, 1991, Hinton, 2002, 2010]. A characterizing property of these models is that the observed units are independent given the states of the hidden units and vice versa. This is a consequence of the bipartiteness of the interaction graph and does not depend on the units’ state spaces. Typically RBMs are deﬁned with binary units, but other types of units have also been considered, including continuous, discrete, and mixed type units [see Welling et al., 2005, Marks and Movellan, 2001, Salakhutdinov et al., 2007, Dahl et al., 2012, Tran et al., 2011]. We study discrete RBMs, also called multinomial or softmax RBMs, which are special types of exponential family harmoniums [Welling et al., 2005]. While each unit Xi of a binary RBM has the state space {0, 1}, the state space of each unit Xi of a discrete RBM is a ﬁnite set Xi = {0, 1, . . . , ri − 1}. Like binary RBMs, discrete RBMs can be trained using contrastive divergence (CD) [Hinton, 1999, 2002, Carreira-Perpi˜nan and Hinton, 2005] or expectation-maximization (EM) [Dempster et al., 1977] and can be used to train the parameters of deep systems layer by layer [Hinton et al., 2006, Bengio et al., 2007].  Non-binary visible units are natural because they can directly encode non-binary features. The situation with hidden units is more subtle. States that appear in different hidden units can be acti-  1  MONT ´UFAR AND MORTON  24  40  79  Figure 1: Examples of probability models treated in this paper, in the special case of binary visible variables. The light (dark) nodes represent visible (hidden) variables with the indicated number of states. The total parameter count of each model is indicated at the top. From left to right: a binary RBM; a discrete RBM with one 8-valued and one binary hidden units; and a binary na¨ıve Bayes model with 16 hidden classes.  vated by the same visible vector, but states that appear in the same hidden unit are mutually exclu- sive. Non-binary hidden units thus allow one to explicitly represent complex exclusive relationships. For example, a discrete RBM topic model would allow some topics to be mutually exclusive and other topics to be mixed together freely. This provides a better match to the semantics of several learning problems, although the learnability of such representations is mostly open. The practical need to represent mutually exclusive properties is evidenced by the common approach of adding activation sparsity parameters to binary RBM hidden states, which artiﬁcially create mutually ex- clusive non-binary states by penalizing models which have more than a certain percentage of hidden units active.  A discrete RBM is a product of experts [Hinton, 1999]; each hidden unit represents an expert which is a mixture model of product distributions, or na¨ıve Bayes model. Hence discrete RBMs cap- ture both na¨ıve Bayes models and binary RBMs, and interpolate between non-distributed mixture representations and distributed mixture representations [Bengio, 2009, Mont´ufar and Morton, 2012]. See Figure 1. Na¨ıve Bayes models have been studied across many disciplines. In machine learning they are most commonly used for classiﬁcation and clustering, but have also been considered for probabilistic modelling [Lowd and Domingos, 2005, Mont´ufar, 2013]. Theoretical work on binary RBM models includes results on universal approximation [Freund and Haussler, 1991, Le Roux and Bengio, 2008, Mont´ufar and Ay, 2011], dimension and parameter identiﬁability [Cueto et al., 2010], Bayesian learning coefﬁcients [Aoyagi, 2010], complexity [Long and Servedio, 2010], and approximation errors [Mont´ufar et al., 2011]. In this paper we generalize some of these theoretical results to discrete RBMs.  Probability models with more general interactions than strictly bipartite have also been consid- ered, including semi-restricted Boltzmann machines and higher-order interaction Boltzmann ma- chines [see Sejnowski, 1986, Memisevic and Hinton, 2010, Osindero and Hinton, 2008, Ranzato et al., 2010]. The techniques that we develop in this paper also serve to treat a general class of RBM-like models allowing within-layer interactions, a generalization that will be carried out in a forthcoming work [Mont´ufar and Morton, 2013].  Section 2 collects basic facts about independence models, na¨ıve Bayes models, and binary RBMs, including an overview on the aforementioned theoretical results. Section 3 deﬁnes discrete RBMs formally and describes them as (i) products of mixtures of product distributions (Proposi- tion 7) and (ii) as restricted mixtures of product distributions. Section 4 elaborates on distributed representations and inference functions represented by discrete RBMs (Proposition 11, Lemma 12,  2  22222222⊆22222222⊆822222⊆16222222222222⊆22222222⊆822222⊆162222DISCRETE RESTRICTED BOLTZMANN MACHINES  Figure 2: The convex support of the independence model of three binary variables (left) and of a binary-ternary pair of variables (right) discussed in Example 1.  and Proposition 14). Section 5 addresses the expressive power of discrete RBMs by describing explicit submodels (Theorem 15) and provides results on their maximal approximation errors and universal approximation properties (Theorem 16). Section 6 treats the dimension of discrete RBM models (Proposition 17 and Theorem 19). Section 7 contains an algebraic-combinatorial discussion of tropical discrete RBM models (Theorem 21) with consequences for their dimension collected in Propositions 24, 25, and 26.  2 Preliminaries  2.1  Independence models  Consider a system of n < ∞ random variables X1, . . . , Xn. Assume that Xi takes states xi in a ﬁnite set Xi = {0, 1, . . . , ri − 1} for all i ∈ {1, . . . , n} =: [n]. The state space of this system is X := X1 × ··· × Xn. We write xλ = (xi)i∈λ for a joint state of the variables with index i ∈ λ for any λ ⊆ [n], and x = (x1, . . . , xn) for a joint state of all variables. We denote by ∆(X ) the set of all probability distributions on X . We write (cid:104)a, b(cid:105) for the inner product a(cid:62)b. (cid:81) The independence model of the variables X1, . . . , Xn is the set of product distributions p(x) = i∈[n] pi(xi) for all x ∈ X , where pi is a probability distribution with state space Xi for all i ∈ [n]. This model is the closure EX (in the Euclidean topology) of the exponential family  (cid:110) 1  exp((cid:104)θ, A(X )(cid:105)) : θ ∈ RdX(cid:111)  ,  EX :=  Z(θ)  (1)  where A(X ) ∈ RdX×X is a matrix of sufﬁcient statistics; with rows equal to the indicator functions 1X and 1{x : xi=yi} for all yi ∈ Xi \ {0} for all i ∈ [n]. The partition function Z(θ) normalizes the distributions. The convex support of EX is the convex hull QX := conv({A(X ) x }x∈X ) of the columns of A(X ), which is a Cartesian product of simplices with QX ∼= ∆(X1) × ··· × ∆(Xn). Example 1. The sufﬁcient statistics of the independence models EX and EX (cid:48) with state spaces  3  ex1=1ex2=1ex3=1ex1=1ex1=2ex2=1MONT ´UFAR AND MORTON  (cid:48) = {0, 1, 2} × {0, 1} are, with rows labeled by indicator functions, (cid:34)1 (cid:35)(cid:34)1 (cid:35)(cid:34)1 (cid:35)(cid:34)1 (cid:35)(cid:34)0 (cid:35)(cid:34)0 (cid:35)(cid:34)0 (cid:35)(cid:34)0 (cid:35) X = {0, 1}3 and X (cid:105)(cid:104)0 (cid:105)(cid:104)1 (cid:105)(cid:104)1 (cid:104)1 (cid:105)(cid:104)0 (cid:105)(cid:104)0 (cid:105)  0 0  0 1  1 0  1 1  0 0  0 1  1 0  1 1   1 1 1 1 1 1 1 1  1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0   x3 = 1  x2 = 1 x1 = 1  A(X ) =  0  1  2  0  1  2   1 1 1 1 1 1  1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0   x2 = 1  x1 = 2 x1 = 1  .  A(X (cid:48)) =  In the ﬁrst case the convex support is a cube and in the second it is a prism. Both convex supports are three-dimensional polytopes, but the prism has fewer vertices and is more similar to a simplex, meaning that its vertex set is afﬁnely more independent than that of the cube. See Figure 2.  2.2 Na¨ıve Bayes models Let k ∈ N. The k-mixture of the independence model, or na¨ıve Bayes model with k hidden classes, with visible variables X1, . . . , Xn is the set of all probability distributions expressible as convex combinations of k points in EX :  λip(i) : p(i) ∈ EX , λi ≥ 0, for all i ∈ [k], and (cid:88)  (cid:110)(cid:88)  MX ,k :=  λi = 1  (cid:111)  (2)  .  i∈[k]  i∈[k]  We write Mn,k for the k-mixture of the independence model of n binary variables. The dimen-  sions of mixtures of binary independence models are known: Theorem 2 (Catalisano et al. [2011]). The mixtures of binary independence models Mn,k have the dimension expected from counting parameters, min{nk + (k − 1), 2n − 1}, except for M4,3, which has dimension 13 instead of 14.  (cid:48)  (cid:48)  Let AX (d) denote the maximal cardinality of a subset X  By results from [Gilbert, 1952, Varshamov, 1957], when q is a power of a prime number and  ⊆ X of minimum Hamming distance ⊆ X with dH (x, y) ≥ d for all distinct points at least d, i.e., the maximal cardinality of a subset X (cid:48), where dH (x, y) := |{i ∈ [n] : xi (cid:54)= yi}| denotes the Hamming distance between x and y. x, y ∈ X The function AX is familiar in coding theory. The k-mixtures of independence models are universal approximators when k is large enough. This can be made precise in terms of AX (2): Theorem 3 (Mont´ufar [2013]). The mixture model MX ,k can approximate any probability distri- bution on X arbitrarily well if k ≥ |X|/maxi∈[n] |Xi| and only if k ≥ AX (2). X = {0, 1, . . . , q − 1}n, then AX = qn−1. In these cases the previous theorem shows that MX ,k is a universal approximator of distributions on X if and only if k ≥ qn−1. In particular, the small- est na¨ıve Bayes model universal approximator of distributions on {0, 1}n has 2n−1(n + 1) − 1 y with dH (x, y) = 1 and it is a strong mode if p(x) >(cid:80) Lemma 4 (Mont´ufar and Morton [2012]). If a mixture of product distributions p =(cid:80)  Some of the distributions not representable by a given na¨ıve Bayes model can be characterized in terms of their modes. A state x ∈ X is a mode of a distribution p ∈ ∆(X ) if p(x) > p(y) for all  y : dH (x,y)=1 p(y).  parameters.  i λip(i) has  strong modes C ⊆ X , then there is a mixture component p(i) with mode x for each x ∈ C.  4  DISCRETE RESTRICTED BOLTZMANN MACHINES  2.3 Binary restricted Boltzmann machines The binary RBM model with n visible and m hidden units, denoted RBMn,m, is the set of distribu- tions on {0, 1}n of the form  p(x) =  1  Z(W, B, C)  h∈{0,1}m  exp(h(cid:62)W x + B(cid:62)x + C(cid:62)h)  for all x ∈ {0, 1}n,  (3)  (cid:88)  (cid:80) Z(W, B, C) = (cid:80) where x denotes states of the visible units, h denotes states of the hidden units, W = (Wji)ji ∈ Rm×n is a matrix of interaction weights, B ∈ Rn and C ∈ Rm are vectors of bias weights, and h∈{0,1}m exp(h(cid:62)W x + B(cid:62)x + C(cid:62)h) is the normalizing partition  x∈{0,1}n  function.  It is known that these models have the expected dimension for many choices of n and m:  Theorem 5 (Cueto et al. [2010]). The dimension of the model RBMn,m is equal to nm + n + m when m + 1 ≤ 2n−(cid:100)log2(n+1)(cid:101) and it is equal to 2n − 1 when m ≥ 2n−(cid:98)log2(n+1)(cid:99).  It is also known that with enough hidden units, binary RBMs are universal approximators:  Theorem 6 (Mont´ufar and Ay [2011]). The model RBMn,m can approximate any distribution on {0, 1}n arbitrarily well whenever m ≥ 2n−1 − 1.  A previous result by Le Roux and Bengio [2008, Theorem 2] shows that RBMn,m is a universal approximator whenever m ≥ 2n+1. It is not known whether the bounds from Theorem 6 are always tight, but they show that for any given n, the smallest RBM universal approximator of distributions on {0, 1}n has at most 2n−1(n + 1) − 1 parameters and hence not more than the smallest na¨ıve  Bayes model universal approximator (Theorem 3).  3 Discrete restricted Boltzmann machines  Let Xi = {0, 1, . . . , ri−1} for all i ∈ [n] and Yj = {0, 1, . . . , sj −1} for all j ∈ [m]. The graphical model with full bipartite interactions {{i, j} : i ∈ [n], j ∈ [m]} on X × Y is the exponential family (4)  (cid:26) 1  (cid:27)  ,  EX ,Y :=  Z(θ)  (cid:16)(cid:80)  with sufﬁcient statistics matrix equal to the Kronecker product A(X ,Y) = A(X ) ⊗ A(Y) of the sufﬁcient statistics matrices A(X ) and A(Y) of the independence models EX and EY. The matrix A(X ,Y) has dX dY = linearly independent rows and |X × Y| columns, each column corresponding to a joint state (x, y) of all variables. Disregard- ing the entry of θ that is multiplied with the constant row of A(X ,Y), which cancels out with the normalization function Z(θ), this parametrization of EX ,Y is one-to-one. In particular, this model has dimension dim(EX ,Y ) = dX dY − 1.  j∈[m](|Yi| − 1) + 1  i∈[n](|Xi| − 1) + 1  The discrete RBM model RBMX ,Y is the following set of marginal distributions:  exp((cid:104)θ, A(X ,Y)(cid:105)) : θ ∈ RdX dY (cid:17) (cid:17)(cid:16)(cid:80)  (cid:111)  .  (5)  (cid:110)  (cid:88)  y∈Y  RBMX ,Y :=  q(x) =  p(x, y) for all x ∈ X : p ∈ EX ,Y  5  MONT ´UFAR AND MORTON  In the case of one single hidden unit, this model is the na¨ıve Bayes model on X with |Y1| hidden classes. When all units are binary, X = {0, 1}n and Y = {0, 1}m, this model is RBMn,m. Note the column-by-column vectorization of the matrix(cid:0) 0 B(cid:62) that the exponent in eq. (3) can be written as (h(cid:62)W x + B(cid:62)x + C(cid:62)h) = (cid:104)θ, A(X ,Y) (x,h) (cid:105), taking for θ  (cid:1).  C W  Conditional distributions  The conditional distributions of discrete RBMs can be described in the following way. Consider a vector θ ∈ RdX dY parametrizing EX ,Y, and the matrix Θ ∈ RdY×dX with column-by-column vec- torization equal to θ. A lemma by Roth [1934] shows that θ(cid:62)(A(X )⊗A(Y))(x,y) = (A(X ) x )(cid:62)Θ(cid:62)A(Y) (cid:68) for all x ∈ X , y ∈ Y, and hence  (cid:68)  (cid:69)  (cid:69)  (cid:68)  (cid:69)  y  θ, A(X ,Y)  (x,y)  ΘA(X )  x , A(Y)  y  =  (cid:62)A(Y) Θ  y  , A(X )  x  =  ∀x ∈ X , y ∈ Y.  The inner product in eq. (6) describes following probability distributions:  1  exp(cid:0)(cid:10)θ, A(X ,Y)(cid:11)(cid:1), (cid:1) exp(cid:0)(cid:10)ΘA(X ) (cid:1) exp(cid:0)(cid:10)Θ  Z(θ) 1  Z(cid:0)ΘA(X ) Z(cid:0)Θ(cid:62)A(Y)  x 1  y  x , A(Y)(cid:11)(cid:1), and , A(X )(cid:11)(cid:1).  (cid:62)A(Y)  y  pθ(·,·) = pθ(·|x) =  pθ(·|y) =  (6)  (7)  (8)  (9)  Geometrically, ΘA(X ) is a linear projection of the columns of the sufﬁcient statistics matrix A(X ) into the parameter space of EY, and similarly, Θ(cid:62)A(Y) is a linear projection of the columns of A(Y) into the parameter space of EX .  Polynomial parametrization  Discrete RBMs can be parametrized not only in the exponential way discussed above, but also by simple polynomials. The exponential family EX ,Y can be parametrized by square free monomials:  p(v, h) =  1 Z  (γ{j,i},(y(cid:48)  j ,x(cid:48)  i))  δy(cid:48)  j  (hj )δx(cid:48)  i  (vi) for all (v, h) ∈ Y × X ,  (10)  where γ{j,i},(y(cid:48)  j ,x(cid:48)  i) are positive reals. The probability distributions in RBMX ,Y can be written as  p(v) =  1 Z  γ{j,1},(hj ,v1) ··· γ{j,n},(hj ,vn)  for all v ∈ X .  (11)  (cid:17)  The parameters γ{j,i},(y(cid:48)  j ,x(cid:48)  i) correspond to exp(θ{j,i},(y(cid:48)  j ,x(cid:48)  i)) in the parametrization given in eq. (4).  6  (cid:89)  {j, i} ∈ [m] × [n], i) ∈ Yj × Xi (y(cid:48) j , x(cid:48)  (cid:89)  (cid:16) (cid:88)  j∈[m]  hj∈Yj  DISCRETE RESTRICTED BOLTZMANN MACHINES  Products of mixtures and mixtures of products In the following we describe discrete RBMs from two complementary perspectives: (i) as products of experts, where each expert is a mixture of products, and (ii) as restricted mixtures of product distributions. The renormalized entry-wise (Hadamard) product of two probability distributions p y∈X p(y)q(y). Here we assume that p and q  and q on X is deﬁned as p ◦ q := (p(x)q(x))x∈X /(cid:80)  have overlapping supports, such that the deﬁnition makes sense. Proposition 7. The model RBMX ,Y is a Hadamard product of mixtures of product distributions:  RBMX ,Y = MX ,|Y1| ◦ ··· ◦ MX ,|Ym| .  Proof. The statement can be seen directly by considering the parametrization from eq. (11). To make this explicit, one can use a homogeneous version of the matrix A(X ,Y) which we denote by A and which deﬁnes the same model. Each row of A is indexed by an edge {i, j} of the bipartite graph and a joint state (xi, hj) of the visible and hidden units connected by this edge. Such a row has a one in any column when these states agree with the global state, and zero otherwise. For any j ∈ [m] let Aj,: denote the matrix containing the rows of A with indices ({i, j}, (xi, hj)) for all xi ∈ Xi for all i ∈ [n] for all hj ∈ Yj, and let A(x, h) denote the (x, h)-column of A. We have  exp((cid:104)θ, A(x, h)(cid:105))  (cid:88) (cid:88) (cid:16)(cid:88)  h  h  h1  p(x) =  =  =  1 Z  1 Z  1 Z  exp((cid:104)θ1,:, A1,:(x, h)(cid:105)) exp((cid:104)θ2,:, A2,:(x, h)(cid:105))··· exp((cid:104)θm,:, Am,:(x, h)(cid:105))  (cid:17)  (cid:16)(cid:88)  (cid:17)  exp((cid:104)θ1,:, A1,:(x, h1)(cid:105))  exp((cid:104)θm,:, Am,:(x, hm)(cid:105))  hm  ··· 1 Z(cid:48) p(1)(x)··· p(m)(x),  1 Z  =  x∈X(cid:80)  (Z1p(1)(x))··· (Zmp(m)(x)) =  hj∈Yj exp((cid:104)θj,:, Aj,:(x, hj)(cid:105)) for all j ∈ [m]. Since the  where p(j) ∈ MX ,|Yj| and Zj =(cid:80) Of course, every distribution in RBMX ,Y is a mixture distribution p(x) =(cid:80)  vectors θj,: can be chosen arbitrarily, the factors p(j) can be made arbitrary within MX ,|Yj|.  h∈Y p(x|h)q(h). The mixture weights are given by the marginals q(h) on Y of distributions from EX ,Y, and the mixture components can be described as follows. Proposition 8. The set of conditional distributions p(·|h), h ∈ Y of a distribution in EX ,Y is the set of product distributions in EX with parameters θh = Θ(cid:62)A(Y) h , h ∈ Y equal to a linear projection of the vertices {A(Y) h : h ∈ Y} of the Cartesian product of simplices QY ∼= ∆(Y1) × ··· × ∆(Ym).  Proof. This is by eq. (6).  4 Products of simplices and their normal fans  Binary RBMs have been analyzed by considering each of the m hidden units as deﬁning a hyper- plane Hj slicing the n-cube into two regions. To generalize the results provided by this analysis, in  7  MONT ´UFAR AND MORTON  (1, 0)  (1, 1)  (0, 0)  (0, 1)  Θ−1(R2)  Θ−1(R1)  Θ−1(R0)  R1  1  0  R0  2  R2  Figure 3: Three slicings of a square by the normal fan of a triangle with maximal cones R0, R1, and R2, corresponding to three possible inference functions of RBM{0,1}2,{0,1,2}.  this section we replace the n-cube with a general product of simplices QX , and replace the two re- gions deﬁned by the hyperplane Hj by the |Yj| regions deﬁned by the maximal cones of the normal fan of the simplex ∆(Yj). Subdivisions of independence models The normal cone of a polytope Q ⊂ Rd at a point x ∈ Q is the set of all vectors v ∈ Rd with (cid:104)v, (x − y)(cid:105) ≥ 0 for all y ∈ Q. We denote by Rx the normal cone of the product of simplices QX = conv{A(X ) x . The normal fan FX is the set of all normal cones of Z(θ) exp((cid:104)θ, A(X )(cid:105)) ∈ EX strictly maximized at x ∈ X , with QX . The product distributions pθ = 1 pθ(x) > pθ(y) for all y ∈ X \ {x}, are those with parameter vector θ in the relative interior of Rx. Hence the normal fan FX partitions the parameter space of the independence model into regions of distributions with maxima at different inputs.  x }x∈X at the vertex A(X )  Inference functions and slicings For any choice of parameters of the model RBMX ,Y, there is an inference function π : X → Y, (or more generally π : X → 2Y), which computes the most likely hidden state given a visible state. These functions are not necessarily injective nor surjective. For a visible state x, the conditional x , A(Y) distribution on the hidden states is a product distribution p(y|X = x) = 1 (cid:105)) which is maximized at the state y for which ΘA(X ) x ∈ Ry. The preimages of the cones Ry by the map Θ partition the input space RdX and are called inference regions. See Figure 3 and Example 10. Deﬁnition 9. A Y-slicing of a ﬁnite set Z ⊂ RdX is a partition of Z into the preimages of the cones Ry, y ∈ Y by a linear map Θ : RdX → RdY . We assume that Θ is generic, such that it maps each element of Z into the interior of some Ry.  Z exp((cid:104)ΘA(X )  y  For example, when Y = {0, 1}, the fan FY consists of a hyperplane and the two closed half-  spaces deﬁned by that hyperplane. A Y-slicing is in this case a standard slicing by a hyperplane. Example 10. Let X = {0, 1, 2} × {0, 1} and Y = {0, 1}4. The maximal cones Ry, y ∈ Y of the normal fan of the 4-cube with vertices {0, 1}4 are the closed orthants of R4. The 6 vertices {A(X ) : x ∈ X} of the prism ∆({0, 1, 2}) × ∆({0, 1}) can be mapped into 6 distinct orthants  x  8  bbbbbbbbbbbbbbbDISCRETE RESTRICTED BOLTZMANN MACHINES  of R4, each orthant with an even number of positive coordinates:   3 −2 −2 −2  1 1 −2 −2 1 −2  2 −2 −2 2 2 −2  Θ     1 1 1 1 1 1  1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0    A(X )   −1 −1  =  1  1 1 −1 −1  1 −3 1 −3 −1  1 1 3 3 −1 −1 1 3 1 3 −1 1   .  (12)  Even in the case of one single hidden unit the slicings can be complex, but the following simple  type of slicing is always available. Proposition 11. Any slicing by k − 1 parallel hyperplanes is a {1, 2, . . . , k}-slicing. Proof. We show that there is a line L = {λr − b : λ ∈ R}, r, b ∈ Rk intersecting all cells of FY, Y = {1, . . . , k}. We need to show that there is a choice of r and b such that for every y ∈ Y the set Iy ⊆ R of all λ with (cid:104)λr − b, (ey − ez)(cid:105) > 0 for all z ∈ Y \ {y} has a non-empty interior. Now, Iy is the set of λ with (13) Choosing b1 < ··· < bk and ry = f (by), where f is a strictly increasing and strictly concave function, we get I1 = (−∞, b2−b1 ) for y = 2, 3, . . . , k − 1, and Ik = r2−r1 ( bk−bk−1 ,∞). The lengths ∞, l2, . . . , lk−1,∞ of the intervals I1, . . . , Ik can be adjusted arbitrarily rk−rk−1 by choosing suitable differences rj+1 − rj for all j = 1, . . . , k − 1. Strong modes Recall the deﬁnition of strong modes given in page 4. Lemma 12. Let C ⊆ X be a set of arrays which are pairwise different in at least two entries (a code of minimum distance two).  λ(ry − rz) > by − bz ), Iy = ( by−by−1 ry−ry−1  for all z (cid:54)= y. , by+1−by ry+1−ry  y  sending at least one vertex into each cell.  • If RBMX ,Y contains a probability distribution with strong modes C, then there is a linear map Θ of {A(Y) : y ∈ Y} into the C-cells of FX (the cones Rx above the codewords x ∈ C) , A(X ) : y ∈ Y} into the C-cells of FX , with maxx{(cid:104)Θ(cid:62)A(Y) • If there is a linear map Θ of {A(Y) c for all y ∈ Y, then RBMX ,Y contains a probability distribution with strong modes C.  y  y  x (cid:105)} =  Proof. This is by Proposition 8 and Lemma 4.  A simple consequence of the previous lemma is that if the model RBMX ,Y is a universal ap- proximator of distributions on X , then necessarily the number of hidden states is at least as large as the maximum code of visible states of minimum distance two, |Y| ≥ AX (2). Hence discrete RBMs may not be universal approximators even when their parameter count surpasses the dimension of the ambient probability simplex.  Example 13. Let X = {0, 1, 2}n and Y = {0, 1, . . . , 4}m. In this case AX (2) = 3n−1. If RBMX ,Y is a universal approximator with n = 3 and n = 4, then m ≥ 2 and m ≥ 3, respectively, although the smallest m for which RBMX ,Y has 3n − 1 parameters is m = 1 and m = 2, respectively. Using Lemma 12 and the analysis of [Mont´ufar and Morton, 2012] gives the following. Proposition 14. If 4(cid:100)m/3(cid:101) ≤ n, then RBMX ,Y contains distributions with 2m strong modes.  9  MONT ´UFAR AND MORTON  5 Representational power and approximation errors  In this section we describe submodels of discrete RBMs and use them to provide bounds on the model approximation errors depending on the number of units and their state spaces. Universal approximation results follow as special cases with vanishing approximation error.  Theorem 15. The model RBMX ,Y can approximate the following arbitrarily well:  • Any mixture of dY = 1 +(cid:80)m • When dY ≥ ((cid:81)  j=1(|Yj| − 1) product distributions with disjoint supports.  i∈[k] |Xi|)/ maxj∈[k] |Xj| for some k ≤ n, any distribution from the model P of distributions with constant value on each block {x1} × ··· × {xk} × Xk+1 × ··· × Xn for all xi ∈ Xi, for all i ∈ [k].  • Any probability distribution with support contained in the union of dY sets of the form {x1}×  ··· × {xk−1} × Xk × {xk+1} × ··· × {xn}.  Proof. By Proposition 7 the model RBMX ,Y contains any Hadamard product p(1) ◦ ··· ◦ p(m) Choosing the factors ˜p(j) with pairwise disjoint supports shows that p =(cid:80)m with mixtures of products as factors, p(j) ∈ MX ,|Yj| for all j ∈ [m]. In particular, it contains p = p(0) ◦ (1 + ˜λ1 ˜p(1)) ◦ ··· ◦ (1 + ˜λm ˜p(m)), where p(0) ∈ EX , ˜p(j) ∈ MX ,|Yj|−1, and ˜λj ∈ R+. j=0 λjp(j), whereby p(0) can be any product distribution and p(j) can be any distribution from MX ,|Yj|−1 for all j ∈ [m], as long as supp(p(j)) ∩ supp(p(j(cid:48))) for all j (cid:54)= j(cid:48). This proves the ﬁrst item. For the second item: Any point in the set P is a mixture of uniform distributions supported (cid:81) (cid:81) on the disjoint blocks {x1} × ··· × {xk} × Xk+1 × ··· × Xn for all (x1, . . . , xk) ∈ X1 × ··· × mixture(cid:80) Xk. Each of these uniform distributions is a product distribution, since it factorizes as px1,...,xk = i∈[n]\[k] ui, where ui denotes the uniform distribution on Xi. For any j ∈ [k] any i∈[k] δxi (cid:17) (cid:89) xj∈Xj Hence any distribution from the set P is a mixture of ((cid:81)  i∈[k] |Xi|)/ maxj∈[k] |Xj| product distribu- For the third item: The model EX contains any distribution with support of the form {x1}×···× {xk−1} × Xk × {xk+1} × ··· × {xn}. Hence, by the ﬁrst item, the RBM model can approximate any distribution arbitrarily well whose support can be covered by dY sets of that form.  λxj px1,...,xk is also a product distribution, since it factorizes as  tions with disjoint supports. The claim now follows from the ﬁrst item.  (cid:16) (cid:88)  tions on X . The Kullback-Leibler divergence from p to q is deﬁned as D(p(cid:107)q) :=(cid:80)  We now analyse the RBM model approximation errors. Let p and q be two probability distribu- x∈X p(x) log p(x) when supp(p) ⊆ supp(q) and D(p(cid:107)q) := ∞ otherwise. The divergence from p to a model M ⊆ ∆(X ) is deﬁned as D(p(cid:107)M) := inf q∈M D(p(cid:107)q) and the maximal approximation error of M is supp∈∆(X ) D(p(cid:107)M). The maximal approximation error of the independence model EX satisﬁes supp∈∆(X ) D(p(cid:107)EX ) ≤ |X|/ maxi∈[n] |Xi|, with equality when all units have the same number of states [see Ay and Knauf, 2006, Corollary 4.10].  (cid:89)  i∈[k]\{j}  i∈[n]\[k]  λxj δxj  xj∈Xj  (14)  δxi  ui.  q(x)  10  DISCRETE RESTRICTED BOLTZMANN MACHINES  Maximal-error bound  Nr. parameters  k  k  m  m  Figure 4: Illustration of Theorem 16. The left panel shows a heat map of the upper bound on the Kullback-Leibler approximation errors of discrete RBMs with 100 visible binary units and the right panel shows a map of the total number of model parameters, both depending on the number of hidden units m and their possible states k = |Yj| for all j ∈ [m].  Theorem 16. If(cid:81)  i∈[n]\Λ |Xi| ≤ 1 +(cid:80)  Leibler divergence from any distribution p on X to the model RBMX ,Y is bounded by  j∈[m](|Yj| − 1) = dY for some Λ ⊆ [n], then the Kullback-  D(p(cid:107) RBMX ,Y ) ≤ log  (cid:81) i∈Λ |Xi| maxi∈Λ |Xi|  .  In particular, the model RBMX ,Y is a universal approximator whenever dY ≥ |X|/maxi∈[n] |Xi|. Proof. The submodel P of RBMX ,Y described in the second item of Theorem 15 is a partition model. The maximal divergence from such a model is equal to the logarithm of the cardinality of the largest block with constant values [see Mat´uˇs and Ay, 2003]. Thus maxp D(p(cid:107) RBMX ,Y ) ≤  maxp D(p(cid:107)P) = log(cid:0)((cid:81) (cid:1), as was claimed. smaller than that of the independence model EX by at least log(1 +(cid:80)  Theorem 16 shows that, on a large scale, the maximal model approximation error of RBMX ,Y is j∈[m](|Yj|− 1)), or vanishes. The theorem is illustrated in Figure 4. The line k = 2 shows bounds on the approximation error of binary RBMs with m hidden units, previously treated in [Mont´ufar et al., 2011, Theorem 5.1], and the line m = 1 shows bounds for na¨ıve Bayes models with k hidden classes.  i∈Λ |Xi|)/ maxi∈Λ |Xi|  6 Dimension  In this section we study the dimension of the model RBMX ,Y. One reason RBMs are attractive is that they have a large learning capacity, e.g. may be built with millions of parameters. Dimension calculations show whether those parameters are wasted, or translate into higher-dimensional spaces of representable distributions. Our analysis builds on previous work by Cueto, Morton, and Sturm- fels [2010], where binary RBMs are treated. The idea is to bound the dimension from below by the dimension of a related max-plus model, called the tropical RBM model [Pachter and Sturmfels, 2004], and from above by the dimension expected from counting parameters.  11    0100200300400500100200300400500  01002003004005001002003004005000.511.522.5x 107828486889092949698MONT ´UFAR AND MORTON  The dimension of a discrete RBM model can be bounded from above not only by its expected  dimension, but also by a function of the dimension of its Hadamard factors:  Proposition 17. The dimension of RBMX ,Y is bounded as  dim(RBMX ,Y ) ≤ dim(MX ,|Yi|) +  dim(MX ,|Yj|−1) + (m − 1)  for all i ∈ [m]. (15)  (cid:88)  j∈[m]\{i}  Proof. Let u denote the uniform distribution. Note that EX◦EX = EX and also EX◦MX ,k = MX ,k. This observation, together with Proposition 7, shows that the RBM model can be factorized as  RBMX ,Y = (MX ,|Y1|) ◦ (λ1u + (1 − λ1)MX ,|Y1|) ◦ ··· ◦ (λmu + (1 − λm)MX ,|Ym|−1),  from which the claim follows.  By the previous proposition, the model RBMX ,Y can have the expected dimension only if (i) the right hand side of eq. (15) equals |X| − 1, or (ii) each mixture model MX ,k has the expected dimension for all k ≤ maxj∈[m] |Yj|. Sometimes none of both conditions is satisﬁed and the models ‘waste’ parameters: Example 18. The k-mixture of the independence model on X1 × X2 is a subset of the set of |X1| × |X2| matrices with non-negative entries and rank at most k. It is known that the set of M × N matrices of rank at most k has dimension k(M + N − k) for all 1 ≤ k < min{M, N}. Hence the model MX1×X2,k has dimension smaller than its parameter count whenever 1 < k < j∈[m](|Yj| − 1) + 1)(|X1| + |X2| − 1) ≤ |X1 × X2| and 1 < |Yj| ≤ min{|X1|,|X2|} for some j ∈ [m], then RBMX1×X2,Y does not have the expected dimension.  min{|X1|,|X2|}. By Proposition 17 if ((cid:80)  The next theorem indicates choices of X and Y for which the model RBMX ,Y has the expected dimension. Given a sufﬁcient statistics matrix A(X ), we say that a set Z ⊆ X has full rank when the matrix with columns {A(X ) Theorem 19. When X contains m disjoint Hamming balls of radii 2(|Yj| − 1) − 1, j ∈ [m] and the subset of X not intersected by these balls has full rank, then the model RBMX ,Y has dimension equal to the number of model parameters,  : x ∈ Z} has full rank.  x  dim(RBMX ,Y ) = (1 +  (|Xi| − 1))(1 +  (|Yj| − 1)) − 1.  (cid:88)  i∈[n]  (cid:88)  j∈[m]  On the other hand, if m Hamming balls of radius one cover X , then  dim(RBMX ,Y ) = |X| − 1.  In order to prove this theorem we will need two main tools: slicings by normal fans of simplices, described in Section 4, and the tropical RBM model, described in Section 7. The theorem will follow from the analysis contained in Section 7.  12  DISCRETE RESTRICTED BOLTZMANN MACHINES  7 Tropical model  Deﬁnition 20. The tropical model RBM  tropical X ,Y  is the image of the tropical morphism  RdX dY (cid:51) θ (cid:55)→ Φ(v; θ) = max{(cid:104)θ, A(X ,Y)  (v,h) (cid:105) : h ∈ Y}  for all v ∈ X ,  (16)  (v,h) (cid:105))) for all v ∈ X for each θ within the max-plus which evaluates log( 1 Z(θ) algebra (addition becomes a + b = max{a, b}) up to additive constants independent of v (i.e., disregarding the normalization factor Z(θ)).  h∈Y exp((cid:104)θ, A(X ,Y)  (cid:80)  The idea behind this deﬁnition is that log(exp(a)+exp(b)) ≈ max{a, b} when a and b have dif- ferent order of magnitude. The tropical model captures important properties of the original model. Of particular interest is following consequence of the Bieri-Groves theorem [see Draisma, 2008], which gives us a tool to estimate the dimension of RBMX ,Y:  tropical  (17)  dim(RBM  The following Theorem 21 describes the regions of linearity of the map Φ. Each of these : x ∈ X} for as the maximum rank of  X ,Y ) ≤ dim(RBMX ,Y ) ≤ min{dim(EX ,Y ),|X| − 1}. regions corresponds to a collection of Yj-slicings (see Deﬁnition 9) of the set {A(X ) all j ∈ [m]. This result allows us to express the dimension of RBM a class of matrices deﬁned by collections of slicings. For each j ∈ [m] let Cj = {Cj,1, . . . , Cj,|Yj|} be a Yj-slicing of {A(X ) otherwise. Let ACj = (ACj,1|···|ACj,|Yj|) ∈ R|X|×|Yj|dX and d =(cid:80) : x ∈ X} and let ACj,k be the |X| × dX -matrix with x-th row equal to (A(X ) x )(cid:62) when x ∈ Cj,k and equal to a row of zeros Theorem 21. On each region of linearity, the tropical morphism Φ is the linear map Rd →  j∈[m] |Yj|dX .  tropical X ,Y  x  x  RBM  tropical X ,Y  represented by the |X| × d-matrix  A = (AC1|···|ACm),  tropical  X ,Y ) + 1 is the maximum rank of A over all modulo constant functions. In particular, dim(RBM possible collections of slicings C1, . . . , Cm. Proof. Again use the homogeneous version of the matrix A(X ,Y) as in the proof of Proposition 7; this will not affect the rank of A. Let θhj = (θ{j,i},(hj ,xi))i∈[n],xi∈Xi and let Ahj denote the submatrix of A(X ,Y) containing the rows with indices {{j, i}, (hj, xi) : i ∈ [n], xi ∈ Xi}. For any given v ∈ X we have  (cid:110)(cid:10)θ, A(X ,Y)  (v,h)  (cid:111)  (cid:11) : h ∈ Y  max  (cid:88)  j∈[m]  =  max  (cid:110)(cid:10)θhj , Ahj (v, hj)(cid:11) : hj ∈ Yj  (cid:111)  ,  from which the claim follows.  by examining good slicings. We focus on slicings by parallel hyperplanes. Lemma 22. For any x∗  In the following we evaluate the maximum rank of the matrix A for various choices of X and Y : dH (x, x∗) = k} has  ∈ X and 0 < k < n the afﬁne hull of the set {A(X )  dimension(cid:80)  x  i∈[n](|Xi| − 1) − 1.  13  MONT ´UFAR AND MORTON  x  Proof. Without loss of generality let x∗ = (0, . . . , 0). The set Z k := {A(X ) : dH (x, x∗) = k} is the intersection of {A(X ) : x ∈ X} with the hyperplane H k := {z : (cid:104)1, z(cid:105) = k + 1}. Now note that the two vertices of an edge of QX either lie in the same hyperplane H l, or in two adjacent parallel hyperplanes H l and H l+1, with l ∈ N. Hence the hyperplane H k does not slice any edges of QX and conv(Z k) = QX ∩ H k. The set Z k is not contained in any proper face of QX and hence conv(Z k) intersects the interior of QX . Thus dim(conv(Z k)) = dim(QX ) − 1, as was claimed.  x  Lemma 22 implies the following.  Corollary 23. Let x ∈ X , and 2k − 3 ≤ n. There is a slicing C1 = {C1,1, . . . , C1,k} of X by k − 1 parallel hyperplanes such that ∪k−1 l=1 C1,l = Bx(2k − 3) is the Hamming ball of radius 2k − 3 centered at x and the matrix AC1 = (AC1,1|···|AC1,k−1) has full rank.  Recall that AX (d) denotes the maximal cardinality of a subset of X of minimum Hamming distance at least d. When X = {0, 1, . . . , q− 1}n we write Aq(n, d). Let KX (d) denote the minimal cardinality of a subset of X with covering radius d. (cid:80) Proposition 24 (Binary visible units). Let X = {0, 1}n and |Yj| = sj for all j ∈ [m]. If X contains m disjoint Hamming balls of radii 2sj − 3, j ∈ [m] whose complement has full rank, then j∈[m](sj − 1)(n + 1) + n, 2n − 1}. RBM  tropical X ,Y In particular, if X = {0, 1}n and Y = {0, 1, . . . , s−1}m with m < A2(n, d) and d = 4(s−1)− j ))(cid:101).  1, then RBMX ,Y has the expected dimension. It is known that A2(n, d) ≥ 2n−(cid:100)log2((cid:80)d−2  has the expected dimension, min{  j=0 (n−1  Proposition 25 (Binary hidden units). Let Y = {0, 1}m and X be arbitrary.  X ,{0,1}m has dimension (1 + m)(1 +(cid:80)  tropical  • If m + 1 ≤ AX (3), then RBM • If m + 1 ≥ KX (1), then RBM  tropical  X ,{0,1}m has dimension |X| − 1.  i∈[n](|Xi| − 1)) − 1.  Let Y = {0, 1}m and X = {0, 1, . . . , q − 1}n, where q is a prime power.  (1 + m)(1 +(cid:80) • If m + 1 ≤ qn−(cid:100)logq(1+(n−1)(q−1)+1)(cid:101), then RBM  i∈[n](|Xi| − 1)) − 1.  tropical X ,Y  has dimension  • If n = (qr − 1)/(q − 1) for some r ≥ 2, then AX (3) = KX (1), and RBM  expected dimension for any m.  tropical X ,Y  has the  In particular, when all units are binary and m < 2n−(cid:100)log2(n+1)(cid:101), then RBMX ,Y has the expected  dimension; this was shown in [Cueto et al., 2010].  Proposition 26 (Arbitrary sized units). If X contains m disjoint Hamming balls of radii 2|Y1| − has the expected 3, . . . , 2|Ym|−3, and the complement of their union has full rank, then RBM dimension.  tropical X ,Y  14  DISCRETE RESTRICTED BOLTZMANN MACHINES  Proof. Propositions 24, 25, and 26 follow from Theorem 21 and Corollary 23 together with the following explicit bounds on A by [Gilbert, 1952, Varshamov, 1957]:  Aq(n, d) ≥  (cid:80)d−1  j=0  qn  (cid:0)n  j  (cid:1)(q − 1)j  .  (cid:80)d−2 qn j=0 (n−1  2n  j )(q−1)j . (n−1)+1 = 2n−log2(n), i.e.,  If q is a prime power, then Aq(n, d) ≥ qk, where k is the largest integer with qk < In particular, A2(n, 3) ≥ 2k, where k is the largest integer with 2k < k = n − (cid:100)log2(n + 1)(cid:101). Example 27. Many results in coding theory can now be translated directly to statements about (cid:106) (3s−t)2 the dimension of discrete RBMs. Here is an example. Let X = {1, 2, . . . , s} × {1, 2, . . . , s} × {1, 2, . . . , t}, s ≤ t. The minimum cardinality of a code C ⊆ X with covering-radius one equals if t ≤ 3s, and KX (1) = s2 otherwise [see Cohen et al., 2005, Theo- KX (1) = s2 − rem 3.7.4]. Hence RBM and t ≤ 3s, and when m + 1 ≥ s2 and t > 3s. 8 Discussion  (cid:107) X ,{0,1}m has dimension |X|− 1 when m + 1 ≥ s2 −  (cid:106) (3s−t)2  (cid:107)  tropical  8  8  In this note we study the representational power of RBMs with discrete units. Our results generalize a diversity of previously known results for standard binary RBMs and na¨ıve Bayes models. They help contrasting the geometric-combinatorial properties of distributed products of experts versus non-distributed mixtures of experts.  We estimate the number of hidden units for which discrete RBM models can approximate any distribution to any desired accuracy, depending on the cardinalities of their units’ state spaces. This analysis shows that the maximal approximation error increases at most logarithmically with the total number of visible states and decreases at least logarithmically with the sum of the number of states of the hidden units. This observation could be helpful, for example, in designing a penalty term to allow comparison of models with differing numbers of units. It is worth mentioning that the submodels of discrete RBMs described in Theorem 15 can be used not only to estimate the maximal model approximation errors, but also the expected model approximation errors given a prior of target distributions on the probability simplex. See [Mont´ufar and Rauh, 2012] for an exact analysis of Dirichlet priors. In future work it would be interesting to study the statistical approximation errors of discrete RBMs and to complement the theory by an empirical evaluation.  The combinatorics of tropical discrete RBMs allows us to relate the dimension of discrete RBM models to the solutions of linear optimization problems and slicings of convex support polytopes by normal fans of simplices. We use this to show that the model RBMX ,Y has the expected dimension for many choices of X and Y, but not for all choices. We based our explicit computations of the dimension of RBMs on slicings by collections of parallel hyperplanes, but more general classes of slicings may be considered. The same tools presented in this paper can be used to estimate the dimension of a general class of models involving interactions within layers, deﬁned as Kronecker products of hierarchical models [see Mont´ufar and Morton, 2013]. We think that the geometric- combinatorial picture of discrete RBMs developed in this paper may be helpful in solving various long standing theoretical problems in the future, for example: What is the exact dimension of na¨ıve  15  MONT ´UFAR AND MORTON  Bayes models with general discrete variables? What is the smallest number of hidden variables that make an RBM a universal approximator? Do binary RBMs always have the expected dimension?  Acknowledgments  We are grateful to the ICLR 2013 community for very valuable comments. This work was accom- plished in part at the Max Planck Institute for Mathematics in the Sciences. This work is supported in part by DARPA grant FA8650-11-1-7145.  References  M. Aoyagi. Stochastic complexity and generalization error of a Restricted Boltzmann Machine in  Bayesian estimation. J. Mach. Learn. Res., 99:1243–1272, August 2010.  N. Ay and A. Knauf. Maximizing multi-information. Kybernetika, 42(5):517–538, 2006.  Y. Bengio. Learning deep architectures for AI. Found. Trends Mach. Learn., 2(1):1–127, 2009.  Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 153–160. MIT Press, Cambridge, MA, 2007.  M. A. Carreira-Perpi˜nan and G. E. Hinton. On contrastive divergence learning. In Proceedings of  the 10-th Interantional Workshop on Artiﬁcial Intelligence and Statistics, 2005. M. V. Catalisano, A. V. Geramita, and A. Gimigliano. Secant varieties of P1 × ··· × P1 (n-times) are not defective for n ≥ 5. J. Algebraic Geometry, 20:295–327, 2011.  G. Cohen, I. Honkala, S. Litsyn, and A. Lobstein. Covering Codes. North-Holland Mathematical  Library. Elsevier Science, 2005.  M. A. Cueto, J. Morton, and B. Sturmfels. Geometry of the restricted Boltzmann machine.  In M. Viana and H. Wynn, editors, Algebraic methods in statistics and probability II, AMS Special Session, volume 2. American Mathematical Society, 2010.  G. E. Dahl, R. P. Adams, and H. Larochelle. Training restricted Boltzmann machines on word  observations. arXiv:1202.5695, 2012.  A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38, 1977.  J. Draisma. A tropical approach to secant dimensions. J. Pure Appl. Algebra, 212(2):349–363,  2008.  Y. Freund and D. Haussler. Unsupervised learning of distributions of binary vectors using 2-layer networks. In J. E. Moody, S. J. Hanson, and R. Lippmann, editors, Advances in Neural Informa- tion Processing Systems 4, pages 912–919. Morgan Kaufmann, 1991.  16  DISCRETE RESTRICTED BOLTZMANN MACHINES  E. N. Gilbert. A comparison of signalling alphabets. Bell System Technical Journal, 31:504–522,  1952.  G. E. Hinton. Products of experts. In Proceedings 9-th ICANN, volume 1, pages 1–6, 1999.  G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Compu-  tation, 14:1771–1800, 2002.  G. E. Hinton. A practical guide to training restricted Boltzmann machines, version 1. Technical  report, UTML2010-003, University of Toronto, 2010.  G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural  Computation, 18:1527–1554, 2006.  N. Le Roux and Y. Bengio. Representational power of restricted Boltzmann machines and deep  belief networks. Neural Computation, 20(6):1631–1649, 2008.  P. M. Long and R. A. Servedio. Restricted Boltzmann machines are hard to approximately evaluate In J. F¨urnkranz and T. Joachims, editors, Proceedings of the 27th International  or simulate. Conference on Machine Learning (ICML-10), pages 703–710. Omnipress, 2010.  D. Lowd and P. Domingos. Naive Bayes models for probability estimation. In Proceedings of the  22nd International Conference on Machine Learning, pages 529–536. ACM Press, 2005.  T. K. Marks and J. R. Movellan. Diffusion networks, products of experts, and factor analysis. In  Proc. 3rd Int. Conf. Independent Component Anal. Signal Separation, pages 481–485, 2001.  F. Mat´uˇs and N. Ay. On maximization of the information divergence from an exponential family. In  Proceedings of the WUPES’03, pages 199–204. University of Economics, Prague, 2003.  R. Memisevic and G. E. Hinton. Learning to represent spatial transformations with factored higher-  order Boltzmann machines. Neural Computation, 22(6):1473–1492, June 2010.  G. Mont´ufar. Mixture decompositions of exponential families using a decomposition of their sample  spaces. Kybernetika, 49(1), 2013.  G. Mont´ufar and N. Ay. Reﬁnements of universal approximation results for deep belief networks  and restricted Boltzmann machines. Neural Computation, 23(5):1306–1319, 2011.  G. Mont´ufar and J. Morton. When does a mixture of products contain a product of mixtures?  http://arxiv.org/abs/1206.0387, 2012.  G. Mont´ufar and J. Morton. Geometry of hierarchical models on hidden-visible products of simpli-  cial complexes. 2013. In preparation.  G. Mont´ufar and J. Rauh. Scaling of model approximation errors and expected entropy distances. In Proc. of the 9th Workshop on Uncertainty Processing (WUPES 2012), pages 137–148, 2012. Preprint available at http://arxiv.org/abs/1207.3399.  G. Mont´ufar, J. Rauh, and N. Ay. Expressive power and approximation errors of restricted Boltz- mann machines. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. C. N. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 415–423, 2011.  17  MONT ´UFAR AND MORTON  S. Osindero and G. E. Hinton. Modeling image patches with a directed hierarchy of Markov random ﬁelds. In J. C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1121–1128. MIT Press, Cambridge, MA, 2008.  L. Pachter and B. Sturmfels. Tropical geometry of statistical models. Proceedings of the National  Academy of Sciences of the United States of America, 101(46):16132–16137, Nov. 2004.  M. Ranzato, A. Krizhevsky, and G. E. Hinton. Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images. In Proc. Thirteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 621–628, 2010.  W. E. Roth. On direct product matrices. Bulletin of the American Mathematical Society, 40:461–  468, 1934.  R. Salakhutdinov, A. Mnih, and G. E. Hinton. Restricted Boltzmann machines for collaborative ﬁltering. In Proceedings of the 24th International Conference on Machine Learning, pages 791– 798, 2007.  T. J. Sejnowski. Higher-order Boltzmann machines.  In Neural Networks for Computing, pages  398–403. American Institute of Physics, 1986.  P. Smolensky. Information processing in dynamical systems: foundations of harmony theory. In  Symposium on Parallel and Distributed Processing, 1986.  T. Tran, D. Phung, and S. Venkatesh. Mixed-variate restricted Boltzmann machines. In Proc. of 3rd  Asian Conference on Machine Learning (ACML), pages 213–229, 2011.  R. R. Varshamov. Estimate of the number of signals in error correcting codes. Doklady Akad. Nauk  SSSR, 117:739–741, 1957.  M. Welling, M. Rosen-Zvi, and G. E. Hinton. Exponential family harmoniums with an application In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural  to information retrieval. Information Processing Systems 17, pages 1481–1488. MIT Press, Cambridge, MA, 2005.  18  ","We describe discrete restricted Boltzmann machines: probabilistic graphicalmodels with bipartite interactions between visible and hidden discretevariables. Examples are binary restricted Boltzmann machines and discrete naiveBayes models. We detail the inference functions and distributed representationsarising in these models in terms of configurations of projected products ofsimplices and normal fans of products of simplices. We bound the number ofhidden variables, depending on the cardinalities of their state spaces, forwhich these models can approximate any probability distribution on theirvisible states to any given accuracy. In addition, we use algebraic methods andcoding theory to compute their dimension."
1301.3572,2013,Indoor Semantic Segmentation using depth information  ,"['Camille Couprie', 'Clement Farabet', 'Laurent Najman', 'Yann LeCun']",https://arxiv.org/pdf/1301.3572.pdf,"3 1 0 2    r a     M 4 1      ]  V C . s c [      2 v 2 7 5 3  .  1 0 3 1 : v i X r a  Indoor Semantic Segmentation  using depth information  Camille Couprie1∗  Cl´ement Farabet2,3  Laurent Najman3  Yann LeCun2  Technology, Computer Science and Applied Mathematics Division  1 IFP Energies Nouvelles  Rueil Malmaison, France  2 Courant Institute of Mathematical Sciences  New York University  New York, NY 10003, USA  3 Universit´e Paris-Est  Laboratoire d’Informatique Gaspard-Monge  ´Equipe A3SI - ESIEE Paris, France  Abstract  This work addresses multi-class segmentation of indoor scenes with RGB-D in- puts. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be pro- cessed in real-time using appropriate hardware such as an FPGA.  1  Introduction  The recent release of the Kinect allowed many progress in indoor computer vision. Most approaches have focused on object recognition [1, 14] or point cloud semantic labeling [2], ﬁnding their appli- cations in robotics or games [6]. The pioneering work of Silberman et al. [22] was the ﬁrst to deal with the task of semantic full image labeling using depth information. The NYU depth v1 dataset [22] guathers 2347 triplets of images, depth maps, and ground truth labeled images covering twelve object categories. Most datasets employed for semantic image segmentation [11, 17] present the objects centered into the images, under nice lightening conditions. The NYU depth dataset aims to develop joint segmentation and classiﬁcation solutions to an environment that we are likely to en- counter in the everyday life. This indoor dataset contains scenes of ofﬁces, stores, rooms of houses containing many occluded objects unevenly lightened. The ﬁrst results [22] on this dataset were obtained using the extraction of sift features on the depth maps in addition to the RGB images. The depth is then used in the gradient information to reﬁne the predictions using graph cuts. Alternative CRF-like approaches have also been explored to improve the computation time performances [4]. The results on NYU dataset v1 have been improved by [19] using elaborate kernel descriptors and a post-processing step that employs gPb superpixels MRFs, involving large computation times. A second version of the NYU depth dataset was released more recently [23], and improves the labels categorization into 894 different object classes. Furthermore, the size of the dataset did also increase, it now contains hundreds of video sequences (407024 frames) acquired with depth maps. Feature learning, or deep learning approaches are particularly adapted to the addition of new image modalities such as depth information. Its recent success for dealing with various types of data is manifest in speech recognition [13], molecular activity prediction, object recognition [12] and many  1∗ Performed the work at New York University.  1  more applications. In computer vision, the approach of Farabet et al. [8, 9] has been speciﬁcally designed for full scene labeling and has proven its efﬁciency for outdoor scenes. The key idea is to learn hierarchical features by the mean of a multiscale convolutional network. Training networks using multiscales representation appeared also the same year in [3, 21]. When the depth information was not yet available, there have been attempts to use stereo image pairs to improve the feature learning of convolutional networks [16]. Now that depth maps are easy to acquire, deep learning approachs started to be considered for improving object recognition [20]. In this work, we suggest to adapt Farabet et al.’s network to learn more effective features for indoor scene labeling. Our work is, to the best of our knowledge, the ﬁrst exploitation of depth information in a feature learning approach for full scene labeling.  2 Full scene labeling  2.1 Multi-scale feature extraction  Good internal representations are hierarchical. In vision, pixels are assembled into edglets, edglets into motifs, motifs into parts, parts into objects, and objects into scenes. This suggests that recog- nition architectures for vision (and for other modalities such as audio and natural language) should have multiple trainable stages stacked on top of each other, one for each level in the feature hierar- chy. Convolutional Networks [15] (ConvNets) provide a simple framework to learn such hierarchies of features. Convolutional Networks are trainable architectures composed of multiple stages. The input and output of each stage are sets of arrays called feature maps. In our case, the input is a color (RGB) image plus a depth (D) image and each feature map is a 2D array containing a color or depth channel of the input RGBD image. At the output, each feature map represents a particular feature extracted at all locations on the input. Each stage is composed of three layers: a ﬁlter bank layer, a non-linearity layer, and a feature pooling layer. A typical ConvNet is composed of one, two or three such 3-layer stages, followed by a classiﬁcation module. Because they are trainable, arbitrary input modalities can be modeled, such as the depth modality that is added to the input channel in this work.  Figure 1: Scene parsing (frame by frame) using a multiscale network and superpixels. The RGB channels of the image and the depth image are transformed through a Laplacian pyramid. Each scale is fed to a 3-stage convolutional network, which produces a set of feature maps. The feature maps of all scales are concatenated, the coarser-scale maps being upsampled to match the size of the ﬁnest-scale map. Each feature vector thus represents a large contextual window around each pixel. In parallel, a single segmentation of the image into superpixels is computed to exploit the natural contours of the image. The ﬁnal labeling is obtained by the aggregation of the classiﬁer predictions into the superpixels.  A great gain has been achieved with the introduction of the multiscale convolutional network de- scribed in [9]. The multi-scale, dense feature extractor produces a series of feature vectors for regions of multiple sizes centered around every pixel in the image, covering a large context. The  2   F3convnet F1 F2F  f1 (X1;𝛉1)labelingl (F, h (I))superpixelsRGBDLaplacianpyramidg (I) table wall ceiling chair chair pict. pict. f3 (X3;𝛉3) f2 (X2;𝛉2)Input RGB imageInput depth imagesegmentationh (I)multi-scale convolutional net contains multiple copies of a single network that are applied to differ- ent scales of a Laplacian pyramid version of the RGBD input image. The RGBD image is ﬁrst pre-processed, so that local neighborhoods have zero mean and unit stan- dard deviation. The depth image, given in meters, is treated as an additional channel similarly to any color channel. The overview scheme of our model appears in Figure 1. Beside the input image which is now including a depth channel, the parameters of the multi-scale network (number of scales, sizes of feature maps, pooling type, etc.) are identical to [9]. The feature maps sizes are 16,64,256, multiplied by the three scales. The size of convolutions kernels are set to 7 by 7 at each layer, and sizes of subsampling kernels (max pooling) are 2 by 2. In our tests we rescaled the images to the size 240 × 320. As in [9], the feature extractor followed by a classiﬁer was trained to minimize the negative log- likelihood loss function. The classiﬁer that follows feature extraction is a 2-layer multi-perceptron, with a hidden layer of size 1024. We use superpixels [10] to smooth the convnet predictions as a post-processing step, by agregating the classiﬁers predictions in each superpixel.  2.2 Movie processing  While the training is performed on single images, we are able to perform scene labeling of video sequences. In order to improve the performances of our frame-by-frame predictions, a temporal smoothing may be applied. In this work, instead of using the frame by frame superpixels as in the previous section, we employ the temporal consistent superpixels of [5]. This approach works in quasi-linear time and reduces the ﬂickering of objects that may appear in the video sequences.  3 Results  We used for our experiments the NYU depth dataset – version 2 – of Silberman and Fergus [23], composed of 407024 couples of RGB images and depth images. Among these images, 1449 frames have been labeled. The object labels cover 894 categories. The dataset is provided with the original raw depth data that contain missing values, with code using [7] to inpaint the depth images.  3.1 Validation on images  The training has been performed using the 894 categories directly as output classes. The frequencies of object appearences have not been changed in the training process. However, we established 14 clusters of classes categories to evaluate our results more easily. The distributions of number of pixels per class categories are given in Table 1. We used the train/test splits as provided by the NYU depth v2 dataset, that is to say 795 training images and 654 test images. Please note that no jitter (rotation, translations or any other transformation) was added to the dataset to gain extra performances. However, this strategy could be employed in future work. The code consists of Lua scripts using the Torch machine learning software [18] available online at http://www.torch.ch/ . To evaluate the inﬂuence of the addition of depth information, we trained a multiscale convnet only on the RGB channels, and another network using the additional depth information. Both networks were trained until the achievement of their best performances, that is to say for 105 epochs and 98 epochs respectively, taking less than 2 days on a regular server. We report in Table 1 two different performance measures:  • the “classwise accuracy”, counting the number of correctly classiﬁed pixels divided by the number of false positive, averaged for each class. This number corresponds to the mean of the confusion matrix diagonal. • the “pixelwise accuracy”, counting the number of correctly classiﬁed pixels divided by the  total number of pixels of the test data.  We observe that considerable gains (15% or more) are achieved for the classes ’ﬂoor’, ’ceiling’, and ’furniture’. This result makes a lot of sense since these classes are characterized by a somehow constant appearance of their depth map. Objects such as TV, table, books can either be located in  3  Ground truths  Results using the Multiscale Convnet  Results using the Multiscale Convnet with depth information  wall bed  books ceiling  chair ﬂoor  furniture pict./deco  sofa table  object window  TV uknw  Ground truths  Depth maps  Results using the Multiscale Convnet  Results using the Multiscale Convnet with depth information  Figure 2: Some scene labelings using our Multiscale Convolutional Network trained on RGB and RGBD images. We observe in Table 1 that adding depth information helps to recognize objects that have low intra-class variance of depth appearance.  the foreground as well as in the background of images. On the contrary, the ﬂoor and ceiling will almost always lead to a depth gradient always oriented in the same direction: Since the dataset has been collected by a person holding a kinect device at a his chest, ﬂoors and ceiling are located at a distance that does not vary to much through the dataset. Figure 2 provides examples of depth  4  bed objects chair furnit. ceiling ﬂoor deco. sofa table wall window books TV unkn.  Occurrences Convnet Acc. [9]  Multiscale  Class  4.4% 7.1 % 3.4% 12.3% 1.4% 9.9% 3.4% 3.2% 3.7% 24.5% 5.1% 2.9% 1.0% 17.8%  MultiScl. Cnet +depth Acc.  38.1 8.7 34.1 42.4 62.6 87.3 40.4 24.6 10.2 86.1 15.9 13.7 6.0 -  30.3 10.9 44.4 28.5 33.2 68.0 38.5 25.8 18.0 89.4 37.8 31.7 18.8  -  Avg. Class Acc. Pixel Accuracy (mean) Pixel Accuracy (median) Pixel Accuracy (std. dev.)  - - - -  35.8 51.0 51.7 15.2  36.2 52.4 52.9 15.2  Table 1: Class occurrences in the test set – Performances per class and per pixel.  maps that illustrate these observations. Overall, improvements induced by the depth information exploitation are present. In the next section, these improvements are more apparent.  3.2 Comparison with Silberman et al.  In order to compare our results to the state-of-the-art on the NYU depth v2 dataset, we adopted a different selection of outputs instead of the 14 classes employed in the previous section. The work of Silberman et al. [23] deﬁnes the four semantic classes Ground, Furniture, Props and Structure. This class selection is adopted in [23] to use semantic labelings of scenes to infer support relations between objects. We recall that the recognition of the semantic categories is performed in [23] by the deﬁnition of diverse features including SIFT features, histograms of surface normals, 2D and 3D bounding box dimensions, color histograms, and relative depth.  Silberman et al.[23] Multiscale convnet [9]  Multiscale+depth convnet  Ground  68 68.1 87.3  Furniture  70 51.1 45.3  Props 42 29.9 35.5  Structure  59 87.8 86.1  Class Acc.  59.6 59.2 63.5  Pixel Acc.  58.6 63.0 64.5  Table 2: Accuracy of the multiscale convnet compared with the state-of-the-art approach of [23].  As reported in Table 2, the results achieved using the Multiscale convnet are improving the structure class predictions, resulting in a 4% gain in pixelwise accuracy over Silberman et al. approach. Adding the depth information results in a considerable improvement of the ground prediction, and performs also better over the other classes, achieving a 4% gain in classwise accuracy over previous works and improves by almost 6% the pixelwise accuracy compared to Silberman et al.’s results. We note that the class ’furniture’ in the 4-classes evaluation is different than the ’furniture’ class of the 14-classes evaluation. The furniture-4 class encompasses chairs and beds but not desks, and cabinets for example, explaining a drop of performances here using the depth information.  5  3.3 Test on videos  The NYU v2 depth dataset contains several hundreds of video sequences encompassing 26 different classes of indoor scenes, going from bedrooms to basements, and dining rooms to book stores. Unfortunately, no ground truth is yet available to evaluate our performances on this video. Therefore, we only present here some illustrations of the capacity of our model to label these scenes. The predictions are computed frame by frame on the videos and are reﬁned using temporally smoothed superpixels using [5]. Two examples of results on sequences are shown at Figure 3. A great advantage of our approach is its nearly real time capabilities. Processing a 320x240 frame takes 0.7 seconds on a laptop [9]. The temporal smoothing only requires an additional 0.1s per frame.  (a) Output of the Multiscale convnet trained using depth information - frame by frame  (b) Results smoothed temporally using [5]  Props  Floor  Structure  Wall  (c) Output of the Multiscale convnet trained using depth information - frame by frame  (d) Results smoothed temporally using [5]  Figure 3: Some results on video sequences of the NYU v2 depth dataset. Note that results (c,d) could be improved by using more training examples. Indeed, only a very small number in the labeled training examples exhibit a wall in the foreground.  4 Conclusion  Feature learning is a particularly satisfying strategy to adopt when approaching a dataset that con- tains new image (or other kind of data) modalities. Our model, while being faster and more efﬁcient than previous approaches, is easier to implement without the need to design speciﬁc features adapted to depth information. Different clusterings of object classes as the ones used in this work may be chosen, reﬂecting this work’s ﬂexibility of applications. For example, using the 4-classes clustering, the accurate results achieved with the multi-scale convolutional network could be applied to perform inference on support relations between objects. Improvements for speciﬁc object recognition could further be achieved by ﬁltering the frequency of the training objects. We observe that the recog- nition of object classes having similar depth appearance and location is improved when using the depth information. On the contrary, it is better to use only RGB information to recognize objects with classes containing high variability of their depth maps. This observation could be used to com- bine the best results in function of the application. Finally, a number of techniques (unsupervised  6  feature learning, MRF smoothing of the convnet predictions, extension of the training set) would probably help to improve the present system.  5 Acknowledgments  We would like to thank Nathan Silberman for his useful input for handling the NYU depth v2 dataset, and fruitful discussions.  References  [1] B3do: Berkeley 3-d object dataset. http://kinectdata.com/. 1 [2] Cornell-rgbd-dataset. http://pr.cs.cornell.edu/sceneunderstanding/data/data.php. 1 [3] Dan Claudiu Ciresan, Alessandro Giusti, Luca Maria Gambardella, and J¨urgen Schmidhuber. Deep neural networks segment neuronal membranes in electron microscopy images. In NIPS, pages 2852–2860, 2012. 1  [4] Camille Couprie. Multi-label energy minimization for object class segmentation.  In 20th European Signal Processing Conference 2012 (EUSIPCO 2012), Bucharest, Romania, August 2012. 1  [5] Camille Couprie, Cl´ement Farabet, and Yann LeCun. Causal graph-based video segmentation,  2013. arXiv:1301.1671. 2.2, 3.3  [6] L. Cruz, D. Lucio, and L. Velho. Kinect and rgbd images: Challenges and applications. SIB-  GRAPI Tutorial, 2012. 1  [7] Anat Levin Dani, Dani Lischinski, and Yair Weiss. Colorization using optimization. ACM  Transactions on Graphics, 23:689–694, 2004. 3  [8] Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers. In Proc. of the 2012 Interna- tional Conference on Machine Learning, Edinburgh, Scotland, June 2012. 1  [9] Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning hierarchical features for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013. In press. 1, 2.1, 3.1, 3.2, 3.3  [10] Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Efﬁcient graph-based image segmentation.  International Journal of Computer Vision, 59:2004, 2004. 2.1  [11] Stephen Gould, Richard Fulton, and Daphne Koller. Decomposing a Scene into Geometric and Semantically Consistent Regions. In IEEE International Conference on Computer Vision, 2009. 1  [12] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut- dinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012. 1  [13] Navdeep Jaitly, Patrick Nguyen, Andrew Senior, and Vincent Vanhoucke. Application of pre- trained deep neural networks to large vocabulary speech recognition. In Proceedings of Inter- speech 2012, 2012. 1  [14] Allison Janoch, Sergey Karayev, Yangqing Jia, Jonathan T. Barron, Mario Fritz, Kate Saenko, and Trevor Darrell. A category-level 3-d object dataset: Putting the kinect to work. In ICCV Workshops, pages 1168–1174. IEEE, 2011. 1  [15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document  recognition. Proceedings of the IEEE, 86(11):2278 –2324, nov 1998. 2.1  [16] Yann LeCun, Fu-Jie Huang, and Leon Bottou. Learning Methods for generic object recognition  with invariance to pose and lighting. In Proceedings of CVPR’04. IEEE, 2004. 1  [17] Ce Liu, Jenny Yuen, and Antonio Torralba. SIFT Flow: Dense Correspondence across Scenes and its Applications. IEEE transactions on pattern analysis and machine intelligence, pages 1–17, August 2010. 1  7  [18] C. Farabet R. Collobert, K. Kavukcuoglu. Torch7: A matlab-like environment for machines  learning. In Big Learning Workshop (@ NIPS’11), Sierra Nevada, Spain, 2011. 3.1  [19] Xiaofeng Ren, Liefeng Bo, and D. Fox. Rgb-(d) scene labeling: Features and algorithms. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2759 –2766, june 2012. 1  [20] Richard Socher and Brody Huval and Bharath Bhat and Christopher D. Manning and Andrew Y. Ng. Convolutional-Recursive Deep Learning for 3D Object Classiﬁcation. In Advances in Neural Information Processing Systems 25. 2012. 1  [21] Hannes Schulz and Sven Behnke. Learning object-class segmentation with convolutional neu- In 11th European Symposium on Artiﬁcial Neural Networks (ESANN), 2012.  ral networks. 1  [22] Nathan Silberman and Rob Fergus. Indoor scene segmentation using a structured light sensor.  In 3DRR Workshop, ICCV’11, 2011. 1  [23] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and  support inference from rgbd images. In ECCV, 2012. 1, 3, 3.2, 2  8  ","This work addresses multi-class segmentation of indoor scenes with RGB-Dinputs. While this area of research has gained much attention recently, mostworks still rely on hand-crafted features. In contrast, we apply a multiscaleconvolutional network to learn features directly from the images and the depthinformation. We obtain state-of-the-art on the NYU-v2 depth dataset with anaccuracy of 64.5%. We illustrate the labeling of indoor scenes in videossequences that could be processed in real-time using appropriate hardware suchas an FPGA."
1301.3557,2013,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks  ,"['Matthew Zeiler', 'Rob Fergus']",https://arxiv.org/pdf/1301.3557.pdf,"3 1 0 2     n a J    6 1      ]  G L . s c [      1 v 7 5 5 3  .  1 0 3 1 : v i X r a  Stochastic Pooling for Regularization of Deep Convolutional Neural Networks  Matthew D. Zeiler  Department of Computer Science  Courant Institute, New York University  zeiler@cs.nyu.edu  Rob Fergus  Department of Computer Science  Courant Institute, New York University  fergus@cs.nyu.edu  Abstract  We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pool- ing region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other ap- proaches that do not utilize data augmentation.  1  Introduction  Neural network models are prone to over-ﬁtting due to their high capacity. A range of regularization techniques are used to prevent this, such as weight decay, weight tying and the augmentation of the training set with transformed copies [9]. These allow the training of larger capacity models than would otherwise be possible, which yield superior test performance compared to smaller un- regularized models. Dropout, recently proposed by Hinton et al. [2], is another regularization approach that stochastically sets half the activations within a layer to zero for each training sample during training. It has been shown to deliver signiﬁcant gains in performance across a wide range of problems, although the reasons for its efﬁcacy are not yet fully understood. A drawback to dropout is that it does not seem to have the same beneﬁts for convolutional layers, which are common in many networks designed for vision tasks. In this paper, we propose a novel type of regularization for convolutional layers that enables the training of larger models without over-ﬁtting, and produces superior performance on recognition tasks. The key idea is to make the pooling that occurs in each convolutional layer a stochastic process. Conventional forms of pooling such as average and max are deterministic, the latter selecting the largest activation in each pooling region. In our stochastic pooling, the selected activation is drawn from a multinomial distribution formed by the activations within the pooling region. An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images [13], which delivers excellent MNIST performance. Other types of data augmentation, such as ﬂipping and cropping differ in that they are global image transforma- tions. Furthermore, using stochastic pooling in a multi-layer model gives an exponential number of deformations since the selections in higher layers are independent of those below.  1  2 Review of Convolutional Networks  Our stochastic pooling scheme is designed for use in a standard convolutional neural network archi- tecture. We ﬁrst review this model, along with conventional pooling schemes, before introducing our novel stochastic pooling approach. A classical convolutional network is composed of alternating layers of convolution and pooling (i.e. subsampling). The aim of the ﬁrst convolutional layer is to extract patterns found within local regions of the input images that are common throughout the dataset. This is done by convolving a template or ﬁlter over the input image pixels, computing the inner product of the template at every location in the image and outputting this as a feature map c, for each ﬁlter in the layer. This output is a measure of how well the template matches each portion of the image. A non-linear function f () is then applied element-wise to each feature map c: a = f (c). The resulting activations a are then passed to the pooling layer. This aggregates the information within a set of small local regions, R, producing a pooled feature map s (of smaller size) as output. Denoting the aggregation function as pool(), for each feature map c we have:  sj = pool(f (ci)) ∀i ∈ Rj  where Rj is pooling region j in feature map c and i is the index of each element within it. The motivation behind pooling is that the activations in the pooled map s are less sensitive to the precise locations of structures within the image than the original feature map c. In a multi-layer model, the convolutional layers, which take the pooled maps as input, can thus extract features that are increasingly invariant to local transformations of the input image. This is important for classiﬁcation tasks, since these transformations obfuscate the object identity. A range of functions can be used for f (), with tanh() and logistic functions being popular choices. In this is paper we use a linear rectiﬁcation function f (c) = max(0, c) as the non-linearity. In general, this has been shown [10] to have signiﬁcant beneﬁts over tanh() or logistic functions. However, it is especially suited to our pooling mechanism since: (i) our formulation involves the non-negativity of elements in the pooling regions and (ii) the clipping of negative responses intro- duces zeros into the pooling regions, ensuring that the stochastic sampling is selecting from a few speciﬁc locations (those with strong responses), rather than all possible locations in the region. There are two conventional choices for pool(): average and max. The former takes the arithmetic mean of the elements in each pooling region:  1 |Rj| while the max operation selects the largest element:  sj =  (cid:88)  i∈Rj  ai  sj = max i∈Rj  ai  (1)  (2)  (3)  Both types of pooling have drawbacks when training deep convolutional networks. In average pool- ing, all elements in a pooling region are considered, even if many have low magnitude. When com- bined with linear rectiﬁcation non-linearities, this has the effect of down-weighting strong activa- tions since many zero elements are included in the average. Even worse, with tanh() non-linearities, strong positive and negative activations can cancel each other out, leading to small pooled responses. While max pooling does not suffer from these drawbacks, we ﬁnd it easily overﬁts the training set in practice, making it hard to generalize well to test examples. Our proposed pooling scheme has the advantages of max pooling but its stochastic nature helps prevent over-ﬁtting.  3 Stochastic Pooling  In stochastic pooling, we select the pooled map response by sampling from a multinomial distri- bution formed from the activations of each pooling region. More precisely, we ﬁrst compute the probabilities p for each region j by normalizing the activations within the region:  pi =  ak  (4)  ai(cid:80)  k∈Rj  2  We then sample from the multinomial distribution based on p to pick a location l within the region. The pooled activation is then simply al:  sj = al where l ∼ P (p1, . . . , p|Rj|)  (5)  The procedure is illustrated in Fig. 1. The samples for each pooling region in each layer for each training example are drawn independently to one another. When back-propagating through the network this same selected location l is used to direct the gradient back through the pooling region, analogous to back-propagation with max pooling. Max pooling only captures the strongest activation of the ﬁlter template with the input for each region. However, there may be additional activations in the same pooling region that should be taken into account when passing information up the network and stochastic pooling ensures that these non-maximal activations will also be utilized.  Figure 1: Toy example illustrating stochastic pooling. a) Input image. b) Convolutional ﬁlter. c) Rectiﬁed linear function. d) Resulting activations within a given pooling region. e) Probabilities based on the activations. f) Sampled activation. Note that the selected element for the pooling region may not be the largest element. Stochastic pooling can thus represent multi-modal distributions of activations within a region.  3.1 Probabilistic Weighting at Test Time  Using stochastic pooling at test time introduces noise into the network’s predictions which we found to degrade performance (see Section 4.7). Instead, we use a probabilistic form of averaging. In this, the activations in each region are weighted by the probability pi (see Eqn. 4) and summed:  (cid:88) ing and the denominator is the sum of activations(cid:80)  i∈Rj  sj =  piai  i∈Rj  This differs from standard average pooling because each element has a potentially different weight- ai, rather than the pooling region size |Rj|. In practice, using conventional average (or sum) pooling results in a huge performance drop (see Section 4.7). Our probabilistic weighting can be viewed as a form of model averaging in which each setting of the locations l in the pooling regions deﬁnes a new model. At training time, sampling to get new locations produces a new model since the connection structure throughout the network is modiﬁed. At test time, using the probabilities instead of sampling, we effectively get an estimate of averaging over all of these possible models without having to instantiate them. Given a network architecture with d different pooling regions, each of size n, the number of possible models is nd where d can be in the 104-106 range and n is typically 4,9, or 16 for example (corresponding to 2 × 2, 3 × 3 or 4 × 4 pooling regions). This is a signiﬁcantly larger number than the model averaging that occurs in dropout [2], where n = 2 always (since an activation is either present or not). In Section 4.7 we conﬁrm that using this probability weighting achieves similar performance compared to using a large number of model instantiations, while requiring only one pass through the network. Using the probabilities for sampling at training time and for weighting the activations at test time leads to state-of-the-art performance on many common benchmarks, as we now demonstrate.  (6)  3  ★!a)!Image!b)!Filter!c)!Rec0ﬁed!Linear!e)!Probabili0es,!pi 0!0!0!0!0!0!0!1.6!2.4!0!0!0!0!0!0!0!0.4!0.6!d)!Ac0va0ons,!ai 1.6!f)!Sampled!!!!!!Ac0va0on,!s!Sample!a!loca0on!from!P():!e.g.!!l = 1 Figure 2: A selection of images from each of the datasets we evaluated. The top row shows the raw images while the bottom row are the preprocessed versions of the images we used for training. The CIFAR datasets (f,h) show slight changes by subtracting the per pixel mean, whereas SVHN (b) is almost indistinguishable from the original images. This prompted the use of local contrast normalization (c) to normalize the extreme brightness variations and color changes for SVHN. 4 Experiments  4.1 Overview  We compare our method to average and max pooling on a variety of image classiﬁcation tasks. In all experiments we use mini-batch gradient descent with momentum to optimize the cross entropy between our network’s prediction of the class and the ground truth labels. For a given parameter x at time t the weight updates added to the parameters, ∆xt are ∆xt = 0.9∆xt−1 − (cid:15)gt where gt is the gradient of the cost function with respect to that parameter at time t averaged over the batch and (cid:15) is a learning rate set by hand. All experiments were conducted using an extremely efﬁcient C++ GPU convolution library [6] wrapped in MATLAB using GPUmat [14], which allowed for rapid development and experimenta- tion. We begin with the same network layout as in Hinton et al. ’s dropout work [2], which has 3 convolutional layers with 5x5 ﬁlters and 64 feature maps per layer with rectiﬁed linear units as their outputs. We use this same model and train for 280 epochs in all experiments aside from one addi- tional model in Section 4.5 that has 128 feature maps in layer 3 and is trained for 500 epochs. Unless otherwise speciﬁed we use 3 × 3 pooling with stride 2 (i.e. neighboring pooling regions overlap by 1 element along the borders) for each of the 3 pooling layers. Additionally, after each pooling layer there is a response normalization layer (as in [2]), which normalizes the pooling outputs at each location over a subset of neighboring feature maps. This typically helps training by suppressing extremely large outputs allowed by the rectiﬁed linear units as well as helps neighboring features communicate. Finally, we use a single fully-connected layer with soft-max outputs to produce the network’s class predictions. We applied this model to four different datasets: MNIST, CIFAR-10, CIFAR-100 and Street View House Numbers (SVHN), see Fig. 2 for examples images.  4.2 CIFAR-10  We begin our experiments with the CIFAR-10 dataset where convolutional networks and methods such as dropout are known to work well [2, 5]. This dataset is composed of 10 classes of natural images with 50,000 training examples in total, 5,000 per class. Each image is an RGB image of size 32x32 taken from the tiny images dataset and labeled by hand. For this dataset we scale to [0,1] and follow Hinton et al. ’s [2] approach of subtracting the per-pixel mean computed over the dataset from each image as shown in Fig. 2(f).  4  CIFAR&100)CIFAR&10)SVHN)MNIST)mean)Local)CN)mean)mean)a))d))e))g))h))f))c))b))!""!""!""Figure 3: CIFAR-10 train and test error rates throughout training for average, max, and stochastic pooling. Max and average pooling test errors plateau as those methods overﬁt. With stochastic pooling, training error remains higher while test errors continue to decrease.1  Cross-validating with a set of 5,000 CIFAR-10 training images, we found a good value for the learning rate (cid:15) to be 10−2 for convolutional layers and 1 for the ﬁnal softmax output layer. These rates were annealed linearly throughout training to 1/100th of their original values. Additionally, we found a small weight decay of 0.001 to be optimal and was applied to all layers. These hyper- parameter settings found through cross-validation were used for all other datasets in our experiments. Using the same network architecture described above, we trained three models using average, max and stochastic pooling respectively and compare their performance. Fig. 3 shows the progression of train and test errors over 280 training epochs. Stochastic pooling avoids over-ﬁtting, unlike average and max pooling, and produces less test errors. Table 1 compares the test performance of the three pooling approaches to the current state-of-the-art result on CIFAR-10 which uses no data augmenta- tion but adds dropout on an additional locally connected layer [2]. Stochastic pooling surpasses this result by 0.47% using the same architecture but without requiring the locally connected layer.  3-layer Conv. Net [2] 3-layer Conv. Net + 1 Locally Conn. layer with dropout [2] Avg Pooling Max Pooling Stochastic Pooling  – –  1.92 0.0 3.40  16.6 15.6 19.24 19.40 15.13  Train Error % Test Error %  Table 1: CIFAR-10 Classiﬁcation performance for various pooling methods in our model compared to the state-of-the-art performance [2] with and without dropout.  To determine the effect of the pooling region size on the behavior of the system with stochastic pooling, we compare the CIFAR-10 train and test set performance for 5x5, 4x4, 3x3, and 2x2 pooling sizes throughout the network in Fig. 4. The optimal size appears to be 3x3, with smaller regions over- ﬁtting and larger regions possibly being too noisy during training. At all sizes the stochastic pooling outperforms both max and average pooling.  4.3 MNIST  The MNIST digit classiﬁcation task is composed of 28x28 images of the 10 handwritten digits [8]. There are 60,000 training images with 10,000 test images in this benchmark. The images are scaled to [0,1] and we do not perform any other pre-processing. During training, the error using both stochastic pooling and max pooling dropped quickly, but the latter completely overﬁt the training data. Weight decay prevented average pooling from over-ﬁtting, but had an inferior performance to the other two methods. Table 2 compares the three pooling ap- proaches to state-of-the-art methods on MNIST, which also utilize convolutional networks. Stochas-  1Weight decay prevented training errors from reaching 0 with average and stochastic pooling methods and  required the high number of epochs for training. All methods performed slightly better with weight decay.  5  5010015020025005101520253035Epochs% Error  Avg (train)Avg (test)Max (train)Max (test)Stochastic (train)Stochastic (test)Figure 4: CIFAR-10 train and test error rates for various pooling region sizes with each method.  tic pooling outperforms all other methods that do not use data augmentation methods such as jittering or elastic distortions [7]. The current state-of-the-art single model approach by Ciresan et al. [1] uses elastic distortions to augment the original training set. As stochastic pooling is a different type of regularization, it could be combined with data augmentation to further improve performance. Train Error % Test Error %  2-layer Conv. Net + 2-layer Classiﬁer [3] 6-layer Conv. Net + 2-layer Classiﬁer + elastic distortions [1] Avg Pooling Max Pooling Stochastic Pooling  – –  0.57 0.04 0.33  0.53 0.35 0.83 0.55 0.47  Table 2: MNIST Classiﬁcation performance for various pooling methods. Rows 1 & 2 show the current state-of-the-art approaches.  4.4 CIFAR-100  The CIFAR-100 dataset is another subset of the tiny images dataset, but with 100 classes [5]. There are 50,000 training examples in total (500 per class) and 10,000 test examples. As with the CIFAR- 10, we scale to [0,1] and subtract the per-pixel mean from each image as shown in Fig. 2(h). Due to the limited number of training examples per class, typical pooling methods used in convolutional networks do not perform well, as shown in Table 3. Stochastic pooling outperforms these methods by preventing over-ﬁtting and surpasses what we believe to be the state-of-the-art method by 2.66%.  Train Error % Test Error %  Receptive Field Learning [4] Avg Pooling Max Pooling Stochastic Pooling  –  11.20 0.17 21.22  45.17 47.77 50.90 42.51  Table 3: CIFAR-100 Classiﬁcation performance for various pooling methods compared to the state- of-the-art method based on receptive ﬁeld learning. 4.5 Street View House Numbers  The Street View House Numbers (SVHN) dataset is composed of 604,388 images (using both the difﬁcult training set and simpler extra set) and 26,032 test images [11]. The goal of this task is to classify the digit in the center of each cropped 32x32 color image. This is a difﬁcult real world problem since multiple digits may be visible within each image. The practical application of this is to classify house numbers throughout Google’s street view database of images. We found that subtracting the per-pixel mean from each image did not really modify the statistics of the images (see Fig. 2(b)) and left large variations of brightness and color that could make clas-  6  16.55	  15.13	  15.71	  15.86	  3.18	  3.4	  4.38	  6.4	  21.11	  19.53	  18.59	  19.25	  0	  0	  0	  0	  20.74	  19.52	  18.83	  19.38	  0.25	  1.8	  4.88	  9.08	  0	  5	  10	  15	  20	  25	  2x2	  3x3	  4x4	  5x5	  %	  Error	  Pooling	  Region	  Size	  Avg	  Train	  Avg	  Test	  Max	  Train	  Max	  Test	  Stochas>c	  Train	  Stochas>c	  Test	  siﬁcation more difﬁcult. Instead, we utilized local contrast normalization (as in [12]) on each of the three RGB channels to pre-process the images Fig. 2(c). This normalized the brightness and color variations and helped training proceed quickly on this relatively large dataset. Despite having signiﬁcant amounts of training data, a large convolutional network can still overﬁt. For this dataset, we train an additional model for 500 epochs with 64, 64 and 128 feature maps in layers 1, 2 and 3 respectively. Our stochastic pooling helps to prevent overﬁtting even in this large model (denoted 64-64-128 in Table 4), despite training for a long time. The existing state-of-the- art on this dataset is the multi-stage convolutional network of Sermanet et al. [12], but stochastic pooling beats this by 2.10% (relative gain of 43%).  Train Error % Test Error %  Multi-Stage Conv. Net + 2-layer Classiﬁer [12] Multi-Stage Conv. Net + 2-layer Classifer + padding [12] 64-64-64 Avg Pooling 64-64-64 Max Pooling 64-64-64 Stochastic Pooling 64-64-128 Avg Pooling 64-64-128 Max Pooling 64-64-128 Stochastic Pooling  – –  1.83 0.38 1.72 1.65 0.13 1.41  5.03 4.90 3.98 3.65 3.13 3.72 3.81 2.80  Table 4: SVHN Classiﬁcation performance for various pooling methods in our model with 64 or 128 layer 3 feature maps compared to state-of-the-art results with and without data augmentation.  4.6 Reduced Training Set Size To further illustrate the ability of stochastic pooling to prevent over-ﬁtting, we reduced the training set size on MINST and CIFAR-10 datasets. Fig. 5 shows test performance when training on a random selection of only 1000, 2000, 3000, 5000, 10000, half, or the full training set. In most cases, stochastic pooling overﬁts less than the other pooling approaches.  Figure 5: Test error when training with reduced dataset sizes on MNIST (left) and CIFAR-10 (right). Stochastic pooling generally overﬁts the least.  Importance of Model Averaging  4.7 To analyze the importance of stochastic sampling at training time and probability weighting at test time, we use different methods of pooling when training and testing on CIFAR-10 (see Table 5). Choosing the locations stochastically at test time degrades performance slightly as could be ex- pected, however it still outperforms models where max or average pooling are used at test time. To conﬁrm that probability weighting is a valid approximation to averaging many models, we draw N samples of the pooling locations throughout the network and average the output probabilities from those N models (denoted Stochastic-N in Table 5). As N increases, the results approach the prob- ability weighting method, but have the obvious downside of an N-fold increase in computations. Using a model trained with max or average pooling and using stochastic pooling at test time per- forms poorly. This suggests that training with stochastic pooling, which incorporates non-maximal elements and sampling noise, makes the model more robust at test time. Furthermore, if these non- maximal elements are not utilized correctly or the scale produced by the pooling function is not correct, such as if average pooling is used at test time, a drastic performance hit is seen.  7  10002000300050001000030000600000123456789# of Training Cases% Error  AvgMaxStochastic10002000300050001000025000500001520253035404550556065# of Training Cases% Error  AvgMaxStochasticWhen using probability weighting during training, the network easily over-ﬁts and performs sub- optimally at test time using any of the pooling methods. However, the beneﬁts of probability weighting at test time are seen when the model has speciﬁcally been trained to utilize it through either probability weighting or stochastic pooling at training time.  Train Method Stochastic Pooling Stochastic Pooling Stochastic Pooling Stochastic Pooling Stochastic Pooling Stochastic Pooling Probability Weighting Probability Weighting Probability Weighting Probability Weighting Max Pooling Max Pooling Max Pooling Avg Pooling Avg Pooling Avg Pooling  Test Method  Train Error % Test Error %  Probability Weighting  Stochastic Pooling  Stochastic-10 Pooling Stochastic-100 Pooling  Max Pooling Avg Pooling  Probability Weighting  Stochastic Pooling  Max Pooling Avg Pooling Max Pooling  Stochastic Pooling  Probability Weighting  Avg Pooling  Stochastic Pooling  Probability Weighting  3.20 3.20 3.20 3.20 3.20 3.20 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.92 1.92 1.92  15.20 17.49 15.51 15.12 17.66 53.50 19.40 24.00 22.45 58.97 19.40 32.75 30.00 19.24 44.25 40.09  Table 5: CIFAR-10 Classiﬁcation performance for various train and test combinations of pooling methods. The best performance is obtained by using stochastic pooling when training (to prevent over-ﬁtting), while using the probability weighting at test time.  4.8 Visualizations Some insight into the mechanism of stochastic pooling can be gained by using a deconvolutional network of Zeiler et al. [15] to provide a novel visualization of our trained convolutional network. The deconvolutional network has the same components (pooling, ﬁltering) as a convolutional net- work but are inverted to act as a top-down decoder that maps the top-layer feature maps back to the input pixels. The unpooling operation uses the stochastically chosen locations selected during the forward pass. The deconvolution network ﬁlters (now applied to the feature maps, rather than the input) are the transpose of the feed-forward ﬁlters, as in an auto-encoder with tied encoder/decoder weights. We repeat this top-down process until the input pixel level is reached, producing the vi- sualizations in Fig. 6. With max pooling, many of the input image edges are present, but average pooling produces a reconstruction with no discernible structure. Fig. 6(a) shows 16 examples of pixel-space reconstructions for different location samples throughout the network. The reconstruc- tions are similar to the max pooling case, but as the pooling locations change they result in small local deformations of the visualized image. Despite the stochastic nature of the model, the multinomial distributions effectively capture the reg- ularities of the data. To demonstrate this, we compare the outputs produced by a deconvolutional network when sampling using the feedforward (FF) proabilities versus sampling from uniform (UN) distributions. In contrast to Fig. 6(a) which uses only feedforward proabilities, Fig. 6(b-h) replace one or more of the pooling layers’ distributions with uniform distributions. The feed forward proba- bilities encode signiﬁcant structural information, especially in the lower layers of the model. Addi- tional visualizations and videos of the sampling process are provided as supplementary material at www.matthewzeiler.com/pubs/iclr2013/. 5 Discussion We propose a simple and effective stochastic pooling strategy that can be combined with any other forms of regularization such as weight decay, dropout, data augmentation, etc. to prevent over- ﬁtting when training deep convolutional networks. The method is also intuitive, selecting from information the network is already providing, as opposed to methods such as dropout which throw information away. We show state-of-the-art performance on numerous datasets, when comparing to other approaches that do not employ data augmentation. Furthermore, our method has negligible computational overhead and no hyper-parameters to tune, thus can be swapped into to any existing convolutional network architecture.  8  Figure 6: Top down visualizations from the third layer feature map activations for the horse image (far left). Max and average pooling visualizations are also shown on the left. (a)–(h): Each image in a 4x4 block is one instantiation of the pooling locations using stochastic pooling. For sampling the locations, each layer (indicated in parenthesis) can either use: (i) the multinomial distribution over a pooling region derived from the feed-forward (FF) activations as in Eqn. 4, or (ii) a uniform (UN) distribution. We can see that the feed-forward probabilities encode much of the structure in the image, as almost all of it is lost when uniform sampling is used, especially in the lower layers.  References [1] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, high  performance convolutional neural networks for image classiﬁcation. In IJCAI, 2011.  [2] G.E. Hinton, N. Srivastave, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving  neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.  [3] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architec-  ture for object recognition? In ICCV, 2009.  [4] Y. Jia and C. Huang. Beyond spatial pyramids: Receptive ﬁeld learning for pooled image  features. In NIPS Workshops, 2011.  [5] A. Krizhevsky. Learning multiple layers of featurs from tiny images. Technical Report TR-  2009, University of Toronto, 2009.  [6] A. Krizhevsky. cuda-convnet. http://code.google.com/p/cuda-convnet/, 2012. [7] Y. LeCun. The MNIST database. http://yann.lecun.com/exdb/mnist/, 2012. [8] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document  recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  [9] G. Montavon, G. Orr, and K.-R. Muller, editors. Neural Networks: Tricks of the Trade.  Springer, San Francisco, 2012.  [10] V. Nair and G.E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In  ICML, 2010.  [11] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural  images with unsupervised feature learning. In NIPS Workshop, 2011.  [12] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house  numbers digit classiﬁcation. In ICPR, 2012.  [13] P. Simard, D. Steinkraus, and J. Platt. Best practices for convolutional neural networks applied  to visual document analysis. In ICDAR, 2003.  [14] http://gp-you.org/.  gpumat/, 2012.  GPUmat.  http://sourceforge.net/projects/  [15] M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolutional networks for mid and high level  feature learning. In ICCV, 2011.  9  a)	  FF(3)	  –	  FF(2)	  –	  FF(1)	  	  b)	  UN(3)	  –	  FF(2)	  –	  FF(1)	  c)	  FF(3)	  –	  UN(2)	  –	  FF(1)	  d)	  FF(3)	  –	  FF(2)	  –	  UN(1)	  h)	  UN(3)	  –	  UN(2)	  –	  UN(1)	  e)	  UN(3)	  –	  UN(2)	  –	  FF(1)	  g)	  FF(3)	  –	  UN(2)	  –	  UN(1)	  f)	  FF(3)	  –	  UN(2)	  –	  UN(1)	  Image	  Avg	  Max	  ","We introduce a simple and effective method for regularizing largeconvolutional neural networks. We replace the conventional deterministicpooling operations with a stochastic procedure, randomly picking the activationwithin each pooling region according to a multinomial distribution, given bythe activities within the pooling region. The approach is hyper-parameter freeand can be combined with other regularization approaches, such as dropout anddata augmentation. We achieve state-of-the-art performance on four imagedatasets, relative to other approaches that do not utilize data augmentation."
1301.3551,2013,Information Theoretic Learning with Infinitely Divisible Kernels  ,"['Luis Gonzalo Sánchez', 'Jose C. Principe']",https://arxiv.org/pdf/1301.3551.pdf,"Information Theoretic Learning with Inﬁnitely  Divisible Kernels  Dept. of Electrical and Computer Engineering  Dept. of Electrical and Computer Engineering  Luis G. Sanchez Giraldo  University of Florida  Gainesville, Florida, USA  Jose C. Principe  University of Florida  Gainesville, Florida, USA  3 1 0 2     n u J    4      ]  G L . s c [      6 v 1 5 5 3  .  1 0 3 1 : v i X r a  sanchez@cnel.ufl.edu  principe@cnel.ufl.edu  Abstract  In this paper, we develop a framework for information theoretic learning based on inﬁnitely divisible matrices. We formulate an entropy-like functional on positive deﬁnite matrices based on Renyi’s axiomatic deﬁnition of entropy and examine some key properties of this functional that lead to the concept of inﬁnite divisibil- ity. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. As an appli- cation example, we derive a supervised metric learning algorithm using a matrix based analogue to conditional entropy achieving results comparable with the state of the art.  1 Introduction  Information theoretic quantities are descriptors of the distributions of the data that go beyond second- order statistics. The expressive richness of quantities such entropy or mutual information has been shown to be very useful for machine learning problems where optimality based on linear and Gaus- sian assumptions no longer holds. Nevertheless, operational quantities in information theory are based on the probability laws underlying the data generation process, which are rarely known in the statistical learning setting where the only information available comes from the sample {zi}n i=1. Therefore, the use of information theoretic quantities as descriptors of data requires the develop- ment of suitable estimators. In [1], the use of Renyi’s deﬁnition of entropy along with Parzen density estimation is proposed as the main tool for information theoretic learning (ITL). The opti- mality criteria is expressed in terms of quantities such as Renyi’s entropy, divergences based on the Cauchy-Schwarz inequality, quadratic mutual information, among others. Part of the research effort in this context has pointed out connections to reproducing kernel Hilbert spaces [2]. Here, we show that these connections are not only valuable from a theoretical point of view, but they can also be exploited to derive novel information theoretic quantities with suitable estimators from data.  i=1 ⊆ X and any set of coefﬁcients {a i}N  Positive deﬁnite kernels have been employed in machine learning as a representational tool allowing algorithms that are based on inner products to be expressed in a rather generic way (the so called “kernel trick”). Algorithms that exploit this property are commonly known as kernel methods. Let X be a nonempty set. A function k : X × X 7→ R is called a positive deﬁnite kernel if for any jk (xi,x j) ≥ 0, ﬁnite set {xi}N if at least one i, a i 6= 0. In this case, there exist an implicit mapping f : X 7→ H that maps any element x ∈ X to an element f (x) in a Hilbert space H , such that k (x,y) = hf (x),f (y)i. The above map provides an implicit representation of the objects of interest that belong to the set X . The generality of this representation has been exploited in many practical applications, even for data that do not come in standard vector representation Rd [3]. This is possible as long as a kernel function is available.  i=1 ⊂ R, it follows that (cid:229)  i, j a ia  More recently, it has been noticed that kernel induced maps are also useful beyond the above ker- nel trick in a rather interesting fashion. Namely, kernels can be utilized to compute higher-order statistics of the data in a nonparametric setting. Some examples exploring this idea are: kernel in- dependent component analysis [4], the work on measures of dependence and independence using Hilbert-Schmidt norms [5], and the quadratic measures of independence proposed in [6]. It is not surprising, yet important to mention, that a similar observation have also been reached from the work on ITL since one of the original motivations in using information theoretic quantities is to go beyond second order statistics. The work we introduce in this paper goes along these lines. The twist is that rather than deﬁning an estimator of a conventional information theoretic quantity such as Shannon entropy, we propose a quantity build from the data that satisﬁes similar axiomatic properties to those of well establish deﬁnitions such as Renyi’s deﬁnition of entropy  The main contribution of this work is to show that the Gram matrix obtained from evaluating a positive deﬁnite kernel on samples can be used to deﬁne a quantity based on the data with properties similar to those of an entropy without assuming that the probability density is being estimated. Therefore, we look at the axiomatic treatment of entropy and adapt it to the Gram matrices describing the data. In this sense, we think about entropy as a measure inversely related to the amount of statistical regularities (structure) directly from the data that can be applied as the optimality criterion in a learning algorithm. As an application example, we derive supervised metric learning algorithm that uses conditional entropy as the cost function. This is the second contribution of this paper, and the empirical results show that the proposed method is competitive with current approaches. The main body of the paper is organized in two parts. First, we introduce the proposed matrix-based entropy measure using the spectral theorem along with a set of axiomatic properties that our quantity must satisfy. Then, the notion of joint entropy is developed based on Hadamard products. We look at some basic inequalities of information and how they translate to the setting of positive deﬁnite matrices, which ﬁnally allow us to deﬁne an analogue to conditional entropies. In the development of these ideas, we ﬁnd that the concept of inﬁnitely divisible kernels arises and become key to our purposes. We revisit some of the theory on inﬁnitely divisible matrices, to show how it links to the the proposed information theoretic framework. In the last part, we introduce an information theoretic supervised metric learning algorithm. We show how the proposed analogue to conditional entropy is a suitable cost function leading naturally to a gradient descent procedure. Finally, we provide some conclusions and future directions.  2 Positive Deﬁnite Matrices, and Renyi’s Entropy Axioms  Let us start with an informal observation that motivated our matrix based entropy. In [1], the use of Renyi’s entropy is proposed as an alternative to the more commonly adopted deﬁnition of en- tropy given by Shannon. In particular, it was found that Renyi’s second-order entropy provides an amenable quantity for practical purposes. An empirical plug in estimator of Renyi’s second-order entropy based on the Parzen density estimator ˆf (x) = 1 n  k (xi,x), can be obtained as follows:  (cid:229) n  i=1  − log  1 n2  n(cid:229)  i, j=1  h(xi,x j),  (1)  where h(x,y) =RX mapping f information potential, can be interpreted in this space as a norm:  k s (x,z)k s (y,z)dz. Note that since h is a positive deﬁnite kernel, there exists a to a RKHS such that h(x,y) = hf (x),f (y)i; and the argument of the log in (1), called the  *1  n  n(cid:229)  i=1  f (xi),  1 n  n(cid:229)  i=1  f (xi)+ =(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  1 n  n(cid:229)  i=1  f (xi)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  2  ,  (2)  with the limiting case given by k E[f (X)]k2. Thus, we can think of this estimator as an statistic computed on the representation space provided by the positive deﬁnite kernel h. Now, let us look at the case where k s is the Gaussian kernel; if we construct the Gram matrix K with elements Ki j = k 2s (xi,x j), it is easy to verify that the estimator of Renyi’s second-order entropy based on (1) corresponds to:  ˆH2(X) = − log(cid:18) 1  n2 tr(KK)(cid:19) +C(s ).  (3)  where C(s ) takes care of the normalization factor of the Parzen window. As we can see, the informa- tion potential estimator can be related to the norm of the Gram matrix K deﬁned as kKk2 = tr(KK). From the above informal argument two important questions arise. First, it seems natural to ask whether other functionals on Gram matrices allow information theoretic interpretations that can be further utilized as objective functions in ITL. Secondly, even though h was originally derived from a convolution of Parzen windows, was there anything about the implicit representation that allows to interpret (2) in information theoretic terms?  2.1 Renyi’s Axioms for Gram matrices  Real Hermitian matrices are considered generalizations of real numbers. It is possible to deﬁne a partial ordering on this set by using positive deﬁnite matrices, which are a generalization of the positive real numbers. Let Mn be the set of all n × n real matrices; for two Hermitian matrices A,B ∈ Mn, we say A < B if A− B is positive deﬁnite. Likewise, A ≻ B means that A− B is strictly positive deﬁnite.  The following spectral decomposition theorem [7] relates to the functional calculus on matrices and provides a reasonable way to extend continuous scalar-valued functions to Hermitian matrices.  Theorem 2.1 Let D ⊂ C be a given set and let Nn(D) := {A ∈ Mn : A is normal and s (A) ⊂ D}, where s (A) denotes the spectrum of A. If f (t) is a continuous scalar-valued function on D, then the primary matrix function  f (l 1) ... 0  f (A) = U   ··· . . . ···  0 ... f (l n)     U∗  (4)  is continuous on Nn(D), where A = UL U∗, L = diag(l 1, . . . ,l n), and U ∈ Mn is unitary. Equipped with the above result, we can deﬁne matrix functions such as f (A) = Ar for r ∈ R+, which will be used in deﬁning the following matrix-based analogue to Renyi’s a -entropy. The functional will then be applied to Gram matrices constructed by pairwise evaluation of a positive deﬁnite kernel on the data samples. Consider the set D + is closed under ﬁnite convex combinations.  n of positive deﬁnite matrices A ∈ Mn for which tr(A) ≤ 1. It is clear that this set  Proposition 2.1 Let A ∈ D +  n and B ∈ D +  satisﬁes the following set of conditions:  Sa (A) =  n and also tr(A) = tr(B) = 1. The functional  log2 [tr(A  a  )],  1  1− a  (5)  (i) Sa (PAP∗) = Sa (A) for any orthonormal matrix P ∈ Mn (ii) Sa (pA) is a continuous function for 0 < p ≤ 1. (iii) Sa ( 1 (iv) Sa (A⊗ B) = Sa (A) + Sa (B). (v) If AB = BA = 0; then for the strictly monotonic and continuous function g(x) = 2(a −1)x for  n I) = log2 n.  a  6= 1 and a > 0, we have that:  Sa (tA + (1− t)B) = g−1 (tg(Sa (A)) + (1− t)g(Sa (B))) .  (6) Proof 2.1 The proof of (i) easily follows from Theorem 2.1. Take A = UL U∗ now PU is also a unitary matrix and thus f (A) = f (PAP∗) the trace functional is invariant under unitary transforma- a . For (iii), a simple calculation tions. For (ii), the proof reduces to the continuity of a −1. Now, for property (iv), notice that if trA = trB = 1, then, tr(A⊗ B) = 1. yields trAa =(cid:0) 1 n(cid:1) Since A = UL U∗ and B = VG V∗ we can write A ⊗ B = (U ⊗ V )(L ⊗ G )(U ⊗ V )∗, from which  log2(p)  1−a  1  a  a  a  )tr(G  = tr(L  = tr(L ⊗ G )  a tr(A⊗ B) ) and thus (iv) is proved. Finally, (v) notice that for any integer power k of tA + (1−t)B we have: (tA + (1−t)B)k = (tA)k + ((1−t)B)k since AB = BA = 0. Under extra conditions such as f (0) = 0 the argument in the proof of Theorem 2.1 can be ex- tended to this case. Since the eigen-spaces for the non-null eigenvalues of A and B are orthogonal we can simultaneously diagonalize A and B with the orthonormal matrix U, that is A = UL U∗ and B = UG U∗ where L and G are diagonal matrices containing the eigenvalues of A and B re- spectively. Since AB = BA = 0, then L G = 0. Under the extra condition f (0) = 0, we have that f (tA + (1− t)B) = f (tA) + f ((1− t)B) yielding the desired result for (v). Notice also that if the rank of A, r (A) = 1, the entropy Sa (A) = 0 for any a It is also true that,  6= 0.  Sa (A) ≤ Sa (  1 n  I) = log2 n.  (7)  As we can see (5) satisﬁes some properties attributed to entropy. Nevertheless, such a character- ization may not fully endow all unit-trace positive deﬁnite matrices with an information theoretic interpretation. Which descriptors are suitable in representing joint-spaces? What properties should be satisﬁed by the matrices in order to be applied to concepts that link them to random variables such as conditioning? In what follows, we address these points by developing notions of joint entropy and conditional entropy, for which, additional properties must be fulﬁlled. Recall that the notions of joint and conditional entropy are not only important for the above reasons, but they also provide the means to propose objective functions for learning that are based on information theoretic quantities.  2.2 Hadamard Products and the Notion of Joint Entropy  Positive kernels are also useful in integrating multiple modalities. Using the the product kernel, we can readily deﬁne the notion of joint-entropy. Consider a sequence of sample pairs {(xi,yi)}N i=1 where xi ∈ X and yi ∈ Y . Assume, we have a positive deﬁnite kernels k 1 deﬁned on X × X and k 2 deﬁned on X × X . The product kernel k ((xi,yi), (x j,y j)) = k 1(xi,x j)k (yi,y j) is a positive deﬁnite kernel on (X × Y ) × (X × Y ). As we can see the Hadamard product arises as a joint representation in a our matrix based entropy. Consider two matrices A and B in D n with unit trace, for which there exists some relation between the elements Ai j and Bi j for all i and j. The joint entropy can be deﬁned as:  Sa (cid:18) A◦ B tr(A◦ B)(cid:19)  It is important then to verify that the deﬁnition of joint entropy (8) satisﬁes a basic intuition about uncertainty. The joint entropy should never be smaller than any of the individual entropies of the variables that conform it. The following proposition veriﬁes this intuition for a subset of the unit trace, positive deﬁnite matrices.  Proposition 2.2 Let A and B be two n× n positive deﬁnite matrices with trace 1 with nonnegative entries, and Aii = 1  n for i = 1,2, . . . ,n. Then, the following inequality holds:  Sa (cid:18) A◦ B  tr(A◦ B)(cid:19) ≥ Sa (B),  (9)  2.3 Conditional Entropy as a Difference Between Entropies  The conditional entropy of X given Y , which can be understood as the uncertainty about X that remains after knowing the joint distribution of X and Y , can be obtained from a difference between two entropies. In the Shannon’s deﬁnition of conditional entropy, H(X|Y ) can be expressed as H(X|Y ) = H(X,Y )− H(Y ). The properties of this deﬁnition has been recently studied in the case of Renyi’s entropies [8] and in the matrix case, this deﬁnition yields:  Sa (A|B) = Sa (cid:18) A◦ B  tr(A◦ B)(cid:19)− Sa (B),  for positive semideﬁnite matrices A and B with nonnegative entries and unit trace, such that Aii = 1 n for all i = 1, . . . ,n. The above quantity is nonnegative and upper bounded by Sa (A). Certainly,  (8)  (10)  normalization is an important property of the matrices involved in the above results. If A and B are normalized to have unit trace, then for r ∈ [0,1] it is true that the Hadamard product of  (11) is also normalized. However, it is not always true that the resulting matrix (11) is positive deﬁnite. This product can be thought as a weighted geometric average for which the resulting matrix will give more emphasis to either one of the matrices. However, if A and B satisfy a property called inﬁnitely divisibility, the product is guaranteed to be positive deﬁnite 1.  A◦r ◦ B◦(1−r),  3 Inﬁnitely Divisible Functions  The theory of inﬁnitely divisible developed below is not new, but it is included because it provides a basic understanding about the role of inﬁnitely divisible kernels in computing the above information theoretic quantities from data. To avoid confusion, let us describe the key points to bear in mind before we move to the mathematical description. Inﬁnitely divisible kernels and negative deﬁnite functions are tied together trough the exponential a logarithm functions. Both functions provide Hilbert space representations of the data. We can think of the RKHS of the inﬁnitely divisible kernel as a representation to compute the higher order descriptors of the data. On the other hand, the Hilber- tian metric can be the representation space for which we want to compute the high order statistics. Normalization, as we show below is not only important in satisfying the conditions for the infor- mation theoretic quantities already deﬁned, but it also shows that many possible representational choices are equivalent.  3.1 Negative Deﬁnite Functions and Hilbertian Metrics  a ia i, j=0  Let M = (X ,d) be a separable metric space. A necessary and sufﬁcient condition for M to be embeddable in a Hilbert space H is that for any set {xi} ⊂ X of n + 1 points, j(cid:0)d2(x0,xi) + d2(x0,x j)− d2(xi,x j)(cid:1) ≥ 0, for any aaa ∈ Rn. This condition is equivalent (cid:229) n i, j=1 jd2(xi,x j) ≤ 0, for any aaa ∈ Rn+1, such that (cid:229) n a i = 0. This condition is known to (cid:229) n as negative deﬁniteness. Interestingly, the above condition implies that exp(−rd2(xi,x j)) is positive deﬁnite in X for all r > 0 [9]. Indeed, matrices derived from functions satisfying the above property conform a special class of matrices know as inﬁnitely divisible.  a ia  i=0  3.2 Inﬁnitely Divisible Matrices According to the Schur product theorem A < 0 implies A◦n = A ◦ A ◦ ··· ◦ A < 0 for any positive integer n. Does the above hold if we to take fractional powers of A? In other words,is the matrix A◦ 1 m < 0 for any positive integer m? This question leads to the concept of inﬁnitely divisible matrices [10, 11]. A nonnegative matrix A is said to be inﬁnitely divisible if A◦r < 0 for every nonnegative r. Inﬁnitely divisible matrices are intimately related to negative deﬁniteness as we can see from the following proposition Proposition 3.1 If A is inﬁnitely divisible, then the matrix Bi j = − logAi j is negative deﬁnite From this fact it is possible to relate inﬁnitely divisible matrices with isometric embeddings into Hilbert spaces. If we construct the matrix  Di j = Bi j −  (12) using the matrix B from proposition 3.1. There exists a Hilbert space H and a mapping f such that (13) Moreover, notice that if A is positive deﬁnite −A is negative deﬁnite and expAi j is inﬁnitely divisible. In a similar way, we can construct a matrix,  Di j = kf (i)− f ( j)k2  (Bii + B j j),  H .  1 2  Di j = −Ai j +  1 2  (Aii + A j j),  (14)  1By this, we also mean positive semideﬁnite  Negative Definite Function  d  X  Hd  Hilbertian Metric Embedding  log  HK  exp  K  Infinitely Divisible Kernel     Data   Driven Operator  Estimator      IT Quantity  Figure 1. Spaces involved in the inﬁnitely divisible matrix framework  with the same property (13). This relation between (12) and (14) suggests a normalization of in- ﬁnitely divisible matrices with non-zero diagonal elements that can be formalized in the following theorem.  i=1,  Theorem 3.1 Let X be a nonempty set, and let d1 and d2 be two metrics on it, such that for any set n(cid:229) a i = 0, is true for ℓ = 1,2. Consider the {xi}n i, j=1 matrices A(ℓ)  ℓ (xi,x j) ≤ 0, for any aaa ∈ Rn, and (cid:229) n jd2  ℓ (xi,x j) and their normalizations ˆA(ℓ), deﬁned as:  a ia i j = exp−d2  i=1  ˆA(ℓ) i j =  A(ℓ) i j  .  qA(ℓ) Then, if ˆA(1) = ˆA(2) for any ﬁnite set {xi}n i=1 ⊆ X , there exist isometrically isomorphic Hilbert spaces H1 and H2, that contain the Hilbert space embeddings of the metric spaces (X ,dℓ), ℓ = 1,2. Moreover, ˆA(ℓ) are inﬁnitely divisible.  ii qA(ℓ)  j j  (15)  Figure 1 summarizes the relation between spaces that are considered in the proposed framework. The object space X can be directly mapped into Hk using an inﬁnitely divisible kernel k , or it can be mapped to a Hilbert space Hd, if a negative deﬁnite function d, is employed as the distance function. The spaces Hk and Hd are related by the log and exp functions.  4 Application to Metric Learning  4.1 Adaptation Using the Matrix-Based Entropy  By deﬁnition, the matrix entropy functional (5) fall into the family of matrix functions know as spectral functions. These functions only depend on the eigenvalues of matrix and therefore their name [12]. Using theorem (1.1) from [13] it is straightforward to obtain the derivative of (5) at A as  (cid:209) Sa (A) =  a  (1− a )tr(Aa  a −1U∗,  UL  )  (16)  where A = UL U∗. It is important to note that this decomposition can be used to our advantage. Instead of computing the full set of eigenvectors and eigenvalues of A, we can approximate the gradient of Sa by using only a few leading eigenvalues. It is easy to see that this approximation will  be optimal in the Frobenius norm kXkFro =ptr(X∗X).  4.2 Metric Learning Using Conditional Entropy  Here, we apply the proposed matrix framework to the problem of supervised metric learning. This problem can be formulated as follows. Given a set of points {(xi,li)}n i=1, we seek a positive  semideﬁnite matrix AAT, that parametrizes a Mahalanobis distance between samples x,x′ ∈ Rd as d(x,x′) = (x− x′)TAAT(x− x′). Our goal is to ﬁnd parametrization matrix A such that the con- ditional entropy of the labels li given the projected samples yi = ATxi with yi ∈ Rp and p ≪ d, is minimized. This can be posed as the following optimization problem:  minimize A∈Rd×p subject to  Sa (L|Y ) ATxi = yi, for i = 1, . . . ,n; tr(ATA) = p,  (17)  where the trace constraint prevents the solution from growing unbounded. We can translate this problem to our matrix-based framework in the following way. Let K be the matrix representing the projected samples  Ki j =  1 n  exp −  (xi − x j)TAAT(xi − x j)  2s 2  !,  and L be the matrix of class co-occurrences where Li j = 1 n if li = l j and zero otherwise. The condi- tional entropy can be computed as Sa (L|Y ) = Sa (nK◦ L)− Sa (K), and its gradient at A, which can be derived based on (24), is given by:  where  XT(P− diag(P1))XA  P = (nL◦ (cid:209) Sa (nK◦ L)− (cid:209) Sa (K))◦ K  (18)  (19)  Finally, we can use (18) to search for A iteratively. UCI Data: To evaluate the results we use the same experimental setup proposed in [14], we com- pares 5 different approaches to supervised metric learning based on the classiﬁcation error obtained from two-fold cross-validation using a 4-nearest neighbor classiﬁer. The reported errors are aver- ages errors from 10 runs on the two folds for each algorithm; in our case the parameters are p = 3, a = 1.01 and s = √3. The feature vectors were centered and scaled to have unit variance. Fig- ure 2(a) shows the results of the proposed approach conditional entropy metric learning (CEML), information theoretic metric learning (ITML) proposed in [14], neighborhood component analysis (NCA) from [15], the maximally collapsing metric learning (MCML) method from [16], the large margin nearest neighbor (LMNN) method found in [17], and, as a baseline, the the inverse covari- ance and Euclidean distances. The results for the Soybean dataset are not reported since there is more than one possible data set in the UCI repository under that name. The errors obtained by the metric learning algorithm using the proposed matrix-based entropy framework are consistently among the best performing methods included in the comparison. Choice of order a : Even though the choice of the entropy order above appears to be arbitrary, there is a motivation in choosing a close to 1. The reason is that the higher the entropy order, the more prone the algorithm is to ﬁnd unimodal solutions. This can be advantageous if prior knowledge or strong assumptions on the class distributions are taken into consideration. In our experiments, we opted for lower entropy order and give the algorithm more ﬂexibility in ﬁnding a good solu- tion. To experimentally show this phenomena, we generated a two-dimensional dataset containing points from two classes. In one direction the classes are very well separated but the distribution has multiple modalities. On the orthogonal direction, the classes are not fully separable, but their distributions are unimodal. Figure 3 shows a sample with points drawn from both classes, as we can see projecting the data onto the horizontal axis provides better separability at the cost of a more complex decision boundary. We run our metric learning algorithm 60 times for different values of a and recorded the direction of the resulting one-dimensional feature extractor. Table 1 shows the number of times a particular direction was picked by our algorithm for different entropy orders. It can be seen that for larger values of a , the algorithm selected the vertical direction more often.  a  Horizontal  Vertical  1.01 58 2  1.3 35 25  2 1 59  5 1 59  Table 1. Occurrence of horizontal and vertical solutions versus the entropy order  r o r r  E  0.35  0.3  0.25  0.2  0.15  0.1  0.05  0        CEML(proposed method) ITML NCA MCML LMNN invCov Euclidean  Wine  Ionosphere  Scale  Iris  (a) Classiﬁcation error UCI data  Projected Features  Gram Matrix Projected Features  10  5  0  −5  −10  −10  −5  0  5  10  (b) Projected faces UMist dataset and resulting Gram matrix s = √2  Figure 2. Results for the Metric learning application  3  2  1  0  −1  −2  −3  −4 −2  Unimodal  Better separability  −1.5  −1  −0.5  0  0.5  1  1.5  2  Figure 3. Artiﬁcial data to illustrate the role of the entropy order  UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people. The total number of images is 575 and the size of each image is 112x92 pixels for a total of 10304 dimensions. Pixel values were normalized by dividing by 255 and removing the mean. Figure 2(b) shows the images projected into R2. It is remarkable  how a linear projection can separate the faces, and it can also be seen from the Gram matrix that it tries to approximate the co-occurrence matrix L.  5 Conclusions  In this paper, we presented a data-driven framework for information theoretic learning based on inﬁnitely divisible matrices. We deﬁne estimators of entropy-like quantities that can be computed from the Gram matrices obtained by evaluating inﬁnitely divisible kernels on pairs of samples. The proposed quantities do not assume that the density of the data has been estimated, this can be advantageous in many scenarios where even deﬁning a density is not feasible. We discuss some key properties of the proposed quantities and show how they can be applied to deﬁne useful analogues to quantities such as conditional entropy. Based on the proposed framework, we introduce a supervised metric learning algorithm with results that are competitive with the state of the art. Nevertheless, we believe that many interesting formulations to learning problems based on the proposed framework are yet to be found. It is also important to highlight that the connection between the RKHS provided by the inﬁnitely divisible kernel, and the Hilbertian metrics associated with the negative deﬁnite functions, opens an interesting avenue to investigate formulations of information theoretic learning algorithms on both spaces, and the implications of choosing one or the other.  References  [1] J. C. Principe, Information Theoretic Learning: Renyi’s Entropy and Kernel Perspectives, ser. Series in  Information Science and Statistics, M. Jordan, R. Nowak, and B. Sch¨olkopf, Eds. Springer, 2010.  [2] J.-W. Xu, A. R. C. Paiva, I. Park, and J. C. Principe, “A reproducing kernel hilbert space framework for information theoretic learning,” IEEE Transactions on Signal Processing, vol. 56, no. 12, pp. 5891–5902, December 2008.  [3] J. Shawe-Taylor and N. Cristianini, Kernel Methods for Pattern Analysis. Cambridge University Press,  2004.  [4] F. R. Bach and M. I. Jordan, “Kernel independent component analysis,” Journal of Machine Learning  Research, vol. 3, pp. 1–48, July 2002.  [5] A. Gretton, O. Bousquet, A. Smola, and B. Sch¨olkopf, “Measuring statistical dependence with hilbert- schmidt norms,” in Proceedings of Algorithmic Learning Theory, S. Jain, H. Simon, and E. Tomita, Eds., 2005, pp. 63–77.  [6] S. Seth, M. Rao, I. Park, and J. C. Pr´ıncipe, “A uniﬁed framework for quadratic measures of indepen-  dence,” IEEE Transactions on Signal Processing, vol. 59, no. 8, pp. 3624–3635, August 2011. [7] R. A. Horn and C. R. Johnson, Topics in Matrix Analysis. Cambridge University Press, 1991. [8] A. Teixeira, A. Matos, and L. Antunes, “Conditional r´enyi entropies,” IEEE Transactions on Information  Theory, vol. 58, no. 7, pp. 4273–4277, July 2012.  [9] I. J. Schoenberg, “Metric spaces and positive deﬁnite functions,” Transactions of the American Mathe-  matical Society, vol. 44, no. 3, pp. 522–536, November 1938.  [10] R. Bhatia, “Inﬁnite divisible matrices,” The American Mathematical Monthly, vol. 113, no. 3, pp. 221–  235, March 2006.  [11] R. A. Horn, “The theory of inﬁnitely divisible matrices and kernels,” Transactions of the American Math-  ematical Society, vol. 136, pp. 269–286, February 1969.  [12] S. Friedland, “Convex spectral functions,” Linear and Multilinear Algebra, vol. 9, pp. 299–316, 1981. [13] A. S. Lewis, “Derivatives of spectral functions,” Mathematics of Operations Research, vol. 21, no. 3, pp.  576–588, August 1996.  [14] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon, “Information-theoretic metric learning,” in ICML,  Corvalis, Oregon, USA, June 2007, pp. 209–216.  [15] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov, “Neighborhood component analysis,” in NIPS,  2004.  [16] A. Globerson and S. Roweis, “Metric learning by collapsing classes,” in NIPS, 2005. [17] K. Q. Weinberger, J. Blitzer, and L. K. Saul, “Distance metric learning for large margin nearest neighbor  classiﬁcation,” in NIPS, 2005.  [18] R. Bhatia, Matrix Analysis, ser. Graduate Texts in Mathematics. Springer, 1996.  A Additional results and proofs  To prove (9), we need to introduce the concept of majorization and some results pertaining the ordering that arises from this deﬁnition. The proposition is replicated in this appendix for the sake of self containment. Deﬁnition A.1 (Majorization): Let p and q be two nonnegative vectors in Rn such that (cid:229) n i=1 qi < ¥ (cid:229) n and q[1] ≥ q[2] ≥ ··· ≥ q[n] denoted by {p[i]}n i=1 and {p[i]}n k(cid:229) p[i] ≤  i=1 pi = . We say p 4 q, q majorizes p, if their respective ordered sequences p[1] ≥ p[2] ≥ ··· ≥ p[n]  q[i] for k = 1, . . . ,n  i=1, satisfy:  (20)  k(cid:229)  i=1  i=1  It can be shown that if p 4 q then p = Aq for some doubly stochastic matrix A [18]. It is also easy to verify that if p 4 q and p 4 h then p 4 tq + (1 − t)h for t ∈ [0,1]. The majorization order is important because it can be associated with the deﬁnition of Schur-concave (convex) functions. A real valued function f on Rn is called Schur-convex if p 4 q implies f (p) ≤ f (q) and Schur-concave if f (q) ≤ f (p). Lemma A.1 The function fa  : S n 7→ R+ (S n denotes the n dimensional simplex), deﬁned as,  is Schur-concave for a > 0.  fa (p) =  1  1− a  log2  n(cid:229)  i=1  a i ,  p  (21)  Notice that, Schur-concavity (Schur-convexity) cannot be confused with concavity (convexity) of a function in the usual sense. Now, we are ready to state the inequality for Hadamard products. Proposition A.1 Let A and B be two n× n positive deﬁnite matrices with trace 1 with nonnegative entries, and Aii = 1  n for i = 1,2, . . . ,n. Then, the following inequality holds:  Sa (cid:18) A◦ B  tr(A◦ B)(cid:19) ≥ Sa (B),  (22)  Proof A.1 In proving (9), we will use the fact that Sa preserves the majorization order (inversely) of nonnegative sequences on the n-dimensional simplex. First look at the identity  xT(A◦ B)x = tr(ADxBDx) =  1 n  xT In particular, if {xi}n i (A◦ B)xi. If we let {xi}n be the eigenvectors of A◦ B ordered according to their respective eigenvalues in decreasing order, then,  i=1 is an orthonormal basis for Rn, tr(A◦ B) =  i=1  n(cid:229) i=1  k(cid:229)  i=1  xT i (A◦ B)xi =  k(cid:229)  i=1  tr (ADxiBDxi) ≤  =  1 n  1 n  k(cid:229)  i=1  k(cid:229)  i=1  tr(cid:0)11TDxi BDxi(cid:1) k(cid:229) xT i Bxi ≤  1 n  i=1  yT i Byi,  (23)  where k = 1, . . . ,n and {yi}n i=1 are the eigenvectors of B ordered according to their respective eigen- values in decreasing order. The inequality (23) is equivalent to say that nl (A◦ B) 4 l (B), that is, the sequence of eigenvalues of (A◦ B)/tr(A◦ B) is majorized by the sequence of eigenvalues of B, which implies (9) by Lemma A.1.  A beautiful observation from Theorem 3.1 is that, according to equation (10), the proposed normal- ization procedure for inﬁnitely divisible matrices can be thought of as ﬁnding the maximum entropy matrix among all matrices for which the Hilbert space embeddings are isometrically isomorphic.  A.1 Derivatives of Spectral Functions  Let Hn denote the vector space of real Hermitian matrices of size n× n endowed with inner product hX,Yi = trXY; and let Un denote the set of n× n unitary matrices. A real valued function f deﬁned on a subset of Hn is unitarily invariant if f (UXU∗) = f (X) for any U ∈ Un. Associated with each spectral function f there is a symmetric function F on Rn. By symmetric we mean that F(x) = F(Px) for any n× n permutation matrix P. Let l (X) denote the vector of ordered eigenvalues of X; then, a spectral function f (X) is of the form F(l (X)) for F a symmetric. We are interested in the differentiation of the composition (F ◦ l )(·) = F(l (·)) at X2. The following result [13] allows us to differentiate a spectral function f at X Theorem A.1 Let the set W ⊂ Rn be open and symmetric, that is, for any x ∈ W and any n × n . Suppose that F is symmetric, Then, the spectral function F(l (·)) permutation matrix P, Px ∈ W is differentiable at a matrix X if and only if F is differentiable at the vector l (X). In this case, the gradient of F ◦ l at X is (cid:209) (F ◦ l )(X) = Udiag((cid:209) F(l (X)))U∗, for any unitary matrix satisfying X = Udiag(l (X))U∗.  (24)  2In here, ◦ denotes composition rather than Hadamard product  ","In this paper, we develop a framework for information theoretic learningbased on infinitely divisible matrices. We formulate an entropy-like functionalon positive definite matrices based on Renyi's axiomatic definition of entropyand examine some key properties of this functional that lead to the concept ofinfinite divisibility. The proposed formulation avoids the plug in estimationof density and brings along the representation power of reproducing kernelHilbert spaces. As an application example, we derive a supervised metriclearning algorithm using a matrix based analogue to conditional entropyachieving results comparable with the state of the art."
1211.4246,2013,What Regularized Auto-Encoders Learn from the Data Generating Distribution  ,"['Guillaume Alain', 'Yoshua Bengio']",https://arxiv.org/pdf/1211.4246.pdf,"4 1 0 2     g u A 9 1         ]  G L . s c [      5 v 6 4 2 4  .  1 1 2 1 : v i X r a  What Regularized Auto-Encoders Learn from the Data  Generating Distribution  Guillaume Alain and Yoshua Bengio guillaume.alain@umontreal.ca, yoshua.bengio@umontreal.ca  Department of Computer Science and Operations Research University of Montreal Montreal, H3C 3J7, Quebec, Canada  Abstract  What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clariﬁes some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that lo- cally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts pre- vious interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contrac- tion applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to max- imum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is conﬁrmed in sampling experiments.  1. Introduction  Machine learning is about capturing aspects of the unknown distribution from which the ob- served data are sampled (the data-generating distribution). For many learning algorithms and in particular in manifold learning, the focus is on identifying the regions (sets of points) in the space of examples where this distribution concentrates, i.e., which conﬁgurations of the observed variables are plausible.  Unsupervised representation-learning algorithms attempt to characterize the data-generating distribution through the discovery of a set of features or latent variables whose variations cap- ture most of the structure of the data-generating distribution. In recent years, a number of unsupervised feature learning algorithms have been proposed that are based on minimizing some form of reconstruction error, such as auto-encoder and sparse coding variants (Olshausen and Field, 1997; Bengio et al., 2007; Ranzato et al., 2007; Jain and Seung, 2008; Ranzato et al., 2008; Vincent et al., 2008; Kavukcuoglu et al., 2009; Rifai et al., 2011b,a; Gregor et al., 2011). An auto-encoder reconstructs the input through two stages, an encoder function f  1  (which outputs a learned representation h = f (x) of an example x) and a decoder function g, such that g(f (x)) ≈ x for most x sampled from the data-generating distribution. These feature learning algorithms can be stacked to form deeper and more abstract representations. Deep learning algorithms learn multiple levels of representation, where the number of levels is data-dependent. There are theoretical arguments and much empirical evidence to suggest that when they are well-trained, deep learning algorithms (Hinton et al., 2006; Bengio, 2009; Lee et al., 2009; Salakhutdinov and Hinton, 2009; Bengio and Delalleau, 2011; Bengio et al., 2013b) can perform better than their shallow counterparts, both in terms of learning features for the purpose of classiﬁcation tasks and for generating higher-quality samples.  Here we restrict ourselves to the case of continuous inputs x ∈ Rd with the data-generating distribution being associated with an unknown target density function, denoted p. Manifold learning algorithms assume that p is concentrated in regions of lower dimension (Cayton, 2005; Narayanan and Mitter, 2010), i.e., the training examples are by deﬁnition located very close to these high-density manifolds. In that context, the core objective of manifold learning algorithms is to identify where the density concentrates.  Some important questions remain concerning many of feature learning algorithms based on reconstruction error. Most importantly, what is their training criterion learning about the input density? Do these algorithms implicitly learn about the whole density or only some aspect? If they capture the essence of the target density, then can we formalize that link and in particular exploit it to sample from the model? The answers may help to establish that these algorithms actually learn implicit density models, which only deﬁne a density indirectly, e.g., through the estimation of statistics or through a generative procedure. These are the questions to which this paper contributes.  The paper is divided in two main sections, along with detailed appendices with proofs of the theorems. Section 2 makes a direct link between denoising auto-encoders (Vincent et al., 2008) and contractive auto-encoders (Rifai et al., 2011b), justifying the interest in the contractive training criterion studied in the rest of the paper. Section 3 is the main contribution and regards the following question: when minimizing that criterion, what does an auto-encoder learn about the data generating density? The main answer is that it estimates the score (ﬁrst derivative of the log-density), i.e., the direction in which density is increasing the most, which also corresponds to the local mean, which is the expected value in a small ball around the current location. It also estimates the Hessian (second derivative of the log-density).  Finally, Section 4 shows how having access to an estimator of the score can be exploited to estimate energy diﬀerences, and thus perform approximate MCMC sampling. This is achieved using a Metropolis-Hastings MCMC in which the energy diﬀerences between the proposal and the current state are approximated using the denoising auto-encoder. Experiments on artiﬁcial datasets show that a denoising auto-encoder can recover a good estimator of the data-generating distribution, when we compare the samples generated by the model with the training samples, projected into various 2-D views for visualization.  2. Contractive and Denoising Auto-Encoders  Regularized auto-encoders (see Bengio et al. (2012b) for a review and a longer exposition) capture the structure of the training distribution thanks to the productive opposition between reconstruction error and a regularizer. An auto-encoder maps inputs x to an internal represen- tation (or code) f (x) through the encoder function f , and then maps back f (x) to the input  2  Figure 1: Regularization forces the auto-encoder to become less sensitive to the input, but minimizing reconstruction error forces it to remain sensitive to variations along the manifold of high density. Hence the representation and reconstruction end up cap- turing well variations on the manifold while mostly ignoring variations orthogonal to it.  space through a decoding function g. The composition of f and g is called the reconstruction function r, with r(x) = g(f (x)), and a reconstruction loss function (cid:96) penalizes the error made, with r(x) viewed as a prediction of x. When the auto-encoder is regularized, e.g., via a sparsity regularizer, a contractive regularizer (detailed below), or a denoising form of regularization (that we ﬁnd below to be very similar to a contractive regularizer), the regularizer basically attempts to make r (or f ) as simple as possible, i.e., as constant as possible, as unresponsive to x as possible. It means that f has to throw away some information present in x, or at least represent it with less precision. On the other hand, to make reconstruction error small on the training set, examples that are neighbors on a high-density manifold must be represented with suﬃciently diﬀerent values of f (x) or r(x). Otherwise, it would not be possible to distinguish and hence correctly reconstruct these examples. It means that the derivatives of f (x) or r(x) in the x-directions along the manifold must remain large, while the derivatives (of f or r) in the x-directions orthogonal to the manifold can be made very small. This is illustrated in Figure 1. In the case of principal components analysis, one constrains the derivative to be exactly 0 in the directions orthogonal to the chosen projection directions, and around 1 in the chosen projection directions. In regularized auto-encoders, f is non-linear, meaning that it is allowed to choose diﬀerent principal directions (those that are well represented, i.e., ideally the manifold tangent directions) at diﬀerent x’s, and this allows a regularized auto-encoder with non-linear encoder to capture non-linear manifolds. Figure 2 illustrates the extreme case when the regularization is very strong (r(·) wants to be nearly constant where density is high) in the special case where the distribution is highly concentrated at three points (three training examples). It shows the compromise between obtaining the identity function at the training examples and having a ﬂat r near the training examples, yielding a vector ﬁeld r(x) − x that points towards the high density points.  Here we show that the denoising auto-encoder (Vincent et al., 2008) with very small Gaussian corruption and squared error loss is actually a particular kind of contractive auto- encoder (Rifai et al., 2011b), contracting the whole auto-encoder reconstruction function rather than just the encoder, whose contraction penalty coeﬃcient is the magnitude of the perturba- tion. This was ﬁrst suggested in (Rifai et al., 2011c).  The contractive auto-encoder, or CAE (Rifai et al., 2011b), is a particular form of regularized  auto-encoder which is trained to minimize the following regularized reconstruction error:  (cid:34)  (cid:35)  (cid:13)(cid:13)(cid:13)(cid:13) ∂f (x)  ∂x  (cid:13)(cid:13)(cid:13)(cid:13)2  F  LCAE = E  (cid:96)(x, r(x)) + λ  (1)  where r(x) = g(f (x)) and ||A||2 F is the sum of the squares of the elements of A. Both the squared loss (cid:96)(x, r) = ||x − r||2 and the cross-entropy loss (cid:96)(x, r) = −x log r − (1 − x) log(1 − r)  3  Figure 2: The reconstruction function r(x) (in turquoise) which would be learned by a high-capacity auto-encoder on a 1-dimensional input, i.e., minimizing reconstruction error at the training examples xi (with r(xi) in red) while trying to be as constant as possible otherwise. The ﬁgure is used to exagerate and illustrate the eﬀect of the regularizer (corresponding to a large σ2 in the loss function L later described by (6)). The dotted line is the identity reconstruction (which might be obtained without the regularizer). The blue arrows shows the vector ﬁeld of r(x) − x pointing towards high density peaks as estimated by the model, and estimating the score (log-density derivative), as shown in this paper.  have been used, but here we focus our analysis on the squared loss because of the easier mathematical treatment it allows. Note that success in minimizing the CAE criterion strongly depends on the parametrization of f and g and in particular on the tied weights constraint used, with f (x) = sigmoid(W x + b) and g(h) = sigmoid(W T h + c). The above regularizing term forces f (as well as g, because of the tied weights) to be contractive, i.e., to have singular values less than 1 1. Larger values of λ yield more contraction (smaller singular values) where it hurts reconstruction error the least, i.e., in the local directions where there are only little or no variations in the data. These typically are the directions orthogonal to the manifold of high density concentration, as illustrated in Fig. 2.  The denoising auto-encoder, or DAE (Vincent et al., 2008), is trained to minimize the  following denoising criterion:  LDAE = E [(cid:96)(x, r(N (x)))]  (2)  where N (x) is a stochastic corruption of x and the expectation is over the training distribution and the corruption noise source. Here we consider mostly the squared loss and Gaussian noise corruption, again because it is easier to handle them mathematically. In many cases, the exact same proofs can be applied to any kind of additive noise, but Gaussian noise serves as a good frame of reference.  1. Note that an auto-encoder without any regularization would tend to ﬁnd many leading singular values near 1 in order to minimize reconstruction error, i.e., preserve input norm in all the directions of variation present in the data.  4  x""r(x)""x1""x2""x3""Theorem 1 Let p be the probability density function of the data. If we train a DAE using the expected quadratic loss and corruption noise N (x) = x + (cid:15) with  (cid:15) ∼ N(cid:0)0, σ2I(cid:1) ,  then the optimal reconstruction function r∗(x) will be given by E(cid:15) [p(x − (cid:15))(x − (cid:15))]  r∗(x) =  E(cid:15) [p(x − (cid:15))]  (3)  for values of x where p(x) (cid:54)= 0. cally as σ → 0, we get that  Moreover, if we consider how the optimal reconstruction function r∗  σ(x) behaves asymptoti-  r∗ σ(x) = x + σ2 ∂ log p(x)  ∂x  + o(σ2)  as σ → 0.  (4)  The proof of this result is found in the Appendix. We make use of the small o notation throughout this paper and assume that the reader is familiar with asymptotic notation. In the context of Theorem 1, it has to be understood that all the other quantities except for σ are ﬁxed when we study the eﬀect of σ → 0. Also, note that the σ in the index of r∗ σ is to indicate that r∗ σ was chosen based on the value of σ. That σ should not be mistaken for a parameter to be learned.  Equation (3) reveals that the optimal DAE reconstruction function at every point x is given by a kind of convolution involving the density function p, or weighted average from the points in the neighbourhood of x, depending on how we would like to view it. A higher noise level σ means that a larger neighbourhood of x is taken into account. Note that the total quantity of “mass” being included in the weighted average of the numerator of (3) is found again at the denominator. Gaussian noise is a simple case in the sense that it is additive and symmetrical, so it avoids the complications that would occur when trying to integrate over the density of pre-images x(cid:48) such that N (x(cid:48)) = x for a given x. The ratio of those quantities that we have in equation (3), however, depends strongly on the decision that we made to minimize the expected square error.  When we look at the asymptotic behavior with equation (4), the ﬁrst thing to observe is σ(x) is x, and then the remainder goes to 0 as σ → 0. that the leading term in the expansion of r∗ When there is no noise left at all, it should be clear that the best reconstruction target for any value x would be that x itself.  We get something even more interesting if we look at the second term of equation (4)  because it gives us an estimator of the score from  ∂ log p(x)  ∂x  = (r∗  σ(x) − x) /σ2 + o(1)  as σ → 0.  (5)  This result is at the core of our paper. It is what allows us to start from a trained DAE, and then recover properties of the training density p(x) that can be used to sample from p(x).  Most of the asymptotic properties that we get by considering the limit as the Gaussian noise level σ goes to 0 could be derived from a family of noise distribution that approaches a point mass distribution in a relatively “nice” way.  5  An interesting connection with contractive auto-encoders can also be observed by using a Taylor expansion of the denoising auto-encoder loss and assuming only that rσ(x) = x + o(1) as σ → 0. In that case we get the following proposition.  the expected quadratic loss and corruption noise N (x) = x + (cid:15), with (cid:15) ∼ N(cid:0)0, σ2I(cid:1). If we  Proposition 1 Let p be the probability density function of the data. Consider a DAE using  assume that the non-parametric solutions rσ(x) satistﬁes  rσ(x) = x + o(1)  as σ → 0,  then we can rewrite the loss as  (cid:34)  LDAE (rσ) = E  (cid:107)rσ(x) − x(cid:107)2  2 + σ2  (cid:13)(cid:13)(cid:13)(cid:13) ∂rσ(x)  ∂x  (cid:13)(cid:13)(cid:13)(cid:13)2  F  (cid:35)  + o(σ2)  as σ → 0  where the expectation is taken with respect to X, whose distribution is given by p.  The proof is in Appendix and uses a simple Taylor expansion around x.  Proposition 1 shows that the DAE with small corruption of variance σ2 is similar to a contractive auto-encoder with penalty coeﬃcient λ = σ2 but where the contraction is imposed explicitly on the whole reconstruction function r(·) = g(f (·)) rather than on f (·) alone2.  This analysis motivates the deﬁnition of the reconstruction contractive auto-encoder (RCAE), a variation of the CAE where loss function is instead the squared reconstruction loss plus con- tractive penalty on the reconstruction:  (cid:34)  (cid:35)  (cid:13)(cid:13)(cid:13)(cid:13) ∂r(x)  ∂x  (cid:13)(cid:13)(cid:13)(cid:13)2  F  LRCAE = E  (cid:107)r(x) − x(cid:107)2  2 + σ2  .  (6)  This is an analytic version of the denoising criterion with small noise σ2, and also corresponds to a contractive auto-encoder with contraction on both f and g, i.e., on r.  Because of the similarity between DAE and RCAE when taking λ = σ2 and because the semantics of σ2 is clearer (as a squared distance in input space), we will denote σ2 for the penalty term coeﬃcient in situations involving RCAE. For example, in the statement of Theorem 2, this σ2 is just a positive constant; there is no notion of additive Gaussian noise, i.e., σ2 does not explicitly refer to a variance, but using the notation σ2 makes it easier to intuitively see the connection to the DAE setting.  The connection between DAE and RCAE established in Proposition 1 motivates the follow- ing Theorem 2 as an alternative way to achieve a result similar to that of Theorem 1. In this theorem we study the asymptotic behavior of the RCAE solution.  Theorem 2 Let p be a probability density function that is continuously diﬀerentiable once and with support Rd (i.e. ∀x ∈ Rd we have p(x) (cid:54)= 0). Let Lσ be the loss function deﬁned by  (cid:90)  (cid:34)  (cid:35)  (cid:13)(cid:13)(cid:13)(cid:13) ∂r(x)  ∂x  (cid:13)(cid:13)(cid:13)(cid:13)2  F  Lσ(r) =  p(x)  Rd  (cid:107)r(x) − x(cid:107)2  2 + σ2  dx  (7)  2. In the CAE there is a also a contractive eﬀect on g(·) as a side eﬀect of the parametrization with weights  tied between f (·) and g(·).  6  for r : Rd → Rd assumed to be diﬀerentiable twice, and 0 ≤ σ2 ∈ R used as factor to the penalty term.  σ(x) denote the optimal function that minimizes Lσ. Then we have that  Let r∗  r∗ σ(x) = x + σ2 ∂ log p(x)  ∂x  + o(σ2)  as σ → 0.  Moreover, we also have the following expression for the derivative  ∂r∗ σ(x) ∂x  = I + σ2 ∂2 log p(x)  ∂x2  + o(σ2)  as σ → 0.  (8)  Both these asymptotic expansions are to be understood in a context where we consider {r∗ σ(x)}σ≥0 to be a family of optimal functions minimizing Lσ for their corresponding value of σ. The asymptotic expansions are applicable point-wise in x, that is, with any ﬁxed x we look at the behavior as σ → 0.  The proof is given in the appendix and uses the Euler-Lagrange equations from the calculus  of variations.  3. Minimizing the Loss to Recover Local Features of p(·) One of the central ideas of this paper is that in a non-parametric setting (without parametric constraints on r), we have an asymptotic formula (as the noise level σ → 0) for the optimal reconstruction function for the DAE and RCAE that allows us to recover the score ∂ log p(x) .  A DAE is trained with a method that knows nothing about p, except through the use of training samples to minimize a loss function, so it comes as a surprise that we can compute the score of p at any point x.  ∂x  In the following subsections we explore the consequences and the practical aspect of this.  In an experimental setting, the expected loss (7) is replaced by the empirical loss  3.1 Empirical Loss  N(cid:88) based on a sample(cid:8)x(n)(cid:9)N  ˆL =  1 N  n=1  (cid:32)(cid:13)(cid:13)(cid:13)r(x(n)) − x(n)(cid:13)(cid:13)(cid:13)2  2  + σ2  (cid:33)  (cid:13)(cid:13)(cid:13)(cid:13) ∂r(x)  ∂x  (cid:12)(cid:12)(cid:12)(cid:12)x=x(n)  (cid:13)(cid:13)(cid:13)(cid:13)2  F  n=1 drawn from p(x).  Alternatively, the auto-encoder is trained online (by stochastic gradient updates) with a stream of examples x(n), which corresponds to performing stochastic gradient descent on the expected loss (7). In both cases we obtain an auto-encoder that approximately minimizes the expected loss.  An interesting question is the following: what can we infer from the data generating density  when given an auto-encoder reconstruction function r(x)?  The premise is that this auto-encoder r(x) was trained to approximately minimize a loss function that has exactly the form of (7) for some σ2 > 0. This is assumed to have been done through minimizing the empirical loss and the distribution p was only available indirectly n=1. We do not have access to p or to the samples. We have only  through the samples(cid:8)x(n)(cid:9)N  r(x) and maybe σ2.  7  We will now discuss the usefulness of r(x) based on diﬀerent conditions such as the model  capacity and the value of σ2.  3.2 Perfect World Scenario  As a starting point, we will assume that we are in a perfect situation, i.e., with no constraint on r (non-parametric setting), an inﬁnite amount of training data, and a perfect minimization. We will see what can be done to recover information about p in that ideal case. Afterwards, we will drop certain assumptions one by one and discuss the possible paths to getting back some information about p.  We use notation rσ(x) when we want to emphasize the fact that the value of r(x) came  from minimizing the loss with a certain ﬁxed σ.  Suppose that rσ(x) was trained with an inﬁnite sample drawn from p. Suppose also that it had inﬁnite (or suﬃcient) model capacity and that it is capable of achieving the minimum of the loss function (7) while satisfying the requirement that r(x) be twice diﬀerentiable. Suppose that we know the value of σ and that we are working in a computing environment of arbitrary precision (i.e. no rounding errors).  As shown by Theorem 1 and Theorem 2, we would be able to get numerically the values of  ∂ log p(x)  ∂x  at any point x ∈ Rd by simply evaluating σ2 → ∂ log p(x)  rσ(x) − x  ∂x  as σ → 0.  (9) In the setup described, we do not get to pick values of σ so as to take the limit σ → 0. However, it is assumed that σ is already suﬃciently small that the above quantity is close to ∂ log p(x) for all intents and purposes.  ∂x  3.3 Simple Numerical Example  To give an example of this in one dimension, we will show what happens when we train a non-parametric model ˆr(x) to minimize numerically the loss relative to p(x). We train both a DAE and an RCAE in this fashion by minimizing a discretized version of their losses deﬁned by equations (2) and (6). The goal here is to show that, for either a DAE or RCAE, the approximation of the score that we get through equation (5) gets arbitrarily close to the actual score ∂  ∂x log p(x) as σ → 0.  The distribution p(x) studied is shown in Figure 3 (left) and it was created to be simple enough to illustrate the mechanics. We plot p(x) in Figure 3 (left) along with the score of p(x) (right). The model ˆr(x) is ﬁtted by dividing the interval [−1.5, 1.5] into M = 1000 partition points x1, . . . , xM evenly separated by a distance ∆. The discretized version of the RCAE loss function is  p(xi)∆ (ˆr(xi) − xi)2 + σ2  p(xi)∆  i=1  i=1  M−1(cid:88)  (cid:18) ˆr(xi+1) − ˆr(xi)  (cid:19)2  ∆  .  (10)  M(cid:88)  Every value ˆr(xi) for i = 1, . . . , M is treated as a free parameter. Setting to 0 the derivative with respect to the ˆr(xi) yields a system of linear equations in M unknowns that we can solve exactly. From that RCAE solution ˆr we get an approximation of the score of p at each point xi. A similar thing can be done for the DAE by using a discrete version of the exact solution (3) from Theorem 1. We now have two ways of approximating the score of p.  8  (a) p(x) = 1  Z exp(−E(x))  (b) ∂  ∂x log p(x) = − ∂  ∂x E(x)  Figure 3: the density p(x) and its score for a simple one-dimensional example.  In Figure 4 we compare the approximations to the actual score of p for decreasingly small  values of σ ∈ {1.00, 0.31, 0.16, 0.06}.  3.4 Vector Field Around a Manifold  We extend the experimentation of section 3.3 to a 1-dimensional manifold in 2-D space, in which one can visualize r(x)− x as a vector ﬁeld, and we go from the non-parametric estimator of the previous section to an actual auto-encoder trained by numerically minimizing the regularized reconstruction error.  Two-dimensional data points (x, y) were generated along a spiral according to the following  equations:  x = 0.04 sin(t),  y = 0.04 cos(t),  t ∼ Uniform (3, 12) .  A denoising auto-encoder was trained with Gaussian corruption noise σ = 0.01. The encoder is f (x) = tanh(b + W x) and the decoder is g(h) = c + V h. The parameters (b, c, V, W ) are optimized by BFGS to minimize the average squared error, using a ﬁxed training set of 10 000 samples (i.e. the same corruption noises were sampled once and for all). We found better results with untied weights, and BFGS gave more accurate models than stochastic gradient descent. We used 1000 hiddens units and ran BFGS for 1000 iterations.  The non-convexity of the problem makes it such that the solution found depends on the initialization parameters. The random corruption noise used can also inﬂuence the ﬁnal out- come. Moreover, the fact that we are using a ﬁnite training sample size with reasonably small noise may allow for undesirable behavior of r in regions far away from the training samples. For those reasons, we trained the model multiple times and selected two of the most visually appealing outcomes. These are found in Figure 5 which features a more global perspective along with a close-up view.  Figure 5 shows the data along with the learned score function (shown as a vector ﬁeld). We see that that the vector ﬁeld points towards the nearest high-density point on the data manifold. The vector ﬁeld is close to zero near the manifold (i.e. the reconstruction error is close to zero), also corresponding to peaks of the implicitly estimated density. The points on the manifolds  9  Figure 4: Comparing the approximation of the score of p given by discrete versions of optimally trained auto-encoders with inﬁnite capacity. The approximations given by the RCAE are in orange while the approximations given by the DAE are in purple. The results are shown for decreasing values of σ ∈ {1.00, 0.31, 0.16, 0.06} that have been selected for their visual appeal. As expected, we see in that the RCAE (orange) and DAE (purple) approximations of the score are close to each other as predicted by Proposition 1. Moreover, they are also converging to the true score (green) as predicted by Theorem 1 and Theorem 2.  play the role of sinks for the vector ﬁeld. Other places where reconstruction error may be low, but where the implicit density is not high, are sources of the vector ﬁeld. In Figure 5(b) we can see that we have that kind of behavior halfway between two sections of the manifold. This shows that reconstruction error plays a very diﬀerent role as what was previously hypothesized: whereas Ranzato et al. (2008) viewed reconstruction error as an energy function, our analysis suggests that in regularized auto-encoders, it is the norm of an approximate score, i.e., the derivative of the energy w.r.t. input. Note that the norm of the score should be small near training examples (corresponding to local maxima of density) but it could also be small at other places corresponding to local minima of density. This is indeed what happens in the spiral example shown. It may happen whenever there are high-density regions separated by a low-density region: tracing paths from one high-density region to another should cross a “median” lower-dimensional region (a manifold) where the density has a local maximum along  10  (a) r(x) − x vector ﬁeld, acting as sink, zoomed out  (b) r(x) − x vector ﬁeld, close-up  Figure 5: The original 2-D data from the data generating density p(x) is plotted along with the vector ﬁeld deﬁned by the values of r(x)− x for trained auto-encoders (corresponding to the estimation of the score ∂ log p(x)  ).  ∂x  the path direction. The reason such a median region is needed is because at these points the vectors r(x) − x must change sign: on one side of the median they point to one of the high- density regions while on the other side they point to the other, as clearly visible in Figure 5(b) between the arms of the spiral.  We believe that this analysis is valid not just for contractive and denoising auto-encoders, but for regularized auto-encoders in general. The intuition behind this statement can be ﬁrmed up by analyzing Figure 2: the score-like behavior of r(x) − x arises simply out of the opposing forces of (a) trying to make r(x) = x at the training examples and (b) trying to make r(x) as regularized as possible (as close to a constant as possible).  Note that previous work (Rifai et al., 2012; Bengio et al., 2013b) has already shown that contractive auto-encoders (especially when they are stacked in a way similar to RBMs in a deep belief net) learn good models of high-dimensional data (such as images), and that these models can be used not just to obtain good representations for classiﬁcation tasks but that good quality samples can be obtained from the model, by a random walk near the manifold of high-density. This was achieved by essentially following the vector ﬁeld and adding noise along the way.  3.5 Missing σ2  When we are in the same setting as in section 3.2 but the value of σ2 is unknown, we can modify (9) a bit and avoid dividing by σ2. That is, for a trained reconstruction function r(x) given to us we just take the quantity r(x)− x and it should be approximatively the score up to a multiplicative constant.  r(x) − x ∝ ∂ log p(x)  ∂x  Equivalently, if one estimates the density via an energy function (minus the unnormalized log density), then x − r(x) estimates the derivative of the energy function.  11  We still have to assume that σ2 is small. Otherwise, if the unknown σ2 is too large we  might get a poor estimation of the score.  3.6 Limited Parameterization We should also be concerned about the fact that r(x) − x is trying to approximate − ∂E(x) as σ → 0 but we have not made any assumptions about the space of functions that r can represent when we are dealing with a speciﬁc implementation.  ∂x  When using a certain parameterization of r such as the one from section 3.3, there is no guarantee that the family of functions in which we select r each represent a conservative vector ﬁeld (i.e. the gradient of a potential function). Even if we start from a density p(x) ∝ exp(−E(x)) and we have that r(x) − x is very close to − ∂ ∂x E(x) in terms of some given norm, there is not guarantee that there exists an associated function E0(x) for which r(x) − x ∝ − ∂ ∂x E0(x) and E0(x) ≈ E(x). In fact, in many cases we can trivially show the non-existence of such a E0(x) by computing the curl of r(x). The curl has to be equal to 0 everywhere if r(x) is indeed the derivative of a potential function. We can omit the x terms from the computations because we can easily ﬁnd its antiderivative by looking at x = ∂  ∂x (cid:107)x(cid:107)2 2.  Conceptually, another way to see this is to argue that if such a function E0(x) existed, its  second-order mixed derivatives should be equal. That is, we should have that  which is equivalent to  ∂2E0(x) ∂xi∂xj  =  ∂2E0(x) ∂xj∂xi  ∀i, j,  ∂ri(x)  ∂xj  =  ∂rj(x)  ∂xi  ∀i, j.  Again in the context of section 3.3, with the parameterization used for that particual kind of denoising auto-encoder, this would yield the constraint that V T = W . That is, unless we are using tied weights, we know that no such potential E0(x) exists, and yet when running the experiments from section 3.3 we obtained much better results with untied weights. To make things worse, it can also be demonstrated that the energy function that we get from tied weights leads to a distribution that is not normalizable (it has a divergent integral over Rd). In that sense, this suggests that we should not worry too much about the exact parameterization of the denoising auto-encoder as long as it has the required ﬂexibility to approximate the optimal reconstruction function suﬃciently well.  3.7 Relation to Denoising Score Matching  There is a connection between our results and previous research involving score matching for denoising auto-encoders. We will summarize here the existing results from Vincent (2011) and show that, whereas they have shown that denoising auto-encoders with a particular form estimated the score, our results extend this to a very large family of estimators (including the non-parametric case). This will provide some reassurance given some of the potential issues mentioned in section 3.6. Motivated by the analysis of denoising auto-encoders, Vincent (2011) are concerned with the case where we explicitly parametrize an energy function E(x), yielding an associated score function ψ(x) = − ∂E(x) and we stochastically corrupt the original samples x ∼ p to obtain  ∂x  12  noisy samples ˜x ∼ qσ(˜x|x). In particular, the article analyzes the case where qσ adds Gaussian noise of variance σ2 to x. The main result is that minimizing the expected square diﬀerence between ψ(˜x) and the score of qσ(˜x|x),  Ex,˜x[||ψ(˜x) − ∂ log qσ(˜x|x)  ||2],  density qσ(˜x) =(cid:82) qσ(˜x|x)p(x)dx, where p(x) generates the training samples x. Note that when  is equivalent to performing score matching (Hyv¨arinen, 2005) with estimator ψ(˜x) and target  ∂ ˜x  a ﬁnite training set is used, qσ(˜x) is simply a smooth of the empirical distribution (e.g. the Parzen density with Gaussian kernel of width σ). When the corruption noise is Gaussian, qσ(˜x|x) ∂ ˜x = x−˜x  σ2 , from which we can deduce that if we deﬁne a reconstruction function  r(˜x) = ˜x + σ2ψ(˜x),  (11)  then the above expectation is equivalent to  Ex,˜x[|| r(˜x) − ˜x  σ2  − x − ˜x  σ2  ||2] =  1  σ2 Ex,˜x[||r(˜x) − x||2]  which is the denoising criterion. This says that when the reconstruction function r is parametrized so as to correspond to the score ψ of a model density (as per eq. 11, and where ψ is a derivative of some log-density), the denoising criterion on r with Gaussian corruption noise is equivalent to score matching with respect to a smooth of the data generating density, i.e., a regularized form of score matching. Note that this regularization appears desirable, because matching the score of the empirical distribution (or an insuﬃciently smoothed version of it) could yield undesirable results when the training set is ﬁnite. Since score matching has been shown to be a consistent induction principle (Hyv¨arinen, 2005), it means that this denoising score matching (Vincent, 2011; Kingma and LeCun, 2010; Swersky et al., 2011) criterion recovers the underlying density, up to the smoothing induced by the noise of variance σ2. By making σ2 small, we can make the estimator arbitrarily good (and we would expect to want to do that as the amount of training data increases). Note the correspondance of this conclusion with the results presented here, which show (1) the equivalence between the RCAE’s regularization coeﬃcient and the DAE’s noise variance σ2, and (2) that minimizing the equivalent analytic criterion (based on a contraction penalty) estimates the score when σ2 is small. The diﬀerence is that our result holds even when r is not parametrized as per eq. 11, i.e., is not forced to correspond with the score function of a density.  3.8 Estimating the Hessian Since we have r(x)−x as an estimator of the score, we readily obtain that the Hessian of the log-density, can be estimated by the Jacobian of the reconstruction function minus the identity matrix:  σ2  ∂x2 as shown by equation (8) of Theorem 2.  ∂2 log p(x)  ∂r(x)  ≈ (  − I)/σ2  ∂x  In spite of its simplicity, this result is interesting because it relates the derivative of the reconstruction function, i.e., a Jacobian matrix, with the second derivative of the log-density (or of the energy). This provides insights into the geometric interpretation of the reconstruction  13  function when the density is concentrated near a manifold. In that case, near the manifold the score is nearly 0 because we are near a ridge of density, and the density’s second derivative matrix tells us in which directions the ﬁrst density remains close to zero or increases. The ridge directions correspond to staying on the manifold and along these directions we expect the second derivative to be close to 0. In the orthogonal directions, the log-density should decrease sharply while its ﬁrst and second derivatives would be large in magnitude and negative in directions away from the manifold.  Returning to the above equation, keep in mind that in these derivations σ2 is near 0 and r(x) is near x, so that ∂r(x) is close to the identity. In particular, in the ridge (manifold) directions, ∂x we should expect ∂r(x) ∂x to be closer to the identity, which means that the reconstruction remains faithful (r(x) = x) when we move on the manifold, and this corresponds to the eigenvalues of ∂r(x) near 0. On the other  that are near 1, making the corresponding eigenvalues of ∂2 log p(x)  ∂x2  ∂x hand, in the directions orthogonal to the manifold, ∂r(x) ∂x corresponding eigenvalues of ∂2 log p(x)  negative.  ∂x2  should be smaller than 1, making the  Besides ﬁrst and second derivatives of the density, other local properties of the density are  its local mean and local covariance, discussed in the Appendix, section 6.4.  4. Sampling with Metropolis-Hastings  4.1 Estimating Energy Diﬀerences  One of the immediate consequences of equation (5) is that, while we cannot easily recover the energy E(x) itself, it is possible to approximate the energy diﬀerence E(x∗) − E(x) between two states x and x∗. This can be done by using a ﬁrst-order Taylor approximation  E(x∗) − E(x) =  (x∗ − x) + o((cid:107)x∗ − x(cid:107)).  ∂E(x)  T  ∂x  To get a more accurate approximation, we can also use a path integral from x to x∗ that we can discretize in suﬃciently many steps. With a smooth path γ(t) : [0, 1] → Rd, assuming that γ stays in a region where our DAE/RCAE can be used to approximate ∂E ∂x well enough, we have that  E(x∗) − E(x) =  (γ(t))  γ(cid:48)(t)dt.  (12)  The simplest way to discretize this path integral is to pick points {xi}n distances on a straight line from x1 = x to xn = x∗. We approximate (12) by  i=1 spread at even  (cid:21)T  (cid:21)T  (cid:90) 1  (cid:20) ∂E  ∂x  0  (cid:20) ∂E  n(cid:88)  ∂x  i=1  (xi)  (x∗ − x)  (13)  E(x∗) − E(x) ≈ 1 n  4.2 Sampling  With equation (12) from section 4.1 we can perform approximate sampling from the estimated distribution, using the score estimator to approximate energy diﬀerences which are needed in the Metropolis-Hastings accept/reject decision. Using a symmetric proposal q(x∗|x), the acceptance ratio is  α =  p(x∗) p(x)  = exp(−E(x∗) + E(x))  14  which can be computed with (12) or approximated with (13) as long as we trust that our DAE/RCAE was trained properly and has enough capacity to be a suﬃciently good estimator of ∂E ∂x . An example of this process is shown in Figure 6 in which we sample from a density concentrated around a 1-d manifold embedded in a space of dimension 10. For this particular task, we have trained only DAEs and we are leaving RCAEs out of this exercise. Given that the data is roughly contained in the range [−1.5, 1.5] along all dimensions, we selected a training noise level σtrain = 0.1 so that the noise would have an appreciable eﬀect while still being relatively small. As required by Theorem 1, we have used isotropic Gaussian noise of variance σ2 train.  The Metropolis-Hastings proposal q(x∗|x) = N (0, σ2  MHI) has a noise parameter σMH that needs to be set. In the situation shown in Figure 6, we used σMH = 0.1. After some hyperpa- rameter tweaking and exploring various scales for σtrain, σMH, we found that setting both to be 0.1 worked well.  When σtrain is too large, the DAE trained learns a “blurry” version of the density that fails to represent the details that we are interested in. The samples shown in Figure 6 are very convincing in terms of being drawn from a distribution that models well the original density. We have to keep in mind that Theorem 1 describes the behavior as σtrain → 0 so we would expect that the estimator becomes worse when σtrain is taking on larger values. In this particular case with σtrain = 0.1, it seems that we are instead modeling something like the original density to which isotropic Gaussian noise of variance σ2  train has been added.  In the other extreme, when σtrain is too small, the DAE is not exposed to any training example farther away from the density manifold. This can lead to various kinds of strange behaviors when the sampling algorithm falls into those regions and then has no idea what to do there and how to get back to the high-density regions. We come back to that topic in section 4.3.  It would certainly be possible to pick both a very small value for σtrain = σMH = 0.01 to avoid the spurius maxima problem illustrated in section 4.3. However, this leads to the same kind of mixing problems that any kind of MCMC algorithm has. Smaller values of σMH lead to higher acceptance ratios but worse mixing properties.  4.3 Spurious Maxima  There are two very real concerns with the sampling method discussed in section 4.2. The ﬁrst problem is with the mixing properties of MCMC and it is discussed in that section. The second issue is with spurious probability maxima resulting from inadequate training of the DAE. It happens when an auto-encoder lacks the capacity to model the density with enough precision, or when the training procedure ends up in a bad local minimum (in terms of the DAE parameters). This is illustrated in Figure 7 where we show an example of a vector ﬁeld r(x) − x for a DAE that failed to properly learn the desired behavior in regions away from the spiral-shaped density.  5. Conclusion  Whereas auto-encoders have long been suspected of capturing information about the data generating density, this work has clariﬁed what some of them are actually doing, showing that they can actually implicitly recover the data generating density altogether. We have shown that  15  original  sampled  original  sampled  Figure 6: Samples drawn from the estimate of ∂E  ∂x given by a DAE by the Metropolis- Hastings method presented in section 4. By design, the data density distribu- tion is concentrated along a 1-d manifold embedded in a space of dimension 10. This data can be visualized in the plots above by plotting pairs of dimensions (x0, x1), . . . , (x8, x9), (x9, x0), going in reading order from left to right and then line by line. For each pair of dimensions, we show side by side the original data (left) with the samples drawn (right).  regularized auto-encoders such as the denoising auto-encoder and a form of contractive auto- encoder are closely related to each other and estimate local properties of the data generating density: the ﬁrst derivative (score) and second derivative of the log-density, as well as the local mean. This contradicts the previous interpretation of reconstruction error as being an energy function (Ranzato et al., 2008) but is consistent with our experimental ﬁndings. Our results do not require the reconstruction function to correspond to the derivative of an energy function as in Vincent (2011), but hold simply by virtue of minimizing the regularized reconstruction error training criterion. This suggests that minimizing a regularized reconstruction error may be an alternative to maximum likelihood for unsupervised learning, avoiding the need for MCMC in the inner loop of training, as in RBMs and deep Boltzmann machines, analogously to score  16  (a) DAE misbehaving when away from manifold  (b) sampling getting trapped into bad attractor  Figure 7: (a) On the left we show a r(x) − x vector ﬁeld similar to that of the earlier Figure 5. The density is concentrated along a spiral manifold and we should have the recon- struction function r bringing us back towards the density. In this case, it works well in the region close to the spiral (the magnitude of the vectors is so small that the arrows are shown as dots). However, things are out of control in the regions outside. This is because the level of noise used during training was so small that not enough of the training examples were found in those regions. (b) On the right we sketch what may happen when we follow a sampling procedure as described in section 4.2. We start in a region of high density (in purple) and we illustrate in red the trajectory that our samples may take. In that situation, the DAE/RCAE was not trained properly. The resulting vector ﬁeld does not reﬂect the density accurately because it should not have this attractor (i.e. stable ﬁxed point) outside of the manifold on which the density is concentrated. Conceptually, the sam- pling procedure visits that spurious attractor because it assumes that it corresponds to a region of high probability. In some cases, this eﬀect is regrettable but not catas- trophic, but in other situations we may end up with completely unusable samples. In the experiments, training with enough of the examples involving suﬃciently large corruption noise typically eliminates that problem.  matching (Hyv¨arinen, 2005; Vincent, 2011). Toy experiments have conﬁrmed that a good estimator of the density can be obtained when this criterion is non-parametrically minimized. The experiments have also conﬁrmed that an MCMC could be setup that approximately samples from the estimated model, by estimating energy diﬀerences to ﬁrst order (which only requires the score) to perform approximate Metropolis-Hastings MCMC.  Many questions remain open and deserve futher study. A big question is how to general- ize these ideas to discrete data, since we have heavily relied on the notions of scores, i.e., of derivatives with respect to x. A natural extension of the notion of score that could be ap- plied to discrete data is the notion of relative energy, or energy diﬀerence between a point x and a perturbation ˜x of x. This notion has already been successfully applied to obtain the equivalent of score matching for discrete models, namely ratio matching (Hyv¨arinen, 2007). More generally, we would like to generalize to any form of reconstruction error (for example  17  many implementations of auto-encoders use a Bernouilli cross-entropy as reconstruction loss function) and any (reasonable) form of corruption noise (many implementations use masking or salt-and-pepper noise, not just Gaussian noise). More fundamentally, the need to rely on σ → 0 is troubling, and getting rid of this limitation would also be very useful. A possible solution to this limitation, as well as adding the ability to handle both discrete and continuous variables, has recently been proposed while this article was under review (Bengio et al., 2013a). It would also be interesting to generalize the results presented here to other regularized auto-encoders besides the denoising and contractive types. In particular, the commonly used sparse auto-encoders seem to ﬁt the qualitative pattern illustrated in Figure 2 where a score-like vector ﬁeld arises out of the opposing forces of minimizing reconstruction error and regularizing the auto-encoder.  We have mostly considered the harder case where the auto-encoder parametrization does not guarantee the existence of an analytic formulation of an energy function. It would be interesting to compare experimentally and study mathematically these two formulations to assess how much is lost (because the score function may be somehow inconsistent) or gained (because of the less constrained parametrization).  Acknowledgements  The authors thank Salah Rifai, Max Welling, Yutian Chen and Pascal Vincent for fruitful discussions, and acknowledge the funding support from NSERC, Canada Research Chairs and CIFAR.  References  Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2  (1):1–127, 2009. Also published as a book. Now Publishers, 2009.  Yoshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. In ALT’2011,  2011.  Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep  networks. In NIPS 19, pages 153–160. MIT Press, 2007.  Yoshua Bengio, Guillaume Alain, and Salah Rifai. Implicit density estimation by local moment matching  to sample from auto-encoders. Technical report, arXiv:1207.0057, 2012a.  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new  perspectives. Technical report, arXiv:1206.5538, 2012b.  Yoshua Bengio, Yao Li, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as  generative models. Technical Report arXiv:1305.6663, Universite de Montreal, 2013a.  Yoshua Bengio, Gr´egoire Mesnil, Yann Dauphin, and Salah Rifai. Better mixing via deep representations.  In ICML’13, 2013b.  Lawrence Cayton. Algorithms for manifold learning. Technical Report CS2008-0923, UCSD, 2005.  B. Dacorogna. Introduction to the Calculus of Variations. World Scientiﬁc Publishing Company, 2004.  Karol Gregor, Arthur Szlam, and Yann LeCun. Structured sparse coding via lateral inhibition.  In  Advances in Neural Information Processing Systems (NIPS 2011), volume 24, 2011.  18  Geoﬀrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets.  Neural Computation, 18:1527–1554, 2006.  Aapo Hyv¨arinen. Estimation of non-normalized statistical models using score matching. Journal of  Machine Learning Research, 6:695–709, 2005.  Aapo Hyv¨arinen. Some extensions of score matching. Computational Statistics and Data Analysis, 51:  2499–2512, 2007.  Viren Jain and Sebastian H. Seung. Natural image denoising with convolutional networks. In NIPS’08,  pages 769–776, 2008.  Koray Kavukcuoglu, Marc’Aurelio Ranzato, Rob Fergus, and Yann LeCun. Learning invariant features In Proceedings of the Computer Vision and Pattern Recognition  through topographic ﬁlter maps. Conference (CVPR’09), pages 1605–1612. IEEE, 2009.  Diederik Kingma and Yann LeCun. Regularized estimation of image statistics by score matching. In J. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1126–1134, 2010.  Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. Convolutional deep belief net- works for scalable unsupervised learning of hierarchical representations. In L´eon Bottou and Michael Littman, editors, ICML 2009. ACM, Montreal (Qc), Canada, 2009.  Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis. In  NIPS’2010. 2010.  B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: a strategy employed by V1? Vision Research, 37:3311–3325, December 1997. URL http://view.ncbi.nlm.nih.gov/ pubmed/9425546.  Marc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Eﬃcient learning of sparse representations with an energy-based model. In NIPS’06, pages 1137–1144. MIT Press, 2007.  Marc’Aurelio Ranzato, Y-Lan Boureau, and Yann LeCun. Sparse feature learning for deep belief net-  works. In NIPS’07, pages 1185–1192, Cambridge, MA, 2008. MIT Press.  Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent  classiﬁer. In NIPS’2011, 2011a. Student paper award.  Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-  encoders: Explicit invariance during feature extraction. In ICML’11, 2011b.  Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-  encoders: Explicit invariance during feature extraction. In ICML’2011, 2011c.  Salah Rifai, Yoshua Bengio, Yann Dauphin, and Pascal Vincent. A generative process for sampling  contractive auto-encoders. In ICML’12, 2012.  R. Salakhutdinov and G.E. Hinton. Deep Boltzmann machines. In Proceedings of the Twelfth Interna-  tional Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2009), volume 8, 2009.  Kevin Swersky, Marc’Aurelio Ranzato, David Buchman, Benjamin Marlin, and Nando de Freitas. On  autoencoders and score matching for energy based models. In ICML’2011. ACM, 2011.  Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation,  23(7):1661–1674, 2011.  19  Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and com-  posing robust features with denoising autoencoders. In ICML’08, pages 1096–1103. ACM, 2008.  6. Appendix  6.1 Optimal DAE solution  Theorem 1 Let p be the probability density function of the data. If we train a DAE using the expected quadratic loss and corruption noise N (x) = x + (cid:15) with  (cid:15) ∼ N(cid:0)0, σ2I(cid:1) ,  then the optimal reconstruction function r∗(x) will be given by E(cid:15) [p(x − (cid:15))(x − (cid:15))]  r∗(x) =  E(cid:15) [p(x − (cid:15))]  (14)  for values of x where p(x) (cid:54)= 0. cally as σ → 0, we get that  Moreover, if we consider how the optimal reconstruction function r∗  σ(x) behaves asymptoti-  r∗ σ(x) = x + σ2 ∂ log p(x)  ∂x  + o(σ2)  as σ → 0.  Proof The ﬁrst part of this proof is to get to equation (14) without assuming that σ → 0.  By using an auxiliary variable ˜x = x + (cid:15), we can rewrite this loss in a way that puts the quantity r(˜x) in focus and allows us to perform the minimization with respect to each choice of r(˜x) independantly. That is, we have that  (cid:90)  Rd  (cid:104)  LDAE =  E(cid:15)∼N (0,σ2I)  p(˜x − (cid:15))(cid:107)r(˜x) − ˜x + (cid:15)(cid:107)2  2  d˜x  (cid:105)  which can be diﬀerentiated with respect to the quantity r(˜x) and set to be equal to 0. Denoting the optimum by r∗(˜x), we get  0 = E(cid:15)∼N (0,σ2I) [p(˜x − (cid:15)) (r∗(˜x) − ˜x + (cid:15))]  E(cid:15)∼N (0,σ2I) [p(˜x − (cid:15))r∗(˜x)] = E(cid:15)∼N (0,σ2I) [p(˜x − (cid:15))(˜x − (cid:15))]  r∗(˜x) =  E(cid:15)∼N (0,σ2I) [p(˜x − (cid:15))(˜x − (cid:15))]  E(cid:15)∼N (0,σ2I) [p(˜x − (cid:15))]  .  (15)  We used ˜x out of convenience in equation (15). Theorem 1 is just nicer to read when using r(x), and we switch back to using r(x) for the rest of this proof.  Now, for the second part of this proof, we study the behavior of the solution r∗  leading term x. Given the symmetry of the distribution of (cid:15) ∼ N(cid:0)0, σ2I(cid:1), we can also simplify  σ(x) as σ approaches 0. We start with equation (15) that we rewrite in a way to be able to pull out the equation (15) by converting all the −(cid:15) into +(cid:15).  20  E(cid:15) [p(x + (cid:15))(x + (cid:15))]  E(cid:15) [p(x + (cid:15))]  =  E(cid:15) [p(x + (cid:15))x] E(cid:15) [p(x + (cid:15))]  + E(cid:15) [p(x + (cid:15))(cid:15)] E(cid:15) [p(x + (cid:15))]  = x +  E(cid:15) [p(x + (cid:15))(cid:15)] E(cid:15) [p(x + (cid:15))]  We now look at the Taylor expansion of p(x + (cid:15)) around x. We perform this expansion  inside of the expectation, where (cid:15) just represents as small quantity of scale σ.  p(x + (cid:15)) = p(x) + ∇p(x)T (cid:15) +  1 2  (cid:15)T ∂2p(x)  ∂x2 (cid:15) + o((cid:15)2)  (cid:15) → 0  as  (16)  When taking the expectation of p(x + (cid:15)) with respect to (cid:15), we get a zero for all the terms containing an odd power of (cid:15), for reasons of symmetry. When we take the expectation of p(x + (cid:15))(cid:15) instead, all the terms with an even power of (cid:15) in the above expectation will vanish. Thus, we get that  E(cid:15) [p(x + (cid:15))(cid:15)] = E(cid:15)[(cid:15)(cid:15)T ]∇p(x) + E(cid:15)[o((cid:15)2)] = σ2∇p(x) + o(σ2) E(cid:15) [p(x + (cid:15))] = p(x) + O(σ2)  Provided that p(x) (cid:54)= 0, our quotient can be rewritten as  σ2∇p(x) + o(σ2) p(x) + O(σ2)  =  σ2∇ log p(x) + o(σ2)  1 + O(σ2)  .  (17)  We can use the basic geometric series expansion to write  1  1 + O(σ2)  = 1 − O(σ2) = 1 + O(σ2)  so that equation (17) is now  σ2∇ log p(x) + o(σ2)  1 + O(σ2)  = σ2∇ log p(x) + o(σ2).  This is the result that we wanted.  Note that p(x) was treated as a constant when we studied the asymptotic behavior as σ → 0. In the Taylor expansion around some given x, we want to stuﬀ all the higher-order derivatives of p(x) into the asymptotic remainder term. It is quite possible that the size of the σ required to do so would depend on the particular x, that there would not be a uniform σ > 0 suitable for all the x. Therefore we can only say that we are dealing with pointwise convergence (not uniform convergence) in our formula for the asymptotic expansion of r∗ σ(x). For practical applications, we do not need more than that. The assumption that p(x) (cid:54)= 0 could also be viewed as problematic, but if we go back to the deﬁnition of the DAE loss, then any point where p(x) = 0 would never produce a training example and would not even contribute in the deﬁnition of the expectation of DAE loss. The correct value to assign to r at such a point is not well-deﬁned.  21  6.2 Relationship between Contractive Penalty and Denoising Criterion  the expected quadratic loss and corruption noise N (x) = x + (cid:15), with (cid:15) ∼ N(cid:0)0, σ2I(cid:1). If we  Proposition 1 Let p be the probability density function of the data. Consider a DAE using  assume that the non-parametric solutions rσ(x) satistﬁes  then we can rewrite the loss as  rσ(x) = x + o(1)  (cid:34)  as σ → 0,  (cid:35)  (cid:13)(cid:13)(cid:13)(cid:13) ∂rσ(x)  ∂x  (cid:13)(cid:13)(cid:13)(cid:13)2  F  LDAE (rσ) = E  (cid:107)rσ(x) − x(cid:107)2  2 + σ2  + o(σ2)  as σ → 0  where the expectation is taken with respect to X, whose distribution is given by p.  Proof We can drop the σ index from rσ to lighten the notation if we just keep in mind that we are considering the particular family of solutions such that r(x) − x = o(1). With a Taylor expansion around x we have that  r(x + (cid:15)) = r(x) +  ∂r(x)  ∂x  (cid:15) + O((cid:15)2).  The DAE loss involves taking the expectation with respect to X and with respect to the noise (cid:15). We substitute the Taylor expansion into LDAE, we express the norm as a dot product, and we show how the expectation with respect to (cid:15) cancels out certain terms.  ∂x  E(cid:15)  ∂r(x)  (cid:18)  r(x) +  (cid:15) + O((cid:15)2)  (cid:34)(cid:13)(cid:13)(cid:13)(cid:13)x − (cid:34) (cid:2)(cid:107)x − r(x)(cid:107)2 (cid:34)(cid:2)(cid:107)r(x) − x(cid:107)2 (cid:2)(cid:107)r(x) − x(cid:107)2  (cid:35) (cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2 (cid:20) ∂r(x) (cid:3) − 2 (x − r(x))T E(cid:15) (cid:32) (cid:3) + o(1)O(σ2) + σ2Tr (cid:34)(cid:13)(cid:13)(cid:13)(cid:13) ∂r(x) (cid:13)(cid:13)(cid:13)(cid:13)2 (cid:3) + o(σ2) + σ2EX  ∂r(x)  (cid:35)  ∂x  ∂x  ∂x  2  2  2  2  EXE(cid:15)  = EX  = EX  = EX  (cid:21) (cid:33)(cid:35)  (cid:34)  + E(cid:15)  (cid:15)T ∂r(x) ∂x  T ∂r(x)  ∂x  (cid:35)(cid:35)  (cid:15) + O((cid:15)3)  + O(σ3)  (cid:15) + O((cid:15)2)  T ∂r(x)  ∂x  that the noise is Gaussian and is independant from X. This explains why E(cid:2)(cid:15)(cid:15)T(cid:3) = σ2I and  The fact that we get a trace (rewritten as Frobenius norm) comes from the assumption  F  E[(cid:15)] = 0. It also motivates many of our substitutions such as E(cid:15)  (cid:2)O((cid:15)k)(cid:3) = O(σk).  6.3 Calculus of Variations  Theorem 2 Let p be a probability density function that is continuously diﬀerentiable once and with support Rd (i.e. ∀x ∈ Rd we have p(x) (cid:54)= 0). Let Lσ be the loss function deﬁned by  (cid:90)  (cid:34)  Lσ(r) =  p(x)  Rd  (cid:35)  (cid:13)(cid:13)(cid:13)(cid:13) ∂r(x)  ∂x  (cid:13)(cid:13)(cid:13)(cid:13)2  F  dx  (cid:107)r(x) − x(cid:107)2  2 + σ2  22  for r : Rd → Rd assumed to be diﬀerentiable twice, and 0 ≤ σ ∈ R used as factor to the penalty term.  σ(x) denote the optimal function that minimizes Lσ. Then we have that  Let r∗  r∗ σ(x) = x + σ2 ∂ log p(x)  ∂x  + o(σ2)  as σ → 0.  Moreover, we also have the following expression for the derivative  ∂r∗ σ(x) ∂x  = I + σ2 ∂2 log p(x)  + o(σ2)  as σ → 0.  ∂x2  Both these asymptotic expansions are to be understood in a context where we consider {r∗ σ(x)}σ≥0 to be a family of optimal functions minimizing Lσ for their corresponding value of σ. The asymptotic expansions are applicable point-wise in x, that is, with any ﬁxed x we look at the behavior as σ → 0.  Proof  This proof is done in two parts. In the ﬁrst part, the objective is to get to equation (20)  that has to be satisﬁed for the optimum solution.  We will leave out the σ indices from the expressions involving r(x) to make the notation lighter. We have a more important need for indices k in rk(x) that denote the d components of r(x) ∈ Rd.  We treat σ as given and constant for the ﬁrst part of this proof. In the second part we work out the asymptotic expansion in terms of σ. We again work  with the implicit dependence of r(x) on σ.  (part 1 of the proof ) We make use of the Euler-Lagrange equation from the Calculus of Variations. We would  refer the reader to either (Dacorogna, 2004) or Wikipedia for more on the topic. Let  (cid:34)  f (x1, . . . , xn, r, rx1, . . . , rxn) = p(x)  (cid:107)r(x) − x(cid:107)2  2 + σ2  where x = (x1, . . . , xd) , r(x) = (r1(x), . . . , rd(x)) and rxi = ∂f ∂xi  We can rewrite the loss L(r) more explicitly as  .   d(cid:88) (ri(x) − xi)2 + σ2  (ri(x) − xi)2 + σ2  i=1  p(x)  p(x)  (cid:90) d(cid:88)  Rd  (cid:90)  Rd  i=1  ∂ri(x)  ∂xj  d(cid:88)  j=1  d(cid:88) d(cid:88)  i=1  j=1  ∂ri(x)  ∂xj  L(r) =  =  (18)  (cid:35)  F  ∂x  (cid:13)(cid:13)(cid:13)(cid:13) ∂r(x) (cid:13)(cid:13)(cid:13)(cid:13)2  dx  dx  2  2  to observe that the components r1(x), . . . , rd(x) can each be optimized separately. The Euler-Lagrange equation to be satisﬁed at the optimal r : Rd → Rd is  ∂f ∂r  =  ∂ ∂xi  ∂f ∂rxi  .  d(cid:88)  i=1  23  (cid:104) ∂r1  ∂xi  ∂xi  ∂x2 i  (cid:104) ∂r1 (cid:104) ∂2r1  ∂p(x) (cid:18) ∂p(x)  ∂p(x) ∂xi  ∂xi  d(cid:88)  i=1  d(cid:88)  In our situation, the expressions from that equation are given by  = 2(r(x) − x)p(x)  ∂f ∂r  = 2σ2p(x)  ∂r2 ∂xi  ···  ∂rd ∂xi  (cid:105)T  ∂f ∂rxi  (cid:18) ∂f  (cid:19)  ∂rxi  ∂ ∂xi  = 2σ2 ∂p(x) ∂xi  +2σ2p(x)  (cid:105)T  (cid:105)T  ···  ∂r2 ∂xi  ∂rd ∂xi  ···  ∂2r2 ∂x2 i  ∂2rd ∂x2 i  and the equality to be satisﬁed at the optimum becomes  (r(x) − x)p(x) = σ2   .  (19)  ∂r1 ∂xi  ∂rd ∂xi  + p(x) ∂2r1 ∂x2 i ... + p(x) ∂2rd ∂x2 i  (cid:19)  .  As equation (18) hinted, the expression (19) can be decomposed into the diﬀerent compo-  nents rk(x) : Rd → R that make r. For k = 1, . . . , d we get  (rk(x) − xk)p(x) = σ2  ∂rk(x)  + p(x)  ∂2rk(x)  ∂x2 i  ∂xi  ∂xi  i=1  As p(x) (cid:54)= 0 by hypothesis, we can divide all the terms by p(x) and note that ∂p(x)  ∂xi  ∂ log p(x)  ∂xi  .  We get  rk(x) − xk = σ2  (cid:18) ∂ log p(x)  ∂xi  d(cid:88)  i=1  (cid:19)  .  ∂rk(x)  ∂xi  +  ∂2rk(x)  ∂x2 i  /p(x) =  (20)  This ﬁrst thing to observe is that when σ2 = 0 the solution is just rk(x) = xk, which translates into r(x) = x. This is not a surprise because it represents the perfect reconstruction value that we get when we the penalty term vanishes in the loss function.  (part 2 of the proof ) This linear partial diﬀerential equation (20) can be used as a recursive relation for rk(x) to  obtain a Taylor series in σ2. The goal is to obtain an expression of the form  r(x) = x + σ2h(x) + o(σ2)  as σ2 → 0  (21)  where we can solve for h(x) and for which we also have that  ∂r(x)  ∂x  = I + σ2 ∂h(x) ∂x  + o(σ2)  as σ2 → 0.  24  We can substitute in the right-hand side of equation (21) the value for rk(x) that we get from equation (21) itself. This substitution would be pointless in any other situation where we are not trying to get a power series in terms of σ2 around 0.  ∂ log p(x)  ∂rk(x)  ∂xj  ∂xj  +  ∂2rk(x)  ∂x2 j  (22)  (cid:33)(23)  (24)  (25)  (26)  (27)  (cid:33)(cid:33)  (cid:19) (cid:32)  ∂rk(x)  ∂xi  ∂2rk(x)  ∂x2 i  +  xk + σ2  d(cid:88)  j=1  ∂ ∂xi  rk(x) = xk +σ2  = xk +σ2  +σ2  = xk +σ2  (cid:18) ∂ log p(x)  ∂ log p(x)  ∂xi  ∂xi  i=1  i=1  d(cid:88) d(cid:88) d(cid:88) d(cid:88) d(cid:88)  i=1  i=1  ∂2rk(x)  ∂x2 i  (cid:32)  ∂xi  d(cid:88)  ∂ log p(x)  I (i = k) + σ2  d(cid:88) (cid:32)  i=1  ∂2rk(x)  ∂x2 i  + σ4ρ(σ2, x)  ∂2rk(x)  (cid:32) d(cid:88) (cid:32) d(cid:88)  i=1  +(σ2)2  i=1  j=1  ∂ log p(x)  ∂ log p(x)  ∂ ∂xi  ∂rk(x)  ∂xj  +  ∂2rk(x)  ∂x2 j  ∂xj  ∂xi  d(cid:88)  rk(x) = xk +σ2 ∂ log p(x)  Now we would like to get rid of that σ2(cid:80)d  ∂x2 i  ∂xk  i=1  + σ2  ∂2rk(x)  term by showing that it is a term that involves only powers of σ4 or higher. We get this by showing what we get by diﬀerentiating the expression for rk(x) in line (27) twice with respect to some l.  ∂x2 i  i=1  ∂rk(x)  ∂xl  = I (i = l) + σ2 ∂2 log p(x) ∂xl∂xk  + σ2 ∂ ∂xl  ∂2rk(x)  ∂x2 i  ∂2rk(x)  ∂x2 l  = σ2 ∂3 log p(x) l ∂xk  ∂x2  + σ2 ∂ ∂x2 l  ∂2rk(x)  ∂x2 i  i=1  + σ2ρ(σ2, x)  (cid:33)  + σ2ρ(σ2, x)  (cid:33)  Since σ2 is a common factor in all the terms of the expression of ∂2rk(x)  we get what we  ∂x2 l  needed. That is,  This shows that  rk(x) = xk + σ2 ∂ log p(x)  ∂xk  + σ4η(σ2, x).  r(x) = x + σ2 ∂ log p(x)  ∂x  + o(σ2)  as σ2 → 0  and  ∂r(x)  ∂x which completes the proof.  = I + σ2 ∂2 log p(x)  ∂x2  + o(σ2)  as σ2 → 0  25  6.4 Local Mean  In preliminary work (Bengio et al., 2012a), we studied how the optimal reconstruction could possibly estimate so-called local moments. We revisit this question here, with more appealing and precise results.  What previous work on denoising and contractive auto-encoders suggest is that regularized auto-encoders can capture the local structure of the density through the value of the encoding (or reconstruction) function and its derivative. In particular, Rifai et al. (2012); Bengio et al. (2012a) argue that the ﬁrst and second derivatives tell us in which directions it makes sense to randomly move while preserving or increasing the density, which may be used to justify sampling procedures. This motivates us here to study so-called local moments as captured by the auto-encoder, and in particular the local mean, following the deﬁnitions introduced in Bengio et al. (2012a).  6.4.1 Definitions for Local Distributions Let p be a continuous probability density function with support Rd. That is, ∀x ∈ Rd we have that p(x) (cid:54)= 0. We deﬁne below the notion of a local ball Bδ(x0), along with an associated local density, which is the normalized product of p with the indicator for the ball:  Bδ(x0) = {x s.t. (cid:107)x − x0(cid:107)2 < δ} Zδ(x0) =  p(x)dx  pδ(x|x0) =  p(x)I (x ∈ Bδ(x0))  (cid:90)  Bδ(x0)  1  Zδ(x0)  where Zδ(x0) is the normalizing constant required to make pδ(x|x0) a valid pdf for a dis- tribution centered on x0. The support of pδ(x|x0) is the ball of radius δ around x0 denoted by Bδ(x0). We stick to the 2-norm in terms of deﬁning the balls Bδ(x0) used, but everything could be rewritten in terms of another p-norm to have slightly diﬀerent formulas. (i.e. local mean and local covariance) of the random variable described by pδ(x|x0).  We use the following notation for what will be referred to as the ﬁrst two local moments  (cid:90)  Rd  mδ(x0)  def =  xpδ(x|x0)dx  (cid:90)  Rd  Cδ(x0)  def =  (x − mδ(x0))(x − mδ(x0))T pδ(x|x0)dx  Based on these deﬁnitions, one can prove the following theorem.  Theorem 3 Let p be of class C3 and represent a probability density function. Let x0 ∈ Rd with p(x0) (cid:54)= 0. Then we have that  mδ(x0) = x0 + δ2  1  ∂ log p(x)  d + 2  ∂x  (cid:12)(cid:12)(cid:12)(cid:12)x0  + o(cid:0)δ3(cid:1) .  This links the local mean of a density with the score associated with that density. Combining this theorem with Theorem 2, we obtain that the optimal reconstruction function r∗(·) also estimates the local mean:  mδ(x) − x =  δ2  σ2(d + 2)  (r∗(x) − x) + A(δ) + δ2B(σ2)  (28)  26  for error terms A(δ), B(σ2) such that  A(δ) ∈ o(δ3) δ → 0, B(σ2) ∈ o(1) as σ2 → 0.  as  This means that we can loosely estimate the direction to the local mean by the direction of  the reconstruction:  mδ(x) − x ∝ r∗(x) − x.  (29)  (cid:20) (cid:20) 1 (cid:12)(cid:12)(cid:12)(cid:12)x0  (cid:90)  6.5 Asymptotic formulas for localised moments Proposition 4 Let p be of class C2 and let x0 ∈ Rd. Then we have that  Zδ(x0) = δd  πd/2  Γ (1 + d/2)  p(x0) + δ2 Tr(H(x0)) 2(d + 2)  + o(δ3)  where H(x0) = ∂2p(x) ∂x2  . Moreover, we have that  1  Zδ(x0)  = δ−d Γ (1 + d/2)  πd/2  p(x0)  − δ2  1  p(x0)2  Tr(H(x0)) 2(d + 2)  (cid:12)(cid:12)(cid:12)x=x0  (cid:21)  (cid:21)  + o(δ3)  .  Proof  Zδ(x0) =  (cid:90)  (cid:34)  Bδ(x0)  p(x0) +  ∂p(x)  ∂x  (x − x0) +  (x − x0)T H(x0)(x − x0)  1 2!  (cid:21)  dx  (x − x0)T H(x0)(x − x0)dx + 0 + o(δd+3)  D(3)p(x0)(x − x0) + o(δ3)  +  1 3!  (cid:90)  = p(x0)  dx + 0 +  1 2  Bδ(x0)  Γ (1 + d/2)  πd/2  (cid:20)  Bδ(x0)  πd/2  4Γ (2 + d/2)  (cid:21)  = δd  πd/2  Γ (1 + d/2)  p(x0) + δ2 Tr(H(x0)) 2(d + 2)  + o(δ3)  = p(x0)δd  + δd+2  Tr (H(x0)) + o(δd+3)  We use Proposition 10 to get that trace come up from the integral involving H(x0). The  expression for 1/Zδ(x0) comes from the fact that, for any a, b > 0 we have that  1  a + bδ2 + o(δ3)  =  =  a−1 a δ2 + o(δ3) a2 δ2 + o(δ3)  1 + b − b 1 a  =  1 − (  1 b a a as δ → 0.  (cid:18)  (cid:19)  δ2 + o(δ3)) + o(δ4)  by using the classic result from geometric series where Now we just apply this to  1  1+r = 1 − r + r2 − . . . for |r| < 1.  1  Zδ(x0)  = δ−d Γ (1 + d/2)  πd/2  p(x0) + δ2 Tr(H(x0))  1 2(d+2) + o(δ3)  (cid:105)  (cid:104)  27  and get the expected result.  + o(cid:0)δ3(cid:1) .  (cid:12)(cid:12)(cid:12)(cid:12)x0 (cid:90)  Theorem 5 Let p be of class C3 and represent a probability density function. Let x0 ∈ Rd with p(x0) (cid:54)= 0. Then we have that  mδ(x0) = x0 + δ2  1  ∂ log p(x)  d + 2  ∂x  (cid:90)  Proof integral into a x − x0 to make the integral easier to integrate.  The leading term in the expression for mδ(x0) is obtained by transforming the x in the  mδ(x0) =  1  Zδ(x0)  Bδ(x0)  xp(x)dx = x0 +  1  zδ(x0)  Bδ(x0)  (x − x0)p(x)dx.  Now using the Taylor expansion around x0  mδ(x0) = x0 +  (cid:90)  1  Bδ(x0)  Zδ(x0) (x − x0)T ∂2p(x) ∂x2  +  1 2  (x − x0)  (cid:12)(cid:12)(cid:12)(cid:12)x0  (cid:34)  p(x0) +  (cid:12)(cid:12)(cid:12)(cid:12)x0  ∂p(x)  ∂x  (x − x0)  (cid:35)  (x − x0) + o((cid:107)x − x0(cid:107)2)  dx.  Remember that(cid:82)  Bδ(x0) f (x)dx = 0 whenever we have a function f is anti-symmetrical (or “odd”) relative to the point x0 (i.e. f (x − x0) = f (−x − x0)). This applies to the terms (x − x0)T . Hence we use Proposition 9 to get (x − x0)p(x0) and (x − x0)(x − x0) ∂2p(x)  mδ(x0) = x0 +  = x0 +  1  Zδ(x0)  1  Zδ(x0)  (x − x0)T ∂p(x) ∂x  (x − x0) + o((cid:107)x − x0(cid:107)3)  dx  ∂p(x)  ∂x  + o(δ3).  Now, looking at the coeﬃcient in front of ∂p(x) ∂x  in the ﬁrst term, we can use Proposition  ∂x2  (cid:12)(cid:12)(cid:12)x=x0  (cid:34) 2Γ(cid:0)2 + d  π  d 2  2  (cid:90) (cid:32)  Bδ(x0)  δd+2  (cid:1)(cid:33) (cid:12)(cid:12)(cid:12)x0  (cid:35)  1  Zδ(x0)  = δ−d Γ (1 + d/2)  πd/2  4 to rewrite it as  d 2  π  δd+2  (cid:1)(cid:33) (cid:32) 2Γ(cid:0)2 + d = δ2 Γ(cid:0)1 + d (cid:1) (cid:1)(cid:20) 1 2Γ(cid:0)2 + d  2  2  2  p(x0)  (cid:20) 1  p(x0)  − δ2  (cid:21)  δd+2  Tr(H(x0)) 2(d + 2)  + o(δ3)  2Γ(cid:0)2 + d  π  d 2  2  (cid:1)  − δ2  1  p(x0)2  Tr(H(x0)) 2(d + 2)  28  + o(δ3)  = δ2  1  1  p(x0)  d + 2  + o(δ3).  (cid:12)(cid:12)(cid:12)(cid:12)x0 (cid:12)(cid:12)(cid:12)(cid:12)x0  1  p(x0)2  (cid:21)  There is no reason the keep the −δ4 Γ(1+ d 2 ) 2Γ(2+ d 2 )  1  p(x0)2  Tr(H(x0))  2(d+2)  in the above expression because  the asymptotic error from the remainder term in the main expression is o(δ3). That would swallow our exact expression for δ4 and make it useless.  We end up with  mδ(x0) = x0 + δ2  1  ∂ log p(x)  d + 2  ∂x  6.6 Integration on balls and spheres  (cid:12)(cid:12)(cid:12)(cid:12)x0  + o(δ3).  This result comes from Multi-dimensional Integration : Scary Calculus Problems from Tim Reluga (who got the results from How to integrate a polynomial over a sphere by Gerald B. Folland).  (cid:111)  x ∈ Rd(cid:12)(cid:12)(cid:12)(cid:80)d (cid:110) (cid:90) d(cid:89)  j=1 x2  j ≤ 1  |xj|aj dx =  be the ball of radius 1 around the origin. Then  (cid:81) Γ Γ(cid:0)1 + d  (cid:16) aj +1 (cid:17) (cid:80) aj  2  2 + 1  2  (cid:1)  Theorem 6 Let B =  for any real numbers aj ≥ 0.  B  j=1  Corollary 7 Let B be the ball of radius 1 around the origin. Then  (cid:90)  d(cid:89)  B  j=1  xaj j dx =    (cid:81) Γ  (cid:17) (cid:16) aj +1 (cid:80) aj)  2  2 + 1  2  Γ(1+ d 0  if all the aj are even integers  otherwise  for any non-negative integers aj ≥ 0. Note the absence of the absolute values put on the xaj  terms. Corollary 8 Let Bδ(0) ⊂ Rd be the ball of radius δ around the origin. Then  (cid:90)  d(cid:89)  xaj j dx =  Bδ(0)  j=1  δd+(cid:80) aj  0  (cid:81) Γ  (cid:16) aj +1 (cid:17) (cid:80) aj)  2  Γ(1+ d  2 + 1  2  if all the ajare even integers  otherwise  for any non-negative integers aj ≥ 0. Note the absence of the absolute values on the xaj  terms.  j  j  Proof  In situations where at least one aj is odd, we have that the function f (x) =(cid:81)d  We take the theorem as given and concentrate here on justifying the two corollaries. Note how in Corollary 7 we dropped the absolute values that were in the original Theorem 6. j becomes odd in the sense that f (−x) = −f (x). Because of the symmetrical nature of the integration on the unit ball, we get that the integral is 0 as a result of cancellations.  j=1 xaj  29  For Corollary 8, we can rewrite the integral by changing the domain with yj = xj/δ so that  (cid:90)  δ−(cid:80) aj  d(cid:89)  (cid:90)  d(cid:89)  (cid:90)  d(cid:89)  xaj j dx =  (xj/δ)aj dx =  yaj δddy.  Bδ(0)  j=1  Bδ(0)  j=1  B1(0)  j=1  We pull out the δd that we got from the determinant of the Jacobian when changing from  dx to dy and Corollary 8 follows.  (cid:90)  (cid:90)  Proposition 9 Let v ∈ Rd and let Bδ(0) ⊂ Rd be the ball of radius δ around the origin. Then  Bδ(0)  where < v, y > is the usual dot product.  y < v, y > dy =  δd+2  (cid:32)  (cid:1)(cid:33)  v  2Γ(cid:0)2 + d  π  d 2  2  Proof  We have that   v1y2  (cid:1) = 1 2 Γ(cid:0) 1 expected result with the constant obtained from Γ(cid:0) 3  y < v, y > =  ... vdy2 d  1  2  2  (cid:1) = 1  2  √  π.  which is decomposable into d component-wise applications of Corollary 8. This yields the  (cid:90)  (cid:90)  Proposition 10 Let H ∈ Rd×d and let Bδ(x0) ⊂ Rd be the ball of radius δ around x0 ∈ Rd. Then  (x − x0)T H(x − x0)dx = δd+2  πd/2  2Γ (2 + d/2)  trace (H) .  Bδ(x0)  Proof  First, by substituting y = (x − x0) /δ we have that this is equivalent to showing that  This integral yields a real number which can be written as  yT Hydy =  πd/2  2Γ (2 + d/2)  B1(0)  trace (H) .  (cid:90)  (cid:88)  (cid:90)  (cid:88)  yT Hydy =  yiHi,jyjdy =  yiyjHi,jdy.  B1(0)  B1(0)  i,j  i,j  B1(0)  Now we know from Corollary 8 that this integral is zero when i (cid:54)= j. This gives  30  (cid:90)  Hi,j  B1(0)  (cid:88)  i,j  yiyjdy =  (cid:90)  Hi,i  B1(0)  (cid:88)  i  y2 i dy = trace (H)  πd/2  2Γ (2 + d/2)  .  31  ","What do auto-encoders learn about the underlying data generatingdistribution? Recent work suggests that some auto-encoder variants do a goodjob of capturing the local manifold structure of data. This paper clarifiessome of these previous observations by showing that minimizing a particularform of regularized reconstruction error yields a reconstruction function thatlocally characterizes the shape of the data generating density. We show thatthe auto-encoder captures the score (derivative of the log-density with respectto the input). It contradicts previous interpretations of reconstruction erroras an energy function. Unlike previous results, the theorems provided here arecompletely generic and do not depend on the parametrization of theauto-encoder: they show what the auto-encoder would tend to if given enoughcapacity and examples. These results are for a contractive training criterionwe show to be similar to the denoising auto-encoder training criterion withsmall corruption noise, but with contraction applied on the wholereconstruction function rather than just encoder. Similarly to score matching,one can consider the proposed training criterion as a convenient alternative tomaximum likelihood because it does not involve a partition function. Finally,we show how an approximate Metropolis-Hastings MCMC can be setup to recoversamples from the estimated distribution, and this is confirmed in samplingexperiments."
1301.3560,2013,Complexity of Representation and Inference in Compositional Models with Part Sharing  ,"['Alan Yuille', 'Roozbeh Mottaghi']",https://arxiv.org/pdf/1301.3560.pdf,"3 1 0 2     n a J    6 1      ]  V C . s c [      1 v 0 6 5 3  .  1 0 3 1 : v i X r a  Complexity of Representation and Inference in  Compositional Models with Part Sharing  Alan L. Yuille  Depts. of Statistics, Computer Science & Psychology  University of California, Los Angeles  yuille@stat.ucla.edu  Roozbeh Mottaghi  Department of Computer Science  University of California, Los Angeles  roozbehm@cs.ucla.edu  Abstract  This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the com- plexity of this model in terms of computation time (for serial computers) and num- bers of nodes (e.g., ”neurons”) for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling be- havior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.  1  Introduction  A fundamental problem of vision is how to deal with the enormous complexity of images and visual scenes 1. The total number of possible images is almost inﬁnitely large [8]. The number of objects is also huge and has been estimated at around 30,000 [2]. How can a biological, or artiﬁcial, vision system deal with this complexity? For example, considering the enormous input space of images and output space of objects, how can humans interpret images in less than 150 msec [16]? There are three main issues involved. Firstly, how can a visual system be designed so that it can efﬁciently represent large classes of objects, including their parts and subparts? Secondly, how can the visual system be designed so that it can rapidly infer which object, or objects, are present in an input image and the positions of their subparts? And, thirdly, how can this representation be learnt in an unsupervised, or weakly supervised fashion? In short, what visual architectures enable us to address these three issues? Many considerations suggest that visual architectures should be hierarchical. The structure of mam- malian visual systems is hierarchical with the lower levels (e.g., in areas V1 and V2) tuned to small in area IT) are tuned to objects 2. Moreover, as ap- image features while the higher levels (i.e. preciated by pioneers such as Fukushima [5], hierarchical architectures lend themselves naturally to  1Similar complexity issues will arise for other perceptual and cognitive modalities. 2But just because mammalian visual systems are hierarchical does not necessarily imply that this is the best  design for computer vision systems.  efﬁcient representations of objects in terms of parts and subparts which can be shared between many objects. Hierarchical architectures also lead to efﬁcient learning algorithms as illustrated by deep belief learning and others [7]. There are many varieties of hierarchical models which differ in details of their representations and their learning and inference algorithms [14, 7, 15, 1, 13, 18, 10, 3, 9]. But, to the best of our knowledge, there has been no detailed study of their complexity properties. This paper provides a mathematical analysis of compositional models [6], which are a subclass of the hierarchical models. The key idea of compositionality is to explicitly represent objects by recursive composition from parts and subparts. This gives rise to natural learning and inference algorithms which proceed from sub-parts to parts to objects (e.g., inference is efﬁcient because a leg detector can be used for detecting the legs of cows, horses, and yaks). The explicitness of the object rep- resentations helps quantify the efﬁciency of part-sharing and make mathematical analysis possible. The compositional models we study are based on the work of L. Zhu and his collaborators [19, 20] but we make several technical modiﬁcations including a parallel re-formulation of the models. We note that in previous papers [19, 20] the representations of the compositional models were learnt in an unsupervised manner, which relates to the memorization algorithms of Valiant [17]. This paper does not address learning but instead explores the consequence of the representations which were learnt. Our analysis assumes that objects are represented by hierarchical graphical probability models 3 which are composed from more elementary models by part-subpart compositions. An object – a graphical model with H levels – is deﬁned as a composition of r parts which are graphical models with H − 1 levels. These parts are deﬁned recursively in terms of subparts which are represented by graphical models of increasingly lower levels. It is convenient to specify these compositional models in terms of a set of dictionaries {Mh : h = 1, .., ,H} where the level-h parts in dictionary Mh are composed in terms of level-h − 1 parts in dictionary Mh−1. The highest level dictionaries MH represent the set of all objects. The lowest level dictionaries M1 represent the elementary features that can be measured from the input image. Part-subpart composition enables us to construct a very large number of objects by different compositions of elements from the lowest-level dictionary. It enables us to perform part-sharing during learning and inference, which can lead to enormous reductions in complexity, as our mathematical analysis will show. There are three factors which enable computational efﬁciency. The ﬁrst is part-sharing, as described above, which means that we only need to perform inference on the dictionary elements. The second is the executive-summary principle. This principle allows us to represent the state of a part coarsely because we are also representing the state of its subparts (e.g., an executive will only want to know that ”there is a horse in the ﬁeld” and will not care about the precise positions of its legs). For example, consider a letter T which is composed of a horizontal and vertical bar. If the positions of these two bars are speciﬁed precisely, then we can specify the position of the letter T more crudely (sufﬁcient for it to be ”bound” to the two bars). This relates to Lee and Mumford’s high- resolution buffer hypothesis [11] and possibly to the experimental ﬁnding that neurons higher up the visual pathway are tuned to increasingly complex image features but are decreasingly sensitive to spatial position. The third factor is parallelism which arises because the part dictionaries can be implemented in parallel, essentially having a set of receptive ﬁelds, for each dictionary element. This enables extremely rapid inference at the cost of a larger, but parallel, graphical model. The compositional section (2) introduces the key ideas. Section (3) describes the inference algo- rithms for serial and parallel implementations. Section (4) performs a complexity analysis and shows potential exponential gains by using compositional models.  2 The Compositional Models  Compositional models are based on the idea that objects are built by compositions of parts which, in turn, are compositions of more elementary parts. These are built by part-subpart compositions.  3These graphical models contain closed loops but with restricted maximal clique size.  2  (a)  (b)  Figure 1: (a) Compositional part-subpart models for T and L are constructed from the same ele- mentary components τ1, τ2, horizontal and vertical bar using different spatial relations λ = (µ, σ), which impose locality. The state x of the parent node gives the summary position of the object, the executive summary, while the positions x1, x2 of the components give details about its components. (b) The hierarchical lattices. The size of the lattices decrease with scale by a factor q which helps en- force executive summary and prevent having multiple hypotheses which overlap too much. q = 1/4 in this ﬁgure.  2.1 Compositional Part-Subparts  We formulate part-subpart compositions by probabilistic graphical model which speciﬁes how a part is composed of its subparts. A parent node ν of the graph represents the part by its type τν and a state variable xν (e.g., xν could indicate the position of the part). The r child nodes Ch(ν) = (ν1, ..., νr) represent the parts by their types τν1, ..., τνr and state variables (cid:126)xCh(ν) = (xν1, ..., xνr ) 4. The type of the parent node is speciﬁed by τν = (τν1, ..., τνr , λν). Here (τν1 , ..., τνr ) are the types of the child nodes, and λν speciﬁes a distribution over the states of the subparts (e.g.,over their relative spatial positions). Hence the type τν of the parent speciﬁes the part-subpart compositional model. The probability distribution for the part-subpart model relates the states of the part and the subparts by:  P ((cid:126)xCh(ν)|xν; τν) = δ(xν − f ((cid:126)xCh(ν)))h((cid:126)xCh(ν); λν).  (1)  Here f (.) is a deterministic function, so the state of the parent node is determined uniquely by the state of the child nodes. The function h(.) speciﬁes a distribution on the relative states of the child nodes. The distribution P ((cid:126)xCh(ν)|xν; τν) obeys a locality principle, which means that P ((cid:126)xCh(ν)|xν; τν) = 0, unless |xνi − xν| is smaller than a threshold for all i = 1, .., r. This require- ment captures the intuition that subparts of a part are typically close together. The state variable xν of the parent node provides an executive summary description of the part. Hence they are restricted to take a smaller set of values than the state variables xνi of the subparts. Intuitively, the state xν of the parent offers summary information (e.g., there is a cow in the right side of a ﬁeld) while the child states (cid:126)xCh(ν) offer more detailed information (e.g., the position of the parts of the cow). In general, information about the object is represented in a distributed manner with coarse information at the upper levels of the hierarchy and more precise information at lower levels. We give examples of part-subpart compositions in ﬁgure (1(a)). The compositions represent the letters T and L, which are the types of the parent nodes. The types of the child nodes are horizontal and vertical bars, indicated by τ1 = H, τ2 = V . The child state variables x1, x2 indicate the image positions of the horizontal and vertical bars. The state variable x of the parent node gives a summary description of the position of the letters T and L. The compositional models for letters T and L differ by their λ parameter which species the relative positions of the horizontal and vertical bars. In this example, we choose h(.; λ) to a Gaussian distribution, so λ = (µ, σ) where µ is the mean relative positions between the bars and σ is the covariance. We set f (x1, x2) = (1/2)(x1 + x2), so the state of the parent node speciﬁes the average positions of the child nodes (i.e. the positions of the two bars). Hence the two compositional models for the T and L have types τT = (H, V, λT ) and τL = (H, V, λL).  4In this paper, we assume a ﬁxed value r for all part-subpart compositions.  3  Figure 2: Left Panel: Two part-subpart models. Center Panel: Combining two part-subpart models by composition to make a higher level model. Right Panel: Some examples of the shapes that can be generated by different parameters settings λ of the distribution.  2.2 Models of Object Categories  a graph V. This graph has a hierarchical structure with levels h ∈ {0, ...,H}, where V =(cid:83)Hh=0 Vh.  An object category can be modeled by repeated part-subpart compositions. This is illustrated in ﬁgure (2) where we combine T ’s and L’s with other parts to form more complex objects. More generally, we can combine part-subpart compositions into bigger structures by treating the parts as subparts of higher order parts. More formally, an object category of type τH is represented by a probability distribution deﬁned over Each object has a single, root node, at level-H (i.e. VH contains a single node). Any node ν ∈ Vh (for h > 0) has r children nodes Ch(ν) in Vh−1 indexed by (ν1, ..., νr). Hence there are rH−h nodes at level-h (i.e. |Vh| = rH−h). At each node ν there is a state variable xν which indicates spatial position and type τν. The type τH of the root node indicates the object category and also speciﬁes the types of its parts. The position variables xν take values in a set of lattices {Dh : h = 0, ...,H}, so that a level-h node, ν ∈ Vh, takes position xν ∈ Dh. The leaf nodes V0 of the graph take values on the image lattice D0. The lattices are evenly spaced and the number of lattice points decreases by a factor of q < 1 for each level, so |Dh| = qh|D0|, see ﬁgure (1(b)). This decrease in number of lattice points imposes the executive summary principle. The lattice spacing is designed so that parts do not overlap. At higher levels of the hierarchy the parts cover larger regions of the image and so the lattice spacing must be larger, and hence the number of lattice points smaller, to prevent overlapping 5. The probability model for an object category of type τH is speciﬁed by products of part-subpart relations:  P ((cid:126)x|τH) =  P ((cid:126)xCh(ν)|xν; τν)U (xH).  (2)  (cid:89)  ν∈V/V0  Here U (xH) is the uniform distribution.  2.3 Multiple Object Categories, Shared Parts, and Hierarchical Dictionaries  Now suppose we have a set of object categories τH ∈ H, each of which can be expressed by an equation such as equation (2). We assume that these objects share parts. To quantify the amount of part sharing we deﬁne a hierarchical dictionary {Mh : h = 0, ...,H}, where Mh is the dictionary of parts at level h. This gives an exhaustive set of the parts of this set of the objects, at all levels h = 0, ...,H. The elements of the dictionary Mh are composed from elements of the dictionary Mh−1 by part-subpart compositions 6. This gives an alternative way to think of object models. The type variable τν of a node at level h (i.e. in Vh) indexes an element of the dictionary Mh. Hence objects can be encoded in terms of the hierarchical dictionary. Moreover, we can create new objects by making new compositions from existing elements of the dictionaries.  5Previous work [20, 19] was not formulated on lattices and used non-maximal suppression to achieve the  same effect.  6The unsupervised learning algorithm in [19] automatically generates this hierarchical dictionary.  4  OR2.4 The Likelihood Function and the Generative Model  To specify a generative model for each object category we proceed as follows. The prior speciﬁes a distribution over the positions and types of the leaf nodes of the object model. Then the likelihood function is speciﬁed in terms of the type at the leaf nodes (e.g., if the leaf node is a vertical bar, then there is a high probability that the image has a vertical edge at that position). More formally, the prior P ((cid:126)x|τH), see equation (2), speciﬁes a distribution over a set of points L = {xν : ν ∈ V0} (the leaf nodes of the graph) and speciﬁes their types {τν : ν ∈ V0}. These points are required to lie on the image lattice (e.g., xν ∈ D0). We denote this as {(x, τ (x)) : x ∈ L} where τ (x) is speciﬁed in the natural manner (i.e. if x = xν then τ (x) = τν). We specify distributions P (I(x)|τ (x)) for the probability of the image I(x) at x conditioned on the type of the leaf node. We specify a default probability P (I(x)|τ0) at positions x where there is no leaf node of the object. This gives a likelihood function for the states (cid:126)x = {xν ∈ V} of the object model in terms of the image I = {I(x) : x ∈ D0}: (3)  (cid:89)  (cid:89)  P (I|(cid:126)x) =  P (I(x)|τ (x)) ×  P (I(x)|τ0).  x∈L  x∈D0/L  The likelihood and the prior, equations (3,2), give a generative model for each object category. We can extend this in the natural manner to give generative models or two, or more, objects in the image provided they do not overlap. Intuitively, this involves multiple sampling from the prior to determine the types of the lattice pixels, followed by sampling from P (I(x)|τ ) at the leaf nodes to determine the image I. Similarly, we have a default background model for the entire image if no object is present:  (cid:89)  (4)  PB(I) =  P (I(x)|τ0).  x∈D0 Inference by Dynamic Programming  3  The inference task is to determine which objects are present in the image and to specify their po- sitions. This involves two subtasks: (i) state estimation, to determine the optimal states of a model and hence the position of the objects and its parts, and (ii) model selection, to determine whether objects are present or not. As we will show, both tasks can be reduced to calculating and comparing log-likelihood ratios which can be performed efﬁciently using dynamic programming methods. We will ﬁrst describe the simplest case which consists of estimating the state variables of a single object model and using model selection to determine whether the object is present in the image and, if so, how many times. Next we show that we can perform inference and model selection for multiple objects efﬁciently by exploiting part sharing (using hierarchical dictionaries). Finally, we show how these inference tasks can be performed even more efﬁciently using a parallel implementation. We stress that we are performing exact inference and no approximations are made. We are simply exploiting part-sharing so that computations required for performing inference for one object can be re-used when performing inference for other objects.  3.1  Inference Tasks: State Detection and Model Selection  We ﬁrst describe a standard dynamic programming algorithm for ﬁnding the optimal state of a single object category model. Then we describe how the same computations can be used to perform model selection and to the detection and state estimation if the object appears multiple times in the image (non-overlapping). Consider performing inference for a single object category model deﬁned by equations (2,3). To calculate the MAP estimate of the state variables requires computing (cid:126)x∗ = arg max(cid:126)x{log P (I|(cid:126)x) + log P ((cid:126)x; τH)}. By subtracting the constant term log PB(I) from the righthand side, we can re- express this as estimating:  (cid:126)x∗ = arg max (cid:126)x {  log  P (I(x)|τ (x)) P (I(x)|τ0)  +  log P ((cid:126)xCh(ν)|xν; τν) + log U (xH)}.  (5)  (cid:88)  x∈L  (cid:88)  ν  5  Figure 3: Left Panel: The feedforward pass propagates hypotheses up to the highest level where the best state is selected. Center Panel: Feedback propagates information from the top node disam- biguating the middle level nodes. Right Panel: Feedback from the middle level nodes propagates back to the input layer to resolve ambiguities there. This algorithm rapidly estimates the top-level executive summary description in a rapid feed-forward pass. The top-down pass is required to allow high-level context to eliminate false hypotheses at the lower levels– ”high-level tells low-level to stop gossiping”.  r(cid:88)  i=1  r(cid:88)  i=1  Here L denotes the positions of the leaf nodes of the graph, which must be determined during inference. We estimate (cid:126)x∗ by performing dynamic programming. This involves a bottom-up pass which recur- sively computes quantities φ(xh, τh) = arg max(cid:126)x/xh{log P (I|(cid:126)x) PB (I) + log P ((cid:126)x; τh)} by the formula:  (cid:126)xCh(ν){ φ(xh, τh) = max  φ(xνi, τνi) + log P ((cid:126)xCh(ν)|x∗ν, τν)}.  (6)  We refer to φ(xh, τh) as the local evidence for part τh with state xh (after maximizing over the states of the lower parts of the graphical model). This local evidence is computed bottom-up. We call this the local evidence because it ignores the context evidence for the part which will be provided during top-down processing (i.e. that evidence for other parts of the object, in consistent positions, will strengthen the evidence for this part). The bottom-up pass outputs the global evidence φ(xH, τH) for object category τH at position xH. We can detect the most probable state of the object by computing x∗ H = arg max φ(xH, τH). Then we can perform the top-down pass of dynamic programming to estimate the most probable states (cid:126)x∗ of the entire model by recursively performing:  (cid:126)x∗Ch(ν) = arg max (cid:126)xCh(ν){  φ(xνi, τνi) + log P ((cid:126)xCh(ν)|x∗ν, τν)}.  (7)  This outputs the most probable state of the object in the image. Note that the bottom-up process ﬁrst estimates the optimal ”executive summary” description of the object (x∗ ) and only later determines H the optimal estimates of the lower-level states of the object in the top-down pass. Hence, the algo- rithm is faster at detecting that there is a cow in the right side of the ﬁeld (estimated in the bottom-up pass) and is slower at determining the position of the feet of the cow (estimated in the top-down pass). This is illustrated in ﬁgure (3). Importantly, we only need to perform slight extensions of this algorithm to compute signiﬁcantly more. First, we can perform model selection – to determine if the object is present in the image – by determining if φ(x∗ H, τH) is the log-likelihood ratio of the probability that the object is present at position x∗ compared to the H probability that the corresponding part of the image is generated by the background image model PB(.). Secondly, we can compute the probability that the object occurs several times in the image, H, τH) > T , to compute the ”executive summary” descriptions by computing the set {xH : φ(x∗ for each object (e.g., the coarse positions of each object). We then perform the top-down pass initialized at each coarse position (i.e. at each point of the set described above) to determine the optimal conﬁguration for the states of the objects. Hence, we can reuse the computations required to detect a single object in order to detect multiple instances of the object (provided there are no  H, τH) > T , where T is a threshold. This is because, by equation (5), φ(x∗  6  Figure 4: Sharing. Left Panel: Two Level-2 models A and B which share Level-1 model b as a subpart. Center Panel: Level-1 model b. Inference computation only requires us to do inference over model b once, and then it can be used to computer the optimal states for models A and B. Right Panel: Note that is we combine models A and B by a root OR node then we obtain a graphical model for both objects. This model has a closed loop which would seem to make inference more challenging. But by exploiting the shape part we can do inference optimally despite the closed loop. Inference can be done on the dictionaries, far right.  overlaps)7. The number of objects in the image is determined by the log-likelihood ratio test with respect to the background model.  Inference on Multiple Objects by Part Sharing using the Hierarchical Dictionaries  3.2 Now suppose we want to detect instances of many object categories τH ∈ MH simultaneously. We can exploit the shared parts by performing inference using the hierarchical dictionaries. The main idea is that we need to compute the global evidence φ(xH, τH) for all objects τH ∈ MH and at all positions xH in the top-level lattice. These quantities could be computed separately for each object by performing the bottom-up pass, speciﬁed by equation (6), for each object. But this is wasteful because the objects share parts and so we would be performing the same computations multiple times. Instead we can perform all the necessary computations more efﬁciently by working directly with the hierarchical dictionaries. More formally, computing the global evidence for all object models and at all positions is speciﬁed as follows.  Let D∗ For xH ∈ D∗ P (I|(cid:126)x) PB(I)  H = {xH ∈ DH s.t. max τH∈MH  φ(xH, τH) > TH}, H, let τ∗ φ(xH, τH), + log P ((cid:126)x; τH∗(xH))} for all xH ∈ D∗ H.  H(xH) = arg max τH∈MH  Detect (cid:126)x∗/xH = arg max  (cid:126)x/xH{log  (8)  All these calculations can be done efﬁciently using the hierarchical dictionaries (except for the max and arg max tasks at level H which must be done separately). Recalling that each dictionary element at level h is composed, by part-subpart composition, of dictionary elements at level h − 1. Hence we can apply the bottom-up update rule in equation (6) directly to the dictionary elements. This is illustrated in ﬁgure (4). As analyzed in the next section, this can yield major gains in computational complexity. Once the global evidences for each object model have been computed at each position (in the top lattice) we can perform winner-take-all to estimate the object model which has largest evidence at each position. Then we can apply thresholding to see if it passes the log-likelihood ratio test compared to the background model. If it does pass this log-likelihood test, then we can use the top-down pass of dynamic programming, see equation (7), to estimate the most probable state of all parts of the object model.  7Note that this is equivalent to performing optimal inference simultaneously over a set of different generative models of the image, where one model assumes that there is one instance of the object in the image, another models assumes there are two, and so on.  7  abAbcBbabAcBOrWe note that we are performing exact inference over multiple object models at the same time. This is perhaps un-intuitive to some readers because this corresponds to doing exact inference over a probability model which can be expressed as a graph with a large number of closed loops, see ﬁgure (4). But the main point is that part-sharing enables us share inference efﬁciently between many models. The only computation which cannot be performed by dynamic programming are the max and arg max tasks at level H, see top line of equation (8). These are simple operations and require order MH × |DH| calculations. This will usually be a small number, compared to the complexity of other computations. But this will become very large if there are a large number of objects, as we will discuss in section (4).  3.3 Parallel Formulation and Inference Algorithm  Finally, we observe that all the computations required for performing inference on multiple objects can be parallelized. This requires computing the quantities in equation (8).  Figure 5: Parallel Hierarchical Implementation. Far Left Panel: Four Level-0 models are spaced densely in the image (here an 8× 2 grid). Left Panel: the four Level-1 models are sampled at a lower rate, and each have 4 × 1 copies. Right Panel: the four Level-2 models are sampled less frequently. Far Right Panel: a bird’s eye view of the parallel hierarchy. The dots represent a ”column” of four Level-0 models. The crosses represent columns containing four Level-1 models. The triangles represent a column of the Level-2 models.  The parallelization is possible, in the bottom-up pass of dynamic programming, calculations are done separately for each position x, see equation (6). So we can compute the local evidence for all parts in the hierarchical dictionary recursively and in parallel for each position, and hence compute the φ(xH, τH) for all xH ∈ DH and τH ∈ MH. The max and arg max operations at level H can also be done in parallel for each position xH ∈ DH. Similarly we can perform the top-down pass of dynamic programming, see equation (7), in parallel to compute the best conﬁgurations of the detected objects in parallel for different possible positions of the objects (on the top-level lattice). The parallel formulation can be visualized by making copies of the elements of the hierarchical dictionary elements (the parts), so that a model at level-h has |Dh| copies, with one copy at each lattice point. Hence at level-h, we have mh ”receptive ﬁelds” at each lattice point in Dh with each one tuned to a different part τh ∈ Mh, see ﬁgure (5). At level-0, these receptive ﬁelds are tuned to speciﬁc image properties (e.g., horizontal or vertical bars). Note that the receptive ﬁelds are highly non-linear (i.e. they do not obey any superposition principle)8. Moreover, they are inﬂuenced both by bottom-up processing (during the bottom-up pass) and by top-down processing (during the top-down pass). The bottom-up processing computes the local evidence while the top-down pass modiﬁes it by the high-level context. The computations required by this parallel implementation are illustrated in ﬁgure (6). The bottom- up pass is performed by a two-layer network where the ﬁrst layer performs an AND operation (to compute the local evidence for a speciﬁc conﬁguration of the child nodes) and the second layer  8Nevertheless they are broadly speaking, tuned to image stimuli which have the mean shape of the cor- responding part τh. In agreement, with ﬁndings about mammalian cortex, the receptive ﬁelds become more sensitive to image structure (e.g., from bars, to more complex shapes) at increasing levels. Moreover, their sensitivity to spatial position decreases because at higher levels the models only encode the executive summary descriptions, on coarser lattices, while the ﬁner details of the object are represented more precisely at the lower levels.  8  Figure 6: Parallel implementation of Dynamic Programming. The left part of the ﬁgure shows the bottom-up pass of dynamic programming. The local evidence for the parent node is obtained by taking the maximum of the scores of the Cr possible states of the child nodes. This can be computed by a two-layer network where the ﬁrst level computes the scores for all Cr child node states, which can be done in parallel, and the second level compute the maximum score. This is like an AND operation followed by an OR. The top-down pass requires the parent node to select which of the Cr child conﬁgurations gave the maximum score, and suppressing the other conﬁgurations.  performs an OR, or max operation, to determine the local evidence (by max-ing over the possible child conﬁgurations)9. The top-down pass only has to perform an arg max computation to determine which child conﬁguration gave the best local evidence.  4 Complexity Analysis  We now analyze the complexity of the inference algorithms for performing the tasks. Firstly, we analyze complexity for a single object (without part-sharing). Secondly, we study the complexity for multiple objects with shared parts. Thirdly, we consider the complexity of the parallel imple- mentation. The complexity is expressed in terms of the following quantities: (I) The size |D0| of the image. (II) The scale decrease factor q (enabling executive summary). (III) The number H of levels of the hierarchy. (IV) The sizes {|Mh| : h = 1, ...,H} of the hierarchical dictionaries. (V) The number r of subparts of each part. (VI) The number Cr of possible part-subpart conﬁgurations.  4.1 Complexity for Single Objects and Ignoring Part Sharing  This section estimates the complexity of inference Nso for a single object and the complexity Nmo for multiple objects when part sharing is not used. These results are for comparison to the complex- ities derived in the following section using part sharing. The inference complexity for a single object requires computing: (i) the number Nbu of compu- tations required by the bottom-up pass, (ii) the number Nms of computations required by model selection at the top-level of the hierarchy, and (iii) the number Ntd of computations required by the top-down pass. The complexity Nbu of the bottom-up pass can be computed from equation (6). This requires a total of Cr computations for each position xν for each level-h node. There are rH−h nodes at level h and each can take |D0|qh positions. This gives a total of |D0|CrqhrH−h computations at level h. This can be summed over all levels to yield:  Nbu =  |D0|CrrH(q/r)h = |D0|CrrH  (q/r)h = |D0|Cr  h=1  the factors (q/r)h decrease rapidly with h. This calculation uses(cid:80)Hh=1 xh = x(1−x  Observe that the main contributions to Nbu come from the ﬁrst few levels of the hierarchy because . For large H  H)  h=1  H−1  1−x  we can approximate Nbu by |D0|Cr  qr  1−q/r (because (q/r)H will be small).  qrH−1  1 − q/r{1 − (q/r)H}.  (9)  9Note that other hierarchical models, including bio-inspired ones, use similar operations but motivated by  different reasons.  9  H(cid:88)  H(cid:88)  Bottom UpTop DownWe calculate Nms = q|H||D0| for the complexity of model selection (which only requires thresh- olding at every point on the top-level lattice). The complexity Ntd of the top-down pass is computed from equation (7). At each level there are r|H|−h nodes and we must compute Cr computations for each. This yields complexity of  (cid:80)|H|h=1 Crr|H|−h for each possible root node. There are at most q|H||D0| possible root nodes (de-  pending on the results of the model selection stage). This yields an upper bound:  Ntd ≤ |D0|Crq|H| r|H|−1  1 − 1/r{1 −  1  r|H−1|}.  (10)  Clearly the complexity is dominated by the complexity Nbu of the bottom-up pass. For simplicity, we will bound/approximate this by:  Nso = |D0|Cr  qrH−1 1 − q/r  (11)  Now suppose we perform inference for multiple objects simultaneously without exploiting shared parts. In this case the complexity will scale linearly with the number |MH| of objects. This gives us complexity:  qrH−1 1 − q/r 4.2 Computation with Shared Parts in Series and in Parallel  Nmo = |MH||D0|Cr  (12)  This section computes the complexity using part sharing. Firstly, for the standard serial implemen- tation of part sharing. Secondly, for the parallel implementation. Now suppose we perform inference on many objects with part sharing using a serial computer. This requires performing computations over the part-subpart compositions between elements of the dictionaries. At level h there are |Mh| dictionary elements. Each can take |Dh| = qh|D| possible states. The bottom-up pass requires performing Cr computations for each of them. This gives a  (cid:80)Hh=1 |Mh|qh computations for the bottom-up process.  total of(cid:80)Hh=1 |Mh|Cr|D0|qh = |D0|Cr  The complexity of model selection is |D0|qH × (H + 1) (this is between all the objects, and the background model, at all points on the top lattice). As in the previous section, the complexity of the top-down process is less than the complexity of the bottom-up process. Hence the complexity for multiple objects using part sharing is given by:  Nps = |D0|Cr  |Mh|qh.  (13)  Next consider the parallel implementation. In this case almost all of the computations are performed in parallel and so the complexity is now expressed in terms of the number of ”neurons” required to encode the dictionaries, see ﬁgure (5). This is speciﬁed by the total number of dictionary elements multiplied by the number of spatial copies of them:  Nn =  |Mh|qh|D0|.  (14)  The computation, both the forward and backward passes of dynamic programming, are linear in the number H of levels. We only need to perform the computations illustrated in ﬁgure (6) between all adjacent levels. Hence the parallel implementation gives speed which is linear in H at the cost of a possibly large number Nn of ”neurons” and connections between them.  4.3 Advantages of Part Sharing in Different Regimes The advantages of part-sharing depend on how the number of parts |Mh| scales with the level h of the hierarchy. In this section we consider three different regimes: (I) The exponential growth  10  H(cid:88)  h=1  H(cid:88)  h=1  regime where the size of the dictionaries increases exponentially with the level h. (II) The empirical growth regime where we use the size of the dictionaries found experimentally by compositional learning [19]. (III) The exponential decrease regime where the size of the dictionaries decreases exponentially with level h. For all these regimes we compare the advantages of the serial and parallel implementations using part sharing by comparison to the complexity results without sharing. Exponential growth of dictionaries is a natural regime to consider. It occurs when subparts are allowed to combine with all other subparts (or a large fraction of them) which means that the number of part-subpart compositions is polynomial in the number of subparts. This gives exponential growth in the size of the dictionaries if it occurs at different levels (e.g., consider the enormous number of objects that can be built using lego). An interesting special case of the exponential growth regime is when |Mh| scales like 1/qh, see ﬁgure (7)(left panel). In this case the complexity of computation for serial part-sharing, and the number of neurons required for parallel implementation, scales only with the number of levels H. This follows from equations (13,14). But nevertheless the number of objects that can be detected scales exponentially as qM. By contrast, the complexity of inference without part-sharing scales exponentially with q, see equation (12, because we have to perform a ﬁxed number of computations, given by equation (11), for each of an exponential number of objects. This is summarized by the following result. Result 1: If the number of shared parts scales exponentially by |Mh| ∝ 1 qh then we can perform inference for order qH objects using part sharing in time linear in H, or with a number of neu- rons linear in H for parallel implementation. By contrast, inference without part-sharing requires exponential complexity. To what extent is exponential growth a reasonable assumption for real world objects? This motivates us to study the empirical growth regime using the dictionaries obtained by the compositional learning experiments reported in [19]. In these experiments, the size of the dictionaries increased rapidly at the lower levels (i.e. small h) and then decreased at higher levels (roughly consistent with the ﬁndings of psychophysical studies – Biederman, personal communication). For these ”empirical dictionaries” we plot the growth, and the number of computations at each level of the hierarchy, in ﬁgure (7)(center panel). This shows complexity which roughly agrees with the exponential growth model. This can be summarized by the following result: Result 2: If |Mh| grows slower than 1/qh and if |Mh| < rH−h then there are gains due to part sharing using serial and parallel computers. This is illustrated in ﬁgure (7)(center panel) based on the dictionaries found by unsupervised computational learning [19]. In parallel implementations, computation is linear in H while requiring a limited number of nodes (”neurons”). Finally we consider the exponential decrease regime. To motivate this regime, suppose that the dictionaries are used to model image appearance, by contrast to the dictionaries based on geometrical features such as bars and oriented edges (as used in [19]). It is reasonable to assume that there are a large number of low-level dictionaries used to model the enormous variety of local intensity patterns. The number of higher-level dictionaries can decrease because they can be used to capture a cruder summary description of a larger image region, which is another instance of the executive summary principle. For example, the low-level dictionaries could be used to provide detailed modeling of the local appearance of a cat, or some other animal, while the higher-level dictionaries could give simpler descriptions like ”cat-fur” or ”dog-fur” or simply ”fur”. In this case, it is plausible that the size of the dictionaries decreases exponentially with the level h. The results for this case emphasize the advantages of parallel computing. Result 3: If |Mh| = rH−h then there is no gain for part sharing if serial computers are used, see ﬁgure (7)(right panel). Parallel implementations can do inference in time which is linear in H but require an exponential number of nodes (”neurons”). Result 3 may appear negative at ﬁrst glance even for the parallel version since it requires an expo- nentially large number of neurons required to encode the lower level dictionaries. But it may relate to one of the more surprising facts about the visual cortex in monkeys and humans – namely that the ﬁrst two visual areas, V1 and V2, where low-level dictionaries would be implemented are enormous compared to the higher levels such as IT where object detection takes places. Current models of V1 and V2 mostly relegate it to being a large ﬁlter bank which seems paradoxical considering their size.  11  (a)  (b)  (c)  Figure 7: The curves are plotted as a function of h. Left panel: The ﬁrst plot is the case where Mh = a/(qh). So we have a constant cost for the computations, when we have shared parts. Center panel: This plot is based on the experiment of [19]. Right panel: The third plot is the case where Mh decreases exponentially. The amount of computation is the same for the shared and non-shared cases. A set of plots with different values of r.  For example, as one theorist [12] has stated when reviewing the functions of V1 and V2 “perhaps the most troublesome objection to the picture I have delivered is that an enormous amount of cortex is used to achieve remarkably little”. Our complexity studies suggest a reason why these visual areas may be so large if they are used to encode dictionaries10.  5 Discussion  This paper provides a complexity analysis of what is arguably one of the most fundamental problem of visions – how, a biological or artiﬁcial vision system could rapidly detect and recognize an enor- mous number of different objects. We focus on a class of hierarchical compositional models [20, 19] whose formulation makes it possible to perform this analysis. But we conjecture that similar results will apply to related hierarchical models of vision (e.g., those cited in the introduction). Technically this paper has required us to re-formulate compositional models so that they can be deﬁned on regular lattices (which makes them easier to compare to alternatives such as deep belief networks) and a novel parallel implementation. Hopefully the analysis has also clariﬁed the use of part-sharing to perform exact inference even on highly complex models, which may not have been clear in the original publications. We note that the re-use of computations in this manner might relate to methods developed to speed up inference on graphical models, which gives an interesting direction to explore. Finally, we note that the parallel inference algorithms used by this class of compositional models have an interesting interpretation in terms of the bottom-up versus top-down debate concerning pro- cessing in the visual cortex [4]. The algorithms have rapid parallel inference, in time which is linear in the number of layers, and which rapidly estimates a coarse “executive summary” interpretation of the image. The full interpretation of the image takes longer and requires a top-down pass where the high-level context is able to resolve ambiguities which occur at the lower levels. Of course, for some simple images the local evidence for the low level parts is sufﬁcient to detect the parts in the bottom-up pass and so the top-down pass is not needed. But more generally, in the bottom-up pass the neurons are very active and represent a large number of possible hypotheses which are pruned out during the top-down pass using context, when “high-level tells low-level to stop gossiping”.  Acknowledgments  Many of the ideas in this paper were obtained by analyzing models developed by L. Zhu and Y. Chen in collaboration with the ﬁrst author. G. Papandreou gave very useful feedback on drafts of this work. D. Kersten gave patient feedback on speculations about how these ideas might relate to  10Of course, this is extremely conjectural.  12  Level (h)Mh∝1qhAmount of computation  Shared PartsNo SharingLevel (h)Amount of computationq = 1/3, r = 2  SharedNo SharingLevel (h)Mh=MHrH−hAmount of computationdifferent values of rthe brain. The WCU program at Korea University, under the supervision of S-W Lee, gave peace and time to develop these ideas.  References [1] N. J. Adams and C. K. I. Williams. Dynamic trees for image modelling. Image and Vision  [2] I. Biederman. Recognition-by-components: a theory of human image understanding. Psycho-  Computing, 20(10):865–877, 2003.  logical Review, 94(2):115–147, Apr. 1987.  [3] E. Borenstein and S. Ullman. Class-speciﬁc, top-down segmentation. ECCV 2002, pages  639–641, 2002.  [4] J. J. DiCarlo, D. Zoccolan, and N. C. Rust. How Does the Brain Solve Visual Object Recogni-  tion? Neuron, 73(3):415–434, Feb. 2012.  [5] K. Fukushima. Neocognitron - a Hierarchical Neural Network Capable of Visual-Pattern  Recognition. Neural Networks, 1(2):119–130, 1988.  [6] S. Geman, D. Potter, and Z. Chi. Composition systems. Quarterly of Applied Mathematics,  [7] G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural  60(4):707–736, 2002.  Computation, 18:1527–54, 2006.  [8] D. Kersten. Predictability and redundancy of natural images. JOSA A, 4(12):2395–2400, 1987. Int. J. [9] I. Kokkinos and A. Yuille.  Inference and learning with hierarchical shape models.  Comput. Vision, 93(2):201–225, June 2011.  [10] Y. LeCun and Y. Bengio. Convolutional networks for images, speech, and time-series. In M. A.  Arbib, editor, The Handbook of Brain Theory and Neural Networks. MIT Press, 1995.  [11] T. Lee and D. Mumford. Hierarchical Bayesian inference in the visual cortex. Journal of the Optical Society of America A, Optics, Image Science, and Vision, 20(7):1434–1448, July 2003.  [12] P. Lennie. Single units and visual cortical organization. Perception, 27:889–935, 1998. [13] H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In Uncertainty  [14] M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature  in Artiﬁcial Intelligence (UAI), 2010.  Neuroscience, 2:1019–1025, 1999.  cortex-like mechanisms. 29:411–426, 2007.  381(6582):520–2, 1996.  [15] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with IEEE Transactions on Pattern Analysis and Machine Intelligence,  [16] S. Thorpe, D. Fize, and C. Marlot. Speed of processing in the human visual system. Nature,  [17] L. G. Valiant. In Circuits of the Mind, 2000. [18] M. Zeiler, D. Krishnan, G. Taylor, and R. Fergus. Deconvolutional networks. Computer Vision  and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2528–2535, 2010.  [19] L. Zhu, Y. Chen, A. Torralba, W. Freeman, and A. L. Yuille. Part and appearance sharing: Re- cursive compositional models for multi-view multi-object detection. In Proceedings of Com- puter Vision and Pattern Recognition, 2010.  [20] L. Zhu, C. Lin, H. Huang, Y. Chen, and A. L. Yuille. Unsupervised structure learning: Hierar- chical recursive composition, suspicious coincidence and competitive exclusion. In Proceed- ings of The 10th European Conference on Computer Vision, 2008.  13  ","This paper describes serial and parallel compositional models of multipleobjects with part sharing. Objects are built by part-subpart compositions andexpressed in terms of a hierarchical dictionary of object parts. These partsare represented on lattices of decreasing sizes which yield an executivesummary description. We describe inference and learning algorithms for thesemodels. We analyze the complexity of this model in terms of computation time(for serial computers) and numbers of nodes (e.g., ""neurons"") for parallelcomputers. In particular, we compute the complexity gains by part sharing andits dependence on how the dictionary scales with the level of the hierarchy. Weexplore three regimes of scaling behavior where the dictionary size (i)increases exponentially with the level, (ii) is determined by an unsupervisedcompositional learning algorithm applied to real data, (iii) decreasesexponentially with scale. This analysis shows that in some regimes the use ofshared parts enables algorithms which can perform inference in time linear inthe number of levels for an exponential number of objects. In other regimespart sharing has little advantage for serial computers but can give linearprocessing on parallel computers."
1301.3605,2013,Feature Learning in Deep Neural Networks - A Study on Speech Recognition Tasks  ,"['Dong Yu', 'Mike Seltzer', 'Jinyu Li', 'Jui-Ting Huang', 'Frank Seide']",https://arxiv.org/pdf/1301.3605.pdf,"3 1 0 2    r a  M 8         ]  G L . s c [      3 v 5 0 6 3  .  1 0 3 1 : v i X r a  Feature Learning in Deep Neural Networks – Studies  on Speech Recognition Tasks  Dong Yu, Michael L. Seltzer, Jinyu Li1, Jui-Ting Huang1, Frank Seide2  Microsoft Research, Redmond, WA 98052  1Microsoft Corporation, Redmond, WA 98052  {dongyu,mseltzer,jinyli,jthuang,fseide}@microsoft.com  2Microsoft Research Asia, Beijing, P.R.C.  Abstract  Recent studies have shown that deep neural networks (DNNs) perform signiﬁ- cantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper, we argue that the im- proved accuracy achieved by the DNNs is the result of their ability to extract dis- criminative internal representations that are robust to the many sources of variabil- ity in speech signals. We show that these representations become increasingly in- sensitive to small perturbations in the input with increasing network depth, which leads to better speech recognition performance with deeper networks. We also show that DNNs cannot extrapolate to test samples that are substantially differ- ent from the training examples. If the training data are sufﬁciently representative, however, internal features learned by the DNN are relatively stable with respect to speaker differences, bandwidth differences, and environment distortion. This enables DNN-based recognizers to perform as well or better than state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization.  1  Introduction  Automatic speech recognition (ASR) has been an active research area for more than ﬁve decades. However, the performance of ASR systems is still far from satisfactory and the gap between ASR and human speech recognition is still large on most tasks. One of the primary reasons speech recognition is challenging is the high variability in speech signals. For example, speakers may have different accents, dialects, or pronunciations, and speak in different styles, at different rates, and in different emotional states. The presence of environmental noise, reverberation, different microphones and recording devices results in additional variability. To complicate matters, the sources of variability are often nonstationary and interact with the speech signal in a nonlinear way. As a result, it is virtually impossible to avoid some degree of mismatch between the training and testing conditions. Conventional speech recognizers use a hidden Markov model (HMM) in which each acoustic state is modeled by a Gaussian mixture model (GMM). The model parameters can be discriminatively trained using an objective function such as maximum mutual information (MMI) [1] or minimum phone error rate (MPE) [2]. Such systems are known to be susceptible to performance degrada- tion when even mild mismatch between training and testing conditions is encountered. To combat this, a variety of techniques has been developed. For example, mismatch due to speaker differ- ences can be reduced by Vocal Tract Length Normalization (VTLN) [3], which nonlinearly warps the input feature vectors to better match the acoustic model, or Maximum Likelihood Linear Re- gression (MLLR) [4], which adapt the GMM parameters to be more representative of the test data. Other techniques such as Vector Taylor Series (VTS) adaptation are designed to address the mis- match caused by environmental noise and channel distortion [5]. While these methods have been  1  successful to some degree, they add complexity and latency to the decoding process. Most require multiple iterations of decoding and some only perform well with ample adaptation data, making them unsuitable for systems that process short utterances, such as voice search. Recently, an alternative acoustic model based on deep neural networks (DNNs) has been proposed. In this model, a collection of Gaussian mixture models is replaced by a single context-dependent deep neural network (CD-DNN). A number of research groups have obtained strong results on a variety of large scale speech tasks using this approach [6–13]. Because the temporal structure of the HMM is maintained, we refer to these models as CD-DNN-HMM acoustic models. In this paper, we analyze the performance of DNNs for speech recognition and in particular, exam- ine their ability to learn representations that are robust to variability in the acoustic signal. To do so, we interpret the DNN as a joint model combining a nonlinear feature transformation and a log- linear classiﬁer. Using this view, we show that the many layers of nonlinear transforms in a DNN convert the raw features into a highly invariant and discriminative representation which can then be effectively classiﬁed using a log-linear model. These internal representations become increasingly insensitive to small perturbations in the input with increasing network depth. In addition, the classi- ﬁcation accuracy improves with deeper networks, although the gain per layer diminishes. However, we also ﬁnd that DNNs are unable to extrapolate to test samples that are substantially different from the training samples. A series of experiments demonstrates that if the training data are sufﬁciently representative, the DNN learns internal features that are relatively invariant to sources of variability common in speech recognition such as speaker differences and environmental distortions. This en- ables DNN-based speech recognizers to perform as well or better than state-of-the-art GMM-based systems without the need for explicit model adaptation or feature normalization algorithms. The rest of the paper is organized as follows. In Section 2 we brieﬂy describe DNNs and illustrate the feature learning interpretation of DNNs. In Section 3 we show that DNNs can learn invariant and discriminative features and demonstrate empirically that higher layer features are less sensitive to perturbations of the input. In Section 4 we point out that the feature generalization ability is effective only when test samples are small perturbations of training samples. Otherwise, DNNs perform poorly as indicated in our mixed-bandwidth experiments. We apply this analysis to speaker adaptation in Section 5 and ﬁnd that deep networks learn speaker-invariant representations, and to the Aurora 4 noise robustness task in Section 6 where we show that a DNN can achieve performance equivalent to the current state of the art without requiring explicit adaptation to the environment. We conclude the paper in Section 7.  2 Deep Neural Networks  A deep neural network (DNN) is conventional multi-layer perceptron (MLP) with many hidden layers (thus deep). If the input and output of the DNN are denoted as x and y, respectively, a DNN can be interpreted as a directed graphical model that approximates the posterior probability py|x(y = s|x) of a class s given an observation vector x, as a stack of (L + 1) layers of log-linear models. The ﬁrst L layers model the posterior probabilities of hidden binary vectors h(cid:96) given input vectors v(cid:96). If h(cid:96) consists of N (cid:96) hidden units, each denoted as h(cid:96) j, the posterior probability can be expressed as  p(cid:96)(h(cid:96)|v(cid:96)) =  ez(cid:96)  j (v(cid:96))·h(cid:96) j (v(cid:96))·1 + ez(cid:96) ez(cid:96)  j  j (v(cid:96))·0  , 0 ≤ (cid:96) < L  N (cid:96)(cid:89)  j=1  where z(cid:96)(v(cid:96)) = (W (cid:96))T v(cid:96) + a(cid:96), and W (cid:96) and a(cid:96) represent the weight matrix and bias vector in the (cid:96)-th layer, respectively. Each observation is propagated forward through the network, starting with the lowest layer (v0 = x) . The output variables of each layer become the input variables of the next, i.e. v(cid:96)+1 = h(cid:96). In the ﬁnal layer, the class posterior probabilities are computed as a multinomial distribution  py|x(y = s|x) = pL(y = s|vL) =  = softmaxs  (1)  (cid:0)vL(cid:1) .  (cid:80)  ezL s (vL) s(cid:48) ezL  s(cid:48) (vL)  Note that the equality between py|x(y = s|x) and pL(y = s|vL) is valid by making a mean-ﬁeld approximation [14] at each hidden layer.  2  In the DNN, the estimation of the posterior probability py|x(y = s|x) can also be considered a two- step deterministic process. In the ﬁrst step, the observation vector x is transformed to another feature vector vL through L layers of non-linear transforms.In the second step, the posterior probability py|x(y = s|x) is estimated using the log-linear model (1) given the transformed feature vector vL. If we consider the ﬁrst L layers ﬁxed, learning the parameters in the softmax layer is equivalent to training a conditional maximum-entropy (MaxEnt) model on features vL. In the conventional MaxEnt model, features are manually designed [15]. In DNNs, however, the feature representations are jointly learned with the MaxEnt model from the data. This not only eliminates the tedious and potentially erroneous process of manual feature extraction but also has the potential to automatically extract invariant and discriminative features, which are difﬁcult to construct manually. In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [6–10] and use speech recognition as our classiﬁcation task. The detailed training procedure and decoding technique for CD-DNN-HMMs can be found in [6–8].  3  Invariant and discriminative features  3.1 Deeper is better  Using DNNs instead of shallow MLPs is a key component to the success of CD-DNN-HMMs. Ta- ble 1, which is extracted from [8], summarizes the word error rates (WER) on the Switchboard (SWB) [16] Hub5’00-SWB test set. Switchboard is a corpus of conversational telephone speech. The system was trained using the 309-hour training set with labels generated by Viterbi align- ment from a maximum likelihood (ML) trained GMM-HMM system. The labels correspond to tied-parameter context-dependent acoustic states called senones. Our baseline WER with the cor- responding discriminatively trained traditional GMM-HMM system is 23.6%, while the best CD- DNN-HMM achives 17.0%—a 28% relative error reduction (it is possible to further improve the DNN to a one-third reduction by realignment [8]). We can observe that deeper networks outperform shallow ones. The WER decreases as the number of hidden layers increases, using a ﬁxed layer size of 2048 hidden units. In other words, deeper mod- els have stronger discriminative ability than shallow models. This is also reﬂected in the improve- ment of the training criterion (not shown). More interestingly, if architectures with an equivalent number of parameters are compared, the deep models consistently outperform the shallow models when the deep model is sufﬁciently wide at each layer. This is reﬂected in the right column of the table, which shows the performance for shallow networks with the same number of parameters as the deep networks in the left column. Even if we further increase the size of an MLP with a single hidden layer to about 16000 hidden units we can only achieve a WER of 22.1%, which is signiﬁ- cantly worse than the 17.1% WER that is obtained using a 7×2k DNN under the same conditions. Note that as the number of hidden layers further increases, only limited additional gains are obtained and performance saturates after 9 hidden layers. The 9x2k DNN performs equally well as a 5x3k DNN which has more parameters. In practice, a tradeoff needs to be made between the width of each layer, the additional reduction in WER and the increased cost of training and decoding as the number of hidden layers is increased.  3.2 DNNs learn more invariant features  We have noticed that the biggest beneﬁt of using DNNs over shallow models is that DNNs learn more invariant and discriminative features. This is because many layers of simple nonlinear processing can generate a complicated nonlinear transform. To show that this nonlinear transform is robust to small variations in the input features, let’s assume the output of layer l − 1, or equivalently the input to the layer l is changed from v(cid:96) to v(cid:96) + δ(cid:96), where δ(cid:96) is a small change. This change will cause the output of layer l, or equivalently the input to the layer (cid:96) + 1 to change by  δ(cid:96)+1 = σ(z(cid:96)(v(cid:96) + δ(cid:96))) − σ(z(cid:96)(v(cid:96))) ≈ diag(cid:0)σ(cid:48)(z(cid:96)(v(cid:96)))(cid:1) (w(cid:96))T δ(cid:96).  3  Table 1: Effect of CD-DNN-HMM network depth on WER (%) on Hub5’00-SWB using the 309- hour Switchboard training set. DBN pretraining is applied.  L × N WER 1 × 2k 24.2 2 × 2k 20.4 3 × 2k 18.4 4 × 2k 17.8 5 × 2k 17.2 7 × 2k 17.1 9 × 2k 17.0 5 × 3k 17.0  –  –  1 × N WER – – – –  – – – –  1 × 3772 1 × 4634  – –  1 × 16k  22.5 22.6  – –  22.1  Figure 1: Percentage of saturated activations at each layer  The norm of the change δ(cid:96)+1 is  (cid:107)δ(cid:96)+1(cid:107) ≈ (cid:107)diag(σ(cid:48)(z(cid:96)(v(cid:96))))((w(cid:96))T δ(cid:96))(cid:107) ≤ (cid:107)diag(σ(cid:48)(z(cid:96)(v(cid:96))))(w(cid:96))T(cid:107)(cid:107)δ(cid:96)(cid:107) = (cid:107)diag(v(cid:96)+1 ◦ (1 − v(cid:96)+1))(w(cid:96))T(cid:107)(cid:107)δ(cid:96)(cid:107)  (2)  where ◦ refers to an element-wise product. Note that the magnitude of the majority of the weights is typically very small if the size of the hidden layer is large. For example, in a 6×2k DNN trained using 30 hours of SWB data, 98% of the weights in all layers except the input layer have magnitudes less than 0.5. While each element in v(cid:96)+1 ◦ (1 − v(cid:96)+1) is less than or equal to 0.25, the actual value is typically much smaller. This means that a large percentage of hidden neurons will not be active, as shown in Figure 1. As a result, the average norm (cid:107)diag(v(cid:96)+1 ◦ (1 − v(cid:96)+1))(w(cid:96))T(cid:107)2 in (2) across a 6-hr SWB development set is smaller than one in all layers, as indicated in Figure 2. Since all hidden layer values are bounded in the same range of (0, 1), this indicates that when there is a small perturbation on the input, the perturbation shrinks at each higher hidden layer. In other words, features generated by higher hidden layers are more invariant to variations than those represented by lower layers. Note that the maximum norm over the same development set is larger than one, as seen in Figure 2. This is necessary since the differences need to be enlarged around the class boundaries to have discrimination ability.  4 Learning by seeing  In Section 3, we showed empirically that small perturbations in the input will be gradually shrunk as we move to the internal representation in the higher layers. In this section, we point out that the  4  Figure 2: Average and maximum (cid:107)diag(v(cid:96)+1 ◦ (1 − v(cid:96)+1))(w(cid:96))T(cid:107)2 across layers on a 6×2k DNN  Figure 3: Illustration of mixed-bandwidth speech recognition using a DNN  above result is only applicable to small perturbations around the training samples. When the test samples deviate signiﬁcantly from the training samples, DNNs cannot accurately classify them. In other words, DNNs must see examples of representative variations in the data during training in order to generalize to similar variations in the test data. We demonstrate this point using a mixed-bandwidth ASR study. Typical speech recognizers are trained on either narrowband speech signals, recorded at 8 kHz, or wideband speech signals, recorded at 16 kHz. It would be advantageous if a single system could recognize both narrow- band and wideband speech, i.e. mixed-bandwidth ASR. One such system was recently proposed using a CD-DNN-HMM [17]. In that work, the following DNN architecture was used for all experi- ments. The input features were 29 mel-scale log ﬁlter-bank outputs together with dynamic features. An 11-frame context window was used generating an input layer with 29 · 3 · 11 = 957 nodes. The DNN has 7 hidden layers, each with 2048 nodes. The output layer has 1803 nodes, corresponding to the number of senones determined by the GMM system. The 29-dimensional ﬁlter bank has two parts: the ﬁrst 22 ﬁlters span 0–4 kHz and the last 7 ﬁlters span 4–8 kHz, with the center frequency of the ﬁrst ﬁlter in the higher ﬁlter bank at 4 kHz. When the speech is wideband, all 29 ﬁlters have observed values. However, when the speech is narrowband, the high-frequency information was not captured so the ﬁnal 7 ﬁlters are set to 0. Figure 3 illustrates the architecture of the mixed-bandwidth ASR system. Experiments were conducted on a mobile voice search (VS) corpus. This task consists of internet search queries made by voice on a smartphone.There are two training sets, VS-1 and VS-2, con- sisting of 72 and 197 hours of wideband audio data, respectively. These sets were collected during  5  Table 2: WER (%) on wideband (16k) and narrowband (8k) test sets with and without narrowband training data.  training data 16 kHz VS-1 + 16 kHz VS-2 16 kHz VS-1 + 8 kHz VS-2  16 kHz VS-T 8 kHz VS-T  27.5 28.3  53.5 29.3  different times of year. The test set, called VS-T, has 26757 words in 9562 utterances. The narrow band training and test data were obtained by downsampling the wideband data. Table 2 summarizes the WER on the wideband and narrowband test sets when the DNN is trained with and without narrowband speech. From this table, it is clear that if all training data are wideband, the DNN performs well on the wideband test set (27.5% WER) but very poorly on the narrowband test set (53.5% WER). However, if we convert VS-2 to narrowband speech and train the same DNN using mixed-bandwidth data (second row), the DNN performs very well on both wideband and narrowband speech. To understand the difference between these two scenarios, we take the output vectors at each layer for the wideband and narrowband input feature pairs, h(cid:96)(xwb) and h(cid:96)(xnb), and measure their Eu- clidean distance. For the top layer, whose output is the senone posterior probability, we calculate the KL-divergence in nats between py|x(sj|xwb) and py|x(sj|xnb). Table 3 shows the statistics of dl and dy over 40, 000 frames randomly sampled from the test set for the DNN trained using wideband speech only and the DNN trained using mixed-bandwidth speech.  Table 3: Euclidean distance for the output vectors at each hidden layer (L1-L7) and the KL di- vergence (nats) for the posteriors at the top layer between the narrowband (8 kHz) and wideband (16 kHz) input features, measured using the wideband DNN or the mixed-bandwidth DNN.  wideband DNN mixed-band DNN variance  layer L1 L2 L3 L4 L5 L6 L7 Top  Eucl  dist mean 13.28 10.38 8.04 8.53 9.01 8.46 5.27 2.03  KL  variance mean 7.32 5.39 4.49 4.74 5.39 4.75 3.12 0.22  3.90 2.47 1.77 2.33 2.96 2.60 1.85  –  3.62 1.28 1.27 1.85 2.30 1.57 0.93  –  From Table 3 we can observe that in both DNNs, the distance between hidden layer vectors generated from the wideband and narrowband input feature pair is signiﬁcantly reduced at the layers close to the output layer compared to that in the ﬁrst hidden layer. Perhaps what is more interesting is that the average distances and variances in the data-mixed DNN are consistently smaller than those in the DNN trained on wideband speech only. This indicates that by using mixed-bandwidth training data, the DNN learns to consider the differences in the wideband and narrowband input features as irrelevant variations. These variations are suppressed after many layers of nonlinear transformation. The ﬁnal representation is thus more invariant to this variation and yet still has the ability to distinguish between different class labels. This behavior is even more obvious at the output layer since the KL-divergence between the paired outputs is only 0.22 in the mixed-bandwidth DNN, much smaller than the 2.03 observed in the wideband DNN.  5 Robustness to speaker variation  A major source of variability is variation across speakers. Techniques for adapting a GMM-HMM to a speaker have been investigated for decades. Two important techniques are VTLN [3], and feature- space MLLR (fMLLR) [4]. Both VTLN and fMLLR operate on the features directly, making their application in the DNN context straightforward.  6  Table 4: Comparison of feature-transform based speaker-adaptation techniques for GMM-HMMs, a shallow, and a deep NN. Word-error rates in % for Hub5’00-SWB (relative change in parentheses).  GMM-HMM CD-MLP-HMM CD-DNN-HMM  adaptation technique speaker independent + VTLN + {fMLLR/fDLR}×4  40 mix  23.6 21.5 (-9%) 20.4 (-5%)  1×2k  24.2 22.5 (-7%) 21.5 (-4%)  7×2k  17.1 16.8 (-2%) 16.4 (-2%)  VTLN warps the frequency axis of the ﬁlterbank analysis to account for the fact that the precise lo- cations of vocal-tract resonances vary roughly monotonically with the physical size of the speaker. This is done in both training and testing. On the other hand, fMLLR applies an afﬁne transform to the feature frames such that an adaptation data set better matches the model. In most cases, in- cluding this work, ‘self-adaptation’ is used: generate labels using unsupervised transcription, then re-recognize with the adapted model. This process is iterated four times. For GMM-HMMs, fM- LLR transforms are estimated to maximize the likelihood of the adaptation data given the model. For DNNs, we instead maximize cross entropy (with back propagation), which is a discriminative criterion, so we prefer to call this transform feature-space Discriminative Linear Regression (fDLR). Note that the transform is applied to individual frames, prior to concatenation. Typically, applying VTLN and fMLLR jointly to a GMM-HMM system will reduce errors by 10– 15%. Initially, similar gains were expected for DNNs as well. However, these gains were not realized, as shown in Table 4 [9]. The table compares VTLN and fMLLR/fDLR for GMM-HMMs, a context-dependent ANN-HMM with a single hidden layer, and a deep network with 7 hidden layers, on the same Switchboard task described in Section 3.1. For this task, test data are very consistent with the training, and thus, only a small amount of adaptation to other factors such as recording conditions or environmental factors occurs. We use the same conﬁguration as in Table 1 which is speaker independent using single-pass decoding. For the GMM-HMM, VTLN achieves a strong relative gain of 9%. VTLN is also effective with the shallow neural-network system, gaining a slightly smaller 7%. However, the improvement of VTLN on the deep network with 7 hidden layers is a much smaller 2% gain. Combining VTLN with fDLR further reduces WER by 5% and 4% relative, for the GMM-HMM and the shallow network, respectively. The reduction for the DNN is only 2%. We also tried transplanting VTLN and fMLLR transforms estimated on the GMM system into the DNN, and achieved very similar results [9]. The VTLN and fDLR implementations of the shallow and deep networks are identical. Thus, we conclude that to a signiﬁcant degree, the deep neural network is able to learn internal representations that are invariant with respect to the sources of variability that VTLN and fDLR address.  6 Robustness to environmental distortions  In many speech recognition tasks, there are often cases where the despite the presence of variability in the training data, signiﬁcant mismatch between training and test data persists. Environmental factors are common sources of such mismatch, e.g. ambient noise, reverberation, microphone type and capture device. The analysis in the previous sections suggests that DNNs have the ability to generate internal representations that are robust with respect to variability seen in the training data. In this section, we evaluate the extent to which this invariance can be obtained with respect to distortions caused by the environment. We performed a series of experiments on the Aurora 4 corpus [18], a 5000-word vocabulary task based on the Wall Street Journal (WSJ0) corpus. The experiments were performed with the 16 kHz multi-condition training set consisting of 7137 utterances from 83 speakers. One half of the ut- terances was recorded by a high-quality close-talking microphone and the other half was recorded using one of 18 different secondary microphones. Both halves include a combination of clean speech and speech corrupted by one of six different types of noise (street trafﬁc, train station, car, babble, restaurant, airport) at a range of signal-to-noise ratios (SNR) between 10-20 dB.  7  The evaluation set consists of 330 utterances from 8 speakers. This test set was recorded by the pri- mary microphone and a number of secondary microphones. These two sets are then each corrupted by the same six noises used in the training set at SNRs between 5-15 dB, creating a total of 14 test sets. These 14 test sets can then be grouped into 4 subsets, based on the type of distortion: none (clean speech), additive noise only, channel distortion only, and noise + channel. Notice that the types of noise are common across training and test sets but the SNRs of the data are not. The DNN was trained using 24-dimensional log mel ﬁlterbank features with utterance-level mean normalization. The ﬁrst- and second-order derivative features were appended to the static feature vectors. The input layer was formed from a context window of 11 frames creating an input layer of 792 input units. The DNN had 7 hidden layers with 2048 hidden units in each layer and the ﬁnal softmax output layer had 3206 units, corresponding to the senones of the baseline HMM system. The network was initialized using layer-by-layer generative pre-training and then discriminatively trained using back propagation. In Table 5, the performance obtained by the DNN acoustic model is compared to several other systems. The ﬁrst system is a baseline GMM-HMM system, while the remaining systems are repre- sentative of the state of the art in acoustic modeling and noise and speaker adaptation. All used the same training set. To the authors’ knowledge, these are the best published results on this task. The second system combines Minimum Phone Error (MPE) discriminative training [2] and noise adaptive training (NAT) [19] using VTS adaptation to compensate for noise and channel mismatch [20]. The third system uses a hybrid generative/discriminative classiﬁer [21] as follows . First, an adaptively trained HMM with VTS adaptation is used to generate features based on state likelihoods and their derivatives. Then, these features are input to a discriminative log-linear model to obtain the ﬁnal hypothesis. The fourth system uses an HMM trained with NAT and combines VTS adaptation for environment compensation and MLLR for speaker adaptation [22]. Finally, the last row of the table shows the performance of the DNN system.  Table 5: A comparison of several systems in the literature to a DNN system on the Aurora 4 task.  Systems  GMM baseline MPE-NAT + VTS [20] NAT + Derivative Kernels [21] NAT + Joint MLLR/VTS [22] DNN (7×2048)  none (clean) 14.3 7.2 7.4 5.6 5.6  distortion  noise 17.9 12.8 12.6 11.0 8.8  channel  20.2 11.5 10.7 8.8 8.9  noise + channel  31.3 19.7 19.0 17.8 20.0  AVG  23.6 15.3 14.8 13.4 13.4  It is noteworthy that to obtain good performance, the GMM-based systems required complicated adaptive training procedures [19, 23] and multiple iterations of recognition in order to perform ex- plicit environment and/or speaker adaptation. One of these systems required two classiﬁers. In contrast, the DNN system required only standard training and a single forward pass for classiﬁ- cation. Yet, it outperforms the two systems that perform environment adaptation and matches the performance of a system that adapts to both the environment and speaker. Finally, we recall the results in Section 4, in which the DNN trained only on wideband data could not accurately classify narrowband speech. Similarly, a DNN trained only on clean speech has no ability to learn internal features that are robust to environmental noise. When the DNN for Aurora 4 is trained using only clean speech examples, the performance on the noise- and channel-distorted speech degrades substantially, resulting in an average WER of 30.6%. This further conﬁrms our earlier observation that DNNs are robust to modest variations between training and test data but perform poorly if the mismatch is more severe.  7 Conclusion  In this paper we demonstrated through speech recognition experiments that DNNs can extract more invariant and discriminative features at the higher layers. In other words, the features learned by  8  DNNs are less sensitive to small perturbations in the input features. This property enables DNNs to generalize better than shallow networks and enables CD-DNN-HMMs to perform speech recogni- tion in a manner that is more robust to mismatches in speaker, environment, or bandwidth. On the other hand, DNNs cannot learn something from nothing. They require seeing representative samples to perform well. By using a multi-style training strategy and letting DNNs to generalize to similar patterns, we equaled the best result ever reported on the Aurora 4 noise robustness benchmark task without the need for multiple recognition passes and model adaptation.  References [1] L. Bahl, P. Brown, P.V. De Souza, and R. Mercer, “Maximum mutual information estimation of hidden  markov model parameters for speech recognition,” in Proc. ICASSP, Apr, vol. 11, pp. 49–52.  [2] D. Povey and P. C. Woodland, “Minimum phone error and i-smoothing for improved discriminative  training,” in Proc. ICASSP, 2002.  [3] P. Zhan et al., “Vocal tract length normalization for lvcsr,” Tech. Rep. CMU-LTI-97-150, Carnegie Mellon  Univ, 1997.  [4] M. J. F. Gales, “Maximum likelihood linear transformations for HMM-based speech recognition,” Com-  puter Speech and Language, vol. 12, pp. 75–98, 1998.  [5] A. Acero, L. Deng, T. Kristjansson, and J. Zhang, “HMM Adaptation Using Vector Taylor Series for  Noisy Speech Recognition,” in Proc. of ICSLP, 2000.  [6] D. Yu, L. Deng, and G. Dahl, “Roles of pretraining and ﬁne-tuning in context-dependent DBN-HMMs for real-world speech recognition,” in Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.  [7] G.E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent pretrained deep neural networks for large vocabulary speech recognition,” IEEE Trans. Audio, Speech, and Lang. Proc., vol. 20, no. 1, pp. 33–42, Jan. 2012.  [8] F. Seide, G. Li, and D. Yu, “Conversational speech transcription using context-dependent deep neural  networks,” in Proc. Interspeech, 2011.  [9] F. Seide, G.Li, X. Chen, and D. Yu, “Feature engineering in context-dependent deep neural networks for  conversational speech transcription,” in Proc. ASRU, 2011, pp. 24–29.  [10] D. Yu, F. Seide, G.Li, and L. Deng, “Exploiting sparseness in deep neural networks for large vocabulary  speech recognition,” in Proc. ICASSP, 2012, pp. 4409–4412.  [11] N. Jaitly, P. Nguyen, A. Senior, and V. Vanhoucke, “An application of pretrained deep neural networks to large vocabulary conversational speech recognition,” Tech. Rep. Tech. Rep. 001, Department of Computer Science, University of Toronto, 2012.  [12] T. N. Sainath, B. Kingsbury, B. Ramabhadran, P. Fousek, P. Novak, and A. r. Mohamed, “Making deep belief networks effective for large vocabulary continuous speech recognition,” in Proc. ASRU, 2011, pp. 30–35.  [13] G. E. Dahl, D. Yu, L. Deng, and A. Acero, “Large vocabulary continuous speech recognition with context-  dependent dbn-hmms,” in Proc. ICASSP, 2011, pp. 4688–4691.  [14] L. Saul, T. Jaakkola, and M. I. Jordan, “Mean ﬁeld theory for sigmoid belief networks,” Journal of  Artiﬁcial Intelligence Research, vol. 4, pp. 61–76, 1996.  [15] D. Yu, L. Deng, and A. Acero, “Using continuous features in the maximum entropy model,” Pattern  Recognition Letters, vol. 30, no. 14, pp. 1295–1300, 2009.  [16] J. Godfrey and E. Holliman, Switchboard-1 Release 2, Linguistic Data Consortium, Philadelphia, PA,  1997.  [17] J. Li, D. Yu, J.-T. Huang, and Y. Gong, “Improving wideband speech recognition using mixed-bandwidth  training data in CD-DNN-HMM,” in Proc. SLT, 2012.  [18] N. Parihar and J. Picone, “Aurora working group: DSR front end LVCSR evaluation AU/384/02,” Tech.  Rep., Inst. for Signal and Information Process, Mississippi State University.  [19] O. Kalinli, M. L. Seltzer, J. Droppo, and A. Acero, “Noise adaptive training for robust automatic speech  recognition,” IEEE Trans. on Audio, Sp. and Lang. Proc., vol. 18, no. 8, pp. 1889 –1901, Nov. 2010.  [20] F. Flego and M. J. F. Gales, “Discriminative adaptive training with VTS and JUD,” in Proc. ASRU, 2009. [21] A. Ragni and M. J. F. Gales, “Derivative kernels for noise robust ASR,” in Proc. ASRU, 2011. [22] Y.-Q. Wang and M. J. F. Gales, “Speaker and noise factorisation for robust speech recognition,” IEEE  Trans. on Audio Speech and Language Proc., vol. 20, no. 7, 2012.  [23] H. Liao and M. J. F. Gales, “Adaptive training with joint uncertainty decoding for robust recognition of  noisy data,” in Proc. of ICASSP, Honolulu, Hawaii, 2007.  9  ","Recent studies have shown that deep neural networks (DNNs) performsignificantly better than shallow networks and Gaussian mixture models (GMMs)on large vocabulary speech recognition tasks. In this paper, we argue that theimproved accuracy achieved by the DNNs is the result of their ability toextract discriminative internal representations that are robust to the manysources of variability in speech signals. We show that these representationsbecome increasingly insensitive to small perturbations in the input withincreasing network depth, which leads to better speech recognition performancewith deeper networks. We also show that DNNs cannot extrapolate to test samplesthat are substantially different from the training examples. If the trainingdata are sufficiently representative, however, internal features learned by theDNN are relatively stable with respect to speaker differences, bandwidthdifferences, and environment distortion. This enables DNN-based recognizers toperform as well or better than state-of-the-art systems based on GMMs orshallow networks without the need for explicit model adaptation or featurenormalization."
1301.3342,2013,Barnes-Hut-SNE  ,['Laurens van der Maaten'],https://arxiv.org/pdf/1301.3342.pdf,"Barnes-Hut-SNE  Pattern Recognition and Bioinformatics Group, Delft University of Technology  Laurens van der Maaten  Mekelweg 4, 2628 CD Delft, The Netherlands  lvdmaaten@gmail.com  3 1 0 2    r a  M 8         ]  G L . s c [      2 v 2 4 3 3  .  1 0 3 1 : v i X r a  Abstract  The paper presents an O(N log N )-implementation of t-SNE — an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N 2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational ad- vantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.  1  Introduction  Data-visualization techniques are an essential tool for any data analyst, as they allow the analyst to visually explore the data and generate hypotheses. One of the key limitations of traditional visual- ization techniques such as histograms, scatter plots, and parallel coordinate plots (see, e.g., [10] for an overview) is that they only facilitate the visualization of one or a few data variables at a time. In order to get an idea of the structure of all variables in the data, it is therefore necessary to per- form an automatic analysis of the data before making the visualization, for instance, by learning a low-dimensional embedding of the data. In such an embedding, each data object is represented by a low-dimensional point in such a way, that nearby points correspond to similar data objects and that distant points correspond to dissimilar data objects. The low-dimensional embedding can readily be visualized in, e.g., a scatter plot or a parallel coordinate plot. A plethora of embedding techniques have been proposed over the last decade, e.g., [5, 15, 20, 23, 25, 26]. For creating two- or three-dimensional embeddings that can be readily visualized in a scatter plot, a family of techniques based on stochastic neighbor embedding (SNE; [11]) has recently become very popular. These techniques compute an N×N similarity matrix in both the original data space and in the low-dimensional embedding space; the similarities take the form of a probability distribution over pairs of points in which high probabilities correspond to similar objects or points. The probabilities are generally deﬁned as normalized Gaussian or Student-t kernels, which makes that SNE focuses on preserving local data structure. The embedding is learned by minimizing the Kullback-Leibler divergence between the probability distributions in the original data space and the embedding space with respect to the locations of the points in the embedding. As the resulting cost function is non-convex, this minimization is typically performed using ﬁrst- or second-order gradient-descent techniques [5, 11, 27]. The gradient of the Kullback-Leibler divergence may be interpreted as an N-body system in which all of the N points exert forces on each other. One of the key limitations of SNE (and of its variants) is that its computational and memory com- plexity scales quadratically in the number of data objects N. In practice, this limits the applicability of SNE to data sets with only a few thousand points. To visualize larger data sets, landmark imple- mentations of SNE may be used [25], but this is hardly a satisfactory solution.  1  In this paper, we develop a new algorithm for (t-)SNE that requires only O(N log N ) computation and O(N ) memory. Our new algorithm computes a sparse approximation of the similarities between the original data objects using vantage-point trees [31], and subsequently, approximates the forces between the points in the embedding using a Barnes-Hut algorithm [1] — an algorithm commonly used by astronomers to perform N-body simulations. The Barnes-Hut algorithm reduces the number of pairwise forces that needs to be computed by exploiting the fact that the forces exerted by a group of points on a point that is relatively far away are all very similar.  2 Related work  A large body of previous work has focused on decreasing the computational complexity of algo- rithms that scale quadratically in the amount of data when implemented naively. Most of these studies focus on speeding up nearest-neighbor searches using space-partitioning (metric) trees (e.g., B-trees [2], cover trees [3], and vantage-point trees [31]) or using locality sensitive hashing ap- proaches (e.g., [12, 29]). Motivated by their strong performance reported in earlier work in [17], we opt to use metric trees to approximate the similarities of the input objects in our algorithm. Several prior studies have also developed algorithms to speed up N-body computations. Most prominently, [7, 8] developed a dual-tree algorithm that is similar in spirit to the Barnes-Hut al- gorithm we use in this work. The dual-tree algorithm does not consider interactions between single points and groups of points like the Barnes-Hut algorithm, but it only considers interactions between groups of points. In preliminary experiments (see appendix), we found the dual-tree and Barnes- Hut algorithms to perform on par when used in the context of t-SNE — we opt for the Barnes-Hut algorithm here because it is conceptually simpler. Prior work [6] has also used the fast Gaussian transform [9, 30] (a special case of a fast multipole method [19]) to speed up the computation of Gaussian N-body interactions. Since in t-SNE, the forces exerted on the bodies are non-Gaussian, such an approach cannot readily be applied here.  3  t-Distributed Stochastic Neighbor Embedding  t-Distributed Stochastic Neighbor Embedding (t-SNE) minimizes the divergence between two dis- tributions: a distribution that measures pairwise similarities between the original data objects and a distribution that measures pairwise similarities between the corresponding points in the embedding. Suppose we are given a data set of objects D = {x1, x2, . . . , xN} and a function d(xi, xj) that computes a distance between a pair of objects, e.g., their Euclidean distance. Our aim is to learn an s-dimensional embedding in which each object is represented by a point, E = {y1, y2, . . . , yN} with yi ∈ Rs. To this end, t-SNE deﬁnes joint probabilities pij that measure the pairwise similarity between objects xi and xj by symmetrizing two conditional probabilities as follows:  (cid:80) exp(−d(xi, xj)2/2σ2 i ) k(cid:54)=i exp(−d(xi, xk)2/2σ2 i )  pj|i =  pij =  pj|i + pi|j  2N  .  ,  pi|i = 0  (1)  (2)  Herein, the bandwidth of the Gaussian kernels σi is set such that the perplexity of the conditional distribution Pi equals a predeﬁned perplexity u. The optimal value of σi varies per object, and is found using a simple binary search; see [11] for details. A heavy-tailed distribution is used to measure the similarity qij between the two corresponding points yi and yj in the embedding:  (cid:80) (1 + (cid:107)yi − yj(cid:107)2)−1 k(cid:54)=l(1 + (cid:107)yk − yl)(cid:107)2)−1 ,  qij =  qii = 0.  (3)  In the embedding, a normalized Student-t kernel is used to measure similarities rather than a nor- malized Gaussian kernel to account for the difference in volume between high- and low-dimensional spaces [25]. The locations of the embedding points yi are learned by minimizing the Kullback- Leibler divergence between the joint distributions P and Q:  pij log  pij qij  .  (4)  (cid:88)  i(cid:54)=j  C(E) = KL(P||Q) =  2  (cid:88)  This cost function is non-convex; it is typically minimized by descending along the gradient:  = 4  ∂C ∂yi  (pij − qij)qijZ(yi − yj),  where we deﬁned the normalization term Z =(cid:80)  (5) k(cid:54)=l(1 + (cid:107)yk − yl)(cid:107)2)−1. The evaluation of both joint distributions P and Q is O(N 2), because their respective normalization terms sum over all N 2 pairs of points. Since t-SNE scales quadratically in the number of objects N, its applicability is limited to data sets with only a few thousand data objects; beyond that, learning becomes very slow.  j(cid:54)=i  4 Barnes-Hut-SNE Barnes-Hut-SNE uses metric trees to approximate P by a sparse distribution in which only O(uN ) values are non-zero, and approximates the gradients ∂C ∂yi  using a Barnes-Hut algorithm.  4.1 Approximating Input Similarities  As the input similarities are computed using a (normalized) Gaussian kernel, probabilities pij corre- sponding to dissimilar input objects i and j are (nearly) inﬁnitesimal. Therefore, we can use a sparse approximation to the probabilities pij without a substantial negative effect on the quality of the ﬁ- nal embeddings. In particular, we compute the sparse approximation by ﬁnding the (cid:98)3u(cid:99) nearest neighbors of each of the N data objects, and redeﬁning the pairwise similarities pij as:  exp(−d(xi,xj )2/2σ2 i ) k∈Ni  exp(−d(xi,xk)2/2σ2  i ) , 0,  if j ∈ Ni otherwise  (6)  (cid:40)  (cid:80)  pj|i =  pj|i + pi|j  .  2N  pij =  (7) Herein, Ni represents the set of the (cid:98)3u(cid:99) nearest neighbors of xi, and σi is set such that the perplexity of the conditional distribution equals u. The nearest neighbor sets are found in O(uN log N ) time by building a vantage-point tree on the data set. Vantage-point tree. In a vantage-point tree, each node stores a data object and the radius of a (hyper)ball that is centered on this object [31]. All non-leaf nodes have two children: data objects that are located inside the ball are stored under the left child of the node, whereas data objects that are located outside the ball are stored under the right child. The tree is constructed by presenting the data objects one-by-one, traversing the tree based on whether the current data object lies inside or outside a ball, and creating a new leaf node in which the object is stored. The radius of the new leaf node is set to the median distance between its object and all other objects that lie inside the ball represented by its parent node. To construct a vantage-point tree, the objects need not necessarily be points in a high-dimensional feature space; the availability of a metric d(xi, xj) sufﬁces. (In our experiments, however, we use xi ∈ RD and d(xi, xj) = (cid:107)xi − xj(cid:107).) A nearest-neighbor search is performed using a depth-ﬁrst search on the tree that computes the dis- tance of the objects stored in the nodes to the target object, whilst maintaining i) a list of the current nearest neighbors and ii) the distance τ to the furthest nearest neighbor in the current neighbor list. The value of τ determines whether or not a node should be explored: if there can still be objects inside the ball whose distance to the target object is smaller than τ, the left node is searched, and if there can still be objects outside the ball whose distance to the target object is smaller than τ, the right node is searched. The order in which children are searched depends on whether or not the tar- get object lies inside or outside the current node ball: the left child is examined ﬁrst if the object lies inside the ball, because the odds are that the nearest neighbors of the target object are also located inside the ball. The right child is searched ﬁrst whenever the target object lies outside of the ball.  4.2 Approximating t-SNE Gradients  To approximate the t-SNE gradient, we start by splitting the gradient into two parts as follows:  = 4(Fattr − Frep) = 4  ∂C ∂yi  ijZ(yi − yj) q2  j(cid:54)=i  (8)   ,  (cid:88)  j(cid:54)=i  pijqijZ(yi − yj) −(cid:88)  3  where Fattr denotes the sum of all attractive forces (the left sum), whereas Frep denotes the sum of all repulsive forces (the right sum). Computing the sum of all attractive forces, Fattr, is computa- tionally efﬁcient; it can be done by summing over all non-zero elements of the sparse distribution P in O(uN ). (Note that the term qijZ = (1 + (cid:107)yi − yj(cid:107)2)−1 can be computed in O(1).) However, a naive computation of the sum of all repulsive forces, Frep, is O(N 2). We now develop a Barnes-Hut algorithm to approximate Frep efﬁciently in O(N log N ). Consider three points yi, yj, and yk with (cid:107)yi − yj(cid:107)≈(cid:107)yi − yk(cid:107)(cid:29)(cid:107)yj − yk(cid:107). In this situation, the contributions of yj and yk to Frep will be roughly equal. The Barnes-Hut algorithm [1] exploits this by i) constructing a quadtree on the current embedding, ii) traversing the quadtree using a depth-ﬁrst search, and iii) at every node in the quadtree, deciding whether the corresponding cell can be used as a “summary” for the gradient contributions of all points in that cell. Quadtree. A quadtree is a tree in which each node represents a rectangular cell with a partic- ular center, width, and height. Non-leaf nodes have four children that split up the cell into four smaller cells (quadrants) that lie “northwest”, “northeast”, “southwest”, and “southeast” of the center of the parent node (see Figure 1 for an illustration). Leaf nodes represent cells that contain at most one point of the embedding; the root node represents the cell that contains the complete embedding. In each node, we store the center-of-mass of the embedding points that are located inside the corresponding cell, ycell, and the total number of points that lie inside the cell, Ncell. A quadtree has O(N ) nodes and can be constructed in O(N ) time by inserting the points one-by-one, splitting a leaf node when- ever a second point is inserted in its cell, and updating ycell and Ncell of all visited nodes. Approximating the gradient. To approximate the repulsive part of the gradient, Frep, we note that if a cell is sufﬁciently small and suf- ﬁciently far away from point yi, the contri- ijZ(yi − yj) to Frep will be roughly similar for all points yj inside that cell. We butions q2 i,cellZ(yi − ycell), where we deﬁne can, therefore, approximate these contributions by Ncellq2 ijZ 2(yi − yj) by perform- qi,cellZ = (1 + (cid:107)yi − ycell(cid:107)2)−1. We ﬁrst approximate FrepZ = q2 ing a depth-ﬁrst search on the quadtree, assessing at each node whether or not that node may be used as a “summary” for all the embedding points that are located in the corresponding cell. During i(cid:54)=j(1 + (cid:107)yi − yj(cid:107)2)−1 in the same way. The two  Figure 1: Quadtree constructed on a two- dimensional t-SNE embedding of 500 MNIST digits (the colors of the points correspond to the digit classes). Note how the quadtree adapts to the local point density in the embedding.  this search, we construct an estimate of Z =(cid:80)  approximations thus obtained are then used to compute Frep via Frep = FrepZ Z . We use the condition proposed by [1] to decide whether a cell may be used as a “summary” for all points in that cell. The condition compares the distance of the cell to the target point with its size:  (cid:107)yi − ycell(cid:107)2/rcell < θ,  (9) where rcell represents the length of the diagonal of the cell under consideration and θ is a threshold that trades off speed and accuracy (higher values of θ lead to poorer approximations). In prelimi- nary experiments, we also explored various other conditions that take into account the rapid decay of the Student-t tail, but we did not ﬁnd to lead these alternative conditions to lead to a better accuracy-speed trade-off. (The problem of more complex conditions is that they require expensive computations at each cell. By contrast, the condition in Equation 9 can be evaluated very rapidly.) Dual-tree algorithms. Whilst the Barnes-Hut algorithm considers point-cell interactions, further speed-ups may be obtained by computing only cell-cell interactions. This can be done using a dual-tree algorithm [7] that simultaneously traverses the quadtree twice, and for every pair of nodes decides whether the interaction between the corresponding cells can be used as “summary” for the interactions between all points inside these two cells. Perhaps surprisingly, we did ﬁnd such an  4  Figure 2: Computation time (in seconds) required to embed 70, 000 MNIST digits using Barnes-Hut- SNE (left) and the 1-nearest neighbor errors of the corresponding embeddings (right) as a function of the trade-off parameter θ.  approach to perform on par with the Barnes-Hut algorithm in preliminary experiments. The com- putational advantages of the dual-tree algorithm evaporate because after computing an interaction between two cells, one still needs to determine to which set of points the interaction applies. This can be done by searching the cell or by storing a list of children in each node during tree construction. Both these approaches are computationally costly. (It should be noted that the dual-tree algorithm is, however, much faster in approximating the value of the t-SNE cost function.) The results of our experiments with dual-tree algorithms are presented in the appendix.  5 Experiments  We performed experiments on four large data sets to evaluate the performance of Barnes-Hut-SNE. Code for our algorithm is available from http://homepage.tudelft.nl/19j49/tsne. Data sets. We performed experiments on four data sets: i) the MNIST data set, ii) the CIFAR-10 data set, iii) the NORB data set, and iv) the TIMIT data set. The MNIST data set contains N = 70, 000 grayscale handwritten digit images of size D = 28× 28 = 784 pixels, each of which corresponds to one of ten classes. The CIFAR-10 data set [14] is an annotated subset of the 80 million tiny images data set [24] that contains N = 70, 000 RGB images of size 32× 32 pixels, leading to a D = 32×32×3 = 3, 072-dimensional input objects; each image corresponds to one of ten classes. The (small) NORB data set [16] contains grayscale images of toys from ﬁve different classes, rendered on a uniform background under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees). All images contain D = 96× 96 = 9, 216 pixels. The TIMIT data set contains speech data from which MFCC, delta, and delta-delta features were extracted, leading to D = 39-dimensional features [22]; each frame in the data has one of 39 phone labels. We used the TIMIT training set of N = 1, 105, 455 frames in our experiments. Experimental setup. In all our experiments, we follow the experimental setup of [25] as closely as possible. In particular, we initialize the embedding points by sampling from a Gaussian with a variance of 10−4. We run a gradient-descent optimizer for 1, 000 iterations, setting the initial step size to 200; the step size is updated during the optimization use the scheme of [13]. We use an additional momentum term that has weight 0.5 during the ﬁrst 250 iterations, and 0.8 afterwards. The perplexity u is ﬁxed to 30. Following [25], all data sets with a dimensionality D larger than 50 were preprocessed using PCA to reduce their dimensionality to 50. During the ﬁrst 250 learning iterations, we multiplied all pij-values by a user-deﬁned constant α > 1. As explained in [25], this trick enables t-SNE to ﬁnd a better global structure in the early stages of the optimization. In preliminary experiments, we found that this trick becomes increasingly important to obtain good embeddings when the data set size increases, as it becomes harder for the optimization  5  Computation timeNearest neighbor errorFigure 3: Compution time (in seconds) required to embed MNIST digits (left) and the 1-nearest neighbor errors of the corresponding embeddings (right) as a function of data set size N for both standard t-SNE and Barnes-Hut-SNE. Note that the required computation time, which is shown on the y-axis of the left ﬁgure, is plotted on a logarithmic scale.  to ﬁnd a good global structure when there are more points in the embedding because there is less space for clusters to move around. In our experiments, we ﬁx α = 12 (by contrast, [25] used α = 4). We present the results of three sets of experiments. In the ﬁrst experiment, we investigate the effect of the trade-off parameter θ on the speed and the quality of embeddings produced by Barnes-Hut- SNE on the MNIST data set. In the second experiment, we investigate the computation time required to run Barnes-Hut-SNE as a function of the number of data objects N (also on the MNIST data set). In the third experiment, we construct and visualize embeddings of all four data sets. Results. Figure 2 presents the results of an experiment in which we varied the speed-accuracy trade-off parameter θ used to learn the embedding. The ﬁgure shows the computation time required to construct embeddings of all 70, 000 MNIST digit images, as well as the 1-nearest neighbor error (computed based on the digit labels) of the corresponding embeddings. The results presented in the ﬁgure show that the trade-off parameter θ may be increased to a value of approximately 0.5 without negatively affecting the quality of the embedding. At the same time, increasing the value of θ to 0.5 leads to very substantial improvements in terms of the amount of computation required: the time required to embed all 70, 000 MNIST digits is reduced to just 645 seconds when θ = 0.5. (Note that the special case θ = 0 corresponds to standard t-SNE [25]; we did not run an experiment with θ = 0 because standard t-SNE would take days to complete on the full MNIST data set.) In Figure 3, we compare standard t-SNE and Barnes-Hut-SNE in terms of i) the computation time required for the embedding of MNIST digit images as a function of the data set size N and ii) the 1-nearest neighbor errors of the corresponding embeddings. (Note that the y-axis of the left ﬁgure, which represents the required computation time in seconds, uses a logarithmic scale.) In the experiments, we ﬁxed the parameter θ that trades off speed and accuracy to 0.5. The results presented in the ﬁgure show that Barnes-Hut-SNE is orders of magnitude faster than standard t-SNE, whilst the difference in quality of the constructed embeddings (which is measured by the nearest-neighbor errors) is negligible. Most prominently, the computational advantages of Barnes-Hut-SNE rapidly increase as the number of objects in the data set N increases. Figure 4 presents embeddings of all four data sets constructed using Barnes-Hut-SNE. The colors of the points indicate the classes of the corresponding objects; the titles of the plots indicate the computation time that was used to construct the corresponding embeddings. As before, we ﬁxed θ = 0.5 in all four experiments. The results in the ﬁgure shows that Barnes-Hut-SNE can construct high- quality embeddings of, e.g., the 70, 000 MNIST handwritten digit images in just over 10 minutes. (Although our MNIST embedding contains many more points, it may be compared with that in [25]. Visually, the structure of the two embeddings is very similar.) The results also show that Barnes-  6  Computation timeNearest neighbor errorFigure 4: Barnes-Hut-SNE visualizations of four data sets: MNIST handwritten digits (top-left), CIFAR-10 tiny images (top-right), NORB object images (bottom-left), and TIMIT speech frames (bottom-right). The colors of the point indicate the classes of the corresponding objects. The titles of the ﬁgures indicate the computation time that was used to construct the corresponding embeddings. Figure best viewed in color.  Hut-SNE makes it practical to embed data sets with more than a million data points: the TIMIT embedding shows all 1, 105, 455 data points, and was constructed in less than four hours. A version of the MNIST embedding in which the original digit images are shown is presented in Fig- ure 5. The results show that, like standard t-SNE, Barnes-Hut-SNE is very good at preserving local structure of the data in the embedding: for instance, the visualization clearly shows that orientation is one of the main sources of variation within the cluster of ones.  6 Conclusion and Future Work  We presented a new t-SNE algorithm [25], called Barnes-Hut-SNE, that i) constructs a sparse ap- proximation to the similarities between input objects using vantage-point trees, and ii) approxi- mates the t-SNE gradient using a variant of the Barnes-Hut algorithm. The new algorithm runs in O(N log N ) rather than O(N 2), and requires only O(N ) memory. Our experimental evaluation of Barnes-Hut-SNE shows that it is substantially faster than standard t-SNE, and that it facilitates the visualization of data sets with millions of data objects in scatter plots. A drawback of Barnes-Hut-SNE is that it does not provide any error bounds [21]. Indeed, there exist alternative algorithms that do provide such error bounds (e.g., [28]); we aim to explore these alternatives in future work to see whether they can be used to bound the error made in our t-SNE  7  Figure 5: Barnes-Hut-SNE visualization of all 70, 000 MNIST handwritten digit images (con- structed in 10 minutes and 45 seconds). Zoom in on the visualization for more detailed views.  gradient computations, and to bound the error in the ﬁnal embedding. Another limitation of Barnes- Hut-SNE is that it can only be used to embed data in two or three dimensions. Generalizations to higher dimensions are infeasible because the size of the tree grows exponentially in the dimensional- ity of the embedding space. Having said that, this limitation is not very severe since t-SNE is mainly used for visualization (i.e. for embedding in two or three dimensions). Moreover, it is relatively straightforward to replace the quadtree by metric trees that scale better to high-dimensional spaces. In future work, we plan to further scale up our algorithm by developing parallelized implementations that can run on data sets that are too large to be fully stored in memory. We also aim to investigate the effect of varying the value of θ during the optimization. In addition, we plan to explore to what extent adapted versions of our algorithm (that use metric trees instead of quadtrees) can be used to speed up techniques for relational embedding (e.g., [4, 18]).  Acknowledgments  The author is supported by EU-FP7 Social Signal Processing (SSPNet) and by the Netherlands In- stitue for Advanced Study (NIAS). The author thanks Geoffrey Hinton for many helpful discussions, and two anonymous reviewers for their helpful comments.  8  References [1] J. Barnes and P. Hut. A hierarchical O(N log N) force-calculation algorithm. Nature, 324(4):446–449,  1986.  [2] R. Bayer and E. McCreight. Organization and maintenance of large ordered indexes. Acta Informatica,  1(3):173–189, 1972.  [3] A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In Proceedings of the  International Conference on Machine Learning, pages 97–104, 2006.  [4] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured embeddings of knowledge bases.  In Proceedings of the 25th Conference on Artiﬁcial Intelligence (AAAI), 2011.  [5] M. ´A. Carreira-Perpi˜n´an. The elastic embedding algorithm for dimensionality reduction. In Proceedings  of the 27th International Conference on Machine Learning, pages 167–174, 2010.  [6] N. DeFreitas, Y. Wang, M. Mahdaviani, and D. Lang. Fast Krylov methods for N-body learning.  Advances in Neural Information Processing Systems, volume 18, pages 251–258, 2006.  In  [7] A.G. Gray and A.W. Moore. N-body problems in statistical learning. In Advances in Neural Information  Processing Systems, pages 521–527, 2001.  [8] A.G. Gray and A.W. Moore. Rapid evaluation of multiple density models. In Proceedings of the Interna-  tional Conference on Artiﬁcial Intelligence and Statistics, 2003.  [9] L. Greengard and V. Rokhlin. A fast algorithm for particle simulations. Journal of Computational Physics,  73:325–348, 1987.  [10] J. Heer, M. Bostock, and V. Ogievetsky. A tour through the visualization zoo. Communications of the  ACM, 53:59–67, 2010.  [11] G.E. Hinton and S.T. Roweis. Stochastic Neighbor Embedding.  Processing Systems, volume 15, pages 833–840, 2003.  In Advances in Neural Information  [12] P. Indyk and R. Motwani. Approximate nearest neighbors: Towards removing the curse of dimensionality.  In Proceedings of 30th Symposium on Theory of Computing, 1998.  [13] R.A. Jacobs. Increased rates of convergence through learning rate adaptation. Neural Networks, 1:295–  307, 1988.  [14] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of  Toronto, 2009.  [15] N.D. Lawrence. Spectral dimensionality reduction via maximum entropy. In Proceedings of the Inter- national Conference on Artiﬁcial Intelligence and Statistics, JMLR W&CP, volume 15, pages 51–59, 2011.  [16] Y. LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 97–104, 2004.  [17] T. Liu, A.W. Moore, A. Gray, and K. Yang. An investigation of practical approximate nearest neighbor  algorithms. In Advances in Neural Information Processing Systems, volume 17, pages 825–832, 2004.  [18] A. Paccanaro and G.E. Hinton. Learning distributed representations of concepts using linear relational  embedding. IEEE Transactions on Knowledge and Data Engineering, 13(2):232–244, 2001.  [19] V. Rokhlin. Rapid solution of integral equations of classic potential theory. Journal of Computational  Physics, 60:187–207, 1985.  [20] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by Locally Linear Embedding. Science,  290(5500):2323–2326, 2000.  [21] J.K. Salmon and M.S. Warren. Skeletons from the treecode closet. Journal of Computational Physics,  111(1):136–155, 1994.  [22] F. Sha and L.K. Saul. Large margin hidden Markov models for automatic speech recognition. In Advances  in Neural Information Processing Systems, volume 19, pages 1249–1456, 2007.  [23] J.B. Tenenbaum, V. de Silva, and J.C. Langford. A global geometric framework for nonlinear dimension-  ality reduction. Science, 290(5500):2319–2323, 2000.  [24] A. Torralba, R. Fergus, and W.T. Freeman. 80 million tiny images: A large dataset for non-parametric IEEE Transactions on Pattern Analysis and Machine Intelligence,  object and scene recognition. 30(11):1958–1970, 2008.  [25] L.J.P. van der Maaten and G.E. Hinton. Visualizing data using t-SNE. Journal of Machine Learning  Research, 9(Nov):2431–2456, 2008.  9  [26] J. Venna, J. Peltonen, K. Nybo, H. Aidos, and S. Kaski. Information retrieval perspective to nonlinear dimensionality reduction for data visualization. Journal of Machine Learning Research, 11(Feb):451– 490, 2010.  [27] M. Vladymyrov and M. ´A. Carreira-Perpi˜n´an. Partial-Hessian strategies for fast learning of nonlinear In Proceedings of the International Conference on Machine Learning, pages 345–352,  embeddings. 2012.  [28] M.S. Warren and J.K. Salmon. A parallel hashed octtree N-body algorithm.  ACM/IEEE Conference on Supercomputing, pages 12–21, 1993.  In Proceedings of the  [29] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In Advances in Neural Information Processing  Systems, pages 1753–1760, 2008.  [30] C. Yang, R. Duraiswami, N.A. Gumerov, and L. Davis.  Improved fast Gauss transform and efﬁcient In Proceedings of the IEEE International Conference on Computer Vision,  kernel density estimation. pages 664–671, 2003.  [31] P.N. Yianilos. Data structures and algorithms for nearest neighbor search in general metric spaces. In  Proceedings of the ACM-SIAM Symposium on Discrete Algorithms, pages 311–321, 1993.  10  A Experiments with Dual-Tree t-SNE  We also performed experiments with a dual-tree implementation [7] of t-SNE. Dual-tree t-SNE differs from Barnes-Hut-SNE in that it considers only cell-cell instead of point-cell interactions. It simultaneously traverses the quadtree twice, and decides for each pair of nodes whether the interaction between these nodes can be used as a “summary” for all points in the cells corresponding to these two nodes. We use the following condition to check whether the interaction between a pair of nodes may be used as a “summary” interaction:  (cid:107)ycell1 − ycell2(cid:107)2/ max(rcell1, rcell2) < ρ,  (10) where ycell1 and ycell2 represent the center-of-mass of the two cells, rcell1 and rcell2 represent the diameter of the two cells, and ρ is a speed-accuracy trade-off parameter (similar to θ in Barnes-Hut-SNE). Figure 6 presents the results of an experiment in which we investigate the inﬂuence of the trade-off parameter ρ on the learning time and the quality of the embedding on the MNIST data set. The results in the ﬁgure may be readily compared to those in Figure 2. The results in the ﬁgure show that, whilst the dual-tree algorithm provides additional speed-ups compared to the Barnes-Hut algorithm, the quality of the embedding also de- teriorates much faster as the trade-off parameter ρ increases. The quality of the embedding obtained with a dual-tree algorithm with ρ = 0.25 roughly equals that of a Barnes-Hut embedding with θ = 0.5, and these two embeddings are constructed in roughly the same time (viz. in approximately 650–700 seconds). Figure 7 shows the performance of dual-tree t-SNE with ρ = 0.25 as a function of the number of MNIST digits N. The results in Figure 7 can be readily compared to those in Figure 3. Again, the results show that dual-tree t-SNE performs roughly on par with Barnes-Hut-SNE, irrespective of the size of the data set N.  Figure 6: Computation time (in seconds) required to embed 70, 000 MNIST digits using dual-tree t-SNE (left) and the 1-nearest neighbor errors of the corresponding embeddings (right) as a function of the trade-off parameter ρ. The results may be compared to those in Figure 2.  Figure 7: Compution time (in seconds) required to embed MNIST digits (left) and the 1-nearest neighbor errors of the corresponding embeddings (right) as a function of data set size N for both standard t-SNE and dual-tree t-SNE. The results may be compared to those in Figure 3.  11  Computation timeNearest neighbor errorComputation timeNearest neighbor error","The paper presents an O(N log N)-implementation of t-SNE -- an embeddingtechnique that is commonly used for the visualization of high-dimensional datain scatter plots and that normally runs in O(N^2). The new implementation usesvantage-point trees to compute sparse pairwise similarities between the inputdata objects, and it uses a variant of the Barnes-Hut algorithm - an algorithmused by astronomers to perform N-body simulations - to approximate the forcesbetween the corresponding points in the embedding. Our experiments show thatthe new algorithm, called Barnes-Hut-SNE, leads to substantial computationaladvantages over standard t-SNE, and that it makes it possible to learnembeddings of data sets with millions of objects."
1301.3224,2013,Efficient Learning of Domain-invariant Image Representations  ,"['Judy Hoffman', 'Erik Rodner', 'Jeff Donahue', 'Kate Saenko', 'Trevor Darrell']",https://arxiv.org/pdf/1301.3224.pdf,"3 1 0 2    r p A 9         ]  G L . s c [      5 v 4 2 2 3  .  1 0 3 1 : v i X r a  Efﬁcient Learning of Domain-invariant Image  Representations  Judy Hoffman  UCB EECS & ICSI  jhoffman@eecs.berkeley.edu  Erik Rodner  UCB EECS & ICSI  erik.rodner@gmail.com  Jeff Donahue  UCB EECS & ICSI  jdonahue@eecs.berkeley.edu  Trevor Darrell  UCB EECS & ICSI  trevor@eecs.berkeley.edu  Kate Saenko  University of Massachusetts, Lowell  saenko@cs.uml.edu  Abstract  We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efﬁciently realized as linear classiﬁers. Speciﬁcally, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classiﬁer. We optimize both the transformation and classiﬁer parameters jointly, and introduce an efﬁcient cost function based on misclassiﬁcation loss. Our method combines several features previously unavailable in a single algorithm: multi-class adapta- tion through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several im- age datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.  Introduction  1 We address the problem of learning domain-invariant image representations for multi-class classi- ﬁers. The ideal image representation often depends not just on the task but also on the domain. Recent studies have demonstrated a signiﬁcant degradation in the performance of state-of-the-art image classiﬁers when input feature distributions change due to different image sensors and noise conditions [1], pose changes [2], a shift from commercial to consumer video [3, 4], and, more generally, training datasets biased by the way in which they were collected [5]. Learning adaptive representations for linear classiﬁers is particularly interesting as they are efﬁcient and prevalent in vision applications, with fast linear SVMs forming the core of some of the most popular object detection methods [6, 7]. Previous work proposed to adapt linear SVMs [8, 9, 10], learning a perturbation of the source hyper- plane by minimizing the classiﬁcation error on labeled target examples for each binary task. These perturbations can be thought of as new feature representations that correct for the domain change. The recent HFA method [11] learns both the perturbed classiﬁer and a latent domain-invariant fea- ture representation, allowing domains to have heterogeneous features with different dimensionali- ties. However, existing SVM-based methods are limited to learning a separate representation for each binary problem and cannot transfer a common, class-independent component of the shift (such as global lighting change) to unlabeled categories, as illustrated in Figure 1. Additionally, the HFA algorithm cannot be solved in linear space and therefore scales poorly to large datasets.  1  Figure 1: (a) Linear classiﬁers (shown as decision boundaries) learned for a four-class problem on a fully labeled source domain. (b) Problem: classiﬁers learned on the source domain do not ﬁt the target domain points shown here due to a change in feature distribution. (c) Existing SVM-based methods only adapt the features of classes with labels (crosses and triangles). (d) Our method adapts all points, including those from classes without labels, by transforming all target features to a new domain-invariant representation.  Recently proposed feature adaptation methods [1, 2, 12, 13, 14, 15] offer a solution by learning a category-independent feature transform that maps target features into the source, pooling all train- ing labels across categories. This enables multi-class adaptation, i.e. transferring the category- independent component of the domain-invariant representation to unlabeled categories. For exam- ple, a map learned on the labeled “triangle” class in Figure 1 can also be used to map the unlabeled “star” class to the source domain. An additional advantage of the asymmetric transform method ARC-t [12] over metric learning [1] or the recently proposed Geodesic Flow Kernel (GFK) [15], is that, like HFA [11], ARC-t can map between heterogeneous feature spaces. However, ARC-t has two major limitations: First, the feature learning does not optimize the objective function of a strong, discriminative classiﬁer directly; rather, it maximizes some notion of similarity between the transformed target points and points in the source. Second, it does not scale well to domains with large numbers of points due to the high number of constraints, which is proportional to the product of the number of labeled data points in the source and target. In this paper, we present a novel technique that combines the desirable aspects of recent methods in a single algorithm, which we call Max-Margin Domain Transforms, or MMDT for short. MMDT uses an asymmetric (non-square) transform W to map target features x to a new representation W x maximally aligned with the source, learning the transform jointly on all categories for which target labels are available (Figure 1(d)). MMDT provides a way to adapt max-margin classiﬁers in a multi-class manner, by learning a shared component of the domain shift as captured by the feature transformation W . Additionally, MMDT can be optimized quickly in linear space, making it a feasible solution for problem settings with a large amount of training data. The key idea behind our approach is to simultaneously learn both the projection of the target features into the source domain and the classiﬁer parameters themselves, using the same classiﬁcation loss to jointly optimize both. Thus our method learns a feature representation that combines the strengths of max-margin learning with the ﬂexibility of the feature transform. Because it operates over the input features, it can generalize the learned shift in a way that parameter-based methods cannot. On the other hand, it overcomes the two ﬂaws of the ARC-t method: by optimizing the classiﬁcation loss directly in the transform learning framework, it can achieve higher accuracy; furthermore, replacing similarity constraints with more efﬁcient hyperplane constraints signiﬁcantly reduces the training time of the algorithm and learning a transformation directly from target to source allows optimization in linear space. The main contributions of our paper can be summarized as follows (also see Table 1):  • Experiments show that MMDT in linear feature space outperforms competing methods in  terms of multi-class accuracy even compared to previous kernelized methods.  • MMDT learns a representation via an asymmetric category independent transform. There- fore, it can adapt features even when the target domain does not have any labeled examples for some categories and when the target and source features are not equivalent.  2  (a) SOURCE (b) TARGET, no adaptation (c) TARGET, existing methods (d) TARGET, our method unlabeled labeled ARC-t [12] HFA [11] GFK [15] MMDT (ours)  multi-class large datasets heterogeneous features optimize max-margin objective  yes no yes no  no no yes yes  yes yes no no  yes yes yes yes  Table 1: Unlike previous methods, our approach is able to simultaneously learn muti-class represen- tations that can transfer to novel classes, scale to large training datasets, and handle different feature dimensionalities.  • The optimization of MMDT is scalable to large datasets because the number of constraints to optimize is linear in the number of training data points and because it can be optimized in linear feature space.  • Our ﬁnal iterative solution can be solved using standard QP packages, making MMDT easy  to implement.  2 Related Work Domain adaptation, or covariate shift, is a fundamental problem in machine learning, and has at- tracted a lot of attention in the machine learning and natural language community, e.g. [16, 17, 18, 19] (see [20] for a comprehensive overview). It is related to multi-task learning but differs from it in the following way: in domain adaptation problems, the distribution over the features p(X) varies across domains while the output labels Y remain the same; in multi-task learning or knowledge transfer, p(X) stays the same (single domain) while the output labels vary (see [20] for more de- tails). In this paper, we perform multi-task learning across domains, i.e. both p(X) and the output labels Y can change between domains. Domain adaptation has been gaining considerable attention in the vision community. Several SVM- based approaches have been proposed for image domain adaptation, including: weighted combi- nation of source and target SVMs and transductive SVMs applied to adaptation in [21]; the feature replication method of [17]; Adaptive SVM [8, 9], where the source model parameters are adapted by adding a perturbation function, and its successor PMT-SVM [10]; Domain Transfer SVM [3], which learns a target decision function while reducing the mismatch in the domain distributions; and a re- lated method [4] based on multiple kernel learning. In the linear case, feature replication [17] can be shown to decompose the learned parameter into θ = ˆθ + θ(cid:48), where ˆθ is shared by all domains [22], in a similar fashion to adaptive SVMs. Several authors considered learning feature representations for unsupervised and transfer learn- ing [23], and for domain adaptation [18, 24]. For visual domain adaptation, transform-based adap- tation methods [1, 12, 13, 2, 14, 11] have recently been proposed. These methods attempt to learn a perturbation over the feature space rather than a class-speciﬁc perturbation over the model parame- ters, typically in the form of a transformation matrix/kernel. The most closely related are the ARC-t method [12], which learns a transformation that maximizes similarity constraints between points in the source and those projected from the target domain, and the recent HFA method [11], which learns a transformation both from the source and target into a common latent space, as well as the classiﬁer parameters. Another related method is the recently proposed GFK [15], which computes a symmetric kernel between source and target points based on geodesic ﬂow along a latent manifold. We will present a detailed comparison to these three methods in the next section.  3 Max-Margin Domain Transforms We propose a novel method for multi-task domain adaptation of linear SVMs by learning a target feature representation. Denote the normal to the afﬁne hyperplane associated with the k’th binary SVM as θk, k = 1, ..., K, and the offset of that hyperplane from the origin as bk. Intuitively, we would like to learn a new target feature representation that is shared across multiple categories. We propose to do so by estimating a transformation W of the input features, or, equivalently, a nS denote the training transformation W T of the source hyperplane parameters θk. Let xs points in the source domain (DS), with labels ys nS . Let xt nT denote the labeled  1, . . . , xs 1, . . . , xt  1, . . . , ys  3  1, . . . , yt  points in the target domain (DT ), with labels yt nT . Thus our goal is to jointly learn 1) afﬁne hyperplanes that separate the classes in the common domain consisting of the source domain and target points projected to the source and 2) the new feature representation of the target domain determined by the transformation W mapping points from the target domain into the source domain. The transformation should have the property that it projects the target points onto the correct side of each source hyperplane. For simplicity of presentation, we ﬁrst show the optimization problem for a binary problem (drop- ping k) with no slack variables. Our objective is as follows:  min W,θ,b  s.t.  2  1 2  F +  ||W||2  (cid:32)(cid:20)xs (cid:21)T(cid:20)θ (cid:32)(cid:20)xt (cid:21)T  ||θ||2  (cid:21)(cid:33) (cid:20)θ  i 1  b  W T  1 2  ys i  yt i  i 1  b  ≥ 1  (cid:21)(cid:33)  ∀i ∈ DS  ≥ 1 ∀i ∈ DT  (1)  (2)  (3)  Note that this can be easily extended to the multi-class case by simply adding a sum over the regu- larizers on all θk parameters and pooling the constraints for all categories. The objective function, written as in Equations (1)-(3), is not a convex problem and so is both hard to optimize and is not guaranteed to have a global solution. Therefore, a standard way to solve this problem is to do alter- nating minimization on the parameters, in our case W and (θ, b). We can effectively do this because when each parameter vector is ﬁxed, the resulting optimization problem is convex. We begin by re-writing Equations (1)-(3) for the more general problem with soft constraints and K categories. Let us denote the hinge loss as: L(y, x, θ) = max{0, 1 − δ(y, k) · xT θ}. We deﬁne a cost function  J(W, θk, bk) =  1 2  ||W||2  F +  K(cid:88) (cid:18)  k=1  (cid:20) 1 (cid:20)xs  2  i 1  L  ys i ,  nS(cid:88)  i=1  +CS  ||θk||2  (cid:21)  ,  2  (cid:20)θk  bk  (cid:21)(cid:19)  +CT  (cid:18)  nT(cid:88)  i=1  L  i , W · yt  (cid:21)(cid:19)(cid:35)  (cid:21)  (cid:20)xt  i 1  ,  (cid:20)θk  bk  (4)  where the constant CS penalizes the source classiﬁcation error and CT penalizes the target adapta- tion error. Finally, we deﬁne our objective function with soft constraints as follows:  min  W,θk,bk  J(W, θk, bk)  (5)  To solve the above optimization problem we perform coordinate descent on W and (θ, b).  1. Set iteration j = 0, W (j) = 0. 2. Solve the sub-problem (θ(j+1)  k  (cid:20) 1  K(cid:88)  2  k=1  min θ,b  , b(j+1)  k  ) = arg minθk,bk J(W (j), θk, bk) by solving:  (cid:18)  nS(cid:88)  i=1  (cid:20)xs  (cid:21)  i 1  (cid:20)θk; (cid:21)(cid:19)  ,  bk  (cid:18)  nT(cid:88)  i=1  ||θk||2  2 + CS  L  ys i ,  +CT  L  i , W (j) · yt  (cid:20)xt  (cid:21)  i 1  (cid:21)(cid:19)(cid:35)  (cid:20)θk  bk  ,  Notice, this corresponds to the standard SVM objective function, except that the target points are ﬁrst projected into the source using W (j). Therefore, we can solve this intermediate problem using a standard SVM solver package.  3. Solve the subproblem W (j+1) = arg minW J(W, θ(j+1), b(j+1)) by solving  min W  ||W||2  F + CT  1 2  (cid:32)  K(cid:88)  nT(cid:88)  k=1  i=1  L  i , W · yt  (cid:35)(cid:33)  (cid:34)  (cid:20)xt  (cid:21)  i 1  ,  θ(j+1) k b(j+1) k  and increment j. This optimization sub-problem is convex and is in a form that a standard QP optimization package can solve.  4  4. Iterate steps 2 & 3 until convergence.  i )T W xt  i < l if the labels are different, for some constants u, l.  i in the source and labeled point xt i )T W xt  It is straightforward to show that both stages (2) and (3) cannot increase the global cost function J(W, θ, b). Therefore, this algorithm is guaranteed to converge to a local optimum. A proof is included in the supplemental material. It is important to note that since both steps of our iterative algorithm can be solved using standard QP solvers, the algorithm can be easily implemented. Additionally, since the constraints in our algorithm grow linearly with the number of training points and it can be solved in linear feature space, the optimization can be solved efﬁciently even as the number of training points grows. Relation to existing work: We now analyze the proposed algorithm in the context of the previous feature transform methods ARC-t [12], HFA [11] and GFK [15]. ARC-t introduced similarity-based constraints to learn a mapping similar to that in step 3 in our algorithm. This approach creates a i in the target, and then learns constraint for each labeled point xs a transformation W that satisﬁes constraints of the form (xs i > u if the labels of xs i and xt i are the same, and (xs The ARC-t formulation has two distinct limitations that our method overcomes. First, it must solve nS·nT constraints, whereas our formulation only needs to solve K·nT constraints, for a K category problem. In general, our method scales to much larger source domains than with ARC-t. The second beneﬁt of our max-margin transformation learning approach is that the transformation learned using the max-margin constraints is learned jointly with the classiﬁer, and explicitly seeks to optimize the ﬁnal SVM classiﬁer objective. While ARC-t’s similarity-based constraints seek to map points of the same category arbitrarily close to one another, followed by a separate classiﬁer learning step, we seek simply to project the target points onto the correct side of the learned hyperplane, leading to better classiﬁcation performance. The HFA formulation also takes advantage of the max-margin framework to directly optimize the classiﬁcation objective while learning transformations. HFA learns the classiﬁer and transforma- tions to a common latent feature representation between the source and target. However, HFA is formulated to solve a binary problem so a new feature transformation must be learned for each cat- egory. Therefore, unlike MMDT, HFA cannot learn a representation that generalizes to novel target categories. Additionally, due to the difﬁculty of deﬁning the dimension of the latent feature repre- sentation directly, the authors optimize with respect to a larger combined transformation matrix and a relaxed constraint. This transformation matrix becomes too large when the feature dimensions in source and target are large so the HFA must usually be solved in kernel space. This can make the method slow and cause it to scale poorly with the number of training examples. In contrast, our method can be efﬁciently solved in linear feature space which makes it fast and potentially more scalable. Finally, GFK [15] formulates a kernelized representation of the data that is equivalent to computing the dot product in inﬁnitely many subspaces along the geodesic ﬂow between the source and target domain subspaces. The kernel is deﬁned by the authors to be symmetric and so can not handle source and target domains of different initial dimension. Additionally, GFK does not directly optimize a classiﬁcation objective. In contrast, our method, MMDT, can handle source and target domains of different feature dimensions via an asymmetric W , as well as directly optimizes the classiﬁcation objective.  4 Experiments on Image Datasets  We now present experiments using the Ofﬁce [1], Caltech256 [25] and Bing [21] datasets to evaluate our algorithm according to the following four criteria. 1) Using a subset of the Ofﬁce and Caltech256 datasets we evaluate multi-class accuracy performance in a standard supervised domain adaptation setting, where all categories have a small number of labeled examples in the target. 2) Using the full Ofﬁce dataset we evaluate multi-class accuracy for the supervised domain adaptation setting where the source and target have different feature dimensions. 3) Using the full Ofﬁce dataset we evaluate multi-class accuracy in the multi-task domain adaptation setting with novel target categories at test time. 4) Using the Bing dataset we assess the ability to scale to larger datasets by analyzing timing performance.  5  svms  33.9 ± 0.7 a → w 35.0 ± 0.8 a → d 35.7 ± 0.4 w → a 66.6 ± 0.7 w → d 34.0 ± 0.3 d → a d → w 74.3 ± 0.5 a → c 35.1 ± 0.3 w → c 31.3 ± 0.4 31.4 ± 0.3 d → c 35.9 ± 0.4 c → a c → w 30.8 ± 1.1 c → d 35.6 ± 0.7 40.0 ± 0.6 mean  svmt  62.4 ± 0.9 55.9 ± 0.8 45.6 ± 0.7 55.1 ± 0.8 45.7 ± 0.9 62.1 ± 0.8 32.0 ± 0.8 30.4 ± 0.7 31.7 ± 0.6 45.3 ± 0.9 60.3 ± 1.0 55.8 ± 0.9 48.5 ± 0.8  arct [12] 55.7 ± 0.9 50.2 ± 0.7 43.4 ± 0.5 71.3 ± 0.8 42.5 ± 0.5 78.3 ± 0.5 37.0 ± 0.4 31.9 ± 0.5 33.5 ± 0.4 44.1 ± 0.6 55.9 ± 1.0 50.6 ± 0.8 49.5 ± 0.6  hfa [11] 61.8 ± 1.1 52.7 ± 0.9 45.9 ± 0.7 51.7 ± 1.0 45.8 ± 0.9 62.1 ± 0.7 31.1 ± 0.6 29.4 ± 0.6 31.0 ± 0.5 45.5 ± 0.9 60.5 ± 0.9 51.9 ± 1.1 47.4 ± 0.8  gfk[15] 58.6 ± 1.0 50.7 ± 0.8 44.1 ± 0.4 70.5 ± 0.7 45.7 ± 0.6 76.5 ± 0.5 36.0 ± 0.5 31.1 ± 0.6 32.9 ± 0.5 44.7 ± 0.8 63.7 ± 0.8 57.7 ± 1.1 51.0 ± 0.7  mmdt (ours) 64.6 ± 1.2 56.7 ± 1.3 47.7 ± 0.9 67.0 ± 1.1 46.9 ± 1.0 74.1 ± 0.8 36.4 ± 0.8 32.2 ± 0.8 34.1 ± 0.8 49.4 ± 0.8 63.8 ± 1.1 56.5 ± 0.9 52.5 ± 1.0  Table 2: Multi-class accuracy for the standard supervised domain adaptation setting: All results are from our implementation. When averaged across all domain shifts the reported average value for gfk was 51.65 while our implementation had an average of 51.0 ± 0.7. Therefore, the result difference is within the standard deviation over data splits. Red indicates the best result for each domain split. Blue indicates the group of results that are close to the best performing result. The domain names are shortened for space: a: amazon, w: webcam, d: dslr, c: Caltech256  Ofﬁce Dataset The Ofﬁce dataset is a collection of images that provides three distinct domains: amazon, webcam, and dslr. The dataset has 31 categories consisting of common ofﬁce objects such as chairs, backpacks and keyboards. The amazon domain contains product images (from ama- zon.com) containing a single object, centered, and usually on a white background. The webcam and dslr domains contain images taken in“the wild” using a webcam or a dslr camera, respectively. They are taken in an ofﬁce setting and so have different lighting variation and background changes (see Figure 1 for some examples.) We use the SURF-BoW image features provided by the au- thors [1]. More details on how these features were computed can be found in [1]. The available features are vector quantized to 800 dimensions for all domains and additionally for the dslr do- main there are 600 dimensional features available (we denote this as dslr-600). Ofﬁce + Caltech256 Dataset This dataset consists of the 10 common categories shared by the Ofﬁce and Caltech256 datasets. To better compare to previously reported performance, we use the features provided by [15], which are also SURF-BoW 800 dimensional features. Bing Dataset To demonstrate the effect that constraint set size has on run-time performance, we use the Bing dataset from [21], which has a larger number of images in each domain than Ofﬁce. The source domain has images from the Bing search engine and the target domain is from the Caltech256 benchmark. We run experiments using the ﬁrst 20 categories and set the number of source examples per category to be 50. We use the train/test split from [21] and then vary the number of labeled target examples available from 5 to 25. Baselines We use the following baselines as a comparison in the experiments where applicable.1  • svms: A support vector machine using source training data. • svmt: A support vector machine using target training data. • arc-t: A category general feature transform method proposed by [12]. We implement the transform learning and then apply both a KNN classiﬁer (as originally proposed) and an SVM classiﬁer.  • hfa: A max-margin transform approach that learns a latent common space between source  and target as well as a classiﬁer that can be applied to points in that common space [11].  • gfk: The geodesic ﬂow kernel [15] applied to all source and target data (including test data).  Following [15], we use a 1-nearest neighbor classiﬁer with the kernel.  1We used the LIBSVM package [26] for kernelized methods and Liblinear [27] package for linear methods.  6  target  source amazon dslr-600 webcam dslr-600  svmt  52.9 ± 0.7 51.8 ± 0.6  arc-t  58.2 ± 0.6 58.2 ± 0.7  hfa  57.8 ± 0.6 60.0 ± 0.6  mmdt 62.3 ± 0.8 63.3 ± 0.5  Table 3: Multiclass accuracy results on the standard supervised domain adaptation task with different feature dimensions in the source and target. The target domain is dslr for both cases.  source amazon webcam  svms  10.3 ± 0.6 51.6 ± 0.5  arc-t  41.4 ± 0.3 59.4 ± 0.4  gfk  38.9 ± 0.4 62.9 ± 0.5  mmdt 44.6 ± 0.3 58.3 ± 0.5  Table 4: Multiclass accuracy results on the Ofﬁce dataset for the domain shift of webcam→dslr for target test categories not seen in at training time. Following the experimental setup of [12]. We compare against pmt-svm [10] and ARC-t [12] using both knn and svm classiﬁcation.  Standard Domain Adaptation Experiment For our ﬁrst experiment, we use the Of- ﬁce+Caltech256 domain adaptation benchmark dataset to evaluate multi-class accuracy in the stan- dard domain adaptation setting where a few labeled examples are available for all categories in the target domain. We follow the setup of [1] and [15]: 20 training examples for amazon source (8 for all other domains as source) and 3 labeled examples per category for the target domain. We created 20 random train/test splits and averaged results across them. The multi-class accuracy for each domain pair is shown in Table 2. Our method produced the highest multi-class accuracy for 9 out of 12 of the domain shifts and competitively on the other 3 shifts. This experiment demonstrates that our method achieves a high recognition performance and is able to outperform the most recent domain adaptation algorithms. Our method especially stands out in the settings where the domains are initially very different. The most similar domains in this dataset are webcam and dslr and we see that our algorithm does not perform as well on those two shifts as gfk. This ﬁts with our intuition since gfk is a 1-nearest neighbor approach and so is more suitable when the domains are initially similar. Additionally, an important observation is that our linear method on average outperforms all the baselines, even though they each learn a non-linear transformation. Asymmetric Transform Experiment Next, we analyze the effectiveness of our asymmetric trans- form learning by experimenting with the source and target having different feature dimensions. We use the same experimental setup as previously, but use the Ofﬁce dataset and the alternate represen- tation for the dslr domain that is 600-dimensional (denoted as dslr-600). We compare against svmt, arc-t and hfa, the baselines that can handle this scenario. The results are shown in Table 3. Again, we ﬁnd that our method can effectively learn a feature representation for the target domain that optimizes the ﬁnal classiﬁcation objective. Generalizing to Novel Categories Experiment We next consider the setting of practical impor- tance where labeled target examples are not available for all objects. Recall that this is a setting that many category speciﬁc adaptation methods cannot generalize to, including hfa [11]. Therefore, we compare our results for this setting to the arc-t [12] method which learns a category independent feature transform and the gfk [15] method which learns a category independent kernel to compare the domains. Following the experimental setup of [12], we use the full Ofﬁce dataset and allow 20 labeled examples per category in the source for amazon and 10 labeled examples for the ﬁrst 15 object categories in the target (dslr). For the webcam→dslr shift, we use 8 labeled examples per category in the source for webcam and 4 labeled examples for the ﬁrst 15 object categories in the target dslr. The experimental results for the domain shift of webcam→dslr are evaluated and shown in Ta- ble 4; MMDT outperforms the baselines for the amazon→dslr shift and offers adaptive beneﬁt over svms for the shift from webcam to dslr. As in the ﬁrst experiment, both arc-t and gfk use nearest neighbor classiﬁers on a learned kernel are more suitable to the shift between webcam and dslr, which are initially very similar.  Scaling to Larger Datasets Experiment With our last experiment we show that our method not only offers high accuracy performance it also scales well with an increasing dataset size. Speciﬁ-  7  Figure 2: Left: multiclass accuracy on the Bing dataset using 50 training examples in the source and varying the number of available labeled examples in the target. Right: training time comparison.  cally, the number of constraints our algorithm optimizes scales linearly with the number of training points. Conversely, the number of constraints that need to be optimized for the arc-t baseline is quadratic in the number of training points. To demonstrate the effect that constraint set size has on run-time performance, we use the Bing [21] dataset, which has a larger number of images in each domain than Ofﬁce. The source domain has images from the Bing search engine and the target domain is from the Caltech256 benchmark. We run experiments using the ﬁrst 20 categories and set the number of source examples per category to be 50. We use the train/test split from [21] and then vary the number of labeled target examples available from 5 to 20. The left-hand plot in Figure 2 presents multi-class accuracy for this setup. Additionally, the training time of our method (run to convergence) and that of the baselines is shown on the right-hand plot. Our mmdt method provides a considerable improvement over all the baselines in terms of multi- class accuracy. It is also considerably faster than all but the gfk method. An important point to note is that both our method and arc-t scale approximately linearly with the number of target training points which is empirical veriﬁcation for our claims. Note that hfa and gfk do not vary signiﬁcantly as the number of target training points increases. However, for hfa the main bottleneck time is consumed by a distance computation between each pair of training points. Therefore, since there are many more source training points than target, adding a few more target points does not signiﬁcantly increase the overall time spent for this experiment, but would present a problem as the size of the dataset grew in general.  5 Conclusion In this paper, we presented a feature learning technique for domain adaptation that combines the ability of feature transform-based methods to perform multi-task adaptation with the performance beneﬁts of directly adapting classiﬁer parameters. We validated the computational efﬁciency and effectiveness of our method using two standard benchmarks used for image domain adaptation. Our experiments show that 1) our method is a competitive domain adaptation algorithm able to outperform previous methods, 2) is successfully able to generalize to novel target categories at test time, and 3) can learn asymmetric transforma- tions. In addition, these beneﬁts are offered through a framework that is scalable to larger datasets and achieves higher classiﬁcation accuracy than previous approaches. So far we have focused on linear transforms because of its speed and scalability; however, our method can also be kernelized to include nonlinear transforms. In future work, we would like to explore the kernelized version of our algorithm and especially experiment with the geodesic ﬂow kernel as input to our algorithm. Acknowledgements: This work was supported by NSF grants IIS-1116411 and IIS-1212798, DARPA, and the Toyota Corporation.  8  51015203040506070Number of Labeled Target ExamplesMulticlass accuracy (%)Multiclass Accuracy vs #Target Example  51015200204060Number of Labeled Target ExamplesTime (min)Time vs #Target Examples  51015203035404550556065Number of Labeled Target ExamplesMulticlass accuracy (%)  svmssvmtarcthfagfkmmdt (ours)References [1] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In Proc.  ECCV, 2010.  [2] A. Farhadi and M. K. Tabrizi. Learning to recognize activities from the wrong view point. In Proc. ECCV,  2008.  [3] L. Duan, I. W. Tsang, D. Xu, and S. J. Maybank. Domain transfer svm for video concept detection. In  CVPR, 2009.  [4] L. Duan, D. Xu, I. Tsang, and J. Luo. Visual event recognition in videos by learning from web data. In  Proc. CVPR, 2010.  [5] A. Torralba and A. Efros. Unbiased look at dataset bias. In Proc. CVPR, 2011. [6] D. McAllester P. Felzenszwalb, R. Girshick and D. Ramanan. Object detection with discriminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9), 2010. [7] Lubomir Bourdev and Jitendra Malik. Poselets: Body part detectors trained using 3d human pose anno-  tations. In Proc. ICCV, 2009.  [8] J. Yang, R. Yan, and A. Hauptmann. Adapting svm classiﬁers to data with shifted distributions. In ICDM  Workshops, 2007.  [9] X. Li. Regularized adaptation: Theory, algorithms and applications. In PhD thesis, University of Wash-  ington, USA, 2007.  [10] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for object category detection. In Proc. ICCV,  2011.  [11] Lixin Duan, Dong Xu, and Ivor W. Tsang. Learning with augmented features for heterogeneous domain  adaptation. In Proc. ICML, 2012.  [12] B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation using  asymmetric kernel transforms. In Proc. CVPR, 2011.  [13] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsupervised ap-  proach. In Proc. ICCV, 2011.  [14] W. Dai, Y. Chen, G. Xue, Q. Yang, and Y. Yu. Translated learning: Transfer learning across different  feature spaces. In Proc. NIPS, 2008.  [15] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow kernel for unsupervised domain adaptation. In  Proc. CVPR, 2012.  [16] J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adap-  tation for sentiment classiﬁcation. 2007.  [17] H. Daume III. Frustratingly easy domain adaptation. In Proc. ACL, 2007. [18] S. Ben-david, J. Blitzer, K. Crammer, and O. Pereira. Analysis of representations for domain adaptation.  In In NIPS. MIT Press, 2007.  [19] J. Jiang and C. X. Zhai. Instance weighting for domain adaptation in nlp. In Proceedings of the 45th  Annual Meeting of the Association of Computational Linguistics, pages 264–271, 2007.  [20] J. Jiang. A literature survey on domain adaptation of statistical classiﬁers. http://sifaka.cs.  uiuc.edu/jiang4/domain_adaptation/survey/.  [21] A. Bergamo and L. Torresani. Exploiting weakly-labeled web images to improve object classiﬁcation: a  domain adaptation approach. In Proc. NIPS, 2010.  [22] W. Jiang, E. Zavesky, S. Chang, and A. Loui. Cross-domain learning methods for high-level visual  concept classiﬁcation. In ICIP, 2008.  [23] Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. Journal of  Machine Learning Research - Proceedings Track, 27:17–36, 2012.  [24] John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 120–128, 2006.  [25] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, Cali-  fornia Institute of Technology, 2007.  [26] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Trans- actions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http: //www.csie.ntu.edu.tw/˜cjlin/libsvm.  [27] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR: A  library for large linear classiﬁcation. Journal of Machine Learning Research, 9:1871–1874, 2008.  9  ","We present an algorithm that learns representations which explicitlycompensate for domain mismatch and which can be efficiently realized as linearclassifiers. Specifically, we form a linear transformation that maps featuresfrom the target (test) domain to the source (training) domain as part oftraining the classifier. We optimize both the transformation and classifierparameters jointly, and introduce an efficient cost function based onmisclassification loss. Our method combines several features previouslyunavailable in a single algorithm: multi-class adaptation throughrepresentation learning, ability to map across heterogeneous feature spaces,and scalability to large datasets. We present experiments on several imagedatasets that demonstrate improved accuracy and computational advantagescompared to previous approaches."
1301.2811,2013,Cutting Recursive Autoencoder Trees  ,"['Christian Scheible', 'Hinrich Schuetze']",https://arxiv.org/pdf/1301.2811.pdf,"3 1 0 2    r p A 6 2         ] L C . s c [      3 v 1 1 8 2  .  1 0 3 1 : v i X r a  Cutting Recursive Autoencoder Trees  Christian Scheible  Institute for Natural Language Processing  University of Stuttgart, Germany  scheibcn@ims.uni-stuttgart.de  Hinrich Sch¨utze  Center for Information and Language Processing  University of Munich, Germany  Abstract  Deep Learning models enjoy considerable success in Natural Language Process- ing. While deep architectures produce useful representations that lead to improve- ments in various tasks, they are often difﬁcult to interpret. This makes the analy- sis of learned structures particularly difﬁcult. In this paper, we rely on empirical tests to see whether a particular structure makes sense. We present an analysis of the Semi-Supervised Recursive Autoencoder, a well-known model that produces structural representations of text. We show that for certain tasks, the structure of the autoencoder can be signiﬁcantly reduced without loss of classiﬁcation accu- racy and we evaluate the produced structures using human judgment.  1  Introduction  Deep Learning (DL) approaches are gaining more and more attention in Natural Language Pro- cessing. Many NLP tasks have been addressed: syntactic parsing (Socher et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Deselaers et al., 2009), and document classiﬁcation (Glorot et al., 2011). One important issue in applying DL lies in the structure of language. DL originated in image pro- cessing where the neighborhood of two entities, e.g. pixels, has a straightforward interpretation through spatial relations. In NLP, neighborhood relations are not always interpretable easily and don’t necessarily translate into a semantic relationship. Often, long distance dependencies hinder the use of locally sensitive models. A frequently cited DL model for NLP tasks is a model introduced by (Socher et al., 2011), the Semi-Supervised Recursive Autoencoder (RAE). This model aims for a compositional approach, representing each word as a vector and recursively applying an autoencoder function to unify pairs of nodes, yielding a tree. One appealing property of RAEs is that the resulting structure lends itself to a syntactic or even semantic interpretation. However, so far no in-depth analysis of RAEs in terms of these structures has been performed. Our main interest in this paper is to analyze the behavior of RAEs. We will investigate the following key questions: (i) Are the structures produced by RAEs interpretable syntactically or semantically? (ii) Can the structures be simpliﬁed? We will analyze these issues empirically in a sentiment classiﬁcation task that RAE has previously been used for (Socher et al., 2011). We introduce two methods for analysis. First, we try to simplify the RAE structures automatically and evaluate the resulting models on a classiﬁcation task. Second, we let humans rate the structures according to syntactic and semantic criteria.  1  In Section 2, we describe RAEs, particularly highlighting some details regarding implementation. We then turn to different ways of structural simpliﬁcation in Section 3. Section 4 introduces the task we use for evaluation. Section 5 contains error analysis of RAEs conducted by human annotators. In Section 6 we carry out the experiments on structural simpliﬁcations.  2 Semi-Supervised Recursive Autoencoders  The central model in this paper is the Semi-Supervised Recursive Autoencoder (RAE) (Socher et al., 2011). This section describes this model and discusses some important implementation details. The RAE is a structural model. It recursively applies a neural network, the autoencoder, to construct a tree structure over the words in a sentence. Each word is represented as a vector which is inde- pendent of the context in which the word occurs. In addition to the usual autoencoder objective of reconstruction, the representation at each node is also used to predict the class of the whole sentence by applying a softmax neural network to the nodes. The basic representations of words are randomly initialized vectors of dimensionality h, stored in a matrix W where every row represents one word. This representation is enhanced using an embed- ding matrix L which is optimized during training. The ﬁnal representation of a word indexed by n is obtained by Wn + Ln. This representation serves as the base representation for tree construction.1 First, we assume the words to be the leaf nodes of the tree. Trees are then constructed by iteratively joining two adjacent nodes using an autoencoder which consists of two layers. The encoding layer takes two nodes n1 and n2 and outputs a combined representation r:  Subsequently, the reconstruction layer tries to reproduce the original inputs:  r = f (A1[n1; n2] + b1)  [n1, n2] = A2 r + b2  f is a non-linear function. Ax is the weight matrix, bx the bias for the respective layer. Note that the dimensionality of r needs to be the same as of n1 and n2 so that the autoencoder can be applied recursively. The resulting output of the autoencoder (again of dimensionality h) serves as the representation of a new node that has the two encoded nodes as its children. The combination operation is carried out greedily by autoencoding the pair of nodes ﬁrst that has minimal reconstruction error Erec. Each node output is then used to predict the sentence label individually using a softmax layer:  c = softmax(Al r),  where Al is a weight matrix and r the representation of a node. The representations have an inﬂuence on both reconstruction and classiﬁcation. Therefore, there are two objectives to be minimized: the reconstruction error Erec that speciﬁes how well the resulting node represents the two children, and a classiﬁcation error Ecl that measures how well the correct label of the sentence can be predicted from the information at the node. Erec is the Euclidean distance between the original and reconstructed nodes. Ecl is the cross-entropy error between the correct label and the output of a softmax layer that is trained. In particular, the embedding matrix L is optimized by calculating the classiﬁcation errors over all words in the training set. For batch optimization arithmetic means of the errors ¯Erec and ¯Ecl and the corresponding gradients are calculated over all nodes. Higher-order nodes are penalized in favor of leaf nodes with a factor β. Erec and ¯Ecl are added with weight α, 1 − α. The model parameters are optimized with L-BFGS. After the autoencoder is trained, a feature extraction step follows. Following Socher et al. (2011), we performed this step differently from the way the autoencoder is optimized. First, we recursively  1Some of these details are best described in the RAE implementation available at http://www.socher.  org.  2  (cid:80)  apply the aforementioned greedy autoencoder to build a tree for each sentence. In contrast to the RAE training, where the errors of individual nodes are averaged, we ﬁrst calculate the arithmetic mean of all node features to get a single feature for the tree. Let f1 . . . + fn be the features of the n nodes in a tree. Feature extraction returns ¯f = as the tree’s representation. Finally, a softmax neural network is trained on this representation. Taking the mean of all nodes resembles convolution operations which have been successfully applied in NLP previously, e.g. by Collobert et al. (2011) who calculate the maximum over the dimensions of their representation vectors. The original implementation (Socher et al., 2011) also uses the top feature separately. We leave this feature out in our experiments as it did not improve the results signiﬁcantly. The RAE has several parameters that need to be set. First, the weighting of reconstruction and classiﬁcation error α. Second, the penalty of higher-order node errors versus leaf-level errors β. In addition, all activation matrices vectors are regularized with the L2 norm. We adopt the parameter settings used in (Socher et al., 2011).  i fi n  3 Structural simpliﬁcations  In a complex model like the RAE it is difﬁcult to see which components are responsible for the results it achieves. In order to analyze these structures, we try to simplify them automatically. Evaluating the model on a task will then show us whether the structure omitted made a contribution. In the following sections we will present three ways of structural simpliﬁcation which we will apply to the RAE trees.  3.1 Tree level cuts  As a ﬁrst, simple approach to determine the inﬂuence of higher level nodes in the tree, we simply remove nodes from the representation. One straightforward operation that achieves this is a level cut. We count levels starting at the leaves, the basic units for the RAE. All terminal nodes t are deﬁned to have level l(t) = 1, and each non-terminal n with children (cid:104)c1, c2(cid:105) has the level In practice, we compute the full tree and then prune away all nodes l(n) = max(l(c1), l(c2)). that have a level higher than lmax. We call this a (tree) level cut.  3.2 Subtree selection  Another approach to simpliﬁcation follows from the idea that not every word is important for sen- timent classiﬁcation, but rather that there is a region in the sentence that is sufﬁcient to recognize sentiment (cf. (Tu et al., 2012)). A good example is the tree in Figure 1(b). In order to predict the correct sentiment of the sentence, it is sufﬁcient to analyze the second clause (“it never took off and always seemed static”). This way of simpliﬁcation is orthogonal to level cuts. Level cuts reduce the amount of structure induced by the autoencoder but keep the complete input. Subtree selection reduces the input, but it makes use of and is based on the full tree representation. In order to select a region, we greedily select a central word: We apply the softmax function of the autoencoder to each word in the sequence and pick the one with the lowest Ecl as the central word. For training examples, we compute the error for the gold class. For testing examples, we compute the score for all classes and select the word with the overall minimal error. Starting from this point, we select the largest subtree of the tree produced by the RAE whose top node n has a level of at most lmax.  3.3 Window selection  Related to this is the window approach. Here, we again identify a central word as shown before and take the representations of all words within a window w to either side as input to the classiﬁer, where the central word is the left- or rightmost word, respectively. All other words are ignored. For example w = 3 means that we take the two words to the left and to the right of the central word and drop everything else; and w = 1 only uses the central word and no context. In this approach, no tree structures are used, only the embeddings.  3  4 Task  We use the same task and data set for our RAE investigation as (Socher et al., 2011): sentiment classiﬁcation for the sentence polarity dataset by Pang and Lee (2005). It contains 10,662 sentences from movie reviews that were manually labeled as expressing positive or negative sentiment towards the movie. We use the implementation provided at http://www.socher.org. We set the hidden layer dimensionality to h = 50. All experiments are carried out on a random 90/10 training- testing split. Accuracy is used as the evaluation measure since the class distribution in the dataset is balanced.  5 Error Analysis  Error analysis proves to be difﬁcult for automatically generated representations. In general, the di- mensions produced by autoencoding in NLP applications cannot be interpreted easily. Therefore, we resort to empirical evaluation in the context of our task. In this section, we will provide analyses conducted by human annotators of two properties of the trees: syntactic and semantic composition- ality.  5.1 Syntactic Coherence  Naturally, it is tempting to think of RAE tree structures in terms of syntactic analysis. In this section, we will show that there is a large divergence between traditional syntactic trees as most theories of grammar would posit and the trees produced by RAE. We analyze two phenomena: coordinating conjunctions and negation. While coordinations are notoriously difﬁcult even in supervised prob- lems, negations are less problematic (Collins, 1999; McDonald, 2006). We asked 2 humans to judge whether the parse of 10 randomly selected examples for each of the phenomena was correct with respect to the phenomenon. None of the 20 parses (10 for each of the two phenomena) were unani- mously determined to be correct by the human judges. The example trees in Figure 1 illustrate these results. We will ﬁrst take a look at examples for negation. In sentence 1(a), the autoencoder used not at a low level in the tree, constituting a mod- iﬁcation of a. Their joint representation is itself joined with bad. The correct analysis would use not as a modiﬁer to a joint structure a bad journey where bad and journey are combined directly. Sentences 1(b) and 1(c) represent cases where the autoencoder introduced long distances between the negating and the negated phrase (never to took off and not to describe). We now turn to coordinations. In sentence 1(b), we ﬁnd an instance of a coordination of two clauses. The clause always seemed static should receive a joint analysis and should then be modiﬁed by and. Instead, and is put in a subtree containing two words from each of the coordinated clauses. The underlying reason for the resulting structures may actually arise through the property of greed- iness. As RAEs are trained greedily by joining the least error-prone combinations ﬁrst, pairings of frequent words are common. For example, in sentence 1(a), frequent words are joined ﬁrst (not to a, and all to .). The most uncommon words are added last (bad and journey). In around 75% of all occurrences, periods are adjoined directly to their neighbors, which is not desirable from a syntactic point of view.  5.2 Semantic Coherence  We will now analyze the behavior of RAEs from a semantic point of view. Sentiment analysis, our example task, relies heavily on semantic composition. Usually, sentiment is only carried by a small number of expressions, for example adjectives like great or awful in combination with their close syntactic environment (Tu et al., 2012). The sentiment of an expression can then be modiﬁed by applying an intensifying or reversing construction. A simple check for whether RAEs are able to learn compositionality is to check the produced trees for instances of these modiﬁers and see whether they behave as expected. Intensiﬁers such as very or little are difﬁcult to analyze as there is no straightforwardly quantiﬁable result that one would expect to occur, especially in the case of discrete labels as opposed to continu-  4  (a) Sentence 1: not a bad journey at all .  (b) Sentence 2: though everything might be literate and smart , it never took off and always seemed static .  (c) Sentence 3: the *UNKNOWN* elaborate continuation of “ the lord of the rings ” trilogy is so huge that a column of words can not adequately describe *UNKNOWN* peter *UNKNOWN* ’s expanded vision of *UN- KNOWN* . r . r . *UNKNOWN* ’s *UNKNOWN* .  Figure 1: Example trees. Leaf nodes contain the word that is encoded by them. All nodes contain the sentiment predicted using softmax, 1 being positive and 0 negative sentiment. We follow (Socher et al., 2011) and replace words for which no embedding is available by “*UNKNOWN*”.  5  lmax/w extract  1 2 3 5 7 10 15 20 ∞  77.30 75.98 75.61 74.86 76.08 76.17 76.75 76.75 76.75  train+ extract 77.67 68.95 54.60 52.60 54.04 62.02 63.23 63.60 77.24  no- embed 58.07 62.89 66.98 69.14 72.20 74.11 77.39 77.49 77.49  subtree  window  25.33 25.33 26.17 27.67 55.25 71.58 77.02 76.74 77.20  25.33 30.96 73.55 74.20 74.86 75.61 77.20 77.20 77.21  Table 1: Accuracy using different structure reduction techniques.  ous sentiment scores. Therefore, we only consider reversers in this analysis. There is no consensus as to which words constitute the set of reversers. Often, reversing properties are context-dependent (Polanyi and Zaenen, 2006). We therefore picked a small set of reversers with general applicability: not/n’t, no, and never. To check whether reversal occurs in a tree, we ﬁrst calculate the classiﬁcation decision by evaluating the softmax decision function at each node. We search the trees for occurrences of any reverser and check whether its sibling and its parent node are assigned opposite classes, which should be the case if the reverser was correctly applied. We ﬁnd that reversal happens only in around 31% of all reverser occurrences. Of course, since reversing is context-dependent, we need human input to verify this result. From all trees containing reversers, we randomly selected 3 trees in which the reverser reverses sentiment and 7 trees in which the reverser does not reverse sentiment (the goal being to mimick the 31% rate of reversals). We asked 2 human judges to check the examples for correct reversal. The judges unanimously found that only 3 out of 10 candidates are behaving correctly, conﬁrming that reversal is very likely not a property captured correctly by the RAE model. We again turn to Figure 1 for examples of errors. In sentence 1(a), not does not reverse the polarity of a – which is correct. However, modifying bad with the resulting structure still does not reverse. The polarity of the whole sentence seems to be determined by the polarity of journey which gets reversed at the top node, leading to misclassiﬁcation. In sentence 1(b), never should reverse took off, yielding a negative sentiment overall. However at the point where the two phrases are joined, the topmost node depicted, positive sentiment is predicted. We reiterate that the effect of reversers is quite complex and in many contexts – e.g., “not awesome, but pretty good” – they do not simply reverse sentiment. However, the examples seem to show that the syntactic and semantic role of reversers is not modeled well in thoses cases where they act as simple reversers.  6 Automatic structural simpliﬁcation  In the previous section we showed that the structures produced by RAEs cannot be easily inter- preted in terms of traditional linguistic categories from syntax and semantics. We will now turn to empirically evaluating the contributions of these structures in a practical classiﬁcation task.  6.1 Level cuts during feature extraction  In the ﬁrst experiment, we train the autoencoders to produce full trees and only apply level cuts in the feature extraction. We report accuracies in Table 1, column extract for different values of lmax. The trees produced by the RAE on the whole dataset have a mean height of 10 and a maximum height of 23.  6  First note that the best accuracy is achieved by cutting directly above the leaves, i.e. using no trees at all. Increasing the maximum level lowers accuracy signiﬁcantly; e.g. using lmax = 5, we lose over 2 percentage points compared to lmax = 0. Only when lmax is increased further, accuracy recovers. This result suggests that the leaf representations – the embeddings – carry great weight in classiﬁcation. In order to demonstrate the signiﬁcance of the embeddings, we again train full RAE trees but resort to the random representations during feature extraction. The results of this run are shown in Table 1, column noembed. Naturally, using random representations only we achieve low accuracy. Note that the results are still over chance level, an effect which may be caused by the random representations being sim- ilar to low-dimensional random indexing (Kanerva et al., 2000). Nevertheless, using higher-level tree representations successively increases accuracy to a similar level as observed in the previous experiment. Our interpretation of this experiment is that while the trees seem to be able to create useful represen- tations of the underlying words, these representations are redundant with respect to the embeddings. Combining both does not lead to improved classiﬁcation accuracy.  6.2 Level cuts during training  One could argue that using a fully-trained RAE to extract pruned trees is unfair since the model is still able to use the full information induced during training. Taking the level cut approach one step further, we also cut the trees during training, using the same maximum level. Column train+extract in Table 1 shows the results for this experiment. First, we observe that we get a well-performing model if the maximum level is 1 in both RAE training and feature extraction. This is not surprising as we are not using the node-combining part of the RAE at all which makes this particular model equal to the one with maximum level 1 in the previous experiment. Next, we can see that accuracy drops quickly as we introduce more levels and only recovers after raising the threshold to ∞, using full trees. A possible explanation for this phenomenon is that when enforcing low levels there are also fewer training instances for the RAE and thus the resulting models are worse. Another possibility is that when full trees are constructed, all applications of the RAE depend on each other since errors are propagated through the structure. Thus, inconsistencies should be optimized away. However, there are fewer inconsistencies in lower- level cuts since the resulting subtrees are likely to be disconnected. These experiments show that the best accuracy is achieved by a model that does not use the tree structures at all. Our conclusion from this evidence is that the strength of the RAE lies in the embeddings, not in the induced tree structure.  6.3 Subtree selection  We now turn to subtree selection. As stated previously, sentiment is a local phenomenon, so it might be sufﬁcient to use part of a sentence to classify the data. Table 1, column sub shows the results for this experiment. We observe low accuracies for low lmax. Only when using large contexts (recall that the maximum number of levels is around 23), the results become competitive. From this experiment, it is not clear whether the height of the trees or the size of the contexts – which grows with the height – is responsible for the gain. We will investigate this issue in the following section.  6.4 Window selection  As a last experiment, we concentrate on the embeddings as they seem to be sufﬁcient to achieve high accuracies on this task. This will also show whether subtrees or embedded words were responsible for the improvements with increasing tree height in the previous section. We vary the window size w starting from 0 which is only the word itself, to ∞ which is the maximum number of words. The data has a mean sentence length of around 21 words and a maximum sentence length of 63 words. Results are shown in Table 1, column win.  7  While a small window size (0 or 1) produces bad results – which might be an effect of choosing the wrong most conﬁdent word resulting in strong overﬁtting – vast improvements are visible soon. Taking a window of 15 words (in each direction) is sufﬁcient. There are around 16% of the data that have more than 31 words, so it seems that for sentences of that length, there are no context effects that the model can exploit.  6.5 Discussion  We presented multiple experiments in which we simpliﬁed the RAE tree structures. All of these experiments point towards the embedding having the most inﬂuence on the end result. If embeddings are not used, accuracy drops almost to chance level. Using full trees and no embeddings seem to have the same effect as using only embeddings. However, using both representations together does not yield any improvement. This suggests that there is a large overlap between what the trees model and what the embeddings model.  7 Conclusion  In this paper, we conducted two different experiments concerning the structures learned and gener- ated by Semi-Supervised Recursive Autoencoders. First, we automatically reduced the structure in different ways and showed that on our sentiment analysis task the embedded words were sufﬁcient to achieve state-of-the-art accuracy. Our experi- ments on window selection suggest that a structure as simple as a well-chosen subset of the words in a sentence produces a good model. In a human evaluation, we showed that there is no simple way to interpret the structures produced by RAEs in terms of traditional linguistic categories of syntax and semantics. Overall, we conclude that structural simpliﬁcations are possible at least for a sentiment analysis task.  Acknowledgements  This work was supported by the Deutsche Forschungsgemeinschaft through the Sonderforschungs- bereich 732.  References Collins, M., 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, Univer-  sity of Pennsylvania.  Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P., Nov. 2011. Natural  language processing (almost) from scratch. J. Mach. Learn. Res. 999888, 2493–2537.  Deselaers, T., Hasan, S., Bender, O., Ney, H., 2009. A deep learning approach to machine translit- eration. In: Proceedings of the Fourth Workshop on Statistical Machine Translation. StatMT ’09. Association for Computational Linguistics, Stroudsburg, PA, USA, pp. 233–241.  Glorot, X., Bordes, A., Bengio, Y., June 2011. Domain adaptation for large-scale sentiment classi- ﬁcation: A deep learning approach. In: Getoor, L., Scheffer, T. (Eds.), Proceedings of the 28th International Conference on Machine Learning (ICML-11). ICML ’11. ACM, New York, NY, USA, pp. 513–520.  Kanerva, P., Kristoferson, J., Holst, A., 2000. Random indexing of text samples for latent semantic analysis. In: In Proceedings of the 22nd Annual Conference of the Cognitive Science Society. Erlbaum, pp. 103–6.  McDonald, R., 2006. Discriminative learning and spanning tree algorithms for dependency parsing.  Ph.D. thesis, University of Pennsylvania.  Pang, B., Lee, L., June 2005. Seeing stars: Exploiting class relationships for sentiment categoriza- tion with respect to rating scales. In: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05). Association for Computational Linguistics, Ann Arbor, Michigan, pp. 115–124.  8  Polanyi, L., Zaenen, A., 2006. Contextual valence shifters. Computing attitude and affect in text:  Theory and applications, 1–10.  Socher, R., Manning, C., Ng, A., 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In: Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.  Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., Manning, C. D., 2011. Semi-Supervised Re- cursive Autoencoders for Predicting Sentiment Distributions. In: Proceedings of the 2011 Con- ference on Empirical Methods in Natural Language Processing (EMNLP).  Tu, Z., He, Y., Foster, J., van Genabith, J., Liu, Q., Lin, S., 2012. Identifying high-impact sub- structures for convolution kernels in document-level sentiment classiﬁcation. In: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2. ACL ’12. Association for Computational Linguistics, Stroudsburg, PA, USA, pp. 338–343.  9  ","Deep Learning models enjoy considerable success in Natural LanguageProcessing. While deep architectures produce useful representations that leadto improvements in various tasks, they are often difficult to interpret. Thismakes the analysis of learned structures particularly difficult. In this paper,we rely on empirical tests to see whether a particular structure makes sense.We present an analysis of the Semi-Supervised Recursive Autoencoder, awell-known model that produces structural representations of text. We show thatfor certain tasks, the structure of the autoencoder can be significantlyreduced without loss of classification accuracy and we evaluate the producedstructures using human judgment."
1301.3577,2013,Saturating Auto-Encoder  ,"['Ross Goroshin', 'Yann LeCun']",https://arxiv.org/pdf/1301.3577.pdf,"Saturating Auto-Encoders  Courant Institute of Mathematical Science  Courant Institute of Mathematical Science  Rostislav Goroshin ∗  New York University  goroshin@cs.nyu.edu  Yann LeCun  New York University yann@cs.nyu.edu  3 1 0 2    r a     M 0 2      ]  G L . s c [      3 v 7 7 5 3  .  1 0 3 1 : v i X r a  Abstract  We introduce a simple new regularizer for auto-encoders whose hidden-unit ac- tivation functions contain at least one zero-gradient (saturated) region. This reg- ularizer explicitly encourages activations in the saturated region(s) of the corre- sponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE’s ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.  1  Introduction  An auto-encoder is a conceptually simple neural network used for obtaining useful data rep- resentations through unsupervised training. It is composed of an encoder which outputs a hidden (or latent) representation and a decoder which attempts to reconstruct the input using the hidden representation as its input. Training consists of minimizing a reconstruction cost such as L2 error. However this cost is merely a proxy for the true objective: to obtain a useful latent representation. Auto-encoders can implement many dimensionality reduction techniques such as PCA and Sparse Coding (SC) [5] [6] [7]. This makes the study of auto-encoders very appealing from a theoretical standpoint. In recent years, renewed interest in auto-encoders net- works has mainly been due to their empirical success in unsupervised feature learning [1] [2] [3] [4].  When minimizing only reconstruction cost, the standard auto-encoder does not typically learn any meaningful hidden representation of the data. Well known theoretical and experimental results show that a linear auto-encoder with trainable encoding and decoding matrices, W e and W d re- spectively, learns the identity function if W e and W d are full rank or over-complete. The linear auto-encoder learns the principle variance directions (PCA) if W e and W d are rank deﬁcient [5]. It has been observed that other representations can be obtained by regularizing the latent repre- sentation. This approach is exempliﬁed by the Contractive and Sparse Auto-Encoders [3] [1] [2]. Intuitively, an auto-encoder with limited capacity will focus its resources on reconstructing portions of the input space in which data samples occur most frequently. From an energy based perspective, auto-encoders achieve low reconstruction cost in portions of the input space with high data density (recently, [8] has examined this perspective in depth). If the data occupies some low dimensional manifold in the higher dimensional input space then minimizing reconstruction error achieves low energy on this manifold. Useful latent state regularizers raise the energy of points that do not lie on the manifold, thus playing an analogous role to minimizing the partition function in maximum likelihood models. In this work we introduce a new type of regularizer that does this explicitly for  ∗The authors thank Joan Bruna and David Eigen for their useful suggestions and comments.  1  auto-encoders with a non-linearity that contains at least one ﬂat (zero gradient) region. We show ex- amples where this regularizer and the choice of nonlinearity determine the feature set that is learned by the auto-encoder.  2 Latent State Regularization  Several auto-encoder variants which regularize their latent states have been proposed, they include the sparse auto-encoder and the contractive auto-encoder [1] [2] [3]. The sparse auto-encoder in- cludes an over-complete basis in the encoder and imposes a sparsity inducing (usually L1) penalty on the hidden activations. This penalty prevents the auto-encoder from learning to reconstruct all possible points in the input space and focuses the expressive power of the auto-encoder on repre- senting the data-manifold. Similarly, the contractive auto-encoder avoids trivial solutions by intro- ducing an auxiliary penalty which measures the square Frobenius norm of the Jacobian of the latent representation with respect to the inputs. This encourages a constant latent representation except around training samples where it is counteracted by the reconstruction term. It has been noted in [3] that these two approaches are strongly related. The contractive auto-encoder explicitly encourages small entries in the Jacobian, whereas the sparse auto-encoder is encouraged to produce mostly zero (sparse) activations which can be designed to correspond to mostly ﬂat regions of the nonlinearity, thus also yielding small entries in the Jacobian.  2.1 Saturating Auto-Encoder through Complementary Nonlinearities  Our goal is to introduce a simple new regularizer which explicitly raises reconstruction error for inputs not near the data manifold. Consider activation functions with at least one ﬂat region; these include shrink, rectiﬁed linear, and saturated linear (Figure 1). Auto-encoders with such nonlineari- ties lose their ability to accurately reconstruct inputs which produce activations in the zero-gradient regions of their activation functions. Let us denote the auto-encoding function xr = G(x, W ), x being the input, W the trainable parameters in the auto-encoder, and xr the reconstruction. One can deﬁne an energy surface through the reconstruction error:  EW (x) = ||x − G(x, W )||2  Let’s imagine that G has been trained to produce a low reconstruction error at a particular data point x∗. If G is constant when x varies along a particular direction v, then the energy will grow quadratically along that particular direction as x moves away from x∗. If G is trained to produce low reconstruction errors on a set of samples while being subject to a regularizer that tries to make it constant in as many directions as possible, then the reconstruction energy will act as a contrast function that will take low values around areas of high data density and larger values everywhere else (similarly to a negative log likelihood function for a density estimator). The proposed auto-encoder is a simple implementation of this idea. Using the notation W = {W e, Be, W d, Bd}, the auto-encoder function is deﬁned as  G(x, W ) = W dF (W ex + Be) + Bd  where W e, Be, W d, and Bd are the encoding matrix, encoding bias, decoding matrix, and decoding bias, respectively, and F is the vector function that applies the scalar function f to each of its components. f will be designed to have ”ﬂat spots”, i.e. regions where the derivative is zero (also referred to as the saturation region). The loss function minimized by training is the sum of the reconstruction energy EW (x) = ||x − G(x, W )||2 and a term that pushes the components of W ex + Be towards the ﬂat spots of f. This is performed through the use of a complementary function fc, associated with the non-linearity f (z). The basic idea is to design fc(z) so that its value corresponds to the distance of z to one of the ﬂat spots of f (z). Minimizing fc(z) will push z towards the ﬂat spots of f (z). With this in mind, i ) which encourages the argument to be in the saturation regime of the activation function (f). We refer to auto-encoders which include this regularizer as Saturating Auto-Encoders (SATAEs). For activation functions with zero-gradient regime(s) the complementary nonlinearity (fc) can be deﬁned as the distance to the nearest saturation region. Speciﬁcally, let S = {z | f(cid:48)(z) = 0} then we deﬁne fc(z) as:  we introduce a penalty of the form fc((cid:80)d  ijxj + be  j=1 W e  2  Figure 1: Three nonlinearities (top) with their associated complementary regularization func- tions(bottom).  fc(z) = inf z(cid:48)∈S  |z − z(cid:48)|.  (1)  Figure 1 shows three activation functions and their associated complementary nonlinearities. The complete loss to be minimized by a SATAE with nonlinearity f is:  (cid:88)  x∈D  1 2  L =  (cid:107)x −(cid:0)W dF (W ex + Be) + Bd(cid:1)(cid:107)2 + α  dh(cid:88)  i=1  fc(W e  i x + be  i ),  (2)  where dh denotes the number of hidden units. The hyper-parameter α regulates the trade-off between reconstruction and saturation.  3 Effect of the Saturation Regularizer  We will examine the effect of the saturation regularizer on auto-encoders with a variety of activation functions. It will be shown that the choice of activation function is a signiﬁcant factor in determining the type of basis the SATAE learns. First, we will present results on toy data in two dimensions followed by results on higher dimensional image data.  3.1 Visualizing the Energy Landscape  Given a trained auto-encoder the reconstruction error can be evaluated for a given input x. For low-dimensional spaces (Rn, where n ≤ 3) we can evaluate the reconstruction error on a regular grid in order to visualize the portions of the space which are well represented by the auto-encoder. 2(cid:107)x − xr(cid:107)2 for all x within some bounded region of More speciﬁcally we can compute E(x) = 1 the input space. Ideally, the reconstruction energy will be low for all x which are in the training set and high elsewhere. Figures 2 and 3 depict the resulting reconstruction energy for inputs x ∈ R2, and −1 ≤ xi ≤ 1. Black corresponds to low reconstruction energy. The training data consists of a one dimensional manifold shown overlain in yellow. Figure 2 shows a toy example for a SATAE which uses ten basis vectors and a shrink activation function. Note that adding the saturation regu- larizer decreases the volume of the space which is well reconstructed, however good reconstruction is maintained on or near the training data manifold. The auto-encoder in Figure 3 contains two  3  Figure 2: Energy surfaces for unregularized (left), and regularized (right) solutions obtained on SATAE-shrink and 10 basis vectors. Black corresponds to low reconstruction energy. Training points lie on a one-dimensional manifold shown in yellow.  Figure 3: SATAE-SL toy example with two basis elements. Top Row: three randomly initialized so- lutions obtained with no regularization. Bottom Row: three randomly initialized solutions obtained with regularization.  encoding basis vectors (red), two decoding basis vectors (green), and uses a saturated-linear activa- tion function. The encoding and decoding bases are unconstrained. The unregularized auto-encoder learns an orthogonal basis with a random orientation. The region of the space which is well recon- structed corresponds to the outer product of the linear regions of two activation functions; beyond that the error increases quadratically with the distance. Including the saturation regularizer induces the auto-encoder basis to align with the data and to operate in the saturation regime at the extreme points of the training data, which limits the space which is well reconstructed. Note that because the encoding and decoding weights are separate and unrestricted, the encoding weights were scaled up to effectively reduce the width of the linear regime of the nonlinearity.  3.2 SATAE-shrink  Consider a SATAE with a shrink activation function and shrink parameter λ. The corresponding complementary nonlinearity, derived using Equation 1 is given by:  (cid:26)abs(x), |x| > λ  0, elsewhere  .  shrinkc(x) =  Note that shrinkc(W ex + be) = abs(shrink(W ex + be)), which corresponds to an L1 penalty on the activations. Thus this SATAE is equivalent to a sparse auto-encoder with a shrink activation function. Given the equivalence to the sparse auto-encoder we anticipate the same scale ambiguity which occurs with L1 regularization. This ambiguity can be avoided by normalizing the decoder weights to unit norm. It is expected that the SATAE-shrink will learn similar features to those obtained with a sparse auto-encoder, and indeed this is what we observe. Figure 4(c) shows the decoder ﬁlters learned by an auto-encoder with shrink nonlinearity trained on gray-scale natural image patches. One can recognize the expected Gabor-like features when the saturation penalty is activated. When trained on the binary MNIST dataset the learned basis is comprised of portions of digits and strokes. Nearly identical results are obtained with a SATAE which uses a rectiﬁed-linear  4  activation function. This is because a rectiﬁed-linear function with an encoding bias behaves as a positive only shrink function, similarly the complementary function is equivalent to a positive only L1 penalty on the activations.  3.3 SATAE-saturated-linear  Unlike the SATAE-shrink, which tries to compress the data by minimizing the number of active elements; the SATAE saturated-linear (SATAE-SL) tries to compress the data by encouraging the latent code to be as close to binary as possible. Without a saturation penalty this auto-encoder learns to encode small groups of neighboring pixels. More precisely, the auto-encoder learns the identity function on all datasets. An example of such a basis is shown in Figure 4(b). With this basis the auto-encoder can perfectly reconstruct any input by producing small activations which stay within the linear region of the nonlinearity. Introducing the saturation penalty does not have any effect when training on binary MNIST. This is because the scaled identity basis is a global minimizer of Equation 2 for the SATAE-SL on any binary dataset. Such a basis can perfectly reconstruct any binary input while operating exclusively in the saturated regions of the activation function, thus incurring no saturation penalty. On the other hand, introducing the saturation penalty when training on natural image patches induces the SATAE-SL to learn a more varied basis (Figure 4(d)).  3.4 Experiments on CIFAR-10  SATAE auto-encoders with 100 and 300 basis elements were trained on the CIFAR-10 dataset, which contains small color images of objects from ten categories. In all of our experiments the auto- encoders were trained by progressively increasing the saturation penalty (details are provided in the next section). This allowed us to visually track the effect of the saturation penalty on individual basis elements. Figure 4(e)-(f) shows the basis learned by SATAE-shrink with small and large saturation penalty, respectively. Increasing the saturation penalty has the expected effect of reducing the number of nonzero activations. As the saturation penalty increases, active basis elements become responsible for reconstructing a larger portion of the input. This induces the basis elements to become less spatially localized. This effect can be seen by comparing corresponding ﬁlters in Figure 4(e) and (f). Figures 4(g)-(h) show the basis elements learned by SATAE-SL with small and large saturation penalty, respectively. The basis learned by SATAE-SL with a small saturation penalty resembles the identity basis, as expected (see previous subsection). Once the saturation penalty is increased small activations become more heavily penalized. To increase their activations the encoding basis elements may increase in magnitude or align themselves with the input. However, if the encoding and decoding weights are tied (or ﬁxed in magnitude) then reconstruction error would increase if the weights were merely scaled up. Thus the basis elements are forced to align with the data in a way that also facilitates reconstruction. This effect is illustrated in Figure 5 where ﬁlters corresponding to progressively larger values of the regularization parameter are shown. The top half of the ﬁgure shows how an element from the identity basis (α = 0.1) transforms to a localized edge (α = 0.5). The bottom half of the ﬁgure shows how a localized edge (α = 0.5) progressively transforms to a template of a horse (α = 1).  4 Experimental Details  Because the regularizer explicitly encourages activations in the zero gradient regime of the nonlin- earity, many encoder basis elements would not be updated via back-propagation through the non- linearity if the saturation penalty were large. In order to allow the basis elements to deviate from their initial random states we found it necessary to progressively increase the saturation penalty. In our experiments the weights obtained at a minimum of Equation 2 for a smaller value of α were used to initialize the optimization for a larger value of α. Typically, the optimization began with α = 0 and was progressively increased to α = 1 in steps of 0.1. The auto-encoder was trained for 30 epochs at each value of α. This approach also allowed us to track the evolution of basis elements as a function of α (Figure 5). In all experiments data samples were normalized by subtracting the mean and dividing by the standard deviation of the dataset. The auto-encoders used to obtain the results shown in Figure 4 (a),(c)-(f) used 100 basis elements, others used 300 basis elements. In- creasing the number of elements in the basis did not have a strong qualitative effect except to make the features represented by the basis more localized. The decoder basis elements of the SATAEs with  5  (a)  (b)  (c)  (d)  (e)  (f)  (g)  (h)  Figure 4: Basis elements learned by the SATAE using different nonlinearities on: 28x28 binary MNIST digits, 12x12 gray scale natural image patches, and CIFAR-10. (a) SATAE-shrink trained on MNIST, (b) SATAE-saturated-linear trained on MNIST, (c) SATAE-shrink trained on natural image patches, (d) SATAE-saturated-linear trained on natural image patches, (e)-(f) SATAE-shrink trained on CIFAR-10 with α = 0.1 and α = 0.5, respectively, (g)-(h) SATAE-SL trained on CIFAR-10 with α = 0.1 and α = 0.6, respectively.  6  Figure 5: Evolution of two ﬁlters with increasing saturation regularization for a SATAE-SL trained on CIFAR-10. Filters corresponding to larger values of α were initialized using the ﬁlter corre- sponding to the previous α. The regularization parameter was varied from 0.1 to 0.5 (left to right) in the top ﬁve images and 0.5 to 1 in the bottom ﬁve  shrink and rectiﬁed-linear nonlinearities were reprojected to the unit sphere after every 10 stochastic gradient updates. The SATAEs which used saturated-linear activation function were trained with tied weights. All results presented were obtained using stochastic gradient descent with a constant learning rate of 0.05.  5 Discussion  In this work we have introduced a general and conceptually simple latent state regularizer. It was demonstrated that a variety of feature sets can be obtained using a single framework. The utility of these features depend on the application. In this section we extend the deﬁnition of the saturation regularizer to include functions without a zero-gradient region. The relationship of SATAEs with other regularized auto-encoders will be discussed. We conclude with a discussion on future work.  5.1 Extension to Differentiable Functions  We would like to extend the saturation penalty deﬁnition (Equation 1) to differentiable functions without a zero-gradient region. An appealing ﬁrst guess for the complimentary function is some positive function of the ﬁrst derivative, fc(x) = |f(cid:48)(x)| for instance. This may be an appropriate choice for monotonic activation functions which have their lowest gradient regions at the extrema (e.g. sigmoids). However some activation functions may contain regions of small or zero gradient which have negligible extent, at the extrema for instance. We would like our deﬁnition of the com- plimentary function to not only measure the local gradient in some region, but to also measure it’s extent. For this purpose we employ the concept of average variation over a ﬁnite interval. We deﬁne the average variation of f at x in the positive and negative directions at scale l, respectively as:  (cid:90) x+l (cid:90) x  x  x−l  |f(cid:48)(u)|du = |f(cid:48)(x)| ∗ Π+  l (x)  |f(cid:48)(u)|du = |f(cid:48)(x)| ∗ Π−  l (x).  ∆+  l f (x) = ∆− l f (x) =  1 l 1 l  Where ∗ denotes the continuous convolution operator. Π+ l (x) are uniform averaging kernels in the positive and negative directions, respectively. Next, deﬁne a directional measure of variation of f by integrating the average variation at all scales.  l (x) and Π−  7  Figure 6: Illustration of the complimentary function (fc) as deﬁned by Equation 3 for a non- monotonic activation function (f). The absolute derivative of f is shown for comparison.  (cid:90) +∞ (cid:90) +∞  0  0  ∆+  l f (x)w(l)dl =  ∆− l f (x)w(l)dl =  (cid:20)(cid:90) +∞ (cid:20)(cid:90) +∞  0  0  M +f (x) =  M−f (x) =  (cid:21) (cid:21)  w(l)Π+  l (x)dl  w(l)Π−  l (x)dl  ∗ |f(cid:48)(x)|  ∗ |f(cid:48)(x)|.  Where w(l) is chosen to be a sufﬁciently fast decreasing function of l to insure convergence of the integral. The integral with which |f(cid:48)(x)| is convolved in the above equation evaluates to some decreasing function of x for Π+ with support x ≥ 0. Similarly, the integral involving Π− evaluates to some increasing function of x with support x ≤ 0. This function will depend on w(l). The functions M +f (x) and M−f (x) measure the average variation of f (x) at all scales l in the positive and negative direction, respectively. We deﬁne the complimentary function fc(x) as:  fc(x) = min(M +f (x), M−f (x)).  (3)  An example of a complimentary function deﬁned using the above formulation is shown in Figure 6. Whereas |f(cid:48)(x)| is minimized at the extrema of f, the complimentary function only plateaus at these locations.  5.2 Relationship with the Contractive Auto-Encoder  Let hi be the output of the ith hidden unit of a single-layer auto-encoder with point-wise nonlinearity f (·). The regularizer imposed by the contractive auto-encoder (CAE) can be expressed as follows:  (cid:88)  (cid:18) ∂hi  (cid:19)2  ∂xj  ij  =  dh(cid:88)  f(cid:48)(  d(cid:88)  i  j=1   ,  ijxj + bi)2(cid:107)W e i (cid:107)2 W e  where x is a d-dimensional data vector, f(cid:48)(·) is the derivative of f (·), bi is the bias of the ith encoding unit, and W e i denotes the ith row of the encoding weight matrix. The ﬁrst term in the above equation tries to adjust the weights so as to push the activations into the low gradient (saturation) regime of the nonlinearity, but is only deﬁned for differentiable activation functions. Therefore the CAE indirectly encourages operation in the saturation regime. Computing the Jacobian, however, can be cumbersome for deep networks. Furthermore, the complexity of computing the Jacobian is O(d × dh), although a more efﬁcient implementation is possible [3], compared to the O(dh) for the saturation penalty.  8  5.3 Relationship with the Sparse Auto-Encoder  In Section 3.2 it was shown that SATAEs with shrink or rectiﬁed-linear activation functions are equivalent to a sparse auto-encoder. Interestingly, the fact that the saturation penalty happens to correspond to L1 regularization in the case of SATAE-shrink agrees with the ﬁndings in [7]. In their efforts to ﬁnd an architecture to approximate inference in sparse coding, Gregor et al. found that the shrink function is particularly compatible with L1 minimization. Equivalence to sparsity only for some activation functions suggests that SATAEs are a generalization of sparse auto-encoders. Like the sparsity penalty, the saturation penalty can be applied at any point in a deep network for the same computational cost. However, unlike the sparsity penalty the saturation penalty is adapted to the nonlinearity of the particular layer to which it is applied.  5.4 Future Work  We intend to experimentally demonstrate that the representations learned by SATAEs are useful as features for learning common tasks such as classiﬁcation and denoising. We will also address several open questions, namely: (i) how to select (or learn) the width parameter (λ) of the nonlinearity, and (ii) how to methodically constrain the weights. We will also explore SATAEs that use a wider class of non-linearities and architectures.  References [1] Marc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra and Yann LeCun. Efﬁcient Learn- ing of Sparse Representations with an Energy- Based Model, in J. Platt et al. (Eds), Advances in Neural Information Processing Systems (NIPS 2006), 19, MIT Press, 2006.  [2] Marc’Aurelio Ranzato, Fu-Jie Huang, Y-Lan Boureau and Yann LeCun: Unsupervised Learn- ing of Invariant Feature Hierarchies with Applications to Object Recognition, Proc. Computer Vision and Pattern Recognition Conference (CVPR’07), IEEE Press, 2007  [3] Rifai, S. and Vincent, P. and Muller, X. and Glorot, X. and Bengio, Y. Contractive auto-encoders: Explicit invariance during feature extraction, Proceedings of the Twenty-eight International Conference on Machine Learning, ICML 2011  [4] P. Vincent, H. Larochelle, Y. Bengio, P.A. Manzagol. Extracting and Composing Robust Fea- tures with Denoising Autoencoders Proceedings of the 25th International Conference on Ma- chine Learning (ICML’2008), 2008.  [5] R.O. Duda, P.E. Hart, and D.G. Stork, Pattern Classiﬁcation, New York: John Wiley & Sons,  2001, pp. xx + 654, ISBN: 0-471-05669-3  [6] Olhausen, Bruno A.; Field, David J. (1997). Sparse Coding with an Overcomplete Basis Set: A  Strategy Employed by V1?. Vision Research 37 (23): 3311-3325.  [7] Karol Gregor and Yann LeCun: Learning Fast Approximations of Sparse Coding, Proc. Inter-  national Conference on Machine learning (ICML’10), 2010  [8] Guillaume Alain and Yoshua Bengio, What Regularized Auto-Encoders Learn from the Data  Generating Distribution. arXiv:1211.4246v3 [cs.LG]  9  ","We introduce a simple new regularizer for auto-encoders whose hidden-unitactivation functions contain at least one zero-gradient (saturated) region.This regularizer explicitly encourages activations in the saturated region(s)of the corresponding activation function. We call these SaturatingAuto-Encoders (SATAE). We show that the saturation regularizer explicitlylimits the SATAE's ability to reconstruct inputs which are not near the datamanifold. Furthermore, we show that a wide variety of features can be learnedwhen different activation functions are used. Finally, connections areestablished with the Contractive and Sparse Auto-Encoders."
1301.3611,2013,Jitter-Adaptive Dictionary Learning - Application to Multi-Trial Neuroelectric Signals  ,"['Sebastian Hitziger', 'Maureen Clerc', 'Alexandre Gramfort', 'Sandrine Saillet', 'Christian Bénar', 'Théodore Papadopoulo']",https://arxiv.org/pdf/1301.3611.pdf,"3 1 0 2     n u J    4 2      ] L M  . t a t s [      4 v 1 1 6 3  .  1 0 3 1 : v i X r a  Jitter-Adaptive Dictionary Learning - Application to  Multi-Trial Neuroelectric Signals  Sebastian Hitziger Project-Team Athena  Maureen Clerc  Project-Team Athena  INRIA Sophia Antipolis, France  sebastian.hitziger@inria.fr  INRIA Sophia Antipolis, France maureen.clerc@inria.fr  Alexandre Gramfort  Institut Mines-Telecom, Telecom ParisTech, CNRS LTCI alexandre.gramfort@telecom-paristech.fr  Sandrine Saillet  Christian B´enar  Institut de Neurosciences des Syst`emes  Institut de Neurosciences des Syst`emes  UMR 1106 INSERM  Aix-Marseille Universit´e  UMR 1106 INSERM  Aix-Marseille Universit´e  Facult´e de M´edecine La Timone  Facult´e de M´edecine La Timone  Marseille, France  Marseille, France  ssaillet53@gmail.com  christian.benar@univmed.fr  Th´eodore Papadopoulo  Project-Team Athena  INRIA Sophia Antipolis, France  theodore.papadopoulo@inria.fr  Abstract  Dictionary Learning has proven to be a powerful tool for many image processing tasks, where atoms are typically deﬁned on small image patches. As a drawback, the dictionary only encodes basic structures. In addition, this approach treats patches of different locations in one single set, which means a loss of informa- tion when features are well-aligned across signals. This is the case, for instance, in multi-trial magneto- or electroencephalography (M/EEG). Learning the dictio- nary on the entire signals could make use of the alignment and reveal higher-level features. In this case, however, small misalignments or phase variations of fea- tures would not be compensated for. In this paper, we propose an extension to the common dictionary learning framework to overcome these limitations by allowing atoms to adapt their position across signals. The method is validated on simulated and real neuroelectric data.  1 Introduction  The analysis of electromagnetic signals induced by brain activity requires sophisticated tools capable of efﬁciently treating redundant, multivariate datasets. Redundancy originates for example from the spatial dimension as in multi-channel magneto- or electroencephalography (M/EEG). It can also  1  result from repetitions of the same phenomenon, for instance when a stimulus is presented multiple times to a subject (typically between 50 and 500 times) and the neuronal response is recorded; we will refer to this case as multi-trial analysis.  In the case of multi-channel data, principal component analysis (PCA) (Pearson, 1901) and inde- pendent component analysis (ICA) (Comon, 1994) have been successfully used to decompose the data into a few waveforms, providing insight into the underlying neuronal activity and allowing to enhance the often poor signal-to-noise-ratio (Lagerlund et al., 1997; Makeig et al., 1996). They both use the fact that the data of all channels are recorded synchronically such that features appear well-aligned and phase-locked.  This condition typically does not hold for multi-trial analysis though. In (Woody, 1967) a method is provided to compensate for temporal jitter across signals, but it assumes a single underlying waveform. Matching pursuit (MP) algorithms (Mallat & Zhang, 1993; Durka & Blinowska, 1995) in turn allow to extract several different features and have recently been adapted to deal with multi- channel (Durka et al., 2005; Gribonval, 2003) as well as multi-trial (B´enar et al., 2009) M/EEG data by compensating for different types of variability. However, these methods only allow to extract waveforms that have previously been deﬁned in a dictionary.  In the ﬁeld of image processing, learning dictionaries directly from the data has shown to yield state- of-the-art results in several applications (Elad & Aharon, 2006; Mairal et al., 2008). Typically these dictionaries are learned on small patches and represent the basic structures of the images, e.g. edges. When using this technique for neuroelectric multi-trial analysis though, the framework should be carefully adapted to the properties of the data: (i) the waveforms of interest occur approximately at the same time across trials; (ii) however, they may have slightly different time delays (of a small fraction of the signal lenth) or phase variations; (iii) the noise-level is high, partially due to neuronal background activity which is non-Gaussian; (iv) data is often limited to a few hundred signals. The latter two properties make the analysis a difﬁcult problem; it is therefore necessary to incorporate all prior information about the data, in particular (i) and (ii). We note that similar properties can be found in many other signal processing applications, such as in other bioelectric or biomagnetic data (e.g. ECG, EMG).  We suggest that atoms should be learned on the entirety of the signals to provide global high-level features. The common dictionary learning formulation as a matrix factorization problem, however, cannot compensate for the time delays (ii). Existing extensions known as convolutional or shift- invariant sparse coding (SISC) (Smith & Lewicki, 2005; Blumensath & Davies, 2006; Grosse et al., 2007; Ekanadham et al., 2011) learn atoms that are typically smaller than the signal and can occur at arbitrary and possibly multiple positions per signal. This framework is very general for our purpose and does not make use of property (i), the approximate alignment of waveforms. In addition, the SISC framework leads to a highly complex algorithm since all shifts of all atoms are considered in the optimization. The sparse coding step is therefore often handled by heuristic preselection of the active atoms, as described in (Blumensath & Davies, 2006). But the update of the dictionary elements is also a difﬁcult task, as it involves solving a convolutional problem (Grosse et al., 2007).  In this paper, we present a novel dictionary learning framework that is designed speciﬁcally to compensate for small temporal jitter of waveforms across trials, leading to the name jitter-adaptive dictionary learning (JADL). In contrast to SISC, atoms learned by JADL are deﬁned on the entire signal domain and are allowed to shift only up to a small fraction of the signal length. The most important difference, however, is a constraint for atoms to occur at most once (i.e. in one position) per signal, see section 3.2.1. On the one hand, this constraint is reasonable since we do not want to encode a waveform with multiple slightly shifted copies of one atom. On the other hand, it signiﬁcantly reduces complexity compared to SISC.  An important difference to previous dictionary learning frameworks is the size of the dictionary; while for image processing dictionaries are often overcomplete, JADL aims at learning only a small number of atoms. This is not only desired for easy interpretability, but also because of the difﬁculties introduced by (iii) and (iv), that make it infeasible to learn a large number of atoms. The “unrolled” version of the dictionary, i.e. the set of all allowed shifts of all atoms may still be large; it therefore makes sense to speak of sparse solutions with respect to this unrolled dictionary. However, JADL enforces sparsity to a major part by the explicit constraint mentioned; sparse regularization only plays a minor role.  2  We begin by brieﬂy stating the common dictionary learning problem after which we will present the theory and implementation details of JADL. Finally, JADL is evaluated on synthetic and experimen- tal data.  2 Dictionary learning: prior art  A dictionary consists of a matrix D ∈ RN ×K that contains for columns the N -dimensional column vectors {di}K j=1 the problem of ﬁnding a sparse code aj ∈ RK for each xj can be formulated as the following minimization problem:  i=1, its atoms. For a set of signals {xj ∈ RN }M  aj = argmin aj ∈RK  1 2  kxj − Daj k2  2 + λkajk1 ,  (1)  where k · k1 denotes the l1-norm and λ > 0 is a regularization parameter. This problem is known as Lasso (Tibshirani, 1996) and can be solved efﬁciently with algorithms such as least angle regression (LARS) (Efron et al., 2004) or the proximal methods ISTA (Combettes & Wajs, 2005) and its accelerated version FISTA (Beck & Teboulle, 2009). The case where D is not known beforehand but shall be estimated given the signals {xj}, leads to the dictionary learning problem. It is a minimization problem over both the dictionary and the sparse code, which reads:  1 2  MX  j=1  (cid:16)kxj − Daj k2  2 + λ kajk1(cid:17) ,  kdik2 = 1,  i = 1, . . . , K,  min D,aj  s.t.  (2)  where the latter constraint prevents atoms from growing arbitrarily large.  Most algorithms tackle this non-convex problem iteratively by alternating between the convex sub- problems: (i) the sparse coding (with D ﬁxed) and (ii) the dictionary update ({aj}M j=1 ﬁxed). The ﬁrst such algorithm was provided in the pioneer work on dictionary learning in (Olshausen & Field, 1997). Many alternatives have been proposed, such as the method of optimal directions (MOD) in (Engan et al., 1999), K-SVD (Aharon et al., 2006), or more recently an online version (Mairal et al., 2010) to handle large datasets.  3 Jitter-adaptive dictionary learning  This section introduces the main contribution of this paper: a novel technique designed to overcome the limitations of purely linear signal decomposition methods such as PCA, ICA, and dictionary learning.  We suppose that atoms present in a signal can suffer from unknown time delays, which we will refer to as jitter. This type of variability addresses the issue of varying latencies of transient features as well as oscillations with different phases across trials.  This issue is very important for interpretation of M/EEG data, to answer fundamental questions such as the link between evoked responses and oscillatory activity (Hanslmayer et al., 2007; Mazaheri & Jensen, 2008), the correlation between single-trial activity and behavioral data (Jung et al., 2001), the variability of evoked potentials such as the P300 (Holm et al., 2006). Cross-trial variability is also a precious source of information for simultaneous EEG-fMRI interpretation (B´enar et al., 2007).  We therefore provide a dictionary learning framework in which atoms may adapt their position across trials. This framework can be generalized to address other kinds of variability; the shift operator introduced below can simply be replaced by the desired operators. The entire framework remains the same, only the formula for the update of the dictionary needs to be adapted as described in Section 3.2.2.  3  3.1 Model and problem statement  Our model is based on the hypothesis that the set of signals of interest {xj}M a dictionary D = {di}K ∆ 1 , for every j there exist coefﬁcients aij ∈ R and shift operators δij ∈ ∆, such that  j=1 can be generated by i=1 with few atoms K in the following way: Given a set of shift operations  xj =  KX  i=1  aijδij (di) .  (3)  We assume that ∆ contains only small shifts relative to the size of the time window. Now, we can formulate the jitter-adaptive dictionary learning problem    1  2  MX  j=1  min  di,aij ,δij  s.t.  kdik2 = 1,  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  xj −  KX  i=1  aij δij(di)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  2  2  + λ kajk1    ,  (4)  δij ∈ ∆,  i = 1, . . . , K,  j = 1, . . . , M.  Note that for ∆ = {I} this reduces to Eq. (2), the problem is thus also non-convex and we solve it similarly using alternate minimizations.  3.2 Implementation  The algorithm that we propose for solving Eq. (4) is based on an implementation in (Mairal et al., 2010) for common dictionary learning, which iteratively alternates between (i) sparse coding and (ii) dictionary update. We adapt the algorithm least angle regression (LARS) used for solving (i) to ﬁnd not only the coefﬁcients {aij} but also the latencies {δij}. For (ii), block coordinate descent is used. The entire procedure is summarized in Algorithm 1 (notation and details are explained in the following).  Algorithm 1 Jitter-Adaptive Dictionary Learning Require: signals {x1, x2, . . . , xM }, shift operators ∆, K ∈ N, λ ∈ R, 1: Initialize D = {d1, d2, . . . , dK} 2: repeat 3: 4: 5: 6:  Set up “unrolled” dictionary DS from D Sparse coding (solve using modiﬁed LARS) for j = 1 to M do  aS j ← argmin  1  2 (cid:13)(cid:13)xj − DS aS j(cid:13)(cid:13)2 2 + λ(cid:13)(cid:13)aS j (cid:13)(cid:13)1 ,  s.t.  end for Convert {aS Dictionary update (solve using block coordinate descent)  j } to {aij}, {δij}  7: 8: 9: 10:  D ← argmin  {di}K  i=1  MX  j=1  1 2  11: until convergence  xj −  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  KX  i=1  aijδij (di)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  2  2  ,  s.t.  kdik = 1  j (cid:13)(cid:13)(cid:13)0 (cid:13)(cid:13)(cid:13)aS,i  ≤ 1  1There are different possibilities to deﬁne these shifts. As our entire framework is valid even for arbitrary linear transforms, we do not specify the choice at this point. While circular shifts, i.e. δn(d) = dn for n ∈ N and dn[i] := d[(i − n)modN ], result in a slightly simpler formulation of the dictionary update and may have minor computational advantages, they can introduce unwanted boundary effects. Our implementation actually uses atoms deﬁned on a slightly larger domain (N + S − 1 sample points) than the signals, this way avoiding circular shifts. For the sake of simplicity, however, we here assume atoms to be deﬁned on the same domain as the signals. Although the right way to handle boundary effects can be an important question, it is out of the scope of this paper to discuss this issue in detail. In our experiments, we found the impact of the concrete deﬁnition of the δ to be small.  4  3.2.1 Sparse coding  When D is ﬁxed, the minimization Eq. (4) can be solved independently for each signal xj,  min aij ,δij  1 2  xj −  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  KX  i=1  aijδij(di)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  2  2  + λ kaj k1 .  (5)  We now rewrite this problem into a form similar to the Lasso, which allows us to solve it using a modiﬁcation of LARS. Let us ﬁrst deﬁne an “unrolled” version of the dictionary containing all possible shifts of all its atoms; this is given by DS = {δ(d) : d ∈ D, δ ∈ ∆}, a matrix of dimension N × KS, where S = |∆| is the number of allowed shifts. The decomposition in Eq. (5) can now be rewritten as a linear combination over the unrolled dictionary  KX  i=1  aijδij(di) = DS aS j ,  j ∈ RKS denotes the corresponding coefﬁcient vector. This vector is extremely sparse; where aS in fact, each subvector aS,i j that contains the coefﬁcients corresponding to the shifts of atom di shall maximally have one non-zero entry. If such a non-zero entry exists, its position indicates which shift was used for atom di. Now Eq. (5) can be rewritten as  of aS  j  aS j ← argmin  s.t.  1  2 (cid:13)(cid:13)xj − DS aS j (cid:13)(cid:13)2 2 + λ(cid:13)(cid:13)aS j (cid:13)(cid:13)1 , j (cid:13)(cid:13)(cid:13)0 (cid:13)(cid:13)(cid:13)aS,i  i = 1, . . . , K.  ≤ 1,  Clearly, Eq. (6) is the Lasso, but the constraint (7) leads to a non-convex problem. Therefore the modiﬁcation of the LARS that we propose below only guarantees convergence to a local minimum.  The LARS algorithm (Efron et al., 2004) follows a stepwise procedure; in each step the coefﬁcient of one atom is selected to change from “inactive” to “active” (i.e. it changes from zero to non-zero) or vice versa. In order to enforce the constraint (7), we make the following modiﬁcation. When a coefﬁcient is selected for activation, we determine the index i such that this coefﬁcient lies in aS,i . We then block all other coefﬁcients contained in the subvector aS,i such that they cannot get j j activated in a later step. In the same manner, we unblock all entries of aS,i j when its active coefﬁcient is deactivated.  As mentioned in the introduction, the constraint (7), which is the main difference to SISC, helps to reduce the complexity of the optimization. In fact, each time an atom is activated, all its translates are blocked and do not need to be considered in the following steps, which facilitates the calculation. In addition, maximally K steps have to be performed (given that no atom is deactivated in a later step, which we observed to occur rarely). As suggested for example in (Grosse et al., 2007) the inital correlations of the shifted atoms with the signal can be computed using fast convolution via FFT, which speeds up computation in the case of a large number of shifts S.  3.2.2 Dictionary update  For the dictionary update, its unrolled version cannot be used as this would result in updating differ- ent shifts of the same atom in different ways. Instead, the shifts have to be explicitly included in the update process. We use block coordinate descent to iteratively solve the constrained minimization problem  dk = argmin  xj −  ,  s.t.  kdkk2 = 1  for each atom dk. This can be solved in two steps, the solution of the unconstrained problem by differentiation followed by normalization. This is summarized by  (6)  (7)  (8)  KX  i=1  2  2  aijδij (di)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)  xj −X  i6=k  akj δ−1 kj  aijδij (di)  ,  dk  j=1  1 2  MX  (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) MX fdk = dk = fdk kfdkk2  j=1  .  5  Atom 1  Atom 2  Atom 3  Signal 1  Signal 2  Signal 3  l  a n g i r  i  O  A C P  L D  L D A J  ρ  1  2  atom  3  ¯E     1  2  atom  3     PCA DL JADL  n a e c  l  s u o i r u p s  i  y s o n  A C P  L D  L D A J  0  2  time [s]  4  0  2  time [s]  4  0  2  time [s]  4  Figure 1: Original dictionary and reconstruc- tions with PCA, DL, and JADL, respectively; row 5: similarity ρ and average energy across signals ¯E for each atom, the dashed line marks the value 1 in both plots.  Figure 2: Clean, noisy and denoised signals, row 1: clean signals; row 2: signals plus spu- rious events; row 3: signals above plus white noise, row 4-6: denoised signals using PCA, DL, and JADL, respectively.  with δ−1 the opposite shift of δ. As in (Mairal et al., 2010), we found that one update loop through all of the atoms was enough to ensure fast convergence of Algorithm 1.  The only difference of this update compared to common dictionary learning are the shift opera- tors δij. In contrast to the sparse coding step, the jitter adaptivity therefore does not increase the complexity for this step. When high efﬁciency of the algorithm is needed, this fact can be used by employing mini-batch or online techniques (Mairal et al., 2010), which increase the frequency of dictionary update steps with respect to sparse coding steps.  We note that in Eq. (8) we assumed the shifts to be circular; being orthogonal transforms, i.e. In the case of non-circular shifts or other δδt = I, they provide for a simple update formula. linear operators, the inverse δ−1 kj. In kj  in the update Eq. (8) needs to be replaced by the adjoint δt kj )−1 has to be applied to the update term.  addition, the rescaling function ψ = (PM  kj δkjδt  j=1 a2  3.2.3 Hyperparameters and initial dictionary  As mentioned before, the optimal number K of atoms will typically be lower than for dictionary learning in image processing frameworks; this is due to the high redundancy of the electromagnetic brain signals as well as the adaptive property of the atoms. When oscillatory activity is sought, ∆ should contain shifts of up to the largest period expected to be found. The choice of λ plays a less important role than in common dictionary learning; most of the sparsity is induced directly by the constraint (7).  The choice of the initial dictionary can be crucial for the outcome, due to the non-convex nature of the problem. Especially when the initial dictionary already produces a small sparse coding error, the algorithm may converge to a local minimum that is very close to the initialization. While this property allows us to incorporate a priori knowledge by initializing with a predeﬁned dictionary, this also provides the risk of preventing the algorithm from learning interesting features if they are “far” from the initial dictionary.  When no dictionary is given a priori, initializations often found in the literature include random values as in (Olshausen & Field, 1997) as well as random examples (Aharon et al., 2006) or linear combinations (e.g. PCA) of the input signals. In order to keep the bias towards the initialization as small as possible, we favor random initializations independent from the signals.  6  4 Experiments  Jitter-adaptive dictionary learning (JADL) is next evaluated on synthetic and on experimental data. Its performance is compared to results obtained with principal component analysis (PCA)(Pearson, 1901) and dictionary learning (DL); for the latter we used the open-source software package SPAMS2 whose implementation is described in (Mairal et al., 2010).  4.1 Synthetic data  Table 1: First three rows: average similarity ¯ρ of origi- nal and reconstructed atoms; last two rows: parameter λ used for reconstruction.  K=3 K=4 K=5 K=6 K=8 K=10 K=12 ¯ρ PCA 0.522 0.522 0.522 0.522 0.522 0.522 0.522 ¯ρ DL 0.563 0.566 0.598 0.615 0.589 0.595 0.581 ¯ρ JADL 0.955 0.954 0.946 0.911 0.931 0.881 0.801 λ DL 0.2 0.001 0.005 0.001 0.001 0.005 0.050 λ JADL 0.001 0.005 0.005 0.01 0.005 0.1 0.4  Generating the dictionary: First, a dic- tionary D with K = 3 normalized atoms was deﬁned, as shown in Figure 1 (ﬁrst row). The ﬁrst atom is a delta-shaped spike and the other two are oscillatory features. The length of the time window was chosen as 4 seconds and the sampling rate as 128 Hz, hence N = 512. Generating the signals: From the dictio- nary, M = 200 signals were generated ac- cording to the model Eq. (3). The coefﬁ- cients aij and shifts δij were drawn independently from Gaussian distributions with mean µ = 1 and standard deviation σ = 0.3 for the coefﬁcients and µ = 0, σ = 0.2 s for the shifts. Three examples are shown in the ﬁrst row of Figure 2. These signals were then corrupted by two types of noise: (i) to every signal between 0 and 3 oscillatory events were added, their amplitudes, frequencies, temporal supports and positions in time were drawn randomly (Fig. 2, row 2); (ii) then white Gaussian noise with a resulting average SNR (energy of clean signals/energy of noise) of 0.790 was added (row 3). Reconstructions: Performance of the three methods PCA, DL, and JADL given the noisy signals (Fig. 2, row 3) was measured on both their ability of recovering the original dictionary and the denoising of the signals. We performed reconstruction for different dictionary sizes K. For DL and JADL we chose the λ that gave the best results; we noticed relatively small sensitivity of the methods to the choice of λ, especially for small values of K. Recovering the dictionary: For PCA, the dictionary atoms were deﬁned as the ﬁrst K principal components. For JADL, the set ∆ was deﬁned to contain all time shifts of maximally ±0.6 seconds, resulting in S = 128 Hz · 1.2 s ≈ 154 allowed shifts. For each reconstructed dictionary, the three atoms that showed the highest similarity ρ to the original atoms were used to calculate the average similarity ¯ρ. ρ was deﬁned as the maximal correlation of the reconstructed atom and all shifts of maximally ±0.6 seconds of the original atom. The values ¯ρ are shown for PCA, DL, and JADL in Table 1 for each K (for K > 12 we observed decreasing similarity values for all three methods). The similarity for JADL is signiﬁcantly higher than for PCA and DL; its optimal value is obtained for K = 3 atoms, but it remains high for larger K, showing its robustness to overestimation of original atoms. Note that dictionaries obtained by PCA for different values of K always have their ﬁrst atoms in common, hence the constant similarity values.  K=1 K=2 K=3 K=4 K=5 K=6 K=8 K=10 K=12 ǫ PCA 0.871 0.750 0.638 0.539 0.522 0.508 0.502 0.537 0.570 ǫ DL 0.869 0.747 0.635 0.535 0.515 0.498 0.487 0.505 0.539 ǫ JADL 0.505 0.283 0.214 0.230 0.277 0.284 0.317 0.325 0.330 λ DL 0.1 λ JADL 0.05 0.5  For each method, the K giv- ing the highest similarity was determined and the three atoms with largest ρ-values of the corresponding dictionaries are shown in Figure 1 (row 2 - 4). The atoms found by PCA and DL contain only mixtures of the oscillatory atoms, the spike does not appear at all. JADL succeeds in separating the three atoms; their shapes and average energy across signals ¯E are very close to the originals, as shown in the bar plots.  Table 2: Relative l2-error ǫ produced by each method for different K; the last two rows show the values of λ used for DL and JADL.  0.05 0.05 0.05 0.05 0.05 0.05 0.3  0.1 0.4  0.1 0.5  0.2  0.1  0.2  0.2  2http://spams-devel.gforge.inria.fr/  7  Epoch 21  Epoch 101  Epoch 161  Evoked potential  0.1  0  −0.1  −0.2  −4  −2  0  time [s]  2  4  −4  −2  0  time [s]  2  4  −4  −2  0  time [s]  2  4  −4  −2  0  time [s]  2  4  Figure 3: Different epochs of the local ﬁeld potential (LFP) showing spikes with decreasing energy; the evoked potential (last) is the average over all epochs.  Atom 1  Atom 2  Atom 3  Atom 4  Atom 5  Av. energy  0.2  0.1  0  −0.1  −0.2  0.2  0.1  0  −0.1  −0.2  0.2  0.1  0  −0.1  −0.2     A C P     L D     L D A J  −2 0 2 time [s]  −2 0 2 time [s]  −2 0 2 time [s]  −2 0 2 time [s]  −2 0 2 time [s]  1 2 3 4 5  atoms  Figure 4: Dictionaries learned on LFP epochs using PCA, DL, and JADL.  1  0.5  0  1  0.5  0  1  0.5  0  Denoising the signals: For PCA, denoising was performed by setting the coefﬁcients of all but the ﬁrst K components to zero. For DL and JADL, the noisy signals were encoded over the learned dictionaries according to Eq. (1) and (5), respectively. Table 2 shows the average relative l2-errors ǫ of all denoised signals with respect to the original ones. For each method, three denoised signals for optimal K are shown in the last three rows of Figure 2. Despite the larger dictionary size (K = 8) in the case of PCA and DL, the spike could not be reconstructed due to its jitter across signals. In addition, the locations of the oscillatory events in the signals denoised with JADL correspond better to their true locations than it is the case for PCA and DL, especially in the last signal shown. Finally, the white noise appears more reduced for JADL, which is due to its smaller dictionary size. All three methods succeeded in removing the spurious events from the signals. 4.2 Real data  Local ﬁeld potentials (LFP) were recorded with an intra-cranial electrode in a Wistar-Han rat, in an animal model of epilepsy. Bicuculline (a blocker of inhibition) was injected in the cortex in order to elicit epileptic-like discharges. Discharges were selected visually on the LFP traces, and the data was segmented in 169 epochs centered around the spikes. To simplify the analysis, the epochs were scaled with a constant factor such that the maximal l2-energy among epochs was equal to 1. Three examples of epochs are shown in Figure 3, as well as the evoked potential, measured as the average over the epochs. The only structure visible to the eye is a spike with changing shape and decreasing energy across epochs. Learning the dictionary: Five normalized atoms were learned on the data with PCA, DL and JADL; see Figure 4. They were ordered in descending order of their average energy across epochs.  All three methods produce for their ﬁrst atom a spike that resembles the evoked potential (Figure 3); also, all methods reveal an oscillatory artifact around 1.2 Hz which is not visible in the evoked potential. However, while the oscillations in the PCA and DL dictionaries are encoded in two atoms  8  Absolute coefficient values  Latency distributions  s m o  t  a  1  2  3  4  5        0.8  0.6  0.4  0.2  0  50  100  epochs  150  −0.3 −0.2 −0.1  0  0.1  0.2  0.3  latency [s]  ] s [   e m  i t  50  40  30  20  10     0 0  Calculation time     K=3 K=5 K=7 K=10 K=15  100  S  200  300  (a) Absolute coefﬁcients of atoms learned with JADL.  (b) Distribution of latencies for each atom.  (c) Computation times for differ- ent values of S and K.  Figure 5: Code visualization and computation times.  (4 and 5) that differ mostly in phase, they are concentrated in atom 5 for JADL. Additionally, in the case of JADL the oscillations are almost completely separated from the spike, only a small remainder is still visible. This shows the need of PCA and DL for several atoms to compensate for phase shifts while JADL is able to associate oscillations with different phases; moreover JADL makes use of the varying phases to clearly separate transient from oscillatory events. In addition, we can observe a smoothing effect in the case of PCA and DL: the oscillations look very much like sinusoids whereas atom 5 in JADL shows a less smooth, rather spiky structure. Interpreting the code: We visualized the coefﬁcients and the shifts obtained by decomposing all the epochs over the dictionary learned with JADL, see Figure 5. Interestingly, each of the ﬁrst three spikes in the dictionary is active during a contiguous set of epochs during which the other atoms are hardly used. This allows to segment the epochs into three sets of different spike shapes. The fourth atom is only active during a few epochs where the dominant atom is changing from the ﬁrst to the second spike. The oscillatory artifact (atom 5) in contrast shows very low but constant activity across all epochs.  The latency distributions (Fig. 5(b)) can provide further insight into the data. The highly peaked distribution for the ﬁrst atoms gives evidence of the accurate alignment of the spikes. The last atom shows shifts in the whole range from −0.4 to 0.4 seconds, indicating that the phases of the oscillations were uniformly distributed across epochs. Computation times: The dictionary and the code above were obtained for 189 iterations of JADL after which the algorithm converged. The time of computation on a laptop (Intel Core CPU, 2.4 GHz) was 4.3 seconds. As JADL is designed for datasets of similar complexity to the one investi- gated here, computation time should not be an issue for ofﬂine analysis. However, applications such as M/EEG based brain computer interfaces (BCI) may require computations in real time. We remark that our implementation has not yet been optimized and could be speeded up signiﬁcantly by paral- lelization or online techniques mentioned in section 3.2.2. If training data is available beforehand, the dictionary may also be calculated ofﬂine in advance; the sparse encoding of new data over this dictionary then only takes up a small fraction of the training time and can be performed online.  Figure 5(c) illustrates the effect of changing the number of atoms K and shifts S on computation time t; for each calculation 200 iterations were performed. We can see that t is linearly correlated with S but increases over-linearly with K: while both, S and K affect the size of the unrolled dic- tionary DS, an increase of S is handled more efﬁciently by using calculation advantages described in section 3.2.1; e.g., the non-smooth behavior of the curves at S = 40 results from the fact that for S > 40 an FFT-based convolution is used. In addition, the dictionary update step only uses the com- pact dictionary D whose size does not increase with S. If the additional factor in computation time due to S is still not acceptable, ∆ may be subsampled by introducing a minimal distance between shifts; the tradeoff is a less exact description of the atoms latencies.  9  5 Conclusion  In this paper, a novel method for the analysis and processing of multi-trial neuroelectric signals was introduced. The method was derived by extending the dictionary learning framework, allowing atoms to adapt to jitter across trials; hence the name jitter-adaptive dictionary learning (JADL). It was shown how the resulting non-convex minimization problem can be tackled by modifying existing algorithms used in common dictionary learning.  The method was validated on synthetic and experimental data, both containing variability in laten- cies of transient and in the phases of oscillatory events. The results obtained showed to be superior to those of common dictionary learning and PCA, both in recovering the underlying dictionary and in denoising the signals. The evaluation furthermore demonstrated the usefulness of JADL as a data exploration tool, capable of extracting global, high-level features of the signals and giving insight into their distributions.  Acknowledgments  This work was supported by the doctoral grant of the region Provence-Alpes-Cˆote d’Azur and the ANR grants CoAdapt (09-EMER-002-01) and MultiModel (2010 BLAN 0309 04).  References  Aharon, M., Elad, M., & Bruckstein, A. 2006. K-SVD: An Algorithm for Designing Overcom- plete Dictionaries for Sparse Representation. IEEE Transactions on Signal Processing, 54(11), 4311–4322.  Beck, A., & Teboulle, M. 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse  problems. SIAM Journal on Imaging Sciences, 2(1), 183–202.  B´enar, C.G., Sch¨on, D., Grimault, S., Nazarian, B., Burle, B., Roth, M., Badier, J.M., Marquis, P., Liegeois-Chauvel, C., & Anton, J.L. 2007. Single-trial analysis of oddball event-related potentials in simultaneous EEG-fMRI. Human Brain Mapping, 28, 602613.  B´enar, C.G., Papadopoulo, T., Torr´esani, B., & Clerc, M. 2009. Consensus matching pursuit for  multi-trial EEG signals. Journal of neuroscience methods, 180(1), 161–170.  Blumensath, Thomas, & Davies, Mike. 2006. Sparse and shift-invariant representations of music.  Audio, Speech, and Language Processing, IEEE Transactions on, 14(1), 50–57.  Combettes, P. L., & Wajs, V. R. 2005. Signal recovery by proximal forward-backward splitting.  Multiscale Modeling and Simulation, 4(4), 1168–1200.  Comon, P. 1994. Independent component analysis, a new concept? Signal processing, 36(3), 287–  314.  Durka, PJ, & Blinowska, KJ. 1995. Analysis of EEG transients by means of matching pursuit.  Annals of biomedical engineering, 23(5), 608–611.  Durka, P.J., Matysiak, A., Montes, E.M., Sosa, P.V., & Blinowska, K.J. 2005. Multichannel match-  ing pursuit and EEG inverse solutions. Journal of neuroscience methods, 148(1), 49–59.  Efron, B., Hastie, T., Johnstone, I., & Tibshirani, R. 2004. Least angle regression. The Annals of  statistics, 32(2), 407–499.  Ekanadham, Chaitanya, Tranchina, Daniel, & Simoncelli, Eero P. 2011. Sparse decomposition of transformation-invariant signals with continuous basis pursuit. Pages 4060–4063 of: Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE.  Elad, M., & Aharon, M. 2006.  Image denoising via sparse and redundant representations over  learned dictionaries. Image Processing, IEEE Transactions on, 15(12), 3736–3745.  Engan, K., Aase, S.O., & Hakon Husoy, J. 1999. Method of optimal directions for frame design. Pages 2443–2446 of: Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, vol. 5. IEEE.  Gribonval, R. 2003. Piecewise linear source separation. Pages 297–310 of: Optical Science and  Technology, SPIE’s 48th Annual Meeting. International Society for Optics and Photonics.  10  Grosse, R, Raina, R, Kwong, H, & Ng, AY. 2007. Shift-invariant sparse coding for audio classiﬁca- tion. In: Proceedings of the Twenty-third Conference on Uncertainty in Artiﬁcial Intelligence (UAI07).  Hanslmayer, S., Klimesch, W., Sauseng, P., Gruber, W., Dopplemayr, M., Freunberger, R., Pecher- storfer, T., & Birbaumer, N. 2007. Alpha phase reset contributes to the generation of ERPs. Cerebral Cortex, 17, 1–8.  Holm, A., Ranta-aho, P., Sallinen, M., Karjalainen, P., & Mller, K. 2006. Relationship of P300 single-trial responses with reaction time and preceding stimulus sequence. Int. J. Psychophys- iol.  Jung, T.P., Makeig, S., Westerﬁeld, M., Townsend, J., Courchesne, E., & Sejnowski, T.J. 2001. Analysis and Visualization of Single-Trial Event-Related Potentials. Human Brain Mapping, 14, 166–185.  Lagerlund, T.D., Sharbrough, F.W., & Busacker, N.E. 1997. Spatial ﬁltering of multichannel elec- troencephalographic recordings through principal component analysis by singular value de- composition. Journal of Clinical Neurophysiology, 14(1), 73–82.  Mairal, J., Bach, F., Ponce, J., Sapiro, G., & Zisserman, A. 2008. Supervised dictionary learning.  arXiv preprint arXiv:0809.3083.  Mairal, J., Bach, F., Ponce, J., & Sapiro, G. 2010. Online Learning for Matrix Factorization and  Sparse Coding. Journal of Machine Learning Research, 11(Aug.), 19–60.  Makeig, S., Bell, A.J., Jung, T.P., Sejnowski, T.J., et al. 1996. Independent component analysis of  electroencephalographic data. Advances in neural information processing systems, 145–151.  Mallat, S.G., & Zhang, Z. 1993. Matching pursuits with time-frequency dictionaries. Signal Pro-  cessing, IEEE Transactions on, 41(12), 3397–3415.  Mazaheri, A., & Jensen, O. 2008. Amplitude modulations of brain oscillations generate slow evoked  responses. Journal of Neuroscience, 28, 7781–7787.  Olshausen, B.A., & Field, D.J. 1997. Sparse coding with an overcomplete basis set: a strategy  employed by V1? Vision research, 37(23), 3311–25.  Pearson, K. 1901. LIII. On lines and planes of closest ﬁt to systems of points in space. The London,  Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11), 559–572.  Smith, Evan, & Lewicki, Michael S. 2005. Efﬁcient coding of time-relative structure using spikes.  Neural Computation, 17(1), 19–45.  Tibshirani, R. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical  Society. Series B (Methodological), 267–288.  Woody, C.D. 1967. Characterization of an adaptive ﬁlter for the analysis of variable latency neuro-  electric signals. Medical and Biological Engineering and Computing, 5(6), 539–554.  11  ","Dictionary Learning has proven to be a powerful tool for many imageprocessing tasks, where atoms are typically defined on small image patches. Asa drawback, the dictionary only encodes basic structures. In addition, thisapproach treats patches of different locations in one single set, which means aloss of information when features are well-aligned across signals. This is thecase, for instance, in multi-trial magneto- or electroencephalography (M/EEG).Learning the dictionary on the entire signals could make use of the alignementand reveal higher-level features. In this case, however, small missalignementsor phase variations of features would not be compensated for. In this paper, wepropose an extension to the common dictionary learning framework to overcomethese limitations by allowing atoms to adapt their position across signals. Themethod is validated on simulated and real neuroelectric data."
1301.3641,2013,Training Neural Networks with Stochastic Hessian-Free Optimization  ,['Ryan Kiros'],https://arxiv.org/pdf/1301.3641.pdf,"3 1 0 2     y a M 1         ]  G L . s c [      3 v 1 4 6 3  .  1 0 3 1 : v i X r a  Training Neural Networks with Stochastic  Hessian-Free Optimization  Ryan Kiros  Department of Computing Science  University of Alberta Edmonton, AB, Canada  rkiros@ualberta.ca  Abstract  Hessian-free (HF) optimization has been successfully used for training deep au- toencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be com- puted on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with gradient and curvature mini-batches independent of the dataset size. We modify Martens’ HF for these settings and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against over- ﬁtting. Stochastic Hessian-free optimization gives an intermediary between SGD and HF that achieves competitive performance on both classiﬁcation and deep autoencoder experiments.  1 Introduction  Stochastic gradient descent (SGD) has become the most popular algorithm for training neural net- works. Not only is SGD simple to implement but its noisy updates often leads to solutions that are well-adapt to generalization on held-out data [1]. Furthermore, SGD operates on small mini-batches potentially allowing for scalable training on large datasets. For training deep networks, SGD can be used for ﬁne-tuning after layerwise pre-training [2] which overcomes many of the difﬁculties of training deep networks. Additionally, SGD can be augmented with dropout [3] as a means of preventing overﬁtting.  There has been recent interest in second-order methods for training deep networks, partially due to the successful adaptation of Hessian-free (HF) by [4], an instance of the more general family of truncated Newton methods. Second-order methods operate in batch settings with less but more substantial weight updates. Furthermore, computing gradients and curvature information on large batches can easily be distributed across several machines. Martens’ HF was able to successfully train deep autoencoders without the use of pre-training and was later used for solving several pathological tasks in recurrent networks [5].  HF iteratively proposes update directions using the conjugate gradient algorithm, requiring only curvature-vector products and not an explicit computation of the curvature matrix. Curvature-vector products can be computed on the same order of time as it takes to compute gradients with an addi- tional forward and backward pass through the function’s computational graph [6, 7]. In this paper we exploit this property and introduce stochastic Hessian-free optimization (SHF), a variation of HF that operates on gradient and curvature mini-batches independent of the dataset size. Our goal in developing SHF is to combine the generalization advantages of SGD with second-order information from HF. SHF can adapt its behaviour through the choice of batch size and number of conjugate gradient iterations, for which its behaviour either becomes more characteristic of SGD or HF. Ad- ditionally we integrate dropout, as a means of preventing co-adaptation of feature detectors. We  1  perform experimental evaluation on both classiﬁcation and deep autoencoder tasks. For classiﬁca- tion, dropout SHF is competitive with dropout SGD on all tasks considered while for autoencoders SHF performs comparably to HF and momentum-based methods. Moreover, no tuning of learning rates needs to be done.  2 Related work  Much research has been investigated into developing adaptive learning rates or incorporating second- order information into SGD. [8] proposed augmenting SGD with a diagonal approximation of the Hessian while Adagrad [9] uses a global learning rate while dividing by the norm of previous gra- dients in its update. SGD with Adagrad was shown to be beneﬁcial in training deep distributed networks for speech and object recognition [10]. To completely avoid tuning learning rates, [11] considered computing rates as to minimize estimates of the expectation of the loss at any one time. [12] proposed SGD-QN for incorporating a quasi-Newton approximation to the Hessian into SGD and used this to win one of the 2008 PASCAL large scale learning challenge tracks. Recently, [13] provided a relationship between HF, Krylov subspace descent and natural gradient due to their use of the Gauss-Newton curvature matrix. Furthermore, [13] argue that natural gradient is robust to overﬁtting as well as the order of the training samples. Other methods incorporating the natural gradient such as TONGA [14] have also showed promise on speeding up neural network training.  Analyzing the difﬁculty of training deep networks was done by [15], proposing a weight initial- ization that demonstrates faster convergence. More recently, [16] argue that large neural networks waste capacity in the sense that adding additional units fail to reduce underﬁtting on large datasets. The authors hypothesize the SGD is the culprit and suggest exploration with stochastic natural gra- dient or stochastic second-order methods. Such results further motivate our development of SHF. [17] show that with careful attention to the parameter initialization and momentum schedule, ﬁrst- order methods can be competitive with HF for training deep autoencoders and recurrent networks. We compare against these methods in our autoencoder evaluation.  Related to our work is that of [18], who proposes a dynamic adjustment of gradient and curvature mini-batches for HF with convex losses based on variance estimations. Unlike our work, the batch sizes used are dynamic with a ﬁxed ratio and are initialized as a function of the dataset size. Other work on using second-order methods for neural networks include [19] who proposed using the Jacobi pre-conditioner for HF, [20] using HF to generate text in recurrent networks and [21] who explored training with Krylov subspace descent (KSD). Unlike HF, KSD could be used with Hessian-vector products but requires additional memory to store a basis for the Krylov subspace. L-BFGS has also been successfully used in ﬁne-tuning pre-trained deep autoencoders, convolutional networks [22] and training deep distributed networks [10]. Other developments and detailed discussion of gradient-based methods for neural networks is described in [23].  3 Hessian-free optimization  In this section we review Hessian-free optimization, largely following the implementation of Martens [4]. We refer the reader to [24] for detailed development and tips for using HF.  We consider unconstrained minimization of a function f : Rn → R with respect to parameters θ. More speciﬁcally, we assume f can be written as a composition f (θ) = L(F (θ)) where L is a convex loss function and F (θ) is the output of a neural network with ℓ non-input layers. We will mostly focus on the case when f is non-convex. Typically L is chosen to be a matching loss to a corresponding transfer function p(z) = p(F (θ)). For a single input, the (i + 1)-th layer of the network is expressed as  yi+1 = si(Wi yi + bi)  (1)  where si is a transfer function, Wi is the weights connecting layers i and i + 1 and bi is a bias vector. Common transfer functions include the sigmoid si(x) = (1 + exp(−x))−1, the hyperbolic tangent si(x) = tanh(x) and rectiﬁed linear units si(x) = max(x, 0). In the case of classiﬁcation tasks, the  2  loss function used is the generalized cross entropy and softmax transfer  L(p(z), t) = −  k  Xj=1  tj log(p(zj)),  p(zj) = exp(zj)/  k  Xl=1  exp(zl)  (2)  where k is the number of classes, t is a target vector and zj the j-th component of output vector z. Consider a local quadratic approximation Mθ(δ) of f around θ:  f (θ + δ) ≈ Mθ(δ) = f (θ) + ∇f (θ)T δ +  1 2  δT Bδ  (3)  where ∇f (θ) is the gradient of f and B is the Hessian or an approximation to the Hessian. If f was convex, then B (cid:23) 0 and equation 3 exhibits a minimum δ∗. In Newton’s method, θk+1, the k where αk ∈ [0, 1] is the rate and δ∗ parameters at iteration k + 1, are updated as θk+1 = θk + αkδ∗ k is computed as  k = −B−1∇f (θk−1) δ∗  (4)  for which calculation requires O(n3) time and thus often prohibitive. Hessian-free optimization alleviates this by using the conjugate gradient (CG) algorithm to compute an approximate minimizer δk. Speciﬁcally, CG minimizes the quadratic objective q(δ) given by  q(δ) =  1 2  δT Bδ + ∇f (θk−1)T δ  (5)  for which the corresponding minimizer of q(δ) is −B−1∇f (θk−1). The motivation for using CG is as follows: while computing B is expensive, compute the product Bv for some vector v can be computed on the same order of time as it takes to compute ∇f (θk−1) using the R-operator [6]. Thus CG can efﬁciently compute an iterative solution to the linear system Bδk = −∇(f (θk−1)) corresponding to a new update direction δk. When f is non-convex, the Hessian may not be positive semi-deﬁnite and thus equation 3 no longer has a well deﬁned minimum. Following Martens, we instead use the generalized Gauss-newton ′′ is the Hessian of L 1. So long matrix deﬁned as B = J T L as f (θ) = L(F (θ)) for convex L then B (cid:23) 0. Given a vector v, the product Bv = J T L Jv is computed successively by ﬁrst computing Jv, then L Jv) [7]. To compute Jv, we utilize the R-operator. The R-operator of F (θ) with respect to v is deﬁned as  J where J is the Jacobian of f and L  (Jv) and ﬁnally J T (L  ′′  ′′  ′′  ′′  Rv{F (θ)} = lim ǫ→0  F (θ + ǫv) − F (θ)  ǫ  = Jv  (6)  Computing Rv{F (θ)} in a neural network is easily done using a forward pass by computing Rv{yi} for each layer output yi. More speciﬁcally,  Rv{yi+1} = Rv{Wi yi + bi}s′  i = (v(Wi)yi + v(bi) + WiR{yi})s′ i  (7)  where v(Wi) is the components of v corresponding to parameters between layers i and i + 1 and R{y1} = 0 (where y1 is the input data). In order to compute J T (L Jv), we simply apply back- propagation but using the vector L Jv instead of ∇L as is usually done to compute ∇f . Thus, Bv may be computed through a forward and backward pass in the same sense that L and ∇f = J T ∇L are.  ′′  ′′  As opposed to minimizing equation 3, Martens instead uses an additional damping parameter λ with damped quadratic approximation  ˆMθ(δ) = f (θ) + ∇f (θ)T δ +  δT ˆBδ = f (θ) + ∇f (θ)T δ +  1 2  1 2  δT (B + λI)δ  (8)  Damping the quadratic through λ gives a measure of how conservative the quadratic approximation is. A large value of λ is more conservative and as λ → ∞ updates become similar to stochastic gradient descent. Alternatively, a small λ allows for more substantial parameter updates especially  1While an abuse of deﬁnition, we still refer to “curvature-vector products” and “curvature batches” even  when B is used.  3  along low curvature directions. Martens dynamically adjusts λ at each iteration using a Levenberg- Marquardt style update based on computing the reduction ratio  ρ = (f (θ + δ) − f (θ))/(Mθ(δ) − Mθ(0))  (9)  If ρ is sufﬁciently small or negative, λ is increased while if ρ is large then λ is decreased. The number of CG iterations used to compute δ has a dramatic effect on ρ which is further discussed in section 4.1.  To accelerate CG, Martens makes use of the diagonal pre-conditioner  P = (cid:20)diag(cid:18) m Xj=1  ∇f (j)(θ) ⊙ ∇f (j)(θ)(cid:19) + λI(cid:21)ξ  (10)  where f (j)(θ) is the value of f for datapoint j and ⊙ denotes component-wise multiplication. P can be easily computed on the same backward pass as computing ∇f . Finally, two backtracking methods are used: one after optimizing CG to select δ and the other a backtracking linesearch to compute the rate α. Both these methods operate in the standard way, backtracking through proposals until the objective no longer decreases.  4 Stochastic Hessian-free optimization  Martens’ implementation utilizes the full dataset for computing objective values and gradients, and mini-batches for computing curvature-vector products. Naively setting both batch sizes to be small causes several problems. In this section we describe these problems and our contributions in modi- fying Martens’ original algorithm to this setting.  4.1 Short CG runs, δ-momentum and use of mini-batches  The CG termination criteria used by Martens is based on a measure of relative progress in optimizing ˆMθ. Speciﬁcally, if xj is the solution at CG iteration j, then training is terminated when  ˆMθ(xj) − ˆMθ(xj−k)  ˆMθ(xj )  < ǫ  (11)  where k =max(10, j/10) and ǫ is a small positive constant. The effect of this stopping criteria has a dependency on the strength of the damping parameter λ, among other attributes such as the current parameter settings. For sufﬁciently large λ, CG only requires 10-20 iterations when a pre- conditioner is used. As λ decreases, more iterations are required to account for pathological curva- ture that can occur in optimizing f and thus leads to more expensive CG iterations. Such behavior would be undesirable in a stochastic setting where preference would be put towards having equal length CG iterations throughout training. To account for this, we ﬁx the number of CG iterations to be only 3-5 across training for classiﬁcation and 25-50 for training deep autoencoders. Let ζ denote this cut-off. Setting a limit on the number of CG iterations is used by [4] and [20] and also has a damping effect, since the objective function and quadratic approximation will tend to diverge as CG iterations increase [24]. We note that due to the shorter number of CG runs, the iterates from each solution are used during the CG backtracking step.  A contributor to the success of Martens’ HF is the use of information sharing across iterations. At iteration k, CG is initialized to be the previous solution of CG from iteration k − 1, with a small decay. For the rest of this work, we denote this as δ-momentum. δ-momentum helps correct proposed update directions when the quadratic approximation varies across iterations, in the same sense that momentum is used to share gradients. This momentum interpretation was ﬁrst suggested by [24] in the context of adapting HF to a setting with short CG runs. Unfortunately, the use of δ- momentum becomes challenging when short CG runs are used. Given a non-zero CG initialization, ˆMθ may be more likely to remain positive after terminating CG and assuming f (θ + δ) − f (θ) < 0, means that the reduction ratio will be negative and thus λ will be increased to compensate. While this is not necessarily unwanted behavior, having this occur too frequently will push SHF to be too conservative and possibly result in the backtracking linesearch to reject proposed updates. Our  4  solution is to utilize a schedule on the amount of decay used on the CG starting solution. This is motivated by [24] suggesting more attention on the CG decay in the setting of using short CG runs. Speciﬁcally, if δ0  k is the initial solution to CG at iteration k, then  k = γeδζ δ0  k−1, γe = min(1.01γe−1, .99) where γe is the decay at epoch e, δ0 1 = 0 and γ1 = 0.5. While in batch training a ﬁxed γ is suitable, in a stochastic setting it is unlikely that a global decay parameter is sufﬁcient. Our schedule has an annealing effect in the sense that γ values near 1 are feasible late in training even with only 3-5 CG iterations, a property that is otherwise hard to achieve. This allows us to beneﬁt from sharing more information across iterations late in training, similar to that of a typical momentum method.  (12)  A remaining question to consider is how to set the sizes of the gradient and curvature mini-batches. [24] discuss theoretical advantages to utilizing the same mini-batches for computing the gradient and curvature vector products. In our setting, this may lead to some difﬁculties. Using same-sized batches allows λ → 0 during training [24]. Unfortunately, this can become incompatible with our short hard-limit on the number of CG iterations, since CG requires more work to optimize ˆMθ when λ approaches zero. To account for this, on classiﬁcation tasks where 3-5 CG iterations are used, we opt to use gradient mini-batches that are 5-10 times larger than curvature mini-batches. For deep autoencoder tasks where more CG iterations are used, we instead set both gradient and curvature batches to be the same size. The behavior of λ is dependent on whether or not dropout is used during training. Figure 1 demonstrates the behavior of λ during classiﬁcation training with and without the use of dropout. With dropout, λ no longer converges to 0 but instead plummets, rises and ﬂattens out. In both settings, λ does not decrease substantially as to negatively effect the proposed CG solution and consequently the reduction ratio. Thus, the amount of work required by CG remains consistent late in training. The other beneﬁt to using larger gradient batches is to account for the additional computation in computing curvature-vector products which would make training longer if both mini-batches were small and of the same size. In [4], the gradients and objectives are computed using the full training set throughout the algorithm, including during CG backtracking and the backtracking linesearch. We utilize the gradient mini-batch for the current iteration in order to compute all necessary gradient and objectives throughout the algorithm.  4.2 Levenberg-Marquardt damping  Martens makes use of the following Levenberg-Marquardt style damping criteria for updating λ:  ifρ >  3 4  , λ ←  2 3  λ elseifρ <  1 4  , λ ←  3 2  λ  (13)  which given a suitable initial value will converge to zero as training progresses. We observed that the above damping criteria is too harsh in the stochastic setting in the sense that λ will frequently oscillate, which is sensible given the size of the curvature mini-batches. We instead opt for a much softer criterion, for which lambda is updated as  ifρ >  3 4  , λ ←  99 100  λ elseifρ <  1 4  , λ ←  100 99  λ  (14)  This choice, although somewhat arbitrary, is consistently effective. Thus reduction ratio values computed from curvature mini-batches will have less overall inﬂuence on the damping strength.  4.3 Integrating dropout  Dropout is a recently proposed method for improving the training of neural networks. During train- ing, each hidden unit is omitted with a probability of 0.5 along with optionally omitting input fea- tures similar to that of a denoising autoencoder [25]. Dropout can be viewed in two ways. By randomly omitting feature detectors, dropout prevents co-adaptation among detectors which can im- prove generalization accuracy on held-out data. Secondly, dropout can be seen as a type of model averaging. At test time, outgoing weights are halved. If we consider a network with a single hidden layer and k feature detectors, using the mean network at test time corresponds to taking the geomet- ric average of 2k networks with shared weights. Dropout is integrated in stochastic HF by randomly omitting feature detectors on both gradient and curvature mini-batches from the last hidden layer  5  a d b m a  l  1  0.8  0.6  0.4  0.2  0   0     100  200  Epoch  300  400  500  a d b m a  l  1  0.8  0.6  0.4  0.2  0   0     200  400  Epoch  600  800  1000  Figure 1: Values of the damping strength λ during training of MNIST (left) and USPS (right) with and without dropout using λ = 1 for classiﬁcation. When dropout is included, the damping strength initially decreases followed by a steady increase over time.  during each iteration. Since we assume that the curvature mini-batches are a subset of the gradient mini-batches, the same feature detectors are omitted in both cases.  Since the curvature estimates are noisy, it is important to consider the stability of updates when different stochastic networks are used in each computation. The weight updates in dropout SGD are augmented with momentum not only for stability but also to speed up learning. Speciﬁcally, at iteration k the parameter update is given by  ∆θk = pk∆θk−1 − (1 − pk)αkh∇f i,  (15) where pk and ak are the momentum and learning rate, respectively. We incorporate an additional exponential decay term βe when performing parameter updates. Speciﬁcally, each parameter update is computed as  θk = θk−1 + ∆θk  (16) where c ∈ (0, 1] is a ﬁxed parameter chosen by the user. Incorporating βe into the updates, along with the use of δ-momentum, leads to more stable updates and ﬁne convergence particularly when dropout is integrated during training.  θk = θk−1 + βeαkδk, βe = cβe−1  4.4 Algorithm  k and curvature minibatch X c  Pseudo-code for one iteration of our implementation of stochastic Hessian-free is presented. Given a gradient minibatch X g k, we ﬁrst sample dropout units (if applicable) for the inputs and last hidden layer of the network. These take the form of a binary vector, which are k, ∇f, P, ζ) is used to multiplied component-wise by the activations yi. In our pseudo-code, CG(δ0 denote applying CG with initial solution δ0 k, gradient ∇f , pre-conditioner P and ζ iterations. Note that, when computing δ-momentum, the ζ-th solution in iteration k − 1 is used as opposed to the solution chosen via backtracking. Given the objectives fk−1 computed with θ and fk computed with θ + δk, the reduction ratio ρ is calculated utilizing the un-damped quadratic approximation Mθ(δk). This allows updating λ using the Levenberg-Marquardt style damping. Finally, a backtracking line- search with at most ω steps is performed to compute the rate and serves as a last defense against potentially poor update directions.  Since curvature mini-batches are sampled from a subset of the gradient mini-batch, it is then sensible to utilize different curvature mini-batches on different epochs. Along with cycling through gradient mini-batches during each epoch, we also cycle through curvature subsets every h epochs, where h is the size of the gradient mini-batches divided by the size of the curvature mini-batches. For example, if the gradient batch size is 1000 and the curvature batch size is 100, then curvature mini-batch sampling completes a full cycle every 1000/100 = 10 epochs.  Finally, one simple way to speed up training as indicated in [24], is to cache the activations when initially computing the objective fk. While each iteration of CG requires computing a curvature- vector product, the network parameters are ﬁxed during CG and is thus wasteful to re-compute the network activations on each iteration.  6  Algorithm 1 Stochastic Hessian-Free Optimization  k ← gradient minibatch, X c  X g Sample dropout units for inputs and last hidden layer if start of new epoch then  k ← curvature minibatch, |X g  k | = h|X c  k|, h ∈ Z+  γe ← min(1.01γe−1, .99)  k ; θ), ∇f ← ∇f (X g  end if k ← γeδζ δ0 k−1 fk−1 ← f (X g Solve (B + λI)δk = −∇f using CG(δ0 fk ← f (X g k ; θ + δk) for j = ζ - 1 to 1 do k) ← f (X g  k ; θ + δj k)  f (θ + δj if f (θ + δj  k) < fk then  fk ← f (θ + δj  k), δk ← δj  k  k ; θ), P ← Precon(X g  k; θ)  k, ∇f, P, ζ)  end if end for ρ ← (fk − fk−1)/( 1 if ρ < .25, λ ← 1.01λ elseif ρ > .75, λ ← .99λ end if αk ← 1, j ← 0 while j < ω do  k Bδk + ∇f T δk)  2 δT  if fk > fk−1 + .01αk∇f T δk then αk ← .8αk, j ← j + 1 else break end if  end while θ ← θ + βeαkδk, k ← k + 1  {δ-momentum}  {Using X c {CG backtracking}  k to compute Bδk}  {Using X c  k to compute Bδk}  {Backtracking linesearch}  {Parameter update}  5 Experiments  We perform experimental evaluation on both classiﬁcation and deep autoencoder tasks. The goal of classiﬁcation experiments is to determine the effectiveness of SHF on test error generalization. For autoencoder tasks, we instead focus just on measuring the effectiveness of the optimizer on the training data. The datasets and experiments are summarized as follows:  • MNIST: Handwritten digits of size 28 × 28 with 60K training samples and 10K testing samples. For classiﬁcation, we train networks of size 784-1200-1200-10 with rectiﬁer activations. For deep autoencoders, the encoder architecture of 784-1000-500-250-30 with a symmetric decoding archi- tecture is used. Logistic activations are used with a binary cross entropy error. For classiﬁcation experiments, the data is scaled to have zero mean and unit variance.  • CURVES: Artiﬁcial dataset of curves of size 28 × 28 with 20K training samples and 10K testing samples. We train a deep autoencoder using an encoding architecture of 784-400-200-100-50-25- 6 with symmetric decoding. Similar to MNIST, logistic activations and binary cross entropy error are used.  • USPS: Handwritten digits of size 16 × 16 with 11K examples. We perform classiﬁcation using 5 randomly sampled batches of 8K training examples and 3K testing examples as in [26] Each batch has an equal number of each digit. Classiﬁcation networks of size 256-500-500-10 are trained with rectiﬁer activations. The data is scaled to have zero mean and unit variance.  • Reuters: A collection of 8293 text documents from 65 categories. Each document is represented as a 18900-dimensional bag-of-words vector. Word counts C are transformed to log(1 + C) as is done by [3]. The publically available train/test split of is used. We train networks of size 18900-65 for classiﬁcation due to the high dimensionality of the inputs, which reduces to softmax- regression.  For classiﬁcation experiments, we perform comparison of SHF with and without dropout against dropout SGD [3]. All classiﬁcation experiments utilize the sparse initialization of Martens [4] with initial biases set to 0.1. The sparse initialization in combination with ReLUs make our networks similar to the deep sparse rectiﬁer networks of [28]. All algorithms are trained for 500 epochs on MNIST and 1000 epochs on USPS and Reuters. We use weight decay of 5 × 10−4 for SHF and 2 × 10−5 for dropout SHF. A held-out validation set was used for determining the amount of input  7  0.025  0.02  0.015  0.01  0.005  r o r r e   n o i t a c i f i s s a c  l  0   0  100  MNIST  SHF dSHF dSGD−a dSGD−l     0.035  USPS  r o r r e n o     i t  a c i f i s s a c  l  0.03  0.025  0.02  0.015  0.01  0.005  200  300  Epoch  400  500  0   0  200  400  600  Epoch     SHF dSHF dSGD−a dSGD−l  0.2  0.18  0.16  0.14  0.12  0.1  0.08  0.06  0.04  0.02  r o r r e     n o  i t  a c i f i s s a c  l  800  1000  0   0  200  Reuters     SHF dSHF dSGD  400  600  Epoch  800  1000  Figure 2: Training and testing curves for classiﬁcation. dSHF: dropout SHF, dSGD: dropout SGD, dSGD-a: dropout on all layers, dSGD-l: dropout on last hidden layer only (as well as the inputs).  dropout for all algorithms. Both SHF and dropout SHF use initial damping of λ = 1, gradient batch size of 1000, curvature batch size of 100 and 3 CG iterations per batch.  Dropout SGD training uses an exponential decreasing learning rate schedule initialized at 10, in combination with max-norm weight clipping [3]. This allows SGD to use larger learning rates for greater exploration early in training. A linearly increasing momentum schedule is used with initial momentum of 0.5 and ﬁnal momentum of 0.99. No weight decay is used. For additional comparison we also train dropout SGD when dropout is only used in the last hidden layer, as is the case with dropout SHF.  For deep autoencoder experiments, we use the same experimental setup as in Chapter 7 of [17]. In particular, we focus solely on training error without any L2 penalty in order to determine the effectiveness of the optimizer on modeling the training data. Comparison is made against SGD, SGD with momentum, HF and Nesterov’s accelerated gradient (NAG). On CURVES, SHF uses an initial damping of λ = 10, gradient and curvature batch sizes of 2000 and 25 CG iterations per batch. On MNIST, we use initial λ = 1, gradient and curvature batch sizes of 3000 and 50 CG iterations per batch. Autoencoder training is ran until no sufﬁcient progress is made, which occurs at around 250 epochs on CURVES and 100 epochs on MNIST.  5.1 Classiﬁcation results  Figure 2 summarizes our classiﬁcation results. At epoch 500, dropout SHF achieves 107 errors on MNIST. This result is similar to [3] which achieve 100-115 errors with various network sizes when training for a few thousand epochs. Without dropout or input corruption, SHF achieves 159 errors on MNIST, on par with existing methods that do not incorporate prior knowledge, pre-training, image distortions or dropout. As with [4], we hypothesize that further improvements can be made by ﬁne-tuning with SHF after unsupervised layerwise pre-training.  After 1000 epochs of training on ﬁve random splits of USPS, we obtain ﬁnal classiﬁcation errors of 1%, 1.1%, 0.8%, 0.9% and 0.97% with a mean test error of 0.95%. Both algorithms use 50% input corruption. For additional comparison, [29] obtains a mean classiﬁcation error of 1.14% using a pre-trained deep network for large-margin nearest neighbor classiﬁcation with the same size splits. Without dropout, SHF overﬁts the training data.  On the Reuters dataset, SHF with and without dropout both demonstrate accelerated training. We hypothesize that further speedup may also be obtained by starting training with a much smaller λ initialization, which we suspect is conservative given that the problem is convex.  8  Table 1: Training errors on the deep autoencoder tasks. All results are obtained from [17]. M(0.99) refers to momentum capped at 0.99 and similarily for M(0.9). SGD-VI refers to SGD using the variance normalized initialization of [15].  problem CURVES MNIST  NAG M(0.99) M(0.9) 0.220 0.078 0.730 0.990  0.110 0.770  SGD SGD-VI [19] 0.250 1.100  0.160 0.900  HF 0.110 0.780  SHF 0.089 0.877  MNIST     SHF NAG HF SGD−VI SGD  CURVES     SHF NAG HF SGD−VI SGD  1.8  1.6  1.4  1.2  1  0.8  0.6  0.4  0.2  2 L _ n a r t  i  100  150  Epoch  200  250  0   0  20  40  60  Epoch  80  100  0.25  0.2  2 L _ n a r t  i  0.15  0.1  0.05  0   0  50  Figure 3: Learning curves for the deep autoencoder tasks. The CG decay parameter γ is shut off at epoch 160 on CURVES and epoch 60 on MNIST.  5.2 Deep autoencoder results  Figure 3 and table 1 summarize our results. Inspired by [17] we make one additional modiﬁcation to our algorithms. As soon as training begins to diverge, we turn off the CG decay parameter γ in a similar fashion as the the momentum parameter µ is decreased in [17]. When γ = 0, CG is no longer initialized from the previous solution and is instead initialized to zero. As with [17], this has a dramatic effect on the training error but to a lesser extent as momentum and Nesterov’s accelerated gradient. [17] describes the behaviour of this effect as follows: with a large momentum, the optimizer is able to make steady progress along slow changing directions of low curvature. By decreasing the momentum late in training, the optimizer is then able to quickly reach a local minimum from ﬁner optimization along high curvature directions, which would otherwise be too difﬁcult to obtain with an aggressive momentum schedule. This observation further motivates the relationship between momentum and information sharing through CG.  Our experimental results demonstrate that SHF does not perform signiﬁcantly better or worse on these datasets compared to existing approaches. It is able to outperform HF on CURVES but not on MNIST. An attractive property that is shared with both HF and SHF is not requiring the careful schedule tuning that is necessary for momentum and NAG. We also attempted experiments with SHF using the same setup for classiﬁcation with smaller batches and 5 CG iterations. The results were worse: on CURVES the lowest training error obtained was 0.19. This shows that while such a setup is useful from the viewpoint of noisy updates and test generalization, they hamper the effectiveness of making progress on hard to optimize regions.  6 Conclusion  In this paper we proposed a stochastic variation of Martens’ Hessian-free optimization incorporating dropout for training neural networks on classiﬁcation and deep autoencoder tasks. By adapting the batch sizes and number of CG iterations, SHF can be constructed to perform well for classiﬁcation  9  against dropout SGD or optimizing deep autoencoders comparing HF, NAG and momentum meth- ods. While our initial results are promising, of interest would be adapting stochastic Hessian-free optimization to other network architectures:  • Convolutional networks. The most common approach to training convolutional networks has been SGD incorporating a diagonal Hessian approximation [8]. Dropout SGD was recently used for training a deep convolutional network on ImageNet [30].  • Recurrent Networks. It was largely believed that RNNs were too difﬁcult to train with SGD due to the exploding/vanishing gradient problem. In recent years, recurrent networks have become popular again due to several advancements made in their training [31].  • Recursive Networks. Recursive networks have been successfully used for tasks such as sentiment classiﬁcation and compositional modeling of natural language from word embeddings [32]. These architectures are usually trained using L-BFGS.  It is not clear yet whether this setup is easily generalizable to the above architectures or whether improvements need to be considered. Furthermore, additional experimental comparison would in- volve dropout SGD with the adaptive methods of Adagrad [9] or [11], as well as the importance of pre-conditioning CG. None the less, we hope that this work initiates future research in developing stochastic Hessian-free algorithms.  Acknowledgments  The author would like to thank Csaba Szepesvári for helpful discussion as well as David Sussillo for his guidance when ﬁrst learning about and implementing HF. The author would also like to thank the anonymous ICLR reviewers for their comments and suggestions.  References  [1] L. Bottou and O. Bousquet. The tradeoffs of large-scale learning. Optimization for Machine  Learning, page 351, 2011.  [2] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep  networks. NIPS, 19:153, 2007.  [3] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving  neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.  [4] J. Martens. Deep learning via hessian-free optimization. In ICML, volume 951, 2010. [5] J. Martens and I. Sutskever. Learning recurrent neural networks with hessian-free optimization.  In ICML, 2011.  [6] B.A. Pearlmutter. Fast exact multiplication by the hessian. Neural Computation, 6(1):147–160,  1994.  [7] N.N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.  Neural computation, 14(7):1723–1738, 2002.  [8] Y. LeCun, L. Bottou, G. Orr, and K. Müller. Efﬁcient backprop. Neural networks: Tricks of  the trade, pages 546–546, 1998.  [9] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and  stochastic optimization. JMLR, 12:2121–2159, 2010.  [10] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, A. Senior, P. Tucker,  K. Yang, et al. Large scale distributed deep networks. In NIPS, pages 1232–1240, 2012.  [11] T. Schaul, S. Zhang, and Y. LeCun. No more pesky learning rates. arXiv:1206.1106, 2012. [12] A. Bordes, L. Bottou, and P. Gallinari. Sgd-qn: Careful quasi-newton stochastic gradient  descent. JMLR, 10:1737–1754.  [13] Razvan Pascanu and Yoshua Bengio.  arXiv:1301.3584, 2013.  Natural gradient  revisited.  arXiv preprint  [14] N. Le Roux, P.A. Manzagol, and Y. Bengio. Topmoumoute online natural gradient algorithm.  In NIPS, 2007.  10  [15] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward  neural networks. In AISTATS, 2010.  [16] Yann N Dauphin and Yoshua Bengio. Big neural networks waste capacity. arXiv preprint  arXiv:1301.3583, 2013.  [17] I. Sutskever. Training Recurrent Neural Networks. PhD thesis, University of Toronto, 2013. [18] R.H. Byrd, G.M. Chin, J. Nocedal, and Y. Wu. Sample size selection in optimization methods  for machine learning. Mathematical Programming, pages 1–29, 2012.  [19] O. Chapelle and D. Erhan. Improved preconditioner for hessian free optimization. In NIPS  Workshop on Deep Learning and Unsupervised Feature Learning, 2011.  [20] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent neural networks. In  ICML, 2011.  [21] O. Vinyals and D. Povey. Krylov subspace descent for deep learning. arXiv:1111.4259, 2011. [22] Q.V. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A.Y. Ng. On optimization methods  for deep learning. In ICML, 2011.  [23] Y. Bengio. Practical recommendations for gradient-based training of deep architectures.  arXiv:1206.5533, 2012.  [24] J. Martens and I. Sutskever. Training deep and recurrent networks with hessian-free optimiza-  tion. Neural Networks: Tricks of the Trade, pages 479–535, 2012.  [25] P. Vincent, H. Larochelle, Y. Bengio, and P.A. Manzagol. Extracting and composing robust  features with denoising autoencoders. In ICML, pages 1096–1103, 2008.  [26] R. Min, D.A. Stanley, Z. Yuan, A. Bonner, and Z. Zhang. A deep non-linear feature mapping for large-margin knn classiﬁcation. In Ninth IEEE International Conference on Data Mining, pages 357–366. IEEE, 2009.  [27] Deng Cai, Xuanhui Wang, and Xiaofei He. Probabilistic dyadic data analysis with local and  global consistency. In ICML, pages 105–112, 2009.  [28] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer neural networks. In AISTATS, 2011. [29] R. Min, D.A. Stanley, Z. Yuan, A. Bonner, and Z. Zhang. A deep non-linear feature mapping  for large-margin knn classiﬁcation. In ICDM, pages 357–366, 2009.  [30] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional  neural networks. NIPS, 25, 2012.  [31] Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. Advances in optimizing recurrent  networks. arXiv:1212.0901, 2012.  [32] R. Socher, B. Huval, C.D. Manning, and A.Y. Ng. Semantic compositionality through recursive  matrix-vector spaces. In EMNLP, pages 1201–1211, 2012.  11  ","Hessian-free (HF) optimization has been successfully used for training deepautoencoders and recurrent networks. HF uses the conjugate gradient algorithmto construct update directions through curvature-vector products that can becomputed on the same order of time as gradients. In this paper we exploit thisproperty and study stochastic HF with gradient and curvature mini-batchesindependent of the dataset size. We modify Martens' HF for these settings andintegrate dropout, a method for preventing co-adaptation of feature detectors,to guard against overfitting. Stochastic Hessian-free optimization gives anintermediary between SGD and HF that achieves competitive performance on bothclassification and deep autoencoder experiments."
1301.3545,2013,Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines  ,"['Guillaume Desjardins', 'Razvan Pascanu', 'Aaron Courville', 'Yoshua Bengio']",https://arxiv.org/pdf/1301.3545.pdf,"3 1 0 2    r a     M 6 1      ]  G L . s c [      2 v 5 4 5 3  .  1 0 3 1 : v i X r a  Metric-Free Natural Gradient for Joint-Training of  Boltzmann Machines  Guillaume Desjardins, Razvan Pascanu, Aaron Courville and Yoshua Bengio  D´epartement d’informatique et de recherche op´erationnelle  Universit´e de Montr´eal  Abstract  This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efﬁcient matrix-vector product to avoid explicitly storing the natural gradient metric L. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the covariance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive.  1  Introduction  Boltzmann Machines (BM) have become a popular method in Deep Learning for performing fea- ture extraction and probability modeling. The emergence of these models as practical learning algorithms stems from the development of efﬁcient training algorithms, which estimate the negative log-likelihood gradient by either contrastive [4] or stochastic [18, 19] approximations. However, the success of these models has for the most part been limited to the Restricted Boltzmann Machine (RBM) [6], whose architecture allows for efﬁcient exact inference. Unfortunately, this comes at the cost of the model’s representational capacity, which is limited to a single layer of latent variables. The Deep Boltzmann Machine (DBM) [15] addresses this by deﬁning a joint energy function over multiple disjoint layers of latent variables, where interactions within a layer are prohibited. While this affords the model a rich inference scheme incorporating top-down feedback, it also makes train- ing much more difﬁcult, requiring until recently an initial greedy layer-wise pretraining scheme. Since, Montavon and Muller [9] have shown that this difﬁculty stems from an ill-conditioning of the Hessian matrix, which can be addressed by a simple reparameterization of the DBM energy function, a trick called centering (an analogue to centering and skip-connections found in the de- terministic neural network literature [17, 14]). As the barrier to joint-training 1 is overcoming a challenging optimization problem, it is apparent that second-order gradient methods might prove to be more effective than simple stochastic gradient methods. This should prove especially important as we consider models with increasingly complex posteriors or higher-order interactions between latent variables. To this end, we explore the use of the Natural Gradient [2], which seems ideally suited to the stochas- tic nature of Boltzmann Machines. Our paper is structured as follows. Section 2 provides a detailed derivation of the natural gradient, including its speciﬁc form for BMs. While most of these equations  1Joint-training refers to the act of jointly optimizing θ (the concatenation of all model parameters, across all layers of the DBM) through maximum likelihood. This is in contrast to [15], where joint-training is preceded by a greedy layer-wise pretraining strategy.  1  have previously appeared in [3], our derivation aims to be more accessible as it attempts to derive the natural gradient from basic principles, while minimizing references to Information Geometry. Sec- tion 3 represents the true contribution of the paper: a practical natural gradient algorithm for BMs which exploits the persistent Markov chains of Stochastic Maximum Likelihood (SML) [18], with a Hessian-Free (HF) like algorithm [8]. The method, named Metric-Free Natural Gradient (MFNG) (in recognition of the similarities of our method to HF), avoids explicitly storing the natural gradient metric L and uses a linear solver to perform the required matrix-vector product L−1Eq [∇ log pθ]. Preliminary experimental results on DBMs are presented in Section 4, with the discussion appearing in Section 5.  2 The Natural Gradient  2.1 Motivation and Derivation The main insight behind the natural gradient is that the space of all probability distributions P = {pθ(x); θ ∈ Θ, x ∈ χ} forms a Riemannian manifold. Learning, which typically proceeds by iteratively adapting the parameters θ to ﬁt an empirical distribution q, thus traces out a path along this manifold. An immediate consequence is that following the direction of steepest descent in the original Euclidean parameter space does not correspond to the direction of steepest descent along P. To do so, one needs to account for the metric describing the local geometry of the manifold, which is given by the Fisher Information matrix [1], shown in Equation 4. While this metric is typically derived from Information Geometry, a derivation more accessible to a machine learning audience can be obtained as follows. The natural gradient aims to ﬁnd the search direction ∆θ which minimizes a given objective func- tion, such that the Kullback–Leibler divergence KL(pθ (cid:107) pθ+∆θ) remains constant throughout optimization. This constraint ensures that we make constant progress regardless of the curvature of the manifold P and enforces an invariance to the parameterization of the model. The natural gradient for maximum likelihood can thus be formalized as:  ∇N := ∆θ∗ ← argmin∆θ  Eq [− log pθ+∆θ(x)]  s.t. KL(pθ (cid:107) pθ+∆θ) = const.  In order to derive a useful parameter update rule, we will consider the KL divergence under the assumption ∆θ → 0. We also assume we have a discrete and bounded domain χ over which we deﬁne the probability mass function2 pθ. Taking the Taylor series expansion of log pθ+∆θ around θ, and denoting ∇f as the column vector of partial derivatives with ∂f as the i-th entry, and ∇2f the Hessian matrix with ∂2f ∂θi∂θj  in position (i, j), we have:  ∂θi  log pθ + (∇ log pθ)T ∆θ +  1 2  ∆θT(cid:0)∇2 log pθ  (cid:1) ∆θ  (cid:21)  (cid:20) pθ log pθ −(cid:88) KL(pθ (cid:107) pθ+∆θ) ≈ (cid:88) (cid:3) ∆θ (cid:2)−∇2 log pθ with the transition stemming from the fact that(cid:80)  ∆θT Epθ  χ 1 2  pθ  =  χ  x∈χ pθ(x) = 0. Replacing the objective function of Equation 1 by its ﬁrst-order Taylor expansion and rewriting the constraint as a Lagrangian, we arrive at the following formulation for L(θ, ∆θ), the loss function which the natural gradient seeks to minimize.  = ∂ ∂θi  χ pθ  ∂ log pθ  ∂θi  (cid:80)  Setting ∂L  L(θ, ∆θ) = Eq [− log pθ] + Eq [−∇ log pθ]T ∆θ + ∂∆θ to zero yields the natural gradient direction ∇N : ∇N = L−1Eq [∇ log pθ] with L = Epθ or equivalently L = Epθ  2When clear from context, we will drop the argument of pθ to save space.  2  (cid:3) ∆θ.  (cid:2)−∇2 log pθ (cid:3)  ∆θT Epθ  λ 2  (cid:2)−∇2 log pθ (cid:2)∇ log pθ∇T log pθ  (cid:3)  (1)  (2)  (3) (4)  While its form is reminiscent of the Newton direction, the natural gradient multiplies the estimated gradient by the inverse of the expected Hessian of log pθ (Equation 3) or equivalently by the Fisher Information matrix (FIM, Equation 4). The equivalence between both expressions can be shown trivially, with the details appearing in the Appendix. We stress that both of these expectations are computed with respect to the model distribution, and thus computing the metric L does not involve the empirical distribution in any way. The FIM for Boltzmann Machines is thus not equal to the uncentered covariance of the maximum likelihood gradients. In the following, we pursue our derivation from the form given in Equation 4.  2.2 Natural Gradient for Boltzmann Machines  ables x ∈ {0, 1}N by way of an energy function E(x) = −(cid:80) Z =(cid:80)  Derivation. Boltzmann machines deﬁne a joint distribution over a vector of binary random vari- k bkxk, with weight matrix W ∈ RN×N and bias vector b ∈ RN . Energy and probability are related by the Z exp (−E(x)), with Z the partition function deﬁned by Boltzmann distribution, such that p(x) = 1  k<l Wklxkxl −(cid:80)  x exp (−E(x)).  Starting from the expression of L found in Equation 3, we can derive the natural gradient metric for Boltzmann Machines.  (cid:2)∇2E(x) + ∇2 log Z(cid:3) = Epθ  (cid:2)∇2 log Z(cid:3)  L(BM ) = Epθ  The natural gradient metric for ﬁrst-order BMs takes on a surprisingly simple form: it is the expected Hessian of the log-partition function. With a few lines of algebra (whose details are presented in the Appendix), we can rewrite it as follows:  (cid:105) (∇E(x) − Epθ [∇E(x)])T (∇E(x) − Epθ [∇E(x)])  L(BM ) = Epθ  (cid:104)  (5)  L(BM ) is thus given by the covariance of ∇E, measured under the model distribution pθ. Concretely, if we denote Wkl and Wmn as the i and j-th parameters of the model respectively, the entry Lij will take on the value −E [xkxlxmxn] + E [xkxl] E [xmxn].  h pθ(v, h) (cid:107)(cid:80)  the constraint KL ((cid:80)  Discussion. When computing the Taylor expansion of the KL divergence in Equation 2, we glossed over an important detail. Namely, how to handle latent variables in pθ(x), a topic ﬁrst dis- cussed in [3]. If x = [v, h], we could just as easily have derived the natural gradient by considering h pθ+∆θ(v, h)) = const. Alternatively, since the distinction between visible and hidden units is entirely artiﬁcial (since the KL divergence does not involve the empirical distribution), we may simply wish to consider the distribution obtained by analytically integrating out a maximal number of random variables. In a DBM, this would entail marginalizing over all odd or even layers, a strategy employed with great success in the context of AIS [15]. In this work however, we only consider the metric obtained by considering the KL divergence between the full joint distributions pθ and pθ+∆θ.  3 Metric-Free Natural Gradient Implementation We can compute the natural gradient ∇N by ﬁrst replacing the expectations of Equation 5 by a ﬁnite sample approximation. We can do this efﬁciently by reusing the model samples generated by the persistent Markov chains of SML. Given the size of the matrix being estimated however, we expect this method to require a larger number of chains than is typically used. The rest of the method is similar to the Hessian-Free (HF) algorithm of Martens [8]: we exploit an efﬁcient matrix-vector implementation combined with a linear-solver, such as Conjugate Gradient or MinRes[12], to solve the system Ly = Eq [∇ log pθ] for y ∈ RN . Additionally, we replace the expectation on the rhs. of this previous equation by an average computed over a mini-batch of training examples (sampled from the empirical distribution q), as is typically done in the stochastic learning setting. For Boltzmann Machines, the matrix-vector product Ly can be computed in a straightforward man- ner, without recourse to Pearlmutter’s R-operator [13]. Starting from a sampling approximation to Equation 5, we simply push the dot product inside of the expectation as follows:  3  Algorithm 1 MFNG iteration(θ,X +,Z− old) θ: parameters of the model. N :=| θ |. X +: mini-batch of training examples, with X + = {xm; m ∈ [1, M ]}. Z− old: previous state of Markov chains, with Z = {zm := (vm, h(1)  m , h(2)  m ); m ∈ [1, M ]}  m , h(2)+  m = ∂E(z+ m)  • Generate positive phase samples Z + = {z+ m ); m ∈ [1, M ]} m := (xm, h(1)+ • Initializing M Markov chains from state Z− old, generate negative phase samples Z− m = ∂E(z− • Compute the vectors s+ and s− • Compute negative log-likelihood gradient as g = 1 • Denote S ∈ RM×N as the matrix with rows s− # Solve the system “Ly = g” for y, given L = (S − ¯S)T (S − ¯S) and an initial zero vector. # computeLy is a function which performs equation 6, without instantiating L. • ∇N θ ← CGSolve(computeLy, S, g, zeros(N ))  (cid:80) m − s− m). m s− m.  , ∀m. m (s+  m and ¯S = 1  (cid:80)  new.  m)  ∂θ  ∂θ  M  M  L(BM )y ≈ (cid:0)S − ¯S(cid:1)T(cid:2)(cid:0)S − ¯S(cid:1) y(cid:3)  (6)  with  and  and  S ∈ RM×N , the matrix with entries smj = ¯S ∈ RN , the vector with entries sj = xm ∼ pθ(x), m ∈ [1, M ].  1 M  (cid:88)  m  ∂E(xm)  ∂θj  smj  By ﬁrst computing the matrix-vector product (S − ¯S)y, we can easily avoid computing the full N × N matrix L. Indeed, the result of this operation is a vector of length M, which is then left- multiplied by a matrix of dimension N × M, yielding the matrix-vector product Ly ∈ RN . A single iteration of the MFNG is presented in Algorithm 1. A full open-source implementation is also available online.3.  4 Experiments  We performed a proof-of-concept experiment to determine whether our Metric-Free Natural Gra- dient (MFNG) algorithm is suitable for joint-training of complex Boltzmann Machines. To this end, we compared our method to Stochastic Maximum Likelihood and a diagonal approximation of MFNG on a 3-layer Deep Boltzmann Machine trained on MNIST [7]. All algorithms were run in conjunction with the centering strategy of Montavon and Muller [9], which proved crucial to suc- cessfully joint-train all layers of the DBM (even when using MFNG) 4. We chose a small 3-layer DBM with 784-400-100 units at the ﬁrst, second and third layers respectively, to be comparable to [10]. Hyper-parameters were varied as follows. For inference, we ran 5 iterations of either mean- ﬁeld as implemented in [15] or Gibbs sampling. The learning rate was kept ﬁxed during training and chosen from the set {5 · 10−3, 10−3, 10−4}. For MinRes, we set the damping coefﬁcient to 0.1 and used a ﬁxed tolerance of 10−5 (used to determine convergence). Finally, we tested all algorithms on minibatch sizes of either 25, 128 or 256 elements 5. Finally, since we are comparing optimiza- tion algorithms, hyper-parameters were chosen based on the training set likelihood (though we still report the associated test errors). All experiments used the MinRes linear solver, both for its speed and its ability to return pseudo-inverses when faced with ill-conditioning.  3https://github.com/gdesjardins/MFNG 4The centering coefﬁcients were initialized as in [9], but were otherwise held ﬁxed during training. 5We expect larger minibatch sizes to be preferable, however simulating this number of Markov chains in  parallel (on top of all other memory requirements) was sufﬁcient to hit the memory bottlenecks of GPUs.  4  Figure 1: Estimated model likelihood as a function of (left) epochs and (right) CPU-time for MFNG, its diagonal approximation (MFNG-diag) and SML. All methods were run in conjunction with the DBM centering trick [9], with centering coefﬁcients held ﬁxed during training. Our grid search yielded the following hyper-parameters: batch size of 256/128 for MFNG(-diag)/SGD; 5 steps of mean-ﬁeld / sampling-based inference for MFNG(-diag)/SGD and a learning rate of 5 · 10−3.  Figure 1 (left) shows the likelihood as estimated by Annealed Importance Sampling [15, 11] as a function of the number of epochs 6. Under this metric, MFNG achieves the fastest convergence, obtaining a training/test set likelihood of −71.26/−72.84 nats after 94 epochs. In comparison, MFNG-diag obtains −73.22/−74.05 nats and SML −80.12/−79.71 nats in 100 epochs. The picture changes however when plotting likelihood as a function of CPU-time, as shown in Figure 1 (right). Given a wall-time of 8000s for MFNG and SML, and 5000s for MFNG-diag7, SML is able to perform upwards of 1550 epochs, resulting in an impressive likelihood score of −64.94 / −67.73. Note that these results were obtained on the binary-version of MNIST (thresholded at 0.5) in order to compare to [10]. These results are therefore not directly comparable to [15], which binarizes the dataset through sampling (by treating each pixel activation as the probability p of a Bernouilli distribution). Figure 2 shows a breakdown of the algorithm runtime, for various components of the algorithm. These statistics were collected in the early stages of training, but are generally representative of the bigger picture. While the linear solver clearly dominates the runtime, there are a few interesting observations to make. For small models and batch sizes greater than 256, a single evaluation of Ly appears to be of the same order of magnitude as a gradient evaluation. In all cases, this cost is smaller than that of sampling, which represents a non-negligible part of the total computational budget. This suggests that MFNG could become especially attractive for models which are expensive to sample from. Overall however, restricting the number of CG/MinRes iterations appears key to computational performance, which can be achieved by increasing the damping factor α. How this affects convergence in terms of likelihood is left for future work.  5 Discussion and Future Work  While the wall-clock performance of MFNG is not currently competitive with SML, we believe there are still many avenues to explore to improve computational efﬁciency. Firstly, we performed almost no optimization of the various MinRes hyper-parameters. In particular, we ran the algorithm to convergence with a ﬁxed tolerance of 10−5. While this typically resulted in relatively few iterations (around 15), this level of precision might not be required (especially given the stochastic nature of  6While we do not report error margins for AIS likelihood estimates, the numbers proved robust to changes in the number of particles and temperatures being simulated. To obtain such robust estimates, we implemented all the tricks described in Salakhutdinov and Hinton [15] and [16]: pA a zero-weight base-rate model whose biases are set by maximum likelihood; interpolating distributions pi ∝ p(1−βi) p(βi) B , with pB the target distribution; and ﬁnally analytical integration of all odd-layers.  A  7This discrepancy will be resolved in the next revision.  5  Figure 2: Breakdown of algorithm runtime, when we vary (left) the batch size (with ﬁxed model architecture 784 − 400 − 100) and (right) the model size (with ﬁxed batch size of 256). Runtime is additive in the order given by the labels (top to bottom). Dotted lines denote intermediate steps, while continuous lines denote full steps. Data was collected on a Nvidia GTX 480 card.  the algorithm). Additionally, it could be worth exploiting the same strategy as HF where the linear solver is initialized by the solution found in the previous iteration. This may prove much more efﬁcient than the current approach of initializing the solver with a zero vector. Pre-conditioning is also a well-known method for accelerating the convergence speed of linear solvers [5]. Our implementation used a simple diagonal regularization of L. The Jacobi preconditioner could be implemented easily however by computing the diagonal of L in a ﬁrst-pass. Finally, while our single experiment offers little evidence in support of either conclusion, it may very well be possible that MFNG is simply not computationally efﬁcient for DBMs, compared to SML with centering. In this case, it would be worth applying the method to either (i) models with known ill-conditioning, such as factored 3-rd order Boltzmann Machines or (ii) models and distributions exhibiting complex posterior distributions. In such scenarios, we may wish to maximize the use of the positive phase statistics (which were obtained at a high computational cost) by performing larger jumps in parameter space. It remains to be seen how this would interact with SML, where the burn-in period of the persistent chains is directly tied to the magnitude of ∆θ.  Appendix  We include the following derivations for completeness.  5.1 Expected Hessian of log Z and Fisher Information.  (cid:20)  Epθ  − ∂2 log p(x)  ∂θi∂θj  (cid:21)  (cid:21)  ∂p(x) ∂θj ∂p(x) ∂θi  p(x)  p(x)2  (cid:20) 1 (cid:20)(cid:18) 1 (cid:20) ∂ log p(x) (cid:20) ∂ log p(x) (cid:20) ∂ log p(x) (cid:20) ∂ log p(x)  ∂θi  ∂θi  ∂θi  − 1 p(x)  ∂p(x) ∂θi  (cid:19)(cid:18) 1  ∂2p(x) ∂θi∂θj  (cid:19)  p(x)  ∂p(x) ∂θj  −(cid:88) −(cid:88) − ∂2(cid:80)  x  x  − 1 p(x)  p(x)  1  p(x)  ∂2p(x) ∂θi∂θj  x p(x)  ∂θi∂θj  (cid:21) (cid:21) (cid:21) (cid:21)  ∂ log p(x)  ∂θj  ∂ log p(x)  ∂θj  ∂ log p(x)  ∂θj  ∂ log p(x)  = Epθ  = Epθ  = Epθ  = Epθ  = Epθ  = Epθ  (cid:21)  ∂2p(x) ∂θi∂θj ∂2p(x) ∂θi∂θj  ∂θi  ∂θj  6  5.2 Derivation of Equation 5  [exp (−E(x))]  log p(x) = −E(x) − log Z − 1 Z  ∂θi  ∂ log p(x)  = − ∂E(x) ∂θi = − ∂E(x) ∂θi  x  ∂ ∂θi  (cid:88) (cid:20) ∂E(x) ∂θi (cid:21) − Epθ  (cid:21) (cid:20) ∂E(x)  ∂θi − Epθ  + Epθ  (cid:20)(cid:18) ∂E(x) (cid:20) ∂E(x)  ∂θi  ∂E(x)  ∂θi  ∂θj  = Epθ  = Epθ  (cid:20)  E(cid:112)θ  − ∂2 log p(x)  ∂θi∂θj  (cid:21)  (cid:21)(cid:19)(cid:18) ∂E(x) (cid:20) ∂E(x) (cid:21)  ∂θj Epθ  ∂θi  (cid:21)(cid:19)(cid:21)  (cid:20) ∂E(x) (cid:21)  ∂θj  − Epθ  (cid:20) ∂E(x)  ∂θj  References [1] Amari, S. (1985). Differential geometrical methods in statistics. Lecture notes in statistics, 28. [2] Amari, S. (1998). Natural gradient works efﬁciently in learning. Neural Computation, 10(2), 251–276. [3] Amari, S., Kurata, K., and Nagaoka, H. (1992).  Information geometry of Boltzmann machines.  IEEE  Trans. on Neural Networks, 3, 260–271.  [4] Carreira-Perpi˜nan, M. A. and Hinton, G. E. (2005). On contrastive divergence learning. In AISTATS’2005,  pages 33–40.  [5] Chapelle, O. and Erhan, D. (2011). Improved preconditioner for hessian free optimization. NIPS Workshop  on Deep Learning and Unsupervised Feature Learning.  [6] Freund, Y. and Haussler, D. (1992). A fast and exact learning rule for a restricted class of Boltzmann  machines. pages 912–919, Denver, CO. Morgan Kaufmann, San Mateo.  [7] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient based learning applied to document  recognition. Proc. IEEE.  [8] Martens, J. (2010). Deep learning via Hessian-free optimization. pages 735–742. [9] Montavon, G. and Muller, K.-R. (2012). Deep Boltzmann machines and the centering trick. In G. Mon- tavon, G. Orr, and K.-R. Muller, editors, Neural Networks: Tricks of the Trade, volume 7700 of Lecture Notes in Computer Science, pages 621–637.  [10] Montavon, G. and M¨uller, K.-R. (2012). Learning feature hierarchies with centered deep Boltzmann  machines. CoRR, abs/1203.4416.  [11] Neal, R. M. (2001). Annealed importance sampling. Statistics and Computing, 11(2), 125–139. [12] Paige, C. C. and Saunders, M. A. (1975). Solution of Sparse Indeﬁnite Systems of Linear Equations.  SIAM Journal on Numerical Analysis, 12(4), 617–629.  [13] Pearlmutter, B. (1994). Fast exact multiplication by the Hessian. Neural Computation, 6(1), 147–160. [14] Raiko, T., Valpola, H., and LeCun, Y. (2012). Deep learning made easier by linear transformations in  perceptrons. In AISTATS’2012.  [15] Salakhutdinov, R. and Hinton, G. (2009). Deep Boltzmann machines.  In Proceedings of the Twelfth  International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2009), volume 8.  [16] Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief networks. volume 25,  pages 872–879.  [17] Schraudolph, N. N. (1998). Centering neural network gradient factors. In G. B. Orr and K.-R. Muller,  editors, Neural Networks: Tricks of he Trade, pages 548–548. Springer.  [18] Tieleman, T. (2008). Training restricted Boltzmann machines using approximations to the likelihood  gradient. pages 1064–1071.  [19] Younes, L. (1999). On the convergence of Markovian stochastic algorithms with rapidly decreasing  ergodicity rates. Stochastics and Stochastic Reports, 65(3), 177–228.  7  ","This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm fortraining Boltzmann Machines. Similar in spirit to the Hessian-Free method ofMartens [8], our algorithm belongs to the family of truncated Newton methodsand exploits an efficient matrix-vector product to avoid explicitely storingthe natural gradient metric $L$. This metric is shown to be the expected secondderivative of the log-partition function (under the model distribution), orequivalently, the variance of the vector of partial derivatives of the energyfunction. We evaluate our method on the task of joint-training a 3-layer DeepBoltzmann Machine and show that MFNG does indeed have faster per-epochconvergence compared to Stochastic Maximum Likelihood with centering, thoughwall-clock performance is currently not competitive."
1301.3764,2013,"Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients  ","['Tom Schaul', 'Yann LeCun']",https://arxiv.org/pdf/1301.3764.pdf,"3 1 0 2    r a     M 7 2      ]  G L . s c [      2 v 4 6 7 3  .  1 0 3 1 : v i X r a  Adaptive learning rates and parallelization for  stochastic, sparse, non-smooth gradients  Tom Schaul  Yann LeCun  Courant Institute of Mathematical Sciences  New York University, 715 Broadway,  Courant Institute of Mathematical Sciences  New York University, 715 Broadway,  10003, New York  schaul@cims.nyu.edu  10003, New York  yann@cims.nyu.edu  Abstract  Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non- stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process re- placing the diagonal Hessian estimation procedure that may not always be avail- able by a robust ﬁnite-difference approximation. The ﬁnal algorithm integrates all these components, has linear complexity and is hyper-parameter free.  1  Introduction  Many machine learning problems can be framed as minimizing a loss function over a large (maybe inﬁnite) number of samples. In representation learning, those loss functions are generally built on top of multiple layers of non-linearities, precluding any direct or closed-form optimization, but admitting (sample) gradients to guide iterative optimization of the loss. Stochastic gradient descent (SGD) is among the most broadly applicable and widely-used algo- rithms for such learning tasks, because of its simplicity, robustness and scalability to arbitrarily large datasets. Doing many small but noisy updates instead of fewer large ones (as in batch meth- ods) gives both a speed-up, and makes the learning process less likely to get stuck in sensitive local optima. In addition, SGD is eminently well-suited for learning in non-stationary environments, e.g., when that data stream is generated by a changing environment; but non-stationary adaptivity is use- ful even on stationary problems, as the initial search phase (before a local optimum is located) of the learning process can be likened to a non-stationary environment. Given the increasingly wide adoption of machine learning tools, there is an undoubted beneﬁt to making learning algorithms, and SGD in particular, easy to use and hyper-parameter free. In recent work, we made SGD hyper-parameter free by introducing optimal adaptive learning rates that are based on gradient variance estimates [1]. While broadly successful, the approach was limited to smooth loss functions, and to minibatch sizes of one. In this paper, we therefore complement that work, by addressing and resolving the issues of  • minibatches and parallelization, • sparse gradients, and • non-smooth loss functions  all while retaining the optimal adaptive learning rates. All of these issues are of practical importance: minibatch parallelization has strong diminishing returns, but in combination with sparse gradients and adaptive learning rates, we show how that effect is drastically mitigated. The importance of  1  robustly dealing with non-smooth loss functions is also a very practical concern: a growing num- ber of learning architectures employ non-smooth nonlinearities, like absolute value normalization or rectiﬁed-linear units. Our ﬁnal algorithm addresses all of these, while remaining simple to imple- ment and of linear complexity.  2 Background  There are a number of adaptive settings for SGD learning rates, or equivalently, diagonal precondi- tioning schemes, to be found in the literature, e.g., [2, 3, 4, 5, 6, 7]. The aim of those is generally to increase performance on stochastic optimization tasks, a concern complementary to our focus of producing an algorithm that works robustly without any hyper-parameter tuning. Often those adaptive schemes produce monotonically decreasing rates, however, which makes them no longer applicable to non-stationary tasks. The remainder of this paper build upon the adaptive learning rate scheme of [1], which is not mono- tonically decreasing, so we recapitulate its main results here. Using an idealized quadratic and separable loss function, it is possible to derive an optimal learning rate schedule which preserves the convergence guarantees of SGD. When the problem is approximately separable, the analysis is simpliﬁed as all quantities are one-dimensional. The analysis also holds as a local approximation in the non-quadratic but smooth case. In the idealized case, and for any dimension i, the optimal learning rate can be derived analytically, and takes the following form  ·  1 hi  η∗ i =  (θi − θ∗ i )2  · (E[∇θi])2 E[∇2 θi i ) is the distance to the optimal parameter value, and σ2 i and hi are the local sample  where (θi − θ∗ variance and curvature, respectively. We use an exponential moving average with time-constant τ (the approximate number of samples considered from recent memory) for online estimates of the quantities in equation 1:  (θi − θ∗  i )2 + σ2 i  1 hi  (1)  =  ]  gi ← (1 − τ−1 vi ← (1 − τ−1 hi ← (1 − τ−1  i  i  i  i  ) · gi + τ−1 ) · vi + τ−1 ) · hi + τ−1  i  i  · ∇θi · (∇θi)2 · h(bbprop)  i  where the diagonal Hessian entries h(bbprop) time-constant (memory) is adapted according to how large a step was taken:  are computed using the ‘bbprop’ procedure [8], and the  i  (cid:18)  (cid:19)  τi(t + 1) =  1 − gi(t)2 vi(t)  · τi(t) + 1  The ﬁnal algorithm is called vSGD, and used the learning rates from equation 1 to update the pa- rameters (element-wise):  θ ← θ − η∗ · ∇θ  3 Parallelization with minibatches  Compared to the pure online SGD, computation time can be reduced by “minibatch”-parallelization: n sample-gradients are computed (simultaneously, e.g., on multiple cores) and then a single update on the resulting averaged minibatch gradient is performed.  n(cid:88)  k=1  ∇θ =  1 n  ∇(k)  θ  (2)  While n can be seen as a hyperparameter of the algorithm [9], it is often constrained to a large extent by the computational hardware, memory requirements and communication bandwidth. A derivation just like the one that led to equation 1 can be used to determine the optimal learning  2  Figure 1: Diminishing returns of minibatch parallelization. Plotted is the relative log-loss gain (per number of sample gradients evaluated) of a given minibatch size compared to the gain of the n = 1 case (in the noisy quadratic scenario from section 2, for different noise levels σ, and assuming optimal learning rates as in equation 4); each ﬁgure corresponds to a different sparsity level. For example, the ratio is 0.02 for n = 100 (left plot, low noise): This means that it takes 50 times more samples to obtain the same gain in loss than with pure SGD. Those are strongly diminishing returns, but they are less drastic if the noise level is high (only 5 times more samples in this example). If the sample gradients are somewhat sparse, however, and we use that fact to increase learning rates appropriately, then the diminishing returns kick in only for much larger minibatch sizes; see the left two ﬁgures.  rates automatically, for an arbitrary minibatch size n. The key difference is that the averaging in equation 2 reduces the effective variance by a factor n, leading to:  η∗ i =  ·  1 hi  (θi − θ∗ i )2 i )2 + 1  (θi − θ∗  n σ2  =  ·  1 hi  E[∇2  θi  1 n  (E[∇θi])2 ] + n−1  n (E[∇θi])2  (3)  This expresses the intuition that using minibatches reduces the sample noise, in turn permitting larger step sizes: if the noise (or sample diversity) is small, those gains are minimal, if it is large, they are substantial (see Figure 1, left). Varying minibatch sizes tend to be impractical1 to implement however, and so common practice is to simply ﬁx a minibatch size, and then re-tune the learning rates (by a factor between 1 and n). With our adaptive minibatch-aware scheme (equation 3) this is no longer necessary: in fact, we get an automatic transition from initially small effective minibatches (by means of the learning rates) to large minibatches toward the end, when the noise level is higher.  4 Sparse gradients  Many common learning architectures (e.g., those using rectiﬁed linear units, or sparsity penalties) lead to sample gradients that are increasingly sparse, that is, they are non-zero only in small fraction of the problem dimensions. It is possible to exploit this to speed up learning, by averaging many sparse gradients in a minibatch, or by doing asynchronous updates [10]. Here, we investigate how to set the learning rates in the presence of sparsity, and our result is simply based on the observation that doing an update using a set of sparse gradients is equivalent to doing the same update, but with a smaller effective minibatch size, while ignoring all the zero entries. We can do this again on an element-by-element basis, where we deﬁne zi to be the number of non-zero elements in dimension i, within the current minibatch. In each dimension, we rescale the minibatch gradient accordingly by a factor n/(n − zi), and at the same time reduce the learning rate to reﬂect the smaller effective minibatch size. Compounding those two effects gives the optimal learning rate for sparse minibatches (we ignore the case zi = n, when there is no update):  η∗ i =  n  n − zi  · 1 hi  ·  1  n−zi  E[∇2  θi  (E[∇θi])2 ] + n−zi−1 n−zi  (E[∇θi])2  (4)  Figure 1 shows how using minibatches with such adaptive learning rates reduces the impact of diminishing returns if the sample gradients are sparse. In other words, with the right learning rates, higher sparsity can be directly translated into higher parallelizability.  1If the implementation/computational architecture is ﬂexible enough, the variance-term of the learning rate  can also be used to adapt the minibatch size adaptively to its optimal trade-off.  3  100101102103104minibatch size10-310-210-1100relative log-loss gain per samplesparsity=1.0100101102103104minibatch size10-210-1100sparsity=0.2100101102103104minibatch size10-1100sparsity=0.05low noisemed noisehigh noiseFigure 2: Difference between global or instance-based computation of effective minibatch sizes in the presence of sparse gradients. Our proposed method computes the number of non-zero entries (n−zi) in the current mini-batch to set the learning rate (green). This involves some additional com- putation compared to just using the long-term average sparsity p(nz) (red), but obtains a substantially higher relative gain (see ﬁgure 1), especially in the regime where the sparsity level produces mini- batches with just one or a few non-zero entries (dent in the curve near n = 1/p(nz) ). If the noise level is low (left two ﬁgures), the effect is much more pronounced than if the noise is higher. For comparison, the performance for 40 different ﬁxed learning-rate SGD settings (between 0.01 and 100) are plotted as yellow dots.  i  i  Figure 3: Illustrating the effect of reweighting minibatch gradients. Assume the samples are drawn from 2 different noisy clusters (yellow and light blue vectors), but one of the clusters has a higher probability of occurrence. The regular minibatch gradient is simply their arithmetic average (red), dominated by the more common cluster. The reweighted minibatch gradient (blue) does a full step toward each of the clusters, closely resembling the gradient one would obtain by performing a hard clustering (difﬁcult in practice) on the samples, in dotted green.  An alternative to computing zi for each minibatch (and each dimension) anew would be to just use = E [n − zi] instead. Figure 2 shows that this is suboptimal, the long-term average sparsity p(nz) especially if the noise level is small, and in the regime where each minibatch is expected to contain just a few non-zero entries. This ﬁgure also shows that equation 4 produces a higher relative gain compared to the outer envelope of the performance of all ﬁxed learning rates.  i  4  10010110210310-1100rel. gain (sparsity=0.1)low noise10010110210310-1100med noise100101102103minibatch size10-1100rel. gain (sparsity=0.01)100101102103minibatch size10-1100mb-sparsityavg-sparsity0.00.51.01.50.00.51.0Figure 4: Illustrating the expectation over non-smooth sample losses. In dotted blue, the loss func- tions for a few individual samples are shown, each a non-smooth function. However, the expectation over a distribution of such functions is smooth, as shown by the thick magenta curve. Left: absolute value, right: rectiﬁed linear function; samples are identical but offset by a value drawn from N (0, 1).  4.1 Orthogonal gradients  One reason for the boost in parallelizability if the gradients are sparse comes from the fact that sparse gradients are mostly orthogonal, allowing independent progress in each direction. But sparse gradients are in fact a special case of orthogonal gradients, for which we can obtain similar speedups with a reweighting of the minibatch gradients:  n(cid:88)  i=1  ∇θ =  (cid:80)n  j=1  1 |∇(i)(cid:62) θ ∇(j) θ | θ (cid:107)·(cid:107)∇(j) (cid:107)∇(i) θ (cid:107)  ∇(i)  θ  (5)  In other words, each sample is weighted by one over the number of times (smoothed) that its gradient is interfering (non-orthogonal) with another sample’s gradient. In the limit, this scheme simpliﬁes to the sparse-gradient cases discussed above: if all sample gra- dients are aligned, they are averaged (reweighted by 1/n, corresponding to the dense case in equa- tion 2), and if all sample gradients are orthogonal, they are summed (reweighted by 1, corresponding to the maximally sparse case zi = n − 1 in equation 4). See Figure 3 for an illustration. In practice, this reweighting comes at a certain cost, increasing the computational expense of a single iteration from O(nd) to O(n2d), where d is the problem dimension. In other words, it is only likely to be viable if the forward-backward passes of the gradient computation are non-trivial, or if the minibatch size is small.  5 Non-smooth losses  Many commonly used non-linearities (rectiﬁed linear units, absolute value normalization, etc.) pro- duce non-smooth sample loss functions. However, when optimizing over a distribution of samples (or just a large enough dataset), the variability between samples can lead to a smooth expected loss function, even though each sample has a non-smooth contribution. Figure 4 illustrates this point for samples that have an absolute value or a rectiﬁed linear contribution to the loss. It is clear from this observation that it is not possible to reliably estimate the curvature of the true expected loss function, from the curvature of the individual sample losses (which are all zero in the two examples above), if the sample losses are non-smooth. This means that our previous approach of estimating the hi term in the optimal learning rate expression by a moving average of sample curvatures, as estimated by the “bbprop” procedure [8] (which computes a Gauss-Newton approx- imation of the diagonal Hessian, at the cost of one additional backward pass) is limited to smooth sample loss functions, and we need a different approach for the general case2.  2This also alleviates potential implementation effort, e.g., when using third-party software that does not  implement bbprop.  5  ?4?20240123?4?20240123Algorithm 1: vSGD-fd: minibatch-SGD with ﬁnite-difference-estimated adaptive learning rates repeat  draw n samples, compute the gradients ∇(j) compute the gradients on the same samples, with the parameters shifted by δi = gi for i ∈ {1, . . . , d} do  θ for each sample j  (cid:12)(cid:12)(cid:12)(cid:12)∇(j) f d(cid:12)(cid:12)(cid:12) > 2  θi  −∇(j) δi  (cid:114)  vi  θi+δi  i  (cid:12)(cid:12)(cid:12)hf d  i − hi  (cid:112)  compute ﬁnite-difference curvatures hf d(j)  =  if |∇θi − gi| > 2  vi − gi  2  or  increase memory size for outliers τi ← τi + 1  end  then  hi  (cid:12)(cid:12)(cid:12)(cid:12) f d −(cid:16) f d(cid:17)2 (cid:80)n (cid:16)∇(j) (cid:80)n j=1 ∇(j) (cid:80)n (cid:16) (cid:80)n  · 1 · 1  j=1  θi  θi  n  n  n  i  n  (cid:17)2  (cid:17)2  j=1 hf d(j) i hf d(j) i  j=1  i  gi  vi  ← (1 − τ−1 ← (1 − τ−1 f d ← (1 − τ−1 hi f d ← (1 − τ−1  i  i  vi  i  i  · 1 · 1 + τ−1 f d + τ−1  ) · gi + τ−1 ) · vi + τ−1 ) · hi ) · vi n · (gi)2  f d  i  i  vi + (n − 1) · (gi)2  f d  f d · i ← hi (cid:18) vi 1 − (gi)2 vi i · 1  (cid:19) · τi + 1 (cid:80)n j=1 ∇(j)  n  θi  update moving averages  estimate learning rate η∗  update memory size τi ← update parameter θi ← θi − η∗  end  until stopping criterion is met  5.1 Finite-difference curvature  A good estimate of the relevant curvature for our purposes (i.e., for determining a good learning rate) is to not to compute the true Hessian at the current point, but to take the expectation over noisy ﬁnite-difference steps, where those steps are on the same scale than the actually performed update steps, because this is the regime we care about. In practice, we obtain this ﬁnite-difference estimates by computing two gradients of the same sample loss, on points differing by the typical update distance3:  (cid:12)(cid:12)(cid:12)(cid:12)∇θi − ∇θi+δi  δi  (cid:12)(cid:12)(cid:12)(cid:12)  hf d i =  (6)  where δi = gi. This approach is related to the diagonal Hessian preconditioning in SGD-QN [11], but the step-difference used is different, and the moving average scheme there is decaying with time, which thus loses the suitability for non-stationary problems.  5.2 Curvature variability  To further increase robustness, we reuse the same intuition that originally motivated vSGD, and take into account the variance of the curvature estimates (produced by the ﬁnite-difference method) to reduce the likelihood of becoming overconﬁdent (underestimating curvature, i.e., overestimating learning rates) by using a variance-normalization based on the signal-to-noise ratio of the curvature estimates.  3Of course, this estimate does not need to be computed at every step, which can save computation time.  6  For this purpose we maintain two additional moving averages: + τ−1 f d + τ−1  f d ← (1 − τ−1 f d ← (1 − τ−1  ) · hi ) · vi  hi  vi  f d  i  i  i  i  (cid:17)2  ·(cid:16)  · hf d i hf d i  and then compute the curvature term simply as hi = vi  f d/hi  f d.  5.3 Outlier detection  If an outlier sample is encountered while the time constants τi is close to one (i.e., the history is mostly discarded from the moving averages at each update), this has the potential to disrupt the optimization process. Here, the statistics we keep for the adaptive learning rates have an additional, unforeseen beneﬁt: they make it trivial to detect outliers. The outlier’s effect can be mitigated relatively simply by increasing the time-constant τi before incorporating the sample into the statistics (to make sure old samples are not forgotten), and then due to the perceived variance shooting up, the learning rate is automatically reduced. If it was not an outlier, but a genuine change in the data distribution, the algorithm will quickly adapt, increase the learning rates again. In practice, we use a detection threshold of two standard deviations, and increase the corresponding τi by one (see pseudocode).  5.4 Algorithm  Algorithm 1 gives the explicit pseudocode for this ﬁnite-difference estimation, in combination with the minibatch size-adjusted rates from equation 3, termed “vSGD-fd”. Initialization is akin to the one of vSGD, in that all moving averages are bootstrapped on a few samples (10) before any updates are done. It is also wise to add an tiny (cid:15) = 10−5 term where necessary to avoid divisions by zero.  6 Simulations  An algorithm that has the ambition to work out-of-the-box, without any tuning of hyper-parameters, must be able to pass a number of elementary tests: those may not be sufﬁcient, but they are necessary. To that purpose, we set up a collection of elementary (one-dimensional) stochastic optimization test cases, varying the shape of the loss function, its curvature, and the noise level. The sample loss functions are  fquad = A ·(cid:16) θ − ξ(j)(cid:17)2 fabs = A ·(cid:12)(cid:12)(cid:12)θ − ξ(j)(cid:12)(cid:12)(cid:12) (cid:26)  A · (θ − ξ(j)) 0  frectlin = fgauss = A − Ae− (θ−ξ(j))2  2  if θ − ξ(j) > 0 otherwise  where A is the curvature setting and the ξ(j) are drawn from N (0, σ2). We vary curvature and noise levels by two orders of magnitude, i.e., A ∈ {0.1, 1, 10} and σ2 ∈ {0.1, 1, 10}, giving us 9x4 test cases. To visualize the large number of results, we summarize the each test case and algorithm combination in a concise heatmap square (see Figure 5 for the full explanation). In Figure 6, we show the results for all test cases on a range of algorithms and minibatch sizes n. Each square shows the gain in loss for 100 independent runs of 1024 updates each. Each group of columns corresponds to one of the four functions, with the 9 inner columns using different curvature and noise level settings. Color scales are identical for all heatmaps within a column, but not across columns. Each group of rows corresponds to one algorithm, with each row using a different hyper- parameter setting, namely initial learning rates η0 ∈ {0.01, 0.1, 1, 10} (for SGD, ADAGRAD [6] and the natural gradient [7]) and decay rate γ ∈ {0, 1} for SGD. All rows come in pairs, with the upper one using pure SGD (n = 1) and the lower one using minibatches (n = 10).  7  Figure 5: Explanation of how to read our concise heatmap performance plots (right), based on the more common representation as learning curves (left). In the learning curve representation, we plot one curve for each algorithm and each trial (3x8 total), with a unique color/line-type per algorithm, and the mean performance per algorithm with more contrast. Performance is measured every power of 2 iterations. This gives a good idea of the progress, but becomes quickly hard to read. On the right side, we plot the identical data in heatmap format. Each square corresponds to one algorithm, the horizontal axis are still the iterations (on log2 scale), and on the vertical axis we arrange (sort) the performance of the different trials at the given iteration. The color scale is as follows: white is the initial loss value, the stronger the blue, the lower the loss, and if the color is reddish, the algorithm overjumped to loss values that are bigger than the initial one. Good algorithm performance is visible when the square becomes blue on the right side, instability is marked in red, and the variability of the algorithm across trials is visible by the color range on the vertical axis.  The ﬁndings are clear: in contrast to the other algorithms tested, vSGD-fd does not require any hyper-parameter tuning to give reliably good performance on the broad range of tests: the learning rates adapt automatically to different curvatures and noise levels. And in contrast to the predecessor vSGD, it also deals with non-smooth loss functions appropriately. The learning rates are adjusted automatically according to the minibatch size, which improves convergence speed on the noisier test cases (3 left columns), where there is a larger potential gain from minibatches. The earlier variant (vSGD) was shown to work very robustly on a broad range of real-world bench- marks and non-convex, deep neural network-based loss functions. We expect those results on smooth losses to transfer directly to vSGD-fd. This bodes well for future work that will determine its per- formance on real-world non-smooth problems.  7 Conclusion  We have presented a novel variant of SGD with adaptive learning rates that expands on previous work in three directions. The adaptive rates properly take into account the minibatch size, which in combination with sparse gradients drastically alleviates the diminishing returns of parallelization. Also, the curvature estimation procedure is based on a ﬁnite-difference approach that can deal with non-smooth sample loss functions. The ﬁnal algorithm integrates these components, has linear complexity and is hyper-parameter free. Unlike other adaptive schemes, it works on a broad range of elementary test cases, the necessary condition for an out-of-the-box method. Future work will investigate how to adjust the presented element-wise approach to highly non- separable problems (tightly correlated gradient dimensions), potentially relying on a low-rank or block-decomposed estimate of the gradient covariance matrix, as in TONGA [12].  8  100101102103iterations10-410-310-210-1100lossSGDvSGDoracleSGDvSGDoracleAcknowledgments  The authors want to thank Sixin Zhang, Durk Kingma, Daan Wierstra, Camille Couprie, Cl´ement Farabet and Arthur Szlam for helpful discussions. We also thank the reviewers for helpful sugges- tions, and the ‘Open Reviewing Network’ for perfectly managing the novel open and transparent reviewing process. This work was funded in part through AFR postdoc grant number 2915104, of the National Research Fund Luxembourg.  References [1] Schaul, T, Zhang, S, and LeCun, Y. No More Pesky Learning Rates. Technical report, June  2012.  [2] Jacobs, R. A. Increased rates of convergence through learning rate adaptation. Neural Net-  works, 1(4):295–307, January 1988.  [3] Almeida, L and Langlois, T. Parameter adaptation in stochastic optimization. On-line learning  in neural . . . , 1999.  [4] George, A. P and Powell, W. B. Adaptive stepsizes for recursive estimation with applications  in approximate dynamic programming. Machine Learning, 65(1):167–198, May 2006.  [5] Nicolas Le Roux, A. F. A fast natural Newton method. [6] Duchi, J. C, Hazan, E, and Singer, Y. Adaptive subgradient methods for online learning and  stochastic optimization. 2010.  [7] Amari, S, Park, H, and Fukumizu, K. Adaptive method of realizing natural gradient learning  for multilayer perceptrons. Neural Computation, 12(6):1399–1409, 2000.  [8] LeCun, Y, Bottou, L, Orr, G, and Muller, K. Efﬁcient backprop. In Orr, G and K., M, editors,  Neural Networks: Tricks of the trade. Springer, 1998.  [9] Byrd, R, Chin, G, Nocedal, J, and Wu, Y. Sample size selection in optimization methods for  machine learning. Mathematical Programming, 2012.  [10] Niu, F, Recht, B, Re, C, and Wright, S. J. Hogwild!: A lock-free approach to parallelizing  stochastic gradient descent. Matrix, (1):21, 2011.  [11] Bordes, A, Bottou, L, and Gallinari, P. Sgd-qn: Careful quasi-newton stochastic gradient  descent. Journal of Machine Learning Research, 10:1737–1754, July 2009.  [12] Le Roux, N, Manzagol, P, and Bengio, Y. Topmoumoute online natural gradient algorithm,  2008.  9  Figure 6: Performance comparisons for a number of algorithms (row groups) under different setting variants (rows) and sample loss functions (columns), the latter grouped by loss function shape. Red tones indicate a loss value worsening from its initial value, white corresponds to no progress, and darker blue tones indicate a reduction of loss (in log-scale). For a detailed explanation of how to read the heatmaps, see Figure 5. The new proposed algorithm vSGD-fd (bottom row group) performs well across all functions and noise-level settings, namely ﬁxing the vSGD instability on non-smooth functions like the absolute value. The other algorithms need to have their hyper-parameters tuned to the task to work well.  10  quadabsrectlingaussSGDNaturalGradAdaGradvSGDvSGD-fd","Recent work has established an empirically successful framework for adaptinglearning rates for stochastic gradient descent (SGD). This effectively removesall needs for tuning, while automatically reducing learning rates over time onstationary problems, and permitting learning rates to grow appropriately innon-stationary tasks. Here, we extend the idea in three directions, addressingproper minibatch parallelization, including reweighted updates for sparse ororthogonal gradients, improving robustness on non-smooth loss functions, in theprocess replacing the diagonal Hessian estimation procedure that may not alwaysbe available by a robust finite-difference approximation. The final algorithmintegrates all these components, has linear complexity and is hyper-parameterfree."
1301.3527,2013,Block Coordinate Descent for Sparse NMF  ,"['Vamsi Potluru', 'Sergey M. Plis', 'Jonathan Le Roux', 'Barak A. Pearlmutter', 'Vince D. Calhoun', 'Thomas P. Hayes']",https://arxiv.org/pdf/1301.3527.pdf,"3 1 0 2    r a     M 8 1      ]  G L . s c [      2 v 7 2 5 3  .  1 0 3 1 : v i X r a  Block Coordinate Descent for Sparse NMF  Vamsi K. Potluru  Department of Computer Science,  University of New Mexico  ismav@cs.unm.edu  Sergey M. Plis  Mind Research Network,  splis@mrn.org  Jonathan Le Roux  Barak A. Pearlmutter  Mitsubishi Electric Research Labs  Department of Computer Science,  leroux@merl.com  National University of Ireland Maynooth  barak@cs.nuim.ie  Vince D. Calhoun  Thomas P. Hayes  Electrical and Computer Engineering, UNM and  Department of Computer Science,  Mind Research Network  vcalhoun@mrn.org  University of New Mexico  hayes@cs.unm.edu  Abstract  Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L0 norm, however its optimization is NP-hard. Mixed norms, such as L1/L2 measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L1 norm. However, present algorithms designed for optimizing the mixed norm L1/L2 are slow and other formulations for sparse NMF have been pro- posed such as those based on L1 and L0 norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacriﬁcing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster com- pared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.  1 Introduction  Matrix factorization arises in a wide range of application domains and is useful for extracting the latent features in the dataset (Figure 1). In particular, we are interested in matrix factorizations which impose the following requirements:  • nonnegativity • low-rankedness • sparsity  Nonnegativity is a natural constraint when modeling data with physical constraints such as chemical concentrations in solutions, pixel intensities in images and radiation dosages for cancer treatment. Low-rankedness is useful for learning a lower dimensionality represen- tation. Sparsity is useful for modeling the conciseness of the representation or that of the  1  latent features. Imposing all these requirements on our matrix factorization leads to the sparse nonnegative matrix factorization (SNMF) problem.  SNMF enjoys quite a few formulations [2, 14, 13, 11, 24, 17, 25, 26] with successful applica- tions to single-channel speech separation [27] and micro-array data analysis [17, 25].  However, algorithms [14, 11] for solving SNMF which utilize the mixed norm of L1/L2 as their sparsity measure are slow and do not scale well to large datasets. Thus, we develop an eﬃcient algorithm to solve this problem and has the following ingredients:  • A theoretically eﬃcient projection operator (O(m log m)) to enforce the user-deﬁned sparsity where m is the dimensionality of the feature vector as opposed to the previous approach [14]. • Novel sequential updates which provide the bulk of our speedup compared to the  previously employed batch methods [14, 11].  Figure 1: (Left) Features learned from the ORL dataset2with various matrix factorization methods such as principal component analysis (PCA), independent component analysis (ICA), and dictionary learning. The relative merit of the various matrix factorizations depends on both the signal domain and the target application of interest. (Right) Features learned under the sparse NMF formulation where roughly half the features were constrained to lie in the interval [0.2, 0.4] and the rest are ﬁxed to sparsity value 0.7. This illustrates the ﬂexibility that the user has in ﬁne tuning the feature sparsity based on prior domain knowledge. White pixels in this ﬁgure correspond to the zeros in the features.  2 Preliminaries and Previous Work  In this section, we give an introduction to the nonnegative matrix factorization (NMF) and SNMF problems. Also, we discuss some widely used algorithms from the literature to solve them.  Both these problems share the following problem and solution structure. At a high-level, given a nonnegative matrix X of size m × n, we want to approximate it with a product of two nonnegative matrices W, H of sizes m × r and r × n, respectively: (1)  X ≈ WH.  The nonnegative constraint on matrix H makes the representation a conical combination of features given by the columns of matrix W. In particular, NMF can result in sparse representations, or a parts-based representation, unlike other factorization techniques such as principal component analysis (PCA) and vector quantization (VQ). A common theme in  2Scikit-learn package was used in generating the ﬁgure.  2  the algorithms proposed for solving these problems is the use of alternating updates to the matrix factors, which is natural because the objective function to be minimized is convex in W and in H, separately, but not in both together. Much eﬀort has been focused on optimizing the eﬃciency of the core step of updating one of W, H while the other stays ﬁxed.  2.1 Nonnegative Matrix Factorization  Factoring a matrix, all of whose entries are nonnegative, as a product of two low-rank non- negative factors is a fundamental algorithmic challenge. This has arisen naturally in diverse areas such as image analysis [20], micro-array data analysis [17], document clustering [31], chemometrics [19], information retrieval [12] and biology applications [4]. For further appli- cations, see the references in the following papers [1, 7].  We will consider the following version of the NMF problem, which measures the reconstruc- tion error using the Frobenius norm [21]:  1 2  min W,H  (cid:107)X − WH(cid:107)2  F s.t. W ≥ 0, H ≥ 0, (cid:107)Wj(cid:107)2 = 1, ∀j ∈ {1,··· , r}  (2) where ≥ is element-wise. We use subscripts to denote column elements. Simple multiplica- tive updates were proposed by Lee and Seung to solve the NMF problem. This is attractive for the following reasons:  rameter that needs to be set.  • Unlike additive gradient descent methods, there is no arbitrary learning rate pa- • The nonnegativity constraint is satisﬁed automatically, without any additional pro- • The objective function converges to a limit point and the values are non-increasing  jection step.  across the updates, as shown by Lee and Seung [21].  Algorithm 1 is an example of the kind of multiplicative update procedure used, for instance, by Lee and Seung [21]. The algorithm alternates between updating the matrices W and H (we have only shown the updates for H—those for W are analogous).  Algorithm 1 nnls-mult(X, W, H) 1: repeat 2: H = H (cid:12) W(cid:62)X 3: until convergence 4: Output: Matrix H.  W(cid:62)WH .  Here, (cid:12) indicates element-wise (Hadamard) product and matrix division is also element- wise. To remove the scaling ambiguity, the norm of columns of matrix W are set to unity. Also, a small constant, say 10−9, is added to the denominator in the updates to avoid division by zero.  Besides multiplicative updates, other algorithms have been proposed to solve the NMF problem based on projected gradient [22], block pivoting [18], sequential constrained opti- mization [6] and greedy coordinate-descent [15].  2.2 Sparse Nonnegative Matrix Factorization  The nonnegative decomposition is in general not unique [9]. Furthermore, the features may not be parts-based if the data resides well inside the positive orthant. To address these issues, sparseness constraints have been imposed on the NMF problem.  Sparse NMF can be formulated in many diﬀerent ways. From a user point of view, we can split them into two classes of formulations: explicit and implicit. In explicit versions of SNMF [14, 11], one can set the sparsities of the matrix factors W , H directly. On the other  3  hand, in implicit versions of SNMF [17, 25], the sparsity is controlled via a regularization parameter and is often hard to tune to speciﬁed sparsity values a priori. However, the algorithms for implicit versions tend to be faster compared to the explicit versions of SNMF.  [14]. In this paper, we consider the explicit sparse NMF formulation proposed by Hoyer To make the presentation easier to follow, we ﬁrst consider the case where the sparsity is imposed on one of the matrix factors, namely the feature matrix W —the analysis for the symmetric case where the sparsity is instead set on the other matrix factor H is analogous. The case where sparsity requirements are imposed on both the matrix factors is dealt with in the Appendix. The sparse NMF problem formulated by Hoyer [14] with sparsity on matrix W is as follows:  min W,H  (3)  f (W , H) =  (cid:107)X − WH(cid:107)2 1 2 (cid:107)Wj(cid:107)2 = 1, sp(Wj) = α, ∀j ∈ {1,··· , r}  F s.t. W ≥ 0, H ≥ 0,  Sparsity measure for a d-dimensional vector x is given by: d − (cid:107)x(cid:107)1/(cid:107)x(cid:107)2  √  sp(x) =  √  d − 1  (4)  The sparsity measure (4) deﬁned above has many appealing qualities. Some of which are as follows:  • The measure closely models the intuitive notion of sparsity as captured by the L0 norm. So, it easy for the user to specify sparsity constraints from prior knowledge of the application domain.  • Simultaneously, it is able to avoid the pitfalls associated with directly optimizing the L0 norm. Desirable properties for sparsity measures have been previously ex- plored [16] and it satisﬁes all of these properties for our problem formulation. The properties can be brieﬂy summarized as: (a) Robin Hood — Spreading the energy from larger coordinates to smaller ones decreases sparsity, (b) Scaling — Sparsity is invariant to scaling, (c) Rising tide — Adding a constant to the coordinates decreases sparsity, (d) Cloning — Sparsity is invariant to cloning, (e) Bill Gates — One big coordinate can increase sparsity, (f) Babies — coordinates with zeros increase sparsity.  • The above sparsity measure enables one to limit the sparsity for each feature to lie in a given range by changing the equality constraints in the SNMF formulation (3) to inequality constraints [11]. This could be useful in scenarios like fMRI brain analysis, where one would like to model the prior knowledge such as sizes of artifacts are diﬀerent from that of the brain signals. A sample illustration on a face dataset is shown in Figure 1 (Right). The features are now evenly split into two groups of local and global features by choosing two diﬀerent intervals of sparsity.  A gradient descent-based algorithm called Nonnegative Matrix Factorization with Sparse- ness Constraints (NMFSC) to solve SNMF was proposed [14]. Multiplicative updates were used for optimizing the matrix factor which did not have sparsity constraints spec- iﬁed. Heiler and Schn¨orr[11] proposed two new algorithms which also solved this prob- lem by sequential cone programming and utilized general purpose solvers like MOSEK (http://www.mosek.com). We will consider the faster one of these called tangent-plane constraint (TPC) algorithm. However, both these algorithms, namely NMFSC and TPC, solve for the whole matrix of coeﬃcients at once. In contrast, we propose a block coordinate- descent strategy which considers a sequence of vector problems where each one can be solved in closed form eﬃciently.  3 The Sequential Sparse NMF Algorithm  We present our algorithm which we call Sequential Sparse NMF (SSNMF) to solve the SNMF problem as follows:  4  First, we consider a problem of special form which is the building block (Algorithm 2) of our SSNMF algorithm and give an eﬃcient, as well as exact, algorithm to solve it. Second, we describe our sequential approach (Algorithm 3) to solve the subproblem of SNMF. This uses the routine we developed in the previous step. Finally, we combine our routines developed in the previous two steps along with standard solvers (for instance Algorithm 1) to complete the SSNMF Algorithm (Algorithm 4).  3.1 Sparse-opt  Sparse-opt routine solves the following subproblem which arises when solving problem (3):  (5)  (cid:62) b  y s.t. (cid:107)y(cid:107)1 = k,(cid:107)y(cid:107)2 = 1  max y≥0  where vector b is of size m. This problem has been previously considered [14], and an algorithm to solve it was proposed which we will henceforth refer to as the Projection- Hoyer. Similar projection problems have been recently considered in the literature and solved eﬃciently [10, 5]. Observation 1. For any i, j, we have that if bi ≥ bj, then yi ≥ yj.  Let us ﬁrst consider the case when the vector b is sorted. Then by the previous observation, we have a transition point p that separates the zeros of the solution vector from the rest.  Observation 2. By applying the Cauchy-Schwarz inequality on y and the all ones vector, we get p ≥ k2.  Setting the partial derivatives of the Lagrangian to zero, we get by observation 1:  The Lagrangian of the problem (5) is :  (cid:62) L(y, µ, λ, γ) = b  y + µ  yi − k  +  λ 2  (cid:32) m(cid:88)  (cid:33) i − 1 y2  i=1  + γ(cid:62)y  (cid:32) m(cid:88) m(cid:88)  i=1  i  (cid:33)  m(cid:88)  i  yi = k,  y2 i = 1  bi + µ(p) + λ(p)yi = 0,∀i ∈ {1, 2,··· , p}  γi = 0,∀i ∈ {1,··· , p} yi = 0,∀i ∈ {p + 1,··· , m}  where we account for the dependence of the Lagrange parameters λ, µ on the transition point p. We compute the objective value of problem (5) for all transition points p in the range from k2 to m and select the one with the highest value. In the case, where the vector b is not sorted, we just simply sort it and note down the sorting permutation vector. The complete algorithm is given in Algorithm 2. The dominant contribution to the running time of Algorithm 2 is the sorting of vector b and therefore can be implemented in O(m log m) time3. Contrast this with the running time of Projection-Hoyer whose worst case is O(m2) [14, 28].  3.2 Sequential Approach —Block Coordinate Descent  Previous approaches for solving SNMF [14, 11] use batch methods to solve for sparsity constraints. That is, the whole matrix is updated at once and projected to satisfy the con- straints. We take a diﬀerent approach of updating a column vector at a time. This gives us the beneﬁt of being able to solve the subproblem (column) eﬃciently and exactly. Subse- quent updates can beneﬁt from the newly updated columns resulting in faster convergence as seen in the experiments.  3This can be further reduced to linear time by noting that we do not need to fully sort the input  in order to ﬁnd p∗.  5  Algorithm 2 Sparse-opt(b, k) 1: Set a = sort(b) and p∗ = m. Get a mapping π such that ai = bπ(i) and aj ≥ aj+1 for  all valid i, j.  4:  i=1 ai)2  (cid:114) 2: Compute values of µ(p), λ(p) as follows: 3: for p ∈ {(cid:100)k2(cid:101),m} do i −((cid:80)p p(cid:80)p λ(p) = − µ(p) = −(cid:80)p i=1 a2 (p−k2) − k p λ(p) if a(p) < −µ(p) then p∗ = p − 1 break  5: 6: 7: 8: end if 9: 10: end for 11: Set xi = − ai+µ(p∗) λ(p∗) 12: Output: Solution vector y where yπ(i) = xi.  i=1 ai p  ,∀i ∈ {1,··· , p∗} and to zero otherwise.  In particular, consider the optimization problem (3) for a column j of the matrix W while ﬁxing the rest of the elements of matrices W, H:  g(cid:107)Wj(cid:107)2  ˜f (Wj) =  min Wj≥0 j Hj and u = −XH(cid:62)  j +(cid:80)  1 2  2 + u(cid:62)Wj s.t. (cid:107)Wj(cid:107)2 = 1, (cid:107)Wj(cid:107)1 = k  where g = H(cid:62) i(cid:54)=j Wi(HH(cid:62))ij. This reduces to the problem (5) for which we have proposed an exact algorithm (Algorithm 2). We update the columns of the matrix factor W sequentially as shown in Algorithm 3. We call it sequential for we update the columns one at a time. Note that this approach can be seen as an instance of block coordinate descent methods by mapping features to blocks and the Sparse-opt projection operator to a descent step.  for j = 1 to r (randomly) do  Algorithm 3 sequential-pass(X, W, H) 1: C = −XH(cid:62) + WHH(cid:62) 2: G = HH(cid:62) 3: repeat 4: 5: 6: 7: 8: 9: 10: until convergence 11: Output: Matrix W.  Uj = Cj − WjGjj t = Sparse-opt(−Uj, k). C = C + (t − Wj) G(cid:62) Wj = t.  end for  j  3.3 SSNMF Algorithm for Sparse NMF  We are now in a position to present our complete Sequential Sparse NMF (SSNMF) algo- rithm. By combining Algorithms 1, 2 and 3, we obtain SSNMF (Algorithm 4).  Algorithm 4 ssnmf(X, W, H) 1: repeat 2: W = sequential-pass(X, W, H) 3: H = nnls-mult(X, W, H) 4: until convergence 5: Output: Matrices W, H.  6  4 Implementation Issues  For clarity of exposition, we presented the plain vanilla version of our SSNMF Algorithm 4. We now describe some of the actual implementation details.  • Initialization: Generate a positive random vector v of size m and obtain z = m − 1 (from equation (4)). Use the solu- Sparse-opt(v, k) where k = tion z and its random permutations to initialize matrix W. Initialize the matrix H to uniform random entries in [0, 1].  m − α  √  √  • Incorporating faster solvers: We use multiplicative updates for a fair comparison with NMFSC and TPC. However, we can use other NNLS solvers [22, 18, 6, 15] to solve for matrix H. Empirical results (not reported here) show that this further speeds up the SSNMF algorithm.  • Termination: In our experiments, we ﬁx the number of alternate updates or equiv- alently the number of times we update matrix W . Other approaches include spec- ifying total running time, relative change in objective value between iterations or approximate satisfaction of KKT conditions.  • Sparsity constraints: We have primarly considered the sparse NMF model as for- mulated by Hoyer [14]. This has been generalized by Heiler and Schn¨orr [11] by relaxing the sparsity constraints to lie in user-deﬁned intervals. Note that, we can handle this formulation [11] by making a trivial change to Algorithm 3.  5 Experiments and Discussion  In this section, we compare the performance of our algorithm with the state-of-the-art NMFSC and TPC algorithms [14, 11]. Running times for the algorithms are presented when applied to one synthetic and three real-world datasets. Experiments report reconstruction error ((cid:107)X−WH(cid:107)F ) instead of objective value for convenience of display. For all experiments on the datasets, we ensure that our ﬁnal reconstruction error is always better than that of the other two algorithms. Our algorithm was implemented in MATLAB (http://www. mathworks.com) similar to NMFSC and TPC. All of our experiments were run on a 3.2Ghz Intel machine with 24GB of RAM and the number of threads set to one.  5.1 Datasets  For comparing the performance of SSNMF with NMFSC and TPC, we consider the following synthetic and three real-world datasets :  • Synthetic: 200 images of size 9 × 9 as provided by Heiler and Schn¨orr [11] in their  code implementation.  • CBCL face dataset consists of 2429 images of size 19 × 19 and can be obtained at  http://cbcl.mit.edu/cbcl/software-datasets/FaceData2.html.  • ORL: Face dataset that consists of 400 images of size 112 × 92 and can be ob- tained at http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase. html. http://www.kyb.tuebingen.mpg.de/bethge/vanhateren/iml/. We use 400 of these images in our experiments.  • sMRI: Structural MRI scans of 269 subjects taken at the John Hopkins University were obtained. The scans were taken on a single 1.5T scanner with the imaging parameters set to 35mm TR, 5ms TE, matrix size of 256 × 256. We segment these images into gray matter, white matter and cerebral spinal ﬂuid images, using the software program SPM5 (http://www.ﬁl.ion.ucl.ac.uk/spm/software/spm5/), followed by spatial smoothing with a Gaussian kernel of 10 × 10 × 10 mm. This results in images which are of size 105 × 127 × 46  7  Figure 2: Mean running times for Sparse-opt and the Projection-Hoyer are presented for random problems. The x-axis plots the dimension of the problem while the y-axis has the running time in seconds. Each of the subﬁgures corresponds to a single sparsity value in {0.2, 0.4, 0.6, 0.8}. Each datapoint corresponds to the mean running time averaged over 40 runs for random problems of the same ﬁxed dimension.  Figure 3: Running times for SSNMF and NMFSC and TPC algorithms on the synthetic dataset where the sparsity values range from 0.2 to 0.8 and number of features is 5. Note that SSNMF and NMFSC are over an order of magnitude faster than TPC.  5.2 Comparing Performances of Core Updates  We compare our Sparse-opt (Algorithm 2) routine with the competing Projection- Hoyer [14]. In particular, we generate 40 random problems for each sparsity constraint in {0.2, 0.4, 0.6, 0.8} and a ﬁxed problem size. The problems are of size 2i × 100 where i takes integer values from 0 to 12. Input coeﬃcients are generated by drawing samples uniformly at random from [0, 1]. The mean values of the running times for Sparse-opt and the Projection-Hoyer for each dimension and corresponding sparsity value are plotted in Figure 2.  We compare SSNMF with SSNMF+Proj on the CBCL dataset. The algorithms were run with rank set to 49. The running times are shown in Figure 6. We see that in low-dimensional datasets, the diﬀerence in running times are very small.  5.3 Comparing Overall Performances  SSNMF versus NMFSC and TPC: We plot the performance of SSNMF against NMFSC and TPC on the synthetic dataset provided by Heiler and Schn¨orr [11] in Fig- ure 3. We used the default settings for both NMFSC and TPC using the software provided  8  Problemsize(omittedscalefactor=100,000)oyer0.51.01.52.0reconstruction error0.2SSNMFNMFSCTPC0.30.40.510-210-1100101time (seconds)0.51.01.52.0reconstruction error0.610-210-1100101time (seconds)0.710-210-1100101time (seconds)0.810-210-1100101time (seconds)0.9Figure 4: Convergence plots for the ORL dataset with sparsity from [0.1, 0.8] for the NMFSC and SSNMF algorithms. Note that we are an order of magnitude faster, especially when the sparsity is higher.  Figure 5: Running times for SSNMF and NMFSC algorithms for the sMRI dataset with rank set to 40 and sparsity values of α from 0.1 to 0.8. Note that for higher sparsity values we converged to a lower reconstruction error and are also noticeably faster than the NMFSC algorithm.  by the authors. Our experience with TPC was not encouraging on bigger datasets and hence we show its performance only on the synthetic dataset. It is possible that the performance of TPC can be improved by changing the default settings but we found it non-trivial to do so.  SSNMF versus NMFSC: To ensure fairness, we removed logging information from NMFSC code [14] and only computed the objective for equivalent number of matrix updates as SSNMF. We do not plot the objective values at the ﬁrst iteration for convenience of display. However, they are the same for both algorithms because of the shared initialization . We ran the SSNMF and NMFSC on the ORL face dataset. The rank was ﬁxed at 25 in both the algorithms. Also, the plots of running times versus objective values are shown in Figure 4 corresponding to sparsity values ranging from 0.1 to 0.7. Additionally, we ran our SSNMF algorithm and NMFSC algorithm on a large-scale dataset consisting of the structural MRI images by setting the rank to 40. The running times are shown in Figure 5.  9  5678910reconstruction error0.1SSNMFNMFSC0.20.30.4101102103104time (seconds)5678910reconstruction error0.5101102103104time (seconds)0.6101102103104time (seconds)0.7101102103104time (seconds)0.8300400500600700800900reconstruction error0.1SSNMFNMFSC0.20.30.4103104time (seconds)300400500600700800900reconstruction error0.5103104time (seconds)0.6103104time (seconds)0.7103104time (seconds)0.8Figure 6: Running times for SSNMF and SSNMF+Proj algorithms for the CBCL face dataset with rank set to 49 and sparsity values ranging from 0.2 to 0.9  5.4 Main Results  We compared the running times of our Sparse-opt routine versus the Projection-Hoyer and found that on the synthetically generated datasets we are faster on average.  Our results on switching the Sparse-opt routine with the Projection-Hoyer did not slow down our SSNMF solver signiﬁcantly for the datasets we considered. So, we conclude that the speedup is mainly due to the sequential nature of the updates (Algorithm 3).  Also, we converge faster than NMFSC for fewer number of matrix updates. This can be seen by noting that the plotted points in Figures 4 and 5 are such that the number of matrix updates are the same for both SSNMF and NMFSC. For some datasets, we noted a speedup of an order of magnitude making our approach attractive for computation purposes.  Finally, we note that we recover a parts-based representation as shown by Hoyer [14]. An example of the obtained features by NMFSC and ours is shown in Figure 7.  (a) sparsity 0.5  (b) sparsity 0.6  (c) sparsity 0.75  Figure 7: Feature sets from NMFSC algorithm (Left) and SSNMF algorithm (Right) using the ORL face dataset for each sparsity value of α in {0.5, 0.6, 0.75}. Note that SSNMF algorithm gives a parts-based representation similar to the one recovered by NMFSC.  6 Connections to Related Work  Other SNMF formulations have been considered by Hoyer [13], Mørup et al. [24] , Kim and Park [17], Pascual-Montano et al. [25] (nsNMF) and Peharz and Pernkopf [26]. SNMF formulations using similar sparsity measures as used in this paper have been considered for applications in speech and audio recordings [30, 29].  We note that our sparsity measure has all the desirable properties, extensively discussed by Hurley and Rickard [16], except for one (“cloning”). Cloning property is satisﬁed when  10  405060708090100reconstruction error0.2SSNMFSSNMF+Proj0.30.40.5100101102time (seconds)405060708090100reconstruction error0.6100101102time (seconds)0.7100101102time (seconds)0.8100101102time (seconds)0.9two vectors of same sparsity when concatenated maintain their sparsity value. Dimensions in our optimization problem are ﬁxed and thus violating the cloning property is not an issue. Compare this with the L1 norm that satisﬁes only one of these properties (namely “rising tide”). Rising tide is the property where adding a constant to the elements of a vector decreases the sparsity of the vector. Nevertheless, the measure used in Kim and Park is based on the L1 norm. The properties satisﬁed by the measure in Pascual-Montano et al. are unclear because of the implicit nature of the sparsity formulation.  Pascual-Montano et al. [25] claim that the SNMF formulation of Hoyer, as given by prob- lem (3) does not capture the variance in the data. However, some transformation of the sparsity values is required to properly compare the two formulations [14, 25]. Preliminary results show that the formulation given by Hoyer [14] is able to capture the variance in the data if the sparsity parameters are set appropriately. Peharz and Pernkopf [26] propose to tackle the L0 norm constrained NMF directly by projecting from intermediate uncon- strained solutions to the required L0 constraint. This leads to the well-known problem of getting stuck in local minima. Indeed, the authors re-initialize their feature matrix with an NNLS solver to recover from the local suboptimum. Our formulation avoids the local minima associated with L0 norm by using a smooth surrogate.  7 Conclusions  We have proposed a new eﬃcient algorithm to solve the sparse NMF problem. Experiments demonstrate the eﬀectiveness of our approach on real datasets of practical interest. Our algorithm is faster over a range of sparsity values and generally performs better when the sparsity is higher. The speed up is mainly because of the sequential nature of the updates in contrast to the previously employed batch updates of Hoyer. Also, we presented an exact and eﬃcient algorithm to solve the problem of maximizing a linear objective with a sparsity constraint, which is an improvement over the heuristic approach in Hoyer.  Our approach can be extended to other NMF variants [13]. Another possible application is the sparse version of nonnegative tensor factorization. A diﬀerent research direction would be to scale our algorithm to handle large datasets by chunking [23] and/or take advantage of distributed/parallel computational settings [3].  Acknowledgement  The ﬁrst author would like to acknowledge the support from NIBIB grants 1 R01 EB 000840 and 1 R01 EB 005846. The second author was supported by NIMH grant 1 R01 MH076282- 01. The latter two grants were funded as part of the NSF/NIH Collaborative Research in Computational Neuroscience Program.  References  [1] Sanjeev Arora, Rong Ge, Ravindran Kannan, and Ankur Moitra. Computing a nonneg- ative matrix factorization – provably. In Proceedings of the 44th symposium on Theory of Computing, STOC ’12, pages 145–162, New York, NY, USA, 2012. ACM.  [2] Michael W Berry, Murray Browne, Amy N Langville, V Paul Pauca, and Robert J Plemmons. Algorithms and applications for approximate nonnegative matrix factor- ization. Computational Statistics & Data Analysis, 52(1):155–173, 2007.  [3] Joseph K. Bradley, Aapo Kyrola, Danny Bickson, and Carlos Guestrin. Parallel coor-  dinate descent for L1-regularized loss minimization. In ICML, pages 321–328, 2011.  [4] G. Buchsbaum and O. Bloch. Color categories revealed by non-negative matrix factor-  ization of munsell color spectra. Vision research, 42(5):559–563, 2002.  [5] Yunmei Chen and Xiaojing Ye.  Projection onto a simplex.  arXiv preprint  arXiv:1101.6081, 2011.  11  [6] A. Cichocki and A. H. Phan. Fast local algorithms for large scale nonnegative matrix IEICE Transactions on Fundamentals of Electronics, 92:  and tensor factorizations. 708–721, 2009.  [7] J. E. Cohen and U. G. Rothblum. Nonnegative ranks, decompositions, and factor- izations of nonnegative matrices. Linear Algebra and its Applications, 190:149–168, 1993.  [8] Chris Ding, Tao Li, Wei Peng, and Haesun Park. Orthogonal nonnegative matrix t- factorizations for clustering. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06, pages 126–135, New York, NY, USA, 2006. ACM.  [9] David Donoho and Victoria Stodden. When does non-negative matrix factorization give a correct decomposition into parts? In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨olkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA, 2004.  [10] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Eﬃcient pro- In Proceedings of the 25th  jections onto the l1-ball for learning in high dimensions. international conference on Machine learning, pages 272–279, 2008.  [11] Matthias Heiler and Christoph Schn¨orr. Learning sparse representations by non- negative matrix factorization and sequential cone programming. The Journal of Ma- chine Learning Research, 7:2006, 2006.  [12] T. Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Machine  Learning, 42(1):177–196, 2001.  [13] P. O. Hoyer. Non-negative sparse coding. In Neural Networks for Signal Processing,  2002. Proceedings of the 2002 12th IEEE Workshop on, pages 557–565, 2002.  [14] Patrik O. Hoyer. Non-negative matrix factorization with sparseness constraints. J.  Mach. Learn. Res., 5:1457–1469, December 2004.  [15] C. J. Hsieh and I. Dhillon. Fast coordinate descent methods with variable selection for non-negative matrix factorization. ACM SIGKDD Internation Conference on Knowl- edge Discovery and Data Mining, page xx, 2011.  [16] Niall Hurley and Scott Rickard. Comparing measures of sparsity. IEEE Trans. Inf.  Theor., 55:4723–4741, October 2009.  [17] Hyunsoo Kim and Haesun Park. Sparse non-negative matrix factorizations via alter- nating non-negativity-constrained least squares for microarray data analysis. Bioinfor- matics, 23(12):1495–1502, 2007.  [18] Jingu Kim and Haesun Park. Toward faster nonnegative matrix factorization: A new algorithm and comparisons. Data Mining, IEEE International Conference on, 0:353– 362, 2008.  [19] W. H. Lawton and E. A. Sylvestre. Self modeling curve resolution. Technometrics,  pages 617–633, 1971.  [20] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix  factorization. Nature, 401(6755):788–791, October 1999.  [21] Daniel D. Lee and Sebastian H. Seung. Algorithms for non-negative matrix factoriza-  tion. In NIPS, pages 556–562, 2000.  [22] Chih-Jen Lin. Projected gradient methods for nonnegative matrix factorization. Neural  Comp., 19(10):2756–2779, October 2007.  [23] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11:19–60, 2010.  12  [24] Morten Mørup, Kristoﬀer Hougaard Madsen, and Lars Kai Hansen. Approximate L0 constrained non-negative matrix and tensor factorization. In ISCAS, pages 1328–1331, 2008.  [25] A. Pascual-Montano, J. M. Carazo, K. Kochi, D. Lehmann, and R. D. Pascual-Marqui. Nonsmooth nonnegative matrix factorization (nsNMF). Pattern Analysis and Machine Intelligence, IEEE Transactions on, 28(3):403–415, March 2006.  [26] R. Peharz and F. Pernkopf. Sparse nonnegative matrix factorization with l0-constraints.  Neurocomputing, 2011.  [27] M. N. Schmidt and R. K. Olsson. Single-channel speech separation using sparse non- negative matrix factorization. In International Conference on Spoken Language Pro- cessing (INTERSPEECH), volume 2, page 1. Citeseer, 2006.  [28] Fabian J Theis, Kurt Stadlthanner, and Toshihisa Tanaka. First results on uniqueness of sparse non-negative matrix factorization. In Proceedings of the 13th European Signal Processing Conference (EUSIPCO05), 2005.  [29] Tuomas Virtanen. Monaural sound source separation by nonnegative matrix factor- ization with temporal continuity and sparseness criteria. Audio, Speech, and Language Processing, IEEE Transactions on, 15(3):1066–1074, 2007.  [30] Felix Weninger, Jordi Feliu, and Bjorn Schuller. Supervised and semi-supervised sup- pression of background music in monaural speech recordings. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 61–64. IEEE, 2012.  [31] W. Xu, X. Liu, and Y. Gong. Document clustering based on non-negative matrix factorization. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267–273. ACM, 2003.  Appendix  Bi-Sparse NMF  In some applications, it is desirable to set the sparsity on both matrix factors. However, this can lead to the situation where the variance in the data is poorly captured [25]. To ameliorate this condition, we formulate it as the following optimization problem and call it as Bi-Sparse NMF:  min  W,H,D  1 2  (cid:107)X − WDH(cid:107)2  F  s.t.W ≥ 0, H ≥ 0, D ≥ 0 (cid:107)Wj(cid:107)2 = 1, sp(Wj) = α,∀j ∈ {1,··· , r} (cid:107)Hi(cid:107)2 = 1, sp(Hi) = β,∀i ∈ {1,··· , r}  (6) where D is a r × r matrix. In the above formulation, we constrain the L2 norms of the columns of matrix W to unity. Similarly, we constrain the L2 norms of rows of matrix H to be unity. This scaling is absorbed by the matrix D. Note that this formulation with the matrix D constrained to be diagonal is equivalent to the one proposed in Hoyer when both the matrix factors have their sparsity speciﬁed.  We can solve for the matrix D with any NNLS solver. A concrete algorithm is the one presented in Ding et al. and is reproduced here for convenience (Algorithm 5). If D is a diagonal matrix, we only update the diagonal terms and maintain the rest at zero. Algo- rithms 1 and 5 can be sped up by pre-computing the matrix products which are unchanged during the iterations.  Also, the matrix D captures the variance of the dataset when we have sparsity set on both the matrices W, H.  13  Algorithm 5 Diag-mult(X, W, H, D)  repeat  D = D (cid:12) W(cid:62)XH  W(cid:62)WDHH(cid:62)  until convergence Output: Matrix D.  14  ","Nonnegative matrix factorization (NMF) has become a ubiquitous tool for dataanalysis. An important variant is the sparse NMF problem which arises when weexplicitly require the learnt features to be sparse. A natural measure ofsparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms,such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, basedon intuitive attributes that such measures need to satisfy. This is in contrastto computationally cheaper alternatives such as the plain L$_1$ norm. However,present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slowand other formulations for sparse NMF have been proposed such as those based onL$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed normsparsity constraints while not sacrificing computation time. We presentexperimental evidence on real-world datasets that shows our new algorithmperforms an order of magnitude faster compared to the current state-of-the-artsolvers optimizing the mixed norm and is suitable for large-scale datasets."
1301.3389,2013,The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization  ,['Hugo Van hamme'],https://arxiv.org/pdf/1301.3389.pdf,"                  The Diagonalized New ton Algorithm for Non-  negative Matrix Factorization   Hugo Van hamme   University of Leuven, dept. ESAT   Kasteelpark Arenberg 10 – bus 2441, 3001 Leuven, Belgium   hugo.vanhamme@esat.kuleuven.be   Abstract   Non-negative  matrix  factorization  (NMF)  has  become  a  popular  machine  learning  approach  to  many  problems  in  text  mining,  speech  and  image  processing,  bio-informatics  and  seismic  data  analysis  to  name  a  few.  In  NMF,  a  matrix  of  non-negative  data  is  approximated  by  the  low-rank  product  of  two  matrices  with  non-negative  entries.  In  this  paper,  the  approximation  quality  is  measured  by  the  Kullback-Leibler  divergence  between  the  data  and  its  low-rank  reconstruction.  The  existence  of  the  simple  multiplicative  update  (MU)  algorithm  for  computing  the  matrix  factors  has  contributed  to  the  success  of  NMF.  Despite  the  availability  of  algorithms  showing  faster  convergence,  MU  remains  popular  due  to  its  simplicity.  In  this  paper,  a  diagonalized  Newton  algorithm  (DNA)  is  proposed  showing  faster  convergence  while  the  implementation  remains  simple  and  suitable  for  high-rank  problems. The  DNA  algorithm  is  applied  to  various  publicly  available  data  sets,  showing  a  substantial  speed-up  on  modern hardware.      1   Int roduction    Non-negative  matrix  factorization  (NMF)  denotes  the  process  of  factorizing  a  N×T  data  matrix  V  of  non-negative  real  numbers  into  the  product  of  a  N×R  matrix  W  and  a  R×T  matrix  H,  where  both  W  and  H  contain  only  non-negative  real  numbers.  Taking  a  column- wise  view  of  the  data,  i.e.  each  of  the  T  columns  of  V  is  a  sample  of  N-dimensional  vector  data,  the  factorization  expresses  each  sample  as  a  (weighted)  addition  of  columns  of  W,  which can hence be interpreted as the R parts that make up the data [1]. Hence, NMF can be  used  to  learn  data  representations  from  samples.  In  [2],  speaker  representations  are  learnt  from  spectral  data  using  NMF  and  subsequently  applied  to  separate  their  signals.  Another  example  in speech processing is [3] and [4],  where phone representations are learnt using a  convolutional  extention  of  NMF.  In  [5],  time-frequency  representations  reminiscent  of  formant  traces  are  learnt  from  speech  using  NMF.  In  [6],  NMF  is  used  to  learn  acoustic  representations for words in a vocabulary acquisition and recognition task. Applied to image  processing,  local  features  are  learnt  from  examples  with  NMF  in  order  to  represent  human  faces in a detection task [7].    In this paper, the metric to measure the closeness of reconstruction Z = WH to its target V is  measured by their Kullback-Leibler divergence:      1   2   3   4   5   6  7  8  9  10  11  12  13  14  15  16  17  18  19  20   21   22   23  24  25  26  27  28  29  30  31  32  33  34  35  36   37  38   39   (cid:1)(cid:2)(cid:3)(cid:4)VVVV,	ZZZZ(cid:9)=(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:12)(cid:13)(cid:14)(cid:19)(cid:13)(cid:14)(cid:20)−(cid:11)(cid:12)(cid:13)(cid:14)  (cid:13),(cid:14)  (cid:13),(cid:14)  +(cid:11)(cid:19)(cid:13)(cid:14) (cid:13),(cid:14)     (1)  40  41  42  43  44  45  46  47  48  49  50  51  52   53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76   77   78  79   80  81   82  83   Given  a  data  matrix  V,  the  matrix  factors  W  and  H  are  then  found  by  minimizing  cost  function  (1),  which  yields  the  maximum  likelihood  estimate  if  the  data  are  drawn  from  a  Poisson  distribution.  The  multiplicative  updates  (MU)  algorithm  proposed  in  [1]  solves  exactly  this  problem  in  an  iterative  manner.  Its  simplicity  and  the  availability  of  many  implementations make it a popular algorithm to date to solve NMF problems. However, there  are some drawbacks to the algorithm. Firstly, it only converges locally and is not guaranteed  to  yield  the  global  minimum  of  the  cost  function.  It  is  hence  sensitive  to  the  choice  of  the  initial guesses for W and H. Secondly, MU is very slow to converge. The goal of this paper  is  to  speed  up  the  convergence  while  the  local  convergence  property  is  retained.  The  resulting Diagonalized Newton Algorithm (DNA) uses only simple element-wise operations,  such  that  its  implementation  requires  only  a  few  tens  of  lines  of  code,  while  memory  requirements and computational efforts for a single iteration are about the double of an MU  update.   The  faster  convergence  rate  is  obtained  by  applying  Newton’s  method  to  minimize  dKL(V,WH) over W and H in alternation. Newton updates have been explored for the Frobe- nius  norm  to  measure  the  distance  between  V  and  Z  in  e.g.  [8]-[13].  Specifically,  in  [11]  a  diagonal  Newton  method  is  applied  Frobenius  norms.  For  the  Kullback-Leibler  divergence,  fewer studies are available. Since each optimization problem is multivariate, Newton updates  typically  imply  solving  sets  of  linear  equations  in  each  iteration.  In  [16],  the  Hessian  is  reduced  by  refraining  from  second  order  updates  for  the  parameters  close  to  zero.  In  [17],  Newton  updates  are  applied  per  coordinate,  but  in  a  cyclic  order,  which  is  troublesome  for  GPU implementations. In the proposed method, matrix inversion is avoided by diagonalizing  the Hessian matrix. The resulting updates resemble the ones derived in [18] to the extent that  they  involve  second  order  derivatives.  Important  differences  are  that  [18]  involves  the  non- negative k-residuals hence requiring flooring to zero. Of course, the diagonal approximation  may  affect  the  convergence  rate  adversely. Also,  Newton  algorithms  only  show  (quadratic)  convergence when the estimate is sufficiently close to the local minimum and therefore need  damping,  e.g.  Levenberg-Marquardt  as  in  [14],  or  step  size  control  as  in  [15]  and  [16].  In  DNA,  these  convergence  issues  are  addressed  by  computing  both  the  MU  and  Newton  solutions  and  selecting  the  one  leading  to  the  greatest  reduction  in  dKL(V,Z).  Hence,  since  the cost is non-decreasing under MU, it  will also be under DNA  updates. This robust safety  net  can  be  constructed  fairly  efficiently  because  the  quantities  required  to  compute  the  MU  have already been computed in the Newton update. The net result is that DNA iterations are  only about two to three  times as  slow as MU iterations, both on a CPU and on  a GPU. The  experimental  analysis  shows  that  the  increased  convergence  rate  generally  dominates  over  the increased cost per iteration such that overall balance is positive and can lead to speedups  of up to a factor 6.   2  NMF fo rmula tion    To  induce  sparsity  on  the  matrix  factors,  the  KL-divergence  is  often  regularized,  i.e.  one  seeks to minimize:   	(cid:1)(cid:2)(cid:3)(cid:4)VVVV,	WWWW	(cid:24)(cid:9)+ ρ(cid:11)(cid:25)(cid:13)(cid:26)  (cid:13),(cid:26)  + λ(cid:11)ℎ(cid:26)(cid:14)  (cid:26),(cid:14)     (2)  subject to non-negativity constraints on all entries of W and H. Here, ρ and λ are non-negative re- gularization parameters.   Minimizing the regularized KL-divergence (2) can be achieved by alternating updates of W and H  for which the cost is non-increasing. The updates for this form of block coordinate descent are:   (cid:24)# ≥%		&(cid:1)(cid:2)(cid:3)(cid:4)VVVV,	WWWW	(cid:24)#(cid:9)+ λ(cid:11)ℎ(cid:26)(cid:14)# (cid:24)				←	arg	min  (cid:26),(cid:14)  '   (3)  (				←	arg	min(# ≥%		&(cid:1)(cid:2)(cid:3)(cid:4)VVVV,	(#(cid:24)(cid:9)+ ρ(cid:11)(cid:25)(cid:13)(cid:26)#  (cid:13),(cid:26)  '   (4)  Because  of  the  symmetry  property  dKL(V,WH) = dKL(Vt,HtWt),  where  superscript-t  denotes  matrix transpose,  it suffices to consider only the  update on H. Furthermore, because of  the  summation  over  all  columns  in  (1),  minimization  (3)  splits  up  into  T  independent  optimiza- tion  problems.  Let  v  denote any column of V and let h denote the corresponding column of H,  then the following is the core minimization problem to be considered:   )# ≥%		(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)VVVV,	WWWW	)#(cid:9)+ λ*(cid:14))#(cid:9)  )				←	arg	min  where 1 denotes a vector of ones of appropriate length. The solution of (5) should satisfy the KKT  conditions, i.e. for all r with hr > 0   +(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)v	v	v	v	,	WWWW	)(cid:9)+ λ*(cid:14))(cid:9)  +ℎ(cid:26)  =−(cid:11)(cid:12)(cid:13) (cid:25)(cid:13)(cid:26) (cid:4)W	hW	hW	hW	h(cid:9)(cid:13)  (cid:13)  +(cid:11)(cid:25)(cid:13)(cid:26) (cid:13)  + λ=0   where  hr  denotes  the  r-th  component  of  h.  If  hr = 0,  the  partial  derivative  is  positive.  Hence  the  product of hr and the partial derivative is always zero for a solution of (5), i.e. for r = 1 … R:   (cid:11)(cid:12)(cid:13) (cid:25)(cid:13)(cid:26)ℎ(cid:26) (cid:4)W	hW	hW	hW	h(cid:9)(cid:13) (cid:13)  (cid:13)  −ℎ(cid:26)&(cid:11)(cid:25)(cid:13)(cid:26)  + λ'=0   (6)  Since W-columns with all-zeros do not contribute to Z, it can be assumed that column sums of W  are non-zero, so the above can be recast as:   where qn = vn / (Wh)n. To facilitate the derivations below, the following notations are introduced:    which are functions of h via q. The KKT conditions are hence recast as [20]   (cid:4)WWWW(cid:14)qqqq(cid:9)(cid:26) (cid:4)WWWW(cid:14)1111(cid:9)(cid:26)+ λ−ℎ(cid:26) =0  ℎ(cid:26) 1(cid:26) = (cid:4)WWWW(cid:14)qqqq(cid:9)(cid:26) (cid:4)WWWW(cid:14)1111(cid:9)(cid:26)+ λ−1  	1(cid:26)	ℎ(cid:26) =0					for	r	=	1	…	R  (cid:11)(cid:4)(cid:4)((cid:14)*(cid:9)(cid:26) + λ(cid:9)ℎ(cid:26) =(cid:11)(cid:12)(cid:13) (cid:13) (cid:26)     Finally, summing (6) over r yields   which is satisfied for any guess h by renormalizing:   ℎ(cid:26) ⇠ℎ(cid:26)  2 . 1   M u l t i p l i c a t i v e   u p d a t e s    vvvv(cid:14)1111  )(cid:14)(cid:4)((cid:14)*+ λ(cid:9)   (5)  (7)  (8)  (9)  (10)  (11)  For the  more generic class  of Bregman divergences, it  was shown in a.o. [20] that  multipli- cative updates (MU) are non-decreasing at each update of W and H. For KL-divergence, MU  are identical to a fixed point update of (6), i.e.   ℎ(cid:26) ⇠ℎ(cid:26)(cid:4)1+1(cid:26)(cid:9)=ℎ(cid:26)  (cid:4)WWWW(cid:14)qqqq(cid:9)(cid:26) (cid:4)WWWW(cid:14)1111(cid:9)(cid:26)+ λ     Update (11) has two fixed points: hr = 0 and ar = 0. In the former case, the KKT conditions imply  that ar is negative.   105      84  85  86  87  88   89  90   91  92   93  94   95   96   97   98   99   100  101  102   103  104   106   107  108   109   2 . 2   N e w t o n   u p d a t e s    To find the stationary points of (2), R equations (8) need to be solved for h.  In general, let g(h) be  an R-dimensional vector function of an R-dimensional variable h. Newton’s update then states:   Applied to equations (8):   )⇠)−	(cid:4)∇8(cid:9)9:8(cid:4))(cid:9)			with			(cid:4)∇8(cid:9)(cid:26)= =∂(cid:17)(cid:26)(cid:4))(cid:9) ∂ℎ= (cid:4)WWWW(cid:14)1111(cid:9)(cid:26)+ λ(cid:11)(cid:12)@(cid:25)@(cid:26)(cid:25)@= ℎ(cid:26) (cid:4)()(cid:9)@A  (cid:4)∇8(cid:9)(cid:26)= =1=?(cid:26)=−        @  (12)  (13)  110  111  112   where  δrl  is  Kronecker’s  delta.  To  avoid  the  matrix  inversion  in  update  (12),  the  last  term  in  equation (13) is diagonalized, which is equivalent to solving the r-th equation in (8) for hr with all  other components fixed. With   which is always positive, an element-wise Newton update for h is obtained:   B(cid:26) =     (cid:4)WWWW(cid:14)1111(cid:9)(cid:26)+ λ(cid:11)(cid:12)(cid:13) (cid:25)(cid:13)(cid:26)A 1 (cid:4)W	hW	hW	hW	h(cid:9)(cid:13)A ℎ(cid:26) ⇠ℎ(cid:26)  (cid:13) ℎ(cid:26)B(cid:26) ℎ(cid:26)B(cid:26)−1(cid:26)   Notice  that  this  update  does  not  automatically  satisfy  (9),  so  updates  should  be  followed  by  a  renormalization  (10).  One  needs  to  pay  attention  to  the  fact  that  Newton  updates  will  attract  towards both local  minima and local maxima.  Like for the  EM-update, hr = 0 and ar = 0 are the  only fixed points of update (15), which are now shown to be locally stable. In case the optimizer is  at  hr  =  0,  ar  is  negative  by  the  KKT  conditions,  and  update  (15)  will  indeed  decrease  hr.  In  a  sufficiently small neighborhood of a point where the gradient vanishes, i.e. ar = 0, update (15) will  increase  (decrease)  hr  if  and  only  if  (11)  increases  (decreases)  its  estimate.  Since  if  (11)  never  increases the cost, update (15) attracts to a minimum.   However,  this  only  guarantees  local  convergence  for  per-element  updates  and  Newton  methods  are known to suffer from potentially small convergence regions. This also applies to update (15),  which can  indeed result in limit cycles  in  some cases. In the next  subsections, two  measures are  taken to respectively increase the convergence region and to make the update non-increasing.   2 . 3   S t e p   s i z e   l i mi t a t i o n    When  ar  is  positive,  update  (15)  may  not  be  well-behaved  in  the  sense  that  its  denominator  can  become  negative  or  zero.  To  respect  nonnegativity  and  to  avoid  the  singularity,  it  is  bounded  below by a function with the same local behavior around zero:    Hence, if ar ≥ 0, the following update is used:   1 1− 1(cid:26)ℎ(cid:26)B(cid:26)  ℎ(cid:26)B(cid:26) ≥1+ 1(cid:26)ℎ(cid:26)B(cid:26)  ℎ(cid:26)B(cid:26)−1(cid:26) = ℎ(cid:26) ⇠ℎ(cid:26)(cid:18)1+ 1(cid:26)ℎ(cid:26)B(cid:26)(cid:20)=ℎ(cid:26)+1(cid:26)B(cid:26)    Finally, step sizes are further limited by flooring resp. ceiling the multiplicative gain applied to hr  in update (15) and (17) (see Algorithm 1, steps 11 and 24 for details).   2 . 4   N o n - i n c r e a s e   o f   t h e   c o s t    Despite  the  measures  taken  in  section  2.3,  the  divergence  can  still  increase  under  the  Newton  update.  A  very  safe  option  is  to  compute  the  EM  update  additionally  and  compare  the  cost  function value for both updates. If the EM update is be better, the Newton update is rejected and   113   114  115  116  117  118  119  120  121   122  123  124  125   126   127  128  129   130  131   132   133  134  135   (14)  (15)  (16)  (17)  136  137  138   139   140  141  142   143   144  145  146   147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185         the  EM  update  is  taken  instead.  This  will  guarantee  non-increase  of  the  cost  function.  The  computational  cost  of  this  operation  is  dominated  by  evaluating  the  KL-divergence,  not  in  computing the update itself.   3  The  Diag onali zed  Newton  Algo ri th m fo r  KL D-NMF   In Algorithm 1, the arguments given above are joined to form the Diagonalized Newton Algorithm  (DNA)  for  NMF  with  Kullback-Leibler  divergence  cost.  MatlabTM  code  is  available  from  www.esat.kuleuven.be/psi/spraak/downloads both for the case when V is sparse or dense.      Algorithm 1: pseudocode for the DNA KLD-NMF algorithm. ⊘ and ⊙ are element-wise division   and multiplication respectively and [x]ε = max(x,ε). Steps not labelsed with “MU” is the additional   code required for DNA.   Input: data V, initial guess for W and H, regularization weights ρ and λ.   MU - Step 1: divide the r-th column of W by ∑ (cid:25)(cid:13)(cid:26) MU - Step 3: F=G⊘H   MU - Step 2: Z = WH   Repeat until convergence   (cid:13)  + λ. Multiply the r-th row of H by the same number.   MU - Step 4: precompute (⨀(  MU - Step 5: J=((cid:14)F−1  MU - Step 6: (cid:24)KL =(cid:24)+J⨀(cid:24)  MU - Step 7: HKL =((cid:24)KL   MU - Step 8: FMN =G⊘HKL  MU - Step 9: OKL =*(cid:14)PG⨀log	(cid:4)FKL(cid:9)R  Step 10: FS =G⊘(cid:4)T⨀T(cid:9);	U=(cid:4)(⨀((cid:9)VFS   Step 11:  (cid:24)WXY =	(cid:24)⊙ZU⊘(cid:4)U−J(cid:9)[\for the entries for which A < 0  Step 12: HWXY =((cid:24)WXY  Step 13: Fabc =G⊘Habc  Step 14: OWXY =*(cid:14)PG⨀log	(cid:4)FWXY(cid:9)R   (cid:24)WXY =	(cid:24)+]^_(cid:4)J⊘(cid:4)(cid:24)⊙U(cid:9),`(cid:24)(cid:9) for the entries for which A ≥ 0  multiply t-th column of HDNA with the t-th entry of (cid:4)*(cid:14)G(cid:9)⊘(cid:4)*(cid:14)(cid:24)WXY(cid:9)   Step 15:  copy H, Z and Q from:      (cid:14)  HDNA, ZDNA and QDNA for the columns for which dDNA < dMU  HEM, ZEM and QEM for the columns for which dDNA ≥ dMU   + ρ.    MU - Step 16: divide (multiply) the r-th row (column) of H (W) by ∑ ℎ(cid:13)(cid:14) Step 17: precompute (cid:24)⨀(cid:24)   MU - Step 18: J=F(cid:24)V−1  MU - Step 19: (KL =(+J⨀(  MU - Step 20: HKL =(KL(cid:24)  MU - Step 21: FMN =G⊘HKL  MU - Step 22: OKL =PG⨀log	(cid:4)FKL(cid:9)R*  Step 23: FS =G⊘(cid:4)T⨀T(cid:9);	U=FS(cid:4)(cid:24)⨀(cid:24)(cid:9)V   Step 24:  (WXY =	(⊙ZU⊘(cid:4)U−J(cid:9)[\for the entries for which A < 0  Step 25: HWXY =(WXY(cid:24)  Step 26: Fabc =G⊘Habc  Step 27: OWXY =PG⨀log	(cid:4)FWXY(cid:9)R*   (WXY =	(+]^_(cid:4)J⊘(cid:4)(cid:24)⊙U(cid:9),`((cid:9) for the entries for which A ≥ 0  multiply the n-th row of WDNA with the n-th entry of (cid:4)G*(cid:9)⊘(cid:4)(WXY*(cid:9)   Step 28:  copy W, Z and Q from:      WDNA, ZDNA and QDNA for the rows for which dDNA < dMU  WEM, ZEM and QEM for the rows for which dDNA ≥ dMU   MU - Step 29: divide(multiply) the r-th column (row) of W (H) by ∑ (cid:25)(cid:13)(cid:26)  (cid:13)  + λ.    186  187  188  189  190  191   192   193  194  195  196  197  198  199  200   201   202  203  204  205  206  207  208   209   210  211   212  213  214  215  216   Notice that step 9, 14 22 and 27 require some care for the zeros in V, which should not contribute  to the cost. In terms of complexity, the most expensive steps are the computation of A, B, ZMU and  ZDNA,  which  require  O(NRT)  operations.  All  other  steps  require  O(NR),  O(RT)  or  O(NR)  operations. Hence, it is expected that a DNA iteration is about twice as slow as MU iteration. On  modern  hardware,  parallelization  may  however  distort  this  picture  and  hence  experimental  verification is requied.   4  Experi men ts   DNA and MU are run on several publicly available1 data sets. In all cases, W is initialized with a  random matrix with uniform distribution, normalized column-wise. Then H is initialized as WtV  and one MU iteration is performed. The same initial values are used for both algorithms. Sparsity  is not included in this study, so ρ = λ = 0. The algorithm parameters are set to ε = 0.01 and  α=4.  CPU timing measurements are obtained on a quad-core AMDTM Opteron 8356 processor running  the  MATLABTM  code  available  at  www.esat.kuleuven.be/psi/spraak/downloads  which  uses  the  built-in  parallelization  capability.  Timing  measurements  on  the  graphical  processing  unit  (GPU)  are obtained on a TESLA C2070 running MATLAB and Accelereyes Jacket v2.3.   4 . 1   D e n s e   d a t a   ma t r i c e s    The  first  dataset  considered  is  a  set  of  400  frontal  face  greyscale  64×64  images  of  40  people  showing  10  different  expressions.    The  resulting  4096×165  dense  matrix  is  decomposed  with  factors of a common dimension R of 10, 20, 40 and 80. Figure 1 shows the  KL divergence as a  function of iteration number and CPU time as measured on the CPU. The superiority of DNA is  obvious:  for  instance,  at  R  =  40,  DNA  reaches  the  same  divergence  after  33  iterations  as  MU  obtains after 500 iterations. This implies a speed-up of a factor 15 in terms of iterations or 6.3 in  terms of CPU time.    6 x 10  7  6  5  4  3  2  1  D L K     MU DNA  R=10  R=20  R=40 R=80  6 x 10  7  6  5  4  3  2  1  D L K     MU DNA  R=10  R=20  R=40 R=80  0   0  100  200  300  400  500  iteration  0   0  10  20  30  40  50  time (s)     Figure 1: convergence of DNA and MU on the ORL image dataset as a function of the number of   iterations (left) and CPU time (right) for different ranks R.   The second test case is the CMU PIE dataset which consists of 11554 greyscale images of 32×32  pixels  showing  human  faces  under  different  illumination  conditions  and  poses.  The  data  are  shaped  to  a  dense  1024×11554  matrix  and  a  decomposition  of  rank  R  =  10,  20,  40  and  80  are  attempted  with  the  MU  and  DNA  algorithms.    As  observed  in  Figure  2,  the  proposed DNA  still  outperforms MU, but by a smaller margin.                                                               1 www.cad.zju.edu.cn/home/dengcai/Data/data.html   217   218  219   220  221  222  223  224  225   226   227  228  229  230  231  232  233  234   Figure 2: convergence of DNA and MU on the CMU PIE image dataset as a function of the   number of iterations (left) and CPU time (right).   An overview of the time required for a single iteration on both data sets is given in Table 1. For  MU, the first row lists the time if the KL divergence is not computed as this is not required if the  number of iterations is fixed in advance instead of stopping the algorithm based on a decrease in  KLD. The table shows that the computational cost of MU can be reduced by about a third by not  computing  KLD.  Compared  to  MU  with  cost  calculation,  DNA  requires  typically  about  2.5  to 3  times more time per iteration on the CPU. On the GPU, the ratio is rather 2 to 2.5.        4 . 2   S p a r s e   d a t a   ma t r i c e s    The  third  matrix  considered  originates  from  the  NIST  Topic  Detection  and  Tracking  Corpus  (TDT2).  For  10212  documents  (columns  of  V),  the  frequency  of  36771  terms  (rows  of  V)  was  counted  leading  to  a  sparse  36771×10212    matrix  with  only  0.35%  non-zero  entries.  The  fourth  matrix  originates  from  the  Newsgroup  corpus  results  in  a  61188×18774  sparse  frequency  matrix  with 0.2% non-zeros. Both for MU and DNA a MATLAB implementation using the sparse matrix  class  was  made.  In  this  case,  an  iteration  of  DNA  is  twice  as  slow  a  MU  iteration.  Again,  the  convergence of both algorithms is shown in Figure 3. In this case, DNA is only marginally faster  than MU in terms of CPU time.   235      236  237   Table 1: time per iteration in milliseconds as measured on the CPU and GPU implementations for   different ranks (R) and dense matrices (ORL/PIE).     dataset  R  ORL  PIE  10  20  40  80  10  20  40  80  processor  CPU GPU CPU GPU CPU GPU CPU GPU CPU GPU CPU GPU CPU GPU CPU GPU  MU without cost  78 3.7  85 4.1  96 5.0 115 6.8 310  18 310  20 330  27 400  MU with cost  114 6.4 118 7.0 130 7.9 161 9.4 480  35 490  37 510  44 580  38  55  238  239   DNA    269 15.5 280 15.9 319 17.9 425 23.6 1180  71 1430  76 1720  95 1960 125  6 x 10  7  6.5  6  D L K  5.5  5  4.5  4   0     MU DNA  R=10  R=20  R=40  R=80  20  40  60  80  100  iteration  6 x 10  11  10.5  10  9.5  9  8.5  D L K  8    0     MU DNA  R=10  R=20  R=40  R=80  20  40  60  80  100  iteration     Figure 3: convergence of DNA and MU on the sparse TDT2 (left) and Newsgroup (right) data.   5  Con clusion s    The  DNA  algorithm  is  based  on  Newton’s  method  for  solving  the  stationarity  conditions  of  the  constrained  optimization  problem  implied  by  NMF.  This  paper  only  addresses  the  Kullback- Leibler  divergence  as  a  cost  function.  To  avoid  matrix  inversion,  a  diagonal  approximation  is  made, resulting in element-wise updates. Experimental verification on publicly available matrices  with  a  CPU  and  GPU  MATLAB  implementation  for  dense  data  matrices  and  a  CPU  MATLAB  implementation for sparse data matrices show that, depending on the case and matrix sizes, DNA  iterations are 2 to 3 times slower than MU iterations. In most cases, the diagonal approximation is  good enough such that faster convergence is observed and a net gain results.   Since Newton updates can in general not ensure monotonic decrease of the cost function, the step  size was controlled with a brute force strategy of falling back to MU in case the cost is increased.  More  refined  step  damping  methods  could  speed  up  DNA  by  avoiding  evaluations  of  the  cost  function, which is next on the research agenda.   A c k n o w l e d g m e n t s    This work is supported by IWT-SBO project 100049 (ALADIN) and by KU Leuven research  grant OT/09/028(VASI).      240   241   242   243  244  245  246  247  248  249  250   251  252  253  254   255   256  257   258   259  260   261  262   263  264   265  266  267   268  269  270   271  272  273   274  275  276   277  278   279  280   281  282   283  284   285  286   287  288  289   290  291   292  293   294  295   296  297  298   299  300  301   302  303   304  305   R e f e r e n c e s    [1]  D.  Lee,  and  H.  Seung,  “Algorithms  for  non-negative  matrix  factorization,”  Advances  in  Neural  Information Processing Systems, vol. 13, pp. 556–562, 2001.   [2]  B. Raj, R. Singh and P. Smaragdis, “Recognizing Speech from Simultaneous Speakers”, in proceedings  of Eurospeech, pp. 3317-3320, Lisbon, Portugal, September 2005   [3]  P.  Smaragdis,  “Convolutive  Speech  Bases  and  Their  Application  to  Supervised  Speaker  Separation,”  IEEE Transactions on Audio, Speech and Language Processing, vol. 15, pp. 1-12, January 2007    [4]  P.  D.  O'Grady  and  B.  A.  Pearlmutter,  “Discovering  Speech  Phones  Using  Convolutive  Non-negative  Matrix Factorisation with a Sparseness Constraint.,” Neurocomputing, vol. 72, no. 1-3, pp. 88-101, December  2008, ISSN 0925-2312.   [5]  M.  Van  Segbroeck  and  H.  Van  hamme,  “Unsupervised  learning  of  time-frequency  patches  as  a  noise- robust  representation  of  speech,”  Speech  Communication,  volume  51,  no.  11,  pp.  1124-1138,  November  2009.   [6]  H.  Van  hamme,  “HAC-models:  a  Novel  Approach  to  Continuous  Speech  Recognition,”  In  Proc.  International  Conference  on  Spoken  Language  Processing,  pp.  2554-2557,  Brisbane,  Australia,  September  2008.   [7]  X. Chen, L. Gu, S. Z. Li and H.-J. Zhang, “Learning representative local features for face detection,” in  proceedings  of  the  IEEE  Computer  Society  Conference  on  Computer  Vision  and  Pattern  Recognition,  pp.  1126-1131, Kauai, HI, USA, December 2001.   [8]  D.  Kim,  S.  Sra  and  I.  S.  Dhillon,  “Fast  Projection-Based  Methods  for  the  Least  Squares  Nonnegative  Matrix Approximation Problem,” Statistical Analy Data Mining, vol. 1, 2008   [9]  C.-J. Lin, “Projected gradient methods for non-negative matrix factorization,” Neural Computation, vol.  19, pp. 2756-2779, 2007   [10] R.  Zdunek,  A.  H.  Phan  and  A.  Cichocki,  “Damped  Newton  Iterations  for  Nonnegative  Matrix  Factorization,” Australian Journal of Intelligent Information Processing Systems, 12(1), pp. 16-22, 2010   [11] Y.  Zheng,  and  Q.  Zhang,  “Damped  Newton  based  Iterative  Non-negative  Matrix  Factorization  for  Intelligent Wood Defects Detection,” Journal of software, vol. 5, no. 8, pp. 899-906, August 2010.   [12] P.  Gong,  and  C.  Zhang,  “Efficient  Nonnegative  Matrix  Factorization  via  projected  Newton  method”,  Pattern Recognition, vol. 45, no. 9, pp. 3557-3565, September 2012.   [13] S. Bellavia, M. Macconi, and B. Morini, “An interior point Newton-like method for nonnegative least- squares problems with degenerate solution,” Numerical Linear Algebra with Applications, vol. 13, no. 10, pp.  825-846, December 2006.   [14] R.  Zdunek  and  A.  Cichocki,  “Non-Negative  Matrix  Factorization  with  Quasi-Newton  Optimization,”   Lecture Notes in Computer Science, Artificial Intelligence and Soft Computing 4029, pp. 870-879, 2006   [15] R.  Zdunek  and  A.  Cichocki,  ""Nonnegative  Matrix  Factorization  with  Constrained  Second-Order  Optimization"", Signal Processing, vol. 87, pp. 1904-1916, 2007   [16] G. Landi and E. Loli Piccolomini, “A projected Newton-CG method for nonnegative astronomical image  deblurring,” Numerical Algorithms, no. 48, pp. 279–300, 2008   [17] C.-J.  Hsieh  and  I.  S.  Dhillon,  “Fast  Coordinate  Descent  Methods  with  Variable  Selection  for  Non- negative  Matrix  Factorization,”  in  proceedings  of  the  17th  ACM  SIGKDD  International  Conference  on  Knowledge Discovery & Data Mining (KDD), San Diego, CA, USA, August 2011   [18] L.  Li,  G.  Lebanon  and  H.  Park,  “Fast  Bregman  Divergence  NMF  using  Taylor  Expansion  and  Coordinate  Descent,”  in  proceedings  of  the  18th  ACM  SIGKDD  Conference  on  Knowledge  Discovery  and  Data Mining, 2012   [19] A. Cichocki, S. Cruces, and S.-I. Amari, “Generalized Alpha-Beta Divergences and Their Application to  Robust Nonnegative Matrix Factorization,” Entropy, vol. 13, pp. 134-170, 2011; doi:10.3390/e13010134   [20] I. S. Dhillon and S. Sra, “Generalized Nonnegative Matrix Approximations with Bregman Divergences,”  Neural Information Proc. Systems, pp. 283-290, 2005   ","Non-negative matrix factorization (NMF) has become a popular machine learningapproach to many problems in text mining, speech and image processing,bio-informatics and seismic data analysis to name a few. In NMF, a matrix ofnon-negative data is approximated by the low-rank product of two matrices withnon-negative entries. In this paper, the approximation quality is measured bythe Kullback-Leibler divergence between the data and its low-rankreconstruction. The existence of the simple multiplicative update (MU)algorithm for computing the matrix factors has contributed to the success ofNMF. Despite the availability of algorithms showing faster convergence, MUremains popular due to its simplicity. In this paper, a diagonalized Newtonalgorithm (DNA) is proposed showing faster convergence while the implementationremains simple and suitable for high-rank problems. The DNA algorithm isapplied to various publicly available data sets, showing a substantial speed-upon modern hardware."
1109.0093,2013,Local Component Analysis  ,"['Nicolas Le Roux', 'Francis Bach']",https://arxiv.org/pdf/1109.0093.pdf,"2 1 0 2    c e D 0 1         ]  G L . s c [      4 v 3 9 0 0  .  9 0 1 1 : v i X r a  Local Component Analysis  Nicolas Le Roux  Francis Bach  nicolas@le-roux.name  francis.bach@ens.fr  INRIA - SIERRA Project - Team  Laboratoire d’Informatique de l’´Ecole Normale Sup´erieure  Paris, France  Abstract  Kernel density estimation, a.k.a. Parzen windows, is a popular density estimation method, which can be used for outlier detection or clustering. With mul- tivariate data, its performance is heavily reliant on the metric used within the kernel. Most earlier work has focused on learning only the bandwidth of the kernel (i.e., a scalar multiplicative factor). In this paper, we propose to learn a full Euclidean metric through an expectation-minimisation (EM) procedure, which can be seen as an unsupervised counterpart to neighbourhood component analysis (NCA). In order to avoid overﬁtting with a fully nonparametric den- sity estimator in high dimensions, we also consider a semi-parametric Gaussian-Parzen density model, where some of the variables are modelled through a jointly Gaussian density, while others are modelled through Parzen windows. For these two models, EM leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions. We show empirically that our method leads to density estima- tors with higher test-likelihoods than natural compet- ing methods, and that the metrics may be used within most unsupervised learning techniques that rely on lo- cal distances, such as spectral clustering or manifold learning methods. Finally, we present a stochastic ap- proximation scheme which allows for the use of this method in a large-scale setting.  1  Introduction  Most unsupervised learning methods rely on a metric on the space of observations. The quality of the met- ric directly impacts the performance of such techniques and a signiﬁcant amount of work has been dedicated to learning this metric from data when some supervised information is available [27, 16, 2]. However, in a fully unsupervised scenario, most practitioners use the Ma-  halanobis distance obtained from principal component analysis (PCA). This is an unsatisfactory solution as PCA is essentially a global linear dimension reduction method, while most unsupervised learning techniques, such as spectral clustering or manifold learning, are local.  In this paper, we cast the unsupervised metric learning as a density estimation problem with a Parzen win- dows estimator based on a Euclidean metric. Using the maximum likelihood framework, we derive in Sec- tion 3 an expectation-minimisation (EM) procedure that maximizes the leave-one-out log-likelihood, which may be considered as an unsupervised counterpart to neighbourhood component analysis (NCA) [16]. As opposed to PCA, which performs a whitening of the data based on global information, our new algorithm globally performs a whitening of the data using only local information, hence the denomination local com- ponent analysis (LCA).  Like all non-parametric density estimators, Parzen windows density estimation is known to overﬁt in high dimensions [25], and thus LCA should also overﬁt. In order to keep the modelling ﬂexibility of our density es- timator while avoiding overﬁtting, we propose a semi- parametric Parzen-Gaussian model; following [4], we linearly transform then split our variables in two parts, one which is modelled through a Parzen windows es- timator (where we assume the interesting part of the data lies), and one which is modelled as a multivari- ate Gaussian (where we assume the noise lies). Again, in Section 4, an EM procedure for estimating the lin- ear transform may be naturally derived and leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions. This procedure con- tains no hyperparameters, all the parameters being learnt from data.  Since the EM formulation of LCA scales quadratically in the number of datapoints, making it impractical for large datasets, we introduce in Section 5 both a stochastic approximation and a subsampling technique  1  allowing us to achieve a linear cost and thus to scale LCA to much larger datasets.  Finally, in Section 6, we show empirically that our method leads to density estimators with higher test- likelihoods than natural competing methods, and that the metrics may be used within unsupervised learn- ing techniques that rely on such metrics, like spectral clustering.  2 Previous work  Many authors aimed at learning a Mahalanobis dis- tance suited for local learning. While some techniques required the presence of labelled data [16, 27, 2], oth- ers proposed ways to learn the metric in a purely un- supervised way, e.g., [28] who used the distance to the k-th nearest neighbour as the local scaling around each datapoint. Most of the other attempts at unsuper- vised metric learning were developed in the context of kernel density estimation, a.k.a. Parzen windows. The Parzen windows estimator [21] is a nonparametric density estimation model which, given n datapoints {x1, . . . , xn} in Rd, deﬁnes a mixture model of the form p(x) = 1 j=1 K(x, xj, θ) where K is a kernel with compact support and parameters θ. We relax the compact support assumption and choose K to be the normal kernel, that is  nPn  N (x, xj , Σ)  p(x) =  ∝  1 n  nXj=1 np|Σ|  1  exp(cid:20)−  (x − xj)⊤Σ−1(x − xj)(cid:21) ,  1 2  nXj=1  where Σ is the covariance matrix of each Gaussian. As the performance of the Parzen windows estimator is more reliant on the covariance matrix than on the kernel, there has been a large body of work, originating from the statistics literature, attempting to learn this matrix. However, almost all attempts are focused on the asymptotic optimality of the estimators obtained with little consideration for the practicality in high dimensions. Thus, the vast majority of the work is limited to isotropic matrices, reducing the problem to ﬁnding a single scalar h [22, 13, 23, 9, 7, 20, 24], the bandwidth, and the few extensions to the non-isotropic cases are numerically expensive [14, 18].  An exception is the approach proposed in [26], which is very similar to our method, as the authors learn the covariance matrix of the Parzen windows estimator us- ing local neighbourhoods. However, their algorithm does not minimize a well-deﬁned cost function, mak- ing it unsuitable for kernels other than the Gaussian one, and the locality used to compute the covariance  matrix depends on parameters which must be hand- tuned or cross-validated. Also, the modelling of all the dimensions using the Parzen windows estimator makes the algorithm unsuitable when the data lie on a high-dimensional manifold. In an extension to [26], [3] uses a neural network to compute the leading eigen- vectors of the local covariance matrix at every point in the space, then uses these matrices to do density estimation and classiﬁcation. Despite the algorithm’s impressive performance, it does not correspond to a linear reparametrisation of the space and thus cannot be used as a preprocessing step.  3 Local Component Analysis  Seeing the density as a mixture of Gaussians, one can easily optimize the covariances using the EM algo- rithm [12]. However, maximizing the standard log- likelihood of the data would trivially lead to the degen- erate solution where Σ goes to 0 to yield a sum of Dirac distributions. One solution to that problem is to pe- nalize some norm of the precision matrix to prevent it from going to inﬁnity. Another, more compelling, way is to optimize the leave-one-out log-likelihood, where the probability of each datapoint xi is computed un- der the distribution obtained when xi has been re- moved from the training set. This technique is not new and has already been explored both in the su- pervised [16, 15] and in the unsupervised setting [13]. However, in the latter case, the cross-validation was then done by hand, which explains why only one band- width parameter could be optimized1. We will thus use the following criterion:  L(Σ) = −  nXi=1  ≤ cst −  logh nXi=1Xj6=i  1  n − 1Xj6=i  N (xi, xj, Σ)i  (1)  λij log N (xi, xj, Σ)  +  nXi=1Xj6=i  λij log λij ,  (2)  with the constraints ∀i , Pj6=i λij = 1. This varia-  tional bound is obtained using Jensen’s inequality.  The EM algorithm optimizes the right-hand side of Eq. (2) by alternating between the optimisations of λ and Σ in turn. The algorithm is guaranteed to con- verge, and does so to a stationary point of the true  1Most of the literature on estimating the covariance matrix discards the log-likelihood cost because of its sensitivity to out- liers and prefers AMISE (see, e.g., [14]). However, in all our experiments, the number of datapoints was large enough so that LCA did not suﬀer from the presence of outliers.  2  function over Σ deﬁned in Eq. (1). At each step, the optimal solutions are:  4 LCA with a multiplicative  Gaussian component  if j 6= i  λ∗ ij =  λ∗ ii = 0  N (xi, xj, Σ)  Pk6=i N (xi, xk, Σ)  Σ∗ = Pij λij (xi − xj )(xi − xj)T  n  (3)  (4)  (5)  .  The “responsibilities” λ∗ ij deﬁne the relative proximity of xj to xi (compared to the proximity of all the xk’s to xi) and Σ∗ is the average of all the local covariance matrices.  This algorithm, which we coin LCA, for local compo- nent analysis, transforms the data to make it locally isotropic, as opposed to PCA which makes it glob- ally isotropic. Fig. (1) shows a comparison of PCA and LCA on the word sequence “To be or not to be”. Whereas PCA is highly sensitive to the length of the text, LCA is only aﬀected by the local shapes, thus providing a much less distorted result.2 First, one may note at this point that Manifold Parzen Windows [26] is equivalent to LCA with only one step of EM. This makes Manifold Parzen Windows more sensitive to the choice of the original covariance ma- trix whose parameters must be carefully chosen. As we shall see later in the experiments, running EM to con- vergence is important to get good accuracy when using spectral clustering on the transformed data. Second, it is also worth noting that, similarly to Manifold Parzen Windows, LCA can straightforwardly be extended to the cases where each datapoint uses its own local co- variance matrix (possibly with a smoothing term), or where the covariance Σ∗ is the sum of a low-rank ma- trix and some scalar multiplied by the identity matrix.  Not only may LCA be used to learn a linear trans- formation of the space, but it also deﬁnes a density model. However, there are two potential negative as- pects associated with this method. First, in high di- mensions, Parzen windows is prone to overﬁtting and must be regularized [25]. Second, if there are some directions containing a small Gaussian noise, the local isotropy will blow them up, swamping the data with clutter. This is common to all the techniques which renormalise the data by the inverse of some variance. A solution to both of these issues is to consider a prod- uct of two densities: one is a low-dimensional Parzen windows estimator, which will model the interesting signal, and the other is a Gaussian, which will model the noise.  2Since both methods  any linear reparametrisation of the data, we do not include the original data in the ﬁgure.  insensitive  are  to  3  We now assume that there are irrelevant dimensions in our data which can be modelled by a Gaussian. In other words, we consider an invertible linear transfor- mation (BG, BL)⊤x of the data, modelling B⊤ G x as a multivariate Gaussian and B⊤ L x through kernel density estimation, the two parts being independent, leading to p(x) ∝ p(B⊤ L x), where pG is a Gaussian and pL is the Parzen windows esti- mator, i.e.,  L x) = pG(B⊤  G x)pL(B⊤  G x, B⊤  p(xi) ∝ (cid:12)(cid:12)BGB⊤ × exp(cid:20)− × Xj6=i  1  2  L(cid:12)(cid:12)  G + BLB⊤ n − 1 1 2  exp(cid:20)−  1 2  (xi − µ)⊤BGB⊤  G (xi − µ)(cid:21)  (xi − xj)⊤BLB⊤  L (xi − xj)(cid:21)  ,  with (BG, BL) a full-rank square matrix. Using EM, we can upper-bound the negative log-likelihood:  −2Xi  with  log p(xi) ≤ tr(B⊤  G CGBG) + tr(B⊤  L CLBL)  − log |BGB⊤  G + BLB⊤  L | ,  (6)  (xi − µ)(xi − µ)⊤ ,  λij (xi − xj )(xi − xj)⊤ .  CG =  CL =  1  nXi nXij  1  The matrices BG and BL minimizing the right-hand side of Eq. (6) may be found using the following propo- sition (see proof in the appendix):  Proposition 1 Let BG ∈ Rd×d1 and BL ∈ Rd×d2, with d = d1 + d2 and B = (BG, BL) ∈ Rd×d invertible. Consider two symmetric positive matrices M1 and M2 in Rd×d. The problem  min BG,BL  tr B⊤  G M1BG+tr B⊤  L M2BL−log det(BGB⊤  G +BLB⊤ L )  (7) has a ﬁnite solution only if M1 and M2 are invertible and, if these conditions are met, reaches its optimum at  BG = M −1/2  1  U+ , BL = M −1/2  1  U−D−1/2  −  ,  where U+ are the eigenvectors of M −1/2 as- sociated with eigenvalues greater than or equal to 1, U− are the eigenvectors of M −1/2 associated  1 M2M −1/2  1 M2M −1/2  1  1  Figure 1: Results obtained when transforming “To be or not to be” using PCA (left) and LCA (right). Left: To make the data globally isotropic, PCA awkwardly compresses the letters horizontally. Right: Since LCA is insensitive to the spacing between letters and to the length of the text, there is no horizontal compression.  with eigenvalues smaller than 1 and D− is the diagonal 1 M2M −1/2 matrix containing the eigenvalues of M −1/2 smaller than 1.  1  The resulting procedure is described in Algorithm 1, where all dimensions are initially modelled by the Parzen windows estimator, which empirically yielded the best results.  Algorithm 1 LCA - Gauss Input: X (dataset), iterMax (maximum number of  iterations), ν (regularisation)  Output: BG (Gaussian part transformation), BL  {Initialize C to the global  2  (Parzen windows transformation) CG ← cov(X) + νId covariance} − 1 IG ← C G BG = 0, BL = chol(C −1 the Parzen windows estimator} for iter = 1:iterMax do  Mij ← exph− (xi−xj)⊤B⊤  λij ← Mij  Pk Mik Pij λij (xi−xj)(xi−xj )⊤  G ) {Assign all dimensions to  L BL(xi−xj ) 2  i, Mii ← 0  n  + νId  CL ← [V, D] ← eig(IGCLIG) {Eigendecomposition of IGCLIG} t1 = maxz D(z, z) ≤ 1 {Cut-oﬀ between eigenval- ues smaller and larger than 1} t+ = {t|t1 ≤ t ≤ d} , t− = {t|1 ≤ t < t1} V+ ← V (:, t+), D(t−, t−) BL = IGV−D−1/2  V− ← V (:, t−),  , BG = IGV+  D− =  −  end for  Independent component Relationship with ICA. analysis (ICA) can be seen as a density model where x = As and s has independent components (see, e.g., [17]). In the Parzen windows framework, this cor- responds to modelling the density of s by a product of univariate kernel density estimators [6]. This however causes two problems: ﬁrst, while this assumption is appropriate in settings such as source separation, it is violated in most settings, and having a multivariate kernel density estimation is preferable. Second, most algorithms are dedicated to ﬁnding independent com- ponents which are non-Gaussian. In the presence of  more than one Gaussian dimension, most ICA frame- works become unidentiﬁable, while our explicit mod- elling of such Gaussian components allows us to tackle this situation (a detailed analysis of the identiﬁability of our Parzen/Gaussian model is out of the scope of this paper).  Relationship with NGCA. NGCA [4] makes an assumption similar to ours (they rather assume an additive Gaussian noise on top of a low-dimensional non-Gaussian signal) but uses a projection pursuit algorithm to iteratively ﬁnd the directions of non- Gaussianity. Unlike in FastICA, the contrast functions used to ﬁnd the interesting directions can be diﬀerent for each direction. However, like all projection pur- suit algorithms, the identiﬁcation of interesting direc- tions gets much harder in higher dimensions, as most of them will be almost Gaussian. Our use of a non- parametric density estimator with a log-likelihood cost allows us to globally optimize all directions simultane- ously and does not rely on the model being correct. Finally, LCA estimates all its parameters from data as opposed to NGCA which requires the number of non-Gaussian directions to be set.  G  G CLC −1/2  Escaping local optima. Though our model allows for the modiﬁcation of the number of dimensions mod- elled by the Gaussian through the analysis of the spec- trum of C −1/2 , it is sensitive to local optima. It is for instance rare that a dimension modelled by a Gaussian is switched to the Parzen windows estima- tor. Even though the algorithm will more easily switch from the Parzen windows estimator to the Gaussian model, it will typically stop too early, that is model many dimensions using the Parzen windows estimator rather than the better Gaussian. To solve these issues, we propose an alternate algorithm, LCA-Gauss-Red, which explores the space of dimensions modelled by a Gaussian more aggressively using a search algorithm, namely:  1. We run the algorithm LCA - Gauss for a few it-  erations (40 in our experiments);  2. We then “transfer” some columns from BL (the Parzen windows model) to BG (the Gaussian  4  model), and rerun LCA - Gauss using these new matrices as initialisations;  3. We iterate step 2 using a dichotomic search of the optimal number of dimensions modelled by the Gaussian, until a local optimum is found;  4. Once we have a locally optimum number of di- mensions modelled by the Gaussian model, we run LCA - Gauss to convergence.  5 Speeding up LCA  Computing the local covariance matrix of the points using Eq. (3), (4) and (5) has a complexity in O(dn2 + d2n + d3), with d the dimensionality of the data and n the number of training points. Since this is imprac- tical for large datasets, we can resort to sampling to keep the cost linear in the number of datapoints. We may further use low-rank or diagonal approximation to achieve a complexity which grows quadratically with d instead of cubically.  5.1 Averaging a subset of the local co-  variance matrices  Instead of averaging the local covariances over all dat- apoints, we may only average them over a subset of datapoints. This estimator is unbiased and, if the lo- cal covariance matrices are not too dissimilar, which is the assumption underlying LCA, then its variance should remain small. This is equivalent to using a minibatch procedure: every time we have a new mini-  batch of size B, we compute its local covariance cCL,  which is then averaged with the previously computed CL using  CL ← γ  B  n CL + (1 − γ  (8)  B  n )cCL  to yield the updated CL. The exponent B/n is so that γ, the discount factor, determines the weight of the old covariance matrix after an entire pass through the data, which makes it insensitive to the particular choice of batch size. As opposed to many such al- gorithms where the choice of γ is critical as it helps retaining the information of previous batches, the lo- cality of the EM estimate makes it less so. However, if the number of datapoints used to estimate CL is not much larger than the dimension of the data, we need to set a higher γ to avoid degenerate covariance ma- trices. In simulations, we found that using a value of γ = .6 worked well. Similarly, the size of the mini- batch inﬂuences only marginally the ﬁnal result and we found a value of 100 to be large enough.  5.2 Computing the local covariance matrices using a subset of the dat- apoints  Rather than using only a subset of local covariance ma- trices, one may also wonder if using the entire dataset to compute these matrices is necessary. Also, as the number of datapoints grows, the chances of overﬁtting increase. Thus, one may choose to use only a sub- set of the datapoints to compute these matrices. This will increase the local covariances, yielding a biased estimate of the ﬁnal result, but may also act as a reg- ulariser. In practice, for very large datasets, one will want the largest neighbourhood size while keeping the computational cost tractable.  Denoting ni the number of locations at which we esti- mate the local covariance and nj the number of neigh- bours used to estimate this covariance, the cost per update is now O(d2[ni + nj] + dninj + d3). Since only nj should grow with n, this is linear in the total num- ber of datapoints.  Though they may appear similar, these are not “land- mark” techniques (see, e.g., [11]) as there is still one Gaussian component per datapoint, and the ni data- points around which we compute the local covariances are randomly sampled at every iteration.  6 Experiments  LCA has three main properties: ﬁrst, it transforms the data to make it locally isotropic, thus being well-suited for preprocessing the data before using a clustering algorithm like spectral clustering; second, it extracts relevant, non-Gaussian components in the data; third, it provides us with a good density model through the use of the Parzen windows estimator.  In the experiments, we will assess the performance of the following algorithms: LCA, the original algorithm; LCA-Gauss, using a multiplicative Gaussian compo- nent, as described in Section 4; LCA-Gauss-Red, the variant of LCA-Gauss using the more aggressive search to ﬁnd a better number of dimensions to be modelled by the Gaussian component. The MATLAB code for LCA, LCA-Gauss and LCA-Gauss-Red is available at http://nicolas.le-roux.name/code.html.  6.1  Improving clustering methods  We ﬁrst try to solve three clustering problems: one for which the clusters are convex and the direc- tion of interest does not have a Gaussian marginal (Fig. (2), left), one for which the clusters are not con- vex (Fig. (2), middle), and one for which the directions  5  of interest have almost Gaussian marginals (Fig. (2), right). Following [1, 2], the data is progressively cor- rupted by adding dimensions of white Gaussian noise, then whitened. We compare here the clustering ac- curacy, which is deﬁned as 100 n minP tr(EP ) where E is the confusion matrix and P is the set of permuta- tions over cluster labels, obtained with the following ﬁve techniques:  1. Spectral clustering (SC) [19] on the whitened data  (using the code of [8]);  2. SC on the projection on the ﬁrst two components found by FastICA using the best contrast function and the correct number of components;  3. SC on the data transformed using the metric  learnt with LCA;  4. SC on the data transformed using the metric learnt with the product of LCA and a Gaussian;  5. SC on the projection of the data found using NGCA [4] with the correct number of compo- nents.  Our choice of spectral clustering stems from its higher clustering performance compared to K-means. Re- sults are reported in Fig. (3). Because of the whiten- ing, the Gaussian components in the ﬁrst dataset are shrunk along the direction containing information. As a result, even with little noise added, the information gets swamped and spectral clustering fails completely. On the other hand, LCA and its variants are much more robust to the presence of irrelevant dimensions. Though NGCA works very well on the ﬁrst dataset, where there is only one relevant component, its per- formance drops quickly when there are two relevant components (note that, for all datasets, we provided the true number of relevant dimensions as input to NGCA). This is possibly due to the deﬂation proce- dure which is not adapted when no single component can be clearly identiﬁed in isolation. This is in contrast with LCA and its variants which circumvent this issue, thanks to their global optimisation procedure. Note also that LCA-Gauss allows us to perform unsuper- vised dimensionality reduction with the same perfor- mance as previously proposed supervised algorithms (e.g., [2]).  Figure (4) shows the clustering accuracy on the three datasets for various numbers of EM iterations, one iteration corresponding to Manifold Parzen Win- dows [26] with a Gaussian kernel whose covariance ma- trix is the data covariance kernel. As one can see, run- ning the EM algorithm to convergence yields a signiﬁ- cant improvement in clustering accuracy. The perfor- mance of Manifold Parzen Windows could likely have  been improved with a careful initialisation of the orig- inal kernel, but this would have been at the expense of the simplicity of the algorithm.  6.2 LCA as a density model  We now assess the quality of LCA as a density model. We build a density model of the USPS digits dataset, a 256-dimensional dataset of handwritten digits. We compared several algorithms:  • An isotropic Parzen windows estimator with the bandwidth estimated using LCA (replacing Σ∗ of Eq. (5) by λI so that the two matrices have the same trace);  • A Parzen windows estimator with diagonal metric  (equal to the diagonal of Σ∗ in Eq. (5);  • A Parzen windows estimator with the full metric  as obtained using LCA;  • A single Gaussian model;  • A product of a Gaussian and a Parzen windows  estimator (as described in Section 4).  The models were trained on a set of 2000 datapoints and regularized by penalizing the trace of Σ−1 (in the case of the last model, both covariance matrices, local and global, were penalized). The regularisation pa- rameter was optimized on a validation set of 1000 dat- apoints. For the last model, the regularisation param- eter of the global covariance was set to the one yielding the best performance for the full Gaussian model on the validation set. Thus, we only had to optimize the regularisation parameter for the local covariance.  The ﬁnal performance was then evaluated on a set of 3000 datapoints which had not been used for training nor validation. We ran the experiment 20 times, ran- domly selecting the training, validation and test set each time.  Fig. (5) shows the mean and the standard error of the negative log-likelihood on the test set. As one can see, modelling all dimensions using the Parzen windows es- timator leads to poor performance in high dimensions, despite the regulariser and the leave-one-out criterion. On the other hand, LCA-Gauss and LCA-Gauss-Red clearly outperform all the other models, justifying our choice of modelling some dimensions using a Gaussian. Also, as opposed to the previous experiments, there is no performance gain induced by the use of LCA-Gauss- Red as opposed to LCA-Gauss, which we believe stems from the fact that the switch from one model to the other is easier to make when there are plenty of di- mensions to choose from. The poor performance of  6  Figure 2: Noise-free data used to assess the robustness of K-means to noise. Left: mixture of two isotropic Gaussians of unit variance and means [−3, 0]⊤ and [3, 0]⊤. Centre: two concentric circles with radii 1 and 2, with added isotropic Gaussian noise of standard deviation .1. Right: mixture of ﬁve Gaussians. The centre cluster contains four times as many datapoints as the other ones.  100  90  80  70  60  50 0  100  90  80  70  60  50 0  60  100  90  80  70  60  50  40  30 0  20  5 Noise dimensions  10  15  20  40  Noise dimensions  20  40  60  Noise dimensions  Figure 3: Average clustering accuracy (100% = perfect clustering, chance is 50% for the ﬁrst two datasets and 20% for the last one) on 100 runs for varying number of dimensions of noise added. The error bars represent one standard error. Left: mixture of isotropic Gaussians presented in Fig. (2) (left). Centre: two concentric circles presented in Fig. (2) (centre). Right: mixture of ﬁve Gaussians presented in Fig. (2) (right).  100  90  80  70  60     50 0  Raw MPW LCA−10 LCA−50 LCA−200  20  40  Noise dimensions     100  90  80  70  60     50 0  60     Raw MPW LCA−10 LCA−50 LCA−200  5 Noise dimensions  10  15  20  100  90  80  70  60  50  40  30 0        Raw MPW LCA−10 LCA−50 LCA−200  20  40  Noise dimensions  60  Figure 4: Average clustering accuracy (100% = perfect clustering, chance is 50% for the ﬁrst two datasets and 20% for the last one) on 100 runs for varying number of dimensions of noise added and varying number of EM iterations in the LCA algorithm (MPW = one iteration). The error bars represent one standard error. Left: mixture of isotropic Gaussians presented in Fig. (2) (left). Centre: two concentric circles presented in Fig. (2) (centre). Right: mixture of ﬁve Gaussians presented in Fig. (2) (right).  LCA-Full is a clear indication of the problems suﬀered by Parzen windows in high dimensions.  6.3 Subsampling  We now evaluate the loss in performance incurred by the use the subsampling procedure described in Section 5, both on the train and test negative log-  7  N = 1000  N = 3000  N = 6000  N = 1000  N = 3000  N = 6000  B = 1000  6.43 ± 0.10  2.70 ± 0.06 −0.10 ± 0.03  6.43 ± 0.10  2.80 ± 0.06 −0.17 ± 0.02  B = 3000  6.58 ± 0.07  2.73 ± 0.05 −0.06 ± 0.03  6.54 ± 0.07  2.80 ± 0.06 −0.11 ± 0.02  B = 6000  6.22 ± 0.08  2.21 ± 0.03  0.00 ± 0.01  6.18 ± 0.07  1.98 ± 0.03  0.02 ± 0.02  N = 1000  N = 3000  N = 6000  N = 1000  N = 3000  N = 6000  B = 1000  2.12 ± 0.16  0.20 ± 0.08  0.65 ± 0.05  2.01 ± 0.17  0.07 ± 0.09  0.15 ± 0.03  B = 3000  2.11 ± 0.16  0.30 ± 0.07  0.46 ± 0.04  2.01 ± 0.17  0.16 ± 0.09  0.09 ± 0.03  B = 6000  1.54 ± 0.16 −0.75 ± 0.07 −0.01 ± 0.01  1.43 ± 0.15 −1.07 ± 0.08  0.01 ± 0.02  Figure 6: Train (top) and test (bottom) negative log-likelihood diﬀerences induced by the use of smaller batch and neighbourhood sizes compared to the original model (γ = 0, B = 6000, N = 6000) for γ = 0.3 (left) and γ = 0.6 (right). A negative value means better performance.  7 Conclusion  Despite its importance, the learning of local or global metrics is usually an overseen step in many practical algorithms. We have proposed an extension of the gen- eral bandwidth selection problem to the multidimen- sional case, with a generalisation to the case where sev- eral components are Gaussian. Additionally, we pro- posed an approximate scheme suited to large datasets which allows to ﬁnd a local optimum in linear time. We believe LCA can be an important preprocessing tool for algorithms relying on local distances, such as manifold learning methods or many semi-supervised algorithms. Another use would be to cast LCA within the mean-shift algorithm, which ﬁnds the modes of the Parzen windows estimator, in the context of image seg- mentation [10]. In the future, we would like to extend this model to the case where the metric is allowed to vary with the position in space, to account for more complex geometries in the dataset.  Acknowledgements  Nicolas Le Roux and Francis Bach are supported in part by the European Research Council (SIERRA- ERC-239993). We would also like to thank Warith Harchaoui for its valuable input.  LCA - Isotropic LCA - Diagonal  LCA - Full Gaussian  LCA - Gauss  LCA - Gauss - Red  269.78 ± 0.18 109.59 ± 0.56 32.98 ± 0.35 32.27 ± 0.36 19.09 ± 0.39 19.09 ± 0.39  Figure 5: Test negative log-likelihood on the USPS digits dataset, averaged over 20 runs.  likelihoods. For that purpose, we used the USPS digit recognition dataset, which contains 8298 datapoints in dimension 256, which we randomly split into a train- ing set of n = 6000 datapoints, using the rest as the test set. We tested the following hyperparameters:  • Discount factor γ = 0.3, 0.6, 0.9 ,  • Batch size B = 1000, 3000, 6000 ,  • Neighbourhood size N = 1000, 3000, 6000 .  Fig. (6) show the log-likelihood diﬀerences induced by the use of smaller batch sizes and neighbourhood sizes. For each set of hyperparameters, 20 experiments were run using diﬀerent training and test sets, and the means and standard errors are reported. The results for γ = 0.9 were very similar and are not included due to space constraints.  Three observations may be made. First, reducing the batchsize has little eﬀect, except when γ is small. Sec- ond, reducing the neighbourhood size has a regular- izing eﬀect at ﬁrst but drastically hurts the perfor- mance if reduced too much. Third, the value of γ, the discount factor, has little inﬂuence, but larger values proved to yield more consistent test performance, at the expense of slower convergence. The consistency of these results shows that it is safe to use subsampling (with values of γ = 0.6, B = 100 and N = 3000, for instance) especially if the training set is very large.  8  References  [1] F. Bach and Z. Harchaoui. Diﬀrac: a discrimi- native and ﬂexible framework for clustering. In Advances in Neural Information Processing Sys- tems 20, 2007.  [2] F. Bach and M. I. Jordan. Learning spectral clus- tering. In Advances in Neural Information Pro- cessing Systems 16, 2004.  [3] Y. Bengio, H. Larochelle, and P. Vincent. Non- local manifold Parzen windows. In Advances in Neural Information Processing Systems 18, pages 115–122. MIT Press, 2006.  [4] G. Blanchard, M. Kawanabe, M. Sugiyama, V. Spokoiny, and K.-R. M¨uller. In search of non- Gaussian components of a high-dimensional dis- tribution. J. Mach. Learn. Res., 7:247–282, De- cember 2006.  [5] J.M. Borwein and A.S. Lewis. Convex analysis and nonlinear optimization, theory and examples, 2000.  [6] R. Boscolo, H. Pan, and V.P. Roychowdhury. Independent component analysis based on non- parametric density estimation. Neural Networks, IEEE Transactions on, 15(1):55–65, 2004.  [7] A. Bowman. An alternative method of cross- validation for the smoothing of density estimates. Biometrika, 71(2):pp. 353–360, 1984.  [8] W.-Y. Chen, Y. Song, H. Bai, C.-J. Lin, and E. Y. Chang. Parallel spectral clustering in distributed systems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(3):568–586, 2011.  [9] Y.-S. Chow, S. Geman, and L.-D. Wu. Consistent cross-validated density estimation. The Annals of Statistics, 11(1):pp. 25–38, 1983.  [10] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24:603–619, 2002.  [11] V. De Silva and J.B. Tenenbaum. Sparse multi- dimensional scaling using landmark points. Tech- nology, pages 1–41, 2004.  [12] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):pp. 1– 38, 1977.  [13] R.P.W. Duin. On the choice of smoothing param- eters for Parzen estimators of probability density functions. Computers, IEEE Transactions on, C- 25(11):1175 –1179, 1976.  [14] T. Duong and M. Hazelton. Cross-validation bandwidth matrices for multivariate kernel den- sity estimation. Scandinavian Journal of Statis- tics, 32(3):pp. 485–506, 2005.  [15] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Advances in Neural In- formation Processing Systems, volume 18, pages 451–458. MIT Press, 2006.  [16] J. Goldberger, S. Roweis, G.E. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In Advances in Neural Information Pro- cessing Systems 17, 2004.  [17] A. Hyv¨arinen, J. Karhunen, and E. Oja.  Inde- pendent component analysis. Wiley, New York, 2001.  [18] M. C. Jones and D. A. Henderson. Maximum likelihood kernel density estimation: On the po- tential of convolution sieves. Comput. Stat. Data Anal., 53:3726–3733, August 2009.  [19] A. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In Ad- vances in Neural Information Processing Systems 14, 2001.  [20] B. Park and J. S. Marron. Comparison of data- driven bandwidth selectors. Journal of the Amer- ican Statistical Association, 85(409):pp. 66–72, 1990.  [21] E. Parzen. On estimation of a probability density function and mode. The Annals of Mathematical Statistics, 33(3):pp. 1065–1076, 1962.  [22] M. Rosenblatt. Remarks on some nonparametric estimates of a density function. The Annals of Mathematical Statistics, 27(3):pp. 832–837, 1956.  [23] M. Rudemo. Empirical choice of histograms and kernel density estimators. Scandinavian Journal of Statistics, 9(2):pp. 65–78, 1982.  [24] S. J. Sheather and M. C. Jones. A reliable data- based bandwidth selection method for kernel den- sity estimation. Journal of the Royal Statistical Society. Series B (Methodological), 53(3):pp. 683– 690, 1991.  [25] B. W. Silverman. Density estimation for statistics  and data analysis. 1986.  9  [26] P. Vincent and Y. Bengio. Manifold Parzen win- dows. In Advances in Neural Information Pro- cessing Systems 15, pages 825–832. MIT Press, 2002.  [27] E. Xing, A. Ng, M. I. Jordan, and S. Russell. Dis- tance metric learning with application to cluster- ing with side-information. In Advances in Neural Information Processing Systems 14, 2002.  [28] L. Zelnik-Manor and P. Perona. Self-tuning spec- tral clustering. In Advances in Neural Informa- tion Processing Systems 17, 2004.  Appendix  We prove here Proposition 1.  If M1 is singular, then the minimum value is Proof −∞, because we can have B⊤ G M1BG bounded while BGB⊤ G tends to +∞ (for example, if d1 = 1, and u1 is such that M1u1 = 0, select BG = λu1 with λ → +∞). The reasoning is similar for M2. We thus assume that M1 and M2 are invert- ible. eigendecomposition of M −1/2 = U Diag(e)U ⊤, which corresponds to the generalized eigendecomposition of the pair (M1, M2). Denoting A2 = U ⊤M 1/2 we have:  1 BL and A1 = U ⊤M 1/2  1 M2M −1/2  1 BG,  consider  We  the  1  Let s be the vector of singular values of A2, ordered in decreasing order and let the ei be ordered in increasing order. We have:  tr Diag(e)A2A⊤  2 = − tr(− Diag(e)A2A⊤  2 ) >Xi  eis2 i ,  with equality if and only if the eigenvectors of A2A⊤ 2 are aligned with the ones of Diag(e) (the −ei being also in decreasing order) (Theorem 1.2.1, [5]). 2 = diag(s)2 with only d2 non- Thus, we have A2A⊤ zero elements in s. Let J2 be the index of non zero- elements. We thus need to minimize  d1 + log det M1 + Xj∈J2  (ejs2  j − log s2  j ) ,  with optimum s2  j = e−1  j  and value:  d1 + d2 + log det M1 + Xj∈J2  log ej .  Thus, we need to take J2 corresponding to the smallest eigenvalues ej. If we also optimize with respect to d2, then J2 must only contain the elements smaller than 1.  L M2BL − log det(BGB⊤  G + BLB⊤ L )  tr B⊤ = tr A⊤  G M1BG + tr B⊤ 1 A1 + tr A⊤ − log det(A1A⊤ 1 A1 + tr A⊤  = tr A⊤  2 Diag(e)A2 1 + A2A⊤ 2 Diag(e)A2 − log det(A⊤  2 ) + log det M1  2 A2)  − log det(A⊤  1 (I − A2(A⊤  2 A2)−1A⊤  2 )A1) + log det M1 .  By taking derivatives with respect to A1, we get  A1 = (I − Π2)A1(A⊤  1 (I − Π2)A1)−1 ,  (9)  with Π2 = A2(A⊤ sides of Eq. (9) by A⊤  2 A2)−1A⊤  2 , we obtain  2 . By left-multiplying both  A⊤  2 A1 = 0 .  By left-multiplying by A⊤  1 , we get  A⊤  1 A1 = I .  Thus, we now need to minimize with respect to A2 the following cost function  d1 + tr A⊤  2 Diag(e)A2 − log det(A⊤  2 A2) + log det M1  10  ","Kernel density estimation, a.k.a. Parzen windows, is a popular densityestimation method, which can be used for outlier detection or clustering. Withmultivariate data, its performance is heavily reliant on the metric used withinthe kernel. Most earlier work has focused on learning only the bandwidth of thekernel (i.e., a scalar multiplicative factor). In this paper, we propose tolearn a full Euclidean metric through an expectation-minimization (EM)procedure, which can be seen as an unsupervised counterpart to neighbourhoodcomponent analysis (NCA). In order to avoid overfitting with a fullynonparametric density estimator in high dimensions, we also consider asemi-parametric Gaussian-Parzen density model, where some of the variables aremodelled through a jointly Gaussian density, while others are modelled throughParzen windows. For these two models, EM leads to simple closed-form updatesbased on matrix inversions and eigenvalue decompositions. We show empiricallythat our method leads to density estimators with higher test-likelihoods thannatural competing methods, and that the metrics may be used within mostunsupervised learning techniques that rely on such metrics, such as spectralclustering or manifold learning methods. Finally, we present a stochasticapproximation scheme which allows for the use of this method in a large-scalesetting."
1301.5348,2013,Why Size Matters: Feature Coding as Nystrom Sampling  ,"['Oriol Vinyals', 'Yangqing Jia', 'Trevor Darrell']",https://arxiv.org/pdf/1301.5348.pdf,"3 1 0 2    r p A 6 1         ]  G L . s c [      2 v 8 4 3 5  .  1 0 3 1 : v i X r a  Why Size Matters:  Feature Coding as Nystr¨om Sampling  Oriol Vinyals UC Berkeley Berkeley, CA  Yangqing Jia UC Berkeley Berkeley, CA  Trevor Darrell UC Berkeley Berkeley, CA  1  Introduction  Recently, the computer vision and machine learning community has been in favor of feature extrac- tion pipelines that rely on a coding step followed by a linear classiﬁer, due to their overall simplicity, well understood properties of linear classiﬁers, and their computational efﬁciency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystr¨om sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystr¨om sampling step to select a subset of all data points. Furthermore, since bounds are known on the approximation power of Nystr¨om sampling as a func- tion of how many samples (i.e. dictionary size) we consider, we can derive bounds on the approx- imation of the exact (but expensive to compute) kernel matrix, and use it as a proxy to predict accuracy as a function of the dictionary size, which has been observed to increase but also to satu- rate as we increase its size. This model may help explaining the positive effect of the codebook size [2, 7] and justifying the need to stack more layers (often referred to as deep learning), as ﬂat models empirically saturate as we add more complexity.  2 The Nystr¨om View  We speciﬁcally consider forming a dictionary by sampling our training set. To encode a new sample x ∈ Rd, we apply a (generally non-linear) coding function c so that c(x) ∈ Rc. Note that d is the dimensionality of the original feature space, while c is the dictionary size. The standard classiﬁcation pipeline considers c(x) as the new feature space, and typically uses a linear classiﬁer on this space. For example, one may use the threshold encoding function [2] as an example: c(x) = max(0, x(cid:62)D − α) where D ∈ Rd×c is the dictionary. Note that our discussion on coding is valid for many different feed-forward coding schemes. In the ideal case (inﬁnite computation and memory), we encode each sample x using the whole training set X ∈ Rd×N , which can be seen as the best local coding of the training set X (as long as over-ﬁtting is handled by the classiﬁcation algorithm). In general, larger dictionary sizes yield better performance assuming the linear classiﬁer is well regularized, as it can be seen as a way to do manifold learning [6]. We deﬁne the new coded feature space as C = max(0, X(cid:62)X − α), where the i-th row of C corresponds to coding the i-th sample c(xi). The linear kernel function between samples i and j is k(xi, xj) = c(xi)(cid:62)c(xj). The kernel matrix is then K = CC(cid:62). Naively applying Nystr¨om sampling to the matrix K does not save any computation, as every column of K requires computing an inner product with N samples. However, if we decompose the matrix C with Nystr¨om sampling (i.e., with a subsampled dictionary) we obtain C(cid:48) ≈ C, and as a consequence K(cid:48) ≈ K:  C(cid:48) = EW−1E(cid:62), K(cid:48) = C(cid:48)C(cid:48)(cid:62) = EW−1E(cid:62)EW−1E(cid:62) = EΛE(cid:62)  1  where the ﬁrst equation comes from applying Nystr¨om sampling to C, E is a random subsample of the columns of C, and W the corresponding square matrix with the same random subsample of both columns and rows of C.  3 Main Results on Approximation Bounds More interestingly, many bounds on the error made in estimating C by C(cid:48) exist, and ﬁnding better sampling schemes that improve such bounds is an active topic in the machine learning community (see e.g. [4]). The bound we start with is [4]:  ||C − C(cid:48)||F ≤ ||C − Ck||F + (cid:15) max(nCii)  (1) valid if c ≥ 64k/(cid:15)4 (c is the number of columns that we sample from C to form E, i.e. the codebook size), where k is the sufﬁcient rank to estimate the structure of C, and Ck is the optimal rank k approximation (given by Singular Value Decomposition (SVD), which we cannot compute in practice). Note that, if we assume that our training set can be explained by a manifold of dimension k (i.e. the ﬁrst term in the right hand side of eq. 1 vanishes), then the error is proportional to (cid:15) times a constant (that is dataset dependent). Thus, if we ﬁx k to the value that retains enough energy from C, we get a bound that for every c (dimension of code), gives a minimum (cid:15) to plug in equation 1. This gives us a useful bound of the form (cid:15) ≥ M c− 1  4 for some constant M (that depends on k). Putting it all together, we get:  ||C − C(cid:48)||F ≤ O + M c− 1  4  with O and M constants that are dataset speciﬁc. Having bounded the error C is not sufﬁcient to establish how the code size will affect the classiﬁer performance. In particular, it is not clear how the error on C affect the error on the kernel matrix K. However, we are able to prove that the error bound on K(cid:48) is in the same format as that on C:  ||K − K(cid:48)||F ≤ O + M c− 1  (2) Even though we are not aware of an easy way to formally link degradation in Frobenius norm of our approximation K(cid:48) to K to classiﬁcation accuracy, the bound above is informative as one may reasonably expect kernel matrices of different quality to have classiﬁcation performances in the same trend.  4  4 Experiments  We empirically evaluate the bound on the kernel matrix, used as a proxy to model classiﬁcation ac- curacy, which is the measure of interest. To estimate the constants in the bound, we do interpolation of the observed accuracy in the ﬁrst two samples of accuracy versus codebook size, which is of prac- tical interest: one may want to quickly run a new dataset through the pipeline with small codebook sizes, and then quickly estimate what the accuracy would be when running a full experiment with a much larger dictionary size.  Figure 1: Empirical accuracy (solid line) and Nystr¨om model accuracy (dashed line) on the training (red) and testing (blue) sets versus dictionary size, on CIFAR-10 (left) and TIMIT (right).  Figure 1 shows the results on on the CIFAR-10 image classiﬁcation and TIMIT speech recognition datasets respectively. It is observed that the derived model closely follow our own empirical ob- servations, with red dashed line serving as a lower bound of the actual accuracy and following the  2  01000200030004000500060006065707580859095100Codebook sizeAccuracyCIFAR  Empirical TrainNystrom Bound TrainEmpirical Test0100020003000400050006000700080003540455055606570Codebook sizeAccuracyTIMIT  Empirical TrainNystrom Bound TrainEmpirical TestFigure 2: Accuracy values on the CIFAR-10 (left) and STL (right) datasets under different ﬁnal dictionary size. “nx PDL” means overshooting the dictionary from a starting dictionary that is n times larger than the ﬁnal one. We refer to our tech report [3] for more details.  shape of the empirical accuracy, predicting its saturation. The model is never too tight though, due to various factors of our approximation, e.g., the analytical relationship between the approximation of K and the classiﬁcation accuracy is not clear. The Nystr¨om view of feature encoding and the approximation bounds we proposed helps under- standing several key observations in the recent literature: (1) the linear classiﬁer performance is al- ways bounded when using a ﬁxed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efﬁcient in some classiﬁcation pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr¨om sampling context [4]. In addition, in many image classiﬁcation tasks the feature extraction pipeline is composed of more than feature encoding. For example, recent state-of-the-art methods pool locally encoded features spatially to form the ﬁnal feature vector. The Nystr¨om view presented in the paper inspires us to employ ﬁndings in the machine learning ﬁeld to learn better, pooling-aware dictionaries. In one of our related work [3], we form a dictionary by ﬁrst “overshooting” the coding stage with a larger dictionary, and then pruning it using K-centers with pooled features. Figure 2 shows an increase in the ﬁnal classiﬁcation accuracy compared with the baseline that only learns the dictionary on the patch-level, with no additional computation cost for either feature extraction or classiﬁcation.  References [1] A Coates, H Lee, and AY Ng. An analysis of single-layer networks in unsupervised feature  learning. In AISTATS, 2011.  [2] A Coates and AY Ng. The importance of encoding versus training with sparse coding and vector  quantization. In ICML, 2011.  [3] Y Jia, O Vinyals,  and T Darrell.  http://arxiv.org/abs/1302.5056, 2013.  Pooling-invariant  image  feature  learning.  [4] S Kumar, M Mohri, and A Talwalkar. Sampling methods for the nystr¨om method. JMLR,  13(4):981–1006, 2012.  [5] A Saxe, PW Koh, Z Chen, M Bhand, B Suresh, and AY Ng. On random weights and unsuper-  vised feature learning. In ICML, 2011.  [6] J Wang, J Yang, K Yu, F Lv, T Huang, and Y Gong. Locality-constrained linear coding for  image classiﬁcation. In CVPR, 2010.  [7] J Yang, K Yu, and T Huang. Efﬁcient highly over-complete sparse coding using a mixture  model. In ECCV, 2010.  3  10020040080016003200Final Dictionary Size6668707274767880AccuracyK-means2x PDL4x PDL8x PDL10020040080016003200Final Dictionary Size50515253545556575859AccuracyK-means2x PDL4x PDL8x PDL","Recently, the computer vision and machine learning community has been infavor of feature extraction pipelines that rely on a coding step followed by alinear classifier, due to their overall simplicity, well understood propertiesof linear classifiers, and their computational efficiency. In this paper wepropose a novel view of this pipeline based on kernel methods and Nystromsampling. In particular, we focus on the coding of a data point with a localrepresentation based on a dictionary with fewer elements than the number ofdata points, and view it as an approximation to the actual function that wouldcompute pair-wise similarity to all data points (often too many to compute inpractice), followed by a Nystrom sampling step to select a subset of all datapoints.Furthermore, since bounds are known on the approximation power of Nystromsampling as a function of how many samples (i.e. dictionary size) we consider,we can derive bounds on the approximation of the exact (but expensive tocompute) kernel matrix, and use it as a proxy to predict accuracy as a functionof the dictionary size, which has been observed to increase but also tosaturate as we increase its size. This model may help explaining the positiveeffect of the codebook size and justifying the need to stack more layers (oftenreferred to as deep learning), as flat models empirically saturate as we addmore complexity."
1301.4171,2013,Affinity Weighted Embedding  ,"['Jason Weston', 'Ron Weiss', 'Hector Yee']",https://arxiv.org/pdf/1301.4171.pdf,"3 1 0 2     n a J    7 1      ]  R  I . s c [      1 v 1 7 1 4  .  1 0 3 1 : v i X r a  Afﬁnity Weighted Embedding  Jason Weston Google Inc.,  New York, NY, USA.  jweston@google.com  Ron Weiss Google Inc.,  New York, NY, USA. ronw@google.com  Hector Yee Google Inc.,  San Bruno, CA, USA. hyee@google.com  Abstract  Supervised (linear) embedding models like Wsabie [5] and PSI [1] have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and typically underﬁt. We propose a new class of models which aim to provide improved performance while retaining many of the beneﬁts of the existing class of embedding models. Our new approach works by iteratively learning a linear embedding model where the next iteration’s features and labels are reweighted as a function of the previous iteration. We describe several variants of the family, and give some initial results.  1 (Supervised) Linear Embedding Models  Standard linear embedding models are of the form:  f (x, y) = x⊤U ⊤V y = X  xiU ⊤  i Vj yj .  ij  where x are the input features and y is a possible label (in the annotation case), document (in the information retrieval case) or item (in the recommendation case). These models are used in both supervised and unsupervised settings. In the supervised ranking case, they have proved successful in many of the tasks described above, e.g. the Wsabie algorithm [5, 4, 6] which approximately opti- mizes precision at the top of the ranked list has proven useful for annotation and recommendation.  These methods scale well to large data and are simple to implement and use. However, as they contain no nonlinearities (other than in the feature representation in x and y) they can be limited in their ability to ﬁt large complex datasets, and in our experience typically underﬁt.  2 Afﬁnity Weighted Embedding Models  In this work we propose the following generalized embedding model:  f (x, y) = X  Gij (x, y) xiU ⊤  i Vj yj.  ij  where G is a function, built from a previous learning step, that measures the afﬁnity between two points. Given a pair x, y and feature indices i and j, G returns a scalar. Large values of the scalar indicate a high degree of match. Different methods of learning (or choosing) G lead to different variants of our proposed approach:  1  • Gij (x, y) = G(x, y). In this case each feature index pair i, j returns the same scalar so the  model reduces to:  f (x, y) = G(x, y) x⊤U ⊤V y.  • Gij (x, y) = Gij. In this case the returned scalar for i, j is the same independent of the input vector x and label y, i.e. it is a reweighting of the feature pairs. This gives the model:  f (x, y) = X  Gij xiU ⊤  i Vj yj .  ij  This is likely only useful in large sparse feature spaces, e.g. if Gij represents the weight of a word-pair in an information retrieval task or an item-pair in a recommendation task. Further, it is possible that Gij could take a particular form, e.g. it is represented as a low i Vj yj. rank matrix Gij = g⊤  i gj. In that case we have the model f (x, y) = Pij g⊤  i gjxiU ⊤  While it may be possible to learn the parameters of G jointly with U and V here we advocate an iterative approach:  1. Train a standard embedding model: f (x, y) = x⊤U ⊤V y. 2. Build G using the representation learnt in (1). 3. Train a weighted model: f (x, y) = Pij Gij (x, y) xi ¯U ⊤ 4. Possibly repeat the procedure further: build ¯G from (3). (So far we have not tried this).  ¯Vj yj.  i  Note that the training algorithm used for (3) is the same as for (1) – we only change the model. In the following, we will focus on the Gij (x, y) = G(x, y) case (where we only weight examples, not features) and a particular choice of G1:  G(x, y) =  m  X  i=1  exp(−λx||U x − U xi||2) exp(−λy||y − yi||2)  (1)  where x and y are the sets of vectors from the training set.  G is built using the embedding U learnt in step (1), and is then used to build a new embedding model in step (3). Due to the iterative nature of the steps we can compute G for all examples in parallel using a MapReduce framework, and store the training set necessary for step (3), thus making learning straight-forward. To decrease storage, instead of computing a smooth G as above we can clip (sparsify) G by taking only the top n nearest neighbors to U x, and set the rest to 0. Further we take λy suitably large such that exp(−λy||y − yi||2) either gives 1 for yi = y or 0 otherwise2. In summary, then, for each training example, we simply have to ﬁnd the (n = 20 in our experiments) nearest neighboring examples in the embedding space, and then we reweight their labels using eq. 1. (All other labels would then receive a weight of zero, although one could also add a constant bias to guarantee those labels can receive non-zero ﬁnal scores.)  3 Experiments  So far, we have conducted two preliminary experiments on Magnatagatune (annotating music with text tags) and ImageNet (annotation images with labels). Wsabie has been applied to both tasks previously [4, 5].  On Magnatagatune we used MFCC features for both Wsabie and our method, similar to those used in [4]. For both models we used an embedding dimension of 100. Our method improved over Wsabie marginally as shown in Table 1. We speculate that this improvement is small due to the small size  1Although perhaps G(x, y) = Pm  i=1 exp(−λx||U x − U xi||2) exp(−λy||V y − V yi||2) would be more natural. Further we could also consider Gorig(x, y) = Pm i=1 exp(−λx||x− xi||2) exp(−λy||y − yi||2) which does not make use of the embedding in step (1) at all. This would likely perform poorly when the input features are too sparse, which would be the point of improving the representation by learning it with U and V .  2This is useful in the label annotation or item ranking settings, but would not be a good idea in an information  retrieval setting.  2  Table 1: Magnatagatune Results Algorithm  Prec@1 Prec@3  k-Nearest Neighbor k-Nearest Neighbor (Wsabie space) Wsabie Afﬁnity Weighted Embedding  39.4% 45.2% 48.7% 52.7%  28.6% 31.9% 37.5% 39.2%  Table 2: ImageNet Results (Fall 2011, 21k labels)  Algorithm  Prec@1  Wsabie (KPCA features) k-Nearest Neighbor (Wsabie space) Afﬁnity Weighted Embedding Convolutional Net [2]  9.2% 13.7% 16.4% 15.6% (NOTE: on a different train/test split)  of the dataset (only 16,000 training examples, 104 input dimensions for the MFCCs and 160 unique tags). We believe our method will be more useful on larger tasks.  On the ImageNet task (Fall 2011, 10M examples, 474 KPCA features and 21k classes) the improve- ment over Wsabie is much larger, shown in Table 2. We used similar KPCA features as in [5] for both Wsabie and our method. We use an embedding dimension of 128 for both. We also compare to nearest neighbor in the embedding space. For our method, we used the max instead of the sum in eq. (1) as it gave better results. Our method is competitive with the convolutional neural network model of [2] (note, this is on a different train/test split). However, we believe the method of [3] would likely perform better again if applied in the same setting.  4 Conclusions  In conclusion, by incorporating a learnt reweighting function G into supervised linear embedding we can increase the capacity of the model leading to improved results. One issue however is that the cost of reducing underﬁtting by using G is that it both increases the storage and computational requirements of the model. One avenue we have begun exploring in that regard is to use approximate methods in order to compute G.  References  [1] B. Bai, J. Weston, D. Grangier, R. Collobert, K. Sadamasa, Y. Qi, C. Cortes, and M. Mohri. Polynomial  semantic indexing. In NIPS, 2009.  [2] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, A. Senior, P. Tucker, K. Yang, et al. Large scale distributed deep networks. In Advances in Neural Information Processing Systems 25, pages 1232–1240, 2012.  [3] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural net-  works. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.  [4] J. Weston, S. Bengio, and P. Hamel. Large-scale music annotation and retrieval: Learning to rank in joint  semantic spaces. In Journal of New Music Research, 2012.  [5] J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to large vocabulary image annotation. In Intl.  Joint Conf. Artiﬁcial Intelligence, (IJCAI), pages 2764–2770, 2011.  [6] J. Weston, C. Wang, R. Weiss, and A. Berenzeig. Latent collaborative retrieval. ICML, 2012.  3  ","Supervised (linear) embedding models like Wsabie and PSI have provensuccessful at ranking, recommendation and annotation tasks. However, despitebeing scalable to large datasets they do not take full advantage of the extradata due to their linear nature, and typically underfit. We propose a new classof models which aim to provide improved performance while retaining many of thebenefits of the existing class of embedding models. Our new approach works byiteratively learning a linear embedding model where the next iteration'sfeatures and labels are reweighted as a function of the previous iteration. Wedescribe several variants of the family, and give some initial results."
1301.3666,2013,Zero-Shot Learning Through Cross-Modal Transfer  ,"['Richard Socher', 'Milind Ganjoo', 'Hamsa Sridhar', 'Osbert Bastani', 'Christopher Manning', 'Andrew Y. Ng']",https://arxiv.org/pdf/1301.3666.pdf,"Zero-Shot Learning Through Cross-Modal Transfer  Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert Bastani, Christopher D. Manning, Andrew Y. Ng  Computer Science Department, Stanford University, Stanford, CA 94305, USA  richard@socher.org, {mganjoo, hsridhar, obastani, manning, ang}@stanford.edu  3 1 0 2    r a     M 0 2      ]  V C . s c [      2 v 6 6 6 3  .  1 0 3 1 : v i X r a  Abstract  This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a se- mantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by ﬁrst using outlier detection in the semantic space and then two sepa- rate recognition models. Furthermore, our model does not require any manually deﬁned semantic features for either words or images.  1  Introduction  The ability to classify instances of an unseen visual class, called zero-shot learning, is useful in many situations. There are many species, products or activities without labeled data and new visual categories, such as the latest gadgets or car models are introduced frequently. In this work, we show how to make use of the vast amount of knowledge about the visual world available in natural language to classify unseen objects. We attempt to model people’s ability to identify unseen objects even if the only knowledge about that object came from reading about it. For instance, after reading the description of a two-wheeled self-balancing electric vehicle, controlled by a stick, with which you can move around while standing on top of it, many would be able to identify a Segway, possibly after being brieﬂy perplexed because the new object looks different to any previously observed object class. We introduce a zero-shot model that can predict both seen and unseen classes. For instance, without ever seeing a cat image, it can determine whether an image shows a cat or a known category from the training set such as a dog or a horse. The model is based on two main ideas. First, images are mapped into a semantic space of words that is learned by a neural network model [15]. Word vectors capture distributional similarities from a large, unsupervised text corpus. By learning an image mapping into this space, the word vectors get implicitly grounded by the visual modality, allowing us to give prototypical instances for various words. Second, because classiﬁers prefer to assign test images into classes for which they have seen training examples, the model incorporates an outlier detection probability which determines whether a new image is on the manifold of known categories. If the image is of a known category, a standard classiﬁer can be used. Otherwise, images are assigned to a class based on the likelihood of being an unseen category. The probability of being an outlier or a known category is integrated into our probabilistic model. The model is illustrated in Fig 1. Unlike previous work on zero-shot learning which can only predict intermediate features or dif- ferentiate between various zero-shot classes [26], our joint model can achieve both state of the art accuracy on known classes as well as reasonable performance on unseen classes. Furthermore, com-  1  Figure 1: Overview of our multi-modal zero-shot model. We ﬁrst map each new testing image into a lower dimensional semantic space. Then, we use outlier detection to determine whether it is on the manifold of seen images. If the image is not on the manifold, we determine its class with the help of unsupervised semantic word vectors. In this example, the unseen classes are truck and cat.  pared to related work in knowledge transfer [19, 27] we do not require manually deﬁned semantic or visual attributes for the zero-shot classes. Our language feature representations are learned from unsupervised and unaligned corpora. We ﬁrst brieﬂy describe a selection of related work, followed by the model description and experi- ments on CIFAR10.  2 Related Work  We brieﬂy outline connections and differences to ﬁve related lines of research. Due to space con- straints, we cannot do justice to the complete literature. Zero-Shot Learning. The work most similar to ours is that by Palatucci et al. [26]. They map fMRI scans of people thinking about certain words into a space of manually designed features and then classify using these features. They are able to predict semantic features even for words for which they have not seen scans and experiment with differentiating between several zero-shot classes. However, the do not classify new test instances into both seen and unseen classes. We extend their approach to allow for this setup using outlier detection. Larochelle et al. [21] describe the unseen zero-shot classes by a “canonical” example or use ground truth human labeling of attributes. One-Shot Learning One-shot learning [17, 18] seeks to learn a visual object class by using very few training examples. This is usually achieved by either sharing of feature representations [2], model parameters [12] or via similar context [14]. A recent related work on one-shot learning is that of Salakhutdinov et al. [28]. Similar to their work, our model is based on using deep learning tech- niques to learn low-level image features followed by a probabilistic model to transfer knowledge. However, our work is able to classify object categories without any training data due to the cross-  2  Manifold of known classesautohorsedogtruckNew test image from unknown classcatmodal knowledge transfer from natural language and at the same time obtain high performance on classes with many training examples. Knowledge and Visual Attribute Transfer. Lambert et al. and Farhadi et al. [19, 10] were two of the ﬁrst to use well-designed visual attributes of unseen classes to classify them. This is different to our setting since we only have distributional features of words learned from unsupervised, non- parallel corpora and can classify between categories that have thousands or zero training images. Qi et al. [27] learn when to transfer knowledge from one category to another for each instance. Domain Adaptation. Domain adaptation is useful in situations in which there is a lot of training data in one domain but little to none in another. For instance, in sentiment analysis one could train a classiﬁer for movie reviews and then adapt from that domain to book reviews [4, 13]. While related, this line of work is different since there is data for each class but the features may differ between domains. Multimodal Embeddings. Multimodal embeddings relate information from multiple sources such as sound and video [24] or images and text. Socher et al. [30] project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Their model does require a small amount of training data however for each class. Among other recent work is that by Srivastava and Salakhutdinov [31] who developed multimodal Deep Boltzmann Machines. Similar to their work, we use techniques from the broad ﬁeld of deep learning to represent images and words. Some work has been done on multimodal distributional methods [11, 22]. Most recently, Bruni et al. [5] worked on perceptually grounding word meaning and showed that joint models are better able to predict the color of concrete objects.  3 Word and Image Representations  We begin the description of the full framework with the feature representations of words and images. Distributional approaches are very common for capturing semantic similarity between words. In these approaches, words are represented as vectors of distributional characteristics – most often their co-occurrences with words in context [25, 9, 1, 32]. These representations have proven very effective in natural language processing tasks such as sense disambiguation [29], thesaurus extraction [23, 8] and cognitive modeling [20]. We initialize all word vectors with pre-trained 50-dimensional word vectors from the unsupervised model of Huang et al. [15]. Using free Wikipedia text, their model learns word vectors by predict- ing how likely it is for each word to occur in its context. Their model uses both local context in the window around each word and global document context. Similar to other local co-occurrence based vector space models, the resulting word vectors capture distributional syntactic and semantic information. For further details and evaluations of these embeddings, see [3, 7]. We use the unsupervised method of Coates et al. [6] to extract F image features from raw pixels in an unsupervised fashion. Each image is henceforth represented by a vector x ∈ RF .  4 Projecting Images into Semantic Word Spaces  In order to learn semantic relationships and class membership of images we project the image feature vectors into the 50-dimensional word space. During training and testing, we consider a set of classes Y . Some of the classes y in this set will have available training data, others will be zero-shot classes without any training data. We deﬁne the former as the seen classes Ys and the latter as the unseen classes Yu. Let W = Ws ∪ Wu be the set of word vectors capturing distributional information for both seen and unseen visual classes, respectively. All training images x(i) ∈ Xy of a seen class y ∈ Ys are mapped to the word vector wy correspond- ing to the class name. To train this mapping, we minimize the following objective function with  3  Figure 2: T-SNE visualization of the semantic word space. Word vector locations are highlighted and mapped image locations are shown both for images for which this mapping has been trained and unseen images. The unseen classes are cat and truck.  respect to the matrix θ ∈ R50×F :  J(θ) =  (cid:88)  (cid:88)  y∈Ys  x(i)∈Xy  (cid:107)wy − θx(i)(cid:107)2.  (1)  By projecting images into the word vector space, we implicitly extend the word semantics with a visual grounding, allowing us to query the space, for instance for prototypical visual instances of a word or the average color of concrete nouns. Fig. 2 shows a visualization of the 50-dimensional semantic space with word vectors and images of both seen and unseen classes. The unseen classes are cat and truck. The mapping from 50 to 2 dimensions was done with t-SNE [33]. We can observe that most classes are tightly clustered around their corresponding word vector while the zero-shot classes (cat and truck for this mapping) do not have close-by vectors. However, the images of the two zero-shot classes are close to semantically similar classes. For instance, the cat testing images are mapped most closely to dog, and horse and are all very far away from car or ship. This motivated the idea for ﬁrst ﬁnding outliers and then classifying them to the zero-shot word vectors. Now that we have covered the representations for words and images as well as the image to word space mapping we can describe the probabilistic model for joint zero-shot learning and standard image classiﬁcation.  5 Zero-Shot Learning Model  In this section we ﬁrst give an overview of our model and then describe each of its components. In general, we want to predict p(y|x), the conditional probability for both seen and unseen classes y ∈ Ys ∪ Yu given an image x. Because standard classiﬁers will never predict a class that has no training examples, we introduce a binary visibility random variable which indicates whether an  4    airplaneautomobilebirdcatdeerdogfroghorseshiptruckcatautomobiletruckfrogshipairplanehorsebirddogdeer(cid:88)  image is in a seen or unseen class V ∈ {s, u}. Let Xs be the set of all feature vectors for training images of seen classes. We predict the class y for a new input image x via:  p(y|x, Xs, W, θ) =  P (y|V, x, Xs, W, θ)P (V |x, Xs, W, θ).  (2)  marginal: P (x|Xs, Ws, θ) = (cid:80)  V ∈{s,u} Next, we will describe each factor in Eq. 2. The term P (V = u|x, Xs, W, θ) is the probability of an image being in an unseen class. It can be computed by thresholding an outlier detection score. This score is computed on the manifold of training images that were mapped to the semantic word space. We use a threshold on the marginal of each point under a mixture of Gaussians. The mapped points of seen classes are used to obtain this N (θx|wy, Σy)P (y). The Gaussian of each class is parameterized by the corresponding semantic word vector wy for its mean and a covariance matrix Σy that is estimated from all the mapped training points with that label. We restrict the Gaussians to be isometric to prevent overﬁtting. For a new image x, the outlier detector then becomes the indicator function that is 1 if the marginal probability is below a certain threshold T :  P (x|y)P (y) = (cid:80)  y∈Ys  y∈Ys  P (V = u|x, Xs, W, θ) := 1{P (x|Xs, Ws, θ) < T}  (3)  We provide an experimental analysis for various thresholds T below. In the case where V = s, i.e. the point is considered to be of a known class, we can use any clas- siﬁer for obtaining P (y|V = s, x, Xs). We use a softmax classiﬁer on the original F -dimensional features. For the zero-shot case where V = u we assume an isometric Gaussian distribution around each of the zero-shot semantic word vectors. An alternative would be to use the method of Kriegel et al. [16] to obtain an outlier probability for each testing point and then use the weighted combination of classiﬁers for both seen and unseen classes.  6 Experiments  We run most of our experiments on the CIFAR10 dataset. The dataset has 10 classes, each with 5000 32 × 32 × 3 RGB images. We use the unsupervised feature extraction method of Coates and Ng [6] to obtain a 12,800-dimensional feature vector for each image. In the following experiments, we omit the training images of 2 classes for the zero-shot analysis.  6.1 Zero-Shot Classes Only  In this section we compare classiﬁcation between only two zero-shot classes. We observe that if there is no seen class that is remotely similar to the zero-shot classes, the performance is close to random. In other words, if the two zero-shot classes are the most similar classes and the seen classes do not properly span the subspace of the zero-shot classes then performance is poor. For instance, when cat and dog are taken out from training, the resulting zero-shot classiﬁcation does not work well because none of the other 8 categories is similar enough to learn a good feature mapping. On the other hand, if cat and truck are taken out, then the cat vectors can be mapped to the word space thanks to transfer from dogs and trucks can be mapped thanks to car, so the performance is very high. Fig. 3 shows the performance at various cutoffs for the outlier detection. The cutoff is deﬁned on the negative log-likelihood of the marginal of each point in the outlier detection. We can observe that when classifying images of unseen classes into only zero-shot classes (right side of the ﬁgure), we can differentiate images with an accuracy of above 80%.  6.2 Zero-Shot and Seen Classes  In Fig. 3 we can observe that depending on the threshold that splits images into seen or unseen classes at test time we can obtain accuracies of trained classes of approximately 80%. At 70%  5  Figure 3: Visualization of the accuracy of the seen classes (lines from the top left to bottom right) and pairs of zero-shot classes (lines from bottom left to top right) at different thresholds of the negative log-likelihood of each mapped test image vector.  accuracy, unseen classes can be classiﬁed with accuracies of between 30% to 15%. Random chance is 10%.  7 Conclusion  We introduced a novel model for joint standard and zero-shot classiﬁcation based on deep learned word and image representations. The two key ideas are that (i) using semantic word vector repre- sentations can help to transfer knowledge between categories even when these representations are learned in an unsupervised way and (ii) that our Bayesian framework that ﬁrst differentiates outliers from points on the projected semantic manifold can help to combine both zero-shot and seen classi- ﬁcation into one framework. If the task was only to differentiate between various zero-shot classes we could obtain accuracies of up to 90% with a fully unsupervised model.  References  [1] M. Baroni and A. Lenci. Distributional memory: A general framework for corpus-based semantics.  Computational Linguistics, 36(4):673–721, 2010.  [2] E. Bart and S. Ullman. Cross-generalization: learning novel classes from a single example by feature  replacement. In CVPR, 2005.  [3] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. J. Mach.  Learn. Res., 3, March 2003.  [4] J. Blitzer, M. Dredze, and F. Pereira. Biographies, Bollywood, Boom-boxes and Blenders: Domain  Adaptation for Sentiment Classiﬁcation. In ACL, 2007.  [5] E. Bruni, G. Boleda, M. Baroni, and N. Tran. Distributional semantics in technicolor. In ACL, 2012.  6  0102030405060708000.10.20.30.40.50.60.70.80.91Negative log p(x)Classification accuracy  automobile−frogdeer−truckfrog−truckhorse−shiphorse−truck[6] A. Coates and A. Ng. The Importance of Encoding Versus Training with Sparse Coding and Vector  Quantization . In ICML, 2011.  [7] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: deep neural networks  with multitask learning. In ICML, 2008.  [8] J. Curran. From Distributional to Semantic Similarity. PhD thesis, University of Edinburgh, 2004. [9] K. Erk and S. Pad´o. A structured vector space model for word meaning in context. In EMNLP, 2008. [10] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. In CVPR, 2009. [11] Y. Feng and M. Lapata. Visual information in semantic representation. In HLT-NAACL, 2010. [12] M. Fink. Object classiﬁcation from a single example utilizing class relevance pseudo-metrics. In NIPS,  2004.  [13] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for Large-Scale sentiment classiﬁcation: A deep  learning approach. In ICML, 2011.  [14] D. Hoiem, A.A. Efros, and M. Herbert. Geometric context from a single image. In ICCV, 2005. [15] E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.  Improving Word Representations via Global  Context and Multiple Word Prototypes. In ACL, 2012.  [16] H. Kriegel, P. Kr¨oger, E. Schubert, and A. Zimek. LoOP: local Outlier Probabilities. In Proceedings of  the 18th ACM conference on Information and knowledge management, CIKM ’09, 2009. [17] R.; Perona L. Fei-Fei; Fergus. One-shot learning of object categories. TPAMI, 28, 2006. [18] B. M. Lake, J. Gross R. Salakhutdinov, and J. B. Tenenbaum. One shot learning of simple visual concepts.  2011.  [19] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to Detect Unseen Object Classes by Between-  Class Attribute Transfer. In CVPR, 2009.  [20] T. K. Landauer and S. T. Dumais. A solution to Plato’s problem: the Latent Semantic Analysis theory of  acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240, 1997.  [21] H. Larochelle, D. Erhan, and Y. Bengio. Zero-data learning of new tasks. In AAAI, 2008. [22] C.W. Leong and R. Mihalcea. Going beyond text: A hybrid image-text approach for measuring word  relatedness. In IJCNLP, 2011.  [23] D. Lin. Automatic retrieval and clustering of similar words.  768–774, 1998.  In Proceedings of COLING-ACL, pages  [24] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y. Ng. Multimodal deep learning. In ICML, 2011. [25] S. Pado and M. Lapata. Dependency-based construction of semantic space models. Computational Lin-  guistics, 33(2):161–199, 2007.  [26] M. Palatucci, D. Pomerleau, G. Hinton, and T. Mitchell. Zero-shot learning with semantic output codes.  In NIPS, 2009.  [27] Guo-Jun Qi, C. Aggarwal, Y. Rui, Q. Tian, S. Chang, and T. Huang. Towards cross-category knowledge  propagation for learning visual concepts. In CVPR, 2011.  [28] A. Torralba R. Salakhutdinov, J. Tenenbaum. Learning to learn with compound hierarchical-deep models.  In NIPS, 2012.  [29] H. Sch¨utze. Automatic word sense discrimination. Computational Linguistics, 24:97–124, 1998. [30] R. Socher and L. Fei-Fei. Connecting modalities: Semi-supervised segmentation and annotation of images  using unaligned text corpora. In CVPR, 2010.  [31] N. Srivastava and R. Salakhutdinov. Multimodal learning with deep boltzmann machines. In NIPS, 2012. [32] P. D. Turney and P. Pantel. From frequency to meaning: Vector space models of semantics. Journal of  Artiﬁcial Intelligence Research, 37:141–188, 2010.  [33] L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research,  2008.  7  ","This work introduces a model that can recognize objects in images even if notraining data is available for the objects. The only necessary knowledge aboutthe unseen categories comes from unsupervised large text corpora. In ourzero-shot framework distributional information in language can be seen asspanning a semantic basis for understanding what objects look like. Mostprevious zero-shot learning models can only differentiate between unseenclasses. In contrast, our model can both obtain state of the art performance onclasses that have thousands of training images and obtain reasonableperformance on unseen classes. This is achieved by first using outlierdetection in the semantic space and then two separate recognition models.Furthermore, our model does not require any manually defined semantic featuresfor either words or images."
1301.3583,2013,Big Neural Networks Waste Capacity  ,"['Yann Dauphin', 'Yoshua Bengio']",https://arxiv.org/pdf/1301.3583.pdf,"Big Neural Networks Waste Capacity  Yann N. Dauphin & Yoshua Bengio  D´epartement d’informatique et de recherche op´erationnelle  Universit´e de Montr´eal, Montr´eal, QC, Canada  3 1 0 2    r a     M 4 1      ]  G L . s c [      4 v 3 8 5 3  .  1 0 3 1 : v i X r a  Abstract  This article exposes the failure of some big neural networks to leverage added capacity to reduce underﬁtting. Past research suggest diminishing returns when increasing the size of neural networks. Our experiments on ImageNet LSVRC- 2010 show that this may be due to the fact there are highly diminishing returns for capacity in terms of training error, leading to underﬁtting. This suggests that the optimization method - ﬁrst order gradient descent - fails at this regime. Directly attacking this problem, either through the optimization method or the choices of parametrization, may allow to improve the generalization error on large datasets, for which a large capacity is required.  1  Introduction  Deep learning and neural networks have achieved state-of-the-art results on vision 1, language 2 , and audio-processing tasks 3. All these cases involved fairly large datasets, but in all these cases, even larger ones could be used. One of the major challenges remains to extend neural networks on a much larger scale, and with this objective in mind, this paper asks a simple question: is there an optimization issue that prevents efﬁciently training larger networks? Prior evidence of the failure of big networks in the litterature can be found for example in Coates et al. (2011), which shows that increasing the capacity of certain neural net methods quickly reaches a point of diminishing returns on the test error. These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011). Furthermore, Coates et al. (2011) shows that while neural net methods fail to leverage added capacity K-Means can. This has allowed K-Means to reach state-of-the-art performance on CIFAR-10 for methods that do not use artiﬁcial transformations. This is an unexpected result because K-Means is a much dumber unsupervised learning algorithm when compared with RBMs and regularized auto-encoders. Coates et al. (2011) argues that this is mainly due to K-Means making better use of added capacity, but it does not explain why the neural net methods failed to do this.  2 Experimental Setup  We will perform experiments with the well known ImageNet LSVRC-2010 object detection dataset4. The subset used in the Large Scale Visual Recognition Challenge 2010 contains 1000 object cate- gories and 1.2 million training images.  1Krizhevsky et al. (2012) reduced by almost one half the error rate on the 1000-class ImageNet object  2Mikolov et al. (2011) reduced perplexity on WSJ by 40% and speech recognition absolute word error rate  3For speech recognition, Seide et al. (2011) report relative word error rates decreasing by about 30% on  recognition benchmark  by > 1%.  datasets of 309 hours  4http://www.image-net.org/challenges/LSVRC/2010/  1  This dataset has many attractive features:  1. The task is difﬁcult enough for current algorithms that there is still room for much improve- ment. For instance, Krizhevsky et al. (2012) was able to reduce the error by half recently. What’s more the state-of-the-art is at 15.3% error. Assuming minimal error in the human labelling of the dataset, it should be possible to reach errors close to 0%.  2. Improvements on ImageNet are thought to be a good proxy for progress in object recogni-  tion (Deng et al., 2009).  3. It has a large number of examples. This is the setting that is commonly found in industry where datasets reach billions of examples. Interestingly, as you increase the amount of data, the training error converges to the generalization error. In other words, reducing training error is well correlated with reducing generalization error, when large datasets are available. Therefore, it stands to reason that resolving underﬁtting problems may yield signiﬁcant improvements.  the network is a softmax over possible classes (sof tmax(x) = e−x/(cid:80)  We use the features provided by the Large Scale Visual Recognition Challenge 20105. The images are convolved with SIFT features, then K-Means is used to form a visual vocabulary of 1000 visual words. Following the litterature, we report the Top-5 error rate only. The experiments focus on the behavior of Multi-Layer Perceptrons (MLP) as capacity is increased. This is done by increasing the number of hidden units in the network. The ﬁnal classiﬁcation layer of i e−xi). The hidden layers use the logistic sigmoid activation function (σ(x) = 1/(1+e−x)). We initialize the weights of the hidden layer according to the formula proposed by Glorot and Bengio (2010). The parameters of the classiﬁcation layer are initialized to 0, along with all the bias (offset) parameters of the MLP. The hyper-parameters to tune are the learning rate and the number of hidden units. We are in- terested in optimization performance so we cross-validate them based on the training error. We use a grid search with the learning rates taken from {0.1, 0.01} and the number of hiddens from {1000, 2000, 5000, 7000, 10000, 15000}. When we report the performance of a network with a given number of units we choose the best learning rate. The learning rate is decreased by 5% every- time the training error goes up aftern an epoch. We do not use any regularization because it would typically not help to decrease the training set error. The number of epochs is set to 300 s that it is large enough for the networks to converge. The experiments are run on a cluster of Nvidia Geforce GTX 580 GPUs with the help of the Theano library (Bergstra et al., 2010). We make use of HDF5 (Folk et al., 2011) to load the dataset in a lazy fashion because of its large size. The shortest training experiment took 10 hours to run and the longest took 28 hours.  3 Experimental Results  Figure 1 shows the evolution of the training error as the capacity is increased. The common intuition is that this increased capacity will help ﬁt the training set - possibly to the detriment of generalization error. For this reason practitioners have focused mainly on the problem of overﬁtting the dataset when dealing with large networks - not underﬁtting. In fact, much research is concerned with proper regularization of such large networks (Hinton et al., 2012, 2006). However, Figure 2 reveals a problem in the training of big networks. This ﬁgure is the derivative of the curve in Figure 1 (using the number of errors instead of the percentage). It may be interpreted as the return on investment (ROI) for the addition of capacity. The Figure shows that the return on investment of additional hidden units decreases fast, where in fact we would like it to be close to constant. Increasing the capacity from 1000 to 2000 units, the ROI decreases by an order of magnitude. It is harder and harder for the model to make use of additional capacity. The red line is a baseline where the additional unit is used as a template matcher for one of the training errors. In this case, the number of errors reduced per unit is at least 1. We see that the MLP does not manage to beat this baseline after 5000 units, for the given number of training iterations.  5http://www.image-net.org/challenges/LSVRC/2010/download-public  2  Figure 1: Training error with respect to the capacity of a 1-layer sigmoidal neural network. This curve seems to suggest we are correctly leveraging added capacity.  Figure 2: Return on investment on the addition of hidden units for a 1-hidden layer sigmoidal neural network. The vertical axis is the number of training errors removed per additional hidden unit, after 300 epochs. We see here that it is harder and harder to use added capacity.  For reference, we also include the learning curves of the networks used for Figure 1 and 2 in Figure 3. We see that the curves for capacities above 5000 all converge towards the same point.  4 Future directions  This rapidly decreasing return on investment for capacity in big networks seems to be a failure of ﬁrst order gradient descent. In fact, we know that the ﬁrst order approximation fails when there are a lot of interactions between hidden units. It may be that adding units increases the interactions between units and causes the Hessian to be ill-conditioned. This reasoning suggests two research directions:  • methods that break interactions between large numbers of units. This helps the Hessian to be better conditioned and will lead to better effectiveness for ﬁrst-order descent. This  3  Figure 3: Training error with respect to the number of epochs of gradient descent. Each line is a 1-hidden layer sigmoidal neural network with a different number of hidden units.  type of method can be implemented efﬁciently. Examples of this approach are sparsity and orthognality penalties.  • methods that model interactions between hidden units. For example, second order methods (Martens, 2010) and natural gradient methods (Le Roux et al., 2008). Typically, these are expensive approaches and the challenge is in scaling them to large datasets, where stochas- tic gradient approaches may dominate. The ideal target is a stochastic natural gradient or stochastic second-order method.  The optimization failure may also be due to other reasons. For example, networks with more ca- pacity have more local minima. Future work should investigate tests that help discriminate between ill-conditioning issues and local minima issues. Fixing this optimization problem may be the key to unlocking better performance of deep networks. Based on past observations (Bengio, 2009; Erhan et al., 2010), we expect this optimization problem to worsen for deeper networks, and our experimental setup should be extended to measure the effect of depth. As we have noted earlier, improvements on the training set error should be well correlated with generalization for large datasets.  References Bengio, Y. (2009). Learning deep architectures for AI. Now Publishers. Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- In  Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Coates, A., Lee, H., and Ng, A. Y. (2011). An analysis of single-layer networks in unsupervised fea- ture learning. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011).  Courville, A., Bergstra, J., and Bengio, Y. (2011). Unsupervised models of images by spike-and- slab RBMs. In Proceedings of the Twenty-eight International Conference on Machine Learning (ICML’11).  Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A Large-Scale  Hierarchical Image Database. In CVPR09.  Erhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., and Bengio, S. ((11) 2010). Why  does unsupervised pre-training help deep learning? J. Machine Learning Res.  4  Folk, M., Heber, G., Koziol, Q., Pourmal, E., and Robinson, D. (2011). An overview of the hdf5 technology suite and its applications. In Proceedings of the EDBT/ICDT 2011 Workshop on Array Databases, pages 36–47. ACM.  Glorot, X. and Bengio, Y. (2010). Understanding the difﬁculty of training deep feedforward neural networks. In JMLR W&CP: Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2010), volume 9, pages 249–256.  Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets.  Neural Computation, 18, 1527–1554.  Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012).  Im- proving neural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.  Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classiﬁcation with deep convolu- tional neural networks. In Advances in Neural Information Processing Systems 25 (NIPS’2012). Le Roux, N., Manzagol, P.-A., and Bengio, Y. (2008). Topmoumoute online natural gradient algo- rithm. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS’07), Cambridge, MA. MIT Press.  Martens, J. (2010). Deep learning via Hessian-free optimization. In L. Bottou and M. Littman, ed- itors, Proceedings of the Twenty-seventh International Conference on Machine Learning (ICML- 10), pages 735–742. ACM.  Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011). Empirical evaluation and combination of advanced language modeling techniques. In Proc. 12th annual conference of the international speech communication association (INTERSPEECH 2011).  Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., and Glorot, X. (2011). Higher order contractive auto-encoder. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD).  Seide, F., Li, G., and Yu, D. (2011). Conversational speech transcription using context-dependent  deep neural networks. In Interspeech 2011, pages 437–440.  5  ","This article exposes the failure of some big neural networks to leverageadded capacity to reduce underfitting. Past research suggest diminishingreturns when increasing the size of neural networks. Our experiments onImageNet LSVRC-2010 show that this may be due to the fact there are highlydiminishing returns for capacity in terms of training error, leading tounderfitting. This suggests that the optimization method - first order gradientdescent - fails at this regime. Directly attacking this problem, either throughthe optimization method or the choices of parametrization, may allow to improvethe generalization error on large datasets, for which a large capacity isrequired."
1301.3568,2013,Joint Training Deep Boltzmann Machines for Classification  ,"['Ian Goodfellow', 'Aaron Courville', 'Yoshua Bengio']",https://arxiv.org/pdf/1301.3568.pdf,"Joint Training of Deep Boltzmann Machines for  Classiﬁcation  Ian J. Goodfellow  Aaron Courville  Universit´e de Montr´eal  Yoshua Bengio  Abstract  We introduce a new method for training deep Boltzmann machines jointly. Prior methods of training DBMs require an initial learning pass that trains the model greedily, one layer at a time, or do not perform well on classiﬁcation tasks. In our approach, we train all layers of the DBM simultaneously, using a novel train- ing procedure called multi-prediction training. The resulting model can either be interpreted as a single generative model trained to maximize a variational approx- imation to the generalized pseudolikelihood, or as a family of recurrent networks that share parameters and may be approximately averaged together using a novel technique we call the multi-inference trick. We show that our approach performs competitively for classiﬁcation and outperforms previous methods in terms of ac- curacy of approximate inference and classiﬁcation with missing inputs.  1 Deep Boltzmann machines  A deep Boltzmann machine (Salakhutdinov and Hinton, 2009) is a probabilistic model consisting of many layers of random variables, most of which are latent. Typically, a DBM contains a set of D input features v that are called the visible units because they are always observed during both training and evaluation. The DBM is usually applied to classiﬁcation problems and thus often represents the class label with a one-of-k code in the form of a discrete-valued label unit y. y is observed (on examples for which it is available) during training. The DBM also contains several latent variables that are never observed. These hidden units are usually organized into L layers h(i) of size Ni, i = 1, . . . , L, with each unit in a layer conditionally independent of the other units in the layer given the neighboring layers. These conditional independence properties allow fast Gibbs sampling because an entire layer of units can be sampled at a time. Likewise, mean ﬁeld inference with ﬁxed point equations is fast because each ﬁxed point equation gives a solution to roughly half of the variational parameters. Inference proceeds by alternating between updating all of the even numbered layers and updating all of the odd numbered layers. A DBM deﬁnes a probability distribution by exponentiating and normalizing an energy function  3 1 0 2     y a M 1         ] L M  . t a t s [      3 v 8 6 5 3  .  1 0 3 1 : v i X r a  where  Z =  P (v, h, y) =  (cid:88)  exp (−E(v, h, y))  1 Z exp (−E(v(cid:48), h(cid:48), y(cid:48))) .  v(cid:48),h(cid:48),y(cid:48)  Z, the partition function, is intractable, due to the summation over all possible states. Maximum likelihood learning requires computing the gradient of log Z. Fortunately, the gradient can be esti- mated using an MCMC procedure (Younes, 1999; Tieleman, 2008). Block Gibbs sampling of the layers makes this procedure efﬁcient. The structure of the interactions in h determines whether further approximations are necessary. In the pathological case where every element of h is conditionally independent of the others given the  1  Figure 1: The training procedure employed by Salakhutdinov and Hinton (2009) on MNIST. a) An RBM comprising v and h(1) is trained to maximize the log likelihood of v using CD. Next, another RBM is trained with CD, using y and samples of h(1) conditioned on v as observed data, and h(2) as hidden units. b) The two RBMs are stitched together to form one DBM over v, h(1), h(2), and y. This DBM is trained to maximize the log likelihood of v and y using PCD. c) y is deleted from the model. An extra MLP is built on top of v and the mean ﬁeld expectations of h(1) and h(2). The parameters of the DBM are frozen, and the parameters of the MLP are initialized based on the DBM parameters, then trained via nonlinear conjugate gradient descent to predict y from v and the mean ﬁeld features.  visible units, the DBM is simply an RBM and log Z is the only intractable term of the log likelihood. In the general case, interactions between different elements of h render the posterior P (h | v, y) intractable. Salakhutdinov and Hinton (2009) overcome this by maximizing the lower bound on the log likelihood given by the mean ﬁeld approximation to the posterior rather than maximizing the log likelihood itself. Again, block mean ﬁeld inference over the layers makes this procedure efﬁcient. An interesting property of the DBM is that the training procedure thus involves feedback connections between the layers. Consider the simple DBM consisting entirely of binary-valued units, with the energy function  E(v, h) = −vT W (1)h(1) − h(1)T W (2)h(2).  Approximate inference in this model involves repeatedly applying two ﬁxed-point update equations to solve for the mean ﬁeld approximation to the posterior. Essentially it involves running a recurrent net in order to obtain approximate expectations of the latent variables. Beyond their theoretical appeal as a deep model that admits simultaneous training of all components using a generative cost, DBMs have achieved excellent performance in practice. When they were ﬁrst introduced, DBMs set the state of the art on the permutation-invariant1 version of the MNIST handwritten digit recognition task at 0.95%. Recently, new techniques were used in conjunction with DBM pretraining to set a new state of the art of 0.79% test error (Hinton et al., 2012).  2 The joint training problem  Unfortunately, it is not possible to train a deep Boltzmann machine using only the variational bound and approximate gradient described above. See (Goodfellow et al., 2013) for an example of a DBM that has failed to learn using the naive training algorithm. Salakhutdinov and Hinton (2009) found that for CD / PCD to work, the DBM must be initialized by training one layer at a time. After each layer is trained as an RBM, the RBMs can be modiﬁed slightly, assembled into a DBM, and the DBM may be trained with PCD learning rule described above. In order to achieve good classiﬁcation results, an MLP designed speciﬁcally to predict y from v must be trained on top of the DBM model. Simply running mean ﬁeld inference to predict y given v in the DBM model does not work nearly  1By permutation-invariant, we mean that permuting all of the input pixels prior to learning the network should not cause a change in performance, so using synthetic image distortions or convolution to engineer knowledge about the structure of the images into the system is not allowed.  2  c)b)a)as well. See ﬁgure 1 for a graphical description of the training procedure used by Salakhutdinov and Hinton (2009). In this paper, we propose a method that enables the deep Boltzmann machine to be jointly trained, and to achieve excellent performance as a classiﬁer without an additional classiﬁcation-speciﬁc ex- tension of the model. The standard approach requires training L+2 different models using L+2 dif- ferent objective functions, and does not yield a single model that excels at answering all queries. Our approach requires training only one model with only one objective function, and the resulting model outperforms previous approaches at answering all kinds of queries (classiﬁcation, classiﬁcation with missing inputs, predicting arbitrary subsets of variables given arbitrary subsets of variables).  3 Motivation  There are numerous reasons to prefer a single-model, single-training stage approach to deep Boltz- mann machine learning:  1. Optimization As a greedy optimization procedure, layerwise training may be suboptimal. Small-scale experimental work has demonstrated this to be the case for deep belief net- works (Arnold and Ollivier, 2012). In general, for layerwise training to be optimal, the training procedure for each layer must take into account the inﬂuence that the deeper layers will provide. The standard training layerwise procedure simply does not attempt to be optimal. The procedures used by Le Roux and Bengio (2008); Arnold and Ollivier (2012) make an optimistic assumption that the deeper layers will be able to implement the best possi- ble prior on the current layer’s hidden units. This approach is not immediately applica- ble to Boltzmann machines because it is speciﬁed in terms of learning the parameters of P (h(i−1)|h(i)) assuming that the parameters of the P (h(i)) will be set optimally later. In a DBM the symmetrical nature of the interactions between units means that these two distri- butions share parameters, so it is not possible to set the parameters of the one distribution, leave them ﬁxed for the remainder of learning, and then set the parameters of the other distribution. Moreover, model architectures incorporating design features such as sparse connections, pooling, or factored multilinear interactions make it difﬁcult to predict how best to structure one layer’s hidden units in order for the next layer to make good use of them.  2. Probabilistic modeling Using multiple models and having some models specialized for exactly one task (like predicting y from v) loses some of the beneﬁt of probabilistic mod- eling. If we have one model that excels at all tasks, we can use inference in this model to answer arbitrary queries, perform classiﬁcation with missing inputs, and so on.  3. Simplicity Needing to implement multiple models and training stages makes the cost of developing software with DBMs greater, and makes using them more cumbersome. Beyond the practical considerations, it can be difﬁcult to monitor training and tell what kind of results during layerwise DBM pretraining will correspond to good classiﬁcation accuracy later. Our joint training procedure allows the user to monitor the model’s ability of interest (usually ability to classify y given v) from the very start of training.  4 Multi-Prediction Training  Our proposed approach is to directly train the DBM to be good at solving all possible variational inference problems. We call this multi-prediction training because the procedure involves training the model to predict any subset of variables given the complement of that subset of variables. Speciﬁcally, we use stochastic gradient descent on the multi-prediction (MP) objective function  J(v, θ) = −(cid:88)  log Q∗  v−Si  (vSi)  i  3  Figure 2: Graphical description of multi-prediction training. During MP training, we sample ex- amples (v, y) from the training set. For each example, we choose one subset of variables (uniformly at random) to serve as the input to an inference problem, and use the complement of this subset as targets for training. We then run mean ﬁeld inference and backpropagate the prediction error through the entire computational graph of the inference process. Here, we depict the process for three different examples, one example per row. We use black circles to indicate observed data and blue circles to indicate prediction targets. The green arrows show the ﬂow of computation through the mean ﬁeld graph. Each column indicates another time step in the mean ﬁeld inference algorithm. Dotted lines indicate quantities that are not used for this instantiation of the problem, but would be used if we ran another iteration of mean ﬁeld. Here, we show only one iteration of mean ﬁeld. To work well, MP training should be run with 5-10 iterations of mean ﬁeld.  4  Figure 3: Mean ﬁeld inference applied to MNIST digits. The ﬁrst column shows the true digits. The second column shows pixels of the digits to be masked out, with red pixels indicating the region to be witheld from the input to the DBM. Yellow-boxed rows show input pixels. Green-boxed rows represent the class variables. The subsequent columns show the DBM incrementally predicting the missing variables, with each column being one iteration of mean ﬁeld. On rows where the green- boxed class variable was masked out, the uncertainty over the class is represented by displaying a weighted average of templates for the 10 different classes.  where S is a sequence of subsets of the possible indices of v and  Q∗ v−Si  (vSi , h) = argminQDKL (Q(vSi , h)(cid:107)P (vSi, h | v−Si)) .  In other words, the criterion for a single example v is a sum of several terms, with term i measuring the model’s ability to predict a subset of the inputs, vSi, given the remainder of the inputs, v−Si. During SGD training, we sample minibatches of values of v and Si. Sampling an Si uniformly simply requires sampling one bit (1 with probability 0.5) for each variable, to determine whether that variable should be an input to the inference procedure or a prediction target. In this paper, Q is constrained to be factorial, though one could design model families for which it makes sense to use richer structure in Q. In order to accomplish the minimization, we instantiate a recurrent net that repreatedly runs the mean ﬁeld ﬁxed point equations, and backpropagrate the gradient of J through the entire recurrent net. See Fig. 2 for a graphical description of this training procedure, and Fig. 3 for an example of the inference procedure run on MNIST digits.  5 The Multi-Inference Trick  Mean ﬁeld inference can be expensive due to needing to run the ﬁxed point equations several times in order to reach convergence. In order to reduce this computational expense, it is possible to train using fewer mean ﬁeld iterations than required to reach convergence. In this case, we are no longer necessarily minimizing J as written, but rather doing partial training of a large number of ﬁxed- iteration recurrent nets that solve related problems. We can approximately take the geometric mean over all predicted distributions Q and renormalize in order to combine the predictions of all of these recurrent nets. This way, imperfections in the training procedure are averaged out, and we are able to solve inference tasks even if the corresponding recurrent net was never sampled during MP training.  5  Figure 4: Graphical description of the multi-inference trick. Consider the problem of estimating y given v. A mean ﬁeld iteration consists of ﬁrst applying a mean ﬁeld update to h(1) and y, then applying one to h(2). When using the multi-inference trick, we start the iteration by ﬁrst computing r as the mean ﬁeld update v would receive if it were not observed. We then use 0.5(r + v) in place of v and run a regular mean ﬁeld iteration.  Figure 5: Here we use MP training on MNIST with only 5 mean ﬁeld iterations, for a set of hyper- paramters where 10 mean ﬁeld iterations are necessary to get good convergence. The classiﬁcation error rate for mean ﬁeld quits improving after about 150 epochs of training, but the multi-inference trick allows us to extract more information from the model. The model continues improving at multi-inference for 100 epochs after its performance at mean ﬁeld inference has stagnated.  6  Mean Field IterationMulti-Inference Iteration+=Step 1Step 2Previous State + ReconstructionStep 1Step 2Previous StateIn order to approximate this average efﬁciently, we simply take the geometric mean at each step of inference, instead of attempting to take the correct geometric mean of the entire inference process. This is the same type of approximation used to take the average over several MLP predictions when using dropout (Hinton et al., 2012). Here, the averaging rule is slightly different. In dropout, the dif- ferent MLPs we average over either include or exclude diferent each variable. To take the geometric mean over a unit hj that receives input from vi, we average together the contribution viWij from the model that contains vi and the contribution 0 from the model that does not. The ﬁnal contribution from vi is 0.5viWij so the dropout model averaging rule is to run an MLP with the weights divided by 2. For the multi-inference trick, each model we are averaging over solves a different inference problem. In half the problems, vi is observed, and constributes viWij to hj’s total input. In the other half of the problems, vi is inferred. If we represent the mean ﬁeld estimate of vi with ri, then in this case that unit contributes riWij to hj’s total input. To run multi-inference, we thus replace references to v with 0.5(v + r), where r is updated at each mean ﬁeld iteration. The main reason this approach is effective is that it gives a good way to incorporate information from many recurrent nets trained in slightly different ways. However, it can also be understand as including an input denoising step built into the inference. See Fig. 4 for a graphical depiction of the method, and Fig. 5 for an example of it in action.  6  Justiﬁcation and advantages  In the case where we run the recurrent net for predicting Q to convergence, the multi-prediction training algorithm follows the gradient of the objective function J. This can be viewed as a mean ﬁeld approximation to the generalized pseudolikelihood. While both pseudolikelihood and likelihood are asymptotically consistent estimators, their behavior in the limited data case is different. Maximum likelihood should be better if the overall goal is to draw realistic samples from the model, but generalized pseudolikelihood can often be better for training a model to answer queries conditioning on sets similar to the Si used during training. Note that our variational approximation is not quite the same as the way variational approximations are usually applied. We use variational inference to ensure that the distributions we shape using backprop are as close as possible to the true conditionals. This is different from the usual approach to variational learning, where Q is used to deﬁne a lower bound on the log likelihood and variational inference is used to make the bound as tight as possible. In the case where the recurrent net is not trained to convergence, there is an alternate way to justify MP training. Rather than doing variational learning on a single probabilistic model, the MP proce- dure trains a family of recurrent nets to solve related prediction problems by running for some ﬁxed number of iterations. Each recurrent net is trained only a subset of the data (and most recurrent nets are never trained at all, but only work because they share parameters with the others). In this case, the multi-inference trick allows us to justify MP training as approximately training an ensemble of recurrent nets using bagging. Stoyanov et al. (2011) have observed that a similar training strategy is useful because it trains the model to work well with the inference approximations it will be evaluated with at test time. We ﬁnd these properties to be useful as well. The choice of this type of variational learning combined with the underlying generalized pseudolikelihood objective makes an MP-DBM very well suited for solving approximate inference problems but not very well suited for sampling. Our primary design consideration when developing multi-prediction training was ensuring that the learning rule was state-free. PCD training uses persistent Markov chains to estimate the gradient. These Markov chains are used to approximately sample from the model, and only sample from ap- proximately the right distribution if the model parameters evolve slowly. The MP training rule does not make any reference to earlier training steps, and can be computed with no burn in. This means that the accuracy of the MP gradient is not dependent on properties of the training algorithm such as the learning rate–properties which can easily break PCD for many choices of the hyperparameters.  7  Figure 6: During cross-validation, MP training consistently performs better at classiﬁcation than either centering or centering with the special negative phase.  Another beneﬁt of MP is that it is easy to obtain an unbiased estimate of the MP objective from a small number of samples of v and i. This is in contrast to the log likelihood, which requires estimating the log partition function. The best known method for doing so is AIS, which is relatively expensive (Neal, 2001). Cheap estimates of the objective function enable early stopping based on the MP-objective (though we generally use early stopping based on classiﬁcation accuracy) and optimization based on line searches (though we do not explore that possibility in this paper).  7 Regularization  In order to obtain good generalization performance, Salakhutdinov and Hinton (2009) regularized both the weights and the activations of the network. Salakhutdinov and Hinton (2009) regularize the weights using an L2 penalty. We ﬁnd that for joint training, it is critically important not to do this. When the second layer weights are not trained well enough for them to be useful for modeling the data, the weight decay term will drive them to become very small, and they will never have an opportunity to recover. It is much better to use constraints on the norms of the columns of the weight vectors, as advocated by Hinton et al. (2012). Salakhutdinov and Hinton (2009) regularize the activities of the hidden units with a somewhat com- plicated sparsity penalty. See http://www.mit.edu/˜rsalakhu/DBM.html for details. We use max(|Eh∼Q(h)[h] − t| − λ, 0) and backpropagate this through the entire inference graph. t and λ are hyperparameters.  8 Related work: centering  Montavon and M¨uller (2012) showed that reparameterizing the DBM to improve the condition num- ber of the Hessian results in succesful generative training without a greedy layerwise pretraining step. However, this method has never been shown to have good classiﬁcation performance, possibly because the reparameterization makes the features never be zero from the point of view of the ﬁnal classiﬁer. We evaluate its classiﬁcation performance in more detail in this work. We consider two methods of PCD training. In one, we use Rao-Blackwellization of the negative phase particles to reduce the variance of the negative phase. In the other variant, we use a special negative phase that Salakhutdi- nov and Hinton (2009) found useful. This negative phase uses a small amount of mean ﬁeld, which reduces the variance further but introduces some bias, and has better symmetry with the positive phase. See http://www.mit.edu/˜rsalakhu/DBM.html for details.  8  Figure 7: When classifying with missing inputs, the MP-DBM outperforms the other DBMs for most amounts of missing inputs.  Figure 8: When using approximate inference to resolve general queries, the standard DBM, cen- tered DBM, and MP-DBM all perform about the same when asked to predict a small number of variables. For larger queries, the MP-DBM performs the best.  9  9 MNIST experiments  In order to compare MP training and centering to standard DBM performance, we cross-validated each of the new methods by running 25 training experiments for each of three conditions: centered DBMs, centered DBMs with the special negative phase (“Centering+”), and MP training. For these experiments we did not use the multi-inference trick. All three conditions visited exactly the same set of 25 hyperparameter values for the momentum schedule, sparsity regularization hyperparameters, weight and bias initialization hyperparameters, weight norm constraint values, and number of mean ﬁeld iterations. The centered DBMs also re- quired one additional hyperparameter, the number of Gibbs steps to run for PCD. We used different values of the learning rate for the different conditions, because the different con- ditions require different ranges of learning rate to perform well. We use the same size of model, minibatch and negative chain collection as Salakhutdinov and Hinton (2009), with 500 hidden units in the ﬁrst layer, 1,000 hidden units in the second, 100 examples per minibatch, and 100 negative chains. See Fig. 6 for the results of cross-validation. On the validation set, MP training consistently per- forms better and is much less sensitive to hyperparameters than the other methods. This is likely because the state-free nature of the learning rule makes it perform better with settings of the learn- ing rate and momentum schedule that result in the model distribution changing too fast for a method based on Markov chains to keep up. When we ﬁne-tune the best model, the best “Centering+” DBM obtains a classiﬁcation error of 1.22 % on the test set. The best MP-DBM obtains a classiﬁcation error of 0.99 %. This compares to 0.95 % obtained by Salakhutdinov and Hinton (2009). If instead of adding an MLP to the model to do ﬁne tuning, we simply train a larger MP-DBM with twice as many hidden units in each layer, and apply the multi-inference trick, we obtain a slightly better classiﬁcation error rate of 0.91 %. In other words, we are able to classify better using a single large DBM and a generic inference procedure, rather than using a DBM followed by an entirely separate MLP model specalized for classiﬁcation. The original DBM was motivated primarily as a generative model with a high AIS score and as a classiﬁer. Here we explore some more uses of the DBM as a generative model. First, we evaluate the use of the DBM to classify in the presence of missing inputs. See Fig. 7 for details. We ﬁnd that for most amounts of missing inputs, the MP-DBM classiﬁes better than the standard DBM or the best centering DBM. We also explored the ability of the DBM to resolve queries about random subsets of variables. See Fig. 8 for details. Again, we ﬁnd that the MP-DBM outperforms the other DBMs.  10 Conclusion  This paper has demonstrated that MP training and the multi-inference trick provide a means of train- ing a single model, with a single stage of training, that matches the performance of standard DBMs but still works as a general probabilistic model, capable of handling missing inputs and answering general queries. In future work, we hope to obtain state of the art performance by combining MP training with dropout, and also to apply this method to other datasets.  References Arnold, L. and Ollivier, Y. (2012). Layer-wise learning of deep generative models. Technical report,  arXiv:1212.1524.  Goodfellow, I. J., Courville, A., and Bengio, Y. (2013). Scaling up spike-and-slab models for unsu-  pervised feature learning. IEEE, special issue on deep learning. (To appear).  Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012).  Im- proving neural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.  10  Le Roux, N. and Bengio, Y. (2008). Representational power of restricted Boltzmann machines and  deep belief networks. Neural Computation, 20(6), 1631–1649.  Montavon, G. and M¨uller, K.-R. (2012). Learning feature hierarchies with cented deep Boltzmann  machines. CoRR, abs/1203.4416.  Neal, R. M. (2001). Annealed importance sampling. Statistics and Computing, 11(2), 125–139. Salakhutdinov, R. and Hinton, G. (2009). Deep Boltzmann machines. In Proceedings of the Twelfth  International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2009), volume 8.  Stoyanov, V., Ropson, A., and Eisner, J. (2011). Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), volume 15 of JMLR Workshop and Conference Proceedings, pages 725–733, Fort Lauderdale. Supplementary material (4 pages) also available.  Tieleman, T. (2008). Training restricted Boltzmann machines using approximations to the likelihood gradient. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, ICML 2008, pages 1064– 1071. ACM.  Younes, L. (1999). On the convergence of Markovian stochastic algorithms with rapidly decreasing  ergodicity rates. Stochastics and Stochastic Reports, 65(3), 177–228.  11  ","We introduce a new method for training deep Boltzmann machines jointly. Priormethods of training DBMs require an initial learning pass that trains the modelgreedily, one layer at a time, or do not perform well on classification tasks.In our approach, we train all layers of the DBM simultaneously, using a noveltraining procedure called multi-prediction training. The resulting model caneither be interpreted as a single generative model trained to maximize avariational approximation to the generalized pseudolikelihood, or as a familyof recurrent networks that share parameters and may be approximately averagedtogether using a novel technique we call the multi-inference trick. We showthat our approach performs competitively for classification and outperformsprevious methods in terms of accuracy of approximate inference andclassification with missing inputs."
1301.3570,2013,A Nested HDP for Hierarchical Topic Models  ,"['John Paisley', 'Chong Wang', 'David Blei', 'Michael I. Jordan']",https://arxiv.org/pdf/1301.3570.pdf,"A Nested HDP for Hierarchical Topic Models  John Paisley  Department of EECS  UC Berkeley  Chong Wang  Dept. of Machine Learning Carnegie Mellon University  David Blei  Dept. of Computer Science  Princeton University  Michael I. Jordan Department of EECS  UC Berkeley  3 1 0 2     n a J    6 1      ] L M  . t a t s [      1 v 0 7 5 3  .  1 0 3 1 : v i X r a  Abstract  We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP is a generalization of the nested Chinese restaurant process (nCRP) that allows each word to follow its own path to a topic node according to a document-speciﬁc distribution on a shared tree. This alleviates the rigid, single-path formulation of the nCRP, allowing a document to more easily express thematic borrowings as a random effect. We demonstrate our algorithm on 1.8 million documents from The New York Times.1  1  Introduction  Organizing things hierarchically is a natural process of human activity. A hierarchical tree-structured representation of data can provide an illuminating means for understanding and reasoning about the information it contains. The nested Chinese restaurant process (nCRP) [2] is a model that performs this task for the problem of topic modeling. It does this by learning a tree structure for the underlying topics, with the inferential goal being that topics closer to the root are more general, and gradually become more speciﬁc in thematic content when following a path down the tree. The nCRP is limited in the hierarchies it can model. We illustrate this limitation in Figure 1. The nCRP models the topics that go into constructing a document as lying along one path of the tree. This signiﬁcantly limits the underlying structure that can be learned from the data. The nCRP performs document-speciﬁc path clustering; our goal is to develop a related Bayesian nonparametric prior that performs word-speciﬁc path clustering. We illustrate this objective in Figure 1. To this end, we make use of the hierarchical Dirichlet process [3], developing a novel prior that we refer to as the nested hierarchical Dirichlet process (nHDP). With the nHDP, a top-level nCRP becomes a base distribution for a collection of second-level nCRPs, one for each document. The nested HDP provides the opportunity for cross-thematic borrowing that is not possible with the nCRP.  2 Nested Hierarchical Dirichlet Processes  Nested CRPs. Nested Chinese restaurant processes (nCRP) are a tree-structured extension of the CRP that are useful for hierarchical topic modeling. A natural interpretation of the nCRP is as a tree where each parent has an inﬁnite number of children. Starting from the root node, a path is traversed down the tree. Given the current node, a child node is selected with probability proportional to the previous number of times it was selected among its siblings, or a new child is selected with probability proportional to a parameter α > 0. For hierarchical topic modeling with the nCRP, each node has an associated discrete distribution on word indices drawn from a Dirichlet distribution. Each document selects a path down the tree according to a Markov process, which produces a sequence of topics. A stick-breaking process provides a distribution on the selected topics, after which words for a document are generated by ﬁrst drawing a topic, and then drawing the word index. HDPs. The HDP is a multi-level version of the Dirichlet process. It makes use of the idea that the base distribution for the DP can be discrete. A discrete base distribution allows for multiple draws from the DP prior to place probability mass on the same subset of atoms. Hence different groups of data can share the same atoms, but place different probability distributions on them. The HDP draws its discrete base from a DP prior, and so the atoms are learned.  1This is a workshop version of a longer paper. Please see [1] for more details, including a scalable inference algorithm.  1  Figure 1: An example of path structures for the nested Chinese restaurant process (nCRP) and the nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. With the nCRP, the topics for a document are restricted to lying along a single path to a root node. With the nHDP, each document has access to the entire tree, but a document- speciﬁc distribution on paths will place high probability on a particular subtree. The goal of the nHDP is to learn a thematically consistent tree as achieved by the nCRP, while allowing for the cross-thematic borrowings that naturally occur within a document.  Nested HDPs. In building on the nCRP framework, our goal is to allow for each document to have access to the entire tree, while still learning document-speciﬁc distributions on topics that are thematically coherent. Ideally, each document will still exhibit a dominant path corresponding to its main themes, but with offshoots allowing for random effects. Our two major changes to the nCRP formulation toward this end are that (i) each word follows its own path to a topic, and (ii) each document has its own distribution on paths in a shared tree. With the nHDP, all documents share a global nCRP. This nCRP is equivalently an inﬁnite collection of Dirichlet processes with a transition rule between DPs starting from a root node. With the nested HDP, we use each Dirichlet process in the global nCRP as a base for a second-level DP drawn independently for each document. This constitutes an HDP. Using a nesting of HDPs allows each document to have its own tree where the transition probabilities are deﬁned over the same subset of nodes, but where the values of these probabilities are document-speciﬁc. For each node in the tree, we also generate document-speciﬁc switching probabilities that are i.i.d. beta random vari- ables. When walking down the tree for a particular word, we stop with the switching probability of the current node, or continue down the tree according to the probabilities of the child HDP. We summarize this process in Algorithm 1. Sample Results. We present some qualitative results for the nHDP topic model on a set of 1.8 million documents from The New York Times. These results were obtained using a scalable variational inference algorithm [4] after one pass through the data set. In Figure 2 we show example topics from the model and their relative structure. We show four topics from the top level of the tree (shaded), and connect topics according to parent/child relationship. The model learns a meaningful hierarchical structure; for example, the sports subtree branches into the various sports, which themselves appear to branch by teams. In the foreign affairs subtree, children tend to group by major subregion and then branch out into subregion or issue.  Algorithm 1 Generating Documents with the Nested Hierarchical Dirichlet Process  Step 1. Generate a global tree by constructing an nCRP. Step 2. For each document, generate a document tree from the HDP and switching probabilities for each node. Step 3. Generate a document. For word n in document d,  a) Walking down the tree using HDPs. At current node, continue/stop according to its switching probability. b) Sample word n from the discrete distribution at the terminal node.  2  nCRPnHDPFigure 2: Tree-structured topics from The New York Times. A shaded node is a top-level node.  References [1] J. Paisley, C. Wang, D. Blei, and M. I. Jordan, “Nested hierarchical Dirichlet processes,” arXiv:1210.6738, 2012. [2] D. Blei, T. Grifﬁths, and M. Jordan, “The nested Chinese restaurant process and Bayesian nonparametric inference  of topic hierarchies,” Journal of the ACM, vol. 57, no. 2, pp. 7:1–30, 2010.  [3] Y. Teh, M. Jordan, M. Beal, and D. Blei, “Hierarchical Dirichlet processes,” Journal of the American Statistical  Association, vol. 101, no. 476, pp. 1566–1581, 2006.  [4] M. Hoffman, D. Blei, C. Wang, and J. Paisley, “Stochastic variational inference,” arXiv:1206.7051, 2012.  3  gameplayteamgoingthinkseasontournament   seeded   final   second   victory   roundcupteamsocceramericansmatchgoalsouthclubwestspadebridgeopengolfwoodstennistourholehockeyseasondevilsteamgamesfootballseasongamequarterbackbowlbasketballseasongameteamknicksseasonbaseballgameleagueseriesruninninghitgamerunsyankeestorresoxredjetermetsvalentinepiazzafrancosheapointsnetsgamescoredhalfknicksewingrileygamepatrickjordanbullschicagomichaeljacksonphilgiantsjetsseasonparcellsfasselyardstouchdowngamepassquarterdamenotrebowlfootballseasonmiamifloridagoalperiodscoredgamesecondpenguinspittsburghmariocivicmeadowlandsrangersmessierrichtergardenmadisonsquareboardmemberspublicyesterdaydecisionhousesenaterepublicanbillcongresssenatorbudgetagreementplanunionsbillspaycourtjudgejusticesupremecasedecisionlawyerscaselawyercourtjudgechargeshealthcaremedicaldoctorspatientshospitalscommitteepanelethicschairmansubcommitteelegislatureassemblygovernorlegislativesenatesenatejudiciarycommitteenominationrobertswalsholivernorthirancontratrialjurycaseevidencecrimegeneralattorneyspitzerofficeeliotagencyenronlaygateswebsterunionworkerslaborcontractemployeesbenefitsratepercentdiscountpreviousindexpercentmoneygovernmentpaybusinesseconomicagencyadvertisingaccountmediagroupbilliondealcompaniesbusinesscorporationlargestcomputertechnologysystemcompaniessoftwaremarketpercentstockinvestorstradingproductsfoodindustrydrugcompaniescarsworkersautofordplantvehiclesenergyfueloilgaspriceselectricitynaturalcarbonpowerplantssteelairelectricemissionspollutionservicephonenetworktelephonewirelessinternetresearchscientistsinstitutenationallaboratorymicrosoftsoftwarewindowsoperatinggatessunsalesstoresretailersmartwaldrinkcolasoftwatercokebankinterestloansratesdebtfundnetmutualmoneyinvestorsstocksrealestatemarketmortgagebrokerssecuritiesfirmsmorganstanleyaccounting  companiescommunications  telephone  cable  serviceboeingmilitaryaircraftpentagondefensepercenttreasuryyieldpricesfedrates  share  offershareholders  stock  bid  takeoverinsurancesavingsloanestaterealgovernmentofficialsforeignadministrationleaderspoliticalwashingtonnationsisraelpalestinianpeacearafatministerlebanonsyriabeiruthezbollahchristianhostagesarabkingegyptjordanhusseinmilitarynatotroopsforcesdefenseaircraftfighterplaneairjetnuclearmissilearmsdefensetreatyreaganspaceintelligenceafghanistanbinladentalibanqaedaeuropeangermanyfranceunioneastnatomilosevicbosniayugoslaviaserbiatribunalswitzerland  nazi  holocaust  jewish  banks  goldsovietuniongorbachevmoscowcommunistnatoeuropepolandmissilessolidarityrussiayeltsinmoscowkremlinchechnyaclintontreatyarmsnuclearreaganrangesummitbushiraqadministrationwarhusseincouncilnationsiraqsecurityresolutionsaudiarabiabinladenosamanuclearweaponsprograminspectorschemicalirangulfpersianoilshippingchinanorthkoreasouthbeijingnucleartaiwanhongkongboatinsurancereturnvietnamasiaindonesiasingaporecambodia","We develop a nested hierarchical Dirichlet process (nHDP) for hierarchicaltopic modeling. The nHDP is a generalization of the nested Chinese restaurantprocess (nCRP) that allows each word to follow its own path to a topic nodeaccording to a document-specific distribution on a shared tree. This alleviatesthe rigid, single-path formulation of the nCRP, allowing a document to moreeasily express thematic borrowings as a random effect. We demonstrate ouralgorithm on 1.8 million documents from The New York Times."
1301.3592,2013,Deep Learning for Detecting Robotic Grasps  ,"['Ian Lenz', 'Honglak Lee', 'Ashutosh Saxena']",https://arxiv.org/pdf/1301.3592.pdf,"Deep Learning for Detecting Robotic Grasps  Ian Lenz,† Honglak Lee,∗ and Ashutosh Saxena†  † Department of Computer Science, Cornell University.  ∗ Department of EECS, University of Michigan, Ann Arbor.  Email: ianlenz@cs.cornell.edu, honglak@eecs.umich.edu, asaxena@cs.cornell.edu  4 1 0 2     g u A 1 2         ]  G L . s c [      6 v 2 9 5 3  .  1 0 3 1 : v i X r a  Abstract—We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the ﬁrst are re-evaluated by the second. The ﬁrst network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms. 1  Keywords: Robotic Grasping, deep learning, RGB-D multi- modal data, Baxter, PR2, 3D feature learning.  I. INTRODUCTION  Robotic grasping is a challenging problem involving percep- tion, planning, and control. Some recent works [54, 56, 28, 67] address the perception aspect of this problem by converting it into a detection problem in which, given a noisy, partial view of the object from a camera, the goal is to infer the top loca- tions where a robotic gripper could be placed (see Figure 1). Unlike generic vision problems based on static images, such robotic perception problems are often used in closed loop with controllers, so there are stringent requirements on performance and computational speed. In the past, hand-designing features has been the most popular method for several robotic tasks [40, 32]. However, this is cumbersome and time-consuming, especially when we must incorporate new input modalities such as RGB-D cameras.  Recent methods based on deep learning [1] have demon- strated state-of-the-art performance in a wide variety of tasks, including visual recognition [35, 60], audio recognition [39, 41], and natural language processing [12]. These techniques are especially powerful because they are capable of learning useful features directly from both unlabeled and labeled data, avoiding the need for hand-engineering.  1Parts of this work were presented at ICLR 2013 as a workshop paper, and at RSS 2013 as a conference paper. This version includes signiﬁcantly extended related work, algorithmic descriptions, and extensive robotic exper- iments which were not present in previous versions.  However, most work in deep learning has been applied in the context of recognition. Grasping is inherently a detection problem, and previous applications of deep learning to detec- tion have typically focused on speciﬁc vision applications such as face detection [45] and pedestrian detection [57]. Our goal is not only to infer a viable grasp, but to infer the optimal grasp for a given object that maximizes the chance of successfully grasping it, which differs signiﬁcantly from the problem of object detection. Thus, the ﬁrst major contribution of our work is to apply deep learning to the problem of robotic grasping, in a fashion which could generalize to similar detection problems. The second major contribution of our work is to propose a new method for handling multimodal data in the context of feature learning. The use of RGB-D data, as opposed to simple 2D image data, has been shown to signiﬁcantly improve grasp detection results [28, 14, 56]. In this work, we present a multimodal feature learning algorithm which adds a structured regularization penalty to the objective function to be optimized during learning. As opposed to previous works in deep learning, which either ignore modality information at the ﬁrst layer (i.e., encourage all features to use all modalities) [59] or train separate ﬁrst-layer features for each modality [43, 61], our approach allows for a middle-ground in which each feature is encouraged to use only a subset of the input modalities, but is not forced to use only particular ones.  We also propose a two-stage cascaded detection system based on deep learning. Here, we use fewer features for the ﬁrst pass, providing faster, but only approximately accurate detections. The second pass uses more features, giving more accurate detections. In our experiments, we found that the ﬁrst deep network, with fewer features, was better at avoiding overﬁtting but less accurate. We feed the top-ranked rectangles from the ﬁrst layer into the second layer, leading to robust early rejection of false positives. Unlike manually designed two-step features as in [28], our method uses deep learning, which allows us to learn detectors that not only give higher performance, but are also computationally efﬁcient.  We test our approach on a challenging dataset, where we show that our algorithm improves both recognition and detection performance for grasping rectangle data. We also show that our two-stage approach is not only able to match the performance of a single-stage system, but, in fact, improves results while signiﬁcantly reducing the computational time needed for detection.  In summary, the contributions of this paper are: • We present a deep learning algorithm for detecting  Fig. 1: Detecting robotic grasps: Left: A cluttered lab scene labeled with rectangles corresponding to robotic grasps for objects in the scene. Green lines correspond to robotic gripper plates. We use a two-stage system based on deep learning to learn features and perform detection for robotic grasping. Center: Our Baxter robot “Yogi” successfully executing a grasp detected by our algorithm. Right: The grasp detected for this case, in the RGB (top) and depth (bottom) images obtained from Kinect.  robotic grasps. To the best of our knowledge, this is the ﬁrst work to do so.  • In order to handle multimodal inputs, we present a new way to apply structured regularization to the weights to these inputs based on multimodal group regularization. • We present a multi-step cascaded system for detection,  signiﬁcantly reducing its computational cost.  • Our method outperforms the state-of-the-art for rectangle- based grasp detection, as well as previous deep learning algorithms.  • We implement our algorithm on both a Baxter and a PR2 robot, and show success rates of 84% and 89%, respectively, for executing grasps on a highly varied set of objects.  The rest of the paper is organized as follows: We discuss related work in Section II. We present our two-step cascaded detection system in Section III, and some additional details in Section IV. We then describe our feature learning algo- rithm and structured regularization method in Section V. We present our experiments in Section VI, and discuss results in Section VII. We then present experiments on both Baxter and PR2 robots in Section VIII. We present several interesting directions for future work in Section IX, then conclude in Section X.  II. RELATED WORK  A. Robotic Grasping  In this section, we will focus on perception- and learning- based approaches for robotic grasping. For a more complete review of the ﬁeld, we refer the reader to review papers by Bohg et al. [4], Sahbani et al. [53], Bicchi and Kumar [2] and Shimoga [58].  Most works deﬁne a “grasp” as an end-effector conﬁg- uration which achieves partial or complete form- or force- closure of a given object. This is a challenging problem because it depends on the pose and conﬁguration of the robotic gripper as well as the shape and physical properties of the object to be grasped, and typically requires a search over a large number of possible gripper conﬁgurations. Early works  [34, 44, 49] focused on testing for form- and force-closure, and synthesizing grasps fulﬁlling these properties according to some hand-designed “quality score” [17]. More recent works have reﬁned these deﬁnitions [50]. These works assumed full knowledge of object shape and physical properties. Grasping Given 3D Model: Fast synthesis of grasps for known 3D models remains an active research topic [14, 20, 65], with recent methods using advanced physical simulation to ﬁnd optimal grasps. Gallegos et al. [18] performed opti- mization of grasps given both a 3D model of the object to be grasped and the desired contact points for the robotic gripper. Pokorny et al. [48] deﬁne spaces of graspable objects, then map new objects to these spaces to discover grasps. However, these works are only applicable when the full 3D model of the object is exactly known, which may not be the case when a robot is interacting with a new environment. We note that some of these physics-based approaches might be combined with our approach in a multi-pass system, discussed further in Sec. IX. Sensing for Grasping: In a real-world robotic setting, a robot will not have full knowledge of the 3D model and pose of an object to be grasped, but rather only incomplete information from some set of sensors such as color or depth cameras, tactile sensors, etc. This makes the problem of grasping signiﬁcantly more challenging [4], as the algorithm must use more limited and potentially noisier information to detect a good grasp. While some works [10, 46] simply attempt to estimate the poses of known objects and then apply full-model grasping algorithms based on these results, others avoid this assumption, functioning on novel objects which the algorithm has not seen before.  Such works often made use of other simplifying assump- tions, such as assuming that objects belong to one of a set of primitive shapes [47, 6], or are planar [42]. Other works produced impressive results for speciﬁc cases, such as grasping the corners of towels [40]. While such works escape the assumption of a fully-known object model, hand-coded grasping rules have a hard time dealing with the wide range of objects seen in real-world human environments, and are  difﬁcult and time-consuming to create. Learning for Grasping: Machine learning methods have proven effective for a wide range of perception problems [64, 22, 38, 59, 3], allowing a perception system to learn a mapping from some feature set to various visual proper- ties. Early work by Kamon et al. [31] showed that learning approaches could also be applied to the problem of grasping from vision, introducing a learning component to grasp quality scores.  Recent works have employed richer features and learning methods, allowing robots to grasp known objects which might be partially occluded [27] or in an unknown pose [13] as well as fully novel objects which the system has not seen before [54]. Here, we will address the latter case. Earlier work focused on detecting only a single grasping point from 2D partial-view data, using heuristic methods to determine a gripper pose based on this point. [55]. The use of 3D data was shown to signiﬁcantly improve these results [56] thanks to giving direct physical information about the object in question. With the advent of low-cost RGB-D sensors such as the Kinect, the use of depth data for robotic grasping has become ubiquitous.  Several other works attempted to use the learning algorithm to more fully constrain the detected grasps. Ekvall and Kragic [15] and Huebner and Kragic [23] used shape-based approxi- mations as bases for learning algorithms which directly gave an approach vector. Le et al. [36] treated grasp detection as a ranking problem over sets of contact points in image space. Jiang et al. [28] represented a grasp as a 2D oriented rectangle in image space, with two edges corresponding to the gripper plates, using surface normals to determine the grasp approach vector. These approaches allow the detection algorithm to detect more exactly the gripper pose which should be used for grasping. In this work, we will follow the rectangle-based method.  Learning-based approaches have shown impressive results in grasping novel objects, showing that learning some param- eters of the detection system can outperform human tuning. However, these approaches still require a signiﬁcant degree of hand-engineering in the form of designing good input features. Other Applications with RGBD Data. Due to the avail- ability of inexpensive depth sensors, RGB-D data has been a signiﬁcant research focus in recent years for various robotics applications. For example, Jiang et al. [30] consider robotic placement of objects, while Teuliere and Marchand [63] used RGB-D data for visual servoing. Several works, including those of Endres et al. [16] and Whelan et al. [66] have ex- tended and improved Simultaneous Localization and Mapping (SLAM) for RGB-D data. Object detection and recognition has been a major focus in research on RGB-D data [11, 33, 7]. Most such works use hand-engineered features such as [52]. The few works that perform feature learning for RGB-D data [59, 3] largely ignore the multimodal nature of the data, not distinguishing the color and depth channels. Here, we present a structured regularization approach which allows us to learn  more robust features for RGB-D and other multimodal data.  B. Deep Learning  Deep learning approaches have demonstrated the ability to learn useful features directly from data for a wide variety of tasks. Early work by Hinton and Salakhutdinov [22] showed that a deep network trained on images of hand-written digits will learn features corresponding to pen-strokes. Later work using localized convolutional features [38] showed that these networks learn features corresponding to object parts when trained on natural images. This demonstrates that even the basic features learned by these systems will adapt to the data given. In fact, these approaches are not restricted to the visual domain, but rather have been shown to learn useful features for a wide range of domains, such as audio [39, 41] and natural language data [12]. Deep Learning for Detection: However, the vast majority of work in deep learning focuses on classiﬁcation problems. Only a handful of previous works have applied these methods to detection problems [45, 37, 9]. For example, Osadchy et al. [45] and LeCun et al. [37] applied a deep energy-based model to the problem of face detection, Sermanet et al. [57] applied a convolutional neural network for pedestrian detection, and Coates et al. [9] used a deep learning approach to detect text in images. Girshick et al. [19] used learned convolutional features over image regions for object detection, while Szegedy et al. [62] used a multi-scale approach based on deep networks for the same task.  in which the goal  All these approaches focused on object detection and similar problems, is to ﬁnd a bounding box which tightly contains the item to be detected, and for each item, all valid bounding boxes will be similar. However, in robotic grasp detection, there may be several valid grasps for an object in different regions, making it more important to select the one with the highest chance success. In addition, orientation matters much more to robotic grasp detection, as most grasps will only be viable for a small subset of the possible gripper orientations. Our approach to grasp detection will also generalize across object classes, and even to classes never seen before by the system, as opposed to the class- speciﬁc nature of object detection. Multimodal Deep Learning: Recent works in deep learning have extended these methods to handle multiple modalities of input data, such as audio and video [43], text and image data [61], and even RGB-D data [59, 3]. However, all of these approaches have fallen into two camps - either learning com- pletely separate low-level features for each modality [43, 61], or simply concatenating the modalities [59, 3]. The former approaches have proven effective for data where the basic modalities differ signiﬁcantly, such as the aforementioned case of text and images, while the latter is more effective in cases where the modalities are more similar, such as RGB-D data. For some new combinations of modalities and tasks, it may not be clear which of these approaches will give better performance. In fact, in the ideal feature set, different features  Fig. 2: Detecting and executing grasps: From left to right: Our system obtains an RGB-D image from a Kinect mounted on the robot, and searches over a large space of possible grasps, for which some candidates are shown. For each of these, it extracts a set of raw features corresponding to the color and depth images and surface normals, then uses these as inputs to a deep network which scores each rectangle. Finally, the top-ranked rectangle is selected and the corresponding grasp is executed using the parameters of the detected rectangle and the surface normal at its center. Red and green lines correspond to gripper plates, blue in RGB-D features indicates masked-out pixels.  may use different subsets of the modalities. In this work, we will give a structured regularization method which guides the learning algorithm to select such subsets, without imposing hard constraints on network structure. Structured Learning and Structured Regularization: Sev- eral approaches have been proposed which attempt to use a specially-designed regularization function to impose structure on a set of learned parameters without directly enforcing it. Jalali et al. [26] used a group regularization function in the multitask learning setting, where one set of features is used for multiple tasks. This function applies high-order regularization separately to particular groups of parameters. Their function regularized the number of features used for each task in a set of multi-class classiﬁcation tasks solved by softmax regression. Intuitively, this encodes the belief that only some subset of the input features will be useful for each task, but this set of useful features might vary between tasks.  A few works have also explored the use of structured regularization in deep learning. The Topographic ICA algo- rithm [24] is a feature-learning approach that applies a similar penalty term to feature activations, but not to the weights themselves. Coates and Ng [8] investigate the problem of selecting receptive ﬁelds, i.e., subsets of the input features to be used together in a higher-level feature. The structure of the network is learned ﬁrst, then ﬁxed before learning the parameters of the network.  III. DEEP LEARNING FOR GRASP DETECTION:  SYSTEM AND MODEL  In this work, we will present an algorithm for robotic grasp detection from a single RGB-D view. Our approach will be based on machine learning, but distinguish itself from previous approaches by learning not only the weights used to rank prospective grasps, but also the features used to rank them, which were previously hand-engineered.  We will do this using deep learning methods, learning a set of RGB-D features which will be extracted from each  candidate grasp, then used to score that grasp. Our approach will include a structured multimodal regularization method which improves the quality of the features learned from RGB-D data without constraining network structure.  In our system for robotic grasping, as shown in Fig. 2, the robot ﬁrst obtains an RGB-D image of the scene containing objects to be grasped. A small deep network is used to score potential grasps in this image, and a small candidate set of the top-ranked grasps is provided to a larger deep network, which yields a single best-ranked grasp.  In this work, we will represent potential grasps using oriented rectangles in the image plane as seen on the left in Fig. 2, with one pair of parallel edges corresponding to the robotic gripper [28]. Each rectangle is thus parameterized by the X and Y coordinates of its upper-left corner, its width, height, and orientation in the image plane, giving a ﬁve- dimensional search space for potential grasps. Grasps will be ranked based on features extracted from the RGB-D image region contained inside their corresponding rectangle, aligned to the gripper plates, as seen in the center of Fig. 2.  To translate a rectangle such as that shown on the right in Fig. 2 into a gripper pose for grasping we ﬁnd the point with the minimum depth inside the central third (horizontally) of the rectangle. We then use the averaged surface normal around this point to determine the approach vector for the gripper. The orientation of the detected rectangle is translated to a rotation around this vector to orient the gripper. We use the X-Y coordinates of the rectangle center along with the depth of the closest point to determine a grasping point in the robot’s coordinate frame. We compute a pre-grasp position by shifting 10 cm back from the grasping point along this approach vector and position the gripper at this point. We then approach the object along the approach vector and grasp it.  Using a standard feature learning approach such as sparse auto-encoder [21], a deep network can be trained for the problem of grasping rectangle recognition (i.e., does a given rectangle in image space correspond to a valid robotic grasp?).  Fig. 3: Illustration of our two-stage detection process: Given an image of an object to grasp, a small deep network is used to exhaustively search potential rectangles, producing a small set of top-ranked rectangles. A larger deep network is then used to ﬁnd the top-ranked rectangle from these candidates, producing a single optimal grasp for the given object.  detections for the second stage. Using deep learning allows us to circumvent the costly manual design of features by simply training networks of two different sizes, using the smaller for the exhaustive ﬁrst pass, and the larger to re-rank the candidate detection results. Model: To detect robotic grasps from the rectangle repre- sentation, we model the probability of a rectangle G(t), with features x(t) ∈ RN being graspable, using a random variable ˆy(t) ∈ {0, 1} which indicates whether or not we predict G(t) to be graspable. We use a deep network, as shown in Fig. 4-left, with two layers of sigmoidal hidden units h[1] and h[2], with K1 and K2 units per layer, respectively. A logistic classiﬁer over the outputs of the second-layer hidden units then predicts P (ˆy(t)|x(t); Θ), so chosen because ground-truth graspability is represented as binary. Each layer (cid:96) will have a set of weights W [(cid:96)] mapping from its inputs to its hidden units, so the param- eters of our model are Θ = {W [1], W [2], W [3]}. Each hidden unit forms output by a sigmoid σ(a) = 1/(1 + exp(−a)) over its weighted input:  (cid:32) N(cid:88) (cid:32) K1(cid:88) (cid:32) K2(cid:88)  i=1  i=1  (cid:33)  (cid:33) (cid:33)  x(t) i W [1]  i,j  h[1](t) i W [2]  i,j  h[1](t) j  = σ  h[2](t) j  = σ  P (ˆy(t) = 1|x(t); Θ) = σ  h[2](t) i W [3]  i  (1)  i=1  A. Inference and Learning  During inference, our goal is to ﬁnd the single grasping rectangle with the maximum probability of being graspable for some new object. With G representing a particular grasping rectangle position, orientation, and size, we ﬁnd this best rectangle as:  G∗ = arg max  P (ˆy(t) = 1|φ(G); Θ)  (2)  G  Here, the function φ extracts the appropriate input representa- tion for rectangle G.  During learning, our goal is to learn the parameters Θ that optimize the recognition accuracy of our system. Here, input data is given as a set of pairs of features x(t) ∈ RN and  Fig. 4: Deep network and auto-encoder: Left: A deep network with two hidden layers, which transform the input representation, and a logistic classiﬁer at the top layer, which uses the features from the second hidden layer to predict the probability of a grasp being feasible. Right: An auto-encoder, used for pretraining. A set of weights projects input features to a hidden layer. The same weights are then used to project these hidden unit outputs to a reconstruction of the inputs. In the sparse auto-encoder (SAE) algorithm, the hidden unit activations are also penalized.  However, in a real-world robotic setting, our system needs to perform detection (i.e., given an image containing an object, how should the robot grasp it?). This task is signiﬁcantly more challenging than simple recognition. Two-stage Cascaded Detection: In order to perform detec- tion, one naive approach could be to consider each possible oriented rectangle in the image (perhaps discretized to some level), and evaluate each rectangle with a deep network trained for recognition. However, such near-exhaustive search of possible rectangles (based on positions, sizes, and orienta- tions) can be quite expensive in practice for real-time robotic grasping.  Motivated by multi-step cascaded approaches in previous work [28, 64], we instead take a two-stage approach to detection: First, we use a reduced feature set to determine a set of top candidates. Then, we use a larger, more robust feature set to rank these candidates.  However, these approaches require the design of two sepa- rate sets of features. In particular, it can be difﬁcult to manually design a small set of ﬁrst-stage features which is both quick to compute and robust enough to produce a good set of candidate  ground-truth labels y(t) ∈ {0, 1} for t = 1, . . . , M. As in most deep learning works, we use a two-phase learning approach. In the ﬁrst phase, we will use unsupervised feature learning to initialize the hidden-layer weights W [1] and W [2]. Pre- training weights this way is critical to avoid overﬁtting. We will use a variant of a sparse auto-encoder (SAE) [21], as illustrated in Fig. 4-right. We deﬁne g(h) as a sparsity penalty function over hidden unit activations, with λ controlling its weight. With f (W ) as a regularization function, weighted by β, and ˆx(t) as the reconstruction of x(t), SAE solves the following to initialize hidden-layer weights:  M(cid:88)  K(cid:88)  W ∗ = arg min  (||ˆx(t) − x(t)||2  2 + λ  t=1  j=1  x(t) i Wi,j)  h(t) j = σ(  W  N(cid:88) K(cid:88)  i=1  j=1  ˆx(t) i =  h(t) j Wi,j  Fig. 5: Preserving aspect ratio: Left: a pair of sunglasses with a potential grasping rectangle. Red edges indicate gripper plates. Center: image taken from the rectangle and rescaled to ﬁt a square aspect ratio. Right: same image, padded and centered in the receptive ﬁeld. Blue areas indicate masked-out padding. When rescaled, the rectangle incorrectly appears graspable. Preserving aspect ratio and padding allows the rectangle to correctly appear non-graspable.  g(h(t)  j )) + βf (W )  (3)  Fig. 6: Improvement from mask-based scaling: Left: Result with- out mask-based scaling. Right: Result with mask-based scaling.  M(cid:88)  t=1  We ﬁrst use this algorithm to initialize W [1] to reconstruct x. We then ﬁx W [1] and learn W [2] to reconstruct h[1].  During the supervised phase of the learning algorithm, we then jointly learn classiﬁer weights W [3] and ﬁne-tune hidden layer weights W [1] and W [2] for recognition. We maximize the log-likelihood of the data along with regularization penalties on hidden layer weights:  Θ∗ = arg max  Θ  log P (ˆy(t) = y(t)|x(t); Θ) − β1f (W [1]) − β2f (W [2])  (4)  Two-stage Detection Model: During inference for two-stage detection, we will ﬁrst use a smaller network to produce a set of the top T rectangles with the highest probability of being graspable according to network parameters Θ1. We will then use a larger network with a separate set of parameters Θ2 to re-rank these T rectangles and obtain a single best one. The only change to learning for the two-stage model is that these two sets of parameters are learned separately, using the same approach.  IV. SYSTEM DETAILS  In this section, we will deﬁne the set of raw features which our system will use, forming x in the equations above, and how they are extracted from an RGB-D image. Some examples of these features are shown in Fig 2.  Our algorithm uses only local information - speciﬁcally, we extract the RGB-D sub-image contained within each rectangle, and use this to generate features for that rectangle. This image is rotated so that its left and right edges correspond to the gripper plates, and then re-scaled to ﬁt inside the network’s receptive ﬁeld.  From this 24x24 pixel image, seven channels’ worth of features are extracted, giving 24x24x7 = 4032 input features.  The ﬁrst three channels are the image in YUV color space, used because it represents image intensity and color separately. The next is simply the depth channel of the image. The last three are the X, Y, and Z components of surface normals computed based on the depth channel. These are computed after the image is aligned to the gripper so that they are always relative to the gripper plates.  A. Data Pre-Processing  Whitening data is critical for deep learning approaches to work well, especially in cases such as multimodal data where the statistics of the input data may vary greatly. While PCA- based approaches have been shown to be effective [25], they are difﬁcult to apply in cases such as ours where large portions of the data may be masked out.  Depth data, in particular, can be difﬁcult to whiten because the range of values may be very different for different patches in the image. Thus, we ﬁrst whiten each depth patch indi- vidually, subtracting the patch-wise mean and dividing by the patch-wise standard deviation, down to some minimum.  For multimodal data, the statistics of the data for each modality should match as closely as possible, to avoid learning features which are biased towards or away from using partic- ular modes. This is particularly important when regularizing each modality separately, as in our approach. Thus, we drop mean values for each feature separately, but scale the data for each channel by dividing by the standard deviation of all its features combined.  B. Preserving Aspect Ratio.  It is important for to preserve aspect ratio when feeding features into the network. This is because distorting image fea- tures may cause non-graspable rectangles to appear graspable, as shown in Fig. 5. However, padding with zeros can cause rectangles with less padding to receive higher graspability  Fig. 7: Three possible models for multimodal deep learning: Left: fully dense model—all visible features are concatenated and modality information is ignored. Middle: modality-speciﬁc sparse model - separate ﬁrst layer features are trained for each modality. Right: group-sparse model—a structured regularization term encourages features to use only a subset of the input modes.  scores, as the network will have more nonzero inputs. It is important to account for this because in many cases the ideal grasp for an object might be represented by a thin rectangle which would thus contain many zero values in its receptive ﬁeld from padding.  To address this problem, we scale up the magnitude of the available input for each rectangle based on the fraction of the rectangle which is masked out. In particular, we deﬁne a multiplicative scaling factor for the inputs from each modality, based on the fraction of each mode which is masked out, since each mode may have a different mask.  (cid:17)  i=1 Sr,i/  is 1 if x(t) i  (cid:16)(cid:80)N  r = (cid:80)N i =(cid:80)R  In the multimodal setting, we assume that the input data x is known to come from R distinct modalities, for example audio and video data, or depth and RGB data. We deﬁne the modality matrix S as an RxN binary matrix, where each element Sr,i indicates membership of visible unit xi in a particular modality r, such as depth or image intensity. The scaling factor for mode r is then deﬁned as: Ψ(t) , where µ(t) is masked in, 0 otherwise. The scaling i factor for case i is: ψ(t) We could simply scale up each value of x by its correspond- ing scale factor when training our model, as x(cid:48)(t) i x(t) . However, since our sparse autoencoder penalizes squared error, scaling x linearly will scale the error for the corresponding cases quadratically, causing the learning algorithm to lend increased signiﬁcance to cases where more data is masked out. Instead, we can use the scaled x(cid:48) as input to the network, but penalize reconstruction based on the original x, only scaling after the squared error has been computed:  r=1 Sr,iΨ(t) r .  i=1 Sr,iµ(t)  i = ψ(t)  i  i  W ∗ = arg min  W  ψ(t) i (ˆx(t)  i − x(t)  i )2 + λ  g(h(t) j )  t=1  i=1  j=1  (5) We redeﬁne the hidden units to use the scaled visible input:  (cid:32) N(cid:88)  (cid:33)  h(t) j = σ  x(cid:48)(t)  i Wi,j  This approach is equivalent to adding additional, potentially fractional, ‘virtual’ visible units to the model based on the  i=1  M(cid:88)   N(cid:88)    (6)  K(cid:88)  r , c).  scaling factor for each mode. In practice, we found it necessary to limit the scaling factor to a maximum of some value c, as Ψ(cid:48)(t)  r = min(Ψ(t) As shown in Table III our mask-based scaling technique at the visible layer improves grasping results by over 25% for both metrics. As seen in Figure 6, it removes the network’s inherent bias towards square rectangles, exhibiting a much wider range of aspect ratios that more closely matches that of the ground-truth data.  V. STRUCTURED REGULARIZATION FOR  FEATURE LEARNING  A naive way of applying feature learning to multimodal data is to simply take x (as a concatenated vector) as input to the model described above, ignoring information about speciﬁc modalities, as seen on the lefthand side of Figure 7. This approach may either 1) prematurely learn features which include all modalities, which can lead to overﬁtting, or 2) fail to learn associations between modalities with very different underlying statistics.  Instead of concatenating multimodal  input as a vector, Ngiam et al. [43] proposed training a ﬁrst layer representation for each modality separately, as shown in Figure 7-middle. This approach makes the assumption that the ideal low-level features for each modality are purely unimodal, while higher- layer features are purely multimodal. This approach may work better for some problems where the modalities have very different basic representations, such as the video and audio data (as used in [43]), so that separate ﬁrst layer features may give better performance. However, for modalities such as RGB-D data, where the input modes represent different channels of an image, learning low-level correlations can lead to more robust features – our experiments in Section VI show that simply concatenating the input modalities signiﬁcantly outperforms training separate ﬁrst-layer features for robotic grasp detection from RGB-D data.  For many problems, it may be difﬁcult to tell which of these approaches will perform better, and time-consuming to tune and comparatively evaluate multiple algorithms. In addition, the ideal feature set for some problems may contain features  (a) Features corresponding to positive grasps.  (b) Features corresponding to negative grasps.  Fig. 8: Features learned from grasping data: Each feature contains seven channels - from left to right, depth, Y, U, and V image channels, and X, Y, and Z surface normal components. Vertical edges correspond to gripper plates. Left: eight features with the strong positive correlations to rectangle graspability. Right: similar, but negative correlations. Group regularization eliminates many modalities from many of these features, making them more robust.  which use some, but not all, of the input modalities, a case which neither of these approaches are designed to handle.  To solve these problems, we propose a new algorithm for feature learning for multimodal data. Our approach incorpo- rates a structured penalty term into the optimization problem to be solved during learning. This technique allows the model to learn correlated features between multiple input modalities, but regularizes the number of modalities used per feature (hidden unit), discouraging the model from learning weak correlations between modalities. With this regularization term, the algorithm can specify how mode-sparse or mode-dense the features should be, representing a continuum between the two extremes outlined above. Regularization in Deep Learning: In a typical deep learn- ing model, L2 regularization (i.e., f (W ) = ||W||2 2) or L1 regularization (i.e., f (W ) = ||W||1) are commonly used in training (e.g., as speciﬁed in Equations (3) and (4)). These are often called a “weight cost” (or “weight decay”), and are left implicit in many works.  Applying regularization is well known to improve the gen- eralization performance of feature learning algorithms. One might expect that a simple L1 penalty would eliminate weak correlations in multimodal features, leading to features which use only a subset of the modes each. However, we found that in practice, a value of β large enough to cause this also degraded the quality of features for the remaining modes and lead to decreased task performance. Multimodal Regularization: Structured regularization, such as in [26], takes a set of groups of weights, and applies some regularization function (typically high-order) separately to each group. In our structured multimodal regularization algorithm, each modality will be used as a regularization group separately for each hidden unit. For example, a group-wise p- norm would be applied as:  K(cid:88)  R(cid:88)  (cid:32) N(cid:88)  (cid:33)1/p i,j| Sr,i|W p  f (W ) =  lower-valued ones. This also means that forming a high-valued weight in a group with other high-valued weights will accrue a lower additional penalty than doing so for a group with only low-valued weights. At the limit (p → ∞), this group regularization becomes equivalent to the inﬁnity (or max) norm:  f (W ) =  Sr,i|Wi,j|  max  i  (8)  K(cid:88)  R(cid:88)  j=1  r=1  which penalizes only the maximum weight from each mode to each feature. In practice, the inﬁnity norm is not differentiable and therefore is difﬁcult to apply gradient-based optimization methods; in this paper, we use the log-sum-exponential as a differentiable approximation to the max norm.  In experiments, this regularization function produces ﬁrst- layer weights concentrated in fewer modes per feature. How- ever, we found that at values of β sufﬁcient to induce the desired mode-wise sparsity patterns, penalizing the maximum also had the undesirable side-effect of causing many of the weights for other modes to saturate at their mode’s maximum, suggesting that the features were overly constrained. In some cases, constraining the weights in this manner also caused the algorithm to learn duplicate (or redundant) features, in effect scaling up the feature’s contribution to reconstruction to compensate for its constrained maximum. This is obviously an undesirable effect, as it reduces the effective size (or diversity) of the learned feature set.  This suggests that  the max-norm may be overly con- straining. A more desirable sparsity function would penalize nonzero weight maxima for each mode for each feature without additional penalty for larger values of these maxima. We can achieve this effect by applying the L0 norm, which takes a value of 0 for an input of 0, and 1 otherwise, on top of the max-norm from above:  (7)  f (W ) =  I{(max  i  Sr,i|Wi,j|) > 0}  (9)  K(cid:88)  R(cid:88)  j=1  r=1  j=1  r=1  i=1  where Sr,i is 1 if feature i belongs to group r and 0 otherwise. Using a high value of p allows us to penalize higher-valued weights from each mode to each feature more strongly than  where I is the indicator function, which takes a value of 1 if its argument is true, 0 otherwise. Again, for a gradient- based method, we used an approximation to the L0 norm, such as log(1 + x2). This regularization function now encodes  TABLE I: Recognition results for Cornell grasping dataset.  Algorithm Chance Jiang et al. [28] Jiang et al. [28] + FPFH Sparse AE, separate layer-1 feat. Sparse AE Sparse AE, group reg.  Accuracy (%) 50 84.7 89.6 92.8 93.7 93.7  exhaustive search using this network, then used the 200-unit network to re-rank the 100 highest-ranked rectangles found by the 50-unit network.  B. Baselines  We compare our recognition results in the Cornell grasping dataset with the features from [28], as well as the combination of these features and Fast Point Feature Histogram (FPFH) features [51]. We used a linear SVM for classiﬁcation, which gave the best results among all other kernels. We also report chance performance, obtained by randomly selecting a label in the recognition case, and randomly assigning scores to rectangles in the detection case.  We also compare our algorithm to other deep learning approaches. We compare to a network trained only with standard L1 regularization, and a network trained in a manner similar to [43], where three separate sets of ﬁrst layer features are learned for the depth channel, the combination of the Y, U, and V channels, and the combination of the X, Y, and Z surface normal components.  C. Metrics for Detection  For detection, we compare the top-ranked rectangle for each method with the set of ground-truth rectangles for each image. We present results using two metrics, the “point” and “rectangle” metric.  For the point metric, similar to Saxena et al. [55], we com- pute the center point of the predicted rectangle, and consider the grasp a success if it is within some distance from at least one ground-truth rectangle center. We note that this metric ignores grasp orientation, and therefore might overestimate the performance of an algorithm for robotic applications.  For the rectangle metric, similar to Jiang et al. [28], let G be the top-ranked grasping rectangle predicted by the algorithm, and G∗ be a ground-truth rectangle. Any rectangles with an orientation error of more than 30o from G are rejected. From the remaining set, we use the common bounding box evaluation metric of intersection divided by union - i.e. Area(G∩ G∗)/Area(G∪ G∗). Since a ground-truth rectangle can deﬁne a large space of graspable rectangles (e.g., covering the entire length of a pen), we consider a prediction to be correct if it scores at least 25% by this metric.  VII. RESULTS AND DISCUSSION A. Deep Learning for Robotic Grasp Detection  Figure 8 shows the features learned by the unsupervised phase of our algorithm which have a high correlation to  Fig. 9: Example objects from the Cornell grasping dataset: [28]. This dataset contains objects from a large variety of categories.  a direct penalty on the number of modes used for each weight, without further constraining the weights of modes with nonzero maxima.  Figure 8 shows features learned from the unsupervised stage of our group-regularized deep learning algorithm. We discuss these features, and their implications for robotic grasping, in Section VII.  VI. EXPERIMENTS  A. Dataset  We used the extended version of the Cornell grasping dataset for our experiments. This dataset, along with code for this paper, is available at http://pr.cs.cornell.edu/ deepgrasping. We note that this is an updated version of the dataset used in [28], containing several more complex objects, and thus results for their algorithms will be different from those in [28]. This dataset contains 1035 images of 280 graspable objects, several of which are shown in Fig. 9. Each image is annotated with several ground-truth positive and negative grasping rectangles. While the vast majority of possible rectangles for most objects will be non-graspable, the dataset contains roughly equal numbers of graspable and non- graspable rectangles. We will show that this is useful for an unsupervised learning algorithm, as it allows learning a good representation for graspable rectangles even from unlabeled data.  We performed ﬁve-fold cross-validation, and present results for splits on per image (i.e., the training set and the validation set do not share the same image) and per object (i.e., the training set and the validation set do not share any images from the same object) basis. Hyper-parameters were selected by validating performance on a separate set of 300 grasps not used in any of the cross-validation splits.  We take seven 24x24 pixel channels as described in Sec- tion IV as input, giving 4032 input features to each network. We trained a deep network with 200 hidden units each at the ﬁrst and second layers using our learning algorithm as described in Sections III and V. Training this network took roughly 30 minutes. For trials involving our two-pass system, we trained a second network with 50 hidden units at each layer in the same manner. During inference we performed an  e v i t i s o P  e v i t a g e N  Fig. 10: Learned 3D depth features: 3D meshes for depth channels of the four features with strongest positive (top) and negative(bottom) correlations to rectangle graspability. Here X and Y coordinates corresponds to positions in the deep network’s receptive ﬁeld, and Z coordinates corresponds to weight values to the depth channel for each location. Feature shapes clearly correspond to graspable and non- graspable structures, respectively.  positive and negative grasping cases. Many of these features show non-zero weights to the depth channel, indicating that it learns the correlation of depths to graspability. We can see that weights to many of the modalities for these features have been eliminated by our structured regularization approach. In particular, many of these features lack weights to the U and V (3rd and 4th) channels, which correspond to color, allowing the system to be more robust to different-colored objects.  Figure 10 shows 3D meshes for the depth channels of the four features with the strongest positive and negative correlations to valid grasps. Even without any supervised infor- mation, our algorithm was able to learn several features which correlate strongly to graspable cases and non-graspable cases. The ﬁrst two positive-correlated features represent handles, or other cases with a raised region in the center, while the second two represent circular rims or handles. The negatively- correlated features represent obviously non-graspable cases, such as ridges perpendicular to the gripper plane and “valleys” between the gripper plates. From these features, we can see that even during unsupervised feature learning, our approach is able to learn a representation useful for the task at hand, thanks purely to the fact that the data used is composed of half graspable and half non-graspable cases.  From Table I, we see that the recognition performance is signiﬁcantly improved with deep learning methods, improving 9% over the features from [28] and 4.1% over those features combined with FPFH features. Both L1 and group regulariza- tion performed similarly for recognition, but training separate ﬁrst layer features decreased performance slightly. This shows that learned features, in addition to avoiding hand-design, are able to improve performance signiﬁcantly over the state of the art. It demonstrates that a deep network is able to learn the concept of “graspability” in a way that generalizes to new objects it hasn’t seen before.  Table II shows that even using any one of the three input modalities (RGB, depth, or surface normals), our algorithm is able to learn features which outperform hand-engineered ones for recognition. Depth gives the highest performance of any single-mode network. Combining depth and normal  TABLE II: Recognition results for different modalities, for a deep network pre-trained using SAE.  Modes Chance RGB Depth Surf. Normals Depth + Surf. Normals RGB + Depth + Surf. Normals  Accuracy (%) 50 90.3 92.4 90.3 92.8 93.7  information improves results over either alone, indicating that they give non-redundant information.  The highest accuracy is still obtained by using all  the input modalities. This shows that combining depth and color information leads to a system which is more robust than either modality alone. This is due to the fact that some graspable cases (rims of monochromatic objects, etc.) can only be detected using depth information, while in others, the depth channel may be extremely noisy, requiring the use of color information. From this, we can see that integrating multimodal information, a major focus of this work, in recognizing good robotic grasps.  is important  Table III shows that the performance gains from deep learn- ing for recognition carry over to detection, as well. Once mask- based scaling has been applied, all deep learning approaches except for training separate ﬁrst-layer features outperform the hand-engineered features from [28] by up to 13% for the point metric and 17% for the rectangle metric, while also avoiding the need to design task-speciﬁc features. Without mask-based scaling, the system performs poorly, due to the bias illustrated in Fig. 6. Separate ﬁrst-layer features also give weak detection performance, indicating that the relative scores assigned by this form of network are less robust than those learned using our structured regularization approach.  Using structured multimodal regularization also improves results over standard L1regularization by up to 1.8%, showing that our method also learns more robust features than standard approaches which ignore modality information. Even though using the ﬁrst-pass network alone underperforms the second-  TABLE III: Detection results for point and rectangle metrics, for various learning algorithms.  Algorithm Chance Jiang et al. [28] SAE, no mask-based scaling SAE, separate layer-1 feat. SAE, L1 reg. SAE, struct. reg., 1st pass only SAE, struct. reg., 2nd pass only SAE, struct. reg. two-stage  Image-wise split Point 35.9 75.3 62.1 70.3 87.2 86.4 87.5 88.4  Rect 6.7 60.5 39.9 43.3 72.9 70.6 73.8 73.9  Object-wise split Point 35.9 74.9 56.2 70.7 88.7 85.2 87.6 88.1  Rect 6.7 58.3 35.4 40.0 71.4 64.9 73.2 75.6  r e p p i r g  e d i W  r e p p i r g  n i h T  0 degrees  45 degrees  90 degrees  Fig. 11: Visualization of grasping scores for different grippers: Red indicates maximum score for a grasp with left gripper plane centered at each point, blue is similar for the right plate. Best-scoring rectangle shown in green/yellow.  pass network alone by up to 8.3%, integrating both in our two- pass system outperforms the solo second-pass network by up to 2.4%. This shows that the two-pass system improves not only efﬁciency, but accuracy as well. The performance gains from multimodal regularization and the two-pass system are discussed in detail below.  Our system outperforms all baseline approaches by all metrics except for the point metric in the object-wise split case. However, we can see that the chance performance is much higher for the point metric than for the rectangle metric. This shows that the point metric can overstate performance, and the rectangle metric is a better indicator of the accuracy of a grasp detection system. Adaptability: One important advantage of our detection system is that we can ﬂexibly specify the constraints of the gripper in our detection system. This is particularly important for a robot like Baxter, where different objects might require different gripper settings to grasp. We can constrain the detectors to handle this. Figure 11 shows detection scores for systems constrained based on two different settings of Baxter’s gripper, one wide and one thin. The implications of these results for other types of grippers will be discussed in Section IX.  B. Multimodal Group Regularization  Our group regularization term improves detection accuracy over simple L1 regularization. The improvement is more signiﬁcant for the object-wise split than for the image-wise split because the group regularization helps the network to avoid overﬁtting, which will tend to occur more when the learning algorithm is evaluated on unseen objects.  Fig. 12: Improvements from group regularization: Cases where our group regularization approach produces a viable grasp (shown in green and yellow), while a network trained only with simple L1 regularization does not (shown in blue and red). Top: RGB image, bottom: depth channel. Green and blue edges correspond to gripper.  Fig. 13: Improvements from two-stage system: Example cases where the two-stage system produces a viable grasp (shown in green and yellow), while the single-stage system does not (shown in blue and red). Top: RGB image, bottom: depth channel. Green and blue edges correspond to gripper.  Figure 12 shows typical cases where a network trained using our group regularization ﬁnds a valid grasp, but a network trained with L1 regularization does not. In these cases, the grasp chosen by the L1-regularized network appears valid for some modalities – the depth channel for the sunglasses and nail polish bottle, and the RGB channels for the scissors. However, when all modalities are considered, the grasp is clearly invalid. The group-regularized network does a better job of combining information from all modalities and is more robust to noise and missing data in the depth channel, as seen in these cases.  C. Two-stage Detection System  Using our two-pass system enhanced both computational performance and accuracy. The number of rectangles the full- size network needed to evaluate was reduced by roughly a factor of 1000. Meanwhile, detection performance increased by up to 2.4% as compared to a single pass with the large- size network, even though using the small network alone signiﬁcantly underperforms the larger network. In most cases, the top 100 rectangles from the ﬁrst pass contained the top- ranked rectangle from an exhaustive search using the second- stage network, and thus results were unaffected.  Figure 13 shows some cases where the ﬁrst-stage network pruned away rectangles corresponding to weak grasps which might otherwise be chosen by the second-stage network. In  PR2: Our second platform was our PR2 robot, “Kodiak.” Similar to Baxter, PR2 has two 7-DoF arms with approx- imately 1 m reach, and we used only the left for these experiments. PR2’s grippers open to a width of 8 cm, and are capable of closing completely from that span, so we did not need to use two settings as with Baxter. We augmented PR2’s gripper friction with gaffer tape on the ﬁngertips.  For the experiments on PR2, we used the Kinect already mounted to Kodiak’s head, and used ROS’s built-in function- ality to obtain 3D locations from that Kinect and transform these to Kodiak’s body frame for manipulation. Control was performed using the ee cart stiffness controller [5] with tra- jectories provided by our own custom MATLAB code. Experimental Setup: For each experiment, we placed a single object within a 25 cm x 25 cm square on the ta- ble, approximately 1.2 m below the mounting point of the Kinect. This square was chosen to be well-contained within each robot’s workspace, allowing objects to be reached from most approach vectors. Object positions and orientations were varied between trials, although objects were always placed in conﬁgurations in which at least one viable grasp was visible and accessible to the robot.  When using Baxter, due to the limited stroke (span from open to closed) of its gripper, we pre-selected one of the two gripper settings discussed above for each object. We constrained the search space as illustrated in Fig. 11 to ﬁnd grasps for that particular setting.  To detect grasps, we ﬁrst took an RGB-D image from the Kinect with no objects in the scene as a background image. The depth channel of this image was used to segment objects from the scene, and to correct for the slant of the Kinect. Once an object was segmented, we used our algorithm, as described above, to obtain a single best-ranked grasping rectangle.  The search space for the ﬁrst-pass network progressed in 15- degree increments from 15 to 180 degrees (angles larger than 180 being mirror-images of grasps already tested), searching over 10-pixel increments across the image for the X and Y coordinates of the upper-left corner of the rectangle. For the thin gripper setting, rectangle widths and heights from 10 to 40 pixels in 10-pixel increments were searched, while for the thick setting these ranged from 40 pixels to 100 pixels in 20-pixel increments. In both cases, rectangles taller than they were wide were ignored. Once a single best-scoring grasp was detected, we translated it to a robotic grasp consisting of a grasping point and an approach vector using the rectangle’s parameters and the surface normal at the rectangle’s center as described above.  To execute the grasp, we ﬁrst positioned the gripper at a location 10 cm back from the grasping point along the approach vector. The gripper was oriented to the approach vector, and rotated around it based on the orientation of the detected grasping rectangle.  Since Baxter’s arms are highly compliant, slight impreci- sions in end-effector positioning are to be expected – we found that errors of up to 2 cm were typical. Thus, we implemented a visual servoing system using its hand camera, which provides  Fig. 14: Robotic experiment objects: Several of the objects used in experiments, including challenging cases such as an oddly-shaped RC car controller, a cloth towel, plush cat, and white ice cube tray.  these cases, the grasp chosen by the single-stage system might be feasible for a robotic gripper, but the rectangle chosen by the two-stage system represents a grasp which would clearly be successful.  The two-stage system also signiﬁcantly increases the com- putational efﬁciency of our detection system. Average infer- ence time for a MATLAB implementation of the deep network was reduced from 24.6s/image for an exhaustive search using the larger network to 13.5s/image using the two-stage system.  VIII. ROBOTIC EXPERIMENTS  In order to evaluate the performance of our algorithms in the real world, we ran an extensive series of robotic experiments. To explore the generalizability and effect of the robot on the success rate of our algorithms, we performed experiments on two different robotic platforms, a Baxter Research Robot (“Yogi”) and a PR2 (“Kodiak”). Baxter: The ﬁrst platform used is our Baxter Research Robot, which we call “Yogi.” Baxter has two arms with seven degrees of freedom each and a maximum reach of 104 cm, although we used only the left arm for these experiments. The end-effector for this arm is a two-ﬁnger parallel gripper. We augmented the gripper tips using rubber bands for additional friction. Baxter’s grippers are interchangable, and we used two settings for these experiments - a “wide” setting with an open width of 8 cm and closed width of 4 cm, and a “thin” setting with an open width of 4 cm and a closed width of 0 cm (completely closed, gripper tips touching).  To detect grasps, we mounted a Kinect sensor to Yogi’s head, approximately 1.75 m above the ground. angled down- wards at roughly a 75o angle towards a table in front of it. The Kinect gives RGB-D images at a resolution of 640x480 pixels. We calibrated the transformation between the Kinect’s and Yogi’s coordinate frames by marking four points corre- sponding to a set of 3D axes, and obtaining the coordinates of these points in both Kinect’s and Yogi’s frames.  All control for Baxter was done by specifying an end- effector position and orientation, and using the inverse kine- matics provided with Baxter to determine a set of joint angles for this pose. Baxter’s built-in control systems were used to drive the arm to these new joint angles.  TABLE IV: Results for robotic experiments for Baxter, sorted by object category, for a total of 100 trials.  Tr. indicates number of trials, Acc. indicates accuracy (in terms of success percentage.)  Kitchen tools  Lab tools  Containers  Object Can opener Knife Brush Tongs Towel Grater Average Overall  3 3 3 3 3 3  Object Kinect  Tr. Acc. 100 100 Wire bundle 100 Mouse 100 100 100 100 84  Hot glue gun Quad-rotor Duct tape roll Average  Object Colored cereal box  Tr. Acc. 100 100 White cereal box Cap-shaped bowl 100 67 Coffee mug 75 Ice cube tray 100 Martini glass 90  Average  5 3 3 3 4 4  Tr. Acc. 100 50 100 100 100 0 75  3 4 3 3 3 3  Toys  Object Plastic whale Plastic elephant Plush cat RC controller XBox controller Plastic frog Average  Others  Object Tr. Acc. Electric shaver 75 Umbrella 100 Desk lamp 75 Remote control 67 50 Metal bookend 67 72  Glove Average  4 4 4 3 4 3  TABLE V: Results for robotic experiments for PR2, sorted by object category, for a total of 100 trials.  Tr. indicates number of trials, Acc. indicates accuracy (in terms of success percentage.)  Kitchen tools  Lab tools  Containers  Object Can opener Knife Brush Tongs Towel Grater Average Overall  3 3 3 3 3 3  Object Kinect  Tr. Acc. 100 100 Wire bundle 100 Mouse 100 100 100 100 89  Hot glue gun Quad-rotor Duct tape roll Average  Object Colored cereal box  Tr. Acc. 100 100 White cereal box Cap-shaped bowl 100 67 Coffee mug 100 Ice cube tray 100 Martini glass 95  Average  5 3 3 3 4 4  Tr. Acc. 100 100 100 100 100 0 83  3 4 3 3 3 3  Toys  Object Plastic whale Plastic elephant Plush cat RC controller XBox controller Plastic frog Average  Others  Object Tr. Acc. Electric shaver 75 Umbrella 100 Desk lamp 100 Remote control 67 25 Metal bookend 67 72  Glove Average  4 4 4 3 4 3  Tr. Acc. 100 75 100 100 33 100 85  3 4 3 5 3 3  Tr. Acc. 100 100 100 100 67 100 95  3 4 3 5 3 3  Fig. 15: Robots executing grasps: Our robots grasping several objects from the experimental dataset. Top row: Baxter grasping a quad-rotor casing, coffee mug, ice cube tray, knife, and electric shaver. Middle row: Baxter grasping a desk lamp, cheese grater, umbrella, cloth towel, and hot glue gun. Bottom row: PR2 grasping a plush cat, RC car controller, cereal box, toy elephant, and glove.  RGB images at a resolution of 320x200 pixels. We used color segmentation to separate the object from the background, and used its lateral position in image space to drive Yogi’s end- effector to center the object. We did not implement visual servoing for PR2 because its gripper positioning was found to be precise to within 0.5 cm.  After visual servoing was completed, we drove the gripper 14 cm forwards from its current position along the approach vector, so that the grasping point was well-contained within it. We then closed the gripper, grasping the object, and moved it 30 cm upwards. A grasp was determined to be successful if it was sufﬁcient to lift the object and hold it for one second. Objects to be Grasped: For our robotic experiments, we collected a diverse set of 35 objects within a size of .3 m x .3 m x .3 m and weighing at most 2.5 kg (although most were less than 1 kg) from our ofﬁces, homes, and lab. Many of them are shown in Fig. 14. Most of these objects were not present in the training dataset, and thus were completely new to the grasp detection algorithm.  Due to the physical  limitations of the robots’ grippers, we found that ﬁve of these objects were not graspable even when given a hand-chosen grasp. The small pair of pliers was too low to the table to grip properly. The spray paint can was too smooth for the gripper to get enough friction to lift it. The weight of the hammer was too imbalanced, causing the hammer to rotate and slip out of the gripper when grasped. Similar problems were encountered with the bicycle U-lock. The bevel spatula’s handle was too close to the thin- set size of Baxter’s gripper, so that we could not position it precisely enough to grasp it reliably. We did not consider these objects for purposes of our experimental results, since our focus was on evaluating the performance of our grasp detection algorithm. Results: Table IV shows the results of our robotic experiments on Baxter for the remaining 30 objects, a total of 100 trials. Using our algorithm, Yogi was able to successfully execute a grasp in 84% of the trials. Figure 15 shows Yogi executing several of these grasps. In 8% of the trials, our algorithm detected a valid grasp which was not executed correctly by Yogi. Thus, we were able to successfully detect a good grasp in 92% of the trials. Video of some of these trials is available at http://pr.cs.cornell.edu/deepgrasping.  PR2 yielded a higher success rate as seen in Table V, succeeding in 89% of trials. This is largely due to the much wider span of PR2’s gripper from open to closed and its ability to fully close from its widest position, as well as PR2’s ability to apply a larger gripping force. Some speciﬁc instances where PR2 and Baxter’s performance differed are discussed below. For comparison purposes, we ran a small set of control experiments for 16 of the objects in the dataset. The control algorithm simply returned a ﬁxed-size rectangle centered at the object’s center of mass, as determined by depth segmentation from the background. The rectangle was aligned so that the gripper plates ran parallel to the object’s principal axis. This algorithm was only successful in 31% of cases, signiﬁcantly  underperforming our system.  On Baxter, our algorithm sometimes detected a grasp which was not realizable by the current setting of its gripper, but might be executable by others. For example, our algorithm detected grasps across the leg of the plush cat, and the region between the handle and body of the umbrella, both too thin for the wide setting of Baxter’s gripper to grasp since it has a minimum span of 4 cm. Since PR2’s gripper can close completely from any position, it did not encounter these issues and thus achieved a 100% success rate for both these objects. The XBox controller proved to be a very difﬁcult object for either robot to grasp. From a top-down angle, there is only a small space of viable grasps with a span of less than 8 cm, but many which have either a slightly larger span (making them non-realizable by either gripper), or are subtly non-viable (e.g. grasps across the two “handles,” which tend to slip off.) All viable grasps are very near to the 8 cm span of both grippers, meaning that even slight imprecision in positioning can lead to failure. Due to this, Baxter achieved a higher success rate for the XBox controller thanks to visual servoing, succeeding in 50% of cases as compared to the 25% success rate for PR2. Our algorithm was able to consistently detect and execute valid grasps for a red cereal box, but had some failures on a white and yellow one. This is because the background for all objects in the dataset is white, leading the algorithm to learn features relating white areas at the edges of the gripper region to graspable cases. However, it was able to detect and execute correct grasps for an all-white ice cube tray, and so does not fail for all white objects. This could be remedied by extending the dataset to include cases with different background colors. Interestingly, even though the parameters of grasps detected for the white box were similar for PR2 and Baxter, PR2 was able to succeed in every case while Baxter succeeded only half the time. This is because PR2’s increased gripper strength allowed it to execute grasps across corners of the box, crushing it slightly in the process.  Other failures were due to the limitations of the Kinect sensor. We were never able to properly grasp the martini glass because its glossy ﬁnish prevented Kinect from returning any depth estimates for it. Even if a valid grasp were detected using color information only, there was no way to infer a proper grasping position without depth information. Grasps for the metal bookend failed for similar reasons, but it was not as glossy as the martini glass, and gave enough returns for some to succeed.  However, our algorithm also had many noteworthy suc- cesses. It was able to consistently detect and execute grasps for a crumpled cloth towel, a complex and irregular case which bore little resemblance to any object in the dataset. It was also able to ﬁnd and grasp the rims of objects such as the plastic baseball cap and coffee mug, cases where there is little visual distinction between the rim and body of the object. These objects underscore the importance of the depth channel for robotic grasping, as none of these grasps would be detectable without depth information.  Our algorithm was also able to successfully detect and  execute many grasps for which the approach vector was non- vertical. The grasps shown for the coffee mug, desk lamp, cereal box, RC car controller, and toy elephant shown in Fig. 15 were all executed by aligning the gripper to such an approach vector. Indeed, many of these grasps may have failed had the gripper been aligned vertically. This shows that our algorithm is not restricted to detecting top-down grasps, but rather encodes a more general notion of graspability which can be applied to grasps from many angles, albeit within the constraints of visibility from a single-view perspective.  While a few failures occurred, our algorithm still achieved a high rate of accuracy for other oddly-shaped objects such as the quad-rotor casing, RC car controller, and glue gun. For objects with clearly deﬁned handles, such as the cheese grater, kitchen tongs, can opener, and knife, our algorithm was able to detect and execute successful grasps in every trial, showing that there is a wide range of objects which it can grasp extremely consistently.  IX. DISCUSSION AND FUTURE WORK  Our algorithm focuses on the problem of grasp detection for a two-ﬁngered parallel-plate style gripper. It would be directly applicable to other grippers with ﬁxed conﬁgurations, simply requiring new training data labeled with grasps for the gripper in question. Our system would allow even the basic features used for grasp detection to adapt to the gripper. This might be useful in cases such as jamming grippers [29], or two-ﬁngered grippers with differently-shaped contact surfaces, which might require different features to determine a graspable area.  Our detection algorithm does not directly address the prob- lem of 3D orientation of the gripper – this orientation is determined only after an optimal rectangle has been detected, orienting the grasp based on the object’s surface normals. However, just as our approach here considers aligns a 2D feature window to the gripper, an extension of this work might align a 3D window – using voxels, rather than pixels, as its basic unit of representation for input features to the network. This would allow the system to search across the full 6-DoF 3D pose of the gripper, while still leveraging the power of feature learning.  Our system gives only a gripper pose as output, but multi- ﬁngered reconﬁgurable hands also require a conﬁguration of the ﬁngers in order to grasp an object. In this case, our algorithm could be used as a heuristic to ﬁnd one or more locations likely to be graspable (similar to the ﬁrst pass in our two-pass system), greatly reducing the search space needed to ﬁnd an optimal gripper conﬁguration.  Our algorithm also depends only on local features to de- termine grasping locations. However, many household objects may have some areas which are strongly preferable to grasp over others - for example, a knife might be graspable by the blade, or a hot glue gun by the barrel, but both should actually be grasped by their respective handles. Since these regions are more likely to be labeled as graspable in the data, our system already weakly encodes this, but some may not be readily distinguishable using only local information.  Adding a term modeling the probability of each region of the image being a semantically-appropriate area to grasp the object would allow us to incorporate this information. This term could be computed once for the entire image, then added to each local detection score, keeping detection efﬁcient.  In this work, our visual-servoing algorithm was purely heuristic, simply attempting to center the segmented object underneath the hand camera. However, in future work, a simi- lar feature-learning approach might be applied to hand camera images of graspable and non-graspable regions, improving the visual servoing system’s ability to ﬁne-tune gripper position to ensure a good grasp.  Many robotics problems require the use of perceptual infor- mation, but can be difﬁcult and time-consuming to engineer good features for, particularly when using RGB-D data. In future work, our approach could be extended to a wide range of such problems. Our system could easily be applied to other detection problems such as object detection or obstacle detection. However, it could also be adapted to other similar problems, such as object tracking and visual servoing.  Multimodal data has become extremely important  for robotics, due both to the advent of new sensors such as the Kinect and the application of robots to more challenging tasks which require multiple modalities of information to perform well. However, it can be very difﬁcult to design features which do a good job of integrating many modalities. While our work focuses on color, depth, and surface normals as input modes, our structured multimodal regularization algorithm might also be applied to others. This approach could improve performance while allowing roboticists to focus on other engineering challenges.  X. CONCLUSIONS  We presented a system for detecting robotic grasps from RGB-D data using a deep learning approach. Our method has several advantages over current state-of-the-art methods. First, using deep learning allows us to avoid hand-engineering features, learning them instead. Second, our results show that deep learning methods signiﬁcantly outperform even well- designed hand-engineered features from previous work.  We also presented a novel feature learning algorithm for multimodal data based on group regularization. In extensive experiments, we demonstrated that this algorithm produces better features for robotic grasp detection than existing deep learning approaches to multimodal data. Our experiments and results, both ofﬂine and on real robotic platforms, show that our two-stage deep learning system with group regularization is capable of robustly detecting grasps for a wide range of objects, even those previously unseen by the system.  ACKNOWLEDGEMENTS  We would like to thank Yun Jiang and Marcus Lim for useful discussions and help with baseline experiments. This research was funded in part by ARO award W911NF-12-1- 0267, Microsoft Faculty Fellowship and NSF CAREER Award (Saxena), and Google Faculty Research Award (Lee).  REFERENCES  [1] Y. Bengio. Learning deep architectures for AI. FTML,  [2] A. Bicchi and V. Kumar. Robotic grasping and contact:  2(1):1–127, 2009.  a review. In ICRA, 2000.  [3] L. Bo, X. Ren, and D. Fox. Unsupervised Feature Learning for RGB-D Based Object Recognition. In ISER, 2012.  [4] J. Bohg, A. Morales, T. Asfour, and D. Kragic. Data-  driven grasp synthesis - a survey. accepted.  [5] M. Bollini, J. Barry, and D. Rus. Bakebot: Baking  cookies with the pr2. In IROS PR2 Workshop, 2011.  [6] D. Bowers and R. Lumia. Manipulation of unmodeled objects using intelligent grasping schemes. IEEE Trans Fuzzy Sys, 11(3), 2003.  [7] C. Cadena and J. Kosecka. Semantic parsing for priming object detection in rgb-d scenes. In ICRA Workshop on Semantic Perception, Mapping and Exploration, 2013.  [8] A. Coates and A. Y. Ng. Selecting receptive ﬁelds in  deep networks. In NIPS, 2011.  [9] A. Coates, B. Carpenter, C. Case, S. Satheesh, B. Suresh, T. Wang, D. J. Wu, and A. Y. Ng. Text detection and character recognition in scene images with unsupervised feature learning. In ICDAR, 2011.  [10] A. Collet Romea, D. Berenson, S. Srinivasa, and D. Fer- guson . Object recognition and full pose registration from a single image for robotic manipulation. In ICRA, 2009. [11] A. Collet Romea, M. Martinez Torres, and S. Srinivasa. The moped framework: Object recognition and pose estimation for manipulation. IJRR, 30(10):1284 – 1306, 2011.  [12] R. Collobert,  J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language pro- JMLR, 12:2493–2537, cessing (almost) from scratch. 2011.  [13] R. Detry, E. Baseski, M. Popovic, Y. Touati, N. Kruger, O. Kroemer, J. Peters, and J. Piater. Learning object- speciﬁc grasp affordance densities. In ICDL, 2009.  [14] M. Dogar, K. Hsiao, M. Ciocarlie, and S. Srinivasa. In RSS,  Physics-based grasp planning through clutter. 2012.  [15] S. Ekvall and D. Kragic. Learning and evaluation of the approach vector for automatic grasp generation and planning. In ICRA, 2007.  [16] F. Endres, J. Hess, J. Sturm, D. Cremers, and W. Burgard. International  3d mapping with an RGB-D camera. Journal of Robotics Research (IJRR), 2013.  [17] C. Ferrari and J. Canny. Planning optimal grasps. ICRA,  1992.  [18] C. R. Gallegos, J. Porta, and L. Ros. Global optimization  of robotic grasps. In RSS, 2011.  [19] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.  [20] C. Goldfeder, M. Ciocarlie, H. Dang, and P. K. Allen.  The Columbia grasp database. In ICRA, 2009.  [21] I. Goodfellow, Q. Le, A. Saxe, H. Lee, and A. Y. Ng. Measuring invariances in deep networks. In NIPS, 2009. [22] G. Hinton and R. Salakhutdinov. Reducing the dimen- sionality of data with neural networks. Science, 313 (5786):504–507, 2006.  [23] K. Huebner and D. Kragic. Selection of Robot Pre- Grasps using Box-Based Shape Approximation. In IROS, 2008.  [24] A. Hyv¨arinen, P. O. Hoyer, and M. Inki. Topographic independent component analysis. Neural computation, 13(7):1527–1558, 2001.  [25] A. Hyv¨arinen, J. Karhunen, and E. Oja.  Principal Component Analysis and Whitening, chapter 6, pages 125–144. John Wiley & Sons, Inc., 2002.  [26] A. Jalali, P. Ravikumar, S. Sanghavi, and C. Ruan. A  dirty model for multi-task learning. In NIPS, 2010.  [27] N. R. Jared Glover, Daniela Rus. Probabilistic models  of object geometry for grasp planning. In RSS, 2008.  [28] Y. Jiang, S. Moseson, and A. Saxena. Efﬁcient grasping from RGBD images: Learning using a new rectangle representation. In ICRA, 2011.  [29] Y. Jiang, J. R. Amend, H. Lipson, and A. Saxena. Learn- ing hardware agnostic grasps for a universal jamming gripper. In ICRA, 2012.  [30] Y. Jiang, M. Lim, C. Zheng, and A. Saxena. Learning to  place new objects in a scene. IJRR, 31(9), 2012.  [31] I. Kamon, T. Flash, and S. Edelman. Learning to grasp  using visual information. In ICRA, 1996.  [32] D. Kragic and H. I. Christensen. Robust visual servoing.  IJRR, 2003.  [33] K. Lai, L. Bo, X. Ren, and D. Fox. A large-scale In ICRA,  hierarchical multi-view rgb-d object dataset. 2011.  [34] K. Lakshminarayana. Mechanics of form closure. ASME,  1978.  [35] Q. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. Corrado, J. Dean, and A. Ng. Building high-level features using large scale unsupervised learning. In ICML, 2012.  [36] Q. V. Le, D. Kamm, A. F. Kara, and A. Y. Ng. Learning to grasp objects with multiple contact points. In ICRA, 2010.  [37] Y. LeCun, F. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In CVPR, 2004.  [38] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convo- lutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009. [39] H. Lee, Y. Largman, P. Pham, and A. Y. Ng. Unsu- pervised feature learning for audio classiﬁcation using convolutional deep belief networks. In NIPS, 2009.  [40] J. Maitin-shepard, M. Cusumano-towner, J. Lei, and P. Abbeel. Cloth grasp point detection based on multiple- view geometric cues with application to robotic towel folding. In ICRA, 2010.  [63] C. Teuliere and E. Marchand. Direct 3d servoing using  dense depth maps. In IROS, 2012.  [64] P. Viola and M. Jones. Rapid object detection using a  boosted cascade of simple features. In CVPR, 2001.  [65] J. Weisz and P. K. Allen. Pose error robust grasping from  contact wrench space metrics. In ICRA, 2012.  [66] T. Whelan, H. Johannsson, M. Kaess, J. Leonard, and J. McDonald. Robust real-time visual odometry for dense RGB-D mapping. In ICRA, 2013.  [67] L. Zhang, M. Ciocarlie, and K. Hsiao. Grasp evaluation In RSS Workshop on  with graspable feature matching. Mobile Manipulation, 2011.  [41] A.-R. Mohamed, G. Dahl, and G. E. Hinton. Acoustic modeling using deep belief networks. IEEE Trans Audio, Speech, and Language Processing, 20(1):14–22, 2012. [42] A. Morales, P. J. Sanz, and `Angel P. del Pobil. Vision- based computation of three-ﬁnger grasps on unknown planar objects. In IROS, 2002.  [43] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y.  Ng. Multimodal deep learning. In ICML, 2011.  [44] V. Nguyen. Constructing stable force-closure grasps. In  ACM Fall joint computer conf, 1986.  [45] M. Osadchy, Y. LeCun, and M. Miller. Synergistic face detection and pose estimation with energy-based models. JMLR, 8:1197–1215, 2007.  [46] C. Papazov, S. Haddadin, S. Parusel, K. Krieger, and D. Burschka. Rigid 3d geometry matching for grasping of known objects in cluttered scenes. IJRR, 31(4):538– 553, Apr. 2012.  [47] J. H. Piater. Learning visual features to predict hand  orientations. In ICML, 2002.  [48] F. T. Pokorny, K. Hang, and D. Kragic. Grasp moduli  spaces. In RSS, 2013.  [49] J. Ponce, D. Stam, and B. Faverjon. On computing two- ﬁnger force-closure grasps of curved 2D objects. IJRR, 12(3):263, 1993.  [50] A. Rodriguez, M. Mason, and S. Ferry. From caging to  grasping. In RSS, 2011.  [51] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature histograms (FPFH) for 3D registration. In ICRA, 2009. [52] R. B. Rusu, G. Bradski, R. Thibaux, and J. Hsu. Fast 3D recognition and pose using the viewpoint feature histogram. In IROS, 2010.  [53] A. Sahbani, S. El-Khoury, and P. Bidaud. An overview of 3d object grasp synthesis algorithms. Robot. Auton. Syst., 60(3):326–336, Mar. 2012.  [54] A. Saxena, J. Driemeyer, J. Kearns, and A. Ng. Robotic  grasping of novel objects. In NIPS, 2006.  [55] A. Saxena, J. Driemeyer, and A. Y. Ng. Robotic grasping IJRR, 27(2):157–173,  of novel objects using vision. 2008.  [56] A. Saxena, L. L. S. Wong, and A. Y. Ng. Learning grasp strategies with partial shape information. In AAAI, 2008. [57] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. Le- Cun. Pedestrian detection with unsupervised multi-stage feature learning. In CVPR. 2013.  [58] K. B. Shimoga. Robot grasp synthesis algorithms: A  survey. IJRR, 15(3):230–266, June 1996.  [59] R. Socher, B. Huval, B. Bhat, C. D. Manning, and A. Y. Ng. Convolutional-recursive deep learning for 3D object classiﬁcation. In NIPS, 2012.  [60] K. Sohn, D. Y. Jung, H. Lee, and A. Hero III. Efﬁcient learning of sparse, distributed, convolutional feature rep- resentations for object recognition. In ICCV, 2011.  [61] N. Srivastava and R. Salakhutdinov. Multimodal learning  with deep Boltzmann machines. In NIPS, 2012.  [62] C. Szegedy, A. Toshev, and D. Erhan. Deep neural  networks for object detection. In NIPS. 2013.  ","We consider the problem of detecting robotic grasps in an RGB-D view of ascene containing objects. In this work, we apply a deep learning approach tosolve this problem, which avoids time-consuming hand-design of features. Thispresents two main challenges. First, we need to evaluate a huge number ofcandidate grasps. In order to make detection fast, as well as robust, wepresent a two-step cascaded structure with two deep networks, where the topdetections from the first are re-evaluated by the second. The first network hasfewer features, is faster to run, and can effectively prune out unlikelycandidate grasps. The second, with more features, is slower but has to run onlyon the top few detections. Second, we need to handle multimodal inputs well,for which we present a method to apply structured regularization on the weightsbased on multimodal group regularization. We demonstrate that our methodoutperforms the previous state-of-the-art methods in robotic grasp detection,and can be used to successfully execute grasps on two different roboticplatforms."
1301.6316,2013,Hierarchical Data Representation Model - Multi-layer NMF  ,"['Hyun-Ah Song', 'Soo-Young Lee']",https://arxiv.org/pdf/1301.6316.pdf,"3 1 0 2    r a     M 8 1      ]  G L . s c [      3 v 6 1 3 6  .  1 0 3 1 : v i X r a  Hierarchical Data Representation Model -  Multi-layer NMF  Department of Electrical Engineering  Department of Electrical Engineering  Hyun Ah Song  KAIST  Daejeon, 305-701  hyunahsong@kaist.ac.kr  Soo-Young Lee  KAIST  Daejeon, 305-701  sylee@kaist.ac.kr  Abstract  In this paper, we propose a data representation model that demonstrates hierarchi- cal feature learning using nsNMF. We extend unit algorithm into several layers. Experiments with document and image data successfully discovered feature hier- archies. We also prove that proposed method results in much better classiﬁcation and reconstruction performance, especially for small number of features.  1  Introduction  In order to understand complex data, hierarchical feature extraction strategy has been used [1]. One best known algorithm is Deep Belief Network (DBN) introduced in 2006 [2]. With the success of training deep architectures, several variants of deep learning have been introduced [3]. Although these multi-layered algorithms take hierarchical approaches in feature extraction and provide efﬁ- cient solution to complex problems, they do not provide us the relationships of features in form of hierarchies that are learned throughout the hierarchical structure. In this paper, we propose a hierarchical data representation model, hierarchical multi-layer non- negative matrix factorization. (Similar approach has been introduced in [4].) We extend a variant of NMF algorithm [5], nsNMF [6] into several layers for hierarchical learning. Here, we demon- strate intuitive feature hierarchies present in the data set by learning relationships between features across layers. We also prove that instead of one step learning, hierarchical approach learns more meaningful and helpful features, which leads to better distributed representations, and results in bet- ter performance in classiﬁcation and reconstruction for small number of features, which guarantees reduced loss of performance, even when representing data in small dimensions.  2 Non-smooth non-negative matrix factorization (nsNMF)  Proposed network is constructed by stacking nsNMF [6] into several layers. Non-smooth non- negative matrix factorization (nsNMF) is a variant of NMF that restricts sparsity constraint. Basic NMF decomposes non-negative input data X into non-negative W and H, which are features and cor- responding coefﬁcients or data representation respectively. It aims to reduce error between original data X and its reconstruction WH: C = 1 k=1 WikHkj)2. To apply sparsity constraint to standard NMF, a sparsity matrix S is introduced in [6]: S = (1 − k ones(k). k is number of features, and θ is parameter for smoothing effect, in range of 0 θ)I(k) + θ to 1. I(k) is identity matrix of size k x k, and ones(k) is a matrix of size k x k with all components of 1s. We smooth a matrix by multiplying it with S. The closer θ is to 1, more smoothing effect is applied. During alternative update, we smooth H matrix by multiplying S and H during iterations as H=SH. To compensate the loss of sparsity, W becomes sparse.  j=1(Xij −(cid:80)l (cid:80)n  2(cid:107)X− WH(cid:107)2 = 1  (cid:80)m  i=1  2  1  Figure 1: Concept hierarchies in Reuters. (a) Experimental results, and (b) diagram of result in (a).  3 Multi-layer architecture  (cid:18) H (l)  (cid:19)  j(cid:48)=1  kj M (l) kj  kj = f  , where M (l)  kj = (cid:80)n  The proposed hierarchical multi-layer NMF structure comprise of several layers of unit algo- rithm. We ﬁrst train each layer separately. We process outcome of each layer H(l) to get K(l). n , f(·) is nonlinear function, and l denotes in- H (l) kj(cid:48) K (l) dex of layer, l = 1, 2, ...L. The superscript of each term denotes layer index. Processed data representation of K(l) is used as input to next layer. Using nsNMF, K(l) is decomposed into W(l+1) and H(l+1):K(l) ≈ W(l+1)H(l+1). Then, we use outcome of separate training as ini- tialization, and train the whole network jointly. The cost function for joint training is described: kj , which C = 1 2 can be computed via back propagation of errors from the last layer to the lth layer: (cid:94) H(L−1) ≈ H(L) = H(L). f−1(·) is inverse nonlinear function. (more details on the actual update computation is described in Ap- pendix A). After training until the last layer, ﬁnal data representation H(L) is acquired. This is the activation information of complex features, which is the integration of features throughout the layers, W(1)W(2)...W(L). For more detailed explanation, refer to the pseudo-code for the training procedure in Appendix B.  (cid:80)n (cid:80)m j=1(Xij −(cid:80)l M(L−1) (cid:12) f−1(cid:16) (cid:17) W(L)(cid:93) H(L)  (cid:103)H (1) kj )2, where (cid:103)H (l) ,..., (cid:103)H(1) ≈ M(1) (cid:12) f−1(cid:16)  kj is the reconstruction of H (l)  W(2)(cid:103)H(2)  , where (cid:93)  k=1 W (1)  (cid:17)  i=1  ik  4 Document data feature hierarchies  We applied our proposed network to document database. We used ”Reuters-21578 collection, distri- bution 1.0”1 as database. We sorted top 10 categories from ModApte split, conducted pre-processing of removing stop-words, and reduced dimension to 1000. There are 5786 and 2587 document sam- ples for training data, and test data. We constructed two-layered network with number of hidden neurons as 160. We observed how concepts form hierarchies in document data in Figure 1 (a). First, second, and third W(1) features contain words related to ’oil production’ (exploration, pipeline, production, industry), ’oil contract’ (contract, purchase, barrel, prices), and ’oil reﬁnery processing’ (reﬁnery, reserves, pipeline, petroleum), respectively. These sub-class topic features are combined together and develop into one broader topic ’oil.’ With this combination relationship of features, we can ﬁgure out that those three seemingly independent features can be re-categorized under the same broader topic. (The  1The Reuters-21578, Distribution 1.0 test collection is available from David D. Lewis professional home  page, currently: http://www.research.att.com/∼lewis  2  Figure 2: Reconstruction error (left) and classiﬁcation rate (right) of (a) Reuters and (b) MNIST.  Figure 3: (a) Reconstruction by shallow network (ﬁrst row) and proposed network (third row), with original input (second row). (b) Final data representation (visualized by PCA) comparison of shallow network (middle) and proposed network (right), in comparison to raw data (left)  concept hierarchy learned in Reuters: sub-categories of ’oil production’, ’contract’, and ’reﬁnery processing’ exist under ’oil’ category.) Furthermore we analyzed reconstruction and classiﬁcation performance as shown in Figure 2 (a). The proposed hierarchical feature extraction method results in much better classiﬁcation and recon- struction, especially for small number of features, compared to extracting features at one step. This proves the efﬁciency and effectiveness of our proposed approach in learning of features. We also applied our network to handwritten digit image MNIST2. The ﬁnal data representation HL displayed distinct activation patterns for samples of the different classes, as a result of successful learning of feature hierarchy, which determines the combination of low level features in forming of distinct class features. In Figure 2 (b), the reconstruction error and classiﬁcation performance also demonstrate better performance of our proposed method in small number of dimensions. In Figure 3 (a), we can observe sparser and clear reconstruction of our proposed network. The Fisher discriminant values of ﬁnal data representation of the shallow network and our proposed network were 0.51 and 0.61 respectively. We can infer that proposed network learns more meaningful and helpful features so that it results in better distributed (clustered) representation of data. We can also check this via the visualization of H(L) to 2-D domain shown in Figure 3 (b).  5 Conclusion  In this paper, we proposed a hierarchical data representation model, hierarchical multi-layer NMF by stacking nsNMF into several layers. We demonstrated hierarchical approach in learning of the features. There are mainly two ﬁndings of our research. Taking hierarchical learning by stack- ing NMFs: 1.reveals intuitive feature hierarchies (subcategories) by learning feature relationships throughout the layers, and 2.learns more meaningful features compared to one-step learning. (as a result, our proposed method results in much better classiﬁcation and reconstruction performance, provided small number of dimensions for data representation.) We expect our proposed method to be applied to various types of data for discovering underlying feature hierarchies and at the same time, maintain reconstruction and classiﬁcation performance even with small number of features for data representation.  2Available at: http://yann.lecun.com/exdb/mnist/  3  References  [1] Bengio, Y. (2007). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning, 2(1), 1-127. [2] Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18, 1527-1554. [3] Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy Layer-Wise Training of Deep Networks. NIPS,153-160. [4] Ahn, J., Choi, S., Oh, J. (2004). A Multiplicative Up-Propagation Algorithm. Proceedings of the 21st International Conference on Machine Learning. [5] Lee, D. D., and Seung, H. S. (1999). Learning the parts of objects by non-negative matrix factorization. Nature, 401, 788-791. [6] Pascual-Montano, A., Carazo, J. M., Kochi, K., Lehmann, D., and Pascual-Marqui, R. D. (2006). Non- Smooth Nonnegative Matrix Factorization (nsNMF). IEEE Trans. Pattern Anal. Machine Intell, 403-415.  4  A Appendix: Actual update computation  Continued from Section 3, the actual computation is done as described in (1).  W(l)T  (cid:16) Nu(l)(cid:17) (cid:0)W(l)T De(l)(cid:1) W(l)H(l)(cid:17)(cid:17) W(l)H(l)(cid:17)(cid:17)  kj  , where  kj  if l = 1 otherwise  if l = 1 otherwise  , and H (l)  kj ← H (l)  kj  M(l−1)f−1(cid:48)(cid:16) M(l−1)f−1(cid:48)(cid:16)  (1a)  (1b)  (1c)  ik  (cid:19) (cid:19) Nu(l−1)(cid:17) (cid:12)(cid:16) De(l−1)(cid:17) (cid:12)(cid:16)  ik  ik  Nu(l) =  (cid:18) (cid:18) Nu(l)(cid:94) H(l)T ik ← W (l) W (l) De(l)(cid:94) (cid:40)X H(l)T (cid:16) (cid:40)(cid:101)X (cid:16) H(1). (cid:103)H (l) (cid:103)H(l) =  W(l−1)T  W(l−1)T  De(l) =  (cid:40)H(l) M(l) (cid:12) f−1(cid:16)  Here, (cid:101)X = W(1)(cid:93)  errors from the last layer to the lth layer as shown in (2).  kj is the reconstruction of H (l)  kj , which can be computed via back propagation of  (cid:17)  if l = L if l = L − 1, ..., 1  (2)  W(l+1) (cid:94) H(l+1)  M(l) is a matrix of column-wise mean of H(l), and f−1(·) is inverse nonlinear function.  5  B Appendix: Pseudo-code for training procedure of proposed network  %% Separate training of layers in extending mode for l = 1 : L do  Randomly initialize W(l) and H(l) if l = 1 then  K(l−1) = X  end if for iteration = 1 : (untilconvergence) do  )ik  (K(l−1)H(l)T (W(l)H(l)H(l)T (W(l)T (W(l)T  )ik K(l−1))kj W(l)H(l))kj  ik  ik ← W (l) W (l) kj ← H (l) H (l) end for M (l)  kj =(cid:80)n (cid:18) H  kj  (cid:19)  (l) kj (l) kj  M  kj = f  K (l) end for  j(cid:48)=1 H (l)  kj(cid:48) /n  %% Joint training the whole network Use W(l) and H(l), and use M(l) acquired from above for iteration = 1 : (untilconvergence) do  for l = 1 : L do  (cid:17)  Nu(l) = X  end if if l = 1 then  which can be written in full length as:  W(l+1) (cid:94) H(l+1) which can be written in full length as:  if l = L then(cid:103)H(l) = H(l) else(cid:103)H(l) = M(l) (cid:12) f−1(cid:16) (cid:103)H(l) = M(l) (cid:12) f−1(W(l+1)(M(l+1) (cid:12) f−1(W(l+2)(...(M(L−1) (cid:12) f−1(W(L)H(L)))))) De(l) = (cid:101)X (cid:101)X = W(1)(cid:93) Nu(l−1)(cid:17) (cid:12)(cid:16) (cid:16) De(l−1)(cid:17) (cid:12)(cid:16) (cid:16) W(l−1)T W(l−1)T H(l)T(cid:19) (cid:18) end if Nu(l) (cid:94) H(l)T(cid:19) (cid:18) ik ← W (l) W (l) (cid:94) (cid:19) (cid:18) De(l)(cid:17) (cid:16)  H(1) = W(1)(M(1) (cid:12) f−1(W(2)(M(2) (cid:12) f−1(W(3)(...  M(l−1)f−1(cid:48)(cid:16) M(l−1)f−1(cid:48)(cid:16)  W(l)H(l)(cid:17)(cid:17) W(l)H(l)(cid:17)(cid:17)  M(L−1) (cid:12) f−1(W(L)H(L)))))))  kj ← H (l) H (l)  Nu(l) =  De(l) =  W(l)T  W(l)T  else  Nu(l)  De(l)  kj  ik  kj  ik  ik  kj  end for  end for  6  ","In this paper, we propose a data representation model that demonstrateshierarchical feature learning using nsNMF. We extend unit algorithm intoseveral layers. Experiments with document and image data successfullydiscovered feature hierarchies. We also prove that proposed method results inmuch better classification and reconstruction performance, especially for smallnumber of features. feature hierarchies."
1301.4293,2013,Latent Relation Representations for Universal Schemas  ,"['Sebastian Riedel', 'Limin Yao', 'Andrew McCallum']",https://arxiv.org/pdf/1301.4293.pdf,"3 1 0 2     n a J    8 2      ]  G L . s c [      2 v 3 9 2 4  .  1 0 3 1 : v i X r a  Latent Relation Representations for Universal  Schemas  Sebastian Riedel  Department of Computer Science  University College London  Limin Yao, Andrew McCallum Department of Computer Science  University of Massachusetts at Amherst  S.Riedel@cs.ucl.ac.uk  {lmyao,mccallum}@cs.umass.edu  1 Introduction  Supervised relation extraction uses a pre-deﬁned schema of relation types (such as born-in or employed-by). This approach requires labeling textual relations, a time-consuming and difﬁcult process. This has led to signiﬁcant interest in distantly-supervised learning. Here one aligns exist- ing database records with the sentences in which these records have been “rendered”, and from this labeling one can train a machine learning system as before [1, 2]. However, this method relies on the availability of a large database that has the desired schema.  The need for pre-existing databases can be avoided by not having any ﬁxed schema. This is the approach taken by OpenIE [3]. Here surface patterns between mentions of concepts serve as rela- tions. This approach requires no supervision and has tremendous ﬂexibility, but lacks the ability to generalize. For example, OpenIE may ﬁnd FERGUSON–historian-at–HARVARD but does not know FERGUSON–is-a-professor-at–HARVARD.  One way to gain generalization is to cluster textual surface forms that have similar meaning [4, 5, 6, 7]. While the clusters discovered by all these methods usually contain semantically related items, closer inspection invariably shows that they do not provide reliable implicature. For example, a cluster may include historian-at, professor-at, scientist-at, worked-at. However, scientist-at does not necessarily imply professor-at, and worked-at certainly does not imply scientist-at. In fact, we contend that any relational schema would inherently be brittle and ill-deﬁned––having ambiguities, problematic boundary cases, and incompleteness.  In response to this problem, we present a new approach: implicature with universal schemas. Here we embrace the diversity and ambiguity of original inputs. This is accomplished by deﬁning our schema to be the union of all source schemas: original input forms, e.g. variants of surface patterns similarly to OpenIE, as well as relations in the schemas of pre-existing structured databases. But unlike OpenIE, we learn asymmetric implicature among relations and entity types. This allows us to probabilistically “ﬁll in” inferred unobserved entity-entity relations in this union. For example, after observing FERGUSON–historian-at–HARVARD, our system infers that FERGUSON–professor- at–HARVARD, but not vice versa.  At the heart of our approach is the hypothesis that we should concentrate on predicting source data––a relatively well deﬁned task that can be evaluated and optimized––as opposed to modeling semantic equivalence, which we believe will always be illusive.  To reason with a universal schema, we learn latent feature representations of relations, tuples and en- tities. These act, through dot products, as natural parameters of a log-linear model for the probability that a given relation holds for a given tuple. We show experimentally that this approach signiﬁcantly outperforms a comparable baseline without latent features, and the current state-of-the-art distant supervision method.  1  2 Model  We use R to denote the set of relations we seek to predict (such as works-written in Freebase, or the X–heads–Y pattern), and T to denote the set of input tuples. For simplicity we assume each relation to be binary. Given a relation r ∈ R and a tuple t ∈ T the pair hr, ti is a fact, or relation instance. The input to our model is a set of observed facts , and the observed facts for a given tuple t := {hr, ti ∈ }. Our goal is a model that can estimate, for a given relation r (such as X–historian-at–Y) and a given tuple t (such as <FERGUSON,HARVARD>) a score cr,t for the fact hr, ti. This matrix completion problem is related to collaborative ﬁltering. We can think of each tuple as a customer, and each relation as a product. Our goal is to predict how the tuple rates the relation (rating 0 = false, rating 1 = true), based on observed ratings in . We interpret cr,t as the probability p (yr,t = 1) where yr,t is a binary random variable that is true iff hr, ti holds. To this end we introduce a series of exponential family models inspired by generalized PCA [8], a probabilistic generalization of Principle Compo- nent Analysis. These models will estimate the conﬁdence in hr, ti using a natural parameter θr,t and the logistic function: cr,t := p (yr,t|θr,t) :=  1  1+exp(−θr,t) .  We follow[9] and use a ranking based objective function to estimate parameters of our models.  Latent Feature Model One way to deﬁne θr,t is through a latent feature model F. We measure compatibility between relation r and tuple t as a dot product of two latent feature representations of size K F: ar for relation r, and vt for tuple t. This gives θF k ar,kvt,k and corresponds to the original generalized PCA that learns a low-rank factorization of Θ = (θr,t).  r,t := PKF  Neighborhood Model We can interpolate the conﬁdence for a given tuple and relation based on the trueness of other similar relations for the same tuple. In Collaborative Filtering this is referred as a neighborhood-based approach [10]. We implement a neighborhood model N via a set of weights wr,r′, where each corresponds to a directed association strength between relations r and r′. Sum- ming these up gives θN  r,t := Pr′∈t\{r} wr,r′.1  Entity Model Relations have selectional preferences: they allow only certain types in their ar- gument slots. To capture this observation, we learn a latent entity representation from data. For each entity e we introduce a latent feature vector te ∈ Rl. In addition, for each relation r and argument slot i we introduce a feature vector di. Measuring compatibility of an entity tuple and relation amounts to summing up the compatibilities between each argument slot representation and the corresponding entity representation: θE  r,t := Parity(r)  i=1 PKE  k di,ktti,k.  Combined Models Hence we also use various combinations, such as θN,F,E  In practice all the above models can capture important aspects of the data.  := θN  r,t + θF  r,t + θE  r,t.  r,t  3 Experiments  Does reasoning jointly across a universal schema help to improve over more isolated approaches? In the following we seek to answer this question empirically.  Data Our experimental setup is roughly equivalent to previous work [2], and hence we omit de- tails. To summarize, we consider each pair ht1, t2i of Freebase entities that appear together in a corpus. Its set of observed facts t correspond to: Extracted surface patterns (in our case lexicalized dependency paths) between mentions of t1 and t2, and the relations of t1 and t2 in Freebase. We divide all our tuples into approximately 200k training tuples, and 200k test tuples. The total number of relations (patterns and from Freebase) is approximately 4k.  1Notice that the neighborhood model amounts to a collection of local log-linear classiﬁers, one for each  relation r with weights wr.  2  Predicting Freebase and Surface Pattern Relations For evaluation we use two collections of relations: Freebase relations and surface patterns. In either case we compare the competing systems with respect to their ranked results for each relation in the collection.  Our ﬁrst baseline is MI09, a distantly supervised classiﬁer based on the work of [1]. We also compare against YA11, a version of MI09 that uses preprocessed pattern cluster features according to [7]. The third baseline is SU12, the state-of-the-art Multi-Instance Multi-Label system by [11]. The remaining systems are our neighborhood model (N), the factorized model (F), their combination (NF) and the combined model with a latent entity representation (NFE).  The results in terms of mean average precision (with respect to pooled results from each system) are in the table below:  Relation Total Freebase Total Pattern  # 334 329  MI09 YA11 0.48 0.52  SU12 0.57  N 0.52 0.28  F  0.66 0.56  NF NFE 0.67 0.69 0.46 0.50  For Freebase relations, we can see that adding pattern cluster features (and hence incorporating more data) helps YA11 to improve over MI09. Likewise, we see that the factorized model F improves over N, again learning from unlabeled data. This improvement is bigger than the corresponding change between MI09 and YA11, possibly indicating that our latent representations are optimized directly towards improving prediction performance. Our best model, the combination of N, F and E, outperforms all other models in terms of total MAP, indicating the power of selectional preferences learned from data.  MI09, YA11 and SU12 are designed to predict structured relations, and so we omit them for results on surface patterns. Look at our models for predicting tuples of surface patterns. We again see that learning a latent representation (F, NF and NFE models) from additional data helps substantially over the non-latent N model.  All our models are fast to train. The slowest model trains in just 30 minutes. By contrast, training the topic model in YA11 alone takes 4 hours. Training SU12 takes two hours (on less data). Also notice that our models not only learn to predict Freebase relations, but also approximately 4k surface pattern relations.  4 Conclusion  We represent relations using universal schemas. Such schemas contain surface patterns as relations, as well as relations from structured sources. We can predict missing tuples for surface pattern rela- tions and structured schema relations. We show this experimentally by contrasting a series of popular weakly supervised models to our collaborative ﬁltering models that learn latent feature representa- tions across surface patterns and structured relations. Moreover, our models are computationally efﬁcient, requiring less time than comparable methods, while learning more relations.  Reasoning with universal schemas is not merely a tool for information extraction. It can also serve as a framework for various data integration tasks, for example, schema matching. In future work we also plan to integrate universal entity types and attributes into the model.  References  [1] Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Process- ing of the AFNLP (ACL ’09), pages 1003–1011. Association for Computational Linguistics, 2009.  [2] Sebastian Riedel, Limin Yao, and Andrew McCallum. Modeling relations and their mentions without labeled text. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10), 2010.  [3] Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. Open information  extraction from the web. Commun. ACM, 51(12):68–74, 2008.  3  [4] Dekang Lin and Patrick Pantel. DIRT - discovery of inference rules from text. In Knowledge  Discovery and Data Mining, pages 323–328, 2001.  [5] Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy.  ISP: Learning Inferential Selectional Preferences. In Proceedings of NAACL HLT, 2007.  [6] Alexander Yates and Oren Etzioni. Unsupervised methods for determining object and relation  synonyms on the web. Journal of Artiﬁcial Intelligence Research, 34:255–296, 2009.  [7] Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. Structured relation discovery using generative models. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’11), July 2011.  [8] Michael Collins, Sanjoy Dasgupta, and Robert E. Schapire. A generalization of principal  component analysis to the exponential family. In Proceedings of NIPS, 2001.  [9] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr:  Bayesian personalized ranking from implicit feedback. In Proceedings of UAI, 2009.  [10] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering In Proceedings of the 14th ACM SIGKDD international conference on Knowledge  model. discovery and data mining, KDD ’08, pages 426–434, New York, NY, USA, 2008. ACM.  [11] Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. Multi- instance multi-label learning for relation extraction. In Proceedings of EMNLP-CoNLL, 2012.  4  ","Traditional relation extraction predicts relations within some fixed andfinite target schema. Machine learning approaches to this task require eithermanual annotation or, in the case of distant supervision, existing structuredsources of the same schema. The need for existing datasets can be avoided byusing a universal schema: the union of all involved schemas (surface formpredicates as in OpenIE, and relations in the schemas of pre-existingdatabases). This schema has an almost unlimited set of relations (due tosurface forms), and supports integration with existing structured data (throughthe relation types of existing databases). To populate a database of suchschema we present a family of matrix factorization models that predict affinitybetween database tuples and relations. We show that this achieves substantiallyhigher accuracy than the traditional classification approach. More importantly,by operating simultaneously on relations observed in text and in pre-existingstructured DBs such as Freebase, we are able to reason about unstructured andstructured data in mutually-supporting ways. By doing so our approachoutperforms state-of-the-art distant supervision systems."
1301.2840,2013,Unsupervised Feature Learning for low-level Local Image Descriptors  ,"['Christian Osendorfer', 'Justin Bayer', 'Patrick van der Smagt']",https://arxiv.org/pdf/1301.2840.pdf,"3 1 0 2    r p A 5 2         ]  V C . s c [      4 v 0 4 8 2  .  1 0 3 1 : v i X r a  Unsupervised Feature Learning for low-level Local  Image Descriptors  Christian Osendorfer, Justin Bayer, Sebastian Urban, Patrick van der Smagt  {osendorf, bayerj, surban, smagt}@in.tum.de  Technische Universit¨at M¨unchen  Abstract  Unsupervised feature learning has shown impressive results for a wide range of in- put modalities, in particular for object classiﬁcation tasks in computer vision. Us- ing a large amount of unlabeled data, unsupervised feature learning methods are utilized to construct high-level representations that are discriminative enough for subsequently trained supervised classiﬁcation algorithms. However, it has never been quantitatively investigated yet how well unsupervised learning methods can ﬁnd low-level representations for image patches without any additional supervi- sion. In this paper we examine the performance of pure unsupervised methods on a low-level correspondence task, a problem that is central to many Computer Vi- sion applications. We ﬁnd that a special type of Restricted Boltzmann Machines (RBMs) performs comparably to hand-crafted descriptors. Additionally, a simple binarization scheme produces compact representations that perform better than several state-of-the-art descriptors.  1  Introduction  In this paper we tackle a recent computer vision dataset [2] from the viewpoint of unsupervised feature learning. Why yet another dataset? There are already enough datasets that serve well for evaluating feature learning algorithms. In particular for feature learning from image data, several well-established benchmarks exist: Caltech-101 [10], CIFAR-10 [19], NORB [23], to name only a few. Notably, these benchmarks are all object classiﬁcation tasks. Unsupervised learning algorithms are evaluated by considering how well a subsequent supervised classiﬁcation algorithm performs on high-level features that are found by aggregating the learned low-level representations [8]. We think that mingling these steps makes it difﬁcult to assess the quality of the unsupervised algorithms. A more direct way is needed to evaluate these methods, preferably where a subsequent supervised learning step is completely optional. We are not only at odds with the methodology of evaluating unsupervised learning algorithms. Gen- eral object classiﬁcation tasks are always based on orientation- and scale-rectiﬁed pictures with objects or themes ﬁrmly centered in the middle. We are looking for a dataset where it is possible to show that unsupervised feature learning is beneﬁcial to the wide range of Computer Vision tasks beyond object classiﬁcation, like tracking, stereo vision, panoramic stitching or structure from mo- tion. One might argue, that object classiﬁcation acts as a good proxy for all these other tasks but this hypothesis has not shown to be correct either theoretically or through empirical evidence. In- stead, we chose the most general and direct task that can be be applied to low-level representations: matching these representations, i.e. determining if two data samples are similar given their learned representation. Matching image descriptors is a central problem in Computer Vision, so hand-crafted descriptors are always evaluated with respect to this task [28]. Given a dataset of labeled correspondences, supervised learning approaches will ﬁnd representations and the accompanying distance metric that  1  are optimized with respect to the induced similarity measure. It is remarkable that hand-engineered descriptors perform well under this task without the need to learn such a measure for their represen- tations in a supervised manner. To the best of our knowledge it has never been investigated whether any of the many unsupervised learning algorithms developed over the last couple of years can match this performance without relying on any supervision signals. While we propose an additional benchmark for unsupervised learning algorithms, we do not introduce a new learning algorithm. We rather investigate the per- formance of the Gaussian RBM (GRBM) [39], its sparse variant (spGRBM) [29] and the mean covariance RBM (mcRBM) [33] without any supervised learning with respect to the matching task. As it turns out, the mcRBM performs comparably to hand-engineered feature descriptors. In fact using a simple heuristic, the mcRBM produces a compact binary descriptor that performs better than several state-of-the-art hand-crafted descriptors. We begin with a brief description of the dataset used for evaluating the matching task, followed by a section on details of the training procedure. In section 4 we present our results, both quantitatively and qualitatively and also mention other models that were tested but not further analyzed because of overall bad performance. Section 5 concludes with a brief summary and an outlook for future work. A review of GRBMs, spGRBMs and mcRBMs is provided in the appendix, section 6, for completeness.  Related work Most similar in spirit to our work are [6, 20, 22]: Like us, [6, 22] are interested in the behavior of unsupervised learning approaches without any supervised steps afterwards. Whereas both investigate high-level representations. [20] learns a compact, binary representation with a very deep autoencoder in order to do fast content-based image search (semantic hashing, [36]). Again, these representations are studied with respect to their capabilities to model high-level object concepts. Additionally, various algorithms to learn high-level correspondences have been studied [4, 37, 16] in recent years. Finding (compact) low-level image descriptors should be an excellent machine learning task: Even hand-designed descriptors have many free parameters that cannot (or should not) be optimized man- ually. Given ground truth data for correspondences, the performance of supervised learning algo- rithms is impressive [2]. Very recently, boosted learning with image gradient-based weak learners has shown excellent results [43, 42] on the same dataset used in this paper. See section 2 of [43] for more related work in the space of supervised metric learning.  2 Dataset  At the heart of this paper is a recently introduced dataset for discriminative learning of local image descriptors [2]. It attempts to foster learning optimal low-level image representations using a large and realistic training set of patch correspondences. The dataset is based on more than 1.5 million image patches (64 × 64 pixels) of three different scenes: the Statue of Liberty (about 450,000 patches), Notre Dame (about 450,000 patches) and Yosemite’s Half Dome (about 650,000 patches). The patches are sampled around interest points detected by Difference of Gaussians [27] and are normalized with respect to scale and orientation1. As shown in Figure 1, the dataset has a wide variation in lighting conditions, viewpoints, and scales. The dataset contains also approximately 2.5 million image correspondences. Correspondences be- tween image patches are established via dense surface models obtained from stereo matching (stereo matching, with its epipolar and multi-view constraints, is a much easier problem than unconstrained 2D feature matching). The exact procedure to establish correspondences is more involved and de- scribed in detail in [2, Section II]. Because actual 3D correspondences are used, the identiﬁed 2D patch correspondences show substantial perspective distortions resulting in a much more realistic dataset than previous approaches [24, 28]. The dataset appears very similar to an earlier benchmark of the same authors [47], yet the correspondences in the novel dataset resemble a much harder prob- lem. The error rate at 95% detection of correct matches for the SIFT descriptor [27] raises from 6% to 26%, the error rate for evaluating patch similarity in pixel space (using normalized sum squared differences) raises from 20% to at least 48% (all numbers are take from [47] and [2] respectively),  1A similar dataset of patches centered on multi-scale Harris corners is also available.  2  Figure 1: Patch correspondences from the Liberty dataset. Note the wide variation in lighting, viewpoint and level of detail. The patches are centered on interest points but otherwise can be considered random, e.g. there is no reasonable notion of an object boundary possible. Figure taken from [2].  for example. In order to facilitate comparison of various descriptor algorithms a large set of prede- termined match/non-match patch pairs is provided. For every scene, sets comprising between 500 and 500,000 pairs (with 50% matching and 50% non-matching pairs) are available. We don’t argue that this dataset subsumes or substitutes any of the previously mentioned bench- marks. Instead, we think that it can serve to complement those. It constitutes an excellent testbed for unsupervised learning algorithms: Experiments considering self-taught learning [32], effects of semi-supervised learning, supervised transfer learning over input distributions with a varying de- gree of similarity (the scenes of Statue of Liberty and Notredame show architectural structures, while Half Dome resembles a typical natural scenery) and the effect of enhancing the dataset with arbitrary image patches around keypoints can all be conducted in a controlled environment. Further- more, end-to-end trained systems for (large) classiﬁcation problems (like [21, 5]) can be evaluated with respect to this type of data distribution and task.  3 Training Setup  Different to [2], our models are trained in an unsupervised fashion on the available patches. We train on one scene (400,000 randomly selected patches from this scene) and evaluate the performance on the test set of every scene. This allows us to investigate the self-taught learning paradigm [32]. We also train on all three scenes jointly (represented by 1.2 million image patches) and then evaluate again every scene individually.  3.1 GRBM/spGRBM  The GRBM and spGRBM (see Appendix, section 6.2) only differ in the setting of the sparsity penalty λsp, all other settings are the same. We use CD1 [13] to compute the approximate gradient of the log-likelihood and the recently proposed rmsprop [41] method as gradient ascent method. Compared to standard minibatch gradient ascent, we ﬁnd that rmsprop is a more efﬁcient method with respect to the training time necessary to learn good representations: it takes at most half of the training time necessary for standard minibatch gradient ascent. Before learning the parameters, we ﬁrst scale all image patches to 16×16 pixels. Then we preprocess all training samples by subtracting the vectors’ mean and dividing by the standard deviation of its elements. This is a common practice for visual data and corresponds to local brightness and contrast normalization. [39, Section 2.2] gives also a theoretical justiﬁcation for why this preprocessing step is necessary to learn a reasonable precision matrix Λ. We ﬁnd that this is the only preprocessing scheme that allows GRBM and spGRBM to achieve good results. In addition, it is important to learn  3  Λ—setting it to the identity matrix, a common practice [14], also produces dissatisfying error rates. Note that originally it was considered that learning Λ is mostly important when one wants to ﬁnd a good density (i.e. generative) model of the data. Both GRBM and spGRBM have 512 hidden units. The elements of W are initialized according to N (0, 0.1), the biases are initialized to 0. rmsprop uses a learning rate of 0.001, the decay factor is 0.9, the minibatch size is 128. We train both models for 10 epochs (this takes about 15 minutes on a consumer GPU for 400000 patches). For the spGRBM we use a sparsity target of ρ = 0.05 and a sparsity penalty of λsp = 0.2. spGRBM is very sensitive to settings of λsp [38]—setting it too high results in dead representations (samples that have no active hidden units) and the results deteriorate drastically.  3.2 mcRBM  mcRBM (see Appendix, section 6.3) training is performed using the code from [33]. We resam- ple the patches to 16 × 16 pixels. Then the samples are preprocessed by subtracting their mean (patchwise), followed by PCA whitening, which retains 99% of the variance. The overall training procedure (with stochastic gradient descent) is identical to the one described in [33, Section 4]. We train all architectures for a total of 100 epochs, however updating P is only started after epoch 50. We consider two different mcRBM architectures: The ﬁrst has 256 mean units, 512 factors and 512 covariance units. P is not constrained by any ﬁxed topography. We denote this architec- ture by mcRBM(256, 512/512). The second architecture is concerned with learning more compact representations: It has 64 mean units, 576 factors and 64 covariance units. P is initialized with a two-dimensional topography that takes 5 × 5 neighborhoods of factors with a stride equal to 3. We denote this model by mcRBM(64, 576/64). On a consumer grade GPU it takes 6 hours to train the ﬁrst architecture on 400000 samples and 4 hours to train the second architecture on the same number of samples.  4 Results  For the results presented in this section (Table 1) we follow the evaluation procedure of [2]: For every scene (Liberty (denoted by LY), Notredame (ND) and Half Dome (HD)), we use the labeled dataset with 100,000 image pairs to assess the quality of a trained model on this scene. In order to save space we do not present ROC curves and only show the results in terms of the 95% error rate which is the percent of incorrect matches when 95% of the true matches are found: After computing the respective distances for all pairs in a test set, a threshold is determined such that 95% of all matching pairs have a distance below this threshold. Non-matching pairs with a distance below this threshold are considered incorrect matches. Table 1 consists of two subtables. Table 1a presents the error rates for GRBM, spGRBM and mcRBM when no limitations on the size of representations are placed. Table 1b only considers descriptors that have an overall small memory footprint. For GRBM and spGRBM we use the activations of the hidden units given a preprocessed input patch v as descriptor D(v) (see eq. 5, section 6.1):  D(v) = σ(vT Λ  1 2 W + b)  For the mcRBM a descriptor is formed by using the activations of the latent covariance units alone, see eq. 8, section 6.3:  D(v) = σ(P T (CT v)2 + c)  This is in accordance with manually designed descriptors. Many of these rely on distributions (i.e. histograms) of intensity gradients or edge directions [27, 28, 1], structural information which is encoded by the covariance units (see also [35, Section 2])2.  4.1 Distance metrics  As we explicitly refrain from learning a suitable (with respect to the correspondence task) distance metric with a supervised approach, we have to resort to standard distance measures. The Euclidean  2Extending the descriptor with mean units degrades results.  4  Method Training set LY ND HD SIFT 28.1 20.9 24.7  –  Test set  47.6 33.5 41.4 LY 50.0 33.4 42.5 GRBM ND HD 49.0 34.0 41.5 (L1(cid:96)1) LY/ND/HD 48.7 33.5 42.1  37.9 26.9 34.3 LY 40.0 28.0 35.4 spGRBM ND HD 39.1 27.9 34.9 (L1(cid:96)1) LY/ND/HD 37.5 26.6 33.6  31.3 25.1 34.5 LY 34.0 25.6 33.0 mcRBM ND HD 31.2 22.3 25.7 (L1(cid:96)2) LY/ND/HD 30.8 24.8 33.3  Method SIFT BRIEF BRISK SURF  Test set  Training set LY ND HD 31.7 22.8 25.6 – – 59.1 54.5 54.9 79.3 74.8 73.2 – – 54.0 45.5 43.5  LY BinBoost ND (8 bytes) HD  LY ITQ-SIFT ND (8 bytes) HD  LY D-Brief ND (4 bytes) HD  –  –  –  16.9 22.8 18.9  –  20.4 21.6 14.5  31.1 34.4 34.3  –  37.0 37.3 30.5  43.1 47.2 51.3  –  46.2 53.3 43.9  –  –  –  34.7 24.2 38.6 LY 33.3 24.8 44.9 mcRBM ND HD 29.9 22.7 37.6 (JSD) LY/ND/HD 30.0 23.1 39.8  LY mcRBM ND (8 bytes) YM  36.2 39.9 64.9 46.2 34.5 56.1 43.4 37.4 53.0 LY/ND/HD 40.5 36.6 55.4  (a)  (b)  Table 1: Error rates, i.e. the percent of incorrect matches when 95% of the true matches are found. All numbers for GRBM, spGRBM and mcRBMs are given within ±0.5%. Every subtable, indicated by an entry in the Method column, denotes a descriptor algorithm. Descriptor algorithms that do not require learning (denoted by – in the column Training set) are represented by one line. The numbers in the columns labeled LY, ND and HD are the error rates of a method on the respective test set for this scene. Supervised algorithms are not evaluated (denoted by –) on the scene they are trained on. The Training set LY/ND/HD encompasses 1.2 million patches of all three scenes; this setting is only possible for unsupervised learning methods. (a) Error rates for several unsupervised algorithms without restricting the size of the learned representation. GRBM, spGRBM and mcRBM learn descriptors of dimensionality 512. (L1(cid:96)1) denotes that the error rates for a method are with respect to (cid:96)1 normalization of the descriptor under the L1 distance. (b) Results for compact descriptors. BRIEF (32 bytes) [3] and BRISK (64 bytes) [25] are binary descriptors, SURF [1] is a real valued descriptor with 64 dimensions. BinBoost [42], ITQ-SIFT [12] and D-Brief [44] learn compact binary descriptors with supervision. Numbers for BRIEF, BRISK, SURF, BinBoost and ITQ-SIFT are from [42].  distance is widely used when comparing image descriptors. Yet, considering the generative nature of our models we follow the general argumentation of [17] and choose the Manhattan distance, denoted in this text by L1. We also consider two normalization schemes for patch representations, (cid:96)1 and (cid:96)2 (i.e. after a feature vector x is computed, its length is normalized such that (cid:107)x(cid:107)1 = 1 or (cid:107)x(cid:107)2 = 1). Given a visible input both (sp)GRBM and mcRBM compute features that resemble parameters of (conditionally) independent Bernoulli random variables. Therefore we consider the Jensen-Shannon divergence (JSD) [26] as an alternative similarity measure. Finally, for binary descriptors, we use the Hamming distance.  4.2 SIFT Baseline  SIFT [27] (both as interest point detector and descriptor) was a landmark for image feature matching. Because of its good performance it is one of the most important basic ingredients for many different kinds of Computer Vision algorithms. It serves as a baseline for evaluating our models. We use vlfeat [45] to compute the SIFT descriptors.  5  The performance of the SIFT descriptor, (cid:96)1-normalized, is reported (using L1 distance) in Table 1a, ﬁrst entry. (cid:96)1 normalization provides better results than (cid:96)2 normalization or no normalization at all. SIFT performs descriptor sampling at a certain scale relative to the Difference of Gaussians peak. In order to achieve good results, it is essential to optimize this scale parameter [2, Figure 6] on every dataset. Table 1b is concerned with evaluating compact descriptors: the ﬁrst entry shows the performance of SIFT when used as a 128-byte descriptor (i.e. no normalization applied, but again optimized for the best scale parameter) with L1 distance.  4.3 Quantitative analysis  Table 1a shows that SIFT performs better than all three unsupervised methods. mcRBM(256, 512/512) performs similar to SIFT when trained on Half Dome, albeit at the cost of a 4.5 times larger descriptor representation. The compact binary descriptor (the simple binarization scheme is described below) based on mcRBM(64, 576/64) performs remarkably well, comparable or even better than several state-of-the-art descriptors (either manually designed or trained in a supervised manner), see Table 1b, last entry. We discuss in more detail several aspects of the results in the following paragraphs.  GRBM and spGRBM spGRBM performs considerably better than its non-sparse version (see Table 1a, second and third entries). This is not necessarily expected: Unlike e.g. in classiﬁcation [8] sparse representations are considered problematic with respect to evaluating distances directly. Lifetime sparsity may be after all beneﬁcial in this setting compared to strictly enforced population sparsity. We plan to investigate this issue in more detail in future work by comparing spGRBM to Cardinality restricted boltzman machines [38] on this dataset.  Self-taught paradigm We would expect that the performance of a model trained on the Liberty dataset and evaluated on the Notre Dame scene (and vice versa) should be noticeably better than the performance of a model trained on Half Dome and evaluated on the two architectural datasets. However, this is not what we observe. In particular for the mcRBM (both architectures) it is the opposite: Training on the natural scene data leads to much better performance than the assumed optimal setting.  Jensen-Shannon Divergence Both GRBM and spGRBM perform poorly under the Jensen- Shannon divergence similarity (overall error rates are around 60%), therefore we don’t report these numbers in the table. Similar, results for mcRBM under JSD are equally bad. However, if one scales down P by a constant (we found the value of 3 appropriate), the results with respect to JSD improve noticeably, see Table 1a, the last entry. The performance on the Half Dome dataset is still not good – the scaling factor should be learned [9], which we also plan for future work.  Compact binary descriptor We were not successful in ﬁnding a good compact representa- tion with either GRBM or spGRBM. Finding compact representations for any kind of input data should be done with multiple layers of nonlinearities [20]. But even with only two layers (mcRBM(64, 576/64)) we learn relatively good compact descriptors. If features are binarized, the representation can be made even more compact (64 bits, i.e. 8 bytes). In order to ﬁnd a suitable binarization threshold we employ the following simple heuristic: After training on a dataset is ﬁn- ished we histogram all activations (values between 0 and 1) of the training set and use the median of this histogram as the threshold.  4.4 Qualitative analysis  We brieﬂy comment on the developed ﬁlters (Figure 2). Unsurprisingly, spGRBM (Figure 2a) and mcRBM (Figure 2b—these are columns from C) learn Gabor like ﬁlters. At a closer look we make some interesting observations: Figure 2c shows the diagonal elements of Λ1/2 from a spGRBM. When computing a latent representation, the input v is scaled (elementwise) by this matrix, which, visualized as a 2D image, resembles a Gaussian that is dented at the center, the location of the keypoint of every image patch. The mcRBM also builds ﬁlters around the keypoint: Figure 2d shows some unusual ﬁlters from C. They are centered around the keypoint and bear a strong resemblance to discriminative projections (Figure 2e) that are learned in a supervised way on this dataset [2,  6  (a)  (b)  (c)  (d)  (e)  (f)  Figure 2: (a) Typical ﬁlters learned with spGRBM. (b) Filters from an mcRBM. (c) The pixelwise inverted standard deviations learned with a spGRBM plotted as a 2D image (darker gray intensities resemble lower numerical values). An input patch is elementwise multiplied with this image when computing the latent representation. This ﬁgure is generated by training on 32 × 32 patches for better visibility, but the same qualitative results appear with 16 × 16 patches. (d) The mcRBM also learns some variants of log-polar ﬁlters centered around the DoG keypoint. These are very similar to ﬁlters found when optimizing for the correspondence problem in a supervised setting. Several of such ﬁlters are shown in subﬁgure (e), taken from [2, Figure 5]. Finally (f), the basic keypoint ﬁlters are combined with Garbor ﬁlters, if these are placed close to the center; the Garbor ﬁlters get systematically arranged around the keypoint ﬁlters.  Figure 5]. Qualitatively, the ﬁlters in Figure 2d resemble log-polar ﬁlters that are used in several state-of-the-art feature designs [28]. The very focused keypoint ﬁlters (ﬁrst column in Figure 2d) are often combined with Gabor ﬁlters placed in the vicinity of the center – the Garbor ﬁlters appear on their own, if they are too far from the center. If an mcRBM is trained with a ﬁxed topography for P , one sees that the Gabor ﬁlters get systematically arranged around the keypoint (Figure 2f).  4.5 Other models  We also trained several other unsupervised feature learning models: GRBM with nonlinear rectiﬁed hidden units3 [30], various kinds of autoencoders (sparse [7] and denoising [46] autoencoders), K-  3Our experiments indicate that rmsprop is in this case also beneﬁcial with respect to the ﬁnal results: It  learns models that perform about 2-3% better than those trained with stochastic gradient descent.  7  means [7] and two layer models (stacked RBMs, autoencoders with two hidden layers, cRBM [34]). None of these models performed as good as the spGRBM.  5 Conclusion  We start this paper suggesting that unsupervised feature learning should be evaluated (i) without us- ing subsequent supervised algorithms and (ii) more directly with respect to its capacity to ﬁnd good low-level image descriptors. A recently introduced dataset for discriminatively learning low-level local image descriptors is then proposed as a suitable benchmark for such an evaluation scheme that complements nicely the existing benchmarks. We demonstrate that an mcRBM learns real-valued and binary descriptors that perform comparably or even better to several state-of-the-art methods on this dataset. In future work we plan to evaluate deeper architectures [20], combined with sparse convolutional features [18] on this dataset. Moreover, ongoing work investigates several algorithms [4, 37] for supervised correspondence learning on the presented dataset.  References [1] H. Bay, T. Tuytelaars, and L. Van Gool. Surf: Speeded up robust features. In Proc. ECCV, 2006. [2] M. Brown, G. Hua, and S. Winder. Discriminative learning of local image descriptors. IEEE PAMI, 2010. [3] M. Calonder, V. Lepetit, M. Ozuysal, T. Trzcinski, C. Strecha, and P. Fua. Brief: Computing a local binary descriptor very fast. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(7):1281–1298, 2012.  [4] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to  face veriﬁcation. In Proc. CVPR, 2005.  [5] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.  In Proc. CVPR, 2012.  [6] A. Coates, A. Karpathy, and A. Ng. Emergence of object-selective features in unsupervised feature  learning. In Proc. NIPS, 2012.  [7] A. Coates, H. Lee, and A. Ng. An analysis of single-layer networks in unsupervised feature learning. In  Proc. AISTATS, 2011.  [8] A. Coates and A. Ng. The importance of encoding versus training with sparse coding and vector quanti-  zation. In Proc. ICML, 2011.  [9] G. Dahl, M. Ranzato, A. Mohamed, and G. Hinton. Phone recognition with the mean-covariance restricted  boltzmann machine. In Proc. NIPS, 2010.  [10] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understand- ing, 2007.  [11] Y. Freund and D. Haussler. Unsupervised learning of distributions on binary vectors using two layer  networks. Technical report, University of California, Santa Cruz, 1994.  [12] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin. Iterative quantization: A procrustean approach to  learning binary codes for large-scale image retrieval. Pattern Analysis and Machine Intelligence, 2012.  [13] G. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,  14(8):1771–1800, 2002.  [14] G. Hinton. A practical guide to training restricted boltzmann machines. Technical report, University of  Toronto, 2010.  [15] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,  313(5786):504–507, 2006.  [16] G. Huang, M. Mattar, H. Lee, and E. Learned-Miller. Learning to align from scratch. In Proc. NIPS,  2012.  [17] Y. Jia and T. Darrell. Heavy-tailed distances for gradient based image descriptors. In Proc. NIPS, 2011. [18] K. Kavukcuoglu, P. Sermanet, Y. Boureau, K. Gregor, M. Mathieu, and Y. LeCun. Learning convolutional  feature hierarchies for visual recognition. In Proc. NIPS, 2010.  [19] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of  Toronto, 2009.  8  [20] A. Krizhevsky and G. Hinton. Using very deep autoencoders for content-based image retrieval. In Proc.  ESANN, 2011.  [21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural net-  works. In Proc. NIPS, 2012.  [22] Q. Le, R. Monga, M. Devin, G. Corrado, K. Chen, M. Ranzato, J. Dean, and A. Ng. Building high-level  features using large scale unsupervised learning. In Proc. ICML, 2012.  [23] Y. LeCun, F. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to  pose and lighting. In Proc. CVPR, 2004.  [24] V. Lepetit and P. Fua. Keypoint recognition using randomized trees. IEEE PAMI, 28(9):1465–1479, 2006. [25] S. Leutenegger, M. Chli, and R. Siegwart. Brisk: Binary robust invariant scalable keypoints. In Proc.  ICCV, 2011.  [26] J. Lin. Divergence measures based on the shannon entropy. Information Theory, IEEE Transactions on,  37(1):145–151, 1991.  [27] D. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer  Vision, 60(2):91–110, 2004.  [28] K. Mikolajczyk and C. Schmid. A performance evaluation of local descriptors. IEEE PAMI, 2005. [29] V. Nair and G. Hinton. 3-d object recognition with deep belief nets. In Proc. NIPS, 2009. [30] V. Nair and G. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Proc. ICML,  2010.  [31] R. Neal. Probabilistic inference using markov chain monte carlo methods. Technical report, University  of Toronto, 1993.  [32] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng. Self-taught learning: Transfer learning from unlabeled  data. In Proc. ICML, 2007.  [33] M. Ranzato and G. Hinton. Modeling pixel means and covariances using factorized third-order boltzmann  machines. In Proc. CVPR, 2010.  [34] M. Ranzato, A. Krizhevsky, and G. Hinton. Factored 3-way restricted boltzmann machines for modeling  natural images. In Proc. AISTATS, 2010.  [35] M. Ranzato, V. Mnih, and G. Hinton. Generating more realistic images using gated mrf’s. In Proc. NIPS,  2010.  [36] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning,  2008.  [37] J. Susskind, R. Memisevic, G. Hinton, and M. Pollefeys. Modeling the joint density of two images under  a variety of transformations. In Proc. CVPR, 2011.  [38] K. Swersky, D. Tarlow, I. Sutskever, R. Salakhutdinov, R. Zemel, and R. Adams. Cardinality restricted  boltzmann machines. In Proc. NIPS, 2012.  [39] Y. Tang and A. Mohamed. Multiresolution deep belief networks. In Proc. AISTATS, 2012. [40] T. Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient. In  Proc. ICML, 2008.  [41] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent  magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  [42] T. Trzcinski, M. Christoudias, P. Fua, and V. Lepetit. Boosting binary image descriptors. Technical report,  EPFL, 2012.  [43] T. Trzcinski, M. Christoudias, V. Lepetit, and P. Fua. Learning image descriptors with the boosting-trick.  In Proc. NIPS, 2012.  [44] T. Trzcinski and V. Lepetit. Efﬁcient discriminative projections for compact binary descriptors. In Proc.  ECCV, 2012.  [45] A. Vedaldi and B. Fulkerson. Vlfeat: An open and portable library of computer vision algorithms. In  Proceedings of the International Conference on Multimedia, 2010.  [46] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol. Extracting and composing robust features with  denoising autoencoders. In Proc. ICML, 2008.  [47] S. Winder and M. Brown. Learning local image descriptors. In Proc. CVPR, 2007.  9  6 Appendix  6.1 Gaussian-Binary Restricted Boltzmann Machine  The Gaussian-Binary Restricted Boltzmann Machine (GRBM) is an extension of the Binary-Binary RBM [11] that can handle continuous data [15, 39]. It is a bipartite Markov Random Field over a set of visible units, v ∈ RNv, and a set of hidden units, h ∈ {0, 1}Nh. Every conﬁguration of units v and units h is associated with an energy E(v, h), deﬁned as  1 2  vT Λv − vT Λa − hT b − vT ΛW h  E(v, h; θ) =  (1) with θ = (W ∈ RNv×Nh , a ∈ RNv , b ∈ RNh, Λ ∈ RNv×Nv ), the model parameters. W rep- resents the visible-to-hidden symmetric interaction terms, a and b represent the visible and hidden biases respectively and Λ is the precision matrix of v, taken to be diagonal. E(v, h) induces a probability density function over v and h:  exp(cid:0)−E(v, h; θ)(cid:1) where Z(θ) is the normalization partition function, Z(θ) =(cid:82)(cid:80)  p(v, h; θ) =  Z(θ)  (2)  Learning the parameters θ is accomplished by gradient ascent in the log-likelihood of θ given N i.i.d. training samples. The log-probability of one training sample is  h exp(cid:0)−E(v, h; θ)(cid:1) dv. (cid:33)(cid:33)  log p(v) = − 1 2  vT Λv + vT Λa +  Nh(cid:88)  j  (cid:32)  (cid:32) Nv(cid:88)  i  log  1 + exp  1  vT  i (Λ  2 W )ij + bj  − Z(θ)  (3)  Evaluating Z(θ) is intractable, therefore algorithms like Contrastive Divergence (CD) [13] or per- sistent CD (PCD) [40] are used to compute an approximation of the log-likelihood gradient. The bipartite nature of an (G)RBM is an important aspect when using these algorithms: The visible units are conditionally independent given the hidden units. They are distributed according to a diagonal Gaussian:  p(v | h) ∼ N (Λ− 1  2 W h + a, Λ−1)  (4)  Similarly, the hidden units are conditionally independent given the visible units. The conditional distribution can be written compactly as  p(h | v) = σ(vT Λ  1 2 W + b)  (5)  where σ denotes the element-wise logistic sigmoid function, σ(z) = 1/(1 + e−z).  6.2 Sparse GRBM  In many tasks it is beneﬁcial to have features that are only rarely active [29, 8]. Sparse activation of a binary hidden unit can be achieved by specifying a sparsity target ρ and adding an additional penalty term to the log-likelihood objective that encourages the actual probability of unit j of being active, qj, to be close to ρ [29, 14]. This penalty is proportional to the negative KL divergence between the hidden unit marginal qj = 1 N  (cid:80) n p(hj = 1 | vn) and the target sparsity:  (cid:0)ρ log qj + (1 − ρ) log(1 − qj)(cid:1),  λsp  (6)  where λsp represents the strength of the penalty. This term enforces sparsity of feature j over the training set, also referred to as lifetime sparsity. The hope is that the features for one training sample are then encoded by a sparse vector, corresponding to population sparsity. We denote a GRBM with a sparsity penalty λsp > 0 as spGRBM.  10  6.3 Mean-Covariance Restricted Boltzmann Machine  In order to model pairwise dependencies of visible units gated by hidden units, a third-order RBM can be deﬁned with a weight wijk for each triplet vi, vj, hk. By factorizing and tying these weights, parameters can be reduced to a ﬁlter matrix C ∈ RNv×F and a pooling matrix P ∈ RF×Nh. C connects the input to a set of factors and P maps factors to hidden variables. The energy function for this cRBM [34] is (7) where (·)2 denotes the element-wise square operation and θ = {C, P , c}. Note that P has to be non-positive [34, Section 5]. The hidden units of the cRBM are still conditionally independent given the visible units, so inference remains simple. Their conditional distribution (given visible state v) is  Ec(v, hc; θ) = −(vT CT )2P hc − cT hc  (8) The visible units are coupled in a Markov Random Field determined by the setting of the hidden units:  p(hc | v) = σ(P T (CT v)2 + c)  p(v | hc) ∼ N (0, Σ)  (9)  with  (10) As equation 9 shows, the cRBM can only model Gaussian inputs with zero mean. For general Gaussian-distributed inputs the cRBM and the GRBM can be combined into the mean-covariance RBM (mcRBM) by simply adding their respective energy functions:  Σ−1 = Cdiag(−P hc)CT  Emc(v, hm, hc; θ, θ(cid:48)) = Em(v, hm; θ) + Ec(v, hc, θ(cid:48))  (11) Em(v, hm; θ) denotes the energy function of the GRBM (see eq. 1) with Λ ﬁxed to the identity matrix. The resulting conditional distribution over the visible units, given the two sets of hidden units hm (mean units) and hc (covariance units) is  p(v | hm, hc) ∼ N (ΣW hm, Σ)  (12) with Σ deﬁned as in eq. 10. The conditional distributions p(hm|v) and p(hc|v) are still as in eq. 5 and eq. 7 respectively. The parameters θ, θ(cid:48) can again be learned using approximate Maximum Likelihood Estimation, e.g. via CD or PCD. These methods require to sample from p(v|hm, hc), which involves an expensive matrix inversion (see eq. 10). Instead, samples are obtained by using Hybrid Monte Carlo (HMC) [31] on the mcRBM free energy [33].  11  ","Unsupervised feature learning has shown impressive results for a wide rangeof input modalities, in particular for object classification tasks in computervision. Using a large amount of unlabeled data, unsupervised feature learningmethods are utilized to construct high-level representations that arediscriminative enough for subsequently trained supervised classificationalgorithms. However, it has never been \emph{quantitatively} investigated yethow well unsupervised learning methods can find \emph{low-levelrepresentations} for image patches without any additional supervision. In thispaper we examine the performance of pure unsupervised methods on a low-levelcorrespondence task, a problem that is central to many Computer Visionapplications. We find that a special type of Restricted Boltzmann Machines(RBMs) performs comparably to hand-crafted descriptors. Additionally, a simplebinarization scheme produces compact representations that perform better thanseveral state-of-the-art descriptors."
1212.3767,2013,Visual Objects Classification with Sliding Spatial Pyramid Matching  ,"['Hao Wooi Lim', 'Yong Haur Tay']",https://arxiv.org/pdf/1212.3767.pdf,"e r e h     s e o g  2 1 0 t 2   c x e D e 8 t 1 V r C e . s c h [     2 t v 7 o 6 7 3      ]  .  e 2 1 m 2 1 : v i o X r s a  : v i X r a  Visual Objects Classiﬁcation with Sliding Spatial  Pyramid Matching  Hao Wooi Lim1 and Yong Haur Tay2  1 haowooilim@1utar.my, 2 tayyh@utar.edu.my  Computer Vision & Intelligent Systems (CVIS) group, Faculty of Engineering and Science,  University of Tunku Abdul Rahman  Abstract  We present a method for visual object classiﬁcation using only a single feature, transformed color SIFT [15] with a variant of Spatial Pyramid Matching (SPM) that we called Sliding Spatial Pyramid Matching (SSPM), trained with an ensemble of linear regression (provided by LINEAR) to obtained state of the art result on Caltech-101 [22] of 83.46%. SSPM is a special version of SPM where instead of dividing an image into K number of regions, a subwindow of ﬁxed size is slide around the image with a ﬁxed step size. For each subwindow, a histogram of visual words is generated. To obtained the visual vocabulary, instead of performing K-means clustering [26], we randomly pick N exemplars from the training set and encode them with a soft non-linear mapping method from [16]. We then trained 15 models, each with a diﬀerent visual word size with linear regression. All 15 models are then averaged together to form a single strong model.  1  Introduction  Recently, there has been a lot of development in the area of Deep Learning (DL). Not unlike traditional Multi-layer Neural Networks, DL sought to solve the diﬃcult problems such as visual objects classiﬁcation by utilizing many layers of neurons, which has been traditionally too computationally intensive to train, with the help of fast, many-cores Graphics Processing Unit (GPU) to help speedup training. One interesting outcome of the research in DL is such that, DL allows for learning of feature detectors, thus mostly eliminating the need for hand- engineered features. And sure enough, recent papers such as [18] has shown that DL could even obtained state of the art results on various visual object data set such as MNIST [19], NIST SD 19 [20], CIFAR-10 [21] and etc. However, noticed that most of the data set tested are usually consists of very small images. This is due to the fact that data set with huge dimensionality would pose an even bigger challenge for GPU, sometimes even requiring weeks of training, even with a cluster of GPUs. Moreover, not everyone has a huge cluster of GPUs at their disposal.  Looking at some of the best-performing methods ([1], [4], [8], [9]), shown on Fig. 1 on Caltech-101 [22], one trend seems apparent: many diﬀerent features are usually employed to obtained the published results, most notably Scale-invariant feature transform (SIFT) [23], Pyramid histogram of orientated gradients (PHOG) [9], Self-similarity (SSIM) [25], Geometric Blur (GB) [24] and etc. While it is easy to obtain better accuracy by employing many diﬀerent features, it signiﬁcantly increases the computation power required to train them.  1  e r e h  s e o g  t x e t  r e h t o  e m o s  : v i X r a  Figure 1: Figure compares the accuracy on Caltech-101 data set, all trained on 30 samples.  Some algorithm like kernel-based Support Vector Machine (SVM) which requires O(nd) testing time, where n is the number of support vectors and d is the dimensionality of the feature vector [27], it is just not feasible when the model is complex, resulting in large number of support vectors or when the feature dimension is large. Unfortunately, it is pretty easy to feed a lot of features for training, allowing the training process to discover salient features useful for accurate classiﬁcation. Thus, in most cases, it is diﬃcult to obtain good results with kernel-based SVM without also making training intractable.  In this paper, our goal is to lower the error bound siginiﬁcantly on challenging visual object data set such as Caltech-101 [22] that has high enough number of categories and dimensionality, while also employing methods that are much more eﬃcient to train than DL-based methods, relying on hand-engineered feature but refrained from combining many diﬀerent kind of features so that training remains quick and eﬃcient.  2 Our method  Here, we describe our method, along with the best parameters that we have found.  Our method begins with extraction of features. Following the method employed by [15], we use Harris-Laplace salient point detector to localize all the local keypoints in each image. For each keypoint, we use the Transformed Color SIFT as the descriptor.  To obtained a visual vocabulary, we randomly pick N exemplars from the training set instead of performing K-means clustering [26], which was the usual method employed by most paper.  To encode keypoint into feature vector, we encode them with a soft non-linear mapping  method as described in [16].  Next, we use a special version of Spatial Pyramid Matching (SPM) where instead of dividing an image into K number of regions whereby for each region, a histogram of visual words is generated, a subwindow of ﬁxed size is slide around the image with ﬁxed step size.  2  e r e h  s e o g  t x e t  r e h t o  e m o s  : v i X r a  Each subwindow then, is process the same as you would with a region in SPM. We call this Sliding Spatial Pyramid Matching (SSPM). Here, we set the width and height of the subwindow to be one-third of the original image’s width and height respectively, while step size x and y to be the one-third of the subwindow’s width and height respectively.  Since for each region we obtained a feature vector with size equal to the visual vocabulary size, we append each feature vector from each region into a single feature vector to be trained by linear regression.  To train the system, we used LIBLINEAR [17] for this purpose (Due to the high dimen- sionality of our feature vector, kernel-based SVM is unfeasible). We choose L2-regularized logistic regression (primal) with probabilistic output.  Finally, to improve the accuracy, we trained multiple models and combined them by multiplying together the probabilistic output to obtained the ﬁnal probability distribution over class label.  3 Data sets and Experimental Protocol  We conduct our experiment on Caltech-101 [22]. The data set consists of images from 101 object categories, and contains from 31 to 800 images per category. Most images are medium resolution, about 300 × 300 pixels. The signiﬁcance of this database is its large inter-class variability. Following standard procedures, the Caltech-101 data is split into 30 training im- ages (chosen randomly) per category and 50 for testing. BACKROUND GOOGLE category was not included in the experiment. The classiﬁcation process is repeated 4 times (changing the training and test sets) and the average performance score is reported.  4 Results  We study the inﬂuence of a variable visual vocabulary size (N = {1000, 900, ..., 100}) or ﬁxed visual vocabulary size (N = 1000) on several models that was then averaged together to obtain a ﬁnal result. Note that each model has a diﬀerent visual vocabulary due to the fact that visual word is obtained with randomly chosen exemplars. To speedup experiment, SIFT is used in this case.  As shown in Tab. 1, variable visual word seems to performed slightly better. This makes sense, when combining several models together, models that are diﬀerent from each other tends to performed better after being combined.  We then experiment with diﬀerent step size and subwindow size. For method A, we set the width and height of the subwindow to be half of the original image’s width and height respectively, while step size x and y to be the half of the subwindow’s width and height respectively. For method B, we set the width and height of the subwindow to be one-third of the original image’s width and height respectively, while step size x and y to be the one-third of the subwindow’s width and height respectively. As one might exect, method B would result in features with higher dimension compared to method A, due to the fact that method B generates more regions. To speedup experiment, SIFT is used in this case.  As shown in Tab. 2, method B seems to be performing better, with almost negligible increase in training time due to the higher dimension feature vector generated. Interestingly,  3  e r e h  s e o g  t x e t  r e h t o  e m o s  : v i X r a  Number of models Fixed visual vocabulary size Variable visual vocabulary size  1 2 3 4 5 6 7 8 9 10  71.13% 73.18% 73.37% 73.67% 73.61% 73.65% 73.67% 73.80% 73.69% 73.78%  70.92% 72.95% 72.70% 72.97% 73.69% 73.57% 73.54% 73.71% 73.48% 73.84%  Table 1: Table compares the accuracy using variable visual vocabulary size vs ﬁxed visual vocabulary size.  Number of models Method A Method B  1 2 3 4 5 6 7 8 9 10  70.92% 72.95% 72.70% 72.97% 73.69% 73.57% 73.54% 73.71% 73.48% 73.84%  72.78% 73.95% 74.65% 74.90% 75.03% 75.45% 75.30% 75.64% 75.41% 75.47%  Table 2: Table compares the accuracy using smaller subwindow size vs larger subwindow size.  it also seems to show there is diminishing returns with averaging more models. Once more than 8 models are averaged together, accuracy drops slightly.  Next, we determine the best performing local keypoint descriptors. We test SIFT, HSV-  SIFT, Opponent-SIFT and Transformed Color SIFT.  As shown in Tab. 3, color variants of SIFT seems to be performed signiﬁcantly better  than SIFT, indicating that color information is a pretty strong discriminating factor.  Finally, we investigate if combining more models would improve the accuracy even fur- ther. In method C, we combine 10 models and set visual vocabulary size to Eq. 1, while in method D, we combine 15 models and set visual vocabulary size to Eq. 2. In both cases, we used Transformed Color Sift as the local keypoint descriptor.  N = 1000, 1000, 800, 800, ..., 200, 200  N = 1000, 1000, 1000, 800, 800, 800, ..., 200, 200, 200  (1)  (2)  4  e r e h  s e o g  t x e t  r e h t o  e m o s  : v i X r a  Number of models  1 2 3 4 5 6 7 8 9 10  SIFT HSV-SIFT Opponent-SIFT Transformed color SIFT 72.27% 73.97% 74.90% 75.20% 75.32% 75.15% 75.49% 75.66% 75.62% 75.85%  79.03% 81.38% 82.44% 82.93% 82.99% 82.93% 82.76% 82.95% 83.12% 83.14%  74.22% 76.64% 77.44% 77.89% 77.91% 78.01% 78.20% 78.88% 78.86% 78.94%  78.18% 79.7% 80.72% 81.1% 81.70% 82.10% 81.9% 81.99% 82.27% 82.44%  Table 3: Table compares the accuracy using diﬀerent local keypoint descriptors.  Method C Method D 83.46%  83.13%  Table 4: Table compares the accuracy using diﬀerent number of models.  As shown in Tab. 4, perhaps unsurprisingly, combining more models did help to im- prove the accuracy. We tried increasing the number more, unfortunately, we hit a point of diminishing returns.  On a ﬁnal note, the reason having more models help obtained a higher accuracy, is most probably due to the fact that since visual words are obtained randomly, each model’s feature was encoded with a diﬀerent set of visual word, thereby giving each model a diﬀerent viewpoint of the problem.  In terms of training time, the test program which was coded in C++ and speed up with OpenMP to take advantage of all 4 cores, took about 5 hours to train for the 10-model classiﬁer and about 8 hours for the 15-model classiﬁer on a single quad-core Intel Core i5-2500K machine.  5 Conclusion & Future works  To conclude, we have presented our proposed visual object classiﬁcation method that utilizes only a single best-performing local keypoint descriptor (Transformed color SIFT) while being eﬃcient to train. We managed to obtained state of the art result on Caltech-101 of 83.46%, showing the potential of such architecture. We suspect better results might be possible if multiple models utilizes diﬀerent type local keypoint descriptor. However, it is unclear yet if signiﬁcantly better results would be possible or if it would slow down training too much.  References  [1] J. Yang, Y. Li., Y. Tian, L. Duan and W. Gao. Group sensitive multiple kernel learning  for object categorization. In ICCV, 2009  5  e r e h  s e o g  t x e t  r e h t o  e m o s  : v i X r a  [2] S. Todorovic and N. Ahuja. Learning subcategory relevances for category recognition.  In CVPR, 2008  [3] F. Li, J. Carreira and C. Sminchisescu. Object Recognition as Ranking Holistic Figure-  ground Hypotheses. In CVPR, 2010  [4] A. Bosch, A. Zisserman and X. Munoz. Image classiﬁcation using random forests and  ferns. In ICCV, 2007  [5] S. McCann and D. G. Lowe. Spatially Local Coding for Object Recognition. In ACCV,  2012  [6] Gabriel L. Oliveira, Erickson R. Nascimento, Antonio W. Vieira and Mario F. M. Cam- pos. Sparse spatial coding: A novel approach for eﬃcient and accurate object recogni- tion. In ICRA, 2012  [7] O. Boiman, E. Shechtman and M. Irani. In defense of nearest-neighbor based image  classiﬁcation. In CVPR, 2008  [8] P. Gehler and S. Nowozin. On Feature Combination for Multiclass Object Classiﬁcation.  In ICCV, 2009  [9] A. Bosch, A. Zisserman and X. Munoz. Representing shape with a spatial pyramid  kernel. In CIVR, 2007  [10] K. Chatﬁeld, V. Lempitsky, A. Vedaldi and A. Zisserman. The devil is in the details:  an evaluation of recent feature encoding method. In BMVC, 2011  [11] L. Bo, K. Lai, X. Ren and D. Fox. Object recognition with hierarchical kernel descrip-  tors. In CVPR, 2011  [12] L. Bo, X. Ren and D. Fox. Kernel Descriptors for Visual Recognition. In NIPS, 2010  [13] Y. Boureau, F. Bach, Y. LeCun and J. Ponce. Learning mid-level features for recogni-  tion. In CVPR, 2010  [14] T. Jia, C. Huang and T. Darrell. Beyond spatial pyramids: Receptive ﬁeld learning for  pooled image features. In CVPR, 2012  [15] Koen E. A. van de Sande, Theo Gevers and Cees G. M. Snoek. Evaluating Color De-  scriptors for Object and Scene Recognition. In PAMI, 2010  [16] Adam Coates, Honglak Lee and Andrew Ng. An analysis of single-layer networks in  unsupervised feature learning. In NIPS, 2010  [17] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang and C.-J. Lin. LIBLINEAR: A library  for large linear classiﬁcation. In JMLR, 2008.  [18] D. C. Ciresan, U. Meier and J. Schmidhuber. Multi-column deep neural networks for  image classiﬁcation. In CVPR, 2012  6  e r e h  s e o g  t x e t  r e h t o  e m o s  : v i X r a  [19] Y. LeCun, L. Bottou, Y. Bengio and P. Haﬀner. Gradient-based learning applied to  document recognition. In Proceedings of the IEEE, 86(11):22782324, November 1998  [20] P. J. Grother. NIST special database 19 - Handprinted forms and characters database.  Technical report, National Institute of Standards and Thechnology (NIST), 1995  [21] A. Krizhevsky. Learning multiple layers of features from tiny images. Masters thesis,  Computer Science Department, University of Toronto, 2009  [22] L. Fei-Fei, R. Fergus and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE CVPR Workshop of Generative Model Based Vision, 2004  [23] D. G. Lowe. Object recognition from local scale-invariant features. In ICCV, 1999  [24] A. C. Berg, T. L. Berg and J. Malik. Shape matching and object recognition using low  distortion correspondences. In CVPR, 2005  [25] E. Shechtman and M. Irani. Matching local self-similarities across images and videos.  In CVPR, 2007  [26] K. Alsabti, S. Ranka and V. Singh. An eﬃcient k-means clustering algorithm. In Pro- ceedings of the First Workshop on High Performance Data Mining, Orlando, FL, 1998  [27] H. Cao, T. Naito and Y. Ninomiya. Approximate RBF kernel SVM and its applications in pedestrian classiﬁcation. In 1st International Workshop on Machine Learning for Vision-based Motion Analysis, 2008.  7  ","We present a method for visual object classification using only a singlefeature, transformed color SIFT with a variant of Spatial Pyramid Matching(SPM) that we called Sliding Spatial Pyramid Matching (SSPM), trained with anensemble of linear regression (provided by LINEAR) to obtained state of the artresult on Caltech-101 of 83.46%. SSPM is a special version of SPM where insteadof dividing an image into K number of regions, a subwindow of fixed size isslide around the image with a fixed step size. For each subwindow, a histogramof visual words is generated. To obtained the visual vocabulary, instead ofperforming K-means clustering, we randomly pick N exemplars from the trainingset and encode them with a soft non-linear mapping method. We then trained 15models, each with a different visual word size with linear regression. All 15models are then averaged together to form a single strong model."
1301.3537,2013,Learning Stable Group Invariant Representations with Convolutional Networks  ,"['Joan Bruna', 'Arthur Szlam', 'Yann LeCun']",https://arxiv.org/pdf/1301.3537.pdf,"3 1 0 2     n a J    6 1      ] I  A . s c [      1 v 7 3 5 3  .  1 0 3 1 : v i X r a  Learning Stable Group Invariant Representations  with Convolutional Networks  Joan Bruna, Arthur Szlam and Yann LeCun  Courant Institute  New York University New Nork, NY, 10013  {bruna,lecun}@cims.nyu.edu  1 Introduction  Many signal categories in vision and auditory problems are invariant to the action of transformation groups, such as translations, rotations or frequency transpositions. This property motivates the study of signal representations which are also invariant to the action of these transformation groups. For instance, translation invariance can be achieved with a registration or with auto-correlation measures.  Transformation groups are in fact low-dimensional manifolds, and therefore mere group invariance is in general not enough to efﬁciently describe signal classes. Indeed, signals may be perturbed with additive noise and also with geometrical deformations, so one can then ask for invariant representa- tions which are stable to these perturbations. Scattering convolutional networks [1] construct locally translation invariant signal representations, with additive and geometrical stability, by cascading complex wavelet modulus operators with a lowpass smoothing kernel. By deﬁning wavelet decom- positions on any locally compact Lie Group, scattering operators can be generalized and cascaded to provide local invariance with respect to more general transformation groups [2, 3]. Although such transformation groups are present across many recognition problems, they require prior information which sometimes cannot be assumed.  Convolutional networks [4] cascade ﬁlter banks with point-wise nonlinearities and local pooling operators. By remapping the output of each layer with the input of the following one, the trainable ﬁlters implement convolution operators. We show that the invariance properties built by deep convo- lutional networks can be cast as a form of stable group invariance. The network wiring architecture determines the invariance group, while the trainable ﬁlter coefﬁcients characterize the group action.  Deep convolutional architectures cascade several layers of convolutions, non-linearities and pooling. These architectures have the capacity to generate local invariance to the action of more general groups. Under appropriate conditions, these groups can be factorized as products of smaller groups. Each of these factors can then be associated with a subset of consecutive layers of the convolutional network. In these conditions, the invariance properties of the ﬁnal representation can be studied from the group structure generated by each layer.  2 Problem statement  2.1 Stable Group Invariance  A transformation group G acts on the input space X (assumed to be a Hilbert space) with a linear group action (g, x) 7→ g.x ∈ X , which is compatible with the group operation. A signal representation Φ : X −→ Z is invariant to the action of G if ∀ g ∈ G , x ∈ X , Φ(g.x) = Φ(x) . However, mere group invariance is in general too weak, due to the presence of a much larger, high dimensional variability which does not belong to the low-dimensional group. It is then necessary to incorporate the notion of outer “deformations” with another group action ϕ : H ×  1  X −→ X , where H is a larger group containing G. The geometric stability can be stated with a Lipschitz continuity property  kΦ(ϕ(h, x)) − Φ(x)k ≤ C kxk k(h, G) ,  (1) where k(h, G) measures the “distance” from h to the invariance group G. For instance, when G is the translation group of Rd and H ⊃ G is the group of C2 diffeomorphisms of Rd, then ϕ(h, x) = x ◦ h and one can select as distance the elastic deformation metric k(h, G) := |∇τ |∞ + |Hτ |∞, where τ (u) = h(u) − u [2]. Even though the group invariance formalism describes global invariance properties of the represen- tation, it also provides a valid and useful framework to study local invariance properties. Indeed, if one replaces (1) by  kΦ(ϕ(h, x)) − Φ(x)k ≤ C kxk (khGkG + k(h, G)) ,  (2) where hG is a projection of h to G and kgkG is a metric on G measuring the amount of transforma- tion being applied, then the local invariance is expressed by adjusting the proportionality between the two metrics.  2.2 Convolutional Networks  A generic convolutional network deﬁned on a space X = L2(Ω0) of square-integrable signals starts with a ﬁlter bank {ψλ}λ∈Λ1, ψλ ∈ L1(Ω0) ∀λ, which for each input x(u) ∈ X produces the collection  z(1)(u, λ) = x ⋆ ψλ(u) =Z x(u − v)ψλ(v)dv , u ∈ Ω0 , λ ∈ Λ1 .  If the ﬁlter bank deﬁnes a stable, invertible frame, then there exist two constants a, A > 0 such that  where kz(1)k2 = Pλ∈Λ1 kz(1)(·, λ)k2. By deﬁning Ω1 = Ω0 × Λ1, the ﬁrst layer of the network  can be written as the linear mapping  ∀x , akxk ≤ kz(1)k ≤ Akxk ,  F1 : L2(Ω0) −→ L2(Ω1)  x(u)  7−→ z(1)(u, λ) .  z(1) is then transformed with a point-wise nonlinear operator M : L2(Ω) → L2(Ω) which is usually non-expansive, meaning that kM zk ≤ kzk. Finally, a local pooling operator P can be deﬁned as any linear or nonlinear operator  which reduces the resolution of the signal along one or more coordinates and which avoids “alias- ing”. If Ω = Ω0 × Λ1, , ×Λk and (2J0 , , 2Jk ) denote the loss of resolution along each coordinate, it  P : L2(Ω) −→ L2(eΩ)  results that eΩ = fΩ0 ×fΛ1, , ×fΛk, with |fΩ0| = 2−αJ0|Ω0|, |fΛi| = 2−αJi|Λi|, where α is an oversam-  pling factor. Linear pooling operators are implemented as lowpass ﬁlters φJ (u, λ1, , λm) followed by a downsampling.  Then, a k-layer convolutional network is a cascade  L2(Ω0)  M ◦F1−→ L2(Ω1)  M ◦F2−→ L2(Ω2) · · ·  (3)  P1−→ L2(fΩ1)  Pk−→ L2(fΩk) ,  which produces successively z(1), z(2), . . . , z(k). The ﬁlter banks (Fi)i≤k, together with the pooling operators (Pi)i≤k, progressively transform the signal domain; ﬁlter bank steps lift the domain of deﬁnition by adding new coordinates, whereas pooling steps reduce the resolution along certain coordinates.  3 Invariance Properties of Convolutional Networks  3.1 The case of one-parameter transformation groups  Let us start by assuming the simplest form of variability produced by a transformation group. A one-parameter transformation group is a family {Ut}t∈R of unitary linear operators of L2(Ω) such  2  that (i) t 7→ Ut it is strongly continuous: limt→t0 Utz = Ut0 z for every z ∈ L2(Ω), and (ii) Ut+s = UtUs. One parameter transformation groups are thus homeomorphic to R (with the addition as group operation), and deﬁne an action which is continuous in the group variable. Uni-dimensional translations Utx(u) = x(u − tv0), frequency transpositions Utx = F −1(F x(ω − tω0)) (where F, F −1 are respectively the forward and inverse Fourier transform) or unitary dilations Utx(u) = 2−t/2x(2−tu) are examples of one-parameter transformation groups. One-parameter transformation groups are particularly simple to study thanks to Stone’s theorem [5], which states that unitary one-parameter transformation groups are uniquely generated by a complex exponential of a self-adjoint operator:  Ut = eitA , t ∈ R .  Here, the complex exponential of a self-adjoint operator should be interpreted in terms of its spec- tra. In the ﬁnite dimensional case (when Ω is discrete), this means that there exists an orthogonal transform O such that if ˆz(ω) = Oz, then  ∀ z , Utz = O−1diag(eitω.ˆz(ω)) .  (4) In other words, the group action can be expressed as a linear phase change in the basis which diagonalizes the unique self-adjoint operator A given by Stone’s theorem. In the particular case of translations, the change of basis O is given by the Fourier transform. As a result, one can obtain a representation which is invariant to the action of {Ut}t with a single layer of a neural network: a linear decomposition which expresses the data in the basis given by O followed by a point-wise complex modulus. In the case of the translation group, this corresponds to taking the modulus of the Fourier transform.  3.2 Presence of deformations  Stone’s theorem provides a recipe for global group invariance for strongly continuous group actions. Without noise nor deformations, an invariant representation can be obtained by taking complex moduli in a basis which diagonalizes the group action, which can be implemented in a shallow 1- layer architecture. However, the underlying low-dimensional assumption is rarely satisﬁed, due to the presence of more complex forms of variability.  This complex variability can be modeled as follows. If O is the basis which diagonalizes a given one- parameter group, then the group action is expressed in the basis F −1O as the translation operator Tsz(u) = z(u − s). Whereas the group action consists in rigid translations on this basis, by analogy a deformation is deﬁned as a non-rigid warping in this domain: Lτ z(u) = z(u − τ (u)), where τ is a displacement ﬁeld along the indexes of the decomposition.  The amount of deformation can be measured with the regularity of τ (u), which controls how dis- tant the warping is from being a rigid translation and hence an element of the group. This suggests that, in order to obtain stability to deformations, rather than looking for eigenvectors of the in- ﬁnitesimal group action, one should look for linear measurements which are well localized in the domain where deformations occur, and which nearly diagonalize the group action. In particular, these measurements can be implemented with convolutions using compactly supported ﬁlters, such as in convolutional networks. Let z(n)(u, λ1, . . . , λn) be an intermediate representation in a convolutional network, and whose ﬁrst layer is fully connected. Suppose that G is a group acting on z via  g.z(n)(u, λ1, . . . , λn) = z(n)(u, λ1 + η(g), λ2, . . . , λn) ,  (5) where η : G → Λ1. This corresponds to the idealized case where the transformation only modiﬁes one component of the representation. A local pooling operator along the variable λ1, at a certain scale 2J , attenuates the transformation by g as soon as |η(g)| ≪ 2J . It thus produces local invariance with respect to the action of G.  3.3 Group Factorization with Deep Networks  Deep convolutional networks have the capacity to learn complex relationships of the data and to build invariance with respect to a large family of transformations. These properties can be partly explained in terms of a factorization of the invariance groups performed successively.  3  Whereas pooling operators efﬁciently produce stable local invariance, convolution operators pre- serve the invariance generated by previous layers. Indeed, suppose z(n)(u, λ1) is an interme- diate representation in a convolutional network, and that G acts on z(n) via g.z(n)(u, λ1) = z(n)(f (g, u), λ1 + η(g)). It follows that if the next layer is constructed as  z(n+1)(u, λ1, λ2) := z(n)(u, ·) ⋆ ψλ2 (λ1) ,  then G acts on z(n+1) via g.z(n+1)(u, λ1, λ2) = z(n)(f (g, u), λ1 + η(g), λ2), since convolutions commute with the group action, which by construction is expressed as a translation in the coefﬁcients λ1. The new coordinates λ2 are thus unaffected by the action of G. As a consequence, this property enables a systematic procedure to generate invariance to groups of the form G = G1 ⋊ G2 ⋊ . . . ⋊ Gs, where H1 ⋊ H2 is the semidirect product of groups. In this decomposition, each factor Gi is associated with a range of convolutional layers, along the coordinates where the action of Gi is perceived.  4 Perspectives  The connections between group invariance and deep convolutional networks offer an interpretation of their efﬁciency on several recognition tasks. In particular, they might explain why the weight sharing induced by convolutions is a valid regularization method in presence of group variability.  More concretely, we shall also concentrate on the following aspects:  • Group Discovery. One might ask for the group of transformations which best explains the variability observed in a given dataset {xi}. In the case where no geometric deformations are present, one can start by learning the (complex) eigenvectors of the group action:  U ∗ = arg min  U T U=1  var(|U xi|) .  When the data corresponds to a uniform measure on the group, then this decomposition can be obtained from the diagonalization of the covariance operator Σ = E(xT x). In that case, the real eigenvectors of Σ are grouped into pairs of vectors with identical eigenvalue, which then deﬁne the complex decomposition diagonalizing the group action. In presence of deformations, the global invariance is replaced by a measure of local invari- ance. This problem is closely related to the sparse coding with slowness from [6].  • Structured Convolutional Networks. Groups offer a powerful framework to incorporate structure into the families of ﬁlters, similarly is in [7]. On the one hand, one can enforce global properties of the group by deﬁning the convolutions accordingly. For instance, by wrapping the domain of the convolution, one is enforcing a periodic group to emerge. On the other hand, one could further regularize the learning by enforcing a group structure within a ﬁlter bank. For instance, one could ask a certain ﬁlter bank F = {h1, . . . , hn} to have the form F = {Rθh0}θ, where Rθ is a rotation with an angle θ.  References  [1] J. Bruna, S. Mallat, “Invariant Scattering Convolutional Networks”, IEEE TPAMI, 2012. [2] S. Mallat, “Group Invariant Scattering”, CPAM, 2012. [3] L.Sifre, S.Mallat, “Combined Scattering for Rotation Invariant Texture Analysis”, ESANN,  2012.  [4] Y.LeCun, L.Bottou, Y.Bengio, P.Haffner, “Gradient-Based Learning Applied to Document  Recognition”, IEEE, 1998  [5] M.H. Stone, “On one-parameter unitary Groups in Hilbert Space”, Ann. of Mathematics, 1932. [6] C.Cadieu, B.Olshausen, “Learning Transformational Invariants from Natural Movies”, NIPS  2009.  [7] K.Gregor, A. Szlam, Y.Lecun, “Structured Sparse Coding via lateral inhibition”, NIPS, 2011.  4  ","Transformation groups, such as translations or rotations, effectively expresspart of the variability observed in many recognition problems. The groupstructure enables the construction of invariant signal representations withappealing mathematical properties, where convolutions, together with poolingoperators, bring stability to additive and geometric perturbations of theinput. Whereas physical transformation groups are ubiquitous in image and audioapplications, they do not account for all the variability of complex signalclasses.We show that the invariance properties built by deep convolutional networkscan be cast as a form of stable group invariance. The network wiringarchitecture determines the invariance group, while the trainable filtercoefficients characterize the group action. We give explanatory examples whichillustrate how the network architecture controls the resulting invariancegroup. We also explore the principle by which additional convolutional layersinduce a group factorization enabling more abstract, powerful invariantrepresentations."
1301.3644,2013,Regularized Discriminant Embedding for Visual Descriptor Learning  ,"['Kye-Hyeon Kim', 'Rui Cai', 'Lei Zhang', 'Seungjin Choi']",https://arxiv.org/pdf/1301.3644.pdf,"3 1 0 2     n a J    6 1      ]  V C . s c [      1 v 4 4 6 3  .  1 0 3 1 : v i X r a  Regularized Discriminant Embedding for Visual  Descriptor Learning  Kye-Hyeon Kim,a Rui Cai,b Lei Zhang,b Seungjin Choia∗  a Department of Computer Science, POSTECH, Pohang 790-784, Korea  fenrir@postech.ac.kr, {ruicai, leizhang}@microsoft.com,  b Microsoft Research Asia, Beijing 100080, China  seungjin@postech.ac.kr  Abstract  Images can vary according to changes in viewpoint, resolution, noise, and illu- mination. In this paper, we aim to learn representations for an image, which are robust to wide changes in such environmental conditions, using training pairs of matching and non-matching local image patches that are collected under various environmental conditions. We present a regularized discriminant analysis that em- phasizes two challenging categories among the given training pairs: (1) matching, but far apart pairs and (2) non-matching, but close pairs in the original feature space (e.g., SIFT feature space). Compared to existing work on metric learning and discriminant analysis, our method can better distinguish relevant images from irrelevant, but look-alike images.  1  Introduction  In many computer vision problems, images are compared using their local descriptors. A local descriptor is a feature vector, representing characteristics of an interesting local part in an image. Scale-invariant feature transform (SIFT) [2] is popularly used for extracting interesting parts and their local descriptors from an image. Then comparing two images is done by aggregating pairs between each local descriptor in one image and its closest local descriptor in another image, whose pairwise distances are below some threshold. The assumption behind this procedure is that local descriptors corresponding to the same local part (“matching descriptors”) are usually close enough in the feature space, whereas local descriptors belonging to different local parts (“non-matching descriptors”) are far apart. However, this assumption does not hold when there are signiﬁcant changes in environmental condi- tions (e.g., viewpoint, illumination, noise, and resolution) between two images. For the same local part, varying environment conditions can yield varying local image patches, leading to matching descriptors far apart in the feature space. On the other hand, for different local parts, their image patches can look similar to each other in some environmental conditions, leading to non-matching descriptors close together. Fig. 1 shows some examples: in each triplet, the ﬁrst two image patches belong to the same local part but captured under different environment conditions, while the third patch belongs to a different part but looks similar to the second one, resulting that the SIFT descrip- tors between non-matching local parts are closer than those between matching parts. Consequently, comparing two images using their local descriptors cannot be done correctly when their are signiﬁ- cant differences in environmental conditions between the images. Fig. 2(a) shows the cases. In this paper, we address this problem by learning more robust representations for local image patches where matching parts are more similar together than non-matching parts even under widely varying environmental conditions.  ∗The full version of this manuscript is currently under review in an international journal.  1  Figure 1: Some examples where a local part (center in each triplet) is closer to a non-matching part (right) than a matching part (left) in terms of the Euclidean distances between their SIFT descriptors. Using linear discriminant embedding (LDE) [1], non-matching pairs are still closer than matching pairs in the ﬁrst three examples. Compared to existing work on metric learning and discriminant analysis, our learning method focuses more on “far but matching” and “close but non-matching” training pairs, so that can distinguish look-alike irrelevant parts successfully.  (a) 15 closest SIFT pairs  (b) 15 closest RDE pairs  Figure 2: (a) When two images of the same scene are captured under considerably different con- ditions, many irrelevant pairs of local parts are chosen as closest pairs in the local feature space, which may lead to undesirable results of comparison. (b) In our RDE space, matching pairs are successfully chosen as closest pairs.  2 Proposed Method  In descriptor learning [1, 3], a projection is obtained from training pairs of matching and non- matching descriptors in order to map given local descriptors (e.g., SIFT) to a new feature space where matching descriptors are closer to each other and non-matching descriptors are farther from each other. Traditional techniques for supervised dimensionality reduction, including linear discrim- inant analysis (LDA) and local Fisher discriminant analysis (LFDA) [4], can be applied to descriptor learning after a slight modiﬁcation. For example, linear discriminant embedding (LDE) [1] is come from LDA with a simple modiﬁcation for handling pairwise training data. We propose a regularized learning framework in order to further emphasize (1) matching, but far apart pairs and (2) non-matching, but look-alike pairs, under wide environmental conditions. First, we divide given training pairs of local descriptors into four subsets, Relevant-Near (Rel-Near), Relevant-Far (Rel-Far), Irrelevant-Near (Irr-Near), and Irrelevant-Far (Irr-Far). For example, the “Irr-Near” subset consists of irrelevant (i.e., non-matching), but near pairs. We deﬁne an irrelevant pair (xi, xj) as “near” if xi is one of the k nearest descriptors1 among all non-matching descriptors of xj or vice versa. Similarly, a relevant pair (xi, xj) is called “near” if xi is one of k nearest de- scriptors among all matching descriptors of xj. All the other pairs belong to “Irr-Far” or “Rel-Far”. Then we seek a linear projection T that maximizes the following regularized ratio:  J(T ) =  βIN βRN  (i,j)∈PIN (i,j)∈PRN  dij(T ) + βIF dij(T ) + βRF  (i,j)∈PIF (i,j)∈PRF  dij(T ) dij(T )  ,  (1)  (cid:80) (cid:80)  (cid:80) (cid:80)  1In our experiments, setting 1 ≤ k ≤ 10 achieved a reasonable performance improvement.  2  SIFT:    304 LDE:     336 Ours:    360 SIFT:    213 LDE:     268 Ours:    295 SIFT:    268 LDE:     283 Ours:    301 231 275 425 SIFT:    336 LDE:     371 Ours:    362 > > < 246 314 372 > ≈ < 199 264 388 SIFT:    267 LDE:     240 Ours:    257 > < < 257 319 405 > < < 232 291 335 SIFT:    290 LDE:     305 Ours:    349 > > < 221 278 365 > > < (a)  (b)  Figure 3: Toy examples of projections learned by LDE, LFDA, and our RDE.  (a) SIFT feature space  (b) LDE feature space  (c) Our RDE feature space  Figure 4: Distribution of Euclidean distance in a given feature space for each subset of pairs. Err (Rel vs Irr) measures the proportion of overlapping region between {Rel-Near, Rel-Far} and {Irr- Near, Irr-Far}, while Err (RFar vs INear) measures the overlap between Rel-Far and Irr-Near. In our RDE space, non-matching pairs are well distinguished from matching pairs.  where dij(T ) denotes the squared distance ||T (xi − xj)||2 between two local descriptors xi and xj in the projected space, and PRN ,PRF ,PIN ,PIF denote the subsets of Rel-Near, Rel-Far, Irr-Near, and Irr-Far, respectively. Four regularization constants βRN , βRF , βIN , βIF control the importance of each subset.  • In LDE, all pairs are equally important, i.e., βRN = βRF = βIN = βIF = 1. • In LFDA , “near” pairs are more important, i.e., βRN (cid:29) βRF and βIN (cid:29) βIF . • In our method, we propose to emphasize Rel-Far (matching but far apart) and Irr-Near  (non-matching but close) pairs, i.e., βRN (cid:28) βRF and βIN (cid:29) βIF .  Fig. 3 shows when and why our method can better distinguish Irr-Near pairs from Rel-Far pairs. In Fig. 3(a), the global intra-class distribution forms a diagonal, while each local cluster has no meaningful direction of scattering. Since LFDA focuses on “near” pairs, it cannot capture the true intra-class scatter well, leading to the undesirable projection. In Fig. 3(b), LDE obtains a projection that maximizes the inter-class variance, but the shape of the class boundary cannot be considered well, leading to an overlap between two classes. In this case, focusing more on Irr-Near pairs (i.e., the pairs of opposite clusters near the class boundary) can preserve the separability of classes. Fig. 4 shows the distance distribution of local descriptors, where 20,000 pairs of each subset are randomly chosen from 500,000 local patches of Flickr images. As shown in Fig. 4(a), Rel-Near and Irr-Far pairs are already well separated in the SIFT space, but Rel-Far and Irr-Near pairs are not distinguished well (∼30% overlapped) and many Rel-Far pairs lie farther than Irr-Near pairs. Learning by LDE can achieve only a marginal improvement (Fig. 4(b)). By contrast, our RDE achieves a signiﬁcant improvement in the separability between matching and non-matching pairs, especially two challenging subsets, Rel-Far and Irr-Near (Fig. 4(c)). Fig. 1 and 2 also show the superiority of our method over the existing work.  3  −1−0.500.51−1−0.8−0.6−0.4−0.200.20.40.60.81  LDELFDARDE−1−0.500.51−1−0.8−0.6−0.4−0.200.20.40.60.81  LDELFDARDE01002003004005006007000500100015002000250030003500400045005000DistanceThe number of pairs  Err (Rel vs Irr) = 15.58%Err (RFar vs INear) = 29.92%RelNearIrrFarIrrNearRelFar01002003004005006007000500100015002000250030003500400045005000DistanceThe number of pairs  Err (Rel vs Irr) = 13.99%Err (RFar vs INear) = 27.00%RelNearIrrFarIrrNearRelFar01002003004005006007000500100015002000250030003500400045005000DistanceThe number of pairs  Err (Rel vs Irr) = 8.34%Err (RFar vs INear) = 16.30%RelNearIrrFarIrrNearRelFarReferences [1] G. Hua, M. Brown, and S. Winder, “Discriminant embedding for local image descriptors,” in Proceedings  of the International Conference on Computer Vision (ICCV), 2007, pp. 1–8.  [2] D. G. Lowe, “Object recognition from local scale-invariant features,” in Proceedings of the International  Conference on Computer Vision (ICCV), 1999, pp. 1150–1157.  [3] J. Philbin, M. Isard, J. Sivic, and A. Zisserman, “Descriptor learning for efﬁcient retrieval,” in Proceedings  of the European Conference on Computer Vision (ECCV), 2010, pp. 677–691.  [4] M. Sugiyama, “Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis,”  Journal of Machine Learning Research, vol. 5, pp. 1027–1061, 2007.  4  ","Images can vary according to changes in viewpoint, resolution, noise, andillumination. In this paper, we aim to learn representations for an image,which are robust to wide changes in such environmental conditions, usingtraining pairs of matching and non-matching local image patches that arecollected under various environmental conditions. We present a regularizeddiscriminant analysis that emphasizes two challenging categories among thegiven training pairs: (1) matching, but far apart pairs and (2) non-matching,but close pairs in the original feature space (e.g., SIFT feature space).Compared to existing work on metric learning and discriminant analysis, ourmethod can better distinguish relevant images from irrelevant, but look-alikeimages."
1301.3627,2013,Two SVDs produce more focal deep learning representations  ,"['Hinrich Schuetze', 'Christian Scheible']",https://arxiv.org/pdf/1301.3627.pdf,"3 1 0 2     y a M 1 1         ] L C . s c [      2 v 7 2 6 3  .  1 0 3 1 : v i X r a  Two SVDs produce more focal deep learning  representations  Hinrich Sch¨utze  Center for Information and Language Processing  University of Munich, Germany  Christian Scheible Institute for NLP  University of Stuttgart, Germany  hs999@ifnlp.org  scheibcn@ims.uni-stuttgart.de  Abstract  A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the ﬁeld has focused on efﬁcient and effective methods for computing representations. In this paper, we propose an alternative method that is more ef- ﬁcient than prior work and produces representations that have a property we call focality – a property we hypothesize to be important for neural network represen- tations. The method consists of a simple application of two consecutive SVDs and is inspired by (Anandkumar et al., 2012).  In this paper, we propose to generate representations for deep learning by two consecutive applica- tions of singular value decomposition (SVD). In a setup inspired by (Anandkumar et al., 2012), the ﬁrst SVD is intended for denoising. The second SVD rotates the representation to increase what we call focality. In this initial study, we do not evaluate the representations in an application. In- stead we employ diagnostic measures that may be useful in their own right to evaluate the quality of representations independent of an application.  We use the following terminology. SVD1 (resp. SVD2) refers to the method using one (resp. two) applications of SVD; 1LAYER (resp. 2LAYER) corresponds to a single-hidden-layer (resp. two- hidden-layer) architecture.  In Section 1, we introduce the two methods SVD1 and SVD2 and show that SVD2 generates better (in a sense to be deﬁned below) representations than SVD1. In Section 2, we compare 1LAYER and 2LAYER SVD2 representations and show that 2LAYER representations are better. Section 3 discusses the results. We present our conclusions in Section 4.  1 SVD1 vs. SVD2  Given a base representation of n objects in Rd, we ﬁrst compute the ﬁrst k dimensions of an SVD on the corresponding n × d matrix C. Ck = U SV T (where Ck is the rank-k approximation of C). We then use U S to represent each object as a k-dimensional vector. Each vector is normalized to unit length because our representations are count vectors where the absolute magnitude of a count contains little useful information – what is important is the relative differences between the counts of different dimensions. This is the representation SVD1. It is motivated by standard arguments for representations produced by dimensionality reduction: compactness and noise reduction. Denoising is also the motivation for the ﬁrst SVD in the method proposed by Anandkumar et al. (2012).  We then perform a second SVD on the resulting matrix C ′ of dimensionality n × k. C ′ = U ′S ′V ′T (full-rank, no dimensionality reduction). We again use U ′S ′ to represent each object as a k- dimensional vector. Each vector is normalized to unit length. This is the representation SVD2.  1  Figure 1: 1LAYER architecture. Distributional word vectors form the bottom layer. A trigram – represented as a triple of word vectors – is transformed by the ﬁrst SVD into a 100-dimensional vector (layer “SVD1”). These vectors are then rotated (layer “SVD2”).  Note that the operation we are applying is not equivalent to a single linear operation because of the lenght normalization that we perform between the two SVDs.  SVD2 is intended to be a rotation of SVD1 that is more “focal” in the following sense. Consider a classiﬁcation problem f over a k-dimensional representation space R. Let Mθ(f, R) be the size k′ of the smallest subset of the dimensions that support an accuracy above a threshold θ for f . Then a representation R is more focal than R′ if Mθ(f, R) < Mθ(f, R′). The intuition is that good deep learning representations have semantically interpretable hidden units that contribute input to a decision that is to some extent independent of other hidden units. We want the second SVD to rotate the representation into a “more focal” direction.  The role of the second SVD is somewhat analogous to that of the second SVD in the approach of Anandkumar et al. (2012), where the goal also is to ﬁnd a representation that reveals the underlying structure of the data set.  The architecture of the 1LAYER setup is depicted in Figure 1. Experimental setup. We use a corpus of movie review sentences (Pang and Lee, 2004). Following Sch¨utze (1995), we ﬁrst compute a left vector and a right vector for each word. The dimensionality of the vectors is 250. Entry i for the left (right) vector of word w is the number of times that the word with frequency rank i occurred immediately to the left (right) of w. Vectors are then tf-idf weighted and length-normalized. We randomly select 100,000 unique trigrams from the corpus, e.g., “tangled feelings of” or “as it pushes”. Each trigram is represented as the concatentation of six vectors, the left and right vectors of the three words. This deﬁnes a matrix of dimensionality n × d (n = 100000, d = 1500). We then compute SVD1 and SVD2 on this matrix for k = 100. Analysis of correlation coefﬁcients. Figure 2 shows histograms of the 10,000 correlation coefﬁ- cients of SVD1 (left) and SVD2 (right). Each correlation coefﬁcient is the correlation of two columns in the corresponding 100000 × 100 matrix and is transformed using the function f (c) = log10 |c| to produce a histogram useful for the analysis. The histogram of SVD2 is shifted by about 0.5 to the left. This is a factor of 100.5 ≈ 3. Thus, SVD2 dimensions have correlations that are only a third as large as SVD1 correlations on average.  We take this to indicate that SVD2 representations are more focal than SVD1 representations because the distribution of correlation coefﬁcients would change the way it changes from SVD2 to SVD1 if we took a focal representation (in the most extreme case one where each dimension by itself supported a decision) and rotated it. Discrimination task. We randomly selected 200 words in the frequency range [25, 250] from the corpus; and randomly arranged them into 100 pairs. An example of such a pair is (documentary, special). For each pair, we ﬁrst retrieved the SVD1 and SVD2 representations of all triples from the set of 100,000 in which one of the two words was the central word. For the example, “typical documentary footage”, “good documentary can”, and “really special walk” are such triples. Then we determined for each dimension i of the 100 dimensions (for both SVD1 and SVD2) the optimal discrimination value θ by exhaustive search; that is, we determined the threshold θ for which the accuracy of the classifer ~vi > θ (or ~vi < θ) was greatest – where the discrimination task was to distinguish triples that had one word vs the other as their central word. So for “typical documentary footage” and “good documentary can” the classiﬁer should predict class 1 (“documentary”), for “re-  2  Figure 2: Histograms of log10 |c| of the 10,000 correlation coefﬁcients of SVD1 (above) and SVD2 (below).  ally special walk” the classiﬁer should predict class 2 (“special”). Finally, of the 100 discrimination accuracies we chose the largest one for this word pair.  On this discrimination task, SVD2 was better than SVD1 55 times, the two were equal 15 times and SVD2 was worse 30 times. On average, discrimination accuracy of SVD2 was 0.7% better than that of SVD1. This is evidence that SVD2 is better for this discrimination task than SVD1.  This indicates again that SVD2 representations are more focal than SVD1 representations: each dimension is more likely to provide crucial information by itself as opposed to only being useful in conjunction with other dimensions.  To illustrate in more detail why this discrimination task is related to focality, assume that for a particular 100-dimensional representation r of trigrams t, the decision rule “if r(t)27 > 0.2 then ‘documentary’ else ‘special’ ” (i.e., if the value of dimension 27 is greater than 0.2, then the trigram center is predicted to be “documentary”, else “special”) has an accuracy of 0.99; and that the decision rule “if r(t)27 > 0.2 and r(t)91 < −0.1 then ‘documentary’ else ‘special’ ” has an accuracy of 1.0. Then M0.99(f, documentary-vs-special) = 1, M1.00(f, documentary-vs-special) = 2 and we can view the representation r as highly focal since a single dimension sufﬁces for high accuracy and two dimensions achieve perfect classiﬁcation results.  2 1LAYER vs. 2LAYER  We compare two representations of a word trigram: (i) the 1LAYER representation from Section 1 and (ii) a 2LAYER representation that goes through two rounds of autoencoding, which is a deep learning representation in the sense that layer 2 represents more general and higher-level properties of the input than layer 1.  The architecture of the 2LAYER is depicted in Figure 3.  To create 2LAYER representations, we ﬁrst create a vector for each of the 20701 word types oc- curring in the corpus. This vector is the concatenation of its left vector and its right vector. The resulting 20701 × 500 matrix is the input representation to SVD1. We again set k = 100. A trigram  3  Figure 3: 2LAYER architecture. Distributional word vectors form the bottom layer. Each word vec- tor is transformed by SVD into a 100-dimensional vector (ﬁrst layer “SVD1”). This layer constitutes the 1LAYER part of this architecture. A triple of vectors of three consecutive words is transformed by SVD into a 100-dimensional vector (second layer “SVD1”). These vectors are then rotated (layer “SVD2”). The last two layers constitute the 2LAYER part of this architecture.  is then represented as the concatenation of three of these 100-dimensional vectors. We apply the SVD2 construction algorithm to the resulting 100000 × 300 matrix and truncate to k = 100. We now have – for each trigram – two SVD2 representations, the 1LAYER representation from Section 1 and the 2LAYER representation we just described. We compare these two trigram repre- sentations, again using the task from Section 1: discrimination of the 100 pairs of words.  2LAYER is better than 1LAYER 64 times on this task, the same in 18 cases and worse in 18 cases. This is statistically signiﬁcant (p < .01, binomial test) evidence that 2LAYER SVD2 representations are more focal than 1LAYER SVD2 representations.  3 Discussion  3.1 Focality  One advantage of focal representations is that many classiﬁers cannot handle conjunctions of several features unless they are explicitly deﬁned as separate features. Compare two representations ~x and ~x′ where ~x′ is a rotation of ~x (as it might be obtained by an SVD). Since one vector is a rotation of the other, they contain exactly the same information. However, if (i) an individual “hidden unit” of the rotated vector ~x′ can directly be interpreted as “is verb” (or a similar property like “is adjective” or “takes NP argument”) and (ii) the same feature requires a conjunction of several hidden units for ~x, then the rotated representation is superior for many upstream statistical classiﬁers.  Focal representations can be argued to be closer to biological reality than broadly distributed repre- sentations (Thorpe, 2010); and they have the nice property that they become categorical in the limit. Thus, they include categorical representations as a special case.  A ﬁnal advantage of focal representations is that in some convolutional architectures the input to the top-layer statistical classiﬁer consists of maxima over HU (hidden unit) activations. E.g., one way to classify a sentence as having positive/negative sentiment is to slide a neural network whose input is a window of k words (e.g., k = 4) over it and to represent each window of k words as a vector of HU activations produced by the network. In a focal representation, the hidden units are more likely to have clear semantics like “the window contains a positive sentiment word”. In this type of scenario, taking the maximum of activations over the n − k + 1 sliding windows of a sentence of length n results in hidden units with interpretable semantics like “the activation of the positive-sentiment HU of the window with the highest activation for this HU”. These maximum values are then a good basis for sentiment classiﬁcation of the sentence as a whole.  The notion of focality is similar to disentanglement (Glorot et al., 2011) – in fact, the two notions may be identical. However, Glorot et al. (2011) introduce disentanglement in the context of domain adaptation, focusing on the idea that “disentangled” hidden units capture general cross-domain prop- erties and for that reason are a good basis for domain adaptation. The contributions of this paper are: proposing a way of measuring “entanglement” (i.e., measuring it as correlation), deﬁning focality  4  in terms of classiﬁcation accuracy (a deﬁnition that covers single hidden units as well as groups of hidden units) and discussing the relationship to convolution and biological systems.  It is important to point out that we have not addressed how focality would be computed efﬁciently in a particular context. In theory, we could use brute force methods, but these would be exponential in the number of dimensions (systematic search over all subsets of dimensions). However, certain interesting questions about focality can be answered efﬁciently; e.g., if we have M (f, R) = 1 for one representation and M (f, R′) > 1 for another, then this can be shown efﬁciently and in this case we have established that R is more focal than R′.  3.2 mSVD method  In this section, we will use the abbreviation mSVD to refer to a stacked applications of our method with an arbitrary number of layers even though we only experiment with m = 2 in this paper (2LAYER, 2-layer-stacking).  SVD and other least squares methods are probably the most widely used dimensionality reduction techniques for the type of matrices in natural language processing that we work with in this paper (cf. (Turney and Pantel, 2010)). Stacking a second least squares method on top of the ﬁrst has not been considered widely because these types of representations are usually used directly in vector classiﬁers such as Rocchio and SVM (however, see the discussion of (Chen et al., 2012) below). For this type of classiﬁer, performing a rotation has no effect on classiﬁcation performance. In contrast, our interest is to use SVD2 representations as part of a multilevel deep learning architecture where the hidden unit representations of any given layer are not simply interpreted as a vector, but decisions of higher layers can be based on individual dimensions.  The potential drawback of SVD and other least squares dimensionality reductions is that they are linear: reduced dimensions are linear combinations of orginal dimensions. To overcome this limitation many nonlinear methods have been introduced: probabilistic latent semantic indexing (Hofmann, 1999), kernel principal component analysis (Sch¨olkopf et al., 1998), matrix factorization techniques that obey additional constraints – such as non-negativity in the case of non-negative ma- trix factorization (Lee and Seung, 1999) – , latent dirichlet allocation (Blei et al., 2003) and different forms of autoencoding (Bengio, 2009; Chen et al., 2012). All of these can be viewed as dimension reduction techniques that do not make the simplistic assumptions of SVD and should therefore be able to produce better representation if these simplistic assumptions are not appropriate for the do- main in question.  However, this argument does not apply to the mSVD method we propose in this paper since it is also nonlinear. What should be investigated in the future is to what extent the type of nonlinearity implemented by mSVD offers advantages over other forms of nonlinear dimensionality reduction; e.g., if the quality of the ﬁnal representations is comparable, then mSVD would have the advantage of being more efﬁcient.  Finally, there is one big difference between mSVD and deep learning representations such as those proposed by Hinton et al. (2006), Collobert and Weston (2008) and Socher et al. (2012). Most deep learning representations are induced in a setting that also includes elements of supervised learn- ing as is the case in contrastive divergence or when labeled data are available for adjusting initial representations produced by a process like autoencoding or dimensionality reduction.  This is the most important open question related to the research presented here: how can one modify hidden layer representations initialized by multiple SVDs in a meaningful way?  The work most closely related to what we are proposing is probably mDA (Chen et al., 2012) – an approach we only became aware of after the initial publication of this paper. There are a number of differences between mDA and mSVD. Non-linearity in mDA is achieved by classical deep learning encoding functions like tanh() whereas we renormalize vectors and then rotate them. Second, we do not add noise to the input vectors – mSVD is more efﬁcient for this reasons, but it remains to be seen if it can achieve the same level of performance as mDA. Third, the mSVD architecture proposed here, which changes the objects to be represented from small frequent units to larger less frequent units when going one layer up, can be seen as an alternative (though less general since it’s customized for natural language) way of extending to very high dimensions.  5  4 Conclusion  As a next step a direct comparison should be performed of SVD2 with traditional deep learning (Hinton et al., 2006). As we have argued, SVD2 would be an interesting alternative to deep learn- ing initialization methods currently used since SVD is efﬁcient and a simple and well understood formalism. But this argument is only valid if the resulting representations are of comparable qual- ity. Datasets and tasks for this comparative evaluation could e.g. be those of Turian et al. (2010), Maas et al. (2011), and Socher et al. (2011).  References  Anandkumar, Animashree, Dean P. Foster, Daniel Hsu, Sham M. Kakade, and Yi-Kai Liu. 2012. Two svds sufﬁce: Spectral decompositions for probabilistic topic modeling and latent dirichlet allo- cation. CoRR, abs/1204.6703. Bengio, Yoshua. 2009. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1–127. Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. JMLR, 3:993–1022. Chen, Minmin, Zhixiang Eddie Xu, Kilian Q. Weinberger, and Fei Sha. 2012. Marginalized denois- ing autoencoders for domain adaptation. In ICML. Collobert, Ronan and Jason Weston. 2008. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In ICML. Glorot, Xavier, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sen- timent classiﬁcation: A deep learning approach. In ICML, pages 513–520. Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527–1554. Hofmann, Thomas. 1999. Probabilistic latent semantic indexing. In SIGIR, pages 50–57. Lee, David D. and H. Sebastian Seung. 1999. Learning the parts of objects by non-negative matrix factorization. Nature, 401:788. Maas, Andrew L., Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In ACL, pages 142–150. Pang, Bo and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proc. of ACL. Sch¨olkopf, Bernhard, Alex J. Smola, and Klaus-Robert M¨uller. 1998. Nonlinear component analy- sis as a kernel eigenvalue problem. Neural Computation, 10(5):1299–1319. Sch¨utze, Hinrich. 1995. Distributional part-of-speech tagging. In Conference of the European Chapter of the Association for Computational Linguistics, pages 141–148. Socher, Richard, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In EMNLP, pages 151–161. Socher, Richard, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic com- positionality through recursive matrix-vector spaces. In EMNLP-CoNLL, pages 1201–1211. Thorpe, Simon. 2010. Grandmother cells and distributed representations. In Kriegeskorte, Nikolaus and Gabriel Kreiman, editors, Understanding visual population codes. Toward a common multivari- ate framework for cell recording and functional imaging. MIT Press. Turian, Joseph, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In ACL, pages 384–394. Turney, Peter D. and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. J. Artif. Intell. Res. (JAIR), 37:141–188.  6  ","A key characteristic of work on deep learning and neural networks in generalis that it relies on representations of the input that support generalization,robust inference, domain adaptation and other desirable functionalities. Muchrecent progress in the field has focused on efficient and effective methods forcomputing representations. In this paper, we propose an alternative method thatis more efficient than prior work and produces representations that have aproperty we call focality -- a property we hypothesize to be important forneural network representations. The method consists of a simple application oftwo consecutive SVDs and is inspired by Anandkumar (2012)."
1301.3584v4,2013,Natural Gradient Revisited  ,"['Razvan Pascanu', 'Yoshua Bengio']",https://arxiv.org/pdf/1301.3584v4.pdf,"Revisiting Natural Gradient for Deep Networks  Razvan Pascanu and Yoshua Bengio  Dept. IRO  University of Montreal  Montreal, QC  3 1 0 2    r a     M 3 1      ]  G L . s c [      4 v 4 8 5 3  .  1 0 3 1 : v i X r a  Abstract  The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gra- dient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overﬁtting and particularly it can be robust to the order in which the training data is presented to the model.  1  Introduction  Several recent papers tried to address the issue of using better optimization techniques for machine learning, especially for training deep architectures or neural networks of various kinds. Hessian-Free optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories: those which make use of second order information and those which use the geometry of the underlying parameter manifold (natural gradient). One particularly interesting pipeline to scale up such algorithms was originally proposed in Pearl- mutter (1994), ﬁnetuned in Schraudolph (2001) and represents the backbone behind both Hessian- Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012). The core idea behind it is to make use of the forward (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efﬁcient products between Jacobian or Hes- sian matrices and vectors. These products are used within a truncated-Newton approach (Nocedal and Wright, 2000) which considers the exact Hessian and only inverts it approximately without the need for explicitly storing the matrix in memory, as opposed to other approaches which perform a more crude approximation of the Hessian (or Fisher) matrix (either diagonal or block-diagonal). The contributions of this paper to the study of the natural gradient are as follows. We provide a de- tailed derivation of the natural gradient, avoiding elements of information geometry. We distinguish natural gradient descent from TONGA and provide arguments suggesting that natural gradient may also beneﬁts from a form of robustness that should yield better generalization. The arguments for this robustness are different from those invoked for TONGA. We show experimentally the effects of this robustness when we increase the accuracy of the metric using extra unlabeled data. We also provide evidence that the natural gradient is robust to the order of training examples, resulting in lower variance as we change the order. The ﬁnal contribution of the paper is to show that Martens’ Hessian-Free approach of Martens (2010) (and implicitly Krylov Subspace Descent (KSD) algo- rithm) can be cast into the framework of the natural gradient, showing how these methods can be seen as doing natural gradient rather then second order optimization.  1  2 Natural Gradient Natural gradient can be traced back to Amari’s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al. (2011). The algorithm has also been successfully applied in the reinforcement learning community (Kakade, 2001; Peters and Schaal, 2008) and for stochastic search (Sun et al., 2009). Le Roux et al. (2007) introduces a different formulation of the algorithm for deep models. Although similar in name, the algorithm is motivated differently and is not equivalent to Amari’s version, as will be shown in section 4.1. Let us consider a family of density functions F : RP → (B → [0, 1]), where for every θ ∈ RP , F(θ) deﬁnes a density function from B → [0, 1] over the random variable z ∈ B, where B is some suitable numeric set of values, for e.g. B = RN . We also deﬁne a loss function that we want to minimize L : RP → R. Any choice of θ ∈ RP deﬁnes a particular density function pθ(z) = F(θ) and by considering all possible θ values, we explore the set F, which is our functional manifold. Because we can deﬁne a similarity measures between nearby density functions, given by the KL- divergence which in its inﬁnitesimal form behaves like a distance measure, we are dealing with a Riemannian manifold whose metric is given by the Fisher Information matrix. Natural gradient attempts to move along the manifold by correcting the gradient of L according to the local curvature of the KL-divergence surface 1:  ∇NL(θ) =  ∂L(θ) ∂θ  Ez  (cid:34)(cid:18) ∂ log pθ(z)  (cid:19)T(cid:18) ∂ log pθ(z)  (cid:19)(cid:35)−1  ∂θ  ∂θ  ∂L(θ) ∂θ  =  G−1  (1)  We can derive this result without relying on information geometry. We consider the natural gradient to be deﬁned as the algorithm which, at each step, picks a descent direction such that the KL- divergence between pθ and pθ+∆θ is constant. At each step, we need to ﬁnd ∆θ such that:  arg min∆θ L(θ + ∆θ) s. t. KL(pθ||pθ+∆θ) = constant  (2)  Using this constraint we ensure that we move along the functional manifold with constant speed, without being slowed down by its curvature. This also makes learning robust to re-parametrizations of the model, as the functional behaviour of p does not depend on how it is parametrized. Assuming ∆θ → 0, we can approximate the KL divergence by its Taylor series:  KL(pθ(z) (cid:107) pθ+∆θ(z)) ≈ (Ez [log pθ] − Ez [log pθ]) − Ez  (cid:20) ∂2 log pθ  ∂θ2  ∆θ − 1 2  ∆θT Ez  (cid:20) ∂ log pθ (cid:21) (cid:19)T(cid:18) ∂ log pθ(z)  ∆θ  ∂θ  (cid:21) (cid:19)(cid:35)  ∂θ  ∆θ  (cid:21)  ∆θ  (3)  (4)  =  =  ∆θT Ez  ∆θT Ez  1 2  1 2  ∂θ2  − ∂2 log pθ(z)  (cid:20) (cid:34)(cid:18) ∂ log pθ(z) (cid:105) (cid:104) ∂ log pθ(z)  ∂θ  ∂θ  The ﬁrst term cancels out and because Ez = 0, 2 we are left with only the last term. The Fisher Information Matrix form can be obtain from the expected value of the Hessian through algebraic manipulations (see the Appendix). We now express equation (2) as a Lagrangian, where the KL divergence is approximated by (4) and L(θ + ∆θ) by its ﬁrst order Taylor series L(θ) + ∂L(θ) (cid:17)  1 Throughout this paper we use the mathematical convention that a partial derivative ∂ log pθ 2Proof: Ez  is a row-vector ∂θ = 0. The proof holds for  θ pθ(z)(cid:1) = ∂1  (cid:104) ∂ log pθ (z)  = (cid:80)  (cid:0)(cid:80)  ∂θ ∆θ:  = ∂ ∂θ  pθ(z)  (cid:16)  ∂pθ (z)  (cid:105)  ∂θ  ∂θ  ∂θ  z  1  pθ (z)  the continuous case as well, replacing sums for integrals.  2  L(θ) +  ∂L(θ) ∂θ  ∆θ +  λ∆θT Ez  1 2  (cid:34)(cid:18) ∂ log pθ(z)  (cid:19)T(cid:18) ∂ log pθ(z)  (cid:19)(cid:35)  ∂θ  ∂θ  ∆θ = 0  (5)  Solving equation (5) for ∆θ gives us the natural gradient formula (1). Note that we get a scalar λ times the natural gradient. We fold this scalar into the learning rate, and hence the factor of 2 1 learning rate also controls the difference between pθ and pθ+∆θ that we impose at each step. Also the approximations we make are meaningful only around θ. Schaul (2012) suggests that using a large step size might be harmful for convergence. We deal with such issues both by using damping (i.e. setting a trust region around θ) and by properly selecting a learning rate. 3 Natural Gradient for Neural Networks The natural gradient for neural networks relies on their probabilistic interpretation (which induces a similarity measure between different parametrization of the model) given in the form of conditional probabilities pθ(t|x), with x representing the input and t the target. We make use of the following notation. q(x) describes the data generating distribution of x and q(t|x) is the distribution we want to learn. y is the output of the model, and by an abuse of no- tation, it will refer to either the function mapping inputs to outputs, or the vector of output acti- vations. r is the output of the model before applying the output activation function σ. t(i) and x(i) are the i-th target and input samples of the training set. Jy stands for the Jacobian matrix  . Finally, lower indices such as yi denote the i-th element of a vector. (cid:104) n(cid:88)  n(cid:88)  (cid:105)  (cid:104)  (cid:105)  log pθ(t(i)|σ(r(x(i))))  log pθ(t(i)|y(x(i)))  =  We deﬁne the neural network loss as follows:   ∂y1  ∂θ1 .. ∂yo ∂θ1  .. .. ..  ∂y1 ∂θP .. ∂y0 ∂θP  Jy =  (6)  L(θ) =  1 n  i  1 n  i  Because we have a conditional density function pθ(t|x) the formulation for the natural gradient changes slightly. Each value of x now deﬁnes a different family of density functions pθ(t|x), and hence a different manifold. In order to measure the functional behaviour of pθ(t|x) for different values of x, we use the expected value (with respect to x ∼ ˜q(x)) of the KL-divergence between pθ(t|x) and pθ+∆θ(t|x).  arg min∆θ L(θ + ∆θ) s. t. Ex∼˜q(x) [KL(pθ(t|x)||pθ+∆θ(t|x))] = constant  (7) The metric G is now an expectation over ˜q(x) of an expectation over p(t|x). The former aver- ages over possible manifolds generated by different choices of x, while the latter comes from the deﬁnition of the Fisher Information Matrix.  ∇NL(θ) =  ∂L(θ) ∂θ  Ex∼˜q(x)  Et∼p(t|x)  (cid:34)  (cid:34)(cid:18) ∂ log pθ(t|x)  (cid:19)T(cid:18) ∂ log pθ(t|x)  (cid:19)(cid:35)(cid:35)−1  ∂θ  ∂θ  ∂L(θ) ∂θ  =  G−1  (8) Note that we use the distribution ˜q instead of the empirical q. This is done in order to emphesis that the theory does not force us to use the empirical distribution. However, in practice, we do want ˜q to be as close as possible to q such that the curvature of the KL-divergence matches (in some sense) the curvature of the error surface. To clarify the effects of ˜q let us consider an example. Assume that ˜q is unbalanced with respect to q. Namely it contains twice the amount of elements of a class A versus the other B. This means that a change in θ that affects elements of class A is seen as having a larger impact on p (in the KL sense) than a change that affects the prediction of elements in B. Due to the formulation of natural gradient, we will move slower along any direction that affects A at the expense of B landing with higher probability on solutions θ∗ that favour predicting A. In practice we approximate this expectation over ˜q by a sample average over minibatches.  3  In what follows we consider typical output activation functions and the metrics G they induce. In the Appendix we provide a detailed description of how these matrices were obtained starting from equation (8). Similar derivations were done in Park et al. (2000), which we repeat for convenience. The formulas we get for the linear, sigmoid and softmax activation functions are:  (cid:35)  (cid:34)  (cid:20) (cid:34) o(cid:88)  Glinear = β2Ex∼˜q  ∂y ∂θ  T ∂y ∂θ  = β2Ex∼˜q  Gsigmoid = Ex∼˜q  Gsof tmax = Ex∼˜q  y diag( JT  1 yi  i  y(1 − y)  1  (cid:18) ∂yi  )Jy  (cid:19)T ∂yi  ∂θ  ∂θ  (cid:2)JT  y Jy  (cid:3)  (9)  (10)  (11)  (cid:21) (cid:35)  To efﬁciently implement the natural gradient, we use a truncated Newton approach following the same pipeline as Hessian-Free (Martens, 2010) (more details are provided in the Appendix). We rely on Theano (Bergstra et al., 2010) for both ﬂexibility and in order to use GPUs to speed up. The advantages of this pipeline are two-fold: (1) it uses the full-rank matrix, without the need for explicitely storing it in memory and (2) it does not rely on a smoothness assumption of the metric. Unlike other algorithms such as nonlinear conjugate gradient or BFGS, it does not assume that the curvature changes slowly as we change θ. This seems to be important for recurrent neural networks (as well as probably for deep models) where the curvature can change quickly (Pascanu et al., 2013).  Insights into natural gradient  4 Figure 1 considers a one hidden unit auto-encoder, where we minimize the error (x − w · sigmoid(wx + b) + b)2 and shows the path taken by Newton method (blue), natural gradi- ent (gray), Le Roux’s version of natural gradient (orange) and gradient descent (purple). On the left (larger plot) we show the error surface as a contour plot, where the x-axis represents b and y-axis is w. We consider two different starting points ([1] and [3]) and draw the ﬁrst 100 steps taken by each algorithm towards a local minima. The length of every other step is depicted by a different shade of color. Hyper-parameters, like learning rate and damping constant, were chosen such to improve convergence speed while maintaining stability (i.e. we looked for a smooth path). Values are pro- vided in the Appendix. At every point θ during optimization, natural gradient considers a different KL divergence surface, KL(pθ||pθ+∆θ) parametrized by ∆θ, which has a minima at origin. On the right we have contour plots of four different KL surfaces. They correspond to locations indicated by black arrows on the path of natural gradient. The x-axis is ∆b and y-axis is ∆w for the KL surfaces subplots. On top of these contour plots we show the direction and length of the steps proposed by each of the four considered algorithms. The point of this plot is to illustrate that each algorithm can take a different path in the parame- ter space towards local minima. In a regime where we have a non-convex problem, with limited resources, these path can result in qualitatively different kinds of minima. We can not draw any general conclusions about what kind of minima each algorithm ﬁnds based on this toy example, however we make two observations. First, as showed on the KL-surface plot for [3] the step taken by natural gradient can be smaller than gradient descent (i.e. the KL curvature is high) even though the error surface curvature is not high (i.e. Newton’s method step is larger than gradient descent step). Secondly, the direction chosen by natural gradient can be quite different from that of gradient descent (see for example in [3] and [4]), which can result in ﬁnding a different local minima than gradient descent (for e.g. when the model starts at [3]).  4.1 A comparison between Amari’s and Le Roux’s natural gradient In Le Roux et al. (2007) a different approach is taken to derive natural gradient. Speciﬁcally one assumes that the gradients computed over different minibatches are distributed according to a Gaus- sian centered around the true gradient with some covariance matrix C. By using the uncertainty provided by C we can correct the step that we are taking to maximize the probability of a downward move in generalization error (expected negative log-likelihood), resulting in a formula similar to that  4  Figure 1: Path taken by four different learning algorithms towards a local minima. Newton method (blue), natural gradient (gray), Le Roux’s natural gradient (orange) and gradient descent (purple). See text for details.  of natural gradient. If g = ∂L direction ˜g = ∂L(θ)  ∂θ C−1 where C is:  ∂θ is the gradient, then Le Roux et al. (2007) proposes following the  (cid:80) i (g − (cid:104)g(cid:105))T (g − (cid:104)g(cid:105))  (12)  C = 1 n  While the probabilistic derivation requires the use of the centered covariance, equation (12), in Le Roux et al. (2007) it is argued that using the uncentered covariance U is equivalent up to a constant resulting in a simpliﬁed formula which is sometimes confused with the metric derived by Amari.  (cid:80) i gT g ≈ E(x,t)∼q  (cid:20)(cid:16) ∂ log p(t|x)  (cid:17)T(cid:16) ∂ log p(t|x)  (cid:17)(cid:21)  ∂θ  ∂θ  U = 1 n  (13)  The misunderstanding comes from the fact that the equation has the form of an expectation, though the expectation is over the empirical distribution q(x, t). It is therefore not clear if U tells us how pθ would change, whereas it is clear that G does. The two methods are just different, and one can not straightforwardly borrow the interpretation of one for the other. However, we believe that there is an argument strongly suggesting that the protection against drops in generalization error afforded by Le Roux’s U is also a property shared by the natural gradient’s G. If KL(p (cid:107) q) is small, than U can be seen as an approximation to G. Speciﬁcally we approximate the second expectation from equation (8), i.e. the expectation over t ∼ pθ(t|x), by a single point, the corresponding t(i). This approximation makes sense when t(i) is a highly probable sample under p which happens when we converge. Note that at convergence, U, G and the Hessian are very similar, hence both versions of natural gradient and most second order methods would behave similarly. An interesting question is if these different paths taken by each algorithm represent qualitatively dif- ferent kinds of solutions. We will address this question indirectly by enumerating what implications each choice has. The ﬁrst observation has to do with numerical stability. One can express G as a sum of n × o outer products (where n is the size of the minibatch over which we estimate the matrix and o is the number  5  of output units) while U is a sum of only n outer products. Since the number of terms in these sums provides an upper bound on the rank of each matrix, it follows that one could expect that U will be lower rank than G for the same size of the minibatch n. This is also pointed out by Schraudolph (2002) to motivate the extended Gauss-Newton matrix as middle ground between natural gradient and the true Hessian3 A second difference regards plateaus of the error surface. Given the formulation of our error function in equation (6) (which sums the log of pθ(t|x) for speciﬁc values of t and x), ﬂat regions of the objective function are intrinsically ﬂat regions of the functional manifold4. Moving at constant speed in the functional space means we should not get stalled near such plateaus. In Park et al. (2000) such plateaus are found near singularities of the functional manifold, providing a nice framework to study them (as is done for example in Rattray et al. (1998) where they hypothesize that such singularities behave like repellors for the dynamics of natural gradient descent). An argument can also be made in favour of U at plateaus. If a plateau at θ exists for most possible inputs x, than the covariance matrix will have a small norm (because the vectors in each outer product will be small in value). The inverse of U consequentially will be large, meaning that we will take a large step, possibly out of the plateau region. This suggest both methods should be able to escape from some plateaus, though the reasoning behind the functional manifold approach more clearly motivates this advantage. Another observation that is usually made regarding the functional manifold interpretation is that it is parametrization-independent. That means that regardless of how we parametrize our model we should move at the same speed, property assured by the constraint on the KL-divergence between pθ and pθ+∆θ. In Sohl-Dickstein (2012), following this idea, a link is made between natural gradient and whitening in parameter space. This property does not transfer directly to the covariance matrix. On the other hand Le Roux’s method is designed to obtain better generalization errors by moving mostly in the directions agreed upon by the gradients on most examples. We will argue that the functional manifold approach can also provide a similar property. One argument relies on large detrimental changes of the expected log-likelihood, which is what Le Roux’s natural gradient step protects us from with higher probability. The metric of Amari’s natural gradient measures the expected (over x) KL-divergence curvature. We argue that if ∆θ induces a large change in log-likelihood computed over x ∈ D, where D corresponds to some minibatch, then it produces a large change in pθ (in the KL sense), i.e. it results in a high KL-curvature. Because we move at constant speed on the manifold, we slow down in these high KL-curvature regions, and hence we do not allow large detrimental changes to happen. This intuition becomes even more suitable when D is larger than the training set, for example by incorporating unlabeled data, and hence providing a more accurate measure of how pθ changes (in the KL sense). This increase in accuracy should allow for better predictions of large changes in the generalization error as opposed to only the training error. Information matrix argument A second gradients, has which the t pθ(t|x) the gradients ∂L Ex ∂θ that we follow towards a local minima. By using this matrix natural gradient moves in the expected direction of low variance for pθ. As the cost L just evaluates pθ at certain points t(i) for a given x(i), we argue that with high probability expected directions of low variance for pθ correspond to directions of low variance for L. Note that directions of high variance for L indicates direction in which pθ changes quickly which should be reﬂected in large changes of the KL. Therefore, in the same sense as TONGA, natural gradient avoids directions of high variance that can lead to drops in generalization error.  the covariance matrix these are  (cid:17)T(cid:16) ∂ log pθ(t|x)  from looking at uncentered weighted  (cid:16) ∂ log pθ(t|x)  (cid:20)(cid:80)  . Note that  comes an  form of  (cid:17)(cid:21)  Fisher  not  of  ∂θ  ∂θ  Le Roux et al. (2007) proposed, an assumption made in Martens (2010) as well.  3Note that Schraudolph (2002) assumes a form of natural gradient that uses U as a metric, similar to the 4One could argue that p might be such that L has a low curvature while the curvature of KL is much larger. This would happen for example if p is not very sensitive to θ for the values x(i), t(i) provided in the training set, but it is for other pairings of x and t. However we believe that in such a scenario is more useful to move slowly, as the other parings of x and t might be relevant for the generalization error  6  4.2 Natural gradient descent versus second order methods Even though natural gradient is usually assumed to be a second order method it is more useful, and arguably more correct to think of it as a ﬁrst order method. While it makes use of curvature, it is the curvature of the functional manifold and not that of the error function we are trying to minimize. The two quantities are different. For example the manifold curvature matrix is positive semi-deﬁnite by construction while for the Hessian we can have negative curvature. To make this distinction clear we can try to see what information carries the metric that we invert (as it was done in Roux and Fitzgibbon (2010) for Newton’s and Le Roux’s methods). The functional manifold metric can be written as either the expectation of the Hessian ∂2pθ  ∂θ2 or the (see (3) and (4)). The ﬁrst form tells us  expectation of the Fisher Information Matrix  (cid:20)(cid:16) ∂pθ  ∂θ  (cid:17)T  (cid:21)  ∂pθ ∂θ  the that the matrix measures how a change in θ affects the gradients ∂pθ ∂θ of pθ (as the Hessian would do for the error). The second form tells us how the change in the input affects the gradients ∂pθ ∂θ , as the covariance matrix would do for Le Roux’s TONGA. However, while the matrix measures both the effects of a change in the input and θ it does so on the functional behaviour of pθ who acts as a surrogate for the training error. As a consequence we need to look for density functions pθ which are correlated with the training error, as we do in the examples discussed here. Lastly, compared to second order methods, natural gradient lends itself very well to the online optimization regime. In principle, in order to apply natural gradient we need an estimate of the gradient, which can be the stochastic gradient over a single sample and some reliable measure of how our model, through pθ, changes with θ (in the KL sense), which is given by the metric. For e.g. in case of probabilistic models like DBMs, the metric relies only on negative samples obtained from pθ and does not depend on the empirical distribution q at al Desjardins et al. (2013), while for a second order method the Hessian would depend on q. For conditional distributions (as is the case for neural networks), one good choice is to compute the metric on a held out subset of input samples, offering this way an unbiased estimate of how p(t|x) changes with θ. This can easily be done in an online regime. Given that we do not even need to have targets for the data over which we compute the metric, as G integrates out the random variable t, we could even use unlabeled data to improve the accuracy as long as it comes from the same distribution q, which can not be done for second order methods.  5 Natural gradient robustness to overﬁtting We explore the robustness hypothesis from section 4.1 empirically. The results of all experiments carried out are summarized in table 1 present in the Appendix. Firstly we consider the effects of using extra unlabeled data to improve the accuracy of the metric. A similar idea was proposed in Sun et al. (2009). The idea is that for G to do a good job in this robustness sense, it has to accurately predict the change in KL divergence in every direction. If G is estimated from too little data (e.g., a small labeled set) and that data happens to be the training set, then it might “overﬁt” and underestimate the effect of a change in some directions where the training data would tend to push us. To protect us against this, what we propose here is the use a large unlabeled set to obtain a more generalization-friendly metric G. Figure 2 describes the results on the Toronto Face Dataset (TFD), where using unlabeled data results in 83.04% accuracy vs 81.13% without. State of the art is 85%Rifai et al. (2012), though this result is obtained by a larger model that is pre-trained. Hyper-parameters were validated using a grid-search (more details in the Appendix). As you can see from the plot, it suggests that using unlabeled data helps to obtain better testing error, as predicted by our argument in Sec. 4.1. This comes at a price. Convergence (on the training error) is slower than when we use the same training batch. Additionally we explore the effect of using different batches of training data to compute the metric. The full results as well as experimental setup are provided in the Appendix. It shows that, as most second order methods, natural gradient has a tendency to overﬁt the current minibatch if both the metric and the gradient are computed on it. However, as suggested in Vinyals and Povey (2012) using different minibatches for the metric helps as we tend not to ignore directions relevant for other minibatches.  7  Figure 2: (left) train error (cross entropy over the entire training set) on a log scale and (right) test error (percentage of misclassiﬁed examples) as a function of number of updates for the Toronto Faces Dataset. ‘kl, unlabeled‘ stands for the functional manifold version of natural gradient, where the metric is computed over unlabeled data. for ’KL, different training minibatch’ we compute the metric on a different minibatch from the training set, while ’KL, same minibatch’ we compute the metric over the same minibatch we computed the gradient hence matching the standard use of hessian-free. ’covariance’ stands for tonga that uses the covariance matrix as a metric, while msgd is minibatch stochastic gradient descent. note that the x axis was interrupted, in order to improve the visibility of how the natural gradient methods behave.  Figure 3: The plot describes how much the model is inﬂuenced by different parts of an online training set, for the two learning strategies compared (minibatch stochastic gradient descent and natural gradient descent). The x-axis indicates which part (1st 10th, 2nd 10th, etc.) of the ﬁrst half of the data was randomly resampled, while the y-axis measures the resulting variance of the output due to the change in training data.  6 Natural gradient is robust to the order of the training set We explore the regularization effects of natural gradient descent by looking at the variance of the trained model as a function of training samples that it sees. To achieve this we repeat the experiment described in Erhan et al. (2010) which looks at how resampling different fraction of the training set affects the variance of the model and focuses speciﬁcally to the relative higher variance of the early examples. Our intuition is that by forbidding large jumps in the KL divergence of pθ and following the direction of low variance natural gradient will try to limit the amount of overﬁtting that occurs at any stage of learning. We repeat the experiment from Erhan et al. (2010), using the NISTP dataset introduced in Bengio et al. (2011) (which is just the NIST dataset plus deformations) and use 32.7M samples of this data. We divide the ﬁrst 16.3M data into 10 equal size segments. For each data point in the ﬁgure, we ﬁx 9 of the 10 data segments, and over 5 different runs we replace the 10th with 5 different random sets  8  of samples. This is repeated for each of the 10 segments to produce the down curves. By looking at the variance of the model outputs on a held out dataset (of 100K samples) after the whole 32.7M online training samples, we visualize the inﬂuence of each of the 10 segments on the function learnt (i.e., at the end of online training). The curves can be seen in ﬁgure 3. There are two observation to be made regarding this plot. Firstly, it seems that early examples have a relative larger effect on the behaviour of the function than latter ones (phenomena sometimes called early-overﬁtting). This happens for both methods, natural gradient and stochastic gradient descent. The second observation regards the overall variance of the learnt model. Note that the variance at each point on the curve depends on the speed with which we move in functional space. For a ﬁxed number of examples one can artiﬁcially tweak the curves for e.g. by decreasing the learning rate. With a smaller learning rate we move slower, and since the model, from a functional point of view, does not change by much, the variance is lower. In the limit, with a learning rate of 0, the model always stays the same. If we increase the number of steps we take (i.e. measure the variance after k times more samples) the curve recovers some of its shape.This is because we allow the model to move further away from the starting point. In order to be fair to the two algorithms, we use the validation error as a measure of how much we moved in the functional space. This helps us to chose hyper-parameters such that after 32.7M sam- ples both methods achieve the same validation error of 49.8% (see Appendix for hyper-parameters). The results are consistent with our hypothesis that natural gradient avoids making large steps in function space during training, staying on the path that induces least variance. Such large steps may be present with SGD, possibly yielding the model to overﬁt (e.g. getting forced into some quadrant of parameter space based only on a few examples) resulting in different models at the end. By reducing the variance overall the natural gradient becomes more invariant to the order in which examples are presented. Note that the relative variance of early examples to the last re-sampled fraction is about the same for both natural gradient and stochastic gradient descent. However, the amount of variance induced in the learnt model by the early examples for natural gradient is on the same magnitude as the variance induce by the last fraction of examples for MSGD (i.e. in a global sense natural gradient is less sensitive the order of samples it sees).  7 The relationship between Hessian-Free and natural gradient  Hessian-Free as well as Krylov Subspace Descent rely on the extended Gauss-Newton approxima- tion of the Hessian, GN instead of the actual Hessian (see Schraudolph (2002)).  (cid:34)(cid:18) ∂r  (cid:88)  (cid:19)T ∂2 log p(t(i)|x(i))  (cid:18) ∂r  (cid:19)(cid:35)  ∂θ  i  ∂r2  ∂θ  GN =  1 n  (cid:2)JT  r  (cid:0)Et∼˜q(t|x) [HL◦r](cid:1) Jr  (cid:3)  = Ex∼˜q  (14)  The reason is not computational, as computing both can be done equally fast, but rather better behaviour during learning. This is usually assumed to be caused by the fact that the Gauss-Newton is positive semi-deﬁnite by construction, so one needs not worry about negative curvature issues. In this section we show that in fact the extended Gauss-Newton approximation matches perfectly the natural gradient metric, and hence by choosing this speciﬁc approximation, one can view both algorithms as being implementations of natural gradient rather than typical second order methods. The last step of equation (14) is obtained by using the normal assumption that (x(i), t(i)) are i.i.d samples. We will consider the three activation functions and corresponding errors for which the extended Gauss-Newton is deﬁned and show it matches perfectly the natural gradient metric for the same activation. For the linear output units with square errors we can derive the matrix HL◦r as follows:  HL◦rij,i(cid:54)=j = ∂2(cid:80) = ∂2(cid:80)  HL◦rii  k(rk−tk)2 ∂ri∂rj k(rk−tk)2 ∂ri∂ri  9  = ∂2(ri−ti) = ∂2(ri−ti)  ∂rj  ∂ri  = 0  = 2  (15)  (cid:88)  (cid:88)  GN =  1 n  JT r HL◦rJr =  1 n  JT y HL◦yJy =  1 n  x(i),t(i)  x(i),t(i)  x(i)  (cid:88)  y (2I) Jy = 2Ex∈q(x) JT  (cid:2)JT  y Jy  (cid:3)  (16) The result is summarized in equation 16, where we make use of the fact that r = y. It matches the corresponding natural gradient metric, equation (24) from section 3, up to a constant. In the case of sigmoid units with cross-entropy objective (σ is the sigmoid function), HL◦r is  HL◦rij,i(cid:54)=j = ∂2(cid:80) (cid:16)−ti  ∂  k(−tk log(σ(rk))−(1−tk) log(1−σ(rk))) σ(ri) σ(ri)(1−σ(ri))+(1−ti)  ∂ri∂rj  1  1  1−σ(ri) σ(ri)(1−σ(ri))  (cid:17)  = ∂σ(ri)−ti  ∂rj  = 0  (17)  HL◦rii  = = ... = ∂σ(ri)−ti  ∂ri  ∂rj  = σ(ri)(1 − σ(ri))  If we insert this back into the Gauss-Newton approximation of the Hessian and re-write the equation in terms of Jy instead of Jr, we get, again, the corresponding natural gradient metric, equation (10).  (cid:80)  (cid:104)  (cid:80) (cid:105)  (cid:17)  (cid:16)  GN = 1 n  = Ex∼˜q  x(i),t(i) JT y diag JT  r HL◦rJr = 1 n Jy  y(1−y)  1  x(i) JT  r diag (y(1 − y)) diag  1  y(1−y)  diag (y(1 − y)) Jr  (cid:16)  (cid:17)  (18) The last matching activation and error function that we consider is the softmax with cross-entropy.  HL◦rij,i(cid:54)=j = ∂2(cid:80)  HL◦rii  k(−tk log(φ(rk)))  k(tkφ(ri))−ti  = ... = ∂φ(ri)−ti  ∂ri∂rj  = φ(ri) − φ(ri)φ(ri)  ∂rj  ∂ri  = ∂(cid:80)  = −φ(ri)φ(rj)  (19)  Equation (20) starts from the natural gradient metric and singles out a matrix M in the formula such that the metric can be re-written as the product JT r MJr (similar to the formula for the Gauss-Newton approximation). In (21) we show that indeed M equals HL◦r and hence the natural gradient metric is the same as the extended Gauss-Newton matrix for this case as well. Note that δ is the Kronecker delta, where δij,i(cid:54)=j = 0 and δii = 1.  (cid:20)  (cid:18)(cid:80)o  = Ex∼˜q  JT r  k=1  1 yk  (cid:16) ∂yk  (cid:17)T(cid:16) ∂yk  ∂r  ∂r  (cid:17)(cid:19)  (cid:21)  Jr  (20)  G = Ex∼˜q  (cid:20)(cid:80)o (cid:0)JT (cid:80) Mij,i(cid:54)=j = (cid:80)o = (cid:80)o  = 1 N  k=1  x(i)  Mii  1 yk 1 yk  k=1  k=1  1 yk  r MJr  ∂yk ∂ri ∂yk ∂yi  (cid:21)  ∂θ  ∂yk ∂θ  (cid:16) ∂yk (cid:17)T (cid:1) =(cid:80)o i ((cid:80)o  ∂yk ∂rj ∂yk ∂rj  = y2  k=1(δki − yi)yk(δkj − yj) = yiyj − yiyj − yiyj = −φ(ri)φ(rj)  k=1 yk) + yi − 2y2  i = φ(ri) − φ(ri)φ(ri)  (21)  8 Conclusion In this paper we re-derive natural gradient, by imposing that at each step we follow the direction that minimizes the error function while resulting in a constant change in the KL-divergence of the probability density function that represents the model. This approach minimizes the amount of differential geometry needed, making the algorithm more accessible. We show that natural gradient, as proposed by Amari, is not the same as the algorithm proposed by Le Roux et al, even though it has the same name. We highlight a few differences of each algorithm and hypothesis that Amari’s natural gradient should exhibit the same robustness against overﬁtting that Le Roux’s algorithm has, but for different reasons.  10  We explore empirically this robustness hypothesis, by proving better test errors when unlabeled data is used to improve the accuracy of the metric. We also show that natural gradient may reduce the worrisome early specialization effect previously observed with online stochastic gradient descent applied to deep neural nets, and reducing the variance of the resulting learnt function (with respect to the sampled training data). By computing the speciﬁc metrics needed for standard output activation functions we showed that the extended Gauss-Newton approximation of the Hessian coincides with the natural gradient metric (provided that the metric is estimated over the same batch of data as the gradient). Given this identity one can re-interpret the recently proposed Hessian-Free and Krylov Subspace Descent as natural gradient. Finally we point out a few differences between typical second order methods and natural gradient. The latter seems more suitable for online or probabilistic models, and relies on a surrogate probabil- ity density function pθ in place of the error function in case of deterministic models.  Acknowledgements We would like to thank Guillaume Desjardens, Aaron Courville, Li Yao, David Warde-Farley and Ian Goodfellow for the interesting discussion on the topic, or for any help provided during the development of this work. Reviewers at ICLR were particularly helpful, and we want to thank them, especially one of the reviewers that suggested several links with work from the reinforcement learning community. Also special thanks goes to the Theano development team as well (particularly to Frederic Bastien, Pascal Lamblin and James Bergstra) for their help. We acknowledge NSERC, FQRNT, CIFAR, RQCHP and Compute Canada for the resources they provided.  References Amari, S. (1985). Differential geometrical methods in statistics. Lecture notes in statistics, 28. Amari, S. (1997). Neural learning in structured parameter spaces - natural Riemannian gradient. In In Advances  in Neural Information Processing Systems, pages 127–133. MIT Press.  Amari, S., Kurata, K., and Nagaoka, H. (1992). Information geometry of Boltzmann machines. IEEE Trans.  on Neural Networks, 3, 260–271.  Amari, S.-I. (1998). Natural gradient works efﬁciently in learning. Neural Comput., 10(2), 251–276. Arnold, L., Auger, A., Hansen, N., and Olivier, Y. (2011). Information-geometric optimization algorithms: A  unifying picture via invariance principles. CoRR, abs/1106.3708.  Bengio, Y., Bastien, F., Bergeron, A., Boulanger-Lewandowski, N., Breuel, T., Chherawala, Y., Cisse, M., Cˆot´e, M., Erhan, D., Eustache, J., Glorot, X., Muller, X., Pannetier Lebeuf, S., Pascanu, R., Rifai, S., Savard, F., and Sicard, G. (2011). Deep learners beneﬁt more from out-of-distribution examples. In JMLR W&CP: Proc. AISTATS’2011.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy).  Chapelle, O. and Erhan, D. (2011). Improved Preconditioner for Hessian Free Optimization. In NIPS Workshop  on Deep Learning and Unsupervised Feature Learning.  Choi, S.-C. T., Paige, C. C., and Saunders, M. A. (2011). MINRES-QLP: A Krylov subspace method for  indeﬁnite or singular symmetric systems. 33(4), 1810–1836.  Desjardins, G., Pascanu, R., Courville, A., and Bengio, Y. (2013). Metric-free natural gradient for joint-training  of boltzmann machines. CoRR, abs/1301.3545.  Erhan, D., Courville, A., Bengio, Y., and Vincent, P. (2010). Why does unsupervised pre-training help deep  learning? In JMLR W&CP: Proc. AISTATS’2010, volume 9, pages 201–208.  Kakade, S. (2001). A natural policy gradient. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, NIPS,  pages 1531–1538. MIT Press.  Le Roux, N., Manzagol, P.-A., and Bengio, Y. (2007). Topmoumoute online natural gradient algorithm. Tech-  nical Report 1299, D´epartement d’informatique et recherche op´erationnelle, Universit´e de Montr´eal.  Le Roux, N., Manzagol, P.-A., and Bengio, Y. (2008). Topmoumoute online natural gradient algorithm. In  NIPS’07.  11  Le Roux, N., Bengio, Y., and Fitzgibbon, A. (2011). Improving ﬁrst and second-order methods by modeling  uncertainty. In Optimization for Machine Learning. MIT Press.  Martens, J. (2010). Deep learning via hessian-free optimization. In ICML, pages 735–742. Nocedal, J. and Wright, S. J. (2000). Numerical Optimization. Springer. Park, H., Amari, S.-I., and Fukumizu, K. (2000). Adaptive natural gradient learning algorithms for various  stochastic models. Neural Networks, 13(7), 755 – 764.  Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difﬁculty of training recurrent neural networks. CoRR,  abs/1211.5063.  Pearlmutter, B. A. (1994). Fast exact multiplication by the hessian. Neural Computation, 6, 147–160. Peters, J. and Schaal, S. (2008). Natural actor-critic. (7-9), 1180–1190. Rattray, M., Saad, D., and Amari, S. I. (1998). Natural Gradient Descent for On-Line Learning. Physical  Review Letters, 81(24), 5461–5464.  Rifai, S., Bengio, Y., Courville, A., Vincent, P., and Mirza, M. (2012). Disentangling factors of variation for facial expression recognition. In Proceedings of the European Conference on Computer Vision (ECCV 6), pages 808–822.  Roux, N. L. and Fitzgibbon, A. W. (2010). A fast natural newton method. In J. F¨urnkranz and T. Joachims,  editors, ICML, pages 623–630. Omnipress.  Schaul, T. (2012). Natural evolution strategies converge on sphere functions. In Genetic and Evolutionary  Computation Conference (GECCO).  Schraudolph, N. N. (2001). Fast curvature matrix-vector products. In ICANN, pages 19–26. Schraudolph, N. N. (2002). Fast curvature matrix-vector products for second-order gradient descent. Neural  Computation, 14(7), 1723–1738.  Sohl-Dickstein, J. (2012). The natural gradient by analogy to signal whitening, and recipes and tricks for its  use. CoRR, abs/1205.1828.  Sun, Y., Wierstra, D., Schaul, T., and Schmidhuber, J. (2009). Stochastic search using the natural gradient. In  ICML, page 146.  Sutskever, I., Martens, J., and Hinton, G. E. (2011). Generating text with recurrent neural networks. In ICML,  pages 1017–1024.  Vinyals, O. and Povey, D. (2012). Krylov Subspace Descent for Deep Learning. In AISTATS.  Appendix  The Fisher Information Matrix form can be obtain from the expected value of the Hessian :  8.1 Expected Hessian to Fisher Information Matrix  (cid:20)  (cid:21)  Ez  − ∂2 log pθ  ∂θ  = Ez  (cid:34)  (cid:35)  (cid:34)  ∂pθ ∂θ  − ∂ 1  pθ ∂θ  (cid:32)(cid:88) (cid:34)(cid:18) ∂ log pθ(z)  pθ(z)  = Ez  pθ(z)  − 1  ∂2pθ ∂θ2 +  (cid:33) (cid:19)T(cid:18) ∂ log pθ(z)  (cid:34)(cid:18) ∂ log pθ(z) (cid:19)(cid:35)  + Ez  ∂θ  z  ∂θ  ∂θ  = − ∂2 ∂θ2  = Ez  (cid:19)(cid:35)  (cid:18) 1 (cid:19)T(cid:18) 1 (cid:19)T(cid:18) ∂ log pθ(z)  ∂pθ ∂θ  pθ  pθ  ∂pθ ∂θ  (cid:19)(cid:35)  ∂θ  (22)  8.2 Derivation of the natural gradient metrics  8.2.1 Linear activation function  In the case of linear outputs we assume that each entry of the vector t, ti comes from a Gaussian distribution centered around yi(x) with some standard deviation β. From this it follows that:  pθ(t|x) =  N (ti|y(x, θ)i, β2)  (23)  o(cid:89)  i=1  12  (cid:20)(cid:80)o  i=1  Et∼N (t|y(x,θ),β2I)  i=1  i=1  (cid:20) (cid:20)(cid:80)o (cid:20)(cid:80)o (cid:20)(cid:80)o (cid:20)(cid:80)o (cid:20)(cid:80)o (cid:20)(cid:80)o (cid:2)JT  i=1  i=1  i=1  G = Ex∼˜q = Ex∼˜q = Ex∼˜q = Ex∼˜q = Ex∼˜q = Ex∼˜q = β2Ex∼˜q = β2Ex∼˜q  (cid:20) (cid:20) (cid:20) (cid:20) (cid:20)  i=1 y Jy  Et∼N (t|y(x,θ),β2I) Et∼N (t|y(x,θ),β2I) Et∼N (t|y(x,θ),βI) Et∼N (t|y(x,θ),β2I) Et∼N (t|y(x,θ),β2I)  (cid:17)T(cid:16) ∂yi  (cid:20)(cid:16) ∂yi (cid:3)  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  (cid:17)T(cid:16) ∂ log pθ(ti|y(x)i (cid:17)(cid:21)(cid:21)(cid:21) (cid:17)T(cid:16) ∂(ti−yi)2 (cid:17)(cid:21)(cid:21)(cid:21) (cid:17)T(cid:16) ∂yi (cid:17)(cid:21)(cid:21) (cid:17)T(cid:16) ∂yi (cid:17)(cid:21)(cid:21) (cid:17)T(cid:16) ∂yi (cid:17)(cid:21)(cid:21) (cid:17)T(cid:16) ∂yi  (cid:16) ∂ logθ p(ti|y(x)i (cid:20)(cid:16) ∂(ti−yi)2 (cid:20) (ti − yi)2(cid:16) ∂yi (cid:2)(ti − yi)2(cid:3)(cid:16) ∂yi (cid:2)(ti − yi)2(cid:3)(cid:16) ∂yi (cid:2)(ti − yi)2(cid:3)(cid:16) ∂yi (cid:17)(cid:21)(cid:21)  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  (cid:17)(cid:21)(cid:21)  (24)  8.2.2 Sigmoid activation function  In the case of the sigmoid units, i,e, y = sigmoid(r), we assume a binomial distribution which gives us:  p(t|x) =  i (1 − yi)1−ti yti  (25)  (cid:89)  i  log p gives us the usual cross-entropy error used with sigmoid units. We can compute the Fisher information matrix as follows:  (cid:20)(cid:80)o  Et∼p(t|x)  (cid:20) (cid:20)(cid:80)o (cid:104)  1  yi(1−yi)  i=1 y diag( JT  1  y(1−y) )Jy  i=1  (ti−yi)2 i (1−yi)2 y2 ∂yi ∂θ  (cid:16) ∂yi (cid:17)T (cid:105)  ∂θ  G = Ex∼˜q = Ex∼˜q = Ex∼˜q  (cid:21)(cid:21)  ∂yi ∂θ  (cid:17)T  (cid:16) ∂yi (cid:21)  ∂θ  8.2.3 Softmax activation function For the softmax activation function, y = softmax(r), p(t|x) takes the form of a multinomial:  yti i  o(cid:89) (cid:18) ∂yi  i  (cid:19)T ∂yi  (cid:35)  ∂θ  ∂θ  p(t|x) =  (cid:34) o(cid:88)  1 yi  G = Ex∼˜q  8.3  Implementation Details  i  We have implemented natural gradient descent using a truncated Newton approach similar to the pipeline proposed by Pearlmutter (1994) and used by Martens (2010). In order to better deal with singular and ill-conditioned matrices we use the MinRes-QLP algorithm (Choi et al., 2011) instead of linear conjugate gradient. Both Minres-QLP as well as linear conjugate gradient can be found im- plemented in Theano at https://github.com/pascanur/theano optimize. We used the Theano library (Bergstra et al., 2010) which allows for a ﬂexible implementation of the pipeline, that can automat- ically generate the computational graph of the metric times some vector for different models:  13  (26)  (27)  (28)  import theano.tensor as TT # ‘params‘ is the list of Theano variables containing the parameters # ‘vs‘ is the list of Theano variable representing the vector ‘v‘ # with whom we want to multiply the metric # ‘Gvs‘ is the list of Theano expressions representing the product # between the metric and ‘vs‘  # ‘out_smx‘ is the output of the model with softmax units Gvs = TT.Lop(out_smx,params,  TT.Rop(out_smx,params,vs)/(out_smx*out_smx.shape[0]))  # ‘out_sig‘ is the output of the model with sigmoid units Gvs = TT.Lop(out_sig,params,  TT.Rop(out_sig,params,vs)/(out_sig*  (1-out_sig)* out_sig.shape[0]))  # ‘out‘ is the output of the model with linear units Gvs = TT.Lop(out,params,TT.Rop(out,params,vs)/out.shape[0])  The full pseudo-code of the algorithm (which is very similar to the one for Hessian-Free) is given below. The full Theano implementation can be retrieved from https://github.com/pascanur/natgrad.  Algorithm 1 Pseudocode for natural gradient algorithm  # ‘gfn‘ is a function that computes the metric times some vector gfn ← (lambda v → Gv) while not early stopping condition do  ∂θ  g ← ∂L # linear cg solves the linear system Gx = ∂L ng← linear cg(gfn, g, max iters = 20, rtol=1e-4) # γ is the learning rate θ ← θ − γ ng  ∂θ  end while  Even though we are ensured that G is positive semi-deﬁnite by construction, and MinRes-QLP is able to ﬁnd a suitable solutions in case of singular matrices, we still use a damping strategy for two reasons. The ﬁrst one is that we want to take in consideration the inaccuracy of the metric (which is approximated only over a small minibatch). The second reason is that natural gradient makes sense only in the vicinity of θ as it is obtained by using a Taylor series approximation, hence (as for ordinary second order methods) it is appropriate to enforce a trust region for the gradient. See Schaul (2012), where the convergence properties of natural gradient (in a speciﬁc case) are studied. Following the functional manifold interpretation of the algorithm, we can recover the Levenberg- Marquardt heuristic used in Martens (2010) by considering a ﬁrst order Taylor approximation, where for any function f,  f  θt − ηG−1 ∂f (θt) ∂θt  ≈ f (θt) − η  ∂f (θt)  ∂θt  G−1 ∂f (θt) ∂θt  T  (29)  This gives as the reduction ratio given by equation (30) which can be shown to behave identically with the one in Martens (2010).  (cid:32)  T(cid:33)  (cid:16)  T(cid:17) − f (θt)  f  ρ =  θt − ηG−1 ∂f (θt) −η ∂f (θt)  ∂θt  ∂θt  G−1 ∂f (θt)  T  ∂θt  14  (30)  8.4 Additional experimental results  For the one hidden unit auto-encoder we selected hyper-parameters such to ensure stability of train- ing, while converging as fast as possible to a minima. We compute the inverse of the metric or Hessian exactly (as it is just a 2 by 2 matrix). The learning rate for SGD is set to .1, for Amari’s natural gradient .5 and for the covariance of gradience 1. (Newton’s method usually does not use a learning rate). We damped the Hessian and the covariance of gradients by adding I and Amari’s metric using 0.01 · I.  8.5 Restricted MNIST experiment  For the restricted MNIST, we train a one hidden layer MLP of 1500 hidden units. The hyper- parameters where chosen based on a grid search over learning rate, damping factor and damping strategy. Note that beside using unlabeled data, the regularization effect of natural gradient is strongly connected to the damping factor which accounts for the uncertainty in the metric (in a similar way to how it does in the uncentered covariance version of natural gradient). The minibatch size was kept constant to 2500 samples for natural gradient methods and 250 for MSGD. We used a constant learning rate and used a budged of 2000 iterations for natural gradient and 40000 iterations for MSGD. We used a learning rate of 1.0 for MSGD and 5.0 for the functional manifold NGD using unlabeled data or the covariance based natural gradient. For the functional manifold NGD using either the same training minibatch or a different batch from the training set for computing the metric we set the learning rate to 0.1. We use a Levenberg-Marquardt heuristic only when using unlabeled data, otherwise the damping factor was kept constant. Its initial value was 2.0 for when using unlabeled data, and 0.01 for every case except when using the covariance of the gradients as the metric, when is set to 0.1.  Figure 4: (left) train error (cross entropy over the entire training set) on a log scale in order to im- prove visibility and (right) test error (percentage of misclassiﬁed examples) as a function of number of updates for the restricted mnist dataset.  8.6 MNIST experiment  The model used has 3 layers, where the ﬁrst two are convolutional layers both with ﬁlters of size 5x5. We used 32 ﬁlters on the ﬁrst layer and 64 on the second. The last layer forms an MLP with 750 hidden units. We used minibatches of 10000 examples (for both the gradient and the metric), and a t decaying learning rate strategy. The learning rate was kept constant for the ﬁrst 200 updates and 1 then it was computed based on the formula , where t is the number of the current update. We used a budged ot 2000 update. The learning rate was set to 0.5 for the functional manifold approach when using a different batch for computing the metric and 1.0 when using the same batch for computing the metric, or for using the covariance of gradients as metric. We use a Levenberg-Marquardt heuristic to adapt the damping  1+ t−200  l0  20  15  Table 1: Results on the three datasets considered (restricted MNIST, MNIST and TFD). Note that different models are used for different datasets. The training error is given as cross-entropy error, while the test error is percentage of miss-classiﬁed examples. The algorithms name are the same as in the legend of ﬁgure 2  DATA SET  DATA FOLD MSGD KL, UNLABELED KL, DIFFERENT KL, SAME  COVARIANCE  RESTRICTED  MNIST MNIST  TFD  TRAIN TEST TRAIN TEST TRAIN TEST  0.0523 5.22%  0.0017 4.63%  0.054 16.96%  BATCH 0.0023 4.91% 0.0011 0.82%  0.0006 4.74% 0.024 1.07%  BATCH 0.0012 4.89% 0.00010 0.78% 0.098 18.87%  factor which initially is 5.0 for the functional manifold approach, and a constant damping factor of 0.1 for using the covariance as metric. These values were validated by a grid search.  8.7 TFD experiment  The Toronto Face Dataset (TFD), has a large amount of unlabeled data of poorer quality than the training set. To ensure that the noise in the unlabeled data does not affect the metric, we compute the metric over the training batch plus unlabeled samples. We used a three hidden layer model, where the ﬁrst layer is a convolutional layer of 300 ﬁlters of size 12x12. The second two layers from a 2 hidden layer MLP of 2048 and 1024 hidden units respectively. For the TFD experiment we used the same decaying learning rate strategy introduced above, in subsection 8.6, where we computed gradients over the minibatch of 960 examples. When using the unlabeled data, we added 480 unlabeled examples to the 960 used to compute the gradient (therefore the metric was computed over 1440 examples) otherwise we used the same 960 examples for the metric. In both cases we used an initial damping factor of 8, and the Levenberg-Marquardt heuristic to adapt this damping value. Initial learning rate l0 was set to 1 in both cases. Note that we get only 83.04% accuracy on this dataset, when the state of the art is 85.0% Rifai et al. (2012), but our ﬁrst layer is roughly 3 times smaller (300 ﬁlters versus 1024).  8.8 NISTP exepriment (robustness to the order of training samples)  The model we experimented with was an MLP of only 500 hidden units. We compute the gradients for both MSGD and natural gradient over minibatches of 512 examples. In case of natural gradient we compute the metric over the same input batch of 512 examples. Additionally we use a constant damping factor of 3 to account for the noise in the metric (and ill-conditioning since we only use batches of 512 samples). The learning rates were kept constant, and we use .2 for the natural gradient and .1 for MSGD.  16  Figure 5: Train and test error (cross entropy) on a log scale as a function of number of updates for the MNIST dataset. The legend is similar to ﬁgure 2  17  ","The aim of this paper is three-fold. First we show that Hessian-Free(Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can bedescribed as implementations of natural gradient descent due to their use ofthe extended Gauss-Newton approximation of the Hessian. Secondly we re-derivenatural gradient from basic principles, contrasting the difference between twoversions of the algorithm found in the neural network literature, as well ashighlighting a few differences between natural gradient and typical secondorder methods. Lastly we show empirically that natural gradient can be robustto overfitting and particularly it can be robust to the order in which thetraining data is presented to the model."
1301.3516,2013,Learnable Pooling Regions for Image Classification  ,"['Mateusz Malinowski', 'Mario Fritz']",https://arxiv.org/pdf/1301.3516.pdf,"5 1 0 2     y a M 5         ]  V C . s c [      3 v 6 1 5 3  .  1 0 3 1 : v i X r a  Learnable Pooling Regions for Image Classiﬁcation  Mateusz Malinowski  Mario Fritz  Computer Vision and Multimodal Computing  Computer Vision and Multimodal Computing  Max Planck Institute for Informatics  Campus E1 4, 66123 Saarbr¨ucken, Germany  mmalinow at mpi-inf.mpg.de  Max Planck Institute for Informatics  Campus E1 4, 66123 Saarbr¨ucken, Germany  mfritz at mpi-inf.mpg.de  Abstract  Biologically inspired, from the early HMAX model to Spatial Pyramid Match- ing, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial in- formation. Despite the predominance of this approach in current recognition sys- tems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme – including previously proposed hand-crafted pooling schemes as a particular in- stantiation. In our work, we investigate the role of different regularization terms showing that the smooth regularization term is crucial to achieve strong perfor- mance using the presented architecture. Finally, we propose an efﬁcient and par- allel method to train the model. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets – in particular improving the state-of-the-art to 56.29% on the latter.  1  Introduction  Spatial pooling plays a crucial role in modern object recognition and detection systems. Motivated from biology [Riesenhuber and Poggio, 2009] and statistics of locally orderless images [Koenderink and Van Doorn, 1999], the spatial pooling approach has been found useful as an intermediate step of many today’s computer vision methods. For instance, the most popular visual descriptors such as SIFT [Lowe, 2004] and HOG [Dalal and Triggs, 2005], which compute local histograms of gradi- ents, can be in fact seen as a special version of the spatial pooling strategy. In order to form more robust features under translation or small object deformations, activations of the codes are pooled over larger areas in a spatial pyramid scheme [Lazebnik et al., 2006, Yang et al., 2009]. Unfortu- nately, this critical decision, namely the spatial division, is most prominently based on hand-crafted algorithms and therefore data independent.  Related Work As large amounts of training data is available to us today,, there is an increasing interest to push the boundary of learning based approaches towards fully optimized and adaptive architectures where design choices, that would potentially constrain or bias a model, are kept to a minimum. Neural networks have a great tradition of approaching hierarchical learning problems and training intermediate representations [Ranzato et al., 2007, Le et al., 2012a]. Along this line, we propose a learnable spatial pooling strategy that can shape the pooling regions in a discriminative manner. Our architecture has a direct interpretation as a pooling strategy and therefore subsumes popular spatial pyramids as a special case. Yet we have the freedom to investigate different regular- ization terms that lead to new pooling strategies when optimized jointly with the classiﬁer. Recent progress has been made in learning pooling regions in the context of image classiﬁcation using the Spatial Pyramid Matching (SPM) pipeline [Lazebnik et al., 2006, Yang et al., 2009]. Jia  1  and Huang [2011], Jia et al. [2012] and Feng et al. [2011] have investigated how to further liber- ate the recognition from preconceptions of the hand crafted recognition pipelines, and include the pooling strategy into the optimization framework jointly with the classiﬁer. However, these methods still make strong assumptions on the solutions that can be achieved. For instance Jia and Huang [2011] optimizes binary pooling strategies that are given by the superposition of rectangular basis functions, and Feng et al. [2011] ﬁnds pooling regions by applying a linear discriminant analysis for individual pooling strategies and training a classiﬁer afterwards. Also as opposed to Ranzato and Hinton [2010], we aim for discriminative pooling over large neighborhoods in the SPM fashion where the information about the image class membership is available during training.  Outline We question restrictions imposed by the above methods and suggest to learn pooling strategies under weaker assumptions. Indeed, our method discovers new pooling shapes that were not found previously as they were suppressed by the more restrictive settings. The generality that we are aiming for comes at the price of a high dimensional parameters space. This manifests in a complex optimization problem that is more demanding on memory requirements as well as computations needs, not to mention a possibility of over-ﬁtting. Therefore, we also discuss two approximations to our method. First approximation introduces a pre-pooling step and therefore reduces the spatial dimension of the codes. The second approximation divides the codes into a set of smaller batches (subset of codes) that can be optimized independently and therefore in parallel. Finally, we evaluate our method on the CIFAR-10 and show strong improvements over hand-crafted pooling schemes in the regime of small dictionaries where our more ﬂexible model shows its capa- bility to make best use of the representation by exploring spatial pooling strategies speciﬁc to each coordinate of the code. Despite the diminishing return, the performance improvements persist up to largest codes we have investigated. We also show strong classiﬁcation performance on the CIFAR- 100 dataset where our method outperforms, to the best of our knowledge, the state-of-the-art.  2 Method  As opposed to the methods that use ﬁxed spatial pooling regions in the object classiﬁcation task [Lazebnik et al., 2006, Yang et al., 2009] our method jointly optimizes both the classiﬁer and the pooling regions. In this way, the learning signal available in the classiﬁer can help shaping the pooling regions in order to arrive at better pooled features.  2.1 Parameterized pooling operator  be expressed as Σ(U ) := (cid:80)M  The simplest form of the spatial pooling is computing histogram over the whole image. This can j=1 uj, where uj ∈ RK is a code (out of M such codes) and an index j refers to the spatial location that the code originates from1. A code is an encoded patch extracted from the image. The proposed method is agnostic to the patch extraction method and encoding scheme. Since the pooling approach looses spatial information of the codes, Lazebnik et al. [2006] proposed to ﬁrst divide the image into subregions, and afterwards to create pooled features by concatenating histograms computed over each subregion. There are two problems with such an approach: ﬁrst, the division is largely arbitrary and in particular independent of the data; second, discretization artifacts occur as spatially nearby codes can belong to two different regions as the ’hard’ division is made. In this paper we address both problems by using a parameterized version of the pooling operator  Θw(U ) :=  wj ◦ uj  (1)  where a ◦ b is the element-wise multiplication. Standard spatial division of the image can be re- covered from Formula 1 by setting the vectors wj either to a vector of zeros 0, or ones 1. For instance, features obtained from dividing the image into 2 subregions can be recovered from Θ by  j=1  1That is j = (x, y) where x and y refer to the spatial location of the center of the extracted patch.  2  M(cid:88)  2  2  concatenating two vectors:(cid:80) M where(cid:8)1, ..., M  (cid:9) and(cid:8) M  j=1 1 ◦ uj +(cid:80)M 2 + 1, ..., M(cid:9) refer to the ﬁrst and second half of the image respectively.  2 +1 0 ◦ uj, and(cid:80) M  j=1 0 ◦ uj +(cid:80)M  2 +1 1 ◦ uj,  In general, let F := {Θw}w be a family of the pooling functions given by Eq. 1, parameterized by the vector w, and let w∗,l be the ’best’ parameter chosen from the family F based on the initial conﬁguration l and a given set of images.2 First row of Figure 2 shows four initial conﬁgurations that mimic the standard 2-by-2 spatial image division. Every initial conﬁguration can lead to different w∗,l as it is shown in Figure 2. Clearly, the family F contains all possible ’soft’ and ’hard’ spatial divisions of the image, and therefore can be considered as their generalization.  j= M  j= M  2  2.2 Learnable pooling regions  In SPM architectures the pooling weights w are designed by hand, here we aim for joint learning w together with the parameters of the classiﬁer. Intuitively, the classiﬁer during training has access to the classes that the images belong to, and therefore can shape the pooling regions. On the other hand, the method aggregates statistics of the codes over such learnt regions and pass them to the classiﬁer allowing to achieve higher accuracy. Such joint training of the classiﬁer and the pooling regions can be done by adapting the backpropagation algorithm [Bishop, 1999, LeCun et al., 1998], and so can be interpreted as a densely connected multilayer perceptron [Collobert and Bengio, 2004, Bishop, 1999]. Consider a sampling scheme and an encoding method producing M codes each K dimensional. Every coordinate of the code is an input layer for the multilayer perceptron. Then we connect every j . Since the receptive j-th input unit at the layer k to the l-th pooling unit ak ﬁeld of the pooling unit ak j , and so in the vector notation  l via the relation wk l consists of all codes at the layer k, we have ak  l :=(cid:80)M  j=1 wk  ljuk  ljuk  al :=  j ◦ uj = Θwl (U ) wl  (2)  M(cid:88)  j=1  Next, we connect all pooling units with the classiﬁer allowing the information to circulate between the pooling layers and the classiﬁer. Although our method is independent of the choice of a dictionary and an encoding scheme, in this work we use K-means with triangle coding fk(x) := max{0, µ(z) − zk} [Coates et al., 2011]. Similarly, every multi-class classiﬁer that can be interpreted in terms of an artiﬁcial neural network can be used. In our work we employ logistic regression. This classiﬁer is connected to the pooling units via the formula  1{y(i) = j} log p(y(i) = j|a(i); Θ)  (3)  D(cid:88)  C(cid:88)  i=1  j=1  J(Θ) := − 1 D  where D denotes the number of all images, C is the number of all classes, y(i) is a label assigned to the i-th input image, and a(i) are responses from the ’stacked’ pooling units [al]l for the i-th image3. We use the logistic function to represent the probabilities: p(y = j|x; Θ) := l x). Since the classiﬁer is connected to the pooling units, our task is to learn jointly the pooling parameters W together with the classiﬁer parameters Θ, where W is the matrix containing all pooling weights. Finally, we use standard gradient descent algorithm that updates the parameters using the following ﬁxed point iteration  exp(θT j x) l=1 exp(θT  (cid:80)C  (4) where in our case X is a vector consisting of the pooling parameters W and the classiﬁer parameters Θ. In practice, however, we employ a quasi-Newton algorithm LBFGS4.  X t+1 := X t − γ∇J(X t)  2 We will show the learning procedure that can select such parameter vectors in the following subsection. 3Providing the codes U (i) are collected from the i-th image and a(i) ]l. 4The algorithm, developed by Mark Schmidt, can be downloaded from the following webpage:  := Θwl (U (i)) then a(i) := [a(i)  l  l  http://www.di.ens.fr/ mschmidt/Software/minFunc.html  3  2.3 Regularization terms  and(cid:80)  l2  k ||W k||2  In order to improve the generalization, we introduce regularization of our network as we deal with a large number of the parameters. For the classiﬁcation Θ and pooling parameters W , we employ a simple L2 regularization terms: ||Θ||2 . We improve the interpretability of the pooling weights as well as to facilitate a transfer among models by adding a projection onto a unit cube. To reduce quantization artifacts of the pooling strategy as well as to ensure smoothness of the output w.r.t. small translations of the image, the model penalizes weights whenever the pooling surface is non-smooth. This can be done by measuring the spatial variation, that is ||∇xW k||2 + ||∇yW k||2 for every layer k. This regularization enforces soft transition between the pooling l2 subregions. Every regularization term comes with its own hyper-parameter set by cross-validation. The overall objective that we want to optimize is  l2  l2  1{y(i) = j} log p(y(i) = j|a(i); Θ)  minimize  JR(Θ, W ) :=  W ,Θ  D(cid:88) C(cid:88) (cid:0)||∇xW||2  ||Θ||2  j=1  i=1  +  l2  α2 2  l2  − 1 D  +  +  α1 2 α3 2  l2  ||W||2 + ||∇yW||2  l2  (cid:1)  (5)  subject to W ∈ [0, 1]K×M×L  where al is the l-th pooling unit described by Formula 2, and ||W||l2 is the Frobenius norm.  2.4 Approximation of the model  The presented approach is demanding to train in the means of the CPU time and memory storage when using high dimensional representations. That is, the number of the pooling parameters to learn grows as K × M × L, where K is dimensionality of codes, M is the number of patches taken from the image and L is the number of pooling units. Therefore, we propose two approximations to our method making the whole approach more scalable towards bigger dictionaries. However, we emphasize that learnt pooling regions have very little if any overhead compared to standard spatial division approaches at test time. First approximation does a ﬁne-grained spatial partition of the image, and then pools the codes over such subregions. This operation, we call it a pre-pooling step, reduces the number of considered spatial locations by the factor of the pre-pooling size. For instance, if we collect M codes and the pre-pooling size is S per dimension, then we reduce the number of codes to a number M S2 . The pre- pooling operation ﬁts well into our generalization of the SPM architectures as by choosing S := M 2 we obtain a weighted quadrants scheme. Moreover, the modeler has the option to start with the larger S when little data is available and gradually decreases S as more parameters can be learnt using more data. D batches, each D dimensional The second approximation divides a K dimensional code into K (where D ≤ K and K is divisible by D). Then we train our model on all such batches in parallel to obtain the pooling weights. Later, we train the classiﬁer on top of the concatenation of the trained, partial models. As opposed to Le et al. [2012b] our training is fully independent and doesn’t need communication between different machines. Since the ordering of the codes is arbitrary, we also consider D dimensional batches formed from the permuted version of the original codes, and combine them together with the concatenated batches to boost the classiﬁcation accuracy (we call this approximation redundant batches). Given a ﬁxed sized dictionary, this approximation performs slightly better, although it comes at the cost of increased number of features due to the redundant batches.  4  Finally, our approximations not only lead to a highly parallel training procedure with reduced mem- ory requirements and computational demands, but also have shown to greatly reduce the number of required iterations as they tend to converge roughly 5 times faster than the full model on large dictionaries.  3 Experimental Results  We evaluate our method on the CIFAR-10 and CIFAR-100 datasets [Krizhevsky and Hinton, 2010]. Furthermore, we provide insights into the learnt pooling strategies as well as investigate transfer between datasets. In this section we describe our experimental setup, and present our results on both datasets.  3.1 CIFAR-10 and CIFAR-100 datasets  The CIFAR-10 and CIFAR-100 datasets contain 50000 training color images and 10000 test color images from respectively 10 and 100 categories, with 6000 and 600 images per class respectively. All images have the same size: 32 × 32 pixels, and were sampled from the 80 million tiny images dataset [Torralba et al., 2008].  3.2 Evaluation pipeline  In this work, we follow the Coates and Ng [2011] pipeline. We extract normalized and whitened 6 × 6 patches from images using a dense, equispaced grid with a unit sample spacing. As the next step, we employ the K-means assignment and triangle encoding [Coates and Ng, 2011, Coates et al., 2011] to compute codes – a K-dimensional representation of the patch. We classify images using either a logistic regression, or a linear SVM in the case of transferred pooling regions. Optionally we use two approximations described in subsection 2.4. As we want to be comparable to Coates et al. [2011], who use a spatial division into 2-by-2 subregions which results in 4 · K pooled features, we use 4 pooling units. Furthermore, we use standard division (ﬁrst row of Figure 2) as an initialization of our model. To learn parameters of the model we use the limited-memory BFGS algorithm (details are described in subsection 2.2), and limit the number iterations to 3000. After the training, we can also concate- l=1. This yields a 4· K dimensional nate the results of the parameterized pooling operator [Θwl (U )]4 feature vector that can be again fed into the classiﬁer, and trained independently with the already trained pooling regions. We call this procedure transfer of pooling regions. The reason behind the transfer is threefold. Firstly, we can combine partial models trained with our approximation in batches to a full, originally intractable, model5. Secondly, the transfer process allows to combine both the codes and the learnt model from the dictionaries of different sizes. Lastly, it enables training of the pooling regions together with the classiﬁer on one dataset, and then re-train the classiﬁer alone on a target dataset. To transfer the pooling regions, we tried logistic regression classiﬁer and linear SVM showing that both classifying procedures can beneﬁt from the learnt pooling regions. However, since we achieve slightly better results for the linear SVM (about 0.5% for bigger dictionaries), only those results are reported. Similarly, we don’t notice signiﬁcant difference in the classiﬁcation accuracy for smaller dictionaries when the pre-pooling is used (with the pre-pooling size S := 3), and therefore all experiments refer only to this case. Finally, we select hyper-parameters of our model based on the 5-fold cross-validation.  3.3 Evaluation of our method on small dictionaries  Figure 1(a) shows the classiﬁcation accuracy of our full method against the baseline [Coates and Ng, 2011]. Since we train the pooling regions without any approximations in this set of experiments the results are limited to dictionary sizes up to 800. Our method outperforms the approach of Coates by 10% for dictionary size 16 (our method achieves the accuracy 57.07%, whereas the baseline only 46.93%). This improvement is consistent up to the bigger dictionaries although the margin is getting  5The reader can ﬁnd details of such approximation in subsection 2.4.  5  (a)  (b)  Figure 1: Figure 1(a) shows accuracy of the classiﬁcation with respect to the number of dictionary elements on smaller dictionaries. Figure 1(b) shows the accuracy of the classiﬁcation for bigger dictionaries when batches, and the redundant batches were used. Experiments are done on CIFAR-10.  smaller. Our method is about 2.5% and 1.88% better than the baseline for 400 and 800 dictionary elements respectively.  3.4 Scaling up to sizable dictionaries  In subsection 2.4 we have discussed the possibility of dividing the codes into low dimensional batches and learning the pooling regions on those. In the following experiments we use batches with 40 coordinates extracted from the original code, as those ﬁt conveniently into the memory of a single, standard machine (about 5 Gbytes for the main data) and can all be trained in parallel. Besides a reduction in the memory requirements, the batches have shown multiple beneﬁts in prac- tice due to smaller number of parameters. We need less computations per iterations as well as observe faster convergence. Figure 1(b) shows the classiﬁcation performance for larger dictionar- ies where we examined the full model [Our], the baseline [Coates], random pooling regions (de- scribed in subsection 3.5), bag of features, and two possible approximation - the batched model [Our (batches)], and the redundantly batched model [Our (redundant batches)]. Our test results are presented in Table 1. When comparing our full model to the approximated versions with batches for dictionaries of size 200, 400 and 800, we observe that there is almost no drop in performance and we even slightly improve for the bigger dictionaries. We attribute this to the better conditioned learning problem of the smaller codes within one batch. With an accuracy for the batched model of 79.6% we outperform the Coates baseline by 1.7%. Interestingly, we gain another small improvement to 80.02% by adding redundant batches which amounts to a total improvement of 2.12% compared to the baseline. Our method performs comparable to the pooling strategy of Jia and Huang [2011] which uses more restrictive assumptions on the pooling regions and employs feature selection algorithm.  Method Jia Coates Our (batches) Our (redundant)  Dict. size  Features  1600 1600 1600 1600  6400 6400 6400 12800  Acc. 80.17% 77.9% 79.6% 80.02%  Table 1: Comparison of our methods against the baseline [Coates and Ng, 2011] and Jia and Huang [2011] with respect to the dictionary size, number of features and the test accuracy on CIFAR-10.  To the best of our knowledge Ciresan et al. [2012] achieves the best results on the CIFAR-10 dataset with an accuracy 88.79% with a method based on a deep architecture – different type of architecture to the one that we investigate in our study. More recently Goodfellow et al. [2013] has achieved accuracy 90.62% with new maxout model that takes an advantage of dropout.  6  05010015020025030035040035404550556065707580Dictionary sizeAccuracy  OurCoatesRandom PoolingBag of Features200400600800100012001400160055606570758085Dictionary sizeAccuracy  Our (redundant batches)Our (batches)OurCoatesRandom PoolingBag of Featuresregularization  Coates (no learn.)  l2  smooth  smooth & l2  smooth & batches  smooth & batches  pooling weights  dataset: CIFAR-10 ; dictionary size: 200  dataset: CIFAR-10 ; dictionary size: 1600  dataset: CIFAR-100 ; dictionary size: 1600  Table 2: Visualization of different pooling strategies obtained for different regularizations, datasets and dic- tionary size. Every column shows the regions from two different coordinates of the codes. First row presents the initial conﬁguration also used in standard hand-crafted pooling methods. Brighter regions denote larger weights.  3.5 Random pooling regions  Our investigation also includes results using random pooling regions where the weights for the parameterized operator (Eq. 2) were sampled from normal distribution with mean 0.5 and standard j ∼ N (0.5, 0.1) for all l. This notion of the random pooling differs from deviation 0.1, that is wl the Jia et al. [2012] where random selection of rectangles is used. The experiments show that the random pooling regions can compete with the standard spatial pooling (Figure 1(a) and 1(b)) on the CIFAR-10 dataset, and suggest that random projection can still preserve some spatial information. This is especially visible in the regime of bigger dictionaries where the difference is only 1.09%. The obtained results indicate that hand-crafted division of the image into subregions is questionable, and call for a learning-based approach.  3.6  Investigation of the regularization terms  and(cid:0)||∇xW||2  Our model (Eq. 5) comes with two regularization terms associated with the pooling weights, each imposing different assumptions on the pooling regions. Hence, it is interesting to investigate their role in the classiﬁcation task by considering all possible subsets of {l2, smooth}, where “l2” and “smooth” refer to ||W||2 Table 3 shows our results on CIFAR-10. We choose a dictionary size of 200 for these experiments, so that we can evaluate different regularization terms without any approximations. We conclude that the spatial smoothness regularization term is crucial to achieve a good predictive performance of our method whereas the l2-norm term can be left out, and thus also reducing the number of hyper- parameters. Based on the cross-validation results (second column of Table 3), we select this setting for further experiments.  (cid:1) respectively.  + ||∇yW||2  l2  l2  l2  Regularization CV Acc. Test Acc. free 69.59% l2 68.39% smooth 73.96% l2 + smooth 70.32%  68.48% 67.86% 73.36% 70.42%  Table 3: We investigate the impact of the regularization terms on the CIFAR-10 dataset with dictionary size equals to 200. Term “free” denotes the objective function without the l2-norm and smoothness regularization terms. The cross-validation accuracy and test accuracy are shown.  7  3.7 Experiments on the CIFAR-100 dataset  Although the main body of work is conducted on the CIFAR-10 dataset, we also investigate how the model performs on the much more demanding CIFAR-100 dataset with 100 classes. Our model with the spatial smoothness regularization term on the 40 dimensional batches achieves 56.29% accuracy. To our best knowledge, this result consitutes the state-of-the-art performance on this dataset, outperforming Jia and Huang [2011] by 1.41%, and the baseline by 4.63%. Using different architecture Goodfellow et al. [2013] has achieved accuracy 61.43%.  Method Jia Coates Our (batches)  Dict. size  Features  1600 1600 1600  6400 6400 6400  Acc. 54.88% 51.66% 56.29%  Table 4: The classiﬁcation accuracy on CIFAR-100, where our method is compared against the Coates and Ng [2011] (we downloaded the framework from https://sites.google.com/site/kmeanslearning, we also use 5- fold cross-validation to choose hyper-parameter C) and Jia and Huang [2011] (here we refer to the NIPS 2011 workshop paper).  3.8 Transfer of the pooling regions between datasets  Beyond the standard classiﬁcation task, we also examine if the learnt pooling regions are trans- ferrable between datasets. In this scenario the pooling regions are ﬁrst trained on the source dataset and then used on the target dataset to train a new classiﬁer. We use dictionary of 1600 with 40- dimensional batches. Our results (Table 5) suggest that the learnt pooling regions are indeed trans- ferable between both datasets. While we observe a decrease in performance when learning the pooling strategy on the less diverse CIFAR-10 dataset, we do see improvements for learning on the richer CIFAR-100 dataset. We arrive at a test accuracy of 80.35% which is an additional improve- ment of 0.75% and 0.18% over our best results (batch-based approximation) and Jia and Huang [2011] respectively.  Source CIFAR-10 CIFAR-100  Target  CIFAR-100 CIFAR-10  Accuracy 52.86% 80.35%  Table 5: We train the pooling regions on the ’Source’ dataset. Next, we use such regions to train the classiﬁer on the ’Target’ dataset where the test accuracy is reported.  3.9 Visualization and analysis of pooling strategies  Table 2 visualizes different pooling strategies investigated in this paper. The ﬁrst row shows the widely used rectangular spatial division of the image. The other visualizations correspond to pooling weights discovered by our model using different regularization terms, datasets and dictionary size. The second row shows the results on CIFAR-10 with the “l2” regularization term. The pooling is most distinct from the other results, as it learns highly localized weights. This pooling strategy has also performed the worst in our investigation (Table 3). The ”smooth” pooling performs the best. Visualization shows that weights are localized but vary smoothly over the image. The weights expose a bias towards initialization shown in the ﬁrst row. All methods with the spatial smoothness regularization tend to focus on similar parts of the image, however “l2 & smooth” is more conservative in spreading out the weights. The last two rows show weights trained using our approximation by batches. From visual inspection, they show a similar level of localization and smoothness to the regions obtained without approxima- tion. This further supports the use of our approximation into independent batches.  8  4 Conclusion  In this paper we propose a ﬂexible parameterization of the pooling operator which can be trained jointly with the classiﬁer. In this manner, we study the effect of different regularizers on the pooling regions as well as the overall system. To be able to train the large set of parameters we propose approximations to our model allowing efﬁcient and parallel training without loss of accuracy. Our experiments show there is a room to improve the classiﬁcation accuracy by advancing the spatial pooling stage. The presented method outperforms a popular hand-crafted pooling based method and previous approaches to learn pooling strategies. While our improvements are consistent over the whole range of dictionary sizes that we have investigated, the margin is most impressive for small codes where we observe improvements up to 10% compared to the baseline of Coates. Finally, our method achieves an accuracy of 56.29% on CIFAR-100, which is to the best of our knowledge the new state-of-the-art on this dataset. As we believe that our method is a good framework for further investigations of different pooling strategies and in order to speed-up progress on the pooling stage we will make our code publicly available at time of publication.  References David H Hubel and Torsten N Wiesel. Receptive ﬁelds, binocular interaction and functional architecture in the  cat’s visual cortex. The Journal of physiology, 160(1):106, 1962.  K. Fukushima and S. Miyake. Neocognitron: A new algorithm for pattern recognition tolerant of deformations  and shifts in position. Pattern recognition, 15(6):455–469, 1982.  Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Handwritten  digit recognition with a back-propagation network. In NIPS, 1990.  M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature Neuroscience,  2009.  J. J. Koenderink and A. J. Van Doorn. The structure of locally orderless images.  Computer Vision, 31(2):159–168, 1999.  International Journal of  D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004. N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing  natural scene categories. In CVPR, 2006.  J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spatial pyramid matching using sparse coding for image  classiﬁcation. In CVPR, 2009.  M. A. Ranzato, F. J. Huang, Y. Boureau, and Y. LeCun. Unsupervised learning of invariant feature hierarchies  with applications to object recognition. In CVPR, 2007.  Q. V. Le, M. A. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Corrado, J. Dean, and A. Y. Ng. Building  high-level features using large scale unsupervised learning. In ICML, 2012a.  Y. Jia and C. Huang. Beyond spatial pyramids: Receptive ﬁeld learning for pooled image features. In NIPS  Workshop on Deep Learning, 2011.  Y. Jia, C. Huang, and T. Darrell. Beyond spatial pyramids: Receptive ﬁeld learning for pooled image features.  In CVPR, 2012.  J. Feng, B. Ni, Q. Tian, and S. Yan. Geometric lp-norm feature pooling for image classiﬁcation. In CVPR,  2011.  M. A. Ranzato and G. E. Hinton. Modeling pixel means and covariances using factorized third-order boltzmann  machines. In CVPR, 2010.  C. M. Bishop. Neural Network for Pattern Recognition. Oxford University Press, 1999. Y. LeCun, L. Bottou, G. Orr, and K. M¨uller. Efﬁcient backprop. Neural networks: Tricks of the trade, pages  546–546, 1998.  R. Collobert and S. Bengio. Links between perceptrons, mlps and svms. In ICML, 2004. A. Coates, H. Lee, and A. Y. Ng. An analysis of single-layer networks in unsupervised feature learning. In  AISTATS, 2011.  Q. V. Le, R. Monga, M. Devin, G. Corrado, K. Chen, M. A. Ranzato, J. Dean, and A. Y. Ng. Building high-level  features using large scale unsupervised learning. 2012b.  9  A. Krizhevsky and G. Hinton. Convolutional deep belief networks on cifar-10. Technical report, 2010. A. Torralba, R. Fergus, and W. T. Freeman. 80 million tiny images: A large data set for nonparametric object  and scene recognition. PAMI, 2008.  A. Coates and A. Y. Ng. The importance of encoding versus training with sparse coding and vector quantization.  In ICML, 2011.  D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation. In  CVPR, 2012.  I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. In ICML, 2013.  10  ","Biologically inspired, from the early HMAX model to Spatial Pyramid Matching,pooling has played an important role in visual recognition pipelines. Spatialpooling, by grouping of local codes, equips these methods with a certain degreeof robustness to translation and deformation yet preserving important spatialinformation. Despite the predominance of this approach in current recognitionsystems, we have seen little progress to fully adapt the pooling strategy tothe task at hand. This paper proposes a model for learning task dependentpooling scheme -- including previously proposed hand-crafted pooling schemes asa particular instantiation. In our work, we investigate the role of differentregularization terms showing that the smooth regularization term is crucial toachieve strong performance using the presented architecture. Finally, wepropose an efficient and parallel method to train the model. Our experimentsshow improved performance over hand-crafted pooling schemes on the CIFAR-10 andCIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% onthe latter."
1301.3618,2013,Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors  ,"['Danqi Chen', 'Richard Socher', 'Christopher Manning', 'Andrew Y. Ng']",https://arxiv.org/pdf/1301.3618.pdf,"3 1 0 2    r a     M 6 1      ] L C . s c [      2 v 8 1 6 3  .  1 0 3 1 : v i X r a  Learning New Facts From Knowledge Bases With  Neural Tensor Networks and Semantic Word Vectors  Danqi Chen, Richard Socher, Christopher D. Manning, Andrew Y. Ng Computer Science Department, Stanford University, Stanford, CA 94305, USA  {danqi,manning,ang}@stanford.edu, richard@socher.org  Abstract  Knowledge bases provide applications with the beneﬁt of easily accessible, sys- tematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by ﬁnding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting addi- tional true relationships between entities, based on generalizations that can be dis- cerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vec- tors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 75.8%.  1 Introduction  Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval (Google Knowledge Graph), or generally providing inference over structured knowledge to users. Much work has focused on extending existing knowledge bases [5, 6, 2] using patterns or classiﬁers applied to large corpora.  We introduce a model that can accurately learn to add additional facts to a database using only that database. This is achieved by representing each entity (i.e., each object or individual) in the database by a vector that can capture facts and their certainty about that entity. Each relation is deﬁned by the parameters of a novel neural tensor network which can explicitly relate two entity vectors and is more powerful than a standard neural network layer.  Furthermore, our model allows us to ask whether even entities that were not in the database are in certain relationships by simply using distributional word vectors. These vectors are learned by a neural network model [7] using unsupervised text corpora such as Wikipedia. They capture syntactic and semantic information and allow us to extend the database without any manually designed rules or additional parsing of other textual resources.  The model outperforms previously introduced related models such as that of Bordes et al. [8]. We evaluate on a heldout set of relationships in WordNet. The accuracy for predicting unseen relations is 75.8%. We also evaluate in terms of ranking. For WordNet, there are 38,696 different entities and we use 11 relationship types. On average for each left entity there are 100 correct entities in a speciﬁc relationship. For instance, dog has many hundreds of hyponyms such as puppy, barker or dachshund. In 20.9% of the relationship triplets, the model ranks the correct test entity in the top 100 out of 38,696 possible entities.  1  2 Related Work  There is a vast amount of work extending knowledge bases using external corpora [5, 6, 2], among many others. In contrast, little work has been done in extensions based purely on the knowledge base itself. The work closest to ours is that by Bordes et al. [9]. We implement their approach and compare to it directly. Our model outperforms it by a signiﬁcant margin in terms of both accuracy and ranking. Both models can beneﬁt from initialization with unsupervised word vectors.  Another related approach is that by Sutskever et al. [10] who use tensor factorization and Bayesian clustering for learning relational structures. Instead of clustering the entities in a nonparametric Bayesian framework we rely purely on learned entity vectors. Their computation of the truth of a relation can be seen as a special case of our proposed model. Instead of using MCMC for inference, we use standard backpropagation which is modiﬁed for the Neural Tensor Network. Lastly, we do not require multiple embeddings for each entity. Instead, we consider the subunits (space separated words) of entity names. This allows more statistical strength to be shared among entities.  Many methods that use knowledge bases as features such as [3, 4] could beneﬁt from a method that maps the provided information into vector representations. We learn to modify unsupervised word representations via grounding in world knowledge. This essentially allows us to analyze word embeddings and query them for speciﬁc relations. Furthermore, the resulting vectors could be used in other tasks such as NER [7] or relation classiﬁcation in natural language [11].  Lastly, Ranzato et al. [12] introduced a factored 3-way Restricted Boltzmann Machine which is also parameterized by a tensor.  3 Neural Tensor Networks  In this section we describe the full neural tensor network. We begin by describing the representation of entities and continue with the model that learns entity relationships.  We compare using both randomly initialized word vectors and pre-trained 100-dimensional word vectors from the unsupervised model of Collobert and Weston [13, 7]. Using free Wikipedia text, this model learns word vectors by predicting how likely it is for each word to occur in its context. The model uses both local context in the window around each word and global document context. Similar to other local co-occurrence based vector space models, the resulting word vectors cap- ture distributional syntactic and semantic information. For further details and evaluations of these embeddings, see [14, 13, 15].  For cases where the entity name has multiple words, we simply average the word vectors.  The Neural Tensor Network (NTN) replaces the standard linear layer with a bilinear layer that di- rectly relates the two entity vectors. Let e1, e2 ∈ Rd be the vector representations of the two entities. We can compute a score of how plausible they are in a certain relationship R by the following NTN- based function:  g(e1, R, e2) = U T f (cid:18)eT  1 W [1:k]  R e2 + VR(cid:20)e1  e2(cid:21) + bR(cid:19) ,  (1)  hi = eT  where f = tanh is a standard nonlinearity. We deﬁne W [1:k] ∈ Rd×d×k as a tensor and the bilinear tensor product results in a vector h ∈ Rk, where each entry is computed by one slice of the tensor: (2) The remaining parameters for relation R are the standard form of a neural network: VR ∈ Rk×2d and U ∈ Rk, bR ∈ Rk. The main advantage of this model is that it can directly relate the two inputs instead of only implicitly through the nonlinearity. The bilinear model for truth values in [10] becomes a special case of this model with VR = 0, bR = 0, k = 1, f = identity. In order to train the parameters W, U, V, E, b, we minimize the following contrastive max-margin objective:  1 W [i]e2.  J(W, U, V, E, b) =  N  C  Xi=1  Xc=1  max(0, 1 − g(e(i)  1 , R(i), e(i)  2 ) + g(e(i)  1 , R(i), ec)),  (3)  2  where N is the number of training triplets and we score the correct relation triplets higher than a corrupted one in which one of the entities was replaced with a random entity. For each correct triplet we sample C random corrupted entities.  The model is trained by taking gradients with respect to the ﬁve sets of parameters and using mini- batched L-BFGS.  4 Experiments  In our experiments, we follow the data settings of WordNet in [9]. There are a total of 38,696 different entities and 11 relations. We use 112,581 triplets for training, 2,609 for the development set and 10,544 for ﬁnal testing.  The WordNet relationships we consider are has instance, type of, member meronym, member holonym, part of, has part, subordinate instance of, domain region, synset domain region, similar to, domain topic.  We compare our model with two models in Bordes et al. [9, 8], which have the same goal as ours. The model of [9] has the following scoring function:  g(e1, R, e2) = kWR,lef te1 − WR,righte2k1,  (4)  where WR,lef t, WR,right ∈ Rd×d. The model of [8] also maps each relation type to an embedding eR ∈ Rd and scores the relationships by:  g(e1, R, e2) = −(W1e1 ⊗ Wrel,1eR + b1) · (W2e2 ⊗ Wrel,2eR + b2),  (5)  where W1, Wrel,1, W2, Wrel,2 ∈ Rd×d, b1, b2 ∈ Rd×1. In the comparisons below, we call these two models the similarity model and the Hadamard model respectively. While our function scores correct triplets highly, these two models score correct triplets lower. All models are trained in a contrastive max-margin objective functions. Our goal is to predict “correct” relations (e1, R, e2) in the testing data. We can compute a score for each triplet (e1, R, e2). We can consider either just a classiﬁcation accuracy result as to whether the relation holds, or look at a ranking of e2, for considering relative conﬁdence in particular relations holding. We use a different evaluation set from Bordes et al. [9] because it has became apparent to us and them that there were issues of overlap between their training and testing sets which impacted the quality and interpretability of their evaluation.  Ranking  For each triplet (e1, R, e2), we compute the score g(e1, R, e) for all other entities in the knowledge base e ∈ E. We then sort values by decreasing order and report the rank of the correct entity e2. For WordNet the total number of entities is |E| = 38, 696. Some of the questions relating to triplets are of the form “A is a type of ?” or “A has instance ?” Since these have multiple correct answers, we report the percentage of times that e2 is ranked in the top 100 of the list (recall @ 100). The higher this number, the more often the speciﬁc correct test entity has likely been correctly estimated.  After cross-validation of the hyperparameters of both models on the development fold, our neural tensor net obtains a ranking recall score of 20.9% while the similarity model achieves 10.6%, and the Hadamard model achieves only 7.4%. The best performance of the NTN with random initialization instead of the semantic vectors drops to 16.9% and the similarity model and the Hadamard model only achieve 5.7% and 7.1%.  Classiﬁcation  In this experiment, we ask the model whether any arbitrary triplet of entities and relations is true or not. With the help of the large vocabulary of semantic word vectors, we can query whether certain WordNet relationships hold or not even for entities that were not originally in WordNet. We use the development fold to ﬁnd a threshold TR for each relation such that if f (e1, R, e2) ≥ TR, the relation (e1, R, e2) holds, otherwise it is considered false. In order to create negative examples,  3  we randomly switch entities and relations from correct testing triplets, resulting in a total of 2 × 10, 544 triplets. The ﬁnal accuracy is based on how many of of triplets are classiﬁed correctly. The Neural Tensor Network achieves an accuracy of 75.8% with semantically initialized entity vec- tors and 70.0% with randomly initialized ones. In comparison, the similarity based model only achieve 66.7% and 51.6%, the Hadamard model achieve 71.9% and 68.2% with the same setup. All models improve in performance if entities are represented as an average of their word vectors but we will leave experimentation with this setup to future work.  5 Conclusion  We introduced a new model based on Neural Tensor Networks. Unlike previous models for predict- ing relationships purely using entity representations in knowledge bases, our model allows direct interaction of entity vectors via a tensor. This architecture allows for much better performance in terms of both ranking correct answers out of tens of thousands of possible ones and predicting unseen relationships between entities. It enables the extension of databases even without external textual resources but can also beneﬁt from unsupervised large corpora even without manually designed extraction rules.  References  [1] G.A. Miller. WordNet: A Lexical Database for English. Communications of the ACM, 1995. [2] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of semantic knowledge. In Proceedings of the  16th international conference on World Wide Web, 2007.  [3] J. Graupmann, R. Schenkel, and G. Weikum. The SphereSearch engine for uniﬁed ranked retrieval of heterogeneous XML and web documents. In Proceedings of the 31st international conference on Very large data bases, VLDB, 2005.  [4] V. Ng and C. Cardie. Improving machine learning approaches to coreference resolution. In ACL, 2002. [5] R. Snow, D. Jurafsky, and A. Y. Ng. Learning syntactic patterns for automatic hypernym discovery. In  NIPS, 2005.  [6] A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information extraction. In EMNLP,  2011.  [7] J. Turian, L. Ratinov, and Y. Bengio. Word representations: a simple and general method for semi-  supervised learning. In Proceedings of ACL, pages 384–394, 2010.  [8] A. Bordes, X. Glorot, J. Weston, and Y. Bengio. Joint Learning of Words and Meaning Representations  for Open-Text Semantic Parsing. AISTATS, 2012.  [9] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured embeddings of knowledge bases.  In AAAI, 2011.  [10] I. Sutskever, R. Salakhutdinov, and J. B. Tenenbaum. Modelling relational data using Bayesian clustered  tensor factorization. In NIPS, 2009.  [11] R. Socher, B. Huval, C. D. Manning, and A. Y. Ng. Semantic Compositionality Through Recursive  Matrix-Vector Spaces. In EMNLP, 2012.  [12] M. Ranzato and A. Krizhevsky G. E. Hinton. Factored 3-Way Restricted Boltzmann Machines For Mod-  eling Natural Images. AISTATS, 2010.  [13] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: deep neural networks  with multitask learning. In ICML, 2008.  [14] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. J. Mach.  Learn. Res., 3, March 2003.  [15] E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.  Context and Multiple Word Prototypes. In ACL, 2012.  Improving Word Representations via Global  4  ","Knowledge bases provide applications with the benefit of easily accessible,systematic relational knowledge but often suffer in practice from theirincompleteness and lack of knowledge of new entities and relations. Much workhas focused on building or extending them by finding patterns in largeunannotated text corpora. In contrast, here we mainly aim to complete aknowledge base by predicting additional true relationships between entities,based on generalizations that can be discerned in the given knowledgebase. Weintroduce a neural tensor network (NTN) model which predicts new relationshipentries that can be added to the database. This model can be improved byinitializing entity representations with word vectors learned in anunsupervised fashion from text, and when doing this, existing relations caneven be queried for entities that were not present in the database. Our modelgeneralizes and outperforms existing models for this problem, and can classifyunseen relationships in WordNet with an accuracy of 75.8%."
1301.3755,2013,Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models  ,"['Derek Rose', 'Itamar Arel']",https://arxiv.org/pdf/1301.3755.pdf,"3 1 0 2     n a J    6 1      ]  V C . s c [      1 v 5 5 7 3  .  1 0 3 1 : v i X r a  Gradient Driven Learning for Pooling in Visual  Pipeline Feature Extraction Models  Derek Rose and Itamar Arel  Deptartment of Electrical Engineering and Computer Science  University of Tennessee  derek@utk.edu, itamar@ieee.org  Abstract  Hyper-parameter selection remains a daunting task when building a pattern recog- nition architecture which performs well, particularly in recently constructed visual pipeline models for feature extraction. We re-formulate pooling in an existing pipeline as a function of adjustable pooling map weight parameters and propose the use of supervised error signals from gradient descent to tune the established maps within the model. This technique allows us to learn what would otherwise be a design choice within the model and specialize the maps to aggregate areas of invariance for the task presented. Preliminary results show moderate poten- tial gains in classiﬁcation accuracy and highlight areas of importance within the intermediate feature representation space.  1  Introduction  Multi-stage visual pipelines for learning feature representations of images have recently proven valuable for classifying small objects from a variety of lighting conditions, scales, and poses. An effective variant of these was explored in experiments by [1] that compared features learned and encoded by stages that originate in encoder-decoder networks, deep learning [2], and bags of fea- tures models [3]. This architecture partitions images into patches to perform local learning of an over-complete codebook and uses this codebook to form global representations of the images for classiﬁcation. Intermediate or mid-level representations are of high-dimensionality and retain the spatial structure of the image. Pooling is an integral late stage that performs the same role as the sub-sampling layer of a convolutional neural network (CNN): reduction in the ﬁnal number of fea- tures (passed to the classiﬁer or next layer) by an aggregation operation meant to improve invariance to small changes [4]. Unfortunately, each additional stage of the architecture often adds hyper-parameters for model selec- tion that must be explored. For the pooling layer the number of pools, their structure or spatial lay- out, weights within this region, and operator (often max, average, or p-norm) are hyper-parameters that are frequently chosen by rules of thumb. Recently, [5] explored pool selection by optimization over a full training set with an over-complete number of pools and achieved excellent improvements over standard pyramid models, although their method uses a feature selection stage with retraining to tractably create a classiﬁer. In this paper we present a method for learning pooling maps with weight parameters that may optimize or tune the feature space representation for better discrimina- tion through stochastic gradient descent. This converts two of the model choices above to parameters which may be learned from a limited stream of labeled training data. Back-propogation through the architecture in [1] is used to obtain the appropriate weight updates. This technique stems back at least to the inception of convolutional neural networks and graph transformer networks [4], where each module or layer of the network may be utilized in a forward pass output calculation and backward pass parameter update.  1  2 Architecture Description and Design  The multi-stage image recognition architecture considered from [1] consists of patch extraction, nor- malization and whitening, codebook learning, feature encoding, spatial pooling, and classiﬁcation. As guided by [1] we use hyper-parameters such as the number of codewords, k = 400, patch size w = 6 pixels, stride= 1 for dense extraction, triangle encoding, and use of patch whitening and pre-processing with a focus on the CIFAR-10 dataset. Quadrant based average pooling was used to sub-sample the intermediate feature space representation down to 4k. This choice stemmed from spatial pyramids and was not a heavy focus for [1], although motivating work in [5] has shown there are both a large number of options for pooling regions and much performance to be gained. Although building pools for invariance can be partially based on intuition when patch extraction is performed spatially relevant to the original image (as a single layer in the architecture considered), pooling region choice for layers of features that build upon those generated in the lowest layer is not as straightforward (as noted by [6]). Special care must be taken to reorder features and assemble pools that maintain the structure inherent in the image. It would be beneﬁcial if the region could change relative to problem demands as well as layer context, and we propose the use of pooling weight maps that may be adjusted as learning proceeds with gradient descent.  3 Stochastic Gradient Descent Based Method for Weighted Pooling  To obtain continuous updates as well as a gradient signal we replace the support vector machine originally used for classiﬁcation in [1] with a feed-forward neural network with single hidden layer and mean-square error cost function. The outputs are one-hot binary vectors for the t = 10 target classes and the inputs are pooled features h. If we have a n × n pixel square image and let P = (n(w − 1) + 1), with dense patch extraction we obtain a P × P × k mid-level representation (this may be modiﬁed for non-square images). Pooling reduces this representation to p × k that can be ﬂattened to form h. We then extend the network back to include an additional input layer that holds our p weight maps with size P × P that we denote as Wi. The inputs to this new layer are encoded features g computed from the P × P × k patches. This method shares many similarities with the sub-sampling layer in a CNN. In the sub-sampling layer each neuron receives the average of the features from the prior layer in its receptive ﬁeld or pool (receptive ﬁelds of units do not overlap). Each unit has a coefﬁcient and bias that are trained with gradient information and feed into a sigmoid activation function that controls the response of the sub-sampling unit [4]. Aside from its application in an alternate context, our approach differs in that the features which are averaged are no longer restrained to being equally weighted and multiple pools may utilize the same encoded patches. Learned weights combine encoded features g from the mid-level representation and pass these through a linear activation with unity weight to a neural network classiﬁer, i.e. pooled inputs h over encoded patches g in pool i are computed1 by  hi = fpool (g) =  Wi  m,n ∗ gm,n.  (1)  m=1  n=1  We currently have not explored using non-linear activation units with adjustable biases and coef- ﬁcients and although we have no restriction that pools must be non-overlapping as in CNNs, we are currently examining modiﬁcation to the loss function to maximize diversity among the pools. The weight sharing scheme employed by [4] for learning feature maps is similarly used here to reduce the parameter search space by sharing weights in codeword dimension k of the mid-level representation. Figure 1(a) illustrates the positioning of the learned parameters within the architec- ture. To replicate the quadrant based pooling we employ four maps that connect to every patch and are initially set to Wm,n = 1/(P/2)2 inside the hot zone of the corresponding quadrant map and zero elsewhere. After learning the codebook we train the original network with 128 hidden neurons and stochastic average mini-batch gradient descent (random batches of 10 images) while the pool weights are ﬁxed. Network inputs are normalized to ¯h post pooling with a mean and variance that is not modiﬁed. After a learning period we ﬁx the classiﬁer weights and learn the pooling weights. We have found this alternate training to perform better than simultaneous learning where the neural network can compensate for pooling weight changes.  1Codeword index has been omitted for clarity. h and g have a k-size dimension which W is shared over.  2  P(cid:88)  P(cid:88)  Figure 1: (a) Pool weight map connections and feed-forward net extension, (b) Learned weight maps  (a)  (b)  3.1 Weight Update Rule and Preliminary Results  ∂¯hi ∂W i  To update each pool i’s weights with loss function J note we want ∆W i −η ∂J ∂¯hi δ0 = −∂J/∂ ¯h = v1T network layer. Noting ¯hi = (hi−µi)/σi and ∂¯hi/∂W i  m,n = . From the original backpropagation pass we may create a new set of sensitivities δ1 (v representing the original input to hidden weights) for the prepended  m,n = gm,n/σi we obtain the update rule  m,n  m,n = −η∂J/∂W i  ∆W i  m,n = ηδ0 i  .  (2)  gm,n σi  The pool weight gradient updates for the images in each mini-batch are averaged; in each, every codeword dimension k contributes to the update as a sum. In preliminary tests we freeze network learning after presenting 250k examples from 80% of the CIFAR-10 training set. The last validation accuracy before pool learning on the remaining 20% averaged over ﬁve trials was 67.56%. After training with an additional 15k training images and checking the validation set every 500 images, the average best post pool learning accuracy was 68.03% for an improvement of 0.57%. Figure 1(b) contains an example of the weights learned that highlights the inﬁnite number of possible pools gradient descent searches over. This method may also be used to tune the choice of p within the weighted p-norm. Open issues remain, particularly in the learning rate choice (for which we have traded the pooling structure for hyper-parameter η = 5e−5) and the number of maps needed to cover separate areas of invariance. It would be preferred to select more than enough maps than necessary and later trim down redundant features, although we need to be careful to avoid overﬁtting the training set here (and in general for this approach). Unfortunately, each additional map adds k inputs to the classiﬁer. This problem relates closely to feature map count or hidden neuron count hyper-parameters in CNNs.  References [1] A. Coates, H. Lee, and A. Y. Ng, “An analysis of single-layer networks in unsupervised feature learning,”  in AISTATS 14, 2011.  [2] Y. Bengio, “Learning deep architectures for AI,” Foundations and Trends in Machine Learning, vol. 2,  no. 1, pp. 1–127, 2009.  [3] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial pyramid matching for recogniz- ing natural scene categories,” in Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, vol. 2, 2006, pp. 2169–2178.  [4] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recogni-  tion,” Proceedings of the IEEE, vol. 86, no. 11, p. 22782324, 1998.  [5] Y. Jia, H. Chang, and T. Darrel, “Beyond spatial pyramids: Receptive ﬁeld learning for pooled image  features,” in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, 2012.  [6] A. Coates and A. Y. Ng, “Learning feature representations with k-means,” in Neural Networks: Tricks of  the Trade, Reloaded. Springer LNCS, 2012.  3  P=(n-w)/stride + 1 (fully connected) 1 2 k … * starting maps * FFNN h1 h2 h3 h4 target (one hot) g11 g12 g21 gPP … … … (k-dim) Mid-Level Representation W1 (shared) 1234","Hyper-parameter selection remains a daunting task when building a patternrecognition architecture which performs well, particularly in recentlyconstructed visual pipeline models for feature extraction. We re-formulatepooling in an existing pipeline as a function of adjustable pooling map weightparameters and propose the use of supervised error signals from gradientdescent to tune the established maps within the model. This technique allows usto learn what would otherwise be a design choice within the model andspecialize the maps to aggregate areas of invariance for the task presented.Preliminary results show moderate potential gains in classification accuracyand highlight areas of importance within the intermediate featurerepresentation space."
1301.3541,2013,Deep Predictive Coding Networks  ,"['Rakesh Chalasani', 'Jose C. Principe']",https://arxiv.org/pdf/1301.3541.pdf,"3 1 0 2    r a     M 5 1      ]  G L . s c [      3 v 1 4 5 3  .  1 0 3 1 : v i X r a  Deep Predictive Coding Networks  Rakesh Chalasani  Jose C. Principe  Department of Electrical and Computer Engineering  University of Florida, Gainesville, FL 32611  rakeshch@ufl.edu, principe@cnel.ufl.edu  Abstract  The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used ﬁxed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context- sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic network which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top- down connections by showing the robustness of the proposed model to structured noise.  1 Introduction  The performance of machine learning algorithms is dependent on how the data is represented. In most methods, the quality of a data representation is itself dependent on prior knowledge imposed on the representation. Such prior knowledge can be imposed using domain speciﬁc information, as in SIFT [1], HOG [2], etc., or in learning representations using ﬁxed priors like sparsity [3], temporal coherence [4], etc. The use of ﬁxed priors became particularly popular while training deep networks [5–8]. In spite of the success of these general purpose priors, they are not capable of adjusting to the context in the data. On the other hand, there are several advantages to having a model that can “actively” adapt to the context in the data. One way of achieving this is to empirically alter the priors in a dynamic and context-sensitive manner. This will be the main focus of this work, with emphasis on visual perception.  Here we propose a predictive coding framework, where a deep locally-connected generative model uses “top-down” information to empirically alter the priors used in the lower layers to perform “bottom-up” inference. The centerpiece of the proposed model is extracting sparse features from time-varying observations using a linear dynamical model. To this end, we propose a novel proce- dure to infer sparse states (or features) of a dynamical system. We then extend this feature extraction block to introduce a pooling strategy to learn invariant feature representations from the data. In line with other “deep learning” methods, we use these basic building blocks to construct a hierarchical model using greedy layer-wise unsupervised learning. The hierarchical model is built such that the output from one layer acts as an input to the layer above. In other words, the layers are arranged in a Markov chain such that the states at any layer are only dependent on the representations in the layer below and above, and are independent of the rest of the model. The overall goal of the dynamical system at any layer is to make the best prediction of the representation in the layer below using the top-down information from the layers above and the temporal information from the previous states. Hence, the name deep predictive coding networks (DPCN).  1  1.1 Related Work  The DPCN proposed here is closely related to models proposed in [9, 10], where predictive cod- ing is used as a statistical model to explain cortical functions in the mammalian brain. Similar to the proposed model, they construct hierarchical generative models that seek to infer the underlying causes of the sensory inputs. While Rao and Ballard [9] use an update rule similar to Kalman ﬁlter for inference, Friston [10] proposed a general framework considering all the higher-order moments in a continuous time dynamic model. However, neither of the models is capable of extracting dis- criminative information, namely a sparse and invariant representation, from an image sequence that is helpful for high-level tasks like object recognition. Unlike these models, here we propose an efﬁcient inference procedure to extract locally invariant representation from image sequences and progressively extract more abstract information at higher levels in the model.  Other methods used for building deep models, like restricted Boltzmann machine (RBM) [11], auto- encoders [8, 12] and predictive sparse decomposition [13], are also related to the model proposed here. All these models are constructed on similar underlying principles: (1) like ours, they also use greedy layer-wise unsupervised learning to construct a hierarchical model and (2) each layer consists of an encoder and a decoder. The key to these models is to learn both encoding and decoding concurrently (with some regularization like sparsity [13], denoising [8] or weight sharing [11]), while building the deep network as a feed forward model using only the encoder. The idea is to approximate the latent representation using only the feed-forward encoder, while avoiding the decoder which typically requires a more expensive inference procedure. However in DPCN there is no encoder. Instead, DPCN relies on an efﬁcient inference procedure to get a more accurate latent representation. As we will show below, the use of reciprocal top-down and bottom-up connections make the proposed model more robust to structured noise during recognition and also allows it to perform low-level tasks like image denoising.  To scale to large images, several convolutional models are also proposed in a similar deep learning paradigm [5–7]. Inference in these models is applied over an entire image, rather than small parts of the input. DPCN can also be extended to form a convolutional network, but this will not be discussed here.  2 Model  In this section, we begin with a brief description of the general predictive coding framework and proceed to discuss the details of the architecture used in this work. The basic block of the proposed model that is pervasive across all layers is a generalized state-space model of the form:  ˜yt = F (xt) + nt xt = G(xt−1, ut) + vt  (1) where ˜yt is the data and F and G are some functions that can be parameterized, say by θ. The terms ut are called the unknown causes. Since we are usually interested in obtaining abstract information from the observations, the causes are encouraged to have a non-linear relationship with the obser- vations. The hidden states, xt, then “mediate the inﬂuence of the cause on the output and endow the system with memory” [10]. The terms vt and nt are stochastic and model uncertainty. Several such state-space models can now be stacked, with the output from one acting as an input to the layer above, to form a hierarchy. Such an L-layered hierarchical model at any time ’t’ can be described as1:  t  u(l−1 ) x(l) t  = F (x(l) = G(x(l)  t ) + n(l) t−1, u(l)  t  t ) + v(l)  t  ∀l ∈ {1, 2, ..., L}  (2)  and n(l) t  The terms v(l) form stochastic ﬂuctuations at the higher layers and enter each layer in- t dependently. In other words, this model forms a Markov chain across the layers, simplifying the inference procedure. Notice how the causes at the lower layer form the “observations” to the layer above — the causes form the link between the layers, and the states link the dynamics over time. The important point in this design is that the higher-level predictions inﬂuence the lower levels’  1When l = 1, i.e., at the bottom layer, u(i−1)  t  = yt, where yt the input data.  2  Causes (ut)  States (xt)  Observations (yt)  - States (xt)  - Causes (ut) (Invariant Units)  { {  Layer 1  Layer 2  (a) Shows a single layered dynamic network depicting a basic computational block.  (b) Shows the distributive hierarchical model formed by stacking several basic blocks.  Figure 1: (a) Shows a single layered network on a group of small overlapping patches of the input video. The green bubbles indicate a group of inputs (y(n) ,∀n), red bubbles indicate their corre- sponding states (x(n) ) and the blue bubbles indicate the causes (ut) that pool all the states within the group. (b) Shows a two-layered hierarchical model constructed by stacking several such basic blocks. For visualization no overlapping is shown between the image patches here, but overlapping patches are considered during actual implementation.  t  t  inference. The predictions from a higher layer non-linearly enter into the state space model by em- pirically altering the prior on the causes. In summary, the top-down connections and the temporal dependencies in the state space inﬂuence the latent representation at any layer.  In the following sections, we will ﬁrst describe a basic computational network, as in (1) with a particular form of the functions F and G. Speciﬁcally, we will consider a linear dynamical model with sparse states for encoding the inputs and the state transitions, followed by the non-linear pooling function to infer the causes. Next, we will discuss how to stack and learn a hierarchical model using several of these basic networks. Also, we will discuss how to incorporate the top-down information during inference in the hierarchical model.  2.1 Dynamic network  To begin with, we consider a dynamic network to extract features from a small part of a video sequence. Let {y1, y2, ..., yt, ...} ∈ RP be a P -dimensional sequence of a patch extracted from the same location across all the frames in a video2 . To process this, our network consists of two distinctive parts (see Figure.1a): feature extraction (inferring states) and pooling (inferring causes). For the ﬁrst part, sparse coding is used in conjunction with a linear state space model to map the inputs yt at time t onto an over-complete dictionary of K-ﬁlters, C ∈ RP ×K(K > P ), to get sparse states xt ∈ RK. To keep track of the dynamics in the latent states we use a linear function with state-transition matrix A ∈ RK×K. More formally, inference of the features xt is performed by ﬁnding a representation that minimizes the energy function:  E1(xt, yt, C, A) = kyt − Cxtk2  2 + λkxt − Axt−1k1 + γkxtk1  (3)  Notice that the second term involving the state-transition is also constrained to be sparse to make the state-space representation consistent.  Now, to take advantage of the spatial relationships in a local neighborhood, a small group of states x(n) , where n ∈ {1, 2, ...N} represents a set of contiguous patches w.r.t. the position in the image t space, are added (or sum pooled) together. Such pooling of the states may be lead to local translation invariance. On top this, a D-dimensional causes ut ∈ RD are inferred from the pooled states to obtain representation that is invariant to more complex local transformations like rotation, spatial frequency, etc. In line with [14], this invariant function is learned such that it can capture the dependencies between the components in the pooled states. Speciﬁcally, the causes ut are inferred  2Here yt is a vectorized form of √P × √P square patch extracted from a frame at time t.  3  by minimizing the energy function:  E2(ut, xt, B) =  N  K  Xk=1  Xn=1(cid:16)  |γk · x(n) γk = γ0h 1 + exp(−[But]k)  2  i  t,k |(cid:17) + βkutk1  (4)  where γ0 > 0 is some constant. Notice that here ut multiplicatively interacts with the accumulated states through B, modeling the shape of the sparse prior on the states. Essentially, the invariant matrix B is adapted such that each component ut connects to a group of components in the ac- cumulated states that co-occur frequently. In other words, whenever a component in ut is active it lowers the coefﬁcient of a set of components in x(n) ,∀n, making them more likely to be active. Since co-occurring components typically share some common statistical regularity, such activity of ut typically leads to locally invariant representation [14]. Though the two cost functions are presented separately above, we can combine both to devise a uniﬁed energy function of the form:  t  E(xt, ut, θ) =  N  Xn=1(cid:16) 1  2ky(n)  t − Cx(n) t k2  2 + λkx(n)  t − Ax(n)  t−1k1 +  K  Xk=1  |γt,k · x(n)  t,k |(cid:17) + βkutk1  (5)  γt,k =γ0h 1 + exp(−[But]k)  2  i  where θ = {A, B, C}. As we will discuss next, both xt and ut can be inferred concurrently from (5) by alternatively updating one while keeping the other ﬁxed using an efﬁcient proximal gradient method.  2.2 Learning  To learn the parameters in (5), we alternatively minimize E(xt, ut, θ) using a procedure similar to block co-ordinate descent. We ﬁrst infer the latent variables (xt, ut) while keeping the parameters ﬁxed and then update the parameters θ while keeping the variables ﬁxed. This is done until the parameters converge. We now discuss separately the inference procedure and how we update the parameters using a gradient descent method with the ﬁxed variables.  2.2.1 Inference  We jointly infer both xt and ut from (5) using proximal gradient methods, taking alternative gradient descent steps to update one while holding the other ﬁxed. In other words, we alternate between updating xt and ut using a single update step to minimize E1 and E2, respectively. However, updating xt is relatively more involved. So, keeping aside the causes, we ﬁrst focus on inferring sparse states alone from E1, and then go back to discuss the joint inference of both the states and the causes. Inferring States: Inferring sparse states, given the parameters, from a linear dynamical system forms the crux of our model. This is performed by ﬁnding the solution that minimizes the energy function E1 in (3) with respect to the states xt (while keeping the sparsity parameter γ ﬁxed). Here there are two priors of the states: the temporal dependence and the sparsity term. Although this energy function E1 is convex in xt, the presence of two non-smooth terms makes it hard to use standard optimization techniques used for sparse coding alone. A similar problem is solved using dynamic programming [15], homotopy [16] and Bayesian sparse coding [17]; however, the optimization used in these models is computationally expensive for use in large scale problems like object recognition.  To overcome this, inspired by the method proposed in [18] for structured sparsity, we propose an approximate solution that is consistent and able to use efﬁcient solvers like fast iterative shrinkage thresholding alogorithm (FISTA) [19]. The key to our approach is to ﬁrst use Nestrov’s smoothness method [18, 20] to approximate the non-smooth state transition term. The resulting energy function  4  is a convex and continuously differentiable function in xt with a sparsity constraint, and hence, can be efﬁciently solved using proximal methods like FISTA. To begin, let Ω(xt) = ketk1 where et = (xt − Axt−1). The idea is to ﬁnd a smooth approximation to this function Ω(xt) in et. Notice that, since et is a linear function on xt, the approximation will also be smooth w.r.t. xt. Now, we can re-write Ω(xt) using the dual norm of ℓ1 as  Ω(xt) = arg max kαk∞≤1  αT et  where α ∈ Rk. Using the smoothing approximation from Nesterov [20] on Ω(xt):  Ω(xt) ≈ fµ(et) = arg max  kαk∞≤1  [αT et − µd(α)]  (6)  where d(·) = 1 2 is a smoothing function and µ is a smoothness parameter. From Nestrov’s theorem [20], it can be shown that fµ(et) is convex and continuously differentiable in et and the gradient of fµ(et) with respect to et takes the form  2kαk2  where α∗ is the optimal solution to fµ(et) = arg max kαk∞≤1  (7) [αT et − µd(α)] 3. This implies, by using the chain rule, that fµ(et) is also convex and continuously differentiable in xt and with the same gradient.  ∇et fµ(et) = α∗  With this smoothing approximation, the overall cost function from (3) can now be re-written as  xt = arg min  xt  1 2kyt − Cxtk2  2 + λfµ(et) + γkxtk1  (8)  with the smooth part h(xt) = 1 by  2kyt − Cxtk2  2 + λfµ(et) whose gradient with respect to xt is given  ∇xt h(xt) = CT (yt − Cxt) + λα∗  (9)  Using the gradient information in (9), we solve for xt from (8) using FISTA [19]. Inferring Causes: Given a group of state vectors, ut can be inferred by minimizing E2, where we  deﬁne a generative model that modulates the sparsity of the pooled state vector,Pn |x(n)|. Here we  observe that FISTA can be readily applied to infer ut, as the smooth part of the function E2:  h(ut) =  K  Xk=1(cid:16)γ0h 1 + exp(−[But]k)  2  N  Xn=1  i ·  t,k |(cid:17) |x(n)  (10)  is convex, continuously differentiable and Lipschitz in ut [21] 4. Following [19], it is easy to obtain a bound on the convergence rate of the solution. Joint Inference: We showed thus far that both xt and ut can be inferred from their respective energy functions using a ﬁrst-order proximal method called FISTA. However, for joint inference we have to minimize the combined energy function in (5) over both xt and ut. We do this by alternately updating xt and ut while holding the other ﬁxed and using a single FISTA update step at each iteration. It is important to point out that the internal FISTA step size parameters are maintained between iterations. This procedure is equivalent to alternating minimization using gradient descent. Although this procedure no longer guarantees convergence of both xt and ut to the optimal solution, in all of our simulations it lead to a reasonably good solution. Please refer to Algorithm. 1 (in the supplementary material) for details. Note that, with the alternating update procedure, each xt is now inﬂuenced by the feed-forward observations, temporal predictions and the feedback connections from the causes.  3Please refer to the supplementary material for the exact form of α∗. 4The matrix B is initialized with non-negative entries and continues to be non-negative without any addi-  tional constraints [21].  5  2.2.2 Parameter Updates  With xt and ut ﬁxed, we update the parameters by minimizing E in (5) with respect to θ. Since the inputs here are a time-varying sequence, the parameters are updated using dual estimation ﬁltering [22]; i.e., we put an additional constraint on the parameters such that they follow a state space equation of the form:  θt = θt−1 + zt  (11)  where zt is Gaussian transition noise over the parameters. This keeps track of their temporal rela- tionships. Along with this constraint, we update the parameters using gradient descent. Notice that with a ﬁxed xt and ut, each of the parameter matrices can be updated independently. Matrices C and B are column normalized after the update to avoid any trivial solution. Mini-Batch Update: To get faster convergence, the parameters are updated after performing infer- ence over a large sequence of inputs instead of at every time instance. With this “batch” of signals, more sophisticated gradient methods, like conjugate gradient, can be used and, hence, can lead to more accurate and faster convergence.  2.3 Building a hierarchy  So far the discussion is focused on encoding a small part of a video frame using a single stage network. To build a hierarchical model, we use this single stage network as a basic building block and arrange them up to form a tree structure (see Figure.1b). To learn this hierarchical model, we adopt a greedy layer-wise procedure like many other deep learning methods [6, 8, 11]. Speciﬁcally, we use the following strategy to learn the hierarchical model.  For the ﬁrst (or bottom) layer, we learn a dynamic network as described above over a group of small patches from a video. We then take this learned network and replicate it at several places on a larger part of the input frames (similar to weight sharing in a convolutional network [23]). The outputs (causes) from each of these replicated networks are considered as inputs to the layer above. Similarly, in the second layer the inputs are again grouped together (depending on the spatial proximity in the image space) and are used to train another dynamic network. Similar procedure can be followed to build more higher layers.  We again emphasis that the model is learned in a layer-wise manner, i.e., there is no top-down information while learning the network parameters. Also note that, because of the pooling of the states at each layers, the receptive ﬁeld of the causes becomes progressively larger with the depth of the model.  2.4 Inference with top-down information  With the parameters ﬁxed, we now shift our focus to inference in the hierarchical model with the top-down information. As we discussed above, the layers in the hierarchy are arranged in a Markov chain, i.e., the variables at any layer are only inﬂuenced by the variables in the layer below and the layer above. Speciﬁcally, the states x(l) and t (l+1) ) 5. Ideally, to perform inference are inﬂuenced by x t in this hierarchical model, all the states and the causes have to be updated simultaneously depending on the present state of all the other layers until the model reaches equilibrium [10]. However, such a procedure can be very slow in practice. Instead, we propose an approximate inference procedure that only requires a single top-down ﬂow of information and then a single bottom-up inference using this top-down information.  at layer l are inferred from u(l−1) (l+1) (through the prediction term C(l+1)x t  and the causes u(l) t  t  5The sufﬁxes n indicating the group are considered implicit here to simplify the notation.  6  For this we consider that at any layer l a group of input u(l−1,n) using a group of states x(l,n)  ,∀n ∈ {1, 2, ..., N} are encoded t by minimizing the following energy function:  t  t  El(x(l)  t  , u(l)  t  , θ(l)) =  N  ,∀n and the causes u(l) Xn=1(cid:16) 1  2ku(l−1,n)  t  t  t  K  − C(l)x(l,n)  2 + λkx(l,n) k2 t,k |(cid:17) + βku(l) t,k · x(l,n) |γ(l) t,k = γ0h 1 + exp(−[B(l)u(l) i  Xk=1  ]k)  +  2  t  t k1 +  γ(l)  − A(l)x(l,n) t−1 k1  1  2ku(l)  t − ˆu(l+1)  t  k2 2 (12)  where θ(l) = {A(l), B(l), C(l)}. Notice the additional term involving ˆu(l+1) when compared to (5). This comes from the top-down information, where we call ˆu(l+1) as the top-down prediction of the causes of layer (l) using the previous states in layer (l + 1). Speciﬁcally, before the “arrival” of a new observation at time t, at each layer (l) (starting from the top-layer) we ﬁrst propagate the most likely causes to the layer below using the state at the previous time instance x(l) t−1 and the predicted causes ˆu  . More formally, the top-down prediction at layer l is obtained as  t  t  (l+1) t  ˆu(l) t  = C(l)ˆx(l)  t  where ˆx(l) t  = arg min  λ(l)kx(l) ˆγt,k = (exp(−[B(l)ˆu(l+1)  (l) t  x  t  and  t − A(l)x(l)  t−1k1 + γ0  K  Xk=1  |ˆγt,k · x(l) t,k|  (13)  ]k))/2 t = ˆu(L)  At the top most layer, L, a “bias” is set such that ˆu(L) t−1, i.e., the top-layer induces some temporal coherence on the ﬁnal outputs. From (13), it is easy to show that the predicted states for layer l can be obtained as  ˆx(l)  t,k = ([A(l)x(l) These predicted causes ˆu(l) , ∀l ∈ {1, 2, ..., L} are substituted in (12) and a single layer-wise bottom- t up inference is performed as described in section 2.2.1 6. The combined prior now imposed on the causes, βku(l) 2, is similar to the elastic net prior discussed in [24], leading to k2 a smoother and biased estimate of the causes.  t−1]k, γ0γt,k < λ(l) γ0γt,k ≥ λ(l)  t − ˆu(l+1)  t k1 + 1  2ku(l)  (14)  0,  t  3 Experiments  3.1 Receptive ﬁelds of causes in the hierarchical model  Firstly, we would like to test the ability of the proposed model to learn complex features in the higher-layers of the model. For this we train a two layered network from a natural video. Each frame in the video was ﬁrst contrast normalized as described in [13]. Then, we train the ﬁrst layer of the model on 4 overlapping contiguous 15 × 15 pixel patches from this video; this layer has 400 dimensional states and 100 dimensional causes. The causes pool the states related to all the 4 patches. The separation between the overlapping patches here was 2 pixels, implying that the receptive ﬁeld of the causes in the ﬁrst layer is 17 × 17 pixels. Similarly, the second layer is trained on 4 causes from the ﬁrst layer obtained from 4 overlapping 17 × 17 pixel patches from the video. The separation between the patches here is 3 pixels, implying that the receptive ﬁeld of the causes in the second layer is 20 × 20 pixels. The second layer contains 200 dimensional states and 50 dimensional causes that pools the states related to all the 4 patches. Figure 2 shows the visualization of the receptive ﬁelds of the invariant units (columns of matrix B) at each layer. We observe that each dimension of causes in the ﬁrst layer represents a group of  6Note that the additional term 1  2ku(l)  t − ˆu(l+1)  t  k2 2 in the energy function only leads to a minor modiﬁcation  in the inference procedure, namely this has to be added to h(ut) in (10).  7  (a) Layer 1 invariant matrix, B(1)  (b) Layer 2 invariant matrix, B(2)  Figure 2: Visualization of the receptive ﬁelds of the invariant units learned in (a) layer 1 and (b) layer 2 when trained on natural videos. The receptive ﬁelds are constructed as a weighted combination of the dictionary of ﬁlters at the bottom layer.  primitive features (like inclined lines) which are localized in orientation or position 7. Whereas, the causes in the second layer represent more complex features, like corners, angles, etc. These ﬁlters are consistent with the previously proposed methods like Lee et al. [5] and Zeiler et al. [7].  3.2 Role of top-down information  In this section, we show the role of the top-down information during inference, particularly in the presence of structured noise. Video sequences consisting of objects of three different shapes (Refer to Figure 3) were constructed. The objective is to classify each frame as coming from one of the three different classes. For this, several 32 × 32 pixel 100 frame long sequences were made using two objects of the same shape bouncing off each other and the “walls”. Several such sequences were then concatenated to form a 30,000 long sequence. We train a two layer network using this sequence. First, we divided each frame into 12× 12 patches with neighboring patches overlapping by 4 pixels; each frame is divided into 16 patches. The bottom layer was trained such the 12 × 12 patches were used as inputs and were encoded using a 100 dimensional state vector. A 4 contiguous neighboring patches were pooled to infer the causes that have 40 dimensions. The second layer was trained with 4 ﬁrst layer causes as inputs, which were itself inferred from 20× 20 contiguous overlapping blocks of the video frames. The states here are 60 dimensional long and the causes have only 3 dimensions. It is important to note here that the receptive ﬁeld of the second layer causes encompasses the entire frame.  We test the performance of the DPCN in two conditions. The ﬁrst case is with 300 frames of clean video, with 100 frames per shape, constructed as described above. We consider this as a single video without considering any discontinuities. In the second case, we corrupt the clean video with “struc- tured” noise, where we randomly pick a number of objects from same three shapes with a Poisson distribution (with mean 1.5) and add them to each frame independently at a random locations. There is no correlation between any two consecutive frames regarding where the “noisy objects” are added (see Figure.3b).  (l)  First we consider the clean video and perform inference with only bottom-up inference, i.e., during inference we consider ˆu t = 0, ∀l ∈ {1, 2}. Figure 4a shows the scatter plot of the three dimen- sional causes at the top layer. Clearly, there are 3 clusters recognizing three different shape in the video sequence. Figure 4b shows the scatter plot when the same procedure is applied on the noisy video. We observe that 3 shapes here can not be clearly distinguished. Finally, we use top-down information along with the bottom-up inference as described in section 2.4 on the noisy data. We argue that, since the second layer learned class speciﬁc information, the top-down information can help the bottom layer units to disambiguate the noisy objects from the true objects. Figure 4c shows the scatter plot for this case. Clearly, with the top-down information, in spite of largely corrupted sequence, the DPCN is able to separate the frames belonging to the three shapes (the trace from one cluster to the other is because of the temporal coherence imposed on the causes at the top layer.).  7Please refer to supplementary material for more results.  8  (a) Clear Sequences  (b) Corrupted Sequences  Figure 3: Shows part of the (a) clean and (b) corrupted video sequences constructed using three different shapes. Each row indicates one sequence.  6  4  2  0 0  Object 1 Object 2 Object 3  6  4  2  0 0  0  Object 1 Object 2 Object 3  6  4  2  0 0  0  2  5  2  4  5  1  2  4  4  10  (a)  6  10  (b)  3  6  (c)  Object 1 Object 2 Object 3  0  2  Figure 4: Shows the scatter plot of the 3 dimensional causes at the top-layer for (a) clean video with only bottom-up inference, (b) corrupted video with only bottom-up inference and (c) corrupted video with top-down ﬂow along with bottom-up inference. At each point, the shape of the marker indicates the true shape of the object in the frame.  4 Conclusion  In this paper we proposed the deep predictive coding network, a generative model that empirically alters the priors in a dynamic and context sensitive manner. This model composes to two main com- ponents: (a) linear dynamical models with sparse states used for feature extraction, and (b) top-down information to adapt the empirical priors. The dynamic model captures the temporal dependencies and reduces the instability usually associated with sparse coding 8, while the task speciﬁc informa- tion from the top layers helps to resolve ambiguities in the lower-layer improving data representation in the presence of noise. We believe that our approach can be extended with convolutional methods, paving the way for implementation of high-level tasks like object recognition, etc., on large scale videos or images.  Acknowledgments  This work is supported by the Ofﬁce of Naval Research (ONR) grant #N000141010375. We thank Austin J. Brockmeier and Matthew Emigh for their comments and suggestions.  References [1] David G. Lowe. Object recognition from local scale-invariant features. In Proceedings of the International Conference on Computer Vision-Volume 2 - Volume 2, ICCV ’99, pages 1150–, 1999. ISBN 0-7695-0164-8.  [2] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection.  In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) - Volume 1 - Volume 01, CVPR ’05, pages 886–893, 2005. ISBN 0-7695-2372-2.  [3] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning  a sparse code for natural images. Nature, 381(6583):607–609, June 1996. ISSN 0028-0836.  [4] L. Wiskott and T.J. Sejnowski. Slow feature analysis: Unsupervised learning of invariances.  Neural computation, 14(4):715–770, 2002.  8Please refer to the supplementary material for more details.  9  [5] Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 609–616, 2009. ISBN 978-1-60558-516-1.  [6] K. Kavukcuoglu, P. Sermanet, Y.L. Boureau, K. Gregor, M. Mathieu, and Y. LeCun. Learn- ing convolutional feature hierarchies for visual recognition. Advances in Neural Information Processing Systems, pages 1090–1098, 2010.  [7] M.D. Zeiler, D. Krishnan, G.W. Taylor, and R. Fergus. Deconvolutional networks. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2528–2535. IEEE, 2010.  [8] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.A. Manzagol. Stacked denoising autoen- coders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11:3371–3408, 2010.  [9] Rajesh P. N. Rao and Dana H. Ballard. Dynamic model of visual recognition predicts neural  response properties in the visual cortex. Neural Computation, 9:721–763, 1997.  [10] Karl Friston. Hierarchical models in the brain. PLoS Comput Biol, 4(11):e1000211, 11 2008. [11] Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A Fast Learning Algorithm for Deep  Belief Nets. Neural Comp., (7):1527–1554, July .  [12] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise train-  ing of deep networks. In In NIPS, 2007.  [13] Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. Fast inference in sparse coding  algorithms with applications to object recognition. CoRR, abs/1010.3467, 2010.  [14] Yan Karklin and Michael S. Lewicki. A hierarchical bayesian model for learning nonlinear statistical regularities in nonstationary natural signals. Neural Computation, 17:397–423, 2005. [15] D. Angelosante, G.B. Giannakis, and E. Grossi. Compressed sensing of time-varying signals.  In Digital Signal Processing, 2009 16th International Conference on, pages 1 –8, july 2009.  [16] A. Charles, M.S. Asif, J. Romberg, and C. Rozell. Sparsity penalties in dynamical system estimation. In Information Sciences and Systems (CISS), 2011 45th Annual Conference on, pages 1 –6, march 2011.  [17] D. Sejdinovic, C. Andrieu, and R. Piechocki. Bayesian sequential compressed sensing in sparse dynamical systems. In Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on, pages 1730 –1736, 29 2010-oct. 1 2010. doi: 10.1109/ALLERTON. 2010.5707125.  [18] X. Chen, Q. Lin, S. Kim, J.G. Carbonell, and E.P. Xing. Smoothing proximal gradient method for general structured sparse regression. The Annals of Applied Statistics, 6(2):719–752, 2012. [19] Amir Beck and Marc Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, (1):183–202, March . ISSN 19364954. doi: 10.1137/080716542.  [20] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103  (1):127–152, 2005.  [21] Karol Gregor and Yann LeCun. Efﬁcient Learning of Sparse Invariant Representations. CoRR,  abs/1105.5307, 2011.  [22] Alex Nelson. Nonlinear estimation and modeling of noisy time-series by dual Kalman ﬁltering  methods. PhD thesis, 2000.  [23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comput., 1 (4):541–551, December 1989. ISSN 0899-7667. doi: 10.1162/neco.1989.1.4.541. URL http://dx.doi.org/10.1162/neco.1989.1.4.541.  [24] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the  Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005.  10  A Supplementary material for Deep Predictive Coding Networks  A.1 From section 2.2.1, computing α∗  The optimal solution of α in (6) is given by  where S(.) is a function projecting(cid:16) et  α∗ = arg max kαk∞≤1  µ 2 kαk2]  [αT et − et  2  α −  = arg min  µ(cid:13)(cid:13)(cid:13)  kαk∞≤1(cid:13)(cid:13)(cid:13) =S(cid:16) et µ(cid:17) µ(cid:17) onto an ℓ∞-ball. This is of the form: S(x) =   x, −1 ≤ x ≤ 1 1, x > 1 −1, x < −1  (15)  A.2 Algorithm for joint inference of the states and the causes.  Algorithm 1 Updating xt,ut simultaneously using FISTA-like procedure [19]. Require: Take Lx 1: Initialize x0,n ∈ RK ∀n ∈ {1, 2, ..., N}, u0 ∈ RD and set ξ1 = u0, z1,n = x0,n. 2: Set step-size parameters: τ1 = 1. 3: while no convergence do 4:  0,n > 0 ∀n ∈ {1, 2, ..., N}, Lu  0 > 0 and some η > 1.  Update  γ = γ0(1 + exp(−[Bui])/2  for n ∈ {1, 2, ..., N} do  Line search: Find the best step size Lx Compute α∗ from (15) Update xi,n using the gradient from (9) with a soft-thresholding function. Update internal variables zi+1 with step size parameter τi as in [19].  k,n.  end for  ComputePN  n=1 |xi,n|  Line search: Find the best step size Lu k. Update ui,n using the gradient of (10) with a soft-thresholding function. Update internal variables ξi+1 with step size parameter τi as in [19]. Update  τi+1 =(cid:16)1 +q(4τ 2  i + 1)(cid:17)/2  5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15:  .  .  Check for convergence. i = i + 1  16: 17: 18: end while 19: return xi,n ∀n ∈ {1, 2, ..., N} and ui  11  A.3 Inferring sparse states with known parameters  E S M  r   e t a t s   y d a e t s  3  2.5  2  1.5  1  0.5  0    20     Kalman Filter Proposed Sparse Coding [20]  40  60  80  100  Observation Dimensions  Figure 5: Shows the performance of the inference algorithm with ﬁxed parameters when compared with sparse coding and Kalman ﬁltering. For this we ﬁrst simulate a state sequence with only 20 non-zero elements in a 500-dimensional state vector evolving with a permutation matrix, which is different for every time instant, followed by a scaling matrix to generate a sequence of observations. We consider that both the permutation and the scaling matrices are known apriori. The observation noise is Gaussian zero mean and variance σ2 = 0.01. We consider sparse state-transition noise, which is simulated by choosing a subset of active elements in the state vector (number of elements is chosen randomly via a Poisson distribution with mean 2) and switching each of them with a randomly chosen element (with uniform probability over the state vector). This resemble a sparse innovation in the states. We use these generated observation sequences as inputs and use the apriori know parameters to infer the states from the dynamic model. Figure 5 shows the results obtained, where we compare the inferred states from different methods with the true states in terms of relative mean squared error (rMSE) (deﬁned as kxest k). The steady state error (rMSE) after 50 time instances is plotted versus with the dimensionality of the observation sequence. Each point is obtained after averaging over 50 runs. We observe that our model is able to converge to the true solution even for low dimensional observation, when other methods like sparse coding fail. We argue that the temporal dependencies considered in our model is able to drive the solution to the right attractor basin, insulating it from instabilities typically associated with sparse coding [24].  t − xtrue  k/kxtrue  t  t  12  A.4 Visualizing ﬁrst layer of the learned model  Active state  element at (t-1)  Corresponding,    predicted states at (t)  (a) Observation matrix (Bases)  (b) State-transition matrix  Figure 6: Visualization of the parameters. C and A, of the model described in section 3.1. (A) Shows the learned observation matrix C. Each square block indicates a column of the matrix,  reshaped as √p × √p pixel block. (B) Shows the state transition matrix A using its connections strength with the observation matrix C. On the left are the basis corresponding to the single active element in the state at time (t − 1) and on the right are the basis corresponding to the ﬁve most “active” elements in the predicted state (ordered in decreasing order of the magnitude).  (a) Connections  (b) Centers and Orientations  (c) Orientations and Frequencies  Figure 7: Connections between the invariant units and the basis functions. (A) Shows the connec- tions between the basis and columns of B. Each row indicates an invariant unit. Here the set of basis that a strongly correlated to an invariant unit are shown, arranged in the decreasing order of the magnitude. (B) Shows spatially localized grouping of the invariant units. Firstly, we ﬁt a Gabor function to each of the basis functions. Each subplot here is then obtained by plotting a line indicat- ing the center and the orientation of the Gabor function. The colors indicate the connections strength with an invariant unit; red indicating stronger connections and blue indicate almost zero strength. We randomly select a subset of 25 invariant units here. We observe that the invariant unit group the basis that are local in spatial centers and orientations. (C) Similarly, we show the correspond- ing orientation and spatial frequency selectivity of the invariant units. Here each plot indicates the orientation and frequency of each Gabor function color coded according to the connection strengths with the invariant units. Each subplot is a half-polar plot with the orientation plotted along the angle ranging from 0 to π and the distance from the center indicating the frequency. Again, we observe that the invariant units group the basis that have similar orientation.  13  ","The quality of data representation in deep learning methods is directlyrelated to the prior model imposed on the representations; however, generallyused fixed priors are not capable of adjusting to the context in the data. Toaddress this issue, we propose deep predictive coding networks, a hierarchicalgenerative model that empirically alters priors on the latent representationsin a dynamic and context-sensitive manner. This model captures the temporaldependencies in time-varying signals and uses top-down information to modulatethe representation in lower layers. The centerpiece of our model is a novelprocedure to infer sparse states of a dynamic model which is used for featureextraction. We also extend this feature extraction block to introduce a poolingfunction that captures locally invariant representations. When applied on anatural video data, we show that our method is able to learn high-level visualfeatures. We also demonstrate the role of the top-down connections by showingthe robustness of the proposed model to structured noise."
1301.3590,2013,Tree structured sparse coding on cubes  ,['Arthur Szlam'],https://arxiv.org/pdf/1301.3590.pdf,"3 1 0 2     n a J    6 1      ] T I . s c [      1 v 0 9 5 3  .  1 0 3 1 : v i X r a  Tree structured sparse coding on cubes  Arthur Szlam  City College of New York  aszlam@ccny.cuny.edu  Several recent works have discussed tree structured sparse coding [8, 10, 7, 3], where N data points in Rd written as the d × N matrix X are approximately decomposed into the product of matrices W Z. Here W is a d×K dictionary matrix, and Z is a K ×N matrix of coefﬁcients. In tree structured sparse coding, the rows of Z correspond to nodes on a tree, and the columns of Z are encouraged to be nonzero on only a few branches of the tree; or alternatively, the columns are constrained to lie on at most a speciﬁed number of branches of the tree.  When viewed from a geometric perspective, this kind of decomposition is a “wavelet analysis” of the data points in X [9, 6, 11, 1]. As each row in Z is associated to a column of W , the columns of W also take a tree structure. The decomposition corresponds to a multiscale clustering of the data, where the scale of the clustering is given by the depth in the tree, and cluster membership corresponds to activation of a row in Z. The root node rows of Z corresponds to the whole data set, and the root node columns of W are a best ﬁt linear representation of X. The set of rows of Z corresponding to each node specify a cluster- a data point x is in that cluster if it has active responses in those rows. The set of columns of W corresponding to a node specify a linear correction to the best ﬁt subspace deﬁned by the nodes ancestors; the correction is valid on the corresponding cluster.  Here we discuss the analagous construction on the binary cube {−1, 1}d. Linear best ﬁt is replaced by best ﬁt subcubes.  1 The construction on the cube  1.1 Setup  We are given N data points in Bd = {−1, 1}d written as the d × N binary matrix X. Our goal is to decompose X as a tree of subcubes and “subcube corrections”. A q dimensional subcube C = Cc,I r of Bd is determined by a point c ∈ Bd, along with a set of d − q restricted indices I r = r1, ..., rd−q. The cube Cc,I r consists of the points b ∈ Bd such that bri = cri for all ri ∈ I r, that is  Cc,I r = {b ∈ Bd s.t. bri = cri ∀ri ∈ I r}. The unrestricted indices I u = {1, ..., d} \ I r can take on either value.  1.2 The construction  Here I will describe a simple version of the construction where each node in the tree corresponds to a subcube of the same dimension q, and a hard binary clustering is used at each stage. Suppose our tree has depth l. Then the construction consists of  1. A tree structured clustering of X into sets Xij at depth (scale) i ∈ {1, ..., l} such that  2. and cluster representatives (that is d − iq-dimensional subcubes)  Xij = X,  [  j  Ccij ,I r  ij  1  such that the restricted sets have the property that if ij is an ancestor of i′j ′,  and  for all s ∈ I r ij  I r ij ∩ I r  i′j ′ = I r ij ,  cij(s) = ci′j ′ (s)  Here each cij is a vector in Bd; the complete set of cij roughly corresponds to W from before. However, note that each cij has precisely d − iq entries that actually matter; and moreover because of the nested equalities, the leaf nodes carry all the information on the branch. This is not to say that the tree structure is not important or not used- it is, as the leaf nodes have to share coordinates. However once the full construction is speciﬁed, the leaf representatives are all that is necessary to code a data point.  1.3 Algorithms  We can build the partitions and representatives starting from the root and descending down the tree as follows: ﬁrst, ﬁnd the best ﬁt d − q dimensional subcube for the whole data set. This is given by a coordinate-wise mode; the free coordinates are the ones with the largest average discrepancy from their modes. Remove the q ﬁxed coordinates from consideration. Cluster the reduced (d − q dimensional) data using K means with K = 2; on each cluster ﬁnd the best ﬁt (d − q) − q cube. Continue to the leaves.  1.3.1 Reﬁnement  The terms Ccij ,I r and Xij can be updated with a Lloyd type alternation. With all of the Xij ﬁxed, loop through each C from the root of the tree ﬁnding the best subcubes at each scale for the current partition. Now update the partition so that each x is sent to its best ﬁt leaf cube.  ij  1.3.2 Adaptive q, l, etc.  In [1], one of the important points is that many of the model parameters, including the q, l, and the number of clusters could be determined in a principled way. While it is possible that some of their analysis may carry over to this setting, it is not yet done. However, instead of ﬁxing q, we can ﬁx a percentage of the energy to be kept at each level, and choose the number of free coordinates accordingly.  2 Experiments  We use a binarized the MNIST training data by thresholding to obtain X. Here d = 282 and N = 60000. Replace 70% of the entries in X with noise sampled uniformly from {−1, 1}, and train a tree structured cube dictionary with q = 80 and depth l = 9. The subdivision scheme used to generate the multiscale clustering is 2-means initialized via randomized farthest insertion [2]; this means we can cycle spin over the dictionaries [5], to get many different reconstructions to average over. In this experiment the reconstruction was preformed 50 times for the noise realization. The results are visualized below.  References  [1] W. Allard, G. Chen, and M. Maggioni. Multiscale geometric methods for data sets II: Geomet-  ric multi-resolution analysis. to appear in Applied and Computational Harmonic Analysis.  [2] David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In Pro- ceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, SODA ’07, pages 1027–1035, Philadelphia, PA, USA, 2007. Society for Industrial and Applied Mathe- matics.  [3] Richard G. Baraniuk, Volkan Cevher, Marco F. Duarte, and Chinmay Hegde. Model-Based  Compressive Sensing. Dec 2009.  2  Figure 1: Results of denoising using the tree structured coding. The top left image is the ﬁrst 64 binarized MNIST digits after replacing 70% of the data matrix with uniform noise. The top right image is recovered, using a binary tree of depth l = 9 and q = 90, and 100 cycle spins, thus the non- binary output, as the ﬁnal result is the average of the random clustering initialization (of course with the same noise realization). The bottom left image is recovered using robust pca [4], for comparison. The bottom right is the true binary data.  3  [4] Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component  analysis? J. ACM, 58(3):11, 2011.  [5] R. R. Coifman and D. L. Donoho. Translation-invariant de-noising. Technical report, Depart-  ment of Statistics, 1995.  [6] G. David and S. Semmes. Singular integrals and rectiﬁable sets in Rn: au-del`a des graphes  Lipschitziens. Ast´erisque, 193:1–145, 1991.  [7] Laurent Jacob, Guillaume Obozinski, and Jean-Philippe Vert. Group lasso with overlap and graph lasso. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 433–440, New York, NY, USA, 2009. ACM.  [8] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical  dictionary learning. In International Conference on Machine Learning (ICML), 2010.  [9] P. W. Jones. Rectiﬁable sets and the traveling salesman problem. Invent Math, 102(1):1–15,  1990.  [10] Seyoung Kim and Eric P. Xing. Tree-guided group lasso for multi-task regression with struc-  tured sparsity. In ICML, pages 543–550, 2010.  [11] Gilad Lerman. Quantifying curvelike structures of measures by using L2 Jones quantities.  Comm. Pure Appl. Math., 56(9):1294–1365, 2003.  4  ",A brief description of tree structured sparse coding on the binary cube.
1301.3539,2013,Learning Features with Structure-Adapting Multi-view Exponential Family Harmoniums  ,"['YoonSeop Kang', 'Seungjin Choi']",https://arxiv.org/pdf/1301.3539.pdf,"3 1 0 2     n a J    6 1      ]  G L . s c [      1 v 9 3 5 3  .  1 0 3 1 : v i X r a  Learning Features with Structure-Adapting Multi-view Exponential Family Harmoniums  Yoonseop Kang1  Seungjin Choi1,2,3  Department of Computer Science and Engineering1,  Division of IT Convergence Engineering2,  Department of Creative Excellence Engineering3,  Pohang University of Science and Technology (POSTECH)  Pohang, South Korea, 790-784.  {e0en,seungjin}@postech.ac.kr  Abstract  We propose a graphical model for multi-view feature extraction that automatically adapts its structure to achieve better representation of data distribution. The pro- posed model, structure-adapting multi-view harmonium (SA-MVH) has switch parameters that control the connection between hidden nodes and input views, and learn the switch parameter while training. Numerical experiments on syn- thetic and a real-world dataset demonstrate the useful behavior of the SA-MVH, compared to existing multi-view feature extraction methods.  1 Introduction  Earlier multi-view feature extraction methods including canonical correlation analysis [1] and dual- wing harmonium (DWH) [2] assume that all views can be described using a single set of shared hidden nodes. However, these methods fail when real-world data with partially correlated views are given. More recent methods like factorized orthogonal latent space [3] or multi-view harmonium (MVH) [4] assume that views are generated from two sets of hidden nodes: view-speciﬁc hidden nodes and shared ones. Still, these models rely on the pre-deﬁned connection structure, and deciding the number of shared and view-speciﬁc hidden nodes requires a great human effort.  In this paper, we propose structure-adapting multi-view harmonium (SA-MVH) which avoids all of the problems mentioned above. Instead of explicitly deﬁning view-speciﬁc and hidden nodes in prior to the training, we only use one set of hidden nodes and let each one of them to decide the existence of connection to views using switch parameters during the training. In this manner, SA- MVH automatically decides the number of view-speciﬁc latent variables and also captures partial correlation among views.  2 The Proposed Model  The deﬁnition of SA-MVH begins with choosing marginal distributions of visible node sets v(k) and a set of hidden nodes h from exponential family distributions:  p(v(k)  i  ) ∝ exp(X  a  p(hj) ∝ exp(X  b  ia f (k) ξ(k)  ia (v(k)  i  ) − A(k)  i  ({ξ(k)  ia })),  λjbgjb(hj) − Bj({λjb})),  (1)  f (·), g(·) are sufﬁcient statistics, ξ, λ are natural parameters, and A, B are log-partition functions.  1  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  (a) DWH  (b) MVH  (c) SA-MVH  Figure 1: Graphical models of (a) dual-wing harmonium, (b) multi-view harmonium, and (c) structure-adapting multi-view harmonium.  Connections between visible nodes and hidden nodes of SA-MVH are deﬁned by weight matrices {W (k)} and switch parameters σ(skj ) ∈ [0, 1], where σ(·) is a sigmoid function. A switch skj controls the connection between k-th view and j-th hidden node by being multiplied to the j-th column of weight matrix W (k) (Figure 1). When σ(skj ) is large (> 0.5), we consider the view and the hidden node to be connected. With the quadratic term including weights and switch parameters, the joint distribution of SA-MVH is deﬁned as below: p({v(k)}, h) ∝ exp(cid:0)X note that indices a and b are omitted to keep the notations uncluttered.  )gj(hj) − X  λjgj(hj)(cid:1).  σ(skj )W (k)  ) − X  i f (k) ξ(k)  ij f (k)  (v(k)  (v(k)  k,i,j  k,i  j  i  i  i  i  (2)  We learn the parameters W (k), ξ(k), λ, and switch parameters skj by maximizing the likelihood of model via gradient ascent. The likelihood of SA-MVH is deﬁned as the joint distribution of nodes summed over hidden nodes h:  L = hlog p({v(k)})idata = (cid:10)logX  h  p({v(k)}, h)(cid:11)data  ,  (3)  where h·idata represents expectation over data distribution. Then the gradient of log-likelihood with respect to the parameters W (k), ξ(k), λ, and skj are derived as follows:  ∂L  ∂W (k) ij ∂L ∂ξ(k) i ∂L ∂λj ∂L ∂skj  ∝ (cid:10)σ(skj )fi(v  (k) i  )B ′  j(ˆλj )(cid:11)data − (cid:10)σ(skj )fi(v  (k) i  )B ′  j(ˆλj )(cid:11)model  ∝ (cid:10)f (k)  i  (v  (k) i  )(cid:11)data − (cid:10)f (k)  i  (v  (k) i  )(cid:11)model  ∝ (cid:10)B ′ ∝ Dσ′(skj )W (k)  j(ˆλj )(cid:11)data − (cid:10)B ′ ij fi(v(k)  i  j(ˆλj)(cid:11)model  ,  )B ′  j(ˆλj )Edata  − Dσ′(skj )W (k)  ij fi(v(k)  i  )B ′  where h·imodel represents expectation over model distribution p({v(k)}, h) and ˆξ(k) Pj σ(skj )W (k) ) are shifted parameters. 3 Numerical Experiments  ij gj(hj), ˆλj = λj + Pk,i σ(skj )W (k)  ij fi(v(k)  i  (4)  (5)  (6)  (7)  j(ˆλj )Emodel i = ξ(k)  i +  3.1 Feature Extraction on Noisy Arabic-Roman Digit Dataset  To simulate the view-speciﬁc and shared properties of multi-view data, we designed a synthetic dataset which contains 11,800 pairs of Arabic digits and the corresponding Roman digits written in various fonts. For each pair, we added random vertical line noises to Arabic digits, and horizontal line noises to Roman digits (Figure 2-(a)). SA-MVH trained with 200 hidden nodes found 95 shared features (with connection to both views), and 47 view-speciﬁc features for Roman digits, and 32 for Arabic digits. Remaining 26 were not connected to any views and ignored. Most of the shared features were noise-free and encoded parts of Roman and Arabic numbers (Figure 2-(b)). On the other hand, the view-speciﬁc features had components with horizontal or vertical noises, as well as the parts of the numbers (Figure 2-(c)). In this example, SA-MVH automatically separated view- speciﬁc and shared information without any prior speciﬁcation of the graph structure.  2  (a)  (b) Shared features  (c) View-speciﬁc features  Figure 2: (a) 10 samples from Noisy Arabic-Roman digit dataset, (b) shared features, and (c) view- speciﬁc features learned by SA-MVH.  Table 1: Image classiﬁcation accuracy of k-nn classiﬁer using feature extraction methods trained on Caltech-256 dataset. For each value of k, the best result is marked as bold text.  Method  Sparse Filtering  DWH MVH  SA-MVH  # 10-NN 30-NN 50-NN 70-NN 100-NN 0.155 0.194 0.191 0.198  0.161 0.237 0.239 0.246  0.165 0.231 0.225 0.232  0.163 0.217 0.216 0.223  0.16 0.207 0.203 0.212  3.2 Image Classiﬁcation on Caltech-256 Dataset  We extracted 512 dimensions of GIST features and 1,536 dimensions of histogram of gradients (HoG) features from Caltech-256 dataset to simulate multi-view settings. SA-MVH and other multi- view feature extraction methods based on harmonium – DWH and MVH were trained on the dataset for comparison. We also compared our method to Sparse Filtering [5], which is not a harmonium- based method. We trained the feature extraction methods and tested the methods with k-nearest neighbor classiﬁers (Table 1). SA-MVH resulted better than other feature extraction models in this experiment, regardless of the value of k for nearest neighbor classiﬁer.  4 Conclusion  In this paper, we have proposed the multi-view feature extraction model that automatically decides relations between latent variables and input views. The proposed method, SA-MVH models multi- view data distribution with less restrictive assumption and also reduces the number of parameters to tune by human hand. SA-MVH introduces switch parameters that control the connections between hidden nodes and input views, and ﬁnd the desirable conﬁguration while training. We have demon- strated the effectiveness of our approach by comparing our model to existing models in experiments on synthetic dataset, and image classiﬁcation with simulated multi-view setting.  References  [1] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor, “Canonical correlation analysis: An overview  with applications to learning methods,” Neural Computation, vol. 16, pp. 2639–2664, 2004.  [2] E. P. Xing, R. Yan, and A. G. Hauptmann, “Mining associated text and images with dual-wing harmonium,” in Proceedings of the Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI), Edinburgh, UK, 2005.  [3] M. Salzmann, C. H. Ek, R. Urtasun, and T. Darrell, “Factorized orthogonal latent spaces,” in Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), Sardinia, Italy, 2010.  [4] Y. Kang and S. Choi, “Restricted deep belief networks for multi-view learning,” in Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), Athens, Greece, 2011.  [5] J. Ngiam, P. W. Koh, Z. Chen, S. A. Bhaskar, and A. Y. Ng, “Sparse ﬁltering,” in Advances in  Neural Information Processing Systems (NIPS), vol. 23. MIT Press, 2011.  3  ","We proposea graphical model for multi-view feature extraction thatautomatically adapts its structure to achieve better representation of datadistribution. The proposed model, structure-adapting multi-view harmonium(SA-MVH) has switch parameters that control the connection between hidden nodesand input views, and learn the switch parameter while training. Numericalexperiments on synthetic and a real-world dataset demonstrate the usefulbehavior of the SA-MVH, compared to existing multi-view feature extractionmethods."
1301.3781,2013,Efficient Estimation of Word Representations in Vector Space  ,"['Tomas Mikolov', 'Kai Chen', 'Greg Corrado', 'Jeffrey Dean']",https://arxiv.org/pdf/1301.3781.pdf,"3 1 0 2     p e S 7         ] L C . s c [      3 v 1 8 7 3  .  1 0 3 1 : v i X r a  Efﬁcient Estimation of Word Representations in  Vector Space  Tomas Mikolov  Kai Chen  Google Inc., Mountain View, CA  Google Inc., Mountain View, CA  tmikolov@google.com  kaichen@google.com  Greg Corrado  Jeffrey Dean  Google Inc., Mountain View, CA  Google Inc., Mountain View, CA  gcorrado@google.com  jeff@google.com  Abstract  We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities.  1  Introduction  Many current NLP systems and techniques treat words as atomic units - there is no notion of similar- ity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data. An example is the popular N-gram model used for statistical language modeling - today, it is possible to train N-grams on virtually all available data (trillions of words [3]). However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any signiﬁcant progress, and we have to focus on more advanced techniques. With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models. Probably the most successful concept is to use distributed representations of words [10]. For example, neural network based language models signiﬁcantly outperform N-gram models [1, 27, 17].  1.1 Goals of the Paper  The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more  1  than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100. We use recently proposed techniques for measuring the quality of the resulting vector representa- tions, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inﬂectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to ﬁnd words that have similar endings [13, 14]. Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are per- formed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vec- tor(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20]. In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities1, and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.  1.2 Previous Work  Representation of words as continuous vectors has a long history [10, 26, 8]. A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others. Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are ﬁrst learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are learned using a simple model. It was later shown that the word vectors can be used to signiﬁcantly improve and simplify many NLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were made available for future research and comparison2. However, as far as we know, these architectures were signiﬁcantly more computationally expensive for training than the one proposed in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices are used [23].  2 Model Architectures  Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform signiﬁcantly better than LSA for preserving linear regularities among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets. Similar to [18], to compare different model architectures we deﬁne ﬁrst the computational complex- ity of a model as the number of parameters that need to be accessed to fully train the model. Next, we will try to maximize the accuracy, while minimizing the computational complexity.  1The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt 2http://ronan.collobert.com/senna/  http://metaoptimize.com/projects/wordreprs/ http://www.fit.vutbr.cz/˜imikolov/rnnlm/ http://ai.stanford.edu/˜ehhuang/  2  For all the following models, the training complexity is proportional to  O = E × T × Q,  (1)  where E is number of the training epochs, T is the number of the words in the training set and Q is deﬁned further for each model architecture. Common choice is E = 3− 50 and T up to one billion. All models are trained using stochastic gradient descent and backpropagation [26].  2.1 Feedforward Neural Net Language Model (NNLM)  The probabilistic feedforward neural network language model has been proposed in [1]. It consists of input, projection, hidden and output layers. At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a projection layer P that has dimensionality N × D, using a shared projection matrix. As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation. The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense. For a common choice of N = 10, the size of the projection layer (P ) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000 units. Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity per each training example is  Q = N × D + N × D × H + H × V,  (2) where the dominating term is H × V . However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training [4, 9]. With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log2(V ). Thus, most of the complexity is caused by the term N × D × H. In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary tree. This follows previous observations that the frequency of words works well for obtaining classes in neural net language models [16]. Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log2(U nigram perplexity(V )). For example when the vocabulary size is one million words, this results in about two times speedup in evaluation. While this is not crucial speedup for neural network LMs as the computational bottleneck is in the N ×D×H term, we will later propose architectures that do not have hidden layers and thus depend heavily on the efﬁciency of the softmax normalization.  2.2 Recurrent Neural Net Language Model (RNNLM)  Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N), and because theoretically RNNs can efﬁciently represent more complex patterns than the shallow neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and output layer. What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections. This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step. The complexity per training example of the RNN model is  Q = H × H + H × V,  (3)  where the word representations D have the same dimensionality as the hidden layer H. Again, the term H × V can be efﬁciently reduced to H × log2(V ) by using hierarchical softmax. Most of the complexity then comes from H × H.  3  2.3 Parallel Training of Neural Networks  To train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called DistBelief [6], including the feedforward NNLM and the new models proposed in this paper. The framework allows us to run multiple replicas of the same model in parallel, and each replica synchronizes its gradient updates through a centralized server that keeps all the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use one hundred or more model replicas, each using many CPU cores at different machines in a data center.  3 New Log-linear Models  In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity. The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model. While this is what makes neural networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efﬁciently. The new architectures directly follow those proposed in our earlier work [13, 14], where it was found that neural network language model can be successfully trained in two steps: ﬁrst, continuous word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these distributed representations of words. While there has been later substantial amount of work that focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one. Note that related models have been proposed also much earlier [26, 8].  3.1 Continuous Bag-of-Words Model  The ﬁrst proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged). We call this archi- tecture a bag-of-words model as the order of words in the history does not inﬂuence the projection. Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classiﬁer with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word. Training complexity is then  (4) We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context. The model architecture is shown at Figure 1. Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.  Q = N × D + D × log2(V ).  3.2 Continuous Skip-gram Model  The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classiﬁcation of a word based on another word in the same sentence. More precisely, we use each current word as an input to a log-linear classiﬁer with continuous projection layer, and predict words within a certain range before and after the current word. We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples. The training complexity of this architecture is proportional to  (5) where C is the maximum distance of the words. Thus, if we choose C = 5, for each training word we will select randomly a number R in range < 1; C >, and then use R words from history and  Q = C × (D + D × log2(V )),  4  Figure 1: New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.  R words from the future of the current word as correct labels. This will require us to do R × 2 word classiﬁcations, with the current word as input, and each of the R + R words as output. In the following experiments, we use C = 10.  4 Results  To compare the quality of different versions of word vectors, previous papers typically use a table showing example words and their most similar words, and understand them intuitively. Although it is easy to show that word France is similar to Italy and perhaps some other countries, it is much more challenging when subjecting those vectors in a more complex similarity task, as follows. We follow previous observation that there can be many different types of similarities between words, for example, word big is similar to bigger in the same sense that small is similar to smaller. Example of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further denote two pairs of words with the same relationship as a question, as we can ask: ”What is the word that is similar to small in the same sense as biggest is similar to big?” Somewhat surprisingly, these questions can be answered by performing simple algebraic operations with the vector representation of words. To ﬁnd a word that is similar to small in the same sense as biggest is similar to big, we can simply compute vector X = vector(”biggest”)− vector(”big”) + vector(”small”). Then, we search in the vector space for the word closest to X measured by cosine distance, and use it as the answer to the question (we discard the input question words during this search). When the word vectors are well trained, it is possible to ﬁnd the correct answer (word smallest) using this method. Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented.  5  w(t-2)w(t+1)w(t-1)w(t+2)w(t)SUM       INPUT         PROJECTION         OUTPUTw(t)          INPUT         PROJECTION      OUTPUTw(t-2)w(t-1)w(t+1)w(t+2)                   CBOW                                                   Skip-gramTable 1: Examples of ﬁve types of semantic and nine types of syntactic questions in the Semantic- Syntactic Word Relationship test set.  Type of relationship Common capital city All capital cities Currency City-in-state Man-Woman Adjective to adverb Opposite Comparative Superlative Present Participle Nationality adjective Past tense Plural nouns Plural verbs  Word Pair 1  Word Pair 2  Athens Astana Angola Chicago brother apparent possibly  great easy think  Switzerland  walking mouse work  Greece  Kazakhstan  kwanza Illinois sister  apparently impossibly  greater easiest thinking Swiss walked mice works  Oslo Harare Iran  Norway Zimbabwe  rial  Stockton grandson  California  granddaughter  rapid ethical tough lucky read  Cambodia swimming  dollar speak  rapidly unethical tougher luckiest reading  Cambodian  swam dollars speaks  4.1 Task Description  To measure quality of the word vectors, we deﬁne a comprehensive test set that contains ﬁve types of semantic questions, and nine types of syntactic questions. Two examples from each category are shown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions in each category were created in two steps: ﬁrst, a list of similar word pairs was created manually. Then, a large list of questions is formed by connecting two word pairs. For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have included in our test set only single token words, thus multi-word entities are not present (such as New York). We evaluate the overall accuracy for all question types, and for each question type separately (se- mantic, syntactic). Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely to be impossible, as the current models do not have any input information about word morphology. However, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric. Further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions.  4.2 Maximization of Accuracy  We have used a Google News corpus for training the word vectors. This corpus contains about 6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we are facing time constrained optimization problem, as it can be expected that both using more data and higher dimensional word vectors will improve the accuracy. To estimate the best choice of model architecture for obtaining as good as possible results quickly, we have ﬁrst evaluated models trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words. The results using the CBOW architecture with different choice of word vector dimensionality and increasing amount of the training data are shown in Table 2. It can be seen that after some point, adding more dimensions or adding more training data provides diminishing improvements. So, we have to increase both vector dimensionality and the amount of the training data together. While this observation might seem trivial, it must be noted that it is currently popular to train word vectors on relatively large amounts of data, but with insufﬁcient size  6  Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word vectors from the CBOW architecture with limited vocabulary. Only questions containing words from the most frequent 30k words are used.  Dimensionality / Training words  50 100 300 600  24M 49M 98M 196M 391M 783M 23.2 13.4 32.2 19.4 23.2 45.9 50.4 24.0  15.7 23.1 29.2 30.1  18.6 27.8 35.3 36.5  19.1 28.7 38.6 40.8  22.5 33.4 43.7 46.6  Table 3: Comparison of architectures using models trained on the same data, with 640-dimensional word vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set, and on the syntactic relationship test set of [20]  Model  Semantic-Syntactic Word Relationship test set  MSR Word Relatedness  Architecture  Semantic Accuracy [%]  Syntactic Accuracy [%]  Test Set [20]  RNNLM NNLM CBOW  Skip-gram  9 23 24 55  36 53 64 59  35 47 61 56  (such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the same increase of computational complexity as increasing vector size twice. For the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi- ent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so that it approaches zero at the end of the last training epoch.  4.3 Comparison of Model Architectures  First we compare different model architectures for deriving the word vectors using the same training data and using the same dimensionality of 640 of the word vectors. In the further experiments, we use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to the 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic similarity between words3. The training data consists of several LDC corpora and is described in detail in [18] (320M words, 82K vocabulary). We used these data to provide a comparison to a previously trained recurrent neural network language model that took about 8 weeks to train on a single CPU. We trained a feed- forward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6], using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the projection layer has size 640 × 8). In Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly on the syntactic questions. The NNLM vectors perform signiﬁcantly better than the RNN - this is not surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden layer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the same on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but still better than the NNLM), and much better on the semantic part of the test than all the other models. Next, we evaluated our models trained using one CPU only and compared the results against publicly available word vectors. The comparison is given in Table 4. The CBOW model was trained on subset  3We thank Geoff Zweig for providing us the test set.  7  Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation- ship test set, and word vectors from our models. Full vocabularies are used.  Model  Vector  Dimensionality  Training words  Accuracy [%]  Collobert-Weston NNLM Turian NNLM Turian NNLM Mnih NNLM Mnih NNLM Mikolov RNNLM Mikolov RNNLM Huang NNLM Our NNLM Our NNLM Our NNLM CBOW Skip-gram  50 50 200 50 100 80 640 50 20 50 100 300 300  Semantic  9.3 1.4 1.4 1.8 3.3 4.9 8.6 13.3 12.9 27.9 34.2 15.5 50.0  Syntactic Total 11.0 2.1 1.8 5.8 8.8 12.7 24.6 12.3 20.3 43.2 50.8 36.1 53.3  12.3 2.6 2.2 9.1 13.2 18.4 36.5 11.6 26.4 55.8 64.5 53.1 55.9  660M 37M 37M 37M 37M 320M 320M 990M  6B 6B 6B  783M 783M  Table 5: Comparison of models trained for three epochs on the same data and models trained for one epoch. Accuracy is reported on the full Semantic-Syntactic data set.  Model  Vector  Dimensionality  Training words  Accuracy [%]  Training time  [days]  3 epoch CBOW 3 epoch Skip-gram 1 epoch CBOW 1 epoch CBOW 1 epoch CBOW 1 epoch Skip-gram 1 epoch Skip-gram 1 epoch Skip-gram  300 300 300 300 600 300 300 600  Semantic  15.5 50.0 13.8 16.1 15.4 45.6 52.2 56.7  Syntactic Total 36.1 53.3 33.6 36.1 36.2 49.2 53.8 55.5  53.1 55.9 49.9 52.6 53.3 52.2 55.1 54.5  783M 783M 783M 1.6B 783M 783M 1.6B 783M  1 3 0.3 0.6 0.7 1 2 2.5  of the Google News data in about a day, while training time for the Skip-gram model was about three days. For experiments reported further, we used just one training epoch (again, we decrease the learning rate linearly so that it approaches zero at the end of training). Training a model on twice as much data using one epoch gives comparable or better results than iterating over the same data for three epochs, as is shown in Table 5, and provides additional small speedup.  4.4 Large Scale Parallel Training of Models  As mentioned earlier, we have implemented various models in a distributed framework called Dis- tBelief. Below we report the results of several models trained on the Google News 6B data set, with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada- grad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an  8  Table 6: Comparison of models trained using the DistBelief distributed framework. Note that training of NNLM with 1000-dimensional vectors would take too long to complete.  Model  Vector  Dimensionality  Training words  Accuracy [%]  Training time  [days x CPU cores]  NNLM CBOW Skip-gram  100 1000 1000  Semantic  34.2 57.3 66.1  Syntactic Total 50.8 63.7 65.6  64.5 68.9 65.1  6B 6B 6B  14 x 180 2 x 140 2.5 x 125  Table 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.  Architecture 4-gram [32] Average LSA similarity [32] Log-bilinear model [24] RNNLMs [19] Skip-gram Skip-gram + RNNLMs  Accuracy [%]  39 49 54.8 55.4 48.0 58.9  estimate since the data center machines are shared with other production tasks, and the usage can ﬂuctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of the CBOW model and the Skip-gram model are much closer to each other than their single-machine implementations. The result are reported in Table 6.  4.5 Microsoft Research Sentence Completion Challenge  The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing language modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one word is missing in each sentence and the goal is to select word that is the most coherent with the rest of the sentence, given a list of ﬁve reasonable choices. Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear model [24] and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark [19]. We have explored the performance of Skip-gram architecture on this task. First, we train the 640- dimensional model on 50M words provided in [32]. Then, we compute score of each sentence in the test set by using the unknown word at the input, and predict all surrounding words in a sentence. The ﬁnal sentence score is then the sum of these individual predictions. Using the sentence scores, we choose the most likely sentence. A short summary of some previous results together with the new results is presented in Table 7. While the Skip-gram model itself does not perform on this task better than LSA similarity, the scores from this model are complementary to scores obtained with RNNLMs, and a weighted combination leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and 58.7% on the test part of the set).  5 Examples of the Learned Relationships  Table 8 shows words that follow various relationships. We follow the approach described above: the relationship is deﬁned by subtracting two word vectors, and the result is added to another word. Thus for example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, although there is clearly a lot of room for further improvements (note that using our accuracy metric that  9  Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip- gram model trained on 783M words with 300 dimensionality).  Relationship France - Paris big - bigger  Miami - Florida Einstein - scientist Sarkozy - France  copper - Cu  Berlusconi - Silvio Microsoft - Windows Microsoft - Ballmer  Example 1 Italy: Rome small: larger  Baltimore: Maryland  Example 2 Japan: Tokyo cold: colder Dallas: Texas  Messi: midﬁelder Berlusconi: Italy  Mozart: violinist Merkel: Germany  Example 3  Florida: Tallahassee  quick: quicker Kona: Hawaii Picasso: painter Koizumi: Japan  zinc: Zn  Sarkozy: Nicolas Google: Android Google: Yahoo  gold: Au  uranium: plutonium  Putin: Medvedev  IBM: Linux  IBM: McNealy France: tapas  Obama: Barack Apple: iPhone Apple: Jobs USA: pizza  Japan - sushi  Germany: bratwurst  assumes exact match, the results in Table 8 would score only about 60%). We believe that word vectors trained on even larger data sets with larger dimensionality will perform signiﬁcantly better, and will enable the development of new innovative applications. Another way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test. It is also possible to apply the vector operations to solve different tasks. For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and ﬁnding the most distant word vector. This is a popular type of problems in certain human intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.  6 Conclusion  In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That is several orders of magnitude larger than the best previously published results for similar models. An interesting task where the word vectors have recently been shown to signiﬁcantly outperform the previous state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman’s rank correlation over the previous best result [31]. The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can be expected that these applications can beneﬁt from the model architectures described in this paper. Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for veriﬁcation of correctness of existing facts. Results from machine translation experiments also look very promising. In the future, it would be also interesting to compare our techniques to Latent Relational Analysis [30] and others. We believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors. We also expect that high quality word vectors will become an important building block for future NLP applications.  10  7 Follow-Up Work  After the initial version of this paper was written, we published single-machine multi-threaded C++ code for computing the word vectors, using both the continuous bag-of-words and skip-gram archi- tectures4. The training speed is signiﬁcantly higher than reported earlier in this paper, i.e. it is in the order of billions of words per hour for typical hyperparameter choices. We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of our follow-up work will be published in an upcoming NIPS 2013 paper [21].  References [1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-  chine Learning Research, 3:1137-1155, 2003.  [2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-  chines, MIT Press, 2007.  [3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, 2007.  [4] R. Collobert and J. Weston. A Uniﬁed Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008.  [5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan- guage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493- 2537, 2011.  [6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A.  Senior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.  [7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and  stochastic optimization. Journal of Machine Learning Research, 2011.  [8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990. [9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations via Global Context and Multiple Word Prototypes. In: Proc. Association for Computational Linguistics, 2012.  [10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis- tributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations, MIT Press, 1986.  [11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring degrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), 2012.  [12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for  sentiment analysis. In Proceedings of ACL, 2011.  [13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-  versity of Technology, 2007.  [14] T. Mikolov, J. Kopeck´y, L. Burget, O. Glembek and J. ˇCernock´y. Neural network based lan-  guage models for higly inﬂective languages, In: Proc. ICASSP 2009.  [15] T. Mikolov, M. Karaﬁ´at, L. Burget, J. ˇCernock´y, S. Khudanpur. Recurrent neural network  based language model, In: Proceedings of Interspeech, 2010.  [16] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock´y, S. Khudanpur. Extensions of recurrent neural  network language model, In: Proceedings of ICASSP 2011.  [17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇCernock´y. Empirical Evaluation and Com- bination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.  4The code is available at https://code.google.com/p/word2vec/  11  [18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ˇCernock´y. Strategies for Training Large Scale Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understand- ing, 2011.  [19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-  sity of Technology, 2012.  [20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-  tations. NAACL HLT 2013.  [21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of  Words and Phrases and their Compositionality. Accepted to NIPS 2013.  [22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,  2007.  [23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural  Information Processing Systems 21, MIT Press, 2009.  [24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language  models. ICML, 2012.  [25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,  2005.  [26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-  propagating errors. Nature, 323:533.536, 1986.  [27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,  2007.  [28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and  Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.  [29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for  Semi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.  [30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-  tional Joint Conference on Artiﬁcial Intelligence, 2005.  [31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for  Measuring Relational Similarity. NAACL HLT 2013.  [32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft  Research Technical Report MSR-TR-2011-129, 2011.  12  ","We propose two novel model architectures for computing continuous vectorrepresentations of words from very large data sets. The quality of theserepresentations is measured in a word similarity task, and the results arecompared to the previously best performing techniques based on different typesof neural networks. We observe large improvements in accuracy at much lowercomputational cost, i.e. it takes less than a day to learn high quality wordvectors from a 1.6 billion words data set. Furthermore, we show that thesevectors provide state-of-the-art performance on our test set for measuringsyntactic and semantic word similarities."
1301.3461,2013,Factorized Topic Models  ,"['Cheng Zhang', 'Carl Henrik Ek', 'Hedvig Kjellstrom']",https://arxiv.org/pdf/1301.3461.pdf,"3 1 0 2    r p A 3 2         ]  G L . s c [      7 v 1 6 4 3  .  1 0 3 1 : v i X r a  Factorized Topic Models  Cheng Zhang  CVAP/CAS, KTH Stockholm, Sweden  Carl Henrik Ek CVAP/CAS, KTH Stockholm, Sweden  Andreas Damianou University of Shefﬁeld  Shefﬁeld, UK  Hedvig Kjellstr¨om CVAP/CAS, KTH Stockholm, Sweden  chengz@kth.se  chek@kth.se  andreas.damianou@sheffield.ac.uk  hedvig@kth.se  Abstract  In this paper we present a modification to a latent topic model, which makes the model exploit supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior over the topic space. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for image, text, and video classification.  1 Introduction Representing data in terms of latent variables is an important tool in many applications. A generative latent variable model provides a parameterization that encodes the variations in the observed data, relating them to an underlying representation, e.g., a set of classes, using some kind of mapping. It is important to note that any modeling task is inherently ill-conditioned as there exists an infinite number of combinations of mappings and parameterizations that could have generated the data. To that end, we choose different models, based on different assumptions and preferences that will induce different representations, motivated by how well they fit the data and for what purpose we wish to use the representation.  Inference in generative models meets difﬁculties if the variations in the observed data are not rep- resentative of the variations in the underlying state to be inferred. As an example, consider a visual animal classiﬁer, trained with, e.g., SIFT [20] features extracted from training images of horses, cows and cats with a variation of fur texture. The task is now to classify an image of a spotted horse. Based on the features, which will mostly pick up the fur texture, the classiﬁer will be unsure of the class, since there are spotted horses, cows and cats in the training data. The core of the problem is that fur texture is a weak cue to animal class given this data: Horses, cows and cats can all be red, spotted, brown, black and grey. Shape is on the other hand a strong cue to distinguish between these classes. However, the visual features will mostly capture texture information – the shape in- formation (signal) is “hidden” among the signiﬁcantly richer texture information (structured noise) making up the dominant part of the variation in the data.  In this paper we address this issue by explicitly factorizing the data into a structured noise part, whose variations are shared between all classes, and a signal part, whose variations are characteristic of a certain class. For our purposes, it is very useful to think about data as composed of topics. Probabilistic topic models [23, 11, 4, 2] model a data example as a collection of words (in the case of images, visual words), each sampled from a latent distribution of topics. The topics can be thought of as different aspects of the data – a topic model trained with the data in our animal example above might model one topic for shape and another for fur texture, and a certain data instance is modeled as a combination of a certain shape and a certain texture.  Our approach is to encourage the topics to assume either a very high correlation or a very low correlation with class. The class can then be inferred using only the class-speciﬁc topics, while the  This research has been supported by the EU through TOMSY, IST-FP7- Collaborative Project-270436, and  the Swedish Research Council (VR).  1  shared topics are used to explain away the aspects of the data that are not interesting to this particular inference problem. We present a variant of a Latent Dirichlet Allocation (LDA) [2] model which is able to model the signal and structured noise separately from the data. This new model is trained using a factorizing prior, which partitions the topic space into a private signal part and a shared noise part. The model is described in Section 3.  Experiments in Section 4 show that the proposed model outperforms both the standard LDA and a supervised variant, SLDA [3], on classification of images, text, and video. Furthermore, the explicit noise model increases the sparsity of the topic representation. This is encouraging for two reasons: firstly, it indicates that the factorized LDA model is a better model of class compared to the unrestricted LDA; enabling better performance on any inference or data synthesis task. Secondly, it enables a more economical data representation in terms of storage and computation; crucial for applications with very large data sets. The factorization method can be applied to other topic models as well, and the sparse factorized topic representation is beneficial not only for classification, as shown here, but also for synthesis [5], ambiguity modeling [7], and domain transfer [21].  2 Related Work  In this section we will create a context for the model that we are about to propose by relating it to factorized latent variable models in general and topic models in specific. Providing a complete review of either is beyond the scope of this paper, why here we will focus on only the most relevant subset of work needed to motivate the model.  The motivation for learning a latent variable model is to exploit the structure of the new representa- tion to perform tasks such as synthesis or prediction of novel data, or to ease an association task such as classiﬁcation. For continuous observations, several classic algorithms such as Principal Compo- nent Analysis (PCA) and Canonical Correlation Analysis (CCA) can be interpreted as latent variable models [1, 16, 17, 27]. Another modeling scenario is when observations are provided in the form of collections of discrete entities. An example is text data where a document consists of a collection of words. One approach to encode such data is using a latent representation that groups words in terms of topics. Several approaches for automatically learning topics from data have been suggested in the literature. A first proposal of a generative topic model was Probabilistic Latent Semantic Indexing (pLSI) [11]. The model represents each document as a mixture of topics. The next important devel- opment in terms of a Bayesian version of pLSI by adding a prior to the mixture weights. This was done by the adaptation of a Dirichlet layer and referred to as Latent Dirichlet Allocation (LDA) [4].  Central to the work presented in this paper is a specific latent structure simultaneously proposed by several authors [7, 13, 14, 18]. Given multiple observation modalities of a single underlying state, the purpose of these models is to learn a representation that separately encodes the modality- independent variance from the modality-dependent. The latent representation is factorized such that the modality-independent and modality-dependent are encoded in separate subspaces [6]. This factorization has an intuitive interpretation in that the private space encodes variations that exists in only one modality and does therefore encode variations representing the ambiguities between the modalities [7].  In this paper we will exploit a similar type of factorization within a topic model, but instead of ex- ploiting correlations between observation modalities, we employ a single observation modality and a class label associated with each observation. In speciﬁc, our approach will encourage a factoriza- tion relating to class, such that the topics will be split into those encoding within-class variations from those that encode between-class variations. Such a factorization becomes interesting for in- ferring the class label from unseen data; the class-shared topics can be considered as representing “structured noise” while only the private class topics contain the relevant for class inference.  However, it is not easy to directly transfer the above factorization, formulated between modalities and described for continuous data, to topic models, which are inherently discrete. Results have been presented [12, 25, 28] for the case of two conditionally independent observation modalities, addressing the image and text cross-modal multimedia retrieval problem with topic representation. In [12] a model that can be seen as a Markov random field of LDA topic models is presented. The topic distribution of each topic model affect the underlying topic spaces of other topic models, connected to that model through the Markov random field. Further, in [25] CCA is applied to the topic space of the text data, which in turn has been learned from LDA and the image feature space.  2  π  z  β  K  w  N  M  c  θ  α  C  θ  M  α  c  θ  π  z  β  K  w  N  M  (a) LDA with class label [8]  (b) Our factorized LDA  Figure 1: Graphic representation of LDA structures. The notation in (b) is adopted from Jia et al. [12].  LDA and CCA are used as two separate steps. Differently, [28] instead use a Hierarchical Dirichlet Process (HDP) based method which has a complexity selection property. It takes the topics that only describe variance in only one modality as the private space, which explains away the information that cannot be matched between different modalities. This is an extension of [22] to multi-modalities, hence it can not be generalized to other topic models, such as LDA or pLSI and it can not be used to model the private and shared information with only one modality.  Differently from [12, 28, 25], which need to model the shared topics and private topics in the joint topic space across different observation modalities, our factorization takes place over one modality across different classes, where the structured noise is modeled in the class-shared topics and the signal is modeled in class-private topics. Furthermore, and importantly, our approach is ﬂexible and can be easily transferred to any type of topic model. Our choice of LDA stems from the fact that it has previously been successfully applied for a large range of data and has desirable sparsity properties that makes for an efficient model.  Topic models, and the LDA model in specific, are motivated by the benefit of representations that are sparse in terms of the distribution of topics for each document. In addition to this, the model we are about to present aims to encourage a specific structure of the topic themselves. This notion is not new and have been proposed by several other authors. In [9] the topics are represented as combinations of a small number of latent components as such leading to a more compact model. In [29] the each topic is constrained by the words in the vocabulary. However, none of these models aim to learn a topic structure that is related to class.  3 Model As described in the introduction, we add factorization to a model that describes variations of data in terms of a set of latent topics. We seek a structured representation that encodes topics containing within-class, or class private, variations separately from those containing variations that are shared between the classes. We apply our factorization framework to an adaptation of LDA, which in- corporates additional class information to recover such a factorized latent space. In this section, the traditional LDA model [4] is first revisited, followed by the description of our factorized topic model.  3.1 LDA Revisited Formally a document w consist of a collection of words w = [w1, . . . , wN ] from a vocabulary indexed by {1, . . . , V }. Within a topic model each document of N words is described as a mixture of K topics such that each word is associated with a specific topic: z = [z1, . . . , zN ], where zn ∈ {1, . . . , K}. The mixture is defined as  N  K  p(w|z, β) =  p(wn|zn, βk)  (1)  where βk is the distribution over the vocabulary for topic k. The novelty, and the reason for the success, of the LDA model is how the topics z and the topic vocabulary β are constructed within the framework. The underpinning intuition is that the topics should present a compact representation with K ≪ N , and that the structure of the topics should be sparse such to achieve a robust and interpretable model. Assuming the topics z to be governed by a multinomial distribution, z ∼ Multi (z|θ), sparsity can be achieved by choosing the parameters θ as governed by a Dirichlet distribution, θ ∼ Dir (θ|α). By the same motivation a Dirichlet prior is placed over the topic- vocabulary distribution β ∼ Dir (β|π). As the Dirichlet is conjugate to the multinomial distribution,  Yn=1  Xk=1  3  the marginal likelihood can be reached analytically by combining the likelihood with the prior and performing the integration,  p(w|α, π) =Z p(θ|α)  N  Yn=1""Xzn  p(zn|θ)p(wn|zn, βzn )# p(β|π)! dθ  (2)  from which the parameters of the model can be learned.  class variable c as a “switch”; p(θ|α, c) = QC  One way of incorporating class information within the LDA framework was suggested in [8] where the use of a class dependent topic distribution was proposed. This was implemented by using the Dir (θ|αj)δcj where δij is the Kronecker delta ∗ through a maximum  function. Using this model the class can be inferred for a new document w likelihood procedure ˆc = arg maxc p(w In this paper we take inspiration from the work presented in [8]. However, we choose to incorporate the class information in a slightly different manner. In specific, we use a factorizing prior over the topic distribution, which firstly encourages sparsity, and secondly introduces a preference for a class conditioned structure, such that separate topics encode within-class variations and between-class variations in the data. Thus, the model we will propose have a stronger class dependency compared to [8]. We will now proceed to describe and motivate the relevance of this class dependency.  ∗|α, π, c) [8].  j=1  3.2 Factorized Topic Model As motivated in Section 1, our idea is to separate the topic space into two parts, where the class- private part explains the class-dependent information (signal) and the shared part explains the class- independent information (structured noise). To achieve this we introduce an additional prior p(θ) to the model presented in [8]. This will encourage a factorized structure such that the K topics can be “softly” split into Kp class-private topics and Ks shared topics where Kp +Ks = K. The advantage of such a structured topic space is that it will be more compact than a regular model; all aspects of the data that correlate with class will be pushed into the class-private part of the topic space. Since the other, class-shared, part of the topic space will then only contain noise, the class of a new document ∗ will in effect be inferred using only the class-private part. Further, in our model, we will use the same sparsity prior α over the topics for all classes. This removes the additional ﬂexibility of allowing a different topic sparsity for each class – which can be relevant in certain special cases — but the gain is a more robust model with fewer free parameters, requiring less training data.  w  In the following, let θclass be the topic distributions of all classes, obtained by marginalizing θ over class. Its rows are defined as θclass m=1 θmδcmc, where cm indicates the class label of the mth document and δij the Kronecker delta function. Examples of θclass distributions can be seen in Figures 3, 4, 5, and 6.  ∝ PM  c  Intuitively, the private topics would concentrate to a certain class in θclass, while the shared top- ics would be more spread among all classes (more uniformly distributed over a column in θclass). Information entropy, widely used in different fields [19, 24], provides a good measurement of this property. In this case, we employ an entropy-like measure H(k) over class for each topic k:  H(k) =  1  log(1/C)  C  Xc=1  θclass c,k ξ=1 θclass ξ,k  log(  θclass c,k ξ=1 θclass ξ,k  PC  PC  )  (3)  c,k  is the element in row c and column k of θclass. H(k) ∈ [0, 1], 0 if all the probability in  where θclass the topic k is concentrated to one class, 1 if all classes are equally probable to contain the topic k. To split the topics into a private and shared part, we wish the prior p(θ|κ) to encourage topics k to either have a low H(k) (be very class-specific) or high H(k) (be very class-unspecific). Hence, we introduce a function as:  A(k) = H(k)2 − H(k) + 1 .  (4)  The prior is defined as:  K  p(θ) ∝  A(k) .  (5)  This prior thus treats each column of θclass independently. With the additional prior (Figure 1(b)), the generative model becomes:  Yk=1  4  p(w|α, π, c) =Z p(θ|α, c)p(θ)  N  Yn=1""Xzn  p(zn|θ)p(wn|zn, β)# p(β|π)! dθ .  (6)  Learning. We use Gibbs sampling for learning the parameters of the model, more speciﬁcally, collapsed Gibbs sampling [10] in the same manner as [12]. The factorizing prior presents itself in the learning as an additional factor in the objective function over z, compared to the original LDA model. It should be noted that the factorizing prior in Equation 5 is independent of the type of learning procedure – the model in Equation 6 can also be trained using, e.g., a variational method.  When training the model, the topics are initialized randomly, which means that they all have a H close to 1. During Gibbs sampling, it would be very unlikely to find a topic with low H, given the bimodality of A in Equation (4). To address this problem, we introduce an “auto-annealing” procedure, where A is replaced with a dynamic cooling function starting off by encouraging low H only, and gradually encouraging high H more and more, as the average H decreases (i.e., when some topics have found a class-specific state). Hence, A is changed to a dynamic function  A(k) = H(k)2 − 2 ˆHH(k) + 1  (7)  where the average H, ˆH = PK  k=1 H(k)/K, is used as an annealing parameter in the function. As with other annealing procedures, the “auto-annealing” procedure means that the factorizing prior p(θ) changes in each step of the iterative learning procedure. In a normal annealing procedure, this change would be actuated by changing the annealing parameter. Here, ˆH can be thought of as an autonomous annealing parameter since it converges automatically to a value reﬂecting the fraction of the class-dependent versus class-independent variation in the data. For example, the text data set (Figure 4) has a lower ˆH than the natural scene dataset (Figure 5).  Segmenting the topic space. When the model have been trained we can evaluate the structure of the learned topic space by computing H(k) for each topic k. We consider topics with low H as class-dependent while topics with high H are considered as independent. As such the topic space can be “softly” segmented and interpreted in a class conditioned manner. As an example, the words building up the shared topics can be considered as stop words. In text processing, there is usually some standard stop words list, which can be used to pre-process the text. However, these stop words are predefined, for example, “the”, “at” etc. However, they sometimes also provide class-relevant information, for example, some topics are more location dependent or have more nouns. On the other hand, there are words, like “learning”, “performance” etc, which do not carry much information in, say, a machine learning conference corpus. In our model, we automatically learn the real stop words for the given domain. Furthermore, while it is easy to predefine the stop words in text data, this problem becomes much more challenging in computer vision applications. The “stop-visual-words” are ill-defined and much less intuitive to find, why an algorithm which automatically learns them, such as the one we propose, is very beneficial. We would like to emphasize again that there is still only one topic space; no hard splitting or removal of topics is done, neither for learning, nor for inference.  4 Experiments The proposed model is evaluated on four different classiﬁcation tasks, and compared to two baselines consisting of a regular LDA model with class label [8], and a model with stronger class-supervision in the topic learning, SLDA [3].  4.1 Object Classification We first demonstrate how the factorization works using a toy dataset. The dataset, shown in Fig- ure 2, is constructed to have a very high degree of structured noise. There are four object classes: bulb, car, duck, and mug. All 8 instances of a certain class have the same shape and image loca- tion. However, there is a very high intra-class variability in foreground and background texture. Furthermore, all four classes contain the exact same foreground-background texture combinations. Thus, the texture (which will dominate the variation among features from any visual extractor) can be regarded as structured noise, while the true signal relates to shape. The properties of this dataset can also be found to some extent in natural images: most realistic image and object classes display large intra-class appearance variation, and different classes share appearance aspects. Furthermore,  5  Figure 2: All the instances in the toy object dataset.  (a) θclass with regular LDA  (b) θclass with SLDA  (c) θclass with factorized LDA  Figure 3: Toy object dataset. (a) Regular LDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity. (b) SLDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity. (c) Factorized LDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity, red line indicating partition between θp and θs.  the backgrounds in natural scenes are often complex and varying, introducing even more variation among training data for a class.  SIFT features on two different scales are densely extracted from all images, and a 64-word vocabu- lary is learned in which all SIFT features are represented. Thus, each image is represented by a bag of visual words in this vocabulary.  The experiment is performed in a hold-one-out manner, where each image in turn is classified using a model trained on the other 31 images. In the following, we will by “regular LDA” mean the regular LDA with upstream supervision presented in [8], but trained using Gibbs sampling in the same way as our model, with the same value of α for all documents. With ”SLDA”, we mean the more strongly supervised LDA variant with downstream supervision presented in [3], implemented by Blei et al.  Our proposed factorized LDA, as well as regular LDA and SLDA, are trained with 15 topics, α = 0.1 and π = 0.2. The classification performance for each class is found by averaging over the performances for the 8 images of that class.  It should be noted that the test image always will have a texture that is different from the training images of that class. However, the same texture can be found in other classes. A classifier that tries to explain all variation in the data in terms of class variation will therefore have difficulties in modeling this data set; a regular LDA or SLDA model trained with this data will be forced to represent texture as well as shape in the same topics, since the Dirichlet prior will promote topic sparsity. Thus, very few topics will purely represent one class, as shown in Figures 3(a) and 3(b).  However, our model, which explicitly factorizes the topics into those private to a certain class and those shared between all classes, will allow the relevant shape variation to be represented separately from the texture variation, which will just confuse the classification in this case. Figure 3(c) shows the factorized topic distribution; it is clear that the topics in θp are private to a certain class, while the noise topics in θs are shared equally over all classes; all the structured noise has thus been pushed into θs. Thus, even though the full topic space is used for classification, it is effectively only based on θp, while the shared topics θs (right of the red line in Figure 3(c)) are effectively disregarded in the classification since they appear with equal probability in all classes.  As expected, the explicit noise model greatly improves classification on this dataset: the factorized LDA reaches 81.25%, while a regular LDA reaches a classification rate of 34.38%, only slightly  6  2  3  4  5  6  7  8  9  : 2; 22 23 24 25 26 27 28 29 2: 3;  2  3  4  5  6  7  8  9  : 2; 22 23 24 25 26 27 28 29 2: 3;  !""# ""$%&’ ’!$( )$!*( *(+’$’,+ -(cid:239)./ ,0*1 +$!&’  !""# ""$%&’ ’!$( )$!*( *(+’$’,+ -(cid:239)./ ,0*1 +$!&’  (a) θclass with regular LDA  (b) θclass with SLDA  (c) θclass with factorized LDA  Figure 4: Reuters 21578 R8 dataset. (a) Regular LDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity. (b) SLDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity. (c) Factorized LDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity, red line indicating partition between θp and θs.  (a) θclass with regular LDA  (b) θclass with SLDA  (c) θclass with factorized LDA  Figure 5: Natural scene dataset. (a) Regular LDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity. (b) SLDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity. (c) Factorized LDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity, red line indicating partition between θp and θs.  above chance, and SLDA who is forced by the stronger supervision to represent all variation (where texture is dominating) in terms of class achieves a result of 0% since the texture of the test image is not present in the training data of the same class.  4.2 Text Classification We now evaluate the proposed model in a realistic text classiﬁcation scenario. We use the stan- dard R8 training and testing set from the Reuters 21578 dataset [26], which contains 5485 training documents and 2189 testing documents. The all-terms version of the data is used since we want to illustrate how our model deals with noise.  The regular LDA, SLDA and factorized LDA models are trained with 20 topics, and parameter settings α = 0.5 and π = 0.1.The topic distributions are shown in Figures 4(a), 4(b), and 4(c). The factorized class-private topic distribution θp (left of the red line in Figure 4(c)) is noticeably cleaner than the regular distribution θclass (Figure 4(a)). In the factorized LDA, only θp contributes to the classification, while the shared topics θs (right of the red line in Figure 4(c)) are effectively disregarded since they appear with equal probability in all classes. The topics of the SLDA model are sparser (Figure 4(b)), but all topics are forced to be class-specific by the stronger supervision.  There is a significant classification improvement using the factorized topic space, from 74.63% with regular LDA and 63.75% with SLDA to 83.91% with factorized LDA.  4.3 Scene Classification We also evaluate the proposed model on a challenging natural scene dataset used in [8]. There are four classes: forest, mountain, open country and coast, with 100 training images and 50 test images per class. From each image, SIFT features on two different scales are densely extracted, and labeled according to a 192-word vocabulary learned from the features, as in [8].  The regular LDA, SLDA, and factorized LDA models are trained with 20 topics, and parameter settings α = 0.5 and π = 0.1.Figures 5(a), 5(b), and 5(c) show the respective topic distributions; notably, the class-specific topic space θp effectively used for classiﬁcation in our factorized LDA only contains 8 topics, while 12 topics (θs) are devoted to modeling structured noise. Thus, the factorized representation is notably sparser than a regular LDA representation, which gives the op- portunity to save both storage space and computation time during classiﬁcation – an important factor to take into account for large datasets.  In addition to rendering a notably sparser data representation, the factorized LDA reaches a marginally higher performance rate than with a regular LDA and SLDA: 84.50% for our model compared to 80.50% for the regular LDA and 84.00% for SLDA. All performances are slightly better than the original implementation of the regular LDA [8], which reaches 76.0%.  7  (a) θclass with regular LDA  (b) θclass with SLDA  (c) θclass with factorized LDA  Figure 6: Action dataset. (a) Regular LDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity. (b) SLDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity. (c) Factorized LDA topic distribution marginalized over class θclass, topics sorted in ascending order of class-specificity, red line indicating partition between θp and θs.  4.4 Action Classification We proceed to evaluate the methods on a dataset with more variation independent of class. The dataset consists of three actions from the KTH Action dataset [15]: boxing, handclapping and hand- waving. There are 100 short video sequences of each action, which show 25 different people per- forming the action, recorded in four shooting conditions (zooming and panning of camera, different background ). The shooting condition has large inﬂuence on the motion in the video, as each zoom or panning motion adds global motion to the video and backgrounds contribute to the motion fea- tures as well. However, the variation in shooting condition is not at all correlated with action class in the dataset. Just as in the toy experiment above (but now in a more realistic setting), a large pro- portion of the data variation is thus independent of the action class. Due to the low signal-to-noise ratio, a topic model without factorization will have difﬁculties capturing the aspects of data relevant for discriminating activity class.  The experiment was performed by separating out from the training data all the 25 images of an action filmed with a certain shooting condition. The topic models were then trained with all other data, and evaluated with the 25 removed images. Hence, the certain combination of action and camera condition in the test data was not present in the training data. This was done for all actions in turn, and the result was averaged over actions.  STIP features [15] were extracted from all sequences and clustered into a vocabulary of 128 spatio- temporal words. This representation was used to train the regular LDA, SLDA and factorized LDA models with 10 topics, α = 0.1 and π = 0.1. Figure 6 shows the topic distributions corresponding to these three models. We can see that Fac- torized LDA is able to model the class-dependent information (left of the red line) and the class- independent information ( right of the red line), which makes it be able to archive better performance in noisy data. For the regular LDA, although the topics are not shared, however, it models all the information and assigned that to different classes with new topics which made the topics themselves became noisy. So does SLDA which models the ”noise” as the useful topics.  Factorized LDA gives an accuracy of 65.22%, which is far better than both regular LDA, 38%, and SLDA, 51.33%. This confirms that the findings of the toy experiment above applies to realistic settings as well. Confusion matrices are shown in Figures 6(a), 6(b), and 6(c) respectively.  5 Conclusions  We present a factorized latent topic model, which explicitly represents aspects of the data which are not correlated with model state. Specifically, we train an LDA class model with an additional factorizing prior, which encourages topics to either be very class-specific or evenly shared among classes. The topic space θ is thus partitioned into one part θp whose topics are private to certain classes, and another part θs with topics shared between classes. Only θp contributes effectively to classification.  Experiments show the factorized LDA model to give consistently better classiﬁcation performance and sparser topic representations than both a regular LDA model [8] and SLDA [3]. Sparse repre- sentations are advantageous for large datasets since they save storage space and computation time during classiﬁcation.  Future work includes investigating the effect of this factorization prior on other topic models, such as HDP, and to integrate the prior into models with multiple data views, such as in [12, 28, 30].  8  References  [1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical  report, 2005.  [2] D. M. Blei. Probabilistic topic models. Communications of the ACM, 55(4):77–84, 2012. [3] D. M. Blei and J. D. McAuliffe. Supervised topic models, arxiv:1003.0783, 2010. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning  Research, 3:993–1022, 2003.  [5] A. Damianou, C. H. Ek, M. Titsias, and N. D. Lawrence. Manifold Relevance Determination. In ICML,  2012.  [6] C. H. Ek. Shared Gaussian Process Latent Variable Models, PhD Thesis, 2009. [7] C. H. Ek, J. Rihan, P. H. S. Torr, G. Rogez, and N. D. Lawrence. Ambiguity modeling in latent spaces.  Machine Learning for Multimodal Interaction, 2008.  [8] L. Fei-Fei and P. Perona. A Bayesian hierarchical model for learning natural scene categories. In CVPR,  2005.  [9] M. R. Gormley, M. Dredze, B. Van Durme, and J. Eisner. Shared components topic models. In NAACL,  2012.  [10] G. Heinrich. Parameter estimation for text analysis. Technical report, 2005. [11] T. Hofmann. Probabilistic latent semantic analysis. In UAI, 1999. [12] Y. Jia, M. Salzmann, and T. Darrell. Learning cross-modality similarity for multinomial data. In ICCV,  2011.  [13] A. Klami and S. Kaski. Generative models that discover dependencies between data sets.  Workshop on Machine Learning for Signal Processing, 2006.  In IEEE  [14] A. Klami and S. Kaski. Probabilistic approach to detecting dependencies between data sets. Neurocom-  puting, 72(1-3):39–46, 2008.  [15] I. Laptev. On space-time interest points. IJCV, 64(2/3):107–123, 2005. [16] N.D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent vari-  able models. Journal of Machine Learning Research, 6:1783–1816, 2005.  [17] G. Leen and C. Fyfe. A Gaussian process latent variable model formulation of canonical correlation  analysis. In European Symposium on Artiﬁcial Neural Networks, 2006.  [18] G. Leen and C. Fyfe. Learning shared and separate features from two related data sets using GPLVM’s.  In Learning from Multiple Sources Workshop, Neural Information Processing, 2008.  [19] B. Leibe, A. Leonardis, and B. Schiele. Robust object detection with interleaved categorization and  segmentation. IJCV, 77(1):259–289, 2008.  [20] D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004. [21] R. Navaratnam, A. W. Fitzgibbon, and R. Cipolla. The joint manifold model for semi-supervised multi-  valued regression. In ICCV, 2007.  [22] J. Paisley, C. Wang, and D. Blei. The discrete inﬁnite logistic normal distribution. arXiv preprint  arXiv:1103.4789, 2011.  [23] C. H. Papadimitriou, P. Raghavan, and H. Tamaki. Latent semantic indexing: a probabilistic analysis.  Journal of Computer and System Sciences, (61):217–235, 2000.  [24] J. P.W. Pluim, J.B. A. Maintz, and M. A. Viergever. Interpolation artefacts in mutual information-based  image registration. CVIU, 77(2):1077–3142, 2000.  [25] N. Rasiwasia, J. Pereira, E. Coviello, G. Doyle, G. Lanckriet, R. Levy, and N. Vasconcelos. A new  approach to cross-modal multimedia retrieval. In MMM, 2010.  [26] Reuters-21578. http://csmining.org/index.php/r52-and-r8-of-reuters-21578.html. [27] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal  Statistical Society: Series B (Statistical Methodology), 61(3):611–622, 1999.  [28] S. Virtanen, Y. Jia, A. Klami, and T. Darrell. Factorized multi-modal topic model. In UAI, 2012. [29] C. Wang and D. M. Blei. Decoupling sparsity and smoothness in the discrete hierarchical dirichlet process.  In NIPS, 2009.  [30] C. Wang, D. M. Blei, and L. Fei-Fei. Simultaneous image classiﬁcation and annotation. In CVPR, 2009.  9  ","In this paper we present a modification to a latent topic model, which makesthe model exploit supervision to produce a factorized representation of theobserved data. The structured parameterization separately encodes variance thatis shared between classes from variance that is private to each class by theintroduction of a new prior over the topic space. The approach allows for amore eff{}icient inference and provides an intuitive interpretation of the datain terms of an informative signal together with structured noise. Thefactorized representation is shown to enhance inference performance for image,text, and video classification."
1301.3476,2013,Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities  ,"['Tommi Vatanen', 'Tapani Raiko', 'Harri Valpola', 'Yann LeCun']",https://arxiv.org/pdf/1301.3476.pdf,"3 1 0 2    r a     M 1 1      ]  G L . s c [      3 v 6 7 4 3  .  1 0 3 1 : v i X r a  Pushing Stochastic Gradient towards Second-Order  Methods – Backpropagation Learning with  Transformations in Nonlinearities  Tommi Vatanen, Tapani Raiko, Harri Valpola Department of Information and Computer Science  Aalto University School of Science  P.O.Box 15400, FI-00076, Aalto, Espoo, Finland  first.last@aalto.fi  Yann LeCun  New York University  715 Broadway, New York, NY 10003, USA  firstname@cs.nyu.edu  Abstract  Recently, we proposed to transform the outputs of each hidden neuron in a multi- layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We con- tinue the work by ﬁrstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.  1  Introduction  Learning deep neural networks has become a popular topic since the invention of unsupervised pretraining [4]. Some later works have returned to traditional back-propagation learning in deep models and noticed that it can also provide impressive results [6] given either a sophisticated learning algorithm [9] or simply enough computational power [2]. In this work we study back-propagation learning in deep networks with up to ﬁve hidden layers, continuing on our earlier results in [10]. In learning multi-layer perceptron (MLP) networks by back-propagation, there are known transfor- mations that speed up learning [8, 11, 12]. For instance, inputs are recommended to be centered to zero mean (or even whitened), and nonlinear functions are proposed to have a range from -1 to 1 rather than 0 to 1 [8]. Schraudolph [12, 11] proposed centering all factors in the gradient to have zero mean, and further adding linear shortcut connections that bypass the nonlinear layer. Gradient factor centering changes the gradient as if the nonlinear activation functions had zero mean and zero slope on average. As such, it does not change the model itself. It is assumed that the discrepancy between the model and the gradient is not an issue, since the errors will be easily compensated by the linear shortcut connections in the proceeding updates. Gradient factor centering leads to a signiﬁcant speed-up in learning.  1  In this paper, we transform the nonlinear activation functions in the hidden neurons such that they have on average 1) zero mean, 2) zero slope, and 3) unit variance. Our earlier results in [10] in- cluded the ﬁrst two transformations and here we introduce the third one. cdsaasd We explain the usefulness of these transformations by studying the Fisher information matrix and the Hessian, e.g. by measuring the angle between the traditional gradient and a second order update direction with and without the transformations. It is well known that second-order optimization methods such as the natural gradient [1] or New- ton’s method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with high-dimensional models due to heavy computations with large matrices. In practice, it is possible to use a diagonal or block-diagonal approximation [7] of the Fisher information matrix or the Hessian. Gradient descent can be seen as an approximation of the second-order methods, where the matrix is approximated by a scalar constant times a unit matrix. Our transformations aim at making the Fisher information matrix as close to such matrix as possible, thus diminishing the difference between ﬁrst and second order methods. Matlab code for replicating the experiments in this paper is available at  https://github.com/tvatanen/ltmlp-neuralnet  2 Proposed Transformations  Let us study a MLP-network with a single hidden layer and shortcut mapping, that is, the output column vectors yt for each sample t are modeled as a function of the input column vectors xt with (1)  yt = Af (Bxt) + Cxt + (cid:15)t,  where f is a nonlinearity (such as tanh) applied to each component of the argument vector sepa- rately, A, B, and C are the weight matrices, and (cid:15)t is the noise which is assumed to be zero mean  (cid:1). In order to avoid separate bias vectors that complicate  and Gaussian, that is, p((cid:15)it) = N(cid:0)(cid:15)it; 0, σ2  formulas, the input vectors are assumed to have been supplemented with an additional component that is always one. Let us supplement the tanh nonlinearity with auxiliary scalar variables αi, βi, and γi for each nonlinearity fi. They are updated before each gradient evaluation in order to help learning of the other parameters A, B , and C. We deﬁne  i  where bi is the ith row vector of matrix B. We will ensure that  fi(bixt) = γi [tanh(bixt) + αibixt + βi] ,  T(cid:88) T(cid:88) (cid:35)(cid:34) T(cid:88)  t=1  t=1  fi(bixt) = 0  (cid:34) T(cid:88)  t=1  fi(bixt)2  T  f(cid:48) i (bixt) = 0  (cid:35)  f(cid:48) i (bixt)2  = 1  T  t=1  by setting αi, βi, and γi to  t=1  T(cid:88) T(cid:88) T(cid:88)  t=1  T  t=1  αi = − 1 T  βi = − 1 T  (cid:110) 1  γi =  (cid:48) tanh  (bixt)  [tanh(bixt) + αibixt]  [tanh(bixt) + αibixt + βi]2(cid:111)1/4(cid:110) 1  T  (cid:2)tanh  (cid:48)  T(cid:88)  t=1  (cid:3)2(cid:111)1/4  (bixt) + αi  2  (2)  (3)  (4)  (5)  (6)  (7)  .  (8)  One way to motivate the ﬁrst two transformations in Equations (3) and (4), is to study the expected output yt and its dependency of the input xt: 1 T  f (Bxt) + C  (cid:88)  yt = A  1 T  1 T  (9)  xt  (cid:35)  t  (cid:88) (cid:88)  t  1 T  ∂yt ∂xt  t  = A  (cid:88) (cid:88)  t  t  1 T  (cid:34)  f(cid:48)(Bxt)  BT + C.  (10)  We note that by making nonlinear activations f (·) zero mean in Eq. (3), we disallow the nonlinear mapping Af (B·) to affect the expected output yt, that is, to compete with the bias term. Similarly, by making the nonlinear activations f (·) zero slope in Eq. (4), we disallow the nonlinear mapping Af (B·) to affect the expected dependency of the input, that is, to compete with the linear mapping C. In traditional neural networks, the linear dependencies (expected ∂yt/∂xt) are modeled by many competing paths from an input to an output (e.g. via each hidden unit), whereas our architecture gathers the linear dependencies to be modeled only by C. We argue that less competition between parts of the model will speed up learning. Another explanation for choosing these transformations is that they make the nondiagonal parts of the Fisher information matrix closer to zero (see Section 3). The goal of Equation (5) is to normalize both the output signals (similarly as data is often normalized as a preprocessing step – see,e.g., [8]) and the slopes of the output signals of each hidden unit at the same time. This is motivated by observing that the diagonal of the Fisher information matrix contains elements with both the signals and their slopes. By these normalizations, we aim pushing these diagonal elements more similar to each other. As we cannot normalize both the signals and the slopes to unity at the same time, we normalize their geometric mean to unity. The effect of the ﬁrst two transformations can be compensated exactly by updating the shortcut mapping C by  Cnew = Cold − A(αnew − αold)B  (11) where α is a matrix with elements αi on the diagonal and one empty row below for the bias term, and β is a column vector with components βi and one zero below for the bias term. The third transformation can be compensated by  − A(βnew − βold) [0 0 . . . 1] ,  Anew = Aoldγoldγ−1 new, where γ is a diagonal matrix with γi as the diagonal elements. Schraudolph [12, 11] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar effect on the gradient. Equation (3) corresponds to Schraudolph’s activity centering and Equation (4) corresponds to slope centering.  (12)  3 Theoretical Comparison to a Second-Order Method  Second-order optimization methods, such as the natural gradient [1] or Newton’s method, decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix multiplied by the inverse learning rate. We will show how the ﬁrst two proposed transformations move the non-diagonal elements of the Fisher information matrix closer to zero, and the third transformation makes the diagonal elements more similar in scale, thus making the basic gradient behave closer to the natural gradient. The Fisher information matrix contains elements  (cid:88)  (cid:28) ∂2 log p(yt | xt, A, B, C)  (cid:29)  ,  (13)  Gij =  t  ∂θi∂θj  3  where (cid:104)·(cid:105) is the expectation over the Gaussian distribution of noise (cid:15)t in Equation (1), and vector θ contains all the elements of matrices A, B, and C. Note that here yt is a random variable and thus the Fisher information does not depend on the output data. The Hessian matrix is closely related to the Fisher information, but it does depend on the output data and contains more terms, and therefore we show the analysis on the simpler Fisher information matrix. The elements in the Fisher information matrix are:  (cid:26) 0  ∂  ∂  ∂aij  ∂ai(cid:48)j(cid:48)  log p =  − 1  σ2 i  t fj(bjxt)fj(cid:48)(bj(cid:48)xt)  i(cid:48) (cid:54)= i i(cid:48) = i,  where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly  log p = −(cid:88)  1 σ2 i  i  ∂  ∂  ∂bjk  ∂bj(cid:48)k(cid:48)  and  The cross terms are  ∂  ∂  ∂cik  ∂ci(cid:48)k(cid:48)  log p =  j(bjxt)f(cid:48) f(cid:48)  j(cid:48)(bj(cid:48)xt)xktxk(cid:48)t  i(cid:48) (cid:54)= i i(cid:48) = i.  t xktxk(cid:48)t  ∂  ∂  ∂aij  ∂bj(cid:48)k  ∂  ∂  ∂cik ∂  ∂ai(cid:48)j  ∂  ∂cik  ∂bjk(cid:48)  log p = − 1 σ2 i  (cid:26) 0  log p =  log p = − 1 σ2 i  fj(bjxt)f(cid:48)  j(cid:48)(bj(cid:48)xt)xkt  i(cid:48) (cid:54)= i i(cid:48) = i  t fj(bjxt)xkt f(cid:48) j(bjxt)xktxk(cid:48)t.  t  (cid:80) aijaij(cid:48)(cid:88) (cid:26) 0 (cid:80) aij(cid:48)(cid:88) (cid:80) (cid:88)  − 1  σ2 i  σ2 i  t  − 1  aij  t  (14)  (15)  (16)  (17)  (18)  (19)  j(·), and xit. We ar- Now we can notice that Equations (14–19) contain factors such as fj(·), f(cid:48) gue that by making the factors as close to zero as possible, we help in making nondiagonal ele- ments of the Fisher information closer to zero. For instance, E[fj(·)fj(cid:48)(·)] = E[fj(·)]E[fj(cid:48)(·)] + Cov[fj(·), fj(cid:48)(·)], so assuming that the hidden units j and j(cid:48) are representing different things, that is, fj(·) and fj(cid:48)(·) are uncorrelated, the nondiagonal element of the Fisher information in Equation (14) becomes exactly zero by using the transformations. When the units are not completely uncorrelated, the element in question will be only approximately zero. The same argument applies to all other elements in Equations (15–19), some of them also highlighting the beneﬁt of making the input data xt zero-mean. Naturally, it is unrealistic to assume that inputs xt, nonlinear activations f (·), and their slopes f(cid:48)(·) are all uncorrelated, so the goodness of this approximation is empirically evaluated in the next section. The diagonal elements of the Fisher can be found in Equations (14–16) when i = i(cid:48), j = j(cid:48), and k = k(cid:48). There we ﬁnd f (·)2 and f(cid:48)(·)2 that we aim to keep similar in scale by using the third transformation in Equation (5).  4 Empirical Comparison to a Second-Order Method  Here we investigate how linear transformations affect the gradient by comparing it to a second-order method, namely Newton’s algorithm with a simple regularization to make the Hessian invertible. We compute an approximation of the Hessian matrix using ﬁnite difference method, in which case k-th row vector hk of the Hessian matrix H is given by  hk =  ∂(∇E(θ))  ∂θk  ≈ ∇E(θ + δφk) − ∇E(θ − δφk)  2δ  ,  (20)  where φk = (0, 0, . . . , 1, . . . , 0) is a vector of zeros and 1 at the k-th position, and the error func- t log p(yt | xt, θ). The resulting Hessian might still contain some very small or  tion E(θ) = −(cid:80)  4  (a) Eigenvalues  (b) Angles  Figure 1: Comparison of (a) distributions of the eigenvalues of Hessians (2600 × 2600 matrix) and (b) angles compared to the second-order update directions using LTMLP and regular MLP. In (a), the eigenvalues are distributed most evenly when using LTMLP. (b) shows that gradients of the transformed networks point to the directions closer to the second-order update.  even negative eigenvalues which cause its inversion to blow up. Therefore we do not use the Hes- sian directly, but include a regularization term similarly as in the Levenberg-Marquardt algorithm, resulting in a second-order update direction  ∆θ = (H + µI)−1∇E(θ),  (21) where I denotes the unit matrix. Basically, Equation (21) combines the steepest descent and the second-order update rule in such a way, that when µ gets small, the update direction approaches the Newton’s method and vice versa. Computing the Hessian is computationally demanding and therefore we have to limit the size of the network used in the experiment. We study the MNIST handwritten digit classiﬁcation problem where the dimensionality of the input data has been reduced to 30 using PCA with a random rotation [10]. We use a network with two hidden layers with architecture 30–25–20–10. The network was trained using the standard gradient descent with weight decay regularization. Details of the training are given in the appendix. In what follows, networks with all three transformations (LTMLP, linearly transformed multi-layer perceptron network), with two transformations (no-gamma where all γi are ﬁxed to unity) and a network with no transformations (regular, where we ﬁx αi = 0, βi = 0, and γi = 1) were compared. The Hessian matrix was approximated according to Equation (20) 10 times in regular intervals during the training of networks. All ﬁgures are shown using the approximation after 4000 epochs of training, which roughly corresponds to the midpoint of learning. However, the results were parallel to the reported ones all along the training. We studied the eigenvalues of the Hessian matrix (2600×2600) and the angles between the methods compared and second-order update direction. The distribution of eigenvalues in Figure 1a for the networks with transformations are more even compared to the regular MLP. Furthermore, there are fewer negative eigenvalues, which are not shown in the plot, in the transformed networks. In Figure 1b, the angles between the gradient and the second-order update direction are compared as a function of µ in Equation (21). The plots are cut when H + µI ceases to be positive deﬁnite as µ decreases. Curiously, the update directions are closer to the second-order method, when γ is left out, suggesting that γs are not necessarily useful in this respect. Figure 2 shows histograms of the diagonal elements of the Hessian after 4000 epochs of training. All the distributions are bimodal, but the distributions are closer to unimodal when transformations are used (subﬁgures (a) and (b))1. Furthermore, the variance of the diagonal elements in log-scale is smaller when using LTMLP, σ2 c = 1.43. This suggests that when transformations are used, the second-order update rule in Equation (21) corrects different elements of the gradient vector more evenly compared to a regular back- propagation learning, implying that the gradient vector is closer to the second-order update direction when using all the transformations.  a = 0.90, compared to the other two, σ2  b = 1.71 and σ2  1It can be also argued whether (a) is more unimodal compared to (b).  5  50015002500−6−4−20Eigenvalue orderLog10 eigenvalue  LTMLPno−gammaregular0.010.030.10.5020406080100µAngle / degrees  LTMLPno−gammaregular(a) LTMLP  (b) no-gamma  (c) regular  Figure 2: Comparison of distributions of the diagonal elements of Hessians. Coloring according to legend in (c) shows which layers to corresponding weights connect (1 = input, 4 = output). Diagonal elements are most concentrated in LTMLP and most spread in the regular MLP network. Notice the logarithmic x-axis.  To conclude this section, there is no clear evidence in way or another whether the addition of γ ben- eﬁts the back-propagation learning with only α and β. However, there are some differences between these two approaches. In any case, it seems clear that transforming the nonlinearities beneﬁts the learning compared to the standard back-propagation learning.  5 Experiments: MNIST Classiﬁcation  We use the proposed transformations for training MLP networks for MNIST classiﬁcation task. Experiments are conducted without pretraining, weight-sharing, enhancements of the training set or any other known tricks to boost the performance. No weight decay is used and as only regularization we add Gaussian noise with σ = 0.3 to the training data. Networks with two and three hidden layers with architechtures 784–800–800–10 (solid lines) and 784–400–400–400–10 (dashed lines) are used. Details are given in the appendix. Figure 3 shows the results as number of errors in classifying the test set of 10 000 samples. The results of the regular back-propagation without transformations, shown in blue, are well in line with previously published result for this task. When networks with same architecture are trained using the proposed transformations, the results are improved signiﬁcantly. However, adding γ in addition to previously proposed α and β does not seem to affect results on this data set. The best results, 112 errors, is obtained by the smaller architecture without γ and for the three-layer architecture with γ the result is 114 errors. The learning seems to converge faster, especially in the three-layer case, with γ. The results are in line what was obtained in [10] where the networks were regularized more thoroughly. These results show that it is possible to obtain results comparable to dropout networks (see [5]) using only minimal regularization.  6 Experiments: MNIST Autoencoder  Previously, we have studied an auto-encoder network using two transformations, α and β, in [10]. Now we use the same auto-encoder architecture, 784–500–250–30–250–500–784. Adding the third transformation γ for training the auto-encoder poses problems. Many hidden neurons in decoding layers (i.e., 4th and 5th hidden layers) tend to be relatively inactive in the beginning of training, which induces corresponding γs to obtain very large values. In our experiments, auto-encoder with γs eventually diverge despite simple constraint we experimented with, such as γi ≤ 100. This be- havior is illustrated in Figure 4. The subﬁgure (a) shows the distribution of variances of outputs of all hidden neurons in MNIST classiﬁcation network used in Section 5 given the MNIST training data. The corresponding distribution for hidden neurons in the decoder part of the auto-encoder is shown in the subﬁgure (b). The “dead neurons“ can be seen as a peak in the origin. The corresponding γs, constrained γi ≤ 100, can be seen in the subﬁgure (c). We hypothesize that this behavior is due to the fact, that in the beginning of the learning there is not much information reaching the bottleneck layer through the encoder part and thus there is nothing to learn for the decoding neurons. According to our tentative experiments, the problem described above may be overcome by disabling γs in the  6  10−410−205010015020010−410−210005010015020010−410−2050100150200250  1−21−32−31−42−43−4Figure 3: The error rate on the MNIST test set for LTMLP training, LTMLP without γ and regular back-propagation. The solid lines show results for networks with two hidden layers of 800 neurons and the dashed lines for networks with three hidden layers of 400 neurons.  (a) MNIST classiﬁcation  (b) MNIST auto-encoder  (c) MNIST auto-encoder  Figure 4: Histograms of (a-b) variation of output of hidden neurons given the MNIST training data and (c) γs of the decoder part (4th and 5th hidden layer) in the MNIST auto-encoder. (a) shows a healthy distributions of variances, whereas in (b), which includes only variances of the decoder part, there are many “dead neurons”. These neurons induce corresponding γs, histogram of which is shown in (c), to blow up which eventually lead to divergence.  decoder network (i.e., ﬁx γ = 1). However, this does not seem to speed up the learning compared to our earlier results with only two transformations in [10]. It is also be possible to experiment with weight-sharing or other constraints to overcome the difﬁculties with γs.  7 Discussion and Conclusions  We have shown that introducing linear transformation in nonlinearities signiﬁcantly improves the back-propagation learning in (deep) MLP networks. In addition to two transformation proposed earlier in [10], we propose adding a third transformation in order to push the Fisher information matrix closer to unit matrix (apart from its scale). The hypothesis proposed in [10], that the transfor- mations actually mimic a second-order update rule, was conﬁrmed by experiments comparing the networks with transformations and regular MLP network to a second-order update method. How- ever, in order to ﬁnd out whether the third transformation, γ, we proposed in this paper, is really useful, more experiments ought to be conducted. It might be useful to design experiments where convergence is usually very slow, thus revealing possible differences between the methods. As hy- perparameter selection and regularization are usually nuisance in practical use of neural networks, it would be interesting to see whether combining dropouts [5] and our transformations can provide a robust framework enabling training of robust neural networks in reasonable time.  7  050100150100120140160180200220240EpochsNumber of errors  LTMLPno−gammaregular0123050100150200250Variance00.511.52050100150200Variance0501000100200300γThe effect of the ﬁrst two transformations is very similar to gradient factor centering [12, 11], but transforming the model instead of the gradient makes it easier to generalize to other contexts: When learning by by MCMC, variational Bayes, or by genetic algorithms, one would not compute the basic gradient at all. For instance, consider using the Metropolis algorithm on the weight matrices, and expecially matrices A and B. Without transformations, the proposed jumps would affect the expected output yt and the expected linear dependency ∂yt/∂xt in Eqs. (9)–(10), thus often leading to low acceptance probability and poor mixing. With the proposed transformations included, longer proposed jumps in A and B could be accepted, thus mixing the nonlinear part of the mapping faster. For further discussion, see [10], Section 6. The implications of the proposed transformations in these other contexts are left as future work.  References [1] S. Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10(2):251–276,  1998.  [2] D. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber. Deep big simple neural nets  excel on handwritten digit recognition. CoRR, abs/1003.0358, 2010.  [3] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 249–256, 2010.  [4] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural net-  works. Science, 313(5786):504–507, 2006.  [5] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut- dinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.  [6] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional  neural networks. 2012.  [7] N. Le Roux, P. A. Manzagol, and Y. Bengio. Topmoumoute online natural gradient algorithm.  In Advances in Neural Information Processing Systems 20 (NIPS*2007), 2008.  [8] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁcient backprop. In Neural Networks:  tricks of the trade. Springer-Verlag, 1998.  [9] J. Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th Interna-  tional Conference on Machine Learning (ICML), 2010.  [10] Tapani Raiko, Harri Valpola, and Yann LeCun. Deep learning made easier by linear transfor- mations in perceptrons. Journal of Machine Learning Research - Proceedings Track, 22:924– 932, 2012.  [11] N. N. Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical  Report IDSIA-33-98, Istituto Dalle Molle di Studi sull’Intelligenza Artiﬁciale, 1998.  [12] N. N. Schraudolph. Centering neural network gradient factors. In Genevieve Orr and Klaus- Robert Mller, editors, Neural Networks: Tricks of the Trade, volume 1524 of Lecture Notes in Computer Science, pages 548–548. Springer Berlin / Heidelberg, 1998.  Appendix  Details of Section 4  In experiments of Section 4, networks with all three transformations (LTMLP), only α and β (no- gamma) and network with no transformations (regular) were compared. Full batch training without momentum was used to make things as simple as possible. The networks were regularized using weight decay and adding Gaussian noise to the training data. Three hyperparameters, weight de- cay term, input noise variance and learning rate, were validated for all networks separately. The input data was normalized to zero mean and the network was initialized as proposed in [3], that is, nj + nj+1, where nj is the  the weights were drawn from a uniform distribution between ±√  √  6/  number of neurons on the jth layer.  8  (a) Training error  (b) Test error  Figure 5: Comparison of (a) training and (b) test errors of the algorithms using the MNIST data in the experiment comparing them to the second-order method. Note how the best learning for the regular MLP is relatively high, leading to oscillations until it is annealed towards the end.  We sampled the three hyperparameters randomly (given our best guess intervals) for 500 runs and selected the median of the runs that resulted in the best 50 validation errors as the hyperparameters. Resulting hyperparameters are listed in Table 1. Notable differences occur in step sizes, as it seems that networks with transformations allow using signiﬁcantly larger step size which in turn results in more complete search in the weight space. Our weight updates are given by  (22)  (23)  where the learning rate on iteration τ, ετ , is given by  θτ = θτ−1 − ετ∇θτ .  (cid:26) ε0  ετ =  2(1 − τ  T )ε0  τ ≤ T /2 τ > T /2  (cid:0) 1  that is, the learning rate starts decreasing linearly after the midpoint of the given training time T . Furthermore, the learning rate ετ is dampened for shortcut connection weights by multiplying with  (cid:1)s, where s is number of skipped layers as proposed in [10].2 Figure 5 shows training and test  2 errors for the networks. The LTMLP obtains the best results although there is no big difference compared to training without γ.  Details of Section 5 The MNIST dataset consists of 28 × 28 images of hand-drawn digits. There are 60 000 training samples and 10 000 test samples. We experimented with two networks with two and three hidden layers and number of hidden neurons by arbitrary choice. Training was done in minibatch mode with 1000 samples in each batch and transformations are updated on every iteration using the current minibatch with using (6)–(8). This seems to speed up learning compared to the approach in [10] where transformations were updated only occasionally with the full training data. Random Gaussian noise with σ = 0.3 was injected to the training data in the beginning of each epoch.  2This heuristic is not well supported by analysis of Figure 2 and could be re-examined.  Table 1: Hyperparameters for the neural networks  weight decay  noise  step size  LTMLP 4.6 × 10−5  0.31 1.2  no-gamma 1.3 × 10−5  0.36 2.5  regular 3.9 × 10−5  0.29 0.45  9  204060801000246810Training error / %Training time / %  LTMLPno−gammaregular204060801000246810Test error / %Training time / %  LTMLPno−gammaregularOur weight update equations are given by:  where  ετ =  pτ =  ∆θτ = ∇θ + pτ ∆θt−1, θτ = θτ−1 − ετ ∆θτ ,  (cid:26) ε0 (cid:26) τ ε0f τ−T T pf + (1 − τ pf  τ ≤ T τ > T  T )p0  τ ≤ T τ > T  (24) (25)  (26)  (27)  In the equations above, T is a “burn-in time” where momentum pτ is increased from starting value p0 = 0.5 to pf = 0.9 and learning rate ε = ε0 is kept constant. When τ > T momentum is kept constant and learning rate starts decreasing exponentially with f = 0.9. Hyperparameters were not validated but chosen by arbitrary guess such that learning did not diverge. For the regular training, ε0 = 0.05 was selected since it diverged with higher learning rates. Then according to lessons learned, e.g. in Section 4, ε0 = 0.3 was set for LTMLP with γ and ε0 = 0.7 for the variant with no γ. Basically, it seems that transformations allow using higher learning rates and thus enable faster convergence.  10  ","Recently, we proposed to transform the outputs of each hidden neuron in amulti-layer perceptron network to have zero output and zero slope on average,and use separate shortcut connections to model the linear dependencies instead.We continue the work by firstly introducing a third transformation to normalizethe scale of the outputs of each hidden neuron, and secondly by analyzing theconnections to second order optimization methods. We show that thetransformations make a simple stochastic gradient behave closer to second-orderoptimization methods and thus speed up learning. This is shown both in theoryand with experiments. The experiments on the third transformation show thatwhile it further increases the speed of learning, it can also hurt performanceby converging to a worse local optimum, where both the inputs and outputs ofmany hidden neurons are close to zero."
1301.3468,2013,Boltzmann Machines and Denoising Autoencoders for Image Denoising  ,['KyungHyun Cho'],https://arxiv.org/pdf/1301.3468.pdf,"3 1 0 2    r a  M 4         ] L M  . t a t s [      6 v 8 6 4 3  .  1 0 3 1 : v i X r a  Boltzmann Machines and Denoising Autoencoders  for Image Denoising  KyungHyun Cho  Aalto University School of Science  Department of Information and Computer Science  Espoo, Finland  kyunghyun.cho@aalto.fi  Abstract  Image denoising based on a probabilistic model of local image patches has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good model for this. In this paper, we propose that another popular family of models in the ﬁeld of deep learning, called Boltzmann machines, can perform image denoising as well as, or in certain cases of high level of noise, better than denoising autoencoders. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments conﬁrmed our claim and revealed that the performance can be improved by adding more hidden layers, especially when the level of noise is high.  1 Introduction  Numerous approaches based on machine learning have been proposed for image denoising tasks over time. A dominant approach has been to perform denoising based on local statistics over a whole image. For instance, a method denoises each small image patch extracted from a whole noisy image and reconstructs the clean image from the denoised patches. Under this approach, it is possible to use raw pixels of each image patch [see, e.g. Hyv¨arinen et al., 1999] or the representations in another domain, for instance, in wavelet domain [see, e.g. Portilla et al., 2003].  In the case of using raw pixels, sparse coding has been a method of choice. Hyv¨arinen et al. [1999] proposed to use independent component analysis (ICA) to estimate a dictionary of sparse elements and compute the sparse code of image patches. Subsequently, a shrinkage nonlinear function is applied to the estimated sparse code elements to suppress those elements with small absolute mag- nitude. This sparse code elements are used to reconstruct a noise-free image patch. More recently, Elad and Aharon [2006] also showed that sparse overcomplete representation is useful in denoising images. Some researchers claimed that better denoising performance can be achieved by using a variant of sparse coding methods [see, e.g. Shang and Huang, 2005, Lu et al., 2011].  In essence, these approaches build a probabilistic model of natural image patches using a layer of sparse latent variables. The posterior distribution of each noisy patch is either exactly computed or estimated, and the noise-free patch is reconstructed as an expectation of a conditional distribution over the posterior distribution.  Based on this interpretation, some researchers have proposed very recently to utilize a (probabilis- tic) model that has more than one layers of latent variables for image denoising. Burger et al. [2012] showed that a deep multi-layer perceptron that learns a mapping from a noisy image patch to its corresponding clean version, can perform as good as the state-of-the-art denoising methods. Simi- larly, Xie et al. [2012] proposed a variant of a stacked denoising autoencoder [Vincent et al., 2010]  1  that is more effective in image denoising. They were also able to show that the denoising approach based on deep neural networks performed as good as, or sometimes better than, the conventional state-of-the-art methods.  Along this line of research, we aim to propose a yet another type of deep neural net- works for image denoising, in this paper. A Gaussian-Bernoulli restricted Boltzmann ma- chines (GRBM) [Hinton and Salakhutdinov, 2006] and deep Boltzmann machines (GDBM) [Salakhutdinov and Hinton, 2009, Cho et al., 2011b] are empirically shown to perform well in im- age denoising, compared to stacked denoising autoencoders. Furthermore, we extensively evalu- ate the effect of the number of hidden layers of both Boltzmann machine-based deep models and autoencoder-based ones. The empirical evaluation is conducted using different noise types and lev- els on three different sets of images.  2 Deep Neural Networks  We start by brieﬂy describing Boltzmann machines and denoising autoencoders which have become increasingly popular in the ﬁeld of machine learning.  2.1 Boltzmann Machines  Originally proposed in 1980s, a Boltzmann machine (BM) [Ackley et al., 1985] and especially its structural constrained version, a restricted Boltzmann machine (RBM) [Smolensky, 1986] have be- come increasingly important in machine learning since [Hinton and Salakhutdinov, 2006] showed that a powerful deep neural network can be trained easily by stacking RBMs on top of each other. More recently, another variant of a BM, called a deep Boltzmann machine (DBM), has been pro- posed and shown to outperform other conventional machine learning methods in many tasks [see, e.g. Salakhutdinov and Hinton, 2009].  We ﬁrst describe a Gaussian-Bernoulli DBM (GDBM) that has L layers of binary hidden units and a single layer of Gaussian visible units. A GDBM is deﬁned by its energy function  −E(v, h | θ) =Xi  −  (vi − bi)2  2σ2  +Xi,j  vi σ2  h(1)  j wi,j +Xj j,k  ,  j h(l+1) h(l)  k  u(l)  j c(1) h(1)  j +  (1)  L  Xl=2  j c(l) h(l)   j +Xj,k Xj and h(l) = hh(l)  are Nv Gaussian visible units and Nl binary where v = [vi]i=1...Nv hidden units in the l-th hidden layer. W = [wi,j ] is the set of weights between the visible neurons  j ij=1...Nl and the ﬁrst layer hidden neurons, while U(l) = hu(l)  j,ki is the set of weights between the l-th and  l + 1-th hidden neurons. σ2 is the shared variance of the conditional distribution of vi given the hidden units.  With the energy function, a GDBM can assign a probability to each state vector x = [v; h(1); · · · ; h(L)] using a Boltzmann distribution:  p(x | θ) =  1  Z(θ)  exp {−E(x | θ)} .  Based on this property the parameters can be learned by maximizing the log-likelihood  L = PN (cid:2)h(1); · · · ; h(L)(cid:3).  n=1 logPh p(v(n), h | θ) given N training samples {v(n)}n=1,...,N , where h =  Although the update rules based on the gradients of the log-likelihood function are well de- ﬁned, it is intractable to exactly compute them. Hence, an approach that uses variational approximation together with Markov chain Monte Carlo (MCMC) sampling was proposed by Salakhutdinov and Hinton [2009].  2  It has, however, been found that training a GDBM using this approach starting from randomly initial- ized parameters is not trivial [Salakhutdinov and Hinton, 2009, Desjardins et al., 2012, Cho et al., 2012]. Hence, Salakhutdinov and Hinton [2009] and Cho et al. [2012] proposed pretraining algo- rithms that can initialize the parameters of DBMs. In this paper, we use the pretraining algorithm proposed by Cho et al. [2012].  A Gaussian-Bernoulli RBM (GRBM) is a special case of a GDBM, where the number of hidden layers is restricted to one, L = 1. Due to this restriction it is possible to compute the posterior distri- bution over the hidden units conditioned on the visible units exactly and tractably. The conditional probability of each hidden unit hj = h(1)  is  j  p(hj = 1 | v, θ) = f Xi  wij  vi  σ2 + cj! .  (2)  Hence, the positive part of the gradient, that needs to be approximated with variational approach in the case of GDBMs, can be computed exactly and efﬁciently. Only the negative part, which is computed over the model distribution, still relies on MCMC sampling, or more approximate methods such as contrastive divergence (CD) [Hinton, 2002].  2.2 Denoising Autoencoders  A denoising autoencoder (DAE) is a special form of multi-layer perceptron network with 2L − 1 hidden layers and L − 1 sets of tied weights. A DAE tries to learn a network that reconstructs an input vector optimally by minimizing the following cost function:  N  Xn=1(cid:13)(cid:13)(cid:13)  Wg(1) ◦ · · · ◦ g(L−1) ◦ f (L−1) ◦ · · · ◦ f (1)(cid:16)η(v(n))(cid:17) − v(n)(cid:13)(cid:13)(cid:13)  2  ,  (3)  where  f (l) = φ(W(l)⊤  h(l−1)) and g(l) = φ(W(l)h(2L−l))  are encoding and decoding functions for l-th layer with a component-wise nonlinearity function φ. W(l) is the weights between the l-th and l + 1-th layers and is shared by the encoder and decoder. For notational simplicity, we omit biases to all units.  Unlike an ordinary autoencoder, a DAE explicitly sets some of the components of an input vector randomly to zero during learning via η(·) which explicitly adds noise to an input vector. It is usual to combine two different types of noise when using η(·), which are additive isotropic Gaussian noise and masking noise [Vincent et al., 2010]. The ﬁrst type adds a zero-mean Gaussian noise to each input component, while the masking noise sets a set of randomly chosen input components to zeros. Then, the DAE is trained to denoise the corrupted input.  Training a DAE is straightforward using backpropagation algorithm which computes the gradient of the objective function using a chain-rule and dynamic programming. Vincent et al. [2010] proposed that training a deep DAE becomes easier when the weights of a deep DAE are initialized by greedily pretraining each layer of a deep DAE as if it were a single-layer DAE. In the following experiments section, we follow this approach to initialize the weights and subsequently ﬁnetune the network with the stochastic backpropagation.  3 Image Denoising  There are a number of ways to perform image denoising. In this paper, we are interested in an approach that relies on local statistics of an image.  As it has been mentioned earlier, a noisy large image can be denoised by denoising small patches of the image and combining them together. Let us deﬁne a set of N binary matrices Dn ∈ Rp×d that extract a set of small image patches given a large, whole image x ∈ Rd, where d = whc is the product of the width w, the height h and the number of color channels c of the image and p is the size  3  of image patches (e.g., n = 64 if an 8 × 8 image patch). Then, the denoised image is constructed by  ˜x =  N Xn=1  D⊤  n rθ(Dnx)! ⊘  N Xn=1  D⊤ n  Dn1! ,  (4)  where ⊘ is a element-wise division and 1 is a vector of ones. rθ(·) is an image denoising function, parameterized by θ, that denoises N image patches extracted from the input image x.  Eq. (4) essentially extracts and denoises all possible image patches from the input image. Then, it combines them by taking an average of those overlapping pixels.  There are several ﬂexibilities in constructing a matrix D. The most obvious one is the size of an image patch. Although there is no standard approach, many previous attempts tend to use patch sizes as small as 4 × 4 to 17 × 17. Another one, called a stride, is the number of pixels between two consecutive patches. Taking every possible patch is one option, while one may opt to overlapping patches by only a few pixels, which would reduce the computational complexity.  One of the popular choices for rθ(·) has been to construct a probabilistic model with a set of latent variables that describe natural image patches. For instance, sparse coding which is, in essence, a probabilistic model with a single layer of latent variables has been a common choice. Hyv¨arinen et al. [1999] used ICA and a nonlinear shrinkage function to compute the sparse code of an image patch, while Elad and Aharon [2006] used K-SVD to build a sparse code dictionary.  Under this approach denoising can be considered as a two-step reconstruction. Initially, the posterior distribution over the latent variables is computed, or estimated, given an image patch. Given the estimated posterior distribution, the conditional distribution, or its mean, over the visible units is computed and used as a denoised image patch.  In the following subsections, we describe how rθ(·) can be implemented in the cases of Boltzmann machines and denoising autoencoders.  3.1 Boltzmann machines  We consider a BM with a set of Gaussian visible units v that correspond to the pixels of an image patch and a set of binary hidden units h. Then, the goal of denoising can be written as  p(v | ˜v) =Xh  p(v | h)p(h | ˜v) = Eh|˜v [p(v | h)] ,  (5)  where ˜v is a noisy input patch. In other words, we ﬁnd a mean of the conditional distribution of the visible units with respect to the posterior distribution over the hidden units given the visible units ﬁxed to the corrupted input image patch.  However, since taking the expectation over the posterior distribution is usually not tractable nor exactly computable, it is often easier to approximate the quantity. We approximate the marginal conditional distribution (5) of v given ˜v with  p(v | ˜v) ≈ p(v | h)Q(h) = p(v | h = µ),  Q(h) [h] and Q(h) is a fully factorial distribution that approximates the posterior  where µ = E p(h | ˜v). It is usual to use the mean of v under p(v | ˜v) as the denoised, reconstructed patch. Following this approach, given a noisy image patch ˜v a GRBM reconstructs a noise-free patch by  ˆvi =  Nh  Xj=1  wij E [h | ˜v] + bi,  where bi is a bias to the i-th visible unit. The conditional distribution over the hidden units can be computed exactly from Eq. (2).  Unlike a GRBM, the posterior distribution of the hidden units of a GDBM is neither tractably computable nor has an analytical form. Salakhutdinov and Hinton [2009] proposed to utilize a variational approximation [Neal and Hinton, 1999] with a fully-factored distribution Q(h) =  4  j, where the variational parameters µ(l)  j ’s can be found by the following simple ﬁxed-  QL l=1Qj µl  point update rule:  µ(l)  j ← f   Nl−1  µ(l−1) i  w(l−1)  ij  +  Xi=1  µ(l+1) k  Nl+1  Xk=1  w(l) kj + c(l)  j   ,  (6)  where f (x) =  1  1+exp{−x}.  Once the variational parameters are converged, a GDBM reconstructs a noise-free patch by  ˆvi =  Nl  Xj=1  wij µ(1)  j + bi.  The convergence of the variational parameters can take too much time and may not be suitable in practice. Hence, in the experiments, we initialize the variational parameters by feed-forward prop- agation using the doubled weights [Salakhutdinov and Hinton, 2009] and perform the ﬁxed-point update in Eq. (6) for at most ﬁve iterations only. This turned out to be a good enough compromise that least sacriﬁces the performance while reducing the computational complexity signiﬁcantly.  3.2 Denoising autoencoders  An encoder part of a DAE can be considered as performing an approximate inference of a fully- factorial posterior distribution of top-layer hidden units, i.e. a bottleneck, given an input image patch [Vincent et al., 2010]. Hence, a similar approach to the one taken by BMs can be used for DAEs.  Firstly, the variational parameters µ(L) of the fully-factorial posterior distribution Q(h(L)) =  are computed by  (L) j  Qj µ  µ(L) = f (L−1) ◦ · · · ◦ f (1) (˜v) .  Then, the denoised image patch can be reconstructed by the decoder part of the DAE. This can be done simply by propagating the variational parameters through the decoding nonlinearity functions g(l) such that  ˆv = g(1) ◦ · · · ◦ g(L−1)(cid:16)µ(L)(cid:17) .  Recently, Burger et al. [2012] and Xie et al. [2012] tried a deep DAE in this manner to perform image denoising. Both of them reported that the denoising performance achieved by DAEs is com- parable, or sometimes favorable, to other conventional image denoising methods such as BM3D [Dabov et al., 2007], K-SVD [Portilla et al., 2003] and Bayes Least Squares-Gaussian Scale Mix- ture [Elad and Aharon, 2006].  4 Experiments  In the experiments, we aim to empirically compare the two dominant approaches of deep learning, namely Boltzmann machines and denoising autoencoders, in image denoising tasks.  There are several questions that are of interest to us:  1. Does a model with more hidden layers perform better? 2. How well does a deep model generalize? 3. Which family of deep neural networks is more suitable, Boltzmann machines or denoising  autoencoders?  In order to answer those questions, we vary the depth of the models (the number of hidden layers), the level of noise injection, the type of noise–either white Gaussian additive noise or salt-and-pepper noise, and the size of image patches. Also, as our interest lies in the generalization capability of the models, we use a completely separate data set for training the models and apply the trained models to three distinct sets of images that have very different properties.  5  (a) Textures  (b) Aerials  (c) Miscellaneous  Figure 1: Sample images from the test image sets  4.1 Datasets  We used three sets of images, textures, aerials and miscellaneous, from the USC-SIPI Image Database1 as test images. Tab. 1 lists the details of the image sets, and Fig. 1 presents six sam- ple images from the test sets.  These datasets are, in terms of contents and properties of images, very different from each other. For instance, most of the images in the texture set have highly repetitive patterns that are not present in the images in the other two sets. Most images in the aerials set have both coarse and ﬁne structures, for example, a lake and a nearby road, at the same time in a single image. Also, the sizes of the images vary quite a lot across the test sets and across the images in each set.  Set  # of all images  Textures Aerials  Miscellaneous  64 38 44  # of color images Min. Size 512 × 512 512 × 512 256 × 256  0 37 16  Max. Size 1024 × 1024 2250 × 2250 1024 × 1024  Table 1: Descriptions of the test image sets.  As we are aiming to evaluate the performance of denoising a very general image, we used a large separate data set of natural image patches to train the models. We extracted a set of 100, 000 random image patches of sizes 4×4, 8×8 and 16×16 from CIFAR-10 dataset [Krizhevsky, 2009]. From each image of 50, 000 training samples of the CIFAR-10 dataset, two patches from randomly selected locations have been collected.  We tried denoising only grayscale images. When an image was in an RGB format, we averaged the three channels to make the image grayscale.  4.2 Denoising Settings  We tried three different depth settings for both Boltzmann machines and denoising autoencoders; a single hidden layer, two hidden layers and four hidden layers. The sizes of all hidden layers were set to have the same number of hidden units, which was the constant factor multiplied by the number of pixels in an image patch2. We denote Boltzmann machines with one, two and four hidden layers by GRBM, GDBM(2) and GDBM(4), respectively. Denoising autoencoders are denoted by DAE, DAE(2) and DAE(4), re- spectively. For each model structure, Each model was trained on image patches of sizes 4 × 4, 8 × 8 and 16 × 16. The GRBMs were trained using the enhanced gradient [Cho et al., 2011c] and persistent contrastive divergence (PCD) [Tieleman, 2008]. The GDBMs were trained by PCD after initializing the param- eters with a two-stage pretraining algorithm [Cho et al., 2012]. DAEs were trained by a stochastic backpropagation algorithm, and when there were more than one hidden layers, we pretrained each layer as a single-layer DAE with sparsity target set to 0.1 [Vincent et al., 2010, Lee et al., 2008]. The details on training procedures are described in Appendix A.  One important difference to the recent work by Xie et al. [2012] and Burger et al. [2012] is that the denoising task we consider in this paper is completely blind. No prior knowledge about target  1http://sipi.usc.edu/database/ 2We used 5 as suggested by Xie et al. [2012].  6  White noise  Salt-and-pepper noise  Aerials  R N S P  Textures  R N S P  26  24  22  20  18  16  28  26  24  22  20  18  16  25  Misc.  R N S P  20  0.1  0.2  Noise Level  0.4  0.1  0.2  Noise Level  0.4  R N S P  R N S P  26  24  22  20  18  16  28  26  24  22  20  18  16  25  R N S P  20  0.1  0.2  Noise Level  0.4  0.1  0.2  Noise Level  0.4  15  0.1  0.2  Noise Level  0.4  15  0.1  0.2  Noise Level  0.4     Figure 2: PSNR of grayscale images corrupted by different types and levels of noise. The median PSNRs over the images in each set together.  DAE    DAE(2)  DAE(4)  GRBM  GDBM(2)  GDBM(4)  images and the type or level of noise was assumed when training the deep neural networks. In other words, no separate training was done for different types or levels of noise injected to the test images. Unlike this, Xie et al. [2012], for instance, trained a DAE speciﬁcally for each noise level by changing η(·) accordingly. Furthermore, the Boltzmann machines that we propose here for image denoising, do not require any prior knowledge about the level or type of noise.  Two types of noise have been tested; white Gaussian and salt-and-pepper. White Gaussian noise simply adds zero-mean normal random value with a predeﬁned variance to each image pixel, while salt-and-pepper noise sets a randomly chosen subset of pixels to either black or white. Furthermore, three different noise levels (0.1, 0.2 and 0.4) were tested. In the case of white Gaussian noise, they were used as standard deviations, and in the case of salt-and-pepper noise, they were used as a noise probability.  After noise was injected, each image was preprocessed by pixel-wise adaptive Wiener ﬁltering [see, e.g., Sonka et al., 2007], following the approach of Hyv¨arinen et al. [1999]. The width and height of the pixel neighborhood were chosen to be small enough (3 × 3) so that it will not remove too much detail from the input image.  Denoising performance was measured mainly with peak signal-to-noise ratio (PSNR) computed by −10 log10(ǫ2), where ǫ2 is a mean squared error between an original clean image and the denoised one.  4.3 Results and Analysis  In Fig. 2, the performances of all the tested models trained on 8 × 8 image patches are presented3. The most obvious observation is that the deep neural networks, including both DAEs and BMs, did not show improvement over their shallow counterparts in the low-noise regime (0.1). However, the deeper models signiﬁcantly outperformed their corresponding shallow models as the level of  3Those trained on patches of different sizes showed similar trend, and they are omitted in this paper.  7  injected noise grew. In other words, the power of the deep models became more evident as the injected level of noise grew.  This is supported further by Tab. 2 which shows the performance of the models in the high noise regime (0.4). In all cases, the deeper models, such as DAE(4), GDBM(2) and GDBM(4), were the best performing models.  Another notable phenomenon is that the GDBMs tend to lag behind the DAEs, and even the GRBM, in the low noise regime, except for the textures set. A possible explanation for this rather poor performance of the GDBMs in the low noise regime is that the approximate inference of the pos- terior distribution, used in this experiment, might not have been good enough. For instance, more mean-ﬁeld iterations might have improved the overall performance while dramatically increasing the computational time, which would not allow GDBMs to be of any practical value. The GDBMs, however, outperformed, or performed comparably to, the other models when the level of injected noise was higher.  It should be noticed that the performance depended on the type of the test images. For instance, although the images in the aerials set corrupted with salt-and-pepper noise were best denoised by the DAE with four hidden layers, the GDBMs outperformed the DAE(4) in the case of the textures set. We emphasize here that the deeper neural networks showed less performance variance depending on the type of test images, which suggests better generalization capability of the deeper neural networks.  Visual inspection of the denoised images provides some more intuition on the performances of the deep neural networks. In Fig. 3, the denoised images of a sample image from each test image set are displayed. It shows that BMs tend to emphasize the detailed structure of the image, while DAEs, especially ones with more hidden layers, tend to capture the global structure.  Additionally, we tried the same set of experiments using the models trained on a set of 50, 000 random image patches extracted from the Berkeley Segmentation Dataset Martin et al. [2001]. In this case, 100 patches from randomly chosen locations from each of 500 images were collected to form the training set. We obtained the results similar to those presented in this paper. The results are presented in Appendix B.  Method Aerials 15.7 (0.1) Wiener 16.4 (0.2) DAE DAE(2) 17.6 (0.2) DAE(4) 20.8 (0.7) GRBM 19.2 (0.4) 22.3 (1.4) 22.1 (1.1)  GDBM(2) GDBM(4)  Textures Misc. 15.9 (0.6) 15.5 (0.6) 16.6 (0.8) 16.2 (0.9) 17.7 (1.1) 17.1 (1.2) 18.7 (2.8) 20.2 (2.0) 18.9 (1.5) 18.0 (1.7) 18.7 (3.2) 20.1 (2.4) 18.7 (3.0) 20.2 (2.2)  (a) White Gaussian Noise  Aerials 16.3 (0.5) 17.1 (0.6) 18.1 (0.7) 20.1 (1.1) 18.9 (0.9) 20.3 (1.4) 20.3 (1.3) (b) Salt-and-Pepper Noise  Textures Misc. 15.3 (1.5) 14.9 (1.3) 16.3 (1.7) 15.7 (1.4) 17.3 (2.0) 16.4 (1.7) 17.2 (2.8) 19.0 (2.7) 17.6 (2.2) 16.6 (2.1) 17.5 (2.6) 16.5 (3.0) 16.6 (2.9) 17.6 (2.5)  Table 2: Performance of the models trained on 4 × 4 image patches when the level of injected noise was 0.4. Standard deviations are shown inside the parentheses, and the best performing models are marked bold.  5 Conclusion  In this paper, we proposed that, in addition to DAEs, Boltzmann machines, GRBMs and GDBMS, can also be used for denoising images. Furthermore, we tried to ﬁnd empirical evidence supporting the use of deep neural networks in image denoising tasks.  Our experiments suggest the following conclusions for the questions raised earlier: Does a model with more hidden layers perform better?  In the case of DAEs, the experiments clearly show that more hidden layers do improve performance, especially when the level of noise is high. This does not always apply to BMs, where we found that the GRBMs outperformed, or performed as well as, the GDBMs in few cases. Regardlessly, in the high noise regime, it was always beneﬁcial to have more hidden layers.  8  Original  Noisy  DAE  DAE(4)  GRBM  GDBM(4)  16.79  18.96  18.10  18.96  (a) Textures  16.02  (b) Aerials  18.59  17.75  19.37  17.59  21.34  (c) Miscellaneous  19.42  18.98  Figure 3: Images, corrupted by salt-and-pepper noise with 0.4 noise probability, denoised by various deep neural networks trained on 8 × 8 image patches. The number below each denoised image is the PSNR.  How well does a deep model generalize?  The deep neural networks were trained on a completely separate dataset and were applied to three test sets with very different image properties. It turned out that the performance depended on each test set, however, with only small differences. Also, the trend of deeper models performing better could be observed in almost all cases, again especially with high level of noise. This suggests that a well-trained deep neural network can perform blind image denoising, where no prior information about target, noisy images is available, well. Which family of deep neural networks is more suitable, BMs or DAEs?  The DAE with four hidden layers turned out to be the best performer, in general, beating GDBMs with the same number of hidden layers. However, when the level of noise was high, the Boltzmann machines such as GRBM and GDBM(2) were able to outperform the DAEs, which suggests that Boltzmann machines are more robust to noise.  One noticeable observation was that the GRBM outperformed, in many cases, the DAE with two hidden layers which had twice as many parameter. This potentially suggests that a better inference of approximate posterior distribution over the hidden units might make GDBMs outperform, or comparable to, DAEs with the same number of hidden layers and units. More work will be required in the future to make a deﬁnite answer to this question.  Although it is difﬁcult to make any general conclusion from the experiments, it was evident that deep models, regardless of whether they are DAEs or BMs, performed better and were more robust to the level of noise than their more shallow counterparts. In the future, it might be appealing to investigate the possibility of combining multiple deep neural networks with various depths to achieve better denoising performance.  References  David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for Boltz-  mann machines. Cognitive Science, 9:147–169, 1985.  9  H.C. Burger, C.J. Schuler, and S. Harmeling. Image denoising: Can plain neural networks compete with bm3d? In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2392 –2399, june 2012.  K. Cho, T. Raiko, and A. Ilin. Enhanced gradient for training restricted Boltzmann machines. Neural  Computation, 2013.  KyungHyun Cho, Alexander Ilin, and Tapani Raiko. Improved learning of Gaussian-Bernoulli re- stricted Boltzmann machines. In Proc. of the 20th Int. Conf. on Artiﬁcial Neural Networks (ICANN 2010), 2011a.  KyungHyun Cho, Tapani Raiko, and Alexander Ilin. Gaussian-Bernoulli deep Boltzmann machine. In NIPS 2011 Workshop on Deep Learning and Unsupervised Feature Learning, Sierra Nevada, Spain, December 2011b.  KyungHyun Cho, Tapani Raiko, and Alexander Ilin. Enhanced gradient and adaptive learning rate for training restricted Boltzmann machines. In Proc. of the 28th Int. Conf. on Machine Learning (ICML 2011), pages 105–112, New York, NY, USA, June 2011c. ACM.  KyungHyun Cho, Tapani Raiko, Alexander Ilin, and Juha Karhunen. A Two-Stage Pretraining Algorithm for Deep Boltzmann Machines. In NIPS 2012 Workshop on Deep Learning and Unsu- pervised Feature Learning, Lake Tahoe, December 2012.  Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-D Transform-Domain collaborative ﬁltering. Image Processing, IEEE Transactions on, 16(8):2080–2095, August 2007.  Guillaume Desjardins, Aaron C. Courville, and Yoshua Bengio. On training deep Boltzmann ma-  chines. CoRR (Cornell Univ. Computing Research Repository), abs/1203.4416, 2012.  M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned  dictionaries. Image Processing, IEEE Transactions on, 15(12):3736–3745, December 2006.  G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,  313(5786):504–507, July 2006.  Geoffrey Hinton. Training products of experts by minimizing contrastive divergence. Neural Com-  putation, 14:1771–1800, August 2002.  Aapo Hyv¨arinen, Patrik Hoyer, and Erkki Oja.  Image denoising by sparse code shrinkage.  In  Intelligent Signal Processing. IEEE Press, 1999.  A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Computer  Science Department, University of Toronto, 2009.  Honglak Lee, Chaitanya Ekanadham, and Andrew Ng. Sparse deep belief net model for visual area  V2. pages 873–880, 2008.  Xiaoqiang Lu, Haoliang Yuan, Pingkun Yan, Luoqing Li, and Xuelong Li.  Image denoising via improved sparse coding. In Proceedings of the British Machine Vision Conference, pages 74.1– 74.0. BMVA Press, 2011.  D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Int’l Conf. Computer Vision, volume 2, pages 416–423, July 2001.  Radford M. Neal and Geoffrey E. Hinton. Learning in graphical models. chapter A view of the EM algorithm that justiﬁes incremental, sparse, and other variants, pages 355–368. MIT Press, Cambridge, MA, USA, 1999.  J. Portilla, V. Strela, M.J. Wainwright, and E.P. Simoncelli. Image denoising using scale mixtures of gaussians in the wavelet domain. Image Processing, IEEE Transactions on, 12(11):1338 – 1351, nov. 2003.  Ruslan Salakhutdinov. Learning deep Boltzmann machines using adaptive MCMC. In Johannes F¨urnkranz and Thorsten Joachims, editors, Proc. of the 27th Int. Conf. on Machine Learning (ICML 2010), pages 943–950, Haifa, Israel, June 2010. Omnipress.  Ruslan Salakhutdinov and Geoffrey E. Hinton. Deep Boltzmann machines. In Proc. of the Int. Conf.  on Artiﬁcial Intelligence and Statistics (AISTATS 2009), pages 448–455, 2009.  10  Li Shang and Deshuang Huang. Image denoising using non-negative sparse coding shrinkage algo- rithm. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 1017 – 1022 vol. 1, june 2005.  P. Smolensky. Information processing in dynamical systems: foundations of harmony theory. In Par- allel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations, pages 194–281. MIT Press, Cambridge, MA, USA, 1986.  Milan Sonka, Vaclav Hlavac, and Roger Boyle. Image Processing, Analysis, and Machine Vision.  Thomson-Engineering, 2007. ISBN 049508252X.  Tijmen Tieleman. Training restricted Boltzmann machines using approximations to the likelihood  gradient. ICML ’08. ACM, New York, NY, USA, 2008.  Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11:3371–3408, December 2010.  Junyuan Xie, Linli Xu, and Enhong Chen. Image denoising and inpainting with deep neural net- In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors,  works. Advances in Neural Information Processing Systems 25, pages 350–358. 2012.  11  A Training Procedures: Details  Here, we describe the procedures used for training the deep neural networks in the experiments.  A.1 Data Preprocessing  Prior to training a model, we normalized each pixel of the training set such that, across all the training samples, the mean and variance of each pixel are 0 and 1. The original mean and variance were discarded after training. During test, we computed the mean and variance of all image patches from each test image and used them instead.  A.2 Denoising Autoencoders  A single-layer DAE was trained by the stochastic gradient descent for 200 epochs. A minibatch of size 128 was used at each update, and a single epoch was equivalent to one cycle over all training samples. The initial learning rate was set to η0 = 0.05 and was decreased over training according to the following schedule:  ηt =  η0  1 + t  5000  .  In order to encourage the sparsity of the hidden units, we used the following regularization term  −λ  N  Xn=1  q  Xj=1 ρ − φ  p Xi=1  v(n)  i wij + cj!!2  ,  where λ = 0.1, ρ = 0.1 and p and q are respectively the numbers of visible and hidden units. φ is a sigmoid function.  Before computing the gradient at each update, we added a white Gaussian noise of standard deviation 0.1 to all components of an input sample and forced randomly chosen 20% of input units to zeros. The weights of a deep DAE was ﬁrst initialized by layer-wise pretraining. During the pretraining, each layer was trained as if it were a single-layer DAE, following the same procedure described above, except that no white Gaussian noise was added for the layers other than the ﬁrst one.  After pretraining, we trained the deep DAE with the stochastic backpropagation algorithm for 200 epochs using minibatches of size 128. The initial learning rate was chosen to be η0 = 0.01 and the learning rate was annealed according to  ηt =  η0  1 + t  5000  .  For each denoising autoencoder regardless of its depth, we used a tied set of weights for the encoder and decoder.  A.3 Restricted Boltzmann Machines  We used the modiﬁed energy function of a Gaussian-Bernoulli RBM (GRBM) proposed by Cho et al. [2011a], however, with a single σ2 shared across all the visible units. Each GRBM was trained for 200 epochs, and each update was performed using a minibatch of size 128.  A learning rate was automatically selected by the adaptive learning rate [Cho et al., 2011a] with the initial learning rate and the upper-bound ﬁxed to 0.001 and 0.001, respectively. After 180 epochs, we decreased the learning rate according to  where t denotes the number of updates counted after 180 epochs of training.  η ←  η t  ,  12  A persistent contrastive divergence (PCD) [Tieleman, 2008] was used, and at each update, a single Gibbs step was taken for the model samples. Together with PCD, we used the enhanced gradient [Cho et al., 2013], instead of the standard gradient, at each update.  A.4 Deep Boltzmann Machines  We used the two-stage pretraining algorithm [Cho et al., 2012] to initialize the parameters of each DBM. The pretraining algorithm consists of two separate stages.  We utilized the already trained single-layer and two-layer DAEs to compute the activations of the hidden units in the even-numbered hidden layers of GDBMs. No separate, further training was done for those DAEs in the ﬁrst stage.  In the second stage, the model was trained as an RBM using the coupled adaptive simulated temper- ing [CAST, Salakhutdinov, 2010] with the base inverse temperature set of 0.9 and 50 intermediate chains between the base and model distributions. At least 50 updates were required to make a swap between the slow and fast samples. The initial learning rate was set to η0 = 0.01 and the learning rate was annealed according to  ηt =  η0  1 + t  5000  .  Again, the modiﬁed form of an energy function [Cho et al., 2011b] was used with a shared variance σ2 for all the visible units. However, in this case, we did not use the enhanced gradient.  After pretraining, the GDBMs were further ﬁnetuned using the stochastic gradient method together with the variational approximation [Salakhutdinov and Hinton, 2009]. The CAST was again used with the same hyperparameters. The initial learning rate was set to 0.0005 and the learning rate was decreased according to the same schedule used during the second stage.  13  White noise  Salt-and-pepper noise  Aerials  R N S P  Textures  R N S P  26  24  22  20  18  16  28  26  24  22  20  18  16  25  Misc.  R N S P  20  0.1  0.2  Noise Level  0.4  0.1  0.2  Noise Level  0.4  R N S P  R N S P  26  24  22  20  18  16  28  26  24  22  20  18  16  25  R N S P  20  0.1  0.2  Noise Level  0.4  0.1  0.2  Noise Level  0.4  15  0.1  0.2  Noise Level  0.4  15  0.1  0.2  Noise Level  0.4     DAE    DAE(2)  DAE(4)  GRBM  GDBM(2)  GDBM(4)  Figure 4: PSNR of grayscale images corrupted by different types and levels of noise. The median PSNRs over the images in each set together. The models used for denoising in this case were trained on the training set constructed from the Berkeley Segmentation Dataset.  B Result Using a Training Set From Berkeley Segmentation Dataset  Fig. 4 shows the result obtained by the models trained on the training set constructed from the Berke- ley Segmentation Dataset. Although we see some minor differences, the overall trend is observed to be similar to that from the experiment in the main text (see Fig. 2).  Especially in the high-noise regime, the models with more hidden layers tend to outperform those with only one or two hidden layers. This agrees well with what we have observed with the models trained on the training set constructed from the CIFAR-10 dataset.  14  ","Image denoising based on a probabilistic model of local image patches hasbeen employed by various researchers, and recently a deep (denoising)autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] asa good model for this. In this paper, we propose that another popular family ofmodels in the field of deep learning, called Boltzmann machines, can performimage denoising as well as, or in certain cases of high level of noise, betterthan denoising autoencoders. We empirically evaluate the two models on threedifferent sets of images with different types and levels of noise. Throughoutthe experiments we also examine the effect of the depth of the models. Theexperiments confirmed our claim and revealed that the performance can beimproved by adding more hidden layers, especially when the level of noise ishigh."
1301.3485,2013,A Semantic Matching Energy Function for Learning with Multi-relational Data  ,"['Xavier Glorot', 'Antoine Bordes', 'Jason Weston', 'Yoshua Bengio']",https://arxiv.org/pdf/1301.3485.pdf,"3 1 0 2    r a     M 1 2      ]  G L . s c [      2 v 5 8 4 3  .  1 0 3 1 : v i X r a  A Semantic Matching Energy Function for Learning  with Multi-relational Data  Xavier Glorot(1), Antoine Bordes(2), Jason Weston(3), Yoshua Bengio(1)  (1) DIRO, Université de Montréal, Montréal, QC, Canada – {glorotxa,bengioy}@iro.umontreal.ca (2) CNRS - Heudiasyc, Université de Technologie de Compiègne, France – bordesan@hds.utc.fr (3) Google, New York, NY, USA – jweston@google.com  1 Introduction Multi-relational data, which refers to graphs whose nodes represent entities and edges correspond to relations that link these entities, plays a pivotal role in many areas such as recommender systems, the Semantic Web, or computational biology. Relations are modeled as triplets of the form (subject, relation, object), where a relation either models the relationship between two entities or between an entity and an attribute value; relations are thus of several types. In spite of their appealing ability for representing complex data, multi-relational graphs remain complicated to manipulate for several reasons (noise, het- erogeneity, large-scale dimensions, etc.), and conveniently represent, summarize or de-noise this kind of data is now a central challenge in statistical relational learning [2].  In this work, we propose a new model to learn multi-relational semantics, that is, to encode multi- relational graphs into representations that capture the inherent complexity in the data, while seamlessly deﬁning similarities among entities and relations and providing predictive power. Our work is based on an original energy function, which is trained to assign low energies to plausible triplets of a multi- relational graph. This energy function, termed semantic matching energy, relies on a compact distributed representation: all elements (entity and relation type) are represented into the same relatively low (e.g. 50) dimensional embedding vector space. The embeddings are learnt by a neural network whose partic- ular architecture and training process force them to capture the structure implicit in the training data and generalize the graph formed from training triplets. Unlike in previous work [4, 6, 5, 3], in this model, relation types are modeled similarly as entities. In this way, entities can also play the role of relation type, as in natural language for instance, and this requires less parameters when the number of relation types grows. We show empirically that this model achieves competitive results on benchmark tasks of link prediction, i.e., generalizing outside of the set of given valid triplets.  2 Semantic Matching Energy Function This work considers multi-relational databases as graph models. To each individual node of the graph corresponds an element of the database, which we term an entity, and each link deﬁnes a relation between entities. Relations are directed and there are typically several different kinds of relations. Let C denote the dictionary which includes all entities and relation types, and let R ⊂ C be the subset of entities which are relation types. A relation is denoted by a triplet (lhs, rel, rhs), where lhs is the left entity, rhs the right one and rel the type of relation between them.  2.1 Main ideas The main ideas behind our semantic matching energy function are the following.  • Named symbolic entities (entities and relation types) are associated with a d-dimensional vector space, termed the “embedding space”. The ith entity is assigned a vector Ei ∈ Rd. Note that more general mappings from an entity to its embedding are possible.  • The semantic matching energy value associated with a particular triplet (lhs, rel, rhs) is computed by a parametrized function E that starts by mapping all symbols to their embeddings and then combines them in a structured fashion. Our model is termed “semantic matching” because E relies on a matching criterion computed between both sides of the triplet.  • The energy function E is optimized to be lower for training examples than for other possible  conﬁgurations of symbols.  2.2 Neural network parametrization The energy function E (denoted SME) is encoded using a neural network, whose architecture ﬁrst pro- cesses each entity in parallel, like in siamese networks [1]. The intuition is that the relation type should ﬁrst be used to extract relevant components from each argument’s embedding, and put them in a space where they can then be compared.  (1) Each symbol of the input triplet (lhs, rel, rhs) is mapped to its embedding Elhs, Erel, Erhs ∈ Rd.  (2) The embeddings Elhs and Erel respectively associated with the lhs and rel arguments are used to construct a new relation-dependent embedding Elhs(rel) for the lhs in the context of the relation type represented by Erel, and similarly for the rhs: Elhs(rel) = glef t(Elhs, Erel) and Erhs(rel) = gright(Erhs, Erel), where glef t and gright are parametrized functions whose parameters are tuned during training. The dimension of Elhs(rel) and Erhs(rel), which we denote p, is low-dimensional but not necessarily equal to d, the dimension of the entity embedding space.  (3) The energy is computed by ""matching"" the transformed embeddings of the left-hand and right-  hand sides: E((lhs, rel, rhs)) = h(Elhs(rel), Erhs(rel)), h is a dot product in our experiments.  We studied two options for the g functions, which lead to two versions of SME: • Linear form (denoted SME(linear)), in this case g functions are simply linear layers:  Elhs(rel) = glef t(Elhs, Erel) = Wl1E ⊺  Erhs(rel) = gright(Erhs, Erel) = Wr1E ⊺  lhs + Wl2E ⊺ rhs + Wr2E ⊺  rel + b⊺ l . rel + b⊺ r .  with Wl1, Wl2, Wr1, Wr2 ∈ Rp×d, bl, br ∈ Rp and E ⊺ denotes the transpose of E. This leads to the energy: E((lhs, rel, rhs)) = − (Wl1E ⊺  rhs + Wr2E ⊺  lhs + Wl2E ⊺  l )⊺ (Wr1E ⊺  rel + b⊺  rel + b⊺  r ).  • Bilinear form (denoted SME(bilinear)), g functions are using 3-modes tensors as core weights:  Elhs(rel) = glef t(Elhs, Erel) = (Wl ¯×3E ⊺ Erhs(rel) = gright(Erhs, Erel) = (Wr ¯×3E ⊺  rel) E ⊺ rel) E ⊺  lhs + b⊺ l . rhs + b⊺ r .  with Wl, Wr ∈ Rp×d×d (weights) and bl, br ∈ Rp (biases). ¯×3 denotes the n-mode vector-tensor product along the 3rd mode. This leads to the following form for the energy: E((lhs, rel, rhs)) = − ((Wl ¯×3E ⊺  l )⊺ ((Wr ¯×3E ⊺  rhs + b⊺  lhs + b⊺  rel) E ⊺  rel) E ⊺  r ).  Dataset  UMLS Kinships Nations  Table 1: Statistics of datasets used in this paper. Nb. of relation Nb. of Nb. of observed % valid relations  types  entities  49 26 56  135 104 14  relations 893,025 281,216 11,191  in obs. ones  0.76 3.84 22.9  To train the parameters of the energy function E we loop over all of the training data resources and  use stochastic gradient descent with a ranking objective inspired by [7].  3 Empirical Evaluation To evaluate against existing methods, we performed link prediction experiments on benchmarks from the literature, whose statistics are in Table 1.  The link prediction task consists in predicting whether two entities should be connected by a given relation type. This is useful for completing missing values of a graph, forecasting the behavior of a network, etc. but also to assess the quality of a representation. We evaluate our model on UMLS, Nations and Kinships, following the setting introduced in [4]. The standard evaluation metric is area under the precision-recall curve (AUC). Table 2 presents results of SME along with those of RESCAL, MRC, IRM, CP (CANDECOMP-PARAFAC) and LFM, which have been extracted from [5, 3].  The linear formulation of SME is outperformed by SME(bilinear) on all three tasks. The largest differences for Nations and Kinships indicate that, for these problems, a joint interaction between both lhs, rel and rhs is crucial to represent the data well: relations cannot be simply decomposed as a sum of bigrams. This is particularly true for the complex kinship systems of the Alyawarra. On the contrary, interactions within the UMLS network can be represented by simply considering the various (entity,entity) and (entity,relation type) bigrams. Compared to other methods, SME(bilinear) performs similarly to LFM on UMLS but is slightly outperfomed on Nations. On Kinships, it is outperformed by CP, RESCAL and LFM: on this dataset with complex ternary interactions, either the training process of the tensor factorization methods, based on reconstruction, or the combination of bigram and trigram interactions seems to be beneﬁcial compared to our predictive approach. Compared to MRC, which is not using a matrix-based encoding, SME(bilinear) is highly competitive.  Table 2: Comparisons of area under the precision-recall curve (AUC) for link prediction.  Method SME(linear) SME(bilinear) LFM RESCAL CP MRC IRM  UMLS  Nations  Kinships  0.983 ± 0.004 0.985 ± 0.003 0.990 ± 0.003 0.98 0.95 0.98 0.70  0.777 ± 0.025 0.865 ± 0.015 0.909 ± 0.009 0.84 0.83 0.75 0.75  0.149 ± 0.003 0.894 ± 0.011 0.946 ± 0.005 0.95 0.94 0.85 0.66  Even if experimental results on these benchmarks are mixed, it is worth noting that, contrary to all previous methods, SME models relation types as vectors, lying in the same space as entities. From a conceptual viewpoint, this is powerful, since it models any relation types as a standard entity (and vice- versa). Hence, SME is the only method that could be directly applied on data for which any entity can also create relationships between other entities.  Acknowledgements This work was supported by the French ANR (EVEREST-12-JS02-005-01), the Pascal2 European NoE, the DARPA DL Program, NSERC, CIFAR, the Canada Research Chairs, and Compute Canada.  References  [1] Jame Bromley, Jim W. Bentz, Léon Bottou, Isabelle Guyon, Yann Le Cun, C. Moore, Eduard Säckinger, and Roopak Shah. Signature veriﬁcation using a siamese time delay neural network. International Journal of Pattern Recognition and Artiﬁcial Intelligence, 7(4), 1993.  [2] Lise Getoor and Ben Taskar.  Introduction to Statistical Relational Learning (Adaptive Computation and  Machine Learning). The MIT Press, 2007.  [3] Rodolphe Jenatton, Nicolas Le Roux, Antoine Bordes, and Guillaume Obozinski. A latent factor model for  highly multi-relational data. In Advances in Neural Information Processing Systems 25. 2012.  [4] Charles Kemp, Joshua B. Tenenbaum, Thomas L. Grifﬁths, Takeshi Yamada, and Naonori Ueda. Learning In Proceedings of the 21st national conference on  systems of concepts with an inﬁnite relational model. Artiﬁcial intelligence - Volume 1, AAAI’06, pages 381–388. AAAI Press, 2006.  [5] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi- relational data. In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11, pages 809–816. ACM, 2011.  [6] Ilya Sutskever, Ruslan Salakhutdinov, and Josh Tenenbaum. Modelling relational data using bayesian clustered  tensor factorization. In Adv. in Neur. Inf. Proc. Syst. 22, 2009.  [7] Jason Weston, Samy Bengio, and Nicolas Usunier. Large scale image annotation: learning to rank with joint  word-image embeddings. Machine Learning, 81:21–35, 2010.  ","Large-scale relational learning becomes crucial for handling the huge amountsof structured data generated daily in many application domains ranging fromcomputational biology or information retrieval, to natural language processing.In this paper, we present a new neural network architecture designed to embedmulti-relational graphs into a flexible continuous vector space in which theoriginal data is kept and enhanced. The network is trained to encode thesemantics of these graphs in order to assign high probabilities to plausiblecomponents. We empirically show that it reaches competitive performance in linkprediction on standard datasets from the literature."
1301.3214,2013,The Manifold of Human Emotions  ,"['Seungyeon Kim', 'Fuxin Li', 'Guy Lebanon', 'Irfan Essa']",https://arxiv.org/pdf/1301.3214.pdf,"The Manifold of Human Emotions  Seungyeon Kim, Fuxin Li, Guy Lebanon, Irfan Essa  College of Computing  Georgia Institute of Technology  3 1 0 2     n a J    5 1      ] L C . s c [      1 v 4 1 2 3  .  1 0 3 1 : v i X r a  {seungyeon.kim@, fli@cc., lebanon@cc., irfan@cc.}gatech.edu  Abstract  Sentiment analysis predicts the presence of positive or negative emotions in a text document. In this paper, we consider higher dimensional extensions of the senti- ment concept, which represent a richer set of human emotions. Our approach goes beyond previous work in that our model contains a continuous manifold rather than a ﬁnite set of human emotions. We investigate the resulting model, compare it to psychological observations, and explore its predictive capabilities.  1 Introduction  Sentiment analysis predicts the presence of a positive or negative emotion y in a text document x. Despite its successes in industry, sentiment analysis is limited as it ﬂattens the structure of human emotions into a single dimension. An alternative that has attracted a few researchers in recent years is to construct a ﬁnite collection of emotions and ﬁt a predictive model for each emotion (see for example [1]). There are several signiﬁcant difﬁculties with the above approach. First, it is hard to capture a complex statistical relationship between a large number of binary variables (representing emotions) and a high dimensional vector (representing the document). It is also hard to imagine a reliable procedure for compiling a ﬁnite list of all possible human emotions. Finally, it is not clear how to use documents expressing a certain emotion.Using labeled documents only in ﬁtting models predicting their denoted labels ignores the relationship among emotions, and is problematic for emotions with only a few annotated.  We propose an alternative approach that models a stochastic relationship between the document X, an emotion label Y (such as sleepy or happy), and a position on the mood manifold Z. We assume that all the emotional aspects in the documents are captured by the manifold, implying that the emotion label Y can be inferred directly from the projection Z of the document on the manifold, without needing to consult the document again.  2 The Statistical Model  We make the following four modeling assumptions concerning the document X, the discrete emo- tion label Y ∈ {1, 2, . . . , C}, and the position on the continuous mood manifold Z ∈ Rl.  1. We have the graphical structure: X → Z → Y , implying that the emotion label Y ∈  {1, . . . , C} is independent of the document X given Z.  2. The distribution of Z ∈ Rl given a speciﬁc emotion label Y = y is Gaussian:  {Z|Y = y} ∼ N (µy, Σy)  3. The distribution of Z given the document X (typically in a bag of words or n-gram repre-  sentation) is a linear regression model: {Z|X = x} ∼ N (θ⊤x, Σx).  4. The distances between the vectors in {E(Z|Y = y) : y ∈ C} are similar to the correspond-  ing distances in {E(X|Y = y) : y ∈ C}.  1  accomplished  exhausted  sick  tired  sleepy  blah  contemplative  cold  creative  busy content  hungry  okay  calm  awake  anxious  blank  aggravated  sad  depressed  artistic  chipper  cheerful  bouncy  happy  bored hopeful  annoyed  confused  curious  amused  excited  Figure 1: (left) The two-dimensional structure of emotions from [2]. We can interpret top-left to bottom-right axis as expressing sentiment polarity and the top-right to bottom-left axis as expressing engagement. (right) Mood centroids E(Z|Y = y) on the two most prominent dimensions in emotion space ﬁtted from blog posts. See text for details.  We make the following observations. First, the ﬁrst assumption implies that the emotion label Y is simply a discretization of the continuous Z. It is consistent with well known research in psychology (see Section 2) and with random projection theory, which state that it is often possible to approxi- mate high dimensional data by projecting it on a low dimensional continuous space. Second, while X, Y are high dimensional and discrete, Z is low dimensional and continuous. This, together with the conditional independence in assumption 1 above, implies a higher degree of accuracy than mod- eling directly X → Y . Intuitively, the number of parameters is on the order of dim(X) + dim(Y ) as opposed to dim(X)dim(Y ). Third, the Gaussian models in assumptions 2 and 3 are simple, and lead to efﬁcient computational procedures. We also found them to work well in our experiments. The model may be easily adapted, however, to more complex models such as mixture of Gaussians or non-linear regression models (for example, we experimented with quadratic regression models). Fourth, assumption 4 suggests that we can estimate E(Z|Y = y) for all y ∈ C via multidimen- sional scaling. MDS ﬁnds low dimensional coordinates for a set of points that approximates the spatial relationship between the points in the original high dimensional space. Lastly, the models in assumptions 2 and 3 are statistical and can be estimated from data using maximum likelihood. Motivated by the fourth modeling assumption, we determine the parameters µy = E(Z|Y = y), y ∈ C by running multidimensional scaling (MDS) or Kernel PCA on the empirical versions of {E(X|Y = y) : y ∈ C}. We estimate the parameter θ, deﬁning the regression X → Z, by maximizing the likelihood which requires integrating over Z ∈ Rl, a computationally difﬁcult task when l is not very low. We make use of approximating the Gaussian pdf with Dirac’s delta function on p(z),  ˆθ ≈ arg max  θ  X  log  i  p(y(i))pθ(z(i) ∗ Py p(z(i)  |x(i)) |y)p(y)  ∗  = arg max  θ  X  i  log pθ(z(i) ∗  |x(i))  (1)  = arg maxz p(z|y(i)) = E(Z|y(i)), which is equivalent to a least squares regression.  where z(i)∗ One interpretation of our model X → Z → Y is that Z forms a sufﬁcient statistic of X for Y . We can thus consider adapting a wide variety of predictive models (for example, logistic regression or SVM) on Z 7→ Y . These discriminative classiﬁers are trained on {( ˆZ (i), Y (i)), i = 1, . . . , n}.  3 Experiments  We used crawled Livejournal1 data as the main dataset. About 20% of the blog posts feature these optional annotations in the form of emoticons. The annotations may be chosen from a pre-deﬁned list of possible emotions, or a novel emotion speciﬁed by the author. We crawled 15,910,060 doc-  1http://www.livejournal.com  2  Table 1: Macro F1 score and accuracy over the test set in multiclass emotion classiﬁcation over top 32 moods (left) and sentiment polarity task (right): {cheerful, happy, amused} vs {sad, annoyed, depressed, confused}. See text for details.  LDA  QDA  Log.Reg.  full diag. spher. full diag. spher.  Original Space  F1  Acc.  Mood Manifold Acc.  F1  n/a 0.1229 0.0838 n/a 0.0878 0.0777 0.1231  n/a 0.1441 0.1075 n/a 0.0931 0.0989 0.1360  0.1247 0.1160 0.0896 0.1206 0.1118 0.0873 0.1477  0.1635 0.1600 0.1303 0.1478 0.1463 0.1253 0.1667  LDA  QDA  Log.Reg.  full diag. spher. full diag. spher.  Original Space  F1  Acc.  Mood Manifold Acc.  F1  n/a 0.7183 0.6358 n/a 0.6390 0.6091 0.7350  n/a 0.7436 0.6553 n/a 0.6398 0.6143 0.7624  0.7340 0.7365 0.7482 0.6500 0.6704 0.7472 0.7509  0.7812 0.7663 0.7699 0.7446 0.7510 0.7734 0.7857  uments and selected 1,346,937 documents featuring the most popular 32 emotion labels (in respect to the number of documents annotated in).  In Figure 1, we compare our model to Watson and Tellegen’s well known psychological model. We make the following observations. First, the horizontal axis expresses a sentiment polarity-like emotion. The left part features emotions such as accomplished, happy and excited, while the right part features emotions such as sad and depressed. This is in agreement with Watson and Tellegen’s observations. Second, the vertical axis expresses the level of mental engagement or energy level. The top part features emotions such as exhausted or tired, while the bottom part features emotions such as curious or excited. This agrees partially with the engagement dimension in the psychological model. Third, the neutral moods blank, stay in the middle of the picture. This agreement between our mood manifold and the psychological ﬁndings is remarkable in light of the fact that the two models used completely different experimental methodology (blog data vs. surveys).  We performed emotion classiﬁcation experiment (Table 1, left) on the Livejournal data. We consid- ered the goal of predicting the most popular 32 moods. We also considered a binary classiﬁcation tasks obtained by partitioning the set of moods into two clusters.  Table 1 compare classiﬁcation results using the original bag of words feature space and the manifold model, using different types of classiﬁcation methods: LDA, QDA with different covariance matrix models, and logistic regression. Bold faces are improvements over the baseline with statistical signiﬁcance of t-test of random trials. Most of experimental results show that the mood manifold model results in statistically signiﬁcant improvements than using original bag of words feature.  4 Summary  In this paper, we introduced a continuous representation for human emotions Z and constructed a statistical model connecting it to documents X and to a discrete set of emotions Y . Our ﬁtted model bears close similarities to models developed in the psychological literature, based on human survey data. Several attempts were recently made at inferring insights from social media or news data through sentiment prediction. It is likely that the current multivariate view of emotions will help make progress on these important and challenging tasks.  References  [1] G. Mishne. Experiments with mood classiﬁcation in blog posts. In 1st Workshop on Stylistic Analysis Of  Text For Information Access, 2005.  [2] D. Watson and A. Tellegen. Toward a consensual structure of mood. Psychological bulletin, 98(2):219–  235, September 1985.  3  ","Sentiment analysis predicts the presence of positive or negative emotions ina text document. In this paper, we consider higher dimensional extensions ofthe sentiment concept, which represent a richer set of human emotions. Ourapproach goes beyond previous work in that our model contains a continuousmanifold rather than a finite set of human emotions. We investigate theresulting model, compare it to psychological observations, and explore itspredictive capabilities."
1301.3192,2013,Matrix Approximation under Local Low-Rank Assumption  ,"['Joonseok Lee', 'Seungyeon Kim', 'Guy Lebanon', 'Yoram Singer']",https://arxiv.org/pdf/1301.3192.pdf,"Matrix Approximation  under Local Low-Rank Assumption  Joonseok Leea, Seungyeon Kima, Guy Lebanona, b, Yoram Singerb  a College of Computing, Georgia Institute of Technology, Atlanta, GA 30332  b Google Research, Mountain View, CA 94043  {jlee716@, seungyeon.kim@, lebanon@cc.}gatech.edu, singer@google.com  Abstract  Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements of prediction accuracy in recommendation tasks.  1  Introduction  Matrix approximation is a common task in machine learning. Given a few observed matrix entries {Ma1,b1, . . . , Mam,bm}, matrix approximation constructs a matrix ˆM that approximates M at its unobserved entries. In general, the problem of completing a matrix M based on a few observed entries is ill-posed, as there are an inﬁnite number of matrices that perfectly agree with the observed entries of M. Thus, we need additional assumptions such that M is a low-rank matrix. More formally, we approximate a matrix M ∈ Rn1×n2 by a rank-r matrix ˆM = U V T , where U ∈ Rn1×r, V ∈ Rn2×r, and r (cid:28) min(n1, n2). In this note, we assume that M behaves as a low-rank matrix in the vicinity of certain row-column combinations, instead of assuming that the entire M is low-rank. We therefore construct several low-rank approximations of M, each being accurate in a particular region of the matrix. Smoothing the local low-rank approximations, we express ˆM as a linear combination of low-rank matrices that approximate the unobserved matrix M. This mirrors the theory of non-parametric kernel smoothing, which is primarily developed for continuous spaces, and generalizes well-known compressed sensing results to our setting.  2 Global and Local Low-Rank Matrix Approximation  3 1 0 2     n a J    5 1      ]  G L . s c [      1 v 2 9 1 3  .  1 0 3 1 : v i X r a  We describe in this section two standard approaches for low-rank matrix approximation (LRMA). The original (partially observed) matrix is denoted by M ∈ Rn1×n2, and its low-rank approximation by ˆM = U V T , where U ∈ Rn1×r, V ∈ Rn2×r, r (cid:28) min(n1, n2). Global LRMA Incomplete SVD is a popular approach for constructing a low-rank approximation ˆM by minimizing the Frobenius norm over the set A of observed entries of M:  (U, V ) = arg min  U,V  ([U V T ]a,b − Ma,b)2.  (1)  Another popular approach is minimizing the nuclear norm of a matrix (deﬁned as the sum of singular values of the matrix) satisfying constraints constructed from the training set: (cid:107)ΠA(X − M )(cid:107)F < α  (cid:107)X(cid:107)∗, s.t.  ˆM = arg min  (2)  X  (cid:88)  (a,b)∈A  1  Figure 1: For illustrative purposes, we assume a distance function d whose neighborhood structure coincides with the natural order on indices. That is, s = (a, b) is similar to u = (a(cid:48), b(cid:48)) if |a − a(cid:48)| and |b − b(cid:48)| are small. (Left) For all s ∈ [n1] × [n2], the neighborhood {s(cid:48) : d(s, s(cid:48)) < h} in the original matrix M is approximately described by the corresponding entries of the low-rank matrix T (s) (shaded regions of M are matched by lines to the corresponding regions in T (s) that approximate them). If d(s, u) is small, T (s) is similar to T (u), as shown by their spatial closeness in the embedding space Rn1×n2. (Right) The original matrix M (bottom) is described locally by the low-rank matrices T (t) (near t) and T (u) (near u). The lines connecting the three matrices identify identical entries: Mt = Tt(t) and Mu = Tu(u). The equation at the top right shows a relation tying the three patterned entries. Assuming the distance d(t, u) is small, (cid:15) = Tu(t) − Tu(u) = Tu(t) − Mu(u) is small as well.  where ΠA : Rn1×n2 → Rn1×n2 is the projection deﬁned by [ΠA(M )]a,b = Ma,b if (a, b) ∈ A and 0 otherwise, and (cid:107) · (cid:107)F is the Frobenius norm. Minimizing the nuclear norm (cid:107)X(cid:107)∗ is an effective surrogate for minimizing the rank of X. One advantage of (2) over (1) is that we do not need to constrain the rank of ˆM in advance. However, problem (1) is substantially easier to solve than problem (2). Local LRMA In order to facilitate a local low-rank matrix approximation, we need to pose an assumption that there exists a metric structure over [n1] × [n2], where [n] denotes the set of integers {1, . . . , n}. Formally, d((a, b), (a(cid:48), b(cid:48))) reﬂects the similarity between the rows a and a(cid:48) and columns b and b(cid:48). In the global matrix factorization setting above, we assume that the matrix M ∈ Rn1×n2 has a low-rank structure. In the local setting, however, we assume that the model is characterized by multiple low-rank n1 × n2 matrices. Speciﬁcally, we assume a mapping T : [n1] × [n2] → Rn1×n2 that associates with each row-column combination [n1] × [n2] a low rank matrix that describes the entries of M in its neighborhood (in particular this applies to the observed entries A): T : [n1] × [n2] → Rn1×n2 where Ta,b(a, b) = Ma,b. Note that in contrast to the global estimate in Global LRMA, our model now consists of multiple low-rank matrices, each describing the original matrix M in a particular neighborhood. Figure 1 illustrates this model. Without additional assumptions, it is impossible to estimate the mapping T from a set of m < n1n2 observations. Our additional assumption is that the mapping T is slowly varying. Since the domain of T is discrete, we assume that T is H¨older continuous. Following common approaches in non- parametric statistics, we deﬁne a smoothing kernel Kh(s1, s2), where s1, s2 ∈ [n1] × [n2], as a non-negative symmetric unimodal function that is parameterized by a bandwidth parameter h > 0. A large value of h implies that Kh(s,·) has a wide spread, while a small h corresponds to narrow spread of Kh(s,·). We use, for example, the Epanechnikov kernel, deﬁned as Kh(s1, s2) = 3 4 (1 − d(s1, s2)2)1 the matrix whose (i, j)-entry is Kh((a, b), (i, j)). Incomplete SVD (1) and compressed sensing (2) can be extended to local version as follows  . We denote by K (a,b)  {d(s1,s2)<h}  h  Incomplete SVD: ˆT (a, b) = arg min Compressed Sensing: ˆT (a, b) = arg min  X  X  (cid:107)K (a,b) (cid:107)X(cid:107)∗  h (cid:12) ΠA(X − M )(cid:107)F s.t.  rank(X) = r  s.t.  (cid:107)K (a,b)  h (cid:12) ΠA(X − M )(cid:107)F < α,  (3)  (4)  2  MsuT(u)Rn1⇥n2T(s)MsMtMT(u)Tt(t)T(t)Tu(t)=Tu(u)+✏=Mu+✏Tu(u)MuTu(t)Tt(u)Figure 2: RMSE of global-LRMA, local-LRMA, and other baselines on MovieLens 10M (Left) and Netﬂix (Right) dataset. Local-LRMA models are indicated by thick solid lines, while global-LRMA models are indicated by dotted lines. Models with same rank are colored identically.  where (cid:12) denotes a component-wise product of two matrices, [A (cid:12) B]i,j = Ai,jBi,j. The two optimization problems above describe how to estimate ˆT (a, b) for a particular choice of (a, b) ∈ [n1] × [n2]. Conceptually, this technique can be applied for each test entry (a, b), result- ing in the matrix approximation ˆMa,b = ˆTa,b(a, b), where (a, b) ∈ [n1] × [n2]. However, this requires solving a non-linear optimization problem for each test index (a, b) and is thus computa- tionally prohibitive. Instead, we use Nadaraya-Watson local regression with a set of q local estimates ˆT (s1), . . . , ˆT (sq), in order to obtain a computationally efﬁcient estimate ˆˆT (s) for all s ∈ [n1]×[n2]: (5)  q(cid:88)  ˆˆT (s) =  i=1  Kh(si, s) j=1 Kh(sj, s)  ˆT (si) .  (cid:80)q  Equation (5) is simply a weighted average of ˆT (s1), . . . , ˆT (sq), where the weights ensure that values of ˆT at indices close to s contribute more than indices further away from s. Note that the local version can be faster than global SVD since (a) each low-rank approximation is independent of each other, so can be computed in parallel, and (b) the rank used in the local SVD model can be signiﬁcantly lower than the rank used in a global one. If the kernel Kh has limited support (Kh(s, s(cid:48)) is often zero), the regularized SVD problems would be sparser than the global SVD problem, resulting in additional speedup.  3 Experiments  We compare local-LRMA to global-LRMA and other state-of-the-art techniques on popular recom- mendation systems datasets: MovieLens 10M and Netﬂix. We split the data into 9:1 ratio of train and test set. A default prediction value of 3.0 was used whenever we encounter a test user or item without training observations. We use the Epanechnikov kernel with h1 = h2 = 0.8, assuming a product form Kh((a, b), (c, d)) = K(cid:48)h1 (b, d). For distance function d, we use arccos dis- tance, deﬁned as d(x, y) = arccos ((cid:104)x, y(cid:105)/(cid:107)x(cid:107)(cid:107)y(cid:107)). Anchor points were chosen randomly among observed training entries. L2 regularization is used for local low-rank approximation. Figure 2 graphs the RMSE of Local-LRMA and global-LRMA as well as the recently proposed method called DFC (Divide-and-Conquer Matrix Factorization) as a function of the number of an- chor points. Both local-LRMA and global-LRMA improve as r increases, but local-LRMA with rank r ≥ 5 outperforms global-LRMA with any rank. Moreover, local-LRMA outperforms global- LRMA in average with even a few anchor points (though the performance of local-LRMA improves further as the number of anchor points q increases).  (a, c)K(cid:48)(cid:48)h2  3  510152025303540450.780.790.80.810.820.830.84MovieLens 10MNumber of Anchor PointsRMSE  Global(r=5)Global(r=7)Global(r=10)Global(r=15)Local(r=5)Local(r=7)Local(r=10)Local(r=15)DFC510152025303540450.830.840.850.860.870.880.890.9NetflixNumber of Anchor PointsRMSE  Global(r=5)Global(r=7)Global(r=10)Global(r=15)Local(r=5)Local(r=7)Local(r=10)Local(r=15)DFCNetflix Winner","Matrix approximation is a common tool in machine learning for buildingaccurate prediction models for recommendation systems, text mining, andcomputer vision. A prevalent assumption in constructing matrix approximationsis that the partially observed matrix is of low-rank. We propose a new matrixapproximation model where we assume instead that the matrix is only locally oflow-rank, leading to a representation of the observed matrix as a weighted sumof low-rank matrices. We analyze the accuracy of the proposed local low-rankmodeling. Our experiments show improvements in prediction accuracy inrecommendation tasks."
1210.8442,2013,Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines  ,['Yuanlong Shao'],https://arxiv.org/pdf/1210.8442.pdf,"3 1 0 2     n a J    7 2      ] I  A . s c [      3 v 2 4 4 8  .  0 1 2 1 : v i X r a  Linear-Nonlinear-Poisson Neuron Networks Perform  Bayesian Inference On Boltzmann Machines  Louis Yuanlong Shao  Department of Computer Science & Engineering  The Ohio State University  shaoyu@cse.ohio-state.edu  Abstract  One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back- end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear- Nonlinear-Poisson (LNP) neuron model. We show that with neurally plausible settings, the whole network is capable of representing any Boltzmann machine and performing a semi-stochastic Bayesian inference algorithm lying between Gibbs sampling and variational inference.  1 Introduction  Classical connectionist viewpoint has long been inspired from how the brain works, such as the invention of “perceptron” [1], and the “parallel distributed processing” approach [2]. Modern deep learning and unsupervised feature learning also assume connections between biological brain and certain kinds of deep networks, either probabilistic or not. For example, properties of visual area V2 are found to be comparable to those on the sparse autoencoder networks [3]; the sparse coding learning algorithm [4] is originated directly from neuroscience observations; also psychological phenomenon such as end-stopping is observed in sparse coding experiments [5].  In addition, there are architectural similarity between neuroscience and machine learning. For exam- ple, neuronal spike propagation may correspond to prediction or inference tasks in a learned model; synaptic plasticity may correspond to parameter estimation given the network structure; neuroplas- ticity may correspond to learning the network structure together with inventing new hidden units in the network; axonal path ﬁnding in the help of guidance signals may be regarded as structural priors making learning network structure easier.  For this reason, it may be helpful again to refer to how the brain works when dealing with prob- lems that are currently puzzling machine learning researchers. For example, training deep net- works was made practical mostly after the breakthrough of deep learning starting from 2006 (e.g. [6][7][8][9][10], etc.). The success may lie in a safe way of adapting the network structure, such that the parameter estimation part can be effective enough to reach non-trivial local optima. However, since the brain is not exactly layer-wise, we may wonder is there any other way of choos- ing the network structure along with parameter estimation? If we want to transfer neuroscience knowledge for answering questions as such and build bridge between these two areas, we want to ﬁrst ﬁgure out what exactly is the deep network that the brain is representing.  A good starting point of answering this may be to look at single neuron computation. Through how they transfer information among them, we can reverse engineer both the representation and the prediction / inference algorithm, which is the main purpose of this paper.  1  In theoretical neuroscience, single neuron models are divided into different levels of detail, (see [11] for a ﬁve level survey). Detailed models can be quite realistic and few simpliﬁcation assumptions hold. Yet effective simpliﬁcation holds in different extent. For example, integrate-and-ﬁre models capture essential properties of Hodgkin-Huxley model [12], and its variant exponential integrate- and-ﬁre neuron models [13] are found to be capable of reproducing spike timing of several types of cortical neurons. The community has also been investigating on the Linear-Nonlinear-Poisson (LNP) models for long [14][15][16]. A recent study in [17] analyzed how LNP models effectively reproduce ﬁring rates in more realistic neurons. Based on these results, we start from formally presenting the LNP model and then turn to presenting a semi-stochastic inference algorithm on Boltzmann machines, which is derived from combining Gibbs sampling and variational inference. We then make a detailed matching between LNP model and the inference algorithm. Since the semi-stochastic inference algorithm has not been explored in the learning community, we also show some experiments illustrating its computational property, including its stochastic convergence and its similarity to variational inference.  2 Brief Review on Neural Plausibility of LNP Model  In this section, we brieﬂy review the Linear-Nonlinear-Poisson neuron model and its neural plausi- bility, focusing on what modeling options we have to ﬁt it with useful inference algorithms.  The LNP neuron model [14][15][16] formalizes the spike train generated by each neuron as a non- homogeneous Poisson point process, whose rate function over time depends on the input spike trains from its presynaptic neurons, with certain form of short-term memory in the dependence.  Synaptic Transmission:  Iij (τ ) = Wij · (Xj ∗ α) (τ )  Dendritic Integration:  Spike Generation:  Ii (τ ) = Multi - Linear(cid:16){Iij (τ )}j∈M(i)(cid:17) (cid:26) λi (τ ) = σ ((Ii ∗ D) (τ ))  Xi (τ ) ∼ PoissonPointProcess (λi (τ ))  (a)  (b)  (c) (d)  (1)  Eq. (1)(d), Xi (τ ) = Pf δ(τ − τ (f )  In all these equations, τ is the continuous time index, whose unit is millisecond. Subscript i is the index of the neuron in question, M (i) is the index set of presynaptic neurons of neuron i. In ) represents the spike train, with spikes as Dirac functions lo- cated at time steps τ (f ) . λi (τ ) is the rate function of the Poisson point process. Eq. (1)(a) represents the postsynaptic current at a particular synapse with efﬁcacy Wij. α is a certain non-negative func- tion with certain time course. Evidence for “Poisson-like” statistics in cortical neurons can be found in [18][19], which implies that spike counts over a certain interval may be Poisson distributed.  i  i  Eq. (1)(b) is the dendritic summation. There are different viewpoints on whether linearity holds true. Due to phenomenon like mutual inhibition on different dendritic shafts, it is commonly known that nonlinear summation could happen (see [20][21] for example), yet studies from [22] show that when dendritic spines are presented, linear summation is true, and this happens frequently in excitatory mammalian cortex. In addition, a survey on neuron models in [11] summarized different computations that can be performed in dendritic processing, two notable examples are multiplication and summation. We include these two and generalizes them into the “Multi-Linear” function in Eq. (1)(b), i.e., it is linear with respect to each of its arguments individually. Here we use the set notation {·}j∈M(i) to denote the set of arguments. What is special in a “Multi-Linear” function is that, when taking its expectation with respect to a certain argument, the expectation goes in and the resulted formula simply replaces the argument by its mean. This is useful for mean-ﬁeld algorithms.  After dendritic summation, relationship between the summed current and the ﬁring rate can be well researched by in-vitro neuroscience experiments where input current can be externally con- trolled. Various frequency-current function can also be derived from popular spike generation mod- els, such as Leaky Integrate-and-Fire model (see [12] for an overview), Exponential Integrate-And- Fire Model [13], Spike Response Model [23], or more realistic models such as Hodgkin-Huxley Model [24]. In particular, [14][15][16] formulated the LNP simpliﬁcation through a nonlinear map- ping followed by a Poisson point process random sampling. The analytical study with simulation in [17] show that the simpliﬁcation is effective. We put this in Eq. (1)(c).  2  Since neural spikes has refractory period with at least 1ms, ﬁr- ing rate will not exceed 1000Hz and there won’t be more than one spike happening in 1ms, so λi (τ ) ∈ [0, 1]. In practice, ﬁring rate is often even lower. In addition, discrete time steps in the resolution of 1ms per each step seems quite ﬁne-grained for most perceptual tasks (e.g., visual recognition emerges in 75-80ms [25]). For these reasons, discretizing time into 1ms per each step may be reasonable. By LeCam’s theorem [26], the discrete counterpart of nonhomogeneous Poisson point process is the nonhomogeneous Bernoulli process, and the Bernoulli probability at each discrete time step is approxi- mated by the integration of rate function at that time period. We use t ∈ Z to denote the discrete time steps, whose unit is millisecond. We use φ(t) i ∈ [0, 1] to denote the Bernoulli probability and x(t) i ∈ {0, 1} to denote the spike indicator at time t,  i ∼ Bernoulli(cid:16)φ(t) x(t)  i (cid:17) , φ(t)  i  , Z t+1  t  1  0.8  0.6  0.4  0.2  y c n e u q e r F    t  t  u p u O  0 −10  −5  0  5  10  15  20  Input Current I 0  Figure 1: Frequency-current curve of leaky integrate-and-ﬁre neurons for a particular set of parameters. More from different neuron models can be found in [12].  λi (τ ) dτ  (2)  The corresponding discrete form of Eq. (1)(a,b,c) is by substituting Xi (τ ) with x(t) and substituting α, D functions with their discrete form similar to the deﬁnition of φ(t) in Eq. (2). There are two ways to proceed with the formulation. In case D function is the Dirac function located at 0, or is close to that, the convolution with D can be ignored, the discrete counterpart of Eq. (1)(a,b,c) are  i  i  I (t)  α (k) · x(t−k)  ij = Wij ·XKα i = σ(cid:18)Multi-Linear(cid:18)nI (t)  k=1  i  φ(t)  ij oj∈M(i)(cid:19)(cid:19)  (3)  Or if D is non-trivial, but the Multi-Linear function in Eq. (1)(b) reduces to a linear summation, by the associativity of convolution, we can let ǫ = α ∗ D, and the discrete counterpart of Eq. (1)(a,b,c) reduces to the following, assuming ﬁnite time course of the function ǫ.  φ(t)  i = σ(cid:18)Xj∈M(i)  Wij ·XK  k=1  ǫ (k) · x(t−k)  i  (cid:19)  (4)  In [17], D functions resulted from spiking neuron models are not Dirac functions, but are very close. In the latter part of this paper, we will focus on the case when linear summation is true (hence Eq. (4) holds) although multi-linearity in Eq. (3) may generalize our formulation to high-order Boltzmann machines. Another observation is that in Eq. (4), if D is exponential function (true in [17]) and α is exponential function too (true in [12]), and assuming both functions have the same scale parameter, ǫ becomes the classic α-function [27] used in postsynaptic modeling. Another issue is whether linear summation in Eq. (4) or multi-linear integration in Eq. (3) contains a constant bias term bi. In [28], there is such a bias term in the spiking response model. This will make our latter part easier, but in our derivation later, we focus on the case when there is no such bias. It will be trivial then to turn to the case where bias exists.  To sum up, the LNP neuron model we are going to deal with in the latter part is as the following, assuming H is the index set of neurons whose activity is not bound to observations (external stimuli).  For all i ∈ H, t ∈ Z+,    i (cid:17) i ∼ Bernoulli(cid:16)φ(t) x(t) i = σ(cid:16)Pj∈M(i) Wij ·PK φ(t)  k=1 ǫ (k) · x(t−k)  i  (cid:17)  σ function can be ﬁt from data or from realistic neuron models, and has some relationship with the frequency-current curve. According to [12][17], such nonlinearity in different spiking neuron models may be non-decreasing, close to zero towards the left, increase almost linearly in a certain interval. Also, since ﬁring rate has upper bounds, σ won’t increase unboundedly. For this reason, σ may be considered as a modiﬁcation of sigmoid function to meet certain demands of inference. An example of frequency-current curve is shown in Fig. 1. See also [17] for another type of nonlinearity.  (5)  3  In Fig. 1, parameters of the leaky integrate-and-ﬁre model are set as τm = 20 ms, ∆abs = 1 ms, R = 1 mΩ, υ = 1 mV (with notations following [12]). In the biological brain, all neurons are computing in parallel. Thus we want to investigate on the case when Eq. (5) is executed in parallel, vectors x(t) = (x(t) i=1 over time forms a Markov chain of order K. The latter part focuses on showing its stochastic inference nature.  i=1 and φ(t) = (φ(t)  i )n  i )n  3 Semi-Stochastic Inference on Boltzmann Machines  In this section, ﬁrst we compare Gibbs sampling and variational inference on Boltzmann machines, emphasizing their architectural similarity, then we present the semi-stochastic inference algorithm by combining them. Some notations we use in this section will overlap with the last section. This is intended since we want to link the quantities in both sections. Let Y ∈ {A, B}n be a collection of random variables. Each of them has two possible values A and B (e.g., A = 0 and B = 1 in the binary case). Let V, H be a partition of the index set I = {1, ..., n}. We denote YV as the visible variables, YH as the hidden variables. We use lower case of Y to denote its values, e.g., yi, yH or y. In this paper we use the Boltzmann machines with softmax units [29],  p (y) =  1 Z  exp   1 2  · X  i,j:i6=j;u,v∈{A,B}  [[yi = u]] · [[yi = v]] · Viu,jv − X  i∈I,u∈{A,B}  [[yi = u]] · ciu     (6)  Here we denote [[·]] as the binary indicator. [[·]] = 1 if the statement is true and [[·]] = 0 otherwise. V = (Viu,jv)i,u,j,v is a four-dimensional tensor of size 4n2, c = (ciu)i,u is a matrix of size 2n. Z is the normalization constant (partition function), which depends on V and c. Note that this family has no more capacity than the original Boltzmann machine [30] and the Ising model [31]. In Gibbs sampling without parallelism [32][33], we are given an observed value yV, and we want to ﬁnd a sequence ny(t) which converges in distribution to the true posterior p (YH|YV = yV ). The algorithm proceeds as follows. First we initialize y(0) V take on the observed values. Then at each iteration t ∈ Z+ we pick an index i ∈ H and do the following,  H , and let y(0)  H o∞  t=0  (a) Calculate proposal φ(t)  iu = σ(cid:18)Xj∈M(i),v∈{A,B}  Wiu,jv ·hhy(t−1) iB(cid:17) , pass down samples by y(t)  iA , φ(t)  j  i ∼ Bernoulli(cid:16)φ(t)  ∼i = y(t−1)  ∼i  = vii − biu(cid:19) , ∀u ∈ {A, B}  (7)  (b) Sample y(t)  Here we denote Wiu,jv = Viu,jv −Vi¯u,jv and biu = ciu −ci¯u with ¯u deﬁned as {¯u} = {A, B} \ {u}. σ (x) = 1/ (1 + exp (−x)) is the sigmoid function. M (i) is the index set of Markov blanket of Yi, i.e., M (i) = {j ∈ I|j 6= i, ∃u, v ∈ {A, B}, s.t. Viu,jv 6= 0}. Variational inference with factorized approximation family often takes the form of a mean-ﬁeld [34]). We adopt the same family of variational distributions as version of Gibbs sampling (e.g.,  in [35], such that q (YH|ΘH) = Qi∈H q (Yi|θiA, θiB), where each q (Yi|θiA, θiB) is a Bernoulli distribution such that θiu = q (Yi = u|θiA, θiB) for u ∈ {A, B}. 1 The objective of variational inference is to ﬁnd ΘH by minimizing the loss function L (ΘH) = KL (q (Y|Θ) ||p (Y)), here KL (q||p) = Eq log (q/p) is the KL-divergence.  The algorithm proceeds as: ﬁrst we randomly initialize Θ(0) an index i ∈ H and do the following,  H , then at each iteration t ∈ Z+ we pick  (a) Calculate update φ(t)  (b) Assign θ(t)  iu = φ(t)  iu = σ(cid:18)Xj∈M(i),v∈{A,B} ∼iu = θ(t−1) iu , pass down θ(t)  ∼iu , ∀u ∈ {A, B} .  Wiu,jv · θ(t−1)  jv − biu(cid:19) , ∀u ∈ {A, B}  (8)  Our motivation for combining them is as follows. First, exponential family distributions have sufﬁ- cient statistics [36]. By accumulating the empirical expectation of sufﬁcient statistics, we can online  1For simplicity, when i ∈ V we write θiu and q (Yi|θiA, θiB) as well, except that θiu for all iterations are ﬁxed as the observed value. We denote Θ = (θiu)i∈I,u∈{A,B} and write q (Y|Θ) = Qi∈I q (Yi|θiA, θiB).  4  i=1 x2  i=1 xi and Pt  estimate the parameter if data instances are completely observed, e.g., to online estimate a Gaussian i , and estimate ˆµ and  distribution N (X|µ, σ), we can incrementally calculatePt  ˆσ at each time t by only using these two statistics. Now if we take the examples in Gibbs sampling as the online data instances and estimate a distribu- tion from within the variational approximation family, we will end up getting a marginal probability distribution for each random variable (when mixing is good). However, we can also change the way how the sequence is generated, such that the online estimated variational distribution biases towards whatever we want. In particular, we can take the calculated distribution in Eq. (8)(a) as the proposal distribution, used to sample a new data example, and incrementally update the variational distribu- tion by using that. The updated variational distribution serve as the next input to Eq. (8). If decaying as in [37] is used when accumulating sufﬁcient statistics, the variational distribution estimated in our Bernoulli case, is simply a weighted sum of the most recent examples:  θ(t)  iu = X−∞  k=0  ǫ (k) ·hhy(t−k)  i  = uii, for u ∈ {A, B}  (9)  The −∞ in Eq. (9) simply means the starting time of accumulation. ǫ function is non-negative and should sum up to one for all k in the effective time span. In case constant decaying ratio is used on the last accumulated sufﬁcient statistics as in [37], the ǫ function above decays exponentially. Some choices of ǫ function may not have any online updating scheme that produces it. In practice, we can either do online updating or let ǫ have a ﬁnite time course, up to some K ∈ Z+. In semi-stochastic inference, we choose a non-negative weight function ǫ (k) which sum up to 1 for k ∈ {1, 2, ..., K}. We initialize both y(0) and Θ(0). In each iteration t ∈ Z+ we pick an index i ∈ H and do the following,  (a) φ(t)  iu = σ(cid:18)Xj∈M(i),v∈{A,B} i ∼ Bernoulli(cid:16)φ(t) iB(cid:17) iA, φ(t) y(t) iu = XK ǫ (k) ·hhy(t−k+1) θ(t)  k=1  i  (b)  (c)  Wiu,jv · θ(t−1)  jv − biu(cid:19) , for u ∈ {A, B}  = uii, for u ∈ {A, B}  (10)  In sequential inference, for every other random variable which does not carry out these updating steps, we will pass down θ and most recent K copies of y’s to the next time step. In step (c), additional normalization is needed when t < K. Details are ignored for clarity of presentation. In another way, we can also look at algorithm in Eq. (10) as a “random slowing-down” / stochastic approximation / momentum version of variational inference. When an updated variational distri- bution is computed, we don’t immediately turn to the updated distribution. Instead, we use it to sample a new example, and use it to incrementally update the original variational distribution. The expectation of the new example will be the same as the updated variational distribution.  4 Making Detailed Correspondence  To connect LNP neuron model in Eq. (5) with semi-stochastic inference algorithm in Eq. (10), several issues needs to be resolved:  1. LNP neurons don’t have the bias terms (such as biu), although their nonlinear transform  function may have constant shift identical to all neurons.  2. In Eq. (10), two Linear-Nonlinear procedures in (a)(c) corresponds to only one sampling  step in (b). This is an architectural difference.  3. Biological neurons can only be excitatory or inhibitory.  4. Biological neurons carry out the updates all in parallel.  To resolve issue 1, we notice that when the weights V and biases c in Eq. (6) are turned into W and b in Eq. (10), for ﬁxed i, j ∈ I, ﬁxed u, v ∈ {A, B}, and any constant C ∈ R the following  5  operation is invariant to the inference algorithm in Eq. (10) since θ(t)  iA and θ(t)  iB sum up to one.  iu ← bold bnew iu + C iu,jv ← W old W new W new iu,j¯v ← W old  iu,jv + C iu,j¯v + C  (11)     For this reason, we can always modify the algorithm in Eq. (10) to make any bias we want. In our experiments, when we want to remove a bias biu, we subtract biu/|M (i) | from both Wiu,jv and Wiu,j¯v for each j ∈ M (i). To resolve issue 2, we propose the “Event-Network”. We ﬁrst split every sampling step (b) in Eq. (10) into two, namely [[y(t) iB ). Now we can allocate two neurons to take care of the two parts in all three steps (a)(b)(c). Units of the resulted network no longer corresponds to random variables, they are now probabilistic events. When we do this, there is no guarantee that θ(t) iB will sum up to 1, but this will be approximately true since the weights Wiu,jv’s are compatible with each other. Whether this results in meaningful inference algorithms needs further investigation. We show veriﬁcation in our experiment part.  i = B]] ∼ Bernoulli(φ(t)  i = A]] ∼ Bernoulli(φ(t)  iA ) and [[y(t)  iA and θ(t)  To resolve issue 3, once we have a network containing neurons with both excitatory and inhibitory outgoing synapses, we can duplicate each neuron into two, both have the same incoming synapses with the original neural efﬁcacy. Then one of them take all positive outgoing synapses before split- ting, and the other take all negative ones. Both of this and the last modiﬁcation of the network are exactly invariant for variational inference but may only be approximately true for semi-stochastic inference. We show experimental veriﬁcation for this modiﬁcation as well.  iu = [[y(t)  If after resolving all these issues we denote x(t) i = u]] for u ∈ {A, B}, and re-index every- thing, the resulted algorithm will be exactly the same as Eq. (5) if executed in parallel. Parallelism is not only an issue in semi-stochastic inference. It is noticed in [33] that when adjacent random variables of a Boltzmann machine are updated in parallel in Gibbs sampling, the sample sequence does not converge to the right distribution. This issue is also raised very often recently (e.g. [38]). Variational inference is a ﬁxed point iteration. So it may converge to the same answer when up- dated all in parallel, but we do observed experimentally that the variational parameters converges to the ﬁxed point after severe oscillation. Whether the slowing down version alleviates this problem requires further investigation2.  5 Experimental Veriﬁcation  Figure 2: Samples of reconstruction results (selected randomly). Rows from top to bottom: original images; VarO; SemiO; SemiEN; SemiB; SemiU.  In this section, we focus on illustrating that the semi-stochastic inference, as well as all the modi- ﬁcations in section 4, are valid Bayesian inference algorithms, which behave similar to variational inference. In all experiments, we set ǫ(k) as exp(−k/2) normalized to have summation 1 over its domain k ∈ {1, ..., 30}. All inferences are carried out in synchronized full parallelism. To avoid the complication of learning deep Boltzmann machines, we take the learned Boltzmann machine from code in [35]. Since we focus on inference on Boltzmann machine, we take the learned model before back-propagation. The Boltzmann machine we use consists of three layers, the ﬁrst  2We observe that different choices of ǫ functions result in different smoothness of the trajectories, but it’s  not clear how. We leave this as future work.  6  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  ) t i( θ   s e u a v   n o  l  i t  a v i t c a  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  ) t i( θ   s e u a v    l  n o  i t  a v i t c a  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  ) t i( θ   s e u a v    l  n o  i t  a v i t c a  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  ) t i( θ   s e u a v    l  n o  i t  a v i t c a  0  0  20  40 60 # iterations  80  100  0  0  20  40 60 # iterations  80  100  0  0  20  40 60 # iterations  80  100  0  0  20  40 60 # iterations  80  100  Figure 3: Sample trajectory of θ(t) line is for SemiO, and Red line is the moving average of Green line over window size 30.  in VarO and SemiO. In all above, Blue line is for VarO, Green  i  layer for input image consists of 784 units, the second consists of 500 units and the third consists of 1000 units. In the following, we refer to θ(t) To see the effect of modiﬁcations in section 4, we show results of semi-stochastic inference after each of them. Note that variational inference is exactly invariant on all these modiﬁcations, hence there no need to verify them. We use the following abbreviations in this part:  in both algorithms as the activation of neuron i.  i  • VarO: variational inference applied on the original deep Boltzmann machine.  • SemiO: semi-stochastic inference applied on the original deep Boltzmann machine.  • SemiEN: Same as SemiO except each sampling is duplicated and put on two neurons.  • SemiB: Same as SemiEN except biases are removed by transforming the network.  • SemiU: Same as SemiB except each neuron is duplicated into two, such that no neuron  contains out-going synapses with different signs.  The experiment we did is similar to the reconstruction experiment in [7]. We do the following: (i) plug in an image on the input layer, (ii) apply the inference algorithm, (iii) frozen the activations on the topmost layer and round up to 0 or 1, (iv) set the topmost layer as observed and other layers as hidden, (v) infer back, (vi) read off the image as activations on the input layer. Both (ii) and (v) are randomly initialized and have 100 iterations. For semi-stochastic inference, in steps (iii) and (vi) the activations are taken as averages over the last 50 iterations, while for variational inference we simply use the last activation of each neuron. This is because semi-stochastic inference has ran- dom convergence, activations approach some ﬁxed point with random perturbation (see justiﬁcation later). Some reconstruction examples are shown in Fig. 2. Each algorithm has good and bad cases, also note that because of the randomness, even for identical input image and identical initialization, semi-stochastic inference may get different results in different trials. In ﬁgure 3, we showed sample trajectory of activations in step (ii) of VarO and SemiO for the same variables. Each trajectory are those θ(t) ’s of one neuron over all iterations. We see that activations in variational inference converge to a stable solution with oscillation before convergence, while semi-stochastic inference has their moving average converges to some similar values with random perturbation. Figure 5(a) is a scatter plot of mean of trajectories vs. the corresponding converged activation in variational inference, extracted from 2000 trajectories randomly chosen from 50 trials. We also observed that when the mean is close to extreme values (such as 0 and 1), the standard variance will be smaller, as can be found in ﬁgure 3 and more clearly in ﬁgure 5(b). This means that semi-stochastic inference converges randomly to the variational inference solution, with more conﬁdent variables having less random perturbation.  i  Finally, we investigated on how well the splitting in section 4 preserves identity relationships that are expected on SemiU. In particular, if two neurons are resulted from splitting one sampling into two in resolving issue 2, their summed activation should be close to 1. If two neurons are resulted from splitting one neuron into two in resolving issue 3, their activation should be equal. Figure 4 illustrated three samples and histogram on 2000 random trajectories are given in ﬁgure 5(c)(d).  7  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  s e u a v    l  n o  i t  a v i t c a  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  s e u a v    l  n o  i t  a v i t c a  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  s e u a v    l  n o  i t  a v i t c a  0  0  20  40  60  # iterations (a)  80  100  120  0  0  20  40  60  # iterations (b)  80  100  120  0  0  20  40  80  100  120  60  # iterations (c)  Figure 4: Sample trajectory from SemiU. Each plot shows four neurons resulted from two splits on a single neuron. Thick dash lines: neurons with positive outgoing weights. Thin solid lines: corresponding neurons with negative outgoing weights. Blue and Red lines are for the two neurons from splitting one sampling into two. Green line is the average of Blue and Red. Ideally, lines with same color should be equal, and green lines should be at 0.5.  i  O m e S n     i   s e u a v    l  n o  i t  a v i t c a     n a e m  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  0.2  0.4  0.6  0.8  converged activation values in VarO  l  i  O m e S   n i   s e u a v   n o i t a v i t c a   f o   d t s  0.35  0.3  0.25  0.2  0.15  0.1  0.05  0  0  1  0.2  0.4  0.6  0.8  mean activation values in SemiO  (a)  (b)  1000  800  600  400  200  1  0 0  0.2  0.6  0.8  0.4 (c)  1000  800  600  400  200  0 0  1  0.5 (d)  (a) means of trajectories in SemiO vs. converged Figure 5: Some statistics of the algorithms. activation values in VarO for the same hidden variables. (b) standard deviation vs. mean, from the trajectories in SemiO. (c) histogram of standard deviation(red line + blue line - 1.0). (d) histogram of standard deviation(dash line - solid line). (Here red, blue, dash, solid lines refer to ﬁgure 4.)  6 Conclusions and Discussions  We pointed out the stochastic inference nature of the LNP neuron models, which may serve as an interpretation of neural coding. The stochastic convergence seems reasonable, e.g., when we see ambiguous images, our perception will switch back and forth between plausible explanations.  There are many behavioral experiments showing statistical optimality of perception and learning (e.g. [39]) and many calls for neuronal modeling that achieves this optimality (e.g. [40]). Arguably, the mode-seeking nature of variational inference [41] make it natural for interpreting perception. Since Boltzmann machines with hidden variables are universal approximators (e.g. [42]), modeling knowledge representation by that is safe if the world can be approximately binarized. Yet we only show that with particular choices of weights, neurons are capable of representing a Boltzmann ma- chine and carry out inference. Then the question is what does this do if the network is a recurrent neural network in general. Real biological neural networks may be learned by taking the Boltz- mann machine representation and undergoing discriminative training or reinforcement learning to optimize performance of inference directly (especially plausible when time dimension is involved), which yields a model not necessarily convey consistent probabilistic semantics but may do so in the limit. The particular form of nonlinearity in biological neurons may also result from such opti- mization. Another issue is the low average activity in biological neurons. It may be a certain form of sparse coding [4] on top of inference so that optimizing such inference procedure will not yield degenerate solution. In this case, likelihood-based learning such as contrastive divergence may still be applicable as a further reﬁnement.  Acknowledgement  I want to thank Geoffrey Hinton, Mikhail Belkin, DeLiang Wang, Brian Kulis for helpful discussion and The Ohio Supercomputer Center for providing computation resource.  8  References  [1] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organiza-  tion in the brain. pages 89–114, 1988.  [2] David E Rumelhart, James L Mcclelland, and Geoffrey E Hinton. Parallel Distributed Pro-  cessing: Explorations in the Microstructure of Cognition. MIT Press, August 1986.  [3] Honglak Lee, Chaitanya Ekanadham, and Andrew Y. Ng. Sparse deep belief net model for visual area V2. In Advances in Neural Information Processing Systems 20, pages 873–880. 2008.  [4] Bruno. A. Olshausen and David J. Field. Emergence of simple-cell receptive ﬁeld properties  by learning a sparse code for natural images. Nature, 381:607–609, 1996.  [5] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. Efﬁcient sparse coding algo-  rithms. In Advances in Neural Information Processing Systems 19, pages 801–808. 2007.  [6] Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep  belief nets. Neural Comput., 18(7):1527–1554, July 2006.  [7] Geoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with  neural networks. Science, 313(5786):504–507, 2006.  [8] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise train- ing of deep networks. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 153–160. MIT Press, Cambridge, MA, 2007.  [9] Quoc Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg Corrado, Jeff Dean, and Andrew Ng. Building high-level features using large scale unsupervised learning. In Proceedings of the Twenty-Ninth International Conference on Machine Learning, 2012.  [10] Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 609–616, New York, NY, USA, 2009. ACM.  [11] Andreas V. M. Herz, Tim Gollisch, Christian K. Machens, and Dieter Jaeger. Modelling single- neuron dynamics and computations: A balance of detail and abstraction. Science, 314:80–84, October 2006.  [12] Wulfram Gerstner and Werner M. Kistler. Spiking Neuron Models: Single Neurons, Popula-  tions, Plasticity. Cambridge University Press, 1 edition, 2002.  [13] Nicolas Fourcaud-Trocm, David Hansel, Carl van Vreeswijk, and Nicolas Brunel. How spike generation mechanisms determine the neuronal response to ﬂuctuating inputs. The Journal of Neuroscience, 23(37):11628–11640, 2003.  [14] E. J. Chichilnisky. A simple white noise analysis of neuronal light responses. Network (Bristol,  England), 12(2):199–213, May 2001.  [15] Eero P. Simoncelli, Liam Paninski, Jonathan Pillow, and Odelia Schwartz. Characterization of  neural responses with stochastic stimuli. The cognitive neurosciences, 2004.  [16] Liam Paninski. Maximum likelihood estimation of cascade point-process neural encoding  models. Network: Comput. Neural Syst., 15(04):243–262, November 2004.  [17] Srdjan Ostojic and Nicolas Brunel. From spiking neuron models to Linear-Nonlinear models.  PLoS Comput Biol, 7(1):e1001056+, January 2011.  [18] David J. Tolhurst, J. A. Movshon, and A. F. Dean. The statistical reliability of signals in single  neurons in cat and monkey visual cortex. Vision Research, 23(8):775–785, 1983.  [19] Wei J. Ma, Jeffrey M. Beck, Peter E. Latham, and Alexandre Pouget. Bayesian inference with  probabilistic population codes. Nature Neuroscience, 9(11):1432–1438, October 2006.  [20] Idan Segev. What do dendrites and their synapses tell the neuron? Journal of Neurophysiology,  95(3):1295–1297, March 2006.  [21] Idan Segev and Michael London. Dendritic processing.  In Michael A. Arbib, editor, The handbook of brain theory and neural networks, pages 324–332. MIT Press, Cambridge, MA, USA, 2003.  9  [22] Roberto Araya, Kenneth B. Eisenthal, and Rafael Yuste. Dendritic spines linearize the summa- tion of excitatory potentials. Proceedings of the National Academy of Sciences of the United States of America, 103(49):pp. 18799–18804, 2006.  [23] W. Gerstner. Spike-response model. Scholarpedia, 3(12):1343, 2008. [24] Alan L. Hodgkin and Andrew F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500– 544, 1952.  [25] Ruﬁn Vanrullen and Simon J. Thorpe. The time course of visual processing: From early  perception to decision-making. J. Cognitive Neuroscience, 13(4):454–461, May 2001.  [26] Lucien le Cam. An approximation theorem for the poisson binomial distribution. Paciﬁc  Journal of Mathematics, 10:1181–1197, 1960.  [27] Wilfrid Rall. Distinguishing theoretical synaptic potentials computed for different soma- Journal of neurophysiology, 30(5):1138–1168,  dendritic distributions of synaptic input. September 1967.  [28] Danilo J. Rezende, Daan Wierstra, and Wulfram Gerstner. Variational learning for recurrent spiking networks. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Wein- berger, editors, Advances in Neural Information Processing Systems 24, pages 136–144. 2011. [29] Ruslan Salakhutdinov and Geoffrey Hinton. Replicated softmax: an undirected topic model. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1607–1614. 2009.  [30] David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A Learning Algorithm for  Boltzmann Machines. Cognitive Science 9, pages 147–169, 1985.  [31] Ernst Ising. Beitrag zur theorie des ferromagnetismus. Zeitschrift fr Physik A Hadrons and  Nuclei, 31:253–258, 1925. 10.1007/BF02980577.  [32] Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian IEEE Transactions on Pattern Analysis and Machine Intelligence,  restoration of images. 6:721–741, 1984.  [33] Emile Aarts and Jan Korst. Simulated annealing and Boltzmann machines: a stochastic ap- proach to combinatorial optimization and neural computing. John Wiley & Sons, Inc., New York, NY, USA, 1989.  [34] Yee Whye Teh, David Newman, and Max Welling. A collapsed variational bayesian inference algorithm for latent dirichlet allocation. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 1353–1360. MIT Press, Cam- bridge, MA, 2007.  [35] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. Artiﬁcial Intelligence,  5(2):448C455, 2009.  [36] Erich Leo Lehmann. Theory of point estimation / E.L. Lehmann. John Wiley, New York,  Singapore, 1983.  [37] Radford M. Neal and Geoffrey E. Hinton. A view of the EM algorithm that justiﬁes incremen-  tal, sparse, and other variants. In Learning in graphical models, pages 355–368. 1999.  [38] Arthur Gretton Carlos Guestrin Joseph E. Gonzalez, Yucheng Low. Parallel gibbs sampling: From colored ﬁelds to thin junction trees. In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 2011, pages 324–332, 2011.  [39] Tianming Yang and Michael N. Shadlen. Probabilistic reasoning by neurons. Nature, 2007. [40] J´ozsef Fiser, Pietro Berkes, Gerg˝o Orb´an, and M´at´e Lengyel. Statistically optimal percep- tion and learning: from behavior to neural representations. Trends in Cognitive Sciences, 14(3):119–130, March 2010.  [41] Thomas Minka. Divergence measures and message passing. Technical report, Microsoft Re-  search, 2005.  [42] Nicolas Le Roux and Yoshua Bengio. Representational power of restricted boltzmann ma-  chines and deep belief networks. Neural Comput., 20(6):1631–1649, June 2008.  10  Supplemental Material  Several Justiﬁcations and Open Questions  In this supplemental material, we deal with two issues that are not made clear in the main text: (1) we give the details of discrete time approximation, which eventually leads to the inference algorithm; (2) we present preliminary results on stochastic stability of the neuron model. After that, we also point out several open questions related to the Linear-Nonlinear-Poisson (LNP) neuron model and its interpretation of Bayesian inference, with possible directions for solving these problems.  For now, we focus on the neuron model with a Poisson Point Process (PPP) when there is no linear convolution with function D in the main text and the dendritic integration is linear, as the following  λi (τ ) = σ(cid:16)Pj∈M(i) Wij · (Xj ∗ α) (τ ) + ei(cid:17) ,  Xi (τ ) ∼ PoissonPointProcess (λi (τ )) ,  ∀i ∈ I  (1)  Here τ ∈ R is the continuous time index, which has unit millisecond. An external input term ei is introduced so that we don’t need to explicitly distinguish visible and hidden units. Every ei is constant over time. For example, ei = 0 means the neuron corresponds to a hidden unit, while a very positive and a very negative ei correspond to a visible unit with observation 1 and 0 respectively. We also assume that τ = 0 is the beginning time and each λi (0) is chosen by the user  1 The Stochastic Differential Equation Formulation  In this section, we reformulate Eq. (1) as a stochastic differential equation, which is for the ease of the later parts.  If we choose the parametrization of α function as the exponential function α (τ ) = a · exp (−a · τ ), and deﬁne Yi (τ ) , (Xi ∗ α) (τ ), then Yi is the solution of the differential equation as the following.  dYi dτ  = −a · Yi + a · Xi  (2)  Thus we can rewrite Eq. (1) in terms of Y = (Yi)i∈I, and we get a system of coupled stochastic differential equations (SDE) as the following  3 1 0 2     n a J    7 2      ] I  A . s c [      3 v 2 4 4 8  .  0 1 2 1 : v i X r a  dY  dτ = −a · (Y − λ) + a · (X − λ) , where  ∀i ∈ I, ( λi (τ ) = σ(cid:16)Pj∈M(i) Wij · Yj (τ ) + ei(cid:17)  Xi (τ ) ∼ PoissonPointProcess (λi)  (3)  The right hand side of Eq. (3) contains two terms, the deterministic term −a · (Y − λ) and the stochastic term a · (X − λ). The deterministic term reaches zero when Y = λ, and thus is the equilibrium of the (recurrent) neural network, if there is one. The stochastic part, roughly speaking, has short time integral close to zero, which is referred to as random perturbation. The deterministic counterpart of Eq. (3) is deﬁned as the following.  dY dτ  = −a · (Y − λ)  (4)  In the literature of stochastic differential equations, the stochastic stability of Eq. (3) is deﬁned in many sense analogous to the Lyapunov stability of Eq. (4). For example, in [1] the stability  1  under small random perturbation is introduced, which is deﬁned as a capability of the deterministic system, in the sense that as long as the random force can be made sufﬁciently small, the combined stochastic system will exhibit certain stability, although the requirement imposed upon the strength of the random force may not satisfy It is likely that Eq. (4) satisﬁes this stability property. We haven’t obtained a rigorous proof yet, but in the following part, we give an analysis of this kind of stochastic stability in the discrete time case. We ﬁrst give the detail about how the discrete case is achieved and then proceed by analyzing it.  2 Details of Discrete Time Approximation  In this section, we give the details about how the PPP formulation is discretized into the Bernoulli process formulation, and how it is related to the ﬁxed-point iteration procedure, which are not made clear in the main text.  Let t ∈ Z be the discrete time index such that discrete time t corresponds to continuous time τ = tǫ, where ǫ ∈ R+ is the length of a small time step. The discrete time approximation of PPP in Eq. (3) is as the following.  x(t)  i ∼ Bernoulli(cid:16)ǫ · λ(t)  i (cid:17) , where λ(t)  i =  1  ǫ Z[tǫ,tǫ+ǫ)  λi (τ ) dτ  (5)  By LeCam’s theorem [2], it can be shown that when ǫ → 0, for any ﬁxed time interval [τ1, τ2), the sum of x(t) fall in that interval converges in distribution to a Poisson distribution with rate i λi (τ ) dτ . That means the non-homogeneous Bernoulli process approaches non-homogeneous as the following (in which we ignored the notation issue regarding the starting  R τ2  τ1 PPP We deﬁne y(t) point)  i  With this deﬁnition, we have the following approximation formula  y(t)  i =X∞  k=0  α(k)x(t−1−k)  i  , where α(k) = a · (1 − aǫ)k  α(τ /ǫ) = a · (1 − aǫ)τ /ǫ → a · exp (−aτ ) = α (τ ) , as ǫ → 0  For this reason, y(t) an approximation of PPP. Eq. (6) can also be rewritten recursively as the following.  i deﬁned in Eq. (6) is an approximation of Yi (tǫ) when the Bernoulli process is  y(t+1) i  = (1 − aǫ) · y(t)  i + a · x(t)  i  (8)  In addition, for τ > 0, each Yi (τ ) is right continuous with discontinuity located only at the spiking points indicated by Xi (τ ), and so is λi (τ ). Thus when ǫ → 0, λ(⌈τ /ǫ⌉) → λi (τ ), where ⌈τ /ǫ⌉ is the smallest integer that is larger than τ /ǫ. For this reason, λ(t) is an approximation of λi (tǫ). Since i is approximately a function of y(t) approximates Yi (tǫ), λ(t) λi (tǫ) is a function of Yi (tǫ) and y(t) . i i As a conclusion, the discrete time approximation of Eq. (3) is the following, where the ﬁrst line is derived from Eq. (8). The approximation gets better (closer to the continuous time counterpart) when ǫ → 0  i  i  y(t+1)−y(t)  λ(t)  ǫ  = −a ·(cid:0)y(t) − λ(t)(cid:1) + a ·(cid:0)x(t)/ǫ − λ(t)(cid:1) , where i (cid:17) i ∼ Bernoulli(cid:16)ǫ · λ(t)  i (cid:17) , x(t)  i = σ(cid:16)Pj∈M(i) Wij · y(t)  j + e(t)  We can also replace the ﬁrst formula in Eq. (9) by Eq. (6). The resulted formula matches the neuron model in our main text. Again we can deﬁne the deterministic counterpart of Eq. (9) as the following.  (6)  (7)  (9)  (10)  y(t+1) − y(t)  ǫ  = −a ·(cid:16)y(t) − λ(t)(cid:17)  Note that when aǫ = 1, Eq. (10) is exactly the forward evaluation step of (recurrent) neural net- works. When aǫ < 1, it is the momentum version of the forward step. In case the neural network represents the variational inference algorithm over Boltzmann machines, i.e., weights are symmet- ric, the forward step is guaranteed to converge to a local optimum, no matter where the starting point is.  2  3 Stochastic Stability of the Neuron Model  In this section, we illustrate the stochastic stability of Eq. (9). Eventually, the stability of the stochastic system will rely on the stability of the deterministic system as in Eq. (10). In general it is not clear when the dynamical system, either discrete time in Eq. (10) or continuous time in Eq. (4) has stable ﬁxed-point. Several known results are listed, for example, in [3]. An important case, as what our main text was depending on is that, when the neural weights are symmetric, the network converges no matter where it starts. In case convergence is guaranteed with only one ﬁxed-point, the corresponding dynamical system can be regarded as dissipative with its ﬁxed-point being globally asymptotically stable. In this section, we denote y∗ as the position in the unit hypercube [0, 1]n that satisﬁes the following.  (11) And unless speciﬁed otherwise, it is assumed that there is only one such ﬁxed-point. We further assume that there is a Lyapunov function V of the continuous system, such that for y ∈ [0, 1]n, we have  y∗ = σ (Wy∗ + e)  Here λ is deﬁned as in Eq. (9). We focus on the case when ǫ = 1 in which case Eq. (9) can be rewritten as the following  V (y) = 0, iff y = y∗ V (y) > 0, if y ∈ [0, 1]n \ {y∗}  D ∂V ∂y , −y + λE 6 0, with eq. only at y∗  y(t+1) = (1 − a) · y(t) + a · x(t) x(t)  i (cid:17) i ∼ Bernoulli(cid:16)λ(t)  (12)  (13)  Given the current position y(t) ∈ [0, 1]n within a unit hypercube, λ(t) as deﬁned in Eq. (9) deﬁnes the tentative target where y(t) will evolve towards, in case of a deterministic system. While in Eq. (13), the new position y(t+1) is expressed as a convex combination of the current position y(t) and the target λ(t), with combination ratios 1−a and a respectively. The stability of the stochastic system depends on two factors: the value a and where the ﬁxed-point y∗ is located. When a is closer to 1, y(t+1) has most of its part determined by the random sample rather than the last position y(t). Thus y along with time t evolves with large variation no matter where y∗ is located. In particular, if a > 0.5, we easily have  y(t) i ∈ [0, 1 − a] ∪ [a, 1] , ∀i, t  (14) Thus no matter where y∗ is located, there is a certain region (1 − a, a)n centered at the center of the hypercube where y(t) cannot reach at any time t. In case y∗ is inside that region, any trajectory will be apart from y∗ with at least a ﬁxed distance, which means that none of the stability results 6 a, the next position y(t+1) will be within the range of a small hypercube of length 2a centered at y(t), and the Lyapunov function V in this small hypercube will be approximately linear. We denote the linearized function V passing through y(t) as ¯V . Since the expectation of the next step is better in terms of the Lyapunov function value, as the following  in [1] holds true. In another extreme, when a is very small, since we have that(cid:12)(cid:12)(cid:12)  y(t+1) i  − y(t)  (cid:12)(cid:12)(cid:12)  i  Ehy(t+1)|y(t)i = (1 − a) · y(t) + a · λ(t) = y(t) − a ·(cid:16)y(t) − λ(t)(cid:17)  When a is small, this approximately matches the evolution rule in the deterministic system, which by assumption in Eq. (12), guarantees to decrease the Lyapunov function value. Thus we have  (15)  Eh ¯V (cid:16)y(t+1)(cid:17) |y(t)i < ¯V (cid:16)y(t)(cid:17)  (16)  When the linear approximation is good (in case a → 0), the above relationship with respect to ¯V also holds for V . This means that the trajectory, although being stochastic, will almost deterministically converge to the ﬁxed-point and stay around there jumping randomly with small perturbation. This phenomenon can be easily seen by numerical simulation.  In particular, we found an interesting example. Suppose the parameters of the network are as the following.  W =(cid:20) 0  15  20  0 (cid:21) , e =(cid:20) −15  −10 (cid:21) , a = 0.5, ǫ = 1  (17)  3  Figure. 1 illustrates the deterministic dynamical system deﬁned by Eq. (17). Table 1 (a) is ky − λk2 plotted as color for each y, with red having high value and blue having low value. Table 1 (b) is the vector ﬁeld − (y − λ) showing how the deterministic system will evolve. The dynamical system has two ﬁxed points y∗ 2 = (0.0031e − 4, 0.4540e − 4). Starting from most of the positions in the unit hypercube, the deterministic system will go to the stronger ﬁxed-point 2. However, in the stochastic system as in Eq. (13), there are two situations. Most of the time the y∗ trajectory goes to y∗ 1 and stays there for a while, but eventually jumps to y∗ 2. It could takes long before such trajectory jumps back to y∗  2 and stays there great stability. For very rare cases the trajectory goes to y∗  2, yet in our more than 10000 test trials, no single trajectory stays in y∗  1 = (0.9922, 0.9925), and y∗  1 forever.  1  0.8  0.6  0.4  0.2  0  0  0.2  0.4  0.6  0.8  1  1  0.8  0.6  0.4  0.2  0  0  0.2  0.4  0.6  0.8  1  (a)  (b)  Figure 1: (a) Square of norm of r.h.s of Eq. (9); (b) The vector ﬁeld of r.h.s of Eq. (9).  The above example has very stable converged point, which is typical when the deterministic system has one or more stable ﬁxed-points that are very strong (i.e., located near the vertices of the hyper- cube). In case the ﬁxed-point is not very strong, the situation is different. For example, if when both kWk and kek are small, the following is also very small.  (cid:13)(cid:13)(cid:13) Wy(t) + e(cid:13)(cid:13)(cid:13)  6 kWk ·(cid:13)(cid:13)(cid:13)  y(t)(cid:13)(cid:13)(cid:13)  + kek  (18)  In this case, λ(t) will have its entries close to 0.5 and thus each entry of x(t) will be very random. Making y(t) evolves in changing directions. The stability of y(t) around y∗ still depends on how large a is. In case the ﬁxed-point y∗ has different strength in each of its entries, we found experimen- tally that those stronger entries have more stable converged value, which is simply a combination of what we illustrated in this section.  Experiments in our main text, especially Figure 3 and Figure 5 (a) (b), demonstrated similar stability in this section.  4 Open Questions  Presented in the following are several questions related to Bayesian inference by neural networks.  4.1 Conditional Speciﬁcation of Probability Models  The connection between neuron models and Bayesian inference algorithms as indicated in our main text is based on the observation that the neural network structure, instead of representing the proba- bility structure itself, can be used to represent the “information ﬂow” of a certain message passing algorithm on a probability model. In this case the probability model refers to the Boltzmann ma- chine and the message-passing algorithm refers to variational inference and Gibbs sampling. As mentioned in [4], many times variational inference has the mean-ﬁeld form of Gibbs sampling, thus these two algorithms may share the same information ﬂow structure. However, not necessarily all message-passing algorithms can be directly encoded in neural networks. An important requirement for doing so is that the message each neuron sends to its downstream neurons must be the same.  4  For example, belief-propagation (see [5]) and expectation propagation (see [6]) does not satisfy this requirement, so it’s not clear at this point whether these algorithms can be directly encoded in neural networks.  Assuming an inference algorithm can indeed be encoded in neural networks, the next question is then, given an arbitrary neural network, when does it correspond to a valid joint probability model with the given inference algorithm? This is important, for example, if we are capable of carrying out both inference and learning on the neural networks without assuming the joint probability model, we can simply carry out that algorithm (which should be cheaper than actually learn probability semantics), and very probably the learned model will exhibit correct probability semantics in the limit, in the sense that the network structure and parameters under learning, converge to those that do represent a joint probability model. For example, the proof in [7] can be used to show that if one carry out the generalized backpropagation algorithm [8] on Boltzmann machines, and preserve the symmetric weights constraint, then error-driven learning can yield models with correct proba- bility semantics, but what if the algorithm does not preserve symmetricity, such as the Generalized Recirculation Algorithm [9].  In the context of this paper, the question is that, suppose the network structure is to represent local conditional probability of a random variable, when does the speciﬁcation of a collection of local conditional probability non-conﬂicting, and imply a uniquely determined joint probability model. This is termed as the “conditional speciﬁcation model”, see [10] for a reference. By the methodology in that book, it can be easily seen that, assuming each neuron corresponds to an individual binary random variable and the input set of neurons represent the Markov blanket, the only way to let the neural network imply a joint probability model which is compatible to all speciﬁed conditional probabilities is that the weights must be symmetric. In another way, the joint probability model that the conditionals of a neural network represent has to be a Boltzmann machine. However, if we allow a many-to-one mapping, i.e., different neurons can map to the same binary random variable, the situation is different. As our main text implies, a neural network without symmetric weights can still represent a Boltzmann machine if two neurons specify the same binary random variable. Furthermore, if we allow neurons to represent functions of random variables, or allow some hidden neurons not representing any random variable, such that their purpose is only for the supporting of the information ﬂow, then much more possibilities can happen. How do we even deﬁne this question rigorously?  4.2 Universal Approximation by Distributions over Binaries  Even though there are different ways of utilizing continuous variables in neural networks, it may still be preferable to have all variables to be encoded in binary. Since Boltzmann machines are universal approximator of distributions over binary variables [11], the natural question that follows is whether it is also a good approximation to distributions over non-binary variables. If so, it would be sufﬁcient for representing knowledge when developing intelligent systems.  The question is twofold: (1) Whether a continuous variable can be efﬁciently encoded by a collection of binary variables, with the approximation error being controllable. (2) Whether distributions over the continuous variables can be efﬁciently approximated by distributions over the encoded binary variables.  When the continuous numbers are located on the unit interval, its binary approximation is natural. We can simply take the binary digit representation and truncate it to have the ﬁrst set of digits. This approach may be extended to domains where Fourier series functions can be well deﬁned, in which case the sign of the Fourier series function evaluations can be used as a good binary approximation. In case the continuous domain does not have Fourier series deﬁned or such approximation is inefﬁ- cient, we may be able to look at the eigenfunctions of the Laplacian operator. As indicated in [12], there are relationships between Fourier series and eigenfunctions of Laplacian operator. This may provide a uniﬁed theoretical framework for investigating on binary approximation.  References  [1] Rafail Khasminskii. Stochastic stability of differential equations, volume 66 of Stochastic  Modeling and Applied Probability. Springer, 2nd edition, 2011.  5  [2] Lucien le Cam. An approximation theorem for the poisson binomial distribution. Paciﬁc  Journal of Mathematics, 10:1181–1197, 1960.  [3] Barak A. Pearlmutter. Dynamic recurrent neural networks. 1990. [4] Yee Whye Teh, David Newman, and Max Welling. A collapsed variational bayesian inference algorithm for latent dirichlet allocation. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 1353–1360. MIT Press, Cam- bridge, MA, 2007.  [5] Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference.  Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988.  [6] Thomas Minka. Divergence measures and message passing. Technical report, Microsoft Re-  search, 2005.  [7] Joshua V. Dillon and Guy Lebanon. Stochastic composite likelihood. J. Mach. Learn. Res.,  11:2597–2633, December 2010.  [8] Fernando J. Pineda. Generalization of back-propagation to recurrent neural networks. Phys.  Rev. Lett., 59:2229–2232, Nov 1987.  [9] Randall C. O’Reilly. Biologically plausible error-driven learning using local activation differ-  ences: The generalized recirculation algorithm. Neural Comput., 8(5):895–938, July 1996.  [10] B.C. Arnold, E. Castillo, and J.M. Sarabia. Conditional speciﬁcation of statistical models.  Springer, 1999.  [11] Nicolas Le Roux and Yoshua Bengio. Representational power of restricted boltzmann ma-  chines and deep belief networks. Neural Comput., 20(6):1631–1649, June 2008.  [12] Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, fourier transform,  and learnability. J. ACM, 40:607–620, July 1993.  6  ","One conjecture in both deep learning and classical connectionist viewpoint isthat the biological brain implements certain kinds of deep networks as itsback-end. However, to our knowledge, a detailed correspondence has not yet beenset up, which is important if we want to bridge between neuroscience andmachine learning. Recent researches emphasized the biological plausibility ofLinear-Nonlinear-Poisson (LNP) neuron model. We show that with neurallyplausible settings, the whole network is capable of representing any Boltzmannmachine and performing a semi-stochastic Bayesian inference algorithm lyingbetween Gibbs sampling and variational inference."
1301.3323,2013,Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences  ,"['Sainbayar Sukhbaatar', 'Takaki Makino', 'Kazuyuki Aihara']",https://arxiv.org/pdf/1301.3323.pdf,"3 1 0 2    r a     M 8 1      ]  V C . s c [      4 v 3 2 3 3  .  1 0 3 1 : v i X r a  Auto-pooling: Learning to Improve Invariance of  Image Features from Image Sequences  Sainbayar Sukhbaatar  Dept. of Mathematical Informatics, IST  The University of Tokyo Tokyo 116-8656, Japan  Takaki Makino  Kazuyuki Aihara  Institute of Industrial Science  The University of Tokyo Tokyo 153-8505, Japan  sainaa@sat.t.u-tokyo.ac.jp  {mak,aihara}@sat.t.u-tokyo.ac.jp  Abstract  Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spa- tial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image se- quences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images ex- tracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed tra- ditional spatial pooling on an image classiﬁcation task, even though it does not use the spatial topology of features.  1 Introduction  The main difﬁculty of object recognition is that the appearance of an object can change in complex ways. To build a robust computer vision, one needs representations that are invariant to various transformations. The concept of invariant features dates back to Hubel and Wiesel’s seminal work [7], in which cells in a cat’s visual cortex are studied. They found two types of cells: simple cells that responded to a speciﬁc pattern at a speciﬁc location, and complex cells that showed more invariance to location and orientation.  Inspired by simple and complex cells, the spatial pooling step is introduced to computer vision architectures along with the convolution step [4, 12, 13]. In the convolution step, the same feature is applied to different locations. Then in the pooling step, responses from nearby locations are pooled together (typically with a sum or a max operation) to create invariance to small spatial shifting. However, spatial pooling only works with convolutional models. Also, spatial pooling only improves the invariance to spatial shifting.  An ideal pooling should make features invariant to major types of transformations that appear in the nature. For example, to distinguish people from other objects, one need a representation that is invariant to various transformations of the human body. The complexity of such transformations creates the necessity of more adaptive pooling that does not rely on manual ﬁxed conﬁgurations. One promising way to obtain such an adaptive pooling is an unsupervised learning approach.  In recent years, more adaptive spatial pooling methods have been proposed. Jia et al. [9] showed that it is possible to learn custom pooling regions specialized for a given classiﬁcation task. The training starts with many pooling region candidates, but only a few of them are used in the ﬁnal classiﬁcation. This selection of pooling regions is achieved by supervised learning incorporated into the training of a classiﬁer. Although this method learns pooling regions from data, it is still restricted  1  pooled  representation  low-level features  spatial pooling  auto-pooling  Figure 1: In spatial pooling, low-level features are “hard” clustered by their spatial organization. Auto-pooling, on the other hand, learns “soft” clustering of low-level features from image sequences.  to spatial shifting. Further, it is not suited for deep learning, where lower layers are trained in an unsupervised way.  Another method that improves spatial pooling is proposed by Coates and Ng [3], in which local features are clustered by their similarity. A similarity metric between features is deﬁned from their energy correlations. Then, nearby features from the same cluster are pooled together to create rota- tion invariance. However, the invariance to spatial shifting is still achieved through the same spatial pooling, which restricts this model to convolutional models.  Beside from spatial pooling, there are methods [8, 10, 16] that create invariance by placing fea- tures on a two-dimensional topographic map. During training, nearby features are constrained to be similar to each other. Then, invariant representations are achieved by clustering features in a local neighborhood. However, those methods ﬁx clustering manually, which restricts clusters from having adaptive sizes that depend on the nature of their features. Also, we cannot guarantee that the optimal feature clustering can be mapped into two-dimensional space. For example, edge detectors have at least four dimensions of variation, so an ideal clustering can be achieved by placing edge detectors in a four-dimensional space and grouping nearby features. It will be difﬁcult to approximate such a clustering with a two-dimensional map. Moreover, those methods cannot be used with features already learned by another model.  Slowness has been used in many methods as a criterion for invariant features [1, 15, 2, 6]. The intuition is that if a feature is invariant to various transformations, then its activation should change slowly when presented with an image sequence containing those transformations. Mobahi et al. [15] incorporated unsupervised slowness learning with supervised back-propagation learning, which im- proved the classiﬁcation rate. However, our focus is a simple unsupervised method that can make features invariant without changing them, so it can easily replace and improve spatial pooling in any application.  In this paper, we propose auto-pooling, a novel pooling method that learns soft clustering of features from image sequences in an unsupervised way. Our method improves the invariance of features using temporal coherence of image sequences. Two consecutive frames are likely to contain the same object, so auto-pooling minimizes the distance between their pooled representations. At the same time, the information loss due to pooling is also minimized. This is done by minimizing the reconstruction error in the same way as auto-encoders [17, 18]. Through experiments, we show that our method can pool similar features and increase accuracy of an image classiﬁcation task.  There are several advantages in auto-pooling over traditional spatial pooling (see Figure 1). First, it produces invariance to all types of transformations that present in natural videos. Second, auto- pooling is a more biologically plausible model for complex cells because its parameters are learned from image sequences rather than being manually deﬁned. Third, auto-pooling can be used with non-convolutional models because it does not use spatial information.  2 Auto-pooling  Auto-pooling is a pooling method that learns transformations appeared in image sequences. It is trained by image sequences in an unsupervised way to make features more invariant. The goal of training is to cluster similar features together, so that small transformations would not affect pooled representations. Two features are considered similar if they are traversable by a small transformation such as shift or rotation. We use natural videos as the resource for learning similarity between  2  z  invariance cost  z’  auto-pooling  P  feature  extraction  y  x  PT  y  P  y’  x’  pooled representation  PT  y’  reconstruction cost  Figure 2: Structure of an auto-pooling model  input image  features, because they are rich in various complex transformations. Moreover, image sequences are available to animals and humans as early as their birth, so it is biologically plausible to use them in learning of complex-cell-like invariant features.  We believe that there are two desirable properties in good pooling methods. The ﬁrst property is that if two images show the same object, then their pooled representations should be the same. Auto-pooling tries to meet this invariance property by minimizing the distance between pooled rep- resentations of consecutive frames, which are likely to contain the same object. The second desirable property is that the information loss due to pooling should be minimal. This is the same as max- imizing the cross entropy between inputs and their pooled representations. This entropy property could be obtained by minimizing the error between inputs and reconstructions from their pooled representations.  Instead of image sequences, it is convenient to use image pairs taken from consecutive video frames as training data. Such image pairs can be written as  N }. If an object is present in image xi, then it is likely to be present in x i too, because frames xi and x ′ i have very small time difference (only 33ms for videos with 30 frames per second). Let us assume that the low-level feature extraction is done by  2, ..., xN , x  X = {x1, x  1, x2, x  ′  ′  ′  ′  yi = f (xi), y  ′  ′  i = f (x  i). i are non-negative.  ′  Here, f can be any function as long as yi, y In auto-pooling, clustering of features is parameterized by a pooling matrix P . We require all elements of P to be non-negative because they represent the associations between features and clusters. Pij = 0 means that j-th feature does not belong to i-th cluster. On the other hand, large Pij indicates that i-th cluster contains j-th feature. Then, pooling is done by a simple matrix multiplication, that is  zi = P yi,  ′ i = P y z  ′  i.  ′  If the dimension of feature vectors yi, y i is M , and the dimension of pooled representations zi, z ′ i is K, then P is a K × N matrix. While in spatial pooling, elements of P are ﬁxed to 0 or 1 based on the topology of feature maps, auto-pooling generalizes it by allowing Pij to take any non-negative value. Our main goal is to learn pooling parameters Pij from data, without using the explicit spatial infor- mation. Training of auto-pooling is driven by two cost functions. The ﬁrst cost function  J1 =  1 N  N  X  i=1  1 2  kzi − z  ′  ik2 2  is for the invariance property, and minimizes the distance between pooled representations zi and z i. However, there is a trivial solution of P = 0 if we use only this cost function. The second cost function corresponds to the entropy property, and encourages pooled representa- tions to be more informative of their inputs. Input yi and y i are reconstructed from their pooled  ′  ′  3  representations by  ˆyi = P T  zi,  ′  i = P T ˆy  ′ z i  using the same parameters as the pooling step. Then, the reconstruction error is minimized by the cost function of  J2 =  1 N  N  X  i=1  1 2  (kyi − ˆyik2  2 + ky  ′  i − ˆy  ′  ik2  2).  This prevents auto-pooling from throwing away too much information for the sake of invariance.  Auto-pooling is similar to auto-encoders, which are used in feature learning. In auto-pooling, the input is reconstructed in the same way as auto-encoders. Also, the reconstruction cost function J2 is exactly same as the cost function of auto-encoders. However, there are several important differences between them. First, parameters of auto-pooling are restricted to non-negative values. Second, activation functions of auto-pooling are linear and have no biases. Third, auto-pooling has an additional cost function for temporal coherence.  The ﬁnal cost function of auto-pooling is  J = λJ1 + J2,  where the parameter λ ≥ 0 controls the weight of the invariance cost function. Larger λ will make features more invariant by discarding more information. The training is done by minimizing the cost function with a simple gradient descent algorithm.  2.1 Invariance Score  For evaluating our pooling model, we deﬁne a score for measuring invariance of features (a sim- ilar score is also introduced in [5]). A simple measurement of feature’s invariance is its average activation change between two neighboring frames, which is  G =  1 N  N  X  n=1  kg(xn) − g(x  ′  n)k2.  Here, g(x) := f (x) if we are measuring invariance of raw features, and g(x) := P f (x) if we are measuring invariance of pooled representations.  For invariant features, G should be small. However, features can cheat by simply making its activa- tion constant to reduce G , which is obviously not useful. An ideal invariant feature should take the same value only if the stimuli are from consecutive frames. For frames chosen from random tim- ings, an invariant feature should have different activities because it is likely that the inputs contain different objects. Therefore, the average distance between two random frames  H =  1 N  N  X  n=1  kg(xn) − g(x  ′  σ(n))k2  should be large for invariant features. Here σ is a random permutation of {1, 2, ..., N }. The invari- ance score is deﬁned as  H G which will be large only if a feature is truly invariant.  F =  ,  3 Experiments  (1)  We will show the effectiveness of our method with two types of experiments. In the ﬁrst experiment, we train an auto-pooling model with non-convolutional features. The goal of this experiment is to see whether similar features are being pooled together. We also measured the invariance score of features before and after pooling. In the second experiment, we compared our method with traditional spatial pooling on an image classiﬁcation task.  4  Figure 3: Patch pairs (each column) extracted from natural videos  3.1 Clustering of Image Features  The goal of this set of experiments is to analyze feature clusters learned by auto-pooling. We pre- pared a dataset of 16 × 16 gray-scale patch pairs from natural videos1. Patch pairs are extracted from random locations of consecutive frames. Some of the patch pairs are shown in Figure 3.  Figure 4: Clusters (shown in each column) of features learned from the patch pair dataset  We used a sparse auto-encoder to learn 400 features from patches. Then, we trained an auto-pooling model on those features. Since auto-pooling performs soft clustering, it is hard to visualize the clustering result. For simplicity, we used a small threshold to show some of the learned clusters in Figure 4, where each column represents a single cluster. For i-th cluster, we showed features with Pij > ε. It is evident that similar features are clustered together. Also, one can see that the size of a cluster vary depending on the nature of its features.  Figure 5: Diversity of edge detectors in a single cluster  To display clusters more clearly, some clusters of edge detectors are shown in more detail in Figure 5, in which edge detectors are replaced by corresponding thin lines. This allows us to see the diversity of edge detectors inside each cluster. The important thing is that there is variance in orientations  1We used 44 short videos. All videos are obtained from http://www.vimeo.com and had the Creative Commons license. Although we tried to include videos with the same objects as CIFAR10 (a labeled image dataset used in the next experiment), image patches extracted from the videos were qualitatively different than images from CIFAR10. Even if a video contains a desired object, not all frames show the object. Also, most patches only included a part of an object, because the patch size was much smaller than the frame size,  5  as well as in locations, which means that auto-pooling can create representations invariant to small rotations.  (cid:25)  (cid:23)  (cid:21)  (cid:72) (cid:85) (cid:82) (cid:70) (cid:54) (cid:98) (cid:72) (cid:70) (cid:81) (cid:68) (cid:76) (cid:85) (cid:68) (cid:89) (cid:81) (cid:44)  (cid:19)  (cid:98) (cid:19)  (cid:98)  (cid:83)(cid:82)(cid:82)(cid:79)(cid:72)(cid:71)(cid:98)(cid:73)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86) (cid:85)(cid:68)(cid:90)(cid:98)(cid:73)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86)  (cid:21)(cid:19)  (cid:23)(cid:19)  λ  (cid:25)(cid:19)  Figure 6: Invariance scores before and after pooling  Next, we analyzed the effect of pooling on image features using our invariance score. Figure 6 shows invariance scores measured at various values of λ, which controls the weight of the invariance cost. The invariance score is signiﬁcantly improved by the pooling, especially for large λ values. It is not surprising because larger λ puts more importance on the invariance cost, thus makes pooled representations less likely to change between consecutive frames. As a result, G in equation 1 becomes smaller and increases the invariance cost.  At the same time, however, increase of λ diminishes the role of the reconstruction cost, which was preventing the loss of too much information. Too large λ makes pooled representations over- invariant, having constant values all the time. This results in a small H and decreases the invariance cost. This side effect of large λ can be observed from Figure 6, where the invariance score stopped its increase at large λ.  3.2 Image Classiﬁcation  Figure 7: Samples from the patch pair dataset used in this experiment  Next, we tested the effectiveness of our pooling method by applying it to an image classiﬁcation task. We used two datasets. For image classiﬁcation, we used CIFAR10 dataset [11], which contains 50 thousand labeled images from ten categories. All images were 32×32 pixels in size, and had three- color channels. For training of an auto-pooling model, we prepared a patch pair dataset in the same way as the previous experiment, except patches were 32×32 color images to match CIFAR10 images. Some samples from the patch pair dataset are shown in Figure 7.  Figure 8: Features learned by a sparse auto-encoder from small image patches  In the feature extraction step, we used a convolutional model. We trained a sparse auto-encoder with 100 hidden units on 6×6 small patches extracted from CIFAR10 images. Learned local features are shown in Figure 8. Then, those 6 × 6 local features are duplicated to all possible locations of 32×32 images, resulting in 100 feature maps of 27 × 27.  6  Figure 9: Feature clusters learned by an auto-pooling model. The right side shows pooled regions on feature maps, and the left side shows corresponding local features.  The convolutional feature extraction produced very large (exceeding 100 gigabytes) training data for auto-pooling. Luckily, the training took only few hours because we implemented our algorithm on a graphic card (Tesla K20c) using CUDAMat library [14]. Some of the learned feature clusters are visualized in Figure 9. For each cluster, we showed 15 feature maps with the largest pooling Pij), where Sk is the set of features in k-th feature map). Pij = 0 is shown area (i.e. maxk(Pj∈Sk in gray and large Pij is shown in white. Local features corresponding to the feature maps are also shown in Figure 9.  Unlike spatial pooling, each cluster learned by auto-pooling extended to multiple feature maps. Pooling regions (i.e., white areas in Figure 9) of those maps usually have the same continues spatial distribution, which will create spatial invariance in the same way as spatial pooling. If we observe those pooling regions carefully, however, we can see the small variance in their locations. This location variance is inversely related to the location variance of corresponding local features. For example, if there is a edge detector in the lower part of a 6 × 6 local feature, corresponding pooling regions will have upper position in 27 × 27 feature maps. Beside from pooling by spatial areas, auto-pooling also succeeded in clustering similar local fea- tures. In some clusters, edge detectors of similar orientations are grouped together. This will make pooled representations invariant to small rotations, which is a clear advantage over traditional spa- tial pooling. In addition, clustering of local features only differing in their locations will reduce the redundancy created by convolutional feature extraction.  We compared our pooling method with traditional spatial pooling on a classiﬁcation task, in which a supervised classiﬁer is trained by pooled representations of labeled images. For auto-pooling, we varied the number of clusters from 400 to 2500. For spatial pooling, we can only change the grid size. However, it is possible to use multiple spatial pooling at once [12] to produce better results. We denote a spatial pooling that used 2 × 2 and 3 × 3 grids by 2 × 2 + 3 × 3.  Table 1: Classiﬁcation accuracy on CIFAR10 (AP=auto-pooling, SP=spatial pooling)  Pooling methods AP (400 clusters) AP (800 clusters) AP (1300 clusters) AP (1600 clusters) AP (2000 clusters) AP (2500 clusters)  Accuracy Accuracy (full) 64.6% 67.2% 69.0% 69.4% 69.7% 69.3%  (small) 61.6% 62.9% 63.8% 64.3% 65.0% 63.5%  SP (2 × 2) SP (3 × 3) SP (2 × 2 + 3 × 3) SP (4 × 4) SP (2 × 2 + 3 × 3 + 4 × 4)  63.8% 68.2% 68.2% 68.4% 68.2%  60.7% 61.5% 61.2% 58.7% 57.6%  7  y c a r u c c a  70  68  66  64  62  60  58  56  auto−pooling (full) auto−pooling (small)  spatial pooling (full) spatial pooling (small)  500  1000  1500  2000  2500  3000  the number of clusters  Figure 10: Classiﬁcation accuracies are plotted as functions of the number of features  In classiﬁcation, we trained a linear SVM with pooled representations. The results are shown in Table 1. We trained the classiﬁer with two training data: a full data with 5000 examples per class, and a smaller one with 1000 examples per class. Since the number of features is an important factor in classiﬁcation, we plotted the accuracy of the two pooling methods against the number of clusters in Figure 10. Auto-pooling outperformed traditional spatial pooling for most of the time. Especially for small training data, the difference between the two pooling methods was substantial. This indicates that auto-pooling is better at generalization, which is the main goal of invariant features. The spatial pooling, on other hand, shows the sign of over-ﬁtting when its pooling regions are increased.  4 Conclusions  In this paper, we introduced auto-pooling, a novel pooling method that can generalize traditional spatial pooling to transformations other than spatial shifting. Auto-pooling tries to make features more temporally coherent, having slow changing activation when presented with a continues image sequence. The information loss due to pooling is kept minimum using the same cost function as auto- encoders. The main advantage of our method is that it learns to cluster features from data, rather than relying on manual heuristic spatial divisions. Therefore, auto-pooling is a more biologically plausible model for complex cells.  When trained by image pairs extracted from natural videos, auto-pooling successfully clustered similar features together. We showed that such clustering could signiﬁcantly improve the invariance of features. Also, our pooling model was more effective than traditional spatial pooling when it was used in a real-world classiﬁcation task, where spatial pooling had the advantage of using spatial information of features.  In our experiments, the advantage of auto-pooling over spatial pooling was mainly restricted to learn- ing of rotation invariance. This is because auto-pooling is applied to low-level features, which were mostly edge detectors with the size. Therefore, the only possible variance beside spatial shifting was rotation. We believe that if we use auto-pooling instead of spatial pooling in deep architectures, we can create invariance to more complex transformations such as three-dimensional rotations and distortions.  Acknowledgments  This research is supported by the Aihara Innovative Mathematical Modelling Project, the Japan Society for the Promotion of Science (JSPS) through the “Funding Program for World-Leading  8  Innovative R&D on Science and Technology (FIRST Program),” initiated by the Council for Science and Technology Policy (CSTP).  References  [1] P. Berkes and L. Wiskott. Slow feature analysis yields a rich repertoire of complex cell properties. Journal  of Vision, 5(6), 2005.  [2] C. Cadieu and B. Olshausen. Learning transformational invariants from natural movies. Advances in  Neural Information Processing Systems 21, pages 209–216, 2009.  [3] A. Coates and A. Ng. Selecting receptive ﬁelds in deep networks. In Advances in Neural Information  Processing Systems 24, pages 2528–2536. 2011.  [4] K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recog-  nition unaffected by shift in position. Biological Cybernetics, 36(4):193–202, 1980.  [5] I. Goodfellow, Q. Le, A. Saxe, H. Lee, and A. Ng. Measuring invariances in deep networks. In Advances  in Neural Information Processing Systems 22, pages 646–654. 2009.  [6] K. Gregor and Y. LeCun. Emergence of complex-like cells in a temporal product network with local  receptive ﬁelds. arXiv preprint arXiv:1006.0448, 2010.  [7] D. Hubel and T. Wiesel. Receptive ﬁelds, binocular interaction and functional architecture in the cat’s  visual cortex. The Journal of Physiology, 160(1):106, 1962.  [8] A. Hyv¨arinen, P. Hoyer, and M. Inki. Topographic independent component analysis. Neural Computation,  13(7):1527–1558, 2001.  [9] Y. Jia, C. Huang, and T. Darrell. Beyond spatial pyramids: Receptive ﬁeld learning for pooled image In Computer Vision and Pattern Recognition, 2012 IEEE Conference on, pages 3370–3377.  features. IEEE, 2012.  [10] K. Kavukcuoglu, M. Ranzato, R. Fergus, and Y. LeCun. Learning invariant features through topographic ﬁlter maps. In Computer Vision and Pattern Recognition, 2009 IEEE Conference on, pages 1605–1612. IEEE, 2009.  [11] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s thesis,  Department of Computer Science, University of Toronto, 2009.  [12] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In Computer Vision and Pattern Recognition, 2006 IEEE Conference on, pages 2169–2178. IEEE, 2006.  [13] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, 1998.  [14] V. Mnih. CUDAMat: a CUDA-based matrix class for Python. Technical report, Department of Computer  Science, University of Toronto, 2009.  [15] H. Mobahi, R. Collobert, and J. Weston. Deep learning from temporal coherence in video. In Proceedings  of the 26th International Conference on Machine Learning, pages 737–744. ACM, 2009.  [16] S. Osindero, M. Welling, and G. Hinton. Topographic product models applied to natural scene statistics.  Neural Computation, 18(2):381–414, 2006.  [17] M. Ranzato, S. Chopra, and Y. LeCun. Efﬁcient learning of sparse representations with an energy-based model. In Advances in Neural Information Processing Systems 19, pages 1137–1144. MIT Press, 2007.  [18] P. Vincent, H. Larochelle, Y. Bengio, and P.A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th International Conference on Machine Learning, pages 1096–1103. ACM, 2008.  9  ","Learning invariant representations from images is one of the hardestchallenges facing computer vision. Spatial pooling is widely used to createinvariance to spatial shifting, but it is restricted to convolutional models.In this paper, we propose a novel pooling method that can learn soft clusteringof features from image sequences. It is trained to improve the temporalcoherence of features, while keeping the information loss at minimum. Ourmethod does not use spatial information, so it can be used withnon-convolutional models too. Experiments on images extracted from naturalvideos showed that our method can cluster similar features together. Whentrained by convolutional features, auto-pooling outperformed traditionalspatial pooling on an image classification task, even though it does not usethe spatial topology of features."
1301.2820,2013,Clustering Learning for Robotic Vision  ,"['Eugenio Culurciello', 'Jordan Bates', 'Aysegul Dundar', 'Jose Carrasco', 'Clement Farabet']",https://arxiv.org/pdf/1301.2820.pdf,"Clustering Learning for Robotic Vision  Eugenio Culurciello∗ Purdue University  euge@purdue.edu  Jordan Bates  Purdue University  jtbates@purdue.edu  3 1 0 2    r a     M 3 1      ]  V C . s c [      3 v 0 2 8 2  .  1 0 3 1 : v i X r a  Aysegul Dundar Purdue University  adundar@purdue.edu  J.A. Perez-Carrasco University of Seville jperez2@us.es  Clement Farabet New York University cfarabet@nyu.edu  Abstract  We present the clustering learning technique applied to multi-layer feedforward deep neural networks. We show that this unsupervised learning technique can compute network ﬁlters with only a few minutes and a much reduced set of pa- rameters. The goal of this paper is to promote the technique for general-purpose robotic vision systems. We report its use in static image datasets and object track- ing datasets. We show that networks trained with clustering learning can outper- form large networks trained for many hours on complex datasets.  1  Introduction  In the recent years the fusion of bio-inspired and neuromorphic vision models with machine learning has dominated the development of artiﬁcial vision systems for the categorization of multiple objects in static frames. Bio-inspired deep networks are computer-vision and computational-neuroscience models of the mammalian visual system implemented in deep neural networks [1–6]. Most deep net- work architectures are composed of multiple layers (2, 3 typically), where each layer is composed of: linear two-dimensional ﬁltering, non-linearity, pooling of data, output data normalization [7–9]. Recent machine learning research has focused on the task of training such deep networks from the abundant digital data available in the form of image frames and videos. In particular, deep networks need to learn good feature representations for complex visual tasks such as object categorization and tracking of objects in space and time, identifying object presence and absence. These repre- sentations usually involve learning the linear ﬁlter weight values from labeled and unlabeled input data. Since labeled data is costly and often ridden with human errors [10–12], the recent focus is on learning these features purely from unlabeled input data [13–17]. These recent methods typically learn multiple layers of deep networks by training several layers of features, one layer at a time, with varying complexity of learning models. Recent techniques based on unsupervised clustering algorithms are especially promising because they use simple learning methods that quickly converge [17]. These algorithms are easy to setup and train and are especially suited for robotics research, because less complex knowledge of machine learning is needed, environment-speciﬁc data can be collected quickly with a few minutes of video,  ∗More  laboratory and research can be http://engineering.purdue.edu/elab/. Real time robotic vision systems: http://www.neuﬂow.org/  information on Eugenio Culurciello’s  found here:  1  setup of custom size deep networks is quick and can be adapted to speciﬁc tasks. In addition, real- time operation with efﬁcient networks can be obtained with only a few minutes of training and setup, leading to quick and direct experimentation in robotic experiments. In this paper we present results obtained with unsupervised clustering algorithms on the training and operation of deep neural networks for real-time robotic vision systems. We provide simple techniques and open-source software that allows robotic researchers to use deep network in a short setup time and with little or no knowledge of machine learning necessary. The main goal of the paper is not to present state-of-art results on a speciﬁc dataset. Rather we use standard published datasets to evaluate the performance of prototype robotic vision system for general-purpose use, where no dataset is available. It is thus not useful to train the network to perform only on one dataset, when the levels of performance would not carry over to another dataset or real-world images. The goal is thus mainly to evaluate the use of unsupervised networks that can support at least ten frames-per-second operation on commercial hardware, such as recent laptop computers. The paper presents the following key innovations: (1) use of clustering learning for quickly training robotic systems (Section 2), (2) the use of distance-based ﬁltering, as opposed to the standard con- volution (Section 2.2), (3) the experimental proof that clustering learning networks can outperform supervised networks on general purpose robotic tracking tasks (Section 4).  2 Methods  In this paper we created and tested a model of unsupervised clustering algorithms (CL) that can quickly learn linear ﬁlters’ weight values, and also is amenable to real-time operation with con- ventional mobile hardware. We used the Torch7 software for all our experiments [18], since this software can reduce training and learning of deep networks by 5-10 times compared to similar Mat- lab and Python tools.  2.1  Input data  Input image data was obtained from the CIFAR10 [19] and the Street View House Numbers (SVHN) [20] datasets. The SVHN dataset has a training size of 73,257 32x32 images and test size of 26,032 32x32 images. The CIFAR10 dataset has a training size of 20,000 32x32 images and a test size of 2,000 32x32 images. Both datasets offer a 10 categories classiﬁcation task on 32x32 size images. The train dataset was in all cases learned to 100% accuracy, and training was then stopped. Input data was contrast normalized separately on each RGB channel with a 9x9 gaussian ﬁlter using the Torch7 ”nn.Spatial Contrastive Normalization” function. For the real-time networks in Section 3, we used patches from a contrast-normalized version of a few images from the Berkeley image dataset [21]. As testing data for a general-purposed robotic vision system, we decided to test our networks in a tracking task, where the network has to be able to track an object of interest based on a single presentation. For this purpose we use the challenging benchmark TLD dataset [22]. From this dataset we selected multiple videos with different properties of occlusions, camera movement, pose, scale and illumination changes. Even if other groups showed slight improvements using the YUV color space [7], we were not able to reproduce the beneﬁts of YUV, so we kept the images in their original RGB space. Also we did not use whitening of data (such as ZCA whitening) even if other groups have shown clear advantages of using it. We did not use whitening because of two main reasons: ﬁrst, it is not applicable for general-purpose vision system where an a-priori dataset cannot be obtained. Second, whitening computation is very expensive (similar to the ﬁrst layer of a convolutional neural network) and we instead replaced it with local contrast normalization, which is a bio-inspired technique [23] to whiten the input data removing its mean bias and adapting the input dynamic range.  2.2 Network architecture  The deep neural network architecture is composed of 4 layers, not counting pooling and normaliza- tion operations. Two layers of linear two-dimensional ﬁltering and two layers of output classiﬁer in the form of a fully connected 2-layer neural network. The ﬁrst two layers were composed of  2  Figure 1: Architecture of one layer of the clustering learning (CL) network. Filters were applied with a sum-abs-diff operations, followed by contrastive normalization, L-2 pooling of features, non- linearity tanh, and subtractive normalization.  a two-dimensional convolutional linear ﬁltering stage, a L2 norm pooling stage, and a subtractive normalization layer for removing the mean of all outputs. The ﬁlters of the ﬁrst two layers are gen- erated with unsupervised clustering algorithms, as explained below. Training of the last two fully- connected neural network layers was performed with approximately 50-100 epochs on the SVHN dataset on a quad-core Intel i7 laptop, or about 8 hours. Test data maximum precision usually only needed approximately 15 epochs. The layers in the clustering learning network used the following sequence of operations. Using the naming convention in [8], for each layer l xi is an input feature map, yi is an output feature map. The input of each layer is a 3D array with nl 2D feature maps of size nl1 · nl2. Each component (pixel, neuron) is denoted xijk. The output is also a 3D array, yi composed of ml feature maps of size ml1 · ml2.  the learned CL ﬁlters: yci =(cid:80) 3. L2 pooling over 2x2 regions: ypi =(cid:80)  1. SpatialSAD module: performing sum-abs-diff operation on images convolutionally with  i |kij − xi|, where |x| is the absolute value of x. 2. Spatial Contrastive Normalization: to zero the data mean and standard deviation  n×n(ycij · gnij), computing the weighted average  of the input in an n · n (here n = 2) region with a gaussian kernel gn.  4. Tanh nonlinearity: ynli = tanh(ypi) 5. Spatial Subtractive or Contrastive Normalization:  vijk = ynlijk −(cid:80) yijk = vijk/max(mean(σjk), σjk) where σjk = ((cid:80)  to zero the data mean, reset std to unity. The subtractive normalization operation for a given site ynlijk computes: ipq wpq · ynli,j+p,k+q, where wpq is a normalized truncated Gaus- sian weighting window (typically of size 9x9). The divisive normalization computes i,j+p,k+q)1/2. The local  ipq wpq · v2  contrast normalization layer is inspired by visual neuroscience models.  In order to show the effectiveness of the learning techniques, we compared them to a standard 1- layer and a 2-layers convolutional neural network (CNN) [8, 9, 24], followed by the same 2-layers classiﬁer. The layers in the convolutional neural network used the following sequence of operations:  1. SpatialConvolution module: performing convolutions on images with the learned CL ﬁl- i kij ∗ xi, where ∗ is the 2D discrete convolution operator and bj is a  ters: yci = bj +(cid:80)  trainable bias parameter.  2. L2 pooling over 2x2 regions: as described above. 3. Tanh nonlinearity: as described above. 4. Spatial Subtractive Normalization: as described above.  All networks used 16 ﬁlters on the ﬁrst layer, 128 ﬁlters on the second layer. The ﬁnal classiﬁer was ﬁxed to 128 hidden units and 10 output classes for CIFAR and SVHN. Clustering learning networks used a fully connected (all channels connected to all channels) input to 1st, and 1st to 2nd layer, while convolutional neural networks used 1-out-of-3 and 4-out-of-16 random connection table between the input and 1st layer, and 1st and 2nd layer respectively.  3  input Image1 to N1 sum-abs-diffconstastive normN1 to N1L2 poolingN1 to N1 Tanhsubtractive norm(a) 1st layer ﬁlters  (b) 2nd layer ﬁlters  Figure 2: Filters obtained with clustering learning on the 1st and 2nd layer. The ﬁlters obtained on the 1st layer are quite similar to elongated Gabor patches, and what can be obtained with more complex and numerically involved unsupervised techniques. Filter training with CL was obtained in 10 min time on a modern laptop.  Notice that CL networks used sum-abs-diff metrics to correlate ﬁlters responses to inputs. This is different to the standard approach of deep networks [1, 7, 15, 19] where convolution operations are used. Our choice of sum-abs-diff operations was dictated by improved performance of the CL ﬁlters with respect to convolutions. In a standard modern computer the difference between convolutions (multiplications by weights) and distance metrics (differences, subtractions) is not visible, as multi- pliers are optimized to perform as fast as the simpler difference operations. On the other hand, when programmable hardware as FPGA is used, the use of differences instead of multipliers can reduce the silicon area utilization, and power up to 15 times on a 16 bit operation, and more for larger number of bits. A full comparison of the hardware advantages of using distance operators instead of convolutions will be the subject of future publications.  2.3 Learning  We use k-means clustering algorithm to learn a set of 16 ﬁlters in the ﬁrst layer, and 128 ﬁlters in the second layer. The techniques and scripts are general and can quickly modiﬁed to learn any number of ﬁlters. The ﬁlter sizes on both layers was set to 5 x 5 pixels for the SVHN datasets. In CIFAR 1st layer we used 5 x 5 ﬁlters, and on the 2nd layer, we used 3 x 3 ﬁlters, as features were more spatially constrained in this dataset. Clustering used the same size patches of the normalized images, and we used 1 M patches from each dataset to train the ﬁrst layer. The second layer training was performed by passing the entire dataset through the ﬁrst layer of the deep neural network. The output dataset was then used again with the same script to train another set of linear ﬁlters, by using 1 M patches of the processed dataset. Clustering learning ﬁlters learned on the 2nd layer used as many planes as the output of the 1st layer (16 here). For example we learned 128 ﬁlters each with 16 dimensions of 5 x 5 pixels. This was done to cluster different sets of features for each output of the 1st layer, and increased performance by an average of 5% or more. Examples of the ﬁlters learned with CL techniques on a 1st and 2nd layer are given in ﬁgure 2. Both layer ﬁlters were obtained with training for 10 minutes on a modern quad-core Intel i7 laptop. A supervised network of this size would require several tens of thousands of image examples and several hours of training time. Convolutional neural networks were trained with supervised methods using stochastic gradient de- scent and other techniques as mentioned in [8].  4  3 Real-time network  The goal of this paper was to provide a simple and fast method to train unsupervised networks for general-purposed robotic vision system. For real-time experiments we used the TLD dataset [22]. We then compared the tracking performance of a real-time deep network both trained supervised [25], and with CL techniques described here. The CL network for this task had the same two-layer network architecture used in [25]. This insured real-time operation of 6 frames/s on a quad-code Intel i7 laptop computer. We focused on this network and restricted ourselves to real-time operation because the goal of this project is the use of deep networks in mobile computers. The network operates on 46 x 46 input images, uses 16 ﬁlters with 7 x 7 receptive ﬁeld on the ﬁrst layer and 128 ﬁlters with 7 x 7 receptive ﬁeld on the second layer. The 1st to 2nd layer fan-in was 8, and the 2nd to 3rd layer fan-in was 64. Both layers were connected with random tables. The network produces a 128 feature vector as output. We trained the same size and number of ﬁlters through clustering algorithm for use in this network. We used patches from a contrast-normalized version of a few images from the Berkeley image dataset [21]. Any set of natural-scene images can be used to train the general-purpose network presented in this section, and we on purpose chose not to sample patches from the target TLD dataset, in order to demonstrate the learning invariance properties of our technique.  4 Results  4.1 Static datasets  We report the results in the SVHN dataset in ﬁgure 3. Here we compared results of accuracy in the test set for 4 cases: clustering learning with 1 layer (CL 1 layer), clustering learning with 2 layers (CL 2 layers), a 1-layer and a 2-layers convolutional neural network (CNN 1l, 2l).  Figure 3: Test set accuracy comparison for a convolutional neural network (CNN) and a clustering learning (CL) network with 1 and 2 layers on the SVHN dataset  The data shows that clustering learning and CNN with 1 layer provide remarkably the same levels of accuracy on the dataset. This shows that at least on the 1st layer, the features learned with clustering learning are very effective. Adding a second layer brings the convnet to 91% levels of accuracy, which are standard without using any sophisticated tricks and with a small network with only 16 ﬁlters on the ﬁrst layer. On the other had, the clustering learning network with 2 layers showed more than 1% increase in accuracy, from 88% to 89%. This increase is not as large as one would want and expect from adding a second layer, but is consistent with unsupervised learning results [17, 26].  5  020406080100120epoch [#]80828486889092Accuracy [%]CL 1 layerCL 2 layersCNN 1lCNN 2lIt is interesting to note that with the clustering learning 2 layers network accuracy was above 88%, and plateaued with the train set accuracy plateauing also at 92%. This shows that clustering learning ﬁlters also do not over ﬁt, and present non-perfect, but almost identical results on both train and test sets. We note that CNN network trained supervised on SVHN report state-of-the-art performances of 96% and above. The results above were all obtained with feed-forward hierarchical networks. We also tried to use multiple layers of clustering learning unsupervised networks in parallel, as recommended by other publications [8, 17], but we did not obtain any beneﬁts from that strategy, on the contrary parallel networks always reported losses of 3-5% accuracy with respect to a single layer. This is different from what reported in [17, 26]. We also report here the results in the CIFAR10 dataset in ﬁgure 4. As in the SVHN case, we compared results of accuracy in the test set for 4 cases: clustering learning with 1 layer (CL 1 layer), clustering learning with 2 layers (CL 2 layers), a 1-layer and a 2-layers convolutional neural network (convnet 1l, 2l).  Figure 4: Test set accuracy comparison for a convolutional neural network (CNN) and a clustering learning (CL) network with 1 and 2 layers on the CIFAR10 dataset  The results in the CIFAR10 dataset show a gain of more than 10% from using a single layer convnet to a 2 layer convnet. Clustering learning showed the same behavior as in the SVHN dataset: adding a second layer achieved 3-5% better accuracy on the test set. This dataset uses very small images and is notoriously difﬁcult, and clustering learning only reported a 50% accuracy overall with 2 layers. All results are much lower than the current state of art in this dataset, which is close to 90% [27]. But again we stress that our goal is real-time implementation and other researcher have obtained record results in this dataset only with very large networks, most of which are not amenable to real-time operation. We tested random ﬁlters used on the ﬁrst 2 layers of the networks for both SVHN and CIFAR10 datasets. The 1st layer of the network was trained with CL, as mentioned above. The randomly connected network used a ﬁxed CNN layer as described in section 2.2. Same qualitative results were obtained with a random CL model. The SpatialConvolution module was initialized with uniformly random ﬁlters by the Torch7 tool, and left untouched. We have found that random ﬁlters on the 2nd layer of this networks perform with the same levels of precision as the 2nd layers trained with CL. This was true for both SVNH and CIFAR10 datasets. We also used the ﬁlters obtained with CL on a 2-layers standard CNN network. But the use of convolution operation was giving results that were 5-10% less precise that the results obtained with sum-abs-diff operators.  6  01020304050epoch [#]354045505560657075Accuracy [%]CL 1 layerCL 2 layersCNN 1lCNN 2l4.2 Dynamic datasets  The results for the comparison for the real-time tracking network on the TLD dataset [28] are given in Table 1. Each video contains only one target. The metric used is the number of correctly tracked frames. More information and images on this dataset can be found here [29]. All network we tested were real-time networks performing at or above 5 frames/s. As can be seen in Table 1, the Clustering Learning (CL) network performs better than the convolutional neural network from [25] in all sequences, and is comparable only in one sequence (”Pedestrian 2”). These results are currently not state-of-the-art, which is currently obtained in references [22, 30]. Not only the perfomance of the CL network is higher than CNN on the TLD dataset, but also the CNN from [25] was trained in a week time, while the CL network was tried in 10 minutes. There is a clear advantage to using CL networks for general-purpose, dataset-free robotic vision tasks.  Table 1: Precision comparison between of a Clustering Learning (CL) network and a convolutional neural network (CNN) [25] used as trackers in the TLD dataset [22].  Precision: CL Precision: CNN [25]  Sequence  David Jumping  Pedestrian 1 Pedestrian 2 Pedestrian 3  Car  Carchase  Frames  761 313 140 338 184 945 9928  0.18 0.37 0.81 0.78 0.45 0.67 0.38  0.08 0.20 0.69 0.79 0.44 0.48 0.26  In addition, we present in table 2 a comparison of running time of the most recent deep network work performing at the state-of-the-art in a variety of static datasets. We provide this comparison to show that although these networks perform extremely well on a speciﬁc dataset, we point out that similarly to the experimental results in 1, that performance might not carry over to robotic vision tasks such as tracking of previously unseen objects. We also want to point out that the state-of-the- art networks in table 2 are far from real-time operation, and it is not clear in their publication how that can be achieved down-sizing the networks or what performance they might attain.  Table 2: Comparison of execution time of state-of-the-art networks as compared to the proposed CL network.  1st layer ﬁlters Whitening  Publication  CL (this work)  [25] [31] [26] [32] [27] [7]  Frames/s  5-10 1-2 1? 0.1? 1? 0.3? 2-4?  Precision  89%, SVHN  79.5%, Stanford 80.8%, RGB-D 82%, CIFAR10 98.5% ImageNet 88.8%, CIFAR10 99.5%, MNIST  16 16 128 1600 96 300 32  none none ZCA ZCA none none none  The data provided in table 2 reports published and informally obtained data on the best results obtained in static datasets. Cl networks also reported 50% precision on CIFAR10, a low value compared to [26, 27]. As reference [31] precision we used the RGB only data. Most published results did not report computer time. We asked the author to provide us the data for this table, when available. We estimated other network compute time based on the size of the 1st layer of their network, which is usually the most computationally demanding. We estimated frames/s based on the speed of our networks. We want to stress the importance of reporting computing time for deep networks, and the use of whitening, as it is the only way to compare large and small networks and try to ﬁnd the optimal size for robotic vision system, or other tasks.  7  5 Discussion  We presented results on clustering learning algorithms for general-purpose vision system. These algorithms can be used to train multi-layer feed-forward neural networks in minutes, with fully automated scripts with a reduced set of learning parameters. We show results on static datasets and dynamic tracking of objects in videos. We show results that prove that CL techniques are a viable and quick option to training deep networks. In static datasets, although they do not perform at the state-of-the-art because of their small network size, they can however be run in real-time because of the small network size. The accuracy gap in recognition can be reduced by using more features, as reported in [17], with an evaluation time linearly proportionalto the number of ﬁlters used. We also show that on tracking datasets the CL networks outperforms CNN networks. We believe the tracking dataset is a better approximation of robotic tasks, where locking on a target object is required for approach and manipulation. We show that sum-abs-diff operators can be more effective than convolution operations for ﬁltering in deep networks. We also point out that custom hardware with these operators will be more efﬁcient in power and space. Many groups in the ﬁeld of deep learning work on static datasets to demonstrate the best learning techniques. A lot of these techniques cannot be applied in robotic vision system with the current modern hardware because they cannot perform in real-time ad require too much computational load. We argue that more research is needed in the ﬁeld of applied and real-time deep networks, where shortcuts need to be taken in order to optimize performance for speedy operation. The use of custom hardware is recommended [33] but not necessary in many applications. CL models are also very interesting to bio-inspired vision research because they provide a close connection between computational neuroscience and machine learning. In particular unsupervised clustering algorithms provide a simplistic model of Hebbian Learning methods, where neurons that respond to the same input are clustered [34–37] . Most of previous work was mathematically com- plex and not efﬁcient to implement. In this paper we thus present one of the ﬁrst practical applica- tion of Hebbian-like learning applied to deep networks. Its fast and simple computation can help researchers quickly train complex neural networks. Further research is also needed to justify unsupervised learning on the 2nd or later layers. Our experiments indicate that random ﬁlters are as good as learned CL ﬁlters. This fact was also shown in other papers [26, 31] where random ﬁlters were used in the 2nd layer, after one CL-trained layer with a large number of features (128 or above). All the code for this paper is available here: https://github.com/culurciello/CL_ paper1_code.  Acknowledgments  We are especially grateful to the the Torch7 developing team, in particular Ronan Collobert, who ﬁrst developed this great, easy and efﬁcient tool, Clement Farabet, Koray Kavukcuoglu, Leon Bottou. We could not have done any of this work without standing on these giants shoulders.  References  [1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document  recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998.  [2] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant map- ping. In Proc. Computer Vision and Pattern Recognition Conference (CVPR’06). IEEE Press, 2006.  [3] K. Gregor, A. Szlam, and Y. LeCun. Structured sparse coding via lateral inhibition. In Ad-  vances in Neural Information Processing Systems (NIPS 2011), volume 24, 2011.  [4] M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature  neuroscience, 2:10191025, 1999.  8  [5] T. Serre, A. Oliva, and T. Poggio. A feedforward architecture accounts for rapid categorization.  Proceedings of the National Academy of Sciences, 104(15):64246429, 2007.  [6] T. Serre and T. Poggio. A neuromorphic approach to computer vision. Communications of the  ACM, 53(10):5461, 2010.  [7] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architec- ture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09). IEEE, 2009.  [8] Y. LeCun, K. Kavukvuoglu, and C. Farabet. Convolutional networks and applications in vision.  In Proc. International Symposium on Circuits and Systems (ISCAS’10). IEEE, 2010.  [9] Y. Boureau, J. Ponce, and Y. LeCun. A theoretical analysis of feature pooling in vision algo-  rithms. In Proc. International Conference on Machine learning (ICML’10), 2010.  [10] A.  Karpathy.  classifying CIFAR-10. http://karpathy.ca/myblog/2011/04/27/ lessons-learned-from-manually-classifying-cifar-10-with-code/, April 2011.  manually  Lessons  learned  from  [11] A. Torralba and A. A. Efros. Unbiased look at dataset bias. In Computer Vision and Pattern  Recognition (CVPR), 2011 IEEE Conference on, page 15211528, 2011.  [12] X. Hou, A. Yuille, and H. Koch. A meta-theory of boundary detection benchmarks. In NIPS  Workshop on Human Computation for Science and Computational Sustainability, 2012.  [13] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning  a sparse code for natural images. Nature, 381(6583):607609, 1996.  [14] A. Hyv¨arinen and E. Oja. Independent component analysis: algorithms and applications. Neu-  ral networks, 13(4):411430, 2000.  [15] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural  computation, 18(7):15271554, 2006.  [16] P. Vincent, H. Larochelle, Y. Bengio, and P. A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, page 10961103, 2008.  [17] A. Coates, H. Lee, and A. Y. Ng. An analysis of single-layer networks in unsupervised feature  learning. AISTATS, 14, 2011.  [18] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine  learning. In NIPS Workshop BigLearn, 2011.  [19] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s  thesis, Department of Computer Science, University of Toronto, 2009.  [20] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in nat- In NIPS Workshop on Deep Learning and  ural images with unsupervised feature learning. Unsupervised Feature Learning, volume 2011, 2011.  [21] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on, volume 2, page 416423, 2001.  [22] Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-detection. Pattern Analysis and  Machine Intelligence, IEEE Transactions on, 34(7):14091422, 2012.  [23] B. A. Wandell. Foundations of vision. Sinauer Associates, 1995. [24] P. Sermanet, S. Chintala, and Yann LeCun. Convolutional neural networks applied to house numbers digit classiﬁcation. In International Conference on Pattern Recognition (ICPR 2012), 2012.  [25] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Scene parsing with multiscale feature  learning, purity trees, and optimal covers. arXiv preprint arXiv:1202.2160, 2012.  [26] A. Coates and A. Y. Ng. Selecting receptive ﬁelds in deep networks. Advances in Neural  Information Processing Systems, 24:25282536, 2011.  9  [27] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, page 36423649, 2012.  [28] Z. Kalal, J. Matas, and K. Mikolajczyk. P-n learning: Bootstrapping binary classiﬁers by  structural constraints. Conference on Computer Vision and Pattern Recognition, 2010.  [29] Z. Kalal. TLD dataset. http://info.ee.surrey.ac.uk/Personal/Z.Kalal/  TLD/TLD_dataset.pdf, 2010.  [30] A. Dundar, J. Jin, and E. Culurciello. Visual tracking with similarity matching ratio. arXiv  preprint arXiv:1209.2696, 2012.  [31] R. Socher, B. Huval, B. Bhat, C. D. Manning, and A. Y. Ng. Convolutional-recursive deep learning for 3D object classiﬁcation. Advances in Neural Information Processing Systems, 2012.  [32] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classiﬁcation with deep convolutional  neural networks. Advances in Neural Information Processing Systems, 25, 2012.  [33] C. Farabet, B. Martini, B. Corda, P. Akselrod, E. Culurciello, and Y. LeCun. Neuﬂow: A run- time reconﬁgurable dataﬂow processor for vision. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on, page 109116, 2011.  [34] T. D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural net-  work. Neural networks, 2(6):459473, 1989.  [35] P. F¨oldi´ak. Forming sparse representations by local anti-hebbian learning. Biological cyber-  netics, 64(2):165170, 1990.  [36] E. Oja. Principal components, minor components, and linear neural networks. Neural Net-  works, 5(6):927935, 1992.  [37] A. J. Bell and T. J. Sejnowski. The independent components of natural scenes are edge ﬁlters.  Vision research, 37(23):3327, 1997.  10  ",We present the clustering learning technique applied to multi-layerfeedforward deep neural networks. We show that this unsupervised learningtechnique can compute network filters with only a few minutes and a muchreduced set of parameters. The goal of this paper is to promote the techniquefor general-purpose robotic vision systems. We report its use in static imagedatasets and object tracking datasets. We show that networks trained withclustering learning can outperform large networks trained for many hours oncomplex datasets.
1206.0387,2013,When Does a Mixture of Products Contain a Product of Mixtures?  ,"['Guido F. Montufar', 'Jason Morton']",https://arxiv.org/pdf/1206.0387.pdf,"4 1 0 2     p e S 8 1         ] L M  . t a t s [      5 v 7 8 3 0  .  6 0 2 1 : v i X r a  When Does a Mixture of Products  Contain a Product of Mixtures?  Guido F. Mont´ufar∗1,2 and Jason Morton†2  1Max Planck Institute for Mathematics in the Sciences, Inselstrasse 22,  2Department of Mathematics, Pennsylvania State University, University  04103 Leipzig, Germany.  Park, PA 16802, USA.  September 22, 2014  Abstract  We derive relations between theoretical properties of restricted Boltzmann machines (RBMs), popular machine learning models which form the building blocks of deep learning models, and several natural notions from discrete mathematics and convex geometry. We give implications and equivalences relating RBM-representable proba- bility distributions, perfectly reconstructible inputs, Hamming modes, zonotopes and zonosets, point conﬁgurations in hyperplane arrangements, linear threshold codes, and multi-covering numbers of hypercubes. As a motivating application, we prove results on the relative representational power of mixtures of product distributions and prod- ucts of mixtures of pairs of product distributions (RBMs) that formally justify widely held intuitions about distributed representations. In particular, we show that a mix- ture of products requiring an exponentially larger number of parameters is needed to represent the probability distributions which can be obtained as products of mixtures. Keywords: linear threshold function, Hadamard product, zonotope, tensor rank, hy- perplane arrangement 2000 MSC: 51M20, 60C05, 68Q32, 14Q15  1  Introduction  Two basic ways of combining probability distributions are mixtures, i.e., convex combina- tions, and Hadamard products, i.e., renormalized entry-wise products. Fixing the number of parameters, we may ask: are Hadamard products of small mixtures better than mixtures  ∗montufar@mis.mpg.de †morton@math.psu.edu  Mixture of Products  k  Product of Mixtures of  Products (RBM)  M6,k  RBM6,4  Figure 1: Graphical representation of a mixture of products and a product of mixtures. The dark nodes represent hidden units and the light nodes represent visible units.  at approximating interesting or complex probability distributions? The general intuition among practitioners is that using Hadamard products allows for more modeling power. We compare two canonical representatives of these model classes: mixtures of product distribu- tions, called na¨ıve Bayes models, and Hadamard products of mixtures of pairs of product distributions, called restricted Boltzmann machines (RBMs). The mixture of products model Mn,k is the union of the convex hulls of all choices of k joint distributions for n independent binary variables (Deﬁnition 2.1). The restricted Boltzmann machine model RBMn,m is the union of the Hadamard products of all choices of m mixtures of pairs of joint distributions for n independent binary variables (Deﬁnition 2.2). Both are graphical probability mod- els with hidden variables and bipartite graphs, see Figure 1. Besides deﬁning probability distributions on their visible states, these graphical models deﬁne conditional distributions between visible and hidden states, which makes them interesting in the context of learning representations. This paper is the result of analyzing following problem. Problem 1.1. When does the mixture of product distributions Mn,k contain the product of mixtures of product distributions RBMn,m, and vice versa?  In answer to the title question, our results in Sections 3 and 4 imply the following.  Theorem 1.2. The number of parameters of the smallest mixture of products Mn,k con- taining the product of mixtures RBMn,m grows exponentially in the number of parameters of the latter for any ﬁxed ratio 0 < m/n < ∞. More precisely, the smallest k such that Mn,k contains RBMn,m is bounded by 3 4n ≤ log2(k) ≤ m when 3  4n ≤ log2(k) ≤ n − 1 when m ≥ n, by 3  4n ≤ m ≤ n, and satisﬁes log2(k) = m when m ≤ 3  4n.  The solution is of order log2(k) = Θ(min{m, n}). Hence the smallest mixture of products model that contains a given RBM model is as large as one can possibly expect, having near to one mixture component per joint state of the RBM hidden units, or otherwise containing every possible probability distribution. See Figure 2 for an illustration of the result. Theorem 1.2 is based on the more technical Theorem 4.2. In Theorem 4.5 we show a complementary result stating that, although RBMs naturally contain small mixture models, in general they do not contain mixture models that match their dimension.  To approach Problem 1.1, we study the sets of modes (Hamming-local maxima) of prob- ability distributions that can be represented as mixtures of product distributions and as  2  Figure 2: Smallest mixtures of products that can represent an RBM. Shown  is the heat map of the logarithm of k(n, m) = min{k(cid:48) ∈ N : Mn,k(cid:48) ⊇ RBMn,m}, depending on n, m ∈ N. The domain of this function has three  regions, each with approximately linear behavior (Theorems 1.2 and 4.2). The model RBMn,m has nm+n+m parameters. Fixing nm+n+m = c (the dashed hyperbola), the RBMs which are hardest to represent as mixtures of product distributions are those with m/n ≈ 1.  RBMs. We consider the following problems, showing in many cases that they are equivalent or equivalent after adding some necessary conditions.  Problem 1.3. What sets of length-n binary vectors are  1. the modes or strong modes (Hamming-local maxima) of probability distributions rep-  resented by an RBM with m hidden units?  2. perfectly reconstructible (given a vector in the set, choosing the most likely hidden state, then the most likely visible state, returns the given vector) by an RBM with m hidden units?  3. the outputs of n linear threshold functions with m inputs?  We ﬁnd that probability distributions with many strong modes (for example, probability distributions strictly supported on the binary vectors with even or odd number of ones), can be represented far more compactly by RBMs than by mixtures of products. Modes are described by linear inequalities of the form p(x) > p(x(cid:48)) and can be used to derive polyhedral approximations of probability models. As it turns out, the analysis of modes is closely related to binary classiﬁcation problems (separation of vertex sets of hypercubes by hyperplane arrangements), and leads to problems such as the following.  Problem 1.4. What is the smallest arrangement of hyperplanes, if one exists, that slices each edge of a hypercube a given number of times?  We consider the following six properties of sets of binary vectors, and derive relations  between them, summarized below in Theorem 1.6.  3  (linearscale)dim(RBMn,m)=cmnm=nm=34n123(1)34n≤log2(k)≤n−1(2)log2(k)=m(3)34n≤log2(k)≤mSP  dH(C) ≥ 2 SM  dH(C) ≥ 2 PR  (cid:96)1  HP  LTC  ZP  Figure 3: Illustration of the implications in Theorem 1.6  threshold functions with m inputs (Deﬁnition 3.23).  Deﬁnition 1.5. Let n and m be two non-negative integers and let C be a subset of {0, 1}n. • LTC(n, m,C): The set C is an (n, m)-linear threshold code, i.e., the image of n linear • HP(n, m,C): There exists an arrangement A of n hyperplanes in Rm such that the ver- tices of the m-dimensional unit cube intersect exactly the C-cells of A (Deﬁnition 3.18). • ZP(n, m,C): There is an m-zonoset (i.e., the aﬃne image of the vertices of an m-cube) in Rn which intersects exactly the C-orthants of Rn (Deﬁnition 3.15). • SM(n, m,C): An RBM with n visible and m hidden nodes can represent a distribution  with set of strong modes C (Deﬁnition 3.4).  • PR(n, m,C): The set C is the set of perfectly reconstructible vectors of an RBM with  n visible and m hidden units (Deﬁnition 3.2).  • SP(n, m,C): An RBM with n visible and m hidden units can represent a distribution  which is strictly positive on C and zero elsewhere.  We derive implications among the properties LTC, PR, HP, ZP, SM, and SP in two cases: the set C is arbitrary, and C consists of vectors which are at least Hamming distance 2 apart.  Theorem 1.6. Let n and m be two non-negative integers and let C be a subset of {0, 1}n.  1. The properties LTC, HP, and ZP are equivalent.  2. If C satisﬁes PR or SM, then it is contained in an LTC set. 3. If the vectors in C are at least Hamming distance 2 apart, then SP implies both SM  and PR.  4. If the vectors in C are at least Hamming distance 2 apart and C satisﬁes an (cid:96)1 property  (see Theorem 3.16), then LTC implies SP.  4  Figure 3 illustrates the result. The proof is given in Section 3.8 by combining results  from Section 3.  Section 2 contains basic deﬁnitions and background on mixtures of product distributions and RBMs. Section 3 discusses geometric perspectives on statistical models and inference, elaborated in various subsections. Section 3.1 discusses inference functions, distributed repre- sentations, and reconstructability. Section 3.2 discusses the concept of modes and polyhedral approximations of probability models. Section 3.3 covers the sets of modes of probability distributions realizable as mixtures of product distributions (Theorem 3.7). Section 3.4 makes a few initial observations on the sets of modes of probability distributions realizable by RBMs. In Section 3.5 these sets are related to zonosets and hyperplane arrangements (Theorem 3.16), and in Section 3.6 to linear threshold codes. Section 3.7 discusses multi- covering numbers; the smallest hyperplane arrangements slicing each edge of a hypercube a given number of times. Section 3.8 contains the proof of Theorem 1.6. Turning to the motivating questions, Section 4 contains our analysis of Problem 1.1, treating the inclusion of RBMs in mixture models in Section 4.1, and the reverse inclusion in Section 4.2. Section 5 oﬀers a discussion.  2 Mixtures of products and products of mixtures  satisfying (cid:80) Let Pn denote the (2n − 1)-dimensional simplex of joint probability distributions of n binary i-th variable is the vector pi ∈ P1 with entries pi(xi) = (cid:80) variables. This is the set of vectors p ∈ R2n with entries p(x) ≥ 0, x = (x1, . . . , xn) ∈ {0, 1}n, x∈{0,1}n p(x) = 1. For any given p ∈ Pn, the marginal distribution of the (x1,...,xi−1,xi+1...xn)∈{0,1}n−1 p(x),  xi ∈ {0, 1}. Let Mn,1 denote the n-dimensional set of joint probability distribution of n indepen- dent binary variables. This is the set of distributions p ∈ Pn that factorize as p(x) = p1(x1)··· pn(xn), x = (x1, . . . , xn) ∈ {0, 1}n, called product distributions. One can regard these distributions as the n-way 2 × ··· × 2 tables of the form p1 ⊗ ··· ⊗ pn, pi ∈ P1, i ∈ [n], where ⊗ denotes the tensor product. We note that Mn,1 is the closure of the exponential Z(B) exp(B(cid:62)x), x ∈ {0, 1}n, with natural parameter B ∈ Rn and normaliza- family pB(x) = 1 y∈{0,1}n exp(B(cid:62)y). Closure is needed in order to include probability  tion function Z(B) =(cid:80) is the set of distributions on {0, 1}n expressible as convex combinations p = (cid:80) where λi ≥ 0,(cid:80)  Deﬁnition 2.1. The k-mixture of product distributions of n binary variables, denoted Mn,k, i∈[k] λiq(i),  i∈[k] λi = 1, and q(i) ∈ Mn,1 for all i ∈ [k].  distributions with vanishing entries.  Up to positive scalar multiples, Mn,k corresponds to the set of n-way 2 × ··· × 2 tables with non-negative rank at most k. The Zariski closure of Mn,k in complex projective space is the k-th secant variety of the n-th Segre product of P1s and has the same dimension as Mn,k. As it turns out, this is the dimension expected from counting parameters, equal to min{nk + (k − 1), 2n − 1}, except when (n, k) = (4, 3), in which case it has dimension 13 instead of 14. See [4], which answered this century-old question in algebraic geometry.  5  The set Mn,k is equal to the probability simplex Pn if and only if k ≥ 2n−1, see [18]. In  particular, the smallest Mn,k that equals Pn has 2n−1(n + 1) − 1 parameters.  Deﬁnition 2.2. The RBM model with n visible and m hidden binary units, denoted RBMn,m, is the closure of the set of distributions on {0, 1}n of the form exp(h(cid:62)W x + B(cid:62)x + C(cid:62)h)  (cid:88)  p(x) =  (1)  1  Z(W, B, C)  h∈{0,1}m  is a vector of biases of the hidden units, and Z(W, B, C) =(cid:80) where W ∈ Rm×n is a matrix of interaction weights between hidden and visible units (with state vectors h and x, respectively), B ∈ Rn is a vector of biases of the visible units, C ∈ Rm h∈{0,1}m exp(h(cid:62)W x + B(cid:62)x + C(cid:62)h) is a normalization function.  x∈{0,1}n  for all x ∈ {0, 1}n, (cid:80)  An RBM is a product of experts [13]; each hidden unit corresponds to an expert which is a mixture of two product distributions [6]. For completeness we provide a proof of this statement:  Proposition 2.3. Each RBM distribution of the form (1) is a renormalized entry-wise product of positive mixtures of pairs of positive product distributions, and vice versa.  Proof. Each distribution p of the form (1) can be written as a renormalized entry-wise product  (cid:80)  p(x) =  q(1)(x)··· q(m)(x)  for all x ∈ {0, 1}n,  y∈{0,1}n q(1)(y)··· q(m)(y) j ∈ Mn,2 is a positive mixture of positive product distributions, j x)/Z(Aj) and pA(cid:48) j), for all j ∈ [m]. To  = exp(A(cid:48)  x)/Z(A(cid:48)  (cid:62)  j  exp(h(cid:62)W x + B(cid:62)x + C(cid:62)h) =  =  ∝  exp(hjWjx + 1  m B(cid:62)x + Cjhj)  m B(cid:62)x) + exp(Cj) exp((Wj + 1  m B(cid:62))x)(cid:1)  (cid:17)  λjpAj (x) + (1 − λj)pA(cid:48)  j  (x)  ,  j  (cid:88) (cid:0)exp( 1 (cid:16)  hj∈{0,1}  (cid:89) (cid:89) (cid:89)  j∈[m]  j∈[m]  j∈[m]  where q(j) = λjpAj +(1−λj)pA(cid:48) with λj ∈ (0, 1), pAj = exp(A(cid:62)  see this, note that  (cid:88)  h∈{0,1}m  where Wj is the j-th row of W , Aj = B/m, A(cid:48) Z(A(cid:48) product distributions is of the form (1). To see this, note that, for any choice of Aj, A(cid:48)  j) exp(Cj)). Conversely, each renormalized entry-wise product of positive mixtures of pairs of positive  j + (B/m), and λj = Z(Aj)/(Z(Aj) +  j = W (cid:62)  j ∈ Rn  6  and λj ∈ (0, 1),  (cid:89)  (cid:16)  j∈[m]  λjpAj (x) + (1 − λj)pA(cid:48)  (cid:17) (cid:18) λj  where B =(cid:80)  ∝ j∈[m] Aj, Wj = A(cid:48)  =  ∝  (cid:19)  j  j  (cid:62)  x)  (x)  j∈[m]  (cid:18)  Z(Aj)  j x) +  exp(A(cid:48)  exp(A(cid:62)  (1 − λj) Z(A(cid:48) j)  (cid:89) (cid:19) (cid:89) j − Aj)(cid:62)x) (cid:88) j)(cid:1). j − Aj, and Cj = log(cid:0)Z(Aj)(1 − λj)/λjZ(A(cid:48)  (1 − λj) Z(A(cid:48) j) exp(h(cid:62)W x + B(cid:62)x + C(cid:62)h),  exp((A(cid:48)  h∈{0,1}m  exp(Aj  Z(Aj)  (cid:62)x)  j∈[m]  1 +  λj  An RBM can also be seen as a set of restricted mixtures of product distributions; each  p ∈ RBMn,m is a mixture of 2m product distributions, namely the conditionals  (cid:80)  p(x|h) =  exp((h(cid:62)W + B(cid:62))x)  y∈{0,1}n exp((h(cid:62)W + B(cid:62))y)  = pW (cid:62)h+B(x)  for all h ∈ {0, 1}m.  In general, the dimension of the mixture model Mn,2m is much larger than that of RBMn,m. The set RBMn,m is known to have dimension nm + n + m when m < 2n−(cid:100)log2(n+1)(cid:101), and 2n − 1 when m ≥ 2n−(cid:98)log2(n+1)(cid:99), see [6]. In addition, it is known that RBMn,m equals Pn whenever m ≥ 2n−1 − 1, see [19]. It is not known if the latter bound is always tight, but it shows that the smallest RBMn,m that equals Pn has not more than 2n−1(n + 1) − 1  parameters, and hence not more than the smallest mixture of products model.  We will show that the sets of probability distributions representable by RBMs and mix- tures of products are quite diﬀerent. The intersection of both model classes has been studied in [22], where it is shown that RBMn,m contains any mixture of m + 1 product distributions with disjoint supports, and hence that the intersection RBMn,m ∩Mn,m+1 has dimension of order at least mn + m + 2n + 1 − (m − 1) log2(m + 1). Typically RBMs have many binary hidden variables and mixtures of products models have a single multivalued hidden vari- able. The transition between these two limit cases has been studied in [20], focusing on the Kullback-Leibler model approximation errors and the model dimension.  3 Geometric perspectives  In this section we present ﬁve points of view on the families of probability distributions deﬁned in the previous section. We consider inference functions and hidden representations deﬁned by these models (Section 3.1), modes and strong modes of their marginal distributions (Sections 3.2, 3.3, and 3.4), zonosets, hyperplane arrangements, and linear threshold codes that capture their combinatorial structure (Sections 3.5 and 3.6), and the resulting multi- covering numbers of hypercubes (Section 3.7).  Each point of view comes with particular set of related tools and implications for the capabilities of RBMs and competing models. We determine how these ﬁve concepts are  7  Figure 4: Inference regions of M2,4 (left) and RBM2,3 (right), for a choice of parameters.  related, which imply which, and how properties such as the number of strong modes in a marginal distribution translate into each perspective. These observations are summarized in Section 3.8, and in Section 4, where they are applied to distinguish mixtures of products and products of mixtures.  3.1  Inference functions, distributed representations, reconstructability  Hinton [13] discusses advantages of products of experts (Hadamard products of probabil- ity models) over mixtures of experts (mixtures of probability models), for modeling “high- dimensional data which simultaneously satisﬁes many low-dimensional constraints.” In prod- ucts of experts models, each expert can individually ensure that one constraint is satisﬁed. In the case of RBMs, each hidden unit linearly divides the input space according to its pre- ferred state given the input, which results in a multi-clustering, or a partition of the input space into cells where diﬀerent joint hidden states are most likely. Inference of the most likely hidden state given an input produces a distributed encoding or distributed representation of the input vector, as discussed by Bengio in [2, Section 5.3].  Deﬁnition 3.1. The inference function of a probability model pθ(v, h) with parameter  θ ∈ RN ‘explains’ each value of v by the most likely value of h according to upθ : v (cid:55)→ argmaxh pθ(h|v). This deﬁnes a partition of the input space into the preimages of all possi- ble outputs, called inference regions.  Inference functions provide a combinatorial view on the corresponding probability models. They appear naturally in the context of tropicalization, where they correspond to the linear regions of certain piecewise linear approximations of algebraic varieties and serve to estimate their dimension. This approach has been studied in [8] in the context of secants and in [6, 20, 21] in the context of RBMs.  For each choice of parameters W ∈ Rm×n, B ∈ Rn, C ∈ Rm, the model RBMn,m deﬁnes  the inference function  upW,B,C : Rn ⊃ {0, 1}n → {0, 1}m; v (cid:55)→ argmaxh∈{0,1}m h(cid:62)(W v + C) .  8  (00)(01)(11)(10)1234(00)(01)(11)(10)(111)(100)(001)(010)(011)(110)(101)1 2  The visible state v is explained by the hidden state h which satisﬁes sgn(W v +C) = sgn(h(cid:62) − 1), where 1 := (1, . . . , 1). There may be several explanations for a given observation, but generically there is only one. Geometrically, the input space Rn is partitioned into the (cid:1), (cid:0)m orthants of Rm). The number of inference regions can be as large as Caﬀ(m, d) =(cid:80)d preimages of the orthants of Rm by the aﬃne map ψ : Rn → Rm; v (cid:55)→ W v +C. This partition  corresponds to the intersection of an aﬃne space and the normal fan of an m-cube (the  which is the number of orthants of Rm intersected by a generic d-dimensional aﬃne subspace, where d ≤ min{n, m} is the rank of W . When the rank of W is less than m (for example, when m > n), then the image of the map ψ does not intersect all orthants of Rm and there are ‘empty’ inference regions, i.e., states h which are not the explanation of any input vector v.  i=0  i  inference function  v∈{0,1}n exp(B(cid:62)  i v − log(Z(Bi)) + log(λi)),  upλ,B : Rn ⊃ {0, 1}n → {1, . . . , k}; v (cid:55)→ argmaxi∈[k](B(cid:62)  The mixture model Mn,k, on the other hand, deﬁnes, for any choice of the mixture weights λi and the natural parameters of each mixture component Bi ∈ Rn for i ∈ [k], an where Z(Bi) =(cid:80) i v). In this case, the input space Rn is partitioned into the at most k regions of linearity of the function v (cid:55)→ max{B(cid:62) i v − log(Z(Bi)) + log(λi) : i ∈ [k]}. This partition corresponds to the intersection of an aﬃne space and the normal fan of a (k − 1)-simplex. Figure 4 shows an example of inference regions in {0, 1}2 ⊂ R2 deﬁned by M2,4 (left panel) and RBM2,3 (right panel), for some speciﬁc parameter values. Both models have 7 parameters and are universal approximators of distributions on {0, 1}2, but they deﬁne very (cid:1)), which is exponential in the number of be realized by RBMn,m is of order Θ((cid:0) diﬀerent inference regions. For a ﬁxed input space of dimension n, the number of inference regions in Rn that can Mn,k is linear in the number of parameters of the model. A function g : R+ → R+ is of order Θ(f ) if there exist positive constants C, C(cid:48) and n0 such that Cf (n) ≤ g(n) ≤ C(cid:48)f (n) for all n ≥ n0 [16]. Distributed representations can, in principle, learn diﬀerent explanations to a number of observations that is exponential in the number of model parameters, see [2]. Now we discuss reconstructability. Similarly to the upθ inference function, a model pθ(v, h) deﬁnes a downθ inference function, which outputs the most likely visible state argmaxv pθ(v|h) given a hidden state h. Deﬁnition 3.2. Given a probability model pθ(v, h) on v ∈ X and h ∈ Y, a collection of states C ⊆ X is perfectly reconstructible if there is a choice of the parameter θ for which downθ(upθ(v)) = v for all v ∈ C.  parameters of the model, whereas the number of inference regions that can be realized by  min{n,m}  m  The ability to reconstruct input vectors is sometimes used to evaluate the performance of RBMs in practice, since it can be tested more cheaply than the probability distributions they represent. The reconstructability of input vectors can also be used to deﬁne training algorithms, like in the case of auto-encoders.  When writing the joint probabilities (pθ(v, h))v,h as a matrix with rows labeled by h ∈ Y and columns by v ∈ X , a set C ⊆ X is perfectly reconstructible iﬀ there is a choice of the  9  model parameter θ for which pθ(v, upθ(v)) is the unique maximal entry in the upθ(v)-row (and in the v-column) for all v ∈ C. For the model RBMn,m, this is the case exactly when for each v ∈ C ⊆ {0, 1}n there is an hv ∈ {0, 1}m with sgn(W v + C) = sgn(hv − 1 1) and sgn(h(cid:62)  1).  2  v W + B(cid:62)) = sgn(v − 1  2  Example 3.3. If n, m ≥ k, all cylinder subsets of {0, 1}n of dimension k are perfectly reconstructible by RBMn,m. A k-dimensional cylinder subset of {0, 1}n is a set of the form {x ∈ {0, 1}n : xi = yi for all i ∈ I}, where I is a subset of [n] of cardinality |I| = n − k, and yi ∈ {0, 1}, i ∈ I are ﬁxed values. Without loss of generality let I = {k + 1, . . . , n}. Consider the following choice of parameters. Deﬁne Wi,j = δi,j for i, j ≤ k and zero else, deﬁne Bj = − 1 1. Then sgn(W v +C) = sgn(v1− v W +B(cid:62)) = 2,− 1 2, . . . , vk− 1 sgn(v1 − 1 2, . . . , vk − 1  2 for j ≤ k and Bj = yj− 1 2 else, and C = − 1 2, . . . ,− 1 2, . . . , yn − 1 2).  2), so that hv = (v1, . . . , vk, 0, . . . , 0). Furthermore, sgn(h(cid:62)  2, yk+1 − 1  1  2  Later we will study the ability of RBMs to reconstruct more complicated sets of binary in- puts, and how the sets of reconstructible inputs relate to the visible probability distributions represented by the model.  3.2 Modes  We will characterize the ability of RBMs and mixtures of product distributions to represent distributions with many strong modes, in order to draw a distinction between them. As an interesting side remark, note that similar questions, about the number of modes of mixtures of multivariate normal distributions, have been posed in [28], and that the maximal number of modes realizable by mixtures of k normal distributions on Rn is unknown.  mode if p(x) > (cid:80)  Deﬁnition 3.4. Let p be a probability distribution on a ﬁnite set X of length-n vectors. A vector x ∈ X is a mode of p if p(x) > p(y) for all y ∈ X with dH(y, x) = 1, and a strong y∈X :dH (y,x)=1 p(y). Here dH(y, x) := |{i ∈ [n] : yi (cid:54)= xi}| is the Hamming distance between y and x.  The modes of a distribution are the Hamming-locally most likely events in the space of possible events. Modes are closely related to the support sets and boundaries of statistical models, which have been studied especially for hierarchical and graphical models without hidden variables [12, 15, 27].  We write Gn,m (and Hn,m) for the set of distributions in Pn which have at least m modes (strong modes). For any set C ⊂ {0, 1}n of vectors with Hamming distance at least 2 from each other, we write GC (and HC) for the set of distributions which have modes (strong modes) C. The closures GC (and HC) are convex polytopes inscribed in the probability simplex Pn. The sets of modes that are not realizable by a probability model give a full dimensional polyhedral approximation of the model’s complement. See Figure 5 for an example. We will focus most of our consideration on strong modes. These are easier to study than modes, because they are described by fewer inequalities.  The minimum Hamming distance of a set C ⊆ X is deﬁned as dH(C) := min{dH(x, y) : x (cid:54)= y and x, y ∈ C}. Since any two modes have at least Hamming distance two from each other, a distribution on {0, 1}n has at most 2n−1 modes. There are exactly two subsets of {0, 1}n  10  δ(00)  G+  2  δ(11)  M2,1  δ(01)  − 2  G  δ(10)  Figure 5: The 3-dimensional simplex of probability distributions on {0, 1}2 (a tetrahedron with vertices corresponding to the outcomes (00), (01), (10), (11)), with three sets of probability distributions depicted. The dark curved surface is the 2-dimensional manifold M2,1 of product dis- tributions of two binary variables. The angular regions at the top and bot- − tom are the polyhedra G+ 2 of distributions with two modes. An in- teractive 3-D graphic object is available at http://personal-homepages. mis.mpg.de/montufar/surface.pdf.  2 and G  with cardinality 2n−1 and minimum distance two. These are the sets of binary strings with an even, respectively odd, number of entries equal to one:  (cid:110) (cid:110) (x1, . . . , xn) ∈ {0, 1}n : (x1, . . . , xn) ∈ {0, 1}n :  (cid:88) (cid:88)  i∈[n]  i∈[n]  (cid:111) (cid:111)  .  xi is even  ;  xi is odd  Z+,n  :=  Z−,n  :=  2 ∪ G  − n for short, and similarly Hn = H+  − We write Gn,2n−1 = GZ+,n ∪ GZ−,n, or Gn = G+ n . n ∪ G n ∪ H − Figure 5 illustrates the set G2 = G+ 2 ⊂ P2 (the set of distributions on {0, 1}2 with two modes), and the two-dimensional manifold M2,1 ⊂ P2 (the set of product distributions on {0, 1}2). We see that P2 contains 6 disjoint sets congruent to G+ 2 whose union’s closure equals P2. Hence the Lebesgue volume satisﬁes vol(G+ 2 )/ vol(P2) = 1/6. The case of three bits is as follows. Example 3.5. The subset G+ 3 ⊂ P3 of distributions on {0, 1}3 with four modes Z+,3 is the intersection of P3 and 12 open half-spaces deﬁned by p(x) > p(y) for all y with dH(x, y) = 1 for all x ∈ Z+,3. The closure of this set is a convex polytope G+ 3 with 19 vertices. The vertices are uniform distributions on subsets of {0, 1}3 that can be covered by three disjoint cylinder subsets of {0, 1}3. The list of vertices and vertex-facet incidences are provided in Tables 1 and 2, in the appendix. The Lebesgue volume of this polytope can be computed (e.g., using the software Polymake [11]): vol(G+  3 )/ vol(P3) = 1/56 = 0.017857142.  11  P3 and the 4 open half-spaces deﬁned by p(x) >(cid:80)  3 ⊂ G+ discuss it in Section 3.5.1.  The subset H+  3 of distributions with four strong modes Z+,3 is the intersection of y:dH (x,y)=1 p(y) for all x ∈ Z+,3. We will  3.3 Modes of mixtures of products  In this section we characterize the sets of modes and strong modes that can appear in mixtures of product distributions, and show how these can be used to obtain a polyhedral approximation of the set of probability distributions representable in such models.  Problem 3.6. What is the smallest k ∈ N for which Mn,k contains a distribution with l  (strong) modes?  A mixture of k unimodal discrete probability distributions has at most k strong modes.  For mixtures of products we have the following.  Theorem 3.7. Let X1, . . . ,Xn be ﬁnite sets. Let M be the set of all possible mixtures of k product distributions of n variables with state spaces Xi, i ∈ [n]. If p ∈ M has strong modes C, then every c ∈ C is the mode of one mixture component of p. The sets of strong modes of distributions within M are exactly the sets of strings in X1×···×Xn of minimum Hamming distance at least two and cardinality at most k.  Proof. A product distribution q has at most one mode. This follows from the fact that the value of q(x1, . . . , xn) = q1(x1)··· qn(xn) is either maximal, or can be increased by changing only one entry of x. If q(j), j ∈ [k], are product distributions and x is not a mode of any j∈[k] αjq(j)(x) for any αj ≥ 0. In turn, x is not a strong mode of any mixture of the q(j). On the other hand, the set of product distributions contains every point measure δy, since the latter can be written as δy(x) = q1(x1)··· qn(xn) with qi(yi) = 1 for all i ∈ [n]. Hence the mixture of products model M contains every  q(j), then(cid:80) (cid:80) distribution of the form(cid:80)  j∈[k] αjq(j)(y) ≥  (cid:80)  y:dH (y,x)=1  y∈C 1|C|δy for any C ⊆ X with |C| ≤ k.  By the previous theorem, a mixture of k product distributions can have at most k strong modes. Nevertheless, a mixture of k product distributions can have more than k modes. Here is an example:  Example 3.8. The mixture p = 1 four binary variables has three modes (0000), (1100), and (1111):  2p(1) + 1  2p(2) of the following two product distributions of  p(1) = 1  5 ( 3  2 ) ⊗ 1  5 ( 3  2 ) ⊗ ( 1  p(2) = ( 0  1 ) ⊗ ( 0  1 ) ⊗ 1  5 ( 2  0 ) ⊗ ( 1 3 ) ⊗ 1  5 ( 2  0 ) = 1 25  3 ) = 1 25  (cid:20) 9 6 (cid:20) 0 0  6 4 0 0 0 0  0 4 0 0 0 6  (cid:21) (cid:21)  ;  .  0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 9  Theorem 3.7 shows that the set Hn,k+1 is in the complement of Mn,k for all k. We can triangulate Hn,k+1 and thereby lower bound the (Lebesgue) volume of the complement Pn \ Mn,k ⊇ Hn,k+1. A rough estimate is:  12  Proposition 3.9. Let k < 2n−1. The volume of Hn,k+1 satisﬁes vol(Hn,k+1)/ vol(Pn) ≥ 2−(k+1)nK(k +1), where K(k +1) = 2k+1 if k +1 ≤ 2s < 2n n for some s ∈ N, and K(k +1) = 2  otherwise.  Proof. Let Y ⊆ X := {0, 1}n. Let P(Y) be the simplex of probability distributions with support in Y. This is a regular (|Y| − 1)-simplex in R|X| with edge-length √2. Let H(Y) denote the set of distributions on X with strong modes Y, such that H(Y) = ∩y∈YH(y). Let B1(y) ⊆ X denote the radius-1 Hamming ball centered at y. The set Py(B1(y)) := {p ∈ √ 2 2 and vertices P(B1(y)) : p(y) ≥ p(B1(y) \ {y})} is a regular n-simplex with edge-length √ { 1 2N lN . The 2(δy + δˆy)}dH (ˆy,y)≤1. The volume of a regular N -simplex with edge-length l is set H(y) is the convex hull of Py(B1(y)) and P(X \ B1(y)). Its volume satisﬁes  √ N +1  N !  vol(H(y))/ vol(Pn) = vol(Py(B1(y)))/ vol(P(B1(y))) = 2−n.  If Y has minimum distance 3 or more, then the radius-1 Hamming balls B1(y), y ∈ Y,  are disjoint, and  vol(H(Y))/ vol(Pn) =  (vol(Py(B1(y)))/ vol(P(B1(y)))) = 2−|Y|n.  (cid:89)  y∈Y  If Y has minimum distance 2 instead of 3, then the volume of H(Y) can only go up. To see this, consider a pair y, y(cid:48) of Hamming distance 2 and let Py,y(cid:48)(B1(y) ∪ B1(y(cid:48))) := {p ∈ P(B1(y) ∪ B1(y(cid:48))) : p(y) ≥ p(B1(y) \ {y}), p(y(cid:48)) ≥ p(B1(y(cid:48)) \ {y(cid:48) })} denote the closure of the set of distributions with support in B1(y) ∪ B1(y(cid:48)) and strong modes y and y(cid:48). Then  vol(Py,y(cid:48)(B1(y) ∪ B1(y(cid:48))))/ vol(P(B1(y) ∪ B1(y(cid:48)))) > 2−2n.  This is because, for any z with Hamming distance one to both y and y(cid:48), the inequality p(z) < 1/2 implied by H(y) is also implied by H(y(cid:48)) and hence not each pair of neighbors of the form (y, x) or (y(cid:48), x) translates to a factor 1/2 in the volume. The number K(k + 1) is a lower bound on the number of disjoint sets H(Y) with |Y| = k + 1. By the Gilbert-Varshamov bound, if k + 1 ≤ 2s for some integer s with 2s < 2n n , (cid:48) = then there is a set Y ⊂ X of cardinality |Y| = k + 1 and minimum distance 3. Let Y (cid:48)) = ∅. (Y \{y})∪{y ⊕2 e1} (ﬂip one coordinate of one element of Y), such that H(Y)∩H(Y Since Y has (k + 1) elements, there are 2k+1 disjoint sets of this form. For a general Y ⊂ X of cardinality |Y| = k +1 ≤ 2n−1 and minimum distance 2, the set Y ⊕2 e1 also has minimum distance 2 and cardinality |Y 3.3.1 Polyhedral approximation of the full-dimensional model M3,3 Theorem 3.7 shows that any p ∈ M3,3 has at most three strong modes. We can show that this is true for modes too.  | = k + 1. These two sets satisfy H(Y) ∩ H(Y  (cid:48)) = ∅.  (cid:48)  (cid:48)  Proposition 3.10. The mixture model of three product distributions on {0, 1}3 cannot realize distributions with four modes: M3,3 ∩ G3 = ∅.  13  3  2, pi  Proof. Assume that M3,3 ∩ G+ 1, pi (pi conv{q1, q2} intersects G+ The mixture of q2 and q3 intersects G and q3. Similarly, conv{q1, q2} intersects G+ q1 and q2; a contradiction.  (cid:54)= ∅. By the Lemma 3.11 given below, there are factors − 3}i=1,2,3 intersects G+ 2pi 3) ∈ (P1)3, i = 1, 2, 3 such that conv{qi := pi 2 . Hence − 2 and conv{q2, q3} intersects G 2 (for some enumeration of q1, q2, q3). − 2 only if (01) and (10) are the unique maxima of q2 2 only if (11) and (00) are the unique maxima of  2 and G  The proof of Proposition 3.10 uses the following lemma, which relates the number of modes realizable by Mn,k to the number of modes simultaneously realizable on subsets of variables.  Lemma 3.11. Let n, k ∈ N and n ≥ 2. Let p =(cid:80)  i∈[k] λi  j ∈ Mn,k, with λi ≥ 0 and j ∈ P1 for all (i, j) ∈ [k]× [n]. If p has 2n−1 modes, then for any subset of variables I (cid:40) [n], pi j}i∈[k] ⊂ Pm intersects both G+ |I| = m, the convex hull of the product distributions { and G Proof. We show the special case with I = {1, . . . , n − 1}. The proof of the general case is a straightforward generalization. Any q ∈ Mn,k has the following form:  j∈I pi  − m.  m  j∈[n] pi  (cid:81)  (cid:81)  k(cid:88) for all (x1, x2, . . . , xn) ∈ {0, 1}n, where (cid:80)k  q(x1, x2, . . . , xn) =  i=1  λipi  1(x1)pi  2(x2)··· pi  n(xn),  k(cid:88)  i=1  n(xn),  λ0,ipi  2(x2)··· pi  and c0 =(cid:80)k  j ∈ P1. For the ﬁxed value x1 = 0 this is a mixture of k products with (n − 1) variables, multiplied by a positive constant:  i=1 λi = 1, λi ≥ 0 and pi  where(cid:80)k  q(x1 = 0, x2, . . . , xn) = c0  c0  1(x1=0)  i=1 λ0,i = 1, λ0,i ≥ 0 with λ0,i = λipi  n−1 and q(x1 = 1, x2, . . . , xn) ∈ G  i=1 λ0,ipi 1(x1 = 0). A similar observation can be made for the ﬁxed value x1 = 1. If the distribution q is contained in G+ n , − then q(x1 = 0, x2, . . . , xn) ∈ G+ n−1, since q ∈ G+ n . These two 2 ··· pi conditional distributions are mixtures of the same k product distributions {pi n}i∈[k], even though they may have diﬀerent mixture weights. − Remark 3.12. The sets G+ 3 are intersections of half-spaces that contain the uniform distribution. Although M3,3 is full dimensional in P3, by Proposition 3.10 the complement of M3,3 contains points arbitrarily close to the uniform distribution!  3 and G  3.4 Modes of RBMs  In the following we characterize the sets of modes and strong modes that can appear in RBM-distributions. In analogy to the problem posed in the last section, we ask:  Problem 3.13. What is the smallest m ∈ N for which RBMn,m contains a distribution with  l (strong) modes?  14  In particular, what is the smallest m for which the model RBMn,m can represent the parity function? By Theorem 3.7 and RBMn,m ⊆ Mn,2m, any p ∈ RBMn,m has at most min{2m, 2n−1} strong modes. We will see that this bound is not always tight, but often. In special cases, the analysis of mixtures of products (Section 3.3) is suﬃcient to make statements about RBMs, for example: The model RBM4,2 is contained in M4,4 and has co- dimension one in P4. Its algebraic implicitization was studied in [7], i.e., its description as the set of zeros of a collection of polynomials. It was found to be the zero locus of a polynomial of degree 110 with as many as 5.5 trillion monomials. By Theorem 3.7, M4,4 ∩ H4 = ∅ and so RBM4,2 ∩H4 = ∅. Using Proposition 3.10 and Lemma 3.11 one can show: Proposition 3.14. The models M4,4 and RBM4,2 cannot realize probability distributions with 8 modes: M4,4 ∩ G4 = ∅ and RBM4,2 ∩G4 = ∅.  We note that the model RBMn,m contains any distribution with support of cardinality min{m + 1, 2n} [19, Theorem 1]. Therefore, it contains some distributions with min{m + 1, 2n−1} strong modes. For example, RBMn,m contains any uniform distribution on a set of cardinality min{m + 1, 2n−1} and minimum Hamming distance 2. In particular, whenever Mn,k+1 contains a distribution with strong modes C, then also RBMn,k contains a distribution with strong modes C. Note also that, since the model RBMn,m is symmetric under relabeling of any of its variables, there is an RBM distribution with strong modes C iﬀ there is one with strong modes C ⊕2 x = {c + x mod (2) : c ∈ C} for any x ∈ {0, 1}n. In general, characterizing the sets of modes realizable by RBMs is a more complex prob- lem than it was for mixtures of product distributions, and will necessitate developing charac- terizations in terms of point conﬁgurations called zonosets (Deﬁnition 3.15) and hyperplane arrangements, or in terms of linear threshold functions. We elaborate these notions and characterizations in the next 3 sections.  (cid:80)  3.5 Zonosets and hyperplane arrangements Deﬁnition 3.15. Let m ≥ 0, n > 0, Wi ∈ Rn for all i ∈ [m], and B ∈ Rn. The multiset Z = {  i∈I Wi + B}I⊆[m] is called an m-zonoset.  The convex hull of a zonoset is a zonotope, a well known object in the literature of polytopes. Zonotopes can be identiﬁed with hyperplane arrangements and oriented ma- troids [3, 35].  Given a sign vector s ∈ {−, +}n, the s-orthant of Rn, denoted Rn  s , consists of all vectors x ∈ Rn with sgn(x) = s. We say that an orthant has even (odd) parity if its sign vector has  an even (odd) number of +. The sets of strong modes of RBMs can be described in terms of zonosets as follows.  Theorem 3.16. Let C ⊂ {0, 1}n have minimum Hamming distance at least two.  • If the model RBMn,m contains a distribution with strong modes C (i.e., RBMn,m ∩HC (cid:54)= ∅), or C has cardinality 2m and is perfectly reconstructible by RBMn,m, then there is an m-zonoset with a point in each C-orthant of Rn.  15  • If there is an m-zonoset intersecting exactly the C-orthants of Rn at points of equal (cid:96)1-norm, then RBMn,m ∩HC (cid:54)= ∅ and, furthermore, C is perfectly reconstructible.  2m  2K = 1  2(cid:107)h(cid:62)W + B(cid:62)  2B(cid:62)(1, . . . , 1)(cid:62), where C = − 1  Proof. Assuming that p ∈ RBMn,m ∩HC, for each x ∈ C there is an h ∈ {0, 1}m for which p(·|h) is uniquely maximized by x (Theorem 3.7 and RBMn,m ⊂ Mn,2m). This is also true if C is perfectly reconstructible. In this case, (h(cid:62)W + B(cid:62))x > (h(cid:62)W + B(cid:62))v for all v (cid:54)= x, 2(1, . . . , 1)(cid:62)). The existence of such W and B and, equivalently, sgn(h(cid:62)W + B(cid:62)) = sgn(x − 1 is equivalent to the existence of a zonoset with a point in each C-orthant of Rn. Assume now that W, B can be chosen such that all vectors h(cid:62)W + B(cid:62) have the same (cid:107)1 = (h(cid:62)W + B(cid:62))(xh − 1 2(1, . . . , 1)(cid:62)) = (cid:96)1 norm, equal to K. We have 1 (cid:80) (h(cid:62)W + B(cid:62))xh + h(cid:62)C − 1 2W (1, . . . , 1)(cid:62), for some xh ∈ C 2(1, . . . , 1)(cid:62), and for all h ∈ {0, 1}m. The RBM with parameters αW, αB, C = −αW 1 α → ∞ produces 1 h∈{0,1}m δxh ∈ HC as its visible distribution. This also implies that C is perfectly reconstructible. Remark 3.17. The ﬁrst part of Theorem 3.16 remains true if HC is extended to the set of distributions for which any Mn,2m-decomposition has a mixture component with mode c, for every c ∈ C. Deﬁnition 3.18. A hyperplane arrangement A in Rn is a ﬁnite set of (aﬃne) hyperplanes {Hi}i∈[k] in Rn. Choosing an orientation for each hyperplane, each vector x ∈ Rn receives a sign vector sgnA(x) ∈ {−, 0, +}k, where (sgnA(x))i indicates whether x lies on the negative side, inside, or on the positive side of Hi. The set of all vectors in Rn with the same sign vector is called a cell of A. A necessary condition for the existence of an m-zonoset intersecting all C-orthants of Rn is that the number of orthants of Rn that are intersected by an m-dimensional aﬃne space is at least |C|. The maximal number of orthants intersected by a d-dimensional linear subspace of Rn, denoted C(n, d), was derived in [31]. It is not diﬃcult to derive the corresponding number for a d-dimensional aﬃne subspace, denoted Caﬀ(n, d), as well:  (cid:18)n − 1 (cid:19)  i  d−1(cid:88)  i=0  (cid:18)n  (cid:19)  d(cid:88)  i  i=0  C(n, d) = 2  and  Caﬀ(n, d) =  .  (2)  Cover [5] shows that C(n, d) is also the number of partitions of an n-point set in general position in Rd by central hyperplanes (hyperplanes through the origin). A set of vectors in Rd is in general position if any d or less are linearly independent. Dually, Caﬀ(n, d) can be seen as the number of cells of a real d-dimensional arrangement of n hyperplanes in general position [26, 30].  In particular, there are aﬃne hyperplanes of Rn intersecting all but one orthants. Figure 4 (right) is an example showing the intersection of a 2-dimensional aﬃne subspace of R3 and 7 orthants; four of odd parity and three of even parity. This does not imply, however, that every collection of 2m even, or odd, orthants can be intersected by an m-zonoset. For example:  Proposition 3.19. If n is an odd natural number larger than one, then there is no (n − 1)- zonoset with a point in every even, or every odd, orthant of Rn.  16  Proof. Let Z be a candidate zonoset; Z has (n − 1) generators, so it lies in an aﬃne hyper- plane H of Rn. Let η be a normal vector to H. Assume ﬁrst that 0 ∈ H. All vectors in the orthants sgn(η) and −sgn(η) lie outside H (where we may assign arbitrary sign to zero entries of η). This follows from Stiemke’s theorem, see, e.g., [10]. The two orthants have opposite sign vectors and n is odd, so one orthant is even and the other odd. Hence at least one even and one odd orthants do not intersect Z. Consider now an aﬃne subspace H, and assume it intersects all even orthants. By eq. (2) dim(H) ≥ n − 1, so H is a hyperplane. Assume without loss of generality that a in a d-dimensional arrangement of n hyperplanes in general position, b(n, d) = (cid:0)n−1 normal vector to H has only negative entries. Then H ∩ Rn (−···−) is an (n − 1)-dimensional simplex containing a point of Z. This can be inferred from the number of bounded cells Proposition 2.4]. The orthant Rn (−···−) is separated by (n−1) coordinate hyperplanes from the orthant Rn +··· +) for any i ∈ [n]. Since n is odd and larger than one, (n−1) > 0 is even. Since Z intersects H ∩Rn (−···−) ⊂ conv(Z) (details in Lemma 3.20). On the other hand, the (n − 1)-generated zonotope of dimension (n − 1) is combinatorially equivalent to the (n − 1)-cube, and no point in its zonoset is contained in the convex hull of any other points.  for all i ∈ [n], also H ∩Rn  with sign si = (+··· +−i  (cid:1) [32,  si  si  d  We used the following lemma in the proof of Proposition 3.19.  i , then the polytope conv({v(cid:48)  i . If v(cid:48) is any point in ∩iH−  Lemma 3.20. Let P be a polytope with vertex set V . Let {Hi}r i=1 the supporting hyperplanes of the facets of P incident to a vertex v ∈ V , and assume P is contained in the intersection of closed half-spaces ∩iH + }∪ (V \{v})) contains P . Proof. The case v(cid:48) = v is trivial, so let v(cid:48) (cid:54)= v. It is suﬃcient to show that v is not a vertex of Q := conv({v(cid:48) } ∪ (V \ {v})) follows. The point v(cid:48) is a vertex of Q, because v(cid:48) (cid:54)∈ P . Consider ﬁrst the case where v(cid:48) is in the interior of ∩H− i . If v was a vertex of Q, then one Hi would support a facet of Q (otherwise v(cid:48) would be incident to all facets ∈ ∩H− incident to v). This would contradict the fact that v(cid:48) results from continuity.  i , which is to say that v(cid:48) is not contained in any H +  } ∪ (V \ {v})) and P ⊆ conv({v(cid:48)  } ∪ V ), from which v ∈ conv({v(cid:48)  i . The general case v(cid:48)  (cid:54)∈ H +  i  (cid:48).  Proposition 3.19 allows us to describe some distributions that cannot be represented by ⊂ {0, 1}r, r ≤ n, if restricting C to some  RBMs. A code C ⊂ {0, 1}n extends another code C r indices yields C Corollary 3.21. If m is an even non-zero natural number and m < n, then RBMn,m ∩HC = ∅ for any code C ⊂ {0, 1}n extending Z+,m+1 or Z−,m+1. In particular, when n is an odd natural number larger than one, RBMn,n−1 cannot represent any distribution with 2n−1 strong modes.  (cid:48)  Proof. If there is an m-zonoset with points in every C-orthant of Rn and there is a restriction of C to Z+,m+1 or Z−,m+1, then there is an m-zonoset contradicting Proposition 3.19. By Theorem 3.16, RBMn,m cannot represent distributions with strong modes C.  17  As a side remark, Corollary 3.21 implies, in particular, that the graphical probability model on the bipartite graph Kn,m (a fully observable version of the RBM model) does not contain in its closure any distribution supported on a set Y ⊂ {0, 1}n+m with  {(xi1, . . . , xim+1) ∈ {0, 1}m+1 : (x1, . . . , xn, xn+1, . . . , xn+m) ∈ Y} = Z±,m+1  for some 1 ≤ i1 < ··· < im+1 ≤ n. that RBM6,5 cannot represent distributions with 26−1 strong modes.  In Section 3.7 (Corollary 3.34) we extend the statement of Corollary 3.21 by showing  3.5.1 Polyhedral approximation of the full-dimensional model RBM3,2  The model RBM3,2 is particularly interesting, because it is the smallest candidate of an RBM universal approximator on {0, 1}3 in terms of the number of mixture components of the mixtures of products that it represents, but it has less than 2n−1 − 1 hidden units, the upper bound for the number of hidden units of the smallest RBM universal approximator given in [19]. Note that the model RBM3,1 = M3,2 is readily full dimensional. We illustrate this explicitly: By Theorem 3.16, if RBM3,2 ∩H3 (cid:54)= ∅, then  By Corollary 3.21, RBM3,2 does not contain any distribution with four strong modes.    sgn   =  + − −  − + − − − + + + +    B  W1 + B W2 + B  W1 + W2 + B  (3)  The set H3 is the disjoint union of H+  up to permutations of rows. But it is quickly veriﬁed that this equation cannot be satisﬁed. is the 7-dimensional simplex deﬁned by the intersection of the 8 half-spaces with inequalities Its vertices are the p(z) ≥ uniform distributions on the following sets:  y:dH (z,y)=1 p(y) for z ∈ Z+,3 and p(y) ≥ 0 for all y ∈ Z−,3.  − 3 = HZ−,3. The set H+  3 = HZ+,3 and H  (cid:80)  3  {000, 001, 011, 101},{011, 101, 110, 111},{000, 010, 011, 110},{000, 100, 101, 110},  {000},{011},{101},{110}.  Denote these distributions by u1, . . . , u8 ∈ P3 ⊂ R8. The volume of H+  3 satisﬁes  1 256  = 0.00390625.  3 )/ vol(P3) = det(u1, . . . , u8) =  vol(H+ − 3 are congruent, we obtain vol(H3)/ vol(P3) = 1  3 and H  Since H+ lower bound for vol(P3 \ RBM3,2)/ vol(P3). Now let us brieﬂy discuss to what extent H3 exhausts the complement of RBM3,2. The ﬁrst four vertices of H+ 3 , u1, . . . , u4, are mixtures of two point measures and one uniform distribution on a pair of Hamming distance one. The last four vertices, u5, . . . , u8, are the point measures on Z+,3. By [22, Theorem 1], all these distributions are contained in RBM3,2 − (by symmetry, the vertices of H 3 are also in RBM3,2). The distributions in the relative interiors of the edges between the ﬁrst four vertices are not in RBM3,2. The relative interior  128 = 0.0078125. This is a  18  of edges connecting one of the ﬁrst four and one of the last four vertices are in RBM3,2 if they have support of cardinality four and are not if they have support of cardinality ﬁve. We conjecture that RBM3,2 ∩G3 = ∅.  3.6 Linear threshold codes  Deﬁnition 3.22. A linear threshold function (LTF) with m inputs is a function  f : {0, 1}m → {−, +};  y (cid:55)→ sgn((  wjyj) + b),  (cid:88)  j∈[m]  where w ∈ Rm is called weight vector and b ∈ R bias. A subset C ⊂ {0, 1}m ⊂ Rm is linearly separable iﬀ there exists an LTF with f (C) = + and f ({0, 1}m \C) = −. For convenience we identify −/+ and 0/1 vectors via − ↔ 0 and + ↔ 1. The opposite x of a binary vector x is the vector given by inverting all entries of x.  LTFs are also known as McCulloch-Pitts neurons and have been studied in the context of feed-forward artiﬁcial neural networks. The problem of separating subsets of vertices of the m-dimensional hypercube by hyperplane arrangements (multi-label classiﬁcation) has drawn much attention, see, e.g., [33]. It is known that the logarithm of the number of LTFs with m inputs is asymptotically of order m2, see [37, 25], but the exact number is only known for m ≤ 9, see [34, 23, 24]. The study of LTFs simpliﬁes when f (x1, . . . , xm) = f (x1, . . . , xm) for all x ∈ {0, 1}m, in which case they are called self-dual. If an LTF has an equal number of positive and negative points, then it separates every input from its opposite and is self-dual.  Deﬁnition 3.23. A subset C ⊆ {0, 1}n ∼= {−, +}n is an (n, m)-linear threshold code (LTC) if there exist n linear threshold functions fi : {0, 1}m → {0, 1}, i ∈ [n] with  {(f1(y), f2(y), . . . , fn(y)) ∈ {0, 1}n : y ∈ {0, 1}m} = C.  Equivalently, C is an (n, m)-LTC if it is the image of the down inference function of RBMn,m for some choice of the model parameters. If all fi can be chosen self-dual, then C is called homogeneous.  1 +(cid:80)  In the following examples, an LTF with m inputs is written as a list of the vertices of the m-cube with a bar on inputs with negative output and no bar on inputs with positive output. For notational convenience, each vertex x = (x1, . . . , xm) ∈ {0, 1}m is labeled by j∈[m] 2j−1xj, which is the decimal representation of the binary vector plus one. For example, an LTF with two inputs, mapping (00), (01) to 0 and (10), (11) to 1, is written as 1234.  Example 3.24. Let n = 3 and m = 2. There are only two ways to linearly separate the vertices of the unit square into sets of cardinality two: 1234 and 1234. These are the only possible columns of a homogeneous LTC with two inputs (up to opposites). The code Z±,3 is not a (3, 2)-LTC, because it has three non-equivalent columns. This shows that there does not exist a 2-zonoset with vertices in the four even, or odd, orthants of R3, and that RBM3,2 does not contain any distributions with four strong modes.  19  Figure 6: The four slicings of the 3-cube discussed in Example 3.25.  An alternative way of proving this is as follows. The Hamming distance between any two elements of Z±,n is even. If the distance of any two vertices of the square induced by an arrangement of three lines is even and non-zero, then each edge of the square is sliced at least twice, and in total at least 8 edges are sliced (repetitions allowed). On the other hand, each line slices at most two edges of the square, and so three lines slice at most 6 edges (repetitions allowed).  Example 3.25. Let n = 4 and m = 3. There are 104 ways to linearly separate the vertices of the 3-cube, see [25]. A complete list appears in [3, Section 3.8]. The vertices of the 3-cube are in the Z+,4-cells of an arrangement of four hyperplanes corresponding to the (4, 3)-LTC with following LTFs:  12345678, 12345678, 12345678, 12345678.  This arrangement corresponds to a 3-zonoset with points in the 8 even orthants of R4 (The- orem 1.6). The zonoset can be realized as follows:   −1 −1 −1 (cid:0)3 1 1 1(cid:1) ;  −1 −1 −1  1 1 −1 1 −1 −1  1 2  w =  b =   ;  1 2  Z =    1  1 1 3 3 −1 −1 1 1 −1 3 −1 1 −1 1 −3 3 1 −1 −1 −1 1 1 −3 −1 −3 1 1 −3 −1 −1 −1   .  (5)  This choice of w and b corresponds to a central arrangement of four hyperplanes slicing each edge of the 3-cube exactly twice, as shown in Figure 6.  20  Example 3.26. Let n = 5 and m = 4. There are three symmetry types of self-dual LTFs with four inputs, see [23]. The following are representatives of the three types:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 .  By Proposition 3.19, the code Z±,5 cannot be realized by any 5 LTFs, i.e., as a (5, 4)-LTC, and RBM5,4 does not contain any distribution with 16 strong modes.  The following example presents a kind of binary code C of cardinality 2m with RBMn,m ∩HC =  ∅ which is not covered by Corollary 3.21. Example 3.27. Let n = 5 and m = 3. Let x(cid:48), x(cid:48)(cid:48)  ∈ Z±,4 with dH(x(cid:48), x(cid:48)(cid:48)) = 4, and  if (x1, . . . , x4) ∈ {x(cid:48), x(cid:48)(cid:48)  otherwise  }  (cid:111)  .  (cid:40)  1 0  C =  (x1, . . . , x5) : (x1, . . . , x4) ∈ Z±,4, x5 =  (cid:110)  If C is an LTC, some hyperplane separates two vertices of the 3-cube from the other vertices (corresponding to x5 = 1 only for two points). These two vertices must be connected by an edge of the 3-cube. Since dH(x(cid:48), x(cid:48)(cid:48)) = 4, four hyperplanes pass through this edge. There are only three diﬀerent central hyperplanes through an edge of the 3-cube, but four diﬀerent central hyperplanes are required to produce Z±,4. Hence C is not a (5, 3)-LTC.  3.7 Multi-covering numbers of hypercubes  The previous section shows that the sets of strong modes realizable by RBMn,m are related to the solution of the following problem:  Problem 3.28. Let m ≤ n. Consider an m-zonoset Z in Rn which does not intersect any two orthants separated by a single coordinate hyperplane. How many orthants of Rn does Z intersect at most?  As we discuss in the following, this problem is related to the long standing problem of  computing the covering numbers of hypercubes.  Deﬁnition 3.29. The covering number of a hypercube is the smallest number of hyperplanes that slice each edge of the hypercube at least once. An edge is sliced by a hyperplane if the hyperplane intersects the relative interior of the edge and does not contain any vertices of the hypercube. A cut is the collection of all edges sliced by a hyperplane and corresponds to a linear threshold function.  The m hyperplanes with normal vectors equal to the standard basis of Rm passing through the center of the m-dimensional hypercube slice all its edges. This arrangement is not always optimal. Paterson found 5 hyperplanes slicing all edges of the 6-cube, see [29]. This shows that covering numbers do not behave trivially. The covering numbers are known only for hypercubes of dimension ≤ 6. Computing them in higher dimensions is challenging, even in the cases where all cuts are known. Now:  21  Proposition 3.30. If Z+,n is an (n, n − 1)-LTC, then there exists an arrangement of n hyperplanes through the center of the (n− 1)-cube slicing each edge an even non-zero number of times. Proof. Given the assumption, there exists an arrangement of n hyperplanes in Rn−1 such that each vertex of the (n−1)-cube is in one of the Z+,n-cells of the arrangement. Each vertex is separated by an even, positive number of hyperplanes from any other vertex, since any two elements of Z+,n diﬀer in an even number of entries. Two vertices cannot be contained  in the same cell of the arrangement, since |Z+,n| = 2n−1 equals the total number of vertices of the (n − 1)-cube. The code Z+,n is homogeneous, as each coordinate i ∈ [n] has the same number of zeros and ones, and hence each hyperplane in the arrangement can be chosen through the center of the cube.  Proposition 3.30 motivates the following problem:  Problem 3.31 (Multi-covering number). What is the smallest arrangement of hyperplanes, if one exists, that slices each edge of a hypercube a given number of times?  Of particular interest is the number of hyperplanes needed to slice each edge of the m- cube an even non-zero number of times. The edges of the m-cube can be sliced exactly twice by 2m hyperplanes with normal vectors equal to the standard basis vectors of Rm, counted with multiplicity two. Proposition 3.19 shows that if m is even and larger than zero, there is no arrangement of (m + 1) hyperplanes for which each vertex of the m-cube lies in a diﬀerent cell and any two vertices are separated by an even number of hyperplanes. This suggests that when m is even, there is no arrangement of (m + 1) hyperplanes slicing all edges of the m-cube exactly twice; at least not one for which each vertex lies in a diﬀerent cell.  There is exactly one way to slice all edges of the 3-cube an even non-zero number of times by four hyperplanes, namely the way illustrated in Figure 6. To see that this is the only way, note that the 3-cube has 12 edges and that there are only 4 diﬀerent cuts that slice 6 edges. The 4-cube has 16 vertices, 32 edges, a total of 940 diﬀerent cuts, 3 symmetry classes of central cuts, and 52 diﬀerent central cuts. The maximal number of edges sliced by a cut is 12. Hence:  Proposition 3.32. There is no arrangement of ﬁve hyperplanes, or less, slicing each edge of the four-dimensional cube at least twice.  The complexity of the next easiest example is considerable. We tested all combinations  of six cuts of the 5-cube and found:  Computation 3.33. There is no arrangement of six, or less, central hyperplanes slicing each edge of the ﬁve-dimensional cube an even non-zero number of times.  In the following we explain some details of the computation. The 5-cube has 80 edges. There are 47 285 diﬀerent ways of slicing them with aﬃne hyperplanes, see [9]. A cut is given by the indicator function on the set of edges sliced. A list of the cuts can be found in [36]. An edge of the m-cube corresponds to a pair of binary vectors of length m which diﬀer in exactly one entry. Each edge is parallel to one coordinate vector of Rm. The edges can be organized in m groups, corresponding to their directions. Within each group, the edges  22  are naturally enumerated by the binary vectors of length (m − 1) containing the coordinate values that are equal for the two vertices of each edge. The central cuts can be characterized as the cuts which involve only pairs of opposite edges. The 5-cube allows 7 symmetry classes of central cuts and 941 diﬀerent central cuts. For each choice of 6, or less, central cuts we computed the entry-wise addition of the indicator functions and found that this never produced an even non-zero value in each entry. On the other hand, 5 is the covering number of the 5-cube, see [9], and hence at least 6 hyperplanes are needed to slice each edge twice. As a consequence of Computation 3.33 we have:  Corollary 3.34. The model RBM6,5 cannot represent any probability distribution with 32 strong modes.  Indeed, we trained RBM6,5 to approximate the uniform probability distribution on Z+,6 and found a Kullback-Leibler divergence minimum of 0.6309 (with base-two logarithm), which is a relatively large value. For this computation we used contrastive divergence [14] and likelihood gradient with numerous parameter initializations.  3.8 Proof of Theorem 1.6  The equivalence theorem from the introduction (illustrated in Figure 3) is a summary of observations from the previous subsections. For completeness we provide a proof. Recall the deﬁnition of LTC, PR, HP, ZP, SM, and SP given in Deﬁnition 1.5. Let n and m be two integers and C ⊆ {0, 1}n.  1. The properties LTC, HP, and ZP are equivalent.  Let W and B be parameters making C a linear threshold code, so that C = {sgn(h(cid:62)W + B) : h ∈ {0, 1}m}. Let Wi, i = 1, . . . , n be the columns of W . The sign of h(cid:62)Wi + Bi, h ∈ {0, 1}m indicates which side of hyperplane Hi in the arrangement AW,B this h lies on. Dually, the sign of h(cid:62)Wi +Bi indicates which side of the i-th coordinate hyperplane in Rn the point h(cid:62)W + B of the zonoset lies on.  2. If C satisﬁes PR or SM, then it is contained in an LTC set.  For C to be the perfectly reconstructible, in particular it must be a subset of the image of a down inference function. If C has the SM property, its vectors are at least Hamming distance 2 apart. By Theorem 3.7, each point in C is the unique maximizer of a conditional distribution p(·|h) and an image point of the down inference function.  3. If the vectors in C are at least Hamming distance 2 apart, then SP implies both SM  and PR. SP with Hamming distance two implies that there is a distribution p ∈ RBMn,m with p(v) > 0 for each v ∈ C, and p(v(cid:48)) = 0 for each neighbor v(cid:48) of each v ∈ C. Therefore, each element of C is a strong mode of p, and SM. Writing pθ(v, h) as a matrix with rows labeled by h, the Hamming distance two condition implies by Theorem 3.7 that each row has a single non-zero entry, so downθ ◦ upθ is the identity on C, so PR holds.  23  4. If the vectors in C are at least Hamming distance 2 apart and C satisﬁes an (cid:96)1 property,  then LTC implies SP.  This is by Theorem 3.16.  4 Relative representational power  4.1 When does a mixture of products contain an RBM?  Using the characterizations obtained in Sections 3.3 and 3.4, we now prove the result on relative representational power discussed in the introduction and depicted in Figure 2. To do this, we derive upper bounds for the smallest m such that RBMn,m contains probability distributions with l strong modes, and show thereby that RBMs can represent many more modes than mixtures of products with the same number of parameters.  Any representability result, as Example 3.25, combined with the following observation,  yields lower bounds on the smallest mixture of products which contains the RBM model.  Observation 4.1. Let k ∈ N. Assume that for each i ∈ [k] there is a matrix W (i) ∈ Rmi×ni and a vector B(i) ∈ Rni which generate a zonoset {h(cid:62)W (i) + B(i) : h ∈ {0, 1}mi} intersecting Ki even orthants of Rni. Then  W (1)   and B = (B(1), . . . , B(k)) generate a zonoset {h(cid:62)W + B : h ∈ {0, 1}m1+···+mk} intersecting(cid:81)  i∈[k] Ki even orthants of  W (k)  W =  . . .  Rn1+···+nk.  The following theorem provides the justiﬁcation for the statement in the introduction that “the number of parameters of the smallest mixture of products model containing an RBM model grows exponentially in the number of parameters of the RBM for any ﬁxed ratio 0 < m/n <∞,” and for Figure 2. Theorem 4.2. Let n, m ∈ N.  • If 4(cid:100)m/3(cid:101) ≤ n, then RBMn,m ∩Hn,2m (cid:54)= ∅ and  Mn,k ⊇ RBMn,m iﬀ k ≥ 2m.  • If 4(cid:100)m/3(cid:101) > n, then RBMn,m ∩Hn,L (cid:54)= ∅ and  where L := min{2l + m − l, 2n−1}, l := max{l ∈ N : 4(cid:100)l/3(cid:101) ≤ n}.  Mn,k ⊇ RBMn,m only if k ≥ L,  Proof. Let 4(cid:100)m/3(cid:101) ≤ n. The if direction follows from RBMn,m ⊆ Mn,2m for all n and m. For the only if direction we show that RBMn,m contains a probability distribution supported on a set of cardinality 2m and minimum Hamming distance at least two (a distribution with  24  ;    W = α  2m strong modes). By Theorem 3.7 such a distribution is in Mn,k only if k ≥ 2m. Consider the following parameters:  w  w  . . .  0  w  ˜w  B = α (b, b, . . . , b,−1, . . . ,−1) ; b = 1  2(3, 1, 1, 1);  C = −W 1  2(1, . . . , 1)(cid:62) = α(1, . . . , 1)(cid:62),  of cardinality 2m: {v ∈ {0, 1}n : (cid:80)  where α ∈ R is a constant, w is the 3 × 4-matrix deﬁned in eq. (5), ˜w consists of the ﬁrst or the ﬁrst two rows of w, and B is α times (cid:100)m/3(cid:101) copies of b followed by −1s. Let λi be the set of indices {1, 2, 3, 4} + 4(i − 1) ⊂ [n]. For α → ∞ the visible distribution generated by RBMn,m with parameters W, B, and C is the uniform distribution on following subset of Z+,n vj is even for all i, and vj = 0 for all j > 4(cid:100)m/3(cid:101)}. Now let 4(cid:100)m/3(cid:101) ≥ n. By the ﬁrst part, RBMn,l contains some p with 2l strong modes in Z+,n. Moreover, RBMn,l+1 contains µp + (1 − µ)δx for any p ∈ RBMn,l, x ∈ {0, 1}n and µ ∈ [0, 1] (see [17]), such that each additional hidden unit can be used to increase the number of strong modes by one, until the set of strong modes is Z+,n.  j∈λi  Remark 4.3. The statement of the ﬁrst item of Theorem 4.2 remains true if m = 1 mod (3)  and 4(cid:98)m/3(cid:99) + 2 ≤ n. For n < 3 we have Mn,k = RBMn,k−1 for any k ∈ N. For n = 3 we believe that M3,3 and RBM3,2 are very similar, if not equal.  4.2 When does an RBM contain a mixture of products?  Complementary to question of when a mixture of products contains a product of mixtures, in this section we ask what is the smallest m for which RBMn,m contains Mn,k. We focus on an instance which we ﬁnd particularly interesting:  Problem 4.4. Does RBMn,m contain the mixture of products Mn,m+1?  Both RBMn,m and Mn,m+1 have nm+n+m parameters and expected dimension min{nm+ n + m, 2n − 1}. The expected dimension is also the true dimension of both models for most choices of n and m [4, 6]. In the following we give a negative answer to Problem 4.4. In the previous section we showed that the non-negative rank of probability distribu- tions in the model RBMn,m is as large as 2m; there are tables of probabilities (probability distributions) represented by the RBM model, which cannot be represented as non-negative sums of less than 2m non-negative rank-one tables (product distributions). The rank of a table p is the smallest number k such that p can be written as a sum of k rank-one tables. Here, a multivariate probability distribution p = p(x1, . . . , xn) with xi ∈ Xi, |Xi| = ri for i = 1, . . . , n is expressed as an n-way r1 × ··· × rn table with value p(x1, . . . , xn) at the entry (x1, . . . , xn). A rank-one table is an outer-product of n vectors of lengths r1, . . . , rn. A product distribution in Mn,1 is the outer-product of the marginal distributions on the variables x1 through xn and is a non-negative rank-one table. By deﬁnition, the elements of Mn,k have non-negative rank at most k, and therefore also rank at most k. Since RBMn,m is contained in Mn,2m, any p ∈ RBMn,m has rank at most 2m.  25  Two models A and B are called generically distinguishable if A ∩ B has relative measure zero in A and in B. The restriction “generically” is useful, because in most cases of interest the models do intersect (e.g., mixtures of products and RBMs contain the uniform distribu- tion). A ﬂattening of a table of probabilities is a way of arranging its entries in a two-way table (i.e., a matrix) by grouping the variables in two groups and considering the joint states of the variables in each of the groups as the states of two variables. The following is an example of a ﬂattening of a table p for four binary variables:  p00,00 p00,01 p00,10 p00,11  p01,00 p01,01 p01,10 p01,11 p10,00 p10,01 p10,10 p10,11 p11,00 p11,01 p11,10 p11,11   .  p =  The matrix rank of any ﬂattening of a table p is upper bounded by the outer-product rank of p. In particular, the vanishing of the (k + 1) × (k + 1)-minors of ﬂattenings are algebraic invariants of the model Mn,k. Theorem 4.5. If m ≤ n/2, then the model RBMn,m contains points of rank 2m. If, further- more, m+1 (cid:54)= 3 or n (cid:54)= 4, then the models RBMn,m and Mn,m+1 have dimension nm+n+m and intersect at a set of dimension strictly less than nm + n + m. Proof. We show that if m ≤ n/2, then RBMn,m contains a point p with a ﬂattening of rank 2m, which implies that p has outer-product rank 2m. The ﬂattenings of any q ∈ Mn,k have rank at most k. This gives an algebraic invariant of the mixture of products model Mn,m+1 which is not satisﬁed by elements of RBMn,m. Hence, if both models have the same dimension d, then they intersect at a set of dimension strictly less than d. Consider the m-cube and the 2m hyperplanes through its center consisting of translates of the coordinate hyperplanes with multiplicity two. This hyperplane arrangement slices each edge of the m-cube exactly twice and generates a (2m, m)-LTC C of minimum distance two. The code C consists of the 2m binary vectors x in {0, 1}2m with xi = xi+1 for all odd i, and {(x1, x3, . . . , x2m−1) : x ∈ C} = {0, 1}m. In the case m = 3, for example, the code is  By Theorem 3.16, RBM2m,m contains the uniform distribution on C, uC. View uC as a linear transformation from the 2m-dimensional space of real valued functions of x1, x3, . . . , x2m−1 to the space of functions of x2, x4, . . . , x2m. Then  C =   1/2m  uC =   .  0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1   ,  1/2m  . . .  26  1/2m  which has rank 2m.  5 Discussion  RBMs create a multi-labeling of their input space by the most likely joint states of their hidden units given the inputs. The number of inference regions that can be generated in this way is of exponential order in the number of RBM parameters. The partitions of Rn generated by an RBM with n visible and m hidden units can be identiﬁed with the intersections of aﬃne spaces of dimension d ≤ min{n, m} with the orthants of Rm,  whereby each aﬃne space corresponds to a choice of the RBM parameters. We elaborated on the combinatorics of the resulting hyperplane arrangements, and on the combinatorics of point conﬁgurations in such hyperplane arrangements, in correspondence with the inference  functions on the set of binary input vectors {0, 1}n ⊂ Rn. Although the theory of hyperplane  arrangements and linear separation of points is well studied in the literature, it still poses many questions (see examples below).  We analyzed the sets of strong modes of probability distributions represented by RBMs and related them to the hyperplane arrangements and linear threshold codes (multi-labelings) mentioned above. The products of mixtures represented by RBMs are compact representa-  RBM with n visible and m hidden units (exponential in the number of parameters). At the  tions of probability distributions with many strong modes; of order min{2m, 2n−1} for the same time, Corollaries 3.21 and 3.34 show that the hard bound min{2m, 2n−1} is not always  attained. Mixture models of product distributions (na¨ıve Bayes models), on the other hand, generate less restricted input space partitions but into at most as many regions as mixture components, and can only represent probability distributions with a number of strong modes of linear order in the number of model parameters.  These results imply that the smallest mixture model of product distributions that con- tains an RBM model is, in most cases, as large as one can possibly expect, having one mixture component per joint state of the RBM hidden units, and thus a number of parameters that is exponential in the number of RBM parameters. RBMs can represent distributions with many strong modes much more compactly than standard mixture models. This gives a con- cise combinatorial way of diﬀerentiating the two models. Fixing dimension, the RBMs which are hardest to represent as mixtures of product distributions are those with about the same number of visible and hidden units. At the same time, we note that there may exist small mixtures of product distributions which cannot be compactly represented by RBMs. For instance, Theorem 4.5 shows that Mn,m+1 (cid:54)⊆ RBMn,m when 3 ≤ m ≤ n/2. These results aid our understanding of how models complement each other, and why distributed representations in deep learning [2] can be expected to succeed, or when model selection can be based on theory rather than trial-and-error. They conﬁrm the intuition that distributed representations are exponentially more powerful than non-distributed ones, in the case of binary RBMs and taking the number of strong modes, inference functions, and non-trivial perfectly reconstructible input sets as a measure of complexity. Other measures of complexity of probability distributions, such as multi-information, which is deﬁned as the Kullback-Leibler divergence to the set of product distributions, are interesting but not necessarily best for diﬀerentiating between mixtures of products and RBMs. In terms of  27  multi-information the most complex binary probability distributions have the form p = 1 2(δx + δy) with xi + yi = 1 for all i, see [1], and are contained in any (non-trivial) mixtures of products and RBM models.  Our approach has produced at least an order-of-magnitude or asymptotic understanding of the models we discuss. We have also shown that to understand them fully would proba- bly mean understanding as well some seemingly diﬃcult equivalent combinatorial problems concerning linear threshold codes, hyperplane arrangements, multi-covering numbers, and so on. On the other hand, we expect that our techniques can be used eﬀectively to study more complex models in a similar fashion.  A number of problems is covered only partially by our analysis. Some interesting open  cases include:  • Computing multi-covering numbers for hypercubes of odd dimension larger than ﬁve. • Characterizing the support sets of fully observable RBM models. This problem can be seen to be equivalent to characterizing the face lattices of polytopes deﬁned as Kronecker products of hypercubes.  • Computing the maximal cardinality of linear threshold codes of minimum Hamming distance two. Are there cases where the ﬁrst item of Theorem 4.2 holds for 4(cid:100)m/3(cid:101) > n, m ≤ n − 1, assuming m (cid:54)= n − 1 when n is odd?  • Can RBM8,7 represent probability distributions with 27 strong modes? • Verifying the conjecture that RBM3,2 ∩G3 = ∅, and, in addition, proving or disproving M3,3 = RBM3,2. • For 2 < m < 2n−1 − 1, does RBMn,m contain Mn,m?  Acknowledgments  The authors are grateful to Johannes Rauh for helpful discussions on algebraic invariants, to Nihat Ay for comments on hyperplane arrangements, to Yoshua Bengio for helpful discussions regarding distributed representations, and to Bernd Sturmfels for discussions motivating the analysis of mixtures of products in RBMs. The authors acknowledge use of the RCC-ITS computer cluster at the Pennsylvania State University. This work is supported in part by DARPA grant FA8650-11-1-7145.  References  [1] N. Ay and A. Knauf. Maximizing multi-information. Kybernetika, 42:517–538, 2006.  [2] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine  Learning, 2(1):1–127, 2009.  [3] A. Bj¨orner, M. L. Vergnas, B. Sturmfels, N. White, and G. M. Ziegler. Oriented Ma- troids, volume 46 of Encyclopedia of Mathematics and Its Applications. Cambridge University Press, 1999.  28  [4] M. V. Catalisano, A. V. Geramita, and A. Gimigliano. Secant varieties of P1 × ··· × P1  (n-times) are not defective for n ≥ 5. J. Algebraic Geometry, 20:295–327, 2011.  [5] T. M. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE Transactions on Electronic Computers, EC-14(3):326–334, 1965.  [6] M. A. Cueto, J. Morton, and B. Sturmfels. Geometry of the restricted Boltzmann machine. In M. A. G. Viana and H. P. Wynn, editors, Algebraic methods in statistics and probability II, AMS Special Session, volume 2. American Mathematical Society, 2010.  [7] M. A. Cueto, E. A. Tobis, and J. Yu. An implicitization challenge for binary factor  analysis. J. Symbolic Computation, 45:1296–1315, December 2010.  [8] J. Draisma. A tropical approach to secant dimensions. Journal of Pure Applied Algebra,  212(2):349–363, 2008.  [9] M. R. Emamy-Khansary and M. Ziegler. On the coverings of the d-cube for d ≤ 6.  Discrete Applied Mathematics, 156(17):3156–3165, 2008.  [10] L. Flatto. A new proof of the transposition theorem. Proceedings of the American  Mathematical Society, 24(1):29–31, 1970.  [11] E. Gawrilow and M. Joswig. Polymake: a framework for analyzing convex polytopes.  In Polytopes – Combinatorics and Computation, pages 43–74. Birkh¨auser, 2000.  [12] D. Geiger, C. Meek, and B. Sturmfels. On the toric algebra of graphical models. Annals  of Statistics, 34:1463–1492, 2006.  [13] G. E. Hinton. Products of experts. In Proceedings of the 9-th ICANN, volume 1, pages  1–6, 1999.  [14] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural  Computation, 14:1771–1800, 2002.  [15] T. Kahle, W. Wenzel, and N. Ay. Hierarchical models, marginal polytopes, and linear  codes. Kybernetika, 45:189–208, 2009.  [16] D. E. Knuth. Big omicron and big omega and big theta. SIGACT News, 8(2):18–24,  1976.  [17] N. Le Roux and Y. Bengio. Representational power of restricted Boltzmann machines  and deep belief networks. Neural Computation, 20(6):1631–1649, 2008.  [18] G. Mont´ufar. Mixture decompositions of exponential families using a decomposition of  their sample spaces. Kybernetika, 49(1):23–39, 2013.  29  [19] G. Mont´ufar and N. Ay. Reﬁnements of universal approximation results for deep belief networks and restricted Boltzmann machines. Neural Computation, 23(5):1306–1319, 2011.  [20] G. Mont´ufar and J. Morton. Discrete restricted Boltzmann machines. In International Conference on Learning Representations 2013 (ICLR 2013), Scottsdale, Arizona, USA, 2013.  [21] G. Mont´ufar and J. Morton. Dimension of hidden-visible products of exponential fam-  ilies. Unpublished manuscript, 2014.  [22] G. Mont´ufar, J. Rauh, and N. Ay. Expressive power and approximation errors of  restricted Boltzmann machines. In NIPS 24, pages 415–423, 2011.  [23] S. Muroga, T. Tsuboi, and C. Baugh. Enumeration of threshold functions of eight  variables. IEEE Transactions on Computers, C-19(9):818–825, sept. 1970.  [24] OEIS. The on-line encyclopedia of integer sequences, A000609 Number of threshold functions of n or fewer variables, 2010. Published electronically at http://oeis.org, 2010.  [25] P. C. Ojha. Enumeration of linear threshold functions from the lattice of hyperplane  intersections. IEEE Transactions on Neural Networks, 11(4):839–850, jul 2000.  [26] P. Orlik and H. Terao. Arrangements of Hyperplanes. Springer-Verlag, 1992.  [27] J. Rauh, T. Kahle, and N. Ay. Support sets of exponential families and oriented ma-  troids. International Journal of Approximate Reasoning, 52(5):613–626, 2011.  [28] S. Ray and B. G. Lindsay. The topography of multivariate normal mixtures. Annals of  Statistics, 33(5):2042–2065, 2005.  [29] M. E. Saks. Slicing the hypercube.  In K. Walker, editor, Surveys in combinatorics,  pages 211–255. Cambridge University Press, New York, NY, USA, 1993.  [30] L. Schl¨aﬂi. Theorie der vielfachen Kontinuit¨at. Cornell University Library historical  math monographs. George & Company, 1901.  [31] L. Schl¨aﬂi. Gesammelte mathematische Abhandlungen. Number v. 2 in Gesammelte  mathematische Abhandlungen. Birkh¨auser, 1953.  [32] R. Stanley. An introduction to hyperplane arrangements. In Lecture notes, IAS/Park  City Mathematics Institute, 2004.  [33] W. Wenzel, N. Ay, and F. Pasemann. Hyperplane arrangements separating arbitrary  vertex classes in n-cubes. Advances in Applied Mathematics, 25(3):284–306, 2000.  [34] R. O. Winder. Enumeration of seven-argument threshold functions. IEEE Transactions on Electronic Computers, EC-14(3):315–325, 1965. See also correction in EC-16(2):231, 1967.  30  [35] G. M. Ziegler. Lectures on polytopes. Graduate texts in mathematics. Springer-Verlag,  1995.  [36] M. Ziegler. The cut number home page. http://www2.cs.uni-paderborn.de/cs/  ag-madh/WWW/CUBECUTS/.  [37] Y. A. Zuev. Asymptotics of the logarithm of the number of threshold functions of the  algebra of logic. Soviet Mathematics Doklady, 39(3):512–513, 1989.  0 0 1 0  0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1/4 0 0 0 1/4 1/4 1/4 0 0 1/4 0 0 1/4 1/4 1/4 0 0 0 1/4 0 1/4 1/4 1/4 0 0 0 0 1/4 1/4 1/4 1/4 0 1/6 0 1/6 1/6 1/6 1/6 1/6 0 0 1/6 0 1/6 1/6 1/6 1/6 1/6 1/6 1/6 1/6 1/6 1/6 1/6 0 0 0 0 1/6 1/6 1/6 1/6 1/6 1/6 0 0 1/6 1/6 1/6 1/6 1/6 1/6 1/6 1/6 0 1/6 1/6 1/6 1/6 0 1/7 1/7 1/7 1/7 0 1/7 1/7 1/7 1/7 1/7 1/7 1/7 1/7 1/7 1/7 0 1/7 1/7 1/7 1/7 1/7 1/7 1/7 0 1/7 1/7 1/7 1/7 1/7 1/7 1/7 0 1/8 1/8 1/8 1/8 1/8 1/8 1/8 1/8  (000)  (011)  (101)  (110)  (001)  (010)  (100)  (111)  the polytope G+  3 ⊂ P3 ⊂ R8 Table 1: Vertex-presentation of (the set of probability distributions on {0, 1}3 with modes Z+,3 = {(000), (011), (101), (110)}). Each row is a probability distribution that is a vertex of G+ 3 . The vertices in the ﬁrst group are the point measures on Z+,3. They have degree 18 (i.e., they are incident to 18 edges) and are connected by edges to all other vertices. The vertices in the second and fourth groups have degree 11. There are no edges between pairs of vertices in the second group. The vertices in the third group have degree 8. The uniform distri- bution has degree 12. The f -vector of the polytope, indicating the number + of faces in each dimension, is f (G 3 ) = (19, 110, 290, 387, 270, 96, 16). The volume is vol(G+ 3 )/ vol(P3) = 1/56.  31  7  2 3 4 5 2 3 4 5 2 3 4 5 6 6 6 7  3 4 3 4 3 4 5 6  1 1 1 1 2 1 2 1 2 1 2 3 1 2 3 1 2 3 1 2 3 4 1 2 3 4 5 6 7 1 2 3 4 5 6 1 2 3 4 5  4 4 4 5  6  5  8  8  9  10  10  11  9 9 10 11  12 13 12  14 15  13 14 15 16  12 13 12  14 15  15 16 17  7 8  10  12 13  6 7 7 7 8 9  11 9 10 11  8 11 8 9 10 11  6 7 8  9  11 10 11  8  7 8 9 10  13 14 15 16  15 16 17  12  14 15  13 14 15 16  15 16 17  12 13 14  18  14 15  13  12  16  17  16 17 18 19 17 18 19 18 19 16 17 18 19 17 18 19 19 16 17 18 19 18 19 19 17 18 19 18 19 19  Table 2: Vertex-facet incidence table for the polytope G+ 3 . Each row gives the list of vertices incident to one facet of G+ 3 (the index of each vertex corresponds to the row in which it appears in Table 1). The set of vertices in any face of the polytope is an intersection of some of the 16 sets listed above.  32  ","We derive relations between theoretical properties of restricted Boltzmannmachines (RBMs), popular machine learning models which form the building blocksof deep learning models, and several natural notions from discrete mathematicsand convex geometry. We give implications and equivalences relatingRBM-representable probability distributions, perfectly reconstructible inputs,Hamming modes, zonotopes and zonosets, point configurations in hyperplanearrangements, linear threshold codes, and multi-covering numbers of hypercubes.As a motivating application, we prove results on the relative representationalpower of mixtures of product distributions and products of mixtures of pairs ofproduct distributions (RBMs) that formally justify widely held intuitions aboutdistributed representations. In particular, we show that a mixture of productsrequiring an exponentially larger number of parameters is needed to representthe probability distributions which can be obtained as products of mixtures."
